<R> <C> Model <C> Training data <C> Overall <C> Easy <C> Hard <R> <C> BERT-large-FT <C> B-COPA <C> 74.5 (± 0.7) <C> 74.7 (± 0.4) <C> [BOLD] 74.4 (± 0.9) <R> <C> BERT-large-FT <C> B-COPA (50%) <C> 74.3 (± 2.2) <C> 76.8 (± 1.9) <C> 72.8 (± 3.1) <R> <C> BERT-large-FT <C> COPA <C> [BOLD] 76.5 (± 2.7) <C> [BOLD] 83.9 (± 4.4) <C> 71.9 (± 2.5) <R> <C> RoBERTa-large-FT <C> B-COPA <C> [BOLD] 89.0 (± 0.3) <C> 88.9 (± 2.1) <C> [BOLD] 89.0 (± 0.8) <R> <C> RoBERTa-large-FT <C> B-COPA (50%) <C> 86.1 (± 2.2) <C> 87.4 (± 1.1) <C> 85.4 (± 2.9) <R> <C> RoBERTa-large-FT <C> COPA <C> 87.7 (± 0.9) <C> [BOLD] 91.6 (± 1.1) <C> 85.3 (± 2.0) <CAP> Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.
<R> <C> Model <C> Accuracy <R> <C> BigramPMI Goodwin et al. ( 2012 ) <C> 63.4 <R> <C> PMI Gordon et al. ( 2011 ) <C> 65.4 <R> <C> PMI+Connectives Luo et al. ( 2016 ) <C> 70.2 <R> <C> PMI+Con.+Phrase Sasaki et al. ( 2017 ) <C> 71.4 <R> <C> BERT-large Wang et al. ( 2019 ) <C> 70.5 <R> <C> BERT-large Sap et al. ( 2019 ) <C> 75.0 <R> <C> BERT-large Li et al. ( 2019 ) <C> 75.4 <R> <C> RoBERTa-large (finetuned) <C> 90.6 <R> <C> BERT-large (finetuned)* <C> 76.5 ± 2.7 <R> <C> RoBERTa-large (finetuned)* <C> 87.7 ± 0.9 <CAP> Table 1: Reported results on COPA. With the exception of Wang et al. (2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See §2 for model details. * indicates our replication experiments.
<R> <C> Cue <C> App. <C> Prod. <C> Cov. <R> <C> in <C> 47 <C> 55.3 <C> 9.40 <R> <C> was <C> 55 <C> 61.8 <C> 11.0 <R> <C> to <C> 82 <C> 40.2 <C> 16.4 <R> <C> the <C> 85 <C> 38.8 <C> 17.0 <R> <C> a <C> 106 <C> 57.5 <C> 21.2 <CAP> Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.
<R> <C> Dataset <C> Accuracy <C> Fleiss’ kappa  [ITALIC] k <R> <C> Original COPA <C> 100.0 <C> 0.973 <R> <C> Balanced COPA <C> 97.0 <C> 0.798 <CAP> Table 3: Results of human performance evaluation of the original COPA and Balanced COPA.
<R> <C> Model <C> Method <C> Training Data <C> Overall <C> Easy <C> Hard <C> p-value (%) <R> <C> goodwin-etal-2012-utdhlt <C> PMI <C> unsupervised <C> 61.8 <C> 64.7 <C> 60.0 <C> 19.8 <R> <C> gordon_commonsense_2011-1 <C> PMI <C> unsupervised <C> 65.4 <C> 65.8 <C> 65.2 <C> 83.5 <R> <C> sasaki-etal-2017-handling <C> PMI <C> unsupervised <C> 71.4 <C> 75.3 <C> 69.0 <C> 4.8∗ <R> <C> Word frequency <C> wordfreq <C> COPA <C> 53.5 <C> 57.4 <C> 51.3 <C> 9.8 <R> <C> BERT-large-FT <C> LM, NSP <C> COPA <C> 76.5 (± 2.7) <C> 83.9 (± 4.4) <C> 71.9 (± 2.5) <C> 0.0∗ <R> <C> RoBERTa-large-FT <C> LM <C> COPA <C> 87.7 (± 0.9) <C> 91.6 (± 1.1) <C> 85.3 (± 2.0) <C> 0.0∗ <CAP> Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).
<R> <C> Model <C> Training data <C> Overall <C> Easy <C> Hard <R> <C> BERT-large <C> B-COPA <C> 70.5 (± 2.5) <C> 72.6 (± 2.3) <C> [BOLD] 69.1 (± 2.7) <R> <C> BERT-large <C> B-COPA (50%) <C> 69.9 (± 1.9) <C> 71.2 (± 1.3) <C> 69.0 (± 3.5) <R> <C> BERT-large <C> COPA <C> [BOLD] 71.7 (± 0.5) <C> [BOLD] 80.5 (± 0.4) <C> 66.3 (± 0.8) <R> <C> RoBERTa-large <C> B-COPA <C> [BOLD] 76.7 (± 0.8) <C> 73.3 (± 1.5) <C> [BOLD] 78.8 (± 2.0) <R> <C> RoBERTa-large <C> B-COPA (50%) <C> 72.4 (± 2.0) <C> 72.1 (± 1.7) <C> 72.6 (± 2.1) <R> <C> RoBERTa-large <C> COPA <C> 76.4 (± 0.7) <C> [BOLD] 79.6 (± 1.0) <C> 74.4 (± 1.1) <R> <C> BERT-base-NSP <C> None <C> [BOLD] 66.4 <C> 66.2 <C> [BOLD] 66.7 <R> <C> BERT-large-NSP <C> None <C> 65.0 <C> [BOLD] 66.9 <C> 62.1 <CAP> Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.
<R> <C> Cue <C> [ITALIC] SCOPA <C> [ITALIC] SB_COPA <C> Diff. <C> Prod. <R> <C> woman <C> 7.98 <C> 4.84 <C> -3.14 <C> 0.25 <R> <C> mother <C> 5.16 <C> 3.95 <C> -1.21 <C> 0.75 <R> <C> went <C> 6.00 <C> 5.15 <C> -0.85 <C> 0.73 <R> <C> down <C> 5.52 <C> 4.93 <C> -0.58 <C> 0.71 <R> <C> into <C> 4.07 <C> 3.51 <C> -0.56 <C> 0.40 <CAP> Table 7: Sensitivity of BERT-large to superficial cues identified in §2 (unit: 10−2). Cues with top-5 reduction are shown. SCOPA,SB_COPA indicate the mean contributions of BERT-large trained on COPA, and BERT-large trained on B-COPA, respectively.
<R> <C> Classifier <C> Positive Sentiment Precision <C> Positive Sentiment Recall <C> Positive Sentiment Fscore <R> <C> SVM-w/o neg. <C> 0.57 <C> 0.72 <C> 0.64 <R> <C> SVM-Punct. neg. <C> 0.58 <C> 0.70 <C> 0.63 <R> <C> SVM-our-neg. <C> 0.58 <C> 0.73 <C> 0.65 <R> <C> CNN <C> 0.63 <C> 0.83 <C> 0.72 <R> <C> CNN-LSTM <C> 0.71 <C> 0.72 <C> 0.72 <R> <C> CNN-LSTM-Our-neg-Ant <C> [BOLD] 0.78 <C> [BOLD] 0.77 <C> [BOLD] 0.78 <R> <C> [EMPTY] <C> Negative Sentiment <C> Negative Sentiment <C> Negative Sentiment <R> <C> [EMPTY] <C> Precision <C> Recall <C> Fscore <R> <C> SVM-w/o neg. <C> 0.78 <C> 0.86 <C> 0.82 <R> <C> SVM-Punct. neg. <C> 0.78 <C> 0.87 <C> 0.83 <R> <C> SVM-Our neg. <C> 0.80 <C> 0.87 <C> 0.83 <R> <C> CNN <C> 0.88 <C> 0.72 <C> 0.79 <R> <C> CNN-LSTM. <C> 0.83 <C> 0.83 <C> 0.83 <R> <C> CNN-LSTM-our-neg-Ant <C> [BOLD] 0.87 <C> [BOLD] 0.87 <C> [BOLD] 0.87 <R> <C> [EMPTY] <C> Train <C> [EMPTY] <C> Test <R> <C> Positive tweets <C> 5121 <C> [EMPTY] <C> 1320 <R> <C> Negative tweets <C> 9094 <C> [EMPTY] <C> 2244 <CAP> Table 8: Sentiment classification evaluation, using different classifiers on the test set.
<R> <C> [EMPTY] <C> [BOLD] Punctuation <C> [BOLD] BiLSTM <C> [BOLD] Proposed <R> <C> In-scope (F) <C> 0.66 <C> 0.88 <C> 0.85 <R> <C> Out-scope (F) <C> 0.87 <C> 0.97 <C> 0.97 <R> <C> PCS <C> 0.52 <C> 0.72 <C> 0.72 <CAP> Table 7: Negation classifier performance for scope detection with gold cues and scope.
<R> <C> Total negation cues <C> 2921 <R> <C> True negation cues <C> 2674 <R> <C> False negation cues <C> 247 <R> <C> Average scope length <C> 2.9 <R> <C> Average sentence length <C> 13.6 <R> <C> Average tweet length <C> 22.3 <CAP> Table 3: Cue and token distribution in the conversational negation corpus.
<R> <C> [EMPTY] <C> [BOLD] F-Score  [BOLD] Baseline <C> [BOLD] F-Score  [BOLD] Proposed <C> [BOLD] Support <R> <C> False cues <C> 0.61 <C> 0.68 <C> 47 <R> <C> Actual cues <C> 0.97 <C> 0.98 <C> 557 <CAP> Table 4: Cue classification on the test set.
<R> <C> Model <C> Diversity <C> App <C> Good% <C> OK% <C> Invalid% <R> <C> DAMD <C> 3.12 <C> 2.50 <C> 56.5% <C> [BOLD] 37.4% <C> 6.1% <R> <C> DAMD (+) <C> [BOLD] 3.65 <C> [BOLD] 2.53 <C> [BOLD] 63.0% <C> 27.1% <C> 9.9% <R> <C> HDSA (+) <C> 2.14 <C> 2.47 <C> 57.5% <C> 32.5% <C> [BOLD] 10.0% <CAP> Table 5: Human evaluation results. Models with data augmentation are noted as (+). App denotes the average appropriateness score.
<R> <C> Model & Decoding Scheme <C> Act # w/o <C> Act # w/ <C> Slot # w/o <C> Slot # w/ <R> <C> Single-Action Baselines <C> Single-Action Baselines <C> Single-Action Baselines <C> Single-Action Baselines <C> Single-Action Baselines <R> <C> DAMD + greedy <C> [BOLD] 1.00 <C> [BOLD] 1.00 <C> 1.95 <C> [BOLD] 2.51 <R> <C> HDSA + fixed threshold <C> [BOLD] 1.00 <C> [BOLD] 1.00 <C> 2.07 <C> [BOLD] 2.40 <R> <C> 5-Action Generation <C> 5-Action Generation <C> 5-Action Generation <C> 5-Action Generation <C> 5-Action Generation <R> <C> DAMD + beam search <C> 2.67 <C> [BOLD] 2.87 <C> 3.36 <C> [BOLD] 4.39 <R> <C> DAMD + diverse beam search <C> 2.68 <C> [BOLD] 2.88 <C> 3.41 <C> [BOLD] 4.50 <R> <C> DAMD + top-k sampling <C> 3.08 <C> [BOLD] 3.43 <C> 3.61 <C> [BOLD] 4.91 <R> <C> DAMD + top-p sampling <C> 3.08 <C> [BOLD] 3.40 <C> 3.79 <C> [BOLD] 5.20 <R> <C> HDSA + sampled threshold <C> 1.32 <C> [BOLD] 1.50 <C> 3.08 <C> [BOLD] 3.31 <R> <C> 10-Action Generation <C> 10-Action Generation <C> 10-Action Generation <C> 10-Action Generation <C> 10-Action Generation <R> <C> DAMD + beam search <C> 3.06 <C> [BOLD] 3.39 <C> 4.06 <C> [BOLD] 5.29 <R> <C> DAMD + diverse beam search <C> 3.05 <C> [BOLD] 3.39 <C> 4.05 <C> [BOLD] 5.31 <R> <C> DAMD + top-k sampling <C> 3.59 <C> [BOLD] 4.12 <C> 4.21 <C> [BOLD] 5.77 <R> <C> DAMD + top-p sampling <C> 3.53 <C> [BOLD] 4.02 <C> 4.41 <C> [BOLD] 6.17 <R> <C> HDSA + sampled threshold <C> 1.54 <C> [BOLD] 1.83 <C> 3.42 <C> [BOLD] 3.92 <CAP> Table 1: Multi-action evaluation results. The “w” and “w/o” column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.
<R> <C> Model <C> Belief State Type <C> System Action Type <C> System Action Form <C> Inform (%) <C> Success (%) <C> BLEU <C> Combined Score <R> <C> 1. Seq2Seq + Attention  <C> oracle <C> - <C> - <C> 71.3 <C> 61.0 <C> [BOLD] 18.9 <C> 85.1 <R> <C> 2. Seq2Seq + Copy <C> oracle <C> - <C> - <C> 86.2 <C> [BOLD] 72.0 <C> 15.7 <C> 94.8 <R> <C> 3. MD-Sequicity <C> oracle <C> - <C> - <C> [BOLD] 86.6 <C> 71.6 <C> 16.8 <C> [BOLD] 95.9 <R> <C> 4. SFN + RL (Mehri et al. mehri2019structured) <C> oracle <C> generated <C> one-hot <C> 82.7 <C> 72.1 <C> 16.3 <C> 93.7 <R> <C> 5. HDSA  <C> oracle <C> generated <C> graph <C> 82.9 <C> 68.9 <C> [BOLD] 23.6 <C> 99.5 <R> <C> 6. DAMD <C> oracle <C> generated <C> span <C> [BOLD] 89.5 <C> 75.8 <C> 18.3 <C> 100.9 <R> <C> 7. DAMD + multi-action data augmentation <C> oracle <C> generated <C> span <C> 89.2 <C> [BOLD] 77.9 <C> 18.6 <C> [BOLD] 102.2 <R> <C> 8. SFN + RL (Mehri et al. mehri2019structured) <C> oracle <C> oracle <C> one-hot <C> - <C> - <C> 29.0 <C> 106.0 <R> <C> 9. HDSA  <C> oracle <C> oracle <C> graph <C> 87.9 <C> 78.0 <C> [BOLD] 30.4 <C> 113.4 <R> <C> 10. DAMD + multi-action data augmentation <C> oracle <C> oracle <C> span <C> [BOLD] 95.4 <C> [BOLD] 87.2 <C> 27.3 <C> [BOLD] 118.5 <R> <C> 11. SFN + RL (Mehri et al. mehri2019structured) <C> generated <C> generated <C> one-hot <C> 73.8 <C> 58.6 <C> [BOLD] 16.9 <C> 83.0 <R> <C> 12. DAMD + multi-action data augmentation <C> generated <C> generated <C> span <C> [BOLD] 76.3 <C> [BOLD] 60.4 <C> 16.6 <C> [BOLD] 85.0 <CAP> Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.
<R> <C> [EMPTY] <C> in-domain SQuAD <C> in-domain SQuAD <C> out-of-domain QA-SRL <C> out-of-domain QA-SRL <R> <C> [EMPTY] <C> EM <C> F1 <C> EM <C> F1 <R> <C> MQAN <C> 31.76 <C> 75.37 <C> <bold>10.99</bold> <C> 50.10 <R> <C> +coverage <C> <bold>32.67</bold> <C> <bold>76.83</bold> <C> 10.63 <C> <bold>50.89</bold> <R> <C> BIDAF (ELMO) <C> 70.43 <C> 79.76 <C> 28.35 <C> 49.98 <R> <C> +coverage <C> <bold>71.07</bold> <C> <bold>80.15</bold> <C> <bold>30.58</bold> <C> <bold>52.43</bold> <CAP> Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.
<R> <C> [EMPTY] <C> in-domain MultiNLI <C> out-of-domain SNLI <C> out-of-domain Glockner <C> out-of-domain SICK <R> <C> MQAN <C> 72.30 <C> 60.91 <C> 41.82 <C> 53.95 <R> <C> + coverage <C> <bold>73.84</bold> <C> <bold>65.38</bold> <C> <bold>78.69</bold> <C> <bold>54.55</bold> <R> <C> ESIM (ELMO) <C> 80.04 <C> 68.70 <C> 60.21 <C> 51.37 <R> <C> + coverage <C> <bold>80.38</bold> <C> <bold>70.05</bold> <C> <bold>67.47</bold> <C> <bold>52.65</bold> <CAP> Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.
<R> <C> GP-MBCM <C> ACER <C> PPO <C> ALDM <C> GDPL <R> <C> 1.666 <C> 0.775 <C> 0.639 <C> 1.069 <C> [BOLD] 0.238 <CAP> Table 4: KL-divergence between different dialog policy and the human dialog KL(πturns||pturns), where πturns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy π and the agenda-based user simulator, and pturns for the real human-human dialog.
<R> <C> Method <C> Agenda Turns <C> Agenda Inform <C> Agenda Match <C> Agenda Success <R> <C> GP-MBCM <C> 2.99 <C> 19.04 <C> 44.29 <C> 28.9 <R> <C> ACER <C> 10.49 <C> 77.98 <C> 62.83 <C> 50.8 <R> <C> PPO <C> 9.83 <C> 83.34 <C> 69.09 <C> 59.1 <R> <C> ALDM <C> 12.47 <C> 81.20 <C> 62.60 <C> 61.2 <R> <C> GDPL-sess <C> [BOLD] 7.49 <C> 88.39 <C> 77.56 <C> 76.4 <R> <C> GDPL-discr <C> 7.86 <C> 93.21 <C> 80.43 <C> 80.5 <R> <C> GDPL <C> 7.64 <C> [BOLD] 94.97 <C> [BOLD] 83.90 <C> [BOLD] 86.5 <R> <C> [ITALIC] Human <C> [ITALIC] 7.37 <C> [ITALIC] 66.89 <C> [ITALIC] 95.29 <C> [ITALIC] 75.0 <CAP> Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.
<R> <C> Method <C> VHUS Turns <C> VHUS Inform <C> VHUS Match <C> VHUS Success <R> <C> ACER <C> 22.35 <C> 55.13 <C> 33.08 <C> 18.6 <R> <C> PPO <C> [BOLD] 19.23 <C> [BOLD] 56.31 <C> 33.08 <C> 18.3 <R> <C> ALDM <C> 26.90 <C> 54.37 <C> 24.15 <C> 16.4 <R> <C> GDPL <C> 22.43 <C> 52.58 <C> [BOLD] 36.21 <C> [BOLD] 19.7 <CAP> Table 5: Performance of different agents on the neural user simulator.
<R> <C> VS. <C> Efficiency W <C> Efficiency D <C> Efficiency L <C> Quality W <C> Quality D <C> Quality L <C> Success W <C> Success D <C> Success L <R> <C> ACER <C> 55 <C> 25 <C> 20 <C> 44 <C> 32 <C> 24 <C> 52 <C> 30 <C> 18 <R> <C> PPO <C> 74 <C> 13 <C> 13 <C> 56 <C> 26 <C> 18 <C> 59 <C> 31 <C> 10 <R> <C> ALDM <C> 69 <C> 19 <C> 12 <C> 49 <C> 25 <C> 26 <C> 61 <C> 24 <C> 15 <CAP> Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.
<R> <C> Type <C> Inform Mean <C> Inform Num <C> Match Mean <C> Match Num <C> Success Mean <C> Success Num <R> <C> Full <C> 8.413 <C> 903 <C> 10.59 <C> 450 <C> 11.18 <C> 865 <R> <C> Other <C> -99.95 <C> 76 <C> -48.15 <C> 99 <C> -71.62 <C> 135 <CAP> Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.
<R> <C> Methods <C> # dims <C> Analg. (sem) <C> Analg. (syn) <C> Total <R> <C> GloVe <C> 300 <C> 78.94 <C> 64.12 <C> 70.99 <R> <C> Word2Vec <C> 300 <C> 81.03 <C> 66.11 <C> 73.03 <R> <C> OIWE-IPG <C> 300 <C> 19.99 <C> 23.44 <C> 21.84 <R> <C> SOV <C> 3000 <C> 64.09 <C> 46.26 <C> 54.53 <R> <C> SPINE <C> 1000 <C> 17.07 <C> 8.68 <C> 12.57 <R> <C> Word2Sense <C> 2250 <C> 12.94 <C> 19.44 <C> 5.84 <R> <C> Proposed <C> 300 <C> 79.96 <C> 63.52 <C> 71.15 <CAP> TABLE VII: Precision scores for the Analogy Test
<R> <C> [EMPTY] <C> GloVe <C> Imparted <R> <C> Participants 1 to 5 <C> 80/88/82/78/97 <C> 212/170/207/229/242 <R> <C> Mean/Std <C> 85/6.9 <C> 212/24.4 <CAP> TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions
<R> <C> Dataset (EN-) <C> GloVe <C> Word2Vec <C> OIWE-IPG <C> SOV <C> SPINE <C> Word2Sense <C> Proposed <R> <C> WS-353-ALL <C> 0.612 <C> 0.7156 <C> 0.634 <C> 0.622 <C> 0.173 <C> 0.690 <C> 0.657 <R> <C> SIMLEX-999 <C> 0.359 <C> 0.3939 <C> 0.295 <C> 0.355 <C> 0.090 <C> 0.380 <C> 0.381 <R> <C> VERB-143 <C> 0.326 <C> 0.4430 <C> 0.255 <C> 0.271 <C> 0.293 <C> 0.271 <C> 0.348 <R> <C> SimVerb-3500 <C> 0.193 <C> 0.2856 <C> 0.184 <C> 0.197 <C> 0.035 <C> 0.234 <C> 0.245 <R> <C> WS-353-REL <C> 0.578 <C> 0.6457 <C> 0.595 <C> 0.578 <C> 0.134 <C> 0.695 <C> 0.619 <R> <C> RW-STANF. <C> 0.378 <C> 0.4858 <C> 0.316 <C> 0.373 <C> 0.122 <C> 0.390 <C> 0.382 <R> <C> YP-130 <C> 0.524 <C> 0.5211 <C> 0.353 <C> 0.482 <C> 0.169 <C> 0.420 <C> 0.589 <R> <C> MEN-TR-3k <C> 0.710 <C> 0.7528 <C> 0.684 <C> 0.696 <C> 0.298 <C> 0.769 <C> 0.725 <R> <C> RG-65 <C> 0.768 <C> 0.8051 <C> 0.736 <C> 0.732 <C> 0.338 <C> 0.761 <C> 0.774 <R> <C> MTurk-771 <C> 0.650 <C> 0.6712 <C> 0.593 <C> 0.623 <C> 0.199 <C> 0.665 <C> 0.671 <R> <C> WS-353-SIM <C> 0.682 <C> 0.7883 <C> 0.713 <C> 0.702 <C> 0.220 <C> 0.720 <C> 0.720 <R> <C> MC-30 <C> 0.749 <C> 0.8112 <C> 0.799 <C> 0.726 <C> 0.330 <C> 0.735 <C> 0.776 <R> <C> MTurk-287 <C> 0.649 <C> 0.6645 <C> 0.591 <C> 0.631 <C> 0.295 <C> 0.674 <C> 0.634 <R> <C> Average <C> 0.552 <C> 0.6141 <C> 0.519 <C> 0.538 <C> 0.207 <C> 0.570 <C> 0.579 <CAP> TABLE VI: Correlations for Word Similarity Tests
<R> <C> Questions Subset <C> # of Questions Seen <C> GloVe <C> Word2Vec <C> Proposed <R> <C> All <C> 8783 <C> 78.94 <C> 81.03 <C> 79.96 <R> <C> At least one <C> 1635 <C> 67.58 <C> 70.89 <C> 67.89 <R> <C> concept word <C> 1635 <C> 67.58 <C> 70.89 <C> 67.89 <R> <C> All concept words <C> 110 <C> 77.27 <C> 89.09 <C> 83.64 <CAP> TABLE VIII: Precision scores for the Semantic Analogy Test
<R> <C> GloVe <C> Word2Vec <C> OIWE-IPG <C> SOV <C> SPINE <C> Word2Sense <C> Proposed <R> <C> 77.34 <C> 77.91 <C> 74.27 <C> 78.43 <C> 74.13 <C> 81.21 <C> 78.26 <CAP> TABLE IX: Accuracies (%) for Sentiment Classification Task
<R> <C> [BOLD] Model <C> R <C> MUC P <C> [ITALIC] F1 <C> R <C> B3 P <C> [ITALIC] F1 <C> R <C> CEAF- [ITALIC] e P <C> [ITALIC] F1 <C> CoNLL  [ITALIC] F1 <R> <C> Cluster+Lemma <C> 71.3 <C> 83 <C> 76.7 <C> 53.4 <C> 84.9 <C> 65.6 <C> 70.1 <C> 52.5 <C> 60 <C> 67.4 <R> <C> Disjoint <C> 76.7 <C> 80.8 <C> 78.7 <C> 63.2 <C> 78.2 <C> 69.9 <C> 65.3 <C> 58.3 <C> 61.6 <C> 70 <R> <C> Joint <C> 78.6 <C> 80.9 <C> 79.7 <C> 65.5 <C> 76.4 <C> 70.5 <C> 65.4 <C> 61.3 <C> 63.3 <C> [BOLD] 71.2 <CAP> Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.
<R> <C> [BOLD] Model <C> R <C> MUC P <C> [ITALIC] F1 <C> R <C> B3 P <C> [ITALIC] F1 <C> R <C> CEAF- [ITALIC] e P <C> [ITALIC] F1 <C> CoNLL  [ITALIC] F1 <R> <C> [BOLD] Baselines <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Cluster+Lemma <C> 76.5 <C> 79.9 <C> 78.1 <C> 71.7 <C> 85 <C> 77.8 <C> 75.5 <C> 71.7 <C> 73.6 <C> 76.5 <R> <C> CV Cybulska and Vossen ( 2015a ) <C> 71 <C> 75 <C> 73 <C> 71 <C> 78 <C> 74 <C> - <C> - <C> 64 <C> 73 <R> <C> KCP Kenyon-Dean et al. ( 2018 ) <C> 67 <C> 71 <C> 69 <C> 71 <C> 67 <C> 69 <C> 71 <C> 67 <C> 69 <C> 69 <R> <C> Cluster+KCP <C> 68.4 <C> 79.3 <C> 73.4 <C> 67.2 <C> 87.2 <C> 75.9 <C> 77.4 <C> 66.4 <C> 71.5 <C> 73.6 <R> <C> [BOLD] Model Variants <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Disjoint <C> 75.5 <C> 83.6 <C> 79.4 <C> 75.4 <C> 86 <C> 80.4 <C> 80.3 <C> 71.9 <C> 75.9 <C> 78.5 <R> <C> Joint <C> 77.6 <C> 84.5 <C> 80.9 <C> 76.1 <C> 85.1 <C> 80.3 <C> 81 <C> 73.8 <C> 77.3 <C> [BOLD] 79.5 <CAP> Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.
<R> <C> <bold>Model</bold> <C> R <C> MUC P <C> <italic>F</italic>1 <C> R <C> B3 P <C> <italic>F</italic>1 <C> R <C> CEAF-<italic>e</italic> P <C> <italic>F</italic>1 <C> CoNLL <italic>F</italic>1 <R> <C> Cluster+Lemma <C> 71.3 <C> 83 <C> 76.7 <C> 53.4 <C> 84.9 <C> 65.6 <C> 70.1 <C> 52.5 <C> 60 <C> 67.4 <R> <C> Disjoint <C> 76.7 <C> 80.8 <C> 78.7 <C> 63.2 <C> 78.2 <C> 69.9 <C> 65.3 <C> 58.3 <C> 61.6 <C> 70 <R> <C> Joint <C> 78.6 <C> 80.9 <C> 79.7 <C> 65.5 <C> 76.4 <C> 70.5 <C> 65.4 <C> 61.3 <C> 63.3 <C> <bold>71.2</bold> <CAP> Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.
<R> <C> <bold>Model</bold> <C> R <C> MUC P <C> <italic>F</italic>1 <C> R <C> B3 P <C> <italic>F</italic>1 <C> R <C> CEAF-<italic>e</italic> P <C> <italic>F</italic>1 <C> CoNLL <italic>F</italic>1 <R> <C> <bold>Baselines</bold> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Cluster+Lemma <C> 76.5 <C> 79.9 <C> 78.1 <C> 71.7 <C> 85 <C> 77.8 <C> 75.5 <C> 71.7 <C> 73.6 <C> 76.5 <R> <C> CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>) <C> 71 <C> 75 <C> 73 <C> 71 <C> 78 <C> 74 <C> - <C> - <C> 64 <C> 73 <R> <C> KCP Kenyon-Dean et al. (<ref id='bib-bib14'>2018</ref>) <C> 67 <C> 71 <C> 69 <C> 71 <C> 67 <C> 69 <C> 71 <C> 67 <C> 69 <C> 69 <R> <C> Cluster+KCP <C> 68.4 <C> 79.3 <C> 73.4 <C> 67.2 <C> 87.2 <C> 75.9 <C> 77.4 <C> 66.4 <C> 71.5 <C> 73.6 <R> <C> <bold>Model Variants</bold> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Disjoint <C> 75.5 <C> 83.6 <C> 79.4 <C> 75.4 <C> 86 <C> 80.4 <C> 80.3 <C> 71.9 <C> 75.9 <C> 78.5 <R> <C> Joint <C> 77.6 <C> 84.5 <C> 80.9 <C> 76.1 <C> 85.1 <C> 80.3 <C> 81 <C> 73.8 <C> 77.3 <C> <bold>79.5</bold> <CAP> Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.
<R> <C> Recall <C> 0.1 <C> 0.2 <C> 0.3 <C> 0.4 <C> AUC <R> <C> PCNN+ATT <C> 0.698 <C> 0.606 <C> 0.518 <C> 0.446 <C> 0.323 <R> <C> Rank+ExATT <C> 0.789 <C> 0.726 <C> 0.620 <C> 0.514 <C> 0.395 <R> <C> Our Model <C> 0.788 <C> [BOLD] 0.743 <C> [BOLD] 0.654 <C> [BOLD] 0.546 <C> [BOLD] 0.397 <CAP> Table 1: Precisions on the NYT dataset.
<R> <C> Recall <C> 0.1 <C> 0.2 <C> 0.3 <C> AUC <R> <C> Rank+ExATT <C> 0.584 <C> 0.535 <C> 0.487 <C> 0.392 <R> <C> PCNN+ATT (m) <C> 0.365 <C> 0.317 <C> 0.213 <C> 0.204 <R> <C> PCNN+ATT (1) <C> 0.665 <C> 0.517 <C> 0.413 <C> 0.396 <R> <C> Our Model <C> 0.650 <C> 0.519 <C> 0.422 <C> [BOLD] 0.405 <CAP> Table 2: Precisions on the Wikidata dataset.
<R> <C> Recall <C> 0.1 <C> 0.2 <C> 0.3 <C> AUC <R> <C> -Word-ATT <C> 0.648 <C> 0.515 <C> 0.395 <C> 0.389 <R> <C> -Capsule <C> 0.635 <C> 0.507 <C> 0.413 <C> 0.386 <R> <C> Our Model <C> 0.650 <C> 0.519 <C> 0.422 <C> 0.405 <CAP> Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.
<R> <C> Recall <C> 0.1 <C> 0.2 <C> 0.3 <C> AUC <C> Time <R> <C> [ITALIC] d=1 <C> 0.602 <C> 0.487 <C> 0.403 <C> 0.367 <C> 4h <R> <C> [ITALIC] d=32 <C> 0.645 <C> 0.501 <C> 0.393 <C> 0.370 <C> - <R> <C> [ITALIC] d=16 <C> 0.655 <C> 0.518 <C> 0.413 <C> 0.413 <C> 20h <R> <C> [ITALIC] d=8 <C> 0.650 <C> 0.519 <C> 0.422 <C> 0.405 <C> 8h <CAP> Table 4: Precisions on the Wikidata dataset with different choice of d.
<R> <C> Recall <C> 0.1 <C> 0.2 <C> 0.3 <C> AUC <R> <C> Iteration=1 <C> 0.531 <C> 0.455 <C> 0.353 <C> 0.201 <R> <C> Iteration=2 <C> 0.592 <C> 0.498 <C> 0.385 <C> 0.375 <R> <C> Iteration=3 <C> 0.650 <C> 0.519 <C> 0.422 <C> 0.405 <R> <C> Iteration=4 <C> 0.601 <C> 0.505 <C> 0.422 <C> 0.385 <R> <C> Iteration=5 <C> 0.575 <C> 0.495 <C> 0.394 <C> 0.376 <CAP> Table 5: Precisions on the Wikidata dataset with different number of dynamic routing iterations.
<R> <C> System <C> Reward <C> R-1 <C> R-2 <C> R-L <R> <C> Kryscinski et al. ( 2018 ) <C> R-L <C> 40.2 <C> 17.4 <C> 37.5 <R> <C> Narayan et al. ( 2018b ) <C> R-1,2,L <C> 40.0 <C> 18.2 <C> 36.6 <R> <C> Chen and Bansal ( 2018 ) <C> R-L <C> 41.5 <C> 18.7 <C> 37.8 <R> <C> Dong et al. ( 2018 ) <C> R-1,2,L <C> 41.5 <C> 18.7 <C> 37.6 <R> <C> Zhang et al. ( 2018 ) <C> [EMPTY] <C> 41.1 <C> 18.8 <C> 37.5 <R> <C> Zhou et al. ( 2018 ) <C> [EMPTY] <C> 41.6 <C> 19.0 <C> 38.0 <R> <C> Kedzie et al. ( 2018 ) <C> [EMPTY] <C> 39.1 <C> 17.9 <C> 35.9 <R> <C> (ours) NeuralTD <C> Learned <C> 39.6 <C> 18.1 <C> 36.5 <CAP> Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.
<R> <C> Metric <C> [ITALIC] ρ <C> [ITALIC] r <C> G-Pre <C> G-Rec <R> <C> ROUGE-1 <C> .290 <C> .304 <C> .392 <C> .428 <R> <C> ROUGE-2 <C> .259 <C> .278 <C> .408 <C> .444 <R> <C> ROUGE-L <C> .274 <C> .297 <C> .390 <C> .426 <R> <C> ROUGE-SU4 <C> .282 <C> .279 <C> .404 <C> .440 <R> <C> BLEU-1 <C> .256 <C> .281 <C> .409 <C> .448 <R> <C> BLEU-2 <C> .301 <C> .312 <C> .411 <C> .446 <R> <C> BLEU-3 <C> .317 <C> .312 <C> .409 <C> .444 <R> <C> BLEU-4 <C> .311 <C> .307 <C> .409 <C> .446 <R> <C> BLEU-5 <C> .308 <C> .303 <C> .420 <C> .459 <R> <C> METEOR <C> .305 <C> .285 <C> .409 <C> .444 <R> <C> InferSent-Cosine <C> [BOLD] .329 <C> [BOLD] .339 <C> .417 <C> .460 <R> <C> BERT-Cosine <C> .312 <C> .335 <C> [BOLD] .440 <C> [BOLD] .484 <CAP> Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the “good” summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.
<R> <C> Model <C> Encoder <C> [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] ρ <C> [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r <C> [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre <C> [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec <C> [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] ρ <C> [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r <C> [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre <C> [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec <R> <C> MLP <C> CNN-RNN <C> .311 <C> .340 <C> .486 <C> .532 <C> .318 <C> .335 <C> .481 <C> .524 <R> <C> MLP <C> PMeans-RNN <C> .313 <C> .331 <C> .489 <C> .536 <C> .354 <C> .375 <C> .502 <C> .556 <R> <C> MLP <C> BERT <C> [BOLD] .487 <C> [BOLD] .526 <C> [BOLD] .544 <C> [BOLD] .597 <C> [BOLD] .505 <C> [BOLD] .531 <C> [BOLD] .556 <C> [BOLD] .608 <R> <C> SimRed <C> CNN <C> .340 <C> .392 <C> .470 <C> .515 <C> .396 <C> .443 <C> .499 <C> .549 <R> <C> SimRed <C> PMeans <C> .354 <C> .393 <C> .493 <C> .541 <C> .370 <C> .374 <C> .507 <C> .551 <R> <C> SimRed <C> BERT <C> .266 <C> .296 <C> .458 <C> .495 <C> .325 <C> .338 <C> .485 <C> .533 <R> <C> Peyrard and Gurevych ( 2018 ) <C> Peyrard and Gurevych ( 2018 ) <C> .177 <C> .189 <C> .271 <C> .306 <C> .175 <C> .186 <C> .268 <C> .174 <CAP> Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table 1, all rewards in this table do not require reference summaries.
<R> <C> [EMPTY] <C> Ours <C> Refresh <C> ExtAbsRL <R> <C> Avg. Human Rating <C> [BOLD] 2.52 <C> 2.27 <C> 1.66 <R> <C> Best% <C> [BOLD] 70.0 <C> 33.3 <C> 6.7 <CAP> Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. “Best%”: in how many percentage of documents a system receives the highest human rating.
<R> <C> Reward <C> R-1 <C> R-2 <C> R-L <C> Human <C> Pref% <R> <C> R-L (original) <C> 40.9 <C> 17.8 <C> 38.5 <C> 1.75 <C> 15 <R> <C> Learned (ours) <C> 39.2 <C> 17.4 <C> 37.5 <C> [BOLD] 2.20 <C> [BOLD] 75 <CAP> Table 5: Performance of ExtAbsRL with different reward functions, measured in terms of ROUGE (center) and human judgements (right). Using our learned reward yields significantly (p=0.0057) higher average human rating. “Pref%”: in how many percentage of documents a system receives the higher human rating.
<R> <C> [BOLD] Model <C> [BOLD] Parameters <C> [BOLD] Validation AUC@0.05 <C> [BOLD] Test AUC@0.05 <R> <C> Base <C> 8.0M <C> [BOLD] 0.871 <C> 0.816 <R> <C> 4L SRU → 2L LSTM <C> 7.3M <C> 0.864 <C> [BOLD] 0.829 <R> <C> 4L SRU → 2L SRU <C> 7.8M <C> 0.856 <C> [BOLD] 0.829 <R> <C> Flat → hierarchical <C> 12.4M <C> 0.825 <C> 0.559 <R> <C> Cross entropy → hinge loss <C> 8.0M <C> 0.765 <C> 0.693 <R> <C> 6.6M → 1M examples <C> 8.0M <C> 0.835 <C> 0.694 <R> <C> 6.6M → 100K examples <C> 8.0M <C> 0.565 <C> 0.417 <R> <C> 200 → 100 negatives <C> 8.0M <C> 0.864 <C> 0.647 <R> <C> 200 → 10 negatives <C> 8.0M <C> 0.720 <C> 0.412 <CAP> Table 9: An ablation study showing the effect of different model architectures and training regimes on performance on the proprietary help desk dataset.
<R> <C> [BOLD] Encoder <C> [BOLD] Layer <C> [BOLD] Params <C> [BOLD] Time <R> <C> SRU <C> 2 <C> 3.7M <C> 14.7 <R> <C> SRU <C> 4 <C> 8.0M <C> 21.9 <R> <C> LSTM <C> 2 <C> 7.3M <C> 90.9 <R> <C> LSTM <C> 4 <C> 15.9M <C> 174.8 <R> <C> +rank response <C> - <C> - <C> 0.9 <CAP> Table 8: Inference time (milliseconds) of our model to encode a context using an SRU or an LSTM encoder on a single CPU core. The last row shows the extra time needed to compare the response encoding to 10,000 cached candidate response encodings in order to find the best response.
<R> <C> [BOLD] Metric <C> [BOLD] Validation <C> [BOLD] Test <R> <C> AUC <C> 0.991 <C> 0.977 <R> <C> AUC@0.1 <C> 0.925 <C> 0.885 <R> <C> AUC@0.05 <C> 0.871 <C> 0.816 <R> <C> AUC@0.01 <C> 0.677 <C> 0.630 <CAP> Table 3: AUC and AUC@p of our model on the propriety help desk dataset.
<R> <C> [BOLD] Candidates <C> [BOLD] R@1 <C> [BOLD] R@3 <C> [BOLD] R@5 <C> [BOLD] R@10 <R> <C> 10 <C> 0.892 <C> 0.979 <C> 0.987 <C> 1 <R> <C> 100 <C> 0.686 <C> 0.842 <C> 0.894 <C> 0.948 <R> <C> 1,000 <C> 0.449 <C> 0.611 <C> 0.677 <C> 0.760 <R> <C> 10,000 <C> 0.234 <C> 0.360 <C> 0.421 <C> 0.505 <CAP> Table 4: Recall@k from n response candidates for different values of n using random whitelists. Each random whitelist includes the correct response along with n−1 randomly selected responses.
<R> <C> [BOLD] Whitelist <C> [BOLD] R@1 <C> [BOLD] R@3 <C> [BOLD] R@5 <C> [BOLD] R@10 <C> [BOLD] BLEU <R> <C> Random 10K+ <C> 0.252 <C> 0.400 <C> 0.472 <C> 0.560 <C> 37.71 <R> <C> Frequency 10K+ <C> 0.257 <C> 0.389 <C> 0.455 <C> 0.544 <C> 41.34 <R> <C> Clustering 10K+ <C> 0.230 <C> 0.376 <C> 0.447 <C> 0.541 <C> 37.59 <R> <C> Random 1K+ <C> 0.496 <C> 0.663 <C> 0.728 <C> 0.805 <C> 59.28 <R> <C> Frequency 1K+ <C> 0.513 <C> 0.666 <C> 0.726 <C> 0.794 <C> 67.05 <R> <C> Clustering 1K+ <C> 0.481 <C> 0.667 <C> 0.745 <C> 0.835 <C> 61.88 <R> <C> Frequency 10K <C> 0.136 <C> 0.261 <C> 0.327 <C> 0.420 <C> 30.46 <R> <C> Clustering 10K <C> 0.164 <C> 0.292 <C> 0.360 <C> 0.457 <C> 31.47 <R> <C> Frequency 1K <C> 0.273 <C> 0.465 <C> 0.550 <C> 0.658 <C> 47.13 <R> <C> Clustering 1K <C> 0.331 <C> 0.542 <C> 0.650 <C> 0.782 <C> 49.26 <CAP> Table 5: Recall@k for random, frequency, and clustering whitelists of different sizes. The “+” indicates that the true response is added to the whitelist.
<R> <C> [BOLD] Whitelist <C> [BOLD] R@1 <C> [BOLD] Coverage <R> <C> Frequency 10K <C> 0.136 <C> 45.04% <R> <C> Clustering 10K <C> 0.164 <C> 38.38% <R> <C> Frequency 1K <C> 0.273 <C> 33.38% <R> <C> Clustering 1K <C> 0.331 <C> 23.28% <CAP> Table 6: Recall@1 versus coverage for frequency and clustering whitelists.
<R> <C> [BOLD] Whitelist <C> [BOLD] Great <C> [BOLD] Good <C> [BOLD] Bad <C> [BOLD] Accept <R> <C> Freq. 1K <C> 54% <C> 26% <C> 20% <C> 80% <R> <C> Cluster. 1K <C> 55% <C> 21% <C> 23% <C> 77% <R> <C> Freq. 10K <C> 56% <C> 24% <C> 21% <C> 80% <R> <C> Cluster. 10K <C> 57% <C> 23% <C> 20% <C> 80% <R> <C> Real response <C> 60% <C> 24% <C> 16% <C> 84% <CAP> Table 7: Results of the human evaluation of the responses produced by our model. A response is acceptable if it is either good or great. Note: Numbers may not add up to 100% due to rounding.
<R> <C> [EMPTY] <C> M <C> F <C> B <C> O <R> <C> Random <C> 43.6 <C> 39.3 <C> [ITALIC] 0.90 <C> 41.5 <R> <C> Token Distance <C> 50.1 <C> 42.4 <C> [ITALIC] 0.85 <C> 46.4 <R> <C> Topical Entity <C> 51.5 <C> 43.7 <C> [ITALIC] 0.85 <C> 47.7 <R> <C> Syntactic Distance <C> 63.0 <C> 56.2 <C> [ITALIC] 0.89 <C> 59.7 <R> <C> Parallelism <C> [BOLD] 67.1 <C> [BOLD] 63.1 <C> [ITALIC]  [BOLD] 0.94 <C> [BOLD] 65.2 <R> <C> Parallelism+URL <C> [BOLD] 71.1 <C> [BOLD] 66.9 <C> [ITALIC]  [BOLD] 0.94 <C> [BOLD] 69.0 <R> <C> Transformer-Single <C> 58.6 <C> 51.2 <C> [ITALIC] 0.87 <C> 55.0 <R> <C> Transformer-Multi <C> 59.3 <C> 52.9 <C> [ITALIC] 0.89 <C> 56.2 <CAP> Table 6: Performance of our baselines on the development set. Parallelism+URL tests the page-context setting; all other test the snippet-context setting. Bold indicates best performance in each setting.
<R> <C> [EMPTY] <C> M <C> F <C> B <C> O <R> <C> Lee et al. ( 2013 ) <C> 55.4 <C> 45.5 <C> [ITALIC] 0.82 <C> 50.5 <R> <C> Clark and Manning <C> 58.5 <C> 51.3 <C> [ITALIC] 0.88 <C> 55.0 <R> <C> Wiseman et al. <C> [BOLD] 68.4 <C> 59.9 <C> [ITALIC] 0.88 <C> 64.2 <R> <C> Lee et al. ( 2017 ) <C> 67.2 <C> [BOLD] 62.2 <C> [ITALIC]  [BOLD] 0.92 <C> [BOLD] 64.7 <CAP> Table 4: Performance of off-the-shelf resolvers on the GAP development set, split by Masculine and Feminine (Bias shows F/M), and Overall. Bold indicates best performance.
<R> <C> [EMPTY] <C> M <C> F <C> B <C> O <R> <C> Random <C> 47.5 <C> 50.5 <C> [ITALIC] 1.06 <C> 49.0 <R> <C> Token Distance <C> 50.6 <C> 47.5 <C> [ITALIC] 0.94 <C> 49.1 <R> <C> Topical Entity <C> 50.2 <C> 47.3 <C> [ITALIC] 0.94 <C> 48.8 <R> <C> Syntactic Distance <C> 66.7 <C> 66.7 <C> [ITALIC]  [BOLD] 1.00 <C> 66.7 <R> <C> Parallelism <C> [BOLD] 69.3 <C> [BOLD] 69.2 <C> [ITALIC]  [BOLD] 1.00 <C> [BOLD] 69.2 <R> <C> Parallelism+URL <C> [BOLD] 74.2 <C> [BOLD] 71.6 <C> [ITALIC]  [BOLD] 0.96 <C> [BOLD] 72.9 <R> <C> Transformer-Single <C> 59.6 <C> 56.6 <C> [ITALIC] 0.95 <C> 58.1 <R> <C> Transformer-Multi <C> 62.9 <C> 61.7 <C> [ITALIC] 0.98 <C> 62.3 <CAP> Table 7: Performance of our baselines on the development set in the gold-two-mention task (access to the two candidate name spans). Parallelism+URL tests the page-context setting; all other test the snippet-context setting. Bold indicates best performance in each setting.
<R> <C> HeadLayer <C> L0 <C> L1 <C> L2 <C> L3 <C> L4 <C> L5 <R> <C> H0 <C> 4 6.9 <C> 4 7.4 <C> 4 5.8 <C> 4 6.2 <C> 4 5.8 <C> 4 5.7 <R> <C> H1 <C> 4 5.3 <C> 4 6.5 <C> 4 6.4 <C> 4 6.2 <C> 4 9.4 <C> 4 6.3 <R> <C> H2 <C> 4 5.8 <C> 4 6.7 <C> 4 6.3 <C> 4 6.5 <C> 4 5.7 <C> 4 5.9 <R> <C> H3 <C> 4 6.0 <C> 4 6.3 <C> 4 6.8 <C> 4 6.0 <C> 4 6.6 <C> 4 8.0 <R> <C> H4 <C> 4 5.7 <C> 4 6.3 <C> 4 6.5 <C> 4 7.8 <C> 4 5.1 <C> 4 7.0 <R> <C> H5 <C> 4 7.0 <C> 4 6.5 <C> 4 6.5 <C> 4 5.6 <C> 4 6.2 <C> 5 2.9 <R> <C> H6 <C> 4 6.7 <C> 4 5.4 <C> 4 6.4 <C> 4 5.3 <C> 4 6.9 <C> 4 7.0 <R> <C> H7 <C> 4 3.8 <C> 4 6.6 <C> 4 6.4 <C> 5 5.0 <C> 4 6.4 <C> 4 6.2 <CAP> Table 8: Coreference signal of a Transformer model on the validation dataset, by encoder attention layer and head.
<R> <C> [EMPTY] <C> [EMPTY] <C> Parallelism Correct <C> Parallelism Incorrect <R> <C> Transf. <C> Correct <C> 48.7% <C> 13.4% <R> <C> Transf. <C> Incorrect <C> 21.6% <C> 16.3% <CAP> Table 9: Comparison of the predictions of the Parallelism and Transformer-Single heuristics over the GAP development dataset.
<R> <C> [ITALIC] m <C> NYT10 Prec. <C> NYT10 Rec. <C> NYT10 F1 <C> NYT11 Prec. <C> NYT11 Rec. <C> NYT11 F1 <R> <C> 1 <C> 0.541 <C> 0.595 <C> [BOLD] 0.566 <C> 0.495 <C> 0.621 <C> 0.551 <R> <C> 2 <C> 0.521 <C> 0.597 <C> 0.556 <C> 0.482 <C> 0.656 <C> 0.555 <R> <C> 3 <C> 0.490 <C> 0.617 <C> 0.547 <C> 0.509 <C> 0.633 <C> 0.564 <R> <C> 4 <C> 0.449 <C> 0.623 <C> 0.522 <C> 0.507 <C> 0.652 <C> [BOLD] 0.571 <R> <C> 5 <C> 0.467 <C> 0.609 <C> 0.529 <C> 0.488 <C> 0.677 <C> 0.567 <CAP> Table 3: Performance comparison of our model with different values of m on the two datasets.
<R> <C> Model <C> NYT10 Prec. <C> NYT10 Rec. <C> NYT10 F1 <C> NYT11 Prec. <C> NYT11 Rec. <C> NYT11 F1 <R> <C> CNN zeng2014relation <C> 0.413 <C> 0.591 <C> 0.486 <C> 0.444 <C> 0.625 <C> 0.519 <R> <C> PCNN zeng2015distant <C> 0.380 <C> [BOLD] 0.642 <C> 0.477 <C> 0.446 <C> 0.679 <C> 0.538† <R> <C> EA huang2016attention <C> 0.443 <C> 0.638 <C> 0.523† <C> 0.419 <C> 0.677 <C> 0.517 <R> <C> BGWA jat2018attention <C> 0.364 <C> 0.632 <C> 0.462 <C> 0.417 <C> [BOLD] 0.692 <C> 0.521 <R> <C> BiLSTM-CNN <C> 0.490 <C> 0.507 <C> 0.498 <C> 0.473 <C> 0.606 <C> 0.531 <R> <C> Our model <C> [BOLD] 0.541 <C> 0.595 <C> [BOLD] 0.566* <C> [BOLD] 0.507 <C> 0.652 <C> [BOLD] 0.571* <CAP> Table 2: Performance comparison of different models on the two datasets. * denotes a statistically significant improvement over the previous best state-of-the-art model with p<0.01 under the bootstrap paired t-test. † denotes the previous best state-of-the-art model.
<R> <C> [EMPTY] <C> Prec. <C> Rec. <C> F1 <R> <C> (A1) BiLSTM-CNN <C> 0.473 <C> 0.606 <C> 0.531 <R> <C> (A2) Standard attention <C> 0.466 <C> 0.638 <C> 0.539 <R> <C> (A3) Window size ( [ITALIC] ws)=5 <C> 0.507 <C> 0.652 <C> [BOLD] 0.571 <R> <C> (A4) Window size ( [ITALIC] ws)=10 <C> 0.510 <C> 0.640 <C> 0.568 <R> <C> (A5) Softmax <C> 0.490 <C> 0.658 <C> 0.562 <R> <C> (A6) Max-pool <C> 0.492 <C> 0.600 <C> 0.541 <CAP> Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.
<R> <C> Method <C> Overall <C> people <C> clothing <C> bodyparts <C> animals <C> vehicles <C> instruments <C> scene <C> other <R> <C> QRC - VGG(det) <C> 60.21 <C> 75.08 <C> 55.9 <C> 20.27 <C> 73.36 <C> 68.95 <C> 45.68 <C> 65.27 <C> 38.8 <R> <C> CITE - VGG(det) <C> 61.89 <C> [BOLD] 75.95 <C> 58.50 <C> 30.78 <C> [BOLD] 77.03 <C> [BOLD] 79.25 <C> 48.15 <C> 58.78 <C> 43.24 <R> <C> ZSGNet - VGG (cls) <C> 60.12 <C> 72.52 <C> 60.57 <C> 38.51 <C> 63.61 <C> 64.47 <C> 49.59 <C> 64.66 <C> 41.09 <R> <C> ZSGNet - Res50 (cls) <C> [BOLD] 63.39 <C> 73.87 <C> [BOLD] 66.18 <C> [BOLD] 45.27 <C> 73.79 <C> 71.38 <C> [BOLD] 58.54 <C> [BOLD] 66.49 <C> [BOLD] 45.53 <CAP> Table 3: Category-wise performance with the default split of Flickr30k Entities.
<R> <C> Method <C> Net <C> Flickr30k <C> ReferIt <R> <C> SCRC  <C> VGG <C> 27.8 <C> 17.9 <R> <C> GroundeR (cls)  <C> VGG <C> 42.43 <C> 24.18 <R> <C> GroundeR (det)  <C> VGG <C> 48.38 <C> 28.5 <R> <C> MCB (det)  <C> VGG <C> 48.7 <C> 28.9 <R> <C> Li (cls)  <C> VGG <C> - <C> 40 <R> <C> QRC* (det)  <C> VGG <C> 60.21 <C> 44.1 <R> <C> CITE* (cls)  <C> VGG <C> 61.89 <C> 34.13 <R> <C> QRG* (det) <C> VGG <C> 60.1 <C> - <R> <C> [BOLD] ZSGNet (cls) <C> [BOLD] VGG <C> [BOLD] 60.12 <C> [BOLD] 53.31 <R> <C> [BOLD] ZSGNet (cls) <C> [BOLD] Res50 <C> [BOLD] 63.39 <C> [BOLD] 58.63 <CAP> Table 2: Comparison of our model with other state of the art methods. We denote those networks which use classification weights from ImageNet [41] using “cls” and those networks which use detection weights from Pascal VOC [12] using “det”. The reported numbers are all Accuracy@IoU=0.5 or equivalently Recall@1. Models marked with “*” fine-tune their detection network on the entities in the Flickr30k.
<R> <C> Method <C> Net <C> Flickr- Split-0 <C> Flickr- Split-1 <C> VG-2B 0.3 <C> VG-2B 0.5 <C> VG-2UB 0.3 <C> VG-2UB 0.5 <C> VG-3B 0.3 <C> VG-3B 0.5 <C> VG-3UB 0.3 <C> VG-3UB 0.5 <R> <C> QRG <C> VGG <C> 35.62 <C> 24.42 <C> 13.17 <C> 7.64 <C> 12.39 <C> 7.15 <C> 14.21 <C> 8.35 <C> 13.03 <C> 7.52 <R> <C> ZSGNet <C> VGG <C> 39.32 <C> 29.35 <C> 17.09 <C> 11.02 <C> 16.48 <C> 10.55 <C> 17.63 <C> 11.42 <C> 17.35 <C> 10.97 <R> <C> ZSGNet <C> Res50 <C> [BOLD] 43.02 <C> [BOLD] 31.23 <C> [BOLD] 19.95 <C> [BOLD] 12.90 <C> [BOLD] 19.12 <C> [BOLD] 12.37 <C> [BOLD] 20.77 <C> [BOLD] 13.77 <C> [BOLD] 19.72 <C> [BOLD] 12.82 <CAP> Table 4: Accuracy across various unseen splits. For Flickr-Split-0,1 we use Accuracy with IoU threshold of 0.5. Since Visual Genome annotations are noisy we additionally report Accuracy with IoU threshold of 0.3. The second row denotes the IoU threshold at which the Accuracy is calculated. “B” and “UB” denote the balanced and unbalanced sets.
<R> <C> Model <C> Accuracy on RefClef <R> <C> BM + Softmax <C> 48.54 <R> <C> BM + BCE <C> 55.20 <R> <C> BM + FL <C> 57.13 <R> <C> BM + FL + Img-Resize <C> [BOLD] 61.75 <CAP> Table 6: Ablation study: BM=Base Model, softmax means we classify only one candidate box as foreground, BCE = Binary Cross Entropy means we classify each candidate box as the foreground or background, FL = Focal Loss, Img-Resize: use images of dimension 600×600
<R> <C> [EMPTY] <C> [BOLD] Training scheme <C> [BOLD] News <C> [BOLD] TED <C> [BOLD] IT <R> <C> 1 <C> News <C> 37.8 <C> 25.3 <C> 35.3 <R> <C> 2 <C> TED <C> 23.7 <C> 24.1 <C> 14.4 <R> <C> 3 <C> IT <C> 1.6 <C> 1.8 <C> 39.6 <R> <C> 4 <C> News and TED <C> 38.2 <C> 25.5 <C> 35.4 <R> <C> 5 <C> 1 then TED, No-reg <C> 30.6 <C> [BOLD] 27.0 <C> 22.1 <R> <C> 6 <C> 1 then TED, L2 <C> 37.9 <C> 26.7 <C> 31.8 <R> <C> 7 <C> 1 then TED, EWC <C> [BOLD] 38.3 <C> [BOLD] 27.0 <C> 33.1 <R> <C> 8 <C> 5 then IT, No-reg <C> 8.0 <C> 6.9 <C> 56.3 <R> <C> 9 <C> 6 then IT, L2 <C> 32.3 <C> 22.6 <C> 56.9 <R> <C> 10 <C> 7 then IT, EWC <C> 35.8 <C> 24.6 <C> [BOLD] 57.0 <CAP> Table 4: Test BLEU for en-de adaptive training, with sequential adaptation to a third task. EWC-tuned models give the best performance on each domain.
<R> <C> [EMPTY] <C> [BOLD] Training scheme <C> [BOLD] Health <C> [BOLD] Bio <R> <C> 1 <C> Health <C> [BOLD] 35.9 <C> 33.1 <R> <C> 2 <C> Bio <C> 29.6 <C> 36.1 <R> <C> 3 <C> Health and Bio <C> 35.8 <C> 37.2 <R> <C> 4 <C> 1 then Bio, No-reg <C> 30.3 <C> 36.6 <R> <C> 5 <C> 1 then Bio, L2 <C> 35.1 <C> 37.3 <R> <C> 6 <C> 1 then Bio, EWC <C> 35.2 <C> [BOLD] 37.8 <CAP> Table 3: Test BLEU for es-en adaptive training. EWC reduces forgetting compared to other fine-tuning methods, while offering the greatest improvement on the new domain.
<R> <C> [BOLD] Decoder configuration <C> [BOLD] es-en  [BOLD] Health <C> [BOLD] es-en  [BOLD] Bio <C> [BOLD] en-de  [BOLD] News <C> [BOLD] en-de  [BOLD] TED <C> [BOLD] en-de  [BOLD] IT <R> <C> Oracle model <C> 35.9 <C> 36.1 <C> 37.8 <C> 24.1 <C> 39.6 <R> <C> Uniform <C> 33.1 <C> 36.4 <C> 21.9 <C> 18.4 <C> 38.9 <R> <C> Identity-BI <C> 35.0 <C> 36.6 <C> 32.7 <C> 25.3 <C> 42.6 <R> <C> BI <C> 35.9 <C> 36.5 <C> 38.0 <C> 26.1 <C> [BOLD] 44.7 <R> <C> IS <C> [BOLD] 36.0 <C> 36.8 <C> 37.5 <C> 25.6 <C> 43.3 <R> <C> BI + IS <C> [BOLD] 36.0 <C> [BOLD] 36.9 <C> [BOLD] 38.4 <C> [BOLD] 26.4 <C> [BOLD] 44.7 <CAP> Table 5: Test BLEU for 2-model es-en and 3-model en-de unadapted model ensembling, compared to oracle unadapted model chosen if test domain is known. Uniform ensembling generally underperforms the oracle, while BI+IS outperforms the oracle.
<R> <C> [BOLD] Decoder configuration <C> [BOLD] es-en  [BOLD] Health <C> [BOLD] es-en  [BOLD] Bio <C> [BOLD] en-de  [BOLD] News <C> [BOLD] en-de  [BOLD] TED <C> [BOLD] en-de  [BOLD] IT <R> <C> Oracle model <C> 35.9 <C> 37.8 <C> 37.8 <C> 27.0 <C> 57.0 <R> <C> Uniform <C> 36.0 <C> 36.4 <C> [BOLD] 38.9 <C> 26.0 <C> 43.5 <R> <C> BI + IS <C> [BOLD] 36.2 <C> [BOLD] 38.0 <C> 38.7 <C> [BOLD] 26.1 <C> [BOLD] 56.4 <CAP> Table 6: Test BLEU for 2-model es-en and 3-model en-de model ensembling for models adapted with EWC, compared to oracle model last trained on each domain, chosen if test domain is known. BI+IS outperforms uniform ensembling and in some cases outperforms the oracle.
<R> <C> [BOLD] Language pair <C> [BOLD] Model type <C> [BOLD] Oracle model <C> [BOLD] Decoder configuration  [BOLD] Uniform <C> [BOLD] Decoder configuration  [BOLD] BI + IS <R> <C> es-en <C> Unadapted <C> 36.4 <C> 34.7 <C> 36.6 <R> <C> es-en <C> No-reg <C> 36.6 <C> 34.8 <C> - <R> <C> es-en <C> EWC <C> 37.0 <C> 36.3 <C> [BOLD] 37.2 <R> <C> en-de <C> Unadapted <C> 36.4 <C> 26.8 <C> 38.8 <R> <C> en-de <C> No-reg <C> 41.7 <C> 31.8 <C> - <R> <C> en-de <C> EWC <C> 42.1 <C> 38.6 <C> [BOLD] 42.0 <CAP> Table 7: Total BLEU for test data concatenated across domains. Results from 2-model es-en and 3-model en-de ensembles, compared to oracle model chosen if test domain is known. No-reg uniform corresponds to the approach of Freitag and Al-Onaizan (2016). BI+IS performs similarly to strong oracles with no test domain labeling.
<R> <C> [BOLD] Model <C> [BOLD] Prec.(%) <C> [BOLD] Rec.(%) <C> [BOLD] F1(%) <R> <C> EL <C> 96.02 <C> 81.89 <C> 88.39 <R> <C> CMP <C> 86.39 <C> [BOLD] 88.64 <C> 87.50 <R> <C> Hybrid-EL-CMP1 <C> [BOLD] 97.42 <C> 84.70 <C> 90.62 <R> <C> Hybrid-EL-CMP2 <C> 95.82 <C> 86.42 <C> [BOLD] 90.87 <CAP> Table 8: Semantic role labeling results. Hybrid-EL-CMP1 represents rule-based model and Hybrid-EL-CMP2 represents probability-based model.
<R> <C> [BOLD] Selection Method <C> [BOLD] Prec.(%) <C> [BOLD] Rec.(%) <C> [BOLD] F1(%) <R> <C> Max Logits <C> 80.19 <C> 80.50 <C> 79.85 <R> <C> Add Logits <C> 81.30 <C> 81.28 <C> 80.85 <R> <C> Add Logits+Expert <C> [BOLD] 81.30 <C> [BOLD] 81.41 <C> [BOLD] 80.90 <R> <C> Concat Hidden <C> 80.24 <C> 80.04 <C> 79.65 <R> <C> Max Hidden <C> 80.30 <C> 80.04 <C> 79.63 <R> <C> Add Hidden <C> 80.82 <C> 80.28 <C> 80.08 <CAP> Table 6: Dialog act prediction performance using different selection methods.
<R> <C> Method <C> MAP <C> MRR <R> <C> CNN + LR (unigram) <C> 54.70 <C> 63.29 <R> <C> CNN + LR (bigram) <C> 56.93 <C> 66.13 <R> <C> CNN <C> 66.91 <C> 68.80 <R> <C> CNTN <C> 65.80 <C> 69.78 <R> <C> LSTM (1 layer) <C> 62.04 <C> 66.85 <R> <C> LSTM <C> 59.75 <C> 65.33 <R> <C> MV-LSTM <C> 64.88 <C> 68.24 <R> <C> NTN-LSTM <C> 63.40 <C> 67.72 <R> <C> HD-LSTM <C> 67.44 <C> <bold>75.11</bold> <R> <C> Capsule-Zhao <C> 73.63 <C> 70.12 <R> <C> NLP-Capsule <C> <bold>77.73</bold> <C> 74.16 <CAP> Table 6: Experimental results on TREC QA dataset.
<R> <C> <bold>Datasets</bold> <C> <bold>Metrics</bold> <C> <bold>FastXML</bold> <C> <bold>PD-Sparse</bold> <C> <bold>FastText</bold> <C> <bold>Bow-CNN</bold> <C> <bold>CNN-Kim</bold> <C> <bold>XML-CNN</bold> <C> <bold>Cap-Zhao</bold> <C> <bold>NLP-Cap</bold> <C> <bold>Impv</bold> <R> <C> RCV1 <C> PREC@1 <C> 94.62 <C> 95.16 <C> 95.40 <C> 96.40 <C> 93.54 <C> 96.86 <C> 96.63 <C> <bold>97.05</bold> <C> +0.20% <R> <C> RCV1 <C> PREC@3 <C> 78.40 <C> 79.46 <C> 79.96 <C> 81.17 <C> 76.15 <C> 81.11 <C> 81.02 <C> <bold>81.27</bold> <C> +0.20% <R> <C> RCV1 <C> PREC@5 <C> 54.82 <C> 55.61 <C> 55.64 <C> <bold>56.74</bold> <C> 52.94 <C> 56.07 <C> 56.12 <C> 56.33 <C> -0.72% <R> <C> [EMPTY] <C> NDCG@1 <C> 94.62 <C> 95.16 <C> 95.40 <C> 96.40 <C> 93.54 <C> 96.88 <C> 96.63 <C> <bold>97.05</bold> <C> +0.20% <R> <C> [EMPTY] <C> NDCG@3 <C> 89.21 <C> 90.29 <C> 90.95 <C> 92.04 <C> 87.26 <C> 92.22 <C> 92.31 <C> <bold>92.47</bold> <C> +0.17% <R> <C> [EMPTY] <C> NDCG@5 <C> 90.27 <C> 91.29 <C> 91.68 <C> 92.89 <C> 88.20 <C> 92.63 <C> 92.75 <C> <bold>93.11</bold> <C> +0.52% <R> <C> EUR-Lex <C> PREC@1 <C> 68.12 <C> 72.10 <C> 71.51 <C> 64.99 <C> 68.35 <C> 75.65 <C> - <C> <bold>80.20</bold> <C> +6.01% <R> <C> EUR-Lex <C> PREC@3 <C> 57.93 <C> 57.74 <C> 60.37 <C> 51.68 <C> 54.45 <C> 61.81 <C> - <C> <bold>65.48</bold> <C> +5.93% <R> <C> EUR-Lex <C> PREC@5 <C> 48.97 <C> 47.48 <C> 50.41 <C> 42.32 <C> 44.07 <C> 50.90 <C> - <C> <bold>52.83</bold> <C> +3.79% <R> <C> [EMPTY] <C> NDCG@1 <C> 68.12 <C> 72.10 <C> 71.51 <C> 64.99 <C> 68.35 <C> 75.65 <C> - <C> <bold>80.20</bold> <C> +6.01% <R> <C> [EMPTY] <C> NDCG@3 <C> 60.66 <C> 61.33 <C> 63.32 <C> 55.03 <C> 59.81 <C> 66.71 <C> - <C> <bold>71.11</bold> <C> +6.59% <R> <C> [EMPTY] <C> NDCG@5 <C> 56.42 <C> 55.93 <C> 58.56 <C> 49.92 <C> 57.99 <C> 64.45 <C> - <C> <bold>68.80</bold> <C> +6.75% <CAP> Table 2: Comparisons of our NLP-Cap approach and baselines on two text classification benchmarks, where ’-’ denotes methods that failed to scale due to memory issues.
<R> <C> [BOLD] Label <C> [BOLD] Train <C> [BOLD] Trial <R> <C> [BOLD] Suggestion <C> 2085 <C> 296 <R> <C> [BOLD] Non Suggestion <C> 6415 <C> 296 <CAP> Table 1: Dataset Distribution for Sub Task A - Task 9: Suggestion Mining from Online Reviews.
<R> <C> [BOLD] Model <C> [BOLD] F1 (train) <C> [BOLD] F1 (test) <R> <C> [BOLD] Multinomial Naive Bayes (using Count Vectorizer) <C> 0.641 <C> 0.517 <R> <C> [BOLD] Logistic Regression (using Count Vectorizer) <C> 0.679 <C> 0.572 <R> <C> [BOLD] SVM (Linear Kernel) (using TfIdf Vectorizer) <C> 0.695 <C> 0.576 <R> <C> [BOLD] LSTM (128 LSTM Units) <C> 0.731 <C> 0.591 <R> <C> [BOLD] Provided Baseline <C> 0.720 <C> 0.267 <R> <C> [BOLD] ULMFit* <C> 0.861 <C> 0.701 <CAP> Table 3: Performance of different models on the provided train and test dataset for Sub Task A.
<R> <C> [BOLD] Ranking <C> [BOLD] Team Name <C> [BOLD] Performance (F1) <R> <C> [BOLD] 1 <C> OleNet <C> 0.7812 <R> <C> [BOLD] 2 <C> ThisIsCompetition <C> 0.7778 <R> <C> [BOLD] 3 <C> m_y <C> 0.7761 <R> <C> [BOLD] 4 <C> yimmon <C> 0.7629 <R> <C> [BOLD] 5 <C> NTUA-ISLab <C> 0.7488 <R> <C> [BOLD] 10 <C> [BOLD] MIDAS (our team) <C> [BOLD] 0.7011* <CAP> Table 4: Best performing models for SemEval Task 9: Sub Task A.
<R> <C> Method <C> Training Data <C> Test WER (%) simulated <C> Test WER (%) real <R> <C> AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) <C> simulated <C> 26.1 <C> 25.2 <R> <C> AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) <C> real <C> 37.3 <C> 35.2 <R> <C> AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) <C> simulated + real <C> 25.9 <C> 24.7 <R> <C> FSEGAN <C> simulated <C> 29.1 <C> 29.6 <CAP> TABLE III: WERs (%) of obtained using different training data of CHiME-4
<R> <C> Method <C> WER (%) <C> DCE <R> <C> No enhancement <C> 17.3 <C> 0.828 <R> <C> Wiener filter <C> 19.5 <C> 0.722 <R> <C> Minimizing DCE <C> 15.8 <C> [BOLD] 0.269 <R> <C> FSEGAN <C> 14.9 <C> 0.291 <R> <C> AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0) <C> 15.6 <C> 0.330 <R> <C> AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) <C> [BOLD] 14.4 <C> 0.303 <R> <C> Clean speech <C> 5.7 <C> 0.0 <CAP> TABLE I: WERs (%) and DCE of different speech enhancement methods on Librispeech + DEMAND test set
<R> <C> Method <C> WER (%) <C> DCE <R> <C> No enhancement <C> 38.4 <C> 0.958 <R> <C> Wiener filter <C> 41.0 <C> 0.775 <R> <C> Minimizing DCE <C> 31.1 <C> [BOLD] 0.392 <R> <C> FSEGAN <C> 29.1 <C> 0.421 <R> <C> AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0) <C> 27.7 <C> 0.476 <R> <C> AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) <C> [BOLD] 26.1 <C> 0.462 <R> <C> Clean speech <C> 9.3 <C> 0.0 <CAP> TABLE II: WERs (%) and DCE of different speech enhancement methods on CHiME4-simulated test set
<R> <C> System <C> Accuracy <C> Precision <C> Recall <C> F-Measure <R> <C> Local <C> 56.25% <C> 37.17% <C> 55.71% <C> 44.33% <R> <C> Manual <C> 65.00% <C> 47.82% <C> [BOLD] 55.77% <C> 50.63% <R> <C> Wiki <C> 63.25% <C> 42.07% <C> 46.67% <C> 44.00% <R> <C> Local-Manual <C> 64.50% <C> 46.90% <C> 51.86% <C> 48.47% <R> <C> Wiki-Manual <C> 62.25% <C> 43.56% <C> 52.63% <C> 46.93% <R> <C> Wiki-Manual <C> [BOLD] 68.75%∗∗∗ <C> 51.04% <C> 54.29% <C> [BOLD] 52.20%∗∗ <R> <C> [ITALIC] Our Approach <C> 68.50% <C> [BOLD] 51.39%∗∗∗ <C> 52.76% <C> 51.62% <CAP> TABLE IV: Results investigating RQ1 on the Nepal and Kerala datasets. (b) Kerala
<R> <C> Dataset <C> Unlabeled / Labeled Messages <C> Urgent / Non-urgent Messages <C> Unique Tokens <C> Avg. Tokens / Message <C> Time Range <R> <C> Nepal <C> 6,063/400 <C> 201/199 <C> 1,641 <C> 14 <C> 04/05/2015-05/06/2015 <R> <C> Macedonia <C> 0/205 <C> 92/113 <C> 129 <C> 18 <C> 09/18/2018-09/21/2018 <R> <C> Kerala <C> 92,046/400 <C> 125/275 <C> 19,393 <C> 15 <C> 08/17/2018-08/22/2018 <CAP> TABLE II: Details on datasets used for experiments.
<R> <C> System <C> Accuracy <C> Precision <C> Recall <C> F-Measure <R> <C> Local <C> 63.97% <C> 64.27% <C> 64.50% <C> 63.93% <R> <C> Manual <C> 64.25% <C> [BOLD] 70.84%∗∗ <C> 48.50% <C> 57.11% <R> <C> Wiki <C> 67.25% <C> 66.51% <C> 69.50% <C> 67.76% <R> <C> Local-Manual <C> 65.75% <C> 67.96% <C> 59.50% <C> 62.96% <R> <C> Wiki-Local <C> 67.40% <C> 65.54% <C> 68.50% <C> 66.80% <R> <C> Wiki-Manual <C> 67.75% <C> 70.38% <C> 63.00% <C> 65.79% <R> <C> [ITALIC] Our Approach <C> [BOLD] 69.25%∗∗∗ <C> 68.76% <C> [BOLD] 70.50%∗∗ <C> [BOLD] 69.44%∗∗∗ <CAP> TABLE IV: Results investigating RQ1 on the Nepal and Kerala datasets. (a) Nepal
<R> <C> System <C> Accuracy <C> Precision <C> Recall <C> F-Measure <R> <C> Local <C> 58.76% <C> 52.96% <C> 59.19% <C> 54.95% <R> <C> Transform <C> 58.62% <C> 51.40% <C> [BOLD] 60.32%∗ <C> 55.34% <R> <C> Upsample <C> 59.38% <C> 52.35% <C> 57.58% <C> 54.76% <R> <C> [ITALIC] Our Approach <C> [BOLD] 61.79%∗ <C> [BOLD] 55.08% <C> 59.19% <C> [BOLD] 56.90% <CAP> TABLE V: Results investigating RQ2 using the Nepal dataset as source and Macedonia dataset as target.
<R> <C> System <C> Accuracy <C> Precision <C> Recall <C> F-Measure <R> <C> Local <C> 58.76% <C> 52.96% <C> 59.19% <C> 54.95% <R> <C> Transform <C> 62.07% <C> 55.45% <C> 64.52% <C> 59.09% <R> <C> Upsample <C> [BOLD] 64.90%∗∗∗ <C> [BOLD] 57.98%∗ <C> [BOLD] 65.48%∗∗∗ <C> [BOLD] 61.30%∗∗∗ <R> <C> [ITALIC] Our Approach <C> 62.90% <C> 56.28% <C> 62.42% <C> 58.91% <CAP> TABLE VI: Results investigating RQ2 using the Kerala dataset as source and Macedonia dataset as target.
<R> <C> System <C> Accuracy <C> Precision <C> Recall <C> F-Measure <R> <C> Local <C> 58.65% <C> [BOLD] 42.40% <C> 47.47% <C> 36.88% <R> <C> Transform <C> 53.74% <C> 32.89% <C> [BOLD] 57.47%∗ <C> 41.42% <R> <C> Upsample <C> 53.88% <C> 31.71% <C> 56.32% <C> 40.32% <R> <C> [ITALIC] Our Approach <C> [BOLD] 58.79% <C> 35.26% <C> 55.89% <C> [BOLD] 43.03%∗ <CAP> TABLE VII: Results investigating RQ2 using the Nepal dataset as source and Kerala dataset as target.
<R> <C> [EMPTY] <C> [BOLD] Algorithm <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1 <R> <C> Giga <C> Baseline <C> 0.28 <C> 0.74 <C> 0.41 <R> <C> Giga <C> Threshold <C> 0.60 <C> 0.69 <C> [BOLD] 0.63 <R> <C> NOW <C> Baseline <C> 0.39 <C> 0.88 <C> 0.53 <R> <C> NOW <C> Threshold <C> 0.50 <C> 0.77 <C> [BOLD] 0.60 <CAP> Table 4: Average synchronic performance
<R> <C> [BOLD] Dataset <C> [BOLD] @1 <C> [BOLD] @5 <C> [BOLD] @10 <R> <C> Gigaword <C> 0.356 <C> 0.555 <C> 0.610 <R> <C> NOW <C> 0.442 <C> 0.557 <C> 0.578 <CAP> Table 2: Average recall of diachronic analogy inference
<R> <C> [EMPTY] <C> [BOLD] Algorithm <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1 <R> <C> Giga <C> Baseline <C> 0.19 <C> 0.51 <C> 0.28 <R> <C> Giga <C> Threshold <C> 0.46 <C> 0.41 <C> [BOLD] 0.41 <R> <C> NOW <C> Baseline <C> 0.26 <C> 0.53 <C> 0.34 <R> <C> NOW <C> Threshold <C> 0.42 <C> 0.41 <C> [BOLD] 0.41 <CAP> Table 3: Average diachronic performance
<R> <C> [BOLD] Model <C> [BOLD] Joint Acc. <R> <C> COMER <C> 88.64% <R> <C> - Hierachical-Attn <C> 86.69% <R> <C> - MLP <C> 83.24% <CAP> Table 4: The ablation study on the WoZ2.0 dataset with the joint goal accuracy on the test set. For “- Hierachical-Attn”, we remove the residual connections between the attention modules in the CMR decoders and all the attention memory access are based on the output from the LSTM. For “- MLP”, we further replace the MLP with a single linear layer with the non-linear activation.
<R> <C> [BOLD] DST Models <C> [BOLD] Joint Acc. WoZ 2.0 <C> [BOLD] Joint Acc. MultiWoZ <C> [BOLD] ITC <R> <C> Baselines Mrksic et al. ( 2017 ) <C> 70.8% <C> 25.83% <C> [ITALIC] O( [ITALIC] mn) <R> <C> NBT-CNN Mrksic et al. ( 2017 ) <C> 84.2% <C> - <C> [ITALIC] O( [ITALIC] mn) <R> <C> StateNet_PSI Ren et al. ( 2018 ) <C> [BOLD] 88.9% <C> - <C> [ITALIC] O( [ITALIC] n) <R> <C> GLAD Nouri and Hosseini-Asl ( 2018 ) <C> 88.5% <C> 35.58% <C> [ITALIC] O( [ITALIC] mn) <R> <C> HyST (ensemble) Goel et al. ( 2019 ) <C> - <C> 44.22% <C> [ITALIC] O( [ITALIC] n) <R> <C> DSTRead (ensemble) Gao et al. ( 2019 ) <C> - <C> 42.12% <C> [ITALIC] O( [ITALIC] n) <R> <C> TRADE Wu et al. ( 2019 ) <C> - <C> 48.62% <C> [ITALIC] O( [ITALIC] n) <R> <C> COMER <C> 88.6% <C> [BOLD] 48.79% <C> [ITALIC] O(1) <CAP> Table 3: The joint goal accuracy of the DST models on the WoZ2.0 test set and the MultiWoZ test set. We also include the Inference Time Complexity (ITC) for each model as a metric for scalability. The baseline accuracy for the WoZ2.0 dataset is the Delexicalisation-Based (DB) Model Mrksic et al. (2017), while the baseline for the MultiWoZ dataset is taken from the official website of MultiWoZ Budzianowski et al. (2018).
<R> <C> [BOLD] Model <C> [BOLD] JD Acc. <C> [BOLD] JDS Acc. <C> [BOLD] JG Acc. <R> <C> COMER <C> 95.52% <C> 55.81% <C> 48.79% <R> <C> - moveDrop <C> 95.34% <C> 55.08% <C> 47.19% <R> <C> - postprocess <C> 95.53% <C> 54.74% <C> 45.72% <R> <C> - ShareParam <C> 94.96% <C> 54.40% <C> 44.38% <R> <C> - Order <C> 95.55% <C> 55.06% <C> 42.84% <R> <C> - Nested <C> - <C> 49.58% <C> 40.57% <R> <C> - BlockGrad <C> - <C> 49.36% <C> 39.15% <CAP> Table 5: The ablation study on the MultiWoZ dataset with the joint domain accuracy (JD Acc.), joint domain-slot accuracy (JDS Acc.) and joint goal accuracy (JG Acc.) on the test set. For “- moveDrop”, we move the dropout layer to be in front of the final linear layer before the Softmax. For “- postprocess”, we further fix the decoder embedding layer and remove the post-processing during model evaluation. For “- ShareParam”, we further remove the parameter sharing mechanism on the encoders and the attention modules. For “- Order”, we further arrange the order of the slots according to its global frequencies in the training set instead of the local frequencies given the domain it belongs to. For “- Nested”, we do not generate domain sequences but generate combined slot sequences which combines the domain and the slot together. For “- BlockGrad”, we further remove the gradient blocking mechanism in the CMR decoder.
<R> <C> Source <C> Target <C> Svm <C> Ra-Svm‡ <C> Ra-Cnn‡ <C> Trans† <C> Ra-Trans‡† <C> Ours‡† <C> Oracle† <R> <C> Beer look + Beer aroma + Beer palate <C> Hotel location <C> 78.65 <C> 79.09 <C> 79.28 <C> 80.42 <C> 82.10 <C> [BOLD] 84.52 <C> 85.43 <R> <C> Beer look + Beer aroma + Beer palate <C> Hotel cleanliness <C> 86.44 <C> 86.68 <C> 89.01 <C> 86.95 <C> 87.15 <C> [BOLD] 90.66 <C> 92.09 <R> <C> Beer look + Beer aroma + Beer palate <C> Hotel service <C> 85.34 <C> 86.61 <C> 87.91 <C> 87.37 <C> 86.40 <C> [BOLD] 89.93 <C> 92.42 <CAP> Table 4: Accuracy of transferring between domains. Models with † use labeled data from source domains and unlabeled data from the target domain. Models with ‡ use human rationales on the target task.
<R> <C> Source <C> Target <C> Svm <C> Ra-Svm‡ <C> Ra-Cnn‡ <C> Trans† <C> Ra-Trans‡† <C> Ours‡† <C> Oracle† <R> <C> Beer aroma+palate <C> Beer look <C> 74.41 <C> 74.83 <C> 74.94 <C> 72.75 <C> 76.41 <C> [BOLD] 79.53 <C> 80.29 <R> <C> Beer look+palate <C> Beer aroma <C> 68.57 <C> 69.23 <C> 67.55 <C> 69.92 <C> 76.45 <C> [BOLD] 77.94 <C> 78.11 <R> <C> Beer look+aroma <C> Beer palate <C> 63.88 <C> 67.82 <C> 65.72 <C> 74.66 <C> 73.40 <C> [BOLD] 75.24 <C> 75.50 <CAP> Table 3: Accuracy of transferring between aspects. Models with † use labeled data from source aspects. Models with ‡ use human rationales on the target aspect.
<R> <C> Model <C> Hotel location <C> Hotel cleanliness <C> Hotel service <R> <C> Ours <C> [BOLD] 84.52 <C> [BOLD] 90.66 <C> [BOLD] 89.93 <R> <C> w/o L [ITALIC] wd <C> 82.36 <C> 89.79 <C> 89.61 <R> <C> w/o L [ITALIC] lm <C> 82.47 <C> 90.05 <C> 89.75 <CAP> Table 5: Ablation study on domain transfer from beer to hotel.
<R> <C> Model <C> Score <C> Inspec <C> Krapivin <C> NUS <C> KP20k <R> <C> Catseq(Ex) <C> F1@5 <C> 0.2350 <C> 0.2680 <C> 0.3330 <C> 0.2840 <R> <C> [EMPTY] <C> F1@M <C> 0.2864 <C> 0.3610 <C> 0.3982 <C> 0.3661 <R> <C> catSeq-RL(Ex.) <C> F1@5 <C> [BOLD] 0.2501 <C> [BOLD] 0.2870 <C> [BOLD] 0.3750 <C> [BOLD] 0.3100 <R> <C> [EMPTY] <C> F1@M <C> [BOLD] 0.3000 <C> 0.3630 <C> [BOLD] 0.4330 <C> [BOLD] 0.3830 <R> <C> GAN(Ex.) <C> F1@5 <C> 0.2481 <C> 0.2862 <C> 0.3681 <C> 0.3002 <R> <C> [EMPTY] <C> F1@M <C> 0.2970 <C> [BOLD] 0.3700 <C> 0.4300 <C> 0.3810 <R> <C> catSeq(Abs.) <C> F1@5 <C> 0.0045 <C> 0.0168 <C> 0.0126 <C> 0.0200 <R> <C> [EMPTY] <C> F1@M <C> 0.0085 <C> 0.0320 <C> 0.0170 <C> 0.0360 <R> <C> catSeq-RL(Abs.) <C> F1@5 <C> 0.0090 <C> [BOLD] 0.0262 <C> 0.0190 <C> 0.0240 <R> <C> [EMPTY] <C> F1@M <C> 0.0017 <C> [BOLD] 0.0460 <C> 0.0310 <C> 0.0440 <R> <C> GAN(Abs.) <C> F1@5 <C> [BOLD] 0.0100 <C> 0.0240 <C> [BOLD] 0.0193 <C> [BOLD] 0.0250 <R> <C> [EMPTY] <C> F1@M <C> [BOLD] 0.0190 <C> 0.0440 <C> [BOLD] 0.0340 <C> [BOLD] 0.0450 <CAP> Table 1: Extractive and Abstractive Keyphrase Metrics
<R> <C> Model <C> Inspec <C> Krapivin <C> NUS <C> KP20k <R> <C> Catseq <C> 0.87803 <C> 0.781 <C> 0.82118 <C> 0.804 <R> <C> Catseq-RL <C> 0.8602 <C> [BOLD] 0.786 <C> 0.83 <C> 0.809 <R> <C> GAN <C> [BOLD] 0.891 <C> 0.771 <C> [BOLD] 0.853 <C> [BOLD] 0.85 <CAP> Table 2: α-nDCG@5 metrics
<R> <C> Category <C> Female (%) <C> Male (%) <C> Neutral (%) <R> <C> Service <C> 10.5 <C> 59.548 <C> 16.476 <R> <C> STEM <C> 4.219 <C> 71.624 <C> 11.181 <R> <C> Farming / Fishing / Forestry <C> 12.179 <C> 62.179 <C> 14.744 <R> <C> Corporate <C> 9.167 <C> 66.042 <C> 14.861 <R> <C> Healthcare <C> 23.305 <C> 49.576 <C> 15.537 <R> <C> Legal <C> 11.905 <C> 72.619 <C> 10.714 <R> <C> Arts / Entertainment <C> 10.36 <C> 67.342 <C> 11.486 <R> <C> Education <C> 23.485 <C> 53.03 <C> 9.091 <R> <C> Production <C> 14.331 <C> 51.199 <C> 18.245 <R> <C> Construction / Extraction <C> 8.578 <C> 61.887 <C> 17.525 <R> <C> Total <C> 11.76 <C> 58.93 <C> 15.939 <CAP> Table 7: Percentage of female, male and neutral gender pronouns obtained for each of the merged occupation category, averaged over all occupations in said category and tested languages detailed in Table
<R> <C> Category <C> Female (%) <C> Male (%) <C> Neutral (%) <R> <C> Office and administrative support <C> 11.015 <C> 58.812 <C> 16.954 <R> <C> Architecture and engineering <C> 2.299 <C> 72.701 <C> 10.92 <R> <C> Farming, fishing, and forestry <C> 12.179 <C> 62.179 <C> 14.744 <R> <C> Management <C> 11.232 <C> 66.667 <C> 12.681 <R> <C> Community and social service <C> 20.238 <C> 62.5 <C> 10.119 <R> <C> Healthcare support <C> 25.0 <C> 43.75 <C> 17.188 <R> <C> Sales and related <C> 8.929 <C> 62.202 <C> 16.964 <R> <C> Installation, maintenance, and repair <C> 5.22 <C> 58.333 <C> 17.125 <R> <C> Transportation and material moving <C> 8.81 <C> 62.976 <C> 17.5 <R> <C> Legal <C> 11.905 <C> 72.619 <C> 10.714 <R> <C> Business and financial operations <C> 7.065 <C> 67.935 <C> 15.58 <R> <C> Life, physical, and social science <C> 5.882 <C> 73.284 <C> 10.049 <R> <C> Arts, design, entertainment, sports, and media <C> 10.36 <C> 67.342 <C> 11.486 <R> <C> Education, training, and library <C> 23.485 <C> 53.03 <C> 9.091 <R> <C> Building and grounds cleaning and maintenance <C> 12.5 <C> 68.333 <C> 11.667 <R> <C> Personal care and service <C> 18.939 <C> 49.747 <C> 18.434 <R> <C> Healthcare practitioners and technical <C> 22.674 <C> 51.744 <C> 15.116 <R> <C> Production <C> 14.331 <C> 51.199 <C> 18.245 <R> <C> Computer and mathematical <C> 4.167 <C> 66.146 <C> 14.062 <R> <C> Construction and extraction <C> 8.578 <C> 61.887 <C> 17.525 <R> <C> Protective service <C> 8.631 <C> 65.179 <C> 12.5 <R> <C> Food preparation and serving related <C> 21.078 <C> 58.333 <C> 17.647 <R> <C> Total <C> 11.76 <C> 58.93 <C> 15.939 <CAP> Table 6: Percentage of female, male and neutral gender pronouns obtained for each BLS occupation category, averaged over all occupations in said category and tested languages detailed in Table
<R> <C> Dataset <C> Class <C> ˆ [ITALIC] piblack <C> ˆ [ITALIC] piwhite <C> [ITALIC] t <C> [ITALIC] p <C> ˆ [ITALIC] piblackˆ [ITALIC] piwhite <R> <C> [ITALIC] Waseem and Hovy <C> Racism <C> 0.010 <C> 0.010 <C> -0.632 <C> [EMPTY] <C> 0.978 <R> <C> [EMPTY] <C> Sexism <C> 0.963 <C> 0.944 <C> 20.064 <C> *** <C> 1.020 <R> <C> [ITALIC] Waseem <C> Racism <C> 0.011 <C> 0.011 <C> -1.254 <C> [EMPTY] <C> 0.955 <R> <C> [EMPTY] <C> Sexism <C> 0.349 <C> 0.290 <C> 28.803 <C> *** <C> 1.203 <R> <C> [EMPTY] <C> Racism and sexism <C> 0.012 <C> 0.012 <C> -0.162 <C> [EMPTY] <C> 0.995 <R> <C> [ITALIC] Davidson et al. <C> Hate <C> 0.017 <C> 0.015 <C> 4.698 <C> *** <C> 1.152 <R> <C> [EMPTY] <C> Offensive <C> 0.988 <C> 0.991 <C> -6.289 <C> *** <C> 0.997 <R> <C> [ITALIC] Golbeck et al. <C> Harassment <C> 0.099 <C> 0.091 <C> 6.273 <C> *** <C> 1.091 <R> <C> [ITALIC] Founta et al. <C> Hate <C> 0.074 <C> 0.027 <C> 46.054 <C> *** <C> 2.728 <R> <C> [EMPTY] <C> Abusive <C> 0.925 <C> 0.968 <C> -41.396 <C> *** <C> 0.956 <R> <C> [EMPTY] <C> Spam <C> 0.010 <C> 0.010 <C> 0.000 <C> [EMPTY] <C> 1.000 <CAP> Table 4: Experiment 2, t= “b*tch”
<R> <C> Dataset <C> Class <C> Precision <C> Recall <C> F1 <R> <C> [ITALIC] W. & H. <C> Racism <C> 0.73 <C> 0.79 <C> 0.76 <R> <C> [EMPTY] <C> Sexism <C> 0.69 <C> 0.73 <C> 0.71 <R> <C> [EMPTY] <C> Neither <C> 0.88 <C> 0.85 <C> 0.86 <R> <C> [ITALIC] W. <C> Racism <C> 0.56 <C> 0.77 <C> 0.65 <R> <C> [EMPTY] <C> Sexism <C> 0.62 <C> 0.73 <C> 0.67 <R> <C> [EMPTY] <C> R. & S. <C> 0.56 <C> 0.62 <C> 0.59 <R> <C> [EMPTY] <C> Neither <C> 0.95 <C> 0.92 <C> 0.94 <R> <C> [ITALIC] D. et al. <C> Hate <C> 0.32 <C> 0.53 <C> 0.4 <R> <C> [EMPTY] <C> Offensive <C> 0.96 <C> 0.88 <C> 0.92 <R> <C> [EMPTY] <C> Neither <C> 0.81 <C> 0.95 <C> 0.87 <R> <C> [ITALIC] G. et al. <C> Harass. <C> 0.41 <C> 0.19 <C> 0.26 <R> <C> [EMPTY] <C> Non. <C> 0.75 <C> 0.9 <C> 0.82 <R> <C> [ITALIC] F. et al. <C> Hate <C> 0.33 <C> 0.42 <C> 0.37 <R> <C> [EMPTY] <C> Abusive <C> 0.87 <C> 0.88 <C> 0.88 <R> <C> [EMPTY] <C> Spam <C> 0.5 <C> 0.7 <C> 0.58 <R> <C> [EMPTY] <C> Neither <C> 0.88 <C> 0.77 <C> 0.82 <CAP> Table 1: Classifier performance
<R> <C> Dataset <C> Class <C> ˆ [ITALIC] piblack <C> ˆ [ITALIC] piwhite <C> [ITALIC] t <C> [ITALIC] p <C> ˆ [ITALIC] piblackˆ [ITALIC] piwhite <R> <C> [ITALIC] Waseem and Hovy <C> Racism <C> 0.001 <C> 0.003 <C> -20.818 <C> *** <C> 0.505 <R> <C> [EMPTY] <C> Sexism <C> 0.083 <C> 0.048 <C> 101.636 <C> *** <C> 1.724 <R> <C> [ITALIC] Waseem <C> Racism <C> 0.001 <C> 0.001 <C> 0.035 <C> [EMPTY] <C> 1.001 <R> <C> [EMPTY] <C> Sexism <C> 0.023 <C> 0.012 <C> 64.418 <C> *** <C> 1.993 <R> <C> [EMPTY] <C> Racism and sexism <C> 0.002 <C> 0.001 <C> 4.047 <C> *** <C> 1.120 <R> <C> [ITALIC] Davidson et al. <C> Hate <C> 0.049 <C> 0.019 <C> 120.986 <C> *** <C> 2.573 <R> <C> [EMPTY] <C> Offensive <C> 0.173 <C> 0.065 <C> 243.285 <C> *** <C> 2.653 <R> <C> [ITALIC] Golbeck et al. <C> Harassment <C> 0.032 <C> 0.023 <C> 39.483 <C> *** <C> 1.396 <R> <C> [ITALIC] Founta et al. <C> Hate <C> 0.111 <C> 0.061 <C> 122.707 <C> *** <C> 1.812 <R> <C> [EMPTY] <C> Abusive <C> 0.178 <C> 0.080 <C> 211.319 <C> *** <C> 2.239 <R> <C> [EMPTY] <C> Spam <C> 0.028 <C> 0.015 <C> 63.131 <C> *** <C> 1.854 <CAP> Table 2: Experiment 1
<R> <C> Dataset <C> Class <C> ˆ [ITALIC] piblack <C> ˆ [ITALIC] piwhite <C> [ITALIC] t <C> [ITALIC] p <C> ˆ [ITALIC] piblackˆ [ITALIC] piwhite <R> <C> [ITALIC] Waseem and Hovy <C> Racism <C> 0.010 <C> 0.011 <C> -1.462 <C> [EMPTY] <C> 0.960 <R> <C> [EMPTY] <C> Sexism <C> 0.147 <C> 0.100 <C> 31.932 <C> *** <C> 1.479 <R> <C> [ITALIC] Waseem <C> Racism <C> 0.010 <C> 0.010 <C> 0.565 <C> [EMPTY] <C> 1.027 <R> <C> [EMPTY] <C> Sexism <C> 0.040 <C> 0.026 <C> 18.569 <C> *** <C> 1.554 <R> <C> [EMPTY] <C> Racism and sexism <C> 0.011 <C> 0.010 <C> 0.835 <C> [EMPTY] <C> 1.026 <R> <C> [ITALIC] Davidson et al. <C> Hate <C> 0.578 <C> 0.645 <C> -31.248 <C> *** <C> 0.896 <R> <C> [EMPTY] <C> Offensive <C> 0.418 <C> 0.347 <C> 32.895 <C> *** <C> 1.202 <R> <C> [ITALIC] Golbeck et al. <C> Harassment <C> 0.085 <C> 0.078 <C> 5.984 <C> *** <C> 1.096 <R> <C> [ITALIC] Founta et al. <C> Hate <C> 0.912 <C> 0.930 <C> -15.037 <C> *** <C> 0.980 <R> <C> [EMPTY] <C> Abusive <C> 0.086 <C> 0.067 <C> 16.131 <C> *** <C> 1.296 <R> <C> [EMPTY] <C> Spam <C> 0.010 <C> 0.010 <C> -1.593 <C> [EMPTY] <C> 1.000 <CAP> Table 3: Experiment 2, t= “n*gga”
<R> <C> [EMPTY] <C> MSCOCO spice <C> MSCOCO cider <C> MSCOCO rouge [ITALIC] L <C> MSCOCO bleu4 <C> MSCOCO meteor <C> MSCOCO rep↓ <C> Flickr30k spice <C> Flickr30k cider <C> Flickr30k rouge [ITALIC] L <C> Flickr30k bleu4 <C> Flickr30k meteor <C> Flickr30k rep↓ <R> <C> softmax <C> 18.4 <C> 0.967 <C> 52.9 <C> 29.9 <C> 24.9 <C> 3.76 <C> 13.5 <C> 0.443 <C> 44.2 <C> 19.9 <C> 19.1 <C> 6.09 <R> <C> sparsemax <C> [BOLD] 18.9 <C> [BOLD] 0.990 <C> [BOLD] 53.5 <C> [BOLD] 31.5 <C> [BOLD] 25.3 <C> 3.69 <C> [BOLD] 13.7 <C> [BOLD] 0.444 <C> [BOLD] 44.3 <C> [BOLD] 20.7 <C> [BOLD] 19.3 <C> 5.84 <R> <C> TVmax <C> 18.5 <C> 0.974 <C> 53.1 <C> 29.9 <C> 25.1 <C> [BOLD] 3.17 <C> 13.3 <C> 0.438 <C> 44.2 <C> 20.5 <C> 19.0 <C> [BOLD] 3.97 <CAP> Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.
<R> <C> [EMPTY] <C> caption <C> attention relevance <R> <C> softmax <C> 3.50 <C> 3.38 <R> <C> sparsemax <C> 3.71 <C> 3.89 <R> <C> TVmax <C> [BOLD] 3.87 <C> [BOLD] 4.10 <CAP> Table 2: Human evaluation results on MSCOCO.
<R> <C> [EMPTY] <C> Att. to image <C> Att. to bounding boxes <C> Test-Dev Yes/No <C> Test-Dev Number <C> Test-Dev Other <C> Test-Dev Overall <C> Test-Standard Yes/No <C> Test-Standard Number <C> Test-Standard Other <C> Test-Standard Overall <R> <C> softmax <C> ✓ <C> [EMPTY] <C> 83.08 <C> 42.65 <C> 55.74 <C> 65.52 <C> 83.55 <C> 42.68 <C> 56.01 <C> 65.97 <R> <C> sparsemax <C> ✓ <C> [EMPTY] <C> 83.08 <C> 43.19 <C> 55.79 <C> 65.60 <C> 83.33 <C> 42.99 <C> 56.06 <C> 65.94 <R> <C> soft-TVmax <C> ✓ <C> [EMPTY] <C> 83.13 <C> 43.53 <C> 56.01 <C> 65.76 <C> 83.63 <C> 43.24 <C> 56.10 <C> 66.11 <R> <C> sparse-TVmax <C> ✓ <C> [EMPTY] <C> 83.10 <C> 43.30 <C> 56.14 <C> 65.79 <C> 83.66 <C> 43.18 <C> 56.21 <C> 66.17 <R> <C> softmax <C> [EMPTY] <C> ✓ <C> 85.14 <C> 49.59 <C> 58.72 <C> 68.57 <C> 85.56 <C> 49.54 <C> 59.11 <C> 69.04 <R> <C> sparsemax <C> [EMPTY] <C> ✓ <C> [BOLD] 85.40 <C> [BOLD] 50.87 <C> 58.67 <C> 68.79 <C> [BOLD] 85.80 <C> 50.18 <C> 59.08 <C> 69.19 <R> <C> softmax <C> ✓ <C> ✓ <C> 85.33 <C> 50.49 <C> 58.88 <C> 68.82 <C> 85.58 <C> 50.42 <C> 59.18 <C> 69.17 <R> <C> sparse-TVmax <C> ✓ <C> ✓ <C> 85.35 <C> 50.52 <C> [BOLD] 59.15 <C> [BOLD] 68.96 <C> 85.72 <C> [BOLD] 50.66 <C> [BOLD] 59.22 <C> [BOLD] 69.28 <CAP> Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.
<R> <C> [BOLD] Language <C> [BOLD] Random Character  [BOLD] P@1 <C> [BOLD] Random Character  [BOLD] P@10 <C> [BOLD] Characters Swap  [BOLD] P@1 <C> [BOLD] Characters Swap  [BOLD] P@10 <C> [BOLD] Character Bigrams  [BOLD] P@1 <C> [BOLD] Character Bigrams  [BOLD] P@10 <R> <C> Bengali <C> 91.243 <C> 99.493 <C> 82.580 <C> 99.170 <C> 93.694 <C> 99.865 <R> <C> Czech <C> 94.035 <C> 99.264 <C> 91.560 <C> 99.154 <C> 97.795 <C> 99.909 <R> <C> Danish <C> 84.605 <C> 98.435 <C> 71.805 <C> 97.160 <C> 90.103 <C> 99.444 <R> <C> Dutch <C> 85.332 <C> 98.448 <C> 72.800 <C> 96.675 <C> 91.159 <C> 99.305 <R> <C> English <C> 97.260 <C> 99.897 <C> 93.220 <C> 99.700 <C> 98.050 <C> 99.884 <R> <C> Finnish <C> 97.735 <C> 99.855 <C> 94.510 <C> 99.685 <C> 98.681 <C> 99.972 <R> <C> French <C> 84.332 <C> 98.483 <C> 72.570 <C> 97.215 <C> 91.165 <C> 99.412 <R> <C> German <C> 86.870 <C> 98.882 <C> 73.920 <C> 97.550 <C> 91.448 <C> 99.509 <R> <C> Greek <C> 82.549 <C> 97.800 <C> 71.925 <C> 96.910 <C> 90.291 <C> 99.386 <R> <C> Hebrew <C> 94.180 <C> 99.672 <C> 88.491 <C> 99.201 <C> 95.414 <C> 99.706 <R> <C> Hindi <C> 81.610 <C> 97.638 <C> 67.730 <C> 96.200 <C> 86.274 <C> 99.169 <R> <C> Indonesian <C> 94.735 <C> 99.838 <C> 89.035 <C> 99.560 <C> 96.745 <C> 99.910 <R> <C> Italian <C> 88.865 <C> 99.142 <C> 78.765 <C> 98.270 <C> 93.400 <C> 99.775 <R> <C> Marathi <C> 92.392 <C> 99.493 <C> 85.145 <C> 99.025 <C> 95.449 <C> 99.905 <R> <C> Polish <C> 94.918 <C> 99.743 <C> 90.280 <C> 99.705 <C> 97.454 <C> 99.954 <R> <C> Portuguese <C> 86.422 <C> 98.903 <C> 71.735 <C> 97.685 <C> 90.787 <C> 99.562 <R> <C> Romanian <C> 94.925 <C> 99.575 <C> 90.805 <C> 99.245 <C> 97.119 <C> 99.845 <R> <C> Russian <C> 93.285 <C> 99.502 <C> 89.000 <C> 99.240 <C> 97.196 <C> 99.942 <R> <C> Spanish <C> 84.535 <C> 98.210 <C> 71.345 <C> 96.645 <C> 90.395 <C> 99.246 <R> <C> Swedish <C> 87.195 <C> 98.865 <C> 76.940 <C> 97.645 <C> 92.828 <C> 99.656 <R> <C> Tamil <C> 98.118 <C> 99.990 <C> 96.920 <C> 99.990 <C> 99.284 <C> 99.999 <R> <C> Telugu <C> 97.323 <C> 99.990 <C> 93.935 <C> 99.985 <C> 97.897 <C> 99.998 <R> <C> Thai <C> 97.989 <C> 99.755 <C> 97.238 <C> 99.448 <C> 98.859 <C> 99.986 <R> <C> Turkish <C> 97.045 <C> 99.880 <C> 93.195 <C> 99.815 <C> 98.257 <C> 99.972 <CAP> TABLE IV: Synthetic Data Performance on three error generation algorithm
<R> <C> [BOLD] Token <C> [BOLD] Trie <C> [BOLD] DAWGs <C> [BOLD] SDA <R> <C> 3 <C> 170.50 <C> 180.98 <C> 112.31 <R> <C> 4 <C> 175.04 <C> 178.78 <C> 52.97 <R> <C> 5 <C> 220.44 <C> 225.10 <C> 25.44 <R> <C> 6 <C> 254.57 <C> 259.54 <C> 7.44 <R> <C> 7 <C> 287.19 <C> 291.99 <C> 4.59 <R> <C> 8 <C> 315.78 <C> 321.58 <C> 2.58 <R> <C> 9 <C> 351.19 <C> 356.76 <C> 1.91 <R> <C> 10 <C> 379.99 <C> 386.04 <C> 1.26 <R> <C> 11 <C> 412.02 <C> 419.55 <C> 1.18 <R> <C> 12 <C> 436.54 <C> 443.85 <C> 1.06 <R> <C> 13 <C> 473.45 <C> 480.26 <C> 1.16 <R> <C> 14 <C> 508.08 <C> 515.04 <C> 0.97 <R> <C> 15 <C> 548.04 <C> 553.49 <C> 0.66 <R> <C> 16 <C> 580.44 <C> 584.99 <C> 0.37 <CAP> TABLE I: Average Time taken by suggestion generation algorithms (Edit Distance = 2) (in millisecond)
<R> <C> [BOLD] Language <C> [BOLD] # Test <C> [BOLD] P@1 <C> [BOLD] P@3 <C> [BOLD] P@5 <C> [BOLD] P@10 <C> [BOLD] MRR <R> <C> [BOLD] Language <C> [BOLD] Samples <C> [BOLD] P@1 <C> [BOLD] P@3 <C> [BOLD] P@5 <C> [BOLD] P@10 <C> [BOLD] MRR <R> <C> Bengali <C> 140000 <C> 91.30 <C> 97.83 <C> 98.94 <C> 99.65 <C> 94.68 <R> <C> Czech <C> 94205 <C> 95.84 <C> 98.72 <C> 99.26 <C> 99.62 <C> 97.37 <R> <C> Danish <C> 140000 <C> 85.84 <C> 95.19 <C> 97.28 <C> 98.83 <C> 90.85 <R> <C> Dutch <C> 140000 <C> 86.83 <C> 95.01 <C> 97.04 <C> 98.68 <C> 91.32 <R> <C> English <C> 140000 <C> 97.08 <C> 99.39 <C> 99.67 <C> 99.86 <C> 98.27 <R> <C> Finnish <C> 140000 <C> 97.77 <C> 99.58 <C> 99.79 <C> 99.90 <C> 98.69 <R> <C> French <C> 140000 <C> 86.52 <C> 95.66 <C> 97.52 <C> 98.83 <C> 91.38 <R> <C> German <C> 140000 <C> 87.58 <C> 96.16 <C> 97.86 <C> 99.05 <C> 92.10 <R> <C> Greek <C> 30022 <C> 84.95 <C> 94.99 <C> 96.88 <C> 98.44 <C> 90.27 <R> <C> Hebrew <C> 132596 <C> 94.00 <C> 98.26 <C> 99.05 <C> 99.62 <C> 96.24 <R> <C> Hindi <C> 140000 <C> 82.19 <C> 93.71 <C> 96.28 <C> 98.30 <C> 88.40 <R> <C> Indonesian <C> 140000 <C> 95.01 <C> 98.98 <C> 99.50 <C> 99.84 <C> 97.04 <R> <C> Italian <C> 140000 <C> 89.93 <C> 97.31 <C> 98.54 <C> 99.38 <C> 93.76 <R> <C> Marathi <C> 140000 <C> 93.01 <C> 98.16 <C> 99.06 <C> 99.66 <C> 95.69 <R> <C> Polish <C> 140000 <C> 95.65 <C> 99.17 <C> 99.62 <C> 99.86 <C> 97.44 <R> <C> Portuguese <C> 140000 <C> 86.73 <C> 96.29 <C> 97.94 <C> 99.10 <C> 91.74 <R> <C> Romanian <C> 140000 <C> 95.52 <C> 98.79 <C> 99.32 <C> 99.68 <C> 97.22 <R> <C> Russian <C> 140000 <C> 94.85 <C> 98.74 <C> 99.33 <C> 99.71 <C> 96.86 <R> <C> Spanish <C> 140000 <C> 85.91 <C> 95.35 <C> 97.18 <C> 98.57 <C> 90.92 <R> <C> Swedish <C> 140000 <C> 88.86 <C> 96.40 <C> 98.00 <C> 99.14 <C> 92.87 <R> <C> Tamil <C> 140000 <C> 98.05 <C> 99.70 <C> 99.88 <C> 99.98 <C> 98.88 <R> <C> Telugu <C> 140000 <C> 97.11 <C> 99.68 <C> 99.92 <C> 99.99 <C> 98.38 <R> <C> Thai <C> 12403 <C> 98.73 <C> 99.71 <C> 99.78 <C> 99.85 <C> 99.22 <R> <C> Turkish <C> 140000 <C> 97.13 <C> 99.51 <C> 99.78 <C> 99.92 <C> 98.33 <CAP> TABLE II: Synthetic Data Performance results
<R> <C> [BOLD] Language <C> [BOLD] Detection  [BOLD] Time ( [ITALIC] μs) <C> [BOLD] Suggestion Time  [BOLD] ED=1 (ms) <C> [BOLD] Suggestion Time  [BOLD] ED=2 (ms) <C> [BOLD] Ranking  [BOLD] Time (ms) <R> <C> Bengali <C> 7.20 <C> 0.48 <C> 14.85 <C> 1.14 <R> <C> Czech <C> 7.81 <C> 0.75 <C> 26.67 <C> 2.34 <R> <C> Danish <C> 7.28 <C> 0.67 <C> 23.70 <C> 1.96 <R> <C> Dutch <C> 10.80 <C> 0.81 <C> 30.44 <C> 2.40 <R> <C> English <C> 7.27 <C> 0.79 <C> 39.36 <C> 2.35 <R> <C> Finnish <C> 8.53 <C> 0.46 <C> 15.55 <C> 1.05 <R> <C> French <C> 7.19 <C> 0.82 <C> 32.02 <C> 2.69 <R> <C> German <C> 8.65 <C> 0.85 <C> 41.18 <C> 2.63 <R> <C> Greek <C> 7.63 <C> 0.86 <C> 25.40 <C> 1.87 <R> <C> Hebrew <C> 22.35 <C> 1.01 <C> 49.91 <C> 2.18 <R> <C> Hindi <C> 8.50 <C> 0.60 <C> 18.51 <C> 1.72 <R> <C> Indonesian <C> 12.00 <C> 0.49 <C> 20.75 <C> 1.22 <R> <C> Italian <C> 6.92 <C> 0.72 <C> 29.02 <C> 2.17 <R> <C> Marathi <C> 7.16 <C> 0.43 <C> 10.68 <C> 0.97 <R> <C> Polish <C> 6.44 <C> 0.64 <C> 24.15 <C> 1.74 <R> <C> Portuguese <C> 7.14 <C> 0.66 <C> 28.92 <C> 2.20 <R> <C> Romanian <C> 10.26 <C> 0.63 <C> 18.83 <C> 1.79 <R> <C> Russian <C> 6.79 <C> 0.68 <C> 22.56 <C> 1.72 <R> <C> Spanish <C> 7.19 <C> 0.75 <C> 31.00 <C> 2.41 <R> <C> Swedish <C> 7.76 <C> 0.83 <C> 32.17 <C> 2.57 <R> <C> Tamil <C> 11.34 <C> 0.23 <C> 4.83 <C> 0.31 <R> <C> Telugu <C> 6.31 <C> 0.29 <C> 7.50 <C> 0.54 <R> <C> Thai <C> 11.60 <C> 0.66 <C> 18.75 <C> 1.33 <R> <C> Turkish <C> 7.40 <C> 0.49 <C> 17.42 <C> 1.23 <CAP> TABLE III: Synthetic Data Time Performance results
<R> <C> [EMPTY] <C> [BOLD] P@1 <C> [BOLD] P@3 <C> [BOLD] P@5 <C> [BOLD] P@10 <R> <C> Aspell <C> 60.82 <C> 80.81 <C> 87.26 <C> 91.35 <R> <C> Hunspell <C> 61.34 <C> 77.86 <C> 83.47 <C> 87.04 <R> <C> [ITALIC] Ours <C> 68.99 <C> 83.43 <C> 87.03 <C> 90.16 <CAP> TABLE VI: Public dataset comparison results
<R> <C> [BOLD] Language <C> [BOLD] # Sentences <C> [BOLD] # Total Words <C> [BOLD] # Detected <C> [BOLD] % <R> <C> Bengali <C> 663748 <C> 457140 <C> 443650 <C> 97.05 <R> <C> Czech <C> 6128 <C> 36846 <C> 36072 <C> 97.90 <R> <C> Danish <C> 16198 <C> 102883 <C> 101798 <C> 98.95 <R> <C> Dutch <C> 55125 <C> 1048256 <C> 1004274 <C> 95.80 <R> <C> English <C> 239555 <C> 4981604 <C> 4907733 <C> 98.52 <R> <C> Finnish <C> 3757 <C> 43457 <C> 39989 <C> 92.02 <R> <C> French <C> 164916 <C> 3244367 <C> 3187587 <C> 98.25 <R> <C> German <C> 71025 <C> 1283239 <C> 1250232 <C> 97.43 <R> <C> Greek <C> 1586 <C> 43035 <C> 42086 <C> 97.79 <R> <C> Hebrew <C> 95813 <C> 505335 <C> 494481 <C> 97.85 <R> <C> Hindi <C> 5089 <C> 37617 <C> 37183 <C> 98.85 <R> <C> Indonesian <C> 100248 <C> 84347 <C> 82809 <C> 98.18 <R> <C> Italian <C> 36026 <C> 718774 <C> 703514 <C> 97.88 <R> <C> Marathi <C> 17007 <C> 84286 <C> 79866 <C> 94.76 <R> <C> Polish <C> 3283 <C> 34226 <C> 32780 <C> 95.78 <R> <C> Portuguese <C> 1453 <C> 25568 <C> 25455 <C> 99.56 <R> <C> Romanian <C> 4786 <C> 34862 <C> 34091 <C> 97.79 <R> <C> Russian <C> 27252 <C> 384262 <C> 372979 <C> 97.06 <R> <C> Spanish <C> 108017 <C> 2057481 <C> 2028951 <C> 98.61 <R> <C> Swedish <C> 3209 <C> 66191 <C> 64649 <C> 97.67 <R> <C> Tamil <C> 40165 <C> 21044 <C> 19526 <C> 92.79 <R> <C> Telugu <C> 30466 <C> 17710 <C> 17108 <C> 96.60 <R> <C> Thai <C> 16032 <C> 67507 <C> 49744 <C> 73.69 <R> <C> Turkish <C> 163910 <C> 794098 <C> 775776 <C> 97.69 <CAP> TABLE VII: False Positive Experiment Results
<R> <C> [BOLD] Test <C> F&B <C> A <C> R <C> Ca <C> Se <C> So <C> T <C> E <C> O <R> <C> [BOLD] Train <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Food & Bev. <C> – <C> 58.1 <C> 52.5 <C> 66.4 <C> 59.7 <C> 58.9 <C> 54.1 <C> 61.4 <C> 53.7 <R> <C> Apparel <C> 63.9 <C> – <C> 74.4 <C> 65.1 <C> 70.8 <C> 71.2 <C> 68.5 <C> 76.9 <C> 85.6 <R> <C> Retail <C> 58.8 <C> 74.4 <C> – <C> 70.1 <C> 72.6 <C> 69.9 <C> 68.7 <C> 69.6 <C> 82.7 <R> <C> Cars <C> 68.7 <C> 61.1 <C> 65.1 <C> – <C> 58.8 <C> 67. <C> 59.3 <C> 62.9 <C> 68.2 <R> <C> Services <C> 65. <C> 74.2 <C> 75.8 <C> 74. <C> – <C> 68.8 <C> 74.2 <C> 77.9 <C> 77.9 <R> <C> Software <C> 62. <C> 74.2 <C> 68. <C> 67.9 <C> 72.8 <C> – <C> 72.8 <C> 72.1 <C> 80.6 <R> <C> Transport <C> 59.3 <C> 71.7 <C> 72.4 <C> 67. <C> 74.6 <C> 75. <C> – <C> 72.6 <C> 81.7 <R> <C> Electronics <C> 61.6 <C> 75.2 <C> 71. <C> 68. <C> 75. <C> 69.9 <C> 68.2 <C> – <C> 78.7 <R> <C> Other <C> 56.1 <C> 71.3 <C> 72.4 <C> 70.2 <C> 73.5 <C> 67.2 <C> 68.5 <C> 71. <C> – <R> <C> All <C> 70.3 <C> 77.7 <C> 79.5 <C> 82.0 <C> 79.6 <C> 80.1 <C> 76.8 <C> 81.7 <C> 88.2 <CAP> Table 9: Performance of models trained with tweets from one domain and tested on other domains. All results are reported in ROC AUC. The All line displays results on training on all categories except the category in testing.
<R> <C> [BOLD] Category <C> [BOLD] Complaints <C> [BOLD] Not Complaints <R> <C> Food & Beverage <C> 95 <C> 35 <R> <C> Apparel <C> 141 <C> 117 <R> <C> Retail <C> 124 <C> 75 <R> <C> Cars <C> 67 <C> 25 <R> <C> Services <C> 207 <C> 130 <R> <C> Software & Online Services <C> 189 <C> 103 <R> <C> Transport <C> 139 <C> 109 <R> <C> Electronics <C> 174 <C> 112 <R> <C> Other <C> 96 <C> 33 <R> <C> Total <C> 1232 <C> 739 <CAP> Table 3: Number of tweets annotated as complaints across the nine domains.
<R> <C> [BOLD] Complaints  [BOLD] Feature <C> [BOLD] Complaints  [ITALIC] r <C> [BOLD] Not Complaints  [BOLD] Feature <C> [BOLD] Not Complaints  [ITALIC] r <R> <C> [BOLD] Unigrams <C> [BOLD] Unigrams <C> [BOLD] Unigrams <C> [BOLD] Unigrams <R> <C> not <C> .154 <C> [URL] <C> .150 <R> <C> my <C> .131 <C> ! <C> .082 <R> <C> working <C> .124 <C> he <C> .069 <R> <C> still <C> .123 <C> thank <C> .067 <R> <C> on <C> .119 <C> , <C> .064 <R> <C> can’t <C> .113 <C> love <C> .064 <R> <C> service <C> .112 <C> lol <C> .061 <R> <C> customer <C> .109 <C> you <C> .060 <R> <C> why <C> .108 <C> great <C> .058 <R> <C> website <C> .107 <C> win <C> .058 <R> <C> no <C> .104 <C> ’ <C> .058 <R> <C> ? <C> .098 <C> she <C> .054 <R> <C> fix <C> .093 <C> : <C> .053 <R> <C> won’t <C> .092 <C> that <C> .053 <R> <C> been <C> .090 <C> more <C> .052 <R> <C> issue <C> .089 <C> it <C> .052 <R> <C> days <C> .088 <C> would <C> .051 <R> <C> error <C> .087 <C> him <C> .047 <R> <C> is <C> .084 <C> life <C> .046 <R> <C> charged <C> .083 <C> good <C> .046 <R> <C> [BOLD] POS (Unigrams and Bigrams) <C> [BOLD] POS (Unigrams and Bigrams) <C> [BOLD] POS (Unigrams and Bigrams) <C> [BOLD] POS (Unigrams and Bigrams) <R> <C> VBN <C> .141 <C> UH <C> .104 <R> <C> $ <C> .118 <C> NNP <C> .098 <R> <C> VBZ <C> .114 <C> PRP <C> .076 <R> <C> NN_VBZ <C> .114 <C> HT <C> .076 <R> <C> PRP$ <C> .107 <C> PRP_. <C> .076 <R> <C> PRP$_NN <C> .105 <C> PRP_RB <C> .067 <R> <C> VBG <C> .093 <C> NNP_NNP <C> .062 <R> <C> CD <C> .092 <C> VBP_PRP <C> .054 <R> <C> WRB_VBZ <C> .084 <C> JJ <C> .053 <R> <C> VBZ_VBN <C> .084 <C> DT_JJ <C> .051 <CAP> Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p
<R> <C> [BOLD] Complaints  [BOLD] Label <C> [BOLD] Complaints  [BOLD] Words <C> [BOLD] Complaints  [ITALIC] r <C> [BOLD] Not Complaints  [BOLD] Label <C> [BOLD] Not Complaints  [BOLD] Words <C> [BOLD] Not Complaints  [ITALIC] r <R> <C> [BOLD] LIWC Features <C> [BOLD] LIWC Features <C> [BOLD] LIWC Features <C> [BOLD] LIWC Features <C> [BOLD] LIWC Features <C> [BOLD] LIWC Features <R> <C> NEGATE <C> not, no, can’t, don’t, never, nothing, doesn’t, won’t <C> .271 <C> POSEMO <C> thanks, love, thank, good, great, support, lol, win <C> .185 <R> <C> RELATIV <C> in, on, when, at, out, still, now, up, back, new <C> .225 <C> AFFECT <C> thanks, love, thank, good, great, support, lol <C> .111 <R> <C> FUNCTION <C> the, i, to, a, my, and, you, for, is, in <C> .204 <C> SHEHE <C> he, his, she, her, him, he’s, himself <C> .105 <R> <C> TIME <C> when, still, now, back, new, never, after, then, waiting <C> .186 <C> MALE <C> he, his, man, him, sir, he’s, son <C> .086 <R> <C> DIFFER <C> not, but, if, or, can’t, really, than, other, haven’t <C> .169 <C> FEMALE <C> she, her, girl, mom, ma, lady, mother, female, mrs <C> .084 <R> <C> COGPROC <C> not, but, how, if, all, why, or, any, need <C> .132 <C> ASSENT <C> yes, ok, awesome, okay, yeah, cool, absolutely, agree <C> .080 <R> <C> [BOLD] Word2Vec Clusters <C> [BOLD] Word2Vec Clusters <C> [BOLD] Word2Vec Clusters <C> [BOLD] Word2Vec Clusters <C> [BOLD] Word2Vec Clusters <C> [BOLD] Word2Vec Clusters <R> <C> Cust. Service <C> service, customer, contact, job, staff, assist, agent <C> .136 <C> Gratitude <C> thanks, thank, good, great, support, everyone, huge, proud <C> .089 <R> <C> Order <C> order, store, buy, free, delivery, available, package <C> .128 <C> Family <C> old, friend, family, mom, wife, husband, younger <C> .063 <R> <C> Issues <C> delayed, closed, between, outage, delay, road, accident <C> .122 <C> Voting <C> favorite, part, stars, model, vote, models, represent <C> .060 <R> <C> Time Ref. <C> been, yet, haven’t, long, happened, yesterday, took <C> .122 <C> Contests <C> Christmas, gift, receive, entered, giveaway, enter, cards <C> .058 <R> <C> Tech Parts <C> battery, laptop, screen, warranty, desktop, printer <C> .100 <C> Pets <C> dogs, cat, dog, pet, shepherd, fluffy, treats <C> .054 <R> <C> Access <C> use, using, error, password, access, automatically, reset <C> .098 <C> Christian <C> god, shall, heaven, spirit, lord, belongs, soul, believers <C> .053 <CAP> Table 5: Group text features associated with tweets that are complaints and not complaints. Features are sorted by Pearson correlation (r) between their each feature’s normalized frequency and the outcome. We restrict to only the top six categories for each feature type. All correlations are significant at p
<R> <C> [BOLD] Model <C> [BOLD] Acc <C> [BOLD] F1 <C> [BOLD] AUC <R> <C> Most Frequent Class <C> 64.2 <C> 39.1 <C> 0.500 <R> <C> Logistic Regression <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Sentiment – MPQA <C> 64.2 <C> 39.1 <C> 0.499 <R> <C> Sentiment – NRC <C> 63.9 <C> 42.2 <C> 0.599 <R> <C> Sentiment – V&B <C> 68.9 <C> 60.0 <C> 0.696 <R> <C> Sentiment – VADER <C> 66.0 <C> 54.2 <C> 0.654 <R> <C> Sentiment – Stanford <C> 68.0 <C> 55.6 <C> 0.696 <R> <C> Complaint Specific (all) <C> 65.7 <C> 55.2 <C> 0.634 <R> <C> Request <C> 64.2 <C> 39.1 <C> 0.583 <R> <C> Intensifiers <C> 64.5 <C> 47.3 <C> 0.639 <R> <C> Downgraders <C> 65.4 <C> 49.8 <C> 0.615 <R> <C> Temporal References <C> 64.2 <C> 43.7 <C> 0.535 <R> <C> Pronoun Types <C> 64.1 <C> 39.1 <C> 0.545 <R> <C> POS Bigrams <C> 72.2 <C> 66.8 <C> 0.756 <R> <C> LIWC <C> 71.6 <C> 65.8 <C> 0.784 <R> <C> Word2Vec Clusters <C> 67.7 <C> 58.3 <C> 0.738 <R> <C> Bag-of-Words <C> 79.8 <C> 77.5 <C> 0.866 <R> <C> All Features <C> [BOLD] 80.5 <C> [BOLD] 78.0 <C> [BOLD] 0.873 <R> <C> Neural Networks <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> MLP <C> 78.3 <C> 76.2 <C> 0.845 <R> <C> LSTM <C> 80.2 <C> 77.0 <C> 0.864 <CAP> Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.
<R> <C> [BOLD] Model <C> [BOLD] Acc <C> [BOLD] F1 <C> [BOLD] AUC <R> <C> Most Frequent Class <C> 64.2 <C> 39.1 <C> 0.500 <R> <C> LR-All Features – Original Data <C> 80.5 <C> 78.0 <C> 0.873 <R> <C> Dist. Supervision + Pooling <C> 77.2 <C> 75.7 <C> 0.853 <R> <C> Dist. Supervision + EasyAdapt <C> [BOLD] 81.2 <C> [BOLD] 79.0 <C> [BOLD] 0.885 <CAP> Table 7: Complaint prediction results using the original data set and distantly supervised data. All models are based on logistic regression with bag-of-word and Part-of-Speech tag features.
<R> <C> [BOLD] Domain <C> [BOLD] In-Domain <C> [BOLD] Pooling <C> [BOLD] EasyAdapt <R> <C> Food & Beverage <C> 63.9 <C> 60.9 <C> [BOLD] 83.1 <R> <C> Apparel <C> [BOLD] 76.2 <C> 71.1 <C> 72.5 <R> <C> Retail <C> 58.8 <C> [BOLD] 79.7 <C> [BOLD] 79.7 <R> <C> Cars <C> 41.5 <C> 77.8 <C> [BOLD] 80.9 <R> <C> Services <C> 65.2 <C> 75.9 <C> [BOLD] 76.7 <R> <C> Software <C> 61.3 <C> 73.4 <C> [BOLD] 78.7 <R> <C> Transport <C> 56.4 <C> [BOLD] 73.4 <C> 69.8 <R> <C> Electronics <C> 66.2 <C> 73.0 <C> [BOLD] 76.2 <R> <C> Other <C> 42.4 <C> [BOLD] 82.8 <C> [BOLD] 82.8 <CAP> Table 8: Performance of models in Macro F1 on tweets from each domain.
<R> <C> Model <C> Val. Accuracy <C> Loss <C> Val. Loss <C> Pretraining Time <C> Finetuning Time <R> <C> Siamese Networks <C> 77.42% <C> 0.5601 <C> 0.5329 <C> [EMPTY] <C> 4m per epoch <R> <C> BERT <C> 87.47% <C> 0.4655 <C> 0.4419 <C> 66 hours <C> 2m per epoch <R> <C> GPT-2 <C> 90.99% <C> 0.2172 <C> 0.1826 <C> 78 hours <C> 4m per epoch <R> <C> ULMFiT <C> 91.59% <C> 0.3750 <C> 0.1972 <C> 11 hours <C> 2m per epoch <R> <C> ULMFiT (no LM Finetuning) <C> 78.11% <C> 0.5512 <C> 0.5409 <C> 11 hours <C> 2m per epoch <R> <C> BERT + Multitasking <C> 91.20% <C> 0.3155 <C> 0.3023 <C> 66 hours <C> 4m per epoch <R> <C> GPT-2 + Multitasking <C> 96.28% <C> 0.2609 <C> 0.2197 <C> 78 hours <C> 5m per epoch <CAP> Table 4: Consolidated experiment results. The first section shows finetuning results for base transfer learning methods and the baseline siamese network. The second section shows results for ULMFiT without Language Model Finetuning. The last section shows finetuning results for transformer methods augmented with multitasking heads. BERT and GPT-2 were finetuned for three epochs in all cases and ULMFiT was finetuned for 5 during classifier finetuning.
<R> <C> Finetuning <C> Pretrained? <C> Accuracy <C> Val. Loss <C> Acc. Inc. <C> % of Perf. <R> <C> Multitasking <C> No <C> 53.61% <C> 0.7217 <C> - <C> - <R> <C> [EMPTY] <C> Yes <C> 96.28% <C> 0.2197 <C> +42.67% <C> 44.32% <R> <C> Standard <C> No <C> 51.02% <C> 0.7024 <C> - <C> - <R> <C> [EMPTY] <C> Yes <C> 90.99% <C> 0.1826 <C> +39.97% <C> 43.93% <CAP> Table 5: An ablation study on the effects of pretraining for multitasking-based and standard GPT-2 finetuning. Results show that pretraining greatly accounts for almost half of performance on both finetuning techniques. “Acc. Inc.” refers to the boost in performance contributed by the pretraining step. “% of Perf.” refers to the percentage of the total performance that the pretraining step contributes.
<R> <C> # of Heads <C> Accuracy <C> Val. Loss <C> Effect <R> <C> 1 <C> 89.44% <C> 0.2811 <C> -6.84% <R> <C> 2 <C> 91.20% <C> 0.2692 <C> -5.08% <R> <C> 4 <C> 93.85% <C> 0.2481 <C> -2.43% <R> <C> 8 <C> 96.02% <C> 0.2257 <C> -0.26% <R> <C> 10 <C> 96.28% <C> 0.2197 <C> [EMPTY] <R> <C> 16 <C> 96.32% <C> 0.2190 <C> +0.04 <CAP> Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. “Effect” refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.
<R> <C> Category <C> Question <C> Answer <C> Prediction <C> Pct (%) <R> <C> Annotation <C> Were the films Tonka and 101 Dalmatians released in the same decade? <C> 1958 Walt Disney Western adventure film <C> No <C> 9 <R> <C> Multiple Answers <C> Michael J. Hunter replaced the lawyer who became the administrator of which agency? <C> EPA <C> Environmental Protection Agency <C> 24 <R> <C> Discrete Reasoning <C> Between two bands, Mastodon and Hole, which one has more members? <C> Mastodon <C> Hole <C> 15 <R> <C> Commonsense & External Knowledge <C> What is the name of second extended play by the artists of the mini-abum Code#01? <C> Code#02 Pretty Pretty <C> Code#01 Bad Girl <C> 16 <R> <C> Multi-hop <C> Who directed the film based on the rock opera 5:15 appeared in? <C> Franc Roddam <C> Ken Russell <C> 16 <R> <C> MRC <C> How was Ada Lovelace, the first computer programmer, related to Lord Byron in Childe Byron? <C> his daughter <C> strained relationship <C> 20 <CAP> Table 6: Error analysis of HGN model. For ‘Multi-hop’ errors, the model jumps to the wrong film (“Tommy (1975 film)”) instead of the correct one (“Quadrophenia (film)”) from the starting entity “rock opera 5:15”. The supporting fact for the ‘MRC’ example is “Childe Byron is a 1977 play by Romulus Linney about the strained relationship between the poet, Lord Byron, and his daughter, Ada Lovelace”.
<R> <C> Model <C> Ans EM <C> Ans F1 <C> Sup EM <C> Sup F1 <C> Joint EM <C> Joint F1 <R> <C> DecompRC Min et al. ( 2019b ) <C> 55.20 <C> 69.63 <C> - <C> - <C> - <C> - <R> <C> ChainEx Chen et al. ( 2019 ) <C> 61.20 <C> 74.11 <C> - <C> - <C> - <C> - <R> <C> Baseline Model Yang et al. ( 2018 ) <C> 45.60 <C> 59.02 <C> 20.32 <C> 64.49 <C> 10.83 <C> 40.16 <R> <C> QFE Nishida et al. ( 2019 ) <C> 53.86 <C> 68.06 <C> 57.75 <C> 84.49 <C> 34.63 <C> 59.61 <R> <C> DFGN Xiao et al. ( 2019 ) <C> 56.31 <C> 69.69 <C> 51.50 <C> 81.62 <C> 33.62 <C> 59.82 <R> <C> LQR-Net Grail et al. ( 2020 ) <C> 60.20 <C> 73.78 <C> 56.21 <C> 84.09 <C> 36.56 <C> 63.68 <R> <C> P-BERT† <C> 61.18 <C> 74.16 <C> 51.38 <C> 82.76 <C> 35.42 <C> 63.79 <R> <C> TAP2† <C> 64.99 <C> 78.59 <C> 55.47 <C> 85.57 <C> 39.77 <C> 69.12 <R> <C> EPS+BERT† <C> 65.79 <C> 79.05 <C> 58.50 <C> 86.26 <C> 42.47 <C> 70.48 <R> <C> SAE-large Tu et al. ( 2020 ) <C> 66.92 <C> 79.62 <C> 61.53 <C> 86.86 <C> 45.36 <C> 71.45 <R> <C> C2F ReaderShao et al. ( 2020 ) <C> 67.98 <C> 81.24 <C> 60.81 <C> 87.63 <C> 44.67 <C> 72.73 <R> <C> HGN (ours) <C> [BOLD] 69.22 <C> [BOLD] 82.19 <C> [BOLD] 62.76 <C> [BOLD] 88.47 <C> [BOLD] 47.11 <C> [BOLD] 74.21 <CAP> Table 1: Results on the test set of HotpotQA in the Distractor setting. HGN achieves state-of-the-art results at the time of submission (Dec. 1, 2019). (†) indicates unpublished work. RoBERTa-large is used for context encoding.
<R> <C> Model <C> Ans EM <C> Ans F1 <C> Sup EM <C> Sup F1 <C> Joint EM <C> Joint F1 <R> <C> TPReasoner Xiong et al. ( 2019 ) <C> 36.04 <C> 47.43 <C> - <C> - <C> - <C> - <R> <C> Baseline Model Yang et al. ( 2018 ) <C> 23.95 <C> 32.89 <C> 3.86 <C> 37.71 <C> 1.85 <C> 16.15 <R> <C> QFE Nishida et al. ( 2019 ) <C> 28.66 <C> 38.06 <C> 14.20 <C> 44.35 <C> 8.69 <C> 23.10 <R> <C> MUPPET Feldman and El-Yaniv ( 2019 ) <C> 30.61 <C> 40.26 <C> 16.65 <C> 47.33 <C> 10.85 <C> 27.01 <R> <C> Cognitive Graph Ding et al. ( 2019 ) <C> 37.12 <C> 48.87 <C> 22.82 <C> 57.69 <C> 12.42 <C> 34.92 <R> <C> PR-BERT† <C> 43.33 <C> 53.79 <C> 21.90 <C> 59.63 <C> 14.50 <C> 39.11 <R> <C> Golden Retriever Qi et al. ( 2019 ) <C> 37.92 <C> 48.58 <C> 30.69 <C> 64.24 <C> 18.04 <C> 39.13 <R> <C> Entity-centric BERT Godbole et al. ( 2019 ) <C> 41.82 <C> 53.09 <C> 26.26 <C> 57.29 <C> 17.01 <C> 39.18 <R> <C> SemanticRetrievalMRS Yixin Nie ( 2019 ) <C> 45.32 <C> 57.34 <C> 38.67 <C> 70.83 <C> 25.14 <C> 47.60 <R> <C> Transformer-XH Zhao et al. ( 2020 ) <C> 48.95 <C> 60.75 <C> 41.66 <C> 70.01 <C> 27.13 <C> 49.57 <R> <C> MIR+EPS+BERT† <C> 52.86 <C> 64.79 <C> 42.75 <C> 72.00 <C> 31.19 <C> 54.75 <R> <C> Graph Recur. Retriever Asai et al. ( 2020 ) <C> [BOLD] 60.04 <C> [BOLD] 72.96 <C> 49.08 <C> 76.41 <C> 35.35 <C> [BOLD] 61.18 <R> <C> HGN (ours) <C> 57.85 <C> 69.93 <C> [BOLD] 51.01 <C> [BOLD] 76.82 <C> [BOLD] 37.17 <C> 60.74 <CAP> Table 2: Results on the test set of HotpotQA in the Fullwiki setting. HGN achieves close to state-of-the-art results at the time of submission (Dec. 1, 2019). (†) indicates unpublished work. RoBERTa-large is used for context encoding, and SemanticRetrievalMRS is used for retrieval. Leaderboard: https://hotpotqa.github.io/.
<R> <C> Model <C> Ans F1 <C> Sup F1 <C> Joint F1 <R> <C> w/o Graph <C> 80.58 <C> 85.83 <C> 71.02 <R> <C> PS Graph <C> 81.68 <C> 88.44 <C> 73.83 <R> <C> PSE Graph <C> 82.10 <C> 88.40 <C> 74.13 <R> <C> Hier. Graph <C> [BOLD] 82.22 <C> [BOLD] 88.58 <C> [BOLD] 74.37 <CAP> Table 3: Ablation study on the effectiveness of the hierarchical graph on the dev set in the Distractor setting. RoBERTa-large is used for context encoding.
<R> <C> Model <C> Ans F1 <C> Sup F1 <C> Joint F1 <R> <C> DFGN (BERT-base) <C> 69.38 <C> 82.23 <C> 59.89 <R> <C> EPS (BERT-wwm)† <C> 79.05 <C> 86.26 <C> 70.48 <R> <C> SAE (RoBERTa) <C> 80.75 <C> 87.38 <C> 72.75 <R> <C> HGN (BERT-base) <C> 74.76 <C> 86.61 <C> 66.90 <R> <C> HGN (BERT-wwm) <C> 80.51 <C> 88.14 <C> 72.77 <R> <C> HGN (RoBERTa) <C> [BOLD] 82.22 <C> [BOLD] 88.58 <C> [BOLD] 74.37 <CAP> Table 5: Results with different pre-trained language models on the dev set in the Distractor setting. (†) is unpublished work with results on the test set, using BERT whole word masking (wwm).
<R> <C> Question <C> Ans F1 <C> Sup F1 <C> Joint F1 <C> Pct (%) <R> <C> comp-yn <C> 93.45 <C> 94.22 <C> 88.50 <C> 6.19 <R> <C> comp-span <C> 79.06 <C> 91.72 <C> 74.17 <C> 13.90 <R> <C> bridge <C> 81.90 <C> 87.60 <C> 73.31 <C> 79.91 <CAP> Table 7: Results of HGN for different reasoning types.
<R> <C> AMR Anno. <C> BLEU <R> <C> Automatic <C> 16.8 <R> <C> Gold <C> [BOLD] *17.5* <CAP> Table 4: BLEU scores of Dual2seq on the little prince data, when gold or automatic AMRs are available.
<R> <C> System <C> NC-v11 BLEU <C> NC-v11 TER↓ <C> NC-v11 Meteor <C> Full BLEU <C> Full TER↓ <C> Full Meteor <R> <C> OpenNMT-tf <C> 15.1 <C> 0.6902 <C> 0.3040 <C> 24.3 <C> 0.5567 <C> 0.4225 <R> <C> Transformer-tf <C> 17.1 <C> 0.6647 <C> 0.3578 <C> 25.1 <C> 0.5537 <C> 0.4344 <R> <C> Seq2seq <C> 16.0 <C> 0.6695 <C> 0.3379 <C> 23.7 <C> 0.5590 <C> 0.4258 <R> <C> Dual2seq-LinAMR <C> 17.3 <C> 0.6530 <C> 0.3612 <C> 24.0 <C> 0.5643 <C> 0.4246 <R> <C> Duel2seq-SRL <C> 17.2 <C> 0.6591 <C> 0.3644 <C> 23.8 <C> 0.5626 <C> 0.4223 <R> <C> Dual2seq-Dep <C> 17.8 <C> 0.6516 <C> 0.3673 <C> 25.0 <C> 0.5538 <C> 0.4328 <R> <C> Dual2seq <C> [BOLD] *19.2* <C> [BOLD] 0.6305 <C> [BOLD] 0.3840 <C> [BOLD] *25.5* <C> [BOLD] 0.5480 <C> [BOLD] 0.4376 <CAP> Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. ↓ indicates the lower the better.
<R> <C> [BOLD] Benchmark <C> [BOLD]  Simple Baseline  <C> [BOLD] ELMo <C> [BOLD] GPT <C> [BOLD] BERT <C> [BOLD] MT-DNN <C> [BOLD] XLNet <C> [BOLD] RoBERTa <C> [BOLD] ALBERT <C> [BOLD] Human <R> <C> [BOLD] CLOTH <C> 25.0 <C> 70.7 <C> – <C> [BOLD] 86.0 <C> – <C> – <C> – <C> – <C> 85.9 <R> <C> [BOLD] Cosmos QA <C> – <C> – <C> 54.5 <C> 67.1 <C> – <C> – <C> – <C> – <C> 94.0 <R> <C> [BOLD] DREAM <C> 33.4 <C> 59.5 <C> 55.5 <C> 66.8 <C> – <C> [BOLD] 72.0 <C> – <C> – <C> 95.5 <R> <C> [BOLD] GLUE <C> – <C> 70.0 <C> – <C> 80.5 <C> 87.6 <C> 88.4 <C> 88.5 <C> [BOLD] 89.4 <C> 87.1 <R> <C> [BOLD] HellaSWAG <C> 25.0 <C> 33.3 <C> 41.7 <C> 47.3 <C> – <C> – <C> [BOLD] 85.2 <C> [EMPTY] <C> 95.6 <R> <C> [BOLD] MC-TACO <C> 17.4 <C> 26.4 <C> – <C> 42.7 <C> – <C> – <C> [BOLD] 43.6 <C> – <C> 75.8 <R> <C> [BOLD] RACE <C> 24.9 <C> – <C> 59.0 <C> 72.0 <C> – <C> 81.8 <C> 83.2 <C> [BOLD] 89.4 <C> 94.5 <R> <C> [BOLD] SciTail <C> 60.3 <C> – <C> 88.3 <C> – <C> 94.1 <C> – <C> – <C> – <C> – <R> <C> [BOLD] SQuAD 1.1 <C> 1.3 <C> 81.0 <C> – <C> 87.4 <C> – <C> [BOLD] 89.9 <C> – <C> – <C> 82.3 <R> <C> [BOLD] SQuAD 2.0 <C> 48.9 <C> 63.4 <C> – <C> 80.8 <C> – <C> 86.3 <C> 86.8 <C> [BOLD] 89.7 <C> 86.9 <R> <C> [BOLD] SuperGLUE <C> 47.1 <C> – <C> – <C> 69.0 <C> – <C> – <C> [BOLD] 84.6 <C> – <C> 89.8 <R> <C> [BOLD] SWAG <C> 25.0 <C> 59.1 <C> 78.0 <C> 86.3 <C> 87.1 <C> – <C> [BOLD] 89.9 <C> – <C> 88.0 <CAP> Table 2: Comparison of exact-match accuracy achieved on selected benchmarks by a random or majority-choice baseline, various neural contextual embedding models, and humans. ELMo refers to the highest-performing listed approach using ELMo embeddings. Best system performance on each benchmark in bold. Information extracted from leaderboards (linked to in the first column) at time of writing (October 2019), and original papers for benchmarks introduced in Section 2.
<R> <C> [EMPTY] <C> ACE05 <C> SciERC <C> GENIA <C> WLPC <R> <C> BERT + LSTM <C> 85.8 <C> 69.9 <C> 78.4 <C> [BOLD] 78.9 <R> <C> +RelProp <C> 85.7 <C> 70.5 <C> - <C> 78.7 <R> <C> +CorefProp <C> 86.3 <C> [BOLD] 72.0 <C> 78.3 <C> - <R> <C> BERT Finetune <C> 87.3 <C> 70.5 <C> 78.3 <C> 78.5 <R> <C> +RelProp <C> 86.7 <C> 71.1 <C> - <C> 78.8 <R> <C> +CorefProp <C> [BOLD] 87.5 <C> 71.1 <C> [BOLD] 79.5 <C> - <CAP> Table 2: F1 scores on NER.
<R> <C> Dataset <C> Task <C> SOTA <C> Ours <C> Δ% <R> <C> ACE05 <C> Entity <C> 88.4 <C> [BOLD] 88.6 <C> 1.7 <R> <C> ACE05 <C> Relation <C> 63.2 <C> [BOLD] 63.4 <C> 0.5 <R> <C> ACE05-Event* <C> Entity <C> 87.1 <C> [BOLD] 90.7 <C> 27.9 <R> <C> ACE05-Event* <C> Trig-ID <C> 73.9 <C> [BOLD] 76.5 <C> 9.6 <R> <C> ACE05-Event* <C> Trig-C <C> 72.0 <C> [BOLD] 73.6 <C> 5.7 <R> <C> ACE05-Event* <C> Arg-ID <C> [BOLD] 57.2 <C> 55.4 <C> -4.2 <R> <C> ACE05-Event* <C> Arg-C <C> 52.4 <C> [BOLD] 52.5 <C> 0.2 <R> <C> SciERC <C> Entity <C> 65.2 <C> [BOLD] 67.5 <C> 6.6 <R> <C> SciERC <C> Relation <C> 41.6 <C> [BOLD] 48.4 <C> 11.6 <R> <C> GENIA <C> Entity <C> 76.2 <C> [BOLD] 77.9 <C> 7.1 <R> <C> WLPC <C> Entity <C> 79.5 <C> [BOLD] 79.7 <C> 1.0 <R> <C> WLPC <C> Relation <C> 64.1 <C> [BOLD] 65.9 <C> 5.0 <CAP> Table 1: DyGIE++ achieves state-of-the-art results. Test set F1 scores of best model, on all tasks and datasets. We define the following notations for events: Trig: Trigger, Arg: argument, ID: Identification, C: Classification. * indicates the use of a 4-model ensemble for trigger detection. See Appendix E for details. The results of the single model are reported in Table 2 (c). We ran significance tests on a subset of results in Appendix D. All were statistically significant except Arg-C and Arg-ID on ACE05-Event.
<R> <C> [EMPTY] <C> ACE05 <C> SciERC <C> WLPC <R> <C> BERT + LSTM <C> 60.6 <C> 40.3 <C> 65.1 <R> <C> +RelProp <C> 61.9 <C> 41.1 <C> 65.3 <R> <C> +CorefProp <C> 59.7 <C> 42.6 <C> - <R> <C> BERT FineTune <C> [BOLD] 62.1 <C> 44.3 <C> 65.4 <R> <C> +RelProp <C> 62.0 <C> 43.0 <C> [BOLD] 65.5 <R> <C> +CorefProp <C> 60.0 <C> [BOLD] 45.3 <C> - <CAP> Table 3: F1 scores on Relation.
<R> <C> [EMPTY] <C> SciERC Entity <C> SciERC Relation <C> GENIA Entity <R> <C> Best BERT <C> 69.8 <C> 41.9 <C> 78.4 <R> <C> Best SciBERT <C> [BOLD] 72.0 <C> [BOLD] 45.3 <C> [BOLD] 79.5 <CAP> Table 7: In-domain pre-training: SciBERT vs. BERT
<R> <C> Task <C> Variation <C> 1 <C> 3 <R> <C> Relation <C> BERT+LSTM <C> 59.3 <C> [BOLD] 60.6 <R> <C> Relation <C> BERT Finetune <C> 62.0 <C> [BOLD] 62.1 <R> <C> Entity <C> BERT+LSTM <C> 90.0 <C> [BOLD] 90.5 <R> <C> Entity <C> BERT Finetune <C> 88.8 <C> [BOLD] 89.7 <R> <C> Trigger <C> BERT+LSTM <C> [BOLD] 69.4 <C> 68.9 <R> <C> Trigger <C> BERT Finetune <C> 68.3 <C> [BOLD] 69.7 <R> <C> Arg Class <C> BERT+LSTM <C> 48.6 <C> [BOLD] 51.4 <R> <C> Arg Class <C> BERT Finetune <C> [BOLD] 50.0 <C> 48.8 <CAP> Table 6: Effect of BERT cross-sentence context. F1 score of relation F1 on ACE05 dev set and entity, arg, trigger extraction F1 on ACE05-E test set, as a function of the BERT context window size.
<R> <C> Method <C> En→It best <C> En→It avg <C> En→It iters <C> En→De best <C> En→De avg <C> En→De iters <C> En→Fi best <C> En→Fi avg <C> En→Fi iters <C> En→Es best <C> En→Es avg <C> En→Es iters <R> <C> Artetxe et al., 2018b <C> [BOLD] 48.53 <C> 48.13 <C> 573 <C> 48.47 <C> 48.19 <C> 773 <C> 33.50 <C> 32.63 <C> 988 <C> 37.60 <C> 37.33 <C> 808 <R> <C> Noise-aware Alignment <C> [BOLD] 48.53 <C> [BOLD] 48.20 <C> 471 <C> [BOLD] 49.67 <C> [BOLD] 48.89 <C> 568 <C> [BOLD] 33.98 <C> [BOLD] 33.68 <C> 502 <C> [BOLD] 38.40 <C> [BOLD] 37.79 <C> 551 <CAP> Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En→De, En→Fi and En→Es improvements are significant at p<0.05 according to ANOVA on the different runs.
<R> <C> Approach <C> RST-DTtest <C> Instr-DTtest <R> <C> Right Branching <C> 54.64 <C> 58.47 <R> <C> Left Branching <C> 53.73 <C> 48.15 <R> <C> Hier. Right Branch. <C> [BOLD] 70.82 <C> [BOLD] 67.86 <R> <C> Hier. Left Branch. <C> 70.58 <C> 63.49 <R> <C> [BOLD] Intra-Domain Evaluation <C> [BOLD] Intra-Domain Evaluation <C> [BOLD] Intra-Domain Evaluation <R> <C> HILDAHernault et al. ( 2010 ) <C> 83.00 <C> — <R> <C> DPLPJi and Eisenstein ( 2014 ) <C> 82.08 <C> — <R> <C> CODRAJoty et al. ( 2015 ) <C> 83.84 <C> [BOLD] 82.88 <R> <C> Two-StageWang et al. ( 2017 ) <C> [BOLD] 86.00 <C> 77.28 <R> <C> [BOLD] Inter-Domain Evaluation <C> [BOLD] Inter-Domain Evaluation <C> [BOLD] Inter-Domain Evaluation <R> <C> Two-StageRST-DT <C> × <C> 73.65 <R> <C> Two-StageInstr-DT <C> 74.48 <C> × <R> <C> Two-StageOurs(avg) <C> 76.42 <C> [BOLD] 74.22 <R> <C> Two-StageOurs(max) <C> [BOLD] 77.24 <C> 73.12 <R> <C> Human Morey et al. ( 2017 ) <C> 88.30 <C> — <CAP> Table 3: Discourse structure prediction results; tested on RST-DTtest and Instr-DTtest. Subscripts in inter-domain evaluation sub-table indicate the training set. Best performance in the category is bold. Consistently best model for inter-domain discourse structure prediction is underlined
<R> <C> System <C> TGPC Succ. (%) <C> TGPC #Turns <C> CWC Succ. (%) <C> CWC #Turns <R> <C> Retrieval  <C> 7.16 <C> 4.17 <C> 0 <C> - <R> <C> Retrieval-Stgy  <C> 47.80 <C> 6.7 <C> 44.6 <C> 7.42 <R> <C> PMI  <C> 35.36 <C> 6.38 <C> 47.4 <C> 5.29 <R> <C> Neural  <C> 54.76 <C> 4.73 <C> 47.6 <C> 5.16 <R> <C> Kernel  <C> 62.56 <C> 4.65 <C> 53.2 <C> 4.08 <R> <C> DKRN (ours) <C> [BOLD] 89.0 <C> 5.02 <C> [BOLD] 84.4 <C> 4.20 <CAP> Table 4: Results of Self-Play Evaluation.
<R> <C> Dataset <C> System <C> Keyword Prediction  [ITALIC] Rw@1 <C> Keyword Prediction  [ITALIC] Rw@3 <C> Keyword Prediction  [ITALIC] Rw@5 <C> Keyword Prediction P@1 <C> Response Retrieval  [ITALIC] R20@1 <C> Response Retrieval  [ITALIC] R20@3 <C> Response Retrieval  [ITALIC] R20@5 <C> Response Retrieval MRR <R> <C> TGPC <C> Retrieval  <C> - <C> - <C> - <C> - <C> 0.5063 <C> 0.7615 <C> 0.8676 <C> 0.6589 <R> <C> TGPC <C> PMI  <C> 0.0585 <C> 0.1351 <C> 0.1872 <C> 0.0871 <C> 0.5441 <C> 0.7839 <C> 0.8716 <C> 0.6847 <R> <C> TGPC <C> Neural  <C> 0.0708 <C> 0.1438 <C> 0.1820 <C> 0.1321 <C> 0.5311 <C> 0.7905 <C> 0.8800 <C> 0.6822 <R> <C> TGPC <C> Kernel  <C> 0.0632 <C> 0.1377 <C> 0.1798 <C> 0.1172 <C> 0.5386 <C> 0.8012 <C> 0.8924 <C> 0.6877 <R> <C> TGPC <C> DKRN (ours) <C> [BOLD] 0.0909 <C> [BOLD] 0.1903 <C> [BOLD] 0.2477 <C> [BOLD] 0.1685 <C> [BOLD] 0.5729 <C> [BOLD] 0.8132 <C> [BOLD] 0.8966 <C> [BOLD] 0.7110 <R> <C> CWC <C> Retrieval  <C> - <C> - <C> - <C> - <C> 0.5785 <C> 0.8101 <C> 0.8999 <C> 0.7141 <R> <C> CWC <C> PMI  <C> 0.0555 <C> 0.1001 <C> 0.1212 <C> 0.0969 <C> 0.5945 <C> 0.8185 <C> 0.9054 <C> 0.7257 <R> <C> CWC <C> Neural  <C> 0.0654 <C> 0.1194 <C> 0.1450 <C> 0.1141 <C> 0.6044 <C> 0.8233 <C> 0.9085 <C> 0.7326 <R> <C> CWC <C> Kernel  <C> 0.0592 <C> 0.1113 <C> 0.1337 <C> 0.1011 <C> 0.6017 <C> 0.8234 <C> 0.9087 <C> 0.7320 <R> <C> CWC <C> DKRN (ours) <C> [BOLD] 0.0680 <C> [BOLD] 0.1254 <C> [BOLD] 0.1548 <C> [BOLD] 0.1185 <C> [BOLD] 0.6324 <C> [BOLD] 0.8416 <C> [BOLD] 0.9183 <C> [BOLD] 0.7533 <CAP> Table 3: Results of Turn-level Evaluation.
<R> <C> System <C> Succ. (%) <C> Smoothness <R> <C> Retrieval-Stgy  <C> 54.0 <C> 2.48 <R> <C> PMI  <C> 46.0 <C> 2.56 <R> <C> Neural  <C> 36.0 <C> 2.50 <R> <C> Kernel  <C> 58.0 <C> 2.48 <R> <C> DKRN (ours) <C> [BOLD] 88.0 <C> [BOLD] 3.22 <CAP> Table 5: Results of the Human Rating on CWC.
<R> <C> [EMPTY] <C> Ours Better(%) <C> No Prefer(%) <C> Ours Worse(%) <R> <C> Retrieval-Stgy  <C> [BOLD] 62 <C> 22 <C> 16 <R> <C> PMI  <C> [BOLD] 54 <C> 32 <C> 14 <R> <C> Neural  <C> [BOLD] 60 <C> 22 <C> 18 <R> <C> Kernel  <C> [BOLD] 62 <C> 26 <C> 12 <CAP> Table 6: Results of the Human Rating on CWC.
<R> <C> [BOLD] ResNet-34 <C> [BOLD] Eval set % <C> [BOLD] #param <R> <C> Baseline (No SA)Anderson et al. ( 2018 ) <C> 55.00 <C> 0M <R> <C> SA (S: 1,2,3 - B: 1) <C> 55.11 <C> } 0.107M <R> <C> SA (S: 1,2,3 - B: 2) <C> 55.17 <C> } 0.107M <R> <C> [BOLD] SA (S: 1,2,3 - B: 3) <C> [BOLD] 55.27 <C> } 0.107M <CAP> Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).
<R> <C> [BOLD] ResNet-34 <C> [BOLD] Eval set % <C> [BOLD] #param <R> <C> SA (S: 3 - M: 1) <C> 55.25 <C> } 0.082M <R> <C> [BOLD] SA (S: 3 - B: 3) <C> [BOLD] 55.42 <C> } 0.082M <R> <C> SA (S: 3 - B: 4) <C> 55.33 <C> } 0.082M <R> <C> SA (S: 3 - B: 6) <C> 55.31 <C> } 0.082M <R> <C> SA (S: 3 - B: 1,3,5) <C> 55.45 <C> } 0.245M <R> <C> [BOLD] SA (S: 3 - B: 2,4,6) <C> [BOLD] 55.56 <C> } 0.245M <CAP> Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).
<R> <C> [BOLD] System <C> [BOLD] ROUGE-1  [BOLD] R (%) <C> [BOLD] ROUGE-1  [BOLD] P (%) <C> [BOLD] ROUGE-1  [BOLD] F (%) <C> [BOLD] ROUGE-2  [BOLD] R (%) <C> [BOLD] ROUGE-2  [BOLD] P (%) <C> [BOLD] ROUGE-2  [BOLD] F (%) <C> [BOLD] Sentence-Level  [BOLD] R (%) <C> [BOLD] Sentence-Level  [BOLD] P (%) <C> [BOLD] Sentence-Level  [BOLD] F (%) <R> <C> [BOLD] ILP <C> 24.5 <C> 41.1 <C> 29.3±0.5 <C> 7.9 <C> 15.0 <C> 9.9±0.5 <C> 13.6 <C> 22.6 <C> 15.6±0.4 <R> <C> [BOLD] Sum-Basic <C> 28.4 <C> 44.4 <C> 33.1±0.5 <C> 8.5 <C> 15.6 <C> 10.4±0.4 <C> 14.7 <C> 22.9 <C> 16.7±0.5 <R> <C> [BOLD] KL-Sum <C> 39.5 <C> 34.6 <C> 35.5±0.5 <C> 13.0 <C> 12.7 <C> 12.3±0.5 <C> 15.2 <C> 21.1 <C> 16.3±0.5 <R> <C> [BOLD] LexRank <C> 42.1 <C> 39.5 <C> 38.7±0.5 <C> 14.7 <C> 15.3 <C> 14.2±0.5 <C> 14.3 <C> 21.5 <C> 16.0±0.5 <R> <C> [BOLD] MEAD <C> 45.5 <C> 36.5 <C> 38.5± 0.5 <C> 17.9 <C> 14.9 <C> 15.4±0.5 <C> 27.8 <C> 29.2 <C> 26.8±0.5 <R> <C> [BOLD] SVM <C> 19.0 <C> 48.8 <C> 24.7±0.8 <C> 7.5 <C> 21.1 <C> 10.0±0.5 <C> 32.7 <C> 34.3 <C> 31.4±0.4 <R> <C> [BOLD] LogReg <C> 26.9 <C> 34.5 <C> 28.7±0.6 <C> 6.4 <C> 9.9 <C> 7.3±0.4 <C> 12.2 <C> 14.9 <C> 12.7±0.5 <R> <C> [BOLD] LogReg [ITALIC] r <C> 28.0 <C> 34.8 <C> 29.4±0.6 <C> 6.9 <C> 10.4 <C> 7.8±0.4 <C> 12.1 <C> 14.5 <C> 12.5±0.5 <R> <C> [BOLD] HAN <C> 31.0 <C> 42.8 <C> 33.7±0.7 <C> 11.2 <C> 17.8 <C> 12.7±0.5 <C> 26.9 <C> 34.1 <C> 32.4±0.5 <R> <C> [BOLD] HAN+pretrainT <C> 32.2 <C> 42.4 <C> 34.4±0.7 <C> 11.5 <C> 17.5 <C> 12.9±0.5 <C> 29.6 <C> 35.8 <C> 32.2±0.5 <R> <C> [BOLD] HAN+pretrainU <C> 32.1 <C> 42.1 <C> 33.8±0.7 <C> 11.6 <C> 17.6 <C> 12.9±0.5 <C> 30.1 <C> 35.6 <C> 32.3±0.5 <R> <C> [BOLD] HAN [ITALIC] r <C> 38.1 <C> 40.5 <C> [BOLD] 37.8±0.5 <C> 14.0 <C> 17.1 <C> [BOLD] 14.7±0.5 <C> 32.5 <C> 34.4 <C> [BOLD] 33.4±0.5 <R> <C> [BOLD] HAN+pretrainT [ITALIC] r <C> 37.9 <C> 40.4 <C> [BOLD] 37.6±0.5 <C> 13.5 <C> 16.8 <C> [BOLD] 14.4±0.5 <C> 32.5 <C> 34.4 <C> [BOLD] 33.4±0.5 <R> <C> [BOLD] HAN+pretrainU [ITALIC] r <C> 37.9 <C> 40.4 <C> [BOLD] 37.6±0.5 <C> 13.6 <C> 16.9 <C> [BOLD] 14.4±0.5 <C> 33.9 <C> 33.8 <C> [BOLD] 33.8±0.5 <CAP> Table 1: Results of thread summarization. ‘HAN’ models are our proposed approaches adapted from the hierarchical attention networks [Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (‘T’) and Ubuntuforum (‘U’). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (‘±’). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.
<R> <C> [BOLD] DST Models <C> [BOLD] Joint Acc. DSTC2 <C> [BOLD] Joint Acc. WOZ 2.0 <R> <C> Delexicalisation-Based (DB) Model Mrkšić et al. ( 2017 ) <C> 69.1 <C> 70.8 <R> <C> DB Model + Semantic Dictionary Mrkšić et al. ( 2017 ) <C> 72.9 <C> 83.7 <R> <C> Scalable Multi-domain DST Rastogi et al. ( 2017 ) <C> 70.3 <C> - <R> <C> MemN2N Perez and Liu ( 2017 ) <C> 74.0 <C> - <R> <C> PtrNet Xu and Hu ( 2018 ) <C> 72.1 <C> - <R> <C> Neural Belief Tracker: NBT-DNN Mrkšić et al. ( 2017 ) <C> 72.6 <C> 84.4 <R> <C> Neural Belief Tracker: NBT-CNN Mrkšić et al. ( 2017 ) <C> 73.4 <C> 84.2 <R> <C> Belief Tracking: Bi-LSTM Ramadan et al. ( 2018 ) <C> - <C> 85.1 <R> <C> Belief Tracking: CNN Ramadan et al. ( 2018 ) <C> - <C> 85.5 <R> <C> GLAD Zhong et al. ( 2018 ) <C> 74.5 <C> 88.1 <R> <C> StateNet <C> 74.1 <C> 87.8 <R> <C> StateNet_PS <C> 74.5 <C> 88.2 <R> <C> [BOLD] StateNet_PSI <C> [BOLD] 75.5 <C> [BOLD] 88.9 <CAP> Table 1: Joint goal accuracy on DSTC2 and WOZ 2.0 test set vs. various approaches as reported in the literature.
<R> <C> [BOLD] Initialization <C> [BOLD] Joint Acc. DSTC2 <C> [BOLD] Joint Acc. WOZ 2.0 <R> <C> [ITALIC] food <C> [BOLD] 75.5 <C> [BOLD] 88.9 <R> <C> [ITALIC] pricerange <C> 73.6 <C> 88.2 <R> <C> [ITALIC] area <C> 73.5 <C> 87.8 <CAP> Table 2: Joint goal accuracy on DSTC2 and WOZ 2.0 of StateNet_PSI using different pre-trained models based on different single slot.
<R> <C> [EMPTY] <C> EN → DE R@1 <C> EN → DE R@5 <C> EN → DE R@10 <C> DE → EN R@1 <C> DE → EN R@5 <C> DE → EN R@10 <R> <C> FME <C> 51.4 <C> 76.4 <C> 84.5 <C> 46.9 <C> 71.2 <C> 79.1 <R> <C> AME <C> [BOLD] 51.7 <C> [BOLD] 76.7 <C> [BOLD] 85.1 <C> [BOLD] 49.1 <C> [BOLD] 72.6 <C> [BOLD] 80.5 <CAP> Table 5: Textual similarity scores (asymmetric, Multi30k).
<R> <C> [EMPTY] <C> Image to Text R@1 <C> Image to Text R@5 <C> Image to Text R@10 <C> Image to Text Mr <C> Text to Image R@1 <C> Text to Image R@5 <C> Text to Image R@10 <C> Text to Image Mr <C> Alignment <R> <C> [BOLD] symmetric <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Parallel gella:17 <C> 31.7 <C> 62.4 <C> 74.1 <C> 3 <C> 24.7 <C> 53.9 <C> 65.7 <C> 5 <C> - <R> <C> UVS kiros:15 <C> 23.0 <C> 50.7 <C> 62.9 <C> 5 <C> 16.8 <C> 42.0 <C> 56.5 <C> 8 <C> - <R> <C> EmbeddingNet wang:18 <C> 40.7 <C> 69.7 <C> 79.2 <C> - <C> 29.2 <C> 59.6 <C> 71.7 <C> - <C> - <R> <C> sm-LSTM huang:17 <C> 42.5 <C> 71.9 <C> 81.5 <C> 2 <C> 30.2 <C> 60.4 <C> 72.3 <C> 3 <C> - <R> <C> VSE++ faghri:18 <C> [BOLD] 43.7 <C> 71.9 <C> 82.1 <C> 2 <C> 32.3 <C> 60.9 <C> 72.1 <C> 3 <C> - <R> <C> Mono <C> 41.4 <C> 74.2 <C> 84.2 <C> 2 <C> 32.1 <C> 63.0 <C> 73.9 <C> 3 <C> - <R> <C> FME <C> 39.2 <C> 71.1 <C> 82.1 <C> 2 <C> 29.7 <C> 62.5 <C> 74.1 <C> 3 <C> 76.81% <R> <C> AME <C> 43.5 <C> [BOLD] 77.2 <C> [BOLD] 85.3 <C> [BOLD] 2 <C> [BOLD] 34.0 <C> [BOLD] 64.2 <C> [BOLD] 75.4 <C> [BOLD] 3 <C> 66.91% <R> <C> [BOLD] asymmetric <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Pivot gella:17 <C> 33.8 <C> 62.8 <C> 75.2 <C> 3 <C> 26.2 <C> 56.4 <C> 68.4 <C> 4 <C> - <R> <C> Parallel gella:17 <C> 31.5 <C> 61.4 <C> 74.7 <C> 3 <C> 27.1 <C> 56.2 <C> 66.9 <C> 4 <C> - <R> <C> Mono <C> 47.7 <C> 77.1 <C> 86.9 <C> 2 <C> 35.8 <C> 66.6 <C> 76.8 <C> 3 <C> - <R> <C> FME <C> 44.9 <C> 76.9 <C> 86.4 <C> 2 <C> 34.2 <C> 66.1 <C> 77.1 <C> 3 <C> 76.81% <R> <C> AME <C> [BOLD] 50.5 <C> [BOLD] 79.7 <C> [BOLD] 88.4 <C> [BOLD] 1 <C> [BOLD] 38.0 <C> [BOLD] 68.5 <C> [BOLD] 78.4 <C> [BOLD] 2 <C> 73.10% <CAP> Table 1: Image-caption ranking results for English (Multi30k)
<R> <C> [EMPTY] <C> Image to Text R@1 <C> Image to Text R@5 <C> Image to Text R@10 <C> Image to Text Mr <C> Text to Image R@1 <C> Text to Image R@5 <C> Text to Image R@10 <C> Text to Image Mr <C> Alignment <R> <C> [BOLD] symmetric <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Parallel gella:17 <C> 28.2 <C> 57.7 <C> 71.3 <C> 4 <C> 20.9 <C> 46.9 <C> 59.3 <C> 6 <C> - <R> <C> Mono <C> 34.2 <C> 67.5 <C> 79.6 <C> 3 <C> 26.5 <C> 54.7 <C> 66.2 <C> 4 <C> - <R> <C> FME <C> 36.8 <C> 69.4 <C> 80.8 <C> 2 <C> 26.6 <C> 56.2 <C> 68.5 <C> 4 <C> 76.81% <R> <C> AME <C> [BOLD] 39.6 <C> [BOLD] 72.7 <C> [BOLD] 82.7 <C> [BOLD] 2 <C> [BOLD] 28.9 <C> [BOLD] 58.0 <C> [BOLD] 68.7 <C> [BOLD] 4 <C> 66.91% <R> <C> [BOLD] asymmetric <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Pivot gella:17 <C> 28.2 <C> 61.9 <C> 73.4 <C> 3 <C> 22.5 <C> 49.3 <C> 61.7 <C> 6 <C> - <R> <C> Parallel gella:17 <C> 30.2 <C> 60.4 <C> 72.8 <C> 3 <C> 21.8 <C> 50.5 <C> 62.3 <C> 5 <C> - <R> <C> Mono <C> [BOLD] 42.0 <C> 72.5 <C> 83.0 <C> 2 <C> 29.6 <C> 58.4 <C> 69.6 <C> 4 <C> - <R> <C> FME <C> 40.5 <C> 73.3 <C> 83.4 <C> 2 <C> 29.6 <C> 59.2 <C> [BOLD] 72.1 <C> 3 <C> 76.81% <R> <C> AME <C> 40.5 <C> [BOLD] 74.3 <C> [BOLD] 83.4 <C> [BOLD] 2 <C> [BOLD] 31.0 <C> [BOLD] 60.5 <C> 70.6 <C> [BOLD] 3 <C> 73.10% <CAP> Table 2: Image-caption ranking results for German (Multi30k)
<R> <C> [EMPTY] <C> Image to Text R@1 <C> Image to Text R@5 <C> Image to Text R@10 <C> Image to Text Mr <C> Text to Image R@1 <C> Text to Image R@5 <C> Text to Image R@10 <C> Text to Image Mr <C> Alignment <R> <C> [BOLD] symmetric <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> UVS kiros:15 <C> 43.4 <C> 75.7 <C> 85.8 <C> 2 <C> 31.0 <C> 66.7 <C> 79.9 <C> 3 <C> - <R> <C> EmbeddingNet wang:18 <C> 50.4 <C> 79.3 <C> 89.4 <C> - <C> 39.8 <C> 75.3 <C> 86.6 <C> - <C> - <R> <C> sm-LSTM huang:17 <C> 53.2 <C> 83.1 <C> 91.5 <C> 1 <C> 40.7 <C> 75.8 <C> 87.4 <C> 2 <C> - <R> <C> VSE++ faghri:18 <C> [BOLD] 58.3 <C> [BOLD] 86.1 <C> 93.3 <C> 1 <C> [BOLD] 43.6 <C> 77.6 <C> 87.8 <C> 2 <C> - <R> <C> Mono <C> 51.8 <C> 84.8 <C> 93.5 <C> 1 <C> 40.0 <C> 77.3 <C> 89.4 <C> 2 <C> - <R> <C> FME <C> 42.2 <C> 76.6 <C> 91.1 <C> 2 <C> 31.2 <C> 69.2 <C> 83.7 <C> 3 <C> 92.70% <R> <C> AME <C> 54.6 <C> 85 <C> [BOLD] 94.3 <C> [BOLD] 1 <C> 42.1 <C> [BOLD] 78.7 <C> [BOLD] 90.3 <C> [BOLD] 2 <C> 82.54% <R> <C> [BOLD] asymmetric <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Mono <C> 53.2 <C> 87.0 <C> 94.7 <C> 1 <C> 42.3 <C> 78.9 <C> 90 <C> 2 <C> - <R> <C> FME <C> 48.3 <C> 83.6 <C> 93.6 <C> 2 <C> 37.2 <C> 75.4 <C> 88.4 <C> 2 <C> 92.70% <R> <C> AME <C> [BOLD] 58.8 <C> [BOLD] 88.6 <C> [BOLD] 96.2 <C> [BOLD] 1 <C> [BOLD] 46.2 <C> [BOLD] 82.5 <C> [BOLD] 91.9 <C> [BOLD] 2 <C> 84.99% <CAP> Table 3: Image-caption ranking results for English (MS-COCO)
<R> <C> [EMPTY] <C> Image to Text R@1 <C> Image to Text R@5 <C> Image to Text R@10 <C> Image to Text Mr <C> Text to Image R@1 <C> Text to Image R@5 <C> Text to Image R@10 <C> Text to Image Mr <C> Alignment <R> <C> [BOLD] symmetric <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Mono <C> 42.7 <C> 77.7 <C> 88.5 <C> 2 <C> 33.1 <C> 69.8 <C> 84.3 <C> 3 <C> - <R> <C> FME <C> 40.7 <C> 77.7 <C> 88.3 <C> 2 <C> 30.0 <C> 68.9 <C> 83.1 <C> 3 <C> 92.70% <R> <C> AME <C> [BOLD] 50.2 <C> [BOLD] 85.6 <C> [BOLD] 93.1 <C> [BOLD] 1 <C> [BOLD] 40.2 <C> [BOLD] 76.7 <C> [BOLD] 87.8 <C> [BOLD] 2 <C> 82.54% <R> <C> [BOLD] asymmetric <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Mono <C> 49.9 <C> 83.4 <C> 93.7 <C> 2 <C> 39.7 <C> 76.5 <C> 88.3 <C> [BOLD] 2 <C> - <R> <C> FME <C> 48.8 <C> 81.9 <C> 91.9 <C> 2 <C> 37.0 <C> 74.8 <C> 87.0 <C> [BOLD] 2 <C> 92.70% <R> <C> AME <C> [BOLD] 55.5 <C> [BOLD] 87.9 <C> [BOLD] 95.2 <C> [BOLD] 1 <C> [BOLD] 44.9 <C> [BOLD] 80.7 <C> [BOLD] 89.3 <C> [BOLD] 2 <C> 84.99% <CAP> Table 4: Image-caption ranking results for Japanese (MS-COCO)
<R> <C> [EMPTY] <C> Italian Original <C> Italian Debiased <C> Italian English <C> Italian Reduction <C> German Original <C> German Debiased <C> German English <C> German Reduction <R> <C> Same Gender <C> 0.442 <C> 0.434 <C> 0.424 <C> – <C> 0.491 <C> 0.478 <C> 0.446 <C> – <R> <C> Different Gender <C> 0.385 <C> 0.421 <C> 0.415 <C> – <C> 0.415 <C> 0.435 <C> 0.403 <C> – <R> <C> difference <C> 0.057 <C> 0.013 <C> 0.009 <C> [BOLD] 91.67% <C> 0.076 <C> 0.043 <C> 0.043 <C> [BOLD] 100% <CAP> Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. “Reduction” stands for gap reduction when removing gender signals from the context.
<R> <C> [EMPTY] <C> Italian Same-gender <C> Italian Diff-Gender <C> Italian difference <C> German Same-gender <C> German Diff-Gender <C> German difference <R> <C> 7–10 <C> Og: 4884 <C> Og: 12947 <C> Og: 8063 <C> Og: 5925 <C> Og: 33604 <C> Og: 27679 <R> <C> 7–10 <C> Db: 5523 <C> Db: 7312 <C> Db: 1789 <C> Db: 7653 <C> Db: 26071 <C> Db: 18418 <R> <C> 7–10 <C> En: 6978 <C> En: 2467 <C> En: -4511 <C> En: 4517 <C> En: 8666 <C> En: 4149 <R> <C> 4–7 <C> Og: 10954 <C> Og: 15838 <C> Og: 4884 <C> Og: 19271 <C> Og: 27256 <C> Og: 7985 <R> <C> 4–7 <C> Db: 12037 <C> Db: 12564 <C> Db: 527 <C> Db: 24845 <C> Db: 22970 <C> Db: -1875 <R> <C> 4–7 <C> En: 15891 <C> En: 17782 <C> En: 1891 <C> En: 13282 <C> En: 17649 <C> En: 4367 <R> <C> 0–4 <C> Og: 23314 <C> Og: 35783 <C> Og: 12469 <C> Og: 50983 <C> Og: 85263 <C> Og: 34280 <R> <C> 0–4 <C> Db: 26386 <C> Db: 28067 <C> Db: 1681 <C> Db: 60603 <C> Db: 79081 <C> Db: 18478 <R> <C> 0–4 <C> En: 57278 <C> En: 53053 <C> En: -4225 <C> En: 41509 <C> En: 62929 <C> En: 21420 <CAP> Table 2: Averages of rankings of the words in same-gender pairs vs. different-gender pairs for Italian and German, along with their differences. Og stands for the original embeddings, Db for the debiased embeddings, and En for English. Each row presents the averages of pairs with the respective scores in SimLex-999 (0–4, 4–7, 7–10).
<R> <C> [EMPTY] <C> Italian Orig <C> Italian Debias <C> German Orig <C> German Debias <R> <C> SimLex <C> 0.280 <C> [BOLD] 0.288 <C> 0.343 <C> [BOLD] 0.356 <R> <C> WordSim <C> 0.548 <C> [BOLD] 0.577 <C> 0.547 <C> [BOLD] 0.553 <CAP> Table 6: Results on SimLex-999 and WordSim-353, in Italian and German, before and after debiasing.
<R> <C> [EMPTY] <C> Italian → En <C> Italian En → <C> German → En <C> German En → <R> <C> Orig <C> 58.73 <C> 59.68 <C> 47.58 <C> 50.48 <R> <C> Debias <C> [BOLD] 60.03 <C> [BOLD] 60.96 <C> [BOLD] 47.89 <C> [BOLD] 51.76 <CAP> Table 7: Cross-lingual embedding alignment in Italian and in German, before and after debiasing.
<R> <C> Topic Name <C> Size <C> TF-IDF ARI <C> WMD ARI <C> Sent2vec ARI <C> Doc2vec ARI <C> BERT ARI <C> [ITALIC] OD-w2v ARI <C> [ITALIC] OD-d2v ARI <C> TF-IDF  [ITALIC] Sil. <C> WMD  [ITALIC] Sil. <C> Sent2vec  [ITALIC] Sil. <C> Doc2vec  [ITALIC] Sil. <C> BERT  [ITALIC] Sil. <C> [ITALIC] OD-w2v  [ITALIC] Sil. <C> [ITALIC] OD-d2v  [ITALIC] Sil. <R> <C> Affirmative Action <C> 81 <C> -0.07 <C> -0.02 <C> 0.03 <C> -0.01 <C> -0.02 <C> [BOLD] 0.14 <C> [ITALIC] 0.02 <C> 0.01 <C> 0.01 <C> -0.01 <C> -0.02 <C> -0.04 <C> [BOLD] 0.06 <C> [ITALIC] 0.01 <R> <C> Atheism <C> 116 <C> [BOLD] 0.19 <C> 0.07 <C> 0.00 <C> 0.03 <C> -0.01 <C> 0.11 <C> [ITALIC] 0.16 <C> 0.02 <C> 0.01 <C> 0.02 <C> 0.01 <C> 0.01 <C> [ITALIC] 0.05 <C> [BOLD] 0.07 <R> <C> Austerity Measures <C> 20 <C> [ITALIC] 0.04 <C> [ITALIC] 0.04 <C> -0.01 <C> -0.05 <C> 0.04 <C> [BOLD] 0.21 <C> -0.01 <C> 0.06 <C> 0.07 <C> 0.05 <C> -0.03 <C> 0.10 <C> [BOLD] 0.19 <C> 0.1 <R> <C> Democratization <C> 76 <C> 0.02 <C> -0.01 <C> 0.00 <C> [ITALIC] 0.09 <C> -0.01 <C> [BOLD] 0.11 <C> 0.07 <C> 0.01 <C> 0.01 <C> 0.02 <C> 0.02 <C> 0.03 <C> [BOLD] 0.16 <C> [ITALIC] 0.11 <R> <C> Education Voucher Scheme <C> 30 <C> [BOLD] 0.25 <C> 0.12 <C> 0.08 <C> -0.02 <C> 0.04 <C> 0.13 <C> [ITALIC] 0.19 <C> 0.01 <C> 0.01 <C> 0.01 <C> -0.01 <C> 0.02 <C> [ITALIC] 0.38 <C> [BOLD] 0.40 <R> <C> Gambling <C> 60 <C> -0.06 <C> -0.01 <C> -0.02 <C> 0.04 <C> 0.09 <C> [ITALIC] 0.35 <C> [BOLD] 0.39 <C> 0.01 <C> 0.02 <C> 0.03 <C> 0.01 <C> 0.09 <C> [BOLD] 0.30 <C> [ITALIC] 0.22 <R> <C> Housing <C> 30 <C> 0.01 <C> -0.01 <C> -0.01 <C> -0.02 <C> 0.08 <C> [BOLD] 0.27 <C> 0.01 <C> 0.02 <C> 0.03 <C> 0.03 <C> 0.01 <C> 0.11 <C> [BOLD] 0.13 <C> [ITALIC] 0.13 <R> <C> Hydroelectric Dams <C> 110 <C> [BOLD] 0.47 <C> [ITALIC] 0.45 <C> [ITALIC] 0.45 <C> -0.01 <C> 0.38 <C> 0.35 <C> 0.14 <C> 0.04 <C> 0.08 <C> 0.12 <C> 0.01 <C> 0.19 <C> [BOLD] 0.26 <C> [ITALIC] 0.09 <R> <C> Intellectual Property <C> 66 <C> 0.01 <C> 0.01 <C> 0.00 <C> 0.03 <C> 0.03 <C> [ITALIC] 0.05 <C> [BOLD] 0.14 <C> 0.01 <C> [ITALIC] 0.04 <C> 0.03 <C> 0.01 <C> 0.03 <C> [ITALIC] 0.04 <C> [BOLD] 0.12 <R> <C> Keystone pipeline <C> 18 <C> 0.01 <C> 0.01 <C> 0.00 <C> -0.13 <C> [BOLD] 0.07 <C> -0.01 <C> [BOLD] 0.07 <C> -0.01 <C> -0.03 <C> -0.03 <C> -0.07 <C> 0.03 <C> [BOLD] 0.05 <C> [ITALIC] 0.02 <R> <C> Monarchy <C> 61 <C> -0.04 <C> 0.01 <C> 0.00 <C> 0.03 <C> -0.02 <C> [BOLD] 0.15 <C> [BOLD] 0.15 <C> 0.01 <C> 0.02 <C> 0.02 <C> 0.01 <C> 0.01 <C> [BOLD] 0.11 <C> [ITALIC] 0.09 <R> <C> National Service <C> 33 <C> 0.14 <C> -0.03 <C> -0.01 <C> 0.02 <C> 0.01 <C> [ITALIC] 0.31 <C> [BOLD] 0.39 <C> 0.02 <C> 0.04 <C> 0.02 <C> 0.01 <C> 0.02 <C> [BOLD] 0.25 <C> [BOLD] 0.25 <R> <C> One-child policy China <C> 67 <C> -0.05 <C> 0.01 <C> [BOLD] 0.11 <C> -0.02 <C> 0.02 <C> [BOLD] 0.11 <C> 0.01 <C> 0.01 <C> 0.02 <C> [ITALIC] 0.04 <C> -0.01 <C> 0.03 <C> [BOLD] 0.07 <C> -0.02 <R> <C> Open-source Software <C> 48 <C> -0.02 <C> -0.01 <C> [ITALIC] 0.05 <C> 0.01 <C> 0.12 <C> [BOLD] 0.09 <C> -0.02 <C> 0.01 <C> -0.01 <C> 0.00 <C> -0.02 <C> 0.03 <C> [BOLD] 0.18 <C> 0.01 <R> <C> Pornography <C> 52 <C> -0.02 <C> 0.01 <C> 0.01 <C> -0.02 <C> -0.01 <C> [BOLD] 0.41 <C> [BOLD] 0.41 <C> 0.01 <C> 0.01 <C> 0.02 <C> -0.01 <C> 0.03 <C> [BOLD] 0.47 <C> [ITALIC] 0.41 <R> <C> Seanad Abolition <C> 25 <C> 0.23 <C> 0.09 <C> -0.01 <C> -0.01 <C> 0.03 <C> [ITALIC] 0.32 <C> [BOLD] 0.54 <C> 0.02 <C> 0.01 <C> -0.01 <C> -0.03 <C> -0.04 <C> [ITALIC] 0.15 <C> [BOLD] 0.31 <R> <C> Trades Unions <C> 19 <C> [ITALIC] 0.44 <C> [ITALIC] 0.44 <C> [BOLD] 0.60 <C> -0.05 <C> 0.44 <C> [ITALIC] 0.44 <C> 0.29 <C> 0.1 <C> 0.17 <C> 0.21 <C> 0.01 <C> 0.26 <C> [BOLD] 0.48 <C> [ITALIC] 0.32 <R> <C> Video Games <C> 72 <C> -0.01 <C> 0.01 <C> 0.12 <C> 0.01 <C> 0.08 <C> [ITALIC] 0.40 <C> [BOLD] 0.56 <C> 0.01 <C> 0.01 <C> 0.06 <C> 0.01 <C> 0.05 <C> [ITALIC] 0.32 <C> [BOLD] 0.42 <R> <C> Average <C> 54.67 <C> 0.09 <C> 0.07 <C> 0.08 <C> 0.01 <C> 0.08 <C> [BOLD] 0.22 <C> [ITALIC] 0.20 <C> 0.02 <C> 0.03 <C> 0.04 <C> -0.01 <C> 0.05 <C> [BOLD] 0.20 <C> [ITALIC] 0.17 <CAP> Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.
<R> <C> Methods <C> Seanad Abolition ARI <C> Seanad Abolition  [ITALIC] Sil <C> Video Games ARI <C> Video Games  [ITALIC] Sil <C> Pornography ARI <C> Pornography  [ITALIC] Sil <R> <C> TF-IDF <C> 0.23 <C> 0.02 <C> -0.01 <C> 0.01 <C> -0.02 <C> 0.01 <R> <C> WMD <C> 0.09 <C> 0.01 <C> 0.01 <C> 0.01 <C> -0.02 <C> 0.01 <R> <C> Sent2vec <C> -0.01 <C> -0.01 <C> 0.11 <C> 0.06 <C> 0.01 <C> 0.02 <R> <C> Doc2vec <C> -0.01 <C> -0.03 <C> -0.01 <C> 0.01 <C> 0.02 <C> -0.01 <R> <C> BERT <C> 0.03 <C> -0.04 <C> 0.08 <C> 0.05 <C> -0.01 <C> 0.03 <R> <C> OD-parse <C> 0.01 <C> -0.04 <C> -0.01 <C> 0.02 <C> 0.07 <C> 0.05 <R> <C> OD <C> [BOLD] 0.54 <C> [BOLD] 0.31 <C> [BOLD] 0.56 <C> [BOLD] 0.42 <C> [BOLD] 0.41 <C> [BOLD] 0.41 <CAP> Table 3: ARI and Silhouette coefficient scores.
<R> <C> Baselines <C> Seanad Abolition <C> Video Games <C> Pornography <R> <C> Unigrams <C> 0.54 <C> 0.66 <C> 0.63 <R> <C> Bigrams <C> 0.54 <C> 0.64 <C> 0.56 <R> <C> LSA <C> 0.68 <C> 0.57 <C> 0.57 <R> <C> Sentiment <C> 0.35 <C> 0.60 <C> 0.69 <R> <C> Bigrams <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> + Sentiment <C> 0.43 <C> 0.58 <C> 0.66 <R> <C> TF-IDF <C> 0.50 <C> 0.65 <C> 0.57 <R> <C> WMD <C> 0.40 <C> 0.73 <C> 0.57 <R> <C> Sent2vec <C> 0.39 <C> 0.79 <C> 0.70 <R> <C> Doc2vec <C> 0.27 <C> 0.51 <C> 0.56 <R> <C> BERT <C> 0.46 <C> 0.84 <C> 0.68 <R> <C> Unigrams <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> + Bigrams <C> 0.40 <C> 0.64 <C> 0.78 <R> <C> + Sentiment <C> 0.24 <C> 0.54 <C> 0.54 <R> <C> + LSA <C> 0.73 <C> 0.51 <C> 0.58 <R> <C> + TF-IDF <C> 0.42 <C> 0.65 <C> 0.56 <R> <C> + WMD <C> 0.48 <C> 0.73 <C> 0.53 <R> <C> + Sent2vec <C> 0.56 <C> 0.59 <C> 0.66 <R> <C> + Doc2vec <C> 0.31 <C> 0.56 <C> 0.47 <R> <C> OD-parse <C> 0.50 <C> 0.58 <C> 0.53 <R> <C> OD <C> 0.71 <C> [BOLD] 0.88 <C> 0.88 <R> <C> OD <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> + Unigrams <C> 0.83 <C> [ITALIC] 0.86 <C> [ITALIC] 0.88 <R> <C> + Bigrams <C> [BOLD] 0.87 <C> 0.85 <C> [ITALIC] 0.88 <R> <C> + Sentiment <C> 0.64 <C> [ITALIC] 0.86 <C> 0.86 <R> <C> + LSA <C> [ITALIC] 0.84 <C> 0.82 <C> [BOLD] 0.90 <R> <C> + WMD <C> 0.75 <C> 0.82 <C> 0.86 <CAP> Table 4: The quality of opinion distance when leveraged as a feature for multi-class classification. Each entry in + X feature should be treated independently. The second best result is italicized and underlined.
<R> <C> [EMPTY] <C> Difference Function <C> Seanad Abolition <C> Video Games <C> Pornography <R> <C> OD-parse <C> Absolute <C> 0.01 <C> -0.01 <C> 0.07 <R> <C> OD-parse <C> JS div. <C> 0.01 <C> -0.01 <C> -0.01 <R> <C> OD-parse <C> EMD <C> 0.07 <C> 0.01 <C> -0.01 <R> <C> OD <C> Absolute <C> [BOLD] 0.54 <C> [BOLD] 0.56 <C> [BOLD] 0.41 <R> <C> OD <C> JS div. <C> 0.07 <C> -0.01 <C> -0.02 <R> <C> OD <C> EMD <C> 0.26 <C> -0.01 <C> 0.01 <R> <C> OD (no polarity shifters) <C> Absolute <C> 0.23 <C> 0.08 <C> 0.04 <R> <C> OD (no polarity shifters) <C> JS div. <C> 0.09 <C> -0.01 <C> -0.02 <R> <C> OD (no polarity shifters) <C> EMD <C> 0.10 <C> 0.01 <C> -0.01 <CAP> Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.
<R> <C> [EMPTY] <C> [BOLD] Accuracy <C> [BOLD] Macro F <C> [BOLD] S <C> [BOLD] D <C> [BOLD] Q <C> [BOLD] C <R> <C> Development <C> 0.782 <C> 0.561 <C> 0.621 <C> 0.000 <C> 0.762 <C> 0.860 <R> <C> Testing <C> [BOLD] 0.784 <C> 0.434 <C> 0.403 <C> 0.000 <C> 0.462 <C> 0.873 <CAP> Table 3: Results on the development and testing sets. Accuracy and F1 scores: macro-averaged and per class (S: supporting, D: denying, Q: querying, C: commenting).
<R> <C> [BOLD] LabelPrediction <C> [BOLD] C <C> [BOLD] D <C> [BOLD] Q <C> [BOLD] S <R> <C> [BOLD] Commenting <C> 760 <C> 0 <C> 12 <C> 6 <R> <C> [BOLD] Denying <C> 68 <C> 0 <C> 1 <C> 2 <R> <C> [BOLD] Querying <C> 69 <C> 0 <C> 36 <C> 1 <R> <C> [BOLD] Supporting <C> 67 <C> 0 <C> 1 <C> 26 <CAP> Table 5: Confusion matrix for testing set predictions
<R> <C> Language <C> ABSA <C> No. of Tokens and Opinion Targets Train <C> No. of Tokens and Opinion Targets Train <C> No. of Tokens and Opinion Targets Train <C> No. of Tokens and Opinion Targets Test <C> No. of Tokens and Opinion Targets Test <C> No. of Tokens and Opinion Targets Test <R> <C> [EMPTY] <C> [EMPTY] <C> Token <C> B-target <C> I-target <C> Token <C> B-target <C> I-target <R> <C> en <C> 2014 <C> 47028 <C> 3687 <C> 1457 <C> 12606 <C> 1134 <C> 524 <R> <C> en <C> 2015 <C> 18488 <C> 1199 <C> 538 <C> 10412 <C> 542 <C> 264 <R> <C> en <C> 2016 <C> 28900 <C> 1743 <C> 797 <C> 9952 <C> 612 <C> 274 <R> <C> es <C> 2016 <C> 35847 <C> 1858 <C> 742 <C> 13179 <C> 713 <C> 173 <R> <C> fr <C> 2016 <C> 26777 <C> 1641 <C> 443 <C> 11646 <C> 650 <C> 239 <R> <C> nl <C> 2016 <C> 24788 <C> 1231 <C> 331 <C> 7606 <C> 373 <C> 81 <R> <C> ru <C> 2016 <C> 51509 <C> 3078 <C> 953 <C> 16999 <C> 952 <C> 372 <R> <C> tr <C> 2016 <C> 12406 <C> 1374 <C> 516 <C> 1316 <C> 145 <C> 61 <CAP> Table 1: ABSA SemEval 2014-2016 datasets for the restaurant domain. B-target indicates the number of opinion targets in each set; I-target refers to the number of multiword targets.
<R> <C> Features <C> 2014 P <C> 2014 R <C> 2014 F1 <C> 2015 P <C> 2015 R <C> 2015 F1 <C> 2016 P <C> 2016 R <C> 2016 F1 <R> <C> Local (L) <C> 81.84 <C> 74.69 <C> 78.10 <C> [BOLD] 76.82 <C> 54.43 <C> 63.71 <C> 74.41 <C> 61.76 <C> 67.50 <R> <C> L + BY <C> 77.84 <C> 84.57 <C> 81.07 <C> 71.73 <C> 63.65 <C> 67.45 <C> [BOLD] 74.49 <C> 71.08 <C> 72.74 <R> <C> L + CYF100-CYR200 <C> [BOLD] 82.91 <C> 84.30 <C> 83.60 <C> 73.25 <C> 61.62 <C> 66.93 <C> 74.12 <C> 72.06 <C> 73.07 <R> <C> L + W2VW400 <C> 76.82 <C> 82.10 <C> 79.37 <C> 74.42 <C> 59.04 <C> 65.84 <C> 73.04 <C> 65.52 <C> 69.08 <R> <C> L +  [BOLD] ALL <C> 81.15 <C> [BOLD] 87.30 <C> [BOLD] 84.11 <C> 72.90 <C> [BOLD] 69.00 <C> [BOLD] 70.90 <C> 73.33 <C> [BOLD] 73.69 <C> [BOLD] 73.51 <CAP> Table 3: ABSA SemEval 2014-2016 English results. BY: Brown Yelp 1000 classes; CYF100-CYR200: Clark Yelp Food 100 classes and Clark Yelp Reviews 200 classes; W2VW400: Word2vec Wikipedia 400 classes; ALL: BY+CYF100-CYR200+W2VW400.
<R> <C> Language <C> System <C> F1 <R> <C> es <C> GTI <C> 68.51 <R> <C> es <C> L +  [BOLD] CW600 + W2VW300 <C> [BOLD] 69.92 <R> <C> es <C> Baseline <C> 51.91 <R> <C> fr <C> IIT-T <C> 66.67 <R> <C> fr <C> L +  [BOLD] CW100 <C> [BOLD] 69.50 <R> <C> fr <C> Baseline <C> 45.45 <R> <C> nl <C> IIT-T <C> 56.99 <R> <C> nl <C> L +  [BOLD] W2VW400 <C> [BOLD] 66.39 <R> <C> nl <C> Baseline <C> 50.64 <R> <C> ru <C> Danii. <C> 33.47 <R> <C> ru <C> L +  [BOLD] CW500 <C> [BOLD] 65.53 <R> <C> ru <C> Baseline <C> 49.31 <R> <C> tr <C> L +  [BOLD] BW <C> [BOLD] 60.22 <R> <C> tr <C> Baseline <C> 41.86 <CAP> Table 6: ABSA SemEval 2016: Comparison of multilingual results in terms of F1 scores.
<R> <C> Error type <C> 2014 en <C> 2015 en <C> 2016 en <C> 2016 es <C> 2016 fr <C> 2016 nl <C> 2016 ru <C> 2016 tr <R> <C> FP <C> [BOLD] 230 <C> 151 <C> [BOLD] 189 <C> 165 <C> 194 <C> 117 <C> [BOLD] 390 <C> 62 <R> <C> FN <C> 143 <C> [BOLD] 169 <C> 163 <C> [BOLD] 248 <C> [BOLD] 202 <C> [BOLD] 132 <C> 312 <C> [BOLD] 65 <CAP> Table 7: False Positives and Negatives for every ABSA 2014-2016 setting.
<R> <C> Category Semantic <C> Category no oov words <C> gr_def 60.60% <C> gr_neg10 62.50% <C> cc.el.300  [BOLD] 70.90% <C> wiki.el 37.50% <C> gr_cbow_def 29.80% <C> gr_d300_nosub 62.50% <C> gr_w2v_sg_n5 54.60% <R> <C> [EMPTY] <C> with oov words <C> 54.90% <C> 57.00% <C> [BOLD] 65.50% <C> 35.00% <C> 27.10% <C> 56.60% <C> 49.50% <R> <C> Syntactic <C> no oov words <C> 67.90% <C> 62.90% <C> [BOLD] 69.60% <C> 50.70% <C> 63.80% <C> 56.50% <C> 55.40% <R> <C> [EMPTY] <C> with oov words <C> [BOLD] 55.70% <C> 51.30% <C> 50.20% <C> 33.40% <C> 52.30% <C> 46.40% <C> 45.50% <R> <C> Overall <C> no oov words <C> 65.19% <C> 62.66% <C> [BOLD] 70.12% <C> 45.01% <C> 51.18% <C> 58.73% <C> 55.10% <R> <C> [EMPTY] <C> with oov words <C> 55.46% <C> 53.30% <C> [BOLD] 55.54% <C> 33.94% <C> 43.53% <C> 49.96% <C> 46.87% <CAP> Table 8: Summary for 3CosMul and top-1 nearest vectors.
<R> <C> Relation <C> #pairs <C> #tuples <R> <C> Semantic: (13650 tuples) <C> Semantic: (13650 tuples) <C> Semantic: (13650 tuples) <R> <C> common_capital_country <C> 42 <C> 1722 <R> <C> all_capital_country <C> 78 <C> 6006 <R> <C> eu_city_country <C> 50 <C> 2366 <R> <C> city_in_region <C> 40 <C> 1536 <R> <C> currency_country <C> 24 <C> 552 <R> <C> man_woman_family <C> 18 <C> 306 <R> <C> profession_placeof_work <C> 16 <C> 240 <R> <C> performer_action <C> 24 <C> 552 <R> <C> politician_country <C> 20 <C> 370 <R> <C> Syntactic: (25524 tuples) <C> Syntactic: (25524 tuples) <C> Syntactic: (25524 tuples) <R> <C> man_woman_job <C> 26 <C> 650 <R> <C> adjective_adverb <C> 28 <C> 756 <R> <C> opposite <C> 35 <C> 1190 <R> <C> comparative <C> 36 <C> 1260 <R> <C> superlative <C> 25 <C> 600 <R> <C> present_participle_active <C> 48 <C> 2256 <R> <C> present_participle_passive <C> 44 <C> 1892 <R> <C> nationality_adjective_man <C> 56 <C> 3080 <R> <C> nationality_adjective_woman <C> 42 <C> 1722 <R> <C> past_tense <C> 34 <C> 1122 <R> <C> plural_nouns <C> 72 <C> 5112 <R> <C> plural_verbs <C> 37 <C> 1332 <R> <C> adjectives_antonyms <C> 50 <C> 2450 <R> <C> verbs_antonyms <C> 20 <C> 380 <R> <C> verbs_i_you <C> 42 <C> 1722 <CAP> Table 1: The Greek word analogy test set.
<R> <C> Category Semantic <C> Category no oov words <C> gr_def 58.42% <C> gr_neg10 59.33% <C> cc.el.300  [BOLD] 68.80% <C> wiki.el 27.20% <C> gr_cbow_def 31.76% <C> gr_d300_nosub 60.79% <C> gr_w2v_sg_n5 52.70% <R> <C> [EMPTY] <C> with oov words <C> 52.97% <C> 55.33% <C> [BOLD] 64.34% <C> 25.73% <C> 28.80% <C> 55.11% <C> 47.82% <R> <C> Syntactic <C> no oov words <C> 65.73% <C> 61.02% <C> [BOLD] 69.35% <C> 40.90% <C> 64.02% <C> 53.69% <C> 52.60% <R> <C> [EMPTY] <C> with oov words <C> [BOLD] 53.95% <C> 48.69% <C> 49.43% <C> 28.42% <C> 52.54% <C> 44.06% <C> 43.13% <R> <C> Overall <C> no oov words <C> 63.02% <C> 59.96% <C> [BOLD] 68.97% <C> 36.45% <C> 52.04% <C> 56.30% <C> 52.66% <R> <C> [EMPTY] <C> with oov words <C> 53.60% <C> 51.00% <C> [BOLD] 54.60% <C> 27.50% <C> 44.30% <C> 47.90% <C> 44.80% <CAP> Table 3: Summary for 3CosAdd and top-1 nearest vectors.
<R> <C> Category Semantic <C> Category no oov words <C> gr_def 83.72% <C> gr_neg10 84.38% <C> cc.el.300  [BOLD] 88.50% <C> wiki.el 65.85% <C> gr_cbow_def 52.05% <C> gr_d300_nosub 83.26% <C> gr_w2v_sg_n5 80.00% <R> <C> [EMPTY] <C> with oov words <C> 75.90% <C> 76.50% <C> [BOLD] 81.70% <C> 61.40% <C> 47.20% <C> 75.50% <C> 72.50% <R> <C> Syntactic <C> no oov words <C> 83.86% <C> 80.42% <C> [BOLD] 85.07% <C> 72.56% <C> 76.22% <C> 75.97% <C> 74.55% <R> <C> [EMPTY] <C> with oov words <C> [BOLD] 68.80% <C> 66.00% <C> 61.40% <C> 47.80% <C> 62.60% <C> 62.30% <C> 61.20% <R> <C> Overall <C> no oov words <C> 83.80% <C> 81.90% <C> [BOLD] 86.50% <C> 69.70% <C> 67.20% <C> 78.70% <C> 76.60% <R> <C> [EMPTY] <C> with oov words <C> [BOLD] 71.29% <C> 69.66% <C> 68.48% <C> 52.53% <C> 57.20% <C> 66.93% <C> 65.14% <CAP> Table 7: Summary for 3CosMul and top-5 nearest vectors.
<R> <C> Model <C> Pearson <C> p-value <C> Pairs (unknown) <R> <C> gr_def <C> [BOLD] 0.6042 <C> 3.1E-35 <C> 2.3% <R> <C> gr_neg10 <C> 0.5973 <C> 2.9E-34 <C> 2.3% <R> <C> cc.el.300 <C> 0.5311 <C> 1.7E-25 <C> 4.9% <R> <C> wiki.el <C> 0.5812 <C> 2.2E-31 <C> 4.5% <R> <C> gr_cbow_def <C> 0.5232 <C> 2.7E-25 <C> 2.3% <R> <C> gr_d300_nosub <C> 0.5889 <C> 3.8E-33 <C> 2.3% <R> <C> gr_w2v_sg_n5 <C> 0.5879 <C> 4.4E-33 <C> 2.3% <CAP> Table 4: Word similarity.
<R> <C> [EMPTY] <C> [EMPTY] <C> in-domain CoNLL <C> in-domain LEA <C> out-of-domain CoNLL <C> out-of-domain LEA <R> <C> [EMPTY] <C> [EMPTY] <C> pt (Bible) <C> pt (Bible) <C> pt (Bible) <C> pt (Bible) <R> <C> deep-coref <C> ranking <C> 75.61 <C> 71.00 <C> 66.06 <C> 57.58 <R> <C> deep-coref <C> +EPM <C> 76.08 <C> 71.13 <C> <bold>68.14</bold> <C> <bold>60.74</bold> <R> <C> e2e-coref <C> single <C> 77.80 <C> 73.73 <C> 65.22 <C> 58.26 <R> <C> e2e-coref <C> ensemble <C> <bold>78.88</bold> <C> <bold>74.88</bold> <C> 65.45 <C> 59.71 <R> <C> [EMPTY] <C> [EMPTY] <C> wb (weblog) <C> wb (weblog) <C> wb (weblog) <C> wb (weblog) <R> <C> deep-coref <C> ranking <C> 61.46 <C> 53.75 <C> 57.17 <C> 48.74 <R> <C> deep-coref <C> +EPM <C> 61.97 <C> 53.93 <C> <bold>61.52</bold> <C> <bold>53.78</bold> <R> <C> e2e-coref <C> single <C> 62.02 <C> 53.09 <C> 60.69 <C> 52.69 <R> <C> e2e-coref <C> ensemble <C> <bold>64.76</bold> <C> <bold>57.54</bold> <C> 60.99 <C> 52.99 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 7: In-domain and out-of-domain evaluations for the pt and wb genres of the CoNLL test set. The highest scores are boldfaced.
<R> <C> [EMPTY] <C> MUC <C> <italic>B</italic>3 <C> CEAF<italic>e</italic> <C> CoNLL <C> LEA <R> <C> +EPM <C> 74.92 <C> 65.03 <C> 60.88 <C> 66.95 <C> 61.34 <R> <C> -pairwise <C> 74.37 <C> 64.55 <C> 60.46 <C> 66.46 <C> 60.71 <R> <C> -type <C> 74.71 <C> 64.87 <C> 61.00 <C> 66.86 <C> 61.07 <R> <C> -dep <C> 74.57 <C> 64.79 <C> 60.65 <C> 66.67 <C> 61.01 <R> <C> -NER <C> 74.61 <C> 65.05 <C> 60.93 <C> 66.86 <C> 61.27 <R> <C> -POS <C> 74.74 <C> 65.04 <C> 60.88 <C> 66.89 <C> 61.30 <R> <C> +pairwise <C> 74.25 <C> 64.33 <C> 60.02 <C> 66.20 <C> 60.57 <CAP> Table 5: Impact of different EPM feature groups on the CoNLL development set.
<R> <C> [EMPTY] <C> [EMPTY] <C> MUC R <C> MUC P <C> MUC F1 <C> <italic>B</italic>3 R <C> <italic>B</italic>3 P <C> <italic>B</italic>3 F1 <C> CEAF<italic>e</italic> R <C> CEAF<italic>e</italic> P <C> CEAF<italic>e</italic> F1 <C> CoNLL <C> LEA R <C> LEA P <C> LEA F1 <R> <C> deep-coref <C> ranking <C> 57.72 <C> 69.57 <C> 63.10 <C> 41.42 <C> 58.30 <C> 48.43 <C> 42.20 <C> 53.50 <C> 47.18 <C> 52.90 <C> 37.57 <C> 54.27 <C> 44.40 <R> <C> deep-coref <C> reinforce <C> 62.12 <C> 58.98 <C> 60.51 <C> 46.98 <C> 45.79 <C> 46.38 <C> 44.28 <C> 46.35 <C> 45.29 <C> 50.73 <C> 42.28 <C> 41.70 <C> 41.98 <R> <C> deep-coref <C> top-pairs <C> 56.31 <C> 71.74 <C> 63.09 <C> 39.78 <C> 61.85 <C> 48.42 <C> 40.80 <C> 52.85 <C> 46.05 <C> 52.52 <C> 35.87 <C> 57.58 <C> 44.21 <R> <C> deep-coref <C> +EPM <C> 58.23 <C> 74.05 <C> <bold>65.20</bold> <C> 43.33 <C> 63.90 <C> 51.64 <C> 43.44 <C> 56.33 <C> <bold>49.05</bold> <C> <bold>55.30</bold> <C> 39.70 <C> 59.81 <C> <bold>47.72</bold> <R> <C> e2e <C> single <C> 60.14 <C> 64.46 <C> 62.22 <C> 45.20 <C> 51.75 <C> 48.25 <C> 38.18 <C> 43.50 <C> 40.67 <C> 50.38 <C> 40.70 <C> 47.56 <C> 43.86 <R> <C> e2e <C> ensemble <C> 59.58 <C> 71.60 <C> 65.04 <C> 44.64 <C> 60.91 <C> 51.52 <C> 40.38 <C> 49.17 <C> 44.35 <C> 53.63 <C> 40.73 <C> 56.97 <C> 47.50 <R> <C> [EMPTY] <C> G&L <C> 66.06 <C> 62.93 <C> 64.46 <C> 57.73 <C> 48.58 <C> <bold>52.76</bold> <C> 46.76 <C> 49.54 <C> 48.11 <C> 55.11 <C> - <C> - <C> - <CAP> Table 6: Out-of-domain evaluation on the WikiCoref dataset. The highest F1 scores are boldfaced.
<R> <C> [EMPTY] <C> MUC <C> <italic>B</italic>3 <C> CEAF<italic>e</italic> <C> CoNLL <C> LEA <R> <C> ranking <C> 74.31 <C> 64.23 <C> 59.73 <C> 66.09 <C> 60.47 <R> <C> +linguistic <C> 74.35 <C> 63.96 <C> 60.19 <C> 66.17 <C> 60.20 <R> <C> top-pairs <C> 73.95 <C> 63.98 <C> 59.52 <C> 65.82 <C> 60.07 <R> <C> +linguistic <C> 74.32 <C> 64.45 <C> 60.19 <C> 66.32 <C> 60.62 <CAP> Table 1: Impact of linguistic features on deep-coref models on the CoNLL development set.
<R> <C> [EMPTY] <C> MUC <C> <italic>B</italic>3 <C> CEAF<italic>e</italic> <C> CoNLL <C> LEA <R> <C> ranking <C> 63.10 <C> 48.43 <C> 47.18 <C> 52.90 <C> 44.40 <R> <C> top-pairs <C> 63.09 <C> 48.42 <C> 46.05 <C> 52.52 <C> 44.21 <R> <C> +linguistic <C> 63.99 <C> 49.63 <C> 46.60 <C> 53.40 <C> 45.66 <CAP> Table 2: Out-of-domain evaluation of deep-coref models on the WikiCoref dataset.
<R> <C> [EMPTY] <C> [EMPTY] <C> MUC R <C> MUC P <C> MUC F1 <C> <italic>B</italic>3 R <C> <italic>B</italic>3 P <C> <italic>B</italic>3 F1 <C> CEAF<italic>e</italic> R <C> CEAF<italic>e</italic> P <C> CEAF<italic>e</italic> F1 <C> CoNLL <C> LEA R <C> LEA P <C> LEA F1 <R> <C> deep-coref <C> ranking <C> 70.43 <C> 79.57 <C> 74.72 <C> 58.08 <C> 69.26 <C> 63.18 <C> 54.43 <C> 64.17 <C> 58.90 <C> 65.60 <C> 54.55 <C> 65.68 <C> 59.60 <R> <C> deep-coref <C> reinforce <C> 69.84 <C> 79.79 <C> 74.48 <C> 57.41 <C> 70.96 <C> 63.47 <C> 55.63 <C> 63.83 <C> 59.45 <C> 65.80 <C> 53.78 <C> 67.23 <C> 59.76 <R> <C> deep-coref <C> top-pairs <C> 69.41 <C> 79.90 <C> 74.29 <C> 57.01 <C> 70.80 <C> 63.16 <C> 54.43 <C> 63.74 <C> 58.72 <C> 65.39 <C> 53.31 <C> 67.09 <C> 59.41 <R> <C> deep-coref <C> +EPM <C> 71.16 <C> 79.35 <C> 75.03 <C> 59.28 <C> 69.70 <C> 64.07 <C> 56.52 <C> 64.02 <C> 60.04 <C> 66.38 <C> 55.63 <C> 66.11 <C> 60.42 <R> <C> deep-coref <C> +JIM <C> 69.89 <C> 80.45 <C> 74.80 <C> 57.08 <C> 71.58 <C> 63.51 <C> 55.36 <C> 64.20 <C> 59.45 <C> 65.93 <C> 53.46 <C> 67.97 <C> 59.85 <R> <C> e2e <C> single <C> 74.02 <C> 77.82 <C> 75.88 <C> 62.58 <C> 67.45 <C> 64.92 <C> 59.16 <C> 62.96 <C> 61.00 <C> 67.27 <C> 58.90 <C> 63.79 <C> 61.25 <R> <C> e2e <C> ensemble <C> 73.73 <C> 80.95 <C> 77.17 <C> 61.83 <C> 72.10 <C> 66.57 <C> 60.11 <C> 65.62 <C> 62.74 <C> 68.83 <C> 58.48 <C> 68.81 <C> 63.23 <CAP> Table 4: Comparisons on the CoNLL test set. The F1 gains that are statistically significant: (1) “+EPM” compared to “top-pairs”, “ranking” and “JIM”, (2) “+EPM” compared to “reinforce” based on MUC, B3 and LEA, (3) “single” compared to “+EPM” based on MUC and B3, and (4) “ensemble” compared to other systems. Significance is measured based on the approximate randomization test (p<0.05) Noreen (1989).
<R> <C> target <C> VN <C> WN-V <C> WN-N <R> <C> type <C> 81 <C> 66 <C> 47 <R> <C> x+POS <C> 54 <C> 39 <C> 43 <R> <C> lemma <C> 88 <C> 76 <C> 53 <R> <C> x+POS <C> 79 <C> 63 <C> 50 <R> <C> shared <C> 54 <C> 39 <C> 41 <CAP> Table 4: Lexicon member coverage (%)
<R> <C> Context: w2 <C> Context: w2 SimLex <C> Context: w2 SimLex <C> Context: w2 SimLex <C> Context: w2 SimLex <C> Context: w2 SimVerb <R> <C> target <C> N <C> V <C> A <C> all <C> V <R> <C> type <C> .334 <C> <bold>.336</bold> <C> <bold>.518</bold> <C> .348 <C> .307 <R> <C> x + POS <C> .342 <C> .323 <C> .513 <C> .350 <C> .279 <R> <C> lemma <C> <bold>.362</bold> <C> .333 <C> .497 <C> <bold>.351</bold> <C> .400 <R> <C> x + POS <C> .354 <C> <bold>.336</bold> <C> .504 <C> .345 <C> <bold>.406</bold> <R> <C> * type <C> - <C> - <C> - <C> .339 <C> .277 <R> <C> * type MFit-A <C> - <C> - <C> - <C> .385 <C> - <R> <C> * type MFit-AR <C> - <C> - <C> - <C> .439 <C> .381 <R> <C> Context: dep-W <C> Context: dep-W <C> Context: dep-W <C> Context: dep-W <C> Context: dep-W <C> Context: dep-W <R> <C> type <C> .366 <C> .365 <C> .489 <C> .362 <C> .314 <R> <C> x + POS <C> .364 <C> .351 <C> .482 <C> .359 <C> .287 <R> <C> lemma <C> <bold>.391</bold> <C> .380 <C> <bold>.522</bold> <C> <bold>.379</bold> <C> .401 <R> <C> x + POS <C> .384 <C> <bold>.388</bold> <C> .480 <C> .366 <C> <bold>.431</bold> <R> <C> * type <C> - <C> - <C> - <C> .376 <C> .313 <R> <C> * type MFit-AR <C> - <C> - <C> - <C> .434 <C> .418 <CAP> Table 1: Benchmark performance, Spearman’s ρ. SGNS results with * taken from [morphfit]. Best results per column (benchmark) annotated for our setup only.
<R> <C> [EMPTY] <C> WN-N P <C> WN-N R <C> WN-N F <C> WN-V P <C> WN-V R <C> WN-V F <C> VN P <C> VN R <C> VN F <R> <C> Context: w2 <C> Context: w2 <C> Context: w2 <C> Context: w2 <C> Context: w2 <C> Context: w2 <C> Context: w2 <C> Context: w2 <C> Context: w2 <C> Context: w2 <R> <C> type <C> .700 <C> .654 <C> .676 <C> .535 <C> .474 <C> .503 <C> .327 <C> .309 <C> .318 <R> <C> x+POS <C> .699 <C> .651 <C> .674 <C> .544 <C> .472 <C> .505 <C> .339 <C> .312 <C> .325 <R> <C> lemma <C> .706 <C> .660 <C> .682 <C> .576 <C> .520 <C> .547 <C> .384 <C> .360 <C> .371 <R> <C> x+POS <C> <bold>.710</bold> <C> <bold>.662</bold> <C> <bold>.685</bold> <C> <bold>.589</bold> <C> <bold>.529</bold> <C> <bold>.557</bold> <C> <bold>.410</bold> <C> <bold>.389</bold> <C> <bold>.399</bold> <R> <C> Context: dep <C> Context: dep <C> Context: dep <C> Context: dep <C> Context: dep <C> Context: dep <C> Context: dep <C> Context: dep <C> Context: dep <C> Context: dep <R> <C> type <C> .712 <C> .661 <C> .686 <C> .545 <C> .457 <C> .497 <C> .324 <C> .296 <C> .310 <R> <C> x+POS <C> .715 <C> .659 <C> .686 <C> .560 <C> .464 <C> .508 <C> .349 <C> .320 <C> .334 <R> <C> lemma <C> <bold>.725</bold> <C> <bold>.668</bold> <C> <bold>.696</bold> <C> .591 <C> .512 <C> .548 <C> .408 <C> .371 <C> .388 <R> <C> x+POS <C> .722 <C> .666 <C> .693 <C> <bold>.609</bold> <C> <bold>.527</bold> <C> <bold>.565</bold> <C> <bold>.412</bold> <C> <bold>.381</bold> <C> <bold>.396</bold> <CAP> Table 5: WCS performance, shared vocabulary, k=1. Best results across VSMs in bold.
<R> <C> Feature <C> LR P <C> LR R <C> LR F1 <C> SVM P <C> SVM R <C> SVM F1 <C> ANN P <C> ANN R <C> ANN F1 <R> <C> +BoW <C> 0.93 <C> 0.91 <C> 0.92 <C> 0.94 <C> 0.92 <C> 0.93 <C> 0.91 <C> 0.91 <C> 0.91 <R> <C> +BoC (Wiki-PubMed-PMC) <C> 0.94 <C> 0.92 <C> [BOLD] 0.93 <C> 0.94 <C> 0.92 <C> [BOLD] 0.93 <C> 0.91 <C> 0.91 <C> [BOLD] 0.91 <R> <C> +BoC (GloVe) <C> 0.93 <C> 0.92 <C> 0.92 <C> 0.94 <C> 0.92 <C> 0.93 <C> 0.91 <C> 0.91 <C> 0.91 <R> <C> +ASM <C> 0.90 <C> 0.85 <C> 0.88 <C> 0.90 <C> 0.86 <C> 0.88 <C> 0.89 <C> 0.89 <C> 0.89 <R> <C> +Sentence Embeddings(SEs) <C> 0.89 <C> 0.89 <C> 0.89 <C> 0.90 <C> 0.86 <C> 0.88 <C> 0.88 <C> 0.88 <C> 0.88 <R> <C> +BoC(Wiki-PubMed-PMC)+SEs <C> 0.92 <C> 0.92 <C> 0.92 <C> 0.94 <C> 0.92 <C> 0.93 <C> 0.91 <C> 0.91 <C> 0.91 <CAP> Table 1: Performance of supervised learning models with different features.
<R> <C> Relation type <C> Count <C> Intra-sentential co-occ.  [ITALIC] ρ=0 <C> Intra-sentential co-occ.  [ITALIC] ρ=5 <C> Intra-sentential co-occ.  [ITALIC] ρ=10 <C> BoC(Wiki-PubMed-PMC) LR <C> BoC(Wiki-PubMed-PMC) SVM <C> BoC(Wiki-PubMed-PMC) ANN <R> <C> TherapyTiming(TP,TD) <C> 428 <C> [BOLD] 0.84 <C> 0.59 <C> 0.47 <C> 0.78 <C> 0.81 <C> 0.78 <R> <C> NextReview(Followup,TP) <C> 164 <C> [BOLD] 0.90 <C> 0.83 <C> 0.63 <C> 0.86 <C> 0.88 <C> 0.84 <R> <C> Toxicity(TP,CF/TR) <C> 163 <C> [BOLD] 0.91 <C> 0.77 <C> 0.55 <C> 0.85 <C> 0.86 <C> 0.86 <R> <C> TestTiming(TN,TD/TP) <C> 184 <C> 0.90 <C> 0.81 <C> 0.42 <C> 0.96 <C> [BOLD] 0.97 <C> 0.95 <R> <C> TestFinding(TN,TR) <C> 136 <C> 0.76 <C> 0.60 <C> 0.44 <C> [BOLD] 0.82 <C> 0.79 <C> 0.78 <R> <C> Threat(O,CF/TR) <C> 32 <C> 0.85 <C> 0.69 <C> 0.54 <C> [BOLD] 0.95 <C> [BOLD] 0.95 <C> 0.92 <R> <C> Intervention(TP,YR) <C> 5 <C> [BOLD] 0.88 <C> 0.65 <C> 0.47 <C> - <C> - <C> - <R> <C> EffectOf(Com,CF) <C> 3 <C> [BOLD] 0.92 <C> 0.62 <C> 0.23 <C> - <C> - <C> - <R> <C> Severity(CF,CS) <C> 75 <C> [BOLD] 0.61 <C> 0.53 <C> 0.47 <C> 0.52 <C> 0.55 <C> 0.51 <R> <C> RecurLink(YR,YR/CF) <C> 7 <C> [BOLD] 1.0 <C> [BOLD] 1.0 <C> 0.64 <C> - <C> - <C> - <R> <C> RecurInfer(NR/YR,TR) <C> 51 <C> 0.97 <C> 0.69 <C> 0.43 <C> [BOLD] 0.99 <C> [BOLD] 0.99 <C> 0.98 <R> <C> GetOpinion(Referral,CF/other) <C> 4 <C> [BOLD] 0.75 <C> [BOLD] 0.75 <C> 0.5 <C> - <C> - <C> - <R> <C> Context(Dis,DisCont) <C> 40 <C> [BOLD] 0.70 <C> 0.63 <C> 0.53 <C> 0.60 <C> 0.41 <C> 0.57 <R> <C> TestToAssess(TN,CF/TR) <C> 36 <C> 0.76 <C> 0.66 <C> 0.36 <C> [BOLD] 0.92 <C> [BOLD] 0.92 <C> 0.91 <R> <C> TimeStamp(TD,TP) <C> 221 <C> [BOLD] 0.88 <C> 0.83 <C> 0.50 <C> 0.86 <C> 0.85 <C> 0.83 <R> <C> TimeLink(TP,TP) <C> 20 <C> [BOLD] 0.92 <C> 0.85 <C> 0.45 <C> 0.91 <C> [BOLD] 0.92 <C> 0.90 <R> <C> Overall <C> 1569 <C> 0.90 <C> 0.73 <C> 0.45 <C> 0.92 <C> [BOLD] 0.93 <C> 0.91 <CAP> Table 2: F1 score results per relation type of the best performing models.
<R> <C> [BOLD] Dataset <C> [BOLD] # pairs <C> [BOLD] # words (doc) <C> [BOLD] # sents (docs) <C> [BOLD] # words (summary) <C> [BOLD] # sents (summary) <C> [BOLD] vocab size <R> <C> Multi-News <C> 44,972/5,622/5,622 <C> 2,103.49 <C> 82.73 <C> 263.66 <C> 9.97 <C> 666,515 <R> <C> DUC03+04 <C> 320 <C> 4,636.24 <C> 173.15 <C> 109.58 <C> 2.88 <C> 19,734 <R> <C> TAC 2011 <C> 176 <C> 4,695.70 <C> 188.43 <C> 99.70 <C> 1.00 <C> 24,672 <R> <C> CNNDM <C> 287,227/13,368/11,490 <C> 810.57 <C> 39.78 <C> 56.20 <C> 3.68 <C> 717,951 <CAP> Table 3: Comparison of our Multi-News dataset to other MDS datasets as well as an SDS dataset used as training data for MDS (CNNDM). Training, validation and testing size splits (article(s) to summary) are provided when applicable. Statistics for multi-document inputs are calculated on the concatenation of all input sources.
<R> <C> [BOLD] % novel n-grams <C> [BOLD] Multi-News <C> [BOLD] DUC03+04 <C> [BOLD] TAC11 <C> [BOLD] CNNDM <R> <C> uni-grams <C> 17.76 <C> 27.74 <C> 16.65 <C> 19.50 <R> <C> bi-grams <C> 57.10 <C> 72.87 <C> 61.18 <C> 56.88 <R> <C> tri-grams <C> 75.71 <C> 90.61 <C> 83.34 <C> 74.41 <R> <C> 4-grams <C> 82.30 <C> 96.18 <C> 92.04 <C> 82.83 <CAP> Table 4: Percentage of n-grams in summaries which do not appear in the input documents , a measure of the abstractiveness, in relevant datasets.
<R> <C> [BOLD] Method <C> [BOLD] R-1 <C> [BOLD] R-2 <C> [BOLD] R-SU <R> <C> First-1 <C> 26.83 <C> 7.25 <C> 6.46 <R> <C> First-2 <C> 35.99 <C> 10.17 <C> 12.06 <R> <C> First-3 <C> 39.41 <C> 11.77 <C> 14.51 <R> <C> LexRank Erkan and Radev ( 2004 ) <C> 38.27 <C> 12.70 <C> 13.20 <R> <C> TextRank Mihalcea and Tarau ( 2004 ) <C> 38.44 <C> 13.10 <C> 13.50 <R> <C> MMR Carbonell and Goldstein ( 1998 ) <C> 38.77 <C> 11.98 <C> 12.91 <R> <C> PG-Original Lebanoff et al. ( 2018 ) <C> 41.85 <C> 12.91 <C> 16.46 <R> <C> PG-MMR Lebanoff et al. ( 2018 ) <C> 40.55 <C> 12.36 <C> 15.87 <R> <C> PG-BRNN Gehrmann et al. ( 2018 ) <C> 42.80 <C> 14.19 <C> 16.75 <R> <C> CopyTransformer Gehrmann et al. ( 2018 ) <C> [BOLD] 43.57 <C> 14.03 <C> 17.37 <R> <C> Hi-MAP (Our Model) <C> 43.47 <C> [BOLD] 14.89 <C> [BOLD] 17.41 <CAP> Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.
<R> <C> # gold NLD steps <C> Answer Prec. <C> Derivation Prec. <R> <C> 1 <C> 79.2 <C> 38.4 <R> <C> 2 <C> 64.4 <C> 48.6 <R> <C> 3 <C> 62.3 <C> 41.3 <CAP> Table 5: Performance breakdown of the PRKGC+NS model. Derivation Precision denotes ROUGE-L F1 of generated NLDs.
<R> <C> # steps <C> Reachability <C> Derivability Step 1 <C> Derivability Step 2 <C> Derivability Step 3 <R> <C> 1 <C> 3.0 <C> 3.8 <C> - <C> - <R> <C> 2 <C> 2.8 <C> 3.8 <C> 3.7 <C> - <R> <C> 3 <C> 2.3 <C> 3.9 <C> 3.8 <C> 3.8 <CAP> Table 2: Ratings of annotated NLDs by human judges.
<R> <C> Model <C> Answerability Macro P/R/F <C> # Answerable <C> Answer Prec. <C> Derivation Prec. RG-L (P/R/F) <C> Derivation Prec. BL-4 <R> <C> Shortest Path <C> 54.8/55.5/53.2 <C> 976 <C> 3.6 <C> 56.7/38.5/41.5 <C> 31.3 <R> <C> PRKGC <C> 52.6/51.5/50.7 <C> 1,021 <C> 45.2 <C> 40.7/60.7/44.7 <C> 30.9 <R> <C> PRKGC+NS <C> 53.6/54.1/52.1 <C> 980 <C> 45.4 <C> 42.2/61.6/46.1 <C> 33.4 <CAP> Table 4: Performance of RC-QEDE of our baseline models (see Section 2.1 for further details of each evaluation metrics). “NS” indicates the use of annotated NLDs as supervision (i.e. using Ld during training).
<R> <C> Model <C> Accuracy <R> <C> PRKGC (our work) <C> 51.4 <R> <C> PRKGC+NS (our work) <C> [BOLD] 52.7 <R> <C> BiDAF Welbl2017a <C> 42.1 <R> <C> CorefGRU Dhingra2018NeuralCoreference <C> 56.0 <R> <C> MHPGM+NOIC Bauer2018CommonsenseTasks <C> 58.2 <R> <C> EntityGCN DeCao2018QuestionNetworks <C> 65.3 <R> <C> CFC Zhong2019Coarse-GrainAnswering <C> 66.4 <CAP> Table 7: Accuracy of our baseline models and previous work on WikiHop Welbl2017a’s development set. Note that our baseline models are explainable, whereas the others are not. “NS” indicates the use of annotated NLDs as supervision. Accuracies of existing models are taken from the papers.
<R> <C> ID LSTM-800 <C> 5-fold CV 70.56 <C> Δ 0.66 <C> Single model 67.54 <C> Δ 0.78 <C> Ensemble 67.65 <C> Δ 0.30 <R> <C> LSTM-400 <C> 70.50 <C> 0.60 <C> [BOLD] 67.59 <C> 0.83 <C> [BOLD] 68.00 <C> 0.65 <R> <C> IN-TITLE <C> 70.11 <C> 0.21 <C> [EMPTY] <C> [EMPTY] <C> 67.52 <C> 0.17 <R> <C> [BOLD] SUBMISSION <C> 69.90 <C> – <C> 66.76 <C> – <C> 67.35 <C> – <R> <C> NO-HIGHWAY <C> 69.72 <C> −0.18 <C> 66.42 <C> −0.34 <C> 66.64 <C> −0.71 <R> <C> NO-OVERLAPS <C> 69.46 <C> −0.44 <C> 65.07 <C> −1.69 <C> 66.47 <C> −0.88 <R> <C> LSTM-400-DROPOUT <C> 69.45 <C> −0.45 <C> 65.53 <C> −1.23 <C> 67.28 <C> −0.07 <R> <C> NO-TRANSLATIONS <C> 69.42 <C> −0.48 <C> 65.92 <C> −0.84 <C> 67.23 <C> −0.12 <R> <C> NO-ELMO-FINETUNING <C> 67.71 <C> −2.19 <C> 65.16 <C> −1.60 <C> 65.42 <C> −1.93 <CAP> Table 3: The estimation of impact of various design choices on the final result. The entries are sorted by the out-of-fold scores from CV. The SUBMISSION here uses score from ep_1 run for the single model and ep_2 for the ensemble performance.
<R> <C> Run ID <C> Official score <C> Score with correction <R> <C> ep_1 <C> 60.29 <C> 66.76 <R> <C> ep_2 <C> [BOLD] 60.90 <C> [BOLD] 67.35 <R> <C> ep_3 <C> 60.61 <C> 67.07 <CAP> Table 1: The scores of our three submitted runs for similarity threshold 50%.
<R> <C> Mention class <C> No. examples <C> F1 (5-CV) <C> F1 (Test) <R> <C> Total <C> 15265 <C> 69.90 <C> 67.35 <R> <C> Endpoint <C> 4411 <C> 66.89 <C> 61.47 <R> <C> TestArticle <C> 1922 <C> 63.29 <C> 64.19 <R> <C> Species <C> 1624 <C> 95.33 <C> 95.95 <R> <C> GroupName <C> 963 <C> 67.08 <C> 62.40 <R> <C> EndpointUnitOfMeasure <C> 706 <C> 42.27 <C> 40.41 <R> <C> TimeEndpointAssessed <C> 672 <C> 57.27 <C> 55.51 <R> <C> Dose <C> 659 <C> 78.47 <C> 75.85 <R> <C> Sex <C> 612 <C> 96.27 <C> 98.36 <R> <C> TimeUnits <C> 608 <C> 68.03 <C> 61.26 <R> <C> DoseRoute <C> 572 <C> 69.24 <C> 69.80 <R> <C> DoseUnits <C> 493 <C> 77.50 <C> 72.33 <R> <C> Vehicle <C> 440 <C> 63.03 <C> 67.15 <R> <C> GroupSize <C> 387 <C> 77.79 <C> 75.74 <R> <C> Strain <C> 375 <C> 78.56 <C> 76.00 <R> <C> DoseDuration <C> 216 <C> 59.78 <C> 56.80 <R> <C> DoseDurationUnits <C> 204 <C> 57.83 <C> 56.60 <R> <C> TimeAtDose <C> 117 <C> 34.29 <C> 35.68 <R> <C> DoseFrequency <C> 96 <C> 41.56 <C> 59.78 <R> <C> TimeAtFirstDose <C> 47 <C> 3.92 <C> 0.00 <R> <C> SampleSize <C> 45 <C> 43.84 <C> 50.00 <R> <C> CellLine <C> 39 <C> 50.00 <C> 50.77 <R> <C> TestArticlePurity <C> 28 <C> 34.04 <C> 60.00 <R> <C> TimeAtLastDose <C> 23 <C> 0.00 <C> 0.00 <R> <C> TestArticleVerification <C> 6 <C> 0.00 <C> 0.00 <CAP> Table 2: Detailed results of our best run (after correcting the submission format), along with numbers of mentions in the training set.
<R> <C> [EMPTY] <C> DUC’01 <italic>R</italic>1 <C> DUC’01 <italic>R</italic>2 <C> DUC’02 <italic>R</italic>1 <C> DUC’02 <italic>R</italic>2 <C> DUC’04 <italic>R</italic>1 <C> DUC’04 <italic>R</italic>2 <R> <C> ICSI <C> 33.31 <C> 7.33 <C> 35.04 <C> 8.51 <C> 37.31 <C> 9.36 <R> <C> PriorSum <C> 35.98 <C> 7.89 <C> 36.63 <C> 8.97 <C> 38.91 <C> 10.07 <R> <C> TCSum <C> <bold>36.45</bold> <C> 7.66 <C> 36.90 <C> 8.61 <C> 38.27 <C> 9.66 <R> <C> TCSum− <C> 33.45 <C> 6.07 <C> 34.02 <C> 7.39 <C> 35.66 <C> 8.66 <R> <C> SRSum <C> 36.04 <C> 8.44 <C> <bold>38.93</bold> <C> <bold>10.29</bold> <C> 39.29 <C> 10.70 <R> <C> DeepTD <C> 28.74 <C> 5.95 <C> 31.63 <C> 7.09 <C> 33.57 <C> 7.96 <R> <C> REAPER <C> 32.43 <C> 6.84 <C> 35.03 <C> 8.11 <C> 37.22 <C> 8.64 <R> <C> RELIS <C> 34.73 <C> <bold>8.66</bold> <C> 37.11 <C> 9.12 <C> <bold>39.34</bold> <C> <bold>10.73</bold> <CAP> Table 3: Results of non-RL (top), cross-input (DeepTD) and input-specific (REAPER) RL approaches (middle) compared with RELIS.
<R> <C> [EMPTY] <C> DUC’01 <italic>ρ</italic> <C> DUC’01 ndcg <C> DUC’02 <italic>ρ</italic> <C> DUC’02 ndcg <C> DUC’04 <italic>ρ</italic> <C> DUC’04 ndcg <R> <C> ASRL <C> .176 <C> .555 <C> .131 <C> .537 <C> .145 <C> .558 <R> <C> REAPER <C> .316 <C> .638 <C> .301 <C> .639 <C> .372 <C> .701 <R> <C> JS <C> .549 <C> .736 <C> .525 <C> .700 <C> .570 <C> .763 <R> <C> Our ^<italic>σUx</italic> <C> <bold>.601</bold> <C> <bold>.764</bold> <C> <bold>.560</bold> <C> <bold>.727</bold> <C> <bold>.617</bold> <C> <bold>.802</bold> <CAP> Table 2: The correlation of approximated and ground-truth ranking. ^σUx has significantly higher correlation over all other approaches.
<R> <C> [EMPTY] <C> Micro F1 <R> <C> Baseline <C> 0.709 <R> <C> W2V (<italic>d</italic>=50) <C> 0.748 <R> <C> W2V (<italic>d</italic>=500) <C> 0.756 <R> <C> S2V <C> 0.748 <R> <C> S2V + W2V (<italic>d</italic>=50) <C> 0.755 <R> <C> S2V + K + W2V(<italic>d</italic>=50) <C> 0.751 <R> <C> SIF (DE) <C> 0.748 <R> <C> SIF (DE-EN) <C> <bold>0.757</bold> <CAP> Table 6: Task B results with polarity features
<R> <C> [EMPTY] <C> Micro F1 <R> <C> Baseline <C> 0.882 <R> <C> W2V (<italic>d</italic>=50) <C> 0.883 <R> <C> W2V (<italic>d</italic>=500) <C> <bold>0.897</bold> <R> <C> S2V <C> 0.885 <R> <C> S2V + W2V (<italic>d</italic>=50) <C> 0.891 <R> <C> S2V + K + W2V(<italic>d</italic>=50) <C> 0.890 <R> <C> SIF (DE) <C> 0.895 <R> <C> SIF (DE-EN) <C> 0.892 <CAP> Table 4: Task A results
<R> <C> [EMPTY] <C> Micro F1 <R> <C> Baseline <C> 0.709 <R> <C> W2V (<italic>d</italic>=50) <C> 0.736 <R> <C> W2V (<italic>d</italic>=500) <C> 0.753 <R> <C> S2V <C> 0.748 <R> <C> S2V + W2V (<italic>d</italic>=50) <C> 0.744 <R> <C> S2V + K + W2V(<italic>d</italic>=50) <C> 0.749 <R> <C> SIF (DE) <C> 0.759 <R> <C> SIF (DE-EN) <C> <bold>0.765</bold> <CAP> Table 5: Task B results
<R> <C> [BOLD] Variation <C> [BOLD] Accuracy (%) <C> [BOLD] Δ% <R> <C> Submitted <C> [BOLD] 69.23 <C> - <R> <C> No emoji <C> 68.36 <C> - 0.87 <R> <C> No ELMo <C> 65.52 <C> - 3.71 <R> <C> Concat Pooling <C> 68.47 <C> - 0.76 <R> <C> LSTM hidden=4096 <C> 69.10 <C> - 0.13 <R> <C> LSTM hidden=1024 <C> 68.93 <C> - 0.30 <R> <C> LSTM hidden=512 <C> 68.43 <C> - 0.80 <R> <C> POS emb dim=100 <C> 68.99 <C> - 0.24 <R> <C> POS emb dim=75 <C> 68.61 <C> - 0.62 <R> <C> POS emb dim=50 <C> 69.33 <C> + 0.10 <R> <C> POS emb dim=25 <C> 69.21 <C> - 0.02 <R> <C> SGD optim lr=1 <C> 64.33 <C> - 4.90 <R> <C> SGD optim lr=0.1 <C> 66.11 <C> - 3.12 <R> <C> SGD optim lr=0.01 <C> 60.72 <C> - 8.51 <R> <C> SGD optim lr=0.001 <C> 30.49 <C> - 38.74 <CAP> Table 2: Ablation study results.
<R> <C> [EMPTY] <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1-score <R> <C> anger <C> 0.643 <C> 0.601 <C> 0.621 <R> <C> disgust <C> 0.703 <C> 0.661 <C> 0.682 <R> <C> fear <C> 0.742 <C> 0.721 <C> 0.732 <R> <C> joy <C> 0.762 <C> 0.805 <C> 0.783 <R> <C> sad <C> 0.685 <C> 0.661 <C> 0.673 <R> <C> surprise <C> 0.627 <C> 0.705 <C> 0.663 <R> <C> Average <C> 0.695 <C> 0.695 <C> 0.694 <CAP> Table 3: Classification Report (Test Set).
<R> <C> [EMPTY] <C> [BOLD] Present <C> [BOLD] Not Present <R> <C> Emoji <C> 4805 (76.6%) <C> 23952 (68.0%) <R> <C> Hashtags <C> 2122 (70.5%) <C> 26635 (69.4%) <CAP> Table 4: Number of tweets on the test set with and without emoji and hashtags. The number between parentheses is the proportion of tweets classified correctly.
<R> <C> [BOLD] Emoji alias <C> [BOLD] N <C> [BOLD] emoji # <C> [BOLD] emoji % <C> [BOLD] no-emoji # <C> [BOLD] no-emoji % <C> [BOLD] Δ% <R> <C> mask <C> 163 <C> 154 <C> 94.48 <C> 134 <C> 82.21 <C> - 12.27 <R> <C> two_hearts <C> 87 <C> 81 <C> 93.10 <C> 77 <C> 88.51 <C> - 4.59 <R> <C> heart_eyes <C> 122 <C> 109 <C> 89.34 <C> 103 <C> 84.43 <C> - 4.91 <R> <C> heart <C> 267 <C> 237 <C> 88.76 <C> 235 <C> 88.01 <C> - 0.75 <R> <C> rage <C> 92 <C> 78 <C> 84.78 <C> 66 <C> 71.74 <C> - 13.04 <R> <C> cry <C> 116 <C> 97 <C> 83.62 <C> 83 <C> 71.55 <C> - 12.07 <R> <C> sob <C> 490 <C> 363 <C> 74.08 <C> 345 <C> 70.41 <C> - 3.67 <R> <C> unamused <C> 167 <C> 121 <C> 72.46 <C> 116 <C> 69.46 <C> - 3.00 <R> <C> weary <C> 204 <C> 140 <C> 68.63 <C> 139 <C> 68.14 <C> - 0.49 <R> <C> joy <C> 978 <C> 649 <C> 66.36 <C> 629 <C> 64.31 <C> - 2.05 <R> <C> sweat_smile <C> 111 <C> 73 <C> 65.77 <C> 75 <C> 67.57 <C> 1.80 <R> <C> confused <C> 77 <C> 46 <C> 59.74 <C> 48 <C> 62.34 <C> 2.60 <CAP> Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.
<R> <C> Dataset <C> Metric <C> Illinois <C> IlliCons <C> rahman2012resolving <C> KnowFeat <C> KnowCons <C> KnowComb <R> <C> [ITALIC] Winograd <C> Precision <C> 51.48 <C> 53.26 <C> 73.05 <C> 71.81 <C> 74.93 <C> [BOLD] 76.41 <R> <C> [ITALIC] WinoCoref <C> AntePre <C> 68.37 <C> 74.32 <C> —– <C> 88.48 <C> 88.95 <C> [BOLD] 89.32 <CAP> Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.
<R> <C> System <C> MUC <C> BCUB <C> CEAFe <C> AVG <R> <C> ACE <C> ACE <C> ACE <C> ACE <C> ACE <R> <C> IlliCons <C> [BOLD] 78.17 <C> 81.64 <C> [BOLD] 78.45 <C> [BOLD] 79.42 <R> <C> KnowComb <C> 77.51 <C> [BOLD] 81.97 <C> 77.44 <C> 78.97 <R> <C> OntoNotes <C> OntoNotes <C> OntoNotes <C> OntoNotes <C> OntoNotes <R> <C> IlliCons <C> 84.10 <C> [BOLD] 78.30 <C> [BOLD] 68.74 <C> [BOLD] 77.05 <R> <C> KnowComb <C> [BOLD] 84.33 <C> 78.02 <C> 67.95 <C> 76.76 <CAP> Table 8: Performance results on ACE and OntoNotes datasets. Our system gets the same level of performance compared to a state-of-art general coreference system.
<R> <C> Category <C> Cat1 <C> Cat2 <C> Cat3 <R> <C> Size <C> 317 <C> 1060 <C> 509 <R> <C> Portion <C> 16.8% <C> 56.2% <C> 27.0% <CAP> Table 9: Distribution of instances in Winograd dataset of each category. Cat1/Cat2 is the subset of instances that require Type 1/Type 2 schema knowledge, respectively. All other instances are put into Cat3. Cat1 and Cat2 instances can be covered by our proposed Predicate Schemas.
<R> <C> Schema <C> AntePre(Test) <C> AntePre(Train) <R> <C> Type 1 <C> 76.67 <C> 86.79 <R> <C> Type 2 <C> 79.55 <C> 88.86 <R> <C> Type 1 (Cat1) <C> 90.26 <C> 93.64 <R> <C> Type 2 (Cat2) <C> 83.38 <C> 92.49 <CAP> Table 10: Ablation Study of Knowledge Schemas on WinoCoref. The first line specifies the preformance for KnowComb with only Type 1 schema knowledge tested on all data while the third line specifies the preformance using the same model but tested on Cat1 data. The second line specifies the preformance results for KnowComb system with only Type 2 schema knowledge on all data while the fourth line specifies the preformance using the same model but tested on Cat2 data.
<R> <C> [EMPTY] <C> [BOLD] BB source acc. <C> [BOLD] BB target acc. <C> [BOLD] Non-reject. acc. (10/20/30%) <C> [BOLD] Class. quality (10/20/30%) <C> [BOLD] Reject. quality (10/20/30%) <R> <C> [BOLD] Apply Yelp BB to SST-2 <C> 89.18±0.08% <C> 77.13±0.52% <C> 82.43±0.22% 88.19±0.50% 93.60±0.16% <C> 80.40±0.39% 83.11±0.80% 83.05±0.23% <C> 6.03±0.45 6.04±0.51 4.97±0.07 <R> <C> [BOLD] Apply SST-2 BB to Yelp <C> 83.306±0.18% <C> 82.106±0.88% <C> 87,98±0.18% 92.13±0.38% 94.19±0.33% <C> 85.49±0.88% 84.53±0.38% 78.99±0.46% <C> 8.30±1.63 5.72±0.27 3.73±0.10 <R> <C> [BOLD] Apply Electronics BB to Music <C> 86.39±0.22% <C> 90.38±0.13% <C> 95.04±0.43% 96.45±0.35% 97.26±0.31% <C> 90.67±0.88% 83.93±0.67% 75.77±0.54% <C> 10.7±1.65 4.82±0.35 3.25±0.14 <R> <C> [BOLD] Apply Music BB to Electronics <C> 93.10±0.02% <C> 79.85±0.0% <C> 83.26±0.41% 87.06±0.55% 90.50±0.29% <C> 79.97±0.74% 79.93±0.87% 76.81±0.41% <C> 4.1±0.55 3.80±0.35 3.32±0.09 <CAP> Table 1: Accuracy obtained by training an standalone classifier, applying the API and the proposed wrapper for each domain
