Table 2 compares the throughput of performing inference and training on the TreeLSTM model using our implementation, the iterative approach, and the folding technique. The amount of resources is sufficient for executing forward computations, and therefore our framework outperforms the folding technique for the inference task with up to 4.93x faster throughput.  As a result, the folding technique performs better than the recursive approach for the training task.
Table 1 shows the throughput of training the TreeRNN model using these three datasets. For all batch sizes, the training throughput on the balanced dataset is the highest, while the throughput on the linear dataset is the lowest.  As a result, our implementation can train input data of balanced trees with greater throughput than input data of unbalanced trees.  Another interesting fact in Table 1 is that the training throughput on the linear dataset scales better than the throughput on the balanced dataset, as the batch size increases.  On the contrary, for the linear dataset, the recursive implementation fails to efficiently make use of CPU resources and thus the performance gain provided by increasing the batch size is relatively high.
Table 2 presents the optimal values for each configuration using different dependency representations. We see that the optimized parameter settings vary for the different representations, showing the importance of tuning for these types of comparisons. The results furthermore show that the sdps based on the Stanford Basic (SB) representation provide the best performance, followed by the CoNLL08 representation. We observe that the results for the UD representation are quite a bit lower than the two others.
We find that the effect of syntactic structure varies between the different relation types. However, the sdp information has a clear positive impact on all the relation types (Table 1).
Accordingly, as Table 3 shows for the essay level (paragraph level omitted for space reasons), results are generally weaker:  as in Eq. (1)—the overall result is worst. We find that when we train STagBL with only its main task—with label set  In Y contrast, when we include the 'natural subtasks' "C" (label  performance increases typically by a few percentage points.
We train and test all parsers on the paragraph level,  Mate is slightly better  Kiperwasser performs decently on the approximate match level, but not on exact level.  The best parser by far is the LSTM-Parser. It is over 100% better than Kiperwasser on exact spans and still several percentage points on approximate spans.  : Performance of dependency parsers, STagBLCC, LSTM-ER and ILP (from top to bottom).  On the other hand, our results in Table 2 indicate that the neural taggers BLCC and BLC (in the LSTMER model) are much better at such exact identification than either the ILP model or the neural parsers.
These results detail that the taggers have lower standard deviations than the parsers. The difference is particularly striking on the essay level where the parsers often completely fail to learn, that is, their performance scores are close to 0%.
The results for testing on cleaned data (Table 3, top half) confirm the positive impact of cleaned training data and also show that the cleaned test data is more challenging (cf. Section 3), as reflected in the lower WOMs.  However, the improved results for training and testing on clean data (i.e. seeing equally challenging examples at training and test time), suggest the increase in performance can be attributed to data accuracy rather than diversity.  Looking at the detailed results for the number of added, missing, and wrong-valued slots (Add, Miss, Wrong), we observe more deletions than insertions, i.e. the models more often fail to realise part of the MR, rather than hallucinating additional information.  However, the results in bottom halves  of Tables 2 and 3 do not support our hypothesis: we observe the main effect on SER from cleaning the missed slots, reducing both insertions and deletions. Again, one possible explanation is that cleaning the missing slots provided more complex training examples.
This resulted in 20% reduction for TRAIN and ca. 8% reduction for DEV in terms of references (see Table 1). On the other hand, the number of distinct MRs rose sharply after reannotation; the MRs also have more variance in the number of attributes. This means that the cleaned dataset is more complex overall, with fewer references per MR and more diverse MRs.
The results in Table 2 (top half) for the original setup confirm that the ranking mechanism for TGen is effective for both WOMs and SER, whereas the SC-LSTM seems to have trouble scaling to the E2E dataset. We hypothesise that this is mainly due to the amount of delexicalisation required. However, the main improvement of SER comes from training on cleaned data with up to 97% error reduction with the ranker and 94% without.11 just cleaning the training data has a much more dramatic effect than just using a semantic control mechanism, such as the reranker (0.97% vs. 4.27% SER). WOMs are slightly lower for TGen trained on the cleaned data, except for NIST, which gives more importance to matching less frequent n-grams. This suggests better preservation of content at the expense of slightly lower fluency. In other words,  However, the results in bottom halves  of Tables 2 and 3 do not support our hypothesis: we observe the main effect on SER from cleaning the missed slots, reducing both insertions and deletions. Again, one possible explanation is that cleaning the missing slots provided more complex training examples.
The results in Table 4 confirm the findings of the automatic  metrics: systems trained on the fully cleaned set or the set with cleaned missing slots have nearperfect performance, with the fully-cleaned one showing a few more slight disfluencies than the other. The systems trained on the original data or with cleaned added slots clearly perform worse in terms of both semantic accuracy and fluency. All fluency problems we found were very slight and no added or wrong-valued slots were found, so missed slots are the main problem.
Moreover, we compare our DCGCN(single) and DCGCN(ensemble) model with the state-of-the-art semi-supervised models on the AMR15 test set (Table 3), including non-neural methods such as TSP (Song et al., 2016), PBMT (Pourdamghani et al., 2016), Tree2Str (Flanigan et al., 2016) and SNRG (Song et al., 2017). All these non-neural models train language models on the whole Gigaword corpus. Our ensemble model gives 28.2 BLEU points without external data, which is better than them. Following Konstas et al. (2017); Song et al. (2018), we also evaluate our model using external Gigaword sentences as training data. We first use the additional data to pretrain the model, then finetune it on the gold data. Using additional 0.1M data, the single DCGCN model achieves a BLEU score of 29.0, which is higher than Seq2SeqK (Konstas et al., 2017) and GraphLSTM (Song et al., 2018) trained with 0.2M additional data. When using the same amount of 0.2M data, the performance of DCGCN is 4.2 and 3.4 BLEU points higher than Seq2SeqK and GraphLSTM. DCGCN model is able to achieve a competitive BLEU points (33.2) by using 0.3M external data, while GraphLSTM achieves a score of 33.6 by using 2M data and Seq2SeqK achieves a score of 33.8 by using 20M data. These results show that our model is more effective in terms of using automatically generated AMR graphs. Using 0.3M additional data, our ensemble model achieves the new state-of-the-art result of 35.3 BLEU points.
Table 2 shows the results on AMR17. Our single model achieves 27.6 BLEU points, which is the new state-of-the-art result for single models. In particular, our single DCGCN model consistently outperforms Seq2Seq models by a significant margin when trained without external resources. For example, the single DCGCN model gains 5.9 more BLEU points than the single models of Seq2SeqB on AMR17. These results demonstrate the importance of explicitly capturing the graph structure in the encoder. In addition, our single DCGCN model obtains better results than previous ensemble models. For example, on AMR17, the single DCGCN model is 1 BLEU point higher than the ensemble model of Seq2SeqB. Our model requires substantially fewer parameters, e.g., the parameter size is only 3/5 and 1/9 of those in GGNN2Seq and Seq2SeqB, respectively. The ensemble approach based on combining five DCGCN models initialized with different random seeds achieves a BLEU score of 30.4 and a CHRF++ score of 59.6. Under the same setting, our model also consistently outperforms graph encoders based on recurrent neural networks or gating mechanisms. For GGNN2Seq, our single model is 3.3 and 0.1 BLEU points higher than their single and ensemble models, respectively. We also have similar observations in term of CHRF++ scores for sentence-level evaluations. DCGCN also outperforms GraphLSTM by 2.0 BLEU points in the fully supervised setting as shown in Table 3. Note that GraphLSTM uses char-level neural represen  tations and pretrained word embeddings, while our model solely relies on word-level representations with random initializations. This empirically shows that compared to recurrent graph encoders, DCGCNs can learn better representations for graphs. For GCNSEQ, our single models are 3.1 and 1.3 BLEU points higher than their models trained on AMR17 and AMR15 dataset, respectively. These results demonstrate that DCGCNs are able to capture contextual information without relying on additional RNNs.
Table 4 shows the results for the EnglishGerman (En-De) and English-Czech (En-Cs) translation tasks. BoW+GCN, CNN+GCN and BiRNN+GCN refer to employing the following encoders with a GCN layer on top respectively: 1) a bag-of-words encoder, 2) a one-layer CNN, 3) a bidirectional RNN. PB-SMT is the phrase-based statistical machine translation model using Moses (Koehn et al., 2007). Our single model DCGCN(single) achieves 19.0 and 12.1 BLEU points on the En-De and EnCs tasks, respectively, significantly outperforming all the single models. For example, compared to the best GCN-based model (BiRNN+GCN), our single DCGCN model surpasses it by 2.7 and 2.5 BLEU points on the En-De and En-Cs tasks, respectively. Our models DCGCN(single) and DCGCN(ensemble)consist of full GCN layers, removing the burden of employing a recurrent encoder to extract non-local contextual information in the bottom layers. Compared to non-GCN models, our single DCGCN model is 2.2 and 1.9 BLEU points higher than the current state-of-theart single model (GGNN2Seq) on the En-De and En-Cs translation tasks, respectively. In addition, our single model is comparable to the ensemble results of Seq2SeqB and GGNN2Seq, while the number of parameters of our models is only about 1/6 of theirs. Additionally, the ensemble DCGCN models achieve 20.5 and 13.1 BLEU points on the En-De and En-Cs tasks, respectively. Our ensemble results are significantly higher than those of the state-of-the-art syntax-based ensemble models reported by GGNN2Seq (En-De: 20.5 v.s. 19.6; En-Cs: 13.1 v.s. 11.7 in terms of BLEU).
Layers in the sub-block. Table 5 shows the effect of the number of layers of each sub-block on the AMR15 development set. DenseNets (Huang et al., 2017) use two kinds of convolution filters: 1 × 1 and 3 × 3. Similar to DenseNets, we choose the values of n and m for layers from [1, 2, 3, 6]. We choose this value range by considering the scale of non-local nodes, the abstract information at different level and the calculation efficiency. For brevity, we only show representative configurations. We first investigate DCGCN with one block. In general, the performance increases when we gradually enlarge n and m. For example, when n=1 and m=1, the BLEU score is 17.6; when n=6 and m=6, the BLEU score becomes 22.0. We observe that the three settings (n=6, m=3), (n=3, m=6) and (n=6, m=6) give similar results for both 1 DCGCN block and 2 DCGCN blocks. Since the first two settings contain less parameters than the third setting, it is reasonable to choose either (n=6, m=3) or (n=3, m=6). For later experiments, we use (n=6, m=3).
The first block in Table 6 shows the performance of our two baseline models: multi-layer GCNs with residual connections (GCN+RC) and multi-layer GCNs with both residual connections and layer aggregations (GCN+RC+LA). In general, increasing the number of GCN layers from 2 to 9 boosts the model performance. However, when the layer number exceeds 10, the performance of both baseline models start to drop. For example, GCN+RC+LA (10) achieves a BLEU score of 21.2, which is worse than GCN+RC+LA (9). In preliminary experiments, we cannot manage to train very deep GCN+RC and GCN+RC+LA models. In contrast, our DCGCN models can be trained using a large number of layers. For example, DCGCN4 contains 36 layers. When we increase the DCGCN blocks from 1 to 4, the model performance continues increasing on AMR15 development set. We therefore choose DCGCN4 for the AMR experiments. Using a similar method, DCGCN2 is selected for the NMT tasks. When the layer numbers are 9, DCGCN1 is better than GCN+RC in term of B/C scores (21.7/51.5 v.s. 21.1/50.5). GCN+RC+LA (9) is sightly better than DCGCN1. However, when we set the number to 18, GCN+RC+LA achieves a BLEU score of 19.4, which is significantly worse than the BLEU score obtained by DCGCN2 (23.3). We also try GCN+RC+LA (27), but it does not converge. In conclusion, these results above can show the robustness and effectiveness of our DCGCN models.
We compare DCGCN models with different layers under the same parameter budget. Table 7 shows the results. For example, when both DCGCN1 and DCGCN2 are limited to 10.9M parameters, DCGCN2 obtains 22.2 BLEU points, which is higher than DCGCN1 (20.9). Similarly, when DCGCN3 and DCGCN4 contain 18.6M and 18.4M parameters. DCGCN4 outperforms DCGCN3 by 1 BLEU point with a slightly smaller model. In general, we found when the parameter budget is the same, deeper DCGCN models can obtain better results than the shallower ones.
Table 8 shows the ablation study of the level of density of our model DCGCN4. We use DCGCNs with 4 dense blocks as the full model. Then we remove dense connections gradually from the last block to the first block. In general, the performance of the model drops substantially as we remove more dense connections until it cannot converge without dense connections. The full model gives 25.5 BLEU points on the AMR15 dev set. After removing the dense connections in the last block, the BLEU score becomes 24.8. Without using the dense connections in the last two blocks, the score drops to 23.8. Furthermore, excluding the dense connections in the last three blocks only gives 23.2 BLEU points. Although these four models have the same number of layers, dense connections allow the model to achieve much better performance. If all the dense connections are not considered, the model does not coverage at all. These results indicate dense connections do play a significant role in our model.
Table 9 shows the results. For the encoder, we find that the linear combination and the global node have more contributions in terms of B/C scores. The results drop by 2/2.2 and 1.3/1.2 points respectively after removing them. Without these two components, our model gives a BLEU score of 22.6, which is still better than the best GCN+RC model (21.1) and the best GCN+RC+LA model (22.1). Adding either the global node or the linear combination improves the baseline models with only dense connections. This suggests that enriching input graphs with the global node and including the linear combination can facilitate GCNs to learn better information aggregations, producing more expressive graph representations. Results also show the linear combination is more effective than the global node. Considering them together further enhances the model performance. After removing the graph attention module, our model gives 24.9 BLEU points. Similarly, excluding the direction aggregation module leads to a performance drop to 24.6 BLEU points. The coverage mechanism is also effective in our models. Without the coverage mechanism, the result drops by 1.7/2.4 points for B/C scores.
To verify the effectiveness of our initialization strategy empirically, we evaluate it with the same experimental setup as described in Section 4. The only difference is the initialization strategy, where we include Glorot initialization (Glorot & Bengio, 2010) and the standard initialization from  (0, 0.1). Table 7 shows the results on the probing tasks. While Glorot achieves slightly better results on BShift and TopConst, CMOW's ability to memorize word content is improved by a wide  margin by our initialization strategy.
We have trained five different models: CBOW and CMOW with d = 20 and d = 28, which lead to 400-dimensional and 784-dimensional word embeddings, respectively. We also trained the Hybrid CBOW-CMOW model with d = 20 for each component, so that the total model has 800 parameters per word in the lookup tables. We report the results of two more models: H-CBOW is the 400-dimensional CBOW component trained in Hybrid and H-CMOW is the respective CMOW component. Below, we compare the 800-dimensional Hybrid method to the 784-dimensional CBOW and CMOW models.  Considering the linguistic probing tasks (see Table 1), CBOW and CMOW show complementary results. While CBOW yields the highest performance at word content memorization, CMOW outperforms CBOW at all other tasks. Most improvements vary between 1-3 percentage points. The difference is approximately 8 points for CoordInv and Length, and even 21 points for BShift. The hybrid model yields scores close to or even above the better model of the two on all tasks. In terms of relative numbers, the hybrid model improves upon CBOW in all probing tasks but WC and SOMO. The relative improvement averaged over all tasks is 8%. Compared to CMOW, the hybrid model shows rather small differences. The largest loss is by 4% on the CoordInv task. However, due to the large gain in WC (20.9%), the overall average gain is still 1.6%. We now compare the jointly trained H-CMOW and H-CBOW with their separately trained 400dimensional counterparts. We observe that CMOW loses most of its ability to memorize word content, while CBOW shows a slight gain. On the other side, H-CMOW shows, among others, improvements at BShift.  Regarding the probing tasks, we observe that CMOW embeddings better encode the linguistic prop  erties of sentences than CBOW.  Due to joint training, our hybrid model learns to pick up the best features from CBOW and CMOW simultaneously. It enables both models to focus on their respective strengths. This can best be seen by observing that H-CMOW almost completely loses its ability to memorize word content. In return, H-CMOW has more capacity to learn other properties, as seen in the increase in performance at BShift and others. A complementary behavior can be observed for H-CBOW, whose scores on Word Content are increased. Consequently, with an 8% improvement on average, the hybrid model  Word Content are increased. Consequently, with an 8% i is substantially more linguistically informed than CBOW.
Table 2 shows the scores from the supervised downstream tasks. Comparing the 784-dimensional models, again, CBOW and CMOW seem to complement each other. This time, however, CBOW has the upperhand, matching or outperforming CMOW on all supervised downstream tasks except  TREC by up to 4 points. On the TREC task, on the other hand, CMOW outperforms CBOW by 2.5 points. Our jointly trained model is not more than 0.8 points below the better one of CBOW and CMOW on any of the considered supervised downstream tasks. On 7 out of 11 supervised tasks, the joint model even improves upon the better model, and on SST2, SST5, and MRPC the difference is more than 1 point. The average relative improvement over all tasks is 1.2%.  Our CMOW model produces sentence embeddings that are approximately at the level of fast  Sent (Hill et al., 2016). Thus, CMOW is a reasonable choice as a sentence encoder.  observe that CMOW embeddings better encode the linguistic propCMOW gets reasonably close to CBOW on some downstream tasks.  However, CMOW does not in general supersede CBOW embeddings.
Regarding the unsupervised downstream tasks (Table 3), CBOW is clearly superior to CMOW on all datasets by wide margins. For example, on STS13, CBOW's score is 50% higher. The hybrid model is able to repair this deficit, reducing the difference to 8%. It even outperforms CBOW on two of the tasks, and yields a slight improvement of 0.5% on average over all unsupervised downstream tasks. However, the variance in relative performance is notably larger than on the supervised downstream tasks.
the success of our training schema for the CMOW model are two changes to the original word2vec training. First, our initialization strategy improved the downstream performance by 2.8% compared to Glorot initialization.
alization strategy improved the downstream performance by 2.8% compared Secondly, by choosing the target word of the objective at random, the perfor  mance of CMOW on downstream tasks improved by 20.8% on average.  and on all unsupervised downstream tasks
To test the effectiveness of this modified objective, we evaluate it with the same experimental setup as described in Section 4. Table 4 lists the results on the linguistic probing tasks. CMOW-C and CBOW-C refer to the models where the center word is used as the target. CMOW-R and CBOW-R refer to the models where the target word is sampled randomly. While CMOW-R and CMOW-C perform comparably on most probing tasks, CMOW-C yields 5 points lower scores on WordContent  and BigramShift.
Consequently, CMOW-R also outperforms CMOW-C on 10 out of 11 supervised  downstream tasks  On average over all downstream tasks, the relative improvement is 20.8%.  scores on downstream tasks increase on some tasks and decrease on others. The differences ar miniscule. On average over all 16 downstream tasks, CBOW-R scores 0.1% lower than CBOW-C.  For CBOW, the
In Table 3 we classified errors according to named entity types  PER is the easiest type for all systems. Even name matching, without any learning, can correctly predict in half of the cases.  For LOC, it turns out that candidate selection is a bottleneck: when candidate selection was flawless, the models made only about 12% errors, down from about 57%. For MISC a similar conclusion can be drawn.
Table 2 shows results on the test set. 'Name matching' is far behind the two models.  MIL-ND achieves higher precision, recall, and F1 than MIL,  Using its confidence at test time (τ MIL-ND, 'All' setting) was also beneficial in terms of precision and F1 (it cannot possibly increase recall). Because all the test data points are valid for the 'In E+' setting, using the ND classifier had a slight negative effect on F1.  MIL-ND significantly outperforms MIL: the 95% confidence intervals for them do not overlap. However, this is not the case for MIL-ND and τ MIL-ND.
We perform an entailment experiment using BERT (Devlin et al., 2019) fine-tuned on the MultiNLI dataset (Williams et al., 2018) as a NLI model. We are interested in exploring whether a generated sentence (hypothesis) is semantically entailed by the reference sentence (premise). In a related text generation task, Falke et al. (2019) employ NLI models to rerank alternative predicted abstractive summaries.  Table 6 shows the average probabilities for entailment, contradiction and neutral classes on the LDC2017T10 test set. All G2S models have  higher entailment compared to S2S. G2S-GGNN has 33.5% and 5.2% better entailment performances than S2S, when REF entails GEN and GEN entails REF, respectively. G2S models also generate sentences that contradict the reference sentences less. This suggests that our models are capable of capturing better semantic information from the graph generating outputs semantically related to the reference sentences.
Table 2 shows the comparison between the proposed models, the baseline and other neural models on the test set of the two datasets.  For both datasets, our approach substantially outperforms the baselines. In LDC2015E86, G2S-GGNN achieves a BLEU score of 24.32, 4.46% higher than Song et al. (2018), who also use the copy mechanism. This indicates that our architecture can learn to generate better signals for text generation. On the same dataset, we have competitive results to Damonte and Cohen (2019). However, we do not rely on preprocessing anonymisation not to lose semantic signals. In LDC2017T10, G2S-GGNN achieves a BLEU score of 27.87, which is 3.33 points higher than Damonte and Cohen (2019), a state-of-the-art model that does not employ external information. We also have competitive results to Guo et al. (2019), a very recent state-of-the-art model.  We also outperform Cao and Clark (2019) improving BLEU scores by 3.48% and 4.00%, in LDC2015E86 and LDC2017T10, respectively. In contrast to their work, we do not rely on (i) leveraging supplementary syntactic information and (ii) we do not require an anonymization preprocessing step. G2S-GIN and G2S-GAT have comparable performance on both datasets. Interestingly, G2S-GGNN has better performance among our models. This suggests that graph encoders based on gating mechanisms are very effective in text generation models. We hypothesize that the gating mechanism can better capture longdistance dependencies between nodes far apart in the graph.
We follow the method of Konstas et al. (2017), which is fine-tuning the model on the LDC2015E86 training set after every epoch of pretraining on the Gigaword data. G2S-GGNN outperforms others with the same amount of Gigaword sentences (200K), achieving a 32.23 BLEU score, as shown in Table 3. The results demonstrate that pretraining on automatically generated AMR graphs enhances the performance of our model.
In Table 4, we report the results of an ablation study on the impact of each component of our model on the development set of LDC2017T10 dataset by removing the graph encoders. We also report the number of parameters (including embeddings) used in each model. The first thing we notice is the huge increase in metric scores (17% in BLEU) when applying the graph encoder layer, as the neural model receives signals regarding the graph structure of the input. The dual representation helps the model with a different view of the graph, increasing BLEU and METEOR scores by 1.04 and 0.68 points, respectively. The complete model has slightly more parameters than the model without graph encoders (57.6M vs 61.7M).
Table 5 shows METEOR5 scores for the LDC2017T10 dataset.  The performances of all models decrease as the diameters of the graphs increase. G2S-GGNN has a 17.9% higher METEOR score in graphs with a diameter of at most 7 compared to graphs with diameters higher than 13. This is expected as encoding a bigger graph (containing more information) is harder than encoding smaller graphs.  Since the models have fewer examples of bigger graphs to learn from, this also leads to worse performance when handling graphs with higher diameters. We also investigate the performance with respect to the sentence length. The models have better results when handling sentences with 20 or fewer tokens. Longer sentences pose additional challenges to the models.  G2S-GIN has a better performance in handling graphs with node out-degrees higher than 9. This indicates that GINs can be employed in tasks where the distribution of node degrees has a long tail. Surprisingly, S2S has a better performance than G2S-GGNN and G2S-GAT when handling graphs that contain high degree nodes.
We also evaluate the semantic adequacy of our model (how well does the generated output match the input?) by comparing the number of added and missing tokens that occur in the generated versus reference sentences (GOLD). As shown in Table 8, G2S approaches outperform the S2S baseline. G2S-GIN is closest to GOLD with respect to both metrics suggesting that this model is better able to generate novel words to construct the sentence and captures a larger range of concepts from the input AMR graph, covering
we trained systems using a smaller data size (200K sentences),  The results are shown in Table 4.  we observe a variance in classifier accuracy of 1-2%, based on target language,  This is true for both POS and SEM tagging.
we consider two baselines: most frequent tag (MFT) for each word according to the training set (with the global majority tag for unseen words); and unsupervised word embeddings (UnsupEmb) as features for the classifier,  We also report an upper bound of directly training an encoderdecoder on word-tag sequences (Word2Tag),  Table 2 shows baseline and upper bound results. The UnsupEmb baseline performs rather poorly on both POS and SEM tagging.  NMT word embeddings (Table 3, rows with k = 0) perform slightly better,  the results are still below the most frequent tag baseline (MFT),  This is above the UnsupEmb baseline but only on par with the MFT baseline (Table 2).  The results are also far below the Word2Tag upper bound (Table 2).  which is far above the UnsupEmb and MFT baselines. While these results are below the oracle Word2Tag results (Table 2),
NMT word embeddings (Table 3, rows with k = 0) perform slightly better,  Table 3 summarizes the results of training classifiers to predict POS and SEM tags using features extracted from different encoding layers of 4  layered NMT systems.3 In the POS tagging results (first block), as the representations move above layer 0, performance jumps to around 91–92%.  This is above the UnsupEmb baseline but only on par with the MFT baseline (Table 2).  The results are also far below the Word2Tag upper bound (Table 2).  Comparing layers 1 through 4, we see that in 3/5 target languages (Ar, Ru, Zh), POS tagging accuracy peaks at layer 1 and does not improve at higher layers, with some drops at layers 2 and 3. In 2/5 cases (Es, Fr) the performance is higher at layer 4.  Turning to SEM tagging (Table 3, second block), representations from layers 1 through 4 boost the performance to around 87-88%,  which is far above the UnsupEmb and MFT baselines. While these results are below the oracle Word2Tag results (Table 2),  Going beyond the 1st encoding layer, representations from the 2nd and 3rd layers do not consistently improve semantic tagging performance. However, representations from the 4th layer lead to significant improvement with all target languages except for Chinese.  we found that En-En encoder-decoders (that is, English autoencoders) produce poor representations for POS and SEM tagging (last column in Table 3). This is especially true with higher layer representations (e.g. around 5% below the MT models using representations from layer 4). In contrast, the autoencoder has excellent sentence recreation capabilities (96.6 BLEU).  Table 3 also shows results using features obtained by training NMT systems on different target languages (the English source remains fixed). In both POS and SEM tagging, there are very small differences with different target languages (∼0.5%), except for Chinese which leads to slightly worse representations. While the differences are small,
improvements in both translation (+1-2 BLEU) and SEM tagging quality (+3-4% accuracy), across the board, when using a bidirectional encoder. Some of our bidirectional models obtain 92-93% accuracy. We observed similar improvements on POS tagging. Comparing POS and SEM tagging (Table 5), we note that higher layer representations improve SEM tagging, while POS tagging peaks at layer 1. we noticed small but consistent improvements in both translation (+0.9 BLEU) and POS and SEM tagging (up to +0.6% accuracy) when using features extracted from an NMT model trained with residual connections (Table 5). We also observe similar trends as before: POS tagging does not benefit from features from the upper layers, while SEM tagging improves with layer 4 representations.
Table 8 summarize the training accuracies of the attacker network. The Mention/Race task achieves the highest score of 64.3% whereas the Mention/Gender task achieves the lowest 58.1%.
The results in Table 1 indicate that the classifiers achieve reasonable accuracies for the main tasks.  the protected attributes, race is highly predictable (83.9%) while age and gender can also be recovered at above 64% accuracy.
This experiment suggests an upper bound on the amount of leakage of protected attributes when we do not actively attempt to prevent it. The Balanced section in Table 2 summarizes the validation-set accuracies. While the numbers are lower than when training directly (Table 1), they are still high enough to extract meaningful and possibly highly sensitive information (e.g. DIAL Race direct prediction is 83.9% while DIAL Race leakage on the balanced Sentiment task is 64.5%).  We simulate this more realistic scenario by constructing unbalanced datasets in which the main tasks (sentiment/mention) remain balanced but the protected class proportions within each main class are not, as demonstrated in Figure 1b.  We then follow the leakage experiment on the unbalanced datasets. The attacker is trained and tested on a balanced dataset. Otherwise, the attacker can perform quite well on the male/female task simply by learning to predict sentiment, which does not reflect leakage of gender data to the representation. When training the attacker on balanced data, its decisions cannot rely on the sentiment information encoded in the vectors, and must look for encoded information about the protected attributes. The results in Table 2 indicate that both task accuracy and attribute leakage are stronger in the unbalanced case.
However, training the attacker network on the resulting encoder vectors reveals a different story. For example, when considering the encoder after 50 training epochs (adversary accuracy of 49.0%), the attacker reaches 56.0% accuracy: substantially higher than the adversarial's success rate, despite sharing the exact same architecture, and being trained and tested on the exact same dataset. Table 3 summarizes the attacker's recovery rate on the adversarialy-trained encoders for the different settings. In all cases, the adversarial's success rate is around 50%, while the attacker's rate is substantially higher.
We compare encoders Leaky-EMB and Leaky-RNN to gauge which module has a greater contribution to the data leakage.  Table 6 summarize the results, implying that the leakage is caused mainly by the RNN, and less by the Embedding Matrix.
Table 5 shows the test perplexity of different models.10 In this task, LRN significantly outperforms GRU, ATR and SRU, and achieves near the same perplexity as LSTM.
Table 1 shows the test accuracy and training time of different models. Our implementation outperforms the original model where Rockt¨aschel et al. (2016) report an accuracy of 83.50. Overall results show that LRN achieves competitive performance but consumes the least training time. Although LSTM and GRU outperform LRN by 0.3∼0.9 in terms of accuracy, these recurrent units sacrifice running efficiency (about 7%∼48%) depending on whether LN and BERT are applied. No significant performance difference is observed between SRU and LRN, but LRN has fewer model parameters and shows a speedup over SRU of 8%∼21%.  However, for LSTM, GRU and ATR, LN results in significant computational overhead (about 27%∼71%). In contrast, quasi recurrent models like SRU and LRN only suffer a marginal speed decrease.  Results with BERT show that contextual information is valuable for performance improvement. LRN obtains additional 4 percentage points gain with BERT and reaches an accuracy of around 89.9.  In addition, although the introduction of BERT brings in heavy matrix computation, the benefits from LRN do not disappear. LRN is still the fastest model, outperforming other recurrent units by 8%∼27%.
Table 2 summarizes the classification results. LRN achieves comparable classification performance against ATR and SRU, but slightly  underperforms LSTM and GRU (-0.45∼-1.22).  LRN accelerates the training over LSTM and SRU by about 20%,
The results in Table 3 show that translation quality of LRN is slightly worse than that of GRU (-0.02 BLEU).  however, oLRN yields the best BLEU score of 26.73, outperforming GRU (+0.45 BLEU). In addition, the training time results in Table 3 confirm the computational advantage of LRN over all other recurrent units, where LRN speeds up over ATR and SRU by approximately 25%.  the recurrent unit with the least computation operations, i.e. ATR, becomes the fastest. Still, both LRN and oLRN translate sentences faster than SRU (+15%/+6%).
Table 4 lists the EM/F1 score of different models. In this task, LRN outperforms ATR and SRU in terms of both EM and F1 score. After integrating Elmo for contextual modeling, the performance of LRN reaches the best (76.1  EM and 83.83 F1), beating both GRU and LSTM (+0.33EM, +0.71F1).
As shown in Table 6, the performance of LRN matches that of ATR and SRU, though LSTM and GRU operate better (+1.05 and +0.79).
We experiment with SNLI and PTB tasks. Results in Table 7 show that although the accuracy on SNLI is acceptable, gLRN and eLRN perform significantly worse on the PTB task.
BLEU-2, BLEU-4, ROUGE-2 recall, and METEOR are reported in Table 3 for both setups.  Under system setup, our model CANDELA statistically significantly outperforms all comparisons and the retrieval model in all metrics, based on a randomization test (Noreen, 1989) (p <  .0005). Furthermore, our model generates longer sentences whose lengths are comparable with human arguments, both with about 22 words per sentence. This also results in longer arguments. Under oracle setup, all models are notably improved due to the higher quality of reranked passages, and our model achieves statistically significantly better BLEU scores. Interestingly, we observe a decrease of ROUGE and METEOR, but a marginal increase of BLEU-2 by removing passages from our model input. This could be because the passages introduce divergent content, albeit probably on-topic, that cannot be captured by BLEU.
Inter-annotator agreement scores (Krippendorff's α) of 0.44, 0.58, 0.49 are achieved for the three aspects, implying general consensus to intermediate agreement.  Our system obtains the highest appropriateness and content richness among all automatic systems. This confirms the previous observation that our model produces more informative argument than other neural models. SEQ2SEQAUG has a marginally better grammaticality score, likely due to the fact that our arguments are longer, and tend to contain less fluent generation towards the end.  Furthermore, we see that human arguments are
In this first experiment we reduce the vocabulary to the top 1,000 terms with the highest number of contexts. Table 3 presents the general overview of these values for each method in each corpus, where the highest values are presented in bold.  Analyzing Table 3, we can observe that all values of precision using the Portuguese corpora have higher scores when compared with the English corpora.  A higher number of terms associated in hypernyms tends to increase the precision. Another aspect to be considered is the fact that as Onto.PT is automatically constructed, there are relations that would not exist if it was manually constructed or revised.  As we can observe in Table 3, Patt has the best values of precision for the English corpora while DocSub has the best values for the Portuguese corpora. TF has the best values of recall and f-measure for all corpora but the English version of TED Talks which has in DF the best value of recall and in DocSub the best value of f-measure. It was expected quite similar values of precision, recall and f-measure between TF and DF using the Europarl corpora since the size of each document was set to the size of the phrase because Europarl does not have document borders.  This value differs when a term appears more than once in a phrase, and thus, having a higher value of TF than DF. In some cases it seems to make difference in results, e.g., Europarl in Portuguese which increased the precision from P=0.5984 in DF to P=0.6109 in TF, as well as the recall from R=0.5184 in DF to R=0.6727 in TF, resulting in an increase of f-measure from F=0.5555 in DF to F=0.6403 in TF.  When comparing DF model which takes into account only the number of documents that the word occurs, with DocSub which considers the number of shared documents between two words, DocSub achieved better values of precision, but lower values of recall. In fact, DocSub had worse results in precision only when using Europarl corpus in English, where DF reached best values of precision and f-measure.  Another interesting observation is to compare the results obtained by DF with the results achieved by HClust. This comparison is interesting since HClust uses the values of document frequency over semantically clustered terms.  As we can observe, it seems that clustering semantically related terms will increase the precision (at least for the top 1,000 terms in the English corpora used in this experiment) as expected.  Comparing the values achieved by methods containing all relations (Table 3) and the reduced taxonomies, we can see that all recalls and f-measures decreased, as expected.
Observing results from HClust, which obtained the best f-measure in a cluster containing 1,000 terms (the f-measure was still rising), we decide to perform a second experiment, increasing the number of terms, and consequently clusters.  Table 4 presents the values of precision and recall for all models using a vocabulary containing up to 10,000 words, where DocSub and HClust contain results when the best f-measure was achieved, and Patt consider all patterns with the limited number of words.  As we can see, values of precision were lower for most methods, with exception of Patt and DocSub, which increased for most corpora. When increasing the number of terms to 10,000, the DocSub models using Europarl corpora performed better than when using TED Talks corpora.  Although decreasing the values of precision, TF and DF increased the values of recall, but decreasing the values of f-measure. As occurred in the experiment using the top 1,000 words, this experiment also kept TF with the highest values of f-measure for most methods. TF and DF achieved almost the same values of precision, recall and f-measure using the English corpora, achieving the same value of precision (P=0.0150) and f-measure (F=0.0293) when using the Europarl corpus in English.  When comparing DF with HClust, it seems a good approach in English to verify the hierarchical relation only for terms that are semantically related instead of considering all terms.  The lowest values of precision are achieved by DSim model, and the lowest recalls are obtained by HClust and Patt models.
Table 5 presents the values of precision, recall and f-measure for the methods in all corpora. The filtering on multiple hypernyms is applied in relations extracted using 1,000 terms in the dictionary.  Analyzing Table 5 we observe that Patt achieves again the best precision values for the English corpora. On the other hand, choosing the best hypernym worked very well for DocSub which obtained the best precision for the Portuguese corpora. As filtering out multiple hypernyms might remove also correct relations, the recall values for all corpora are very low.  The values of precision increased for most corpora of the Patt and DocSub models.  Comparing the values achieved by HClust using 1,000 terms and the ones obtained when reducing to one parent, we can see that they are almost the same. Only TED Talks corpora obtained a decrease in precision (from P=0.0664 to P=0.0661 in English and from P=0.5656 to P=0.5295 in Portuguese).  Increasing the number of terms form 1,000 to 10,000 only increased the recall in TF and DF models.
Using the direct graph with transitive reduction for each model, we generated the metrics presented in Table 6. The results are generated for models using the top 1,000 terms of each corpus in English generated for the automatic evaluation, i.e., models using nouns instead of noun phrases.  As we can observe in Table 6, limiting the number of terms to 1,000, Patt and DocSub do not to generate relations for all terms. Patt model could not generate relations for all terms because terms must to be in a pattern in order to have their taxonomic relation identified.  For example, using relations generated by TF model using the Europarl corpus, we can understand the MaxDepth as having 789 terms with different values of term frequency, while having 211 that share the same value of term frequency with other terms. Using the SLQS model, we can understand that 902 terms share different values of entropy, while 98 share the same value with other terms.
Table 7 contains the results for each metric after applying the transitive reduction for relations generated by models using the top 1,000 terms of each corpus in Portuguese.  The results for the Portuguese corpora are quite similar to the ones generated by the English corpora, having terms without relations in Patt and DocSub, and DSim, SLQS, TF and DF generating deep taxonomies, affirming the characteristics of each method. For Portuguese, the number of relations found by Patt model using the TED Talks corpus were smaller than the one found using the English corpus, impacting the maximum depth. On the other hand, the number of siblings for a term was greater.
Table 1 shows the results with different implementations in P2, i.e., question type, answer score sampling, and hidden dictionary learning. Overall, all of the implementations can improve the performances of base models. Specifically, the implementations of P2 can further boost performance by at most 11.75% via hidden dictionary learning.
Table 2 shows the results about applying our principles on four different models (i.e., LF , HCIAE , CoAtt  and RvA ). In general, both of our principles can improve all the models in any ablative condition (i.e., P1, P2, P1+P2). Note that the effectiveness of P1 and P2 are additive, which means combining P1 and P2 performs the best.
We use the representations in the 9-th BERT layer for fair comparison of BERTScore and MoverScore and show results on the machine translation task in Table 5. MoverScore outperforms both asymmetric HMD factors, while if they are combined via harmonic mean, BERTScore is on par with MoverScore.  We also observe that WMD-BIGRAMS slightly outperforms WMD-UNIGRAMS on 3 out of 4 language pairs.
Table 1: In all language pairs, the best correlation is achieved by our word mover metrics that use a BERT pretrained on MNLI as the embedding generator and PMeans to aggregate the embeddings from different BERT layers, i.e., WMD-1/2+BERT+MNLI+PMeans. Note that our unsupervised word mover metrics even outperforms RUSE, a supervised metric. We also find that our word mover metrics outperforms the sentence mover.
Tables 3: Interestingly, no metric produces an even moderate correlation with human judgments, including our own.  However, best correlation is still achieved by our word mover metrics combining contextualized representations.
Table 4: Word mover metrics outperform all baselines except for the supervised metric LEIC, which uses more information by considering both images and texts.
that under similar Acc, M2 has much better semantic similarity for both Yelp and Literature. In fact, cyclic consistency loss proves to be the strongest driver of semantic preservation across all of our model configurations. The other losses do not constrain the semantic relationship across style transfer, so we include the cyclic loss in M3 to M7.  When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss reduces PP, sometimes at a slight cost of semantic preservation.
Overall, the results show the same trends as our automatic metrics. For example, on Yelp, large differences in human judgments of semantic preservation (M2>M0, M7>M0, M7>M2) also show the largest differences in Sim, while M6 and M7 have very similar human judgments and very similar Sim scores.
We describe a human sentence-level validation of our metrics in Table 5.  To validate Acc, human annotators were asked to judge the style of 100 transferred sentences  We then compute the percentage of machine and human judgments that match.  We validate Sim and PP by computing sentence-level Spearman's ρ between the metric and human judgments  From Table 5, all validations show strong correlations on the Yelp dataset and reasonable correlations on Literature.
under similar Acc, M2 has much better semantic similarity for both Yelp and Literature. In fact, cyclic consistency loss proves to be the strongest driver of semantic preservation across all of our model configurations. The other losses do not constrain the semantic relationship across style transfer, so we include the cyclic loss in M3 to M7.  Table 2 shows that the model with paraphrase loss (M1) slightly improves Sim over M0 on both datasets under similar Acc. For Yelp, M1 has better Acc and PP than M0 at comparable semantic similarity. So, when used alone, the paraphrase loss helps. However, when combined with other losses (e.g., compare M2 to M4), its benefits are mixed.  When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss reduces PP, sometimes at a slight cost of semantic preservation.
BLEU scores and post-transfer accuracies are shown in Table 6. The most striking result is that untransferred sentences have the highest BLEU score by a large margin, suggesting that prior work for this task has not yet eclipsed the trivial baseline of returning the input sentence. However, at similar levels of Acc, our models have higher BLEU scores than prior work. We additionally find that supervised BLEU shows a trade-off with Acc: for a single model type, higher Acc generally corresponds to lower BLEU.
For each class, we measured the disfluency detection recall (relative frequency of reparandum tokens that were predicted correctly), as well as the percentage of tokens associated with each class. The results in Table 2 confirm that error rates are higher for restarts, longer rephrasings, and complex disfluencies.
Table 3 breaks down performance for different lengths and word class to explore this difference. We found that rephrase disfluencies that contain content words are harder for the model to detect, compared to rephrases with function words only, and error increases for longer disfluencies.
Our experiments evaluate the use of innovations with two popular multimodal fusion approaches: early fusion and late fusion. Our baselines include models with text-only, prosody cues only (raw), and innovation features only as inputs. Since innovations require both text and raw prosodic cues, this baseline is multimodal. In addition, for the late fusion experiments, we show the optimal value of α, the interpolation weight from Equation 5.  We found that innovations are helpful in both early and late fusion frameworks, while late fusion performs better on average. The interpolation weight α for the late fusion experiments is high when innovations are used, which further indicates that innovation features are useful in overall prediction. Interestingly, innovation features alone perform surprisingly well.
Table 2 shows a comparison of the detection performance. As for the micro F1 evaluation metric, our model achieves the highest performance (83.54%) on the FNC-1 testing subset.
The deep network based NeuralDater model in (Vashishth et al., 2018) outperforms previous feature engi  neered (Chambers, 2012) and statistical methods (Kotsakos et al., 2014) by a large margin.  We observe a similar trend in our case. Compared to the state-of-the-art model NeuralDater, we gain, on an average, a 3.7% boost in accuracy on both the datasets (Table 2).
Attentive Graph Convolution (Section 4.2.2) proves to be effective for OE-GCN, giving a 2% accuracy improvement over non-attentive T-GCN of NeuralDater (Table 3). Similarly the efficacy of word level attention is also prominent from Table 3.
Table 2 illustrates the performance (F1 scores) of JRNN (Nguyen et al., 2016), DMCNN (Chen the two baseline model Embedet al., 2015), ding+T and CNN in Chen et al. (2015) and our framework in trigger classification subtask and argument role labeling subatsk.  We can see that our framework significantly outperforms all the other methods, especially in trigger classification subtask. In the 1/N data split of triggers, our framework is 7.9% better than the JRNN,
Table 1 shows the overall performance comparing to the above state-of-the-art methods with golden-standard entities. From the table, we can see that our JMEE framework achieves the best F1 scores for both trigger classification and argumentrelated subtasks among all the compared methods. There is a significant gain with the trigger classification and argument role labeling performances, which is 2% higher over the best-reported models.
The results of the different models are presented in Table 3. For each model we report both perplexity and accuracy (except for discriminative training, where perplexity is not valid), where each of them is reported according to the best performing model on that measure (on the dev set). We also report the WER of all models, which correlates perfectly with the accuracy measure.  Extending the vocabulary without training those additional words, results in a 2.37-points loss on the perplexity measure, while our evaluation metric (accuracy) stays essentially the same.  The FINETUNED-LM baseline is the strongest baseline, outperforming all others with an accuracy of 65.4%. Similarly, when using discriminative trainthe FINE-TUNED-DISCRIMINATIVE model ing, outperforms the CS-ONLY-DISCRIMINATIVE model. Note that using discriminative training, even with no additional monolingual data, leads to better performance than that of the best language model: the CS-ONLY-DISCRIMINATIVE model achieves an accuracy of 70.5%, 5.1 points more than the accuracy of the FINE-TUNED-LM model. We gain further improvement by adding monolingual data and get an even higher accuracy of 75.5%, which is 10.1 points higher than the best language model.
Table 4 depicts the results when using subsets of the CS training data with discriminative training. The less code-switching data we use, the more the effect of using the monolingual data is significant: we gain 8.8, 6.5, 4.2 and 5 more accuracy points with 25%, 50%, 75% and 100% of the data, respectively. In the case of 25% of the data, the FINE-TUNED-DISCRIMINATIVE model improves over CS-ONLY-DISCRIMINATIVE by 17 relative percents.
Table 5 breaks down the results of the different models according to two conditions: when the gold sentence is code-switched, and when the gold sentence is monolingual.  the FINE-TUNEDDISCRIMINATIVE model is able to prioritize the gold sentence better than all other models, under both conditions. The improvement we get is most significant when the gold sentence is CS: in those cases we get a dramatic improvement of 27.73 accuracy points (a relative improvement of 58%).
Table 7 shows that compared to a baseline without gaze features, the results improve by 3% F1-score.
We achieve a minor, but nonetheless significant improvement (shown in Table 5), which strongly supports the generalizability effect of the typeaggregated features on unseen data.
Table 1 shows that our proposed token level embedding scheme OntoLSTM-PP outperforms the better variant of our baseline LSTM-PP (with GloVe-retro intialization) by an absolute accuracy difference of 4.9%, or a relative error reduction of 32%. OntoLSTM-PP also outperforms HPCD (full), the previous best result on this dataset.  Initializing the word embeddings with GloVeretro (which uses WordNet as described in Faruqui et al. (2015)) instead of GloVe amounts to a small improvement, compared to the improvements obtained using OntoLSTM-PP.
Table 2 shows the effect of using the PP attachment predictions as features within a dependency parser. We note there is a relatively small difference in unlabeled attachment accuracy for all dependencies (not only PP attachments), even when gold PP attachments are used as additional features to the parser. However, when gold PP attachment are used, we note a large potential improve  ment of 10.46 points in PP attachment accuracies (between the PPA accuracy for RBG and RBG + Oracle PP), which confirms that adding PP predictions as features is an effective approach. Our proposed model RBG + OntoLSTM-PP recovers 15% of this potential improvement, while RBG + HPCD (full) recovers 10%,  For example, the unlabeled attachment score (UAS) of the baselines RBG and RBG + HPCD (full) are 94.17 and 94.19, respectively, in Table 2, compared to 93.96 and 94.05, respectively, in Belinkov et al. (2014).
The second row in Table 3 shows the test accuracy of a system trained without sense priors  and the third row shows the effect of making the token representations context-insensitive by giving a similar attention score to all related concepts, essentially making them type level representations, but still grounded in WordNet.
In our first series of experiments, we observed that domain-tuning is very important when using Marian.  Table 2 shows the scores on development data. We also tried decoding with an ensemble of three independent runs, which also pushed the performance a bit.
Table 3 lists the scores we obtained on development data.  For Marian amun, the effect is negligible as we can see in Table 3. For the Transformer, domain labels had little effect on BLEU but were clearly beneficial according to chrF-1.0.
Table 4 summarizes the scores on the three development test sets for English-French and EnglishGerman. We can see that the dual attention model does not work at all and the scores slightly drop. The concatenation approach works better  However, the improvements are small if any
Table 5 shows the BLEU scores for this configuration with different ways of integrating the visual features. The results are inconclusive. The ranking according to chrF-1.0 was not any clearer.
Table 6 shows results of ablation experiments removing or modifying one component or data choice at a time, and results when using ensemble decoding. Using ensemble decoding gave a consistent but small improvement. Multi-lingual models were clearly better than mono-lingual models. For French, 6M sentences of subtitle data gave worse results than 3M.  It appears that the output vocabulary was reduced back towards the vocabulary seen in the multi-modal training set. When the experiment was repeated so that the finetuning phase included the text-only data, the performance returned to approximately the same level as without tuning (+multi-modal finetune row in Table 6).  BLEU scores for French were surprisingly slightly improved by this procedure.
Table 6 shows the metrics  SMT seems to retain the most lexical richness according to the LD metrics we used (TTR, Yule's  and MTLD).
Number of parallel sentences in the train, test and development splits for the language pairs we used.  We used +/- 2M sentence pairs from the Europarl corpora for each of the language pairs.  split the data into train, test and development sets,  Details on the different datasets can be found in Table 1. We chose to include large quantities of data in our test sets
Table 2 (first two columns) shows the training vocabularies for the source and target sides.  Table 3 presents  sizes for the RNN, SMT and Transformer systems.  this table clearly shows how source and target vocabularies are comparable in the original datasets,
we present BLEU and TER for the REV systems in Table 5,  While Transformer models are the best ones according to the evaluation metrics,
Evaluating this speaker-invariant representation gave contradictory results, shown in Table 2: very good scores on paraphrase retrieval, but zero correlation with visual space.
Table 1 shows the evaluation results on synthetic speech. Representations learned by Audio2vec and SegMatch are compared to the performance of random vectors, mean MFCC vectors, as well as visually supervised representations (VGS, model from Chrupała et al. (2017)). Audio2vec works better than chance and mean MFCC on paraphrase retrieval, but does not correlate with the visual space. SegMatch works much better than Audio2vec according to both criteria. It does not come close to VGS on paraphrase retrieval, but it does correlate with the visual modality even better.
Using SIFT we observed substantial differences in the text generated by our fine-tuned decoder when trained with each of the different  While the RNN primarily employs <UNK> tokens or repeats previous words, the CNN masks out <UNK> tokens using determiners or prepositions. In contrast, DAN masks out punctuation and determiners using words indicative of the class label (i.e. nouns, verbs, adjectives). We hypothesize that these patterns stem from the inductive biases of the classifiers. DAN receives a stronger signal by repeating words with a higher sentiment value due to its averaging, while 1NLTK (Loper and Bird, 2002) was used for POS tagging. the CNN does not repeat words (thus having the least amount of changes) and removes uninformative words as its max-pooling layer selects only the most important ones. Similarly, the gates of the LSTM may allow the model to ignore the random and thus noisy <UNK> embeddings, which enables it to use this token as a masking operation to ignore unimportant words.  Examples are in Table 1, results in 2 and 3.
In order to identify differences, we conducted an automated study of the reconstructed text by inspecting changes in the proportion of part-ofspeech tags1 and an increase or decrease in word polarity for sentiment compared to the original input.  To compare our abstractive with an extractive approach (RNP; Lei et al., 2016), we compute the overlap of retained terms in Table 2 (bottom row). We can see that the DAN has the highest overlap, indicating that it retains words, while the CNN and RNN reformulate sentences. These scores highlight the differences of our approach, as our model does not solely extract indicative words, but reformulates the original sentence.
In order to automatically identify if SIFT retains the sentiment of the sentences, we analyze the output using SentiWordNet (Baccianella et al., 2010). By considering only adjectives, we obtain a measure of the positive and negative score for each sentence before and after fine-tuning. The difference of these scores averaged over all examples provides us with a sense of whether the fine-tuning increases the polarity of the sentences (Table 3). We see a constant increase in sentiment value in both directions across all three models after finetuning demonstrating that the framework is able to pick up on words that are indicative of sentiment. This is especially true in the case of DAN where we see a large increase as the decoder repeatedly predicts words having high sentiment value. Overall, these results indicate that SIFT is able to highlight certain inductive biases of the model and is able to reformulate and amplify the meaning of the original text based on the classifier's preferences.
We compare the most frequent SIFT terms with the most indicative words w.r.t. a class using PMI with 100 smoothing, following Gururangan et al. (2018). We list the most frequently used words in Table 4. To understand how similar the SIFT terms are to the PMI terms we calculate the weighted Kendall's tau correlation (Shieh, 1998), which weights terms higher in the list as more important. In the case of high correlation, we hypothesize that the classifier has memorized the artifacts of the data set, which in turn SIFT has leveraged to "fool" the classifier. This "fooling" in the case of sentiment analysis is due to the terms being indicative of the respective class and having high sentiment (cf. Table 3). For SNLI, we find that many of the top PMI and SIFT terms overlap, and slightly correlate (0.366). We find terms (as reported in Appendix A2), e.g. 'sleeping', 'cats', 'cat', that were identified as artifacts by Gururangan et al. (2018). In the PubMed task where the aim is to classify sentences as belonging to one of five classes—background, objective, methods, results, conclusions—the top terms in the setting where we change ground truth from "objective" to "conclusion" are intuitively relevant terms for that class. They do not, however, correlate with PMI, which might indicate that pretraining on large unlabelled data has enabled SIFT to capture these relations. Overall, this shows that SIFT is able to identify both previously known as well as novel artifacts. In contrast to PMI, SIFT uncovers the propensities of the trained model and not only the data set, giving insight into what the model has actually encoded.
