<R> <C> Batch size <C> Throughput (instances/s) Inference <C> Throughput (instances/s) Inference <C> Throughput (instances/s) Inference <C> Throughput (instances/s) Training <C> Throughput (instances/s) Training <C> Throughput (instances/s) Training <R> <C> Batch size <C> Iter <C> Recur <C> Fold <C> Iter <C> Recur <C> Fold <R> <C> 1 <C> 19.2 <C> 81.4 <C> 16.5 <C> 2.5 <C> 4.8 <C> 9.0 <R> <C> 10 <C> 49.3 <C> 217.9 <C> 52.2 <C> 4.0 <C> 4.2 <C> 37.5 <R> <C> 25 <C> 72.1 <C> 269.9 <C> 61.6 <C> 5.5 <C> 3.6 <C> 54.7 <CAP> Table 2: Throughput for processing the TreeLSTM model on our recursive framework, Fold’s folding technique, and TensorFlow’s iterative approach, with the Large Movie Review dataset. The recursive approach performs the best on inference with efficient parallel execution of tree nodes, while the folding technique shows better performance on training thanks to its GPU exploitation.
<R> <C> Batch size <C> Throughput (instances/s) Balanced <C> Throughput (instances/s) Moderate <C> Throughput (instances/s) Linear <R> <C> 1 <C> 46.7 <C> 27.3 <C> 7.6 <R> <C> 10 <C> 125.2 <C> 78.2 <C> 22.7 <R> <C> 25 <C> 129.7 <C> 83.1 <C> 45.4 <CAP> Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.
<R> <C> [BOLD] Representation <C> [BOLD] Hyper parameters Filter size <C> [BOLD] Hyper parameters Num. Feature maps <C> [BOLD] Hyper parameters Activation func. <C> [BOLD] Hyper parameters L2 Reg. <C> [BOLD] Hyper parameters Learning rate <C> [BOLD] Hyper parameters Dropout Prob. <C> [BOLD] F1.(avg. in 5-fold) with default values <C> [BOLD] F1.(avg. in 5-fold) with optimal values <R> <C> CoNLL08 <C> 4-5 <C> 1000 <C> Softplus <C> 1.15e+01 <C> 1.13e-03 <C> 1 <C> 73.34 <C> 74.49 <R> <C> SB <C> 4-5 <C> 806 <C> Sigmoid <C> 8.13e-02 <C> 1.79e-03 <C> 0.87 <C> 72.83 <C> [BOLD] 75.05 <R> <C> UD v1.3 <C> 5 <C> 716 <C> Softplus <C> 1.66e+00 <C> 9.63E-04 <C> 1 <C> 68.93 <C> 69.57 <CAP> Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.
<R> <C> [BOLD] Relation <C> [BOLD] best F1 (in 5-fold) without sdp <C> [BOLD] best F1 (in 5-fold) with sdp <C> [BOLD] Diff. <R> <C> USAGE <C> 60.34 <C> 80.24 <C> + 19.90 <R> <C> MODEL-FEATURE <C> 48.89 <C> 70.00 <C> + 21.11 <R> <C> PART_WHOLE <C> 29.51 <C> 70.27 <C> +40.76 <R> <C> TOPIC <C> 45.80 <C> 91.26 <C> +45.46 <R> <C> RESULT <C> 54.35 <C> 81.58 <C> +27.23 <R> <C> COMPARE <C> 20.00 <C> 61.82 <C> + 41.82 <R> <C> macro-averaged <C> 50.10 <C> 76.10 <C> +26.00 <CAP> Table 1: Effect of using the shortest dependency path on each relation type.
<R> <C> [EMPTY] <C> C-F1 100% <C> C-F1 50% <C> R-F1 100% <C> R-F1 50% <C> F1 100% <C> F1 50% <R> <C> Y-3 <C> 49.59 <C> 65.37 <C> 26.28 <C> 37.00 <C> 34.35 <C> 47.25 <R> <C> Y-3:Y<italic>C</italic>-1 <C> 54.71 <C> 66.84 <C> 28.44 <C> 37.35 <C> 37.40 <C> 47.92 <R> <C> Y-3:Y<italic>R</italic>-1 <C> 51.32 <C> 66.49 <C> 26.92 <C> 37.18 <C> 35.31 <C> 47.69 <R> <C> Y-3:Y<italic>C</italic>-3 <C> <bold>54.58</bold> <C> 67.66 <C> <bold>30.22</bold> <C> <bold>40.30</bold> <C> <bold>38.90</bold> <C> <bold>50.51</bold> <R> <C> Y-3:Y<italic>R</italic>-3 <C> 53.31 <C> 66.71 <C> 26.65 <C> 35.86 <C> 35.53 <C> 46.64 <R> <C> Y-3:Y<italic>C</italic>-1:Y<italic>R</italic>-2 <C> 52.95 <C> <bold>67.84</bold> <C> 27.90 <C> 39.71 <C> 36.54 <C> 50.09 <R> <C> Y-3:Y<italic>C</italic>-3:Y<italic>R</italic>-3 <C> 54.55 <C> 67.60 <C> 28.30 <C> 38.26 <C> 37.26 <C> 48.86 <CAP> Table 3: Performance of MTL sequence tagging approaches, essay level. Tasks separated by “:”. Layers from which tasks feed are indicated by respective numbers.
<R> <C> [EMPTY] <C> Paragraph level Acc. <C> Paragraph level C-F1 <C> Paragraph level C-F1 <C> Paragraph level R-F1 <C> Paragraph level R-F1 <C> Paragraph level F1 <C> Paragraph level F1 <C> Essay level Acc. <C> Essay level C-F1 <C> Essay level C-F1 <C> Essay level R-F1 <C> Essay level R-F1 <C> Essay level F1 <C> Essay level F1 <R> <C> [EMPTY] <C> [EMPTY] <C> 100% <C> 50% <C> 100% <C> 50% <C> 100% <C> 50% <C> [EMPTY] <C> 100% <C> 50% <C> 100% <C> 50% <C> 100% <C> 50% <R> <C> MST-Parser <C> 31.23 <C> 0 <C> 6.90 <C> 0 <C> 1.29 <C> 0 <C> 2.17 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Mate <C> 22.71 <C> 2.72 <C> 12.34 <C> 2.03 <C> 4.59 <C> 2.32 <C> 6.69 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Kiperwasser <C> 52.80 <C> 26.65 <C> 61.57 <C> 15.57 <C> 34.25 <C> 19.65 <C> 44.01 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> LSTM-Parser <C> 55.68 <C> 58.86 <C> 68.20 <C> 35.63 <C> 40.87 <C> 44.38 <C> 51.11 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> STagBLCC <C> 59.34 <C> 66.69 <C> 74.08 <C> 39.83 <C> 44.02 <C> 49.87 <C> 55.22 <C> <bold>60.46</bold> <C> 63.23 <C> 69.49 <C> <bold>34.82</bold> <C> <bold>39.68</bold> <C> <bold>44.90</bold> <C> <bold>50.51</bold> <R> <C> LSTM-ER <C> <bold>61.67</bold> <C> <bold>70.83</bold> <C> <bold>77.19</bold> <C> <bold>45.52</bold> <C> <bold>50.05</bold> <C> <bold>55.42</bold> <C> <bold>60.72</bold> <C> 54.17 <C> <bold>66.21</bold> <C> <bold>73.02</bold> <C> 29.56 <C> 32.72 <C> 40.87 <C> 45.19 <R> <C> ILP <C> 60.32 <C> 62.61 <C> 73.35 <C> 34.74 <C> 44.29 <C> 44.68 <C> 55.23 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 2: Performance of dependency parsers, STagBLCC, LSTM-ER and ILP (from top to bottom). The ILP model operates on both levels. Best scores in each column in bold (signific. at p<0.01; Two-sided Wilcoxon signed rank test, pairing F1 scores for documents). We also report token level accuracy.
<R> <C> [EMPTY] <C> STagBLCC <C> LSTM-Parser <R> <C> Essay <C> 60.62±3.54 <C> 9.40±13.57 <R> <C> Paragraph <C> 64.74±1.97 <C> 56.24±2.87 <CAP> Table 4: C-F1 (100%) in % for the two indicated systems; essay vs. paragraph level. Note that the mean performances are lower than the majority performances over the runs given in Table 2.
<R> <C> Train <C> Test <C> [BOLD] System <C> [BOLD] BLEU <C> [BOLD] NIST <C> [BOLD] METEOR <C> [BOLD] ROUGE-L <C> [BOLD] CIDEr <C> [BOLD] Add <C> [BOLD] Miss <C> [BOLD] Wrong <C> [BOLD] SER <R> <C> Original <C> [BOLD] Cleaned <C> TGen− <C> 36.85 <C> 5.3782 <C> 35.14 <C> 55.01 <C> 1.6016 <C> 00.34 <C> 09.81 <C> 00.15 <C> 10.31 <R> <C> Original <C> [BOLD] Cleaned <C> TGen <C> 39.23 <C> 6.0217 <C> 36.97 <C> 55.52 <C> 1.7623 <C> 00.40 <C> 03.59 <C> 00.07 <C> 04.05 <R> <C> Original <C> [BOLD] Cleaned <C> TGen+ <C> 40.25 <C> 6.1448 <C> 37.50 <C> 56.19 <C> 1.8181 <C> 00.21 <C> 01.99 <C> 00.05 <C> 02.24 <R> <C> Original <C> [BOLD] Cleaned <C> SC-LSTM <C> 23.88 <C> 3.9310 <C> 32.11 <C> 39.90 <C> 0.5036 <C> 07.73 <C> 17.76 <C> 09.52 <C> 35.03 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned <C> [BOLD] Cleaned <C> TGen− <C> 40.19 <C> 6.0543 <C> 37.38 <C> 55.88 <C> 1.8104 <C> 00.17 <C> 01.31 <C> 00.25 <C> 01.72 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned <C> [BOLD] Cleaned <C> TGen <C> 40.73 <C> 6.1711 <C> 37.76 <C> 56.09 <C> 1.8518 <C> 00.07 <C> 00.72 <C> 00.08 <C> 00.87 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned <C> [BOLD] Cleaned <C> TGen+ <C> 40.51 <C> 6.1226 <C> 37.61 <C> 55.98 <C> 1.8286 <C> 00.02 <C> 00.63 <C> 00.06 <C> 00.70 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned <C> [BOLD] Cleaned <C> SC-LSTM <C> 23.66 <C> 3.9511 <C> 32.93 <C> 39.29 <C> 0.3855 <C> 07.89 <C> 15.60 <C> 08.44 <C> 31.94 <R> <C> Cleaned missing <C> [BOLD] Cleaned <C> TGen− <C> 40.48 <C> 6.0269 <C> 37.26 <C> 56.19 <C> 1.7999 <C> 00.43 <C> 02.84 <C> 00.26 <C> 03.52 <R> <C> Cleaned missing <C> [BOLD] Cleaned <C> TGen <C> 41.57 <C> 6.2830 <C> 37.99 <C> 56.36 <C> 1.8849 <C> 00.37 <C> 01.40 <C> 00.09 <C> 01.86 <R> <C> Cleaned missing <C> [BOLD] Cleaned <C> TGen+ <C> 41.56 <C> 6.2700 <C> 37.94 <C> 56.38 <C> 1.8827 <C> 00.21 <C> 01.04 <C> 00.07 <C> 01.31 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added <C> [BOLD] Cleaned <C> TGen− <C> 35.99 <C> 5.0734 <C> 34.74 <C> 54.79 <C> 1.5259 <C> 00.02 <C> 11.58 <C> 00.02 <C> 11.62 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added <C> [BOLD] Cleaned <C> TGen <C> 40.07 <C> 6.1243 <C> 37.45 <C> 55.81 <C> 1.8026 <C> 00.05 <C> 03.23 <C> 00.01 <C> 03.29 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added <C> [BOLD] Cleaned <C> TGen+ <C> 40.80 <C> 6.2197 <C> 37.86 <C> 56.13 <C> 1.8422 <C> 00.01 <C> 01.87 <C> 00.01 <C> 01.88 <CAP> Table 3: Results evaluated on the cleaned test set (cf. Table 2 for column details; note that the numbers are not comparable to Table 2 as the test set is different).
<R> <C> [BOLD] Dataset <C> [BOLD] Part <C> [BOLD] MRs <C> [BOLD] Refs <C> [BOLD] SER(%) <R> <C> Original <C> Train <C> 4,862 <C> 42,061 <C> 17.69 <R> <C> Original <C> Dev <C> 547 <C> 4,672 <C> 11.42 <R> <C> Original <C> Test <C> 630 <C> 4,693 <C> 11.49 <R> <C> [0.5pt/2pt] Cleaned <C> Train <C> 8,362 <C> 33,525 <C> (0.00) <R> <C> [0.5pt/2pt] Cleaned <C> Dev <C> 1,132 <C> 4,299 <C> (0.00) <R> <C> [0.5pt/2pt] Cleaned <C> Test <C> 1,358 <C> 4,693 <C> (0.00) <CAP> Table 1: Data statistics comparison for the original E2E data and our cleaned version (number of distinct MRs, total number of textual references, SER as measured by our slot matching script, see Section 3).
<R> <C> Train <C> Test <C> [BOLD] System <C> [BOLD] BLEU <C> [BOLD] NIST <C> [BOLD] METEOR <C> [BOLD] ROUGE-L <C> [BOLD] CIDEr <C> [BOLD] Add <C> [BOLD] Miss <C> [BOLD] Wrong <C> [BOLD] SER <R> <C> Original <C> [BOLD] Original <C> TGen− <C> 63.37 <C> 7.7188 <C> 41.99 <C> 68.53 <C> 1.9355 <C> 00.06 <C> 15.77 <C> 00.11 <C> 15.94 <R> <C> Original <C> [BOLD] Original <C> TGen <C> 66.41 <C> 8.5565 <C> 45.07 <C> 69.17 <C> 2.2253 <C> 00.14 <C> 04.11 <C> 00.03 <C> 04.27 <R> <C> Original <C> [BOLD] Original <C> TGen+ <C> 67.06 <C> 8.5871 <C> 45.83 <C> 69.73 <C> 2.2681 <C> 00.04 <C> 01.75 <C> 00.01 <C> 01.80 <R> <C> Original <C> [BOLD] Original <C> SC-LSTM <C> 39.11 <C> 5.6704 <C> 36.83 <C> 50.02 <C> 0.6045 <C> 02.79 <C> 18.90 <C> 09.79 <C> 31.51 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned <C> [BOLD] Original <C> TGen− <C> 65.87 <C> 8.6400 <C> 44.20 <C> 67.51 <C> 2.1710 <C> 00.20 <C> 00.56 <C> 00.21 <C> 00.97 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned <C> [BOLD] Original <C> TGen <C> 66.24 <C> 8.6889 <C> 44.66 <C> 67.85 <C> 2.2181 <C> 00.10 <C> 00.02 <C> 00.00 <C> 00.12 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned <C> [BOLD] Original <C> TGen+ <C> 65.97 <C> 8.6630 <C> 44.45 <C> 67.59 <C> 2.1855 <C> 00.02 <C> 00.00 <C> 00.00 <C> 00.03 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned <C> [BOLD] Original <C> SC-LSTM <C> 38.52 <C> 5.7125 <C> 37.45 <C> 48.50 <C> 0.4343 <C> 03.85 <C> 17.39 <C> 08.12 <C> 29.37 <R> <C> Cleaned missing <C> [BOLD] Original <C> TGen− <C> 66.28 <C> 8.5202 <C> 43.96 <C> 67.83 <C> 2.1375 <C> 00.14 <C> 02.26 <C> 00.22 <C> 02.61 <R> <C> Cleaned missing <C> [BOLD] Original <C> TGen <C> 67.00 <C> 8.6889 <C> 44.97 <C> 68.19 <C> 2.2228 <C> 00.06 <C> 00.44 <C> 00.03 <C> 00.53 <R> <C> Cleaned missing <C> [BOLD] Original <C> TGen+ <C> 66.74 <C> 8.6649 <C> 44.84 <C> 67.95 <C> 2.2018 <C> 00.00 <C> 00.21 <C> 00.03 <C> 00.24 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added <C> [BOLD] Original <C> TGen− <C> 64.40 <C> 7.9692 <C> 42.81 <C> 68.87 <C> 2.0563 <C> 00.01 <C> 13.08 <C> 00.00 <C> 13.09 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added <C> [BOLD] Original <C> TGen <C> 66.23 <C> 8.5578 <C> 45.12 <C> 68.87 <C> 2.2548 <C> 00.04 <C> 03.04 <C> 00.00 <C> 03.09 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added <C> [BOLD] Original <C> TGen+ <C> 65.96 <C> 8.5238 <C> 45.49 <C> 68.79 <C> 2.2456 <C> 00.00 <C> 01.44 <C> 00.00 <C> 01.45 <CAP> Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section 5.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table 3 as the test set is different.
<R> <C> [BOLD] Training data <C> [BOLD] Add <C> [BOLD] Miss <C> [BOLD] Wrong <C> [BOLD] Disfl <R> <C> Original <C> 0 <C> 22 <C> 0 <C> 14 <R> <C> Cleaned added <C> 0 <C> 23 <C> 0 <C> 14 <R> <C> Cleaned missing <C> 0 <C> 1 <C> 0 <C> 2 <R> <C> Cleaned <C> 0 <C> 0 <C> 0 <C> 5 <CAP> Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).
<R> <C> [BOLD] Model <C> [BOLD] External <C> B <R> <C> Seq2SeqK (Konstas et al.,  2017 ) <C> - <C> 22.0 <R> <C> GraphLSTM (Song et al.,  2018 ) <C> - <C> 23.3 <R> <C> GCNSEQ (Damonte and Cohen,  2019 ) <C> - <C> 24.4 <R> <C> DCGCN(single) <C> - <C> 25.9 <R> <C> DCGCN(ensemble) <C> - <C> [BOLD] 28.2 <R> <C> TSP (Song et al.,  2016 ) <C> ALL <C> 22.4 <R> <C> PBMT (Pourdamghani et al.,  2016 ) <C> ALL <C> 26.9 <R> <C> Tree2Str (Flanigan et al.,  2016 ) <C> ALL <C> 23.0 <R> <C> SNRG (Song et al.,  2017 ) <C> ALL <C> 25.6 <R> <C> Seq2SeqK (Konstas et al.,  2017 ) <C> 0.2M <C> 27.4 <R> <C> GraphLSTM (Song et al.,  2018 ) <C> 0.2M <C> 28.2 <R> <C> DCGCN(single) <C> 0.1M <C> 29.0 <R> <C> DCGCN(single) <C> 0.2M <C> [BOLD] 31.6 <R> <C> Seq2SeqK (Konstas et al.,  2017 ) <C> 2M <C> 32.3 <R> <C> GraphLSTM (Song et al.,  2018 ) <C> 2M <C> 33.6 <R> <C> Seq2SeqK (Konstas et al.,  2017 ) <C> 20M <C> 33.8 <R> <C> DCGCN(single) <C> 0.3M <C> 33.2 <R> <C> DCGCN(ensemble) <C> 0.3M <C> [BOLD] 35.3 <CAP> Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M
<R> <C> [BOLD] Model <C> [BOLD] T <C> #P <C> B <C> C <R> <C> Seq2SeqB (Beck et al.,  2018 ) <C> S <C> 28,4M <C> 21.7 <C> 49.1 <R> <C> GGNN2Seq (Beck et al.,  2018 ) <C> S <C> 28.3M <C> 23.3 <C> 50.4 <R> <C> Seq2SeqB (Beck et al.,  2018 ) <C> E <C> 142M <C> 26.6 <C> 52.5 <R> <C> GGNN2Seq (Beck et al.,  2018 ) <C> E <C> 141M <C> 27.5 <C> 53.5 <R> <C> DCGCN (ours) <C> S <C> [BOLD] 19.1M <C> 27.9 <C> 57.3 <R> <C> DCGCN (ours) <C> E <C> 92.5M <C> [BOLD] 30.4 <C> [BOLD] 59.6 <CAP> Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; “S” and “E” denote single and ensemble models, respectively.
<R> <C> [BOLD] Model <C> [BOLD] Type <C> [BOLD] English-German #P <C> [BOLD] English-German B <C> [BOLD] English-German C <C> [BOLD] English-Czech #P <C> [BOLD] English-Czech B <C> [BOLD] English-Czech C <R> <C> BoW+GCN (Bastings et al.,  2017 ) <C> Single <C> - <C> 12.2 <C> - <C> - <C> 7.5 <C> - <R> <C> CNN+GCN (Bastings et al.,  2017 ) <C> Single <C> - <C> 13.7 <C> - <C> - <C> 8.7 <C> - <R> <C> BiRNN+GCN (Bastings et al.,  2017 ) <C> Single <C> - <C> 16.1 <C> - <C> - <C> 9.6 <C> - <R> <C> PB-SMT (Beck et al.,  2018 ) <C> Single <C> - <C> 12.8 <C> 43.2 <C> - <C> 8.6 <C> 36.4 <R> <C> Seq2SeqB (Beck et al.,  2018 ) <C> Single <C> 41.4M <C> 15.5 <C> 40.8 <C> 39.1M <C> 8.9 <C> 33.8 <R> <C> GGNN2Seq (Beck et al.,  2018 ) <C> Single <C> 41.2M <C> 16.7 <C> 42.4 <C> 38.8M <C> 9.8 <C> 33.3 <R> <C> DCGCN (ours) <C> Single <C> [BOLD]  29.7M <C> [BOLD] 19.0 <C> [BOLD] 44.1 <C> [BOLD]  28.3M <C> [BOLD] 12.1 <C> [BOLD] 37.1 <R> <C> Seq2SeqB (Beck et al.,  2018 ) <C> Ensemble <C> 207M <C> 19.0 <C> 44.1 <C> 195M <C> 11.3 <C> 36.4 <R> <C> GGNN2Seq (Beck et al.,  2018 ) <C> Ensemble <C> 206M <C> 19.6 <C> 45.1 <C> 194M <C> 11.7 <C> 35.9 <R> <C> DCGCN (ours) <C> Ensemble <C> [BOLD]  149M <C> [BOLD] 20.5 <C> [BOLD] 45.8 <C> [BOLD]  142M <C> [BOLD] 13.1 <C> [BOLD] 37.8 <CAP> Table 4: Main results on English-German and English-Czech datasets.
<R> <C> [ITALIC] Block <C> [ITALIC] n <C> [ITALIC] m <C> B <C> C <R> <C> 1 <C> 1 <C> 1 <C> 17.6 <C> 48.3 <R> <C> 1 <C> 1 <C> 2 <C> 19.2 <C> 50.3 <R> <C> 1 <C> 2 <C> 1 <C> 18.4 <C> 49.1 <R> <C> 1 <C> 1 <C> 3 <C> 19.6 <C> 49.4 <R> <C> 1 <C> 3 <C> 1 <C> 20.0 <C> 50.5 <R> <C> 1 <C> 3 <C> 3 <C> 21.4 <C> 51.0 <R> <C> 1 <C> 3 <C> 6 <C> 21.8 <C> 51.7 <R> <C> 1 <C> 6 <C> 3 <C> 21.7 <C> 51.5 <R> <C> 1 <C> 6 <C> 6 <C> 22.0 <C> 52.1 <R> <C> 2 <C> 3 <C> 6 <C> [BOLD] 23.5 <C> 53.3 <R> <C> 2 <C> 6 <C> 3 <C> 23.3 <C> [BOLD] 53.4 <R> <C> 2 <C> 6 <C> 6 <C> 22.0 <C> 52.1 <CAP> Table 5: The effect of the number of layers inside DCGCN sub-blocks on the AMR15 development set.
<R> <C> [BOLD] GCN +RC (2) <C> B 16.8 <C> C 48.1 <C> [BOLD] GCN +RC+LA (2) <C> B 18.3 <C> C 47.9 <R> <C> +RC (4) <C> 18.4 <C> 49.6 <C> +RC+LA (4) <C> 18.0 <C> 51.1 <R> <C> +RC (6) <C> 19.9 <C> 49.7 <C> +RC+LA (6) <C> 21.3 <C> 50.8 <R> <C> +RC (9) <C> [BOLD] 21.1 <C> 50.5 <C> +RC+LA (9) <C> [BOLD] 22.0 <C> 52.6 <R> <C> +RC (10) <C> 20.7 <C> [BOLD] 50.7 <C> +RC+LA (10) <C> 21.2 <C> [BOLD] 52.9 <R> <C> DCGCN1 (9) <C> 22.9 <C> 53.0 <C> DCGCN3 (27) <C> 24.8 <C> 54.7 <R> <C> DCGCN2 (18) <C> 24.2 <C> 54.4 <C> DCGCN4 (36) <C> [BOLD] 25.5 <C> [BOLD] 55.4 <CAP> Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i×(n+m) layers. The number of layers for each model is shown in parenthesis.
<R> <C> [BOLD] Model <C> D <C> #P <C> B <C> C <R> <C> DCGCN(1) <C> 300 <C> 10.9M <C> 20.9 <C> 52.0 <R> <C> DCGCN(2) <C> 180 <C> 10.9M <C> [BOLD] 22.2 <C> [BOLD] 52.3 <R> <C> DCGCN(2) <C> 240 <C> 11.3M <C> 22.8 <C> 52.8 <R> <C> DCGCN(4) <C> 180 <C> 11.4M <C> [BOLD] 23.4 <C> [BOLD] 53.4 <R> <C> DCGCN(1) <C> 420 <C> 12.6M <C> 22.2 <C> 52.4 <R> <C> DCGCN(2) <C> 300 <C> 12.5M <C> 23.8 <C> 53.8 <R> <C> DCGCN(3) <C> 240 <C> 12.3M <C> [BOLD] 23.9 <C> [BOLD] 54.1 <R> <C> DCGCN(2) <C> 360 <C> 14.0M <C> 24.2 <C> [BOLD] 54.4 <R> <C> DCGCN(3) <C> 300 <C> 14.0M <C> [BOLD] 24.4 <C> 54.2 <R> <C> DCGCN(2) <C> 420 <C> 15.6M <C> 24.1 <C> 53.7 <R> <C> DCGCN(4) <C> 300 <C> 15.6M <C> [BOLD] 24.6 <C> [BOLD] 54.8 <R> <C> DCGCN(3) <C> 420 <C> 18.6M <C> 24.5 <C> 54.6 <R> <C> DCGCN(4) <C> 360 <C> 18.4M <C> [BOLD] 25.5 <C> [BOLD] 55.4 <CAP> Table 7: Comparisons of different DCGCN models under almost the same parameter budget.
<R> <C> [BOLD] Model <C> B <C> C <R> <C> DCGCN4 <C> 25.5 <C> 55.4 <R> <C> -{4} dense block <C> 24.8 <C> 54.9 <R> <C> -{3, 4} dense blocks <C> 23.8 <C> 54.1 <R> <C> -{2, 3, 4} dense blocks <C> 23.2 <C> 53.1 <CAP> Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.
<R> <C> [BOLD] Model <C> B <C> C <R> <C> DCGCN4 <C> 25.5 <C> 55.4 <R> <C> Encoder Modules <C> [EMPTY] <C> [EMPTY] <R> <C> -Linear Combination <C> 23.7 <C> 53.2 <R> <C> -Global Node <C> 24.2 <C> 54.6 <R> <C> -Direction Aggregation <C> 24.6 <C> 54.6 <R> <C> -Graph Attention <C> 24.9 <C> 54.7 <R> <C> -Global Node&Linear Combination <C> 22.9 <C> 52.4 <R> <C> Decoder Modules <C> [EMPTY] <C> [EMPTY] <R> <C> -Coverage Mechanism <C> 23.8 <C> 53.0 <CAP> Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder
<R> <C> Initialization <C> Depth <C> BShift <C> SubjNum <C> Tense <C> CoordInv <C> Length <C> ObjNum <C> TopConst <C> SOMO <C> WC <R> <C> N(0,0.1) <C> 29.7 <C> 71.5 <C> 82.0 <C> 78.5 <C> 60.1 <C> 80.5 <C> 76.3 <C> 74.7 <C> [BOLD] 51.3 <C> 52.5 <R> <C> Glorot <C> 31.3 <C> [BOLD] 72.3 <C> 81.8 <C> 78.7 <C> 59.4 <C> 81.3 <C> 76.6 <C> [BOLD] 74.6 <C> 50.4 <C> 57.0 <R> <C> Our paper <C> [BOLD] 35.1 <C> 70.8 <C> [BOLD] 82.0 <C> [BOLD] 80.2 <C> [BOLD] 61.8 <C> [BOLD] 82.8 <C> [BOLD] 79.7 <C> 74.2 <C> 50.7 <C> [BOLD] 72.9 <CAP> Table 7: Scores for initialization strategies on probing tasks.
<R> <C> Dim <C> Method <C> Depth <C> BShift <C> SubjNum <C> Tense <C> CoordInv <C> Length <C> ObjNum <C> TopConst <C> SOMO <C> WC <R> <C> 400 <C> CBOW/400 <C> 32.5 <C> 50.2 <C> 78.9 <C> 78.7 <C> 53.6 <C> 73.6 <C> 79.0 <C> 69.6 <C> 48.9 <C> 86.7 <R> <C> 400 <C> CMOW/400 <C> [BOLD] 34.4 <C> 68.8 <C> 80.1 <C> [BOLD] 79.9 <C> [BOLD] 59.8 <C> 81.9 <C> [BOLD] 79.2 <C> [BOLD] 70.7 <C> [BOLD] 50.3 <C> 70.7 <R> <C> 400 <C> H-CBOW <C> 31.2 <C> 50.2 <C> 77.2 <C> 78.8 <C> 52.6 <C> 77.5 <C> 76.1 <C> 66.1 <C> 49.2 <C> [BOLD] 87.2 <R> <C> 400 <C> H-CMOW <C> 32.3 <C> [BOLD] 70.8 <C> [BOLD] 81.3 <C> 76.0 <C> 59.6 <C> [BOLD] 82.3 <C> 77.4 <C> 70.0 <C> 50.2 <C> 38.2 <R> <C> 784 <C> CBOW/784 <C> 33.0 <C> 49.6 <C> 79.3 <C> 78.4 <C> 53.6 <C> 74.5 <C> 78.6 <C> 72.0 <C> 49.6 <C> [BOLD] 89.5 <R> <C> 784 <C> CMOW/784 <C> [BOLD] 35.1 <C> [BOLD] 70.8 <C> [BOLD] 82.0 <C> 80.2 <C> [BOLD] 61.8 <C> 82.8 <C> [BOLD] 79.7 <C> 74.2 <C> [BOLD] 50.7 <C> 72.9 <R> <C> 800 <C> Hybrid <C> 35.0 <C> [BOLD] 70.8 <C> 81.7 <C> [BOLD] 81.0 <C> 59.4 <C> [BOLD] 84.4 <C> 79.0 <C> [BOLD] 74.3 <C> 49.3 <C> 87.6 <R> <C> - <C> cmp. CBOW <C> +6.1% <C> +42.7% <C> +3% <C> +3.3% <C> +10.8% <C> +13.3% <C> +0.5% <C> +3.2% <C> -0.6% <C> -2.1% <R> <C> - <C> cmp. CMOW <C> -0.3% <C> +-0% <C> -0.4% <C> +1% <C> -3.9% <C> +1.9% <C> -0.9% <C> +0.1% <C> -2.8% <C> +20.9% <CAP> Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.
<R> <C> Method <C> SUBJ <C> CR <C> MR <C> MPQA <C> MRPC <C> TREC <C> SICK-E <C> SST2 <C> SST5 <C> STS-B <C> SICK-R <R> <C> CBOW/784 <C> 90.0 <C> [BOLD] 79.2 <C> [BOLD] 74.0 <C> 87.1 <C> 71.6 <C> 85.6 <C> 78.9 <C> 78.5 <C> 42.1 <C> 61.0 <C> [BOLD] 78.1 <R> <C> CMOW/784 <C> 87.5 <C> 73.4 <C> 70.6 <C> [BOLD] 87.3 <C> 69.6 <C> [BOLD] 88.0 <C> 77.2 <C> 74.7 <C> 37.9 <C> 56.5 <C> 76.2 <R> <C> Hybrid <C> [BOLD] 90.2 <C> 78.7 <C> 73.7 <C> [BOLD] 87.3 <C> [BOLD] 72.7 <C> 87.6 <C> [BOLD] 79.4 <C> [BOLD] 79.6 <C> [BOLD] 43.3 <C> [BOLD] 63.4 <C> 77.8 <R> <C> cmp. CBOW <C> +0.2% <C> -0.6% <C> -0.4% <C> +0.2% <C> +1.5% <C> +2.3% <C> +0.6% <C> +1.4% <C> +2.9% <C> +3.9% <C> -0.4% <R> <C> cmp. CMOW <C> +3.1% <C> +7.2% <C> +4.4% <C> +0% <C> +4.5% <C> -0.5% <C> +2.9% <C> +6.7% <C> +14.3 <C> +12.2% <C> +2.1% <CAP> Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.
<R> <C> Method <C> STS12 <C> STS13 <C> STS14 <C> STS15 <C> STS16 <R> <C> CBOW <C> 43.5 <C> [BOLD] 50.0 <C> [BOLD] 57.7 <C> [BOLD] 63.2 <C> 61.0 <R> <C> CMOW <C> 39.2 <C> 31.9 <C> 38.7 <C> 49.7 <C> 52.2 <R> <C> Hybrid <C> [BOLD] 49.6 <C> 46.0 <C> 55.1 <C> 62.4 <C> [BOLD] 62.1 <R> <C> cmp. CBOW <C> +14.6% <C> -8% <C> -4.5% <C> -1.5% <C> +1.8% <R> <C> cmp. CMOW <C> +26.5% <C> +44.2% <C> +42.4 <C> +25.6% <C> +19.0% <CAP> Table 3: Scores on unsupervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.
<R> <C> Initialization <C> SUBJ <C> CR <C> MR <C> MPQA <C> MRPC <C> TREC <C> SICK-E <C> SST2 <C> SST5 <C> STS-B <C> SICK-R <R> <C> N(0,0.1) <C> 85.6 <C> 71.5 <C> 68.4 <C> 86.2 <C> [BOLD] 71.6 <C> 86.4 <C> 73.7 <C> 72.3 <C> [BOLD] 38.2 <C> 53.7 <C> 72.7 <R> <C> Glorot <C> 86.2 <C> [BOLD] 74.4 <C> 69.5 <C> 86.5 <C> 71.4 <C> [BOLD] 88.4 <C> 75.4 <C> 73.2 <C> [BOLD] 38.2 <C> 54.1 <C> 73.6 <R> <C> Our paper <C> [BOLD] 87.5 <C> 73.4 <C> [BOLD] 70.6 <C> [BOLD] 87.3 <C> 69.6 <C> 88.0 <C> [BOLD] 77.2 <C> [BOLD] 74.7 <C> 37.9 <C> [BOLD] 56.5 <C> [BOLD] 76.2 <CAP> Table 8: Scores for initialization strategies on supervised downstream tasks.
<R> <C> Method <C> STS12 <C> STS13 <C> STS14 <C> STS15 <C> STS16 <R> <C> CMOW-C <C> 27.6 <C> 14.6 <C> 22.1 <C> 33.2 <C> 41.6 <R> <C> CMOW-R <C> [BOLD] 39.2 <C> [BOLD] 31.9 <C> [BOLD] 38.7 <C> [BOLD] 49.7 <C> [BOLD] 52.2 <R> <C> CBOW-C <C> [BOLD] 43.5 <C> 49.2 <C> [BOLD] 57.9 <C> [BOLD] 63.7 <C> [BOLD] 61.6 <R> <C> CBOW-R <C> [BOLD] 43.5 <C> [BOLD] 50.0 <C> 57.7 <C> 63.2 <C> 61.0 <CAP> Table 6: Scores for different training objectives on the unsupervised downstream tasks.
<R> <C> Method <C> Depth <C> BShift <C> SubjNum <C> Tense <C> CoordInv <C> Length <C> ObjNum <C> TopConst <C> SOMO <C> WC <R> <C> CMOW-C <C> [BOLD] 36.2 <C> 66.0 <C> 81.1 <C> 78.7 <C> 61.7 <C> [BOLD] 83.9 <C> 79.1 <C> 73.6 <C> 50.4 <C> 66.8 <R> <C> CMOW-R <C> 35.1 <C> [BOLD] 70.8 <C> [BOLD] 82.0 <C> [BOLD] 80.2 <C> [BOLD] 61.8 <C> 82.8 <C> [BOLD] 79.7 <C> [BOLD] 74.2 <C> [BOLD] 50.7 <C> [BOLD] 72.9 <R> <C> CBOW-C <C> [BOLD] 34.3 <C> [BOLD] 50.5 <C> [BOLD] 79.8 <C> [BOLD] 79.9 <C> 53.0 <C> [BOLD] 75.9 <C> [BOLD] 79.8 <C> [BOLD] 72.9 <C> 48.6 <C> 89.0 <R> <C> CBOW-R <C> 33.0 <C> 49.6 <C> 79.3 <C> 78.4 <C> [BOLD] 53.6 <C> 74.5 <C> 78.6 <C> 72.0 <C> [BOLD] 49.6 <C> [BOLD] 89.5 <CAP> Table 4: Scores for different training objectives on the linguistic probing tasks.
<R> <C> Method <C> SUBJ <C> CR <C> MR <C> MPQA <C> MRPC <C> TREC <C> SICK-E <C> SST2 <C> SST5 <C> STS-B <C> SICK-R <R> <C> CMOW-C <C> 85.9 <C> 72.1 <C> 69.4 <C> 87.0 <C> [BOLD] 71.9 <C> 85.4 <C> 74.2 <C> 73.8 <C> 37.6 <C> 54.6 <C> 71.3 <R> <C> CMOW-R <C> [BOLD] 87.5 <C> [BOLD] 73.4 <C> [BOLD] 70.6 <C> [BOLD] 87.3 <C> 69.6 <C> [BOLD] 88.0 <C> [BOLD] 77.2 <C> [BOLD] 74.7 <C> [BOLD] 37.9 <C> [BOLD] 56.5 <C> [BOLD] 76.2 <R> <C> CBOW-C <C> [BOLD] 90.0 <C> [BOLD] 79.3 <C> [BOLD] 74.6 <C> [BOLD] 87.5 <C> [BOLD] 72.9 <C> 85.0 <C> [BOLD] 80.0 <C> 78.4 <C> 41.0 <C> 60.5 <C> [BOLD] 79.2 <R> <C> CBOW-R <C> [BOLD] 90.0 <C> 79.2 <C> 74.0 <C> 87.1 <C> 71.6 <C> [BOLD] 85.6 <C> 78.9 <C> [BOLD] 78.5 <C> [BOLD] 42.1 <C> [BOLD] 61.0 <C> 78.1 <CAP> Table 5: Scores for different training objectives on the supervised downstream tasks.
<R> <C> System <C> All LOC <C> All ORG <C> All PER <C> All MISC <C> In  [ITALIC] E+ LOC <C> In  [ITALIC] E+ ORG <C> In  [ITALIC] E+ PER <C> In  [ITALIC] E+ MISC <R> <C> Name matching <C> 96.26 <C> 89.48 <C> 57.38 <C> 96.60 <C> 92.32 <C> 76.87 <C> 47.40 <C> 76.29 <R> <C> MIL <C> 57.09 <C> [BOLD] 76.30 <C> 41.35 <C> 93.35 <C> 11.90 <C> [BOLD] 47.90 <C> 27.60 <C> 53.61 <R> <C> MIL-ND <C> 57.15 <C> 77.15 <C> 35.95 <C> 92.47 <C> 12.02 <C> 49.77 <C> 20.94 <C> 47.42 <R> <C> [ITALIC] τMIL-ND <C> [BOLD] 55.15 <C> 76.56 <C> [BOLD] 34.03 <C> [BOLD] 92.15 <C> [BOLD] 11.14 <C> 51.18 <C> [BOLD] 20.59 <C> [BOLD] 40.00 <R> <C> Supervised learning <C> 55.58 <C> 61.32 <C> 24.98 <C> 89.96 <C> 8.80 <C> 14.95 <C> 7.40 <C> 29.90 <CAP> Table 3: % errors on the development set for different named entity types under two settings. (Smaller is better.)
<R> <C> System <C> All P <C> All R <C> All F1 <C> In  [ITALIC] E+ P <C> In  [ITALIC] E+ R <C> In  [ITALIC] E+ F1 <R> <C> Name matching <C> 15.03 <C> 15.03 <C> 15.03 <C> 29.13 <C> 29.13 <C> 29.13 <R> <C> MIL (model 1) <C> 35.87 <C> 35.87 <C> 35.87 ±0.72 <C> 69.38 <C> 69.38 <C> 69.38 ±1.29 <R> <C> MIL-ND (model 2) <C> 37.42 <C> [BOLD] 37.42 <C> 37.42 ±0.35 <C> 72.50 <C> [BOLD] 72.50 <C> [BOLD] 72.50 ±0.68 <R> <C> [ITALIC] τMIL-ND (model 2) <C> [BOLD] 38.91 <C> 36.73 <C> [BOLD] 37.78 ±0.26 <C> [BOLD] 73.19 <C> 71.15 <C> 72.16 ±0.48 <R> <C> Supervised learning <C> 42.90 <C> 42.90 <C> 42.90 ±0.59 <C> 83.12 <C> 83.12 <C> 83.12 ±1.15 <CAP> Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.
<R> <C> <bold>Model</bold> <C> REF ⇒ GEN <bold>ENT</bold> <C> REF ⇒ GEN <bold>CON</bold> <C> REF ⇒ GEN <bold>NEU</bold> <R> <C> S2S <C> 38.45 <C> 11.17 <C> 50.38 <R> <C> G2S-GIN <C> 49.78 <C> 9.80 <C> 40.42 <R> <C> G2S-GAT <C> 49.48 <C> 8.09 <C> 42.43 <R> <C> G2S-GGNN <C> 51.32 <C> 8.82 <C> 39.86 <R> <C> [EMPTY] <C> GEN ⇒ REF <C> GEN ⇒ REF <C> GEN ⇒ REF <R> <C> <bold>Model</bold> <C> <bold>ENT</bold> <C> <bold>CON</bold> <C> <bold>NEU</bold> <R> <C> S2S <C> 73.79 <C> 12.75 <C> 13.46 <R> <C> G2S-GIN <C> 76.27 <C> 10.65 <C> 13.08 <R> <C> G2S-GAT <C> 77.54 <C> 8.54 <C> 13.92 <R> <C> G2S-GGNN <C> 77.64 <C> 9.64 <C> 12.72 <CAP> Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.
<R> <C> <bold>Model</bold> <C> <bold>BLEU</bold> <C> <bold>METEOR</bold> <R> <C> LDC2015E86 <C> LDC2015E86 <C> LDC2015E86 <R> <C> Konstas et al. (2017) <C> 22.00 <C> - <R> <C> Song et al. (2018) <C> 23.28 <C> 30.10 <R> <C> Cao et al. (2019) <C> 23.50 <C> - <R> <C> Damonte et al.(2019) <C> 24.40 <C> 23.60 <R> <C> Guo et al. (2019) <C> <bold>25.70</bold> <C> - <R> <C> S2S <C> 22.55 ± 0.17 <C> 29.90 ± 0.31 <R> <C> G2S-GIN <C> 22.93 ± 0.20 <C> 29.72 ± 0.09 <R> <C> G2S-GAT <C> 23.42 ± 0.16 <C> 29.87 ± 0.14 <R> <C> G2S-GGNN <C> 24.32 ± 0.16 <C> <bold>30.53</bold> ± 0.30 <R> <C> LDC2017T10 <C> LDC2017T10 <C> LDC2017T10 <R> <C> Back et al. (2018) <C> 23.30 <C> - <R> <C> Song et al. (2018) <C> 24.86 <C> 31.56 <R> <C> Damonte et al.(2019) <C> 24.54 <C> 24.07 <R> <C> Cao et al. (2019) <C> 26.80 <C> - <R> <C> Guo et al. (2019) <C> 27.60 <C> - <R> <C> S2S <C> 22.73 ± 0.18 <C> 30.15 ± 0.14 <R> <C> G2S-GIN <C> 26.90 ± 0.19 <C> 32.62 ± 0.04 <R> <C> G2S-GAT <C> 26.72 ± 0.20 <C> 32.52 ± 0.02 <R> <C> G2S-GGNN <C> <bold>27.87</bold> ± 0.15 <C> <bold>33.21</bold> ± 0.15 <CAP> Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.
<R> <C> <bold>Model</bold> <C> <bold>External</bold> <C> <bold>BLEU</bold> <R> <C> Konstas et al. (2017) <C> 200K <C> 27.40 <R> <C> Song et al. (2018) <C> 200K <C> 28.20 <R> <C> Guo et al. (2019) <C> 200K <C> 31.60 <R> <C> G2S-GGNN <C> 200K <C> <bold>32.23</bold> <CAP> Table 3: Results on LDC2015E86 test set when models are trained with additional Gigaword data.
<R> <C> <bold>Model</bold> <C> <bold>BLEU</bold> <C> <bold>METEOR</bold> <C> <bold>Size</bold> <R> <C> biLSTM <C> 22.50 <C> 30.42 <C> 57.6M <R> <C> <italic>GEt</italic> + biLSTM <C> 26.33 <C> 32.62 <C> 59.6M <R> <C> <italic>GEb</italic> + biLSTM <C> 26.12 <C> 32.49 <C> 59.6M <R> <C> <italic>GEt</italic> + <italic>GEb</italic> + biLSTM <C> 27.37 <C> 33.30 <C> 61.7M <CAP> Table 4: Results of the ablation study on the LDC2017T10 development set.
<R> <C> <bold>Model</bold> <C> <bold>Graph Diameter</bold> 0-7 Δ <C> <bold>Graph Diameter</bold> 7-13 Δ <C> <bold>Graph Diameter</bold> 14-20 Δ <R> <C> S2S <C> 33.2 <C> 29.7 <C> 28.8 <R> <C> G2S-GIN <C> 35.2 +6.0% <C> 31.8 +7.4% <C> 31.5 +9.2% <R> <C> G2S-GAT <C> 35.1 +5.9% <C> 32.0 +7.8% <C> 31.5 +9.51% <R> <C> G2S-GGNN <C> 36.2 +9.0% <C> 33.0 +11.4% <C> 30.7 +6.7% <R> <C> [EMPTY] <C> <bold>Sentence Length</bold> <C> <bold>Sentence Length</bold> <C> <bold>Sentence Length</bold> <R> <C> [EMPTY] <C> 0-20 Δ <C> 20-50 Δ <C> 50-240 Δ <R> <C> S2S <C> 34.9 <C> 29.9 <C> 25.1 <R> <C> G2S-GIN <C> 36.7 +5.2% <C> 32.2 +7.8% <C> 26.5 +5.8% <R> <C> G2S-GAT <C> 36.9 +5.7% <C> 32.3 +7.9% <C> 26.6 +6.1% <R> <C> G2S-GGNN <C> 37.9 +8.5% <C> 33.3 +11.2% <C> 26.9 +6.8% <R> <C> [EMPTY] <C> <bold>Max Node Out-degree</bold> <C> <bold>Max Node Out-degree</bold> <C> <bold>Max Node Out-degree</bold> <R> <C> [EMPTY] <C> 0-3 Δ <C> 4-8 Δ <C> 9-18 Δ <R> <C> S2S <C> 31.7 <C> 30.0 <C> 23.9 <R> <C> G2S-GIN <C> 33.9 +6.9% <C> 32.1 +6.9% <C> 25.4 +6.2% <R> <C> G2S-GAT <C> 34.3 +8.0% <C> 32.0 +6.7% <C> 22.5 -6.0% <R> <C> G2S-GGNN <C> 35.0 +10.3% <C> 33.1 +10.4% <C> 22.2 -7.3% <CAP> Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.
<R> <C> <bold>Model</bold> <C> <bold>ADDED</bold> <C> <bold>MISS</bold> <R> <C> S2S <C> 47.34 <C> 37.14 <R> <C> G2S-GIN <C> 48.67 <C> 33.64 <R> <C> G2S-GAT <C> 48.24 <C> 33.73 <R> <C> G2S-GGNN <C> 48.66 <C> 34.06 <R> <C> GOLD <C> 50.77 <C> 28.35 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 8: Fraction of elements in the output that are not present in the input (ADDED) and the fraction of elements in the input graph that are missing in the generated sentence (MISS), for the test set of LDC2017T10. The token lemmas are used in the comparison. GOLD refers to the reference sentences.
<R> <C> [EMPTY] <C> Ar <C> Es <C> Fr <C> Ru <C> Zh <C> En <R> <C> POS <C> 88.7 <C> 90.0 <C> 89.6 <C> 88.6 <C> 87.4 <C> 85.2 <R> <C> SEM <C> 85.3 <C> 86.1 <C> 85.8 <C> 85.2 <C> 85.0 <C> 80.7 <CAP> Table 4: SEM and POS tagging accuracy using features extracted from the 4th NMT encoding layer, trained with different target languages on a smaller parallel corpus (200K sentences).
<R> <C> [EMPTY] <C> MFT <C> UnsupEmb <C> Word2Tag <R> <C> POS <C> 91.95 <C> 87.06 <C> 95.55 <R> <C> SEM <C> 82.00 <C> 81.11 <C> 91.41 <CAP> Table 2: POS and SEM tagging accuracy with baselines and an upper bound. MFT: most frequent tag; UnsupEmb: classifier using unsupervised word embeddings; Word2Tag: upper bound encoder-decoder.
<R> <C> [ITALIC] k <C> Ar <C> Es <C> Fr <C> Ru <C> Zh <C> En <R> <C> POS Tagging Accuracy <C> POS Tagging Accuracy <C> POS Tagging Accuracy <C> POS Tagging Accuracy <C> POS Tagging Accuracy <C> POS Tagging Accuracy <C> POS Tagging Accuracy <R> <C> 0 <C> 88.0 <C> 87.9 <C> 87.9 <C> 87.8 <C> 87.7 <C> 87.4 <R> <C> 1 <C> 92.4 <C> 91.9 <C> 92.1 <C> 92.1 <C> 91.5 <C> 89.4 <R> <C> 2 <C> 91.9 <C> 91.8 <C> 91.8 <C> 91.8 <C> 91.3 <C> 88.3 <R> <C> 3 <C> 92.0 <C> 92.3 <C> 92.1 <C> 91.6 <C> 91.2 <C> 87.9 <R> <C> 4 <C> 92.1 <C> 92.4 <C> 92.5 <C> 92.0 <C> 90.5 <C> 86.9 <R> <C> SEM Tagging Accuracy <C> SEM Tagging Accuracy <C> SEM Tagging Accuracy <C> SEM Tagging Accuracy <C> SEM Tagging Accuracy <C> SEM Tagging Accuracy <C> SEM Tagging Accuracy <R> <C> 0 <C> 81.9 <C> 81.9 <C> 81.8 <C> 81.8 <C> 81.8 <C> 81.2 <R> <C> 1 <C> 87.9 <C> 87.7 <C> 87.8 <C> 87.9 <C> 87.7 <C> 84.5 <R> <C> 2 <C> 87.4 <C> 87.5 <C> 87.4 <C> 87.3 <C> 87.2 <C> 83.2 <R> <C> 3 <C> 87.8 <C> 87.9 <C> 87.9 <C> 87.3 <C> 87.3 <C> 82.9 <R> <C> 4 <C> 88.3 <C> 88.6 <C> 88.4 <C> 88.1 <C> 87.7 <C> 82.1 <R> <C> BLEU <C> BLEU <C> BLEU <C> BLEU <C> BLEU <C> BLEU <C> BLEU <R> <C> [EMPTY] <C> 32.7 <C> 49.1 <C> 38.5 <C> 34.2 <C> 32.1 <C> 96.6 <CAP> Table 3: SEM and POS tagging accuracy using features extracted from the k-th encoding layer of 4-layered NMT models trained with different target languages. “En” column is an English autoencoder. BLEU scores are given for reference.
<R> <C> Uni <C> POS <C> 0 87.9 <C> 1 92.0 <C> 2 91.7 <C> 3 91.8 <C> 4 91.9 <R> <C> Uni <C> SEM <C> 81.8 <C> 87.8 <C> 87.4 <C> 87.6 <C> 88.2 <R> <C> Bi <C> POS <C> 87.9 <C> 93.3 <C> 92.9 <C> 93.2 <C> 92.8 <R> <C> Bi <C> SEM <C> 81.9 <C> 91.3 <C> 90.8 <C> 91.9 <C> 91.9 <R> <C> Res <C> POS <C> 87.9 <C> 92.5 <C> 91.9 <C> 92.0 <C> 92.4 <R> <C> Res <C> SEM <C> 81.9 <C> 88.2 <C> 87.5 <C> 87.6 <C> 88.5 <CAP> Table 5: POS and SEM tagging accuracy with features from different layers of 4-layer Uni/Bidirectional/Residual NMT encoders, averaged over all non-English target languages.
<R> <C> Data <C> Task <C> Protected Attribute <C> Δ <R> <C> Dial <C> Sentiment <C> Race <C> 12.2 <R> <C> [EMPTY] <C> Mention <C> Race <C> 14.3 <R> <C> PAN16 <C> Mention <C> Gender <C> 8.1 <R> <C> [EMPTY] <C> Mention <C> Age <C> 9.7 <CAP> Table 8: Attacker’s performance on different datasets. Results are on a training set 10% held-out. Δ is the difference between the attacker score and the corresponding adversary’s accuracy.
<R> <C> Data <C> Task <C> Accuracy <R> <C> Dial <C> Sentiment <C> 67.4 <R> <C> [EMPTY] <C> Mention <C> 81.2 <R> <C> [EMPTY] <C> [ITALIC] Race <C> 83.9 <R> <C> PAN16 <C> Mention <C> 77.5 <R> <C> [EMPTY] <C> [ITALIC] Gender <C> 67.7 <R> <C> [EMPTY] <C> [ITALIC] Age <C> 64.8 <CAP> Table 1: Accuracies when training directly towards a single task.
<R> <C> Data <C> Task <C> Protected Attribute <C> Balanced Task Acc <C> Balanced Leakage <C> Unbalanced Task Acc <C> Unbalanced Leakage <R> <C> Dial <C> Sentiment <C> Race <C> 67.4 <C> 64.5 <C> 79.5 <C> 73.5 <R> <C> [EMPTY] <C> Mention <C> Race <C> 81.2 <C> 71.5 <C> 86.0 <C> 73.8 <R> <C> PAN16 <C> Mention <C> Gender <C> 77.5 <C> 60.1 <C> 76.8 <C> 64.0 <R> <C> [EMPTY] <C> [EMPTY] <C> Age <C> 74.7 <C> 59.4 <C> 77.5 <C> 59.7 <CAP> Table 2: Protected attribute leakage: balanced & unbalanced data splits.
<R> <C> Data <C> Task <C> Protected Attribute <C> Task Acc <C> Leakage <C> Δ <R> <C> Dial <C> Sentiment <C> Race <C> 64.7 <C> 56.0 <C> 5.0 <R> <C> [EMPTY] <C> Mention <C> Race <C> 81.5 <C> 63.1 <C> 9.2 <R> <C> PAN16 <C> Mention <C> Gender <C> 75.6 <C> 58.5 <C> 8.0 <R> <C> [EMPTY] <C> Mention <C> Age <C> 72.5 <C> 57.3 <C> 6.9 <CAP> Table 3: Performances on different datasets with an adversarial training. Δ is the difference between the attacker score and the corresponding adversary’s accuracy.
<R> <C> [EMPTY] <C> [EMPTY] <C> Embedding Leaky <C> Embedding Guarded <R> <C> RNN <C> Leaky <C> 64.5 <C> 67.8 <R> <C> RNN <C> Guarded <C> 59.3 <C> 54.8 <CAP> Table 6: Accuracies of the protected attribute with different encoders.
<R> <C> Model <C> Model <C> #Params <C> PTB Base <C> PTB +Finetune <C> PTB +Dynamic <C> WT2 Base <C> WT2 +Finetune <C> WT2 +Dynamic <R> <C> Yang et al. ( 2018 ) <C> Yang et al. ( 2018 ) <C> 22M <C> 55.97 <C> 54.44 <C> 47.69 <C> 63.33 <C> 61.45 <C> 40.68 <R> <C> This <C> LSTM <C> 22M <C> 63.78 <C> 62.12 <C> [BOLD] 53.11 <C> [BOLD] 69.78 <C> [BOLD] 68.68 <C> [BOLD] 44.60 <R> <C> This <C> GRU <C> 17M <C> 69.09 <C> 67.61 <C> 60.21 <C> 73.37 <C> 73.05 <C> 49.77 <R> <C> This <C> ATR <C> 9M <C> 66.24 <C> 65.86 <C> 58.29 <C> 75.36 <C> 73.35 <C> 48.65 <R> <C> Work <C> SRU <C> 13M <C> 69.64 <C> 65.29 <C> 60.97 <C> 85.15 <C> 84.97 <C> 57.97 <R> <C> [EMPTY] <C> LRN <C> 11M <C> [BOLD] 61.26 <C> [BOLD] 61.00 <C> 54.45 <C> 69.91 <C> 68.86 <C> 46.97 <CAP> Table 5: Test perplexity on PTB and WT2 language modeling task. “#Params”: the parameter number in PTB task. Finetune: fintuning the model after convergence. Dynamic dynamic evaluation. Lower perplexity indicates better performance.
<R> <C> Model <C> Model <C> #Params <C> Base ACC <C> Base Time <C> +LN ACC <C> +LN Time <C> +BERT ACC <C> +BERT Time <C> +LN+BERT ACC <C> +LN+BERT Time <R> <C> Rocktäschel et al. ( 2016 ) <C> Rocktäschel et al. ( 2016 ) <C> 250K <C> 83.50 <C> - <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> This <C> LSTM <C> 8.36M <C> 84.27 <C> 0.262 <C> 86.03 <C> 0.432 <C> 89.95 <C> 0.544 <C> [BOLD] 90.49 <C> 0.696 <R> <C> This <C> GRU <C> 6.41M <C> [BOLD] 85.71 <C> 0.245 <C> [BOLD] 86.05 <C> 0.419 <C> [BOLD] 90.29 <C> 0.529 <C> 90.10 <C> 0.695 <R> <C> This <C> ATR <C> 2.87M <C> 84.88 <C> 0.210 <C> 85.81 <C> 0.307 <C> 90.00 <C> 0.494 <C> 90.28 <C> 0.580 <R> <C> Work <C> SRU <C> 5.48M <C> 84.28 <C> 0.258 <C> 85.32 <C> 0.283 <C> 89.98 <C> 0.543 <C> 90.09 <C> 0.555 <R> <C> [EMPTY] <C> LRN <C> 4.25M <C> 84.88 <C> [BOLD] 0.209 <C> 85.06 <C> [BOLD] 0.223 <C> 89.98 <C> [BOLD] 0.488 <C> 89.93 <C> [BOLD] 0.506 <CAP> Table 1: Test accuracy (ACC) on SNLI task. “#Params”: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.
<R> <C> Model <C> Model <C> #Params <C> AmaPolar ERR <C> AmaPolar Time <C> Yahoo ERR <C> Yahoo Time <C> AmaFull ERR <C> AmaFull Time <C> YelpPolar ERR <C> YelpPolar Time <R> <C> Zhang et al. ( 2015 ) <C> Zhang et al. ( 2015 ) <C> - <C> 6.10 <C> - <C> 29.16 <C> - <C> 40.57 <C> - <C> 5.26 <C> - <R> <C> This <C> LSTM <C> 227K <C> [BOLD] 4.37 <C> 0.947 <C> [BOLD] 24.62 <C> 1.332 <C> 37.22 <C> 1.003 <C> 3.58 <C> 1.362 <R> <C> This <C> GRU <C> 176K <C> 4.39 <C> 0.948 <C> 24.68 <C> 1.242 <C> [BOLD] 37.20 <C> 0.982 <C> [BOLD] 3.47 <C> 1.230 <R> <C> This <C> ATR <C> 74K <C> 4.78 <C> 0.867 <C> 25.33 <C> 1.117 <C> 38.54 <C> 0.836 <C> 4.00 <C> 1.124 <R> <C> Work <C> SRU <C> 194K <C> 4.95 <C> 0.919 <C> 24.78 <C> 1.394 <C> 38.23 <C> 0.907 <C> 3.99 <C> 1.310 <R> <C> [EMPTY] <C> LRN <C> 151K <C> 4.98 <C> [BOLD] 0.731 <C> 25.07 <C> [BOLD] 1.038 <C> 38.42 <C> [BOLD] 0.788 <C> 3.98 <C> [BOLD] 1.022 <CAP> Table 2: Test error (ERR) on document classification task. “#Params”: the parameter number in AmaPolar task. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti.
<R> <C> Model <C> #Params <C> BLEU <C> Train <C> Decode <R> <C> GNMT <C> - <C> 24.61 <C> - <C> - <R> <C> GRU <C> 206M <C> 26.28 <C> 2.67 <C> 45.35 <R> <C> ATR <C> 122M <C> 25.70 <C> 1.33 <C> [BOLD] 34.40 <R> <C> SRU <C> 170M <C> 25.91 <C> 1.34 <C> 42.84 <R> <C> LRN <C> 143M <C> 26.26 <C> [BOLD] 0.99 <C> 36.50 <R> <C> oLRN <C> 164M <C> [BOLD] 26.73 <C> 1.15 <C> 40.19 <CAP> Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.
<R> <C> Model <C> #Params <C> Base <C> +Elmo <R> <C> rnet* <C> - <C> 71.1/79.5 <C> -/- <R> <C> LSTM <C> 2.67M <C> [BOLD] 70.46/78.98 <C> 75.17/82.79 <R> <C> GRU <C> 2.31M <C> 70.41/ [BOLD] 79.15 <C> 75.81/83.12 <R> <C> ATR <C> 1.59M <C> 69.73/78.70 <C> 75.06/82.76 <R> <C> SRU <C> 2.44M <C> 69.27/78.41 <C> 74.56/82.50 <R> <C> LRN <C> 2.14M <C> 70.11/78.83 <C> [BOLD] 76.14/ [BOLD] 83.83 <CAP> Table 4: Exact match/F1-score on SQuad dataset. “#Params”: the parameter number of Base. rnet*: results published by Wang et al. (2017).
<R> <C> Model <C> #Params <C> NER <R> <C> LSTM* <C> - <C> 90.94 <R> <C> LSTM <C> 245K <C> [BOLD] 89.61 <R> <C> GRU <C> 192K <C> 89.35 <R> <C> ATR <C> 87K <C> 88.46 <R> <C> SRU <C> 161K <C> 88.89 <R> <C> LRN <C> 129K <C> 88.56 <CAP> Table 6: F1 score on CoNLL-2003 English NER task. “#Params”: the parameter number in NER task. LSTM* denotes the reported result Lample et al. (2016).
<R> <C> Model <C> SNLI <C> PTB <R> <C> LRN <C> [BOLD] 85.06 <C> [BOLD] 61.26 <R> <C> gLRN <C> 84.72 <C> 92.49 <R> <C> eLRN <C> 83.56 <C> 169.81 <CAP> Table 7: Test accuracy on SNLI task with Base+LN setting and test perplexity on PTB task with Base setting.
<R> <C> [EMPTY] <C> [ITALIC] w/ System Retrieval  [BOLD] B-2 <C> [ITALIC] w/ System Retrieval  [BOLD] B-4 <C> [ITALIC] w/ System Retrieval  [BOLD] R-2 <C> [ITALIC] w/ System Retrieval  [BOLD] MTR <C> [ITALIC] w/ System Retrieval  [BOLD] #Word <C> [ITALIC] w/ System Retrieval  [BOLD] #Sent <C> [ITALIC] w/ Oracle Retrieval  [BOLD] B-2 <C> [ITALIC] w/ Oracle Retrieval  [BOLD] B-4 <C> [ITALIC] w/ Oracle Retrieval  [BOLD] R-2 <C> [ITALIC] w/ Oracle Retrieval  [BOLD] MTR <C> [ITALIC] w/ Oracle Retrieval  [BOLD] #Word <C> [ITALIC] w/ Oracle Retrieval  [BOLD] #Sent <R> <C> Human <C> - <C> - <C> - <C> - <C> 66 <C> 22 <C> - <C> - <C> - <C> - <C> 66 <C> 22 <R> <C> Retrieval <C> 7.55 <C> 1.11 <C> 8.64 <C> 14.38 <C> 123 <C> 23 <C> 10.97 <C> 3.05 <C> 23.49 <C> 20.08 <C> 140 <C> 21 <R> <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [EMPTY] <C> [EMPTY] <R> <C> Seq2seq <C> 6.92 <C> 2.13 <C> 13.02 <C> 15.08 <C> 68 <C> 15 <C> 6.92 <C> 2.13 <C> 13.02 <C> 15.08 <C> 68 <C> 15 <R> <C> Seq2seqAug <C> 8.26 <C> 2.24 <C> 13.79 <C> 15.75 <C> 78 <C> 14 <C> 10.98 <C> 4.41 <C> 22.97 <C> 19.62 <C> 71 <C> 14 <R> <C> [ITALIC] w/o psg <C> 7.94 <C> 2.28 <C> 10.13 <C> 15.71 <C> 75 <C> 12 <C> 9.89 <C> 3.34 <C> 14.20 <C> 18.40 <C> 66 <C> 12 <R> <C> H&W Hua and Wang ( 2018 ) <C> 3.64 <C> 0.92 <C> 8.83 <C> 11.78 <C> 51 <C> 12 <C> 8.51 <C> 2.86 <C> 18.89 <C> 17.18 <C> 58 <C> 12 <R> <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [EMPTY] <C> [EMPTY] <R> <C> CANDELA <C> 12.02∗ <C> [BOLD] 2.99∗ <C> [BOLD] 14.93∗ <C> [BOLD] 16.92∗ <C> 119 <C> 22 <C> 15.80∗ <C> [BOLD] 5.00∗ <C> [BOLD] 23.75 <C> [BOLD] 20.18 <C> 116 <C> 22 <R> <C> [ITALIC] w/o psg <C> [BOLD] 12.33∗ <C> 2.86∗ <C> 14.53∗ <C> 16.60∗ <C> 123 <C> 23 <C> [BOLD] 16.33∗ <C> 4.98∗ <C> 23.65 <C> 19.94 <C> 123 <C> 23 <CAP> Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. ∗: statistically significantly better than all comparisons (randomization approximation test Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.
<R> <C> [EMPTY] <C> [ITALIC] K 100 <C> [ITALIC] K 500 <C> [ITALIC] K 1000 <C> [ITALIC] K 2000 <R> <C> Human <C> 44.1 <C> 25.8 <C> 18.5 <C> 12.0 <R> <C> Retrieval <C> 50.6 <C> 33.3 <C> 26.0 <C> 18.6 <R> <C> Seq2seq <C> 25.0 <C> 7.5 <C> 3.2 <C> 1.2 <R> <C> Seq2seqAug <C> 28.2 <C> 9.2 <C> 4.6 <C> 1.8 <R> <C> H&W Hua and Wang ( 2018 ) <C> 38.6 <C> 24.0 <C> 19.5 <C> 16.2 <R> <C> CANDELA <C> 30.0 <C> 10.5 <C> 5.3 <C> 2.3 <CAP> Table 4: Human evaluation on grammaticality (Gram), appropriateness (Appr), and content richness (Cont.), on a scale of 1 to 5 (best). The best result among automatic systems is highlighted in bold, with statistical significance marked with ∗ (approximation randomization test, p<0.0005). The highest standard deviation among all is 1.0. Top-1/2: % of evaluations a system being ranked in top 1 or 2 for overall quality.
<R> <C> [EMPTY] <C> Lang <C> Corpus <C> Patt <C> DSim <C> SLQS <C> TF <C> DF <C> DocSub <C> HClust <R> <C> P <C> EN <C> Europarl <C> [BOLD] 0.1173 <C> 0.0366 <C> 0.0503 <C> 0.0554 <C> 0.0548 <C> 0.0443 <C> 0.0761 <R> <C> P <C> EN <C> Ted Talks <C> [BOLD] 0.1125 <C> 0.0301 <C> 0.0382 <C> 0.0425 <C> 0.0441 <C> 0.0710 <C> 0.0664 <R> <C> P <C> PT <C> Europarl <C> 0.5163 <C> 0.3330 <C> 0.5257 <C> 0.6109 <C> 0.5984 <C> [BOLD] 0.7311 <C> 0.5676 <R> <C> [EMPTY] <C> PT <C> Ted Talks <C> 0.5387 <C> 0.2907 <C> 0.5300 <C> 0.6117 <C> 0.6159 <C> [BOLD] 0.6533 <C> 0.5656 <R> <C> R <C> EN <C> Europarl <C> 0.0396 <C> 0.3999 <C> 0.5499 <C> [BOLD] 0.6045 <C> 0.5887 <C> 0.0023 <C> 0.0017 <R> <C> R <C> EN <C> Ted Talks <C> 0.0018 <C> 0.4442 <C> 0.5377 <C> 0.5657 <C> [BOLD] 0.6077 <C> 0.2666 <C> 0.0019 <R> <C> R <C> PT <C> Europarl <C> 0.0111 <C> 0.3554 <C> 0.5795 <C> [BOLD] 0.6727 <C> 0.5184 <C> 0.0053 <C> 0.0012 <R> <C> [EMPTY] <C> PT <C> Ted Talks <C> 0.0004 <C> 0.3142 <C> 0.5484 <C> [BOLD] 0.6877 <C> 0.5515 <C> 0.4706 <C> 0.0011 <R> <C> F <C> EN <C> Europarl <C> 0.0591 <C> 0.0671 <C> 0.0922 <C> [BOLD] 0.1015 <C> 0.1003 <C> 0.0044 <C> 0.0033 <R> <C> F <C> EN <C> Ted Talks <C> 0.0035 <C> 0.0564 <C> 0.0713 <C> 0.0791 <C> 0.0822 <C> [BOLD] 0.1121 <C> 0.0037 <R> <C> F <C> PT <C> Europarl <C> 0.0217 <C> 0.3438 <C> 0.5513 <C> [BOLD] 0.6403 <C> 0.5555 <C> 0.0105 <C> 0.0024 <R> <C> [EMPTY] <C> PT <C> Ted Talks <C> 0.0008 <C> 0.3020 <C> 0.5390 <C> [BOLD] 0.6475 <C> 0.5819 <C> 0.5471 <C> 0.0022 <CAP> Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.
<R> <C> [EMPTY] <C> Lang <C> Corpus <C> Patt <C> DSim <C> SLQS <C> TF <C> DF <C> DocSub <C> HClust <R> <C> P <C> EN <C> Europarl <C> [BOLD] 0.1192 <C> 0.0083 <C> 0.0137 <C> 0.0150 <C> 0.0150 <C> 0.0445 <C> 0.0326 <R> <C> P <C> EN <C> Ted Talks <C> [BOLD] 0.1022 <C> 0.0069 <C> 0.0060 <C> 0.0092 <C> 0.0090 <C> 0.0356 <C> 0.0162 <R> <C> P <C> PT <C> Europarl <C> 0.5710 <C> 0.1948 <C> 0.3855 <C> 0.5474 <C> 0.4485 <C> [BOLD] 0.8052 <C> 0.4058 <R> <C> [EMPTY] <C> PT <C> Ted Talks <C> [BOLD] 0.6304 <C> 0.1870 <C> 0.3250 <C> 0.5312 <C> 0.4576 <C> 0.6064 <C> 0.3698 <R> <C> R <C> EN <C> Europarl <C> 0.0037 <C> 0.3278 <C> 0.5941 <C> 0.6486 <C> [BOLD] 0.6490 <C> 0.0017 <C> 0.0003 <R> <C> R <C> EN <C> Ted Talks <C> 0.0002 <C> 0.1486 <C> 0.4332 <C> [BOLD] 0.6467 <C> 0.6332 <C> 0.0967 <C> 0.0003 <R> <C> R <C> PT <C> Europarl <C> 0.0002 <C> 0.1562 <C> 0.5157 <C> [BOLD] 0.7255 <C> 0.5932 <C> 0.0032 <C> 0.0001 <R> <C> [EMPTY] <C> PT <C> Ted Talks <C> 2.10-5 <C> 0.0507 <C> 0.4492 <C> [BOLD] 0.7000 <C> 0.5887 <C> 0.1390 <C> 0.0002 <R> <C> F <C> EN <C> Europarl <C> 0.0073 <C> 0.0162 <C> 0.0268 <C> [BOLD] 0.0293 <C> [BOLD] 0.0293 <C> 0.0033 <C> 0.0006 <R> <C> F <C> EN <C> Ted Talks <C> 0.0004 <C> 0.0132 <C> 0.0118 <C> 0.0181 <C> 0.0179 <C> [BOLD] 0.0520 <C> 0.0005 <R> <C> F <C> PT <C> Europarl <C> 0.0005 <C> 0.1733 <C> 0.4412 <C> [BOLD] 0.6240 <C> 0.5109 <C> 0.0064 <C> 0.0002 <R> <C> [EMPTY] <C> PT <C> Ted Talks <C> 4.10-5 <C> 0.0798 <C> 0.3771 <C> [BOLD] 0.6040 <C> 0.5149 <C> 0.2261 <C> 0.0004 <CAP> Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.
<R> <C> [EMPTY] <C> Lang <C> Corpus <C> Patt <C> DSim <C> SLQS <C> TF <C> DF <C> DocSub <C> HClust <R> <C> P <C> EN <C> Europarl <C> [BOLD] 0.1038 <C> 0.0170 <C> 0.0490 <C> 0.0641 <C> 0.0641 <C> 0.0613 <C> 0.0761 <R> <C> P <C> EN <C> Ted Talks <C> [BOLD] 0.1282 <C> 0.0291 <C> 0.0410 <C> 0.0270 <C> 0.0270 <C> 0.1154 <C> 0.0661 <R> <C> P <C> PT <C> Europarl <C> 0.6185 <C> 0.3744 <C> 0.4144 <C> 0.4394 <C> 0.4394 <C> [BOLD] 0.7553 <C> 0.5676 <R> <C> [EMPTY] <C> PT <C> Ted Talks <C> 0.6308 <C> 0.4124 <C> 0.4404 <C> 0.4515 <C> 0.4945 <C> [BOLD] 0.8609 <C> 0.5295 <R> <C> R <C> EN <C> Europarl <C> [BOLD] 0.0021 <C> 0.0004 <C> 0.0011 <C> 0.0014 <C> 0.0014 <C> 0.0013 <C> 0.0017 <R> <C> R <C> EN <C> Ted Talks <C> 0.0011 <C> 0.0008 <C> 0.0011 <C> 0.0008 <C> 0.0008 <C> [BOLD] 0.0030 <C> 0.0018 <R> <C> R <C> PT <C> Europarl <C> 0.0012 <C> 0.0008 <C> 0.0009 <C> 0.0010 <C> 0.0010 <C> [BOLD] 0.0016 <C> 0.0012 <R> <C> [EMPTY] <C> PT <C> Ted Talks <C> 0.0003 <C> 0.0009 <C> 0.0009 <C> 0.0010 <C> 0.0010 <C> [BOLD] 0.0017 <C> 0.0011 <R> <C> F <C> EN <C> Europarl <C> [BOLD] 0.0041 <C> 0.0007 <C> 0.0021 <C> 0.0027 <C> 0.0027 <C> 0.0026 <C> 0.0033 <R> <C> F <C> EN <C> Ted Talks <C> 0.0022 <C> 0.0016 <C> 0.0022 <C> 0.0015 <C> 0.0015 <C> [BOLD] 0.0058 <C> 0.0036 <R> <C> F <C> PT <C> Europarl <C> 0.0024 <C> 0.0016 <C> 0.0018 <C> 0.0019 <C> 0.0019 <C> [BOLD] 0.0031 <C> 0.0023 <R> <C> [EMPTY] <C> PT <C> Ted Talks <C> 0.0005 <C> 0.0018 <C> 0.0018 <C> 0.0020 <C> 0.0021 <C> [BOLD] 0.0034 <C> 0.0022 <CAP> Table 5: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts and selecting the best parent.
<R> <C> Corpus <C> Metric <C> Patt <C> DSim <C> SLQS <C> TF <C> DF <C> DocSub <C> HClust <R> <C> Europarl <C> TotalTerms: <C> 957 <C> 1,000 <C> 1,000 <C> 1,000 <C> 1,000 <C> 836 <C> 1,000 <R> <C> Europarl <C> TotalRoots: <C> 44 <C> 1 <C> 1 <C> 1 <C> 1 <C> 43 <C> 1 <R> <C> Europarl <C> NumberRels: <C> 1,588 <C> 1,025 <C> 1,028 <C> 1,185 <C> 1,103 <C> 1,184 <C> 999 <R> <C> Europarl <C> MaxDepth: <C> 21 <C> 921 <C> 901 <C> 788 <C> 835 <C> 8 <C> 15 <R> <C> Europarl <C> MinDepth: <C> 1 <C> 921 <C> 901 <C> 788 <C> 835 <C> 1 <C> 1 <R> <C> Europarl <C> AvgDepth: <C> 11.82 <C> 921 <C> 901 <C> 788 <C> 835 <C> 3.05 <C> 8.46 <R> <C> Europarl <C> DepthCohesion: <C> 1.78 <C> 1 <C> 1 <C> 1 <C> 1 <C> 2.62 <C> 1.77 <R> <C> Europarl <C> MaxWidth: <C> 20 <C> 2 <C> 3 <C> 4 <C> 3 <C> 88 <C> 41 <R> <C> Europarl <C> MinWidth: <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <R> <C> Europarl <C> AvgWidth: <C> 1.99 <C> 1.03 <C> 1.03 <C> 1.19 <C> 1.10 <C> 4.20 <C> 2.38 <R> <C> TED Talks <C> TotalTerms: <C> 476 <C> 1,000 <C> 1,000 <C> 1,000 <C> 1,000 <C> 1,000 <C> 1,000 <R> <C> TED Talks <C> TotalRoots: <C> 164 <C> 2 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <R> <C> TED Talks <C> NumberRels: <C> 521 <C> 1,029 <C> 1,331 <C> 3,025 <C> 3,438 <C> 3,802 <C> 1,009 <R> <C> TED Talks <C> MaxDepth: <C> 16 <C> 915 <C> 658 <C> 454 <C> 395 <C> 118 <C> 12 <R> <C> TED Talks <C> MinDepth: <C> 1 <C> 913 <C> 658 <C> 454 <C> 395 <C> 110 <C> 1 <R> <C> TED Talks <C> AvgDepth: <C> 5.82 <C> 914 <C> 658 <C> 454 <C> 395 <C> 112.24 <C> 5.95 <R> <C> TED Talks <C> DepthCohesion: <C> 2.75 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1.05 <C> 2.02 <R> <C> TED Talks <C> MaxWidth: <C> 25 <C> 2 <C> 77 <C> 13 <C> 12 <C> 66 <C> 98 <R> <C> TED Talks <C> MinWidth: <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <R> <C> TED Talks <C> AvgWidth: <C> 1.83 <C> 1.03 <C> 1.36 <C> 3.03 <C> 3.44 <C> 6.64 <C> 2.35 <CAP> Table 6: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in English.
<R> <C> Corpus <C> Metric <C> Patt <C> DSim <C> SLQS <C> TF <C> DF <C> DocSub <C> HClust <R> <C> Europarl <C> TotalTerms: <C> 980 <C> 1,000 <C> 1,000 <C> 1,000 <C> 1,000 <C> 996 <C> 1,000 <R> <C> Europarl <C> TotalRoots: <C> 79 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <R> <C> Europarl <C> NumberRels: <C> 1,527 <C> 1,031 <C> 1,049 <C> 1,185 <C> 1,093 <C> 1,644 <C> 999 <R> <C> Europarl <C> MaxDepth: <C> 19 <C> 902 <C> 894 <C> 784 <C> 849 <C> 6 <C> 10 <R> <C> Europarl <C> MinDepth: <C> 1 <C> 902 <C> 894 <C> 784 <C> 849 <C> 1 <C> 1 <R> <C> Europarl <C> AvgDepth: <C> 9.43 <C> 902 <C> 894 <C> 784 <C> 849 <C> 2.73 <C> 4.29 <R> <C> Europarl <C> DepthCohesion: <C> 2.02 <C> 1 <C> 1 <C> 1 <C> 1 <C> 2.19 <C> 2.33 <R> <C> Europarl <C> MaxWidth: <C> 27 <C> 3 <C> 3 <C> 4 <C> 3 <C> 201 <C> 58 <R> <C> Europarl <C> MinWidth: <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <R> <C> Europarl <C> AvgWidth: <C> 1.98 <C> 1.03 <C> 1.05 <C> 1.19 <C> 1.09 <C> 6.25 <C> 2.55 <R> <C> TED Talks <C> TotalTerms: <C> 296 <C> 1,000 <C> 1,000 <C> 1,000 <C> 1,000 <C> 1,000 <C> 1,000 <R> <C> TED Talks <C> TotalRoots: <C> 101 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <R> <C> TED Talks <C> NumberRels: <C> 291 <C> 1,045 <C> 1,229 <C> 3,637 <C> 4,284 <C> 2,875 <C> 999 <R> <C> TED Talks <C> MaxDepth: <C> 10 <C> 860 <C> 727 <C> 388 <C> 354 <C> 252 <C> 17 <R> <C> TED Talks <C> MinDepth: <C> 1 <C> 860 <C> 727 <C> 388 <C> 354 <C> 249 <C> 1 <R> <C> TED Talks <C> AvgDepth: <C> 3.94 <C> 860 <C> 727 <C> 388 <C> 354 <C> 250.43 <C> 6.16 <R> <C> TED Talks <C> DepthCohesion: <C> 2.54 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1.01 <C> 2.76 <R> <C> TED Talks <C> MaxWidth: <C> 37 <C> 3 <C> 79 <C> 18 <C> 13 <C> 9 <C> 41 <R> <C> TED Talks <C> MinWidth: <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <R> <C> TED Talks <C> AvgWidth: <C> 1.79 <C> 1.05 <C> 1.23 <C> 3.64 <C> 4.29 <C> 2.94 <C> 2.37 <CAP> Table 7: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in Portuguese.
<R> <C> Model <C> baseline <C> QT <C> S  [ITALIC] R0 <C> S  [ITALIC] R1 <C> S  [ITALIC] R2 <C> S  [ITALIC] R3 <C> D <R> <C> LF  <C> 57.21 <C> 58.97 <C> 67.82 <C> 71.27 <C> 72.04 <C> 72.36 <C> 72.65 <R> <C> LF +P1 <C> 61.88 <C> 62.87 <C> 69.47 <C> 72.16 <C> 72.85 <C> 73.42 <C> [BOLD] 73.63 <CAP> Table 1: Performance (NDCG%) comparison for the experiments of applying our principles on the validation set of VisDial v1.0. LF is the enhanced version as we mentioned. QT, S and D denote question type, answer score sampling, and hidden dictionary learning, respectively. R0, R1, R2, R3 denote regressive loss, weighted softmax loss, binary sigmoid loss ,and generalized ranking loss, respectively.
<R> <C> Model <C> LF  <C> HCIAE  <C> CoAtt  <C> RvA  <R> <C> baseline <C> 57.21 <C> 56.98 <C> 56.46 <C> 56.74 <R> <C> +P1 <C> 61.88 <C> 60.12 <C> 60.27 <C> 61.02 <R> <C> +P2 <C> 72.65 <C> 71.50 <C> 71.41 <C> 71.44 <R> <C> +P1+P2 <C> [BOLD] 73.63 <C> 71.99 <C> 71.87 <C> 72.88 <CAP> Table 2: Performance (NDCG%) of ablative studies on different models on VisDial v1.0 validation set. P2 indicates the most effective one (i.e., hidden dictionary learning) shown in Table 1. Note that only applying P2 is implemented by the implementations in Section 5 with the history shortcut.
<R> <C> Metrics <C> cs-en <C> de-en <C> fi-en <C> lv-en <R> <C> RUSE <C> 0.624 <C> 0.644 <C> 0.750 <C> 0.697 <R> <C> Hmd-F1 + BERT <C> 0.655 <C> 0.681 <C> 0.821 <C> 0.712 <R> <C> Hmd-Recall + BERT <C> 0.651 <C> 0.658 <C> 0.788 <C> 0.681 <R> <C> Hmd-Prec + BERT <C> 0.624 <C> 0.669 <C> 0.817 <C> 0.707 <R> <C> Wmd-unigram + BERT <C> 0.651 <C> 0.686 <C> <bold>0.823</bold> <C> 0.710 <R> <C> Wmd-bigram + BERT <C> <bold>0.665</bold> <C> <bold>0.688</bold> <C> 0.821 <C> <bold>0.712</bold> <CAP> Table 5: Comparison on hard and soft alignments.
<R> <C> Setting <C> Metrics <C> <bold>Direct Assessment</bold> cs-en <C> <bold>Direct Assessment</bold> de-en <C> <bold>Direct Assessment</bold> fi-en <C> <bold>Direct Assessment</bold> lv-en <C> <bold>Direct Assessment</bold> ru-en <C> <bold>Direct Assessment</bold> tr-en <C> <bold>Direct Assessment</bold> zh-en <C> <bold>Direct Assessment</bold> Average <R> <C> Baselines <C> METEOR++ <C> 0.552 <C> 0.538 <C> 0.720 <C> 0.563 <C> 0.627 <C> 0.626 <C> 0.646 <C> 0.610 <R> <C> Baselines <C> RUSE(*) <C> 0.624 <C> 0.644 <C> 0.750 <C> 0.697 <C> 0.673 <C> 0.716 <C> 0.691 <C> 0.685 <R> <C> Baselines <C> BERTScore-F1 <C> 0.670 <C> 0.686 <C> 0.820 <C> 0.710 <C> 0.729 <C> 0.714 <C> 0.704 <C> 0.719 <R> <C> Sent-Mover <C> Smd + W2V <C> 0.438 <C> 0.505 <C> 0.540 <C> 0.442 <C> 0.514 <C> 0.456 <C> 0.494 <C> 0.484 <R> <C> Sent-Mover <C> Smd + ELMO + PMeans <C> 0.569 <C> 0.558 <C> 0.732 <C> 0.525 <C> 0.581 <C> 0.620 <C> 0.584 <C> 0.595 <R> <C> Sent-Mover <C> Smd + BERT + PMeans <C> 0.607 <C> 0.623 <C> 0.770 <C> 0.639 <C> 0.667 <C> 0.641 <C> 0.619 <C> 0.652 <R> <C> Sent-Mover <C> Smd + BERT + MNLI + PMeans <C> 0.616 <C> 0.643 <C> 0.785 <C> 0.660 <C> 0.664 <C> 0.668 <C> 0.633 <C> 0.667 <R> <C> Word-Mover <C> Wmd-1 + W2V <C> 0.392 <C> 0.463 <C> 0.558 <C> 0.463 <C> 0.456 <C> 0.485 <C> 0.481 <C> 0.471 <R> <C> Word-Mover <C> Wmd-1 + ELMO + PMeans <C> 0.579 <C> 0.588 <C> 0.753 <C> 0.559 <C> 0.617 <C> 0.679 <C> 0.645 <C> 0.631 <R> <C> Word-Mover <C> Wmd-1 + BERT + PMeans <C> 0.662 <C> 0.687 <C> 0.823 <C> 0.714 <C> 0.735 <C> 0.734 <C> 0.719 <C> 0.725 <R> <C> Word-Mover <C> Wmd-1 + BERT + MNLI + PMeans <C> 0.670 <C> 0.708 <C> <bold>0.835</bold> <C> <bold>0.746</bold> <C> <bold>0.738</bold> <C> 0.762 <C> <bold>0.744</bold> <C> <bold>0.743</bold> <R> <C> Word-Mover <C> Wmd-2 + BERT + MNLI + PMeans <C> <bold>0.679</bold> <C> <bold>0.710</bold> <C> 0.832 <C> 0.745 <C> 0.736 <C> <bold>0.763</bold> <C> 0.740 <C> <bold>0.743</bold> <CAP> Table 1: Absolute Pearson correlations with segment-level human judgments in 7 language pairs on WMT17 dataset.
<R> <C> Setting <C> Metrics <C> BAGEL <bold>Inf</bold> <C> BAGEL <bold>Nat</bold> <C> BAGEL <bold>Qual</bold> <C> SFHOTEL <bold>Inf</bold> <C> SFHOTEL <bold>Nat</bold> <C> SFHOTEL <bold>Qual</bold> <R> <C> Baselines <C> BLEU-1 <C> 0.225 <C> 0.141 <C> 0.113 <C> 0.107 <C> 0.175 <C> 0.069 <R> <C> Baselines <C> BLEU-2 <C> 0.211 <C> 0.152 <C> 0.115 <C> 0.097 <C> 0.174 <C> 0.071 <R> <C> Baselines <C> METEOR <C> 0.251 <C> 0.127 <C> 0.116 <C> 0.111 <C> 0.148 <C> 0.082 <R> <C> Baselines <C> BERTScore-F1 <C> 0.267 <C> 0.210 <C> <bold>0.178</bold> <C> 0.163 <C> 0.193 <C> 0.118 <R> <C> Sent-Mover <C> SMD + W2V <C> 0.024 <C> 0.074 <C> 0.078 <C> 0.022 <C> 0.025 <C> 0.011 <R> <C> Sent-Mover <C> SMD + ELMO + PMeans <C> 0.251 <C> 0.171 <C> 0.147 <C> 0.130 <C> 0.176 <C> 0.096 <R> <C> Sent-Mover <C> SMD + BERT + PMeans <C> 0.290 <C> 0.163 <C> 0.121 <C> 0.192 <C> 0.223 <C> 0.134 <R> <C> Sent-Mover <C> SMD + BERT + MNLI + PMeans <C> 0.280 <C> 0.149 <C> 0.120 <C> 0.205 <C> 0.239 <C> 0.147 <R> <C> Word-Mover <C> Wmd-1 + W2V <C> 0.222 <C> 0.079 <C> 0.123 <C> 0.074 <C> 0.095 <C> 0.021 <R> <C> Word-Mover <C> Wmd-1 + ELMO + PMeans <C> 0.261 <C> 0.163 <C> 0.148 <C> 0.147 <C> 0.215 <C> 0.136 <R> <C> Word-Mover <C> Wmd-1 + BERT + PMeans <C> <bold>0.298</bold> <C> <bold>0.212</bold> <C> 0.163 <C> 0.203 <C> 0.261 <C> 0.182 <R> <C> Word-Mover <C> Wmd-1 + BERT + MNLI + PMeans <C> 0.285 <C> 0.195 <C> 0.158 <C> <bold>0.207</bold> <C> <bold>0.270</bold> <C> <bold>0.183</bold> <R> <C> Word-Mover <C> Wmd-2 + BERT + MNLI + PMeans <C> 0.284 <C> 0.194 <C> 0.156 <C> 0.204 <C> 0.270 <C> 0.182 <CAP> Table 3: Spearman correlation with utterance-level human judgments for BAGEL and SFHOTEL datasets.
<R> <C> Setting <C> Metric <C> M1 <C> M2 <R> <C> Baselines <C> LEIC(*) <C> <bold>0.939</bold> <C> <bold>0.949</bold> <R> <C> Baselines <C> METEOR <C> 0.606 <C> 0.594 <R> <C> Baselines <C> SPICE <C> 0.759 <C> 0.750 <R> <C> Baselines <C> BERTScore-Recall <C> 0.809 <C> 0.749 <R> <C> Sent-Mover <C> SMD + W2V <C> 0.683 <C> 0.668 <R> <C> Sent-Mover <C> SMD + ELMO + P <C> 0.709 <C> 0.712 <R> <C> Sent-Mover <C> SMD + BERT + P <C> 0.723 <C> 0.747 <R> <C> Sent-Mover <C> SMD + BERT + M + P <C> 0.789 <C> 0.784 <R> <C> Word-Mover <C> Wmd-1 + W2V <C> 0.728 <C> 0.764 <R> <C> Word-Mover <C> Wmd-1 + ELMO + P <C> 0.753 <C> 0.775 <R> <C> Word-Mover <C> Wmd-1 + BERT + P <C> 0.780 <C> 0.790 <R> <C> Word-Mover <C> Wmd-1 + BERT + M + P <C> <bold>0.813</bold> <C> <bold>0.810</bold> <R> <C> Word-Mover <C> Wmd-2 + BERT + M + P <C> 0.812 <C> 0.808 <CAP> Table 4: Pearson correlation with system-level human judgments on MSCOCO dataset. ’M’ and ’P’ are short names.
<R> <C> [EMPTY] <C> Acc <C> Sim <C> PP <C> GM <R> <C> M0: shen-1 <C> 0.694 <C> 0.728 <C> [BOLD] 22.3 <C> 8.81 <R> <C> M1: M0 [ITALIC] +para <C> 0.702 <C> 0.747 <C> 23.6 <C> 11.7 <R> <C> M2: M0 [ITALIC] +cyc <C> 0.692 <C> 0.781 <C> 49.9 <C> [BOLD] 12.8 <R> <C> M3: M0 [ITALIC] +cyc+lang <C> 0.698 <C> 0.754 <C> 39.2 <C> 12.0 <R> <C> M4: M0 [ITALIC] +cyc+para <C> 0.702 <C> 0.757 <C> 33.9 <C> [BOLD] 12.8 <R> <C> M5: M0 [ITALIC] +cyc+para+lang <C> 0.688 <C> 0.753 <C> 28.6 <C> 11.8 <R> <C> M6: M0 [ITALIC] +cyc+2d <C> 0.704 <C> [BOLD] 0.794 <C> 63.2 <C> [BOLD] 12.8 <R> <C> M7: M6+ [ITALIC] para+lang <C> 0.706 <C> 0.768 <C> 49.0 <C> [BOLD] 12.8 <CAP> Table 3: Literature results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.
<R> <C> Dataset <C> Models A <C> Models B <C> Transfer quality A>B <C> Transfer quality B>A <C> Transfer quality Tie <C> Semantic preservation A>B <C> Semantic preservation B>A <C> Semantic preservation Tie <C> Semantic preservation ΔSim <C> Fluency A>B <C> Fluency B>A <C> Fluency Tie <C> Fluency ΔPP <R> <C> [EMPTY] <C> M0 <C> M2 <C> 9.0 <C> 6.0 <C> 85.1 <C> 1.5 <C> [BOLD] 25.4 <C> 73.1 <C> -0.05 <C> 10.4 <C> [BOLD] 23.9 <C> 65.7 <C> 0.9 <R> <C> Yelp <C> M0 <C> M7 <C> 9.6 <C> 14.7 <C> 75.8 <C> 2.5 <C> [BOLD] 54.5 <C> 42.9 <C> -0.09 <C> 4.6 <C> [BOLD] 39.4 <C> 56.1 <C> 8.3 <R> <C> Yelp <C> M6 <C> M7 <C> 13.7 <C> 11.6 <C> 74.7 <C> 16.0 <C> 16.7 <C> 67.4 <C> 0.01 <C> 10.3 <C> 20.0 <C> 69.7 <C> 14.3 <R> <C> [EMPTY] <C> M2 <C> M7 <C> 5.8 <C> 9.3 <C> 84.9 <C> 8.1 <C> [BOLD] 25.6 <C> 66.3 <C> -0.04 <C> 14.0 <C> [BOLD] 26.7 <C> 59.3 <C> 7.4 <R> <C> Literature <C> M2 <C> M6 <C> 4.2 <C> 6.7 <C> 89.2 <C> 16.7 <C> 20.8 <C> 62.5 <C> 0.01 <C> [BOLD] 40.8 <C> 13.3 <C> 45.8 <C> -13.3 <R> <C> Literature <C> M6 <C> M7 <C> 15.8 <C> 13.3 <C> 70.8 <C> [BOLD] 25.0 <C> 9.2 <C> 65.8 <C> 0.03 <C> 14.2 <C> 20.8 <C> 65.0 <C> 14.2 <CAP> Table 4: Manual evaluation results (%) using models from Table 2 (i.e., with roughly fixed Acc). > means “better than”. ΔSim=Sim(A)−Sim(B), and ΔPP=PP(A)−PP(B) (note that lower PP generally means better fluency). Each row uses at least 120 sentence pairs. A cell is bold if it represents a model win of at least 10%.
<R> <C> Metric <C> Method of validation <C> Yelp <C> Lit. <R> <C> Acc <C> % of machine and human judgments that match <C> 94 <C> 84 <R> <C> Sim <C> Spearman’s  [ITALIC] ρ b/w Sim and human ratings of semantic preservation <C> 0.79 <C> 0.75 <R> <C> PP <C> Spearman’s  [ITALIC] ρ b/w negative PP and human ratings of fluency <C> 0.81 <C> 0.67 <CAP> Table 5: Human sentence-level validation of metrics; 100 examples for each dataset for validating Acc; 150 each for Sim and PP; see text for validation of GM.
<R> <C> [EMPTY] <C> Acc <C> Sim <C> PP <C> GM <R> <C> M0: shen-1 <C> 0.818 <C> 0.719 <C> 37.3 <C> 10.0 <R> <C> M1: M0 [ITALIC] +para <C> 0.819 <C> 0.734 <C> 26.3 <C> 14.2 <R> <C> M2: M0 [ITALIC] +cyc <C> 0.813 <C> 0.770 <C> 36.4 <C> 18.8 <R> <C> M3: M0 [ITALIC] +cyc+lang <C> 0.807 <C> 0.796 <C> 28.4 <C> 21.5 <R> <C> M4: M0 [ITALIC] +cyc+para <C> 0.798 <C> 0.783 <C> 39.7 <C> 19.2 <R> <C> M5: M0 [ITALIC] +cyc+para+lang <C> 0.804 <C> 0.785 <C> 27.1 <C> 20.3 <R> <C> M6: M0 [ITALIC] +cyc+2d <C> 0.805 <C> [BOLD] 0.817 <C> 43.3 <C> 21.6 <R> <C> M7: M6+ [ITALIC] para+lang <C> 0.818 <C> 0.805 <C> [BOLD] 29.0 <C> [BOLD] 22.8 <CAP> Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.
<R> <C> Model <C> BLEU <C> Acc∗ <R> <C> fu-1 <C> [EMPTY] <C> [EMPTY] <R> <C> Multi-decoder <C> 7.6 <C> 0.792 <R> <C> Style embed. <C> 15.4 <C> 0.095 <R> <C> simple-transfer <C> simple-transfer <C> simple-transfer <R> <C> Template <C> 18.0 <C> 0.867 <R> <C> Delete/Retrieve <C> 12.6 <C> 0.909 <R> <C> yang2018unsupervised <C> yang2018unsupervised <C> yang2018unsupervised <R> <C> LM <C> 13.4 <C> 0.854 <R> <C> LM + classifier <C> [BOLD] 22.3 <C> 0.900 <R> <C> Untransferred <C> [BOLD] 31.4 <C> 0.024 <CAP> Table 6: Results on Yelp sentiment transfer, where BLEU is between 1000 transferred sentences and human references, and Acc is restricted to the same 1000 sentences. Our best models (right table) achieve higher BLEU than prior work at similar levels of Acc, but untransferred sentences achieve the highest BLEU. Acc∗: the definition of Acc varies by row because of different classifiers in use. Other results from simple-transfer are not included as they are worse.
<R> <C> [BOLD] Type <C> [BOLD] Reparandum Length  [BOLD] 1-2 <C> [BOLD] Reparandum Length  [BOLD] 3-5 <C> [BOLD] Reparandum Length  [BOLD] 6-8 <C> [BOLD] Reparandum Length  [BOLD] 8+ <C> [BOLD] overall <R> <C> repetition <C> 0.99 <C> 0.99 <C> 1 <C> 1 <C> 0.99 <R> <C> rephrase <C> 0.75 <C> 0.66 <C> 0.44 <C> – <C> 0.70 <R> <C> restart <C> 0.41 <C> 0 <C> – <C> – <C> 0.39 <R> <C> nested∗ <C> 0.79 <C> 0.66 <C> 0.62 <C> 0.21 <C> 0.62 <CAP> Table 2: Percent of reparandum tokens that were correctly predicted as disfluent. *Statistics for nested disfluencies exclude repetition tokens.
<R> <C> [BOLD] Type <C> [BOLD] Reparandum Length  [BOLD] 1-2 <C> [BOLD] Reparandum Length  [BOLD] 3-5 <R> <C> content-content <C> 0.61 (30%) <C> 0.58 (52%) <R> <C> content-function <C> 0.77 (20%) <C> 0.66 (17%) <R> <C> function-function <C> 0.83 (50%) <C> 0.80 (32%) <CAP> Table 3: Relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair (content-content), either the reparandum or repair (content-function) or in neither. Percentages in parentheses show the fraction of tokens belong to each category.
<R> <C> [EMPTY] <C> [BOLD] Model <C> [BOLD] dev mean <C> [BOLD] dev best <C> [BOLD] test mean <C> [BOLD] test best <C> [ITALIC] α <R> <C> single <C> text <C> 86.54 <C> 86.80 <C> 86.47 <C> 86.96 <C> – <R> <C> single <C> raw <C> 35.00 <C> 37.33 <C> 35.78 <C> 37.70 <C> – <R> <C> single <C> innovations <C> 80.86 <C> 81.51 <C> 80.28 <C> 82.15 <C> – <R> <C> early <C> text + raw <C> 86.46 <C> 86.65 <C> 86.24 <C> 86.53 <C> – <R> <C> early <C> text + innovations <C> 86.53 <C> 86.77 <C> 86.54 <C> 87.00 <C> – <R> <C> early <C> text + raw + innovations <C> 86.35 <C> 86.69 <C> 86.55 <C> 86.44 <C> – <R> <C> late <C> text + raw <C> 86.71 <C> 87.05 <C> 86.35 <C> 86.71 <C> 0.2 <R> <C> late <C> text + innovations <C> [BOLD] 86.98 <C> [BOLD] 87.48 <C> [BOLD] 86.68 <C> [BOLD] 87.02 <C> 0.5 <R> <C> late <C> text + raw + innovations <C> 86.95 <C> 87.30 <C> 86.60 <C> 86.87 <C> 0.5 <CAP> Table 4: F1 scores on disfluency detection when using a single set of features (text-only, raw prosody features or innovation features), with early fusion and late fusion. “Raw” indicates the usage of original prosodic features (Section 3.2), while “innovations” indicate the usage of innovation features (Section 3.3).
<R> <C> Model <C> Accuracy (%) agree <C> Accuracy (%) disagree <C> Accuracy (%) discuss <C> Accuracy (%) unrelated <C> Micro F1(%) <R> <C> Average of Word2vec Embedding <C> 12.43 <C> 01.30 <C> 43.32 <C> 74.24 <C> 45.53 <R> <C> CNN-based Sentence Embedding <C> 24.54 <C> 05.06 <C> 53.24 <C> 79.53 <C> 81.72 <R> <C> RNN-based Sentence Embedding <C> 24.42 <C> 05.42 <C> 69.05 <C> 65.34 <C> 78.70 <R> <C> Self-attention Sentence Embedding <C> 23.53 <C> 04.63 <C> 63.59 <C> 80.34 <C> 80.11 <R> <C> Our model <C> 28.53 <C> 10.43 <C> 65.43 <C> 82.43 <C> [BOLD] 83.54 <CAP> Table 2: Performance comparison with the state-of-art algorithms on the FNC-1 test dataset.
<R> <C> Method <C> APW <C> NYT <R> <C> BurstySimDater <C> 45.9 <C> 38.5 <R> <C> MaxEnt-Joint <C> 52.5 <C> 42.5 <R> <C> NeuralDater <C> 64.1 <C> 58.9 <R> <C> Attentive NeuralDater  <C> 66.2 <C> 60.1 <R> <C> OE-GCN  <C> 63.9 <C> 58.3 <R> <C> AC-GCN  <C> 65.6 <C> 60.3 <R> <C> [BOLD] AD3  <C> [BOLD] 68.2 <C> [BOLD] 62.2 <CAP> Table 2: Accuracy (%) of different methods on the APW and NYT datasets for the document dating problem (higher is better). The unified model significantly outperforms all previous models.
<R> <C> Method <C> Accuracy <R> <C> T-GCN of NeuralDater <C> 61.8 <R> <C> OE-GCN <C> [BOLD] 63.9 <R> <C> S-GCN of NeuralDater <C> 63.2 <R> <C> AC-GCN <C> [BOLD] 65.6 <CAP> Table 3: Accuracy (%) comparisons of component models with and without Attention. This results show the effectiveness of both word attention and Graph Attention for this task. Please see Section 6.2 for more details.
<R> <C> [BOLD] Stage <C> [BOLD] Model <C> [BOLD] 1/1 <C> [BOLD] 1/N <C> [BOLD] all <R> <C> [EMPTY] <C> Embedding+T <C> 68.1 <C> 25.5 <C> 59.8 <R> <C> [EMPTY] <C> CNN <C> 72.5 <C> 43.1 <C> 66.3 <R> <C> Trigger <C> DMCNN <C> 74.3 <C> 50.9 <C> 69.1 <R> <C> [EMPTY] <C> JRNN <C> [BOLD] 75.6 <C> 64.8 <C> 69.3 <R> <C> [EMPTY] <C> [BOLD] JMEE <C> 75.2 <C> [BOLD] 72.7 <C> [BOLD] 73.7 <R> <C> [EMPTY] <C> Embedding+T <C> 37.4 <C> 15.5 <C> 32.6 <R> <C> [EMPTY] <C> CNN <C> 51.6 <C> 36.6 <C> 48.9 <R> <C> Argument <C> DMCNN <C> 54.6 <C> 48.7 <C> 53.5 <R> <C> [EMPTY] <C> JRNN <C> 50.0 <C> 55.2 <C> 55.4 <R> <C> [EMPTY] <C> [BOLD] JMEE <C> [BOLD] 59.3 <C> [BOLD] 57.6 <C> [BOLD] 60.3 <CAP> Table 2: System Performance on Single Event Sentences (1/1) and Multiple Event Sentences (1/N)
<R> <C> [BOLD] Method <C> [BOLD] Trigger  [BOLD] Identification (%) <C> [BOLD] Trigger  [BOLD] Identification (%) <C> [BOLD] Trigger  [BOLD] Identification (%) <C> [BOLD] Trigger  [BOLD] Classification (%) <C> [BOLD] Trigger  [BOLD] Classification (%) <C> [BOLD] Trigger  [BOLD] Classification (%) <C> [BOLD] Argument  [BOLD] Identification (%) <C> [BOLD] Argument  [BOLD] Identification (%) <C> [BOLD] Argument  [BOLD] Identification (%) <C> [BOLD] Argument  [BOLD] Role (%) <C> [BOLD] Argument  [BOLD] Role (%) <C> [BOLD] Argument  [BOLD] Role (%) <R> <C> [BOLD] Method <C> [ITALIC] P <C> [ITALIC] R <C> [ITALIC] F1 <C> [ITALIC] P <C> [ITALIC] R <C> [ITALIC] F1 <C> [ITALIC] P <C> [ITALIC] R <C> [ITALIC] F1 <C> [ITALIC] P <C> [ITALIC] R <C> [ITALIC] F1 <R> <C> Cross-Event <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 68.7 <C> 68.9 <C> 68.8 <C> 50.9 <C> 49.7 <C> 50.3 <C> 45.1 <C> 44.1 <C> 44.6 <R> <C> JointBeam <C> 76.9 <C> 65.0 <C> 70.4 <C> 73.7 <C> 62.3 <C> 67.5 <C> 69.8 <C> 47.9 <C> 56.8 <C> 64.7 <C> 44.4 <C> 52.7 <R> <C> DMCNN <C> [BOLD] 80.4 <C> 67.7 <C> 73.5 <C> 75.6 <C> 63.6 <C> 69.1 <C> 68.8 <C> 51.9 <C> 59.1 <C> 62.2 <C> 46.9 <C> 53.5 <R> <C> PSL <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 75.3 <C> 64.4 <C> 69.4 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> JRNN <C> 68.5 <C> [BOLD] 75.7 <C> 71.9 <C> 66.0 <C> [BOLD] 73.0 <C> 69.3 <C> 61.4 <C> 64.2 <C> 62.8 <C> 54.2 <C> 56.7 <C> 55.4 <R> <C> dbRNN <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 74.1 <C> 69.8 <C> 71.9 <C> 71.3 <C> 64.5 <C> 67.7 <C> 66.2 <C> 52.8 <C> 58.7 <R> <C> [BOLD] JMEE <C> 80.2 <C> 72.1 <C> [BOLD] 75.9 <C> [BOLD] 76.3 <C> 71.3 <C> [BOLD] 73.7 <C> [BOLD] 71.4 <C> [BOLD] 65.6 <C> [BOLD] 68.4 <C> [BOLD] 66.8 <C> [BOLD] 54.9 <C> [BOLD] 60.3 <CAP> Table 1: Overall performance comparing to the state-of-the-art methods with golden-standard entities.
<R> <C> [EMPTY] <C> dev perp ↓ <C> dev acc ↑ <C> dev wer ↓ <C> test perp ↓ <C> test acc ↑ <C> test wer ↓ <R> <C> Spanish-only-LM <C> 329.68 <C> 26.6 <C> 30.47 <C> 322.26 <C> 25.1 <C> 29.62 <R> <C> English-only-LM <C> 320.92 <C> 29.3 <C> 32.02 <C> 314.04 <C> 30.3 <C> 32.51 <R> <C> All:CS-last-LM <C> 76.64 <C> 47.8 <C> 14.56 <C> 76.97 <C> 49.2 <C> 14.13 <R> <C> All:Shuffled-LM <C> 68.00 <C> 51.8 <C> 13.64 <C> 68.72 <C> 51.4 <C> 13.89 <R> <C> CS-only-LM <C> 43.20 <C> 60.7 <C> 12.60 <C> 43.42 <C> 57.9 <C> 12.18 <R> <C> CS-only+vocab-LM <C> 45.61 <C> 61.0 <C> 12.56 <C> 45.79 <C> 58.8 <C> 12.49 <R> <C> Fine-Tuned-LM <C> 39.76 <C> 66.9 <C> 10.71 <C> 40.11 <C> 65.4 <C> 10.17 <R> <C> CS-only-disc <C> – <C> 72.0 <C> 6.35 <C> – <C> 70.5 <C> 6.70 <R> <C> Fine-Tuned-disc <C> – <C> [BOLD] 74.2 <C> [BOLD] 5.85 <C> – <C> [BOLD] 75.5 <C> [BOLD] 5.59 <CAP> Table 3: Results on the dev set and on the test set. “perp” stands for perplexity, “acc” stands for accuracy (in percents), and “wer” stands for word-error-rate.
<R> <C> [EMPTY] <C> 25% train dev <C> 25% train test <C> 50% train dev <C> 50% train test <C> 75% train dev <C> 75% train test <C> full train dev <C> full train test <R> <C> CS-only <C> 58.4 <C> 58.9 <C> 65.2 <C> 63.6 <C> 70.8 <C> 68.8 <C> 72.0 <C> 70.5 <R> <C> Fine-Tuned <C> [BOLD] 68.4 <C> [BOLD] 67.7 <C> [BOLD] 71.9 <C> [BOLD] 70.1 <C> [BOLD] 72.8 <C> [BOLD] 73.0 <C> [BOLD] 74.2 <C> [BOLD] 75.5 <CAP> Table 4: Results on the dev set and on the test set using discriminative training with only subsets of the code-switched data.
<R> <C> [EMPTY] <C> dev CS <C> dev mono <C> test CS <C> test mono <R> <C> CS-only-LM <C> 45.20 <C> 65.87 <C> 43.20 <C> 62.80 <R> <C> Fine-Tuned-LM <C> 49.60 <C> 72.67 <C> 47.60 <C> 71.33 <R> <C> CS-only-disc <C> [BOLD] 75.60 <C> 70.40 <C> 70.80 <C> 70.53 <R> <C> Fine-Tuned-disc <C> 70.80 <C> [BOLD] 74.40 <C> [BOLD] 75.33 <C> [BOLD] 75.87 <CAP> Table 5: Accuracy on the dev set and on the test set, according to the type of the gold sentence in the set: code-switched (CS) vs. monolingual (mono).
<R> <C> [BOLD] CoNLL-2003 <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F <R> <C> baseline <C> 72.80 <C> 56.97 <C> 63.92 <R> <C> type combined <C> [BOLD] 74.56 <C> [BOLD] 60.20 <C> [BOLD] 66.61* <CAP> Table 7: Precision (P), recall (R) and F1-score (F) for using type-aggregated gaze features trained on all three eye-tracking datasets and tested on the CoNLL-2003 dataset (* marks statistically significant improvement).
<R> <C> [BOLD] CoNLL-2003 <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F <R> <C> baseline <C> 93.89 <C> 94.16 <C> 94.03 <R> <C> type combined <C> [BOLD] 94.38 <C> [BOLD] 94.32 <C> [BOLD] 94.35* <CAP> Table 5: Precision (P), recall (R) and F1-score (F) for using type-aggregated gaze features on the CoNLL-2003 dataset (* marks statistically significant improvement).
<R> <C> [BOLD] System <C> [BOLD] Initialization <C> [BOLD] Embedding <C> [BOLD] Resources <C> [BOLD] Test Acc. <R> <C> HPCD (full) <C> Syntactic-SG <C> Type <C> WordNet, VerbNet <C> 88.7 <R> <C> LSTM-PP <C> GloVe <C> Type <C> - <C> 84.3 <R> <C> LSTM-PP <C> GloVe-retro <C> Type <C> WordNet <C> 84.8 <R> <C> OntoLSTM-PP <C> GloVe-extended <C> Token <C> WordNet <C> [BOLD] 89.7 <CAP> Table 1: Results on belinkov2014exploring’s PPA test set. HPCD (full) is from the original paper, and it uses syntactic SkipGram. GloVe-retro is GloVe vectors retrofitted Faruqui et al. (2015) to WordNet 3.1, and GloVe-extended refers to the synset embeddings obtained by running AutoExtend Rothe and Schütze (2015) on GloVe.
<R> <C> [BOLD] System <C> [BOLD] Full UAS <C> [BOLD] PPA Acc. <R> <C> RBG <C> 94.17 <C> 88.51 <R> <C> RBG + HPCD (full) <C> 94.19 <C> 89.59 <R> <C> RBG + LSTM-PP <C> 94.14 <C> 86.35 <R> <C> RBG + OntoLSTM-PP <C> 94.30 <C> 90.11 <R> <C> RBG + Oracle PP <C> 94.60 <C> 98.97 <CAP> Table 2: Results from RBG dependency parser with features coming from various PP attachment predictors and oracle attachments.
<R> <C> [BOLD] Model <C> [BOLD] PPA Acc. <R> <C> full <C> 89.7 <R> <C> - sense priors <C> 88.4 <R> <C> - attention <C> 87.5 <CAP> Table 3: Effect of removing sense priors and context sensitivity (attention) from the model.
<R> <C> en-fr <C> flickr16 <C> flickr17 <C> mscoco17 <R> <C> multi30k <C> 61.4 <C> 54.0 <C> 43.1 <R> <C> +subsfull <C> 53.7 <C> 48.9 <C> 47.0 <R> <C> +domain-tuned <C> 66.1 <C> 59.7 <C> [BOLD] 51.7 <R> <C> +ensemble-of-3 <C> [BOLD] 66.5 <C> [BOLD] 60.2 <C> 51.6 <R> <C> en-de <C> flickr16 <C> flickr17 <C> mscoco17 <R> <C> multi30k <C> 38.9 <C> 32.0 <C> 27.7 <R> <C> +subsfull <C> 41.3 <C> 34.1 <C> 31.3 <R> <C> +domain-tuned <C> 43.3 <C> 38.4 <C> 35.0 <R> <C> +ensemble-of-3 <C> [BOLD] 43.9 <C> [BOLD] 39.6 <C> [BOLD] 37.0 <CAP> Table 2: Adding subtitle data and domain tuning for image caption translation (BLEU% scores). All results with Marian Amun.
<R> <C> [EMPTY] <C> en-fr <C> flickr16 <C> flickr17 <C> mscoco17 <R> <C> A <C> subs1M [ITALIC]  [ITALIC] H+MS-COCO <C> 66.3 <C> 60.5 <C> 52.1 <R> <C> A <C> +domain-tuned <C> 66.8 <C> 60.6 <C> 52.0 <R> <C> A <C> +labels <C> [BOLD] 67.2 <C> 60.4 <C> 51.7 <R> <C> T <C> subs1M [ITALIC]  [ITALIC] LM+MS-COCO <C> 66.9 <C> 60.3 <C> [BOLD] 52.8 <R> <C> T <C> +labels <C> [BOLD] 67.2 <C> [BOLD] 60.9 <C> 52.7 <R> <C> [EMPTY] <C> en-de <C> flickr16 <C> flickr17 <C> mscoco17 <R> <C> A <C> subs1M [ITALIC]  [ITALIC] H+MS-COCO <C> 43.1 <C> 39.0 <C> 35.1 <R> <C> A <C> +domain-tuned <C> 43.9 <C> 39.4 <C> 35.8 <R> <C> A <C> +labels <C> 43.2 <C> 39.3 <C> 34.3 <R> <C> T <C> subs1M [ITALIC]  [ITALIC] LM+MS-COCO <C> [BOLD] 44.4 <C> 39.4 <C> 35.0 <R> <C> T <C> +labels <C> 44.1 <C> [BOLD] 39.8 <C> [BOLD] 36.5 <CAP> Table 3: Using automatically translated image captions and domain labels (BLEU% scores). A is short for Amun, T for Transformer.
<R> <C> en-fr <C> flickr16 <C> flickr17 <C> mscoco17 <R> <C> multi30k <C> 61.4 <C> 54.0 <C> 43.1 <R> <C> +autocap (dual attn.) <C> 60.9 <C> 52.9 <C> 43.3 <R> <C> +autocap 1 (concat) <C> 61.7 <C> 53.7 <C> 43.9 <R> <C> +autocap 1-5 (concat) <C> [BOLD] 62.2 <C> [BOLD] 54.4 <C> [BOLD] 44.1 <R> <C> en-de <C> flickr16 <C> flickr17 <C> mscoco17 <R> <C> multi30k <C> 38.9 <C> 32.0 <C> 27.7 <R> <C> +autocap (dual attn.) <C> 37.8 <C> 30.2 <C> 27.0 <R> <C> +autocap 1 (concat) <C> 39.7 <C> [BOLD] 32.2 <C> [BOLD] 28.8 <R> <C> +autocap 1-5 (concat) <C> [BOLD] 39.9 <C> 32.0 <C> 28.7 <CAP> Table 4: Adding automatic image captions (only the best one or all 5). The table shows BLEU scores in %. All results with Marian Amun.
<R> <C> en-fr <C> flickr16 <C> flickr17 <C> mscoco17 <R> <C> IMG [ITALIC] W <C> [ITALIC] 68.30 <C> [BOLD] 62.45 <C> 52.86 <R> <C> enc-gate <C> 68.01 <C> 61.38 <C> [BOLD] 53.40 <R> <C> dec-gate <C> 67.99 <C> 61.53 <C> 52.38 <R> <C> enc-gate + dec-gate <C> [BOLD] 68.58 <C> [ITALIC] 62.14 <C> [ITALIC] 52.98 <R> <C> en-de <C> flickr16 <C> flickr17 <C> mscoco17 <R> <C> IMG [ITALIC] W <C> [ITALIC] 45.09 <C> 40.81 <C> 36.94 <R> <C> enc-gate <C> 44.75 <C> [BOLD] 41.44 <C> [BOLD] 37.76 <R> <C> dec-gate <C> [BOLD] 45.21 <C> 40.79 <C> 36.47 <R> <C> enc-gate + dec-gate <C> 44.91 <C> [ITALIC] 41.06 <C> [ITALIC] 37.40 <CAP> Table 5: Comparison of strategies for integrating visual information (BLEU% scores). All results using Transformer, Multi30k+MS-COCO+subs3MLM, Detectron mask surface, and domain labeling.
<R> <C> en-fr <C> flickr16 <C> flickr17 <C> mscoco17 <R> <C> subs3M [ITALIC]  [ITALIC] LM detectron <C> 68.30 <C> 62.45 <C> 52.86 <R> <C> +ensemble-of-3 <C> 68.72 <C> 62.70 <C> 53.06 <R> <C> −visual features <C> [BOLD] 68.74 <C> [BOLD] 62.71 <C> 53.14 <R> <C> −MS-COCO <C> 67.13 <C> 61.17 <C> [BOLD] 53.34 <R> <C> −multi-lingual <C> 68.21 <C> 61.99 <C> 52.40 <R> <C> subs6M [ITALIC]  [ITALIC] LM detectron <C> 68.29 <C> 61.73 <C> 53.05 <R> <C> subs3M [ITALIC]  [ITALIC] LM gn2048 <C> 67.74 <C> 61.78 <C> 52.76 <R> <C> subs3M [ITALIC]  [ITALIC] LM text-only <C> 67.72 <C> 61.75 <C> 53.02 <R> <C> en-de <C> flickr16 <C> flickr17 <C> mscoco17 <R> <C> subs3M [ITALIC]  [ITALIC] LM detectron <C> 45.09 <C> 40.81 <C> 36.94 <R> <C> +ensemble-of-3 <C> 45.52 <C> [BOLD] 41.84 <C> [BOLD] 37.49 <R> <C> −visual features <C> [BOLD] 45.59 <C> 41.75 <C> 37.43 <R> <C> −MS-COCO <C> 45.11 <C> 40.52 <C> 36.47 <R> <C> −multi-lingual <C> 44.95 <C> 40.09 <C> 35.28 <R> <C> subs6M [ITALIC]  [ITALIC] LM detectron <C> 45.50 <C> 41.01 <C> 36.81 <R> <C> subs3M [ITALIC]  [ITALIC] LM gn2048 <C> 45.38 <C> 40.07 <C> 36.82 <R> <C> subs3M [ITALIC]  [ITALIC] LM text-only <C> 44.87 <C> 41.27 <C> 36.59 <R> <C> +multi-modal finetune <C> 44.56 <C> 41.61 <C> 36.93 <CAP> Table 6: Ablation experiments (BLEU% scores). The row subs3MLM detectron shows our best single model. Individual components or data choices are varied one by one. + stands for adding a component, and − for removing a component or data set. Multiple modifications are indicated by increasing the indentation.
<R> <C> Translation <C> Yule’s I <C> TTR <C> MTLD <R> <C> [EMPTY] <C> [EMPTY] <C> * 1000 <C> [EMPTY] <R> <C> en-fr-HT <C> [ITALIC] 9.2793 <C> [ITALIC] 2.9277 <C> [ITALIC] 127.1766 <R> <C> en-fr-rnn-ff <C> 0.7107 <C> 0.8656 <C> 109.4506 <R> <C> en-fr-smt-ff <C> 6.7492 <C> 2.6442 <C> 118.1239 <R> <C> en-fr-trans-ff <C> 1.1768 <C> 1.0925 <C> 120.5179 <R> <C> en-fr-rnn-back <C> 0.7587 <C> 0.8776 <C> 116.8942 <R> <C> en-fr-smt-back <C> [BOLD] 7.8738 <C> [BOLD] 2.7496 <C> 120.9909 <R> <C> en-fr-trans-back <C> 1.0325 <C> 1.0172 <C> [BOLD] 121.5801 <R> <C> en-es-HT <C> [ITALIC] 12.3065 <C> [ITALIC] 3.7037 <C> [ITALIC] 99.0850 <R> <C> en-es-rnn-ff <C> 0.6298 <C> 0.9394 <C> 89.3562 <R> <C> en-es-smt-ff <C> 7.3249 <C> 3.1170 <C> 95.1146 <R> <C> en-es-trans-ff <C> 1.0022 <C> 1.1581 <C> [BOLD] 96.2113 <R> <C> en-es-rnn-back <C> 0.7355 <C> 0.9829 <C> 95.7198 <R> <C> en-es-smt-back <C> [BOLD] 8.1325 <C> [BOLD] 3.2166 <C> 95.1479 <R> <C> en-es-trans-back <C> 0.9162 <C> 1.1014 <C> 95.0886 <CAP> Table 6: Lexical richness metrics (Train set).
<R> <C> Language pair <C> Train <C> Test <C> Dev <R> <C> EN–FR <C> 1,467,489 <C> 499,487 <C> 7,723 <R> <C> EN–ES <C> 1,472,203 <C> 459,633 <C> 5,734 <CAP> Table 1: Number of parallel sentences in the train, test and development splits for the language pairs we used.
<R> <C> Language pair <C> SRC <C> TRG <R> <C> EN–FR <C> 113,132 <C> 131,104 <R> <C> EN–ES <C> 113,692 <C> 168,195 <CAP> Table 2: Training vocabularies for the English, French and Spanish data used for our models.
<R> <C> System reference <C> BLEU↑ <C> TER↓ <R> <C> en-fr-rnn-rev <C> 33.3 <C> 50.2 <R> <C> en-fr-smt-rev <C> 36.5 <C> 47.1 <R> <C> en-fr-trans-rev <C> [BOLD] 36.8 <C> [BOLD] 46.8 <R> <C> en-es-rnn-rev <C> 37.8 <C> 45.0 <R> <C> en-es-smt-rev <C> 39.2 <C> 44.0 <R> <C> en-es-trans-rev <C> [BOLD] 40.4 <C> [BOLD] 42.7 <CAP> Table 5: Automatic evaluation scores (BLEU and TER) for the REV systems.
<R> <C> [EMPTY] <C> Recall@10 (%) <C> Median rank <C> RSAimage <R> <C> VGS <C> 15 <C> 17 <C> 0.2 <R> <C> SegMatch <C> 12 <C> 17 <C> 0.0 <R> <C> Mean MFCC <C> 0 <C> 711 <C> 0.0 <CAP> Table 2: Results on Flickr8K. The row labeled VGS is the visually supervised model from chrupala2017representations.
<R> <C> [EMPTY] <C> Recall@10 (%) <C> Median rank <C> RSAimage <R> <C> VGS <C> 27 <C> 6 <C> 0.4 <R> <C> SegMatch <C> [BOLD] 10 <C> [BOLD] 37 <C> [BOLD] 0.5 <R> <C> Audio2vec-U <C> 5 <C> 105 <C> 0.0 <R> <C> Audio2vec-C <C> 2 <C> 647 <C> 0.0 <R> <C> Mean MFCC <C> 1 <C> 1,414 <C> 0.0 <R> <C> Chance <C> 0 <C> 3,955 <C> 0.0 <CAP> Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from chrupala2017representations.
<R> <C> Orig <C> <u> turns in a <u> screenplay that <u> at the edges ; it ’s so clever you want to hate it . <R> <C> DAN <C> <u> turns in a <u> screenplay screenplay screenplay of <u> edges edges edges shapes so clever easy want hate hate hate hate hate hate hate hate hate hate <R> <C> CNN <C> she turns on a on ( ( in in the the the edges ’s so clever “ want to hate it ” <R> <C> RNN <C> <u> turns in a <u> screenplay was <u> <u> <u> edges edges edges curves <u> clever clever you want hate hate it . <CAP> Table 1: Example sentences of the different classifiers compared to the original on SST-2. We report further examples in the Appendix. <u> use for <UNK>.
<R> <C> [EMPTY] <C> <bold>RNN</bold> <C> <bold>CNN</bold> <C> <bold>DAN</bold> <R> <C> Nouns <C> +63 <C> −3 <C> +<bold>93</bold> <R> <C> DT <C> −29 <C> +32 <C> −<bold>38</bold> <R> <C> Verbs <C> +20 <C> −4 <C> +<bold>34</bold> <R> <C> Adj. <C> +25 <C> −1 <C> +<bold>66</bold> <R> <C> Prep. <C> +12 <C> +12 <C> −<bold>62</bold> <R> <C> Punct. <C> −<bold>53</bold> <C> −14 <C> −47 <R> <C> <U> <C> +<bold>82</bold> <C> −14 <C> +16 <R> <C> <bold>RNP</bold> <C> 69.0% <C> 70.5% <C> 81.5% <CAP> Table 2: Part-of-Speech (POS) changes in SST-2: , , and indicate that the number of occurrences have increased, decreased or stayed the same through fine-tuning respectively. The symbols are purely analytic without any notion of goodness. The numbers indicate the changes in percentage points with respect to the original sentence. A score of 0 thus means that fine-tuning has not changed the number of words. The last row indicates the overlap with the extractive RNP approach. We report results for PubMed in the Appendix.
<R> <C> [EMPTY] <C> <bold>RNN</bold> <C> <bold>CNN</bold> <C> <bold>DAN</bold> <R> <C> Positive <C> +9.7 <C> +4.3 <C> +<bold>23.6</bold> <R> <C> Negative <C> +6.9 <C> +5.5 <C> +<bold>16.1</bold> <R> <C> Flipped to Positive <C> +20.2 <C> +24.9 <C> +27.4 <R> <C> Flipped to Negative <C> +31.5 <C> +28.6 <C> +19.3 <CAP> Table 3: Sentiment score changes in SST-2. The numbers indicate the changes in percentage points with respect to the original sentence. The last two rows correspond to the case where negative labels are flipped to positive and vice versa. and indicate that the score increases in positive and negative sentiment.
<R> <C> [EMPTY] <C> <bold>SST-2</bold> Positive <C> <bold>SST-2</bold> Negative <C> <bold>PubMed</bold> Objective <C> <bold>PubMed</bold> Conclusion <R> <C> <bold>PMI</bold> <C> best <C> too <C> compare <C> should <R> <C> <bold>PMI</bold> <C> love <C> bad <C> investigate <C> suggest <R> <C> <bold>PMI</bold> <C> fun <C> n’t <C> evaluate <C> findings <R> <C> <bold>SIFT</bold> <C> but <C> nothing <C> to <C> larger <R> <C> <bold>SIFT</bold> <C> come <C> awkward <C> whether <C> confirm <R> <C> <bold>SIFT</bold> <C> it <C> lacking <C> clarify <C> concluded <R> <C> Acc <C> <italic>98%</italic> <C> <italic>98%</italic> <C> <italic>98%</italic> <C> <italic>99%</italic> <R> <C> <italic>Corr</italic> <C> <italic>0.486</italic> <C> <italic>0.5415</italic> <C> <italic>0.398</italic> <C> <italic>0.00089</italic> <CAP> Table 4: Top 3 PMI and SIFT terms for a subset of classes with SIFT on the RNN model. The second last row depicts the SIFT accuracy on the test set for the flipped label setting. The last row indicates the correlation of the PMI and SIFT list using weighted Kendall’s tau correlation Shieh1998 .
