table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . table 2 also shows the performance of our approach compared to the best state - of - the - art approaches on the training dataset .
the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t parallelization . table 1 compares the performance of the balanced and linear datasets with respect to the original treernn model using the best performing batch size of 1 .
the max pooling strategy consistently performs better in all model variations . for example , conll08 achieves f1 scores of 75 . 83 and 75 . 57 on the test set ( i . e . in 5 - fold ) with optimal values for each model with different representation as shown in table 2 . for ud v1 . 3 , the best performance is obtained with a dropout probability of 0 . 63e - 04 .
table 1 : effect of using the shortest dependency path on each relation type . the results are shown in table 1 . our macro - averaged model achieves the best f1 score in 5 - fold test set with a minimum of 50 . 45 % f1 .
the results are shown in table 3 . the results of the best performing models are presented in bold . our model achieves the best results with a f1 score of 50 . 57 % on r - f1 and 50 % on f1 50 % with a corresponding percentage of accuracy of 37 . 57 % .
the results are shown in table 3 . we can see that the mst - parser outperforms all the alternatives in terms of accuracy with a significant margin . the results of the best performing parser are presented in bold . it can be observed that the best performance is obtained when using the paragraph level r - f1 and the accuracy level of paragraph prediction on an essay of 50 % or more .
as shown in table 4 , the average c - f1 ( 100 % ) on the essay and paragraph level is significantly lower than that on the paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . our proposed lstm - parser parser outperforms both the original stagblcc parser and the original embeddings .
the results are shown in table 3 . the results for original and cleaned tgen + are presented in bold . as the results show , when the original tgen is cleaned , it performs better than tgen − and sc - lstm on the test set . however , the results for clean - up are slightly worse than those for original .
table 1 compares the original e2e data with our cleaned version . the results are presented in table 1 . our cleaned version shows that our slot matching script outperforms both the original and the cleaned version in terms of the number of distinct mrs and number of textual references .
the results are shown in table 3 . the results for the original and the add train are presented in bold . as expected , the original train is significantly better than the original test set . adding the wrong answer to the correct match improves the accuracy of the original match , however , the difference between the two approaches is less pronounced for tgen + compared to the original .
results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) are shown in table 4 . as shown in the table , adding the correct values to the training data reduces the number of errors , but it does not improve the quality of the model .
the results are shown in table 3 . table 3 shows the performance of our approach compared to previous approaches . our approach outperforms all the previous approaches except for tree2str , which shows that our proposed graphlstm model can improve upon the previous state - of - the - art in terms of completeness and accuracy .
table 2 shows the performance of our model compared to previous approaches on amr17 . our model achieves a bleu score of 57 . 6 compared to 50 . 4 for seq2seqb ( beck et al . , 2018 ) and 53 . 5 for ggnn2seq ( beck and cohen , 2019 ) .
the results are shown in table 1 . table 1 presents the results for english - german , german - czech and czech . our proposed method outperforms all the previous approaches on both languages except for the case of birnn + gcn , which shows that our proposed method can improve upon the performance of previous approaches .
table 5 : the effect of the number of layers inside the network on the performance of our model is shown in the results presented in table 5 . our model outperforms the previous state - of - the - art model by a significant margin .
comparisons with baselines are shown in table 6 . the results of rc + la ( 6 ) and rc + rc ( 9 ) show that the results obtained with residual connections are comparable to those obtained with the baselines with rc + rc ( 6 ) .
the results are shown in table 4 . we observe that the best performing model is the dcgcn ( 1 ) with a d / b of 10 . 4 and a f1 score of 54 . 2 on the test set , which indicates that the model is well suited to the task at hand . however , it should be noted that the performance of this model is only slightly better than that of the best previous state - of - the - art model .
the results of the ablation study are shown in table 8 . our model obtains the best results with a precision of 25 . 5 % on the dev set of amr15 . 1 .
the results of the ablation study are shown in table 9 . the results show that applying the coverage mechanism improves the results for both the graph encoder and the lstm decoder . however , for the decoder , the results are slightly worse than those for the encoder .
table 7 shows the results for initialization strategies on probing tasks . our system outperforms all the baselines except somo and glorot by a significant margin . the results of our system are shown in table 7 .
the results are shown in table 3 . the h - cmow model outperforms the h - cbow model by a significant margin . it achieves an absolute improvement of 3 . 9 points over the previous state - of - the - art cmow model with a score of 87 . 9 % compared to 77 . 6 % on the previous best result . further improving upon the performance of the previous model with the help of our concatenation of the best features , our approach achieves a final score of 82 . 9 % , a full 3 points higher than previous best results .
the results are shown in table 3 . the results of the best performing method are presented in bold . our approach outperforms all the baselines except sick - e by a margin of 3 . 6 points . we observe that our approach improves upon the previous state - of - the - art approach with a noticeable margin .
table 3 shows the results on unsupervised downstream tasks attained by our models . the cbow and cmow models outperform cmp . cmow on sts12 , sts13 , and sts16 by a significant margin . hybrid also improves upon cbow with respect to sts15 , but only by a marginal margin .
table 8 : scores for initialization strategies on supervised downstream tasks . glorot outperforms all the baselines except sick - e by a large margin . it achieves 87 . 6 % overall improvement over the previous state - of - the - art on sst2 and sst5 , and 86 . 4 % improvement over trec .
table 6 shows the results for different training objectives on the unsupervised downstream tasks for sts12 and sts13 . the cbow - r approach outperforms the cmow approach on all three of these tasks . it achieves the best performance on sts14 , sts15 , and the sts16 tasks with a mean score of 63 . 7 % .
the results are shown in table 3 . we observe that our approach outperforms the previous state - of - the - art approaches on all metrics except for bshift and topconst . our approach achieves the best results with a precision of 79 . 6 % compared to the previous best result of 77 . 9 % .
the results are shown in table 3 . the results of the best performing method are presented in bold . our approach outperforms the previous state - of - the - art cmow - r on all metrics except mpqa and sick - r .
the results are shown in table 3 . the results for all systems are presented in bold . in particular , we see that for all loc and misc data , our system outperforms the previous state - of - the - art model by a significant margin . for example , our model outperforms both the previous best state of the art model and the best supervised learning model on all three metrics .
results on the test set under two settings are shown in table 2 . the results for the name matching task are presented in bold . we observe that our approach outperforms the previous state - of - the - art approach in terms of f1 scores . however , the results on the supervised learning task are slightly better than the results for name matching , as the results of τmil - nd ( model 2 ) are slightly higher than that of the original model ( 37 . 42 ± 0 . 59 vs . 37 . 38 ± 1 . 15 ) .
as shown in table 6 , the model with the best performance is the g2s - gat model , which achieves 77 . 45 % on average compared to the s2s baseline of 38 . 86 % . however , the performance gap between the two approaches is narrower when we consider the fact that the gat model is based on gat data , whereas the gggnn model only relies on gin data . the results of the experiments on the gin dataset are presented in table 7 .
table 3 presents the results of our model compared to previous state - of - the - art g2s models on the ldc2015e86 and ldc2017t10 datasets . the results are presented in table 3 . our model outperforms all the previous state of the art models except for konstas et al . ( 2017 ) in terms of bleu score and meteor score by a significant margin . it also outperforms the performance of the best previous models by a noticeable margin .
results on the test set of the ldc2015e86 test set are shown in table 3 . g2s - ggnn models outperform both the best previous models when trained with additional gigaword data ( konstas et al . , 2017 ; song et al . 2018 ) . the results are slightly better than those of the original models , but still slightly worse than the performance of the best state - of - the - art models trained on the original test set .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model outperforms the previous state - of - the - art lstm models on all metrics except for meteor score .
the results are shown in table 3 . we observe that g2s - gat outperforms all the alternatives on all metrics except for sentence length , with the exception of length of the last sentence , which shows that the gat model can handle longer sentences better .
table 8 shows the results for the test set of ldc2017t10 . gold refers to the reference sentences . the token lemmas are used in the comparison . as shown in table 8 , gold significantly outperforms s2s and g2s - gat in terms of the fraction of elements in the output that are missing in the generated sentence ( miss ) , which indicates that the model is better at generating reference sentences with fewer errors .
table 4 shows the pos and sem accuracy on a smaller parallel corpus ( 200k sentences ) trained with different target languages . pos tagging accuracy is significantly better than that of the original nmt encoding layer ( 88 . 6 % vs . 85 . 7 % ) , indicating that the features extracted from the 4th nmt layer are well - tuned to the target language .
table 2 compares pos and sem tagging accuracy with baselines and an upper bound . the results are presented in table 2 . word2tag significantly outperforms unsupemb in terms of pos accuracy and sem accuracy as well as the upper bound on both metrics .
the results are shown in table 4 . table 4 shows the performance of our system on the pos and sem metrics . our system outperforms the previous state - of - the - art on all metrics except pos tagging accuracy by a significant margin .
table 5 compares the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are presented in table 5 . our proposed system outperforms the previous state - of - the - art approaches on all three target languages except english , with the exception of german , where it performs slightly better .
table 8 shows the performance of the attacker on different datasets . results are on a training set 10 % held - out . δ is the difference between the attacker score and the corresponding adversary ’ s accuracy . for pan16 , we observe that the attacker performs slightly better than the adversary on all three tasks .
the results are shown in table 1 . the pan16 model outperforms pan16 with a significant margin . as the results show , the gender and age gap between pan16 and pan16 does not impact the accuracy of the model .
table 2 compares the performance of pan16 with unbalanced and balanced data splits . the results are presented in table 2 . our proposed approach outperforms the previous state - of - the - art approach by a significant margin . the results of the best balanced and unbalanced data splits are shown in table 1 .
the results of the adversarial training on different datasets are shown in table 3 . as the table 3 shows , for pan16 , gender and age features significantly contribute to the performance of the system , while the presence of race and sentiment features contribute less than age and gender .
the results of rnn and guarded encoders are shown in table 6 . the results for the leaky and guarded embeddings are slightly better than those for the protected ones . however , the difference between the rnn encoder performance and the guarded encoder is less pronounced for the leaky encoder , indicating that the effectiveness of the protected attribute can be improved with the use of a better encoder design .
the results are presented in table 3 . table 3 presents the results of our approach compared to previous work on the ptb and wt2 datasets . our approach outperforms the previous state - of - the - art on both datasets by a significant margin . our proposed lstm achieves a final score of 69 . 36 % on the wt2 dataset , compared to 62 . 86 % by yang et al . ( 2018 ) and 69 . 45 % by wang et al . , ( 2018 ) .
table 3 presents the results of our approach compared to previous work on the lstm model by rocktäschel et al . ( 2016 ) and gru ( 2016 ) . the results are presented in table 3 . table 3 shows that our approach outperforms previous work by a significant margin . our approach achieves the best results with an absolute improvement of 3 . 57 % over the previous state - of - the - art model on both base and time metrics .
the results of zhang et al . ( 2015 ) are shown in table 3 . table 3 presents the results of our final model compared to the previous state - of - the - art lstm and atr models . our proposed approach outperforms both previous work on both datasets by a significant margin . the results are presented in bold . as can be seen , our proposed gru model outperforms the previous best state of the art model on both metrics .
table 3 shows the bleu score of our model on the wmt14 english - german translation task on tesla p100 . our approach outperforms all the state - of - the - art neural networks except for olrn , which has a case - insensitive tokenized score of 26 . 67 on the german translation task . the performance of our approach is comparable to the performance of gnmt and atr on the newstest2014 dataset .
table 4 presents the results of our model with respect to accuracy on the squad dataset . our approach outperforms all the state - of - the - art approaches except for the performance of the lstm model , which is slightly better than the previous state of the art . with the exception of the atr parameter , all the other approaches outperform our approach except the gru model ( table 4 ) . the performance of our approach is comparable to that of the best previous work ( wang et al . , 2017 ) .
table 6 shows the f1 score of our model on conll - 2003 english ner task . our lstm model significantly outperforms the previous state - of - the - art models on both english and german ner tasks . we observe that our model has significantly better performance on the english nert task compared to previous models .
table 7 shows the test results on snli task with base + ln setting and test perplexity on ptb task with base setting . the results are shown in table 7 . with the base setting , our model achieves the best performance on both snli and ptb tasks . however , with the perplexity setting , the model performs slightly worse .
table 2 compares the performance of our system with the best performing systems with respect to word recognition . the results are presented in table 2 . our system outperforms both human and automatic systems in terms of word recognition and sentence prediction accuracy . the results for human are shown in bold . for example , the average number of words per sentence for human is 22 . 38 , while for automatic systems it is 20 . 49 .
table 4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points ( h & w hua and wang ( 2018 ) , with seq2seq ( 25 . 6 % ) and candela ( 30 . 2 % ) ranking in the top 2 for overall quality . table 4 shows the percentage of evaluations a system receives that are ranked in top 1 or 2 for the overall quality of its grammatical evaluation . the results of the best performing system are shown in table 4 .
the results are shown in table 3 . table 3 shows the performance of our approach compared to the best previous approaches . our approach outperforms all the previous approaches except for the ted talks dataset , which shows that our approach has a slight advantage over the previous state of the art .
the results are shown in table 3 . table 3 shows the performance of our approach compared to previous approaches . our approach outperforms all the previous approaches except for the case of ted talks , which shows that our approach has a slight advantage over the previous state - of - the - art approach .
the results are shown in table 3 . table 3 shows the performance of our approach compared to the best previous approaches . our approach outperforms all the previous approaches except for the case of ted talks , which shows that our approach has a slight advantage over the previous state - of - the - art approach .
the results are presented in table 1 . the results of our approach are shown in bold . our approach outperforms the previous state - of - the - art on all metrics except for the average depth metric .
the results are presented in table 1 . the results of our approach are shown in bold . our approach outperforms the previous state - of - the - art on all metrics except for the average depth metric .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . 3 is shown in table 1 . our enhanced model outperforms the baseline model in terms of both qt and ncg % by a significant margin . the results of the enhanced model are shown in bold .
the results of ablative studies on different models on the visdial v1 . 0 validation set are shown in table 2 . as the results show , applying p2 improves the performance of all the models except for the hidden dictionary learning model ( p2 + p1 ) .
the results of hmd - f1 + bert are shown in table 5 . the results are slightly better than those of wmd - bigram and hmd - recall , but still significantly worse than the results obtained by ruse ( 0 . 788 vs . 0 . 817 ) . further , the results are significantly worse on soft alignments . we note that the performance gap between soft and hard alignments is less pronounced with respect to ruse , but it is still significant .
the results are shown in table 3 . the average score of our method compared to the baselines is 0 . 716 vs . 0 . 685 on the direct assessment metric , which shows that our method is comparable to previous state - of - the - art approaches in terms of accuracy .
the results are shown in table 3 . the results for bleu - 1 and blei - 2 are presented in bold . our proposed method outperforms the previous state - of - the - art baselines on all three metrics except for the sent - mover metric , which shows that our proposed method can improve upon the performance of previous approaches with a slight margin .
the results are shown in table 3 . word - mover is the average of the average number of frames per second for all three sets , with the exception of leic ( * ) where it is only slightly better than spice ( 0 . 939 / 0 . 949 ) . the accuracy of bertscore - recall is slightly higher than that of meteor , but still comparable to spice , indicating that the accuracy obtained by bert score alone is not high enough to achieve the best results .
the results are shown in table 7 . the results of the best performing models with and without para - para are presented in bold . our model outperforms all the other models with the exception of m6 , which shows that it is better at learning new features with the help of pre - trained lexical features .
the results are shown in table 3 . table 3 shows the transfer quality and fluency scores of all models trained on the test set of yelp . the results of the best performing models are presented in bold . our proposed method outperforms all the previous approaches except for the case of semantic preservation , which shows that our proposed method can improve upon the performance of previous approaches .
table 5 shows the results of human evaluation for each metric for validation of acc and pp . the results are presented in table 5 . as the table shows , the accuracy of the summaries obtained by the human is high , indicating that the accuracy obtained by our approach is high enough to support the effectiveness of our approach .
the results are shown in table 7 . the results of the best performing models are presented in bold . para - para improves the performance of all models except for m0 [ italic ] + cyc , with the exception of m3 , where it improves only by 0 . 3 points . further improving performance for m3 is seen with the improvement of m6 [ italics in bold ] .
results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . the results in table 6 show that our best model achieves the highest acc score . the classifiers in our system outperform the best previous work on both transfer and transfer accuracy by a significant margin . we note that the accuracy obtained by our model is slightly higher than the accuracy achieved by previous work .
the results are shown in table 2 . reparandum length is the percentage of reparandum tokens that are correctly predicted as disfluent for each type of disfluency . for nested disfluencies , we see that the average length of the disfuncions is slightly longer than that for rephrase tokens , but the average number of repetition tokens is slightly shorter than for restart tokens .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either the reprandum or repair as well as in the function - function , or in neither . the average number of tokens per disfluency prediction for each category is shown in table 3 . percentages in parentheses show the fraction of tokens belong to each category . as shown in the table , content - content tokens are the most frequently predicted to belong to disfuncions , followed by function tokens , which are the least frequently predicted .
the results are shown in table 3 . the results of the best performing model are presented in bold . we observe that the model with the best performance is based on the best combination of text + innovations , with the average number of iterations per test being 86 . 57 . however , we also observe that when the model is trained with only text , the results are slightly worse .
performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . our model achieves the best results with an accuracy of 83 . 43 % on the micro f1 test set compared to the state of the art rnn - based embeddings .
the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . as shown in table 2 , the accuracy of our approach significantly improves over the performance of the previous approaches .
as shown in table 3 , both word attention and graph attention improve the performance of the neuraldater model with and without attention . the accuracy of both approaches improves with the addition of word attention .
the results are shown in table 3 . the results for all stages are presented in bold . as shown in the table , our approach outperforms the state - of - the - art dmcnn model on all stages except for the argument stage , where it performs slightly better . we observe that our approach improves upon the performance of the previous approaches on both event and event prediction .
the results are presented in table 1 . the results of cross - event event detection are shown in bold . our proposed method outperforms the state - of - the - art on both event detection and event prediction using the best match rate on the full test set .
the results for english , spanish , french , german , dutch , russian and turkish are shown in table 3 . the results of all models are presented in bold . we observe that all models perform slightly better than the best baseline on average , with the exception of spanish - only - lm , which performs slightly better on average . however , fine - tuning improves performance for all models except english - only .
results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the results are shown in table 4 . fine - tuned fine - tuned models outperform cs - only models with 75 % accuracy on the full train test and 75 % on the train dev . the results of fine - tuned models with only 50 % training data are slightly better than those with full train data .
as shown in table 5 , fine - tuning improves the accuracy on the dev set and on the test set , according to the type of the gold sentence in the set , for both types of gold sentence . the results are slightly better for the monolingual gold sentence than for the code - switched gold sentence , but the results are still slightly worse than the results obtained with the fixed - disc approach . fine - tuned - disc improves the performance for both gold sentences and for test sentences .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . the type combined approach shows a significant improvement in precision and recall compared to the baseline approach .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset as well as the results of type - combined gaze features . the results are presented in table 5 . type combined gaze features significantly improve the precision and recall of the model compared to the baseline .
results on the test set of belinkov2014exploring ’ s ppa are shown in table 1 . syntactic - sg outperforms lstm - pp and glove - extended in terms of word embeddings . the results of hpcd ( from the original paper ) show that the type and type of tokens generated by syntacticsg are comparable to those generated by gloanve - retro ( farrequi et al . , 2015 ) . however , the difference in performance between the two approaches is less pronounced for wordnet , which shows that syntactic sg can be further improved with further optimization .
results are shown in table 2 . the results show that the hpcd system outperforms the original ontolstm - pp model in terms of ppa acc . and uas accuracy with a significant margin .
table 3 shows the effect of removing the context sensitivity ( attention ) and sense priors from the model . the results are shown in table 3 . as expected , the ppa acc . accuracy improves significantly with the removal of context sensitivity . however , the effect remains the same with respect to the full model .
the results are shown in table 2 . adding subtitle data and domain tuning for image caption translation improves the bleu % scores for both en - de and domain - tuned models , but only slightly for en - fr . the results for mscoco17 show that the domain tuning improves the accuracy of the multi30k caption translation by 0 . 7 bleus . further , the improvement of the precision score for ende model over domain tuning is only 0 . 1 blu % higher than that of domain tuning .
the results of domain - tuned h + ms - coco on the en - de and en - fr datasets are shown in table 3 . the results are summarized in bold . we observe that domain - domain - tuning significantly improves the performance of the subs1m model compared to the results obtained with domain - tuned h + hoco . further , the results are slightly better than those obtained with the h + moco baseline .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . the results for en - de and en - fr settings are presented in table 4 . adding automatic captions improves the performance for both sets of models . as shown in the table , the performance of both sets improves with the addition of the best 5 captions . however , for the mscoco17 model , the improvement is less pronounced , as it only achieves 62 . 7 % .
the results in table 5 show that the encoder and dec - gate strategies outperform the en - de and en - fr strategies for integrating visual information ( bleu % scores ) . however , the performance of enc - de + dec - gate is slightly higher than en - f ( 68 . 38 % vs . 62 . 86 % ) , indicating that it requires more information to interpret the image .
the results for en - fr and en - de are shown in table 3 . we observe that the results obtained with en - familiarization and ensemble - of - 3 are significantly better than those obtained using the approaches with ms - coco and multi - lingual features . however , the results are still slightly worse than those with the visual features .
the results are shown in table 3 . the results of en - fr - ht and en - es - ht are presented in bold . as can be seen in the table , the results are slightly better than those of the baseline models for both ttr and mtld on the original yule ’ s i and ii test set . however , the difference between the two approaches is less pronounced for mtld . it can also be seen that the performance gap between the baseline model and the alternative approaches is narrower for the former , but larger for the latter .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . our proposed en – fr model outperforms the previous state - of - the - art model by a significant margin .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the results are presented in tables 2 and 3 .
table 5 shows the bleu and ter scores for the rev systems . the results of en - fr - rnn - rev and en - es - smt - rev are shown in table 5 . our system outperforms both the previous state - of - the - art systems on both metrics . however , our system performs slightly better on the ter metric , as shown in fig . 3 .
results on flickr are shown in table 2 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the model from flickr2016 . the results are presented in bold . we observe that the vgs model significantly outperforms the rsaimage model in terms of recall .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled u is the model trained on the rsaimage dataset . the results of vgs are comparable to those of audio2vec - u in terms of recall @ 10 and mean mfcc .
we report further examples in the appendix . the rnn classifiers are shown in table 1 . as the table 1 shows , the rnn turns in a screenplay that is easier to hate than the original on sst - 2 . however , for cnn , it is harder to hate the screenplay because the edges are at the edges , making it harder to turn it into a screenplay .
table 2 shows the percentage of occurrences for each part - of - speech in sst - 2 that have increased , decreased or stayed the same through fine - tuning respectively . the numbers indicate the changes in percentage points with respect to the original sentence . for example , the number of instances for nouns and verbs has increased by 3 . 5 % , 3 . 0 % and 7 . 5 % respectively . for verbs , the percentage has decreased by 4 . 5 % . for adjectives , the change is only 0 % . for prepositions , it is only 1 % . for all other parts of speech , the changes are negligible ( p = 0 . 01 ) .
the results are shown in table 3 . and indicate that the score increases in positive and negative sentiment . the numbers indicate the changes in percentage points with respect to the original sentence . the last two rows correspond to the case where negative labels are flipped to positive and vice versa .
the results are shown in table 1 . the results of our approach are presented in bold . our approach outperforms both pubmed and sst - 2 by a significant margin . the difference between the two approaches is statistically significant ( p < 0 . 001 ) .
