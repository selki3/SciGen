table 2 shows the performance of the treelstm model on our recursive framework , fold ’ s folding technique , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . on the other hand , the iterative approach performs better on training and inference than the recursive framework on the large movie review dataset . in addition , the recursive approach outperforms the folding approach on training , but not on inference .
the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t parallelization . table 1 shows the performance of the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . we observe that the balanced dataset outperforms the linear dataset in terms of performance on all three datasets , except for the one that has a lower batch size .
the max pooling strategy consistently performs better in all model variations . for example , the best performing model is ud v1 . 3 , which outperforms conll08 , softplus , sb , and sigmoid in all models with different representation . the best - performing model with the best representation is ud , which performs better than softplus and sb , but not as well as sb .
table 1 shows the results of using the shortest dependency path on each relation type . the results are shown in table 1 . the best f1 ( in 5 - fold ) with sdp and macro - averaged models outperforms all the other models in all the relation types except part_whole . we find that the best - performing model is the one with the shortest dependencies path . the other models have the best performance in all relation types without sdp . our model has the highest f1 score in all three relation types .
table 3 shows the performance of the f1 100 % and r - f1 50 % models . the results are shown in table 3 . the results of the best - performing models are presented in table 4 . we see that the best performing models are the ones with the best performance on all three domains . the best performing model is the one with the highest performance on f1 and f1 50 % . the worst performing model has the lowest performance on both of the two domains .
table 2 shows that mst - parser performs better than paragraph level r - f1 and paragraph level f1 in terms of the accuracy of the essay level f1 . the results are shown in table 2 . we see that the best - performing parser is the one with the highest accuracy . our best performing parser has the highest level of accuracy in all three of our tests .
table 4 shows the performance of stagblcc and lstm - parser on the essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . the performance of the two systems is shown in table 4 .
table 2 shows the results of the training and testing of sc - lstm on tgen + and tgen − systems . the results are shown in table 2 . we see that the original cleaned system outperforms the cleaned one by a significant margin . the cleaned model outperforms both the original and the trained system by a large margin . our results are presented in table 3 . we observe that our system performs significantly better than the trained model on both systems .
table 1 shows the results for the original e2e data and our cleaned version . the results are shown in table 1 . we find that our trained version outperforms both the original and the cleaned version in terms of the number of distinct mrs , total number of textual references , and ser as measured by our slot matching script , see section 3 . our cleaned version also outperforms the original version in the respect to the number and quality of the textual references .
table 2 shows the performance of sc - lstm compared to tgen + and tgen − on the test system . the results are shown in table 2 . we see that the results of the original system outperform those of the tgen - trained system by a significant margin . the difference in performance between the two systems is significant . in particular , the difference in the performance between tgen and tgen + is significant in the case of tgen + , but not the case for tgen + .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) . the results are shown in table 4 . we found that tgen can be used to improve the accuracy of the training data , but not the performance of the test set .
table 2 shows the results of the model performance on all models . the results of all models are shown in table 2 . we see that all models outperform all other models except for graphlstm ( song et al . , 2018 ) and pbmt ( pourdamghani et al , 2016 ) in terms of dcgcn .
table 2 shows the model size in terms of parameters ; “ s ” and “ e ” denote single and ensemble models , respectively . the model size is shown in table 2 . we observe that seq2seq ( beck et al . , 2018 ) achieves 24 . 5 bleu points . gcnseq ( damonte and cohen , 2019 ) , on the other hand , achieves 23 . 3 points . we also observe that dcgcn ( our ) achieves 22 . 5 points . table 2 also shows the results of our model on amr17 . we see that our model achieves the best performance on the single - parameter model .
table 1 shows the performance of the models in table 1 . the results of our model are shown in table 2 . our model outperforms all the other models in terms of performance on english - german and english - czech c . we observe that our model performs better than all other models except for birnn + gcn , which outperforms the other two models by a significant margin .
table 5 shows the effect of the number of layers inside the dc on the performance of the model . the effect of layers on performance of dc is shown in table 5 . we observe that dc has a significant effect on performance on all the layers in the dc model , except for the first layer , which has an effect on only the first two layers . the effect on the third layer is negligible .
table 6 shows the performance of gcns with residual connections on the baselines . gcn + rc and gcn + la are shown in table 6 . + rc denotes gcns that have residual connections , and + rc + la denotes gcn that have no residual connections . the difference in performance between gcns and gcns without residual connections can be seen in the table 6 results . the difference between the two baselines is shown in the table 6 results , and it can be observed that the difference in the performance between dcgcn1 and the other baselines can be significant .
table 3 shows the results of our model . we find that dcgcn ( 1 ) outperforms the other two models by a significant margin . the model with the best performance is the one with the highest number of models . it outperforms all the other models with the lowest number of model scores . it also outperforms those with the worst model performance .
table 8 shows the results of the ablation study on the dev set of amr15 . the results are shown in table 8 . we observe that the dcgcn4 model outperforms the other two models in terms of the density of connections . in particular , we observe a significant difference between the two models when removing the dense connections in the i - th block . - { i } dense block denotes removing all the connections from the ith block , and - { 2 , 3 , 4 } dense blocks denotes removing only the connections on the i th block .
table 9 shows the results of our ablation study for the graph encoder and the lstm decoder . the results of the ablation study are shown in table 9 . we find that the decoder and encoder modules are significantly worse than the global node and global node models . the decoder modules have a lower coverage mechanism and a lower attention rate than the node and global node models . in addition , the coverage mechanism has a lower accuracy rate .
table 7 shows the scores for initialization strategies on probing tasks . our paper outperforms glorot and somo on all probing tasks , except for somo , which outperforms our paper on all tasks except somo . our paper also outperforms somo when it comes to the accuracy of its initialization strategies .
we observe that h - cmow / 400 outperforms h - cbow by a large margin . it outperforms both of the best - performing models by a significant margin . we see that the best performing model is the one with the highest degree of tense , which is the best in the world . the worst performing model has the lowest tense and the highest tense .
table 2 shows the performance of the hybrid and cmow models . the hybrid model outperforms the cmow model by a significant margin . it outperforms both cmow and sick - r by a margin of 0 . 6 % and 0 . 2 % , respectively . the cmow / 784 model outperform the hybrid model by 0 . 3 % and 1 . 2 % respectively .
table 3 shows the results on unsupervised downstream tasks attained by our models . rows starting with “ cmp . ” show the relative change with respect to hybrid . we find that our models outperform hybrid on all three tasks except for the task that requires the most attention . our models perform significantly better than hybrid on the tasks that require the least attention than hybrid .
table 8 shows the scores for initialization strategies on supervised downstream tasks . our paper outperforms glorot and trec in terms of sst2 , sst5 , sts - b , and sick - r . our paper also outperforms trec , sston2 , and sts5 in the performance of our supervised downstream task . we also outperform trec and sston - b in the accuracy of our initialization strategies . the results are shown in table 8 .
table 6 shows the results for different training objectives on the unsupervised downstream tasks . the results are shown in table 6 . we observe that the performance of sts12 and sts14 outperforms cmow - r on all the downstream tasks except sts16 , which outperforms sts15 , sts13 , st14 , st15 , and st16 . the performance of these tasks on the supervised downstream tasks is significantly different from that of the other tasks .
as expected , the results of our model are shown in table 1 . the results of the model are presented in table 2 . our model outperforms the cmow - r model by a significant margin . our model has a higher accuracy than cmow - r by a margin of 0 . 6 points .
we find that the cmow - r model outperforms the sick - e model by a significant margin . it outperforms trec and mpqa by a large margin . we observe that the best - performing model is the one with the best performance in terms of sst2 , sst5 , and sts - b . the best performing model is cmowr , which outperforms all the other models .
table 2 shows the performance of the best - performing system in terms of all loc , per , and misc . the results are shown in table 2 . we observe that the best performing system is the one with the best performance in all loc and per . our system outperforms all the other systems in all three of these categories , except for supervised learning , which outperforms our system in all of them except supervised learning .
table 2 shows the results on the test set under two settings . the results of both settings are shown in table 2 . we find that the model with the highest f1 scores is the one with the best f1 score . the model with lower f1 and lower e + p scores has the highest τmil - nd score , and the model that has the lowest f1 - e + p score is the model without the supervised learning feature . our model has a higher f1 / e + p score than the other two models .
table 6 shows that g2s - gat outperforms all other models in terms of the number of features that are included in the model . the results are shown in table 6 . we see that the model that has the most features in it is the one with the highest level of attention to detail . the model with the most attention to details has the highest rate of accuracy . this is evident in the results of the models with the least amount of features in them .
table 3 shows the results of our model on the ldc2015e86 and ldc2017t10 datasets . we observe that g2s - gin outperforms the model on all the models , except for s2s , which outperforms all the other models on all of the models except the model with the highest bleu and meteor scores . we also observe that our model outperforms our model with respect to the model that has the lowest bbleu score .
table 3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 . we observe that the g2s - ggnn model outperforms the external model in terms of performance on the test set . it outperforms both external and internal models when trained with gigaword data , but outperforms external models when it is trained without gigawrd data .
table 4 shows the results of the ablation study on the ldc2017t10 development set . we observe that bilstm outperforms get and meteor in terms of the size of the model and the number of models in the model .
table 3 shows the results of our model in table 3 . we see that g2s - gin outperforms all other models in terms of s2s , gat , gnn , and gin - ggnn . we also see that our model outperforms the other models by a significant margin . the model with the best model has the best performance in all three domains . our model has a better performance than the other two models in all of the domains .
table 8 shows the results for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . s2s and g2s - ggnn are the two models that outperform the other models in terms of the fraction of elements in the output that are not present in the input ( added ) and the fraction that are missing in the generated sentence ( miss ) , respectively . the results are shown in table 8 . we see that gold outperforms the other two models by a significant margin . we also observe that the accuracy of our model is higher than that of the other model .
table 4 shows the performance of the 4th nmt encoding layer trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the results are shown in table 4 . we observe that the pos tagging accuracy is significantly higher than the sem tagging accuracy .
table 2 shows the results of our tagging accuracy with baselines and an upper bound . the results are shown in table 2 . the results of the tagging accuracy are presented in table 1 . we observe that the best tagging accuracy is achieved using unsupervised word embeddings . the best tag accuracy is obtained using unsupemb , word2tag , and word2 tag .
table 4 shows the results of our model on the pos tagging accuracy and sem tagging accuracy . the results are shown in table 4 . we see that our model outperforms all the other models in terms of the accuracy of our system . our model has the best performance on the sem and the pos tagging accuracy , and the worst performance on both the pos and sem tagging accuracy .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 . we observe that the pos tagging accuracy is significantly higher than the res tagging accuracy in all four layers of nmt encoding .
table 8 shows the performance of the attacker on different datasets . results are on a training set 10 % held - out . δ is the difference between the attacker score and the corresponding adversary ’ s accuracy . the results are shown in table 8 . we observe that the attacker performs better than the adversary on all datasets except pan16 .
table 1 shows the performance of pan16 when training directly towards a single task . the results are shown in table 1 . we observe that pan16 outperforms pan16 in terms of accuracy when trained directly towards the task . pan16 performs better than pan16 on all tasks .
table 2 shows the results of the protected attribute leakage in pan16 and pan16 . the results are shown in table 2 . we observe that the best performance is achieved when the data are split in a balanced manner , and the worst performance is obtained when the task is split in an unbalanced manner .
table 3 shows the performance on different datasets with an adversarial training . the performance on pan16 and pan16 is shown in table 3 . the performance of pan16 outperforms pan16 on all datasets except pan16 . we observe that pan16 performs significantly better than pan16 in terms of performance on gender , age , and gender - related features .
table 6 shows the performance of the protected attribute with different encoders . the results are shown in table 6 . we see that the rnn and rnn encoder perform significantly better than the guarded encoder . in particular , rnn performs better than guarded encoder in all three cases .
table 2 shows the performance of the model with respect to the work done on the model . the results of our model are shown in table 2 . we observe that our model outperforms all other models in terms of performance . our model performs better than all the other models , except for lrn , lrn + finetune , and lstm . it outperforms lrn and lrn + finetune by a significant margin . in addition , it outperforms gru by a large margin .
table 2 shows the performance of the model with respect to the base and base time . the results are shown in table 2 . the results of our model are presented in table 1 . we see that our model outperforms all the other models in terms of performance . our model has the best performance on the base time and the highest accuracy on the base time . our model also outperforms gru and sru by a large margin .
as shown in table 1 , the results of zhang et al . ( 2015 ) are presented in table 2 . the results of this model are shown in figure 1 . our model outperforms all the other models in terms of err , atr , gru , lstm , and sru . we observe that our model performs significantly better than all other models , but not as well as other models . for example , our model has a higher err than all the others , but it has a lower atr and gru .
table 3 shows the bleu score on wmt14 english - german translation task on tesla p100 . the results are shown in table 3 . decode : time in milliseconds used to decode one sentence measured on newstest2014 dataset . gnmt and olrn outperform all other models on the task . gnmt outperforms all the other models in terms of time in seconds . lrn outperforms most of the models , but not all of them . gru outperforms sru , atr , and atr . we observe that the case - insensitive tokenized model performs better than the case insensitive tokenized one - sentence decoder .
table 4 shows the results of our model on the squad dataset . the results are shown in table 4 . “ # params ” : the parameter number of base . we find that our model outperforms all the other models in terms of f1 - score . we also find that lrn and sru outperform all other models , except for gru and atr . lrn outperforms lstm , sru , atr , gru , and gru . rnet outperforms lrn by a significant margin .
table 6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the reported result lample et al . ( 2016 ) and lrn and sru are shown in table 6 . “ # params ” denotes the parameter number in ner tasks . the model with the highest number of # params is the one with the best performance on the task . lrn has the highest score on ner , while sru and gru have the lowest score .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . the results are shown in table 7 . we observe that the accuracy of the snli model is significantly higher than that of the ptb model on both tasks . our model outperforms both ptb and glrn on all three tasks .
table 2 shows the performance of oracle retrieval on the human and the human - based systems . the results are shown in table 2 . the results of the human - based system outperform those of the oracle system by a significant margin . the human system outperforms the oracle system by a large margin . the oracle system outperforms both human and human systems by a substantial margin .
table 4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 0 . top - 1 / 2 : % of evaluations a system being ranked in top 1 or 2 for overall quality . the best results among human evaluation are shown in table 4 , and the worst results among automatic evaluation are seen in table 5 . the results of automatic evaluation on content richness are also shown .
table 2 shows that the performance of europarl and ted talks is significantly better than that of the other two models . the performance gap between the two models is less than 0 . 5 points , but it is still significant . the difference in performance between the models is 0 . 3 points , or 0 . 05 points .
table 3 shows the results of our test set . the results of europarl and ted talks are shown in table 3 . we find that the performance of the two models is significantly better than that of the other two models . the performance of both models is comparable to that of both the other models . however , the performance gap between the two systems is much larger than the difference in the performance between the models .
table 3 shows that the performance of europarl and ted talks is significantly better than that of the other two models . the performance of ted talks outperforms that of all the other models , except that it is significantly worse than the results of the best - performing model , which are those of the worst - performing models . the difference in performance between the two models can be seen in table 3 .
we find that europarl outperforms all the other systems in terms of numberrels , totalroots , and totalterms . we also find that the depthcohesion score is significantly higher than the average of all the systems in our dataset . we see a significant difference in the performance of the two systems on the average . for example , we see that the average depth cohesion score of the three systems is 1 . 46 .
we find that europarl outperforms all the other systems in terms of depth cohesion . for example , we find that the average depthcohesion score of the system is 9 . 29 , which is higher than that of all other systems . however , we see a significant difference between the two systems in the number of numberrels and totalroots . we see that the difference in numberrels and totalroots is due to the large difference in the quality of the dataset .
table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . the results are shown in table 1 . ncdcg % is the difference between the baseline model and the enhanced version as we mentioned . the performance of our enhanced version of the visdial model is significantly higher than that of our baseline model . our enhanced version outperforms both baseline and enhanced versions of the model in terms of performance . in addition , our enhanced model outperforms all the other models in respect to the accuracy of our principles .
table 2 shows the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . in table 2 , we see that p2 outperforms both p1 and p2 on the visdial validation set in all cases except for the case where p1 outperforms p2 .
table 5 shows the performance of hmd - recall + bert on hard and soft alignments . the results are shown in table 5 . we observe that the performance on hard alignments is comparable to that of wmd - unigram and wmd - bigram . the performance of the wmd unigram + bert model is also comparable to wmd bigram + wmd - bigram , but not as much as the hmd - f1 model . our results also show that the accuracy of our model is significantly higher than that of the other two models .
table 3 shows the results of the direct assessment and sent - mover models in table 1 . the results of direct assessment are shown in table 2 . we observe that the best - performing model is bertscore - f1 , which has an average score of 0 . 716 points , while the worst performing model is ruse ( * ) which has a score of 1 . 06 points . the best - scoring model is meteor + + , which achieves 0 . 729 points .
table 3 shows the results of the set - up of sfhotel and bertscore - f1 . the results are shown in table 3 . we see that the performance of bert score and bleu - 2 is significantly higher than that of meteor . the difference in performance between the two sets of settings is significant . in particular , we observe that bert scores are significantly lower than those of meteor , but not as high as those of bagel . our results also show that the difference in the performance between bertscore - f2 and beta score is significant , but it is not as significant as the difference between the baselines .
table 2 shows the performance of the word - mover model on all three settings . the results are shown in table 2 . we see that the word - mover model outperforms all other models on all of the three settings except for leic .
table 2 shows the results of our model . the results are shown in table 2 . we see that our model outperforms all other models in terms of accuracy and performance . our model achieves a significant improvement over all the other models on average , with the exception of shen - 1 , which has a lower score than all the others on average .
table 2 shows the performance of the models in table 2 . we see that yelp outperforms all the other models in terms of transfer quality , transfer quality and transfer quality δsim . yelp has the best transfer quality of all the models . it outperforms yelp in all three of our models except for transfer quality , which has the worst transfer quality . we observe that yelp has a higher transfer quality than all other models except transfer quality . it also has the highest transfer quality in all models .
table 5 shows the results of human sentence - level validation of metrics for each dataset . the results are shown in table 5 . we find that human sentences are more accurate than machine sentences in terms of accuracy and semantic preservation . in addition , human ratings of semantic preservation are higher than machine ratings of fluency . our results show that human ratings are more reliable than machine judgments .
table 2 shows the results of our model . the results are shown in table 2 . we observe that our model outperforms all other models in terms of accuracy and performance . our model achieves a significant improvement in accuracy over all the other models except for m0 and m2 , where we observe a significant decrease in accuracy .
table 6 shows the results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( right table ) achieve higher bleus than prior work at similar levels of acc , but untransferred sentences achieve the highest bleu . our best model , fu - 1 , outperforms all other models in terms of acc ∗ . we also see that our best models outperform all the other models on yelp , except for our best model on yelp . the best models in yelp are the ones that perform best on the unsupervised model . the worst models are those that perform poorly on the supervised model , such as yang2018unsupervised . these results are shown in table 6 .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . we find that nested disfluencies are significantly more likely to be disfluential than those that are not nested . we also find that the number of disfluency tokens that are correctly predicted is significantly higher than the number that are incorrectly predicted . we also observe that rephrase tokens are more likely than repetition tokens to be correctly predicted . we observe that the accuracy of rephrase is higher than that of repetition tokens .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) or in neither . percentages in parentheses show the fraction of tokens belong to each category . reparandum , repair , and function - function are shown in table 3 . content - content and repair are the only two disfluency categories that are correctly predicted to be disfluential . function - function is the only one that is not disfluently predicted .
table 2 shows the performance of the model with the best test mean and best test best in terms of test performance . the model with best test performance is the one with the most innovations . it outperforms the model without innovations by a large margin . we find that the best model with innovations outperforms all the other models by a significant margin . the best model has the highest score on the test mean of all the models . the best models with innovations have the highest test performance on all the tests .
table 2 shows the performance of the state - of - art algorithms on the fnc - 1 test dataset . our model outperforms the state of the art algorithms in terms of accuracy . the accuracy of word2vec and self - attention sentence embedding outperforms both rnn - based and cnn - based sentence embeddings on the test dataset by a significant margin . the average accuracy of our model is higher than the accuracy of the other two algorithms , but not by much .
table 2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . it outperforms oe - gcn and neuraldater on all datasets except for the nyt dataset . the unified models outperform all other models except the nyt datasets on apw dataset . we observe that the unified model outperforms the previous models on all the datasets except nyt datasets .
table 3 shows the results of both word attention and graph attention for this task . this results show the effectiveness of both component models with and without attention . the accuracy of neuraldater and oe - gcn is shown in table 3 . we observe that the accuracy of neuraldater is significantly higher than that of oegcn and sgcn .
as shown in table 1 , the performance of all the models in our model is significantly better than that of cnn and dmcnn . in particular , the model with the highest level of accuracy is the one with the lowest level of error . the models with the best performance are the ones with the least amount of errors . for example , cnn and jmee have the highest percentage of errors in the first stage of the model .
table 2 shows the results of our cross - event model . the results are shown in table 2 . we observe that our model outperforms all other models in terms of accuracy and classification . our model achieves the best performance in the cross - event domain . it outperforms the other models by a significant margin . in addition , our model achieves a significant improvement in the accuracy of our model when compared to other models .
table 3 shows the performance of all the models in table 3 . the results are shown in table 4 . we see that all models perform better than all other models in terms of test acc and test wer . the results of all models are presented in table 5 . all models have a lower dev perp than all the other models , except for english - only - lm .
table 4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the results are shown in table 4 . we observe that fine - tuned training outperforms fine - tuned training on both the dev and test sets . the results of fine - tuned training outperform the results of cs - only training in both the test and dev sets .
table 5 shows the accuracy on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) and code - only - disc vs . code - synchronous ( cs - only ) and fine - tuned - disc ( fine - tuned - disc ) . the results are shown in table 5 . we observe that the performance of the two types of gold sentence is significantly different . the performance of cs - only and fine - tuning - disc is significantly worse than that of the other two types .
table 7 shows the performance of type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . the accuracy and recall of type combined gaze features are shown in table 7 . we observe that type combined features perform significantly better than type combined in all three datasets . the precision of the type combined feature is significantly higher than the accuracy of the other two datasets .
table 5 shows the performance of type - aggregated gaze features on the conll - 2003 dataset compared to type combined gaze features . we observe that type combined features perform significantly better than type aggregated features in terms of precision , recall , and f1 - score . the precision of the type combined feature is significantly higher than the precision of type combined .
table 1 shows the results on belinkov2014exploring ’ s ppa test set . hpcd ( full ) is from the original paper , and it uses syntactic skipgram . glove - extended is from our test set , and its embeddings obtained by running autoextend rothe and schütze ( 2015 ) on wordnet 3 . 1 . the results on the other test set are shown in table 1 . the results of the two test sets are summarized in table 2 . we observe that the results of both test sets show significant differences in the performance of the system . the difference in performance between the two tests sets can be seen in the following table . lstm - pp outperforms the other two tests set in terms of the type of synset embeddeddings . in particular , we observe that ontolstm and ontoglove - retro outperform both of the other tests set .
table 2 shows the results of all the models in table 2 . the results are shown in table 1 . we see that the system with the best performance is the one with ontolstm - pp ( full ) and ontooracle pp ( full ) , while the other two models with the worst performance are hpcd and oracle pp .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . we find that context sensitivity improves the performance of the model by 0 . 5 points . the effect of context sensitivity on ppa acc . is 0 . 4 points .
table 2 shows the results of adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun . the domain tuning scores are shown in table 2 . we find that domain tuning improves the performance of the domain - tuned model by a significant margin over the other two models . our domain tuning model outperforms all the other models in terms of bleu % score . in particular , our domain tuning models outperform all other models .
we find that the domain - tuned subs1m outperforms all the other domains , except en - fr and en - de , by a significant margin . the domain - tuned subs1m outperform all other domains by a large margin , but not all of the domains . in particular , the domains that are domain - untuned outperform the domains of all the domains in terms of performance . we observe that the domains with the best performance are those with the most domain - trained subs .
table 4 shows the results of adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . all results with marian amun are shown in table 4 . the results of all 5 captions are shown on the table . we observe that the best captions outperform the best automatic captions in terms of the number of captions . in addition , we observe that all five captions perform better than the best autocap 1 ( concat ) and all 5 ( dual attn . ) on average . for example , the best caption on flickr16 and flickr17 outperforms all the other captions on multi30k .
table 5 shows the results of the best - performing strategies for integrating visual information ( bleu % scores ) . all results using transformer , multi30k + ms - coco + subs3mlm , detectron mask surface , mscoco17 , en - de and en - fr are shown in table 5 . the best performing strategies are those that integrate visual information with dec - gate and enc - gate . we observe that the best performing strategy is the one that integrates visual information into the img ( img [ italic ] w ) with the highest bleu % score . our results show that the most effective strategies are the ones that integrate information into img with the lowest bbleu % .
table 3 shows the performance of the three models in terms of visual features . the results are shown in table 3 . the performance of all three models is significantly better than that of the other two models . in particular , the results of the two models with the highest number of features are those with the least visual features , such as those with text - only and multi - lingual features , and those with more features with the most features .
table 2 shows the performance of the yule ’ s i model against the mtld model in terms of ttr , mtld , and ttr - trans - back . the results are shown in table 2 . the ttr and mtld models outperform the ttr model by a significant margin . ttr is significantly better than mtld and en - fr - smt - back , but not as much as en - es - rnn - ff . the difference between the two models can be seen in table 3 . in particular , ttr outperforms mtld by a large margin .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the results are shown in table 1 . we see that the parallel sentences we used in the test and test splits are significantly smaller than those of en – fr and en – es .
table 2 shows the results of the training vocabularies for the english , french and spanish data used for our models . the results of our models are shown in table 2 . we find that our models perform significantly better in english and french than in spanish .
table 5 shows the performance of the automatic evaluation scores ( bleu and ter ) for the rev systems . the results are shown in table 5 . bleu scores outperform ter scores by a significant margin . ter scores outperforms both rev scores by an average of 0 . 3 points . the performance of rev is also significantly higher than that of the other systems .
table 2 shows the results on flickr . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled rsaimage is the rsaimage model from rsaimage . we find that vgs outperforms rsaimage on flickr by a significant margin .
table 1 shows the results on synthetically spoken coco . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled audio2vec - u is the auditory supervised model . the results on rsaimage are shown in table 1 . we observe that vgs performs better than rsaimage in terms of recall and recall accuracy .
we report further examples in the appendix . in table 1 , we show the results of the different classifiers compared to the original on sst - 2 . the results are shown in table 1 . the results of cnn and dan show that cnn turns in a screenplay screenplay screenplay that is at the edges , while dan turns in the edges of the screenplay screenplay . the difference between the two classifiers can be seen in table 2 .
table 2 shows the results of fine - tuning in sst - 2 . the symbols are purely analytic without any notion of goodness . the numbers indicate the changes in percentage points with respect to the original sentence . the last row indicates the overlap with the original sentences , and the last row shows the number of occurrences of each word in the final sentence . for example , , , and indicate that the total number of words has increased , decreased or stayed the same . a score of 0 thus means that fine - tuning has not changed the word count , while a score of 1 indicates that finetuning has increased or decreased it . the number of instances of a word in a sentence is also shown in table 2 .
table 3 shows the changes in sentiment score in sst - 2 . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . and indicate that the score increases in positive and negative sentiment . the results are shown in table 3 . the number of negative labels in the original sentence is shown in the first two rows . in the third row , the number of positive labels in negative sentences is shown . the numbers indicate the values of the negative labels .
table 1 shows the results of the sst - 2 and corr models . the results are shown in table 1 . the results of pubmed and sift are shown on table 2 . we see that pubmed outperforms corr by a significant margin .
