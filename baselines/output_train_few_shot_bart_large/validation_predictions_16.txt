table 2 shows the throughput for training and inference of our treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . table 2 also shows the improvement in inference performance over iterating over the plain recursive approach .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t .
the max pooling strategy consistently performs better in all model variations . table 2 shows the results for ud v1 . 3 , conll08 , sb and softplus models with different representation . it is clear from table 2 that the use of softplus improves the performance for all models , as hard core coreference problems are rare in softplus models , we do not need to assume whether softplus or sigmoid is the optimal representation for each model , simply consider the f1 score of the model with the best performing feature maps .
table 1 shows the effect of using the shortest dependency path on each relation type . our model obtains the best f1 score in 5 - fold test set without sdp and with sdp . the macro - averaged approach shows a significant performance gain .
the results are shown in table 3 . the first group shows that , for all models , the accuracy on r - f1 and f1 50 % is close to the state - of - the - art , while for those using y - 3 : y , it is considerably better .
the results of paragraph prediction accuracy are presented in table 1 . we show that our model achieves 100 % accuracy on average with respect to all three aspects of the essay prediction tasks .
as shown in table 4 , the average c - f1 score for the two indicated systems is 60 . 40 ± 13 . 57 % and 56 . 24 ± 2 . 87 % respectively , respectively .
the results are shown in table 3 . the first group shows the results of tgen + and tgen − trained models compared to the original models . as the results show , when the training data is cleaned , the performance of both tgen and sc - lstm models improves significantly . however , when training with original and clean data , the improvement is less pronounced , the second group shows that the performance gap between the original and the cleaned model is much smaller .
table 1 shows the e2e data statistics for the original and the cleaned version . our cleaned version has 17 . 69 % more distinct mrs and 1 , 358 fewer textual references compared to the original .
table 3 presents the results of tgen and original models trained on the same training data . the results are presented in bold . as the results show , tgen models significantly outperform the original models in terms of bleu , meteor , rouge - l and sist scores , table 3 shows that tgen model significantly improves over the original model when trained and tested with the same set of parameters . although the improvement is slim , it is encouraging to continue researching into ways to improve the feature extraction procedure for further improvements .
results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) and the number of instances with correct values ( add , removed , and cleaned ) are shown in table 4 . as a sanity check , we also evaluated each error individually , for both the original and the cleaned set .
the results are shown in table 1 . all models show lower performance than the previous state - of - the - art on the external test set . table 1 shows that the hierarchical clustering approach based on tree2str ( flanigan et al . , 2016 ) and ps graphlstm achieve all or close to all state - ofthe - art results on the external test set , while the multi - headed approach obtained by song et al . ( 2017 ) and parallelism achieves only 25 . 9 % and 28 . 2 % all - learnt results , respectively . the smaller performance gap between the two baselines is mostly due to the smaller size of the data set ( table 1 : alllear and cohen , 2019 ) and the high quality of the training data ( supplementary material ) .
table 2 shows the performance of our model with respect to amr17 . our model obtains 62 . 5 bleu points . by comparison , the previous stateof - the - art models achieve 57 . 6 bleu points and 21 . 6 % overall improvement . table 2 also shows that our model is comparable in terms of performance with both ensemble and single - model configurations .
table 3 presents the results for english - german and czech , compared to english - czech . the results are presented in table 3 . first , we report the average number of iterations per model for each language , followed by the type of error generation algorithm . for english , we observe that the bow + gcn model ( bastings et al . , 2017 ) obtains the best results with a single - model average of 43 . 8 % compared to the previous best performance of 36 . 6 % by birnn . next , we compare our model with other models that use multi - model learning models . we observe that , among all the models , ggnn2seq is the better performer on both english - and czech - language datasets .
the effect of the number of layers inside our model is shown in table 5 . the first group of layers decreases performance , while the second group increases performance . as the table 5 shows , the more layers inside the model , the better performance .
comparisons with baselines are shown in table 6 . rc refers to residual connections , while rc + la refers to connections with residual connections . as the table 6 shows , the rcn with the most residual connections outperforms the gcns with the least .
the results are shown in table 4 . the first group of results show that the simple dcgcn model outperforms all the alternatives with a gap of 10 . 2 % in performance . while the gap is slim , it is significant enough to warrant further study .
table 8 shows the ablation study results for the density of connections on the dev set of amr15 . the results show that removing the dense connections in the i - th row eliminates 21 . 2 % of the connections , which means that our model obtains 25 . 8 % higher performance .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . the results show that , under the best performing model , the domain coverage mechanism , domain attention and hierarchical attention are the most important aspects of the decoder performance . domain attention is beneficial , improving the overall performance by 3 . 8 % over the previous state of the art model . finally , as shown in table 9 , when domain attention is combined with hierarchical attention , the model improves by 2 . 9 % .
table 7 shows the performance of our initialization strategies on various probing tasks . glorot obtains the highest score , followed closely by our framework , with a gap of 10 . 8 points from our paper .
the results are shown in table 3 . we observe that the h - cmow model outperforms the cbow model on every metric by a significant margin . the difference is most prevalent in the sub - differences of length and bshift , subjnum and topconst are the most important components of our model , finally , we see that the presence of concatenated objects in the same paragraph improves the bshift metric by 3 . 6 points , though still performing substantially worse than the cmow model . as shown in the second group of results , the hierarchical clustering of objects improves over the monolingual approach .
the results are shown in table 1 . the first group shows that the cbow model outperforms all the alternatives except cmow in terms of sub - sst2 , sst5 , and mrpc scores . next , we see that the hybrid model obtains the best results , improving upon the previous state - of - the - art model by 3 . 6 % in sst2 score .
table 3 shows the relative improvements on unsupervised downstream tasks attained by our models . the cbow model shows a considerable performance gain over the hybrid model over the best previous state - of - the - art method . cmow also shows a significant performance gain . as shown in table 3 , when cbow is combined with cmp , it achieves 62 . 2 % overall improvement on sts12 and 62 . 6 % overall performance over the strong baselines . hybrid also shows significant performance improvement . with respect to sts15 , the difference between cbow and cmow is less pronounced , but still significant .
table 8 shows the performance of our system for initialization and supervised downstream tasks . glorot outperforms all the stateof - the - art systems in terms of all metrics except for trec score .
table 6 shows the performance of our method for different training objectives on the unsupervised downstream tasks . our cbow model outperforms the cmow model in terms of all metrics except for sts13 .
the results are shown in table 3 . the first group shows that the cbow model outperforms all the baselines except sasaki et al . ( 2012 ) in terms of concatenated keyphrases . it achieves state - of - the - art results on every metric with a gap of 10 . 5 points from the previous state of the art on average .
the results are shown in table 1 . the first group of results show that the cbow - r model outperforms all the alternatives except sst2 and sst5 by a significant margin . in particular , it improves upon the sub - category of mpqa by 3 . 6 points in sub - categories and upsampling .
table 3 presents the results of all loc and misc queries for english , spanish , french , dutch , russian and turkish . in general terms , the results are summarized in table 3 . name matching algorithm outperforms all supervised and unsupervised learning methods except for the case of greek word embeddings . it achieves the best results with an absolute improvement of 2 . 36 points over the previous state of the art model in terms of epm .
results are shown in table 2 . the first set of results show that trained models outperform the unsupervised ones in terms of e - 1 and f1 scores . in particular , trained models achieve the best results with an f1 score of 35 . 59 ± 0 . 59 and 35 . 42 ± 0 . 59 , respectively , compared to the previous state of the art on the test set . with respect to τmil - nd , the results are slightly less clear , but still show that the model with the best performance is derived from the training data . supervised learning improves the results for both sets , finally , the improvement is larger for model 2 than that for model 1 .
table 6 presents the results of models trained and tested on the same dataset . the results are summarized in table 6 . our model obtains the best results with an absolute improvement in both ref and precision .
the results are shown in table 1 . the first group of results show that g2s models significantly outperform the previous stateof - the - art models in terms of meteor , bleu and s2s on all datasets except for the one reported by konstas et al . ( 2015 ) . next , we report the results of the best performing models on the ldc2015e86 and ldc2017t10 datasets . note that the results reported in the previous literature are unadjusted for the fact that the training data are statistically significant only when they are available in the full set , which means that the actual improvement may not be significant even when using the raw data from all the published studies .
table 3 shows the performance of our model with additional gigaword data on the ldc2015e86 test set . our g2s - ggnn model improves upon the previous state - of - the - art model by 3 . 23 % in terms of bleu .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model obtains the best results with a bleu score of 62 . 42 and meteor score of 59 . 62 .
the results are shown in table 1 . we observe that g2s outperforms all stateof - the - art models in terms of average sentence length , average number of instances per sentence and average g2s - gin average distance . the difference is most prevalent in the last 10 % range , we see that , for example , between the two baseline models , gat has the best performance with an average of 3 . 51 % and 6 . 66 % increase in sentence average .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( added ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . s2s outperforms g2s in terms of both the fraction of missing elements and the miss metric , indicating that the model is more effective in generation of reference sentences with fewer errors . gold refers to the reference sentences . as shown in table 8 , the model that obtains the most errors is the one that produces the most gold - colored tokens .
table 4 shows the pos and sem accuracy using different target languages on a smaller parallel corpus ( 200k sentences ) . as expected , the pos features significantly improve as the training set grows ,
table 2 shows pos and sem tagging accuracy with baselines and an upper bound . word2tag significantly outperforms the unsupervised embeddings in terms of most frequent tags and sem tags .
table 3 presents the pos tagging accuracy and sem tagging accuracy for english , german , french , spanish , dutch , russian and turkish captions . for english captions , see § 3 . the results are presented in table 3 . in general terms , the system achieves the best results with 86 . 9 % accuracy on average .
pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . table 5 shows that for english target languages , the residual encoder performs best , while the uni encoder obtains the best performance . as shown in the table , the bi - domain approach further improves the pos accuracy over the strong bias of the original encoder .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . as shown in table 8 , the gender - based and age - based insults are the most difficult for the attacker to distinguish , followed by race and sentiment . finally , the presence of a race - based insult decreases the attacker ' s performance , but does not improve his overall score .
table 1 shows the performance of our model with respect to training directly towards a single task . our model obtains the best results with an accuracy of 83 . 2 % on the dial task . on the task 1 task , it achieves 81 . 4 % accuracy and 77 . 9 % accuracy on the mention task .
the results in table 2 show that gender - based and racial disparities in task prediction are the most prevalent , followed by age and gender . gender - based disparities are less prevalent , but still represent a significant amount of data leakage . finally , we see that the presence of a racial slur in the training data also contributes to the model ' s performance . as shown in the second group of table 2 , all the other protected attributes have a significant impact on prediction performance .
the performance of our model on different datasets with an adversarial training set is shown in table 3 . as the table 3 shows , gender - based and age - based features have the highest impact on the performance , followed by race and sentiment . finally , the presence of a race - based classifier in the training set further boosts the performance for gender - neutral features . gender - neutral feature - based models like pan16 and pan1616 significantly outperform the other ones .
as shown in table 6 , the rnn encoders perform comparably to the guards . the difference in accuracies between the protected and unencoders is most prevalent when the protected attribute is leaky . guarded embeddings perform better than leaky ones .
the results are shown in table 1 . table 1 shows that the training data for wt2 and ptb is significantly more than the previous state - of - the - art work on both datasets . the difference in performance between the baselines is most prevalent for ptb , with an absolute improvement of 3 . 36 % on average compared to yang et al . ( 2018 ) in terms of finetune fine - tuning . lrn also achieves competitive or better results than lstm and atr , with a gap of 2 . 59 % on the ptb and 3 . 97 % on wt2 .
table 3 presents the results of baselines trained and tested on the training data set of rocktäschel et al . ( 2016 ) . the results are presented in table 3 . as the results show , the training time and the average number of iterations for each parameter are the most important factors in the success of our model . the difference in performance between the base and the current state - of - the - art models is most prevalent in terms of acc time , table 3 shows that the combination of lstm and gru with the correct combination of features improves the generalization ability of the model , finally , the results also show that once redundancy removal is added , the model is able to further improve its performance .
the results of zhang et al . ( 2015 ) are shown in table 3 . table 3 presents the results of summaries of the last published results of amapolar and yelppolar time on the training and test sets . as the results show , the training time and the number of iterations for each model is the most important factors in the success of the model , both in terms of err and time taken to compile the results . the summaries presented in the previous section show that , for both datasets , the performance drop over the previous state of the art when using only plain averaged time .
table 3 shows the case - insensitive tokenized bleu score of our model on the wmt14 english - german translation task . the model obtains a 27 . 26 / 71 . 33 bleu score on average compared to the previous best performing model , olrn , with a gap of 2 . 67 / 0 . 40 and 6 . 28 / 2 . 59 points from the last published results .
table 4 shows the exact match / f1 - score of our model on squad dataset . as the results show , all the parameter numbers we considered had a significant impact on our model ' s performance . specifically , the number of parameters in our base set ( + elmo ) was the most important factor in determining the model ' s f1 score . table 4 also shows that the presence of elmo in the base set significantly improved the results for all models except for our lstm model . finally , we observed that atr had the worst performance
table 6 shows the f1 score of our model on the conll - 2003 english ner task . our lstm model achieved the best result with a score of 90 . 56 % in the three stages . the number of parameter numbers in our model is the most important factor in the performance . lrn has the least number of parameters , constituting less than 1 % of the total number of training instances . as shown in table 6 , the approach developed by lample et al . ( 2016 ) achieves the best performance with 245k training examples .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . with the base setting , our model improves its performance on both snli and ptb tasks . it achieves the best results with a gap of 9 . 56 % over the previous state - of - art model .
table 1 presents the system and sentence statistics for english , spanish , french , dutch , russian , turkish and turkish . retrieving the max number of words per sentence is reported in table 1 . for english , we show r - 2 and r - 4 while for spanish we show mtr . for both systems we show svm with trained and unsupervised word embeddings .
table 4 presents the results of human evaluation . our system obtains the best overall performance on grammatical and appropriateness ( gram ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that our system is well - equipped to perform in the low - resource settings . table 4 shows that , for example , human evaluation results in 44 . 2 % overall improvement over the previous best performance .
the results are shown in table 1 . table 1 shows that for all 3 domains , our approach outperforms the previous stateof - the - art approaches in terms of en , r and pt . as the results show , when trained and tested on the same dataset , the performance gap between europarl and ted talks is narrower , but still significant . in particular , the gap between en and tnt is much narrower than that between df and docsub , indicating that the training data quality of docsub is considerably better than that of df . finally , for the two domains , we also consider the feature - rich clustering performance of dsim and slqs , showing that it is comparable to the performance of ted talks .
the results are shown in table 1 . table 1 shows that for all 3 domains , our approach outperforms the previous state - of - the - art approaches on average . we observe that , for example , the en model outperforms both ted talks and wikipedia by a significant margin . on the other hand , the gap between en and pt is narrower still , with europarl achieving higher p < 0 . 01 and r > 0 . 03 .
the results are shown in table 1 . table 1 shows that for english , google translate outperforms both wikipedia and ted talks in terms of en and r = en . according to the table , the average number of frames per second for english and spanish is slightly more than for ted talks , but less than for df . as shown in the second group of table 1 , all models trained on the hclust dataset have significantly higher en scores than those on the df dataset .
as shown in table 1 , the average depth of our data is 11 . 05 , slightly less than the average of the previous state - of - the - art systems . europarl has the best overall performance with a maxdepth of 3 . 46 and a mindepth of 1 . 46 . according to the table , the maxdepth and averagedepth metrics are the most important factors in selecting the best features for a wiki submission .
in table 1 , we present the results of β - lexical feature extraction on the training data . the average number of roots per row is 1 , 527 , while the average length is 999 . europarl has the best overall feature extraction performance .
for the experiments of applying our principles on the validation set of visdial v1 . 0 . 3 , we compare the performance of our enhanced model with the original one ( lf ) and the enhanced version ( p1 ) . in table 1 , we report the ndcg % and r0 , r1 , r2 , r3 scores of our model , and the average number of iterations of hidden dictionary learning , respectively . the enhanced model obtains the best performance with a drop of 9 . 42 % in ncg % compared to the original model .
the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set is shown in table 2 . the best performing model is the one that applies p2 ( the history shortcut ) , followed by the best performing baseline model . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . the difference in performance between baseline and current state - of - the - art models is minimal , but significant .
comparison of soft and hard alignments on hard and soft alignments . the results in table 5 show that hmd - f1 has the better performance on both alignments , while wmd - bigram has the worse performance . further , we see that the recall function alone by itself results in significantly worse performance than the hmd pre - trained model .
the results are shown in table 1 . first , we report the average score of all the baselines and the average number of frames for each type of question for the direct assessment set . for example , meteor + + has the best average score while bertscore - f1 is 0 . 716 . next , we also report the results of baselines with the best multi - factor classification scheme , namely , ruse ( * ) and w2v , which achieves the highest average score . sent - mover also achieves the best results with a minimum of 0 . 866 .
the results are shown in table 1 . the first group shows that bleu and meteor both have good baselines with scores above 0 . 7 , while bertscore - f1 is only slightly better . next , we show the numerical results of the best performing baselines and the average number of frames per second for each setting . as the results show , the baselines are already well - equipped to handle the task of clustering large datasets with high recall scores .
the results are shown in table 1 . word - mover with bertscore - recall and meteor as the baselines achieves the best results . sentiment is relatively stable while word is the most difficult to predict . we observe that word is relatively easier to predict when the bert score is set to 0 . 939 , while for other baselines , it is only 0 . 866 . when using spice as the baseline , the results are slightly better , but still significantly worse than the original embeddings .
the results are shown in table 1 . the first group of results show that , for all models except shen - 1 , the approach that relies on lexical features with the best performance is para - para , i . e . that the model with the most features is the one that gets the highest accuracy . para - based models generally outperform the model without it , the second group shows that it is harder to train models with both lexical and syntactic features than those with it .
table 3 presents the results of model a and model b on the transfer quality , semantic preservation and fluency tests . the results are presented in table 3 . we observe that yelp significantly outperforms all the other models in terms of transfer quality and semantic preservation . semantic preservation is particularly strong , with a gap of 3 . 6 points from the last published results ( yelp , 2016 ) to our model ( kutuzov et al . , 2017 ) . finally , we observe that the drop in transfer quality between yelp and other models is less pronounced , but still suggests some reliance on superficial cues .
table 5 shows the results of human sentence - level validation for each metric . our approach verifies that the summaries generated by our model match the human ratings of semantic preservation and syntactic fluency . it also verifies the accuracy of the generalization algorithm , spearman ’ s [ italic ] ρ b / w α = 0 . 67 .
the results are shown in table 1 . the first group of results show that the model with the most error space is shen - 1 . the second group shows the results of models with the least error space . according to the table , m1 and m2 are the most accurate , while m3 and m4 are the worst .
results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . as shown in table 6 , the best models achieve higher bleus and acc scores than prior work at similar levels of acc , indicating that the training set with the best classifiers is well - equipped to handle the task . note that the definition of acc varies by row because of different classifiers in use . multi - decoder the best model achieves the highest acc score , and the average number of tokens per row is 22 . 3 , slightly higher than the previous best model by a margin of 0 . 024 . with respect to domain specific features , we note that for example , the multidecoder approach gives the best overall performance , but is worse than simple transfer .
in table 2 , we report the percentage of reparandum tokens that were correctly predicted as disfluent . as a baseline , we also consider the average number of repetition tokens and overall length of the reparandum length as well as the overall number of disfluencies for each repetition token , as these are the only two types that are considered in our data set that are strictly disfluential . table 2 shows that for all but one of the three types , the training set is more than balanced .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either the repareal and repair contexts , or in neither . the average number of tokens in each category is reported in table 3 . percentages in parentheses show the fraction of tokens belong to each category . as shown in the table , content - content is the most common type of disfluency , followed by function - function , while the last category is repair .
the results are shown in table 3 . we show that the text - based model with the best average dev mean and best average test best is the one that is trained and tested on the same dataset with the same training data . as the results show , once the innovations are added to the model , the performance on the test set becomes significantly better . moreover , the model becomes more interpretable with the addition of innovations as features as the training data get larger . finally , we show that for models trained on the single dataset , the number of iterations is relatively less than for those using both raw text and innovations .
the performance comparison of our model with the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . word2vec embeddings achieve the highest accuracy , with 28 . 43 % on average compared to the previous state of the art model ' s 24 . 43 % . as the results show , the model learns to reason over a large corpus of words in a relatively short amount of time .
table 2 shows the performance of all the methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . specifically , it improves the performance by 10 . 2 % over the previous best stateof - the - art model , ac - gcn , with a gap of 3 . 6 % in accuracy .
table 3 compares the performance of our approach with and without word attention . our approach obtains the best performance with respect to accuracy . with word attention , our model obtains 62 . 6 % accuracy and 65 . 2 % accuracy on average compared to the s - gcn of neuraldater ( 62 . 9 % ) and oe - ocnn ( 63 . 2 % ) .
the results are shown in table 1 . the first group shows that all models perform well on the one - to - n test set , with the exception of dmcnn , which obtains the best results . next , we show that the argument argument is the most important part of the model , followed by the embedding stage . after applying tn and tn , the model performs well on both 1 / 1 and 1 / n test sets , with a gap of 3 . 5 points from the previous best state - of - the - art model .
table 3 presents the results of cross - event event detection on the training data . the first group shows that event detection is beneficial , improving upon the k9 model by 3 . 7 points in f1 score . next , we present results for argument and event detection . all methods show significant improvements on event detection performance . cross - event event detection improves further ,
results are shown in table 3 . the first group shows that , for english - only and spanish - only learner , the dev perp and test acc metrics are relatively the same while for both languages , dev wer is significantly higher . next , we see that the best performance is obtained by the bilingual model , which shows the advantage of language adaptation . finally , we observe that fine - tuning the model after applying the best features gives the best results . as shown in the second group of table 3 , the model using lexical features alone achieves the highest performance , outperforming both the plain english and spanish learner .
results on the dev set and on the test set using only subsets of the code - switched data . as shown in table 4 , fine - tuning the model improves the results for both sets . cs - only training with 50 % train dev data improves the generalization ability of the model , improving the bias metric by 3 . 8 points in the standard task formulation and to parity in the gold - two - mention case .
the results in table 5 show that fine - tuning the dev set improves the performance for both monolingual and code - switched models , and upsampling the test set to improve the results for both sets . as the results show , once the gold sentence is selected , the model performs better on both test and dev sets .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvements in precision and recall are statistically significant ( p < 0 . 01 ) with respect to baseline , indicating that the type combined approach can significantly improve interpretability without a drop in performance .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . our approach shows a statistically significant improvement over the baseline model in terms of precision , r = k ∈ 0 . 32 , f1 = 0 . 03 and precision > 0 . 05 .
results on belinkov2014exploring ’ s ppa test set . syntactic - sg embeddings outperform wordnet , verbnet and lstm - pp on average . glove - extended tokens outperform original wordnet tokens , but do not exceed the level of syntactic error . further improving performance by 1 . 7 points over the original hpcd model . ontolstm is the best overall result on the test set , with a gap of 0 . 8 points from the original paper .
results in table 2 show that the hpcd model outperforms the original rbg model with features coming from various pp attachment predictors and oracle attachments .
table 3 shows the effect of removing the sense priors and context sensitivity ( attention ) from the model . the model achieves the best performance with a ppa acc . of 89 . 5 and 87 . 4 % respectively .
for marian amun et al . ( 2017 ) , we report the bleu % scores of the models using subtitle data and domain tuning for image caption translation ( table 2 ) . the results are slightly better than the results with en - de and mscoco17 models , but still significantly worse than en - fr and flickr17 , both for multi30k and full30k datasets . as the results show , incorporating subtitle data improves the results for both models , further , the improvement is larger when the ensemble is tuned with domain tuning . finally , when domain tuning is added , the improvements are even larger .
the results of domain - tuned h + ms - coco compared to monolingual models are shown in table 3 . the results show that , for brevity , we only report results for en - de and en - fr datasets , while reporting results for mscoco17 and subs1m is reported in supplementary material . specifically , we report a and t as well as a / f1 scores for both sets , with a improving as the improvement gets larger .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . as marian amun et al . ( 2017 ) report , the automatic captions approach improves the general performance for all models except for those using multi30k images . the results show that , for example , when using only the best 5 frames , the model with the best captioning performs better than the model using both monolingual and dual attn tags .
comparison of en - de , en - fr and mscoco17 visual information strategies for integrating visual information ( bleu % scores ) . all results using transformer , multi30k + ms - coco + subs3mlm , detectron mask surface , etc . as shown in table 5 , the use of encoder and decoder improves the overall bleu % score of all models , encoding the layers of the image brings the model closer in visual information integration . however , it does not improve the overall w or img scores . further improving performance by high margins
the results are shown in table 3 . as en - de et al . ( 2017 ) report , the multi - lingual approach outperforms the monolingual approach we observe that , for example , the multilingual approach by far improves the results for all models except for those using ms - coco . further improving performance by high margins , we observe that the ensemble - of - 3 approach by itself improves the generalization ability of the model , improving it by 3 . 71 points in the standard task formulation .
the results are shown in table 1 . the first group shows that en - fr - ht and en - es - ht achieve the best results with a gap of 9 . 27 % and 2 . 87 % respectively over the previous best state - of - the - art model , respectively . next , we compare these models with the best previous models in terms of ttr , mtld and 1000 word embeddings .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . en – fr has 1 , 467 , 489 total sentences , while en – es has 5 , 734 .
the training vocabularies for the english , french and spanish data used for our models . table 2 shows that our model can learn to predict the most common words for each language with ease .
table 5 shows that en - fr - rnn - rev and en - es - trans - rev systems achieve comparable performance with respect to ter . however , for rev systems , the bleu and ter scores are slightly higher , indicating that the translation task is more difficult .
table 2 shows the results for flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the one supervised by us . our model obtains the highest recall @ 10 and average mfcc .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled u is the audiovisual supervised model . the acoustic embeddings based on audio2vec - u outperform all the alternatives except for segmatch in terms of recall @ 10 .
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . the first example in the table shows that rnn can turn in a screenplay that is at the edges , and that is easily distinguishable from a blank screenplay . next , we report examples of dan and cnn using the learned sentence structure . it is clear from table 1 that dan has the advantage of learning sentence structure that is easier to learn than rnn .
table 2 shows the pos changes in sst - 2 since fine - tuning . the numbers indicate the changes in percentage points with respect to the original sentence . as the table indicates , the number of occurrences has increased , decreased or stayed the same as a result of the improvements in the part - of - speech metric . further , the average number of instances has increased as well as the percentage of instances in the correct sentence .
as shown in table 3 , the sentiment score decreases as a result of the flipped labels being flipped from positive to negative . the flipped labels cause the sentiment scores to decrease as well .
table 1 presents the results of pubmed and sst - 2 . results show that pubmed outperforms both published and unpublished work on every metric by a significant margin .
