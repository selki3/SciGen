table 2 shows the throughput numbers for training and inference on our treelstm model using tensorflow ' s iterative approach and fold ' s folding technique . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . table 2 compares the performance of our iterative and recursive approaches with the best performing single - headed approach .
table 1 shows the throughput numbers for the treernn model using a balanced and a moderately balanced dataset . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization . the smaller size of the balanced dataset also hurts performance , leading to a drop of more than 10 % in throughput as compared to the linear baseline .
the max pooling strategy consistently performs better in all model variations . it is clear from table 2 that selecting the best filtering size and the dropout probability are the most important factors in model performance . the softplus representation also improves over the original conll08 model , leading to a better model performance overall . however , ud v1 . 3 shows a slight performance drop when using the sigmoid representation instead of softplus , indicating the importance of the number of tokens in the training data . we conjecture that softplus also helps the model perform better when using a larger data set .
table 1 shows the results for relation prediction using the shortest dependency path on each relation type . the macro - averaged approach shows a significant performance gain over the best f1 model without sdp , and slightly worse performance with sdp .
we observe that the performance gap between the best performing models on the " y " - 3 and " y - 3 " datasets is narrower than that between the " r - f1 " and " f1 " . as a result , we observe lower performance on the y - 3 dataset for both datasets when using only 50 % and 100 % accuracy on the training data .
table 1 presents the results of paragraph prediction accuracy on the competitive test set of hotpotqa in the distractor and mate setting , respectively . the results are presented in table 1 . our model outperforms all the stateof - the - art parsers except for mst - parser in terms of paragraph accuracy .
we note that the average performance of our system is slightly lower than the majority performances over the two indicated systems , indicating that our parser performs better at the paragraph level .
table 1 shows the results for the original and the cleanup experiments . the results are broken down in terms of training and test set performance . the results show that the original model performs better than the cleaned model when trained and tested , indicating the advantage of redundancy removal . however , when the training data is removed , the results are slightly worse than the original , indicating that tgen models perform better when training and testing with fewer errors . table 1 also shows that the performance gap between original and cleanup model is narrower when training with sc - lstm instead of tgen .
table 1 compares the results of our original e2e dataset with the cleaned version . the difference in statistics between the original and the cleaned versions is less pronounced for the train dataset , but larger for the test dataset . as expected , the smaller difference in ser between the two sets indicates that our cleaned dataset is more interpretable .
table 3 presents the results of training and testing on the original and the improved tgen models . the results are presented in bold . the results show that the original model performs better than the original tgen model when trained and tested on the same dataset . as the results show , the advantage of redundancy removal is most prevalent on datasets trained on sc - lstm , indicating that it is better to rely on pre - trained knowledge instead of random guessing . however , when trained on a larger dataset , the results are slightly less clear , with the exception of those on relis . relis outperforms original and tgen , showing that pre - training knowledge is beneficial , but does not help predictive performance .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . the errors we found were mostly caused by errors caused by adding instances that were already in the training data ( i . e . missing , wrong values , slight disfluencies ) , as shown in table 4 . however , there were a few instances where the correct values were found to belong to different training instances . these errors show that tgen is indeed able to pick out instances that are missing or slightly wrong ( e . g . the number of correct answers for " add " and " miss " a lot more often than expected by random chance .
table 2 presents the performance of our model with respect to entity clustering . our approach outperforms all the base lines with a gap of 10 . 9 % on average compared to the previous state of the art . we observe that our model performs best when trained and tested on a single entity dataset .
table 2 presents the performance of our model with respect to amr17 . our ensemble model achieves 24 . 5 bleu points , a slight improvement over the previous state of the art on seq2seqb ( beck et al . , 2018 ) and surpasses our strong ensemble performance by 3 . 5 points . additionally , our model achieves a significant performance gain over the best ensemble model , surpassing the previous best performance by 4 points .
table 1 presents the results for english - german and czech , both for the original and the debiased embeddings ( bow + gcn ) . as expected , the performance gap between the two sets is much narrower , with seq2seq achieving gains of 1 . 8 and 2 . 4 points over the single model , respectively . table 1 compares the performance of the single and multi - model approaches . the results are broken down by language , with the largest gains coming from english , followed by czech , german , and french .
table 5 shows that the number of layers inside a network is the most important factor in model performance . the performance gain is most prevalent in the second layer , where we see 3 . 5 % overall improvement over the performance of the first layer .
comparisons with baselines are shown in table 6 . we observe that the performance gains from rcn with residual connections are larger than those with no residual connections . moreover , the difference between rcn and rcn + la is less pronounced when we only consider connections with rcn as the baselines , indicating that the model performs better when using only rc and residual connections instead of mixing the two .
the results are shown in table 4 . our model outperforms all the stateof - the - art models with a gap of 10 . 2 points from the previous state of the art on all metrics .
table 8 shows the results of an ablation study of our model for the density of connections on the dev set of amr15 . the results show that our model obtains the best performance with a reduction of connections in the dense connections .
table 9 shows the results of an ablation study for modules used in the graph encoder and the lstm decoder . we used the best performing model , dcgcn4 , for both experiments . the results show that the domain coverage mechanism and the hierarchical attention are the most important aspects of the decoder performance . also , we tried using " - linear combination " and " - direction aggregation " to improve the results .
table 7 shows the performance of our initialization strategies on various probing tasks . glorot initialization outperforms other approaches that rely on tnn and topconst initialization , confirming the value of the importance of depth and length . subjnum initialization is particularly important for scalability , as it reduces repetition and allows the model to learn more compact regions . it achieves state - of - the - art results , as shown in table 7 , with precision scores improving consistently over other initialization strategies .
table 3 presents the results on object prediction using the best performing method . our h - cmow model outperforms the strong baselines on every metric by a noticeable margin . the results are particularly striking when we consider the fact that precision is relatively low while recall is high , indicating that object prediction requires a high level of interpretability .
the results are shown in table 1 . we observe that the cbow model outperforms both cmow and cmp . hybrid models outperform both monolingual and asymmetric approaches , showing that the advantage of redundancy removal comes from a better model design . the results of cbow / 784 show that it is better than cmow at selecting the correct sub - juris or label for each sub - category and its variants , and outperforms trec and sick - e .
table 3 shows the performance on the four downstream tasks that our models performed best on when trained and tested on the hidden test set of sts13 , 14 , 15 and 16 . as expected , the cbow approach outperforms both cmow and cmp . however , the biggest gains are seen on sts16 , where cmow obtains a performance gain of more than 44 % . hybrid also shows a significant performance gain , but only when trained on the unsupervised dataset .
table 8 presents the performance of our system for initialization and supervised downstream tasks . glorot outperforms all the stateof - the - art methods except trec by a significant margin . it closely matches the performance on subj and cr datasets , and closely matches trec ' s performance on sst datasets , confirming the value of parameter sharing .
table 6 shows the scores for different training objectives on the unsupervised downstream tasks . our approach outperforms the best previous approaches across the four sub - topics .
table 1 shows the performance of our method compared to other approaches on the hidden test set of somo and wc . our approach obtains competitive or better results than all the other methods apart from the case of subjnum , indicating the advantage of our data augmentation technique . the results are particularly striking on the subtasks of length , bshift and topconst , where our approach significantly outperforms all the alternatives .
the results are shown in table 1 . the first group of results show that the cbow classification system outperforms all the alternatives except sick - r on all subtasks except sub - sst datasets . specifically , the performance gap between cmow - r and cbow - c is much narrower , with the former achieving gains of 3 . 6 points and 6 . 2 points over the sick scores .
the results are shown in table 1 . supervised learning outperforms all stateof - the - art approaches except for the slightly better performance on loc and per when using only loc and misc datasets . in particular , the performance gap between supervised and unsupervised learning is much narrower , with the exception of name matching , which shows a larger performance gap . name matching is the most difficult part of the task for any system to solve , as it requires input from multiple sources and requires a lot of data . fortunately , this gap is less pronounced for supervised learning , as shown in the second group of results . the performance gap is narrower with respect to per , indicating that more data is required to solve this difficult task . finally , we notice a small performance gap with the best performing approach being the τmil - nd model .
table 2 presents the results on the test set under two settings . we use the best performing model , τmil - nd ( yuan et al . 2018 ) , as our baseline . the results are shown in table 2 , with accuracies of 15 . 38 ± 1 . 03 and 29 . 45 ± 0 . 59 on the f1 test set , respectively , compared to the performance of either the original model or the best supervised model . with the former setting , precision on f1 is higher than the former best state of the art , confirming the effectiveness of the supervised learning approach . supervised learning also boosts precision on both test sets , however , the biggest gains are seen on the name matching set , which shows the diminishing returns from mixing training data with random chance . finally , when using only epm feature - values , precision drops significantly , leading to a lower f1 score .
table 6 presents the results of model training on the hidden test set of hotpotqa in the distractor and fullwiki setting , respectively . the results are presented in table 6 , the performance gap between the " g2s " model and " ggnn " model is slim , with g2s - gat achieving close to 50 % performance on average . however , the difference between the two is less pronounced when we consider the model with " - gat " as the name implies . when using only the original neural network , the performance gap is much larger , reaching up to 99 % .
table 3 presents the results of experiments on the hidden test set of bleu and meteor using the best performing models from the previous literature . results are presented in table 3 . the performance gap between our model and the previous stateof - the - art is slim , with our g2s model performing better than both the best previous models on both datasets .
table 3 shows the performance of our model with additional gigaword data on the ldc2015e86 test set . our g2s - ggnn model outperforms all the previous models with a large margin . the difference in performance between the original and the best performing model is most prevalent when gigawords are used as training data .
table 4 shows the results of the ablation study on the ldc2017t10 development set . our model outperforms all the stateof - the - art models except meteor by a significant margin .
the results are shown in table 1 . we observe that the g2s model outperforms all stateof - the - art models with a gap of 1 . 51 % to 0 . 79 % on average in terms of sentence length .
table 8 shows the results for the test set of ldc2017t10 . our model outperforms the s2s model and g2s - gat model in terms of the fraction of elements in the output that are missing in the generated sentence ( miss ) , confirming the effectiveness of our model .
table 4 shows the pos and sem accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . as expected , the pos features significantly improve over the original nmt model , showing that the semantic features are more useful for target languages . sem also shows a significant drop in performance compared to the original model .
table 2 compares the pos and sem tagging accuracy with baselines and an upper bound . our embeddings outperform both the best previous approaches . word2tag significantly outperforms unsupemb in both pos metrics and sem metrics . the difference in accuracy between the baselines is most prevalent in the more frequent tags , confirming the value of baselines as a lower bound on prediction accuracy .
table 4 presents the system ' s performance on the four types of target objects . the results are presented in table 4 show that precision is relatively consistent across all metrics with the exception of the pos tagging accuracy , with a gap of 10 . 8 % on average between the two groups .
pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we observe that the first layer performs best , followed by bi and res , with a gap of 10 . 5 points from the last iteration .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . as shown in the table , gender and race are the most important classifiers for the attacker , followed by age and sentiment . overall , the attacker performs better than the adversary on all datasets except pan16 .
table 1 shows the performance of our system with respect to training a single task . our system outperforms all stateof - the - art methods with a large margin .
table 2 shows the results for balanced and unbalanced data splits . as expected , the gender and race disparities are the most prevalent , followed by age and sentiment . the balanced data splits result in lower accuracy , but higher precision . overall , our model outperforms all stateof - the - art models with two tasks .
the performance of these models on different datasets is shown in table 3 . as a baseline , we also consider whether the presence of a gender or a race - based classifier in the training data contributes to the model ' s performance . gender - neutral features contribute significantly less than race - neutral ones , however , leading to higher performance . we observe that gender - neutral mentions are the most important for the model to perform well , followed by age and race . finally , we observe that the leakage metric is relatively low , indicating that the model is well - equipped to handle the task at hand .
the results are shown in table 6 . the rnn encoders perform better than guarded and embedding guarded when the protected attribute is encrypted . however , the difference between rnn and guarded is less pronounced , the difference in performance between the protected and unencoders is minimal , however it is statistically significant . it can be observed that when an encoder is guarded , the model performs better overall .
table 1 presents the results of model training on the hidden test set of hotpotqa in the distractor and fullwiki setting , respectively . the results are presented in table 1 . we observe that the training size of the models is relatively small , with the largest group ( around 15 . 5m ) clocking in at the end of a training session . however , this small size does not represent a significant performance drop compared to previous work , as shown in yang et al . ( 2018 ) . the lrn model clas s performing better than the lstm model on both datasets with a gap of 2 . 7m params from the last published results ( yang et al . , 2018 ) . table 1 summarizes our results on hidden test sets . retrieving the hidden messages from the training data is crucial for the model to perform optimally , as lrn only has a limited number of parameters and performs poorly on datasets with finetune . finetune - based training sets outperform wt2 and ptb , indicating the advantage of finetuning over tuning alone . as shown in the second group of the table , the performance gap between lrn and lrn is narrower still , with lrn reaching 7 . 45 % on average compared to lrn ' s 15 . 45 % .
table 3 presents the results of baselines trained and tested on the training data from rocktäschel et al . ( 2016 ) . the results are summarized in table 3 . the first set of results show that the lstm model is comparable to previous stateof - the - art models in terms of performance on all metrics with the exception of time . however , the difference between the two sets is most prevalent on datasets with fewer training examples , table 3 shows that the larger size of training data leads to a lower performance on some of the more difficult tasks . when training with only one parameter , the error reduction is much larger than on the other two sets .
table 1 presents the results of experiments on the amapolar and yelp datasets from zhang et al . ( 2015 ) and ( 2015 ) . the results are summarized in table 1 . as expected , both the err and the time averages of the training epochs are significantly lower than those of the previous work ( i . e . , ama full time vs . yelp full time ) . table 1 summarizes our results on yelp and ama datasets , both for the standalone and the combination of the two datasets . the performance gap between the two sets is slim , with the former achieving an err / yahoo time of 4 . 57 and a full time time of 1 . 86 , respectively , while the latter is 4 . 45 and 1 . 68 respectively .
table 3 presents the case - insensitive tokenized bleu score of our model on the wmt14 english - german translation task . lrn outperforms all the other methods except atr and sru with a large margin . gnmt even gets a slight improvement over lrn on the german translation task , but is still inferior to lrn and olrn in terms of generalization . as shown in the table , the smaller training size and the shorter time to decode the sentence mean that lrn is better at predicting sentence structure and sentence structure . also , the model performs better on the spanish translation task as shown in table 3 .
table 4 shows the exact match / f1 - score of our model on the squad dataset . the model performs well with a f1 score of 76 . 14 / [ bold ] 83 . 83 and 85 . 83 % on average compared to the previous state of the art on the same dataset ( wang et al . , 2017 ) . as the results show , incorporating only elmo improves the model ' s performance , and upsampling has a generally positive effect ( p < 0 . 001 ) . however , when using only sru as parameter , the model performs slightly worse than the other methods . at the same time , the number of parameters used to parameterize the model increases , indicating the scalability of parameter sharing . finally , our model performs better when using atr as a scalability baseline . we observe that atr has the best generalization ability .
table 6 shows the f1 score of our model ( lstm ) on the conll - 2003 english ner task . our model obtains the best performance with a f1 - score of 90 . 56 . the difference between the reported result and the actual result is less pronounced for lrn , indicating that lrn has better interpretability . gru also outperforms lrn in terms of f1 scores . sru , on the other hand , achieves the best overall performance .
table 7 shows the performance of our model on the snli and ptb tasks with the base and the best performing ln setting . with the base setting , our model exhibits the best performance on snli task and the worst performance on ptb task .
table 1 presents the system and word analogy scores for english and german . the results are presented in table 1 . oracle outperforms both human and system when trained and tested on the training data . retrieving the most important words from training data is easier than both systems with a minimum of 50 % accuracy . sentence quality is relatively high with both systems using the current set of features . word analogy quality is high with more than 90 % accuracy on both systems . we observe that human and oracle are comparable in terms of sentence quality , with the exception of sentence prediction quality .
table 4 presents the results of human evaluation . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that our system is well - equipped to perform in the low - resource settings . retrieval is only slightly better than human on three of the four metrics ( appr , k 500 , k 1000 ) and is slightly worse than seq2seq on four of the five metrics ( gram , appr , and content richness ) . overall , our system performs better than all the other systems except candela .
table 1 shows the performance of our approach compared to the best previous approaches on text - similarity test set " ted talks " and " europarl " . the results are broken down in terms of p and r scores , with p indicating that our approach outperforms all the other approaches except " docsub " by a significant margin . table 1 also shows the results of clustering test set using the best performing feature set on the " edinburgh news " dataset and the " hclust " dataset .
table 1 shows the test bias scores for english and spanish on the " ted talks " dataset and " europarl " dataset . the results are presented in table 1 . as expected , the performance gap between en and europarl is much narrower , with p < 0 . 01 on both datasets indicating that the text classification performance of the text - similarity based models is superior . on the other hand , the gap is much larger with the " hclust " dataset , showing that text classification performs better than both text classification and semantic clustering . table 1 compares the performance of " docsub " and " slqs " with " tf " . we notice that " tf " outperforms both " lex and docsub " in both datasets , indicating the competitiveness of text classification . finally , we notice a drop in performance between " h " and " - parl " .
table 1 shows the test bias scores for english and spanish on the " ted talks " dataset and the " europarl " dataset . the results are presented in table 1 . we observe that the performance gap between en and europarl is narrower than that between ted talks and other datasets , indicating that training on the ted talks dataset exposes the training data to a higher level of bias . also , the difference in p scores between en ( from 0 . 0270 to 0 . 5295 ) and r = 0 . 37 shows that the text classification performance of ted talks is significantly worse than that of other datasets . table 1 also shows the results of clustering using the " hclust " and " sim " subsets . as expected , clustering performance drops significantly on both datasets .
we show the performance of our system on the four metrics . the first set of metrics shows that our system performs well on all metrics with the exception of depth cohesion . according to the table , the average depth of our data is 11 . 05 , which marginally leads to a better performance than the previous state of the art . europarl , on the other hand , performs better than the other metrics with a gap of 3 . 46 points from the last published results .
table 1 presents the results on eurparl and docsub datasets . the first set of results show that , overall , the quality of our dataset is considerably better than the competition . europarl has the best overall performance with a gap of 9 . 73 / 10 . 29 points from the last published results .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . 3 is presented in table 1 . the enhanced version of our model ( lf ) outperforms the original version ( kutuzov et al . , 2016 ) in terms of both question type and answer score sampling . also , it achieves a significant improvement in loss performance over the baseline model ( r0 - r1 ) . the difference in performance between the enhanced and the original model is less pronounced , but still indicates significant performance improvement . it is clear from table 1 that the enhanced model performs better than the original one in all aspects .
performance ( ndcg % ) of ablative studies on different models on the visdial v1 . 0 validation set . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 . the model performing best on the validation set is the one using p2 as well as the coatt layer , indicating that p2 methods are more effective than applying only p1 .
table 5 compares the performance of our hmd - f1 model with prior work on hard and soft alignments . the results are presented in table 5 . our hmd model significantly outperforms the previous stateof - the - art on both soft and hard alignments , both when using the best performing feature set and when using only bert .
the results are shown in table 1 . the average score of bertscore - f1 is 0 . 685 , slightly higher than the previous best state - of - the - art on all metrics except for meteor + + , indicating that bert score is significantly better when trained and tested on a larger dataset . when trained only on smd + w2v dataset , the results are slightly better , but still significantly worse than other approaches .
the results of bertscore - f1 are shown in table 1 . the system performs well on all metrics with the exception of " sent - mover " . when using the w2v dataset , the bleu scores drop significantly , indicating that the semantic information injected into the model by the additional cost term is significant enough to result in a significant drop in performance . on the other hand , when using meteor as a baseline , the results are slightly better , reaching 0 . 63 and 0 . 66 respectively . when applying the sfhotel metrics , the system performs better than both the baselines .
the results are shown in table 1 . word - mover and sentence prediction accuracy are relatively consistent across all metrics with the exception of leic ( * ) score , which shows significant performance drop when using only meteor or spice scores . sentance accuracy is relatively consistent with the performance of word prediction with the help of bertscore - recall , indicating that the semantic information injected into the model by the additional training data is significant enough to result in a significant improvement in prediction accuracy . the semantic information quality is also consistent across the board , showing that semantic information is the most important part of the prediction performance . we observe that the clustering quality of the word prediction is high when using all the training data we have available to us .
the results are shown in table 1 . we observe that the performance reach the best when we only consider models trained on simnet with the shen - 1 classification scheme , and only slightly outperform models trained only on python . the performance gap between the two approaches is slim but significant , with m1 achieving competitive or better results than the best performing model without it .
table 3 presents the results on transfer quality , semantic preservation and fluency . the results are statistically significant with respect to all three metrics , with the exception of transfer quality . we observe that yelp performs better than google translate and semanticrecovery when trained and tested on the same dataset , indicating that the semantic features captured by yelp are more important than the transfer quality or the semantic preservation . table 3 shows the results of training and testing on the two datasets with the same training data . semantic preservation and transfer quality are the most important aspects of the model performance . the results show that yelp significantly improves its transfer quality and semantic preservation scores over other models . fluency is the only case where the model performing worse than the other two when tested on both datasets .
table 5 shows the results of human sentence - level validation for each metric . our approach verifies the accuracy of our summaries , and the human ratings of semantic preservation . the results are statistically significant ( p < 0 . 01 ) with respect to both metrics , confirming the effectiveness of our model . however , the difference between human and machine judgments on acc metric is less pronounced , which indicates that human judgments are more accurate .
we show the results of models trained only on simnet with the shen - 1 score as a metric for error reduction . the results are summarized in table 1 . we observe that the performance gains are most prevalent on models trained with only one type of language classification pre - training , namely , those using the lexical classification of " cyc " and " lang " . para - based models perform best with a minimum of 28 . 5 % overall improvement over the performance of models using only λ - para classification .
table 6 shows the bleu and acc scores of our model using only 1000 transferred and human references , compared to the best previous work on yelp sentiment transfer . our best models ( the ones using the best classifiers perform higher than prior work at similar levels of acc , indicating that the training data are more useful for sentiment prediction . however , the results are slightly worse than those using simple transfer . the best model is the one trained on yelp in yang et al . 2018 ( yang2018 ) .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . the shorter reparandum length and the higher overall number of repetition tokens show that nested disfluencies are less common , but still represent a significant drop in performance compared to rephrase tokens . also , the average number of tokens per repetition token is shorter than in the other experiments , indicating that more training instances may need to consider the role of repetition in prediction .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either the reparland and repair contexts , or in neither . as shown in the table , content - content and function - function tokens are the most common types of tokens in the disfluency category , followed by repair tokens . percentages in parentheses show the fraction of tokens belong to each category . the average number of tokens per phrase is slightly less than the average of the other two categories , indicating that more than half of the tokens in each category belong to the correct reparandance category .
the results are shown in table 1 . we observe that the text classification task is easier for the model to perform in the late stages when innovations are considered , compared to the early stages when text classification is used . moreover , the dev mean is higher when using both innovations and text classification , confirming the importance of the role of innovations in the development of the model . as expected , the performance gap between the two approaches is slim , with the former performing best when using only text classification .
performance comparison of our model with the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model shows marked performance improvement over the state of the art on both datasets . the difference in accuracy between the original embeddings and the best performing rnn model is most prevalent on the micro f1 dataset , where our model obtains an accuracy improvement of 3 . 43 % over the previous stateof - the - art model .
table 2 compares the performance of the different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . specifically , our model ( ad3 ) shows marked performance improvement over the best previous approaches .
table 3 compares the performance of our approach with and without word attention . our approach obtains the best performance with a 62 . 6 % accuracy ( t - gcn ) on the word attention task . the accuracy is slightly higher than neuraldater ( 63 . 9 % ) on the graph attention task , however , showing the effectiveness of both attention methods .
the performance gap between the best performing and worst performing approaches is most prevalent on event prediction , embedding + t and argument prediction outperform all other approaches except dmcnn , indicating the advantage of data augmentation . the best performing approach is the jmee model , which shows marked performance improvement on both event prediction and on - stage performance . further , the jrnn model exhibits the best generalization ability . it closely matches the performance of the best previous approaches , with the exception of the argument prediction stage .
table 1 presents the results on event identification and event classification . the results are presented in table 1 . first , we observe that cross - event event identification is beneficial , improving upon the performance of both manual and automatic detection . moreover , it improves the recall scores for both classes , with the boost being most prevalent for argument identification . as expected , automatic detection is beneficial for both classification and event identification , improving the recall numbers for both groups .
the results are shown in table 1 . the first group shows that fine - tuned models perform better than all the alternatives except english - only - lm when trained and tested on the same dataset ( except spanish - only , which is slightly better ) . the second group shows the results of training and testing on a larger corpus , both on the training and test set , with different language combinations contributing differently to the results . as expected , the results are slightly worse for english - language translations than those for spanish - language ones . however , when training on both languages , the accuracy remains the same , with the exception of spanish , where the accuracy is slightly higher . finally , all models show lower precision on the test set when using only one language combination , as shown in fig . 3 . we conjecture that this is due to the smaller size of the training data set and the high error rate of concatenated training instances .
results on the dev set and on the test set using only subsets of the code - switched data are shown in table 4 . the fine - tuned model outperforms the naive approach by a significant margin . the results are particularly striking when we consider only the training data with 50 % dev and 50 % test set coverage , as this shows the advantage of finetuning the model during the development phase . fine - tuning also improves the generalization ability of the model , as shown in fig . 3 .
the results of fine - tuning are shown in table 5 . our approach shows marked performance improvement on the dev set and on the test set , compared to the best performing monolingual approach . the difference in accuracy between the two sets is mostly due to smaller differences in the number of gold sentences in the gold sentence set , which indicates the less pronounced impact of the language adaptation on performance . moreover , fine - tuned - disc improves performance as well . the results are statistically significant even when using only one type of gold sentence in the set , as shown next .
table 7 shows the precision ( p ) and recall ( r ) scores for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvements in precision and recall are statistically significant ( t - test , p < 0 . 01 ) and f1 - score ( f1score ) show that the type combined approach is superior to the baseline model in terms of precision and r = f1 score .
table 5 shows the precision ( p ) and recall ( r ) metrics for using type - aggregated gaze features for the conll - 2003 dataset as compared to the baseline model using the type - combined gaze features . the improvements in precision and recall are statistically significant ( t - test , p < 0 . 01 ) and f1 - score ( f1score ) are also statistically significant , which indicates that the model can further improve interpretability without sacrificing performance .
the results on the belinkov2014exploring ’ s test set are shown in table 1 . our hpcd model outperforms the original syntactic - sg model and glove - extended model by a significant margin . the difference in performance between the two approaches is most prevalent when using the type - based lstm - pp embeddings instead of skipgram , indicating the advantage of pre - training with a pre - trained lexical type . further , the difference between the original and the updated model is less pronounced for wordnet , indicating that more training data is required to reproduce the results . we observe that the performance gain comes from a better generalization of the type and the token , further improving performance by high margins
table 2 shows the performance of our system with various pp prediction features coming from various pp attachment predictors and oracle attachments . the results show that our hpcd - based system outperforms the best ontolstm - pp model by a significant margin . also , the performance gain is larger when we add in the ability to ontolayerlayerlayerpreferences for object prediction .
table 3 shows the effect of removing the sense priors and context sensitivity ( attention ) from the model . the results are statistically significant ( p - value < 0 . 01 ) with an absolute improvement of 2 . 5 points over the original model .
table 2 shows the bleu % scores of the models using subtitle data and domain tuning for image caption translation ( table 2 ) . the ensemble - of - 3 model outperforms the model using only one subtitle data type , indicating the advantage of domain tuning over mixing multiple subtitle files . the model using the best subtitle data performs best when the domain tuning is applied to multi30k dataset , indicating that fine - tuning the domain contributes significantly to the model ' s performance .
we observe that domain - tuned hoco outperforms the plain lm + ms - coco model on all datasets except for those in the en - de domain , confirming the importance of labels . the results are particularly striking for flickr , where the h + ms model performs best , both when labels are added and when the model is tuned . in particular , the results are very consistent across all datasets , with the exception of those in en - france .
table 4 shows the bleu scores of the models using only the best 5 captions ( only the best one or all 5 ) and marian amun . the results are slightly better than those without automatic captions , indicating the advantage of finetuning word embeddings during training . further , the smaller size of the training data helps improve interpretability , since more captions are required to interpret each image .
table 5 compares the bleu % scores of our method with other approaches for visual information integration using transformer , multi30k + ms - coco + subs3mlm and detectron mask surface , the results are presented in table 5 . our encoder outperforms all the other methods except for mscoco17 , indicating the advantage of finetuning word embeddings during training . the decoder also outperforms the en - de approach , showing that decoding information is easier than encoding it . however , our encoder does not generalize well , leading to a lower performance than the encoder .
we observe that the ensemble - of - 3 approach is superior to the monolingual approach , confirming the value of visual features in the low - resource settings . moreover , the results are slightly worse than those of " text - only " and " multi - lingual " . the results of " language features " are less clear , but still show a significant performance drop compared to " visual features " when using only text - only .
table 1 shows the results for english translations on the hidden test set of hotpotqa in the distractor and fullwiki setting , respectively . the results are broken down in terms of ttr and mtld scores , with en - fr and en - es - ht achieving better results on both sets . as expected , the smaller performance gap between the two sets indicates that the effectiveness of text - similarity based learning is less pronounced for non - ht - trained models . however , the results are still encouraging for future work in this direction .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . our model splits the training and development sets 1 , 472 , 203 and 459 , 633 sentences , respectively , making it easier to analyze .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the results are statistically significant ( p < 0 . 01 ) with respect to both src and trg scores , confirming the effectiveness of our model .
table 5 shows the bleu and ter scores for the rev systems . our system outperforms the best previous approaches by a noticeable margin .
table 2 shows the performance of our model compared to the original rsaimage model on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the model supervised using segmatch . our model obtains the highest recall @ 10 and median rank , indicating that our approach is more effective in generation of high recall .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the model from rsaimage . the results are statistically significant ( t - test , p < 0 . 001 ) with a mean mfcc of 1 . 0 and a median rank of 0 . 9 , which indicates that the model performs well in terms of recall and chance . audio2vec - u outperforms all the other approaches with a large margin .
table 1 compares the effectiveness of the different classifiers with the original on sst - 2 . originally , all the classifiers used for this study had to be re - trained after replacing the original with a new one . since the new dan model only works on one domain , we only need to rephrase some of the examples to account for this . we report further examples in the appendix . the most interesting ones are the ones where the author turns on a on ( i . e . , at the edges ) to show that the screenplay is in the right place at the right time . it is also clear from table 1 that dan is more useful for use on languages other than english .
table 2 shows the percentage of occurrences for each part - of - speech that has increased , decreased or stayed the same through fine - tuning of the original sentence in sst - 2 . as the table indicates , the importance of repetition is relatively low , indicating that the number of occurrences has not changed significantly . however , the presence of some highly productive words ( e . g . , " democracy " , " democracy " and " freedom " ) increases as a result of the increased number of tokens in the correct sentence . this indicates that precision has increased .
sentiment score changes in sst - 2 . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the flipped labels result in a significant drop in the sentiment score , which indicates that the flipped labels have a significant impact on the interpretability .
table 1 presents the results of experiments on the pubmed and sst datasets . results are presented in table 1 . the results are statistically significant ( p < 0 . 001 ) with an absolute improvement of 1 . 0 % over the best previous state of the art on both datasets . note that pubmed outperforms sift by a significant margin .
