table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as shown in table 2 , the smaller size of the training set and the smaller number of instances per iteration , the better the model performs on inference and training .
table 1 shows the performance of the treernn model with varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization . with a batch size of 1 , the balanced dataset exhibits the greatest performance improvement , with a f1 of 4 . 7 / 10 .
the max pooling strategy consistently performs better in all model variations . for example , conll08 achieves a f1 score of 1 . 83 / 2 . 57 on average compared to ud v1 . 05 , 1 . 63 / 1 . 63 and 1 . 87 / 1 , respectively , with the same number of feature maps . with the same representation , sb performs better than sigmoid and softplus on all models with different representation . as shown in table 2 , selecting the best feature maps reduces the dropout rate and boosts the learning rate for all models .
table 1 shows the effect of using the shortest dependency path on each relation type . our macro - averaged model achieves the best f1 score ( in 5 - fold test set ) with sdp in all relation types .
the results are shown in table 3 . the results of y - 3 on r - f1 and f1 are presented in bold . we observe that the model performs well on both datasets with a minimum of 50 % f1 score . on the other hand , the results are less clear - cut with respect to the r - score . it can be seen that the smaller size of the training set has a significant impact on the performance of the model , leading to a drop in performance .
table 3 shows that mst - parser outperforms all the other approaches on average in terms of accuracy on average . the results are shown in bold . as can be seen in the results presented in table 3 , all the models that use the pre - trained mst parser perform similarly on average to the original embeddings . however , the difference is less pronounced with respect to accuracy on the " essay " level , which shows that it is easier for the parser to learn the structure of sentences than the original .
table 4 shows the performance of our system compared to the lstm - parser on the essay vs . paragraph level . our system achieves a c - f1 score of 60 . 40 ± 13 . 57 % on average compared to 62 . 24 ± 2 . 87 % for the other system . note that the mean performances are lower than the majority performances over the runs given in table 2 .
the results are shown in table 3 . the original and the cleaned tgen model outperform sc - lstm on all metrics except bleu and nist by a noticeable margin . when the original tgen is cleaned , it achieves a performance improvement of 3 . 88 points over the original model . however , when the original is trained with tgen + and tgen - cleaned , the improvement is only 1 point higher .
table 1 compares the original e2e data with our cleaned version . the difference in the number of distinct mrs between the original and the cleaned version is relatively small ( 0 . 5pt / 2pt ) compared to the difference in ser ( 17 . 69 % vs . 17 . 42 % ) , indicating that the original training data contain fewer references , but are more interesting because they contain more textual references . table 1 also shows that the training data contains more instances of slot matching than the test data .
the results are shown in table 3 . original and original tgen outperform tgen − and sc - lstm on every metric , with the exception of meteor , where tgen + performs slightly better . as shown in fig . 3 , the difference in bleu and nist scores between original and original is less pronounced for tgen than it is for original , indicating that tgen is better at training and test sets . also , when trained with the original rouge layer , the results are slightly better than those with the tgen layer .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) and the number of instances with correct values ( added and missing ) in the training set . as shown in table 4 , adding the correct values to the original instances reduces the overall number of errors , but does not improve the performance .
table 3 shows the performance of our system compared to state - of - the - art models on the single - domain and ensemble test sets . our graphlstm model outperforms all the baselines except tree2str and tsp by a large margin . table 3 also shows that the multi - domain approach is more appealing than the ensemble approach .
table 2 shows the performance of our ensemble model on amr17 . our dcgcn achieves a bleu point improvement over the previous state - of - the - art model by 3 . 5 bleus compared to seq2seqb ( beck et al . , 2018 ) in terms of ensemble performance . gcnseq ( damonte and cohen , 2019 ) achieves 24 . 5 points improvement over ggnnseq by 2 . 6 points . table 2 also shows that the size of the model increases with the growth of the number of parameters , indicating that the model is more specialized .
table 3 presents the results for english - german , czech and slovak . the results are summarized in table 3 . table 3 shows that the single - headed approach outperforms all the baselines except birnn , bow + gcn and cnn + cnn by a significant margin . it can also be seen that the multi - headed baselines outperform the singleheaded approach by a large margin . for example , seq2seqb ( beck et al . , 2018 ) achieves the best results with a f1 score of 43 . 8 out of 100 .
table 5 shows that the number of layers inside dc has a significant effect on the performance of our model , with the effect being most pronounced for blocks containing more than one layer ( i . e . n = 3 ) . table 5 also shows that for blocks with more than two layers , the model performs significantly worse than those without .
table 6 shows that the gcns with residual connections outperform the baselines with respect to bias in terms of rc scores . rc scores significantly higher than those with no residual connections , indicating that the model can rely less on superficial cues .
table 4 shows the performance of our model compared to the state - of - the - art in terms of d - score and bias metric . the results are summarized in table 4 . we observe that our model performs significantly better than the best previous approaches on both metrics .
table 8 shows the ablation study results for the dev set of amr15 . the results are shown in table 8 . the results show that removing the dense connections in the i - th block significantly decreases the density of connections . also , the model performs significantly worse than dcgcn4 with a reduction of the number of connections from 4 to 3 .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . the results are summarized in table 9 . the results show that the adversarial coverage mechanism used in our decoder significantly improves the generalization ability of our model , i . e . the coverage achieved with - linear combination and - direction aggregation reduces decoding performance by 3 . 5 % and 4 . 2 % respectively .
table 7 shows the performance of our initialization strategies on probing tasks . the glorot initialization strategy outperforms all the other initialization strategies with a large margin . it achieves a score of 35 . 8 / 71 . 0 on average compared to our paper ' s 35 . 7 / 59 . 9 and 29 . 2 / 59 on average .
table 3 presents the results of baselines trained on h - cmow and h - cbow with respect to concatenated keyphrases . the results are summarized in table 3 . as can be seen , the hcmow model outperforms all the baselines except somo in terms of generalization . it achieves the best generalization with a f1 score of 87 . 2 / 100 . 7 .
the results are shown in table 3 . hybrid outperforms all the baselines except sst2 and sst5 on all metrics except mpqa , where it achieves a marginal improvement of 0 . 2 % over cbow . cmow achieves an overall improvement of 2 . 6 % over sick - r on all three metrics . the best performance is achieved by cbow / 784 , which achieves a final score of 90 . 7 % higher than hybrid . it is clear from table 3 that the cbow embedding cbow improves the model ' s performance across all metrics .
table 3 shows the performance of our models on unsupervised downstream tasks . hybrid outperforms both cbow and cmow with a margin of 10 . 5 % and 7 . 6 % overall compared to cmp . with respect to sts13 and sts16 , hybrid achieves a performance improvement of 25 . 2 % and 31 . 0 % over cbow , respectively , on average . as shown in table 3 , when cbow is used in combination with cmow , the performance gap between the two methods becomes much narrower . finally , when hybrid is used alone , its performance is reduced by 4 . 6 % .
table 8 shows the performance of our initialization strategies on supervised downstream tasks . glorot outperforms all the baselines except trec and sick - e by a large margin . it achieves a final score of 87 . 6 % on mpqa and 86 . 4 % on sst5 , both of which are higher than the previous state - of - the - art on both datasets .
table 6 shows the scores for different training objectives on the unsupervised downstream tasks . the cmow - r method outperforms cbow in all but one of the tasks . it achieves the best performance on sts13 and sts16 .
table 3 shows the performance of our method compared to the baselines on the hidden test set of somo and wc . the results are presented in bold . cmow achieves the best results , outperforming both cbow and cbow - r on every metric by a significant margin . it is clear from table 3 that our method is more appealing to the task - solicitive type of learner than the traditional baselines .
the results are shown in table 3 . the best performing method is the cmow - r variant , which outperforms cbow - c and sick - r by a large margin . cmow achieves an overall score of 87 . 6 % on average compared to the sst - b average of 77 . 9 % and the mpqa average of 79 . 2 % . the cmow variant outperforms both sst2 and sst5 by a significant margin .
the results are shown in table 3 . name matching is the most difficult part of the task for trained models , followed by supervised learning . supervised learning outperforms all the baselines except loc with a margin of 2 . 57 points . in general terms , trained models perform better than supervised learning on all metrics except loc . the performance gap between trained and unsupervised models is narrower than that between loc and misc , indicating that supervised learning relies less on superficial cues . it is clear from table 3 that training on loc alone does not improve the performance of trained models .
results in table 2 show that the supervised learning approach outperforms the best previous approaches on the test set in terms of f1 score . supervised learning improves the f1 scores by 2 . 38 points in the name matching task , and by 3 . 26 points in f1 on the supervised learning task . the results are shown in bold . in table 2 , we also show that τmil - nd ( model 2 ) outperforms all the other approaches that do not rely on supervised learning .
table 6 shows that g2s - gat outperforms all the baselines on average when trained with only one type of embeddings ( e . g . , s2s , gat , ggnn , gin , gingnn ) with an absolute improvement of 2 . 86 points over the best baseline ( s2s ) on average .
table 3 presents the results of baselines trained on the ldc2015e86 and ldc2017e86 datasets . the results are summarized in table 3 . we observe that the g2s model outperforms all the baselines except for song et al . ( 2018 ) in terms of bleu score , meteor score , and coefficient of extraction ( cao et al . , 2018 ) by a significant margin . also , the model performs significantly worse than the s2s baseline on ldc2016e86 , indicating that it relies more on superficial cues and less on interpretability .
table 3 shows that g2s - ggnn outperforms the best previous models on the ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 . it can be seen that the bleu model outperforms all the other models that have been tested to date .
table 4 shows the results of the ablation study on the ldc2017t10 development set . our model significantly outperforms the previous state - of - the - art models in terms of bleu score and meteor score .
we observe that g2s - ggnn achieves the best results with a f1 score of 3 . 51 % ( out of a possible 4 . 28 % ) and a graph diameter score of 7 . 43 % . the average number of frames per second and average length of sentences per second are the most important factors in our model ' s success . the smaller size of the graph and the shorter sentence length seem to have little effect on the model ' s performance , however , it does help the model to achieve higher f1 scores .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( added ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . s2s and g2s both outperform gold in terms of the fraction of elements missing from the output ( miss ) as shown in table 8 . as shown in fig . 3 , the gat model outperforms gold on both subtasks .
table 4 shows that pos tagging accuracy is comparable to sem accuracy on a smaller parallel corpus ( 200k sentences ) with different target languages trained on the 4th nmt encoding layer . pos also exhibits a slight improvement over sem in terms of semantic features , indicating that the pos features are more useful for semantic tagging .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . word2tag significantly outperforms unsupemb in both tasks , showing that embedding unsupervised word embeddings improves the accuracy of the classifier .
table 4 shows the performance of our system on the four metrics for pos , ame , sem and f1 tasks . the results are summarized in table 4 . ame outperforms all the state - of - the - art systems on all metrics except pos with a large margin . ame achieves the best performance with an accuracy of 91 . 1 % on pos and 91 . 9 % on sem with a gap of 3 . 5 % on f1 task .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 . residual encoder outperforms bi and bi - sem encoder in terms of pos tagging accuracy . as expected , the res - based encoder performs best in the multi - language setting .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . the attacker significantly outperforms the adversary on all three datasets . as shown in table 8 , gender and race are the most important classifiers for the attacker , followed by age and gender . finally , the presence of a race - based classifier significantly boosts the attacker ' s performance .
table 1 shows the performance of our model with respect to training on a single task . our model outperforms all the state - of - the - art models that do not rely on word embeddings .
table 2 shows the performance of our system with balanced and unbalanced data splits . the results are shown in table 2 . our proposed system outperforms all the baselines with a large margin . it achieves the best performance with a balanced data split .
in table 3 , we show the performance of our system on different datasets with an adversarial training set . as shown in the table , the gender - based features contribute significantly less than the other features , which indicates that gender - neutral features contribute less to the task performance . however , the presence of race and age features contribute more than the others , indicating that these features contribute to the overall performance of the system . finally , we see that there is a significant difference in the performance between the attacker and the corresponding adversary when using all the features in the training set as the named attributes .
the results are shown in table 6 . the rnn encoder significantly outperforms the guarded encoder in terms of accuracy . with different encoders , the guarded embeddings perform significantly worse than the leaky ones .
table 3 shows the performance of our model compared to the best previous work on the ptb and wt2 baselines . our model outperforms all the baselines except for the one in yang et al . ( 2018 ) by a large margin . the difference is most striking in the finetune metric , where our model achieves a f1 score of 85 . 97 out of 100 , compared to 62 . 86 for the previous state - of - the - art model .
table 2 presents the results of baselines trained on the lstm dataset in table 2 . the results are presented in table 1 . as shown in the table , using the baselines described in rocktäschel et al . ( 2016 ) and ( 2016a ) gives a significant performance improvement over the previous state - of - the - art model on both base and time metrics . table 2 shows that using the base - time and time - based baselines significantly improves the generalization ability of our model , with an absolute improvement of 3 . 5 % compared to the previous work . also , the performance gain is larger than the performance drop by 2 % when using the combination of ln time and bert time .
table 3 presents the results of baselines trained on the data from zhang et al . ( 2015 ) and ( 2015 ) . the results are summarized in table 3 . as can be seen in the results presented in the table , the performance gap between the baselines is relatively small , with amaerr consistently performing better than yahoo err and yelp time , and sru outperforming both amafull and amapolar err .
table 3 shows the bleu score of our model on the wmt14 english - german translation task on tesla p100 . the gnmt model significantly outperforms all the baselines except olrn , lrn and atr in both cases . gnmt even outperforms lrn in case - insensitive tokenized training as well as decoder tasks . as shown in table 3 , gnmt has the worst performance on the translation task compared to other baselines .
table 4 shows the exact match / f1 - score of our model on the squad dataset . it can be seen that our model outperforms all the state - of - the - art neural models in terms of match rate and f1 score . as shown in the table , our approach relies less on elmo than the previous approaches because the model has less parameter number of base . also , our lstm model has fewer parameters than the other models because it relies more on self - learning neural networks . finally , our lrn model has more parameters than atr , gru , and sru , which results in lower match rate .
table 6 shows the f1 score of our model on the conll - 2003 english ner task . the lstm model achieves the best performance with an f1 of 90 . 56 on the ner test set . as shown in table 6 , lrn outperforms all the other methods with a large margin .
table 7 shows the test results on snli and ptb task with base + ln setting . lrn achieves the best performance with a base - based ln setting of 85 . 56 % on the snli task . with the base setting , glrn achieves a lower performance than elrn ( 85 . 26 % vs . 85 . 49 % ) , but still achieves a higher percentage of correct answers . the results are shown in table 7 .
table 3 shows the performance of our system with respect to word embeddings . the results are presented in table 3 . our system outperforms all the state - of - the - art systems in terms of average number of words per sentence , average average length of sentences per sentence and average average r - 2 score .
table 4 presents the results of human evaluation . our system outperforms all the automatic systems on three of the four metrics ( gram , appropriateness , and content richness ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 6 % ( candela et al . , 2018 ) and seq2seq ( 2018 ) is 2 . 2 % ( h & w hua and wang , 2018 ) higher than human evaluation on all metrics ( appr , contr , k , k 1000 , k 2000 ) .
table 3 shows the performance of our approach compared to previous approaches on the test set of ted talks and europarl . the results are summarized in table 3 . we observe that our approach outperforms all the baselines except ted talks in terms of p < 0 . 05 , with the exception of slqs , which is slightly better than the other two baselines . also , our model outperforms the other baselines on three out of the four test sets .
table 3 shows the performance of our approach compared to previous approaches on the test set of ted talks and europarl . the results are summarized in table 3 . we observe that our approach outperforms all the baselines on average , with the exception of dsim , which is slightly better than the others on both datasets .
table 3 shows the performance of our approach compared to the best previous approaches on the test set of ted talks and europarl . the results are summarized in table 3 . we observe that our approach outperforms all the baselines except ted talks in terms of p < 0 . 01 , with the exception of slqs , which is slightly better than the others . also , we see that our model outperforms the other baselines on all metrics except tf .
the results are shown in table 3 . europarl outperforms all the baselines on all metrics except for docsub . the average depth of the roots is significantly lower than that of other baselines , indicating that the roots are more rooted in the semantic content of the original embeddings , and therefore are more difficult for the parser to extract .
table 3 shows the performance of our system compared to the baselines . europarl outperforms all baselines except docsub and slqs in terms of average depth . the difference is most pronounced for docsub , which has the highest percentage of roots . as shown in fig . 3 , the average depth of the roots is lower than that of the max - depth roots , indicating that there is less overlap between the roots and the lexical features of the data .
in table 1 , we compare the performance of our enhanced version of lf with the baseline model on the validation set of visdial v1 . 0 . 3 . the enhanced version shows that it is comparable in terms of ncg % to the original model , with the difference being less pronounced for the question type , answer score sampling , and hidden dictionary learning , indicating that the enhanced version is more suitable for production use .
table 2 shows the performance of the ablative studies on different models on the visdial v1 . 0 validation set . adding p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 2 , while only applying p2 with the history shortcut shows the effectiveness of the baseline model . adding p1 indicates that the model is more likely to perform better than the baseline , and that p2 is more effective than p1 .
table 5 shows the performance on hard and soft alignments . the hmd - f1 model outperforms ruse and wmd - bigram on all metrics except for cs - en and fi - en , where it is slightly better than ruse . also , the hmd - recall model performs slightly better on soft alignment than wmd - unigram , indicating that the recall function of bert is beneficial for the task at hand .
the results of baselines are shown in table 3 . meteor + + achieves an average score of 0 . 686 on the direct assessment metric , while bertscore - f1 achieves 0 . 685 on the same metric . sent - mover achieves a lower average score than ruse ( * ) and smd + w2v ( 0 . 686 / 0 . 719 ) on both metrics , indicating that the selection of the best baselines may impact the performance of the model .
the results are shown in table 3 . the baselines for bertscore and meteor are presented in bold . meteor achieves the best results with a f1 score of 0 . 176 on average . sfhotel achieves the highest score with 0 . 174 on the f1 scale . sent - mover achieves the second highest f1 on the bleu - 1 scale with a score of 1 . 012 .
the results are shown in table 3 . sent - mover achieves the best results with an f1 score of 0 . 939 on the m1 and m2 metric . word - mover achieves a f1score of 1 . 083 and a m2score of 2 . 083 on the leic metric , both of which are significantly higher than the previous state - of - the - art on both sets .
the results are shown in table 7 . para - para - based models outperform all the baselines except shen - 1 when trained with only one type of lexical feature , namely , that of the lambda - disambiguation ( cyc + para , 2d ) and lexical features ( para + lang ) .
table 3 presents the results of our model on the training data . the results are summarized in table 3 . our model outperforms all the baselines except yelp in terms of transfer quality and transfer quality tie . semantic preservation and fluency are the most important aspects of the model success , we observe that our model performs better than all the other baselines on all three metrics except transfer quality .
table 5 shows the results of human evaluation of the metrics for each dataset for validation of acc and gm . the results are summarized in table 5 . overall , the human evaluation shows that the summaries generated by our system are comparable to those by the best previous work ( e . g . , spearman ’ s ρ b / w positive vs . negative spearman ' s ρ c γ ) on average .
the results are shown in table 6 . para - para - based models outperform all the baselines except shen - 1 when trained with only one type of lexical feature . the results of m1 + cyc + para are presented in bold . when trained with both types of feature - rich lexical features , the results are slightly better than those of m0 [ italic ] + cyc , with a gap of 0 . 7 points from m1 to m2 .
table 6 shows the performance of our model on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( the ones with a minimum of 22 . 4 bleus ) achieve higher acc than prior work at similar levels of accuracy . the best model is the unsupervised yang2018 un - supervised model , which achieves an acc - score of 31 . 4 out of a possible 31 . 6 bleu . it is clear from table 6 that our classifiers perform better than the best previous work on sentiment transfer .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent as well as the overall number of disfluencies for each type of repetition token in our model . for nested disfluency , the average number of tokens is slightly higher than for rephrase tokens , indicating that the repetition tokens are more important for the model to perform well . for example , the length of repetition tokens is the most important part of the model ' s prediction performance , since it determines the length at which a disfuncuation event can occur .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , or in neither . as shown in the table , content - content tokens are more likely to belong to the disfluency category than function - function tokens , indicating that the content word is more important for the task at hand . similarly , the fraction of tokens belonging to each category that is disfluential is higher for reparandament length than for repair length .
the results are shown in table 3 . we observe that the text - based approach outperforms the single - input approach in terms of both test set dev and average number of iterations ( averaged over multiple iterations ) with a margin of 2 . 2 % improvement over the best previous work . moreover , the model performs better on the test set with different combinations of text and innovations , indicating that the training data are more useful for the task at hand .
performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . in table 2 , we can see that word2vec embedding achieves the best performance with a f1 score of 10 . 43 / 10 . 54 and a bdi score of 7 . 59 / 7 . 54 on average . the accuracy of our model is slightly higher than the state of the art rnn - based embeddings , however it is comparable to the performance of self - attention and cnn - based algorithms .
table 2 shows the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . neuraldater achieves the best performance with an accuracy of 69 . 2 % compared to 62 . 3 % by ac - gcn . attentive neuraldater has a lower accuracy than maxent - joint , but is still comparable with ac and oe . as shown in table 2 , the attention - based neuraldater outperforms the other methods with a large margin .
table 3 shows the performance of our neuraldater with and without word attention in the word attention task . our approach outperforms all the other approaches that do not rely on word attention . the accuracy of our neuraldater is 62 . 6 % compared to 61 . 9 % with attention .
the results are shown in table 3 . embedding + t achieves the best results , outperforming all the other approaches that do not rely on t - supervision . the best performance is obtained by jrnn , which achieves 75 . 7 % on average compared to dmcnn ' s 69 . 6 % and cnn ' s 62 . 6 % . jmee achieves the highest performance , achieving 75 . 3 % and 75 . 1 % on the 1 / 1 and 1 / n test sets , respectively . trigger - based neural networks outperform all other approaches except cnn .
table 3 presents the results of cross - event event detection on the training data . the results are presented in table 3 . cross - event event detection outperforms traditional event detection methods in terms of both event identification and classification . as the results are shown in the table , event detection is beneficial for both types of argument , with the exception of the argument identification method , which is beneficial only for cross - event events .
results are shown in table 3 . the results are summarized in bold . we see that the best performing variant of english - only - lm is spanish - only , followed closely by italian - only and german - only . further , fine - tuned - lm outperforms all the other variants in terms of dev perp , test acc , and wer scores .
results in table 4 show that fine - tuning the model with only subsets of the code - switched data improves the results on both the dev set and the test set . fine - tuned models outperform cs - only models with a large margin . cs - trained models achieve state - of - the - art results with a f1 score of 7 . 2 / 10 . 5 and a f2 score of 6 . 8 / 9 . 5 on the dev and test sets , respectively .
table 5 shows that fine - tuning improves the performance on the dev set and the test set , and upsampling has a generally positive effect ( p < 0 . 01 ) on both sets . fine - tuned - disc outperforms cs - only - lm in both cases .
table 7 shows the precision ( p ) and recall ( r ) numbers for using the type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvement in precision is statistically significant ( p < 0 . 01 ) and f1 - score ( f ) is higher than the baseline ( p > 0 . 05 ) for both types of gaze features . type combined gaze features significantly improve recall and precision .
table 5 shows the precision ( p ) and recall ( r ) numbers for using type - aggregated gaze features on the conll - 2003 dataset , compared to using the baseline model . type combined gaze features significantly improve recall and precision compared to baseline , indicating that the type selection process can further improve the model ' s generalization ability .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . glove - extended refers to the synset embeddings obtained by running autoextend rothe and schütze ( 2015 ) on wordnet 3 . 1 . similarly , ontolstm - pp uses the type - based initialization scheme from the original paper ( farrequi et al . , 2015 ) and the syntactic skipgram embedding scheme from schütz et al . ( 2015 ) . syntactic - sg embedding achieves the best results , with an overall score of 88 . 7 % on the test set . hpcd ( full ) achieves the highest score , with a final score of 89 . 8 % . the results are summarized in table 2 . as can be seen in the results table , the lstm embedding method is more appealing than the original one because it relies less on syntactic tokens .
results in table 2 show that the hpcd dependency parser outperforms the original ontolstm - pp system with features derived from various pp attachment predictors and oracle attachments . it also outperforms rbg with the same number of features .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . it can be seen that the ppa acc . score decreases as a result of removing the importance of context sensitivity .
table 2 shows the bleu % scores of the en - de and en - fr ensembled models compared to the multi30k model using subtitle data and domain tuning . adding subtitle data improves the translation performance for both models , the ensemble - of - 3 model significantly outperforms the model using sub - subsfold subtitle data . as the results of domain tuning are shown in table 2 , using domain tuning reduces the translation error of the ensemble of 3 models by 3 . 5 % compared to using the multi - tasked model . subsfolding the subtitle data further improves the results , it is clear that domain tuning helps the ensemble model to improve its translation performance . finally , domain tuning decreases the translation accuracy by 2 . 5 % .
table 3 shows that domain - tuned h + ms - coco outperforms the plain plain hoco model with a margin of 3 . 3 bleu / s in en - de and 3 . 7 bleu / s on average . the results are statistically significant with respect to en - fr and flickr16 as well as mscoco17 , indicating the importance of domain - adaptive learning .
table 4 shows the bleu scores of en - de and en - fr trained models compared to multi - de trained models using automatic captions . adding automatic image captions improves the performance for both sets of models . the results are shown in table 4 . using only the best 5 captions , the multi30k model achieves higher bleus scores than the original flickr model , indicating that the captions are more useful for brevity . in addition , using only the 5 best captions reduces performance for en - fluent models .
table 5 shows the bleu % scores of our encoder and decoder strategies for integrating visual information into the multi30k + ms - coco + subs3mlm task . our encoder achieves the highest bbleu % score ( 68 . 38 % ) compared to en - de and mscoco17 ' s 69 . 40 % on average , indicating that the encoder performs better than the decoder with respect to visual information integration .
table 3 shows the results for en - de , en - fr and mscoco17 compared to the original models . the results are summarized in table 3 . multi - lingual features seem to have a generally positive effect on the performance of the subs3m model , as shown in the table , the multilingual feature - rich ensemble - of - 3 model outperforms the plain text - only model with a large margin .
table 3 shows that en - fr - ht and en - es - ht achieve the best results on the yule ’ s i and mtld datasets . the results are summarized in table 3 . it can be seen that both approaches rely less on word embeddings and more on syntactic analogy . as a result , their performance is lower on the mtld dataset than on the original yule ' s dataset .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . our en – fr embedding model outperforms the en – es model in terms of parallel sentence generation .
table 2 shows the training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 . as the results show , the models are able to learn the most useful vocabulary for each language pair with a low error .
table 5 shows the evaluation scores for the rev systems . the en - fr - rnn - rev and en - es - smt - rev systems achieve relatively high bleu and ter scores , indicating that the system is well - equipped to handle the task at hand . however , the en - e - trans - rev system achieves lower ter scores . it is clear from table 5 that the reliance on the rnn - word embedding model hurts the performance of the system evaluation .
table 2 shows the evaluation results on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com and the row labeled rsaimage is the audiovisual supervised model . the results are shown in table 2 . rsaimage significantly outperforms the vgs model in terms of recall @ 10 and average mfcc score .
results are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com and the row labeled rsaimage is the audiovisual supervised model . the acoustic embeddings based on audio2vec - u outperform all the other approaches except for segmatch , which achieves the highest recall @ 10 . 0 .
table 1 shows the example sentences of the different classifiers compared to the original on sst - 2 . we report further examples in the appendix . the cnn classifier turns in a screenplay that is at the edges of the word " hate " and " want to hate it " . the rnn classifiers turn in screenplay that contains edges and edges . it is clear from table 1 that the dan classifier is more sophisticated than orig and rnn in terms of sentence selection . as shown in fig . 3 , dan is able to learn sentence selection more precisely than orig .
table 2 shows the percentage of occurrences in sst - 2 that have increased , decreased or stayed the same through fine - tuning . as the table shows , the number of occurrences per sentence has increased as a result of the improvement in the part - of - speech metric . also , the average number of tokens in the original sentence has decreased as well as the overall number of instances .
table 3 shows the sentiment score changes in sst - 2 . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . in this case , the positive sentiment score increases as well .
table 1 presents the results of pubmed and sst - 2 . the results are summarized in table 1 . as can be seen , pubmed outperforms sift by a large margin . sift outperforms pubmed by a noticeable margin .
