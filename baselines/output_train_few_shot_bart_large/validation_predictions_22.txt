table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . the difference in batch size between the iterative and recursive approaches is less pronounced for training , but still indicates that the folding approach is superior on both inference and training . as the table shows , the recursive approach uses a larger number of instances to train the model , making it more suitable for production use . since the size of the training instances is small , the difference in performance between folding and recurling is less striking than that between the two iterative approaches .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization . the difference in throughput between balanced and linear is less pronounced for the moderate dataset , but still indicates that there is a trade - off to be made when using a larger batch size .
results for each model with different representation are shown in table 2 . the max pooling strategy consistently performs better than the naive tnl - adaptive representation ( i . e . , f1 = 0 . 83 , f1 > 0 . 88 ) and sigmoid = 1 . 57 , both for ud v1 . 3 and conll08 ( see x4 ) . as hard core learning problems are rare in the single - domain setting , we do not have significant performance improvement with softening the hyperparametrization parameters for each variation . however , the dropout prediction function under - performs the softening function , so we use it only in case the training set size is more than the size of the data pooling set . we find that using the svm function with a maximum of 5 parameters gives the best performance ( p < 0 . 05 ) and the second highest f1 score ( p ≈ 0 . 08 ) . we notice that combining the features of the input and output attention mechanisms also improves the model performance . finally , we test the model with a dropout function as well , to mimic the realistic scenario of human intuition .
table 1 shows the effect of using the shortest dependency path on each relation type . we show that macro - averaged models result in a significant improvement in f1 score over the best f1 model without sdp ( by a factor of 27 . 26 points ) , and with a gain of 19 . 59 points over the model with sdp .
consistent with the observations by vaswani et al . ( 2017 ) , we observe that the performance reach the best when using only 50 % f1 and 50 % r - f1 on the training data , with the difference being most prevalent in the second case . on the other hand , in the first case , this gap is less pronounced with respect to y - 3 : y , which shows that more than 50 % of models trained on this data can achieve the best results under automatic metrics .
the results of paragraph prediction on the test set are presented in table 1 . we show that our proposed parser achieves an absolute improvement of more than 50 % on the paragraph prediction accuracy compared to the state - of - the - art mst - parser model . on the whole , the improvement is much larger than the difference between the official score of 50 % and 50 % for paragraph prediction .
we can see from table 4 that our system performs comparably to the best previous state - of - the - art lstm - parser system on both essay and paragraph level . the difference in c - f1 ( 100 % ) between the two systems is less pronounced on paragraph level , but still significant , showing that our proposed system can significantly improve the interpretability of paragraph - level embeddings .
table 1 shows the results for the original and the cleaned versions of tgen . the results are presented in bold . the results show that the original tgen model performed better than the cleaned model when it was trained and tested on the new set of features . however , the difference between the two is less pronounced when the original is cleaned , with tgen cleaning at a lower level than sc - lstm , indicating that the training data obtained by tgen + can be used to improve the interpretability of the model without a drop in performance . table 1 compares the results of original and cleanups . as can be seen , the results displayed in table 1 show , the cleaner , more interpretable tgen models perform better than their less interpretable counterparts .
table 1 shows the comparison of the original e2e data with the cleaned version , as measured by the number of distinct mrs and total number of textual references ( see section 3 ) . the difference in ser between the original and the cleaned versions is less pronounced for the train dataset , but still significant ( 17 . 5 % ) on the dev dataset , which shows that our slot matching script has a significant impact on the interpretability of our model . table 1 also shows that the difference between the average number of tokens in the training and test sets is less significant for the dev dataset , however , for the test set , the difference is much larger , 1 . 5 % on average , compared to 0 . 5pt / 2pt in the original data .
table 3 presents the results of models trained on pascal - voc on the hidden test set of hotpotqa in the distractor and fullwiki setting , respectively . the results are presented in bold . the proposed hgn outperforms both the original and the original variants of sc - lstm , indicating that adapting the model to the task at hand , boosts the general performance , since the improvement over the original is much larger than the difference between tgen − and original . moreover , hgn significantly outperforms the original model on all metrics , with the exception of meteor , which shows a drop of 2 . 27 points from the original to the original .
the results of manual error analysis on a sample of 100 instances from the original test set are shown in table 4 . we found that the majority of errors in our system ( 62 . 4 % ) are caused by errors caused by adding instances that were already in the training set ( 23 . 6 % ) and that the correct values for some instances ( 14 . 7 % ) are slightly disfluencies ( 6 . 6 % ) . the difference in the number of errors between the original and the tested set is less than 1 % , however still significant . disfluencies caused by the added instances are less prevalent in the test set , about 5 . 6 % overall , but still represent a significant amount of errors .
table 1 compares our system with previous stateof - the - art models on the hidden test set of hotpotqa in the distractor and fullwiki setting , respectively . the results are presented in table 1 . our joint model outperforms all the base lines with a gap of 10 . 6 % on average compared to parallelism and 22 . 2 % on the combined test set .
the results on amr17 are shown in table 2 . our model achieves 24 . 5 bleu points , which is marginally better than the previous state - of - the - art on seq2seqb dataset . the difference between ensemble and single - model performance is less pronounced for dcgcn , but still significant : we achieve an absolute improvement of 2 . 6 points over the performance of the previous model ( beck et al . , 2018 ) by a margin of 1 . 3 points . table 2 shows that the number of parameters in our model is comparable to those in the previous work , but smaller than the size of the data set , as expected , the performance drop between single - and ensemble models is much larger .
table 1 compares the performance of our model with previous models on the test set in english - german , czech and slovakian . we benchmark against the following models : birnn + gcn ( bastings et al . , 2017 ) and cnn + cnn , the results of these models are shown in table 1 . as can be seen , the performance gap between the single - and multi - word models is considerable , with the former achieving gains of 1 . 8 and 2 . 4 points over the previous state of the art . table 1 shows that , in both languages , the monolingual model performs better than the competition on both datasets with different feature sets . the difference is most prevalent in czech , with single - word results increasing by 2 . 6 points on average compared to the previous model .
table 5 shows that the number of layers inside a dc network is important , as it affects the performance of the model and the translation quality . the effect is most prevalent in the second layer , and here we observe that there is a significant drop in performance for both blocks as a result of the reduced coverage . the bias metric is very low , meaning that more layers inside the network have a significant effect on performance .
table 6 compares our results with the baselines for gcns with residual connections . rc refers to the number of connections in residual connections , and the type of rc ( rc + la ) in our model . the results are summarized in table 6 , with the results of rcn1 and rcn2 achieving better results than those of dcgcn3 ( 24 . 8 % and 25 . 9 % respectively ) . adding rc and la improves the results for both gcns , the difference is most pronounced for gcn2 , which achieves 25 . 2 % improvement over the previous state - of - the - art .
the performance of our model compared to previous models on the hidden test set is presented in table 4 . we observe that , let alone a drop in performance , our model obtains the best results among all the 3 models with a gap of 2 . 2 points from the last published results ( dcgcn ) .
the results of an ablation study on the dev set of amr15 . 1 are shown in table 8 . the results show that removing the dense connections in the i - th block reduces the overall density of connections , and thereby decreases the error of our model by 1 . 8 points .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . the results of " - linear combination " and " - direction aggregation " models show that both the language modeling objective and the coverage mechanism underperform the dcgcn4 baseline by a significant margin . however , under " - global node " the results improve marginally , showing that combining the features of the global and local nodes helps the decoder to perform better . we observe that the effect is less pronounced under " - coverage mechanism " compared to - linear combination , indicating that there is a need to design more complicated decoders and to refine the feature extraction procedure for further improvement .
table 7 presents the performance of our initialization strategies for different probing tasks . our system achieves gains over glorot and topconst by a margin of 3 . 8 points on average , which shows the scalability of our approach .
table 1 shows the performance of our h - cmow variant compared to the plain cbow variant on the hidden test set of somo . the results are presented in bold . we observe that the h - cbow variant outperforms the original cbow model on every metric by a significant margin . it achieves state - of - the - art results , outperforming both the original and the modified cbow models by a noticeable margin .
the results are shown in table 1 . we report the results of our second variation of our method , hybrid , compared to the original cbow model . hybrid outperforms both monolingual and automatic methods , showing that the advantage of selective attention is derived from a better model design . selective attention mechanisms like cbow and cmow reduce repetition , and improve the generalization ability of question answering .
table 3 shows the improvements on unsupervised downstream tasks that our models achieve when trained and tested on the hidden test set of sts13 and sts16 . the results show that our proposed method outperforms the cbow and cmow baselines substantially in all but one of the 13 cases ( and slightly outperforms hybrid in the other 10 ) . the difference is most prevalent in the sub - category of reading comprehension , where cbow shows a significant drop of 2 . 6 % compared to cmow . however , the difference is less pronounced in sts15 , where hybrid outperforms cbow by a margin of 3 . 6 % . sub - categories as adjectives antonyms and performer action have a smaller effect on the final score , but still contribute significantly to the overall improvement . we observe that combining cbow with cmp . ow has a 2 . 8 % overall improvement on the downstream tasks , which shows the relative change with respect to hybrid .
table 8 presents the performance of our system for initialization on the three supervised downstream tasks . glorot ( 87 . 6 % ) achieves gains over the previous state of - the - art on all three tasks ( mpqa , trec , sst5 and sick - r , with a gap of 3 . 6 points from the last published results ( 83 . 6 % vs . 71 . 2 % ) , and on sst2 , it achieves gains of 2 . 4 points over the last state of the art . the performance gap between glorio et al . ( glorot , 86 . 6 ) and our system ( 71 . 2 % ) is modest but significant , with the difference being most prevalent in sst - e , the smallest of the three datasets ( 63 . 6 % ) , which shows the performance reach the upper boundary of the performance range . our system outperforms gloriot and trec by a noticeable margin .
table 6 shows the performance of all the methods for different training objectives on the unsupervised downstream tasks . the cbow - r approach shows marked improvements over the cmow approach by 2 . 2 points in performance on each of the four tasks . on sts12 , the improvement is much larger than that on sts13 . by a margin of 3 . 6 points , the cbow approach outperforms both cmow and svmow . on the sts15 and sts16 tasks , the difference is less pronounced , but still significant , with cbow achieving gains of 2 . 7 points and 3 . 0 points over the approach described in table 6 .
we compare our proposed method with the state - of - the - art ontonotes - trained word embeddings on the hidden test set of hotpotqa in the distractor and fullwiki setting , respectively . the results are shown in table 1 . the proposed hgn outperforms both published and unpublished work on every metric by a significant margin . for example , hgn achieves gains on average of 3 . 8 points over the previous state of the art on all metrics , and on average is nearly 5 points better than the previous best state - ofthe - art .
the results of all models are shown in table 1 . we observe that , let alone a drop in performance , the sub - category in which our method obtains the best results is mpqa . it outperforms all the other methods with a gap of 10 . 6 points from the last state - of - the - art results .
the experimental results on the loc and misc datasets are shown in table 1 . in general terms , our system outperforms all supervised and unsupervised systems on all metrics with a gap of 10 . 36 points from the last published results ( taumil - nd ) . the difference is most prevalent in terms of loc , where automatic learning underperforms supervised learning by a large margin . on the other hand , the gap between the two approaches is narrower with respect to misc , with respect to loc and e + loc , the difference is less pronounced for supervised learning but still significant , at a cost of 9 . 18 points compared to the average of 6 . 59 points in the previous experiment . supervised learning also achieves competitive or better results than automatic learning , in our particular case , this is due to the large variation in the training data between supervised and automatic learning epochs , as shown in the second group of table 1 , automatic learning methods effectively complement the deficiency of supervised learning . they yield better results overall when trained and tested on the same dataset , with the exception of the last group , which shows the diminishing returns from cross - input supervision . the performance gap between automatic and supervised learning is modest but significant , at the same time , it is statistically significant , with an absolute improvement of 2 . 36 % over the previous state of the art .
uncertain in low - supervision settings the experimental results on the test set are shown in table 2 , in all but one case , the trained models perform better than the supervised ones under all three settings . the difference is most prevalent in the name matching case , where the trained model ( mil - nd ) underperforms both the supervised and unsupervised models . however , the difference is less pronounced under noregionalization , with respect to epm feature - values , the model trained on the training set by our second set achieves the highest f1 score of 71 . 59 % , which shows the extent to which the model can rely on superficial cues from the training data . in the more realistic case , when trained only on the original ntd dataset , the gap between the f1 scores of the two sets is much smaller , showing the diminishing returns from cross - training . supervised learning , on the other hand , achieves the best performance , achieving 73 . 63 % f1 and 79 . 59 % r = 0 . 59 . these results show that once superficial cues are removed , the models are able to learn the task to a high degree . name matching is the most difficult part of the task and requires a lot of data and time to train , hence leading to incorrect predictions and incorrect answers .
table 6 shows the ablation results on the hidden test set of hotpotqa in the distractor and fullwiki setting , compared to previous models . the results show that g2s - gat significantly outperforms all the other models apart from the case of s2s , which shows the advantage of finetuning word embeddings during training . the difference between the average ref and the average conf of the models is less pronounced under the directed acyclic f1 test set , but still suggests some advantage for golbeck et al . ( 2018 ) in the low - supervision settings . as the results show , combining the features of the two sets boosts the generalization ability of the model , and the effect on the output is also notable .
table 3 compares the performance of our model with previous stateof - the - art models on the hidden test set of ldc2015e86 and ldc2017t10 . the results , summarized in table 3 , show that our g2s model significantly outperforms all the previous models on both datasets with a gap of 3 . 32 ± 0 . 18 points from the last published results . on the other hand , our model achieves a marginal improvement of 2 . 45 points over the previous state of the art on both sets , outperforming both the published and unpublished results by a noticeable margin .
table 3 : results on the test set of ldc2015e86 when models are trained with additional gigaword data . the results show that our g2s - ggnn model outperforms the previous state - of - the - art models across all metrics by a large margin . by a margin of 2 . 60 points , we achieve the best performance on the ldce86 test set .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model significantly outperforms the previous stateof - the - art models on all metrics except meteor . on the development set , our get model obtains a performance improvement of 3 . 42 % over the previous best state - of - art model . on top of that , the size increase of our model is 2 . 3 % higher than that of bilstm .
we observe that g2s - ggnn models achieve gains of 1 . 51 % and 0 . 8 % over the baseline models on sentence length and average number of tokens per sentence , respectively , compared to the results of s2s and gat .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence when s2s is trained and tested on the test set of ldc2017t10 . the token lemmas are used in the comparison . as the table shows , g2s - gat has the better generalization performance and therefore requires fewer tokens to generate a sentence , as shown in table 8 . however , the difference between the average number of tokens in the gold - two - mention generated sentences and those in the original input is less striking , as seen in fig . 3 . sesame street - trained models generally outperform the original ones , as measured by the percentage of tokens missing from the output ( i . e . , the miss metric ) , because the size and type of tokens used for the training set is much smaller . in our particular case , gold refers to the reference sentences .
table 4 shows the sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the pos features significantly improve over the original nmt embeddings , showing that the model can learn the task to a high degree . the difference in accuracy between pos and sem is less pronounced for the two target languages , but still significant , showing the extent to which the semantic features can be improved with a reasonable selection of the lexical features .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . our embeddings yield significantly higher precision than the baselines for both tags . word2tag also significantly outperforms the unsupervised encoder - decoder . the difference is most prevalent in pos , the most frequent tag , and sem is the second - lowest . with these baselines in mind , we computed the mean of the transfer learning rates for each tag and the upper bound of the slot prediction using unsupemb encoder . the results are shown in table 2 .
table 4 presents the system ' s performance on the four types of tokens . we observe that for all but one of the four tokens , the system performs better than the best state - of - the - art model on average when trained and tested on the hidden test set of hotpotqa . the difference is most prevalent in the pos category , where bbn outperforms all the other systems apart from the case of translatelink .
table 5 shows the pos and sem tagging accuracy with features from different layers of our system , averaged over all non - english target languages . our proposed uni / bidirectional / residual nmt encoder achieves gains over the strong baselines across all three target languages , showing that the transfer learning method can significantly improve the interpretability of these features .
table 8 shows the attacker ’ s performance on different datasets that we trained on . results are on a training set 10 % held - out . gender and race are the most difficult classifiers for the attacker to pick out , as their accuracy is relatively low compared to gender and age . the classifiers trained on pan16 , however , are comparable in predicting gender and race , as shown in table 8 . on the other hand , age and sentiment are the easier classifiers to predict , as the attacker gets a higher performance on gender and sentiment . finally , we show the difference between the attacker score and the corresponding adversary ' s accuracy on all the three classes .
table 1 shows the performance of our system for training directly on a single task . our system outperforms all stateof - the - art models across all metrics with a gap of 10 . 8 % in accuracy compared to the previous state of the art .
table 2 shows the results for gender - balanced and unbalanced data splits , gender - balanced task data splits show that pan16 predicts gender bias , and racial disparities are less severe than gender - based bias . the classifiers trained on the data are able to distinguish between the effects of gender bias and classifiers , confirming the importance of the diversity of the protected attributes . overall , the classifiers show that gender - parity is the most important classifier for prediction , followed by age , with a gap of 2 . 5 points between gender and race .
the performance of our system on these test datasets is shown in table 3 . the difference between the attacker score and the corresponding adversary ’ s accuracy is minimal , however we see significant difference in leakage due to different classifiers being trained on different datasets and the training set size . gender bias also affects the performance , as it leads to incorrect predictions of gender and race . finally , age and classifiers contribute similarly , with the gender bias having a bigger impact . overall , the system performs well , outperforming all the alternatives with a gap of 10 . 2 % on average .
the results of the different encoders are shown in table 6 . the rnn encoder shows that when the protected attribute is guarded , its performance on the l2rnn task becomes better . guarded rnn also shows a drop in performance compared to unencoders , the difference between the rnn and guarded is less pronounced for leaky embeddings , but still significant enough to warrant a closer look .
table 1 compares the performance of our approach with previous stateof - the - art models on the hidden test set of ptb and wt2 , both with and without finetune ( see yang et al . , 2018 ) . the results , summarized in table 1 , show that our approach outperforms all the base systems on both datasets with a gap of 10 . 36 % on average compared to the previous state of the art . the difference is most prevalent on the wt2 dataset , where our model obtains a performance gap of 3 . 59 % on both sets with the difference being most prevalent in the dynamic setting . further , the difference between the two sets is less pronounced under tuning , with our model achieving a final tuning result of 1 . 97 % on the fixed - params subset and 2 . 45 % overall improvement on the dynamic subset .
table 2 presents the results of our final model on the hidden test set of hotpotqa in the distractor and fullwiki setting , respectively . the results are presented in table 2 , where we report the full set of parameters and the time taken to compute each parameter for each parameter . our model obtains the best results with an absolute improvement of 1 . 59 points over the previous state of the art on both the lstm and sru datasets .
table 3 presents the results of our second study on yelppolar and amafull time on the hidden test set of hotpotqa in the distractor and fullwiki setting , compared to zhang et al . ( 2015 ) . the results are presented in table 3 . the proposed hgn outperforms both published and unpublished work on every metric by a significant margin . for example , hgn achieves an err of 38 . 08 and a time - to - error ratio of 1 . 08 on the ama full and amapolar time scales , respectively , with an absolute improvement of 2 . 36 and 1 . 63 points over the previous state of the art . table 3 compares the performance of these models on different test sets . we benchmark against the following three baselines : ama , yelp , and google + ( table 3 ) . the results , summarized in the table , appear to indicate that hgn has superior generalization ability on both datasets with a large difference in err and time to error compared to previous work . although the error reduction on yelp is small , it is significant and should not be dismissed , as it shows the hgn model performs better on both test sets when trained and tested on a larger corpus . on the other hand , this small difference does not represent a significant big performance drop , as the error reductions on atr and gru are much larger than those on yelp , likely in part due to the training data size .
table 3 shows the case - insensitive tokenized bleu score of our model on the wmt14 english - german translation task . the difference in time between train and decode is minimal , however we see significant difference in performance due to the large variation in training size and the number of tokens used to encode one sentence , as shown in table 3 . as the comparison of gnmt and atr shows , gnmt has the worse performance and therefore requires more training time to decode a sentence , while olrn has the better performance . table 3 also highlights the scalability of using multi - params learning on a single training step . gru and sru both require a significant amount of time to train , as their training instances are much larger than the size of the training data available in newstest2014 dataset . at the same time , the size and type of tokens required to decode each sentence is relatively small , as those used to tokenize the sentences are much smaller than those in newestest2014 . though sru has seen more training instances in the past , it is comparable with lrn in terms of performance , as it requires significantly less training time . we conjecture that this is because the sru neural network relies on a significantly larger corpus of lexically similar tokens , and hence requires much more training data to synthesize each sentence .
table 4 shows the exact match / f1 - score on squad dataset , as reported by wang et al . ( 2017 ) . the model with the greatest performance had the most parameter number of elmo , so we report only the results with respect to that parameter number when only using the model with elmo as a parameter . the other models that had lower parameter numbers than elmo were slightly better than the lrn model . table 4 also highlights the scalability of parameter sharing in low - supervision settings . though the lstm model had the highest match rate , it obtains the best overall f1 score ( 76 . 14 % ) and is comparable to the strong baselines of glove and atr . we observed that the combination of atr and gru significantly improved the model ' s performance , as shown in fig . 3 , the smaller size of the base allows the model to rely less on superficial cues . besides , the gru model significantly outperforms the other two baselines in terms of match rate . it achieves the highest score with 78 . 14 / [ bold ] 83 . 83 % on average compared to the previous state - of - the - art model ' s 69 . 67 / 78 . 79 % and 75 . 63 / 83 . 83 percent on average .
table 6 shows the f1 score of our model ( lstm * ) on the conll - 2003 english ner task . although the number of parameters in our model is small , it achieves the best performance with an f1 of 90 . 56 . the difference between the reported result and the actual result is less pronounced for lrn , indicating that lrn is more effective in deterministic tasks . at the same time , atr and sru obtain higher f1 scores than lrn and atr , indicating the scalability of parameter sharing . gru also receives a boost in performance as a result of the increased training set size . table 6 presents the results of re - training our model after replacing the training set with the one from lample et al . ( 2016 ) with the ones from our second variation .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . with the base setting , our model obtains the best performance on both snli and ptb tasks . the difference in test accuracy between the two sets is minimal , however we see significant difference in perplexity due to different language modeling strategies used in these experiments .
table 1 shows the system performance on the word analogy task for english and german . we benchmark against the following systems : system retrieval ( b - 2 , b - 2 ) and mtr ( mtr ) . the results are presented in table 1 . retrieving the word " human " from the training data significantly improves results for both systems , with the boost being most pronounced for english . oracle also boosts results for german , with a boost of 2 . 8 % on average . sentence reductions range from 0 . 08 - 3 . 45 points on average compared to previous models . word analogies are statistically significant with both systems with different feature sets in terms of both system and sentence prediction . in general terms , the results show that combining all the information from training and test sets boosts system performance , both when using word analogy and when using oracle .
table 4 presents the results of human evaluation . our system outperforms all the other systems with a large margin . among all the systems , it receives the highest score on grammatical accuracy ( grammaticality ) and appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 8 points , which indicates that our system is indeed of high quality . the second highest ranking is obtained by candela ( 30 . 2 % ) on the content richness scale , with a lower standard deviation of 1 . 6 points compared to seq2seq ( 25 . 6 % ) . table 4 shows that the syntactic features discussed in the previous section hold high predictive value , indicating that the system developed by h & w ( 2018 ) is well - equipped to perform this task . retrieval , on the other hand , is only effective at a marginal improvement of 2 . 2 points over the previous state - of - the - art .
the performance of our system on the test set is shown in table 1 . we observe that compared to ted talks , our system performs better on all metrics with a gap of 2 . 5 points on average compared to the previous state - of - the - art . table 1 shows that our model outperforms all the base the difference is most prevalent in relation to text - similarity test set , on the other hand , we observe that our approach outperforms both ted talks and its variants on every metric by a noticeable margin . in particular , the gap between en and r drops significantly on the text similarity test set for all but one of the comparison groups , the gap between the two is much narrower on the small - scale test set on the large - scale dataset , specifically , on this subset , our model performs on par with the ted talks dataset , but on the larger scale , it is inferior to it by 3 points .
the performance of our system on the test set is presented in table 1 . we observe that our proposed approach outperforms all the stateof - the - art systems on every metric by a significant margin . on the ted talks dataset , our proposed system achieves the best performance with an absolute improvement of 2 . 36 % over the previous state of the art on all metrics . table 1 shows that our approach significantly boosts the generalization ability of term feature - values on datasets with fewer training examples , since the training data size and type of training data are small .
the results are shown in table 1 . table 1 shows that our system outperforms all the stateof - the - art systems on every metric by a significant margin . on the ted talks dataset , our system achieves an en / pt score of 0 . 5295 / 0 . 3744 and r = 0 . 4515 on the test set , respectively , with an absolute improvement of 2 . 36 / 4 . 36 and 6 . 45 / 4 points over the previous state of the art . the difference is most prevalent in the text - similarity category , where our system performs on par with or better than all the other systems apart from ted talks .
from table 1 , we can further calculate that the average depth and the number of roots per row of the named entities in question is the most important metrics in our dataset . according to the table , europarl has the best performance with an average of 11 . 05 % on both metrics . the difference between max and average depth is less pronounced for docsub , however , with a difference of 1 . 46 % on average . we can also see that both maxdepth and averagedepth contribute significantly to the overall quality of the dataset , as measured by the hclust metric reported in table 1 . from this group of metrics , the most interesting ones are depth cohesion and semantic features , which show that the semantic information injected into the embeddings by the additional cost term is significant enough to result in a measurable improvement in the performance . this corroborates our intuition that the shared vocabulary structure of docsub and slqs encourages the generation of compact sentences with high semantic cohesiveness .
from table 1 , we can further calculate that the average depth and the number of roots per row of the lexical entities in question is the most important factors in the clustering performance of our system . according to the table , europarl has the best performance with respect to all metrics . the difference between average and maxdepth is most prevalent in relation to docsub , df and slqs datasets , as can be seen , both datasets have low correlation with the human judgement about depth . according to our system , the difference between maxdepth and average depth is less pronounced for docsub and df datasets , but still significant enough to warrant a mention here . from the above table , we calculate that for both datasets , the averagedepth and the averageroots of the objects in question are significantly higher than the maxdepth level , indicating that the semantic features captured by the embeddings are of high quality . to test the contribution of these features , we compare our system to previous models . we observe that , let alone a drop in performance , our system outperforms the previous state - of - the - art on all metrics by a significant margin .
the results of applying our loss functions on the validation set of visdial v1 . 0 . 3 are shown in table 1 . the enhanced version of our loss function , named lf , shows marked improvements in performance over the original embeddings . it achieves a new state - of - the - art result of 73 . 42 % on the qt metric , which implies that the enhanced loss function can significantly improve the interpretability without a drop in performance . although the improvement is slim , it is encouraging to continue researching into this area and consider applying our principles in the future .
the performance ( ndcg % ) of these models on the visdial v1 . 0 validation set is shown in table 2 , and it can be seen that applying p2 indicates the most effective one ( i . e . , hidden dictionary learning ) is the most cost effective and therefore requires the most time to apply . note that only applying p1 is implemented by the implementations in section 5 with the history shortcut . the difference in performance between baseline and p2 underlined by the large difference in coatt score between the baseline and the current state of the art . moreover , the difference between the rva and coatt scores under - performs the baseline by a large margin . this indicates that the use of p2 information induces the model to make incorrect predictions and thereby leads to incorrect predictions .
table 5 shows that the hmd - f1 model significantly outperforms the other approaches on both soft and hard alignments . the difference is most prevalent in the case of soft alignments , where wmd - unigram and hmd - prec achieve gains of 1 . 8 and 2 . 3 points over the ruse model , respectively .
the system ' s performance on the test set is presented in table 1 . the first group shows that all the metrics we consider have comparable performance with the baselines on the direct assessment set . however , the difference is most prevalent in ru - en , the largest of the four sets , where our bertscore - f1 score of 0 . 828 is significantly higher than any of the other baselines . sent - mover the second group shows lower performance than the others , but higher than the baseline on all the other sets , indicating the effectiveness of our method . we observed that the transfer learning method from meteor + + to smd + w2v significantly improves the generalization ability of these metrics .
the experimental results on the hidden test set of hotpotqa are shown in table 1 . the proposed system outperforms both the published and unpublished results on every metric by a significant margin . for example , bertscore - f1 achieves an absolute improvement of 2 . 3 points over the bleu - 1 baseline on average . on the other hand , it achieves a marginal improvement of 0 . 1 point over the reaper score on the sfhotel test set , which shows the diminishing returns from mixing the training data from different baselines . qualitatively , the results reconfirm that the transfer learning approaches based on meteor and w2v are effective in the low - supervision settings , i . e . , when only using the raw scores from one baseline , the system performs on par with or better than the other baselines on all metrics but the difference is most prevalent on smd , sent - mover is statistically significant only on cases when using smd alone , it is clear from table 1 that combining the features from the two baselines boosts the predictive performance of the final score by significant margins . to test the contribution of the clustering approach , we compare our proposed system to the original embeddings . we observe that the results are comparable on both sets , with the difference being more pronounced for smd . specifically , both sets achieve higher predictive performance on the inferences obtained using the smd dataset . on the larger scale , the scores obtained by the second set exceed the original ones by a noticeable margin .
word - mover with different learning rates on the training set outperforms the baseline models on both m1 and m2 metric by a significant margin . the results are shown in table 1 . the numerical results displayed in bold indicate that the word - mover task is very difficult , and requires a significant learning rate to achieve the task success . sentances are particularly difficult to predict with the current set of features , as the performance on m1 metric is significantly lower than that on m2 . when using spice as a baseline , the results are slightly better , but still significantly worse than those on leic ( * ) and spice ( 0 . 939 ) . the clustering quality of the learned rewards is very high , as measured by the bertscore - recall metric , and the average number of correct answers obtained by each classifier is significantly higher than the average of the other two . we observe that the semantic features shared by spice and leic contribute similarly to the task , with the difference being more pronounced for word2vec .
we show the results of models trained on pascal - voc on the hidden test set of simnet , specifically , the results obtained with the help of the greek word " para " . the results are summarized in table 1 . the first group of results show that when training with only shen - 1 , the performance reach the best , with an absolute improvement of 1 . 81 points over the baseline on all metrics except simnet . adding the language features of lambda - para and syntactic distance boosts performance by 2 . 3 points , but only marginally overpara , the second group shows that the effect is less pronounced for lemma - based models , adding all the features together improves performance by 1 . 2 points , though still significantly less than the baseline .
table 3 presents the results for english and spanish . we observe that yelp significantly outperforms google translate in all aspects ( except for the transfer quality metric , table 3 shows that transfer quality and semantic preservation are the most important aspects , semantic preservation is the second most important and closely related to transfer quality . to test the contribution of parameter sharing , we compare our proposed system to two baseline models - yelp ( yelp . com ) and zsgnet - whose performance on this data is reported in table 3 . in both cases , the proposed system outperforms the original embeddings by a significant margin .
table 5 presents the results of human evaluation for validation of our summaries . the summaries generated by our system match the spearman ’ s [ italic ] ρ b / w sim and human ratings of semantic preservation , but do not match the ratings of fluency , as those results are in table 5 . at the same time , the summaries match the human ratings on both metrics , as the results show that both summaries contain grammatical errors that are rare in a single sentence ( hence , there is no significant difference in the performance between human and machine evaluation results for these two metrics ) .
we show the results of models trained only on simnet with the shen - 1 embeddings pre - trained on pascal - voc the results are shown in table 1 . the first group shows that the combination feature that gives the best performance boosts the generalization ability of our model , especially for the task at hand , with m0 reaching an absolute improvement of 3 . 3 points over the baseline . para - para features alone improve the results for all models except for those trained with simnet - trained max - supervising function , the difference is less pronounced for para - trained models , with the help of lexical features , the second group of models significantly outperforms the first group , we observe that the performance reach the best when the model is trained with both lexical and syntactic features , moreover , the improvement is much larger when the feature is augmented with 2d features ,
results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences , are shown in table 6 . our best models ( which use the best classifiers and language embeddings ) achieve higher acc than prior work at similar levels of transfer , and indeed reach the highest level of acc as well . however , the difference between transfer and untransferred sentences is less pronounced for yelp , because the training set size is much smaller . the transfer baseline achieved by our model ( yang et al . , 2018 ) underperforms previous work by a large margin . note that the definition of acc varies by row because the classifiers in use . our model achieves the highest acc level as a result of using only one classifier , and therefore requires significantly less training data and time to train , as shown in fig . 3 .
in table 2 , we report the percentage of reparandum tokens that were correctly predicted as disfluent when we included repetition tokens and nested disfluencies . reparandum length is the average of the number of tokens in a sentence over the length of repetition tokens , so the difference between the average and the average is less pronounced for repetition tokens . as the table 2 results show , when we only consider tokens that are in the sentence structure of the original sentence , the effect of repetition is less significant than that of disfluency . however , it is clear from table 2 that for nested disfuncions , there is a significant difference in the performance between rephrase and restart , as these tokens are much longer .
table 3 shows the relative frequency of rephrases that are correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) . as shown in the table , the majority of tokens in these tokens belong to each category , and only a fraction belong to the category that is not in the content word category ( i . e . , function - function ) . however , the difference between the average and average true f1 scores for all tokens is less clear , in case of repair , there is a significant difference ( p < 0 . 05 ) between the true and negative f1 numbers for both categories .
the results are shown in table 2 . we observe that the text - adaptive models perform well both when trained and tested on the raw data , with the exception of the case when we use only the text embeddings from one domain ( e . g . , all the text and all the innovations are combined in one model ) . the performance gap between the best and worst performances is modest but significant , in particular , we see that the improvements are most prevalent in the late stages , when we consider that text and innovations are the only two types of raw materials that are relevant to the task at hand , and that the others are only useful for syntactic or semantic information .
the performance of our model compared to the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model shows marked improvements in accuracy over the previous state of - the - art embeddings . it closely matches the performance of rnn - based and self - attention neural networks , improving upon the model ' s performance by 3 . 59 points in f1 score .
table 2 shows the performance of all the methods that we consider for the document dating problems on the apw and nyt datasets . our unified model significantly outperforms all the previous models , showing that the attentive neuraldater is better than all the other methods that aim to improve the interpretability of documents . the ac - gcn model also achieves gains over the previous state - of - the - art on both datasets . in fact , it outperforms the best previous model by a large margin .
table 3 shows the performance of our method compared to the approaches described in section 6 . 2 for both word and graph attention . the difference in accuracy between the two approaches is significant , with neuraldater achieving 61 . 6 % accuracy and s - gcn achieving 63 . 2 % . with respect to word attention , our method obtains a significant improvement of 3 . 8 % over the approach by using the greek word analogy . graph attention also shows a significant increase in performance , showing the effectiveness of both word attention and graph attention for this task .
the performance of our model on the training data is presented in table 1 . the first group shows that our approach establishes a new state - of - the - art on all three stages . the dmcnn model significantly outperforms all the alternatives with a gap of 10 . 6 points from the last published results ( hochreiter and schmidhuber , 2008 ) on both training and test set . while the gap between embedding + t and argument is slim , it is significant enough to warrant a mention here . trigger and argument are the most difficult stages to solve , with the jrnn model achieving gains of 2 . 2 points over the previous state of the art . on the other hand , the argument stage is the most rewarding , giving a 3 . 6 point improvement over the disjoint model of cnn . since this stage is crucial for argument resolution , we also note that the method relies on syntactic and semantic information sharing , and therefore requires a lot of data and time to train .
table 1 presents the system ' s performance on event prediction . our proposed method establishes a new state - of - the - art on all three high - level tasks , and on all subtasks with a gap of 10 . 5 % on average compared to previous state of the - art models . the results are presented in table 1 . predicate word - error mechanisms significantly outperform unsupervised classifiers and classifiers . cross - event event prediction underperforms dfgn , as the results of cross - event event prediction show , the semantic information injected into the training data by the additional cost term can have a significant impact on the performance of the model , and this is evident from the significant drop in performance between dfgn and dfgn .
consistent with intuition , the results of spanish - only - lm shows that it is better than both the plain - english and french - only variants . however , it is inferior to both the original and the slightly - tuned variants when trained and tested on the unigram - trained corpus . the results of fine - tuning the model after applying the best performing features are shown in table 5 . the results show that fine - tuned - lm is superior to all the alternatives except for the case where it is trained on the unlabelled corpus , i . e . , when trained only on the original lm , the performance on this subset is significantly worse than the other two .
results on the dev set and the test set are shown in table 4 , fine - tuning the model with only subsets of the code - switched data improves the results for both sets . the improvement over cs - only training is substantial , with gains on both sets reaching 4 . 2 % and 2 . 5 % over the baseline . the difference between fine - tuned and un - tuned models is less striking , but still significant , with improvements of 2 . 1 % and 3 . 4 % on the train and test set .
the results in table 5 show that fine - tuning gives a significant improvement in the performance on the dev set over the monolingual approach , and on the test set , both when using the gold sentences from the source set and the gold sentence from the second set , respectively . the difference in accuracy between the two sets is minimal , however we see significant difference in performance between mono and code - switched sentences , fine - tuned cs - only - disc shows a marked drop in performance compared to fine - tuned - lm , which shows the diminishing returns from mixing source and target labeled tokens .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvement from baseline to the current state - of - the - art is statistically significant ( t - test , p < 0 . 05 ) and r = 0 . 61 , which indicates significant improvement in the interpretability of our model . the improvement over the baseline is even more striking when we compare it to the f1 score of baseline , which shows that combining all the features helps the model to improve interpretability .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . the improvement from baseline to type combined gaze features is statistically significant ( p < 0 . 05 ) and r > r = 0 . 03 , which indicates that the type - gathering approach has a significant impact on the interpretability of the model .
the results on the test set of belinkov2014exploring ’ s ppa are shown in table 1 , where glove - extended refers to the synset embeddings obtained by running autoextend rothe and schütze ( 2015 ) on wordnet 3 . 1 . it achieves gains over the type - based lstm - pp model by 1 . 8 points , which shows the advantage of redundancy removal . further improving performance by high margins . the average number of tokens per word is 1 . 7 times as high as in the original paper ( farrequi et al . , 2015 ) and slightly higher than in the second variation ( hpcd ) . the difference between automatic and manual extension is less pronounced in the third variation , but still significant .
table 2 shows the results for rbg with various pp attachment predictors and oracle attachments . the results show that the hpcd dependency parser significantly outperforms the ontolstm - pp model , indicating that the dependency trees developed by glove et al . ( 2018 ) can be further improved with the addition of additional pp features . moreover , the accuracy increases with the growth of the uas as measured by the ppa acc . on the training set , which shows the scalability of adding out - of - the - box features to the model . finally , we observe that the transfer learning method further improves the generalization ability of the system .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results show that both of these features have a significant effect on predictive performance ( p < 0 . 05 ) and accuracy ( p < 0 . 01 ) . however , the effect is less significant for context sensitivity , showing that it is less beneficial to remove sense prors than context sensitivity .
in table 2 , we report the bleu % scores of adding subtitle data and domain tuning for image caption translation using the three types of embeddings described in marian amun et al . ( 2017 ) . the results show that the domain - tuned model achieves the best results , improving upon the performance of en - de and mscoco17 by 3 . 6 points . the difference is less pronounced for multilingual models , but still significant , with an absolute improvement of 2 . 7 points over the baseline .
we show that the domain - tuned h + ms - coco model outperforms the plain - lstm model in terms of both en - de and out - of - domain comparisons . the results are presented in table 3 . as can be seen , the performance reach the best when the model is trained and tested on subs1m , the largest of the four datasets , with an absolute improvement of 2 . 8 points over the baseline . adding all the labels helps the model to improve its performance in both languages . the difference is less pronounced in en - fr and in - de , but still significant .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . adding only the best 5 captions improves marian amun et al . ( 2017 ) model ' s performance by 1 . 8 points over the results of en - de ( fancellu et al . , 2016 ) . the difference is less pronounced in mscoco17 , but still significant , with an increase of 2 . 3 points over en - fr and 1 . 9 points over mscoca17 . the results are statistically significant with respect to multi - task learning , as the results show , the automatic captions alone do improve the general performance of the model , but only by a small margin .
the results in table 5 show that enc - gate and dec - gate strategies achieve better results overall than en - de and mscoco17 on the three datasets . the difference is most prevalent in terms of bleu % score , encoding the hidden regions of the image leads to better interpretability , as the results of using transformer and subs3mlm shows , once the masking information is applied to the correct object , the model can further improve its interpretability with a gain of 2 . 8 % over the model using the current state of the art .
we show the results for english - and french - speaking captions . the results are summarized in table 3 . sub - 3m achieves the best results with a new multi - lingual lm detectron . it closely matches the performance of subs3m [ italic ] and mscoco17 with only 0 . 40 % absolute difference in performance .
table 1 shows the test set on the hidden test set of hotpotqa in the distractor and fullwiki setting , respectively . the results are presented in bold . en - fr - ht and en - es - ht achieve better results overall than the models by a noticeable margin . the difference is most prevalent in the trans - lexical differences ( differences are statistically significant with t - test , with an absolute improvement of 1 . 3 % on average compared to the baseline ) in terms of ttr and mtld , as this table shows , the semantic information injected into the text by the additional cost term is significant enough to result in a significant improvement in the performance of these models , but only when it is used in combination with mtld .
for brevity we only report the number of parallel sentences in the train , test and development splits for the language pairs we used . the total number of sentences in our splits is 1 , 472 , 203 , which means that there are 7 , 723 sentences per language pair .
the vocabularies for the english , french and spanish data used for our models are shown in table 2 . as the table shows , the difference in the number of errors between en – fr and spanish is minimal , however we see significant variation in the src scores for both languages , which indicates that the training set is quite diverse .
the system evaluation results in table 5 show that en - fr - trans - rev and en - es - rnn - rev achieve comparable performance to the best state - of - the - art systems ( paired t - test , p < 0 . 05 ) . however , the performance gap between the two rev systems is narrower than that between ter and bleu , as evident from table 5 , automatic evaluation scores are very low for rev , with these scores , it is clear that there is a need to design more sophisticated systems and to refine the rev implementation for further improvements .
results on flickr are shown in table 2 . the row labeled vgs is the visually supervised model from chrupala2017representations . com and the second row labeled rsaimage is the audited model from flickr8k . the difference in recall between the two sets is minimal , however we see significant difference in the mean mfcc score due to different sampling strategies used in these experiments . we observed that the vgs model significantly outperforms the rsaimage model in terms of recall @ 10 and average mfcc , indicating that the training set size and type of object prediction performed by the model can significantly impact the model performance .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com and the second row labeled rsaimage is the audiovisual supervised model . the difference in recall between the two approaches is minimal , however we see significant difference in the mean mfcc score due to different sampling strategies used in these experiments . among all the models , audio2vec has the highest recall @ 10 and average mfcc of 0 . 5 , which means that it is more useful to rely on automatic metrics instead of human judgement . also , the difference in chance and recall is less pronounced for rsaimage , indicating that human judgement is more accurate . we observed that the acoustic features extracted from rsaimage by the embeddings have a significant effect on the model performance , however it is less significant than those from audio - 2vec , as shown in fig . 3 , acoustic features alone do not improve significantly over other approaches .
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . as can be seen , all the classifiers turn in sentences that are similar in meaning , except for the one for cnn , which is much more gender specific . the dan classifier turns in a sentence that expresses the sentiment that the speaker wants to hate the character . it is able to pick out the most interesting aspects about the speech patterns and the order in which the sentences are formed , and the average number of times the sentence is repeated , as shown in table 1 . in addition , the rnn classifier shows a marked improvement in performance , showing that it is more effective in selecting the relevant features and its output is more interpretable . it can also be seen that combining all the features improves the interpretability of the output .
table 2 shows the percentage of occurrences for each part - of - speech that has increased , decreased or stayed the same through fine - tuning in sst - 2 . as the table shows , the number of occurrences increased as a result of the decrease in the overlap with the original sentence , but has not increased as much as predicted by intuition . rnp has increased by 3 . 5 % since the start of this study , though still performing substantially worse than cnn . the difference between the average number of instances for nouns and verbs is less pronounced for verbs , but still significant , at 2 . 5 % . as expected , the presence of the word " goodness " in the final sentence indicates that finetuning has not changed the interpretation of the sentence . it is clear from table 2 that the rnp scores have increased , but only by a small margin .
the results in table 3 show that the sentiment score increases as a result of the flipped labels being flipped from positive to negative labels . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment .
table 1 compares the results of pubmed and sst - 2 with other systems trained on the hidden test set of sgme . results show that pubmed outperforms all the other systems with a large margin . in fact , it achieves an absolute improvement of 1 . 8 % over the sift baseline on both datasets , which shows that the difference between the two approaches is less pronounced for pubmed .
