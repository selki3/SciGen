table 2 : throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ‘ s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . on the training dataset , the recursive approach outperforms the iterative one with a performance boost of 2 . 5 times over the original model .
table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t parallelization .
table 2 : hyper parameter optimization results for each model with different representation . the max pooling strategy consistently performs better in all model variations . conll08 achieves the best performance on all models with the best representation . the best performance is achieved on the connll08 model with the highest number of feature maps .
table 1 : effect of using the shortest dependency path on each relation type . the best f1 ( in 5 - fold ) is shown in table 1 . the macro - averaged model outperforms the model - feature model on all relation types except part_whole , which outperforms both macro - and model_feature .
table 3 shows the performance of the y - 3 : y model with respect to f1 and r - f1 . the performance of our model is shown in table 3 . we observe that our model outperforms all the other models except for f1 , which outperforms the other two by a significant margin .
table 2 shows the performance of the mst - parser on the test set . the performance is comparable to that of the standard mst parser on all test sets , except for the fact that it does not perform as well on the first set of test sets . on the second set , the performance is similar to the standard .
table 4 shows the performance of stagblcc and lstm - parser on the essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . the performance of the two systems is shown in table 4 .
table 2 shows the results of the training and test set . the results are shown in table 2 . we observe that the clean - up of the tgen + model outperforms the original model by a significant margin . this is due to the fact that the training set performs better than the original on all test sets . on the test set , we observe a significant improvement over the original .
table 1 : data statistics comparison for the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . the results are shown in table 1 . our cleaned version outperforms both the original and the original by a significant margin . the difference in ser between the original train and the cleaned version is significant .
table 2 shows the results of the training and test set . the results are shown in table 2 . we observe that sc - lstm outperforms both the original and the original tgen + model on all test sets except for the test set , where the original model outperforms the original .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) . the total number of errors is shown in table 4 . we found that tgen can be trained on the original training data , but not the training data .
table 2 shows the performance of the model on all datasets . the results are shown in table 2 . we observe that the best performing model is the graphlstm ( song et al . , 2018 ) , which outperforms all the other models except for seq2seqk by a large margin .
table 2 shows the model size in terms of parameters ; “ s ” and “ e ” denote single and ensemble models , respectively . table 2 : main results on amr17 . seq2seq ( beck et al . , 2018 ) achieves 24 . 5 bleu points . gcnseq ( damonte and cohen , 2019 ) achieves 28 . 5 points . ggnnseq achieves 27 . 3 points . as shown in table 2 , the best performance is achieved by using a single - parameter model .
table 1 shows the performance of the models on the english - german and german - czech languages . the results are shown in table 1 . we observe that the best performing model is seq2seqb ( beck et al . , 2018 ) , which outperforms all the other models except cnn + gcn .
table 5 shows the effect of the number of layers inside the dc on the performance of the model . the effect of layer number on the model is shown in table 5 . the model achieves the best performance with the most layers . the model outperforms all the other models in terms of performance . in particular , the model with more layers outperforms the model without more layers . on the other hand , it outperforms other models with fewer layers .
table 6 : comparisons with baselines . + rc denotes gcns with residual connections . the results are shown in table 6 . we observe that dcgcn1 ( 9 ) outperforms the baselines by a significant margin . furthermore , we observe that the performance of the gcn + rc + la ( 6 ) is comparable to the baseline .
table 3 shows the performance of the dcgcn model in the real - world setting . we observe that the model outperforms all the other models except dcn ( 2 ) by a significant margin . the best performing model is the one with the best performance , which outperforms the best - performing model with the least number of errors .
table 8 : ablation study for density of connections on the dev set of amr15 . the results of the ablation study are shown in table 8 . we observe that removing the dense connections in the i - th block significantly improves the performance of the model . in particular , the model achieves a significant improvement over the previous model in terms of the number of connections .
table 9 shows the results of the ablation study for modules used in the graph encoder and the lstm decoder . the results are shown in table 9 . we observe that the embeddings in the decoder outperform both the global node and the linear - combination encoder by a significant margin .
table 7 : scores for initialization strategies on probing tasks . glorot outperforms our paper on all the probing tasks , except for the one where it outperforms the other two . our paper outperforms all the other models on all probing tasks except the one in which it is outperformed by a large margin . on the other hand , we outperform all the models except the first one .
table 2 shows the performance of the h - cmow and h - cbow models . the results are shown in table 2 . the performance of both models is comparable to that of the previous model . on the other hand , on the cmow model , the performance is significantly worse than on the cbow model .
table 2 shows the performance of the hybrid and the cbow / 784 models . the hybrid model outperforms the cmow model by 0 . 2 % and 0 . 6 % respectively . the cbow model achieves the best performance on all of the models .
table 3 : scores on unsupervised downstream tasks attained by our models . the results are shown in table 3 . hybrid achieves the best performance on all the downstream tasks , with the exception of sts15 , which achieves the worst performance on sts16 . we observe that the performance of hybrid outperforms all the other models except sts12 .
table 8 : scores for initialization strategies on supervised downstream tasks . glorot outperforms all the other initialization strategies except for trec and mpqa . our model outperforms the other models on all the tasks except sst2 .
table 6 : scores for different training objectives on the unsupervised downstream tasks . on the supervised downstream tasks , sts12 outperforms sts14 and sts15 on all tasks except sts16 . the performance of cmow - r is comparable to that of sts13 .
table 2 shows the performance of the cmow - r and the cbow - c models . the cmow model outperforms the other two models by a significant margin . it outperforms both of the other models in all cases except for the last one , where it outperforms all but one of the models .
table 3 shows the performance of the cmow - r and cmow model on the sick - e test set . the results are shown in table 3 . the performance of cmow is significantly better than that of the cbow model . in particular , it outperforms both cbow and cbow - c on all test sets .
table 1 shows the performance of the system on all three models . the results are shown in table 1 . we observe that the model with the best performance is τmil - nd , which outperforms all the other models with respect to both loc and org . on the other hand , the system with the worst performance is mil - nd .
table 2 shows the results on the test set under two settings . the results are shown in table 2 . we observe that the model with the best performance is the one with the highest f1 score . the best performance of the two settings is achieved by using the supervised learning model . this model achieves the best f1 scores in both sets . in addition , it achieves the highest score in the name matching set .
table 6 shows that g2s - gat outperforms all the other models in terms of performance . the model with the best performance is the one with the highest number of features , which is the gat model . this model outperforms the other two models by a significant margin . the best performance of the model is achieved by using the git model .
table 3 shows the performance of the model with respect to the ldc2015e86 and ldc2017t10 . the g2s - gat model outperforms all the other models with a significant improvement over the previous state - of - the - art model . the model with the best performance is the one with the highest bleu score , which shows that the model can outperform all the previous models .
table 3 : results on ldc2015e86 test set when models are trained with additional gigaword data . g2s - ggnn outperforms all the other models on the test set . the performance of the model is shown in table 3 . we observe that the model outperforms the other two models in terms of performance on the gigaword dataset .
table 4 shows the results of the ablation study on the ldc2017t10 development set . the results are shown in table 4 . we observe that the bilstm model outperforms the geb model on the development set by a significant margin .
the results are shown in table 1 . the g2s - gat model outperforms all the other models in terms of the number of sentences and the length of sentences . it achieves a significant improvement over the previous state - of - the - art model . it achieves an improvement of 0 . 51 % over the state of the art model in the sentence length and 0 . 43 % over that in the length .
table 8 : fraction of elements in the output that are not present in the input ( added ) and the fraction of elements that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . s2s and g2s - ggnn are the only models that outperform the other models in terms of accuracy . the model that outperforms the other two models is the one with the best accuracy .
table 4 shows the pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the results are shown in table 4 . the pos tagging accuracies are significantly better than the sem tagging accuracy .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . the results are shown in table 2 . word2tag achieves 95 . 06 % pos accuracy and 95 . 11 % sem accuracy . unsupemb achieves 91 . 95 % pos and 91 . 55 % sem .
table 4 shows the performance of the system on the pos and sem tagging accuracy test set . the results are shown in table 4 . we observe that the system achieves the best performance on both the pos tagging accuracy test set and on the sem test set , with a significant improvement over the previous state of the art .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 . we observe a significant improvement in the pos accuracy with each layer .
table 8 : attacker ’ s performance on different datasets . results are on a training set 10 % held - out . the performance of the attacker is shown in table 8 . on pan16 , the attacker performs better than the adversary on all datasets except pan16 .
table 1 : accuracies when training directly towards a single task . the results are shown in table 1 . pan16 outperforms pan16 when trained directly towards the task .
table 2 : protected attribute leakage : balanced & unbalanced data splits . the results are shown in table 2 . we observe that the unbalanced and balanced data splits are less likely to leak the data .
table 3 shows the performance on different datasets with an adversarial training . the results are shown in table 3 . on the pan16 dataset , the best performance is achieved on the gender dataset . the best performance on the age dataset is obtained on the task dataset .
table 6 : accuracies of the protected attribute with different encoders . the results are shown in table 6 . rnn and guarded encoder outperforms the rnn encoder in all cases except for leaky , which outperforms rnn in leaky .
table 1 shows the performance of the model on all three models . the results are summarized in table 1 . we observe that the model outperforms all the other models except for the lrn model , which outperforms the other two models by a significant margin . on the other hand , the model with the best performance is the lstm model . it outperforms both lrn and the gru model .
table 2 shows the performance of the lstm model on the 250k test set . the results are shown in table 2 . we observe that the performance on the test set is comparable to that on the 100k set . however , the performance is significantly worse on the 200k set , as shown by the difference in performance between the two sets of models .
table 3 shows the results of the model and the work on the yelppolar dataset . the results are shown in table 3 . we observe that the model outperforms the work by a significant margin . the model outperform the work in all three cases . in particular , it outperforms both the work and the test set . as shown in zhang et al . ( 2015 ) , the model performs much better than the work .
table 3 shows the bleu score on wmt14 english - german translation task on tesla p100 . gnmt outperforms all the other models except olrn , which outperforms the other two by a significant margin . on the other hand , the performance of the case - insensitive tokenized model is significantly worse than the other model .
table 4 : exact match / f1 - score on squad dataset . the results are shown in table 4 . the model outperforms all the other models in terms of accuracy and f1 score . as expected , the best performance is achieved by using only the parameter number of base . we observe that the best performing model is the lstm model , which outperforms the best - performing model by a significant margin .
table 6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the reported result lample et al . ( 2016 ) and lrn and sru denote the reported results . we observe that lrn outperforms lrn on all the tasks except for the ner one .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . the results are shown in table 7 . we observe that the snli model outperforms the ptb model on both tasks . the accuracy of snli on the perplexity task is significantly higher than the accuracy of ptb .
table 2 shows the performance of oracle on human and human - sentenced sentences . the results are shown in table 2 . on human sentences , oracle outperforms human sentences by a significant margin . in the case of the human sentence , oracle performs better than human on all but human sentences .
the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 0 . top - 1 / 2 : % of evaluations a system being ranked in top 1 or 2 for overall quality . table 4 shows the performance of all the automatic systems on the human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best performance is shown in table 4 .
table 3 shows the performance of our model on all three datasets . the results are shown in table 3 . our model outperforms all the other models except for ted talks , which outperforms both ted talks and europarl by a significant margin .
table 3 shows the performance of our model on the ted talks dataset . our model outperforms all the other models except for dsim , which outperforms the other two by a significant margin . on the other hand , the performance on the europarl dataset is comparable to that on the other datasets .
table 3 shows the performance of our model on the best - performing models . the performance of the best performing models is shown in table 3 . our model outperforms all the other models except for ted talks , which outperforms the worst - performing model by a significant margin .
table 1 shows the performance of the europarl model on all three datasets . the best performing dataset is the one with the largest number of roots , which is 1 . 05 times larger than the other two . on the other hand , the worst performing dataset on all the other datasets are the ones with the smallest number of root rows .
table 1 shows the performance of the europarl model on all datasets . the performance of our model on each dataset is shown in table 1 . on all datasets , our model outperforms the previous model by a significant margin . in particular , we observe that our model achieves the best performance on all the datasets .
table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . the results are shown in table 1 . as expected , the enhanced version of the model outperforms the baseline model by a significant margin . the enhanced version outperforms both the baseline and the baseline version by a margin of 0 . 42 points .
table 2 : performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . the performance of the model with p2 on the validation set is shown in the table 2 .
table 5 : comparison on hard and soft alignments . hmd - f1 and wmd - bigram outperforms ruse on both soft and hard alignments , with the latter outperforming the former by a significant margin . the performance of the hmd - recall + bert model outperforms the ruse model by a margin of 0 . 6 points .
table 1 shows the results of the set - up of the bertscore - f1 model . the results are shown in table 1 . we observe that the average score of the model is 0 . 716 , which is significantly higher than the baseline score of 0 . 624 .
table 1 shows the performance of the bertscore - f1 model on the sent - mover dataset . the results are shown in table 1 . we observe that the bleu - 2 model outperforms all the other baselines except for the meteor model by a significant margin . on the other hand , we observe that sfhotel achieves the best performance on the smd dataset .
table 2 shows the performance of the bertscore - recall setting on the word - mover setting . the results are shown in table 2 . we observe that the accuracy of the word - mover setting is significantly higher than that of the other two settings . the performance is comparable on all three settings .
table 1 shows the performance of our model on all models . we observe that our model outperforms all the other models on all but one of them . the performance of the model on each model is shown in table 1 .
table 2 shows the performance of the models on the yelp dataset . the results are shown in table 2 . we observe that the model with the best transfer quality is the one with the highest transfer quality . the transfer quality of the yelp model outperforms all the other models .
table 5 : human sentence - level validation of metrics ; 100 examples for each dataset for validating acc ; 150 each for sim and pp ; see text for validation of gm . the results are shown in table 5 . for each dataset , we compare the accuracy of the machine and human judgments on each metric . the accuracy of each metric is shown in the table .
table 3 shows the performance of our model on all models . we observe that our model outperforms all the other models on all but one of the models except for the first one .
table 6 shows the results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( right table ) achieve higher bleus than prior work at similar levels of acc , but untransferred sentences achieve the highest bleu . the best model achieves the highest acc , and the best classifier achieves the lowest acc . our best model also achieves the best accuracy . we also observe that the accuracy of our model is higher than that of the previous work .
table 2 : percent of reparandum tokens that were correctly predicted as disfluent . note that nested disfluencies exclude repetition tokens . the disfluency rate is 0 . 39 % on average , and 0 . 66 % on the average on the overall test set .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) . the best - performing category is the content - content category , which has the highest frequency of disfluency . the worst - performing class is the function - function category , with the best performance being the repair category . the best performing category is content - function .
table 2 shows the performance of the model on the test set . the model outperforms the best test set by a significant margin . we observe that the model performs best on test set with the single word embeddings . on test set 2 , the model achieves the best performance on all test sets , with the exception of test set 1 , where it achieves the worst performance .
table 2 shows the performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . our model outperforms all the state of the art embeddings except for the self - attention embedding , which outperforms the rnn embedding by a significant margin .
table 2 : accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . the best performing method is the ac - gcn model , which outperforms the previous models by a significant margin . the best performer is the attentive neuraldater model by a large margin . on the nyt dataset , the best performing model is the neuraldater .
table 3 : accuracy ( % ) comparisons of component models with and without attention . this results show the effectiveness of both word attention and graph attention for this task . please see section 6 . 2 for more details . the accuracy of the oe - gcn model is significantly higher than the sgcn of neuraldater .
table 1 shows the performance of the model on all stages . the performance of all stages is shown in table 1 . we observe that the best performing model is the one with the best embeddings . the best performer is the jrnn model , which outperforms all the other models except the dmcnn model .
table 1 presents the results of the cross - event test set . the results are shown in table 1 . we observe that the best performance is achieved on the cross - event test set , with the best results on the argument test set and the worst on the classification test set ( table 1 ) . the best performing method is the method with the highest f1 score , followed by the best classification score .
table 3 shows the performance of all the models . the results are shown in table 3 . the best performing model is cs - only + vocab - lm , which outperforms all the other models except for english - only - lm . it outperforms the best - performing model by a significant margin .
table 4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the results are shown in table 4 . the fine - tuned model outperforms the cs - only model on both test set and dev set . fine - tuned model achieves the best performance on both sets .
table 5 shows the accuracy on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) . the fine - tuned - disc model outperforms both the fine - tuned - disc and fine - tuned - lm models on both the dev and test set . the difference in performance between the two models can be seen in table 5 .
table 7 shows the performance of the type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . the performance of type combined gaze features is shown in table 7 .
table 5 shows the results of using type - aggregated gaze features on the conll - 2003 dataset . the performance of type combined gaze features is shown in table 5 . we observe a significant improvement in precision , recall and f1 score compared to baseline .
table 1 : results on belinkov2014exploring ’ s ppa test set . hpcd ( full ) is from the original paper , and it uses syntactic skipgram . glove - retro is the embeddings obtained by running autoextend rothe and schütze ( 2015 ) on wordnet 3 . 1 , and glove - extended refers to the synset embedds obtained by using syntactic - sg . the results on the test set are shown in table 1 . the best performance is achieved by using syntactic skipgram on the wordnet test set , and the worst performance is obtained by embedding wordnet on the lstm test set ( see table 1 ) .
table 2 : results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 . hpcd ( full ) and ontolstm - pp ( partial ) outperforms the full uas by a significant margin . on the other hand , the performance of the full system is comparable to that of the original uas ( full ) .
table 3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . our model outperforms the full model by a significant margin .
table 2 : adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun . the results are shown in table 2 . we see that the domain - tuned model outperforms the en - de model by a significant margin . in particular , the domain tuning improves the bleu % score by 2 . 7 points over the original model .
table 3 shows the performance of the domain - tuned subs1m subs1 model compared to the subs2m model . the subs2m model outperforms the subs3m model by a significant margin . we observe that the subs4m model performs better than the subs5m model in all cases .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . all results with marian amun . the best automatic captions are shown in table 4 . the best caption is the one with the best number of captions .
table 5 : comparison of strategies for integrating visual information ( bleu % scores ) . all results using transformer , multi30k + ms - coco + subs3mlm , detectron mask surface , etc . are shown in table 5 . we observe that the enc - gate + dec - gate approach outperforms the en - de and mscoco17 strategies by a significant margin . the dec - gate strategy outperforms both en - fr and ende by a margin of 0 . 86 points .
table 3 shows the performance of the subs3m and subs6m models . the results are summarized in table 3 . we observe that the performance improves with the addition of the ensemble of 3 features . the performance is comparable to that of the previous set of models with the additional features .
table 2 shows the performance of the two approaches . the performance of en - fr - ht and en - es - ht is comparable to that of ttr and mtld . however , the performance gap between these two approaches is much larger than that of the previous model . in particular , the difference in performance between the two models is significant .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . for the train and test splits we used , we used the en – fr and en – es pairs , respectively .
table 2 shows the training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 . our model outperforms the previous model on both languages .
table 5 : automatic evaluation scores ( bleu and ter ) for the rev systems . the results are shown in table 5 . bleu scores are significantly higher than ter scores . ter scores are slightly lower than bbleu scores , but are comparable to those of the previous rev system .
table 2 shows the results of our model on flickr . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled rsaimage is the model from the rsaimage dataset . the results are shown in table 2 .
table 1 : results on synthetically spoken coco . the row labeled vgs is the visually supervised model from chrupala2017representations . com and the row labeled audio2vec - u is the auditory supervised model . the results are shown in table 1 . vgs outperforms all the other models in terms of recall and recall rate .
we report further examples in the appendix . the classifiers are shown in table 1 . the classifier turns in a screenplay with the best performance on sst - 2 . the best performance of the classifiers is shown in the example of cnn .
table 2 : part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . the numbers indicate the changes in percentage points with respect to the original sentence . the last row indicates the overlap with the previous sentence , and the last row shows the percentage point difference between the final sentence and the original one . a score of 0 indicates that the finetuning has not changed the total number of words , and a score of 1 indicates that it has increased the number . the number of instances of each word has also been increased .
table 3 : sentiment score changes in sst - 2 . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the results are shown in table 3 .
table 2 summarizes the results of the experiment . the results are shown in table 2 . we observe that the sst - 2 model outperforms the corr model by a significant margin . however , we observe that corr outperforms pubmed by a large margin .
