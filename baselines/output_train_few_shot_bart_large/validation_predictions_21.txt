table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as the results show , once the training instances are optimised , the model performs well both in terms of inference and training . the difference in performance between the iterative and recursive approaches is less pronounced for the training dataset , but still indicates significant performance improvement .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . considering the fact that the balanced dataset has 25 instances , this indicates that there is still a lot of room to improve the performance .
results in table 2 show that the max pooling strategy consistently performs better than the naive f1 approach ( i . e . , f1 = 1 , f1 . in 5 - fold ) with the same number of parameters in the training set , as well as the dropout probability using the same dropout rate as in the original ud v1 . 3 model . with different representation weights , the hyperparametrization performance of our model decreases as well . we find that selecting the correct hyper parameters with the correct number of features helps the model perform better than selecting the incorrect ones with the wrong representation .
table 1 shows the effect of using the shortest dependency path on each relation type . our approach results in a significant improvement in f1 score over the approach described in section 2 . 3 . we find that macro - averaging gives a significant performance gain over traditional dependency trees like topic , usage and model - feature , and compare , which rely on word embeddings with sdp as dependency trees . it also improves the generalization ability of our approach , since it eliminates the dependency trees duplication caused by sdp .
consistent with the observations by vaswani et al . ( 2017 ) , we observe that the performance reach the best when using only 50 % f1 and 50 % r - f1 on the training data , with an absolute improvement of 2 . 5 % on average compared to the previous state of the art .
for completeness , here we also compare mst - parser with other approaches which use word embeddings similar to the ones described in section 3 . 3 . the results are presented in table 3 . we observe that , on average , both approaches yield comparable results with respect to paragraph accuracy and sentence prediction accuracy . however , the results are significantly worse on the essay level , as it can be seen in fig . 3 , when using only the paragraph level acc . level and the number of frames in an essay , the performance gap between the two approaches is much narrower . with respect to accuracy on the r - f1 and f1 level , the best results are obtained by using mate , which achieves 50 % accuracy on average .
the results are shown in table 4 . the average c - f1 score of our system is 60 . 7 % , which means that our proposed lstm - parser parser performs better than the original stagblcc embeddings on essay and paragraph level .
the results are shown in table 1 . the results show that the original tgen model performed better than tgen + and sc - lstm when the training data are cleaned , indicating that tgen training data is more useful for data augmentation . however , when data are only trained once , the difference between original and cleaned tgen is less pronounced , showing that training with original data helps the model to learn more about the task . it should also be noted that when training with only original data , the performance reach the best when using only tgen - trained data , indicating the advantage of using pre - trained tgen data . finally , when combining all the data from the two sets , we get a noticeable drop in performance between the original and cleaned tgen models .
table 1 shows the comparison of the original e2e dataset with the cleaned version . the difference in number of distinct mrs between the original and the cleaned versions is minimal , however we see significant difference in ser as measured by our slot matching script , see section 3 . the difference is much larger in terms of slot references , as expected , the number of instances in our cleaned dataset is significantly less than in the original one .
table 3 shows the results for the original and the original variants of tgen . the results are broken down in terms of training and test set performance . the original tgen model performs much better than the original one on both datasets , with an absolute improvement of 2 . 27 points over the tgen − model . replacing the training data with the data from sc - lstm , however , results in a significant drop in performance and bleu scores , which indicate that the use of pre - trained knowledge leads to incorrect predictions . the difference between the two is less pronounced for original and original variants , as shown in the second group of table 3 , when using only original data , training with tgen + and original data results in significantly better performance . it can be observed that the training set size and the number of correct answers for each error generation step are the most important factors in the success of the model , as shown in fig . 3 . in fact , if you add both training data and the correct answer , the result of adding them both together improves the results considerably .
the results of manual error analysis of tgen on a sample of 100 instances from the original test set are shown in table 4 . we found that the majority of errors in our system ( 71 % ) are caused by errors caused by adding incorrect values or missing correct values ( 23 % ) , about 5 % of the total number of errors we found is caused by mis - additions , about 3 % by missing values .
table 3 compares the performance of our approach with previous approaches on the single - domain and ensemble datasets . our approach establishes a new state - of - the - art on both datasets , outperforming all the previous approaches except for the graphlstm model by a noticeable margin . the difference is most prevalent in the multi - domain dataset , where our dcgcn ensemble model ( 28 . 9 % ) leads to a significant performance drop compared to previous work . on the other hand , the gap between the two approaches is narrower on the smaller - scale dataset , our approach results in a significant improvement on the two - dimensional dataset ( 15 . 6 % ) when using the graph - lstms trained and tested on the seq2seqk dataset . our joint model outperforms all the other methods apart from the fact that it has more data .
table 2 shows the performance of our model with respect to amr17 . our dcgcn model achieves a final improvement of 3 . 5 bleu points over the previous state - of - the - art work on both datasets . it outperforms both the original seq2seqb model ( beck et al . , 2018 ) and the multi - headed ensemble approach by a noticeable margin . gcnseq ( damonte and cohen , 2019 ) achieves 24 . 5 points improvement over previous work , but still performs substantially worse than our model .
table 3 compares the performance of our proposed method with previous approaches on the english - german , german - czech and czech language datasets . the results are broken down in terms of feature extraction . our proposed method outperforms all the previous approaches except birnn + gcn , it achieves the best performance with 43 . 8 % on average compared to the previous state - of - the - art model , while it achieves a slight improvement on the czech language dataset , it is still significantly worse than the previous methods . we observe that ggnn2seq ( beck et al . , 2018 ) achieves the highest performance with 41 . 8 % , which indicates that the performance gain comes from a better model design .
table 5 shows the effect of the number of layers inside our dc network on performance . we find that , when we add 6 layers , our performance improves by 1 . 8 points over the previous state of the art model . this indicates that more layers inside the network helps the model to maintain high performance .
table 6 compares the results of rc and rc + la with the baselines for gcns with residual connections . rc reduces recall , but does not improve performance overall . with residual connections , the results are similar across all gcns , with the exception of dcgcn3 which is better than both rc and la . moreover , the difference between rc and + rc decreases with the growth of the residual connections as well .
consistent with the observations by vaswani et al . ( 2017 ) , we observe that the coreference signal is localized on specific objects and that these objects are in the deep layers of the network ( e . g . , dcgcn , ps graphs , etc . ) and that the number of instances per label is relatively low but consistent with the importance of the clustering quality . our results show that the presence of these objects in the network helps the model to improve its performance and interpret the relevant features .
the results of an ablation study on the dev set of amr15 . 1 are shown in table 8 . the results show that removing the dense connections in the i - th and iii - th blocks reduces the overall density of connections , as expected . however , it does not significantly improve the performance .
the results of an ablation study of the modules used in the graph encoder and the lstm decoder are shown in table 9 . the results show that , under the current coverage mechanism , the global node and the hierarchical clustering of the nodes results in better coverage with a drop of more than 10 % in performance when using the " - global node " and " - linear combination " labels . with " - direction aggregation " we get a 0 . 9 % performance drop and a gain of 10 % overall with the - global node = linearcombination label .
table 7 shows the performance of our initialization strategies on various probing tasks . our framework outperforms all the stateof - the - art methods except glorot , which obtains a performance improvement of 2 . 8 points over the previous state of the art framework . the difference is most prevalent in the subtasks requiring initialization , subjnum and topconst , where our framework performs better than all the other methods apart from the fact that it relies on word embeddings pre - trained on a single entity . our framework establishes a new state - of - art on all three high - level functions , and on all subtasks except concatenation .
the results are shown in table 3 . we observe that the h - cmow variant outperforms the original cbow / 400 and h - cbow in the extraction of most relation types . it achieves state - of - the - art results , outperforming both the original and the variant with a gap of 10 . 6 points from the last published results . the performance gap is modest , but significant with respect to concatenation . subjnum and topconst are the most productive relation types , both for concatenated and abstracted objects , and their average length is close to the average of the two previous methods .
the results are shown in table 2 . our results show that the cbow / 784 variant outperforms all the base methods except cmp . hybrid the difference is most prevalent in sub - subj , where cbow performs best , followed by cmow . sub - subj and muli - domain embeddings perform best , but when we add in the effect of cross - domain training , the results are slightly worse . we find that combining sub - jurisdiction and entity clustering improves results , but does not improve results overall , we observe that cbow has the best overall performance , outperforming both cmp and cmow ,
table 3 shows the performance of our models on the four downstream tasks as well as the improvements in overall performance over the best previous approaches . hybrid mode outperforms both cbow and cmp . with respect to sts13 and sts16 , the results show that when cbow is trained and tested on unsupervised datasets , it achieves gains of more than 20 % over the previous state of the art method . the same tendency is observed for sts14 and 15 , with a slight drop of performance compared to hybrid . as shown in the second group of table 3 , when trained only on supervised datasets , the effectiveness of our method decreases significantly compared to that of hybrid .
table 8 shows the performance of our system with respect to initialization and supervised downstream tasks . our system outperforms glorot and trec by a noticeable margin . it achieves state - of - the - art results on all three supervised and unsupervised downstream tasks , improving upon the previous state of the art work by 4 . 6 points in each case .
table 6 shows the performance of our method compared to the previous state - of - the - art approaches on the four downstream tasks . our cbow - r model significantly outperforms the previous methods in terms of all metrics except for the accuracy on sts13 and sts16 . it achieves a final accuracy of 43 . 2 % , which indicates that it is well - equipped to perform this task in the low - supervision settings .
consistent with the observations by vaswani et al . ( 2017 ) , we observe that the hierarchical clustering method cbow achieves the best performance with a gap of 10 . 8 points from the previous state - of - the - art method , cmow - r . on the other hand , our approach obtains a significant performance improvement over traditional methods like subjnum , topconst and coordinv , improving upon the performance of both these methods by 3 points . we observe that , when trained and tested on the same dataset , the dependency trees of both methods overlap , which indicates that the training set size and the type of dependency trees used by the method are different .
the results are shown in table 3 . the first group shows that the cbow - r method outperforms all the other methods except sst - b and sst5 on all metrics except mpqa . it achieves state - of - the - art results , outperforming all the mod the performance on sub - committees is very similar , with the exception of sst2 , which is sub - par at best . when trained and tested on the same dataset , the results are markedly better for cmow - c , showing that it has better recall and accuracy . in addition , the accuracy remains the same across all metrics , confirming the effectiveness of the clustering feature .
the results are shown in table 1 . supervised learning underperforms all supervised and unsupervised learning methods except for the case of τmil - nd , which shows significant performance improvement under supervised learning . in fact , the improvement over supervised learning is almost entirely due to the superior performance of supervised learning under automatic metrics . name matching and entity clustering give a significant performance boost , however , when trained and tested on all loc and misc datasets , the results remain the same , indicating that supervised learning methods are strictly superior in terms of feature extraction . we observe that the use of multi - factor attention leads to better feature extraction performance , as expected , the performance gap between supervised learning and automatic metrics is narrower with respect to all three datasets , with respect to epm feature extraction , automatic metrics results are slightly worse than supervised learning but still superior than any supervised learning method . mil - nd shows a considerable performance drop compared to the previous methods .
uncertain in low - supervision settings . results in table 2 show that when trained and tested on the test set with only one error level in the eq . , both the name matching and f1 scores are relatively high ( 95 % confidence intervals ) with respect to both training and testing set . however , when using both supervised and unsupervised learning , the results are much lower . with the use of supervised learning , we get a performance improvement of 3 . 5 % on the f1 score compared to the previous state of the art model . supervised learning also gives a significant performance gain . it is clear from table 2 that once the training set is optimised , the model can further improve its performance with further improvements in accuracy .
table 6 shows the results for both approaches . the results are broken down in terms of entailment . our model obtains the best results with an absolute improvement of 3 . 86 points over the previous state - of - the - art model on both datasets .
table 3 compares the performance of our proposed approach with previous state - of - the - art models on the three datasets . the results are broken down in terms of performance on bleu , meteor and g2s datasets . our proposed approach outperforms all the previous work on two of the four datasets by a significant margin . on the other hand , it performs slightly better than the previous state of the art on the ldc2015e86 and ldc2017t10 datasets , while outperforming the previous best results on the third dataset by a margin of 2 . 3 % . we observe a drop in performance between the two baselines when using our approach with different training data , as shown in fig . 3 . the results reconfirm that the reliance on pre - trained word embeddings , especially on lex - lex , does not help the model perform well in the low - supervision settings . when using only plain averaged word2vec , we see a drop of more than 2 % on average compared to previous work . we suspect that this is due to the high overlap between training data and the training data size . our proposed model outperforms both the previous approaches . it achieves a significant performance improvement over both the baselines with a gap of 2 - 3 . 5 % over the last published results .
table 3 shows the results for models trained with additional gigaword data on the ldc2015e86 test set . our g2s - ggnn model improves upon the previous state - of - the - art model by 3 . 60 points in bleu score .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model outperforms the previous state - of - the - art models in terms of both performance on bilstm and bleu dataset with a gap of 3 . 6m terms from the last published results ( get ) to the current state of the art model ( geb ) . the difference is less pronounced for geb , but still significant : we get a performance drop of 2 . 3m terms on the bbleu dataset from the previous generation of models , which indicates that there are still some performance differences to be addressed .
the results are shown in table 1 . we observe that g2s - ggnn model outperforms all the alternatives with a noticeable margin of 3 . 51 % on the three types of graph diameter and sentence length .
table 8 shows the comparison of the fraction of elements in the output that are not present in the input that are missing in the generated sentence when gold is used as the reference sentence for the test set of ldc2017t10 . the token lemmas are used in the comparison . we find that gold significantly outperforms the s2s model in terms of both accuracy andiss percentage , confirming the value of redundancy removal . the g2s - gat model shows lower accuracy , but higher precision , compared to the original model . it also exhibits a significant drop in the miss percentage compared to gggnn ,
table 4 shows the sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the pos feature significantly improves over the sem feature , showing that it is comparable in difficulty to human judgement . it achieves 96 % accuracy on average compared to the previous state of the art on a similar corpus with 200k sentences .
table 2 compares the pos and sem tagging accuracy with the baselines and an upper bound . our word embeddings outperform the best previous approaches using unsupervised word embedding using a baselines - based encoder - decoder . word2tag also achieves a significant improvement in pos accuracy over the previous approaches , confirming the value of redundancy removal . it closely matches the performance of mft with only 0 . 41 % absolute difference in the two approaches .
table 4 presents the system ' s performance on the pos and sem tagging accuracy metrics . in general terms , we observe that the system performs well on both datasets with different features in play , with the exception of the pos tagging accuracy where it performs slightly worse than the previous state of the art . on the pos dataset , our system outperforms all the state - of - the - art methods except for the case of fr where it obtains the best performance .
table 5 shows pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . our proposed hgn outperforms the previous state - of - the - art model on all metrics except for pos , which shows significant performance improvement with the addition of res and bi features .
table 8 shows the attacker ’ s performance on different datasets that we trained on . results are on a training set 10 % held - out . the average attacker score is 14 . 2 % higher than the corresponding adversary on all the datasets except for pan16 . as shown in the table , gender and race features contribute significantly less than age and sentiment , which explains the lower performance of gender - based features . overall , the attacker performs much better than the adversary in all three datasets .
accuracies when training directly towards a single task . results in table 1 show that pan16 significantly outperforms pan16 with respect to accuracy and gender bias in all three aspects . the gender bias is most prevalent in the relation to race and age , followed by sentiment , with a gap of 2 . 5 points in pan16 performance compared to pan16 . overall , the gender bias results in significantly better performance .
table 2 shows the results for the balanced and unbalanced data splits . the results show that the presence of gender - neutral features benefits the model , improving upon the performance of pan16 by 3 . 8 points in the task prediction accuracy and roc time . gender - neutral feature - based features help the model to improve further , as it eliminates the effect of gender bias . overall , the performance gain is modest but significant , and we note that it is encouraging to continue researching gender bias in the future .
the performance of our system on these test datasets is shown in table 3 . it can be seen that the gender - neutral features help the system to improve its performance in the task prediction phase . the difference between the attacker score and the corresponding adversary ’ s accuracy is minimal , however we see significant difference in leakage due to different classifiers being used in the training set . overall , the system performs well , outperforming all supervised and unsupervised baselines except pan16 .
the results presented in table 6 show that when the protected attribute is encrypted , the rnn encoder performs better than the guarded encoder . the difference between the accuracy of the protected and unencrypted encoders is less pronounced for the leaky embeddings , but still indicates significant performance drop .
our results in table 1 show that the training set size and the number of parameters used for parameter sharing are the most important factors in model performance . the training data size and type of parameter sharing used in the previous work are presented in table 1 . we find that the lstm model performs best , with a performance gap of 3 . 3 points with the previous state - of - the - art model , while the work performed by yang et al . ( 2018 ) shows a significant performance drop . lrn , on the other hand , performs much better than previous work , achieving a final accuracy of 73 . 57 % , which shows the advantage of finetuning during training . we also find that our approach exceeds traditional neural models like atr and gru by a significant margin . our approach outperforms both the previous works by a noticeable margin .
table 2 compares the performance of our approach with previous work on the lstm , atr and gru datasets . the results are presented in table 2 , where we report the results of the best performances on both datasets with different combinations of training and test set time . the results show that , when using the correct combination of features , the training time and the correct completion time , the gru model can significantly outperform both the previous state - of - the - art models and the atr model . as the results show , once redundancy removal is applied to the training data , the performance reach the best , with an absolute improvement of 2 . 5 points over the previous work .
table 3 presents the results of experiments performed on the amapolar , yahoo time and yelppolar datasets in the settings described in zhang et al . ( 2015 ) . the results are presented in bold . we observe that the training set size and the number of parameters used for each training epoch are the most important factors in model performance . the performance gap between the original and the current state - of - the - art models is minimal , with the exception of atr , which obtains a significant improvement over the previous state of the art on both datasets . table 3 compares the performance of previous work on the yelp and ama datasets with the results from zhang et al . ( 2015 ) in terms of training epochs . as can be seen , the difference in performance between the two sets is much smaller when using atr and lstm , which results in a significant performance drop . also , when using yelp time and distance metrics , the results are slightly worse than the previous work , indicating that there are not enough data to properly train the model and that training on the dataset is time consuming .
table 3 shows the case - insensitive tokenized bleu score of our model on the wmt14 english - german translation task . the model obtains a significant improvement over the previous state - of - the - art methods by more than 4 % on average . it closely matches the performance of gnmt with only 0 . 2k training steps on tesla p100 . as hard core neural network , lrn outperforms all the other methods except atr and sru in terms of training time . its average training time is slightly longer than gnmt ' s but comparable on newstest2014 dataset , which is larger . although the difference in training time between lrn and olrn is less pronounced , it is significant enough to result in a substantial improvement in decoder performance .
table 4 shows the exact match / f1 - score on the squad dataset of wang et al . ( 2017 ) . it can be seen that the combination of elmo and rnet improves the model ' s performance , however , it still performs substantially worse than the previous state - of - the - art model , lstm . we conjecture that it is due to the high number of parameters required to initialize the model , i . e . the high quality of the parameter number of base . we also observe that atr and lrn have comparable performance with previous work ( wang et al . , 2017 ) , although lrn has more parameters , it obtains a better match rate and f1 score , it is harder to solve cases where there are multiple entities in the same training set , we observe that the presence of elmo helps the lrn model to improve its performance . it closely matches the performance of the previous model ,
table 6 shows the f1 score of our model ( lstm * ) on the conll - 2003 english ner task . the model obtains a significant improvement over the previous state - of - the - art model by more than 3 points . it closely matches the performance of lrn and atr with only 1 point difference . although lrn has more training data , it outperforms atr and sru in terms of number of parameters , it achieves the best performance with 245k parameter number , which indicates the high quality of its training set .
table 7 shows the performance of our model on the snli and ptb tests with the base - and - ln setting compared to the previous state - of - the - art model with both sets of training data . lrn shows 4 − 8 % higher performance on snli task compared to elrn with the same base setting . with the same number of iterations , glrn shows 6 − 4 % increase in performance . it is clear from table 7 that the combination setting helps the lrn model to further improve its performance .
table 1 compares the performance of word analogy task learning on the system and the mtr using the current set of features . we benchmark against the following features : oracle retrieval ( r - 2 ) , mtr ( mtr - 2 ) and human recurlink . the results are presented in table 1 . with respect to oracle , we observe that the current state - of - the - art system is more than 4 . 5x better than the previous state of the art on all metrics with two tasks . on the other hand , the performance gap between oracle and human is much narrower with respect to mtr features , indicating that human is better at selecting the correct word analogies and its context . sentence quality is relatively high with both systems using oracle and human - trained word analogy .
table 4 presents the results of human evaluation . our system outperforms all the automatic systems except candela ( h & w hua and wang , 2018 ) in terms of overall quality . it achieves the best result with 44 . 2 % overall score on the grammatical and appropriateness evaluation . the highest standard deviation among all is 1 . 8 % , which indicates that our approach exceeds traditional approaches by a significant margin . retrieval , on the other hand , is only comparable with human performance , achieving a lower standard deviation of 2 . 6 % overall . our system obtains the highest average score on content richness , showing that it is comparable in syntactic and semantic analogy tasks with human .
the results are shown in table 1 . we observe that the training set performance on the ted talks dataset is relatively comparable to that on the europarl dataset , with the exception of the fact that the latter has seen more data we observe that training on ted talks data results in significantly better performance . when trained on the dsim dataset , our model outperforms both the previous state - of - the - art methods and ted talks datasets in terms of both feature extraction and recall .
the results are shown in table 1 . we observe that the training set performance on the ted talks dataset is relatively comparable to that on the europarl dataset , with the exception of the fact that the latter has seen more training data we find that both datasets use the same feature set , so the performance gap between the two should not be significant . on the other hand , when using dsim and tf datasets , our results show that it is comparable , outperforms both the other two . the clustering quality of our data is very high , which indicates that our training set is well - equipped to handle the large variation in training data between datasets .
the results are shown in table 1 . we observe that the training set performance on the ted talks dataset is relatively comparable to that on the europarl dataset , with the exception of the fact that the latter has seen more data we observe that training on ted talks data results in significantly better performance . when trained on the dsim dataset , our model outperforms both the previous state - of - the - art methods and ted talks datasets in terms of feature extraction .
we first analyze the quality of our dataset with respect to depth cohesion using the greek word analogy test set . for europarl , we observe that the average depth of the roots is 11 . 05 , which means that there is a significant imbalance in the number of roots that can be extracted from a single row , as measured by the average number of tokens in the dataset . this result is statistically significant ( p < 0 . 01 ) compared to the previous state of the art , which shows that the original embedding quality of the dataset is high . our results show that , when using max - depth and averagedepth , both datasets are comparable in terms of quality , with the difference being less pronounced for europarlar .
we first analyze the quality of our dataset with respect to depth cohesion using the distractor and europarl features . the results are presented in table 1 . we observe that , overall , our dataset performs better than the previous state - of - the - art on all metrics with two tasks , namely , averagedepth and totalroots . according to the table , the averagedepth metric used in our dataset is 9 . 43 % , which means that it has the highest correlation with depth . on the other hand , the number of roots per europarl is 1 . 29 , which shows the low quality of the original dataset . our proposed approach results in a significant drop in performance compared to previous methods .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . the results are shown in table 1 . the enhanced version of our lf model outperforms the original model with a significant margin . it achieves 73 . 42 % ndcg % compared to the performance of 62 . 88 % with the original lf model .
the results of ablative studies on different models on the visdial v1 . 0 validation set are shown in table 2 . the results show that applying p2 improves the performance for all the models except for the one that uses the history shortcut . p2 indicates the most effective one ( i . e . , the one using the best hidden dictionary learning ) and thereby results in a significant drop in performance compared to applying p1 . however , the difference is less pronounced for the coatt model , showing that once p2 is applied , it is comparable to using p1 without .
table 5 compares the performance of our hmd - f1 model with previous approaches on hard and soft alignments . the results show that , when using bert and wmd - bigram , both recall and recall functions are comparable , but the performance on de - en and fi - en alignments is significantly worse than that on lv - en , indicating that there is a need to design more complicated features and to refine the recall function for future work . the results also show that when using ruse , the performance reach the best when using only plain hmd , not using the dependency trees or any derivational features .
the results of bertscore - f1 on the direct assessment and sent - mover tasks are shown in table 1 . the results show that bert score is significantly higher than other baselines on all metrics except ruse ( * ) and meteor + + , indicating that the training data used in the bert scores are significantly more accurate . also , when using smd + w2v as a dependency distance metric , the results show a significant drop in performance compared to the baselines we observe that the dependency distance method effectively bridges the gap between the performance of baseline and the sentiment - based methods using the same set of features , improving upon the results of ruse by 4 . 3 points in the direct assessment task .
the results of bertscore - f1 on the training data are shown in table 1 . it can be observed that the quality of the baselines is relatively high compared to other approaches which rely on word embeddings other than meteor , with an absolute improvement of 2 . 3 points over the bleu - 1 baseline . the difference is most prevalent in relation to sentiment analysis , sent - mover performs best among all three baselines , qualitative and quantitative metrics , in particular , we see significant performance improvement on the qualitative metric for smd ,
word - mover performs similarly to the word - mover , with the exception of the fact that it relies on word embeddings with elmo instead of elmo , in that it requires more training data and accuracy . the semantic features derived from word2vec outperform the others in terms of recall , m1 and m2 scores , sentence quality is relatively high as well as recall , we observe that the clustering quality of the semantic features obtained by word2vec is very high compared to other methods ,
the results are shown in table 6 . the results of m1 and m2 show that when only using the shen - 1 - trained models , the performance reach the best , with an absolute improvement of 2 . 3 points over the previous state of the art model . moreover , the improvement is much larger when using all the features from m0 and m1 together , which shows the advantage of finetuning word embeddings during training .
table 3 presents the results on the semantic and transfer quality metrics . our results show that yelp significantly outperforms the previous state - of - the - art models in both transfer quality and semantic preservation metrics . semantic preservation results are notably better than those of google translate ( hochreiter and schmidhuber , 2016 ) in both datasets , in particular , the transfer quality improvement is much larger than that of founta et al . ( 2017 ) which shows that the semantic features extracted by yelp are much more useful for semantic preservation . we observe that , in fact , semantic preservation is beneficial for both datasets with different transfer quality scores in mind , as it helps to improve the recall quality and the recall rate of the final product . finally , we observe that the drop in transfer quality between yelp and google is much smaller than that between semantic and transfer quality .
table 5 shows the results of human sentence - level validation on the three datasets for validation of the metrics . the results show that our approach verifies the accuracy of our summaries with 94 % on average , and matches the human ratings of semantic preservation and fluency .
the results are shown in table 6 . the first group shows that when only using hern - 1 , shen - 2d and para - para - trained models , the performance reach the best , with an absolute improvement of 2 . 3 points over the previous state of the art model . adding the dependency trees of para - lex and cyc gives a significant performance boost , with m6 reaching 3 points higher than m3 and m7 achieving 4 points higher .
results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . the results in table 6 show that our approach achieves higher acc than previous work on similar sentiment transfer using the same level of training data , and with the same number of tokens , because our training data contains more tokens . however , the difference between automatic and manual transfer is less pronounced for yelp , we find that automatic transfer results are slightly worse than simple transfer , but still superior to simple - transfer . we use the best model , yang2018 , supervised and unsupervised , and it achieves the highest acc level .
in table 2 , we report the percent of reparandum tokens that were correctly predicted as disfluent when we included repetition tokens and nested disfluencies . reparandum length is the average of the number of tokens in a tuple divided into repetition and disfluency contexts , and the average number of repetition tokens divided into nested and non - nested contexts . as the results show , when only using repetition tokens , the difficulty of prediction is reduced , but not eliminated .
the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) is shown in table 3 . it can be observed that both the length and the number of tokens in each category belong to each category , making it easier for our model to predict the correct distribution of tokens .
the results are shown in table 1 . we observe that the text - rich innovations model performs best when trained and tested on the single - domain dataset , followed by the late - breaking innovations model . as the results show , when training and testing on both datasets , the impact of the innovations on the dev mean and the average number of iterations is less pronounced in the early stages , but this does not seem to impact the performance negatively for the late stages . it is clear from the results that incorporating all the innovations helps the model to improve its performance . in fact , it helps it to improve by 3 . 8 points in the dev metric over the previous state of the art model .
the performance comparison of our model with the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model obtains substantial gains in accuracy over the previous state of - the - art methods . it closely matches the performance of cnn - based embeddings with only 0 . 43 % absolute difference in accuracy between the two sets , and outperforms both rnn - based and self - attention neural networks . it achieves a new world - leading accuracy of 83 . 43 % , which implies that our model can easily distinguish between the contributions of different classifiers .
table 2 shows the performance of all the methods that we consider for the document dating problems on the apw and nyt datasets . our unified model significantly outperforms all previous methods , showing that the quality of our approach is significantly better than the previous state - of - the - art methods .
table 3 compares the performance of our method with and without word attention on the word attention task for neuraldater . our approach results show that both word attention and graph attention are comparable in terms of accuracy , however the accuracy obtained with graph attention is higher . the accuracy of our approach is 62 . 6 % , which shows the effectiveness of both word and graph attention .
the results are shown in table 1 . the first group shows that the training data structure and the number of iterations used for each stage are the most important factors in model performance . the dmcnn and cnn models both perform well on both training and test data , but do not exceed the performance of the best previous state - of - the - art models . jrnn , on the other hand , obtains the best performance with 75 % on both test data and all iterations , outperforming the previous state of the art on both datasets . when trained and tested on a single dataset , the jmee framework outperforms all the other methods except cnn , in terms of both training data and test set performance . it closely matches the success of embedding + t , in that it eliminates repetition and allows more data to be used in the derivation of the final argument .
table 1 presents the results on event identification , event classification and event recall . our proposed method significantly outperforms dfgn and f1 using the current state - of - the - art features on both datasets with a gap of 10 . 5 % on event and role identification . on the other hand , when using cross - event event features , our proposed method results in a significant performance improvement , with an absolute improvement of 2 . 7 % on argument identification and 1 . 9 % on role identification compared to dfgn .
consistent with the results of vaswani et al . ( 2017 ) , we observe that the best performance on the test set comes from the " english - only " variant , which uses only the word " lexical " . however , when using both english - only and spanish - only word embeddings , results are slightly worse than the original cs - only model . the results of " spanish - only - lm " are slightly better than " cs - only + vocab - lm " . fine - tuning gives better results than applying the shuffled - lm feature , but still performs substantially worse than cs - last - lm . we observe that fine - tuning gives a significant performance improvement over the original approach , which relies on lexical features borrowed from other languages .
results on the dev set and on the test set using only subsets of the code - switched data are shown in table 4 . the results show that fine - tuning reduces recall and increases performance on the training set , both when using 50 % train dev and 50 % test dev data , compared to using full train dev data . cs - only training with only 50 % training dev data results in a significant drop in performance compared to applying the best performing finetuned approach .
the results in table 5 show that fine - tuning gives a significant performance improvement over the monolingual approach by increasing the precision on the dev and test sets , and improving the recall on the test set . moreover , the accuracy increases with the number of gold sentences in the gold sentence set , which shows the diminishing returns from mixing source and target word embeddings . fine - tuned - disc also improves performance , since it eliminates the effect of repetition .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . our approach results in a significant improvement in precision and recall compared to the baseline method using the type - combined gaze features . the improvement is statistically significant ( p < 0 . 01 ) compared to using the naive baseline method , which shows that the ability to combine gaze features induces a significant drop in recall and precision .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features for the conll - 2003 dataset . our approach shows a significant drop in performance compared to the baseline method using only pure recall features . we find that using type combined gaze features improves the recall and precision , and consequently , improves the f1 score by 1 . 3 points over the baseline model .
results on the test set of belinkov2014exploring ’ s ppa are shown in table 1 . the results show that glove - extended embeddings give a significant performance improvement over the original approach , and outperforms ontolstm - pp in terms of training , test set and error reduction . hpcd also achieves a significant improvement over traditional approaches like wordnet , verbnet and syntactic - sg , it closely matches the performance of the original founta et al . ( 2015 ) paper with only 0 . 7 % absolute difference .
results presented in table 2 show that our hpcd system outperforms the previous state - of - the - art ontolstm - pp model with features derived from various pp attachment predictors and oracle attachments . the results also show that the dependency extraction procedure given the correct pp prediction helps the system to improve accuracy and recall . it is clear from table 2 that the use of an oracle attachment helps improve the recall and accuracy of the system .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . we find that the ppa accuracy drop significantly as a result , which shows that the importance of context sensitivity is lessened with the removal of sense prors .
table 2 shows the results for en - de , en - fr and mscoco17 using the subtitle data and domain tuning for image caption translation . the results are slightly better than the results with marian amun ( maurice amun et al . , 2017 ) using the combination of subtitle and domain - tuned domain features , but still substantially worse than marian ( mikolov et al . 2018 ) in terms of bleu % . the improvement is much larger when using the ensemble - of - 3 approach , showing that domain tuning helps the model to improve its performance in the subtitle extraction task .
we find that domain - tuned h + ms - coco outperforms the plain hoco model with a margin of 3 . 8 points over the strong baselines . the results are broken down in terms of labels , with a and t being the most important groups . adding all the labels benefits the model , improving performance by 3 . 6 points in the en - de and mscoco datasets , while lowering performance in the subs1m dataset .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . the results show that adding automatic captions improves the general performance for all models except for those using marian amun ( cf . table 4 ) . the difference is most prevalent in the multi30k dataset , where automatic captioning gives a significant performance improvement . adding only the best 5 captions gives a 2 . 8 % improvement over the performance of using en - de captions .
the results in table 5 show that the use of en - de and dec - gate gateways improves the bleu % score by 1 . 86 points over the en - fr and mscoco gateways , respectively . however , the improvement is less pronounced when using multi30k + ms - coco + subs3mlm , which results in a drop of more than 2 points in the bled % score . with the dec - gated encoder , we get a 0 . 9 % improvement and a 2 . 4 % drop in the f - score . the results illustrate the diminishing returns from using pre - trained word embeddings . using the multi - layer approach by itself does not improve performance , but when using both en - and - dec - gate gates , it improves it by 2 points .
the results are shown in table 3 . we observe that the multi - lingual approach gives a significant performance improvement over the simple text - only approach , we find that the ensemble - of - 3 approach , when combined with visual features , improves the results for both datasets . however , the greatest performance increase is obtained by using ms - coco features , it is clear from the results that incorporating visual features benefits the model , subsequently , we find that it is better to rely on word embeddings alone than on word features .
table 3 shows the results for english and german on the test set of hotpotqa in the distractor and fullwiki setting , respectively . the results are broken down in terms of performance on extractive and abstractive keyphrases , with en - fr - ht achieving the best performance with a f1 score of 1 . 0172 and an f2 score of 0 . 5986 . transforming the word " ht " into " rnn " improves performance by about it achieves , but still performs substantially worse than enfr - trans - ff .
the number of parallel sentences in the train , test and development splits for the language pairs we used is shown in table 1 . our en – fr language pair contains 1 , 472 , 203 sentences , which is 723 , 487 words shorter than en – es .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the average number of words per language pair is 113 , 132 , which means that our models can easily distinguish between the original utterances in both languages .
table 5 shows the automatic evaluation scores for the rev systems . the en - fr - rnn - rev and en - es - smt - rev systems achieve relatively high performance , both in terms of bleu and ter ( differences are statistically significant with t - test , p < 0 . 01 ) . however , the performance gap between the two is narrower than that between rev and tfrev systems , when using ter as a metric , the gap is narrower , but still significant , at which point we maintain performance at the level of the state of the art .
table 2 shows the performance of our approach compared to previous approaches on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled reaper is the one supervised with rsaimage . our approach obtains the best recall @ 10 and median rank , which shows that our approach can significantly improve the interpretability by increasing recall without sacrificing performance .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the audiovisual supervised model . the average recall @ 10 rank of both rsaimage and audio2vec - u is significantly higher than that of vgs , confirming the effectiveness of finetuning word embeddings during training . in addition , the mean mfcc score of our approach is slightly higher than the previous state - of - the - art approach .
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . the first example shows that rnn turns in a screenplay that is slightly easier to hate than the original , because the edges are sharper and the shape is more compact . it also shows that cnn is more accurate , since it turns on a on a smoother , more compact screenplay . we report further examples in the appendix . as the table shows , the use of rnn improves the interpretability by increasing the average true positive rate of turn - in ,
table 2 shows the percentage of occurrences that have increased , decreased or stayed the same since fine - tuning the original sentence in sst - 2 . it can be seen that the number of occurrences in question has increased as a result of the improvements in the quality of the training data , but remains the same as before . the rnp model shows a significant drop in performance compared to cnn ,
sentiment score changes in sst - 2 . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the flipped labels result in a significant drop in sentiment score as compared to flipping negative labels .
table 1 presents the results of the second study . results show that our approach outperforms the approaches by a significant margin . our approach establishes a new state - of - the - art in the low - resource settings , outperforming traditional approaches like pubmed , sift and sst - 2 .
