table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as shown in the table , both the size of the training and the number of instances for each iteration is important for the model to perform well , as the smaller size of recur and iter allows more instances to be trained in parallel , thereby further improving the performance .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization .
table 2 shows the performance of our model with different representation configurations for each iteration . the max pooling strategy consistently performs better in all model variations . the largest gains are seen for ud v1 . 3 and conll08 , which both use the sigmoid and softplus representations . also , when using the dropout prob . in 5 - fold , the model performs better than the model with the default values , indicating that softplus is a better complement to softplus .
table 1 shows the effect of using the shortest dependency path on each relation type . it is clear from table 1 that macro - averaged models have a significant advantage over the models without sdp [ gillick et al . , 2017 ] in terms of f1 score .
consistent with the observations by vaswani et al . ( 2017 ) , we observe that the transition from y - 1 to y - 3 has a significant impact on model performance , both on r - f1 and f1 scores ( see table 4 ) . as can be seen , the difference between the two is most prevalent in the lower bound on f1 score , which indicates that the model performs significantly worse when trained and tested on the unsupervised setting .
the results of paragraph prediction accuracy are presented in table 1 . we show that our model mst - parser achieves 100 % accuracy on average , and 50 % on average on sentence prediction accuracy .
table 4 shows the overall c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level . the difference in performance between the two indicates that the essay parser has higher performance on the training set , indicating that it is better at parsing word embeddings .
table 1 shows that the original and the cleaned tgen models perform comparably to each other when the training data is added and removed . the results show that , compared to the original model , tgen + is more accurate and therefore requires less data to train , while tgen − is less accurate and requires more data to clean . as shown in the second group of results , the performance gap between original and clean is less pronounced for both systems .
table 1 shows the comparison of the original e2e data and the cleaned version . the difference in mr statistics between the two sets is minimal , however we see significant difference in ser as measured by our slot matching script , see section 3 . the difference is most prevalent in the training set , which shows that the training data is much more difficult to reproduce than the original one .
table 3 presents the results of training and testing on the original and the original configurations of tgen + models . as can be seen , the results show that the original model is significantly better than the original one when trained and tested on the large scale dataset of nist , meteor , rouge - l and sist . moreover , the difference between the bleu and nist scores is less pronounced for both sets , indicating that training on large datasets induces the model to rely less on superficial cues .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . as can be seen , the majority of errors in our system ( 71 % ) are caused by errors caused by adding or removing correct values ( 23 % ) , while only 5 % of the instances ( 5 % ) are misclassified as missing ( 14 % ) .
table 1 shows the performance of our model on the hidden test set of ps graphlstm and snrg . our model obtains the best generalization performance , outperforming all the base models except for pbmt ( pourdamghani et al . , 2016 ) .
table 2 shows the performance of our model on amr17 . our model achieves 24 . 5 bleu points , which is marginally better than the previous state - of - the - art results reported by beck et al . , 2018 .
table 3 presents the results for english - german and czech , compared to english - czech . the results show that , let alone a reduction in performance , our model outperforms the previous state - of - the - art models in both languages . we show that it is comparable to the performance of birnn + gcn ( bastings et al . , 2017 ) and cnn + cnn , while it is inferior to seq2seqb in german , and in czech .
table 5 shows that the number of layers inside our model has the most significant effect , i . e . , it decreases the performance in the low - supervision settings . the model performs significantly worse when there are more layers inside , indicating that more information is important for the model to perform well .
table 6 shows that the rcn with residual connections outperforms the gcn without residual connections . gcn with rc and la denote connections with strong baselines and strong correlations ( i . e . , > 90 % ) , while those without show lower correlation ( > 0 . 5 % ) . moreover , the difference between rc and rc + la decreases as well as the gap between the baselines with respect to gcn performance .
the performance of our model compared to previous models on the unsupervised test set is presented in table 4 . we observe that , let alone a reduction in performance , our model outperforms all the other models except for the one that does not use word embeddings , in terms of d - score .
table 8 shows the ablation study results for the density of connections on the dev set of amr15 . the model that obtains the best performance is the dcgcn4 model . it obtains 25 . 8 % f1 score .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . we used the language modeling approach described in section 4 . 2 of the previous paper ( hochreiter and schmidhuber , 2008 ) and the coverage mechanism described in table 9 . the results show that , under all three modeling approaches , our model obtains the best performance .
table 7 shows the performance of our initialization strategies on various probing tasks . our paper establishes a new state - of - the - art on all three high - level metrics , and on all subtasks except coordinv . it significantly boosts the generalization ability of our model , and its score improves over glorot ' s by 3 . 8 points in every metric .
table 3 presents the results of our final model on the hidden test set of somo . we show that our h - cmow model outperforms the strong baselines on every metric by a noticeable margin . it achieves state - of - the - art results , surpassing the strong base case of cbow / 400 by 3 . 6 points . on the small - scale test set , it achieves 6 . 6 % improvement on average compared to the previous model .
the results are shown in table 1 . we confirm what klinger et al . ( 2017 ) report : hybrid model outperforms cbow and cmow , the results show that the effect of cbow / 784 on parameter selection is less pronounced than expected . it appears that cbow has learned to rely less on superficial cues and more on hard core facts , and that its model performs better on datasets with fewer training examples .
table 3 shows the model performance on the four downstream tasks as well as the overall improvement in cbow and cmow over the strong baselines for each of the four scenarios . as can be seen , the cbow model has achieved a considerable performance improvement over the weak baselines ( i . e . , a gain of 26 . 5 % on average compared to hybrid ) in unsupervised tasks , and a further gain of 19 . 6 % compared to the strong baseline ( a gain of 25 . 8 % ) . as shown in the second row , the difference between cbow ( which is less sensitive to noisy input and noisy output ) and cmp . responsive is less pronounced than in the first row , but still represents a significant performance gain . hybrid also shows a large performance gain , constituting 10 . 2 % of the model ' s overall performance .
table 8 shows the performance of our system ' s initialization strategies on supervised downstream tasks . our system outperforms glorot and trec by a large margin . it achieves 87 . 6 % and 71 . 2 % overall improvement over the previous state of the art model on all metrics .
table 6 shows the performance of our method for different training objectives on the unsupervised downstream tasks . our cbow - r model outperforms the other methods with a large margin . it achieves gains on all the three tiers , and even achieves competitive or better results than the best performing cmow model .
the results are shown in table 1 . we show that our proposed method outperforms the strong baselines on every metric by a noticeable margin . our proposed method obtains the best generalization performance , outperforming the strong base case on three of the four metrics . it closely matches the performance of cbow - r with only 0 . 9 % absolute difference .
we show that the cbow - r classification scheme outperforms all the base methods except sst2 and trec , indicating that it has better generalization ability . it achieves state - of - the - art results on all metrics , and outperforms the models trained only on sst datasets .
table 1 shows the e - and per scores of all systems trained and tested on the hidden test set of loc and misc . our model outperforms all supervised and unsupervised systems except for the case of λmil - nd , which shows considerable performance improvement over supervised learning . supervised learning models generally perform better than supervised learning models in terms of e - values , as shown in the second group of results .
table 2 shows the performance of our model in the test set under two settings . the first set shows that our model ( mil - 1 ) obtains the best performance with an f1 score of 43 . 57 % , while the second set shows a slight improvement of 2 . 57 % . the results show that both settings give good performance in terms of e - and f1 scores .
table 6 shows that the model that interacts with g2s - gat is more stable and therefore requires less data entailment . the performance gap between the models is narrower than that between the two baseline models . however , the difference in ref and gen is much larger , confirming the importance of parameter sharing .
table 3 presents the results of our final model on the hidden test set of ldc2015e86 and ldc2017t10 . our model outperforms the previous state - of - the - art models on both datasets by a noticeable margin . we see that g2s - gat is significantly better than s2s on both sets , and outperforms both published and unpublished work on every metric by a significant margin . on the ldc dataset , it achieves a final score of 30 . 57 / 71 . 28 and 29 . 63 / 59 . 53 on the meteor metric , respectively , with an absolute improvement of 2 . 42 / 0 . 55 and 6 . 32 / 4 . 55 points over the previous best state of the art model .
table 3 shows the model performance when trained with additional gigaword data on the ldc2015e86 test set . g2s - ggnn outperforms the best previous models with a gap of 10 . 60 % on the bleu metric .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model outperforms all the base models except meteor by a large margin .
the model with the best performance was g2s - ggnn , which increased sentence length by 3 . 51 % and average number of words per sentence , and accuracy by 1 . 2 % .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence when s2s is trained and tested on the test set of ldc2017t10 . the token lemmas are used in the comparison . as shown in the table , the model with the best performance is g2s - gat , which shows that it is more than able to pick out the most important components and synthesize them into sentences .
table 4 shows the pos and sem accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . as can be seen , the pos features significantly improve over the sem feature , showing that the model can effectively distinguish between the semantic representations of the target languages .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . word2tag encoder - decoder has the best performance . it can be seen that it has the upper bound on most tags indicating that it is well - equipped to handle the high frequency of tags .
table 4 presents the system ' s performance on the four types of tagging accuracy metrics . we show that for all metrics , our model has 4 . 4 % higher accuracy on average compared to the previous state of the art model .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . as can be seen , the first layer shows much higher accuracy than the second and third layers . the size and type of redundancy vary as well , with the bi layer showing much worse performance than the others .
table 8 shows the attacker ’ s performance on different datasets . as shown in the table , the diversity of the protected attributes seem to have little effect on the attacker ' s performance , indicating that all the features he chose to include had a significant impact on his performance . gender and race features had the highest impact , however , showing that he was able to pick out the most interesting ones .
accuracies when training directly towards a single task . as shown in table 1 , gender - neutral pronouns have the best performance , followed by age and classifiers . the classifiers trained on pan16 have the worst performance .
table 2 shows the distribution of the data that has been leaked due to different classifiers ' balanced and unbalanced data splits . gender and racial features have the least and the other two have the most significant differences in leakage . as shown in the table , gender and race have the highest percentage of leaks , indicating that these features are important for prediction performance .
the performance of our model on these test datasets is shown in table 3 . the first group shows that it can distinguish between the attacks and the corresponding adversary ' s accuracy . as the results show , the gender - based and classifiers - based features have the worse performance , which indicates that they are harder for the model to pick out .
table 6 shows the ablation results for different encoders for the protected attribute . the rnn encoder shows lower performance than the guarded encoder , indicating that the leaky attribute is harder to encode than the protected one . as can be seen , the difference between the accuracies of the protected and unencoders is minimal , however significant .
we show the results of our final model on the hidden test set of ptb and wt2 in table 3 . the results show that our model outperforms the previous state - of - the - art models on every metric by a significant margin . on the smaller scale , it achieves gains of 3 . 36 / 0 . 59 and 6 . 63 / 4 . 59 points over the previous best state of the art model on both sets .
table 3 presents the results of our final model on the hidden test set of lstm in the distractor and fullwiki setting , respectively . the results show that our model outperforms both the previous state - of - the - art models on every metric by a significant margin . the difference is most prevalent in the acc metric , which shows that the model is more than 4x faster in training and also exhibits a significant drop in time between iterations .
table 3 presents the results of our final model on the yelppolar and amapolar datasets . the results show that our model significantly outperforms the previous state - of - the - art models on both datasets in terms of both err and time . specifically , our model obtains an err / yahoo time improvement of 1 . 57 / 2 . 57 and 3 . 63 / 4 . 55 points over the previous best state of the art model by a significant margin .
table 3 shows the case - insensitive tokenized bleu score of our model on the wmt14 english - german translation task . as shown in the table , the model learned to reason over more than 20 , 000 sentences in 1 . 2k training steps on tesla p100 . although the training time is relatively short , it takes a considerable amount of time to decode each sentence , as shown by the average time taken to train and decode .
table 4 shows the exact match / f1 - score on squad dataset . the model with the most parameters obtains the best performance . it closely matches the performance of wang et al . ( 2017 ) in terms of parameter number and f1 score . although the model has more parameters , it gets better performance than the other models with lower parameter number . as shown in the table , the number of parameters that make up the model is the most important factor in determining the model ' s performance .
table 6 shows the f1 score of our model ( lstm ) on the conll - 2003 english ner task . the model obtains a significant improvement over the previous state - of - the - art model by more than 3 points . although the model has the smallest number of parameters , it obtains the best performance with respect to ner .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . with the base setting , our model obtains 85 . 72 % accuracy and 169 . 81 % accuracy on the snli and ptb tasks , respectively .
table 1 presents the system and word analogy scores for english and spanish . retrieving the system statistics and word analogies is straightforward ; however , for oracle , it is more difficult than for humans . we show that for english , the difference between human and machine learning is less pronounced , but still significant : for example , for tweets containing more than one word , human outperforms both trained and unsupervised systems . for spanish , there is a 1 . 8 / 3 . 4 gap between the average number of correct answers and the number of incorrect answers , which indicates that human is better at selecting the correct answer and its context .
table 4 presents the results of human evaluation . our system outperforms all the automatic systems except candela ( 30 . 2 % ) and h & w hua and wang ( 2018 ) . the highest standard deviation among all is 1 . 8 points , which indicates that our model can easily distinguish between human evaluation and that of human . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) and large margins of error ( k 500 ) .
table 6 shows that our model outperforms the best previous approaches across the five domains on every metric by a significant margin . for example , it achieves p - value of 1 . 5 times the value of " tables " and " food " on average compared to the previous state - of - the - art model on all metrics except for " food " .
table 1 shows that our model outperforms all the stateof - the - art methods on every metric by a significant margin . for example , it achieves p < 0 . 01 and r > 0 . 36 on the three metrics indicating that it has the best generalization ability . on the four datasets , it gets close to or better than the best performing model on all but one of them .
table 6 shows that our model outperforms the best previous approaches across the five domains on every metric by a significant margin . for example , we see that it achieves p < 0 . 01 on average and r > 0 . 5 on three of the four metrics indicating that it has the best generalization ability . on the other hand , it gets a lower percentage of p than the other two .
we show the performance of our model on the five metrics . the first group shows that it has the best overall performance . according to the table , total terms and roots are the most important metrics , followed by dsim , df and hclust .
the system performs well on both datasets with different feature sets . for example , it achieves the best averagedepth @ k9 with a f1 score of 9 . 43 and the second - best averageroots score of 79 . 29 .
table 1 shows the performance ( ndcg % ) and the enhanced version of our model ( lf ) on the validation set of visdial v1 . 0 . 7 . the enhanced version exhibits significant performance improvement over the baseline model , showing that it can further improve the generalization ability of its model without sacrificing performance in the hidden dictionary learning task .
the model performing the best on the visdial v1 . 0 validation set . as shown in table 2 , applying p2 indicates that it is the most effective one ( i . e . , it learns the most accurate hidden dictionary learning ) and thereby has the highest ndcg % .
table 5 shows the performance of our hmd - f1 model compared to the previous state - of - the - art models on hard and soft alignments . we see that our model performs better on both metrics , confirming the value of parameter sharing and recall .
the results are shown in table 1 . the first group shows that our method outperforms the baselines on all metrics except direct assessment . on average , it achieves 4 . 3 % higher bertscore f1 score compared to the previous state - of - the - art method on both cues .
the experimental results on the hidden test set of hotpotqa are shown in table 1 . the first set shows that all the metrics we considered had good correlation with the human judgement , i . e . , bleu - 1 had 0 . 87 % correlation with human judgement and bertscore - f1 was 0 . 85 % . the second set of results show that the clustering quality of the metrics was relatively high , indicating that the selection of the right metrics had a high impact on the model performance . when we added the w2v metric to the training set , it further improved the results .
word - mover achieved the best results with a bertscore - recall of 0 . 939 on the training set and a meteor - meteor - f1 score of 1 . 083 . the accuracy of these metrics was slightly higher than that of spice , indicating that the semantic information injected into the model by the additional cost term had a significant impact .
we show that our model outperforms the models using only one type of language classifier , namely , that of shen - 1 , when trained and tested on the hidden test set of simnet , and when trained only on the simnet dataset . the results are broken down in terms of accuracy and gm . while the improvement of accuracy is slim , it is encouraging to continue researching into the topic as it helps improve generalization .
table 1 shows the transfer quality and semantic preservation results . our model obtains the best transfer quality . it closely matches the strong baselines of google translate ( g & l ) and wikipedia ( hochreiter and schmidhuber , 2008 ) with only 0 . 3 % absolute difference . semantic preservation results are slightly better than transfer quality , but still significantly worse than linguistic preservation . we observe that the drop in transfer quality between yelp and google is much larger than that between semantic and linguistic features , indicating that the semantic features captured by the model are quite specialized .
table 5 presents the results of human sentence - level validation for each metric . we use spearman ’ s [ italic ] ρ b / w sim and human ratings of semantic preservation and syntactic fluency as our metrics for validation . the results show that both the machine and human judgments that match match the metrics well , confirming the effectiveness of our model . however , the difference between human and machine ratings of linguistic fluency is much lower than that for acc .
we show that our model outperforms the models using only one type of classification scheme , namely , that of para - lang , with an absolute improvement of 2 . 3 points over the strong lemma - trained model ( hochreiter and schmidhuber 2008 ) .
table 6 : results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( yang2018 unsupervised and yang2018unsupervised ) achieve higher accuracy than prior work at similar levels of acc , but untransferred sentences achieve the highest bleu . multi - decoder also achieves higher accuracy , but only when it is used with the correct classifiers in use . we use the language embedding technique of kungfu - 1 to encode the sentiment and the classifiers of delete / retrieve and style embeddings , and to apply the best classifier , lm , to the extracted sentences . finally , we use lexical features such as drop - lstm and classifier - based weight for the generated summaries .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . reparandum length and overall number of repetition tokens exclude repetition tokens , indicating that all the tokens in our model are of high quality . as shown in the table , rephrase tokens have the least significant effect on prediction performance , however , they do have a significant impact on the overall performance .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either the reparlabelled and unreparulated forms , or in neither . as shown in the table , both the length and type of tokens belong to each category , and the fraction of tokens in each category is statistically significant ( p < 0 . 01 ) for both reparandamentals and function - function .
the results of α = 0 . 2 show that when only using text alone , the model performs best , and when using both raw and innovations , it gets the best performance . in particular , the large differences in α between single and multiple models show that it is harder for the model to distinguish between the true response and negative responses .
the model ' s performance comparison with the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model shows marked performance improvement over the state of the art on sentence prediction accuracy . it achieves 28 . 43 % higher accuracy on average compared to the previous best model ( 28 . 63 % on micro f1 ) and 35 . 53 % accuracy compared to rnn - based sentence prediction .
table 2 shows the performance of all the methods that we trained on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . it achieves 68 . 2 % accuracy , which means that it has better generalization ability .
table 3 shows the performance of our method compared to ac - gcn with and without graph attention for this task . our model obtains 62 . 6 % accuracy and 65 . 2 % accuracy on average compared to the original neuraldater model ( 63 . 9 % ) in terms of word attention and 63 . 9 % accuracy without .
the model performance is presented in table 1 . embedding + t achieves the best results , surpassing all the base models except cnn , and closely matching the performance of dmcnn . jmee also achieves competitive or better results than all the other base models ,
table 1 presents the results for event identification and event classification . our model establishes a new state - of - the - art in the production setting , significantly improving upon the previous state of the art in both classification and event identification . it significantly boosts the p - value for argument identification and provides a competitive edge over dfgn and f1 .
consistent with the observations by vaswani et al . ( 2017 ) , all models show lower error on the test set when trained and tested only on english , spanish , french , dutch , russian and turkish , respectively . the difference is most prevalent for spanish , where the model trained only on the original utterances ( cs - only - lm ) shows severe under - fitting , and only slightly outperforms all the other models that use concatenated utterances . note , however , that fine - tuned models do not have the advantage of using lexical features , which explains some of the lower accuracy .
results on the dev set and on the test set using only subsets of the code - switched data are shown in table 4 . fine - tuned models perform better than cs - only models on both sets , showing that the training size and the quality of the training data are the most important factors in model performance . fine - tuning also boosts the generalization ability of the model , since it eliminates some of the noise associated with training large datasets .
table 5 shows the performance of our model on the test set and the dev set when fine - tuned for each gold sentence in the set . the model performs comparably to fine - tuned - lm , showing that it has comparable performance on both sets , i . e . accuracy is the same whether the gold sentence is in the correct language or not .
table 7 shows the precision ( p , r and f1 - score ) and recall ( rn ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . the type combined model shows significant performance improvement over the baseline model ,
table 5 shows the precision ( p , r and f1 - score ) and recall ( rn ) for using type - aggregated gaze features on the conll - 2003 dataset as our training set . the improvement from baseline to type combined model is statistically significant ( t - test , p < 0 . 05 ) and rn > 0 . 1 , which indicates that the model can further improve its performance with the addition of additional gaze features .
the results on the belinkov2014exploring ’ s test set are shown in table 1 . glove - extended refers to the synset embeddings obtained by running autoextend rothe and schütze ( 2015 ) on wordnet 3 . 1 , and it uses syntactic skipgram instead of wordnet , verbnet and syntactic - sg type . hpcd ( full ) and ontolstm - pp ( partial ) use the type - based initialization scheme , and the word - error type is derived from the original paper by faruqui et al . ( 2015 ) . the difference in accuracy between the two approaches is minimal , however we see significant difference in test set performance due to large variation in training set size .
table 2 shows the uas results for rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the hpcd model outperforms rbg and ontolstm - pp , indicating that it has better generalization ability . although the accuracy of the hpd model is slightly higher than the original lstm model , it is still comparable to the performance of rbg with the same set of features .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the model achieves the best performance with a ppa acc . of 89 . 7 % and 89 . 5 % accuracy , respectively .
in table 2 , we report the bleu % and roca % scores for adding subtitle data and domain tuning for image caption translation ( table 2 ) . the model using marian amun ' s domain - tuned subtitle data achieves the best results , confirming the value of domain tuning .
we show that domain - tuned h + ms - coco outperforms the plain model , confirming the importance of the domain - aware classifiers . the results are particularly striking for en - de , where the model with the best performance is obtained with the label " domain " . as the results show , when only using labels , the model performs best in en - fr and in - de .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . adding only the best 5 captions improves the general performance , both en - de and en - fr ( for both flickr16 and mscoco17 ) .
table 5 shows the bleu % and w - score of our model using transformer , multi30k + ms - coco + subs3mlm , detectron mask surface , and detectronmask surface , the enc - gate and dec - gate strategies achieve better results than en - de and mscoco17 , indicating that the semantic information encoder is more useful for visual information integration . further , our model achieves a significant improvement over the strong lemma - based encoder ( 68 . 38 % vs . 62 . 86 % ) . the difference is less pronounced with en - fr , but still indicates significant performance improvement .
we show the results for en - de and en - fr for flickr16 and mscoco17 when we switch from the simple model to the more complicated multi - lingual model of ms - coco . the results are shown in table 2 . the first group shows that , when only using the visual features , the ensemble - of - 3 model can do the job better than the monolingual model alone . while the second group shows the results of incorporating all the features into the model , it is still inferior to the model using only the text - only features .
table 1 shows that en - fr - ht and en - es - ht achieve better results on the hidden test set of yule ’ s i and ii , respectively , compared to the models using mtld and ttr . as can be seen , the smaller size of the difference between the trained and unsupervised corpus indicates that the former relies less on the training data , while the latter relies on it more .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . it is clear from table 1 that our model can handle more than 1 , 500 , 000 sentences in parallel , which indicates that the translation quality of our model is high .
the vocabularies for the english , french and spanish data used for our models are shown in table 2 . as the table indicates , the training vocabulary is relatively small , making it easier for the models to learn the required vocabulary .
table 5 shows the automatic evaluation scores for the rev systems . as can be seen , the en - fr - rnn - rev and en - es - trans - rev systems receive relatively high bleu and ter scores ( see table 5 ) , indicating that these systems are well - equipped to perform this task . however , it should be noted that the ter score is only computed on systems that are already trained on rev data , so the improvement is less pronounced for these systems .
table 2 shows the performance of our model against the best performing rsaimage model on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com . it achieves the highest recall @ 10 and median rank , indicating that it is well - equipped to perform this task . segmatch , on the other hand , has the worst performance .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com . it achieves the highest recall @ 10 and median rank , which indicates that it is well - equipped to handle the difficult task of predicting human - generated captions . the other models that perform similarly to rsaimage do not have significant performance improvement above vgs in terms of recall .
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . it can be seen that all the classifiers turn in sentences that are sufficiently clever to make the task of selecting the correct nouns easier . however , for rnn , the difference is most prevalent in the edges , where it turns in a sentence that is more difficult to pick out than the original . as the table shows , this is mostly due to the size of the rnn dataset , which contains many examples .
table 2 shows that the number of occurrences has increased , decreased or stayed the same through fine - tuning of the original sentence in sst - 2 . as shown in the table , the importance of the word " good " has not changed , indicating that the model has not been impacted by the effects of human judgement . however , the percentage of instances in the correct sentence has increased as well as the overall percentage of occurrences for some particular parts of the speech ( see x4 ) .
sentiment score changes in sst - 2 . the numbers indicate that the sentiment score increases as a function of the flipped labels being flipped from positive to negative sentiment . the last two rows correspond to the case where negative labels are flipped to positive and vice versa .
table 1 presents the results of the second study . our model outperforms all the base the results show that it can distinguish between good and bad responses , confirming that it is indeed possible to distinguish between the two states of the art .
