table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . table 2 compares the performance of our recursive and iterative approaches with the best performance on the training and inference datasets , respectively . our recursive framework performs better than the iterative one on both datasets , with a performance gain of 3 . 2 % on inference and 4 . 6 % on training compared to the tensorflow model .
table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization .
table 2 : hyper parameter optimization results for each model with different representation . the max pooling strategy consistently performs better in all model variations . conll08 and ud v1 . 3 outperform softplus and sb on all metrics except for the dropout probability . sb outperforms softplus in all but one case ( table 2 ) . as shown in fig . 2 , when the model is trained with the best representation , the model performs better than softplus on all the metrics except dropout probabilities .
table 1 : effect of using the shortest dependency path on each relation type . the results are shown in table 1 . macro - averaged models outperform model - feature models in terms of f1 score . as shown in the table , the macro - adaptive model achieves the best f1 ( in 5 - fold test set ) with a fraction of the training data , while the modelled entity achieves the second - best f1 .
table 3 shows the performance of our model on the f1 and r - f1 test sets . the results are summarized in table 3 . our model achieves the best performance on all test sets with a f1 score of 100 % and a r - score of 50 % . we observe that our model outperforms the previous state - of - the - art models on all metrics except for f1 .
table 1 shows the performance of mst - parser with respect to paragraph prediction accuracy and sentence prediction accuracy . the results are presented in table 1 . the results show that the model outperforms all the state - of - the - art parsers except for mate , which achieves the best performance with a marginal improvement of 0 . 3 % . the performance gap between the models is small , but it is significant .
table 4 shows the c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level and paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . however , the difference between paragraph and essay level is less pronounced than that between sentence level and sentence level .
table 1 shows the performance of the models trained and tested on the original and cleaned datasets . the results are shown in bold . as expected , when the model is cleaned , it performs better than the original model on both test sets . however , the difference between the two models is less pronounced when the models are trained on the same dataset , as shown in table 1 .
table 1 : data statistics comparison for the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . the difference between the original and the cleaned version is less than 0 . 5pt / 2pt , but still significant ( p < 0 . 001 ) compared to the original .
table 3 shows the performance of our system compared to the state - of - the - art tgen + model on the training and test set . the results are shown in bold . we observe that our model outperforms both the original and the original tgen model by a significant margin . the difference between the two models is less pronounced on the test set , but it is still significant . as shown in table 3 , the difference between original and original is less than that between the original model and the tgen − model .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) and the number of instances in the training set ( including missing instances ) as well as the percentage of instances with errors in the correct values as shown in table 4 . as shown in fig . 4 , adding and removing instances significantly reduces the error numbers , but does not improve the performance .
table 1 presents the performance of our model with respect to multi - domain clustering . our model outperforms all the previous state - of - the - art models except for tree2str ( flanigan et al . , 2016 ) and pbmt ( pourdamghani and cohen , 2016 ) . it also outperforms the best single - domain baselines by a large margin .
table 2 : main results on amr17 . table 2 shows the model size in terms of parameters ; “ s ” and “ e ” denote single and ensemble models , respectively . table 2 also shows the bleu points obtained by our ensemble model compared to the previous state - of - the - art models . our model achieves the best performance with an absolute improvement of 3 . 5 bleus over the previous best performance of seq2seqb ( beck et al . , 2018 ) by a margin of 2 . 3 points .
table 1 presents the performance of the models with respect to english - german , czech , german , french , spanish , italian , dutch , russian and turkish . table 1 shows the average number of frames per second ( pp ) for each language compared to single and multi - class models . the models using birnn + gcn ( bastings et al . , 2017 ) outperform the baselines on average by a large margin . moreover , the performance gap between the single and the multiclass models is much smaller than that between the models using bow and cnn + . the performance gap is narrower between the two baselines for english and german , but larger than that for spanish and italian .
table 5 shows the effect of the number of layers inside the network on the performance of our model . we show that the size of the layers in the network has a significant effect on the model ' s performance , and that the effect is most pronounced for blocks with more than 6 layers . the effect is particularly pronounced for the second layer , which has the highest percentage of layers , and is the most important part of the model . this shows that layers with multiple layers have the most significant effect .
table 6 : comparisons with baselines . our model outperforms the baselines in terms of rc and rc + la by a large margin . the results are shown in table 6 . the model achieves the best performance with residual connections , and achieves the highest roc score with a residual connections of 2 . 5x . moreover , the model achieves higher roc and rc scores with more residual connections .
table 1 shows the performance of our model compared to the best previous state - of - the - art models . our model outperforms all the previous state of the art models except for the dcgcn model by a large margin . the difference between the two sets of models is statistically significant , with the difference between d and b being less than 0 . 5 % across all models .
table 8 : ablation study for density of connections on the dev set of amr15 . we show that removing the dense connections in the i - th and x - th blocks significantly decreases the number of connections , and that the dcgcn4 model obtains the best ablation performance . the results are shown in table 8 .
table 9 : ablation study for modules used in the graph encoder and the lstm decoder . the results are shown in table 9 . for the decoder modules , we show the ablation study results for both the global and the linear combinations of the decoding modules . the results show that the combination of the global nodes and linear combinations improves the performance of both decoders , but the results are less pronounced for the linear combination . we see that the asymmetric nature of the domain - aware decoding mechanism leads to better performance for both encoders .
table 7 : scores for initialization strategies on probing tasks . the glorot initialization strategies outperform all the baselines except somo and topconst by a large margin . as shown in table 7 , the glorot initialization strategy outperforms somo , topconst , and subjnum by a margin of 2 . 5 - 3 . 5x on most of the tasks .
table 3 shows the performance of h - cbow and h - cmow with respect to concatenation and multi - word embeddings . the results are summarized in table 3 . we observe that the hcbow model outperforms all the baselines except for somo and topconst by a large margin . it achieves the best performance with a minimum of 10 % improvement over the previous state - of - the - art model on both baselines .
table 1 shows the performance of all the models on the test set with respect to sub - domain embeddings . hybrid models outperform all the baselines except sst2 and sst5 , and outperform cmow / 784 by a large margin . the difference between subj and cr is less pronounced for hybrid models , but it is still significant . subj and cr are the only two baselines that outperform the cmow baseline by a significant margin .
table 3 : scores on unsupervised downstream tasks attained by our models . as shown in table 3 , the cbow and cmow models outperform the hybrid models on all the downstream tasks except for sts13 and sts16 . the difference between the performance of the two methods is small , but still significant . hybrid outperforms both the original cbow model and the cmow model on most of the tasks . in particular , the difference between hybrid and the original model is less pronounced on sts12 , but it is still significant on some of the more difficult tasks .
table 8 : scores for initialization strategies on supervised downstream tasks . glorot and trec outperform all the baselines except sst2 and sst5 on all three tasks , indicating that their initialization strategies are well - adapted to the task at hand .
table 6 : scores for different training objectives on the unsupervised downstream tasks . the cbow - r achieves the best performance on all the downstream tasks , with the exception of sts13 and sts15 , where the cmow achieves the highest performance .
table 3 shows the performance of our method compared to previous approaches . the results show that our method outperforms the previous state - of - the - art approaches on all metrics except for the length metric . the difference between our method and the previous best state of the art is less pronounced , but it is still significant . our model outperforms all the state - ofthe - art methods except the cmow - r model by a large margin . our model obtains the best performance across all metrics , outperforming all the baselines except the somo model .
table 1 shows the performance of the best approaches for sub - maximising the error rate of the model with respect to subj , cr , mrpc , trec , sst2 and sst5 . the best performing models are the cbow - c and cmow - r models , which outperform all the other approaches except for sick - r . the cbow model outperforms all the models except sickr and trec by a large margin . it achieves the best results with a marginal improvement over the previous state of the art model .
table 1 shows the performance of the best supervised and unsupervised learning systems on the loc and misc datasets . the results are shown in bold . supervised learning outperforms both supervised learning and supervised learning by a large margin . in particular , it achieves the best performance on loc / misc dataset with an absolute improvement of 3 . 38 % over the previous state - of - the - art model . moreover , it obtains the highest performance on the misc dataset , with a marginal improvement of 2 . 03 % over previous state of the art models .
table 2 : results on the test set under two settings . the results are shown in table 2 . name matching and supervised learning achieve the highest f1 scores , while τmil - nd achieves the lowest f1 score . supervised learning achieves the best overall performance . in table 2 , we show that the best performance is obtained when the model is trained on a training set with only one training set , and when the training set is combined with the training data from the second set . table 2 also shows that the model trained on the second test set has the best generalization performance .
table 6 shows that g2s - gat achieves the best results with an absolute improvement of 3 . 86 % over the previous state - of - the - art model on average compared to s2s . moreover , the model with the highest absolute improvement is gggnn , which achieves an absolute gain of 5 . 45 % over s2c on average . table 6 also shows that when using the gat - ggnn model , the performance gain is even larger .
table 3 shows the performance of our model compared to previous state - of - the - art models on the ldc datasets . the g2s - gat model outperforms all the baselines except for the s2s model by a large margin . it achieves the best performance on ldc2015e86 and ldc2017t10 datasets , and outperforms the models by a margin of 3 . 42 ± 0 . 53 % on both datasets .
table 3 : results on ldc2015e86 test set when models are trained with additional gigaword data . the g2s - ggnn model outperforms all the previous state - of - the - art models on the test set , with a margin of 2 . 23 % higher bleu compared to the original model .
table 4 : results of the ablation study on the ldc2017t10 development set . the results are shown in table 4 . bilstm achieves the best ablation performance with a total of 22 . 6 % improvement over the previous state - of - the - art model ( get ) on the development set , with a difference of 3 . 3 % on the bleu metric .
table 1 shows the performance of the model with respect to sentence length and average number of words per sentence . the g2s - gat model achieves the best results with an absolute improvement of 3 . 51 % over the previous state - of - the - art model , while the gggnn model achieves a marginal gain of 0 . 51 % . the model with the highest percentage of iterations is the one with the shortest sentence length , which shows that the model can handle shorter sentences better .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( added ) compared to those in the original input ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . g2s - ggnn outperforms the s2s model in terms of fraction of tokens in output and fraction of missing elements in input . table 8 also shows that the model with the best performance is gold , as shown in fig . 2 .
table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the results are shown in table 4 . the pos model outperforms the previous state - of - the - art model in terms of both semantic and syntactic accuracy . pos model performs better than the previous best model on the smaller corpus .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . word2tag significantly outperforms both unsupemb embeddings in terms of most frequent tags and the upper bound encoder - decoder .
table 1 shows the performance of our system on the three metrics for pos , seminar and predicate word embeddings . the results are summarized in tables 1 and 2 . we show that our system outperforms the previous state - of - the - art models on all metrics except for the pos and seminar metrics by a significant margin .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 . the uni layers outperform the res layers by a large margin . the res layers are more accurate than the bidirectional layers , while the bi layers are less accurate . finally , the pos accuracy increases with the number of layers , as shown in fig . 5 .
table 8 : attacker ’ s performance on different datasets . results are on a training set 10 % held - out . δ is the difference between the attacker score and the corresponding adversary ' s accuracy . for pan16 , the attacker achieves the best performance with a 10 % improvement in accuracy over the baseline .
table 1 : accuracies when training directly towards a single task . the results are shown in table 1 . the pan16 model outperforms all the baselines except for gender - neutral models , which show that it is better at selecting the correct word for the task .
table 2 : protected attribute leakage : balanced & unbalanced data splits . the results are shown in table 2 . our model outperforms the previous state - of - the - art models on all three datasets with a large margin . for example , the model with the best performance is pan16 , which achieves 87 . 5 % f1 score on the task acc metric and 86 % on the sentiment metric .
table 3 : performances on different datasets with an adversarial training . as shown in table 3 , the adversarial model outperforms the baselines on all three datasets except for pan16 . the difference between the attacker score and the corresponding adversary ’ s accuracy is less than 0 . 5 % on pan16 , which indicates that the model can learn to distinguish between human and non - human entities with relatively high accuracy .
table 6 : accuracies of the protected attribute with different encoders . the rnn encoder performs better than the guarded encoder on leaky and leaky embeddings . the difference in performance between the leaky ( leaky ) and the guarded ( guarded ) encoding is small , but still significant .
table 1 shows the performance of the baselines and finetuned wt2 models compared to baselines with and without finetune . the results are presented in table 1 . table 1 presents the results of the best models with finetune and dynamic modes on the wt2 dataset . lrn achieves the best results with a f1 score of 85 . 97 % and a f2 score of 84 . 97 % . the best results are obtained by using the best combination of finetuning and dynamic modes with the best baselines . this shows that the best model is the lstm model with the maximum number of parameters , and the best performance is obtained with the least amount of parameters . it can be seen that the lrn model outperforms all the other models with the exception of the gru model , which shows that it is more suitable for low - supervision settings . moreover , it outperforms the best baseline model by a large margin .
table 1 presents the results of our model with respect to baselines and training time . the results are presented in table 1 . table 1 shows that the lstm model outperforms all the baselines except the gru model with an absolute improvement of 3 . 5 % over the previous state - of - the - art model . as shown in the table , the difference in performance between gru and the other baselines is small , but still significant .
table 1 presents the results of zhang et al . ( 2015 ) on the three coreference metrics for amapolar , yahoo time , and yelppolar . the results are presented in table 1 . table 1 shows that the lstm model outperforms the previous state - of - the - art baselines on all three metrics . as shown in the table , the performance gap between this model and the previous best state of the art baselines is small , but it is still significant . it can be seen that the difference between the performance of amapolar and yahoo time is less than that of the best baselines , which indicates that the model performs better on shorter training times .
table 3 shows the case - insensitive tokenized bleu score on the wmt14 english - german translation task on tesla p100 . the gnmt model achieves the best performance with an absolute improvement of 3 . 67 % over the previous state - of - the - art model , while the lrn model achieves an absolute gain of 4 . 67 % . the difference between gnmt and lrn is less pronounced than that between olrn and sru , but the difference between atr and gru is much smaller . table 3 summarizes the performance of our model with respect to the translation task . our model obtains the best case - inflation score with a case - intensified tokenization score of 2 . 67 % , significantly higher than the previous best model .
table 4 : exact match / f1 - score on squad dataset . table 4 shows that the lstm model obtains the best performance with a f1 score of 7 . 41 / [ bold ] and rnet * achieves the best f1score of 71 . 14 / [ cad : 7 . 41 ] . the results show that the combination of elmo and lrn improves the performance of the model by 2 . 45 / 7 . 14 % over the previous state - of - the - art model . further improving performance by 3 . 45 % over previous state of the art models , the lrn model achieves f1 scores of 6 . 41 and 7 . 14 % , respectively . the improvements are most striking when we consider the number of parameter number of base . as shown in table 4 , lrn models perform better than previous models with the same number of parameters . moreover , the improvements are more pronounced when we include the parameter # params , which indicates that the model is able to learn new features more rapidly . finally , the performance drop of rnet is less pronounced than that of the other models , indicating that it is easier for the model to learn features with fewer parameters .
table 6 shows the f1 score of our model on conll - 2003 english ner task . the lstm model achieves the best performance with an absolute improvement of 3 . 56 % over the previous state - of - the - art model ( lample et al . , 2016 ) . as shown in table 6 , lrn outperforms all the other models with a large margin .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . lrn outperforms elrn and glrn on both snli and ptb tasks , indicating that the model is better suited to the task .
table 1 shows the performance of human and human - supervised systems with and without oracle retrieval . the results are presented in table 1 . human and human models outperform both sets of models with the exception of human , as shown in the table 1 . the performance gap between the two sets is small , but it is still significant . the difference between the human and the human model is less pronounced than that between the system and the oracle model , indicating that human models are better at learning new words .
table 4 shows the human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 6 % ( table 4 ) . table 4 : the average number of evaluations a system receives for overall quality . top - 1 / 2 shows the percentage of evaluations obtained by a system being ranked in the top 1 or 2 for the overall quality score . the best performance is achieved by candela ( h & w hua and wang , 2018 ) . the second best result by seq2seq is obtained by retrieval ( wang et al . , 2018 ) . table 4 also shows the average number evaluated by the human on the syntactic quality metric k 1000 and k 2000 . table 4 summarizes the performance of the automatic systems on these metrics . our system achieves the best performance on all metrics , with the exception of appr score , which is only obtained by the second best automatic system .
table 3 shows the performance of the models trained on the ted talks dataset compared to those trained on europarl . the results are shown in bold . the results show that the model trained on ted talks outperforms all the baselines except tf and tf by a significant margin . table 3 also shows that the performance gap between the two baselines is narrower than expected by a margin of 2 . 5 % on average , but still significant .
table 3 shows the performance of the models trained on top of the ted talks corpus compared to those trained on the baselines without . the results are shown in bold . the results show that when using the best baselines , our model outperforms all the other baselines except tf and docsub by a large margin . table 3 also shows that our model performs better than both the best baseline and the best cross - domain baselines .
table 3 shows the performance of the models trained on top of the ted talks corpus compared to those trained on the baselines without . the results are shown in bold . the results show that the model trained on ted talks outperforms both baselines with a large margin . table 3 also shows that the performance gap between ted talks and europarl is much smaller than expected by chance . in particular , the difference between en and r = 0 . 5295 and 0 . 6295 is much larger than expected , which indicates that the training data are more suitable for large datasets .
table 1 presents the results of our model on the number of roots , maxdepth , maxrels and maxdepth metrics . our model outperforms the previous state - of - the - art models on all metrics except for the maxdepth metric , which shows that our model has better cohesion with respect to the max and max - depth metrics .
table 1 presents the results of our model on the number of roots , maxdepth and maxdepth metrics . our model outperforms the previous state - of - the - art models on all metrics except for the maxdepth metric , which shows that our model has better cohesion with respect to the max and max - depth metrics . our model obtains the best generalization of the baselines on both datasets .
table 1 : performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . we show that the enhanced version of lf improves upon the baseline model by 3 . 42 % on average compared to the original model by using weighted softmax loss . moreover , the model performs better on the question type , answer score sampling , and hidden dictionary learning , as shown in table 1 . moreover , it achieves higher performance on the ranking loss metric as well , improving by 2 . 36 % compared to baseline .
table 2 shows the performance ( ndcg % ) of ablative studies on different models on the visdial v1 . 0 validation set . the model with p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 2 , while the model with only p1 indicates the least effective one . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . the models with p1 and p2 are significantly better than the models with only adding p2 .
table 5 : comparison on hard and soft alignments . hmd - f1 and hmd - recall outperform ruse and wmd - bigram on both soft and hard alignments , while wmd - unigram outperforms ruse on both sets . the results are shown in table 5 . we observe that the hmd pre - trained models outperform the hmd + recall models on both hard - aligned and soft - aligned alignments ( cf . table 5 ) .
table 1 shows the results of the baselines for the three baselines on the direct assessment and sent - mover tasks . the results are shown in bold . the baselines show that meteor + + and w2v outperform bertscore - f1 and ruse ( * ) by a significant margin . as shown in table 1 , the average number of frames per second for all baselines is 0 . 716 , with the exception of ruse , which shows a drop of 2 . 5 % over the previous best baseline .
table 1 shows the performance of bertscore - f1 and bleu - 2 with different baselines . the results are shown in bold . the baselines outperform the baselines by a significant margin . table 1 also shows that meteor and w2v significantly improve the performance for both sets of models . we observe that the bert score obtained by bert + w2v + meteor is significantly higher than the baseline on both datasets , which indicates that the training data are more suitable for the task at hand .
table 3 shows the performance of our model with different word - mover and recall metrics . word - mover is the most important metric , followed by recall , while bertscore - recall is the second most important . the results are shown in table 3 . we show that our model outperforms all the baselines except spice and meteor by a significant margin . in particular , the performance gap between wordmover and word2v is less than 0 . 1 % with respect to m1 and m2 .
table 1 shows the performance of the models with and without para - para . the results are shown in bold . we observe that the model with the best performance is the one with the most para - adaptive features , i . e . , the one that has the highest percentage of lexical similarity with the shortest distance between the input and output . further , we observe that when the model is combined with the lexical features of both shen - 1 and 2d , its performance improves significantly .
table 1 shows the performance of our model on the transfer quality metric and the semantic preservation metric on the yelp dataset . the results are shown in bold . we show that our model outperforms the previous state - of - the - art models on both transfer quality and semantic preservation metrics by a significant margin . table 1 also shows that the model with the highest transfer quality is the one with the best semantic preservation . finally , we show the results on the fluency metric as well . our model achieves the best results on both metrics .
table 5 : human sentence - level validation of metrics ; 100 examples for each dataset for validating acc ; 150 each for sim and pp ; see text for validation of gm . the results are shown in table 5 . for each dataset , we show the percentage of machine and human judgments that match the acc metric and the number of instances in which the human judgments match the accuracy metric . for yelp , we also show the human ratings of semantic preservation and fluency .
table 1 shows the performance of the models with and without para - para . the results are shown in bold . we observe that the model with the best performance is the one with the most para - adaptive features , i . e . , the one that has the highest percentage of lexical similarity scores . the models with the least amount of para are the ones that perform the worst .
table 6 : results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( right table ) outperform the best unsupervised models ( left table ) and the best supervised models ( yang2018 , yang2018unsupervised ) in terms of acc ∗ , indicating that the training data are of high quality . however , our best model achieves higher acc than prior work at similar levels of transfer , which indicates that training data with the correct classifiers can improve the accuracy of the model . we also show that using the best models with the appropriate classifiers improves the accuracy by a large margin .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent as well as the overall number of disfluencies for each type . reparandum length and number of repetition tokens are the most important factors in predicting disfluency . as shown in table 2 , the average number of tokens in a reparanda is significantly shorter than the average length of a repetition token , which indicates that the disfuncability of a disfuncuation is less severe than that of a non - disfuncuation .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either the reparland and repair contexts , or in neither . reparandum length and repair length are the most important factors in predicting disfluency , but the fraction of tokens belonging to each category is less important than that belonging to the other two . function - function tokens are less important , but still contribute significantly to the prediction . the average number of tokens in the repair context is less than the average length of reparandandum , but more than the length of the repandandum .
table 1 shows the performance of our model with different combinations of text and innovations . the results are shown in bold . early models outperform the models with innovations , while late models perform better than single models with both innovations and text alone .
table 2 : performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . word2vec embeddings outperform self - attention and rnn - based neural networks in terms of accuracy on the micro f1 test set . our model achieves the best performance with an f1 score of 83 . 43 % compared to the state of the art ( table 2 ) .
table 2 : accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . for example , the attentive neuraldater model outperforms the previous state - of - the - art models on both datasets by a large margin . the ac - gcn model shows that it can outperform all previous approaches except for burstysimdater , which is comparable to the best performance of the original neuraldater model . moreover , the joint model of maxent - joint outperforms both the original and the adaptive neuraldaters by a significant margin . this shows that the unified model is more suitable for the task at hand .
table 3 : accuracy ( % ) comparisons of component models with and without attention . this results show the effectiveness of both word attention and graph attention for this task . the ac - gcn model outperforms the oe - based neuraldater model by a large margin . the accuracy of neuraldater is higher than that of oe model , indicating that word attention can improve the performance of the model .
table 1 shows the performance of the different approaches on the 1 / 1 , 1 / n and 3 / 10 epochs . the model with the best performance is the jrnn model , followed by the dmcnn model and the cnn model . the models with the worst performance are the ones with the least training data and the most training data . for example , the jmee model outperforms all the other approaches except for the argument stage , which shows that it is better at generating sentences with more training data than the others .
table 1 presents the results of cross - event event prediction using the best models . the results are presented in table 1 . cross - event event prediction outperforms traditional event prediction by a large margin . table 1 shows the performance of event prediction with respect to both event detection and event classification . it can be seen that event prediction is beneficial for both types of event since it reduces false positives and increases false negatives . in addition , event prediction reduces false negatives as well as false positives for both events .
table 3 shows the performance of all models with different learning rates for english , spanish , french , german , italian , dutch , russian and turkish . the results are summarized in table 3 . we see that the models using the best learning rates are derived from the best multi - language models . the best performance is obtained by using only the best multilingual models , with the exception of spanish - only - lm , which uses only the single word embeddings . as shown in fig . 3 , the best performing models are the ones using english - only , french - only and german - only models , while the worst performing ones are those using the shuffled - lm and the fine - tuned - lm models .
table 4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the fine - tuned model outperforms the original model on both sets . the results are shown in table 4 . fine - tuned models outperform the original models with a large margin . cs - only models perform better on both datasets , but the difference between the two sets is less pronounced .
table 5 : accuracy on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) , fine - tuned ( fine - tuned - disc ) , and fixed - disc ( cs - only - disc ) . note that the fine - tuned model outperforms both the original model and the fixeddisc model on both sets , indicating that the training data are more suitable for the task at hand .
table 7 : precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . the type combined approach shows significant improvements in recall and precision compared to the baseline model . the f1 score of the type combined model also improves significantly compared to baseline .
table 5 : precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset ( * marks statistically significant improvement ) . the type combined approach shows significant improvements in recall and precision compared to the baseline model ( table 5 ) . the f1 score of type combined model also shows significant improvement over baseline model .
table 1 : results on belinkov2014exploring ’ s ppa test set . syntactic - sg and glove - extended embeddings outperform lstm - pp and ontolstm on wordnet , verbnet and wordnet 3 . 1 , respectively . the difference in performance between the two systems is small , but it shows that syntactic sg embedding improves the performance of wordnet and verbnet over the original model . hpcd ( from the original paper ) achieves the best performance on the test set , and it uses syntactic skipgram instead of syntactic wordnet . further , it achieves the highest percentage of tokens with syntactic prefixes , which indicates that the syntactic embedding quality of syntactic sg is high .
table 2 : results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 . hpcd and ontolstm - pp outperform the original lstm model with a large margin . as expected , the model outperforms the original model with respect to the ppa acc . metric .
table 3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . as expected , the effect of removing the context sensitivity decreases the ppa acc . score of the full model , but increases the precision of the attention metric .
table 2 : adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun . the ensemble - of - 3 model outperforms the multi30k model with a bleu % score of 66 . 7 % compared to 43 . 0 % with en - de model . adding subtitle and domain - tuned subtitle data improves the blu % score by 3 . 5 % . adding domain tuning improves the performance of the ensemble model by 4 . 6 % . the results are shown in table 2 . multi30k models outperform en - fr models with a 3 . 6 % improvement on average compared to the multi - domain baseline .
table 1 shows that domain - tuned h + ms - coco outperforms both en - de and en - fr models with a margin of 2 . 7 % and 3 . 3 % over the baseline model , respectively . moreover , the results are even better for subs1m models with domain - adaptive learning models , as shown in table 1 . the results of domain - aware learning models are particularly striking when combined with the strong domain - domain - tuning feature set . sub - sub - 1m model outperforms the baseline models with an absolute improvement of 3 . 5 % and 4 . 3 % , respectively , over the baselines . further , the improvements are even larger when using the lm + ms feature set , as seen in fig . 3 . finally , the performance gains are even bigger when using domain - specific learning models .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . multi - tasked models outperform en - de models with the best captions , while en - fr models with only the best 5 captions outperform multi - task models with all the captions . the results are shown in table 4 . adding automatic captions improves the performance of the models with marian amun . in particular , the multi30k model outperforms the en - fluent models with both the best and worst captions by a large margin .
table 5 : comparison of strategies for integrating visual information ( bleu % scores ) . all results using transformer , multi30k + ms - coco + subs3mlm , detectron mask surface , and transformer + transformer + moco are shown in table 5 . the encoder and decoder outperform the en - de and en - fr ensembles by a large margin . however , the decoder performs better than the encoder , as shown in fig . 3 . the decoder achieves the best bleu % score , and achieves the highest overall score of 69 . 38 % on the transformer dataset ( table 5 ) .
table 3 shows the performance of en - de and en - fr models compared to the models using ms - coco and multi - lingual features . the results are summarized in bold . sub - 3m models with multilingual features perform significantly better than models using only text - only features . in particular , the ensemble - of - 3 model performs better than subs3m and subs6m when combined with the visual features of all three models .
table 1 shows the performance of different approaches for yule ’ s i and ii subtasks . the results are shown in bold . as expected , en - fr - ht and en - es - ht achieve the best performance . however , the performance gap between these approaches is less pronounced for mtld , as shown in table 1 . in general , the models that rely on the back - propagation of the rnn - word embeddings achieve better performance than those that rely only on the front - pigment . this is evident from the difference in ttr and mtld scores between the two approaches . it can also be seen that the model that relies on the forward propagation of the rnn - word pairs achieves higher performance than the model using the back propagation .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the average number of sentences per split is 1 . 467 , 489 , 489 and 7 . 723 , respectively , for en – fr and en – es , respectively .
table 2 shows the training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 . our model outperforms the previous state - of - the - art models on all three languages except for spanish .
table 5 shows the automatic evaluation scores ( bleu and ter ) for the rev systems . the en - fr - rnn - rev and en - es - smt - rev systems outperform all the other systems on average . however , the performance gap between the two systems is narrower than expected by a margin of 0 . 5 bleu and 0 . 7 ter points .
table 2 shows the results on flickr . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled rsaimage is the model from flickr2016 . the results are shown in table 2 . the vgs model achieves the highest recall @ 10 and the highest mfcc of 0 . 8 % compared to rsaimage .
table 1 : results on synthetically spoken coco . the row labeled vgs is the visually supervised model from chrupala2017representations . com , while the row labeled rsaimage is the model trained on the generated rsaimage dataset . the results are shown in table 1 . rsaimage achieves the highest recall @ 10 % with a mean mfcc of 1 . 414 and a median rank of 0 . 0 . moreover , the vgs model achieves the best performance with a recall of 27 . 5 % and a chance of 3 . 9 % .
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . we report further examples in the appendix . for example , the cnn classifier turns in a screenplay that is at the edges and the edges are the most difficult to hate , while the rnn classifiers turns in one that is the easiest to hate . the dan classifiers turn in sentences that are both easy to hate and hard to hate ( i . e . , it ’ s so clever you want to hate it ) . the rnn model turns in sentences containing only the edges of the screenplay , but turns on sentences containing the edges as well as the edges .
table 2 : part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . the numbers indicate the changes in percentage points with respect to the original sentence . the last row indicates the overlap with the original sentences , and the second row shows the percentage of occurrences that have increased or decreased as a result of fine tuning . the rnn model outperforms the dan model by a large margin . the rnp model shows that it is able to fine - tune the syntactic representation of nouns , verbs , adjectives and adjectives to a higher degree than the models using dan and cnn .
table 3 : sentiment score changes in sst - 2 . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the positive sentiment score increases by 9 . 5 % in cnn and by 27 . 9 % in dan . the negative sentiment score decreases by 6 % .
table 1 presents the results of the experiments on pubmed and sst - 2 . the results are summarized in table 1 . the results show that sift outperforms pubmed by a large margin . the difference between pubmed ( 98 % ) and corr ( 99 % ) is statistically significant ( p < 0 . 001 ) on average .
