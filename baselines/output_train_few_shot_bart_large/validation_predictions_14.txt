table 2 shows the performance of our treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . table 2 also shows the throughput numbers for training and inference .
the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . considering the small difference in the number of instances in the balanced dataset , this small performance gap does not represent a significant performance gain . table 1 shows that the balanced dataset exhibits the best performance .
the max pooling strategy consistently performs better in all model variations . for example , ud v1 . 3 achieves the best performance with a f1 score of 75 . 57 ( t - test , p < 0 . 001 ) .
table 1 shows the effect of using the shortest dependency path on each relation type . our model obtains the best f1 score in 5 - fold test set without sdp and with sdp .
the results of r - f1 and f1 are shown in table 3 . the results are presented in bold . for both sets , we report the results of the best performing model with the best f1 score . the results show that the model performs well on all metrics with a minimum of 50 % error . on the other hand , for the f1 dataset , the results are slightly less clear .
the results of paragraph prediction accuracy are presented in table 1 . the results show that mst - parser achieves 100 % accuracy on average , with a gap of 2 . 69 % from the best previous state - of - the - art model .
the performance of the parser for the two indicated systems is shown in table 4 . it achieves the best performances with a c - f1 score of 100 % , which indicates that the parser performs at the grade level .
the results are shown in table 1 . the original and the cleaned tgen models outperform the original ones when trained and tested with the same set of features . when the original tgen + model is cleaned , it achieves the best results with a gap of 3 . 5 bleu points from the original .
table 1 compares the original e2e dataset with the cleaned version . the difference in mr statistics is less pronounced for the train dataset , but is still significant ( 17 . 5 % vs . 17 . 5 % ) for the test dataset . for the dev dataset , the difference is less significant ( 0 . 5 % ) .
the results are shown in table 1 . the results show that original and original tgen models are comparable in difficulty to state - of - the - art systems like sc - lstm , nist and meteor . however , the difference in performance between the original and the original model is less pronounced for tgen + than for original and tgen − . when trained and tested on the same dataset , the results are slightly better than those of original , but still significantly worse than the original .
results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) are shown in table 4 . the difference in the number of errors between the original and the cleaned set is less than 1 % , which shows the extent to which tgen can be improved with a minimal amount of effort .
for completeness , here we also compare our model with other state - of - the - art systems . the first set of results presented in table 1 shows that the ensemble model performs best , while the single - headed model performs slightly worse . the second set shows the performance gap between our model and other state of the art systems .
table 2 shows the performance of our model with respect to amr17 . our model achieves a bleu score of 59 . 6 , marginally outperforms the previous best state - of - the - art model , seq2seqb , by a margin of 2 . 5 bleus points .
table 1 compares the performance of different classifiers for english - german , czech and slovakian . the first set of results in table 1 shows that the single - classifier approach outperforms all the other methods except for birnn + gcn . the second set shows the performance gap between the single and multi - classifiers .
the effect of the number of layers inside each dc is shown in table 5 . the first group shows that more layers inside a network does indeed improve the performance . the second group shows the diminishing returns from mixing multiple layers .
comparisons with baselines are shown in table 6 . rcn with residual connections shows lower performance than rcn without residual connections . gcn with rc + la ( 6 ) and + rc + rc ( 9 ) shows higher performance than those without .
the results are shown in table 4 . the first group of results show that our proposed method outperforms the previous state - of - the - art methods in terms of generalization . the performance gap between dcgcn ( 1 ) and the best state - ofthe - art model ( 2 ) is modest but significant , with a gap of 2 . 5 % in coefficient of generalization ( cid : 27 ) and 6 % in f - score .
table 8 shows the ablation study results for the dev set of amr15 . the results show that removing the dense connections in the i - th block significantly decreases the density of connections .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . the results of " - global node " and " - linear combination " modes show lower performance than " - direction aggregation " and " coverage mechanism " .
table 7 shows the performance of our initialization strategies for different probing tasks . our system achieves state - of - the - art results . it achieves the best performance with a gap of 3 . 8 points from the previous state of the art .
the results of h - cmow and h - cbow are shown in table 1 . the results are summarized in bold . we observe that the concatenation of concatenated object - oriented and object - aware keyphrases achieves state - of - the - art results . it achieves the best generalization performance with a gap of 3 . 8 points from the previous state of the art model .
the results are shown in table 1 . the hybrid method outperforms both monolingual and multi - method approaches . it achieves state - of - the - art results with a cmp score of 43 . 6 % and a mrpc score of 77 . 8 % , respectively , with an absolute improvement of 2 . 2 % over the previous state of the art method .
table 3 shows the performance of our models on unsupervised downstream tasks . the cbow and cmow baselines outperform cmp . however , hybrid outperforms both methods with a margin of error of 2 . 5 % and 3 . 0 % respectively .
table 8 shows the performance of our system for initialization and supervised downstream tasks . our system achieves state - of - the - art results , outperforming glorot and trec by a margin of 3 . 6 points .
the performance of our method on the unsupervised downstream tasks is shown in table 6 . our cbow - r model outperforms the cmow model with a gap of 3 . 2 bleu from the previous state of the art .
for completeness , here we also compare our method with the best state - of - the - art wc - based ontonotes - trained models . the results are presented in table 1 . we observe that our method obtains the best results with a gap of 3 . 5 bshift points from the previous state - ofthe - art model ( gillick et al . , 2013b ) , while the gap is narrower than in previous work .
the results are shown in table 1 . the first group shows that the cbow - r method outperforms the cmow - c model with a gap of 10 . 2 points from the last published results . it achieves state - of - the - art results with a score of 90 . 6 % on subj and 86 . 9 % on mrpc .
the results are shown in table 1 . name matching and supervised learning achieve state - of - the - art results in terms of e - misc and loc , respectively . the performance gap between the two approaches is slim , with supervised learning achieving a marginal improvement of 0 . 03 bleu over the best previous state of the art model . however , the gap is much larger with respect to loc and misc , showing that supervised learning alone does not generalize well across all contexts . in particular , the performance gap with τmil - nd shows that the former relies on superficial cues and the latter relies on syntactic information alone .
results are shown in table 2 . name matching and named entity recognition achieve the best results with accuracies of 15 . 38 ± 1 . 03 and 29 . 03 % respectively . supervised learning achieves the best performance with a f1 score of 71 . 57 ± 0 . 59 and a rn of 3 . 42 . these results show that the adversarial supervision alone does not harm the model performance in terms of name matching .
the results in table 6 show that g2s - gat achieves state - of - the - art results with a gap of 10 . 86 points from the previous state of the art model ( 73 . 45 % ) on the ref and gen metrics ( 71 . 45 % ) . however , the gap is narrower than the gap between the state - ofthe - art models with respect to the gen metrics .
the performance of our model compared to previous stateof - the - art models on the ldc datasets is presented in table 3 . the results are summarized in bold . our g2s model achieves the best results with a bleu score of 29 . 28 / 30 . 03 and meteor score of 30 . 53 / 29 . 86 . these results show that our gat model is well - equipped to handle the task at hand .
results on the test set of ldc2015e86 are shown in table 3 . the g2s - ggnn model improves upon the performance of bleu with additional gigaword data . the performance gap between the original model and the best state - of - the - art model is narrower than expected by chance .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model ( geb + get ) obtains a performance improvement over the previous state - of - the - art model , bilstm , with a gap of 3 . 6m bleu points from the last published results ( gebhard et al . , 2016 ) to 6 . 7m in the current development set . the gap is narrower than the gap between the performance of " geb " + " get " .
the results are shown in table 1 . the first group shows that g2s - gat model has the best performance with a f1 score of 3 . 51 % and a graph diameter of 7 . 28 × 10 . 27 .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . the model with the best performance is s2s , with a f1 score of 50 . 06 .
table 4 shows the pos and sem tagging accuracy using different target languages trained on a smaller parallel corpus ( 200k sentences ) . the pos tagging accuracy is slightly better than sem , however it is still inferior to the original model .
table 2 compares the pos and sem tagging accuracy with baselines and an upper bound . the results show that the encoder encoder - decoder performs better than the word embeddings with a baseline of 87 . 95 % on pos and 87 . 55 % on sem .
for completeness , here we also include the pos tagging accuracy and sem tagging accuracy for english and german descriptions . the results are presented in table 4 . table 4 shows the performance of our system with respect to the three types of tags . our system achieves the best results with an accuracy of 91 . 9 % on average .
pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results in table 5 show that the first layer of our system exhibits the best performance . the second layer shows the performance of the res - based system with the best multi - layer performance . finally , the third layer exhibits the worst performance .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . the difference between the attacker score and the corresponding adversary score is significant ( p < 0 . 001 ) .
accuracies are shown in table 1 . the pan16 model outperforms the best state - of - the - art models with a gap of 3 . 8 % in accuracy .
table 2 shows the results for balanced and unbalanced task data splits . the results show that the gender and racial disparities are the most difficult for our model to handle , however , the age and classifiers seem to have relatively easier time filtering out the racial disparities .
performances on different datasets with an adversarial training set are shown in table 3 . the results show that the gender and age features seem to have little effect on the task performance , but the race and sentiment features have a noticeable impact . finally , the leakage metric shows that the presence of gender and race features do impact the performance , however it does not have a significant effect .
the results are shown in table 6 . the rnn encoder performs better than guarded encoder for the protected attribute . however , the gap between the guarded and unencrypted encoders is narrower .
the performance of these models on the test set is presented in table 1 . the results are summarized in the tables in yang et al . ( 2018 ) and ( 2018 ) . the performance gap between the base and the best - performing model is minimal , with the difference being less than 0 . 5 % on the ptb and wt2 test sets . however , the gap is much larger on the lstm test set , with a gap of 2 . 7 % on wt2 .
table 3 presents the results of our final model ( rocktäschel et al . , 2016 ) on the base acc and time metrics . the results are presented in table 3 . the results show that our model obtains the best performance with a gap of 0 . 5 bert time from the previous state - of - the - art work .
the err and time metrics reported in zhang et al . ( 2015 ) are summarized in table 1 . the results are presented in bold . table 1 presents the performance of the best performing model according to the three metrics . the average time taken to compute each parameter is reported in parentheses . the average number of # params for each time window is presented in tables 1 and 2 . for the yelppolar and amapolar datasets , we show the mean time weighted average of the max and average time weighted over the full time window . table 1 summarizes the results of the third study .
table 3 shows the case - insensitive tokenized bleu score of our model on the wmt14 english - german translation task . our model obtains a case - in - error score of 26 . 86 % on the training set and a corresponding score of 35 . 67 % in the decode set .
table 4 shows the exact match / f1 - score of our model on the squad dataset . our model obtains a match rate of 76 . 14 / 83 . 83 % with an f1 score of 7 . 41 / [ bold ] 79 . 83 % . the performance gap between our model and the previous state - of - the - art model is less pronounced with respect to elmo , but still shows significant performance gap .
table 6 shows the f1 score of our model ( lstm * model ) on the conll - 2003 english ner task . the model obtains a 90 . 56 f1 ( normalized by the number of parameter number ) score .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . with the base setting , our model obtains 85 . 56 % accuracy and 169 . 81 % chance to solve the perplexity task .
table 1 compares the performance of the system and the mtr with the best performing human reviewers . the results are presented in table 1 . oracle retrieval outperforms human reviewers in terms of system and sentence recognition . the performance gap between human and system is less pronounced with respect to sentence recognition , however it remains significant . retrieving the same word twice as often as human requires significantly less effort .
table 4 presents the results of human evaluation . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 8 % ( candela , 2018 ) . the average number of errors per evaluation is less than 1 . 0 . overall quality is high , with retrieval achieving 44 . 2 % overall quality and seq2seq achieving 25 . 6 % .
the results are shown in table 1 . table 1 shows the performance of our en model compared to the best previous state - of - the - art text - similarity models on the test set of ted talks and wikipedia . our model outperforms all the other state - ofthe - art models except for df .
the results are shown in table 1 . table 1 shows the performance of our en model compared to the best previous state - of - the - art systems on the ted talks dataset . our model outperforms all the state - ofthe - art models except for df .
the results are shown in table 1 . table 1 shows the performance of our en model compared to the best previous state - of - the - art systems . our model outperforms all the state of the art systems except docsub and slqs .
the performance of each metric is presented in table 1 . the average depth and maxdepth are the most important metrics for our dataset . according to the table , europarl has the best performance with an average depth of 11 . 05 / 11 . 46 and a maxdepth of 3 . 46 / 3 . 46 .
the performance of each metric is presented in table 1 . the average depth and maxdepth are the most important metrics for our dataset . europarl has the best performance with a maxdepth of 9 . 43 / 9 . 29 and a avgdepth of 2 . 29 / 2 . 29 .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . 1 is shown in table 1 . the enhanced version of lf achieves the best performance with a ndcg % of 73 . 42 % , which implies that it has the best generalization ability .
the performance ( ndcg % ) of these models on the validation set of visdial v1 . 0 is shown in table 2 . the best performing model is the hidden dictionary learning one ( i . e . , p2 ) , followed by the best performing baseline model . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut .
comparison on hard and soft alignments is shown in table 5 . the hmd - f1 model outperforms the other baselines on both alignments with a gap of 0 . 817 and 0 . 866 points , respectively , with the exception of ruse .
the results of baselines are shown in table 1 . the average score of the baselines for each setting is reported in bold . for example , baselines trained on meteor + + and ruse ( * ) achieve average scores of 0 . 864 and 0 . 866 respectively , while those trained on w2v + smd + w2v score 0 . 86 . sent - mover achieves the best average score , with a marginal improvement of 1 . 0 % over the baseline .
the results are shown in table 1 . the first set shows that bertscore - f1 achieves the best results with a f1 score of 0 . 176 on the " inf " and " qual " metrics . next , we see that the " sent - mover " and the " w2v " metrics achieve the highest f1 scores among all the baselines , with a gap of more than 2 points .
the performance of the word - mover method according to these baselines is shown in table 1 . word - mover achieves the best results with an f1 score of 0 . 939 on m1 and m2 metric , while bertscore - recall achieves the highest score . sent - movers achieve the best performance with a f1score of 1 . 0 .
the results are shown in table 6 . the first group shows the results for m0 , m1 , m2 , m3 , m4 , m5 , m6 , m7 and m8 . the results of these models show that the language - para - based models benefit from syntactic and semantic distance - based learning . the performance gap between m0 and m3 with the same classifier is slim but significant .
table 3 presents the results of model a and model b with respect to transfer quality and semantic preservation . the results are presented in tables 3 and 4 . semantic preservation and transfer quality metrics are shown in table 3 . we observe that yelp has the best transfer quality . the semantic preservation metrics show that yelp is well - equipped to handle the task of semantic and syntactic recall . however , the transfer quality metric is slightly worse than the semantic preservation metric , indicating that yelp does not have the best generalization ability .
table 5 shows the results of human evaluation for each metric . the summaries generated by our system match the sentiment evaluation results of the human evaluators , but do not match the accuracy of the system ' s predictions .
the performance of our model with different classifiers is shown in table 6 . the results are summarized in bold . our model achieves state - of - the - art results with accuracies in terms of sim , pp , and gm .
results on yelp sentiment transfer are shown in table 6 . our best models ( right table ) achieve higher bleu than prior work at similar levels of acc with the same 1000 transferred sentences and human references , but their accuracy is lower than the best unsupervised model .
in table 2 , we report the percent of reparandum tokens that were correctly predicted as disfluent as well as the length of the repetition tokens in question . the average number of repetition tokens for each disfluency length is reported in table 2 . as expected , the average length of disfluencies is relatively consistent across all types .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) . the majority of tokens in each category belong to the repair category , however , a fraction belong to each category as well . the length of the tokens in question also indicates the extent to which the content word contains a function word .
the results are shown in table 1 . the text - based model outperforms the single - input approach with a gap of 0 . 2 bleu from the last published results ( gillick et al . , 2017 ) in terms of dev mean . moreover , the model performs best when trained and tested on the test set with the best performing feature set . it is clear from the results that the combination approach gives a competitive advantage . however , the gap between the best and average test set size does not represent a significant performance gap .
the performance comparison of our model with the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model achieves the best performance with a gap of 3 . 43 % in accuracy from the previous state of - the - art algorithm .
table 2 compares the performance of different methods for the apw and nyt datasets . the unified model significantly outperforms all previous models . it achieves the best performance with a gap of 10 . 2 % in accuracy compared to the previous best model .
table 3 compares the performance of our method with and without word attention . our neuraldater model obtains 62 . 6 % accuracy ( t - gcn ) and 63 . 9 % accuracy with graph attention . the performance gap between the methods shows that the use of word attention alone does not improve the performance .
the results are shown in table 1 . the first group shows that the argument argument is the most important part of the model , followed by the embedding stage . next , we show the performance of all the models with different combinations of argument and event tags . we see that the dmcnn model achieves state - of - the - art results with a gap of 10 . 6 points from the previous state of the art model with respect to 1 / 1 , 1 / n and 3 / t .
table 1 presents the results for argument and event identification . the results are presented in terms of p < . 001 and f1 scores . for argument identification , we see that our method is comparable with state - of - the - art . for event identification , it achieves a gap of 2 . 9 points from the previous state of the art . for cross - event identification , the gap is narrower .
for english - only and spanish - only languages , the results are shown in table 3 . the results are summarized in bold . we see that both languages perform comparably to the best previous state - of - the - art systems . however , in english , the performance gap is narrower than in spanish . fine - tuned - lm outperforms all the other language - only variants except for english , which shows the extent to which fine - tuning has impacted the performance of the language - specific features . finally , we see that the concatenated - vocab - lm variants do not perform as well as the original variant .
results in table 4 show that fine - tuning the model with only subsets of the code - switched data improves the results on the test set . the model achieves state - of - the - art results with a f1 score of 7 . 2 / 10 and a bdi score of 75 . 5 / 100 .
the results of fine - tuning the dev set and the test set are shown in table 5 . the results show improvements in accuracy for both sets . the fine - tuned - disc model achieves 75 . 40 % and 75 . 33 % accuracy , respectively , on the dev and test set , compared to the performance of the monolingual model ( which uses the standard concatenated word embeddings ) .
table 7 shows the precision ( p ) and recall ( r ) numbers for using the type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvements in precision and recall are statistically significant ( p < 0 . 01 ) with a f1 - score of 0 . 05 , indicating significant improvement in performance .
table 5 shows the precision ( p ) and recall ( r ) numbers for using type - aggregated gaze features for the conll - 2003 dataset . the improvements in precision and recall are statistically significant ( p < 0 . 01 ) with a f1 - score of 94 . 03 , which indicates significant improvement in recall .
results on belinkov2014exploring ’ s test set are shown in table 1 . syntactic - sg embeddings generated by glove - extended outperform wordnet , verbnet , and ontolstm - pp in terms of type and length . the difference is less pronounced for onto - wordnet , which obtains a performance improvement of 3 . 8 % on the test set . hpcd also achieves a performance gain of 4 . 7 % over wordnet - retro .
results in table 2 show that the hpcd - pp dependency parser outperforms the ontolstm - pp - pp model with features coming from various pp attachment predictors and oracle attachments . further , it achieves the best performance with respect to pp accuracy .
table 3 shows the effect of removing the context sensitivity and the sense priors from our model . our model achieves a ppa acc . score of 89 . 7 / 88 . 5 , marginally improving from the previous state - of - the - art model .
the results in table 2 show that the domain tuning approach further boosts the bleu % score of our model to 66 . 5 % ( from 43 . 6 % in en - de and 43 . 9 % in mscoco17 ) . however , the difference is less pronounced for multi30k , indicating that domain tuning alone does not improve the translation performance .
the results of domain - tuned h + ms - coco models are shown in table 1 . the results are summarized in bold . we observe that the performance gain over the plain plain hoco model is due to better label labelling performance . moreover , the results are slightly superior than those of monolingual models with domain - tune , indicating that more labelling information helps the model to improve its performance .
table 4 shows the bleu scores for en - de , en - fr , and multi30k captions . adding automatic image captions improves performance for all models except for those using marian amun ( cf . table 4 ) . the results show that , for the most part , the automatic captions do not harm the captions performance .
the results in table 5 show that both enc - gate and dec - gate strategies achieve high bleu % scores ( 68 . 86 % and 62 . 40 % ) , respectively , compared to the performance of en - de and mscoco17 using the standard multi30k + ms - coco + subs3mlm embeddings .
the results of " multi - lingual " and " text - only " modes are shown in table 3 . the results are summarized in bold . sub - 3m lm detectron achieves the best results with a score of 62 . 45 % on the test set and a gap of 3 . 86 % with " ensemble - of - 3 " .
the results are shown in table 1 . the performance of each approach is presented in bold . in general terms , we see that en - fr - ht and en - es - ht are comparable in difficulty to each other in terms of generalization . however , the performance gap between the two approaches is narrower with respect to translations for yule ' s i and ii .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . our en – fr model splits the sentences into two parallel sentences .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the results show that the models perform well on both languages .
the evaluation scores ( bleu and ter ) for the rev systems are shown in table 5 . the en - fr - rnn - rev system shows lower performance than en - es - trans - rev , however , it obtains a better automatic evaluation score . the ter system shows a slight performance improvement over the original rev system , but still lags far behind the original .
results on flickr are shown in table 2 . the row labeled vgs is the visually supervised model from chrupala2017representations . com and the row labeled rsaimage is the audiovisual model from flickr2017 . the mean mfcc score of our model is 0 . 0 , which means that our model obtains a significant recall improvement .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled u is the audiovisual supervised model . the average recall @ 10 percentage is 0 . 5 , which means that the model performs slightly better than rsaimage in terms of recall .
we report further examples in the appendix . the first example in table 1 shows that the dan classifier turns in a screenplay that is easily distinguishable from a blank screenplay . the second example shows that rnn classifiers can distinguish between edges and curves . finally , cnn classifiers distinguish between the edges and the shapes of the sentences .
table 2 shows the pos changes in sst - 2 as a result of fine - tuning . the numbers indicate the changes in percentage points with respect to the original sentence . for example , the rnp model has increased the number of occurrences for nouns by 3 . 5 % and for verbs by 6 . 5 % . however , for adjectives , the percentage points remain the same ( hence , there is no change in the pos score for rnp . the same is true for verbs .
the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the results are shown in table 3 .
table 1 presents the results of pubmed and sst - 2 . the results are summarized in table 1 . pubmed outperforms sift by a margin of 98 % and 99 % respectively .
