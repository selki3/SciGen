table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . the summaries in table 2 show that , in theory , a 10 - episode training schedule should give about a 4 . 5 % boost in performance over the performance of a single example without sacrificing too many instances , since the training data are small . however , this boost is less pronounced during inference when we use a larger batch size , which shows the advantage of using a more compact architecture .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . considering the fact that the balanced dataset has 25 instances , this small difference in performance does not represent a significant big performance drop . the difference between the balanced and linear datasets is less pronounced for the moderate dataset , but still shows that there is a significant performance drop when the size of the batch is increased . we conjecture that this is due to the reduced performance in the low - resource settings .
the max pooling strategy consistently performs better in all model variations . for example , ud v1 . 3 achieves the best performance with a f1 score of 75 . 83 % and a dropout probability of 9 . 57 % compared to conll08 ' s 69 . 63 % . also , ud performs better with a larger sample size , making it more suitable for production use . these results show that the use of a smaller sample size induces the model to perform better in the optimization task .
table 1 : effect of using the shortest dependency path on each relation type . we show that our macro - averaged approach gives a significant performance gain over the best f1 model without sdp . in fact , it is more than 27 % better than the model with sdp in most relation types .
we observe that the performance gap between the best and worst performances on average is less pronounced for y - 3 when we switch from " y " - 3 to " y - 3 " . on the other hand , our model performs better than both the best previous state - of - the - art models on the f1 and r - f1 scores , which shows that our model can learn the task to a high degree .
the results are shown in table 1 . our proposed mst - parser outperforms all state - of - the - art parsers and performs on par with or better than the best performing ones on average . for example , it achieves a 100 % success rate on average with respect to paragraph accuracy and achieves 50 % accuracy on sentence prediction accuracy . on average , this means that it parsers the raw material from one submission and applies the best judgement on each submission after applying our proposed feature set .
table 4 shows the mean and average c - f1 scores of our system over the test set in table 2 . our lstm - parser parser outperforms the other two systems in paragraph and essay parsing , respectively , by a large margin .
table 1 shows the results for the four systems . the results are broken down in terms of bleu , meteor , nist , rouge - l and cider scores . from left to right , we see that the original and cleanest tgen models perform better than the cleaned tgen model . however , the difference between the two is less pronounced for the original , since tgen + has seen more training data and is more likely to be trained on a larger corpus . finally , the results are slightly worse than those of tgen − when the original is cleaned , indicating that the cleaning step is crucial for the performance improvement .
table 1 : data statistics comparison for the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . the difference in ser between the original and the cleaned version is less pronounced for the train dataset , but is still significant . for the dev dataset , we see a drop of more than 10 % in ser compared to the original .
table 1 shows the results for the four systems that we trained on the data augmentation task . the results are broken down in terms of bleu , meteor , nist and rouge - l scores . the original and original tgen models perform much better than tgen + and sc - lstm , while the latter is inferior to both the former on average . from left to right , we see that the original t - gen model is better than the original and slightly worse than the tgen − model . on the other hand , the difference between the scores of the two systems is less pronounced when we consider the training data , which shows that tgen is better at selecting the correct answer and the misspelling ones are less prevalent . finally , we notice that the meteor scores are slightly higher than those of tgen , indicating that more training data is required to improve the results .
table 4 : results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) are 22 . 6 % higher than the original ( by a factor of 3 . 6 % ) , which shows that tgen is indeed more difficult to train and maintain than expected .
the results are shown in table 1 . our system outperforms all state - of - the - art models on both the single - domain and ensemble level . for example , our dcgcn model achieves a performance improvement of 3 . 6 % over the previous state of the art on average . graphlstm ( song et al . , 2018 ) achieves a 4 . 4 % overall improvement over the performance of seq2seqk model , and a 3 . 9 % overall increase over the pbmt model . the difference between our system and other approaches is less pronounced for the ensemble level , but still suggests that our approach has superior generalization ability .
table 2 shows the performance of our model on amr17 . our dcgcn achieves 24 . 5 bleu points , which is slightly better than the previous state - of - the - art seq2seq model by a margin of 3 . 6 points . additionally , our model achieves a significant improvement in t - test over other approaches , improving from a single model to an ensemble model with a performance gain of 7 . 5 points . by further adding multi - parametrieval parameters , we reach a final score of 59 . 4 points . the performance gap with other approaches is narrower , but still significant .
table 1 shows the results for english - german , czech and slovak , compared to english - czech . the results are broken down in terms of performance on single and multi - word embeddings . our proposed method outperforms the previous state - of - the - art models across all three languages . for example , seq2seqb ( beck et al . , 2018 ) achieves a single - word score of 41 . 8 / 71 . 4 and a c - score of 35 . 9 / 59 . 6 , respectively , with an absolute improvement of 2 . 4 / 4 . 6 and 6 . 2 / 4 points over the previous best state - ofthe - art model .
table 5 shows that the number of layers inside a dc network is the most important factor in the performance of the model . the model performs best when there are fewer layers , so we need to consider the effect of more layers . we observe that for example , when we add 6 layers , our model performs better than the previous state - of - the - art model on average .
table 6 shows that the baselines for most gcns are comparable to each other , with the exception of those using residual connections . adding rc improves performance for gcns with residual connections , but does not improve performance for those using rc + la . we find that the difference between the bias metric and rc metric is most prevalent in gcns using leap ( table 6 ) .
the results are shown in table 4 . our proposed method outperforms the previous state - of - the - art methods on every metric by a noticeable margin . for example , dcgcn achieves a performance improvement of 4 . 2 % over the state of the art model on average . on the other hand , our proposed method results in a performance drop of 2 . 4 % compared to previous work . this indicates that the performance gain comes from a better model design . we observe that the coreference signal is localized on specific regions of the graph and that these regions are important for the generation of the decoder .
table 8 shows the ablation study results for the density of connections on the dev set of amr15 . the results show that our dcgcn4 model is comparable to the best previous state - of - the - art model , in that it eliminates the dense connections in the i - th and iii - th blocks .
table 9 shows the results of an ablation study for the modules used in the graph encoder and the lstm decoder . the results show that , under the " - graph attention " and " - linear combination " approaches , our model obtains better results than the previous state - of - the - art model dcgcn4 . however , when we switch to the " global node " approach , we see that our model performs slightly worse .
table 7 shows the performance of our initialization strategies on various probing tasks . our paper establishes a new state - of - the - art on all three high - level metrics , and on all subtasks except coordinv . it achieves the best performance among all three metrics with a gap of 10 . 8 % overall from the previous state of the art . subjnum is the only one that gets worse performance than our glorot - based system . on the other hand , topconst and somo are the only ones that get better performance .
the results are shown in table 1 . our h - cmow model outperforms the best previous approaches across the board . it achieves state - of - the - art results in terms of precision , tn and bshift scores , and coordinv scores . it closely matches the performance of our h - cbow model , obtains the best score on subjnum and topconst , and is nearly 7 % better on coordinv . the difference is less pronounced for wc , but still suggests some reliance on superficial cues . we observe that our approach is comparable to other approaches that rely on cbow - based features .
the results are shown in table 1 . the first group shows that our proposed method outperforms the previous state - of - the - art methods on all metrics by a noticeable margin . our proposed cbow / 784 improves upon the previous best results by 3 . 6 % in the sub - category of mpqa and mrpc , and on average improves by 4 . 4 % overall in the sick - r category .
table 3 : scores on unsupervised downstream tasks attained by our models . the results show that our proposed method outperforms the best state - of - the - art method , cmow , in all but one of the 13 sts13 and 16 tasks . in addition , it achieves a considerable performance improvement over the cbow model on the sts15 and sts16 tasks , which shows the relative change brought about by our proposed model design .
table 8 shows the performance of our system for initialization on selected downstream tasks . our system outperforms glorot and trec by a large margin . it achieves state - of - the - art results on all metrics , and even outperforms the best previous approaches on some of the worst ones . on the sst datasets , it achieves the best performance among all three systems .
table 6 shows the performance of our method for different training objectives on the unsupervised downstream tasks . our cbow - r model outperforms the other methods with a large margin . it achieves the best performance on sts12 , 13 , 15 , 16 and 17 .
the results are shown in table 1 . the first group shows that our proposed method outperforms the previous state - of - the - art methods on every metric by a noticeable margin . our proposed method obtains the best results on three out of the four metrics . it closely matches the performance of the best previous methods . on the other hand , it performs slightly worse than our method on two of four metrics indicating that it is better at selecting compact regions and time - bound constraints .
the results are shown in table 1 . the first group shows that cbow - r reaches the best results , outperforming all the other methods apart from sst - e , which shows the advantage of finetuning word embeddings during training . cmow achieves the highest score among all the sub - committees , showing that the training data obtained during the pre - training phase are of high quality . the second group shows the results of re - training after applying the best performing feature set . sst2 achieves the best result , improving upon the previous state - of - the - art model by 4 . 6 points . trec performs the best , improving by 3 . 9 points over the previous best .
the results are shown in table 1 . supervised learning outperforms all state - of - the - art systems that do not rely on supervised learning . specifically , it achieves the best results in the loc / misc task , and in the e + loc and per task , while mil - nd gets the worst performance . the results of τmil - nd show that it is better at selecting the correct utterances and performing the task in the correct context , and its performance is comparable to other supervised learning systems that rely on word analogies .
table 2 shows the performance of our system in the test set under two settings . the first set shows that our approach outperforms both the state - of - the - art supervised and unsupervised models in terms of f1 score . the difference is most prevalent in the last set , where our model ( mil - nd ) obtains the best performance . in comparison , supervised learning gives a performance improvement of 2 . 48 % over the best previous state of the art model . in the second set , our approach achieves a performance gain of 3 . 45 % .
table 6 shows that the g2s - gat model significantly outperforms the models based on the ref and gen scores . moreover , it achieves state - of - the - art results on both the gen and ref metrics , and on the ent and ent - ent metrics as well . the model with the best performance on the three ref measures is the one that we choose for our final model .
table 3 compares our proposed approach to previous state - of - the - art models on the ldc datasets . the results are summarized in table 3 . our proposed g2s - gat improves upon the previous state of the art model by 3 . 32 ± 0 . 08 points on average compared to previous work . on the other hand , it performs slightly worse than our proposed s2s model and is comparable to the performance of the best previous models on meteor and bleu . also , we observe a drop in performance between the two baseline models when we switch from mono - based to baselines - based learning models .
table 3 shows the performance of our g2s - ggnn model on the ldc2015e86 test set when models are trained with additional gigaword data . our model outperforms the previous state - of - the - art models with a large margin .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model outperforms the previous state - of - the - art models on every metric by a significant margin . for example , our get model achieves a precision of 62 . 42 % on bilstm and a performance improvement of 35 . 37 % on meteor .
the results are shown in table 1 . we observe that g2s - ggnn model has 7 . 51 % improvement over the baseline model in terms of sentence length and average number of tokens per sentence , compared to the previous state - of - the - art model . also , it has a 7 . 45 % boost in average sentence length compared to s2s , which shows the performance gain comes from a larger graph diameter .
table 8 shows the results for the test set of ldc2017t10 . our model outperforms the other two baselines with a large margin . the difference is most prevalent in the output , which shows that g2s - gat is better at selecting the most interesting and interesting parts of the input and generating the best - performing summaries . on the other hand , s2s shows a slight improvement over the gat model with a gap of 2 . 7 % in the fraction of elements missing in the generated sentence ( miss ) .
table 4 shows the pos and sem accuracy for the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the pos tagging accuracy is slightly better than sem , but still slightly worse than fme . sem , on the other hand , is better than pos and fme , showing the advantage of finetuning word embeddings during training . we observe that fme has the best overall performance .
table 2 compares the pos and sem tagging accuracy with baselines and an upper bound . word2tag significantly outperforms mft and unsupemb in both metrics . pos even gets better results than sem with a baseline of 91 . 95 % and 87 . 55 % on average , which shows the advantage of finetuning word embeddings without supervision . word2tag also outperforms the mft model in semantic analysis as well .
table 4 presents the system ' s performance on the four metrics for pos , tagging accuracy and sem . we observe that for all metrics , our system performs better than the best previous state - of - the - art systems on average . for pos , we observe that our system achieves the best results with three out of four metrics . on the other hand , our sem model outperforms all the other systems with two exceptions .
pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we observe that the first layer of our system exhibits the best performance , with a pos score of 87 . 9 % and a sem score of 81 . 5 % . the second layer shows that it is closer to the performance of the bi - based system , but still performs slightly worse than the uni layer . finally , the third layer shows the biggest performance gap between bi - and res - based systems . our system outperforms both the other two layers .
table 8 shows the attacker ’ s performance on different datasets that we trained on . results are on a training set 10 % held - out . the average attacker score is 14 . 2 % higher than the corresponding adversary ' s on pan16 , indicating that the attacker has better understanding of the task . on the other hand , age and gender features seem to have a bigger impact on the performance , lowering it to 9 . 7 % and 8 . 3 % respectively .
table 1 : accuracies when training directly towards a single task . our system outperforms all state - of - the - art models across all three metrics . for example , it achieves an accuracy of 83 . 2 % on the dial task , 77 . 9 % on mention task and 77 . 4 % on sentiment task . the gender - neutral pan16 model outperforms other models with similar training data .
table 2 shows the distribution of the leaked data for the balanced and unbalanced data splits . the results show that pan16 significantly outperforms state - of - the - art models across all three data types . for example , gender - neutral pan16 outperforms all supervised and unsupervised baselines with a large margin . the racial disparities are most prevalent in the task prediction phase , where gender bias and age - based bias are the most prevalent . overall , the performance gap between pan16 and other baselines is slim , but significant .
the performance of our system on these test datasets is shown in table 3 . our proposed approach outperforms the best state - of - the - art approaches by a noticeable margin . the gender - neutral approach , pan16 , gives a performance gain of 9 . 2 % on average over the baselines . on the other hand , our approach gives a 4 . 9 % performance gain .
the results are shown in table 6 . the rnn encoders perform similarly to the guards , however , the difference in the precision is less pronounced for the leaked ones . guarded rnns perform better than leaking ones .
the results of applying our finetune and tuning on the training data are shown in table 1 . we observe that our approach outperforms the previous state - of - the - art models on every metric by a significant margin . for example , our lstm model achieves a final score of 62 . 97 % on the finetune metric , which means it performs better than both the best previous approaches on three out of four scenarios . on the other hand , our approach performs better on two of the four scenarios , giving a performance gain of 2 . 36 % overall compared to the previous best state of the art model .
table 1 compares the performance of our approach with previous state - of - the - art models on the test set in terms of acc and time . the results are presented in table 1 . our approach achieves the best results with an overall score of 90 . 3 % , which is slightly higher than the previous best result by rocktäschel et al . ( 2016 ) . the difference is most prevalent in the last published results , when we consider the fact that our lstm model is more than 4x faster than previous state of the art models .
table 1 compares the performance of our proposed method with previous work on the amapolar , yelppolar and yahoo time datasets . the results are presented in table 1 . our proposed method outperforms both published and unpublished work on every metric by a significant margin . for example , on yelp , it obtains err and time improvements of 3 . 48 % and 6 . 08 % over the previous state of the art model . on the other hand , it achieves a full err improvement of 4 . 45 % and a 1 . 68 % time improvement over the last published work by zhang et al . ( 2015 ) . this confirms the value of parameter sharing .
table 3 : case - insensitive tokenized bleu score on wmt14 english - german translation task . from table 3 , we can see that gnmt outperforms all other methods with a large margin . however , the difference between gnmt and other methods is most prevalent in the last sentence , when we consider the training and the decode time . lrn , on the other hand , obtains a significant improvement over atr and sru in both training and decoding time . we can also see that olrn is comparable with atr in both case - in - context and in the overall performance . finally , we notice that gru has the advantage of training on a larger dataset , which gives it a bigger performance boost .
table 4 shows the exact match / f1 - score of our model on squad dataset . we used the base model of wang et al . ( 2017 ) with only elmo as our parameter number of base . the other models use glove , lrn and sru with more elmo parameters . besides , we also used atr and gru with a larger number of parameters to improve the performance . as shown in table 4 , the combination of elmo and gru improves the model ' s performance . however , it does not improve the f1 score , which shows the diminishing returns from adding all the parameters from elmo . finally , we noticed that the lstm model is comparable with the best performing lrn model . retrieving only the parameters with elmo reduces performance marginally .
table 6 shows the f1 score of our model on the conll - 2003 english ner task . our lstm model achieved the best result with a f1 - score of 90 . 56 . by contrast , lrn and atr both receive lower f1 scores . lrn also requires more parameter inputs , which results in a lower performance . gru , on the other hand , obtains the highest score . we observe that our model performs better than the other methods with fewer parameter inputs .
table 7 : test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . with the base setting , our model obtains the best performance on both snli and ptb tasks . with the ptb setting , we get the best result .
table 1 shows the system performance on the test set of hotpotqa in the distractor and fullwiki setting . the system performance is very similar on both systems with two exceptions . the difference is most prevalent in distractor , where oracle outperforms both human and system . on the other hand , for fullwiki , retrieval is closer to human , with a gap of 2 . 8 % on average . this indicates that oracle is better at selecting the relevant words and its output is more interpretable . we observe that the difference between the average number of tokens for each word is less pronounced for distractor than for system , indicating that more effort is required to train the system .
table 4 shows the results of human evaluation . our system outperforms all the other systems that are tested on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 8 , which indicates that our system is indeed more sophisticated than the others . the second best result is obtained by candela ( 30 . 2 % ) on the k 1000 and k 2000 scales , which shows that it is comparable to human evaluation in terms of overall quality . the third best result , obtained by h & w hua and wang ( 2018 ) , shows that seq2seq is comparable with human evaluation on three of four aspects ( appropriateness , appropriateness and content richness ) and is comparable on the four aspects of k 2000 . finally , our system achieves the highest score on k 500 , showing that it can learn from human evaluation to improve its output .
the results are shown in table 1 . we observe that , for the most part , our approach outperforms the best previous approaches across the five domains . however , we see that our approach is slightly better than the others on three out of the four domains . the biggest difference is seen in the " ted talks " dataset , where our en model outperforms all the others except for the one that belongs to the " sqs " category . on the other hand , this gap is less pronounced for " europarl " . our approach shows that our data augmentation technique results in a better performance on these domains than the previous state - of - the - art models .
the results are shown in table 1 . for the ted talks dataset , we observe that our proposed approach outperforms the previous state - of - the - art approach on all metrics by a noticeable margin . on the other hand , our approach performs slightly worse than the best previous approaches on the sub - tables dataset .
the results are shown in table 1 . we observe that , for the most part , our approach outperforms the best previous approaches across the five domains . however , we see that our approach is slightly better than the others on three out of the four domains . this is mostly due to the smaller difference in performance between en and europarl , which shows that our model can rely less on superficial cues . on the other hand , this small difference does not represent a significant performance drop . our model outperforms all the other approaches that do not use text clustering .
from table 1 , we can see that the average depth of our data is 11 . 05 , which means that our average roots are 1 . 05 times deeper than those of other systems . europarl , on the other hand , has the advantage of having more depth . this is reflected in the average number of roots per row , which shows that our data are more compact . our proposed hclust clustering scheme outperforms other baselines in terms of total terms , as shown next .
from table 1 , we observe that the average depth of our data is 9 . 43 % , which is slightly higher than the previous state of the art . europarl , on the other hand , is closer to the state - of - the - art . our data augmentation technique , called depthcohesion , works well , providing a 3 . 29 % boost in average depth without sacrificing too many features .
the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . 7 is shown in table 1 . the enhanced version of our lf model outperforms the baseline model by a noticeable margin . the difference is most prevalent in the performance on question type , answer score sampling , and hidden dictionary learning , where our enhanced model ( lf + p1 ) shows marked performance improvement . on the other hand , the difference between the performance of our model with and without the enhanced p1 is less pronounced , which shows that our approach can further improve the generalization ability .
table 2 shows the performance ( ndcg % ) of the different ablative studies on different models on the visdial v1 . 0 validation set . the best performing model is the one that applies p2 , the most effective one . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . the difference in performance between baseline and p2 indicates that the former is more effective , while the latter is less effective . also , the difference between the rva score of baseline and + p2 indicates the effectiveness of the second approach .
table 5 compares the performance of our hmd - f1 and hmd - recall models with other approaches that rely on bert . for example , wmd - unigram + bert shows 0 . 823 % improvement on hard alignments and 0 . 866 % increase on soft alignments . on the other hand , ruse performs slightly better on both alignments , showing that bert is better at selecting the correct semantic relations and the correct recall . we observe that the wmd - bigram model performs best on both soft and hard aligned alignments as well .
the results are shown in table 1 . the first set of results show that bert score - f1 is relatively consistent with the baselines , while on the other hand , it is significantly lower than meteor + + and ruse ( * ) on average . the second set shows that sent - mover has the best performance . it achieves the best average score on three out of the four metrics , which shows that it is better at selecting the relevant features and its output is more interpretable .
the results are shown in table 1 . the first set of results show that bleu - 1 has the best bert score and bertscore - f1 is the second - best among all the baselines . next , we see that meteor has the worst performance . sent - mover performs the best among all three baselines , and the w2v - based baselines are the only ones that do not exceed the threshold for " best moving average " . finally , we observe that the sfhotel scores are slightly higher than the other baselines on both sets , indicating that the clustering quality is high .
the results are shown in table 1 . word - mover achieves the best results with a bertscore - recall of 0 . 939 on the m1 / m2 metric and a meteor - meteor - f1 score of 1 . 083 . the other baseline systems perform slightly worse than the former best ones on both m1 and m2 . sentance is relatively consistent with the word - mover baseline , but does not exceed the upper boundary of the range of performance for any other baseline , which shows the performance reach is limited by the size of the training data set . we observe that the clustering quality of the baselines is relatively high for all three metrics , indicating that the selection of the right baseline is important .
the results are shown in table 1 . we observe that the performance reach the best when we only consider m0 and m6 from the same training set , with a gap of 10 . 2 % in coverage between the two .
table 1 shows the results for yelp and google translate . the results are broken down in terms of transfer quality , transfer quality and precision . for yelp , we observe that the transfer quality is relatively high while precision is low . semantic preservation is strong , and precision is high , indicating that the model is well - equipped to handle the task . fluency is high as well , indicating the model performs well in the semantic and syntactic tasks . we observe that yelp is better at selecting the best match pairs and the best performing ones for each task .
table 5 shows the results of human evaluation for each metric . our approach verifies the accuracy of our summaries with a minimum of 94 % , which means that 94 % of the summaries that match match the human evaluation criteria . on the other hand , it verifies accuracy with 84 % .
the results are shown in table 1 . we observe that the performance reach the best when we only consider m0 and m6 , with a gap of 10 . 2 points between those with and without para - para features .
table 6 : results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( yang2018 unsupervised and yang2018unsupervised ) achieve higher acc scores than prior work at similar levels of acc , but untransferred sentences achieve the highest bleu . the difference in acc between transfer and transfer is most prevalent in terms of fu - 1 score , which shows that adapting the style embeddings for transfer is beneficial , but does not improve acc . we find that combining the features of both transfer and retrieval achieves the best results . finally , our best model achieves the highest acc score as well .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent for each type of nested disfluency . for example , the number of tokens for " rephrase " and " start " disfluencies is slightly less than for " nested " , but still represents a significant drop .
the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) is shown in table 3 . reparandum length is the fraction of tokens that belong to each category that are in the repair ( the rest belong to other categories ) . function - function consists of tokens whose function is function - function . this analysis shows that the content - function tokens are particularly useful for this task . they account for 44 . 7 % of the overall prediction ( out of a total of 900 tokens ) in the three categories .
the results are shown in table 1 . we observe that the text - rich innovations model outperforms the single - input approach , and that it is better at selecting the features that are most relevant to the task at hand . however , the results are slightly less clear than those of " raw " . perhaps the most striking thing about the change in the dev scores is that text rich innovations are better than raw innovations , but only when they are considered in combination with other features . finally , we see that the dev mean is slightly higher when we switch from single to multiple - input models , indicating that more innovations are crucial for the model to succeed .
the performance comparison of our model to the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model shows marked performance improvement over the previous state of - the - art models . it closely matches the performance of the best performing cnn - based and rnn - based embeddings . moreover , it achieves higher accuracy on topics that are less frequently discussed , such as dispute resolution and agree - disagree rates .
table 2 : accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . in fact , it achieves higher accuracy than the best previous model , ac - gcn , which shows that the learned reward function can be further improved with an increase in the accuracy . neuraldater also outperforms the other methods , showing that the learning reward function is more useful for the task at hand . attention - based neuraldater shows lower performance , but higher accuracy overall than any other approach . we conjecture that this is due to the larger training data size .
table 3 compares the performance of our method with and without graph attention for this task . our approach shows marked performance improvement . the difference in accuracy between word attention and graph attention is less pronounced for neuraldater , but still shows that our approach is comparable to state - of - the - art in terms of performance . ac - gcn shows a slight performance improvement as well .
the results are shown in table 1 . our proposed method outperforms the state - of - the - art dmcnn and cnn models on all metrics with two tasks . on the one hand , jrnn shows marked improvement over the previous state of the art in all but one of the four scenarios , and on the other hand , it performs significantly worse than the best previous approaches . the difference is most prevalent in the last scenario , where jrnn obtains 75 % improvement . this indicates that the jrnn model is better at selecting the relevant features and its output is more interpretable . additionally , the difference is less pronounced in the final scenario , when we consider all the data available from the training data . we observe that , on the two remaining scenarios , argument and derivation , the jree model performs better than both the other methods .
table 1 presents the results for event detection . our proposed method outperforms all state - of - the - art methods and performs well on both event and argument detection . it achieves the best results with a f1 score of 68 . 9 % on the event detection and p < . 01 . the best results are obtained with cross - event event detection , which shows that our proposed method is well - equipped to handle the high frequency of event detection in the low - resource settings . we observe that the ability to pick out the most important events from a single event is crucial for prediction success . this is reflected in the high precision scores reported in table 1 . for argument identification , our proposed system performs better than traditional methods . on the other hand , it requires more data and time to train , which results show that it is harder to learn and refine the feature set for argument identification . finally , our system exhibits the best overall performance .
the results are shown in table 1 . the first group shows that , for english - only and spanish - only languages , the dev perp and test acc are relatively high while the test wer are low . regularization gives a performance gain of 2 . 45 % over " shuffled - lm " . however , fine - tuning gives a 0 . 9 % performance gain over " fine - tuned " . the second group of results show that , overall , precision is relatively high with respect to both language - specific and lexical features , i . e . precision increases with the lexical feature - rich training set .
results on the dev set and on the test set using only subsets of the code - switched data are shown in table 4 . the fine - tuned model outperforms the cs - only model , showing that the training set size and the quality of the training data are the most important factors in the model ' s performance . the results also show that fine - tuned models are better at selecting the relevant features of the relevant training data and the correct salience for each training epoch .
the results in table 5 show that fine - tuning reduces the error on the test set and the dev set , and upsampling has a generally positive effect ( tables 5 - 7 ) . however , fine - tuned - discing has a bigger impact on dev , since it requires more data and performs better on the gold sentences .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvement over the baseline model is statistically significant ( p = 0 . 0088 , t - test , p < 0 . 01 ) which indicates significant performance improvement over type combined gaze features . the type combined approach also improves recall and precision ,
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features for the conll - 2003 dataset . the improvement from baseline to type combined is statistically significant ( t - test , p < 0 . 01 ) . type combined gaze features significantly improve the recall and precision , and the f1 score improves as well .
the results on the belinkov2014exploring test set are shown in table 1 . our hpcd model outperforms the previous state - of - the - art on three out of the four test sets . the difference is most prevalent in the syntactic - sg category , where our glove - extended model beats all the alternatives except wordnet , verbnet and lstm - pp .
table 2 : results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . our hpcd - based system outperforms all the other systems that do not use lstm - pp as an attachment predictor .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . our model achieves the best performance with a ppa acc . of 89 . 7 % and a full score of 87 . 5 % on the attention metric .
in table 2 , we report the bleu % scores of the models that add subtitle data and domain tuning for image caption translation . the results are slightly better than those of en - de and mscoco17 , but still significantly worse than the results obtained using en - fr and noregional subtitle data . the multi30k dataset , on the other hand , achieves the best results , with an f1 score of 62 . 5 % .
the results are shown in table 1 . the first group shows that domain - tuned h + ms - coco achieves the best results , outperforming both the plain hoco model and subs1m model . the second group shows the results of a and t using the best performing labels . a shows that the best performance is obtained when labels are applied to both subs and monolingual models . sub - mul parameters generally perform better than those without , but the results are slightly worse than those using labels .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . the results are slightly better than the results with marian amun . amu ' s multi30k approach outperforms both our approach and the best - performing en - de model , mscoco17 . however , amu outperforms our approach with a slight margin . this is mostly due to larger variation in the number of frames in the captions , which means that amu has to rely less on automatic captions .
the results in table 5 show that enc - gate and dec - gate achieve better results than en - de and mscoco17 on flickr16 and flickr17 , respectively . moreover , the bleu % scores of both approaches are higher when using transformer , multi30k + ms - coco + subs3mlm and detectron mask surface , as shown in fig . 5 . encoding visual information takes a considerable amount of time to process , as measured by our evaluation results . however , once the information is encoded , it is relatively easier for the encoder and decoder to process . this indicates that the advantage of using a gate - based architecture is derived from the reduced effort required to encode the visual information .
" + ensemble - of - 3 " model outperforms all the other models that do not use ms - coco , showing the advantage of finetuning word embeddings during training . the results are slightly worse than those of " text - only " model , but still superior to " multi - lingual " . subsequently , we report the results of our model using the best performing feature set . we observe that our model performs best when we switch from text - only to word - only features .
table 1 shows the results for english and german on the test set of yule ’ s i and ii . from left to right , en - fr - ht and en - es - ht achieves better results than either ttr or mtld . in addition , the average number of frames per second for each translation is slightly higher than the average of ttr and mtld , which shows the advantage of finetuning word embeddings during training .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . en – fr has 1 , 467 , 489 sentences , while en – es has 5 , 734 . for the dev split , we used 7 , 723 .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the results show that our proposed system outperforms the best state - of - the - art approaches in all three languages .
table 5 shows the evaluation scores for the rev systems . the en - fr - rnn - rev and en - es - trans - rev systems achieve high bleu and ter scores , respectively , indicating that the models are well - equipped to perform this task . however , it should be noted that the system performance obtained with en - e - smt - rev is lower than the performance of the other two systems , since the latter relies on word - error generation . the ter scores are slightly higher than those of en - er , indicating the performance gain comes from a better model design .
table 2 shows the results for flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the one supervised with segmatch . the results show that vgs significantly outperforms rsaimage in recall @ 10 and average rank , while segmatch performs slightly better in average rank .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . the second row labeled rsaimage is the audiovisual supervised model . it achieves the best performance with a mean mfcc of 1 . 414 and a median rank of 0 . 0 . the third row labeled u shows the performance of audio2vec - u , which relies on word embeddings from pascal - voc and founta et al . ( 2018 ) . the fourth row labeled chu shows the results of segmatch .
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . we report further examples in the appendix . the dan classifier turns in a screenplay that is easier to hate than the original , because it turns on a on ( in in the the the edges ’ s so clever you want to hate it . as the picture shows , the difference between the effectiveness of the dan and rnn classifiers is less pronounced for the original . cnn also shows that it is harder to hate a screenplay when it is on the edges , because the difference in the average number of edges between the edges and the curves is greater .
table 2 shows the pos changes in sst - 2 since fine - tuning . the numbers indicate the changes in percentage points with respect to the original sentence . for example , the number of occurrences has increased , decreased or stayed the same as a result of the improvements in the quality of the tokens . however , the rnp score has increased by a large amount , which indicates that the importance of the word " borrow " .
table 3 shows the sentiment score changes in sst - 2 . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the flipped labels result in a significant increase in sentiment score as well .
table 1 shows the results for pubmed and sst - 2 . from left to right , we can see that pubmed outperforms both sift and corr significantly in all metrics . in fact , pubmed achieves a better result on average than sift in all but one of the comparison tasks .
