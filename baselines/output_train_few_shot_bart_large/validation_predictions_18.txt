table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . the batch size of the training instances and the number of instances for inference are the most important factors in the model ' s performance improvement . as the table 2 shows , the smaller training instances make it easier for the model to learn and reason while the larger iterations make it harder to learn .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization . with a larger batch size , the performance of the balanced dataset decreases as well .
results in table 2 show that the max pooling strategy consistently performs better in all models with different representation configurations . specifically , for ud v1 . 3 and conll08 , the model performs better with a greater number of parameters in the training set , with a dropout probability of 0 . 63e - 04 and 0 . 79e - 03 respectively compared to using the sigmoid representation . with the sb representation , we get a 0 . 66e + 00 improvement in f1 score compared to the sb model with a reduction of 5 parameters . finally , with the increased filtering size , our model achieves a 1 . 83e + 01 improvement over the dropout prediction by 0 . 57e - 02 in conld08 .
table 1 shows the effect of using the shortest dependency path on each relation type . as expected , macro - averaged models perform better in the five relation types as compared to monolingual models without sdp . however , for topic and model - feature , the effect is much larger .
consistent with the observations by vaswani et al . ( 2017 ) , we observe that the performance drop of the y - 3 generation after the drop of f1 from 50 % to 50 % is most pronounced in the lower f1 range . for example , in the case of german , this drop is most prevalent in the f1 and r - f1 ranges , while in the other two ranges it is much smaller . this indicates that german models are more effective in the low - resource settings .
table 3 shows the results for english and german for english . for english , mst - parser achieves 100 % and 50 % accuracy on average compared to the original mst parser on average . on the other hand , for german , it gets close to 100 % . the results for both languages are slightly better than the original .
for the two indicated systems , we report the mean and average c - f1 scores of the best performances on the test set for each paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . the difference in performance between paragraph and essay is less pronounced for the lstm - parser system , however it is higher for the essay .
table 3 shows the test bleu , meteor , rouge - l and nist scores of all models trained on the original and the cleaned tgen + model . the results are broken down in terms of error reduction for each training and test set . cleaned tgen is more accurate than original tgen , while the difference between the two is less pronounced for the original . as the results show , once tgen has been cleaned , the error reduction from the original to the cleaned model is only 0 . 23 points .
table 1 shows the comparison of the original e2e data with the cleaned version . the difference in overall statistics between the original and the cleaned versions is less pronounced for the train dataset , but still significant . for the test dataset , we see a significant drop in ser as measured by our slot matching script , see section 3 ) .
table 3 presents the results of training on the original and the original tgen models . the results are shown in bold . as the results show , when tgen is trained with original and original models , it performs better than tgen − or tgen + when trained with tgen - original . when trained with sc - lstm , the results are slightly worse than the original , but still superior to tgn − and tgen + . as expected , this bias is most prevalent in the original model , not the original .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . as expected , the majority of errors in tgen are caused by errors caused by added instances ( around 22 % ) and slight disfluencies ( around 5 % ) caused by wrong values ( around 14 % ) .
table 3 compares the performance of our proposed method with previous approaches on the hidden test set of tree2str and ps graphlstm . our proposed approach outperforms all the previous approaches except for the case where it is trained on the seq2seqk dataset . the performance gap between our proposed approach and the previous state - of - the - art approaches is modest but significant , we observe that the gap between the best and the worst performing approaches is narrower for both datasets when trained and tested on the combined dataset .
table 2 shows the performance of our model with respect to amr17 . our dcgcn model achieves 24 . 5 bleu points per model while the original seq2seqb model achieves 21 . 6 points . the difference in performance between ensemble and single - model models is most prevalent in the low - resource settings , as these results show , the performance gain comes from a better model design with fewer training instances and a larger number of parameters , which results in a better generalization of the model .
table 3 presents the results for english - german and czech for both languages . we show that the single - and multi - headed learner models perform better in both languages , with the exception of english - czech , where the bow + gcn model does not perform as well as the birnn model . table 3 compares the performance of the different models in english - and czech - german , compared to the previous state - of - the - art on both datasets . the results are presented in table 3 . as the results show , the monolingual learner model seq2seqb outperforms both the single and the dual - headed model ggnn2seq ( beck et al . , 2018 ) when trained and tested on the two datasets .
table 5 shows that the number of layers inside a dc network is the most important factor in model performance . the model performs best with a minimum of 10 layers , and with a maximum of 23 . 5 layers .
comparisons with baselines are shown in table 6 . rc refers to residual connections in the gcns with residual connections and leap refers to connections with leap connections . leap and rcn both show improvements relative to rcn with rc and rc + la ( 6 ) in terms of bias metric , rcn also improves relative to leap metric for both gcns .
the results are shown in table 4 . we observe that the coreference signal is localized on specific objects and that these are the most interesting for decoding in the dcgcn dataset . from these data , we can further distinguish between the performance of the gcn models with respect to specific words or groups of words . for example , in the " dcgcn " dataset , the average number of tokens per label is 420 , while for the " gcn3 " dataset is 300 .
table 8 shows the ablation study results for the density of connections on the dev set of amr15 . in dfgn4 , removing the dense connections in the i - th and iii - th blocks degrades the performance relative to the dcgcn4 model . also , the model becomes less dense as a result .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . we use the language modeling approaches described in section 4 . 4 . 2 and table 9 . 4 for modeling the decoder modules . the results of " - linear combination " compared to " - global node " show that the global node and the linear combination of the nodes contribute differently to the model performance . " coverage mechanism " and " - direction aggregation " show the opposite effect , with the former having a bigger performance drop and the latter having a larger performance gain . finally , " - graph attention " shows the results of using only one type of attention mechanism , compared to the other two .
table 7 shows the performance for initialization strategies on probing tasks . glorot and topconst receive high scores for initialization , indicating that it is well - equipped to handle the task at hand . dual initialization also boosts performance , since it eliminates some of the overhead caused by concatenation noise .
the results are shown in table 3 . we see that the h - cmow variants outperform the cbow variants in terms of concatenated keyphrases . the h - cbow variants are more useful for data augmentation since they have fewer training instances and therefore do not need to be trained on a larger data set . they perform well in the multi - headed attention task , as shown in the results .
the results are shown in table 1 . hybrid outperforms both monolingual and dual - input methods . dual - input learning models outperform monolingually trained models like cbow and cmow , cbow / 784 shows a significant performance drop while cmow / 783 shows a large performance gain . as the results of combining the input and output data shows , the effect of cbow is less pronounced for sst2 and sst5 than for sick - e .
table 3 shows the performance on the unsupervised downstream tasks attained by our models . hybrid outperforms both cbow and cmp . with respect to sts13 and sts16 , the cbow model shows a relative improvement of more than 25 % in performance over the monolingual cmow model . the difference with respect to hybrid is less pronounced , but still represents a significant performance gain . as shown in the results of sts14 and 15 , when cbow is trained and tested on the same dataset , the performance gap between the two modes becomes much narrower . cbow now outperforms cmp .
table 8 shows the evaluation results for initialization strategies on supervised downstream tasks . glorot outperforms the best performing approaches and performs well in all but one case . for sst2 and sst5 , it achieves gains of 2 . 5 and 6 . 2 points over the previous best state - of - the - art model , respectively .
table 6 shows the performance of each method for different training objectives on the unsupervised downstream tasks . the cbow - r approach outperforms the cmow approach in all but one of the cases .
the results are shown in table 3 . we observe that the cbow - r model outperforms the best existing ontonotes - trained models on every metric by a significant margin . topconst and topconst are particularly strong in the bshift metric , showing that the semantic information injected into the model by cbow is significant enough to result in a measurable improvement in performance . further improving performance by high margins
the results are shown in table 1 . we see that the cbow - r model outperforms all the alternatives except sick - r in terms of test set quality on all subtasks except sst2 .
table 3 shows the test bias scores for loc , loc , per and misc for all systems trained and tested on the same dataset . supervised learning outperforms all the alternatives except for the case of " name matching " when trained on all orgs and " misc " . the performance gap between " supervised learning " and " tmtmil - nd " underperforms the other two methods because the supervised learning baseline has more training data and therefore requires more iterations to learn the task . as the results of training on all loc and all misc datasets show , once the training data is filtered , the model performs well in both languages .
uncertain in low - supervision settings . name matching and named entity recognition are the most difficult aspects of the task for model 1 and model 2 to perform on the test set . supervised learning improves the f1 scores for both models and gives a 0 . 5 / 3 . 7 bleu improvement over the naive model in terms of eq . and f1 . while the improvement on name matching is small , it is encouraging to see that it is possible to further improve the model with a reasonable selection of the training data and the correct label for each entity . finally , τmil - nd ( model 2 ) improves the model performance on both named entity and entity recognition tasks . the results are shown in table 2 .
table 6 shows the results for english and german for models trained on the g2s dataset . the results are broken down in terms of performance on extractive and abstractive keyphrases . for brevity we only report results for ref and gen while also reporting results for ent and con . in general terms , the results are summarized in table 6 with respect to the three types of embeddings . apart from the performance drop on s2s , the models perform similarly on both gen and ref when trained and tested on the golbeck - gat dataset .
table 3 compares the performance of our model with previous state - of - the - art models on the ldc2015e86 and ldc2017t10 datasets . we benchmark against the best performing models from the previous literature on these datasets . for example , g2s - gat shows a performance improvement of 2 . 42 ± 0 . 03 points over the s2s model in the eq . 86 test set and a performance drop of 0 . 53 ± 0 . 10 compared to the previous state of the art . similarly , we show a performance gain of 3 . 45 ± 0 . 10 vs . the previous best state - ofthe - art model by cao et al . ( 2018 ) and damonte et al . , ( 2019 ) while a decrease of 4 . 57 ± 0 . , or 0 . 18 points , compared to our model .
table 3 shows the results for models trained with additional gigaword data on the ldc2015e86 test set . the g2s - ggnn model outperforms both the best previous models when trained with gigaword data .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model outperforms the models from the previous literature with a large margin . the total number of models in this set is 22 . 6m , which means that our model can easily exceed the performance of the previous models with the same number of parameters .
results are shown in table 1 . for brevity we only report results for s2s and g2s - gat . from the above table , we can see that for both datasets , the gat model is more useful for sentence prediction and sentence prediction . gat significantly outperforms the baseline model in terms of both average sentence length and average number of words per sentence .
table 8 shows the results for the test set of ldc2017t10 . our model outperforms the s2s model and the g2s - gat model in terms of the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( miss ) . the difference is most prevalent in the case of gold , which shows the advantage of finetuning word embeddings during training .
table 4 shows the pos and sem tagging accuracy using different target languages on a smaller parallel corpus ( 200k sentences ) trained with different nmt encoding layers . pos significantly outperforms the pos tagging accuracy of the original nmt encodings , showing the translation quality of the pos features is high even under the difficult requirement of a small parallel corpus . sem , on the other hand , is only slightly better than pos , showing that the semantic features extracted from the 4th nmt layer are useful for target languages as well .
pos and sem tagging accuracy with baselines and an upper bound . the results are shown in table 2 . word2tag significantly outperforms the unsupervised embeddings in terms of most frequent tags and pos as well as sem , confirming the value of baselines in low - supervised settings . unsupemb also exhibits a significant improvement in pos accuracy over the plain encoder encoder .
table 4 presents the system ' s performance on the four types of targets for pos and pos tagging accuracy . we show the results for english , german , french , dutch and russian . for english , we show results for " es " and " ru " . for " z " we see that our model outperforms the previous state - of - the - art on all metrics except for " tagging accuracy " .
pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we show in table 5 that for all languages except english , our model performs well in the multi - layer setup with a minimum of 90 % accuracy .
table 8 shows the attacker ’ s performance on different datasets that we trained on . results are on a training set 10 % held - out . gender and age features seem to have little effect on the performance of the attacker , however , they do contribute significantly to the overall performance . sentiment and race features are the most difficult ones for the attacker to pick out , as shown in table 8 . accuracy is relatively high for all three groups , as expected .
table 1 : accuracies when training directly towards a single task . gender bias and age seem to have little effect on sentiment prediction , however , it does impact the performance for gender - neutral tasks . accuracy is relatively high for all groups , with the exception of age , which shows that gender bias does not impact sentiment prediction .
table 2 presents the results for tasks with balanced and unbalanced data splits . as expected , gender - based and racial disparities in the task prediction performance are the most prevalent in pan16 , followed by age and gender . sentiment and age - based disparities are the only ones that show significant difference in performance between genders . overall , the performance gap between gender - neutral and racial - based data is much narrower than in the balanced data split .
the performance on different datasets with an adversarial training set is shown in table 3 . in pan16 , gender bias and age seem to have little effect on the performance , while the presence of sexism and racism seem to contribute significantly . as expected , for the age and gender - based features , the task prediction accuracy is relatively high while the leakage is low . the performance is similar on pan16 and pan1616 .
the results are shown in table 6 . the rnn encoders perform similarly to those of guarded and leaky , with the exception of leaky being slightly worse than guarded . as expected , when the protected attribute is encrypted , its performance is worse than when it is unencrypted .
the results are shown in table 1 . we show that the training set size for the lstm model is 22m while the number of parameters for the lrn model is 11m . the difference in performance between these two models is most prevalent in terms of finetune tuning , which results in a performance drop of more than 2 . 5 % on the ptb and wt2 baselines . also , we see that for the wt2 base and finetune modes , the performance gap is narrower , with a gap of 0 . 36 % on average compared to the previous state of the art . table 1 shows the performance of the baselines and the models trained on the combined training set of lrn / sru and atr . this model outperforms the previous best state - of - the - art models on every metric by a significant margin .
table 3 presents the results of baselines trained on the lstm and sru datasets from rocktäschel et al . ( 2016 ) . the results are summarized in table 3 . as the results show , the training set size and the number of parameters used for each parameter seem to have little effect on performance , indicating that the model is well - equipped to handle the relatively small size of training data set . the performance gap between the base and the current state - of - the - art model is less pronounced for sru , but still suggests that there is a need to consider the scalability of parameter sharing across all training sets . table 3 compares the performance of different combinations of training and testing set with the previous state of the - art models . in general terms , we see that gru is more than 4 . 5x faster on average compared to the previous model in terms of both acc and time .
table 3 presents the results for amapolar , yahoo time and yelppolar time on the training data set of zhang et al . ( 2015 ) . the results are presented in table 3 . as can be seen in the lower right - hand corner of the graph , this model significantly outperforms the previous state - of - the - art models on all metrics except for yelp time . table 3 compares the performance of the best performing models with the unsupervised baselines for yelp and ama . we observe that , for both datasets , the performance drop is most prevalent on ama , while on yelp time is less pronounced .
table 3 shows the case - insensitive tokenized bleu score of our model on the wmt14 english - german translation task . from left to right , olrn , sru , atr and gru obtains higher bleu and atr scores compared to the other methods . the smaller training size and the shorter training time mean that lrn is better at decoding one sentence in seconds compared to other methods such as atr , while sru is worse at decoding more than two sentences in milliseconds . as shown in the second group of table 3 , the atr model is more useful for production use since it has more training instances and hence requires less time to train compared to lrn . table 3 also highlights the differences in performance between the methods for english and german translation tasks . though lrn obtains a lower case - inflation score compared to atr on newstest2014 dataset , it is comparable with the sru score on the german one . finally , the difference between gnmt and gnmt under - performs other methods as shown in table 3 .
table 4 shows the exact match / f1 - score on squad dataset . the models using only elmo and rnet * have better performance than those using only atr and sru . besides , atr has the advantage of having fewer parameters which results in a better match rate . finally , the number of parameters in the base set is less than that of atr , indicating the scalability of adding more parameters to the model to improve the performance . as shown in the next table , the combination of the parameter number of base . matches and f1 scores of all the models with the same parameter set is beneficial for scalability improvement . however , the biggest performance boost is obtained with the addition of elmo , which further boosts the model performance .
table 6 shows the f1 score of our model on the conll - 2003 english ner task . our lstm model obtains the best performance with a f1 - score of 90 . 56 on the ner test set . although the number of parameters in question is small , we managed to improve the model ' s performance with the help of a few additional parameters .
table 7 shows test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . with the base setting , elrn model significantly improves its performance on both snli and ptb tasks . with the additional ln setting , it verifies the effectiveness of the base model in both tasks .
table 1 shows the system performance on the word analogy task for english , german , french , dutch , russian and turkish . the results are presented in table 1 . our system outperforms the best previous approaches on every metric by a significant margin . retrieving the most important words for each sentence is easier than for the others .
table 4 presents the results of human evaluation . our system outperforms all the automatic systems except candela ( 30 . 2 % ) and h & w hua and wang ( 2018 ) on three out of the four metrics . the highest standard deviation among all is 1 . 3 points , which indicates that seq2seq is more than comparable with human in terms of overall quality . retrieval , on the other hand , is comparable to human in all metrics except for appropriateness , with a standard deviation of 2 . 0 points . overall , the quality of the summaries is high , with our system ranking in the top 2 or 3 for all metrics .
the performance of these models on the test set is shown in table 4 . we observe that for all but one of the four datasets , our model outperforms the other two when trained and tested on the training data set with the same set of features . for example , the difference between en and europarl is most prevalent in the " ted talks " dataset , while for " praxis " is less pronounced .
table 3 shows the test bias scores for the " ted talks " subset and " europarl " subset compared to the " other " subset for english , french , dutch , dutch and russian . the results are shown in bold . for english , we see p < 0 . 01 while for the other two sets it is p > 0 . 03 . for both sets we see lower p and r scores than expected by chance .
table 3 shows the test bias scores for " ted talks " and " sds " compared to " europarl " on the training data set of hotpotqa and " docsub " . from left to right , we can see that for both datasets , the average p < 0 . 01 and r > 0 . 005 for english and german , respectively , while for " europarl " is slightly higher than the average . table 3 also highlights the differences in test set performance between the two datasets when trained and tested on the data augmentation task . for example , for " ted talks " the model performs slightly better on both datasets than the one trained on " europarl " .
from table 1 , we can see that the average depth and the number of roots per row are the most important metrics for dsim and docsub . for europarl , maxdepth and mindepth are the only ones that show significant difference in performance between max and mindepth . for docsub , the difference between average and maxdepth is less pronounced , but still suggests that there is a need to consider the role of depth modeling in the development of hclust .
from table 1 , we can see that the corestructure of the hclust model is very similar to that of the original dsim model , with only a few differences . the average depth and the number of roots for each row is slightly different , however , with europarl having the advantage of having more data . from here we can further compare our model with the previous state - of - the - art on the five metrics . as can be seen , the difference in performance between max and mindepth shows that the size of the difference between the roots and the depth of the stems is not significant , however it does indicate that there is a need to consider the quality of the derivational selection for each relation .
in table 1 , we compare the performance of applying our principles to the validation set of visdial v1 . 0 . 7 with the enhanced version of lf in terms of ndcg % . the enhanced version exhibits a significant performance gain over the original version , which shows that the benefits of leveraging softmax loss are more than offset by the lower performance of residual loss . moreover , the p1 factor helps the model to further improve its performance with a noticeable drop in performance between baseline and enhanced version .
in table 2 , we show the performance of applying p2 and p1 to different ablative studies on the visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 . while p1 and p2 do not apply to all models , for those that do , it proves to be more effective than p1 alone . for those that only apply p2 , the performance is lower than the baseline but still comparable with the best baseline .
table 5 shows the performance on hard and soft alignments for the hmd - f1 model compared to prior works on both sets of documents . the wmd - bigram model outperforms the other two baselines on both soft and hard alignments . it achieves the best results on the " recall " and " de - en " datasets , while the " lemma " dataset gets the worst performance .
the results of baselines trained on the training data are shown in table 1 . the average bertscore score for all baselines is 0 . 685 , with the exception of ru - en , which is slightly higher than the others . sent - mover and w2v perform similarly to meteor + + and ruse ( * ) except for the fact that it requires more training data and therefore requires more data to set up the model .
the results are shown in table 1 . we can see that the bleu and meteor based baselines significantly outperform the baselines on all metrics except for the inferences for hotpot and sfhotel . sent - mover is indeed more useful than w2v for sentiment analysis since it eliminates the effect of noise in the translation path . the semantic features of smd seem to have a bigger impact on sentiment analysis than on translation performance , as shown in the second group of results . finally , the bertscore - f1 scores seem to indicate that the semantic features derived from smd are important for the task at hand , as measured by the improvement in f1 score from 0 . 078 to 0 . 082 on the smd dataset .
the results are shown in table 1 . word - mover and sentence prediction using the bertscore - recall and meteor based baselines achieves the best results . sentiment prediction using bert score - recall and meteor achieves the most consistent performance with a minimum of 0 . 939 on m1 and m2 while matching the performance of spice and leic . when using word2vec modeling , the performance gap between the wmd - 1 and w2v baseline decreases significantly with the addition of elmo and bert , which shows the degree to which the semantic information is localized within the word .
we show the results for m0 , m1 , m2 , m3 and m3 as well as m6 on the hidden test set of simnet . the results are shown in table 1 . para - para models seem to be more useful for prediction in low - supervision settings as compared to simply adding the word " para " . as expected , the performance drop between m0 and m6 with different combinations of lexical features is most prevalent for those using the shen - 1 pre - trained model . with the exception of m6 , all the other models benefit from the additional benefit of language modeling .
table 3 shows the transfer quality and fluency numbers for yelp and semanticnet . the results are shown in bold . semantic preservation and transfer quality are relatively high while fluency is low . we observe that yelp significantly outperforms the other two baselines in terms of transfer quality , and its m6 and m7 scores are both close to the best in the world . in addition , the semantic preservation numbers are close to those of google translate , but slightly lower than the others .
table 5 presents the results of human sentence - level validation for each metric for validation of acc and pp . it follows from table 5 that human judgments match more than 90 % of the time , which shows that the human judgments are more accurate in the low - resource settings . also , the difference between the human and machine ratings of semantic preservation and fluency is less pronounced for pp , but still significant .
the results are shown in table 6 . para - para model outperforms all the other models apart from shen - 1 when trained with only one type of language classifier . it achieves the best results with m0 [ italic ] + cyc , while the worst results with para - lang are seen in m1 , m2 and m3 .
results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences is shown in table 6 . our best models ( right table and acc ∗ ) achieve higher accuracy than prior work at similar levels of acc with different classifiers in use , so the training set size is less important than the actual number of tokens in use . multi - decoder and wrapper classifiers achieve higher bleus and acc than does simply transfer , which shows the advantage of redundancy in low - resource settings . with respect to transfer , simple - transfer and delete / retrieve , both cues yield better results than transfer with untransferred sentences . finally , with respect to language embeddings , combining the best features of wrapper and classifier achieves the highest acc . these results show that combining lexical features and the best classifiers can further improve the model ' s performance .
in table 2 , we report the percentage of reparandum tokens that were correctly predicted as disfluent for each type of nested disfluency . for example , the number of tokens for " rephrase " and " start " disfluencies is slightly higher than for " nested " but still considerably lower than " regroup " .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either the reparlabelled and non - reparandum forms . in addition , the fraction of tokens in each category that belong to each category is also shown in table 3 . reparandum length and repair length are the most important factors in predicting whether a disfluency will appear in the repair or in the reparation . function - function tokens are the least important ones , making up only a fraction of the total tokens .
the results are shown in table 1 . in particular , we show that the text - rich innovations model outperforms the single - input model and the text rich innovations model when trained and tested on the same dataset . moreover , the performance gap between the two is much narrower when training and testing on different datasets , with text rich and raw variants yielding different results .
the performance comparison of our model with the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . word2vec embeddings perform well compared to other approaches that rely on automatic neural networks ( rnn - based and cnn - based ) in terms of sentence prediction accuracy . however , the accuracy drop when using self - attention sentence prediction instead of automatic neural network prediction is much larger . this highlights the challenge of finetuning word2vec features to improve interpretability without sacrificing too many facts .
table 2 shows the performance of the different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . attentive neuraldater is better than all the previous models except maxent - joint . ac - gcn is comparable with the best previous model , but slightly worse than neuraldater . the joint model outperforms burstysimdater and applies the best feature set . as shown in the second group of table 2 , combining the best features from the two datasets improves the performance for both datasets .
table 3 compares the performance of our approach with and without word attention for this task . our approach obtains a significant improvement in accuracy compared to the original neuraldater model . with word attention , our model obtains 65 . 6 % accuracy and s - gcn is 63 . 9 % accuracy . with graph attention , the model achieves 65 . 2 % . with both attention methods , we get a significant increase in performance .
the performance on the test set is shown in table 1 . embedding + t and argument are stronger than both dmcnn and cnn , while jrnn is better than both cues . trigger and disambiguation remove some of the noise from the model performance , however , it does not improve significantly . all models show significant performance drop when trained and tested on the same dataset .
table 3 presents the results for argument identification and event prediction . our proposed method outperforms dfgn and f1 on both event and sentence prediction using the best performing features . for argument identification , we see that it is comparable to dfgn with only 0 . 9 % absolute difference . for event prediction , it is 0 . 7 % better than dfgn . on the other hand , cross - event prediction is only comparable with dfgn , with a gap of 2 . 5 % overall .
results are shown in table 3 . we see that , for english - only and spanish - only languages , the dev perp and test acc numbers are relatively stable while for both languages the dev wer and test wer numbers are significantly higher . fine - tuned and shuffled models produce significantly worse results than the original models . finally , the results are slightly worse than those of " shuffled - lm " and " fine - tuned - lm " . the best results are obtained by " all " model , which is derived from the best performing variant of the original model .
results on the dev set and on the test set using only subsets of the code - switched data are shown in table 4 . fine - tuned models perform well over both training sets with different distributions of the training data in the dev and test set . cs - only models show marked performance improvement over the naive fine - tuned model , which shows the advantage of finetuning the model during training .
the results are shown in table 5 . fine - tuned - disc improves the performance on the dev set and on the test set , while fine - tuned - lm does not . the difference in accuracy between the gold sentences in the gold sentence set and the monolingual ones is minimal , however we see significant difference in performance between the two sets for both languages . fine - tuning - disc reduces the noise in the dev and test sets , and the performance is comparable for both sets .
table 7 shows the precision ( p , r and f1 - score ) and recall ( r ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the type combined approach shows a significant improvement in precision and recall compared to the baseline model using only pure gaze features . furthermore , type combined features result in a higher f1 score , which shows that the model can further improve interpretability without sacrificing too many features .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features for the conll - 2003 dataset as compared to the baseline model using only pure gaze features . type combined gaze features result in a significant improvement in precision and recall compared to using type combined features alone , which shows that the model can further improve interpretability without sacrificing too many features .
results on the test set of belinkov2014exploring ’ s ppa test set are shown in table 1 . ontolstm - pp and glove - extended refers to the synset embeddings obtained by running autoextend rothe and schütze ( 2015 ) on wordnet 3 . 1 and wordnet 4 . 1 , respectively , while onto - ldstm applies the type - of - wordnet and token - based encodings from the original paper ( faruqui et al . , 2015 ) onto the new wordnet3 . 1 test set . further , the type and type of tokens used in the hpcd model are derived from the source documents , and the type of wordnet is derived from a derivational derivational subset of the original text . finally , the length of the tokens in question indicates the semantic nature of the lexical clusters in question . overall , the results are very similar across the set , with the exception of the last one , which shows that it is harder to extract the semantic information from the tokens .
table 2 shows the results for rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . ontolstm - pp , hpcd and oracle pp predictors give a significant performance boost which is expected in a single shot . with respect to uas performance , the system performs well with all the pp features in hand . further , the performance increase with the addition of onto - pp and ontohpcd predictors .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . while the model performs slightly worse without context sensitivity , it is still comparable to the original model with the same performance .
table 2 shows the bleu % and roc % scores for adding subtitle data and domain tuning for image caption translation in marian amun ' s final model on the flickr16 and mscoco17 datasets . the results are slightly better than the results with en - de and en - fr for both datasets , but still significantly worse than those with sub - subsfull data . as the results show , domain tuning helps the model interpret subtitle more precisely . finally , the improvement in rocu % is much larger with domain tuning compared to without . it can also be observed that , when domain tuning is added to the subtitle data , the model performs better overall .
we show the results for en - de and en - fr for flickr16 , flickr17 and mscoco17 when domain - tuned . the results are shown in table 1 . as the results show , when labels are added to the model , the performance of subs1m becomes better and the h + ms - coco model gets better .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . adding only the best five captions improves the results for all models except for those using marian amun . the performance of en - de models is slightly worse than en - fr while the performance of mscoco17 is slightly better . finally , adding autocap 1 - 5 improves performance for all but the multi30k model , which shows the advantage of finetuning word selection during training .
table 5 shows the bleu % and wow % scores for visual information integration using transformer , multi30k + ms - coco + subs3mlm and detectron mask surface , encoding and decoding of visual information is easier than en - de for both flickr16 and mscoco17 . while encoding information is more difficult than dec - gate for both datasets , it is easier for encoder and decoder to do so for flickr17 . finally , the wow scores are higher for encoding than decoding , indicating the advantage of using a more compact encoder .
" + ensemble - of - 3 " models outperform the models using " text - only " and " multi - lingual " features , the results are shown in table 3 . as the results of " visual features " show , the semantic features contribute less than the visual features , but still have a significant effect on the performance . the ensemble features - neutral approach outperforms the monolingual approach , adding ms - coco cues to the model boosts performance for both datasets , in particular , it boosts the results for flickr16 and mscoco17 by 3 . 45 points in the standard task formulation .
table 3 shows the results for english and german on the training set of translate + mtld . from left to right , en - fr - ht and en - es - ht represent the best performing translations on both mtld and ttr . the results are slightly different for german and french , however , with the former performing slightly better on ttr and mtld while the latter is inferior on mtld
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . en – fr and en – es have 1 , 472 , 203 and 499 , 487 parallel sentences , respectively , while en - es has 5 , 734 and 7 , 723 .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . for each language pair , we set aside a training word for each model and replace it with the correct one for the target language .
table 5 shows the bleu and ter evaluation scores for the rev systems . the en - fr - rnn - rev and en - es - smt - rev systems perform similarly to each other , with the exception of the case of rev2 . 7 , which is much worse . as the results show , once the transformation baseline is added , the performance gap between the two systems becomes much smaller .
table 2 shows the results for flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled segmatch is the one supervised with rsaimage . in both cases , the average recall is higher than rsaimage , indicating that the model is more suitable for production use . the mean mfcc rank of the vgs model is 0 . 2 , which means that it has higher recall than segmatch .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled u is the model supervised using rsaimage . in both cases , the average recall and mean mfcc are lower than that of rsaimage , indicating that the performance gain comes from a better model design . audio2vec - u also outperforms the other approaches in terms of recall and average mfcc , however , it has the advantage of training on a larger corpus , which results in a lower precision .
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . the dan classifier turns in a screenplay that is easily distinguishable from the original except for the edges at the edges . cnn turns on a on a sentence that is distinguishable between the two states of the original and the current state of the screenplay . rnn shows a marked improvement in performance with the addition of lexical information from cnn to rnn . it is more difficult for dan to distinguish between the edges of a sentence and the actual sentence , but easier for rnn to turn in a sentence containing those edges .
table 2 shows the pos changes in sst - 2 as a result of fine - tuning . the numbers indicate the changes in percentage points with respect to the original sentence . verbs and adjectives have increased , while the number of instances of nouns has decreased or stayed the same . rnp has increased by a large margin , from 69 . 5 % to 81 . 5 % . however , for adjectives , the picture is less clear , with a drop of more than 10 % in performance . predicate part - of - speech ( pos ) changes indicate that the vocabulary size has increased , but that the overall number of occurrences has not changed . in addition , the presence of the word " good " in the sentence has also increased , indicating that the semantic information in question has been expanded .
sentiment score changes in sst - 2 . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . in the case of positive sentiment , the flipped labels result in a significant increase in sentiment score , and in negative sentiment , a decrease of less than the positive score .
table 1 presents the results of the second study on the validation set of pubmed and sst - 2 . results show that compared to pubmed , sift is more interpretable and interpretable , while pubmed is less interpretable . results also show that it is easier for pubmed to pick out the good and the bad from the bad .
