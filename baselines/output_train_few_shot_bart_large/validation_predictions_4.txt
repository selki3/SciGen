table 2 shows the performance of the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . however , on the large movie review dataset , the recursive approach shows the best performance , with a performance improvement of 3 . 3 % over the iterative one .
table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization .
the max pooling strategy consistently performs better in all model variations . for example , conll08 achieves the best performance ( f1 . 15e + 01 ) with a maximum of 9 . 66e + 00 points , while softplus achieves a f1 . 57e + 02 point and sb achieves the second best performance with a dropout probability of 0 . 63e - 04 .
table 1 shows the effect of using the shortest dependency path on each relation type . the macro - averaged approach achieves the best f1 ( in 5 - fold ) with a fraction of a second better performance than the model - feature approach . in particular , the macro - adaptive approach achieves a f1 of 54 . 46 on the topic dataset , which is slightly better than the previous state of the art approach .
the results are shown in table 3 . we observe that the best performing model is the y - 3 : y model , which achieves the highest percentage of f1 and r - f1 on both datasets . it achieves a f1 score of 67 . 58 % and a r / f1 of 50 % on all datasets except for f1 , where it achieves the best performance on all but f1 .
table 1 shows that mst - parser achieves the best performance on all three test sets . it achieves 100 % accuracy on all test sets except for one , where it achieves 50 % . on the other two test sets , it achieves the highest performance .
table 4 shows the performance of our system on the essay vs . paragraph level . our system achieves the highest c - f1 ( 100 % ) on the paragraph level , with a mean performance of 60 . 40 ± 13 . 57 % and a median performance of 9 . 24 ± 2 . 87 . note that the mean performances are lower than the majority performances over the runs given in table 2 . however , the average performance is higher for the essay level .
the results are shown in table 1 . we observe that when the training data are cleaned , the original train is more likely to perform better than the cleaned one . however , this is not true for the test data . the original train performs better on the test set than the cleansed one on all metrics except for meteor and rougelstm .
table 1 shows the results for the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . the results are shown in table 1 . we observe that the original dataset contains more than twice as many textual references as the cleaned version , which indicates that the training data are more likely to contain errors than the test data .
the results are shown in table 1 . we observe that tgen + outperforms sc - lstm on all train and test datasets except for the meteor and rouge - l datasets . the difference between the original and the tgen − model is most pronounced on the meteor dataset , where the original model outperforms the original on all but one of the test datasets .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) . these results are shown in table 4 . adding and removing the missing and wrong values significantly reduces the number of errors , but does not significantly improve the performance .
table 3 shows the results of our approach on the performance of the single - domain and the ensemble models . the results are shown in table 3 . our approach outperforms the previous state - of - the - art approaches on both the single and ensemble domains , with the exception of tree2str .
table 2 shows the main results on amr17 . our model achieves 24 . 5 bleu points compared to seq2seqb ( beck et al . , 2018 ) and gcnseq ( damonte and cohen , 2019 ) in terms of the number of parameters in the ensemble and single parameterized models . however , our ensemble model achieves a larger number of parameterized points than the other two approaches , indicating that our model is more suitable for the task .
table 1 shows the results of our model on the english - german and czech datasets . the results are summarized in table 1 . our model outperforms all the previous approaches except for the bow + gcn model , which is the best performing on both the english and the czech datasets ( table 1 ) .
table 5 shows the effect of the number of layers inside dc on the performance of our model . our model achieves the best performance with a maximum of 6 layers , and a minimum of 3 . 5 layers . however , we observe that this is not the case for all blocks , and that the effect is less pronounced for some blocks .
table 6 shows the results of our model on the baselines with respect to gcns with residual connections . the results are shown in table 6 . our model outperforms all baselines except for dcgcn3 ( 27 ) , which shows that the residual connections do not affect the performance of the model . moreover , our model performs better than the previous state - of - the - art model ( dcgcn2 ) in terms of the number of connections .
the results are shown in table 1 . we observe that the best performing model is the dcgcn ( dcgcn ( 1 ) with a d of 10 . 9 and a b of 12 . 4 , which shows that it has the best performance across all metrics . the best performing models are the ones with the highest d and b scores , which show that it is possible to improve upon the performance of the previous state - of - the - art model by a significant margin .
table 8 : ablation study for density of connections on the dev set of amr15 . the results are shown in table 8 . we observe that removing the dense connections in the i - th and ii - th blocks significantly improves the performance of the model . however , the results are not statistically significant . the best performance is obtained for the dcgcn4 model , which obtains the best ablation result .
table 9 shows the results of the ablation study on the graph encoder and the lstm decoder . the results are shown in table 9 . the results show that the coverage mechanism used in the decoder achieves the best results , with the exception of the direction aggregation module , which achieves the second best result .
table 7 shows the results for initialization strategies on probing tasks . glorot and somo perform better than our paper on all but one of the probing tasks ( n ( 0 , 0 . 1 ) and subjnum , where somo achieves the highest score . our paper , on the other hand , achieves the second highest score on all the tasks except for topconst .
table 3 shows that the h - cmow method outperforms the previous state - of - the - art h - cbow on all metrics except for the length of the nested nested nested objects . it achieves the best performance on the nested objects , with the exception of concatenating nested objects with concatenated ones .
the results are shown in table 1 . we observe that the best performing method is cmow / 784 , which achieves 90 . 6 % improvement over the previous state - of - the - art cbow on all metrics except subj and mrpc . however , the performance of the hybrid method is slightly worse than the previous best state of the art cbow . it achieves a marginal improvement of 0 . 2 % on subj and 0 . 4 % on mrpc , but achieves a slight improvement on cmp .
table 3 : scores on unsupervised downstream tasks attained by our models . the results are shown in table 3 . our model achieves the best performance on sts12 , sts13 , and sts16 , with a relative improvement of 26 . 5 % and 42 . 6 % over the previous state - of - the - art approaches , respectively . hybrid outperforms both the cbow and cmow on all the downstream tasks except for sts15 , where it achieves the highest performance . we observe that the performance of our model is significantly improved with respect to cbow .
table 8 : scores for initialization strategies on supervised downstream tasks . glorot outperforms all the other approaches except for mpqa and trec in terms of the number of subjets and mrpc scores . our paper achieves the best performance on the supervised downstream task , with a score of 87 . 6 % on sst2 and 86 . 4 % on sts5 . on the sst5 and sts - b tasks , it achieves the highest performance .
table 6 : scores for different training objectives on the unsupervised downstream tasks . we observe that the cmow - r achieves the best performance on the sts12 and sts13 tasks , while the cbow - c achieves the highest performance on sts16 . the results are consistent across all training objectives .
table 3 shows the performance of our method compared to the previous state - of - the - art cmow - r and somo - based embeddings . our method outperforms both of the previous approaches in all but one case , with the exception of subjnum , where it performs slightly worse than the previous best performance .
the results are shown in table 3 . the best performing method is the cmow - r , which outperforms all the other approaches except for mpqa and mrpc . it achieves the best performance on all three datasets except for sick - e , where it achieves the second best performance .
the results are shown in table 1 . we observe that the best performance is obtained when trained on all loc and misc datasets , with the exception of τmil - nd , which is trained on only the loc dataset . moreover , the best performing system is the one trained on the misc dataset , with a performance improvement of 3 . 3 % over the best trained system .
table 2 shows the results on the test set under two settings . the results are shown in table 2 . we observe that the best performance is obtained by using the supervised learning approach , which achieves the highest f1 score . however , the best results are obtained when the training set is combined with the training data from the previous work ( table 2 ) . the best performing system is the τmil - nd ( model 2 ) with a f1 of 69 . 38 ± 1 . 59 and a rn of 15 . 03 , which is higher than the previous best performance ( 15 . 03 ± 15 . 13 ) . in addition , the highest performance is achieved by the supervised learning approach ( 42 . 42 ± 0 . 59 ) , which achieves a fn of 71 . 57 ± 0 . 15 and a rn of 83 . 12 ± 1 . 15 , respectively . finally , we see that the performance of the trained model is comparable to that of the unsupervised one .
table 6 shows that the g2s - gat model outperforms all the other approaches in terms of the number of instances in which the model is used in the training set . it also outperforms both the gat and gat - ggnn models in the performance test set . however , it is not comparable to the performance of the other models , as shown in table 6 .
table 3 shows that the g2s - gat model outperforms all the other models on the ldc2015e86 and ldc2017t10 datasets . it also outperforms the previous state - of - the - art models in terms of bleu and meteor scores . however , it does not outperform the best state of the art models on ldc2018t10 , which are reported in table 3 .
table 3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . g2s - ggnn achieves the best performance on the test set , with a bleu score of 31 . 23 % higher than the previous state - of - the - art model .
table 4 shows the results of the ablation study on the ldc2017t10 development set . the results are shown in table 4 . our model outperforms the previous state - of - the - art bilstm and meteor models in terms of both size and performance . the model with the best performance is the one with the highest bleu score , which shows that our model is more suitable for ablation .
the results are shown in table 1 . g2s - gin outperforms all the other models on all metrics except for the length of sentences . it achieves the best performance on all but one metric ( sentence length ) and is the only one that achieves a significant improvement over the previous state - of - the - art model . it achieves a performance gain of 0 . 51 % over the baseline model on all metric except for sentence length .
table 8 shows the fraction of elements in the output that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . g2s - gat outperforms s2s by a large margin , as shown in table 8 . the difference is most pronounced when the tokens are not present in the input , which indicates that the model is more sensitive to missing tokens .
table 4 shows the pos tagging accuracy using different target languages on a smaller parallel corpus ( 200k sentences ) . the results are shown in table 4 . pos tags are significantly better than sem on the smaller corpus , with an accuracy of 88 . 7 % compared to 85 . 2 % .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . word2tag achieves the best performance with a baseline of 91 . 11 % and 91 . 41 % , respectively . unsupemb achieves 95 . 06 % and 95 . 41 % pos accuracy with a lower bound of 81 . 41 % . the best performing embeddings are the ones with the most frequent tags .
table 4 shows the results of our model on the pos and sem datasets . our model achieves the best performance on all metrics except for pos tagging accuracy , where it achieves the second - best performance .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 . the pos tagging accuracy is significantly higher than the res tagging accuracy , which is consistent with the results of the previous study ( see table 5 ) .
table 8 shows the performance of the attacker on different datasets . results are on a training set 10 % held - out . we observe that the attacker performs significantly worse than the corresponding adversary on all datasets except pan16 . the difference between the attacker ’ s performance on pan16 and pan16 is less than 0 . 1 points . on pan16 , the attacker achieves the best performance on all three datasets .
table 1 : accuracies when training directly towards a single task . the results are shown in table 1 . the pan16 model outperforms pan16 on all metrics except for gender , age and gender .
table 2 shows that gender and race are the most frequently reported to leak in pan16 , followed by age and gender . gender is the most likely to leak , while race is most frequently the most common to leak .
table 3 shows the performance on different datasets with an adversarial training . the results are shown in table 3 . we observe that the gender and race features are the most difficult to predict , but the age and gender - based features are relatively easy to predict . gender and race are the two most difficult features to predict and predict , however , we observe that they are also the most likely to leak information .
table 6 shows the performance of the protected attribute with different encoders . the rnn encoder performs better than the guarded encoder on the leaky and guarded embeddings .
table 3 shows the results of our work on the training set . the results are summarized in table 3 . lrn outperforms all the other approaches except for the work of yang et al . ( 2018 ) , which shows that lrn is more suitable for dynamic tuning . it achieves the best performance on the ptb and wt2 baselines , with the exception of the lrn work , which achieves the second - best performance on both baselines . in addition , it achieves the highest performance for the wt2 base and dynamic baselines on both datasets . however , it does not outperform the work by a large margin . this is evident from the results obtained in the previous work , where lrn achieves a performance gap of 3 . 3 % over the previous state of the art .
table 1 presents the results of our work on the lstm and gru models . the results are shown in table 1 . we observe that the work performed by the gru model is comparable to that of the previous work by rocktäschel et al . ( 2016 ) in terms of the number of params and time taken to perform the task . however , the results are slightly worse than those of the other models , as shown in the table . in particular , the work done by gru and sru is significantly worse than that by the other approaches .
table 3 presents the results of our work on yelppolar and amapolar . the results are summarized in zhang et al . ( 2015 ) in table 3 . we observe that the work performed by this approach outperforms the previous work by a large margin . it achieves the best results on both datasets , with the exception of amafull err , which is slightly higher than the previous state - of - the - art .
table 3 shows the case - insensitive tokenized bleu score on wmt14 english - german translation task on tesla p100 . the gnmt model outperforms all the other approaches except for olrn and sru in the case of case - inensitive tokenization . gnmt achieves the best performance on the translation task , with a case insensitive tokenized score of 26 . 67 % compared to the previous state - of - the - art lrn model ( 26 . 57 % ) and srn ( 25 . 40 % ) . the case - sensitive tokenized gnmt score is slightly higher than the previous best performing lrn and atr models .
table 4 shows the exact match / f1 score on the squad dataset . the results are shown in table 4 . lrn outperforms all the other approaches in terms of f1 score , with the exception of atr , which achieves the best performance . moreover , lrn achieves the highest performance with the parameter number of elmo , which indicates that it is more suitable for the task at hand . however , the performance of lrn is slightly lower than that of the previous state - of - the - art lstm and gru models . as shown in wang et al . ( 2017 ) , lrn obtains the highest accuracy with elmo and sru achieves the lowest accuracy with lrn . table 4 also shows that the best performing approach is rnet , which obtains an accuracy of 71 . 41 / [ bold ] 79 . 83 on average .
table 6 shows the f1 score on conll - 2003 english ner task . the lstm model achieves the best performance with a f1 of 90 . 56 . lrn , on the other hand , achieves the highest f1 with a score of 89 . 61 . we observe that lrn performs better than all the other approaches except for atr , sru and gru .
table 7 : test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . the results are shown in table 7 . lrn outperforms elrn and glrn on both snli and ptb tasks . however , the difference is less pronounced for snli tasks , where the lrn model performs better . the difference is more pronounced for ptb , where it performs slightly worse than the other models .
table 1 shows the performance of human and human - based systems with and without oracle retrieval . the results are shown in table 1 . human - based system outperforms all the other systems except for oracle , with the exception of mtr , which outperforms both human and retrieval by a large margin . the performance gap between human and oracle is small , with human outperforming all but oracle by a significant margin .
table 4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that human evaluation is more accurate than the best automatic system . the best performance is achieved by candela ( 2018 ) , which achieves a score of 44 . 6 % higher than human on all three metrics . seq2seq ( 2018 ) and h & w hua and wang ( 2018a ) achieve the best performance on all metrics , with the exception of content richness .
table 3 shows the performance of our approach compared to the previous state - of - the - art approach . our approach outperforms all the other approaches except for ted talks , which shows that our approach is more suitable for the task at hand . the performance gap between our approach and the previous one is small , but still significant .
table 3 shows the performance of our approach compared to the previous state - of - the - art approach . our approach outperforms all the previous approaches except for ted talks , which shows that our approach is more suitable for the task at hand .
table 3 shows the performance of our approach compared to the previous state - of - the - art approach . our approach outperforms all the previous approaches except for ted talks , which shows that it is more suitable for the task at hand . the performance gap between our approach and the previous one is small , but still significant .
table 1 shows the performance of the europarl embeddings with respect to the number of roots and the max and mindepth metrics . the average depth is 11 . 05 and the maximum depth is 3 . 46 , respectively . in particular , the maxdepth metric is significantly higher than the mindepth metric , indicating that it is more sensitive to the presence of nested roots .
table 1 shows the performance of the europarl embeddings on the dsim and docsub datasets . we observe that the number of roots generated by a given number of numberrels is significantly larger than that by a factor of 2 . 29 on the hclust dataset , but smaller than that on the slqs dataset . on the other hand , on the docsub dataset , the total roots are larger than those of the other two datasets .
table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . the results are shown in table 1 . the enhanced version of our approach outperforms the baseline model by a significant margin . in fact , the enhanced version outperforms both the baseline and the baseline by a margin of 3 . 3 points . we observe that the enhanced model is more sensitive to the question type , answer score sampling , and hidden dictionary learning than the baseline one .
table 2 shows the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . however , the performance of p2 on the baseline validation set is significantly higher than that of p1 .
table 5 shows that hmd - f1 + bert outperforms ruse on both soft and hard alignments , with the exception of de - en , where ruse performs slightly better than wmd - unigram and ruse .
the results are shown in table 1 . the average bert score is 0 . 716 , which is slightly higher than the previous state - of - the - art meteor + + baseline ( 0 . 719 ) and ruse ( 0 . 624 ) . we observe that the sent - mover method outperforms all the baselines except ruse by a significant margin . it achieves the best results on all metrics except for ruse , which achieves the highest performance on all but one of the three metrics .
the results are shown in table 1 . we observe that bleu - 2 outperforms meteor and bertscore - f1 by a large margin , with the exception of the sfhotel setting , which shows a significant drop in performance . also , we observe a drop in the performance of smd and w2v on the sent - mover setting as well .
the results are shown in table 1 . word - mover performance on the leic and spice baselines is significantly worse than those on the other baselines . we observe that word - mover performance is significantly lower on the leic baseline than on the spice baseline . in addition , the performance is lower than that of the bert score - recall baseline .
the results are shown in table 1 . para + para improves the performance of the shen - 1 model by 2 . 7 points over the previous state - of - the - art model , but it does not improve the performance for the other two models . in particular , the results show that the para - para model outperforms all the other models except for m0 , where it achieves the best performance .
table 1 shows the results of our model on the yelp dataset . the results are shown in table 1 . we observe that our model outperforms the previous state - of - the - art on all metrics except for transfer quality and semantic preservation . our model achieves a transfer quality improvement of 0 . 7 points over the previous best state - ofthe - art model . it also improves the performance on the semantic preservation metric by 0 . 3 points .
table 5 shows the results of human sentence - level validation on yelp and yelp lit . for each dataset , we use the spearman ’ s ρ ρ b / w metric and human ratings of semantic preservation and fluency . the results are shown in table 5 . for yelp , the human ratings are higher than those of the machine , indicating that the human judgments are more likely to match the predictions of the neural network .
the results are shown in table 1 . para + para improves the performance of the shen - 1 model by 2 . 3 points over the previous state - of - the - art model , but it does not improve the performance for the other two models . the difference is most pronounced in the m0 and m6 regions , where the para - para model outperforms all the other models except for the one with the best performance .
table 6 shows the results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . the results are shown in table 6 . our best models ( right table ) outperform the best previous work by a large margin . we observe that our best model achieves the highest acc with the highest number of transferred sentences ( 31 . 9 % ) and the lowest number of untransferred sentences ( 22 . 8 % ) , indicating that our classifiers perform better than the best prior work . however , our best classifier achieves higher acc with a lower number of transfers , indicating that we need to improve our classifier to achieve higher acc .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . for nested disfluencies , the number of disfluency tokens is higher than for repetition tokens , indicating that the disfluences are more difficult to detect . for rephrase tokens , the percentage is slightly higher than the overall number , but is still higher than that of repetition tokens . for both types , the best performance is obtained by using the rephrase and the restart tokens .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) or in neither . for disfluency that contains a function word , the content word is more likely to belong to the function - function category , while the content - content category is less likely to be used in the repair category .
the results are shown in table 1 . we observe that the best performing model is the one with the best performance on the test set with the highest number of innovations . moreover , the model with the most innovations outperforms all the other models in terms of dev and test mean . the best performance is obtained when the model is combined with the innovations .
performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . our word2vec embeddings achieve the best performance on the micro f1 test set ( table 2 ) . our model achieves the highest accuracy ( 83 . 43 % ) compared to the state of the art rnn and self - attention algorithms . however , it is slightly better than the selfattention algorithm , as shown in table 2 . our model shows that it is better able to distinguish between unrelated and unrelated sentences .
table 2 : accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . for example , the ac - gcn model outperforms the previous state - of - the - art neuraldater by a large margin . however , the performance of the attentive neuraldater is comparable to that of the previous best - performing method .
table 3 shows the performance of our neuraldater with and without graph attention . the results are shown in table 3 . our approach achieves the best performance on the word attention task , with an accuracy of 61 . 9 % compared to 63 . 6 % for the oe approach .
the results are shown in table 1 . we observe that the best performing model is the jmee embeddings , which outperforms all the other approaches except for the dmcnn and cnn models in the 1 / 1 and 1 / n stages . the best performing approach is the jrnn , which achieves the best performance in both stages .
table 1 shows the results of our model on the training data . the results are shown in table 1 . our model outperforms the previous state - of - the - art in all but one case , with the exception of cross - event , where it is slightly worse than the previous best state of the art . we observe that our model is better at detecting and classifying false positives and false negatives than previous models .
the results are shown in table 3 . the best performing language is spanish - only , followed by english - only and french - only . we observe that the performance of these languages is comparable to those of the best - performing english - based languages . however , the performance gap between the best performing and worst performing languages is less pronounced in english , where the best performance is obtained by using the lexical embeddings of both english and french . as a result , we observe that these languages are comparable in terms of dev perp .
table 4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the fine - tuned approach outperforms the cs - only approach by a significant margin . the results are shown in table 4 . fine - tuned training achieves the best results on both the dev and test sets .
table 5 shows the performance on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) . the results are shown in table 5 . the fine - tuned - disc approach achieves the best performance on both the dev and test sets , with an overall score of 75 . 33 % and 75 . 87 % , respectively .
table 7 shows the results of using type - aggregated gaze features on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . the results are shown in table 7 . the type combined approach shows a significant improvement in precision and recall compared to the baseline approach .
table 5 shows the results of using type - aggregated gaze features on the conll - 2003 dataset . the results are shown in table 5 . the type combined approach shows a significant improvement in recall and precision compared to the baseline approach .
table 1 : results on belinkov2014exploring ’ s ppa test set . syntactic - sg and glove - extended embeddings outperform lstm - pp on the test set , with the former having the best performance . the results are shown in table 1 . hpcd ( full ) is from the original paper , and it uses syntactic skipgram . on the other hand , ontolstm is the best performing system on the original test set and the latter has the worst performance .
table 2 : results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 . hpcd and ontolstm - pp ( full ) outperform all the other approaches except for the lstm system , which outperforms all the others . in addition , ontolpstm outperforms the other systems in terms of ppa acc . and acc . ( table 2 ) .
table 3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . we observe that the ppa acc . score of the full model is higher than that of the attention model .
table 2 : adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun . the results are shown in table 2 . multi30k achieves the highest bleu % score ( 66 . 7 % ) compared to en - fr and mscoco17 ( 62 . 1 % ) and en - de ( 59 . 6 % ) . adding domain tuning improves the performance of the multi30k model by 2 . 3 bleus compared to the original model . in particular , the domain - tuned model performs better than the original on all but one of the three datasets ( marian amun et al . , 2017 ) . the performance improvement is most pronounced on the flickr16 dataset , which shows that domain tuning is beneficial for caption translation .
table 3 shows that domain - tuned h + ms - coco outperforms domain - domain tuning on all datasets except for en - de and flickr16 , where it is slightly better than the baseline . domain - tuning improves the performance of the subs1m dataset by 2 . 7 % over the baseline , and by 3 . 3 % over flickr17 .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . the results are shown in table 4 . adding automatic captions improves the performance of marian amun ’ s multi30k and en - de embeddings , but it does not improve the performance for the en - fr and mscoco17 embedding systems . the performance of the multi - 30k embedding system is slightly better than that of en - fran , but still slightly worse than en - d . in addition , the performance is slightly lower than the performance achieved by en - regexp , which shows that it is difficult to learn the best captions for all 5 images .
table 5 : comparison of strategies for integrating visual information ( bleu % scores ) . all results using transformer , multi30k + ms - coco + subs3mlm , detectron mask surface , and mscoco17 are shown in table 5 . the enc - gate + dec - gate approach outperforms the en - de approach in terms of the bleu % score . however , it is slightly worse than the dec - gate - based approach , as shown in fig . 5 . the performance gap between en - fr and mscoco - based approaches is less pronounced when using the transformer - based embeddings .
the results are shown in table 3 . the results show that the multi - lingual detectron outperforms all the other approaches except for the ms - coco model , which shows that it is more sensitive to visual features . however , it is less sensitive to text - only features , as shown in the results of the previous study .
table 3 shows the results of our approach on the yule ’ s i and mtld datasets . the results are shown in table 3 . our approach outperforms all the approaches except for en - fr - smt - ff , which achieves the best performance on the mtld dataset . however , it does not perform as well on the ttr dataset .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . for each language pair , we see that there are more than 1 . 5 times as many parallel sentences for the train and test splits as for the development splits .
table 2 : training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 . we observe that our models perform better on the english and spanish datasets than on the trg dataset .
table 5 : automatic evaluation scores ( bleu and ter ) for the rev systems . the results are shown in table 5 . our system outperforms all the other systems in terms of automatic evaluation scores , with the exception of the en - fr - rnn - rev system , which has a lower bleu score , but a higher ter score . we observe that our system is more suitable for the task at hand .
table 2 shows the results of our model on flickr . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled rsaimage is the one from the rsaimage dataset . the results are shown in table 2 . the vgs model achieves the highest recall on flickr , with a mean mfcc of 0 . 8k .
table 1 shows the results on synthetically spoken coco . the results are shown in table 1 . the vgs model achieves the highest recall @ 10 and the highest chance to recall on rsaimage , with a median rank of 6 . 0 . however , it is significantly less likely to recall rsaimage than the rsaimage model , which achieves a mean recall of 27 . 4 % and a median chance of 0 . 5 % .
table 1 shows the results for the different classifiers on sst - 2 . we report further examples in the appendix . for example , the cnn classifier turns in a screenplay that is at the edges of the screenplay , and the rnn classifiers turns on a on ( in in the the the edges ) and a on curve ( in the the curves ) when the screenplay is in the center of the screen . for the dan classifier , it turns on the on curve when a screenplay is at its edges , and turns in the edges when it is in its center .
table 2 shows the results of fine - tuning in sst - 2 . the numbers indicate the changes in percentage points with respect to the original sentence . the number of occurrences has increased , decreased or stayed the same . the last row indicates the overlap with the original sentences , indicating that the number of words in the final sentence has not changed . a score of 0 indicates that the finetuned rnn has not improved the quality of the sentence , while a score of 1 indicates that it has improved the performance of the rnn . the final row shows the percentage points that the rnp has increased or decreased over time .
table 3 shows that the sentiment score increases in positive and negative sentiment when negative labels are flipped to positive . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the rnn model outperforms the cnn model in terms of sentiment score .
table 1 presents the results of the experiment on pubmed and sst - 2 . the results are shown in table 1 . our approach outperforms all the other approaches except corr and pubmed by a large margin .
