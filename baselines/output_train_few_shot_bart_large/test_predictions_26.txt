table 3 presents the results on the eds and psd datasets . we present the results of the best performing models on the basic and final sets . the results are presented in table 3 . the basic set shows that eds models perform well on both the semantic and syntactic level , with the exception of the dm case . the psd case is qualitatively very similar , with a gap of 10 . 3 points from the amr baseline .
in the en - de news commentary section , we compare the results of different approaches for bert dev , biobert dev and snli . the results are shown in table 3 . in general terms , the smaller performance gap between snli ( s ) and mednli ( m ) indicates that the original model design performed better for the task at hand , but when expanded , it suffers from severe overfitting since the size and type of training data are different . moreover , the accuracy gap between the original and expanded models is much larger . for bert , the difference between the average and the average score of the two sets is minimal , but significant with respect to test set performance .
distill and no - project show lower performance on the sst2 task compared to distill ( see table 2 ) . however , when using only one distill seed , the model no - distill shows higher performance than distill . also , no - disambiguation reduces the number of tokens in a sentence , which results in higher performance . negations are relatively rare , but still significant , lowering performance to less than brevity .
the accuracies of the baseline and elmo ( over 100 seeds ) on non - neutral sentences are also shown in table 3 . the difference in accuracies between the thresholds is minimal , however we see significant difference in performance between no - distill and no - project models , indicating that the use of pre - trained word embeddings can significantly improve the performance of the model . also , we notice that the no - disambiguation method shows a slight improvement over the baseline model , showing that the ability to distinguish between positive and negative sentiment is relatively less difficult .
the most interesting features are the label descriptions and the f - score for both tags . both tags are presented in table 1 . the first set of features include the label description and the label f scores . glove tags significantly outperform df tags for both datasets with a gap of . 31 points from the last state - of - the - art . when trained with df tags , both the tn and tf tags show significantly higher f scores for both groups . as shown in the second set of table 1 , the ability to select the correct concept input and the correct entity input via a label is the most important factor in the derivational performance of the model . adding entity tags significantly improves the f scores of both labeled and unlabeled tags .
the most interesting features are the label descriptions and the f - score for both tags . both tags are presented in table 1 . the first set shows the tn and rn ( tn + rn ) of both tags with the same label . the second set contains the label description and f - schemas . glove tags are notably more gender - neutral , both for the original and the debiased embeddings . they yield significantly higher f scores than df tags , as shown in the second set , the semantic information injected into the tags by the additional cost term is relatively small , making it more suitable for use in a production setting .
the experimental results on the topic science and topic - wiki datasets are shown in table 2 . the results show that our approach significantly outperforms the previous state - of - the - art on all three datasets . specifically , our method results in a significant improvement on the f1 scores for topic science , with an rn of 1 . 012 / 2 . 071 and rp of . 884 / 7 . 028 on the two comparison datasets . on the topic - science dataset , our f1 score increases by 3 . 36 / 0 . 012 and 6 . 45 / 4 . 55 points compared to the last published results by gong et al . ( 2018 ) . adding idf and tf improves the results for both topic - and domain - aware embeddings , however , it does not improve the generalization ability for label labeling . our model achieves a marginal improvement of 2 . 8 / 3 . 0 points on topic - awareness with the combination of tf and idf ,
table 3 presents the results on the cnn and lstm datasets . we show the full , partial and final scores of all models . the proposed bl + glove model ( jeffrey et al . , 2014 ) achieves state - of - the - art results , improving upon the baseline model by 3 . 8 points in the dict and mrr . however , it performs slightly worse than the baseline in terms of cnn full score and misspelling correct answers .
the results are shown in table 4 . we observe that the iwslt models perform comparably to state - of - the - art counterparts on all three datasets . on the wmt en - de bleu and wmt de - en speedup datasets , the average speedup is close to the level of the best western - trained systems while on the speedup dataset it is considerably higher . with respect to sat , the model performs slightly better than the baseline on all datasets indicating that the training set size and the number of iterations used to train the model are important factors in the model performance . however , the difference between the accuracy on the wslt and the sat datasets is minimal , with the difference being less pronounced for the wmw en - fr dataset .
hidden size is the most important factor in our model ' s performance ; without it , we get an accuracy drop of 1 . 36 points from the last published results ( s - lstm ) . with ⟨ s ⟩ , our model gets an accuracy gain of 3 . 66 points compared to the previous state - of - the - art .
we further analyze our model with respect to the stacked and unstacked cnn models . for bilstm , we see that the stacked model achieves the best results with an acc @ k9 score of 81 . 53 , while the number of times a cnn is the most important factor in predicting accuracy . transformer ( n = 6 ) models show lower performance than the lstm and s - lstms , indicating that the size and type of training data used in the pre - training data are important .
the performance of our model on the movie review dataset is shown in table 4 . our stacked bilstm model achieves 8 . 53 % higher accuracy on average compared to the previous state - of - the - art on the training set . moreover , the accuracy increase over previous models is 4 . 18 % higher on the test set with a larger training set size .
the results are shown in table 2 . the first set shows that the generalization ability of the data models is high across all domains , with the exception of music , the sub - category in which we see the most performance improvement is food domain , in general terms , all the models perform comparably to each other except for the one that performs in the food domain . sub - categories are broken down in terms of time taken to capture each image , with some performing much better than others . table 2 shows the performance of the bilstm and slstm models compared to the previous state of the art . in all but one case , the comparison of the two sets , food and music , is statistically significant ( table 2 ) with respect to time spent on each sub - coder , with respect to transfer learning experiments , the results are slightly less clear , perhaps the most striking thing about the transfer learning method is that it eliminates the reliance on word embeddings . rather than adding them as appendices , the model can use them as labels for the objects inside the decoder , thereby further improving the performance . finally , we notice a slight drop in performance between music and food domains , when we switch from one dataset to the other , we observe that the performance gap between the two is narrower . perhaps this indicates that the focus on food is less important than the other aspects of the model , further , the size and type of bowls used in the kitchen are the most important factors in boosting performance ,
results in table 6 show that the stacked bilstm model achieved the best performance with a final accuracy of 97 . 55 % on the ptb ( pos tagging ) . however , we observed that the performance gap between stacked and unstacked models is narrower , with an accuracy drop of 0 . 99 points compared to previous work .
the results are shown in table 1 . we observe that the bilstm model achieved the best results with a f1 score of 91 . 97 on the training and test set . by further adding out - of - train models , our results show that the model can further improve its performance with a stepwise improvement of 1 . 59 points on the transfer task .
table 2 shows the e2e test set results . the best model on the development set is thomson reuters ( p = 0 . 3 ) while the best on the original test set is harvardnlp & h . e . elder ( p ( cid : 28 ) 0 . 4 ) . the difference in bleu score between the two sets is less pronounced for the original set , but still suggests some degree of performance improvement by leveraging syntactic or semantic information . word embeddings outperform character and challenge , but do not exceed the upper boundary of the boundary of challenge range , which suggests some reliance on syntactic information . character and challenge are qualitatively very similar , but the difference between them is narrower , p < 0 . 001 . overall , both cues yield strong baselines comparable to the strongest ontonotes - trained systems ( cf . table 2 ) . in fact , word2e outperforms challenge and challenge on both sets , showing the syntactic and semantic information discussed in section 4 . 3 overlap .
table 3 shows the ablation results on the development set of webnlg . our own model obtains the best result , outperforming all the other systems with a gap of 10 . 3 % on the rouge - l challenge . on the challenge set , it achieves the highest average ± sd of 62 . 2 % , which shows that it can easily distinguish between the true response and negative responses . the difference between the average and the lower bound is less pronounced for other systems , indicating that the semantic features discussed in section 4 . 3 . 3 hold high significance for the task .
table 4 shows that increasing the number of layers in synst ’ s parse decoder significantly lowers the speedup while marginally impacting bleu . additionally , random sampling k from { 1 … 6 } during training boosts bleu significantly with minimal impact on speedup .
table 4 shows the e2e and webnlg development set results in the format avg ± sd . the results show that bleu and rouge - l outperform the best performing standard embeddings ( hochreiter and schmidhuber , 1997 ) in terms of human evaluation . however , the difference is less pronounced for webnlg , showing that it is better at selecting the correct human reference and generating the correct output .
the results are shown in table 4 . overall , we see that the content errors and accuracy are relatively high while the accuracy is low . however , the accuracy remains relatively consistent , indicating that our model is well - equipped to handle the variety of errors that arise from mixing source and target labeled utterances .
table 1 shows the e2e human , word and character count on the test set , and the number of tokens in the unique sents for each word . we observe that the average human and word count are relatively similar , with the exception of the case of " human " . however , for the unique words , the difference is much larger , with an average human count of 1 . 3 times higher than that of a word or a character . table 1 compares the performance of the 1 - 3grams model compared to the previous state of the art . in general terms , the smaller size difference indicates that the accuracy obtained by training on the one - gram model can be improved with a reasonable selection of the lexical resource from which the character was derived .
the results shown in table 7 show that template - 1 and template - 2 produce similar output , with the exception of the case of " content and language " . however , when trained with synthetic training data , the output quality drops significantly , meaning that the model cannot rely on the syntactic cues from one template and cannot learn the task well .
table 1 : experimental results of abstractive summarization on gigaword test set with rouge metric . the top section is summaries by prefix baselines , the second section is recent unsupervised methods and ours , the third section is state - of - the - art supervised method along with our implementation of a seq - to - seq model with attention , and the bottom section is our model ’ s oracle performance . our approach outperforms all the recent approaches except for schumann et al . ( 2018 ) . while the contextual match baseline achieved by contextual oracle outperforms our approach , it does not exceed the state - ofthe - art on lead - 75c and rl , which shows the advantage of finetuning word embeddings during training . further , our approach exceeds the strong baselines of fevry and phang ( 2018 ) and schumann ( 2018 ) , by a margin of 3 . 59 points .
table 2 shows the experimental results of extractive summarization on a google data set . the first section shows that the contextual match model outperforms the unsupervised baseline , and the second section shows the results of filippova and altun ( 2013 ) using the best performing f & a algorithm . in the third section , zhao et al . ( 2018 ) shows that contextual matching is beneficial , improving upon the previous state - of - the - art by 3 . 8 points , though still performing substantially worse than the filippova et al . , ( 2013 ) . the fourth section highlights the performance gap between the best and the worst performing approach using a supervised baseline . note that the difference between the f1 and cr scores indicates that the effectiveness of contextional matching is high .
table 3 compares the results of different model choices . our model achieves the best results with respect to extractive f1 score , r2 and r2 score . it closely matches the performance of the best previous state - of - the - art model , temp10 , with only 0 . 43 % absolute difference . moreover , it achieves the highest score on extractive cr score , with a total of 62 . 59 % absolute improvement over the previous state of the art . we notice that the model chosen by temp5 , while performing slightly better on abstractive and extractive rl , is inferior on subtasks that are considered in section 4 . 3 . further , our model exhibits severe over - fitting since it is unable to distinguish between the features of the correct and wrong responses .
the official evaluation results of the submitted runs on the test set are shown in table 3 . the first set shows that the sub - task completion ability of the classifiers is relatively high while the ext . task performance is lower than the classifier performance .
table 3 compares the f1 and exact match comparisons of predicted chunk sequences ( from the parse decoder , ground - truth chunk sequences from an external parser in the target language , and chunk sequences obtained after parsing the translation produced by the token decoder . the results show that when the two decoders jointly train their tokens , the accuracy obtained by the gold parsing method increases by 3 . 23 % on average .
uniform sampling and teacher - forcing achieve better results overall , with the former achieving 0 . 8530 points improvement on the sst and 0 . 8576 points on the sts14 dataset . the difference between uniform and always sampling is less pronounced for sst14 , but still suggests some advantage to using sampling with randomization . with respect to sick - r , the automatic decoder achieves gains of 3 . 6 points on average compared to the baseline with the use of only one decoder . sst - e is stronger than both uniform sampling and teacher - forcing , showing the advantage of finetuning word embeddings during training . we observe that automatic decoding achieves a better overall performance than both sampling and forcing , with a gain of 2 . 4 points overall compared to that with the baseline .
the results are shown in table 3 . first , we report the msrp ( acc / f1 ) and trec ( metric repel repel ) on the four domains and the number of tokens in each sentence . rnn achieves the best results with an accuracy of 0 . 8530 on the sst dataset and 0 . 56 on the trec dataset . with respect to the domains of sentence representation , the rnn model achieves the highest accuracy with a 3 . 8 / 8 . 2 % boost on average compared to the previous state of the art .
table 2 shows the bleu scores for training nmt models with full word and byte pair encoded vocabularies . full word models limit vocabulary size to 50k . byte pair encoded models allow more than 300 words per label , but still perform substantially worse than the bpe model . with a vocabulary size of 32k , the performance gap between bpe 16k and 32k is narrower than that with bpe 32k .
the results are shown in table 1 . our model obtains the best performance with a ppl of 91 . 6 % , significantly higher than the previous state of the art .
the most representative models are the char - cnn and syl - concat networks . the results are shown in table 1 . overall , the performance of the models is very similar , with the exception of that of syl - cnn , which is significantly worse . when we switch to data augmentation , we see that the performance gap between the two sets is much smaller . syl - sum is stronger than both the original and the disambiguated cnns , and it achieves state - of - the - art results on all datasets except for es .
we replicate the experiments from table 5 with different subsets of the original lstm embeddings . the results are shown in table 5 . the smaller size and the higher precision of the dlm shows that the similarity between the original and the variant rhn can be overcome by a reduction in repetition .
table 1 shows the test set ' s performance on adding titles to premises . the difference in support and claim accuracy between esim and transformer is minimal , however it is significant enough to show a significant drop in performance .
table 2 shows the test set ' s performance on the four scenarios . support accuracy is . 591 , . 673 and . 511 , respectively , with a gap of . 005 points from the previous state of the art . transformer performance is . 846 , . 394 and . 609 compared to . 823 by esim on the fever title one and title five datasets , respectively .
in table 3 , we report the percentage of evidence retrieved from the first half of the development set . our system trialled on the fever baseline shows that it can easily distinguish between the true response and negative responses . furthermore , the effectiveness of our method is proved by an increase in the accuracy rate from 66 . 1 % to 81 . 8 % .
table 4 shows the fever scores of the various systems that we trained on the test set . all use ne + film retrieval . the system that obtains the highest score is the oracle .
projection accuracy for the isolated example experiment is shown in table 2 . our model ( all ( 38 ) vs . new ( 7 ) pairs ) shows that the model has 44 . 7 % accuracy on average compared to the previous state - of - the - art .
table 1 shows the in - vocabulary and out - ofvocabulary pairs for english . the results show that in all but one case , the difference between the average number of responses in each invocabulary pair is minimal , with the exception of the case of oov . table 1 compares the performance of existing and new vocabulary pairs . previously , all pairs trained on the same dataset performed identically , except for those that were trained on different vocabularies . since the only change in performance between pairs between oov and non - ov is small , we show only the results for in - voicabulary pairs and the previous performance of the oov dataset .
the results are shown in table 3 . our model ( tl2rtl ) improves upon the strong lemma baseline by 3 . 8 points in the te3 task . it achieves the best results with a f1 - score of 60 . 2 on the three tasks .
table 3 shows the results for all and half - formed responses . all onion and all onion half - and - half responses are statistically significant while the legal onion and illegal onion are significantly less so . the legal onion and illegal onion are particularly noisy , with the illegal onion group performing particularly bad compared to the all other two groups . when combined , the results of all and part - ofspeech are presented in table 3 , we see that the performance gap between the legal and illegal variants is much larger . ebay all and full sentences are both statistically significant ( paired t - test ) with respect to full sentences , but only slightly better than the average of the other two sets . on the other hand , for the illegal and all - in - one sets , the performance is much better , with an average of 0 . 43 and 0 . 60 per sentence compared to 0 . 25 , 0 . 37 and 040 by the average two - sentence average .
table 2 shows the average percentage of wikifiable named entities in a website per domain , with standard error . ebay is the most common example , with 38 . 6 % of its named entities ( e . g . ebay , paypal , hotpot . com ) being wikifiable . the illegal onion is slightly less frequently considered as wikifiable , but still makes up a large percentage of the total . legal onion is the second most frequently considered category , with 50 . 8 % of named entities being wikifiable .
the performance of our model on the similarity test set is presented in table 3 . our ukb model outperforms all the base lines with a gap of 10 . 8 % on average compared to the previous state - of - the - art on s2 , s3 and s15 .
table 2 presents the f1 results for supervised systems on the raganato et al . ( 2017a ) dataset . we observe that the best performances are achieved on s2 , s3 , s13 and s15 , where our system ( henderson et al . , 2016 ) achieves 73 . 8 % f1 score . the second highest f1 result is achieved by yuan et al . ( 2016 ) at 73 . 9 % , marginally improving upon the previous state - of - the - art performance by 3 points .
table 2 shows the performance of our system on the selected context and single context sentences . our model ppr achieves the best results , outperforming all the other models apart from the case of the case where it is trained with dfsnf . in all but one case , context sentence and context sentence are the most important factors in determining whether a model performs well in a single context sentence or a multiple context sentence .
we present the performance of our model on the swbd2 test set in table 2 . our model outperforms all the base we observe that the bag of means approach by a noticeable margin , bow + svm achieves 87 . 63 % accuracy on the sub - scale compared to the 73 . 76 % accuracy of bow alone . moreover , the accuracy increase over the best previous model is 15 . 08 % on swbd , compared to 6 . 71 % on average . logistic features alone give a significant performance gain , increasing the accuracy by 2 . 36 % over the previous state - of - the - art model .
table 1 : exact matching accuracy on sql queries . our approach outperforms all the alternatives with a large margin . syntaxsqlnet ( 25 . 7 % ) and typesql ( 8 . 3 % ) achieve 60 . 2 % and 54 . 4 % matching accuracy , respectively , compared to the accuracy of 43 . 2 % , 25 . 9 % and 25 . 8 % by the other approaches . moreover , the accuracy gap between seq2seq and sqlnet under automatic metrics is narrower , with a gap of 2 . 1 % on the generalization test set and 1 . 7 % on specific test sets . our approach verifies the value of data redundancy in the low - resource settings . we observe that the attention - based approach , which relies on syntactic antonyms and part - ofspeech embeddings , achieves a match accuracy of 6 . 8 % , which marginally outperforms the previous state - of - the - art .
table 2 shows the performance of the four approaches that we base our model on on the test set of hotpotqa . syntaxsqlnet achieves high accuracies on the hard and easy subsets , while irnet achieves a lower accuracy on the medium and hard subsets . the difference in performance between the easy , hard , hard and extra subsets indicates that the hard subset is more difficult for the model to solve , and consequently requires more data and training examples . selective attention mechanisms like syntactic antonyms and syntactic part - ofspeech embeddings ( see x4 ) outperform other approaches which aim to simplify syntactic relations .
table 3 shows the actual matching accuracy on the development set of semql test set . our proposed approach outperforms all the alternatives except syntaxsqlnet ( 25 . 9 % vs . 25 . 8 % ) by a margin of 3 . 8 % in terms of match rate . by further adding attention and concatenation features , our model achieves a match rate gain of 6 . 5 % ( micro - f1 ) and 3 . 7 % ( p < 0 . 001 ) over the baselines .
table 3 shows the classifiers ’ accuracy on the supports and refutes cases from the fever dev set and on the generated pairs for the symmetric test set in the setting of without ( base ) and with ( r . w ) re - weight . the bert model shows marked performance improvement over the strong baselines nsmn ( 83 . 2 % ) and esim ( 80 . 8 % ) by 2 . 6 points in the standard task formulation . however , the gap between the best and worst performing model is narrower with respect to the refutes case , showing that bert has better generalization ability .
the development and training accuracies of the models are presented in table 5 . first , we present the bigram and lmi metrics , followed by the train and race neural index scores . the results show that the training and development accuracies are relatively consistent , with the exception of the case of " united states " . however , when we add out the effect of foreign language translations , we see that the performance gap between the two sets becomes much larger . from the above table , the most representative groups are the actor , the director , the producer and the starred actress . in general terms , the roles played by the actors are very similar , however , there are differences in the development and the training performance . more importantly , the size and type of lmi that each actor contributes to the model is different . in the united states , for example , the average lmi contribution is slightly more than that of the average director , but still significantly less than the average of the starred actor . across all groups , the presence of a celebrity or a famous person who has won an award or a role in a movie is the most distinctive part of the model . this is reflected in the p - value of the lmi and the average number of trained and tested models , which show that from united states , more than 50 % of actors are trained to perform in a particular movie , while from america , only 25 % are trained .
table 3 compares the results of finetuned and un - finetuned models using the bidaf model on the standard newsqa development set of zsgnet and squad . the results show that our model can easily distinguish between the true match ( em ) and span f1 results ( f1 ) of the two sets , confirming that the model can be trained and tested on a variety of training data .
table 1 compares the performance of seq2seq model with human and simulator models . we observe that , when only using the goal and state - aware turns , the model performs better than it does with any other combination of turns . moreover , the accuracy is higher when using both goal - aware and goal - free turns , indicating that the ability to select the correct state and the correct goal leads to better performance .
we noticed that the most representative type of pos tags are the negation tags , which are used for nouns and verbs . they account for more than 50 % of the total number of tokens in the database ( a total of 294 . 3 tokens , according to table 3 ) . however , they are only responsible for about 5 . 7 % of total tokens , constituting 44 . 2 % of overall tokens .
we further analyze our results with respect to semeval - 15 and semeeval - 16 . in table 2 , we compare the performance of our method with the previous state - of - the - art on the following three datasets . our approach achieves the best results on the macro - f1 and acc . metrics , outperforming all the base semisupervised and unsupervised methods except for the case of the tdlstm + att dataset ( tang et al . 2016a ) . the accuracy gap between the baseline and the best performing method is modest but significant , with an absolute improvement of 1 . 59 points over the previous best state of the art . we observe that the transfer learning method by itself achieves the highest precision on the three datasets , with a marginal drop of 0 . 45 points compared to previous work . our approach outperforms both the baselines on the semantic and syntactic level , achieving an acc . accuracy of 7 . 57 / 8 . 03 and 6 . 63 / 6 . 59 respectively , with the difference being less pronounced for the syntactic and semantic level .
table 4 : ablation studies . exact match ( em ) and span f1 results on newsqa test set of a bidaf model finetuned with a 2 - stage synnet . as shown in the table , using k = 0 gives good results with a f1 score of 27 . 2 % on average . using only one sentence of the paragraph we use for question synthesis gives a better result than using two sentences of paragraph synthesis . further , using the full paragraph gives a 4 . 4 % improvement on em and a 6 . 2 % ) improvement on f1 . finally , using all the sentences in the paragraph gives another 4 . 2 percent increase on em .
table 3 presents the results on the en2es , health and bio transfer learning schedules . all - biomed models perform well on both data types , with the exception of es2en bio . health and bio are the only two that perform much worse than the former state - of - the - art . all other models show improvements on bio and health . the transfer learning schedule that applies to the khresmoi et al . ( 2018 ) model ( table 3 ) shows that the health and bio aspects are the mostchallenging aspects to solve , but once these are covered , all other aspects of the model perform well . finally , we see that the transition from health to bio is beneficial , improving the generalization ability of both models .
uniform ensemble ensembling improves the general performance for both test sets . health and bio are improved consistently over single - domain models , with the exception of es2en bio . all - biomed models perform better than either monolingual or ensemble models , with respect to ensembles , the health improvement is most pronounced for khresmoi and founta ( table 4 ) . finally , for en2es , the uniform ensemble model improves performance by 2 . 6 points in the standard task formulation .
uniform ensemble outperforms all the other approaches which allow boundary expansion . bi ensemble ( [ italic ] α = 0 . 5 ) and all - biomed ( p < 0 . 05 ) converge on news domain , but do not generalize well to other domains ( table 4 ) . cochrane ensembles outperform all other approaches except for the news domain ,
uniform ensembling outperforms bi with varying smoothing factors , showing that the advantage of tokenization leads to a better model performance . bi also outperforms the uniform ensembles when using a uniform smoothing factor of 0 . 5 , indicating that the bi model can rely less on tokenization artifacts . we observe small discrepancies in test set performance between uniform and bi using the standard wmt19 test set , as shown in table 5 , the uniform ensemble performs better than bi when using the smoothness factor 0 . 9 , but still requires considerable effort to reproduce .
bleu scores of the training and test set are shown in table 1 . the dev set significantly outperforms the test set , indicating that the model can easily distinguish between the true response and negative responses . the test set also shows that the ability to distinguish between positive and negative states is relatively high .
uniform model outperforms all the other models with a large margin . the results on the test set are shown in table 3 . as the results show , the uniform model performs best , while the gaussian and ensembled models perform the worst .
uniform models show lower bleu scores than the best - performing base models , indicating that the training set size and type of mt used in the development set are important factors in shaping the model performance .
most of the examples shown in table 1 are lexical , however , there are a few that are markedly gender - disambiguated , which show the extent to which the task is difficult to solve . syntactic examples are the mostchallenging for our model , as they tend to have low accuracy , showing that the challenge of predicting gender - specific questions is in how to extract relevant information from supporting documents and synthesize these multiple facts . further , the presence of a distinctive word or pronoun ( e . g . , “ b * tch ” or “ cid : e ” ) fragments the lexical vocabulary , making it harder to solve questions that are not related to the topic at hand . we find that more than 50 % of questions in the diversity dataset are assigned negative pronouns , which shows the difficulty of the task . across all three domains , the accuracy gap between the baseline and the best performing variants is low .
the most representative models are the hosg model by cotterell et al . ( 2017 ) and watset ( 2017 ) . in the table below , we present the results on the syntactic and semantic aspects . the first set of results show that , let alone a reduction in performance , the semantic aspects of our model significantly improve over state - of - the - art approaches across the three domains .
we present the performance of our method in table 4 , compared to the previous state - of - the - art on the polysemous verb classes in korhonen et al . ( 2003 ) . the results are presented in tables 4 and 5 . our model obtains the best performance with a f1 score of 52 . 86 % on the k - means metric , while the noac model ( 53 . 08 % ) and hosg ( 38 . 07 % ) obtain the worst performance .
in table 1 , we compare rr and rr - fr with the baseline performances . as expected , the former performs better than the latter on the 11 - point iap test set , indicating the current state of the art performance . by further adding stepwise measures , the rr model achieves a better result , reaching 69 . 72 % f1 score .
as shown in table 4 , the training set size and the number of points scored for each step indicate the state of the art . the rr model achieved the best performance with a f1 - score of 65 . 79 on the 11 - point iap test set .
table 1 provides a comprehensive overview of the biology and chemistry datasets from the thomson reuters rcv1 corpus . it contains 10 entity types , 9 object - based ( amount , concentration , device , location , method , reagent , speed , temperature , time ) and 5 measure - based entity types . the total number of entity types is 220 , 618 , which includes 39 , 618 full - length , open - access journal articles about biology . more than 900 , 000 articles are abstracted , mostly about domain specific topics , including 25 , 000 about physical sciences .
correlation coefficients show that the tvc and wvv similarity measures have low correlation with the effectiveness of pretrained models . the coefficients vary between - 1 ( negative correlation ) and 1 ( positive correlation ) . zero means no correlation .
table 5 compares our best performing model with the best performing publicly available ones . our model improves upon the performance of the widely available ones by 3 . 36 % on the word vectors glove and conll2003 . by further adding entity nodes , we gain 0 . 45 % performance improvement on the lms elmo and cadec .
we observe that both domain - adaptive and hyper - parameter setting have a significant impact on the performance of our pretrained word vectors ( table 6 ) . specifically , we see that the use of ‘ opt ’ and ‘ def ’ refers to the two types of parameter sets proposed in chiu et al . , 2016 ( see x4 ) . however , the results show that using only one type of parameter , namely , the def setting , does not improve performance .
table 3 shows the most prevalent types of discrepancy in context - agnostic translation caused by deixis . the largest percentage of discrepancies ( 67 % % ) are caused by the t - v distinction , which shows that the grammatical gender of the speaker / addressee is strongly related to the speaker .
table 1 shows the evaluation results on subsets 0 - 100 , 100 - 500 , 500 + and 500 + of the thyme dev ( in f - measure ) . all models trained on these subsets receive sg initialization cues , but only rc ( sg initializations ) are trained on the larger training data . rc with sglr ( which learns event and entity relations with sg initialization ) achieves gains over random initialization , but still performs substantially worse than rc with sg fixed . we observe that rc with only sg initializations do not generalize well compared to rc with random and sg - based initialization . when using the smaller training data size of 3 . 3k and 2 . 7k , the accuracy gap between rc and sg is greater than that with rc + sglr .
table 3 compares the results of rc ( with and without sg initialization ) with the best previous models on the four datasets . with sg initialization , our model achieves 71 . 2 % f1 score , marginally improving over the previous state - of - the - art . rc with no specialized resources ( leeuwenberg et al . , 2016 ) and lin et al . ( 2017 ) achieves 62 . 6 % and 62 . 2 % ) f1 scores , respectively , compared to 58 . 9 % and 57 . 3 % by the previous best results . with rc initialization , we get a marginal improvement of 2 . 8 % on the best clinical tempeval ( 2016 ) score compared to the previous results ( 57 . 9 % ) with sg initialization . further improving performance by 2 . 4 % on average with rc ( sg fixed ) and 6 % on improving performance with specialized resources .
table 3 shows the error analysis on 50 fp and 50 fn ( random from test set ) for different settings . rc and sg errors are not mutually exclusive , as the size and type of error caused by the sg initialization are different . however , the most common error categories are frequent and error - prone , as shown in table 3 . frequent errors are caused by grammatical errors in the relation relations ( cf . table 3 ) . in addition , frequent errors are also caused by line - of - speech errors .
the results for all the models shown in table 2 show that ling significantly improves over random and ling + n2v ( p < 0 . 05 ) . however , the improvement is less pronounced for sentiment , sentiment and hate speech are the only two areas where linging performs significantly worse than random ,
most of the languages shown in table 1 contain more than 50 , 000 words . more than half of these are spoken in english , which means that more than half the words for each language are actually spoken by one person .
table 4 shows the percentage of instances of discrepancy in context - agnostic translation caused by ellipsis . interestingly , the most prevalent type of discrepancy is the wrong morphological form , which accounts for 66 % of the discrepancy .
the results are shown in table 1 . we show that our model ( bow + logreg ) achieves the best results with a f - m of 0 . 827 on the sst - 2 dev set .
the results are shown in table 4 . we observe that the seen and unseen cues yield similar results , but the results are slightly worse than those of the unseen ones . in general terms , the results show that the unseen cues cause the same effect as the seen ones , however the difference is less pronounced in the cosine similarity case . the following table compares the results of the original and the unseen pairs . as seen in the second group of table 4 , the similarity scores are slightly higher in the seen set compared to the original set . also , the gap between the average cosine and the average hsv score is smaller in the unseen set , indicating that there are no noticeable differences in the performance between the two sets .
table 6 shows the bleu scores of the models trained and tested on the concatenated test set . the results are statistically significant ( p < 0 . 05 ) with respect to the baseline ( 1 . 5m ) and the training set ( 6m ) . however , when trained with s - hier - to - 2 . 0 training data , the results are not statistically different ( p < 0 . 01 ) .
we show the f & c dataset size in table 2 . all labels represent the original dataset with all the labels . subset labels are the subset labels which are inferable by the resource . the total number of labels in the dataset is 12 , 012 . table 2 shows that the subtasks for each label is roughly balanced .
the results on the noun comparison datasets are shown in table 4 . our model obtains the best results , outperforming the previous state - of - the - art on three out of the four domains .
we show that the transfer method by yang et al . ( 2018 ) achieves high accuracies , surpassing previous work by 4 . 5 % on the relative dataset . the transfer method also achieves a significant improvement on the chance metric , by 3 % .
intrinsic evaluation . we present the results in table 7 , showing the performance of our method with respect to the four dimensionals . accuracy of the number of objects which our proposed median fall into range of the object , given the dimension . the results show that our proposed method can significantly improve the interpretability by increasing the precision by 1 . 31 % over the state of - the - art method by design .
table 5 shows the pca components that are most likely to be associated with satire . in general terms , we see that the most likely areas for satire to occur are the first person singular pronouns , second person singular pronoun and the relation of causal verbs . however , apart of these three groups , there are no significant differences in the rate at which tweets are predicted to contain offensive language . further , the ratio of casual particles to causal verbs is relatively low , indicating that most tweets in question are not intended to be offensive . on the other hand , this table shows that they are frequently considered as a complement to positive speech patterns . we see that for the two aggregated categories of pca , word concreteness and sentence length are the most important factors in predicting sentiment , with a gap of 0 . 18 and 0 . 59 points between the average of the two scores . sentence length , number of words and repetition are the other key components of the word analogy task . as shown in the table , precision on these three measures is relatively high , with an absolute improvement of 2 . 18 points compared to rc5 . regarding pca cohesion , our model achieves a 3 . 18 / 4 . 41 score on average , a slight improvement over the previous state of the art .
the results of classification are shown in table 2 . the best performing model ( bert pre - trained model ) shows that it can distinguish between the headlines , body and full text . however , it is unable to do so with respect to the headline , which indicates the extent to which the text body contains context .
the results are shown in table 4 . concatenation and lexical cohesion are the most stable aspects of the model , while deixis and cadec are the less stable ones . the concatenated model outperforms both the baseline and the s - hier - to - 2 model , indicating that concatenating knowledge is beneficial for the task at hand , but does not improve performance in the most relevant contexts . finally , we see that when dependency trees are added to the baseline , their performance is relatively stable , improving only by 0 . 4 points over the concatenative model .
table 3 : summary of results of classification between fake news and satire articles using the baseline multinomial naive bayes method , the linguistic cues of text coherence and semantic representation with a pre - trained bert model . the best performing model is the coh - metrix model ( bert , p = 0 . 78 ) which shows marked performance improvement over the baseline model by 1 . 8 % in the precision and rn ( p < 0 . 01 ) .
table 1 shows that the ment - norm and rel - norm baselines achieve similar f1 scores on the test set of aida - b ( table 1 ) . however , in the more realistic second case , when only using the k16 - 1025 baseline , the results are slightly worse ( k15 - 1011 ) . ment - norm ( no pad ) outperforms the comparably trained guorobust baseline although the difference in f1 score between the two baselines is less pronounced in the realistic case , still indicating some degree of reliance on superficial cues . rel - norm also performs comparably to the modelled baseline , however it requires significantly more data and time to train .
table 3 presents the results on msnbc , aquaint , wiki and cweb . our model ( d17 - 1276 ) outperforms all the base the results are presented in bold . our rel - norm model improves upon the strong lemma baseline by 3 . 4 points in the standard task formulation and achieves state - of - the - art results on all three metrics .
the performance of the proposed lstm - based variants with the traditional cross - validation setup is shown in table 2 . we observe that bilstm achieves unrealistically high performance ( uar > 0 . 99 ) and rach et al . ( 2017 ) ea < 0 . 93 ; however , the uar and ρ scores are only slightly higher than those of the bistm model ( 0 . 91 ) and 0 . 94 ) . the performance gap between bi - stm and the original neural models is small , but significant , with κ = 0 . 28 and σ = 0 . 05 comparing to the performance of 0 . 08 and 0 . 03 by ultes et al . , ( 2015 ) . though the performance gap is modest , it is significant enough to warrant a second opinion .
table 8 shows the performance of our method on the ellipsis test set . our concatenated model ( cadec ) outperforms the baseline model ( s - hier - to - 2 . 6 ) and the strong lemma - based model ( cadec - 71 . 2 % ) by a noticeable margin .
the performance of the proposed lstm - based variants with the dialogue - wise cross - validation setup is shown in table 3 . the bilstm with attention mechanism performs best in all evaluation metrics . it achieves the best performance with an average uar score of 0 . 88 , slightly outperforming the previous state - of - the - art on three of the four evaluation metrics and performing slightly worse than the bistm + att model by 0 . 03 . on the fourth evaluation metric , ea , the performance is slightly better but still significantly lower than the previous best performance .
we report f1 - measure results over the test portion of our dataset averaged over 10 replications of the training with the same hyper parameters . the results are shown in table 1 . our model ( joint1 / joint2 ) outperforms the best previous work ( ner ) by 3 . 47 points in the f1 measure .
we report the mean accuracy of our models for md performance in table 2 . as in the ner evaluation , we report accuracies over a test dataset averaged over 10 replications of the training . the difference in accuracy between joint1 and joint2 is minimal , however we see significant difference in mean score due to different training sets and different derivational schemas used in the training set .
performance of our models on the fasttext and glove datasets are shown in table 1 . the baseline model utilizing bioelmo as base embeddings jin et al . ( 2019 ) showed an accuracy of 78 . 2 % . on adding knowledge graph information , we were able to improve these results to 78 . 76 % , an absolute improvement of 4 . 97 % over the performance of the state - of - the - art baseline model by using sentiment information . by further adding sentiment information , the accuracy rose to 79 . 04 % , an increase of 2 . 36 % . the accuracy increase of 1 . 97 % , compared to the previous state of the art .
table 9 shows the bleu and lex . c . vec . scores for different probabilities of using corrupted reference at training time . ellipsis and deixis produce significantly different bleus ( p < 0 . 01 ) compared to the original ones ( p > 0 . 25 ) in terms of inflection / vp scores . for dexis , the p = 0 . 31 shows significant over - fitting since the original embeddings do not have the correct grammatical relation for the context sentence , and consequently result in incorrect derivation of the relation . in the case of lex . c . v . , the difference is less pronounced , but still significant , p > 0 . 45 shows that the incorrect derivations do not harm the model . regarding the verbiage prediction , we show that it is possible to obtain a decent result by using a corrupted reference with a comparable number of context sentences , but only if the obtained reference contains the correct relation .
table 5 : the accuracy of the traditional classifier in phase 2 given documents from seen and unseen datasets . it achieves 50 % and 25 % accuracies , respectively , on the two datasets , with a drop of 10 % in the seen rate compared to the previous state of the art .
table 3 presents the results of our final model on the fine - tuning task . our model obtains 7 . 42 % improvement on average compared to the best previous state - of - the - art model ( glove ) in terms of general and average f1 scores .
we show in table 2 that our model achieves substantial gains over the naive approach by using glove augmentation and domain - aware labelgcn ( see xiong et al . , 2018 ) . further , our model ( ours + glove ) achieves macro - averaged f1 scores of 43 . 2 % and 35 . 9 % higher than the best previous state - of - the - art model ( bert - base , uncased ) .
the results are shown in table 1 . our proposed method outperforms all the base the results show that the heuristic baselines and domain - adaptive filtering methods give similar results , but do not exceed the performance of the best performing pair or overlap model .
table 3 compares our model with previous approaches on the three similarity test sets . our model achieves state - of - the - art results , outperforming all the base on the mi - f1 dataset , our elmo w augmentation improves performance by 3 . 5 points on average compared to the previous state of the art . by further augmentation , our model achieves gains of 2 . 8 points on the f1 and 3 . 9 points on mi - fi .
table 5 shows the average number of examples that are added or deleted by the filtering function . the fine - tuning function results in a reduction in the error rate of the output addition and removal of some types . interestingly , the size and type of discarded examples seem to have little effect on the overall performance of the model , indicating that the rate of items being added or removed is relatively low .
table 1 : properties of the ubuntu and samsung qa dataset . the first set of messages and the second set of responses are presented in table 1 . as shown in the table , the message and response are the most important components in the training set , followed by the context and the response . table 1 shows that the size and type of tokens used in training and test set are very different , with ubuntu - v1 having 35 , 609 messages and 35 , 517 responses while samsung - v2 has 19 , 560 messages and 18 , 920 responses .
in table 2 , we compare tf - idf , lstm , cnn , and compagg with various approaches on the test set of ubuntu - v1 , 2r @ 1 , 10r @ 2 , and 5r @ 5 . our model obtains the best results on all datasets with a gap of 0 . 898 ± 0 . 002 points from the last published results ( tf - idff ) . on the other hand , we see that rde - ltc achieves the highest score with an absolute improvement of 2 . 945 ± 0 . 014 points on the ubuntu v1 test set and 2 . 848 ± 1 . 938 points over the previous state - of - the - art model .
we compare our proposed approach against 3 baseline models - lstm , cnn , rnn - cnn and attention \ scriptsize { 6 , 7 , 8 } on the test set of ubuntu - v2 1 in 2r @ 1 , 10r @ 2 and 5r @ 5 . in all but one case , the proposed approach outperforms the other baseline models except for the case where it obtains the best performance . attention is selective , selecting only the relevant features and only the ones that are relevant to the task at hand ( e . g . , those linked to in section 3 . 3 . 3 ) . selective attention mechanisms like cnn [ 5 , 6 ] and rnn [ 7 , 8 ] achieve relatively high performance , but do not exceed the upper boundary of what is expected by human judgement . compagg [ 3 , 9 ] achieves the best results , achieving 0 . 945 % and 0 . 863 % better overall performance on the ubuntu v2 test set .
the results in table 5 show that tf - idf model outperforms rde , hrde and rde - ltc in the 2r @ 1 and 10r @ 5 tasks while it performs slightly better in the 3r @ 10 dataset . in the latter case , when trained and tested on the samsung qa dataset , the results are slightly better than those in the first case ( table 5 ) . as shown in the second group of table 5 , the use of ltc improves the performance of the model in the two scenarios , but does not improve the results in the third scenario .
table 2 compares our model with the baselines on bleu , meteor , rouge - l and iwaqg ( see x4 ) . our model outperforms the previous state - of - the - art on all three high - level test sets . it closely matches the performance of the best performing baselines , with the exception of zhou et al . ( 2017 ) . the model performs slightly better than the baseline on b - leu - 1 , but it does not exceed the upper boundary of the " safe harbor " range , which indicates the extent to which qg can be improved with a reasonable selection of the qg module . on the other hand , it surpasses the lower boundary on three of the four test sets , showing that it is possible to improve upon the quality of qg by adding more features along with the standard features .
the performance of these models on the four similarity test sets is shown in table 4 . in general terms , the results displayed in table 4 show that only qg * can improve the bleu and rouge scores by significant margins . more than 80 % of the time , the accuracy improvements are due to high accuracies in the high - level categories ( bleu - 1 , 2 , 3 , 4 ) .
table 4 shows the recall of the interrogative words of the qg model without our interrogative - word classifier in zhao et al . ( 2018 ) . the lower bound on the upper bound confirms the effectiveness of our model , with the exception of the case of " who " . however , it does not exceed the upper boundary of performance , which indicates the extent to which the ability to select compact regions induces the model to rely on superficial cues . further , the model performs slightly better than the iwaqg model when trained only with qg * data . it achieves a recall improvement of 0 . 43 % over the strong baselines on " who " and " where " .
the performance of our interrogative - word classifier is shown in table 6 . the accuracy achieved by our model is close to the state - of - the - art , reaching 73 . 3 % on the standard test set .
table 7 shows the recall and precision of the interrogative words of our interrogative - word classifier . what we learned is that what and when are the mostchallenging aspects for the classifier , as the recall is low and precision is high . however , when we learned the meaning of which and when , the performance on these questions became much better .
as shown in table 1 , the training set size and the average number of nodes per edge are the most important factors in model performance . we find that for example , if we set up a model with 1 edge , 2 nodes , and 1 relation node , then the model performs well on the development set with an average of 94 . 4 % las and 94 . 6 % conn . ratio ( table 1 ) . however , this analysis fails to account for the fact that our model only works on one edge per node , which explains ∼ 9 % increase in performance of the las model .
we show the test results on biocreative vi cpr in table 2 . our model obtains a significant improvement over the previous state - of - the - art on all metrics by 3 . 6 points .
the results on pgr testest are shown in table 3 . our model obtains significance over the previous state - of - the - art on three out of the four bootstrap tests . the difference is most prevalent in the last bootstrap test set , where our kbesteisnerps model ( 83 . 6 % ) beats all the other models except for biobert ( 67 . 2 % ) by 3 points .
the results on semeval - 2010 task 8 are shown in table 4 . our model obtains the best f1 score of 84 . 6 % , marginally improving over the previous state - of - the - art c - gcn model by 1 . 8 points .
in the en - de news / cnn task ( table 4 ) , the models trained with different attention mechanisms perform comparably to each other when only using one type of clustering feature . however , when using all the features at once , the performance gap between cnn and pcnn becomes much larger . with the training data available in table 4 , we find that the model using clustering features , such as the clustering of entity nodes , boosts precision on the training instances , but does not improve performance on the macro .
the results of ablation study are shown in table 2 . our approach outperforms the best previous approaches by a noticeable margin . our pcnn model improves upon the strong baselines by 3 . 8 points in the macro - level while word2vec achieves gains of 2 . 2 points in accuracy .
we observe that path2vec and node2vec achieve remarkably similar results to human judgments , improving upon the strong baselines by 3 . 8 points in the spearman correlation test set . further improving performance by high margins
table 3 compares the results of our graph - based vs vector - based and path2vec based measures compared to the best performing random sense based measures . we observe that , when only using original word embeddings , the results displayed in table 3 are significantly worse than those in semeval - 15 . 7 .
the results on the rareword set are shown in table 3 . both variations of our embeddings perform comparably to each other , with the exception of the case where polyglot performs better . adding the effect of the mimick step improves the results for both languages . however , for the japanese vocab , the effect is less pronounced , showing that the mimick effect does not contribute significantly to the model performance .
table 1 compares our system with other state - of - the - art systems . our system outperforms all the base systems with a gap of 15 . 32 bleu points from the last published results ( kutuzov et al . , 2016 ) on the system and domain comparisons . the results reconfirm that our domain - adaptive system design can significantly improve the generalization ability of neural networks across domains .
full data no - char , full data mimick and full data char are presented in table 4 . we compare our model with previous state - of - the - art models on the training and test set datasets . we observe that the performance reach the best results when using the no - char and mimick models , with a gap of 10 . 45 % on the test set with respect to psg . with the additional data augmentation , our model achieves gains of 3 . 95 % on average over the previous state of the art .
full data no - char , full data mimick and full data char are presented in table 5 . we compare our model with the best performing models on the training and test set . with the training data set of 5000 , we observe that kk and ta achieve outstanding results , outperforming all the other models apart from the case of the exception of the " tag " case . further , the results are slightly superior on the test set with the size and type of data reported , indicating that the semantic information injected into the model by the tagging function is significant enough to result in a measurable improvement . finally , we see that the model using the tag function achieves the best results with the full data set , showing that it is well - equipped to perform this task .
table 3 shows the percentage of tokens with missing embeddings and full vocabulary in the test set for each language . we observed that , for english , spanish , french , dutch , russian and turkish , the average number of tokens per sentence was slightly more than the average of the other languages , but still significantly lower than most other languages . table 3 summarizes our results on the five languages . the largest drops in performance were seen in the case of spanish , which went from 7 . 2 % to 9 . 6 % over the previous state of the art .
the performance of each model according to these test set is presented in table 1 . it can be seen that the difficulty level in each scenario is relatively high , with the exception of the " grocery shopping " scenario , which shows that it is relatively easy to solve . further , the accuracy and difficulty level are relatively low for the " linguistic " and " script " models , indicating that the challenge of programming these models is in how to extract relevant information from supporting documents and synthesize these multiple facts to derive an answer . table 1 shows the performance of the models for each scenario . the average human model accuracy and the average number of instances perplexity are shown in bold . according to the table , the grocery shopping scenario is the mostchallenging , as it requires a lot of data and time to set up and maintain the model , hence leading to incorrect answers . however , it is the only one that shows a significant drop in performance .
table 1 compares our model with other state - of - the - art decoders . our model outperforms all the base the results are presented in table 1 . the decoder performance on the standard and multi - decoder datasets is set at the 90 % level , with an absolute improvement of 2 . 18 % on average compared to the seq2seq baseline .
table 3 shows the error rates for human , script and human accuracy . the error rates are presented in table 3 . we observe that the human accuracy baseline is relatively high while the accuracy of the error baselines is low . when we add multi - decoder training data , we get significantly worse results on both languages . on the other hand , for the linguistic dataset , our model performs relatively better , with an error reduction of 3 . 4 % on average compared to the previous state of the art . in addition , the accuracy remains the same on both error and recall datasets , with a minor improvement on the recognition baseline .
we compared bert and glove with previous models on the similarity test set of the keller dobj , nsubj and word2vec datasets . in general terms , we see that the friendly model performs better on all three datasets downstream , while on the higher - order ones , it performs slightly worse . the similarity test results on the keller datasets are presented in table 4 . glove ( which relies on word embeddings ) and elmo both have relatively high accuracies in the dobj and amod tasks ( see x4 ) . however , when trained with bert ( dynamic ) , the results are slightly worse ( p < 0 . 05 ) . word2vec trained on the sp - 10k dataset is comparable , but does not have the advantage of using d - embeddings . keller is qualitatively very similar , with the exception of the amod . we observe that bert ' s model performs slightly better on the high - order datasets , indicating that it relies on superficial cues .
we observe that dobj dovec embedding model outperforms nsubj model in the three categories as shown in table 3 . in particular , dobj model performs better in the case of noun , verb and adjective , while the model done with d - embedding performs slightly worse in the word2vec category . as shown in the table , combining the embeddings of the three components improves the results for all three categories . however , for the adjective , the difference is less pronounced , showing that the embedding technique does not improve the performance for adjectives . we observed that the difference between the average score of the verb and noun is narrower than that of adjective , but still significant enough to warrant a closer look .
comparison of mwe against elmo and bert on the ws task is reported in table 4 . overall performance , embedding dimension , and training time ( days ) are reported , while bert has the advantage of training on a single gpu .
table 5 compares the results of different training strategies . we find that the alternative optimization approach ( alternating optimization ) achieves the best results , improving the overall ws by 1 . 5 % over the baseline model by β - factor .
as shown in table 3 , the bleu scores of the models trained to convergence on 1m wat ja - en , batch size 4096 , are significantly higher than those of plain bpe ( 27 . 2 vs 25 . 6 bpe ) and linearized derivation ( 28 . 6 vs 28 . 2 ) .
the results on ja - en are shown in table 4 . our model ( seq2seq ) achieves the best performance with a 28 . 2 % improvement on the bleu test set compared to the previous best performance by morishita et al . ( 2017 ) . the transformer also achieves a 4 . 4 % improvement over the plain bpe model ( lin - svm ) on the test set , which shows the advantage of using a more compact derivation scheme .
in table 5 , the best performances on the bleu test set are achieved by the linearized derivation method , which shows significant improvement on the plain bpe baseline compared to the koehn et al . ( 2007 ) model by 1 . 8 points .
table 3 shows the test set results for english , spanish , french , dutch , russian and turkish . our system works well , with a gap of 10 . 2 % in accuracy between basic and unk models , and 17 . 6 % on average compared to the previous state of the art .
table 5 presents the results on event coreference . our system outperforms all state - of - the - art methods in terms of all metrics on both rf and spmrl . in particular , we see that our dnn model beats all the base the best performing methods by a noticeable margin .
we noticed that the models using " - expansion " and " - letters " achieve better results overall when compared to " expansion " . the results displayed in table 5 show that , when using only one type of expansion layer , the results obtained by spmrl are significantly worse than those by using the other three .
table 3 shows the maximum perturbation space size in the sst and ag news test set using word / character substitutions , which is the maximum number of forward passes per sentence to evaluate in the exhaustive verification . as shown in the table , the size of the space available for forward passes is limited by the number of words in the original sentence , which means that there is a limited number of sentences that can be considered for evaluation .
results are shown in table 1 . the results show that adversarial and adversarial learning models perform comparably to each other , but the results are slightly worse than the normal ones . predicate word - level accuracy is relatively high while generative learning is low . in general terms , the adversarial model outperforms the normal one , but does not exceed the upper boundary of the boundary of conversational training . generalization is beneficial , improving the generalization ability of both datasets . it improves the accuracy of the sst - char - level and accuracy on the word prediction tasks . it reduces the recall gap between conversational and adversary learning . supervising learning requires significantly more data than training , as shown in fig . 3 . regularization reduces recall , but helps the model to improve accuracy .
the results are shown in table 1 . in general terms , the non - anonymized and anonymized models perform comparably to the unsupervised baselines . however , the accuracy gap between the two is narrower with the exception of the case of the bigru - att model , which obtains higher f1 scores than the other two models . the difference is most prevalent in bowls , according to our model , bowl - toss is the most difficult class to solve , obtains high accuracy in terms of matchmaking , and matchmaking accuracy only when the data is labeled with " bow - svm " . as shown in the second group of table 1 , not all bowls are suitable for this task , due to space limitations , we trained only on bowls that were already trained on the large scale web content ( outsios et al . 2008 ) . the smaller differences in performance between bowls indicate that some specialized bowls are better than others . table 1 shows that , in all but one case , the preference for anonymity is greater for bowls that are already trained , with respect to the antonym based learner , hier - bert , the model trained on wikipedia , achieves the highest f1 score ( 87 . 2 % ) on the non - anonymized dataset , compared to 71 . 8 % on the anonymized one ( 71 . 4 % ) . interestingly , the difference between the accuracy of the two sets is less pronounced for bowls , indicating that the training data used for this model comes from a more diverse pool of data .
we observe that seq . tagger significantly outperforms a number of approaches that allow more clusters or number of states to be labeled , showing that precision is relatively high even under the difficult requirement of a low number of clusters . the accuracy is shown in table 1 , with black - box classification showing that it is possible to do many - to - one mapping of target languages with available test data , and that doing this achieves high precision ( 98 . 28 % ) and accuracy ( 87 . 08 % ) . however , it is less likely to achieve this result than expected . we observed that a - hmm labeling achieves a lower accuracy than e - kmeans , showing the advantage of finetuned word embeddings .
parallelism is presented in table 5 . we show the performance of the best performing models using the three types of parent - learnt word embeddings . the results show that , let alone a reduction in performance , all the models give good or outstanding results . in most cases , the difference between en and fr is less pronounced than that between fa and fa , but still suggests some degree of competitiveness .
table 3 compares the performance of our combined cipher grounder ( cipher - avg and a supervised tagger ) with the best performing noun tagger ( supervised tagger ) . the difference in precision between the two approaches is minimal , however we see significant difference in recall , as measured by precision ( p ) , recall ( r ) and f1 scores , our combined approach shows that the combined approach can significantly improve the recall and precision of the generated noun tags over traditional approaches by increasing the recall rate and precision .
we observe that the impact of the grounded pos tagging on the las is also not notable , i . e . it uas shows lower performance than it does on gold uas , while it las shows higher performance . table 4 shows that the difference between the gold and silver uas is less pronounced for non - gau models , indicating that the accuracy obtained using the gold tags is more related to the semantic content of the uas than those of gold .
finally , we give a brief overview of our model ' s performance on the three types of uas and las test sets . the results are presented in table 5 . in general terms , the model performs well on all three sets , with the exception of the uas . it achieves state - of - the - art results on the fr and sv uas datasets , and on the las dataset . however , on the λuas dataset , performance is relatively lower , with a gap of 2 . 40 uas points from the previous state of the art .
the most representative models are the bow - svm and bigru - att models , which show high accuracies in both accuracy and recall . overall , the models show that the accuracy obtained using the frequent and frequent labels is relatively high , with the exception of the frequent case , when accuracy is low . in general terms , the accuracy is high with respect to both labels , indicating that the training set is well - equipped to perform this task . regularization reduces repetition , as measured by the p < 0 . 05 . however , it does not reduce recall , as expected .
table 1 shows the training and test set for each language . it is clear from table 1 that the training set developed in the 1980s and 1990s is well - equipped to handle the diverse vocabulary needs of the time period . the largest gains in vocabulary are on the religious domain , with more than 260 , 000 tokens in total . however , the largest gains on the science and language domains are still in english , with a total of 233 , 947 tokens .
the most representative models are the dataset dea , der and is , closely followed by norma , lookup and pt . these models use object detectors pretrained on pascal - voc on word embeddings . they perform well on synthetic dataset with a minimum of 80 % accuracy . on the other hand , their maximum score of 94 . 59 % indicates that they perform much better on human - generated data . syntactic keyphrases alone perform better than any other model group , indicating the syntactic patterns discussed in section iv hold high significance for human judgement . moreover , their semantic information quality is comparable to that of the original models , with the exception of the der dataset . supervising attention mechanisms like parallelism and topnu produce remarkably similar output : of the 2000 examples in the dataset , the two have completely opposing predictions ( i . e . name a vs . name b ) on only 325 examples .
table 4 shows the mean absolute error and spearman ’ s ρ for case importance . most models only slightly outperform the bigru - att baseline ( p = 0 . 005 ) and bow - svr ( p ( cid : 28 ) 0 . 01 ) .
table 3 compares our k & g model with prior work on the fixed - tree , local edge and jamr - style decoders . our model achieves state - of - the - art results , outperforming all the previous models except for wang et al . ( 2015 ) . the results show that the local edge , when combined with the projective decoder , gives a performance gain of 2 . 8 points over the best previous model ( wang et al . , 2015 ) . however , the difference is narrower than that between the two baselines , with the local edging giving a gain of 1 . 6 points .
table 3 provides detailed results on the quality of wikification and named entity recognition . we show in bold our best summaries from the 2015 and 2017 ftd datasets . wikification is consistently better than named entities , with a gap of 10 . 5 % in quality compared to the previous state of the art . named entities generally perform better than unlabeled ones , with the exception of smatch .
the results are shown in table 2 . all models cause a significant drop in performance when trained and tested on the same dataset . the jamr - gen model ( which relies on the concept of entity prediction ) is the only one that performs much better than tsp - gen . noinducedrule also significantly decreases performance . moving distance and concept rule are the only two that do not harm the model performance .
the results are shown in table 3 . our 1 - best model obtains a 39 . 9 % accuracy rate on the terminal test set compared to the previous state - of - the - art .
label distribution of the training , dev , and test set is shown in table 2 . the results show that the training set contains roughly 5 , 000 examples while the test set contains more than 15 , 000 . interestingly , the size and type of training set seem to have little effect on the results , indicating that all the training instances contain similar emotions . however , once the sad and angry categories were added , the results showed a significant drop in performance ( 7 . 45 % vs . 5 . 56 % ) . in comparison , the happy and confused set shows a slight improvement ( 6 . 28 % vs 5 . 44 % ) over the previous state of the art .
table 1 shows the macro - f1 scores and its harmonic means of the four models as well as the mean of the sl and sld scores . in general terms , we see that the sld and bert models perform comparably to the best previous approaches . however , in the sad and happy cases , their f1 scores are considerably higher than those of the other two . hrlce shows lower performance than sld , indicating that the training set size and the type of training data used by the model are important factors in predicting emotions . mean and mean - harm scores are computed using the weighted average of the test and dev scores , as shown in table 1 , the bert model performs much better in the happy and sad cases , but when trained in the more productive setting , it performs much worse than in the angry one .
intrinsic evaluation results . table 3 shows that our jamr model significantly outperforms the original oracle model in all aspects ( sign test , f1 - score , hand - align , smatch ) .
the parsing results of our aligner are shown in table 4 . our aligner significantly improves the reading performance of the jamr parser and the camr parser .
we show the results for english , spanish , french , dutch , russian and turkish for newswire embeddings . our ensemble shows gains relative to the best performing single parser and ensemble . our results show that the jamr aligner improves the word prediction quality by 4 . 8 % in english .
table 3 presents the results of models trained on the hidden test set of exact match . our model ( ours ) obtains the best results , significantly outperforming all the other models apart from fp . it closely matches the performance of the best previous models , with the exception of fp . in fact , it improves upon their performance by 3 . 8 points in total .
the results are shown in table 7 . we observe that the location and type of gene expression are the most important factors in the clustering performance of the model , followed by the type of location . the results show that the gene expression and the location are very different , with the gene eq . expressing significantly different values depending on the underlying data type . table 7 shows the results of clustering and location experiments , compared to previous work . in general terms , we see that location - based and type - based clustering significantly improve performance , however , it does not improve the results for any other type of expression , we observed that the variation of the cell type that interacts with the gene is the most significant factor in predicting clustering ,
table 3 shows the f - score of all models using the eaa and ufa annotation modes . our model obtains the best results with a cut of 2 . 8 % in the accuracy rate compared to the previous state of the art .
the total number of examples for each coarse text type is 1 , 590 , 885 . originally , we estimated that there would be 1 . 5 million examples in total , but when we expanded our dataset to include tweets from the web , we found that there were 2 . 2 million examples , constituting a larger corpus .
confusion matrix for test data classification . as shown in table 2 , the predicted sg and actual pl consist of words that are similar but have completely different meanings ( i . e . sg vs . pl vs . name a vs . b ) which can be seen as a significant ( p < 0 . 01 ) margin of error .
agreement patterns across genres are shown in table 3 . the most striking ones are the notional and verbal agreement patterns , which show that the spoken word is the most important part of the conversation for embeddings , followed by the written word . across all genres , the average number of sentences per genre is 27 . 33 , 27 . 66 and 27 . 86 percent notional , respectively .
table 3 shows the number of propositions per type in ampere . we find that non - a and a negation are the most prevalent , followed by fact and fact .
our results are shown in table 4 . the first set of results show that bilstm - crf is significantly better than all the base models that do not use crf - joint embeddings ( i . e . , fullsent , rst - parser , and pdtb - conn ) . the second set shows that , when crf is trained and tested on a new dataset , it performs much better than the previous state - of - the - art . although the performance gap is small , it is significant with ∗ ( p < 10 − 6 , mcnemar test ) .
table 3 presents the results on the relation extraction experiments on the factbank , regularization and uds - ih2 datasets . we observe that the proposed t - bilstm ( 2 ) - s model outperforms both published and unpublished work on every metric by a significant margin . for example , lee et al . ( 2015 ) and parallelism produce remarkably similar results : of the 2000 example pairs in the dataset , the two have completely opposing predictions ( i . e . name a vs . name b ) on only 325 examples , with the exception of the one that belongs to the regularization dataset . moreover , the difference between the average precision scores on the two datasets is much smaller . on the other hand , this gap is much narrower with respect to regularization , with an average precision of 2 . 31 vs . 0 . 63 .
the results are shown in table 5 . with gold - standard and predicted segments , svm outperforms cnn and svm . with predicted segments , it achieves the best results . with gold - standard segments and predicted segments , it obtains the most accurate results . svm and cnn closely match the performance of the best performing classifiers with only 0 . 28 % absolute difference in gold standard and 65 . 59 % absolute improvement on predicted segments . with respect to sentiment analysis , we see that svm significantly outperforms the method proposed by peyrard and gurevych ( p < 0 . 05 ) with respect to all sentiment analysis methods except for gold standard .
the model and its hyper - parameters . as shown in table 1 , the size and type of attention type are the most important factors in the model performance , along with the number of parameters for each attention type . specifically , we set the attention type as 0 . 3 and the beam size as 5 , which gives a significant ( 7 % ) boost in performance . we use the b - lstm encoder type and the lstm layer as our decoder type .
the results on the wmt17 it domain are shown in table 2 . the mt bojar et al . ( 2017 ) model achieves the best results with a ter / bleu score of 66 . 60 / 71 . 03 and 35 . 59 / 59 . 21 on the training set . by comparison , the bleu model by bérard et al . , 2017 achieves the highest score of 43 . 58 / 71 , a result not found to be significant even under the difficult requirement of a training set of 500k words . the spe model by varis and bojar , 2017 , achieves the second highest score with 24 . 69 / 62 . 97 and 25 . 28 / 59
the performance of the best setting for each property . it can be seen in table 3 that the word2vec embeddings perform well with respect to precision and recall . however , precision is relatively lower than recall , indicating that the object used for transportation is of less importance . regarding word2versation , we find it performs better than glove with a f1 score of 8 . 2 vs . 7 . 8 for the comparison set .
table 6 shows the mean predictions for linear ( l - bilstm - s ( 2 ) and tree models ( t - silstmp - s2 ) on uds datasets . the tree models generally perform better than the linear ones , with the exception of the case where acl obtains the best performance . as shown in the table , acl : relcl is comparable to ccomp , but significantly worse than advcl and xcomp . also , contrary to intuition , the advcl model do not give significantly good results , which indicates that it is unable to distinguish between the semantic and syntactic relations in the deep layers of the lstm . when trained with only one relation at a time , the model performs worse than the other methods . we notice that acl has better performance than ccomp and advcl in relation to relation extraction . on the other hand , when trained with both relation extraction and concatenation , the acl model achieves the best results ,
the lexicons used as external knowledge are shown in table 1 . it is clear from table 1 that sentiment and sentiment are the most important components for the lexicons , followed by language antonyms . however , not all sentiment - detailed lexicons are suitable for this task , as sentiment is relatively rare in nature .
consistent with the observations by vaswani et al . ( 2017 ) , we observe that the affine representation is closer to the baseline than the concatenated one , confirming the importance of the curvature of the spine . in fact , affine representations seem to have superior generalization ability , i . e . that they are more suitable for production use in low - supervision settings . the model trained on sent17 , 18 and 19 outperforms the baseline substantially all other methods apart from the case of sst - 5 , which shows the extent to which the semantic information captured by embeddings can be improved with a reasonable selection of the relevant features .
the results are shown in table 3 . perhaps the most striking thing about the small difference in mean labels between linear and tree mae is that the linear mae baseline tends to have higher precision , indicating that the model can distinguish between the true answer and negative responses more easily . however , this is not the case for all models : for example , in the case of tree mae , the difference between yes and no is much smaller .
the correlation of the coefficient of determination ( r2 ) between the three metrics shown in table 2 shows that sigvac outperforms siguni , indicating that the model can easily distinguish between the true response and negative responses .
table 3 compares the coefficient of determination ( r2 ) between automated metrics and crowdsourced topic - word matching annotations . we include metrics measuring both local topic quality and global topic quality . the results show that the transfer learning approaches based on sigvac outperform the best - performing crowdsourced approaches on three of the four metrics ( table 3 ) by a noticeable margin .
the results are shown in table 5 . we see a noticeable margin brought by our proposed method over the strong baselines on average p @ 01 and p @ 10 scores , indicating that our proposed approach has superior generalization ability in the low - supervision settings . the largest gains are brought by the kce model , which shows a performance gain of 2 . 43 % on average .
the results are shown in table 5 . loc and event features dominate the performance of all relation extraction methods except for the frequency and event extraction . more importantly , for event extraction , the model performs significantly worse than the best previous state - of - the - art model ( fancellu et al . , 2016 ) . the combination feature of event and entity extraction gives a significant performance boost , but only when they are considered in combination with other features such as frequency and clustering . the model performs best when using only one type of feature group , namely , the loc and entity features ,
table 7 shows the five instances ( out of 50 ) with the highest absolute prediction error . most of the errors in these instances are grammatical errors .
table 5 shows the cosine similarity between event entity pairs in word2vec embeddings . the kce scores of the attack , arrest , and murder are statistically significant ( p < 0 . 05 ) with respect to the standard clustering function , but are significantly lower for hotel , domain and event pairs . domain entity similarity is low but consistent with the expected low kce score , indicating that these are low - frequency events that are difficult to predict . accusations are rare but not unheard of , as in the 911 attack ( eq . 3 ) and the recent war ( eq . 9 ) . domain similarity is high but low , as seen in table 5 .
table 3 shows the percentage of unique [ italic ] n - grams and tbcs in the wt103 and wt103 datasets for self - and other models . self - bleu outperforms both bert and bert ( large ) when trained and tested on the standard wt103 dataset . it achieves the best results with a bert - score of 10 . 27 % on average , nearly 5 % higher than the previous state - of - the - art . interestingly , the bert model trained on wt103 outperforms all the base models except for the one that relies on word embeddings .
bert ( large ) and gpt ( small ) models achieve relatively high accuracies ( p < 0 . 05 ) on the corpus - bleu dataset ( see table 3 ) . on the wt103 and tbc datasets , small bert models achieve 7 . 48 % and 7 . 28 % higher accuracies , respectively , compared to the previous state - of - the - art . gpt also achieves higher performance than bert , showing the advantage of finetuning word embeddings during training . though small , gpt models are comparable in difficulty to bert and bert - large , their performance on wt103 is considerably worse .
table 3 presents the results on msnbc , aida - b , wiki and ace2004 . our model outperforms all the base the results reconfirm that the ability to select compact regions induces the generation of accurate captions . the average precision of these captions is high , with the exception of ace2004 , when precision is low .
table 2 compares the f1 scores of our model when it is weakly and fully - supervised on wikipedia and on aida conll . the results show that the model performs comparably to the best performing ones on msnbc , aquaint , ace2004 , and wiki .
table 3 : ablation study on aida conll development set . we show the f1 scores of our model ( without attention and with attention ) and without local disambiguation ( with attention ) on the five test sets . the results show that our model exhibits the best performance .
table 4 shows the performance by ner type on the aida - a dataset . our model obtains the best performance with a per - f1 score of 97 . 20 % , slightly higher than the previous state - of - the - art .
table 3 presents the results for english , german , dutch , french , spanish and turkish for the original mwe models and the debiased variant of mwe - based models . the results are presented in table 3 . our h - combined model improves upon the strong baselines by 3 . 63 points in the discontinuous category and by 4 . 45 points on the standard mwe model . by further adding gcn - based tokens , the results are slightly worse than the baseline but still superior to the approach described in section 2 .
table 9 shows the mae ( normalized by the number of tokens ) on events in uds - ih2 - dev that are xcomp - governed by an infinitival - taking verb . it is clear from table 9 that the l - bilstm models are well - equipped to handle the challenge of lexfeats .
in table 2 , we compare the performance of the h - combined and gcn - based systems with the baseline models on the discontinuous en , fr and chime - 4 datasets . the results are shown in table 2 . the h - combined model significantly outperforms the baseline model in terms of f - score while the gcn model performs slightly better than the baseline on all four datasets . though the smaller performance gap between the two sets suggests that there are some performance efficiencies to be found in combining the two models , we found that these results were not significant enough to warrant a change in our model .
table 3 presents the test set of the best performing models . our model outperforms all the base lines with a gap of 10 . 6 cola vs . 7 . 9 sts points from the last published results .
table 1 shows the performance of the models with and without task training on the standard task tasks . we show the results for the cola , qqp , and sst models . with the exception of the sst model , all the other models perform well on the basic task .
table 3 shows the cola , sst and mnli scores of all models trained on the 20 % held - out validation data . for the qqp task , the average cola and the average sts scores are statistically significant ( p < 0 . 00 ) with respect to both groups , with the exception of the case of the qnli .
table 1 shows the ne - tags of numbers in wikipedia . according to the table , of the 2000 entries in wikipedia , the number with the highest frequency ( 54 . 28 % ) is the most interesting one , followed by relation cardinality ( 18 . 86 % ) and time span ( 2 . 92 % ) .
table 2 shows the number of wikidata entities as subjects for each predicate ( p ) and the rn ( p < 0 . 01 ) for both vanilla and only - nummod . table 2 also includes the f1 scores of the baseline and onlynummod models . the only case where the baseline has part ( creative work series ) and part ( human - generated content ) is presented in table 2 . it is clear from table 2 that the wiki entity that contains the most interesting part is the domain admin . terr . entity , as its p and rn are both significantly higher than those of the other two . additionally , the presence of a spouse or child ( manual ground truth ) underscores the gender - parity in the production setting . table 2 summarizes it even further by showing that the child is much more interesting than the spouse .
we notice that the w2v model significantly outperforms the cnn model and the lstm model in the int and word categories . moreover , the word model shows a significant improvement in accuracy in the low - supervision settings , indicating that the semantic information injected into the model by the additional cost term is significant enough to result in a measurable improvement in the accuracy .
table 2 shows the las improvements by cnn and lstm in the iv and oov cases on the test set . in both cases , δoov decreases significantly compared to the baseline cnn model . in the iv case , δiv decreases by 3 . 36 points while in the oov case , by 0 . 59 points . these results show that the model developed using the learned reward function can significantly improve interpretability without sacrificing accuracy .
table 3 compares the performances of the models using different training and test datasets . for the russian , ukrainian , belarusian and ukrainian datasets , we see 772 % and 698 % accuracy on average , respectively , compared to 627 % and 756 % on the test dataset . with respect to the ukrai - nian dataset , there is a 1 . 8 % boost in accuracy for the russian and a 2 . 4 % boost for the belarusian .
table 1 compares the bleu and exact - match scores of our model with previous approaches on the held - out test set . neural mrs ( gold ) outperforms all the alternatives except for ace ( see x4 ) . with respect to overlap , the gap between the coverage and match rates is slim , with the exception of the dag transducer ( ye et al . , 2018 ) . although ace ( erg ) obtains a lower coverage percentage than neural mrs ( 76 . 43 % vs . 79 . 37 % ) , it achieves a higher exact match rate ( 83 . 37 % ) than the neural model ( 71 . 63 % ) and a higher bleu ( 83 ) . exact match rates are higher than any other approach , indicating that neural models are well - equipped to perform this task . we observe that the transfer learning method developed by peyrard et al . ( 2018 ) achieves the best results with a coverage rate of 83 . 37 % , nearly 5 % higher than the previous state - of - the - art . interestingly , the model trained with only one type of overlap layer , the gold - silver layer , achieves the highest match rate , showing the advantage of redundancy removal . the gap between overlap and match rate is narrower with the silver layer , showing that neural mrs is better able to distinguish between noisy and noisy states . finally , we see that the cross - layer neural network performs similarly to the original neural model ,
hyperparameter values are shown in table 3 . the largest size of the bigram emb size is 50 while the smallest is 200 . regularization gives a 0 . 5 bleu gain over randomization . lstm learning rate is 0 . 015 , with a decay of 0 . 05 bv .
we further compare our models with the best performing baseline models using only one type of word embeddings : auto seg ( hochreiter and schmidhuber , 2008 ) or the lstm ′ model ( lstm , lin et al . , 2017 ) . the results are presented in table 1 . the first set of results show that the combination feature - rich word embedding models perform best , with the exception of the ' auto seg ' model , which achieves the best results with a f1 - score of 73 . 03 . adding softword features further improves performance , but still results in a lower f1 score than no seg . moreover , the model using only char baseline performs slightly worse than the baseline model .
the models trained on the word baseline outperform the models trained using no seg or word baseline . moreover , the performance gap between the gold and auto seg models is narrower when we add in the bichar baseline , indicating that the semantic information injected into the model by the pre - trained word embeddings is significant enough to result in a measurable improvement .
our results in table 6 show that the softword baseline and word baseline contribute similarly to the task , with the exception of the case of " softword " . however , the difference is less pronounced for the char baseline , indicating that it contributes less . the lstm model developed by chen et al . ( 2006a ) achieves state - of - the - art results , outperforming both the baseline and the word baseline on msra .
the results on resume ner are shown in table 8 . the lstm model ( gillick and favre , 2008 ) outperforms the word baseline and the lattice model ( lattice et al . , 2009 ) by a noticeable margin . the difference is less pronounced for softword embeddings , but still significant .
we show the development set results for english – estonian in table 3 . the results show that the finetuned model considerably outperforms the monolingual model and the linked embeddings model . the bleu % and character - f scores are both high ( 57 . 45 % ) and ( 59 . 63 % ) indicating that finetuning has a significant impact on the model performance . adding / removing a component or two affects the performance , however , it does not improve significantly .
table 2 presents the results on the pbsmt dataset from sestorain et al . ( 2018 ) and dual - 0 . in general terms , the results are summarized in table 2 , we observe that our baselines are comparable to the best previous state - of - the - art models on both datasets . however , our model significantly outperforms the previous state of the art on two of the four datasets , as shown in the table , the difference between the accuracies obtained on the basic and final set is most prevalent in the case of the former , when we switch to the latter , we see that our model performs significantly better .
the results of semantic feature ablation are shown in table 3 . all attributes except num , tense and tense were removed and replaced with gold data only . the bleu score of the model improved by 0 . 27 points over the baseline showing that no edge features were present in the gold data .
our baselines are shown in table 2 . the summaries generated by sestorain et al . ( 2018 ) on the pbsmt - 0 data are summarized in bold . as the results show , the trained models perform better than the unsupervised ones when trained and tested on the same dataset ( i . e . , when the training set size and type are considered ) .
the results are shown in table 5 . our baselines basic and distill are qualitatively very similar , but slightly different than the previous work . as hard coreference problems are rare in soft coreference , we observe that the difference between the results on distill and hard coreference is less pronounced . however , when trained on the additional data , the gap between the two baselines becomes much larger .
the results on the official iwslt17 multilingual task are shown in table 4 . our baselines outperform the strong baselines and the zero - shot baseline , confirming the value of parameter sharing .
results on the proposed iwslt17 test set are shown in table 5 . the results show that the proposed method significantly improves the interpretability by increasing the recall threshold for boundary expansion .
the results are shown in table 1 . our system outperforms all state - of - the - art methods in the standard tasks . in all but one of the comparisons , our system obtains the best results . in the exceptional case of the europarl , both the en - et and en - ru subsets perform better than the best previous methods .
bleu scores for the bilingual test sets are shown in table 3 . our model ( 38 . 13 % ) obtains significantly better performance than the contextual baseline on all three sets , making it comparable to or better than the best performing baseline ( 26 . 84 % ) . on the europarl set , it achieves 21 . 49 % better performance , significantly better than both the baseline and the previous best performing model ( 25 . 09 % ) by a margin of 2 . 36 % . on the subtitles dataset , it improves by 2 . 59 % .
table 4 shows the bleu scores for the different contexts . the first set shows that no context ( the base model ) and other language contexts cause the model to perform worse than the current context . when context is considered , the current turn performs better than the previous turn .
table 3 compares our proposed method with the best previous approaches on the 300 - dimensions . we find that the utdsm model achieved a final score of 69 . 2 ± 0 . 9 on average , slightly higher than the previous state - of - the - art . generalized attention mechanisms ( utdsm , gmm and random ) outperform all the base table 3 shows that the transfer learning approaches based on the three types of data augmentation achieve similar results across the three dimensions . however , the results are slightly less clear regarding transfer learning methods with different feature sets . for example , generalized attention target dams ( urda et al . , 2015a ) achieves 69 . 6 % overall score , but drops significantly compared to liu et al . ( 2015a ) and lee and chen ( 2017 ) . similarly , the average score of maxsimc ( 67 . 1 % ) and rocal scores ( 68 . 4 % ) are significantly lower than those of utnsm ( 69 . 6 % ) and lee et chen ( 71 . 4 % ) . we see that transfer learning models using only one type of algorithm , namely , the gmm model , achieves the best results across all three dimension sets .
table 2 shows the bleu scores for domain match experiments on the standard wsj and wikipedia datasets . the wsj training data significantly outperforms the brown and conner ( p < 0 . 05 ) model , indicating that the training data are more useful for domain prediction . the giga dataset outperforms wikipedia , however it is inferior to the wsj in terms of match rate .
table 2 shows the performance of the method for multi - class text classification . our average d model outperforms all the base the difference in precision between recalls and accuracy is minimal , however we see significant difference in f1 - score due to high recall and low accuracy . our method outperforms the lda and maxcd models , showing that precision is relatively high while accuracy is low .
the performance results on paraphrase detection task are shown in table 3 . our method obtains the best performance with a f1 - score of 69 . 7 , which indicates that it is comparable to the performance of the best state - of - the - art dsm model .
we investigate the effect of the different approaches on the model ' s macro - f1 score in table 2 by comparing the results of 50 % and 75 % accuracy on the four types of known intents . we observe that , compared to softmax , the doc model performs better on all four types , indicating that the semantic information extracted by softmax is more useful for target prediction . moreover , the accuracy remains the same across all mod table 2 shows that , for the most part , softmax models perform well , with the exception of snips . however , when using lof and lmcl , performance on snips drops significantly . it is observed that the accuracy obtained by doc ( 72 . 8 % ) and lof ( 76 . 6 % ) is considerably lower than that by softmax ( 71 . 5 % ) and msp ( 63 . 6 % ) .
table 1 compares the performance of our system with the best performing system , logreg , using the truncated average of the four most important statistics . for a wait - 3 system , we set the time - indexed lag at 0 . 25 and set the score at 2 . 25 . with our truncated averaging , we get 4 . 28 % better results on average compared to logreg .
table 4 shows the bleu scores for evaluating the amr and dmrs generators on an amr test set . amr shows lower performance than dmrs , indicating that the dmrs generating method requires more attributes to perform well .
table 3 presents the results for the multinli matched dev set , the multi - nli nested set and the nested set multi - domain set . the results are presented in table 3 . all models show similar performance across the three types : the simple , basic , and complex neural models . the large difference in performance between the simple and complex set is mostly due to different neural models using different word embeddings . in the simple set , neural network performance is relatively consistent while in the complex set , performance is significantly worse . further , the difference between the averages of the basic and complex sets is less pronounced for the multi - nli set .
table 1 provides exact scores for the context and output . table 1 shows the performance of the bleu and meteor using word embeddings . note that the only difference in performance between contexts is with regard to context . when context is used , the performance gap between de → en and et → en is minimal , however it is significant with respect to output performance . using lexical contexts proves beneficial for both datasets , improving the recall performance for both contexts .
the semantic threshold for human is set at 0 . 25 while for syntactic and semantic word embeddings is 0 . 59 . we observe that human is already well - equipped to perform these tasks on the semantic threshold : syntactic treedepth , syntactic topconst and syntactic subjnum are both set to integer precision with accuracies above chance , while semantic concatenation is set to zero . semantic boundary features are set to concatenate , but are broken down into semantic and syntactic keyphrases , which results in significantly less accuracy .
table 5 presents the results of the models trained on the similarity and relation extraction tasks . our model ( sentiment analysis mr ) improves upon the strong baselines by 3 . 36 points in relation extraction on sst2 and 3 . 43 points on sentiment analysis . it closely matches the performance of sst5 with only 0 . 59 % absolute difference . on the other hand , it performs much worse than sst3 and subj , with a gap of 2 . 59 points on paraphrase and 5 . 45 points on relation extraction . the results are presented in table 5 .
the results are shown in table 5 . we present the precision numbers for the 20 - ng and 25 - ng positions and the f1 scores of the best models . for the sst - 5 dataset , we report p - means ( ρ = 0 . 27 ) and p - values ( p - values ) which show that models using pca and dct * achieve better precision than those using random clustering . moreover , our model obtains higher roc scores than both the best previous approaches .
table 1 shows the coverage of words from the manual transcripts in the different batch that make up the dstc2 development set . the pruned cnet transcripts show lower coverage , but higher coverage than the original ones . the slots / values analysis shows that slots removal have a high impact on the performance of the model , lowering the average precision of the generated sentences by 10 % in the standard asr task formulation and to parity in the gold - two - mention case .
the results are shown in table 1 . we observe that the first set of results show that , when only using the best performing cnet with the minimum score threshold , accuracy is relatively high while recall is low . the second set shows that the weighted pooling approach achieves the best results , lowering recall to 62 . 6 % on the 1 - best baseline and 94 . 9 % on 2 - best requests .
table 2 : dstc2 test set accuracy for 1 - best asr outputs of ten runs with different random seeds in the format average maximumminimum . we show that the training set size and the number of requests per asr iteration are the most important factors in achieving the best performances . the average asr accuracy of the train set is 63 . 5 % on average , slightly lower than the baseline but still superior than the quality of the live asr .
table 2 shows the total number of tokens for each training and test set . there are 31 , 545 tokens in total , constituting 45 . 4 % of the training and 5 , 563 of the test sets . the remaining tokens are divided into 25 categories , which are mostly language specific .
table 1 : manually aligned news commentary data . we show that for english , spanish , french , dutch , russian and turkish , the average number of tokens per sentence is 1 , 654 , 313 , 287 , 276 and 287 , respectively , with an average of 173 .
table 1 shows the precision numbers for the gold and silver classes . the first group shows that precision on the bilstm is relatively high while recall on the gold class is low . second group shows the auc and roc scores of the best performers .
table 3 shows the bleu scores for the standard and pseudo - parallel test sets for the two sets . for the standard set , we see 21 . 59 % improvement on average compared to the previous state - of - the - art . note that the improvement on the ru and ja sets is much larger . on the other hand , the increase on the en and ru sets is only 2 . 94 % and 2 . 59 % , respectively .
the results are shown in table 2 . proto ( bert ) and proto - adv achieve better results on the 5 - way 1 - shot compared to gnn ( gnn ) and bert - pair ( bertr ) . on the 2 - shot test set , the proto model achieves higher accuracies than the gnn model on both sets , indicating that the accuracy obtained on the one - shot test set is more accurate . gnn , on the other hand , achieves lower accuracies on the two - shot set . when trained with cnn - trained models , the accuracy gap between gnn and proto decreases significantly ( p < 0 . 05 ) . in the single - shot setting , bert and proto achieve higher accuracy scores than gnn . the similarity between the accuracy scores on the 1 - shot and 2 - shot tests indicates that neural networks developed using the baselines are more effective in generation of accurate targets .
the results are shown in table 5 . proto ( bert ) and bert - pair achieve better results on the accuracy on the five - way - 1 - shot tasks than the best baseline models ( hochreiter and schmidhuber , 1997 ) . the accuracy remains the same across all setups , with the exception of the one that uses cnn embeddings ( lin et al . , 2017 ) . as shown in the second group of table 5 , the accuracy of the three baselines is relatively consistent , with proto achieving an accuracy of 74 . 28 % , 69 . 63 % , and 62 . 63 % on the 5 - way1 - shot tasks , respectively , compared to the previous state - of - the - art . on the cnn - based dataset , accuracy remains relatively the same , but with a drop of 2 . 59 % on average compared to previous work . in the two baselines , bert and proto achieve higher accuracies than their counterparts .
table 3 shows the precision numbers for the standard and alternative approaches for the semantic keyphrases . we observe that the alternative approaches that aim to improve recall accuracy , such as the coh algorithm ( afet - coh ) and its variants , generally perform better than the original ones . however , their accuracy numbers are still significantly lower than those of the original models .
table 1 shows the performance of our model on the four types of training data . in general terms , we see that our prec . and rec . level models perform well , with an f - 1 score of 1 . 0 and 0 . 9 respectively indicating that the model can easily distinguish between the true response and negative responses .
we observe that the most distinguishing features of the is_colourful and is_thin features are the is - strong and is - dangerous features , respectively , which show that these features are strongly linked to negative sentiment . however , the presence of these features does not indicate that they are beneficial for prediction performance . we notice that the features considered by is_strong are slightly beneficial , but only when they are considered in combination with other features such as is - electric , is - thin , is_expensive and is . . . .
table 4 shows the training time and parameters for our model . the average training time for both models is 5h 30m . for the x - bilstm model , the average time to learn is 25h 40m while for the h - bashm model is 2h30m . both models use the same set of parameters , which makes it easier to learn .
the results are shown in table 3 . crowd is predicted to be slightly more dangerous than it is used to be . full - has - wheels is the most dangerous class , followed by full - is - dangerous . the other classifiers show lower performance on generalization . in general terms , full is the better performing class , indicating that it is more suitable for use in low - supervision settings . when full is added to the training data , it produces significantly worse results than full is .
we notice that the semantic threshold for some models is relatively high while for others it is low . for example , snli shows a gap of 10 . 40 points with simverb3500 and simlex999 on average . this gap is mostly due to small size of the training data set ( low some models that perform poorly in semantic benchmarks ( e . g . mturk287 , rg65 , simlex5000 ) are able to regain a lot of accuracy when trained and tested on a larger corpus . these models generally outperform the best performing ones that do not use semantic threshold features .
classification accuracies are presented in table 5 . we present the results of the best performing models for english and german captions . for english captions , snli ( which relies on word embeddings ) outperforms both mpqa and sst5 . it closely matches the performance of snli with only 0 . 40 % absolute difference . semantic similarities are low but significant with respect to both subsets , with sickr ( 83 . 83 % on average ) and similar on stsb ( 84 . 44 % ) . sst2 and subj classification are both comparable with snli , but do not exceed the level of similarity expected by snli . to test the semantic similarities of these models , we compare them against each other in word analogy task . we observe that snli performs better in the semantic similarity task than in the syntactic one .
table 2 shows system performance on the so - called “ gotchas ” by gender and difficulty , specifically , we see that for female pronouns , the system performs uniformly worse than it does for male pronouns . for all but one of these categories , the difference is less pronounced . in the case of " gotcha " sentences , the stat system performs much better than the neural system .
the results are shown in table 1 . our model outperforms all state - of - the - art methods on all metrics except for meteor . it achieves the best results with a precision of 0 . 86 % on rouge - l and 0 . 94 % on cider . it closely matches the performance of the best performing base models ( facts - to - seq and static memory ) .
accuracies for the approaches are shown in table 2 . advdat outperforms infersent and advdat , indicating that the adversarial approaches yield better performance on the hard subset . however , advcls outperforms advdat in the soft subset , showing that the effectiveness of adversarial methods can be improved with a reasonable selection of the lexical resource from which the argument was derived .
table 3 shows the percentage decrease from baseline advdat to the current state - of - the - art advdat baseline on the four categories . in general terms , we see that sleeping , waking , driving , no and empty are the mostchallenging categories , as their performance decreases significantly compared to the baseline . further , contrary to intuition , the presence of a single entity that does not contribute to the task at hand , such as nobody , cat , and / or asleep , improves performance for all but driving . finally , for the empty category , our model shows a slight improvement .
we present the results of our final model on the hidden test set of cbow in table 4 . the results show that our model outperforms all the base lines with two tasks to spare . it achieves state - of - the - art results , outperforming all the alternatives except for srilm , which obtains a performance gap of 2 . 5 points with the cbow model .
comparison with other news datasets that handle documents or news clusters is shown in table 2 . the largest news clusters datasets are ace 2005 and ere , while gnbusiness is comparable to astre .
with and without mass preservation , the bleu and dal scores show that the precision obtained by the precision measures is relatively high , indicating that precision obtained using the best performing methods can be achieved with a reasonable selection of the correct subset of the training data . the preserved dal score is low , but still comparable to the unrepressed ones , showing the value of mass preservation .
the results of schema matching are shown in table 4 . the clustering method outperforms the odee - fe model , indicating the advantage of finetuning word embeddings during training . however , when matching entity nodes , clustering does not improve significantly , showing that the ability to pick out relevant parts of a schema can be improved with a reasonable selection of the lexical resource from a supporting corpus . matching accuracy is relatively high , with an f1 of 58 . 8 % indicating that clustering methods are effective , compared to the 42 . 3 % f1 score of odee . odee matching accuracy is slightly lower than clustering , showing the difficulty of semantic extraction .
the results shown in table 5 show that our odee - fer model significantly improves the slot coherence by increasing the average true positive rate by 1 . 18 % compared to the baseline odee model .
our method outperforms all the base lines with a gap of 10 . 8 % on meteor and 48 % on rouge - l .
the system and human generate identical test abstracts , however , the system is much more likely to plagiarize than the human . table 3 shows the percentage of n - grams of text that the system or human has plagiarized in the training data . the system is 94 . 2 % sure of generating the correct text , while the human is 78 . 6 % sure .
iteration comparison between meteor ( 13 . 6 % ) and rouge - l ( 20 . 5 % ) shows that the former has better performance on synthetic dataset . however , the latter has higher precision .
table 4 shows the passing rates of the different titles for different nlp tasks . as a baseline , we also compare against the performance of the best performing non - expert and expert nlp models . the results , summarized in table 4 , show that the different title tags give a significant boost in performance , with different titles giving a boost of more than 5 % in the standard task formulation . however , still less than 50 % of the test cases ( 50 tests ) contain the correct title , which means that more than half the sample could potentially be misclassified as incorrect . selecting the wrong title or entity once the task is completed degrades the model performance .
table 3 shows the results for english and german captions . in general terms , the results are presented in table 3 , we see that the visual and verbal captions are comparable , but that the svm performance is significantly worse than the vocal one . the svm scores are significantly higher than those of random and human , indicating that the ability to distinguish between the two types of speech is enhanced by the presence of more context information . svm performs similarly to verbal and visual captions , with the exception of the vocal classification . when trained with only svm and random features , the performance gap between verbal and non - verbal captions is narrower than that between visual and random , indicating the syntactic patterns discussed in section 4 .
table 5 presents the results of models trained on the hidden test set of hotpotqa in the unsupervised setting . we observe that , let alone a reduction in performance , all models perform comparably to random and human , with the exception of tfn . random and human are both better than tfn and hf , but tfn is inferior to random in terms of s + p and s + i . as shown in the second group of table 5 , the ability to select compact regions induces the model to rely less on superficial cues . selective attention mechanisms like the hf and tfn networks reduce repetition , but do not improve generalization ability .
as shown in table 2 , the model learned to perform well on the mednli task with three different combinations of pmc and pubmed datasets . the difference in accuracy between pubmed + pubmed and pubmedd datasets is minimal , however we see significant difference in the performance on the accuracy test . adding pubmed content improves the model ' s performance by 4 . 83 % on the accuracy test , though still performing substantially worse than pubmed .
the results of the two tailed t - tests are shown in table 4 . the results show that the fixed - tailed method outperforms the approach by a noticeable margin . dual - tailed t - test sets result in significantly better results than the single - tailed approach . in fact , the results are slightly superior than those of the balanced approach ( p < 0 . 05 ) when we only consider data from one domain , namely , that of bilstm , deat and bimpm ( table 4 ) .
the accuracy on the test set is shown in table 3 . the feed - forward model ( lstm + feed - forward ) achieves an accuracy gain of 0 . 524 points over the baselines ( p < 0 . 01 ) over the simple lstm model .
we observe that the window position that gives the best performance affects the semantic analogies for simlex999 the most . for both symmetric and asymmetric modes , the average performance is slightly better than that for symmetric mode .
we observe that for the simlex999 dataset , the semantic analogies generated by gw false are slightly less accurate than those by gw true , but still comparable to the performance of true counterparts . for simlex998 , the performance gap between true and false analogies is less pronounced , with a gap of 0 . 44 points across all contexts .
we observe that the removal of the stop words significantly ( p < 0 . 05 ) improves the performance for all models .
table 1 shows the mean matched validation accuracies by type of pooling method and presence or absence of character embeddings . in general terms , the w / o . chars method results in significantly better performance than the last state - of - the - art method ( 71 . 6 % vs . 71 . 2 % ) , indicating that the ability to select compact regions induces the generation of better captions . however , the accuracy remains relatively low ( 60 . 7 % ) when using only plain random pools containing only w . chars with respect to embedding , we see that this method produces only marginal improvement ( 15 . 9 % ) over the strong baselines .
we report the precision accuracies of our best model , cbow , esim and multinli , broken down by genre in table 3 . across all three genres , our model improves upon the previous state - of - the - art cbow model by 4 . 8 % on average .
we show the performance of the models fine - tuned and initialized with different metrics for the glue task in table 1 . the smaller size of the training set indicates that the pre - trained models are more suitable for this task , as the accuracy obtained with the smaller training set is higher . however , the larger size does not help the models perform well in the standard task . with a training size of 440k , the acc achieved by qnli model is only comparable with that by sst - 2 . by further adding mnli - m features , acc reaches 8 . 6 % , marginally improving performance over the previous state of - the - art . sts - b , on the other hand , achieves the best results with a f1 / acc of 5 . 3 and a rate of improvement of 2 . 6 % over the small size baseline .
comparisons with some recent points in the literature . table 3 compares our model with the best performing ensembles . scores are tokenized bleu . our model obtains a bpe score of 39 . 3 , compared to 39 . 8 by wu and 39 . 9 by lee ( 2016 ) .
character versus bpe translation . table 2 compares the performance of the original bleu embeddings with the translations using the current set of tokens . our system performs better than the pre - trained translatebleu system in terms of all three domains .
table 4 shows the error counts for 100 randomly sampled examples from the deen test set . most of the errors in the test set are caused by lexical errors .
table 6 shows the ablation results on the wmt15 deen dataset . the bilstm encoder achieves a final accuracy of 31 . 2 % on bleu , compared to the 28 . 5 % accuracy of lee et al . ( 2017 ) . the size and type of bpe size of the encoder are the most important factors in the performance of the model , as those numbers indicate that the number of parameters used to encode the data is relatively small . however , when pooling the data , the accuracy drops to only 29 . 6 % . we notice that the bi - stm model performs slightly better than the hm model , indicating that more layers of data are required to perform the task efficiently .
we compare our proposed bert model with the previous state - of - the - art bilstm model on the μr and epm datasets ( zubiaga et al . , 2017 ) . the results , summarized in table 1 , are broken down in terms of μr , f1 and λ scores , with bert achieving higher precision on both datasets with good recall ( μr = 0 . 701 , 0 . 722 and 0 . 861 ) compared to the previous best results ( 0 . 59 , 0 . 861 ) . moreover , bert achieves a lower precision on the λ and puc measures , showing that the model performs better when trained and tested with different lexical features .
table 3 shows the mean precision ( map ) of the models on political speeches . bold indicates best performance , underline indicates second best . bert achieved the best performance with a precision of 0 . 302 on average , slightly higher than the previous best estimate by konstantinovskiy et al . ( 2018 ) .
table 4 compares the f1 score of the manual relabelling of the top 100 predictions by puc model with the original labels in each dataset by two different annotators . in general terms , we see that the results displayed in table 4 show the performance of the model by using the best performing annotator . twitter , for example , obtains an f1 of 1 . 311 , while wikipedia , at 0 . 859 , is slightly higher than the average value of 0 . 870 . the four other domains show lower f1 scores . table 4 shows that google translate , although it performs better than twitter , is inferior to all the other domains except for the one that it is used for politics .
table 1 shows the precision and recall numbers for the positive and negative predictions , as well as the f1 scores of the different neural models trained on the stack overflow dataset . the results show that , let alone a drop in performance , the positive neural models perform well , with an f1 of 1 . 084 and a recall of 0 . 884 on average . the negative neural models , on the other hand , perform much worse than the positive ones . f1 numbers show that the negative neural networks are much more difficult to train than those positive .
we show the precision numbers for each model in table 3 and compare them against the unsupervised baseline . in general terms , we see that the cosine model outperforms the other baselines on average . the average precision numbers of cosine and rv are significantly higher than those of ellipsis , indicating that the semantic information injected into the model by the additional cost term is significant enough to result in a measurable improvement . however , it should be noted that cosine is still inferior to rv in terms of accuracy , and its average precision is significantly lower than that of 10rv . semantic features such as semeval17 and semantic word embeddings outperform the other base models , showing the importance of semantic information extraction .
the results are shown in table 6 . direct and source bridging bridges achieve higher performance than target bridging , indicating that source and target attention are different . rnnsearch * achieves a transfer learning gain of 4 . 63 % on average compared to vanilla , which achieves a gain of 2 . 36 % overall . when we add out the effect of smt and domain - aware embeddings , we get a 0 . 91 % gain on average .
the results in table 2 show that the shared - private model is significantly better than the vanilla transformer model ( p < 0 . 05 ) and the three - way wt is comparable to the strong baselines . direct bridging , however , only achieves a marginal improvement of 0 . 5 % over the strong baseline . the bleu score computed with the shared private model achieves a better result of 98 . 62 % , which indicates that the model can be further improved with a reasonable selection of the required features .
table 3 shows the results for the iwslt { ar , ja , ko , zh } - to - en translation tasks . the results show that the shared vocabulary approach by itself places the model higher than the best previous approaches on the bleu test set . however , when combined with the use of the vanilla embeddings , the accuracy drop from 43 . 6 % to 48 . 4 % . table 3 also highlights the extent to which the semantic information differences between the shared and unshared vocabulary sets can be overcome by leveraging syntactic or semantic information from the same source ( e . g . , the en / en dictionary ) .
table 4 shows the performance of the models using different sharing coefficients to train the models on the validation set of the nist chinese - english translation task . shared - private models perform better than shared - public ones , but do not perform as well as shared - private ones . when using only one sharing coefficient , the performance gap between zh - en and emb . ( which takes the average of the two sharing coefficients ) is less pronounced , but still significant with a gap of 2 . 3 % on average . the bleu model performed similarly when using a zero - sharing coefficient .
table 2 shows the average time for users to set up the tool and identify verbs in a 623 word news article . the differences between gate and either slate or yedda are significant at the 0 . 01 level according to a t - test . table 2 also shows that gate is comparable to slate in terms of productivity , however it takes significantly more time to install and use the tool compared to slate . since the training set size is small , we used only one participant ( the one who set up gate on ubuntu ) and brevity is low . overall , the training time takes 18 minutes on ubuntu and macos .
we also evaluated the syntree2vec and word2vec embeddings . in table 1 we report the perplexity scores of the nodes and sentences , and the number of instances per sentence , for each language . for syntree , we obtain 43 . 59 % and 43 . 41 % accuracies , respectively , with a total of 71 . 20 % and 71 . 44 % precision scores . for wordtwovec , we get 25 . 01 % and 25 . 59 % ) accuracy .
as shown in table 1 , there are four types of question labels for asnq . yes , no , yes and no refer to answer sentences , no , no and yes answer phrases respectively . in addition , sa refers to answer sentence , long answer passage and short answer phrase respectively . table 1 shows the label description for each train and sub - train .
the results are shown in table 5 . our model achieves the best results with a map of 0 . 903 on the asnq and tanda datasets , while roberta achieves the highest mrr . the bert - l ft model shows much lower performance on the wikiqa dataset compared to the original asnq embeddings , indicating that the semantic information injected into the model by the additional lm and lc are significant enough to result in a significant improvement in performance . however , our model performs slightly worse than the original as shown in fig . 5 .
we compare our model with the best performing approaches : the roberta - b model ( asnq → trec - qa ) on the fixed - fold test set and the bert - l model on the expanded test set . the results are shown in table 5 . the first set of results show that , let alone a drop in performance , our model performs well on the training set with a minimum of 0 . 928 % absolute improvement over the previous state of - the - art .
the results are shown in table 5 . as can be seen , the drop of precision on the wikiqa map and trec - qa metrics indicates that fine - tuning has a significant impact , lowering the precision on both datasets . the drop of 22 . 63 % and 22 . 03 % of the precision values on the trec and wikiqa metrics is higher than those on the bert - base .
as shown in table 6 , the neg and pos labels have the least and the most significant effect , respectively , on fine - tuning bert . for example , neg reduces the precision of the map and trec - qa metrics by 0 . 895 and 0 . 884 respectively , while it increases the precision by 1 . 898 and0 . 883 .
table 7 compares the results of tanda and qnli with asnq and standard wikiqa embeddings . in general terms , the results are similar , with the exception of the mrr score reported in table 7 . as table 7 shows , both sets perform comparably to each other when trained and tested on the same dataset ( i . e . name a vs . name bert - base ) .
we observe that bert ( hochreiter and schmidhuber , 1997 ) obtains the best results with a precision @ 1 and precision @ 3 score on the three sets , while tanda ( clarke et al . , 1997 ) gets the worst performance . ( table 3 )
the results are shown in table 4 . we observe that the three features that have the most significant impact on our model are the relation extraction and the contribution extraction . the first group shows that has_diff and has_int features contribute the most , while dif_too contributes the least . when we add in the second group of features , we see that the bias term and sentiment extraction are the most important factors in the model ' s performance . adding out the sentiment extraction and contribution extraction results show that the bias term is important , but only when it is considered in combination with other features .
table 1 , listing 1 & table 1 : ( right code snippet ) implementation of kiperwasser and goldberg ( 2016 ) ’ s neural parser in only a few lines using uniparse . ( left table and left figure ) we show the time it takes to decode an entire dataset , given a set of scores . although the average number of tokens per decoder is small , the size and type of standard deviation band that make up the scores are large , indicating that the sorting bottleneck that underlies cle is quite large . table 1 also shows the extent to which the standard deviation of the generated data has an impact on the performance of cle , since it tends to have lower standard deviation than other neural parsers .
we show the results of our final model on the error generation algorithm on the treebank ar_padt dataset . our 3 . 5x model significantly outperforms all the base lines with a gap of 10 . 8 % on average compared to the previous state - of - the - art .
comparison of the word form similarities , in % of 1 − vmeasure of the clustering . average and median distances are shown in table 3 , and the average and median of the cosinevalues are reported in table 4 . the jw clustering distance is the most accurate measure of how closely the word forms are related , in that it has the highest correlation with human judgement .
the results in table and ii show that the pre - trained embeddings improve the performance of all the models . w2v ( cbow ) achieves gains of 1 . 8 and 2 . 2 points over the strong baselines on average , respectively , with the improvement being 2 . 1 and 3 . 4 points higher than that by char - cnn ( table ii ) . however , the difference is narrower with respect to sub - par performances for some models , as shown in table ii . these results show that pre - training of the models , although beneficial for some , hurts performance for others .
comparison of the label accuracy on the development set is shown in table 3 . our model ( bert ) shows significantly better performance than the previous state - of - the - art model ( hexaf - ucl , 80 . 18 % vs . 80 . 67 % ) .
performance of the question generation system on the fever dataset is shown in table 1 . the training set size and the number of questions per claim are the most important factors in the performance of the system . the development set size is small , making it difficult to convert claims into questions . however , once questions are converted into questions , the accuracy increases significantly ( p < 0 . 05 ) . the number of claims in the training set is small but significant ( p > 0 . 1 ) indicating that the system can easily distinguish between training and test set content .
performance of the question generation system on fever dataset . it can be seen in table 2 that the training set and test set are the mostchallenging types for the model , as the accuracy of the label prediction is relatively low ( low 0 . 76 ) and low ( 0 . 67 ) compared to the development set ( low 0 . 05 ) . however , when training and testing the same dataset , the accuracy increases significantly ( p < 0 . 01 ) . further , the gap between training and test set performance is less pronounced for test set , indicating that training set is more challenging than the development set .
the results are shown in table 1 . we observe that the transfer learning method outperforms all the base models apart from wnli when using only one type of training data . transductive scenario gap and bias are the most significant factors in the performance gap analysis , with sota achieving gains of 2 . 4 % and 3 . 7 % over bert and 50 . 8 % over no train data ( micro - f1 ) .
pretrained vs non - pretrained embeddings show that the ability to embed sentences in the self - similar manner is considerably better than that by pretraining . furthermore , the performance gap between pretrained and unpretrained models is narrower , with pretrained embedding requiring significantly less data .
table 1 shows the results for english and spanish captions . our model obtains the best results with an error reduction of . 36pp over the best - performing baseline .
we furthermore trained models on additional languages as mentioned before , such as tigrinya , oromo , and pbel ( see table 2 ) . the results are shown in table 2 . transformed models perform comparably to unsupervised and trained models , but do not have the advantage of using transsupervised embeddings , indicating that the training data generated by the model can be used more interpretably .
table 3 shows the entity linking accuracy with pbel , using graphemes , phonemes or articulatory features as input . the hrl used for training and pivoting is shown in parentheses in the first row . as shown in table 3 , the grapheme and phoneme features alone give a significant ( 7 % ) performance boost which is expected in a single shot framework . however , when combining all the features together , the difference between the hrl and pbel is only 2 . 5 % ( p < 0 . 05 ) . using the articulatory part - ofspeech features alone gives a 0 . 7 % boost ( p > 0 . 01 ) which is less significant than expected by random chance .
table 1 shows the performance of the models using the scenario and the out - of - the - box scenario approaches . the results show that , let alone a reduction in performance , the nli and bimpm models perform well in the scenario setting , while the bert model performs less than the best esim model . also , we notice that the diin model significantly outperforms the other models when trained and tested with the same set of questions , indicating that the use of the scenario - based model leads to a better model design . though bimpm performs slightly better than esim , it should be noted that it is trained with a significantly larger corpus of questions . regarding rc , we find that the model trained using bert [ italic ] nli w / o scenario and rc w / out scenario perform similarly , but do not exceed the threshold for success ,
in experiment 1 , we show the results of paragraph selection on the four languages . our model obtains the best results , with an f1 - score of 4 . 18 / 4 . 28 and a f1 + score of 3 . 52 / 3 . 92 . these results show that both the french and german captions are highly interpretable , with the exception of the case of the dutch captions ( see table 1 ) .
in experiment 2 , we compare our model with the best performing baseline models . our model obtains the best results with respect to paragraph embeddings . it closely matches the performance of the best baseline models with only 0 . 03 % absolute difference . moreover , it achieves a new state - of - the - art on sentence prediction , outperforming all the base lines except for the case of paragraph prediction .
we observe that the ensembles trained in the multi30k dataset are comparable in terms of recall , but the performance gap between en and fr is greater than that between cs and all . this indicates that the fr and de aspects contribute differently to the model ' s performance .
table 4 : image recall @ 10 on multi30k dataset with different languages with bv embeddings . the results show that ensembling the models with different language embeddings improves the recall for both train . lang . and model .
non - expert human performance results for a randomly - selected validator per question . dbidaf outperforms the dbert and droberta neural models . the difference in em and f1 scores between the best and worst performing neural models is minimal , but significant with respect to dev . although the difference is small , it indicates significant performance variation in the neural models that neural models trained with different features can be useful for improving prediction performance .
the precision r - l and recall are presented in table 4 . the results show that when only using pg data , the recall and precision are relatively consistent , but when using both pg data and word embeddings , the precision is significantly worse . adding boundary layers , such as m1 - latent , gives a performance gain of 2 . 48 f1 r - 1 and 2 . 63 r - 2 scores on average , but a drop of 4 . 63 f1 and 6 . 45 r - k over the strong baselines .
in table 2 , we replicate the experiments from ( kutuzov et al . , 2017 ) on the mae and mape datasets . the results show that both the shallow and deep layers contribute similarly to the task , with the shallow layer performing better on mae than the deep layer . the difference is less pronounced for mape , but still significant ( p < 0 . 05 ) . bi - lstm w / shallow features ( m1 - latent , m2 - shallow ) is significantly better than the baseline model ( p < 0 . 01 ) on both mae ( p > 0 . 05 ) and parallelism , and by a margin of 2 . 5 points on mape ( paired ttest ) .
the results are shown in table 5 . we observe that the bertsda model significantly outperforms the models trained on the news and ag datasets . the model achieves state - of - the - art results on all three datasets , outperforming the best previous models by a noticeable margin . on the imdb dataset , the model achieves a final score of 5 . 55 , marginally improving over the previous state of the art .
table 3 shows the test set performance on the four datasets . our model obtains the best results with an accuracy of 91 . 44 % on average , marginally outperforming the previous state - of - the - art models .
fine - tuning the bert - large model ( bert - l ) with multi - task learning , we report the test error rate ( % ) . mt - dnn fine - tunes bert with singletask learning . with this approach , the accuracy reaches 91 . 62 % on the imdb and ag ’ s news datasets ( table 4 ) . on the news dataset , it achieves a final accuracy of 91 . 49 % . with the smaller size of bert , fine tuning accuracy reaches only 6 . 59 % , which is still considerably higher than the previous state - of - the - art . with a reduction of training size of 1 . 5x in the multitask learning task , fine - nating accuracy to 91 . 79 % , or 5 . 59 % higher than previous approaches .
table 2 shows the results of automatic evaluation for perplexity . our model obtains the highest score on the oov scale , confirming the effectiveness of our model . we observe that our copynet model outperforms all the alternatives except for transdg . moreover , it gets the best performance on the perplexity scale , showing that the model can easily distinguish between semantic and syntactic information .
table 3 shows the entity evaluation results . our model obtains the highest score on the oov metric , indicating that it has achieved the best generalization ability in the low - resource settings . we notice that our copynet model outperforms all the other models that do not use copynet ,
table 4 shows the bleu scores of all models trained on the hidden test set of seq2seq . our model obtains the best results on three out of the four sets .
table 5 shows the human evaluation results . our model obtains the best performance on the five aspects of relation extraction . it closely matches the performance of transdg ( 2 . 41 % ) and seq2seq ( 1 . 83 % ) . however , it is inferior to ccm and memnet in all aspects ( except for relation extraction performance ) .
table 7 shows the ablation results of transdg on the test set . as shown in the table , w / o qrt , the model performs significantly worse than the model with qrt + kst , indicating that the use of qrt reduces the performance of the entity prediction ability . also , we see that kst and rga contribute similarly , however , the difference is less pronounced with respect to bleu - 1 . with respect to entity prediction , the rga model shows lower performance than the kst model , indicating the importance of parameter sharing . finally , we observe that ssd also contributes negatively to the model ' s performance .
table 1 summarizes our results on event coreference . our method outperforms the previous state - of - the - art methods on both datasets on average . on the one hand , it improves upon the strong baselines performance by 3 . 8 points in the standard task formulation , while on the other hand it verifies the performance of the best previous methods by 2 points .
the performances of the beam search for the reference are shown in table 1 . the results show that la is more directly comparable to the greedy search because of their same beam size . however , the difference in bleu score between the two approaches is less pronounced with the smaller beam size , showing that the look - ahead improves the model performance . table 1 also shows that the use of la improves the generalization ability of the model .
table 3 shows the average number of words per question and answer , and the average length of n - gram overlap between passage and question . question length is shorter than answer length , but longer than passage length , indicating that there is a significant overlap between the two . table 3 also shows that question length and question length overlap are the most important factors in generating an answer , as the overlap tends to exceed the length of the passage .
the performances of the lstm model trained on the wmt16 multimodal translation dataset with different la steps . we show the look - ahead module is able to improve the model on the entire testing set . however , either the la module or the beam search method harm the models when the length of the target sentences is longer than 25 words .
we show the results of applying la module to the transformer model trained on the wmt14 dataset . we find that the la module slightly improves the original model but harms the performance when the la time step is 5 . 56 seconds , which indicates that the eos problem is caused by the small size of the training set .
we find the eos loss not only boosts the performance of the model when using the greedy search , the model is more robust than using the naive two - step approach . the results are shown in table 4 . the greedy search strategy outperforms the naive approach and the 3 - step process , indicating that the loss function can be used to improve the model ' s performance without sacrificing too many correct answers . we notice that the use of the greedy approach , although it reduces performance , does not harm the model in the long run .
table 1 shows the translation quality evaluation results on the wmt ’ 14 , en - fr and iwslt datasets . our model improves upon the previous state - of - the - art on all three datasets by 3 . 7 points .
the results of text - line extraction on the diva - hisdb dataset ( see section iii - a and iv - a ) are shown in table and table ii . our proposed method outperforms state - of - the - art results by reducing the error by 80 . 7 % and achieving nearly perfect results .
table and ii illustrate the results of the experiments shown in table i . our proposed text - line extraction method is superior to state - of - the - art even if both methods run on the same perfect input .
table 1 shows the performance of our approach with respect to word embeddings . our approach establishes a new state - of - the - art approach for vqa . originally , we trained using the embedding network approach developed by peyrard and gurevych ( see x4 ) . using the shared vocabulary approach by itself places us at advantage since we do not have to assume whether a given entity or a relation might contain the correct context . we further developed our approach using the lexical resource extraction method proposed in section 4 . 3 . 2 . 3 , providing a new vocabulary extraction method for the mscoco dataset .
the results are shown in table 4 . we show the performance on the original mscoco , flickr30k and vqa datasets . the average accuracy of the embeddings is 72 . 2 % , slightly higher than the bleu - 4 average , indicating that the domain embedding approach is beneficial for the task . however , it does not improve significantly over using self - attention . referit also shows a drop in performance .
results are shown in table 4 . the first set of results show that grovle ( w / o multi - task pretraining ) and fasttext achieve remarkably similar results , with the exception of the case of the " sentence retrieval " task , when pretraining with ft . moreover , the accuracy gap between the baseline model ( 71 . 8 % on average ) and the best performing model ( 83 . 2 % ) is less pronounced with ft pretraining , showing that the model can be trained with a reasonable selection of the training data and the correct target task . table 4 shows that the accuracy gain over pretraining by adding ft helps the model to improve its recall and accuracy .
we find that the multi - task pretraining approach further improves the results for the task prediction accuracy and recalls , and the accuracy of the named entity recognition tasks . the model achieves state - of - the - art results , improving upon the performance of grovle by 3 . 8 points in the accuracy metric on average .
table 4 shows the results of the adversarial effect for different models that we trained on the hidden test set of bidaf and roberta . the results show that when only using the original seed information , the model performs well , with an absolute improvement of 2 . 6 points over the strong baselines on average . however , when using the additional resource information from dbidaf , the performance gap between the original and the re - trained model becomes much larger .
the most representative models are seq2seq ( hochreiter and schmidhuber , 1997 ) and ataml ( lebanoff et al . , 1997 ) . in the distinct - 1 and diff scores ( differences are computed using the weighted average of the bleu scores of the original and the best performing variant , respectively , with a gap of 0 . 27 and 0 . 59 points from the last published results . moreover , the gap is larger with respect to semantic threshold , as shown in table 2 , the semantic thresholding performed by the best - performing variant is based on the clustering of entity nodes with the same name , i . e . the speaker - f model , which achieves the highest average semantic threshold . further , we observe that the gap between the average semantic and syntactic threshold scores is small but significant , our model achieves the best performance with a distinction score of 105 . 36 on distinct 1 and 50 . 92 on diff score , while the former shows lower performance , the latter shows higher performance . we observe that among all the mod ways of clustering entities , our model performs best in semantic threshold extraction .
table 1 shows the performance of the models trained on the different test sets . the first set shows the results for the evaluation set on the dbidaf , dbert and droberta datasets . the second set includes the results on the test set of the dataset dnq . as shown in table 1 , all models show good performance on the training set . however , the difference between the em and f1 scores on the dnq dataset is minimal , with a f1 of 0 . 6 , our model obtains the best performance among all the models .
corpus - level bleu scores on the validation sets for the same model architecture trained on different data . table 4 shows that source and source split significantly worse than websplit , but still outperform both wikisplit and splithalf ( see x4 ) .
table 6 shows the test set evaluation results , as counts over the simple sentences predicted by each model for a random sample of 50 inputs from websplit 1 . 0 and wikisplit 2 . 0 . the results show that , on average , the model can pick out the correct answer 90 % of the time and the incorrect answer less than 50 % once the training set is trained .
table 5 shows the results on the websplit v1 . 0 test set when varying the training data while keeping the basic bleu and sentence architecture fixed . we choose to use the best model by aharoni : 2018 , which uses the full websplit training set , whereas we downsampled it . the results are shown in table 5 . the source domain embedding model outperforms the wiki - split model by a noticeable margin . the difference is less pronounced for the larger set , but still suggests some advantage to using a smaller training data .
table 2 shows the quality results for local embeddings . our model outperforms all the state - of - the - art methods with a gap of . 31 points in the average score .
table 3 shows the quality results for sm . embdi and refs embeddings are well - equipped to handle low - supervision settings . with a f1 of . 75 on the seep and l r scores , and a p - value of . 81 on the movie f1 scores , we find that all models trained on the sm dataset are more than suitable for this task .
table 4 shows the f - measure results for er . the results show that unsupervised embeddings perform better than supervised ones , indicating that the model design approach can be further improved with a reasonable selection of the training data and the training set architecture .
table 1 shows the evaluation results on the training and test sets for the dsquad , bidaf and dbidaf models . the first set shows that when only using original neural models with the training data , the performance gain on the dsqad model is minimal . when using only dbdaf , the model achieves the best results with a f1 - score of 58 . 7 on the test set . on the evaluation set of dbert , the f1 score of 0 . 9 is significantly higher than the previous state - of - the - art . finally , the epm and f1 scores of the droberta models are both higher than those on the dbidaf dataset
table 4 shows the results of the second study that we performed on a random sample of 1000 sentences . our model outperforms all the base the percentage of same and same scores obtained by our method is significantly higher than the previous state - of - the - art .
the human evaluation results are shown in table 6 . grammaticality ( g ) , meaning preservation ( m ) and structural simplicity ( s ) are measured using a 1 ( very bad ) to 5 ( very good ) scale . overall , the quality of the summaries is relatively high , with an average score of 4 . 36 / 7 . 43 on the grammatical quality scale and a 4 . 43 / 6 . 36 score on the structural simplicity scale .
table 1 shows the most recent tweets from the dblp dataset . more than 80 , 000 tweets contain offensive , abusive , and hateful speech . golbeck2017 and fountadclbsvsk18 find 35 , 000 and 80 , 000 tweets containing offensive language , respectively . note that the number of tweets containing the word " n * gga " is much higher than in previous studies . davidsonwmw17 contains 1 , 914 tweets that are potentially offensive , constituting 25 . 4 % of the total tweets . waseemh16 contains 16 , 914 tweets which are hate speech , 25 . 6 % are offensive , 6 % are bullying , 13 , 000 are harassment , 20 % are normal , and 6 , 608 are non - harassment .
the results are shown in table 1 . pointwise and hnm models perform comparably to pairwise and pairwise ranknet , but do not exceed the performance threshold set by ukp - athene [ athene et al . , 2013a ] . the pointwise model achieves the best performance with a f1 - score of 53 . 00 % on the 25 % held - out validation set , slightly higher than the previous best state - of - the - art . when trained with only one type of threshold , the performance gap between pointwise , pairwise , and ranknet decreases significantly . the hnm thresholding reduces recall , but does not improve precision ( differences are statistically significant with t - test , p < 0 . 05 ) . when training with two types of thresholding , we see that the hnm - trained model performs better than the pairwise model with a larger f1 score .
the results are shown in table 5 . we observe that the three aspects of directness are the most important in the clustering performance . for macro - f1 , the average of the three features is relatively high while the difference between the two averages is low . stsl outperforms mtsl and lr in both aspects . directness and recall are relatively high in the standard models , but the difference is less pronounced in the more specialized micro - f1 models .
the results are shown in table 5 . we observe that the stsl model outperforms both lr and mtsl when trained and tested on the macro - f1 dataset . tweets generated using the stml model tend to have higher precision , as measured by the average f1 score , compared to those using stsl . moreover , the average ar and median f1 scores are slightly higher than those of stsl , indicating that the semantic information injected into the model by the additional cost term is significant enough to result in a measurable improvement . in the test set , we observe that , when using only macro - based features , the accuracy obtained by stml is relatively high , but when using all the data available from the other sources , it is low .
table 1 compares the f1 - score and em - score of the multilingual bert model with the baseline on the two sets of squad benchmarks in french and japanese . as expected , the results are significantly higher than those in english , both for f1 and em ( see table 1 ) .
table 2 shows the exact match and f1 - score of bert on the four languages for each of the cross - lingual squad datasets where they occur . the figures in bold are the best exact match , for each language , among the five languages , and the average f1 score of the framework is the result of a conversation conversation between human and trained bert models .
we show the fever and accuracy scores of our models on the three similarity test sets . the results are shown in table 4 . the smaller performance gap between bert & bert ( pointwise , large & ukp - athene ) on the similarity test set is modest , but significant with respect to accuracy . pointwise and hnm both give significantly better results than bert , indicating that the pointwise cues are more useful in the low - supervision settings . moreover , when trained and tested on the unlabelled dataset , the accuracy gap between the two sets is much larger . bert and bert both trained and trained with pointwise features achieve higher accuracy scores than those using hnm .
table 3 shows the doc - level bleu scores on the dgt valid and test sets of our submitted models in all tracks . our model outperforms the best previous state - of - the - art en models on all three tracks .
table 6 compares our model to the state - of - the - art on the rotowire - test . our model ( 22 . 2 bleu ) outperforms both the best - performing nlg models by a noticeable margin . in fact , our model considerably outperforms the state of - art .
table 7 : english nlg ablation study , starting from a 3 best player baseline ( the submitted nlg model has 4 players ) . bleu averages over 3 runs . standard deviation ranges between 0 . 1 and 0 . 4 .
table 3 shows the f1 score on the development set for low - resource training setups using only one language embeddings ( none , tiny 5k or small 10k labeled danish sentences , respectively ) and large , multi - language source data ( 14k sentences , 203k tokens ) . fine - tuning neural transfer via domain - adaptive domain - aware domain modeling ( lin et al . 2018 ) achieves a significant improvement in the tnt score over the strong baselines ( p < 0 . 05 ) by increasing the training size and the number of tokens in the labeled sentences , but still performing substantially worse than neural transfer using the small and small labeled danish datasets .
table 4 shows the f1 score of all models trained on the danish ner dataset . we observe that the majority perform well , with the exception of bilstm ( 70 . 8 % ) , closely followed by polyglot ( 63 . 6 % ) and dkie ( 59 . 9 % ) . the smaller differences in loc and misc score show that different models interpret the data differently . in some cases , where a model is trained only on one language , the performance gap between the best performing model and the worst performing one may be large . in this case , we see that for danish ner , the gap between majority and majority is narrower than in english , but still large enough to result in significant performance drop .
we show the f1 scores of our models on the four datasets in table 3 . inspec and semeval models perform comparably to state - of - the - art models while catseqd performs better on kp20k and nus datasets . our model achieves the best results with a f1 @ 5 score of 0 . 397 on the krapivin dataset and f1 . 381 on the nus dataset .
we present the results of the models in the present and the past using the best performing models . in the present case , our model obtains the best results with an rf1 score of 3 . 087 out of 10 while the average of 2 . 348 is slightly better than the previous best state - of - the - art .
table 5 shows the results of an ablation study we performed on the kp20k dataset . our model obtains the best results with an f1 @ @ @ score of 0 . 380 on the present and the absent test sets , improving upon the previous state - of - the - art results by 9 % in the absence of rf1 rewards .
table 3 presents the results of the best performing models on the present and the absent test sets . our model obtains the best results with a f1 @ @ @ score of 0 . 382 on the absent and present test sets , while catseqd gets the worst performance . the results are presented in table 3 . the best results are obtained by our model ( catseq - 2 ) which obtains a f2 @ @ 1 score of 1 . 37 on the absent and present test sets and a f3 @ @ 0 . 43 on the new test set .
we compared ar and nonar models in word analogy task . ar achieved the best results with a gap of 3 . 8 % in bleu from the previous state - of - the - art model while nonar achieved the worst results . ar + mmi models outperform nonar and ar + diverse , but do not exceed the ar baseline on stopword and adv metrics . when trained only on word analogies with integer linear programming , the gap between ar - trained and non - ar - trained models is narrower , but still significant ( 7 . 9 % vs 7 . 2 % ) . ar and mmi + rl outperform the nonar model ,
the results are shown in table 4 . ar and nonar models perform comparably to each other on the coherence and content richness tasks . however , ar has higher agr and coherence rates than nonar , indicating that the semantic information injected into the model by the additional ar layer is significant enough to result in a significant improvement in performance . content richness is relatively high than that of ar , we observe that ar + mmi , and ar + rl , both individually and as a group , achieves high ar and content richness scores , when combined , the results are less pronounced than those for nonar . when ar is only trained with sparse regions , it achieves ar - quality results comparable to those of sparse regions . however , when ar is trained with both sparse and multilingual regions , results are markedly worse , showing that diversity is important for the model to perform well .
the performances of our nonar + mmi methods on the wmt14 en ↔ de and wmt16 ro → en tasks are shown in table 4 . the gains from nat ( + 1 . 48 % ) over the nat model ( lee et al . , 2018 ) are modest but significant ( p < 0 . 05 ) compared to that by lee et al . ( 2018 ) . further , our nat model ( + 0 . 57 % ) outperforms the inat model by 1 . 86 points in the accuracy on both sets , which shows the advantage of finetuning word embeddings during training .
the results on the german compounds dataset are shown in table 3 . the transweight model achieved a dropout rate of 53 . 21 % , which means that the model can be seen to perform better than most state - of - the - art models with n = 200 dimensions .
the results are shown in table 1 . first , we show the numerical results for the english and german language . the results show that both the semantic and syntactic similarity are relatively stable , with the semantic difference being less pronounced for english . syntactic similarity is relatively low for both languages , with an absolute drop of 0 . 31 % in the standard model .
the results are shown in table 1 . ent - sent and ent - dep1 achieve state - of - the - art results , improving upon the strong debiased baseline bilstm baseline by 4 . 8 points in the standard task formulation and to parity with the strong boundary - adaptive cnn baseline . the results on the cnn are slightly less clear , but still show that the boundary - based features are beneficial , improving the generalization ability of the model by 3 . 6 points .
table 2 shows the performance of bertbase on the test set of five datasets with different epochs . we observe that for the semevalcqa dataset , the setup achieves the best results with an average mrr of 0 . 942 on the three datasets , while for the yahooqa and trecqas datasets , it achieves the worst results . the sota results are from madabushi et al . ( 2018 ) . ( table 2 ) and sha et al . , ( 2018 ) ( trecqa ) . on the wikiqa set , the performance is slightly better than the previous state - of - the - art , but still significantly lower than the best previous state of the art .
the results are shown in table 1 . ent - dep0 shows that the two types of models perform comparably to each other , with the exception of the former ent - only model . the difference is most pronounced in bilstm , where the ent - sent model ( which learns sentence relations with both word - and sentence - level attention ) performs best . further , the difference between the average f1 scores of the two sets of models is less pronounced for cnn .
we compared the results of the original and the linked embeddings . the results are shown in table 1 . the results show that the original opiec is significantly cleaner and more interpretable , while the linked version is significantly less interpretable . perhaps most striking about the change in interpretation is that the negative polarity polarity has a significant impact , it affects the interpretation of more than half of the triples , constituting 45 . 3 % of the total triples .
table 1 shows the location and time that the associatedmusicalartist and musician are most likely to be in . more than half ( 43 , 842 ) of the 1 , 965 questions in table 1 identify the location where the musician or musician associated with the musician is likely to have been in the past ( see table 1 ) . the remaining questions are about time and place , with a focus on productivity . location tags show the number of times a musician or performer may be in the presence of a non - musical entity ( e . g . , a restaurant , a bar , a hotel , a museum ) . frequently , these questions are accompanied by additional questions about the performer or performer . table 1 summarizes it even further by showing the characteristics of the most frequently visited locations . for example , the location of a bar or restaurant ( where a musician may be seen performing regularly ) is the most common location for musicians to be seen interacting with other musicians . on the other hand , the most frequent time for a musician to appear at a location is when he or she is performing as a member of a band or ensemble . this table also highlights the differences in the roles that musicians perform as part of a relationship .
the experimental results on the framenet 1 . 7 and the test set are shown in table 5 . the results are presented in bold . the first set shows the performance of the models for english and german captions . for english captions , we see 21 . 28 % improvement on average compared to semafor ( 83 . 63 % vs . 75 . 60 % ) and 28 . 22 % improvement compared to botschen et al . ( 87 . 63 vs . 73 . 28 % ) . however , the gap between the frames on the test set and the full set is much narrower , with an absolute improvement of 2 . 36 % on the framenet1 . 7 score compared to the baseline .
table 3 shows the results of bertbase and bertlarge using the best performing model configurations . our model achieves the best results with a mrr of 0 . 927 on the small and large datasets , respectively , compared to the previous state - of - the - art models .
the results in table 6 show that the two types of tuning methods perform comparably to each other when only using the original class or class name . however , in the more realistic second case , when using only the name " ddi " , the results are slightly worse . the automatic and manual ways of tuning the parameters are comparable , but the performance gap is much narrower . selective attention mechanisms like random and automatic search achieve better results overall , but still lag significantly behind the original method .
the results are shown in table 3 . the first set shows that the classifiers trained on the original and the debiased embeddings perform comparably to each other , with the exception of the case of the ddi class . however , in the more realistic second set , when only using the original , the performance gap between the two sets becomes much larger . punct and digit features result in significantly better performance than those in the original . they show that both the feature - rich and feature - free encoder are beneficial for the task at hand , improving upon the performance of the original class by 4 . 59 points in the standard task formulation . in the more sophisticated second set of the model , we see that the ability to detect entity blinding and double - entities is beneficial , but only when it is done in the context of a single entity . regarding the semantic features , we observe that the heuristic gives a performance gain of 2 . 56 points on average compared to the original showing that it is more effective to rely on two entities .
the results are shown in table 5 . in general terms , we see that the two types of classifiers perform comparably to each other when the true response is added . however , in the more realistic second case , when only using part - ofspeech tags , performance on crcnn is significantly worse . the difference is less pronounced for ddi and i2b2 classes , where the crcnn classifier performs better than both the original and the bert - tokens model .
we observe that α - entmax significantly improves the results for both languages , and that the model using softmax augmentation achieves the best results on the kftt dataset .
the results shown in table 6 show that the models trained with cc and arxiv embeddings perform well on both subtasks . the micro - f1 scores of the models are slightly higher than those of cc ( 54 . 61 vs . 54 . 87 ) but still considerably higher than arxiv ( 67 . 96 vs . 67 . 37 ) . the precision numbers of the subtasks is higher than that of subtasks with cc ( 78 . 42 vs . 74 . 61 ) indicating that the model performs well in the production setting .
table 3 shows the test set performance of our system on the unc test set and referit test set . our system outperforms all the state - of - the - art methods except for the one that we base it on : rmi , rrn and kwa . the difference is most prevalent in the high - level test set , where our system obtains 58 . 63 % and 35 . 63 % ) better results on average compared to the previous state of the art .
we show the results of an ablation study of the different attention methods for multimodal features on the unc val set in table 2 . the cross - modal self - attention method shows lower performance , but higher performance than no attention . the word attention method shows higher performance , indicating that the word attention is more useful for the task at hand . the pixel attention method , by contrast , shows much higher performance . it shows that word attention alone does not improve performance , however , it does improve performance for other features .
the results are shown in table 5 . we observe that the pre - trained models perform comparably to state - of - the - art methods on the test set , with the exception of rrn - cnn , which obtains significantly better results . moreover , our model outperforms all the base the performance gap between the best - performing methods is small , due to small size of the training set , we observed that the difference between prec @ 0 . 5 and 0 . 7 indicates that pre - training of models with different features leads to different performance scores . our model , cmsa - s , obtains a performance gap with the best performing model with respect to concatenation of features , p - value < 0 . 005 . moreover , it is inferior to many of the base models , as shown in the second group of table 5 , combining all features boosts performance marginally , but still results in a significant gap with those without . the difference is most prevalent in the concatenated layers , cmsa + ppm , which shows that features derived from convlstm are important for boosting performance , but do not improve performance significantly over pure decoder - based models .
the experimental results of the first and second metrics are shown in table 2 . first metric accuracy is relatively high while the disp . perf . accuracy is low . the difference between the average of the two metrics indicates that the first metric is the most important factor in the generation of the model predictions .
table 3 compares our proposed method with state - of - the - art approaches on the human agreement and swda datasets . our method improves upon the performance of previous approaches by 3 . 8 points on average . on the mrda dataset , it achieves a performance improvement of 2 . 2 points over the previous state of the art .
table 3 shows the f1 scores of the best models for inspec , semeval and krapivin . the inspec model ( 7 . 8 % ) outperforms the best random model ( 6 . 1 % ) on both datasets in terms of f1 score . however , the difference is less pronounced for semevals , which achieves an f1 @ 10 score of 3 . 6 % . the difference between inspec / semeval model and random is narrower than that between kramivin / random , as shown in the second group of table 3 , inspec and random , the two models perform comparably to each other when trained and tested on the same dataset ( i . e . name a vs . name b ) . inspec and semeeval both have high overlap with randomness , the average f1 - score of the two methods is significantly lower than that of inspec / sem .
table 3 shows the f1 scores of the base , bigrnn and transformer models on the five epochs . the transformer 80m model has the best f @ 5 and f @ 10 scores , while the basernn 13m model is close to the bottom of the range . with respect to paramparam , we see that the appear - pre pre - trained model performs better than random and no - sort models , indicating that pre - training of models with different paramatures leads to better model performance . further , when trained with only one param , the results are slightly worse than those with two . we see that combining all param features , including the length and type of clustering , results in a better model . selective attention mechanisms like topn - rank reduce repetition , but do not reduce accuracy . regularization reduces recall , but does not improve performance .
table 3 shows the results on the four aspects of drug and relation recognition . the results are presented in bold . our model elmo - lstm - crf improves upon the strong baselines by 3 . 67 points in all aspects . it achieves state - of - the - art results on all aspects , outperforming the previous approaches by a noticeable margin . in particular , it achieves exceptional results on relation and drug recognition , achieving results comparable to those of simes et al . ( 2018 ) in terms of precision on all three aspects .
the results are presented in table 4 . the first group shows the results on the diagnosis detection and support tasks . support tasks result in an f1 score improvement of 1 . 7 points on average compared to the previous state of the art . prescription reasons support have the greatest performance improvement , improving by 4 . 6 points in the standard task formulation . however , other features such as concern and urgency are only slightly beneficial , improving performance by 0 . 3 points overall . further , the effectiveness of the prescription reasons label decreases by 2 . 4 points . this suggests that some aspects of the prescription rationale process may not be beneficial for some patients . regarding the drug – disease relations task , we see that the current state - of - the - art model performs well on all three domains , with the exception of the united states . on the other hand , when combining all the features , the improvement is only 2 . 6 % on the diagnostic tasks .
corpus statistics and label distributions of the friends and emotionpush datasets are shown in table 2 . according to the table , anger was the most common emotion in the training set , followed by surprise , whereas happiness , sadness , and disgust were the least common ones .
table 1 compares the effectiveness of our method with the fsa baseline on paraphrase extraction . our summaries yield an accuracy drop of 1 . 95 % compared to the baseline , which shows that our method can significantly improve the interpretability by increasing the precision by training with fewer tokens .
classification test scores for classifying r vs u in the br , us , and combined br + us dataset . the results shown in table 5 indicate that our model can do the job well , with an accuracy drop of only 0 . 5 % compared to the baseline .
table 3 summarizes our results on entity recognition performance on the four data types . headers and body tags alone account for more than half of the data , however , they account for less than 25 % of the total . the other two types are orders of magnitude larger than the headers and bodies . in total , there are 202 , 727 . 47 examples in table 3 and 294 , 174 . 27 examples of trie preimages , compared to 173 , 100 . 00 examples in the previous section . table 3 shows that the number of examples per data type is significantly higher than the average of the other two , indicating that more data are being considered in the production set .
table 2 shows the performance of our model on the four data types . the first set shows that the compact sync geth and compact sync ethanos models perform well , while the number of instances per domain is relatively lower than the other two . headers and body tags are the most difficult to solve , however , we managed to solve with a 3 . 44 % improvement on average compared to the last state - of - the - art model . the second set of data show that the performance reach the best when using only one data type , namely , that of the trie nodes . when using both data types , our model obtains a 4 . 28 % increase in performance .
the results are shown in table 3 . our model obtains the best results on the mt02 and he datasets . our transformer - word model ( hochreiter et al . , 2016 ) outperforms all the other methods apart from the case of the rnn - search bpe model , which shows the advantage of finetuning word embeddings during training . specifically , it achieves the best average mt02 – 08 he score of 3 . 28 / 3 . 38 and 3 . 83 / 2 . 63 on the bleu dataset , respectively , with an absolute improvement of 2 . 36 / 0 . 55 and 6 . 45 / 4 . 55 points over the previous state of the art . however , the difference is less pronounced on the smaller mt02 dataset , where our model ( 31 . 2 / 34 . 2 ) and the transformer - bpe model ( 43 . 4 / 27 . 4 ) are the only ones that perform much worse than the other two .
table 3 shows the performance of our model on the task slc and flc r tasks . our multi - granularity model improves upon the strong baselines by 3 . 36 points in each task . by further adding entity nodes , the performance gap between the two baselines becomes narrower . granu and sigmoid both give considerably better results than bert , but still lag significantly behind our joint model .
table 2 : results on cqa dev - random - split with cos - e used during training . as shown in the table , bert ( the baseline ) and cage - reasoning ( the second variation ) achieve comparable performance to the best - performing method ( bert - 63 . 5 % ) , but do not exceed the threshold for accuracy ( 63 . 6 % ) . with the additional training data , we achieve a final accuracy of 72 . 6 % , which marginally outperforms the bert baseline .
the addition of cos - e - open - ended during training dramatically improves performance . our model ( talmor et al . , 2019 ) achieves gains of 10 % over the state - of - the - art on the cqa v1 . 0 test set , and by further improving inference performance by 4 % .
table 4 shows the performance on the cqa dev - random - split test set using different variants of cos - e for both training and validation . the results show that when only training with the correct set of ques , the selected variant ( s ) can significantly improve the performance ( see table 4 ) . however , when validation is used , the difference between the selected and the opened variants ( table 4 ) is much smaller .
table 6 shows the results for explanation transfer from cqa to out - of - domain swag and sotry cloze tasks . our bert model improves upon the strong baselines by 4 . 8 % in the swag task and by 3 . 6 points in the story - cloze task .
the bert model achieved the best f1 score with a precision of 0 . 871 on the 15 seeds tested in table 1 . by contrast , bert + pu ( p < 0 . 001 ) achieves a lower f1 than both the original and the ensembled model ( p > 0 . 012 ) . the difference between the mean and standard deviation of our model is less pronounced for the lqn split , but still suggests significant performance improvement by bert ( + 0 . 7 % ) on the fa split . our model outperforms bert + pu ,
we include the physical and semantic properties of the training and test set in table 1 , where the maximum depth of the hierarchy is counted from ( and including ) the top synset of the domain , the number of basic level concepts , and the inter - rater agreement ( κ ) . table 1 also includes the size and type of synset . it is clear from table 1 that the hand tool and edible fruit are the most basic concepts in the training set , followed by musical instr .
in table 2 , we rank the most important features and sub - features . all the features ranked in order of importance are presented in table 2 .
table 3 shows the accuracy and κ of the model making use of the normalized features in a new domain , with or without normalization . the results show that the language features normalized with the training data give a significant ( 7 % ) boost in accuracy , but still performing substantially worse than those without . syntactic features give a 0 . 8 % boost , but a drop of 2 % overall compared to the training set . in the more realistic second case , when all the features are normalized , the model gives a 4 . 4 % boost . however , this increase is less pronounced for frequency ,
table 1 shows the distribution of the event mentions per pos per token in all datasets of the eventi corpus . overall , there are 18 , 735 event tokens per pos ( 314 vs . 4 , 068 ) in the dataset , which shows that the training and test set consist of different vocabularies .
corpus statistics are shown in table 2 . overall , there are 17 , 528 events mentions per class and 3 , 798 overall events mentions ( per class ) in the event mentions dataset .
table 3 shows the r and p scores for the standard and relaxed evaluations . the relaxed evaluation set is presented in bold while the standard one is in italics . we observe that the relaxed evaluation set is comparable to the original berardi2015 model , but is significantly worse than the original in both sets . the fastext - it model , based on the ilc - itwack framework , achieves the best results with a f1 - score of 0 . 892 on the strict and relaxed evaluations . although it achieves the highest f1 score , it should be noted that it is trained on a significantly smaller corpus . dh - fbk is comparable , but does not receive the best performance . from this group of models , the most interesting ones are the glove et al . ( 2015 ) model , which exhibits the most striking differences in performance between the original and the alternative . the glove performs much worse on the relaxed and strict evaluations , showing that the reliance on word embeddings induces the generation of incorrect answers . also , we notice that the similarity between the learned and unlearnt responses is less pronounced for the relaxed than the strict one .
table 3 shows the bleu scores of our models on the intra - dist and inter - dist distales . our seqgan model ( henderson et al . 2008 ) obtains the best results on both sets with a gap of 17 . 2 % on average in the bow embedding r and f1 scores compared to the previous state - of - the - art . on the interdist dist - 2 , our cvae model ( hred ) achieves the best score with 0 . 836 on the bbileu r and 0 . 763 on the f1 score compared to 0 . 748 on the prior state of the art .
table 5 shows the human judgments for models trained on the dailydialog dataset . the dialogwae model achieved a final accuracy of 29 . 6 % on the diversity and informative tasks , while the cvae - co model achieved 25 . 6 % . the difference between the informative and coherence tasks is less pronounced for the dialogog model , but still suggests significant differences in performance between the two . the vhcr model achieved an accuracy of 25 . 2 % on diversity , but only 6 . 4 % on coherence .
table 2 compares the bleu scores of our proposed methods and the baselines . our rl look - ahead model achieves the highest score for all three aspects : empathy , relevance , and fluency . the multiseq model achieves a significant improvement in the accuracy score on the recognition task , but is not significant in relation to the other two aspects . similarly , the difference in recall score between seq2seq and rl current is less significant , but still significant . grusky et al . ( 2018 ) additionally compare our proposed method with the gold standard on three of the four aspects : empathy , relevance and recall . table 2 shows that the similarity of the proposed methods on these three aspects is less pronounced than those on the other four .
we report the best performance observed in 5 runs on the development sets of both sparc and cosql , since their test sets are not public . our model improves upon the strong baselines by 4 . 6 points in the matchmaking ability .
table 1 shows the average precision of the detection and the average accuracy of the direction for bless , wbless and bibless . as shown in the table , the proposed spon method can significantly improve the performance for both datasets with a boost in accuracy . the difference in accuracy between the original and the proposed method is minimal , however it is significant with respect to detection . with respect to direction accuracy , the difference is much larger with the original bless method ( p < . 05 ) and with the difference between direction accuracy ( p > . 1 ) . hyperboliccones , like the svd method , are similar in that they only allow word vectors to move in one direction at once .
table 4 : ablation tests reporting average precision values on the unsupervised hypernym detection task , signifying the choice of layers utilized in our proposed spon model . relu and tanh are the only two layers that can take negative values , indicating the non - negative nature of the activation layer relu . wbless and eval are the other two high precision neural networks that can be used in spon ( see x4 ) . table 4 shows the precision numbers reported by the ablation tests for each neural network using the original and the residual connections . relu only takes positive values , while tanh only takes negative values .
table 5 : results on the unsupervised hypernym detection task for bless dataset . with 13 , 089 test instances , the improvement in average precision values obtained by spon as compared against smoothed box model is statistically significant with two - tailed p value equals 0 . 00116 .
table 2 shows the rouge recall results on the nyt50 test set . our model ( outsios et al . 2018 ) obtains 45 . 53 % recall on the first sentence , 28 . 94 % on the second sentence and 26 . 53 % ) on the third sentence . the difference between the two approaches is minimal , however we see that the transfer learning method results in significantly higher recall .
the results on semeval 2018 domain specific hypernym discovery task are shown in table 7 . crim is the best system on the domain specific datasets . it achieves the best map and p @ 5 scores , and consequently , is the most accurate on the music and medical datasets . spon is the second best on the medical dataset , however it is inferior on the science dataset .
table 1 shows the average embedding similarity scores between the output and the target output in terms of real target output list . the sparsely trained greedy embeddings outperform the pre - trained greedy ones , indicating that the training set size and the number of parameters in the output are important for the model to perform well . rl beamsearch ( 200 ) is comparable to the greedy approach , however it requires significantly more data to reproduce the results . selective attention mechanisms like rl greedy remove some fraction of noise , however , remaining speech patterns are distorted as well .
we observe that the models trained on the amazon data are comparable in terms of roc and sst , with the exception of bilstm ( table 4 ) . however , when trained on cnn data , the results are markedly worse . sopa outperforms all the mod table 4 shows that the training set size and type of repetition used in the model are the most important factors in predicting the final score . overall , the model performance is comparable with the best previous state - of - the - art models .
the results in table 4 show that the annotators and questions have high correlation with the observed agreement ( κ ) , indicating that the questions asked by the annotators are of high quality . however , the quality of the answers is relatively low ( ao = 0 . 49 ) .
experimental results are shown in table 3 . the best result for wikipedia was achieved by our model , which achieved a final accuracy of 91 . 79 % . the second best result was obtained by our approach , which showed a slight improvement of 2 . 07 % over the baseline baseline . our approach outperforms both the baseline and the best - performing mod doc2vec model by a noticeable margin .
table 4 shows the confusion matrix of the joint model on wikipedia . rows are the actual quality classes and columns are the predicted quality classes . the diagonal ( gray cells ) indicates correct predictions while the rows are the wrong predictions ( pink cells indicate incorrect predictions ) .
table 1 shows the classification performance of these large - scale text classification data sets . the ag news classification data set contains 120 , 000 training instances and 450 , 000 test instances . the chinese news categorization data set is 60 , 000 examples long .
the results are shown in table 4 . our model outperforms all the other approaches that aim to improve semantic classification performance on the sogou dataset . it achieves state - of - the - art results , outperforming the best previous approaches by 4 . 72 points on average .
bleu scores are shown in table and ii . all the pre - trained and non - pre - trained models average to 91 . 72 on the standard model evaluation set , which shows that pre - training of all models helps the model performance . however , for the more complicated prefix and suffix - based models , the results are slightly lower ( 87 . 43 vs . 87 . 94 ) . all the small ( type 1 ) and medium ( type 2 ) models show lower performance than the large ( type 3 ) ones . finally , all infix ( type 4 ) and prefix models show higher performance than all other types of models .
the results are shown in table 4 . we observe that the pre - trained models perform comparably to the best performing ( hosseini et al . , 2016 ; hosseini , et . al . ( cid : 27 ) and hosseani , et al . 2016 ) . however , the difference between the average and the baseline is minimal , postfix - transformer models perform slightly worse than the original ones . while the average of the two sets of models is slightly higher , the gap between the baseline and the final results is much narrower . the results of the best - performing model are reported in the second group of table 4 , where the pre - trained model ( 78 . 5 % average ) outperforms the average ( 75 . 4 % ) of the trained model ( 71 . 5 % ) . the difference is most prevalent in relation to corpora 2 , as shown in fig . 3 , the transfer learning method by itself achieves the best results , outperforming both the original and the trained models by a large margin . it achieves this by training on a large corpus of documents from one domain and cross - training them on other domains ,
slot - dependent sumbt ( proposed by ren et al . ( 2018 ) achieves a joint goal accuracy of 0 . 891 / 0 . 952 on the evaluation dataset of woz 2 . 0 corpus , marginally better than the previous state - of - the - art . statenetpsi ( ren et al . , 2018 ) and gce ( hosseini - asl , 2018 ) achieve 0 . 885 / 0 . 881 and 0 . 855 / 1 . 010 respectively , a slight improvement over the performance of the state - ofthe - art previous state of the art . table 1 compares our proposed method with three baselines : slot - independent , slot - dependent and ontology - dependent bert + rnn ( which takes the ontology - based model pre - trained , but does not add slot - dnn embeddings ) , and slot - neutral bert + .
table 2 shows the joint goal accuracy on the evaluation dataset of multiwoz corpus . the sumbt model ( hosseini - asl et al . , 2018 ) and zhong et al . ( 2018 ) achieve an accuracy of 0 . 4240 ( ± 0 . 0187 ) on the benchmark set , which indicates that the model can easily distinguish between the true response and negative responses .
the results are shown in table 5 . the transfer accuracy of bert , snli , rte and sst is reported in bold . hubert ( transformer ) achieves gains of 2 . 53 % and 0 . 60 % over the strong baselines on transfer accuracy and transfer loss accuracy , respectively , compared to the performance of ( qqp ) .
the results are shown in table 4 . we fine - tuned our model to achieve the best results . our hubert model achieved 94 . 23 % accuracy on the transfer job , marginally outperforming the strong base model of snli by 2 . 67 points .
table 3 shows the ablation results on the four types of source and target neural models . in general terms , the results are presented in table 3 , mnli outperforms snli , rte , snli and snli when trained with the correct target and label . the difference is most prevalent in the transfer tasks , where the true answer of snli is often much higher than that of rte .
the results are shown in table 4 . the first group shows that unigrams and bigrams alone perform best , while the combination of the features of " notes only " and " bigrams " perform best among the bag - of - words . the second group of results show that combining all features , but excluding the notes , results in a lower precision on the death and recall auroc ( table 4 ) . finally , we see that combining features alone improves the recall on the ccs top - 1 and top - 5 recalls , but to a smaller extent than expected by random chance . in general terms , the results are similar across the three sets , with the exception of the case of the " death " row , when we use only the bigrams .
the results are shown in table 2 . our model bert12 achieves state - of - the - art results on all metrics , outperforming all the base methods except for the case of sst - 2 . it achieves 3 . 9x improvement on average over the performance of bert6 , and 3 . 7x improvement over bert3 . 5x increase on average .
the results are shown in table 2 . our model outperforms all the state - of - the - art models on all three structure tasks . adabert - sst - 2 achieves the best results with 91 . 2 % on average . it closely matches the mrpc and qqp scores , and its rte score of 82 . 9 % is nearly 5 % better than the previous state of the art . in addition , it improves its mnli and qnli scores by 2 . 8 % and 3 . 6 % points , respectively , over the strong baselines .
table 5 shows that for sst - 2 and qnli , β = 0 indicates that the loss term has little effect , while for rte , it has a significant effect ( see x4 ) .
table 4 shows the effect of particular knowledge loss terms on our model . the most striking ones are probe and da , which cause significant drop in performance .
we show the performance of our model on the four evaluation metrics in table 1 . our model significantly outperforms the current state of - the - art models across all evaluation metrics . in fact , it improves upon the bert model by 3 . 38 points in performance on average compared to the state of the art model by
gaussian mask only shows the inference times in ms while the rl model shows the length of sentences in ms . inference times between 0 . 01 and 2 . 61 ms are the average time taken to find the correct answer .
table 3 shows the number of images after which the me score falls below threshold . for the omniglot dataset , there are 43 , 328 images that meet the threshold , constituting 71 . 2 % of the total number of examples . for imagenet , there is a larger picture corpus containing 111 , 872 images , making it more difficult to detect .
the results are shown in table 5 . first , the results of the best performing method is the en - de model , blstm , which achieves a success rate of 70 . 8 % on the standard nor task . next , we compare this with random , soft - att and fast - att learning models , the results show that , given the correct training data , the ability to learn the relation relations in the deep layers of the network , is relatively simple . however , when using the hotflip feature , the model performs much worse than expected . in en - fr and out - of - domain settings , the transformer model ( hochreiter and schmidhuber , 1997 ) outperforms all the other methods apart from the case of the random case . further , the success rate is slightly higher than the expected 55 . 2 % under the automatic threshold due to the high overlap between the training and the test set , we observed that soft - att learning methods reduce recall loss during training , but do not improve accuracy . the model using the combination of soft - award and random learning achieves a lower success rate than the previous methods .
in the ensembles presented in table 3 , the results are presented in bold . the results show that random and soft - att attentions achieve better results than either min - grad or hotflip , indicating that the features chosen by the attention layer are beneficial for the task at hand , but do not improve performance in the out - of - vocabulary tasks . moreover , when trained with only one type of attention layer , the performance drop is much worse than that seen with any other combination of features . selective attention mechanisms like random , soft - att and even the combination of the two types of hotflips reduce repetition , improving the generalization ability of the model . however , it does not improve the accuracy .
in the en - de model , the results are presented in table 3 . the results show that random and soft - att attentions achieve better results than both min - grad and hotflip ( differences are statistically significant with t - test , p < 0 . 05 ) . moreover , when trained with blstm data augmentation , results are slightly worse than those without . as shown in the second group of table 3 , both the automatic and automatic learning methods give similar results on the training data . however , in the more realistic second group , the automatic learning method obtained with hotflatt achieves a better result , showing that it is better to rely on superficial cues than superficial ones . further , combining the features of soft and fast attentions improves results for both models , the performance of the automatic learner and the model is slightly better than that of random ,
table 2 shows that the degradation of the sacrebleu model is caused by a high proportion of bitext data that is noised . more than 80 % of the original embeddings in the newstest ’ 12 and ’ 17 sets are noised , constituting 21 . 6 % of total feedxt data .
table 4 compares the sacrebleu scores of the different flavors of bt for wmt16 enro . it . - 2 bt improves upon the strong baselines by 3 . 8 points in the accuracy test set while it . - 3 bt improves by 2 . 6 points . the bitext variant improves by 4 . 4 points . noisedbt improves by 1 . 9 points . by further adding entity nodes , the score increases by 0 . 8pp over the baseline . it . sennrich and sennrich ( 2016 ) and gehring et al . ( 2017 ) show that bitext and noisedbt are comparable , but do not outperform bitext ,
table 5 shows the test set results on the wmt15 enfr dataset , with bitext , bt , noisedbt , and taggedbt . bitext improves the generalization ability of named entity recognition . it improves the bias metric by 4 . 8 points in the standard task formulation and achieves the best average score of 42 . 4 % on the enfr task . compared to the previous state - of - the - art models , bitext is more accurate , improving by 3 . 6 points in standard tasks and to parity with noisedbt . moreover , it achieves a better generalization performance than the best previous bt model by 2 points .
table 6 shows the attention sink ratio on the first and last token and entropy ( at decoder layer 5 ) for the models in table 3 . for asr , data are treated as if it were bt ( noised and / or tagged , resp . ) , whereas for entropy the natural text is used . the difference in attention sink is minimal , however we see significant difference in bitext baseline performance due to large variation in the number of tokens in the feed , which indicates that the size and type of attention sink caused by the different tokens interacting with the decoder is important .
in table 7 , we compare the results of standard and noisedbt decoding with the best performing models from the previous years . the results show that , compared to the standard model , the use of taggedbt improves decoding performance over both the noised and taggedbt models . however , the difference is less pronounced for taggedbt , showing that it is harder to solve tags than for standard .
table 9 shows the source - target overlap for both back - translated data with decoding newstest as if it were bitext or bt data . the gap between the src - tgt unigram overlap and the target overlap of the standard and taggedbt decodes is 8 . 9 % and 11 . 4 % respectively , which indicates that the tagged bt data have higher semantic overlap with the bitext data .
table shows the distribution of the raw documents in the reuters - 8 dataset . most of the samples in the dataset are in the basic categories , which include crude , grain , oil , and trade .
in the en - de news dataset , tf - msm outperforms both mvb and lsa with a large margin . moreover , it achieves a comparable performance to that of svm ( 89 . 23 % ) and tf - svm ( 88 . 29 % ) on the w2v dataset . as shown in table 4 , the accuracy and precision are relatively high when using only one type of clustering feature , namely , binbow , tfidfbow and mvb . however , when using both features , accuracy drops significantly , indicating that the clustering technique has superior generalization ability . selective attention mechanisms such as lsa and svm achieve high accuracies , showing that they are well - equipped to perform this task " out - of - the - box " .
table 3 compares the performance of our system with prior works on the conll test set . the first set of results in the table shows that our system outperforms all the base lines with a gap of 10 . 5 conellerrant f1 points from the last published results ( yannakoudakis et al . , 2014 ) while surpassing our strong lemma baseline by 3 points . by further adding entity nodes , our system achieves a f1 score of 0 . 63 / 0 . 79 and a f0 . 71 / 30 . 33 improvement over the previous state of - the - art .
we managed to classify 9 , 992 messages in total ( out of 10 , 992 total messages ) into three categories ( buy , sell , sell and other ) and found that 86 % of the messages in these categories had a correct description .
table 1 shows the performance of our model in terms of recall and recall . our proposed hgn outperforms all state - of - the - art methods on all metrics by a noticeable margin .
table 1 compares the results of the baselines for semantic and syntactic similarity . semantic similarity is high , with an average score of 94 . 7 % and 95 . 6 % respectively . syntactic similarity is low , with a mean score of 42 . 3 % and a maximum score of 63 . 6 % . we find that the semantic information is the most important part of the task for pooling , followed by the syntactic information . overall , the performance is similar , with the exception of semantic similarity , which is achieved by a drop of 2 . 8 % overall .
we show the precision numbers for each aspect of the model on the wikipassageqa and insuranceqa datasets . the fine - tuned bert model achieves 60 . 2 % p @ 1 and 25 . 8 % p / 10 on average , respectively , with a gap of 2 . 5 % in accuracy with respect to the pre - trained bert embedding model .
we compare our beheshti - ner model with other sophisticated neural models trained on the peyma word embeddings . the results in table 3 compare our model with the previous state - of - the - art on the four languages . our model improves upon the performance of the previous work by 3 . 59 points on average .
table 1 presents the test set of morphobert , beheshti - ner - 1 , and ictrc - nlpgroup . the results on the in domain and out domain test sets are presented in table 1 . the performance of these models on the in - domain and out - ofdomain test sets is presented in tables 1 and 2 . morphobert achieves state - of - the - art results , outperforming all the alternatives except for team - 3 . 8 % on average in terms of f1 score . out - ofdomain performance is much better than the other two baselines , with an average f1 of 8 . 3 % on test data and a total of 15 . 6 % on test data 1 .
table 3 shows the scores for the medical and sports rehab datasets . the first set shows that the domain adaptation strategies generally outperform the competition , with the exception of transfer learning . domain adaptation is beneficial , lowering recall scores for some specific domains , but giving a significant performance drop for others . further , we see that the ability to transfer knowledge is beneficial for all domains , improving scores for all but one sub - category .
table 1 shows the performance of the different neural network models for comparison . our approach outperforms all the base lines with a gap of 10 . 4 % on average .
table 1 compares the performance of the method with other approaches . we observe that , let alone a reduction in performance , the difference between mt and vg is significant , with mt achieving state - of - the - art results and outperforming all the alternatives except for wtp .
we observe that the three approaches give similar performances on the training data . however , in the more realistic second case , when we switch to cnn : rand , our model achieves the best results with 62 . 6 % accuracy . this confirms the effectiveness of our model .
table 2 : the accuracy ( % ) of the ml models for nlu . the results show that golve - based models significantly outperform the svm and hmm ( 87 . 22 % vs . 94 . 20 % ) on the intent detection and slot filling tasks , respectively . svm also outperforms the random baseline on slot filling ,
the results are shown in table 5 . it can be seen that the gui of was suitable for reading the provided answers , and that it was able to provide answers in a reasonable time ( i . e . , in less than a minute ) . however , it was unable to do so with the accuracy of more than 50 % . although the accuracy was relatively high , it should be noted that the accuracy obtained by the gui was more than 90 % on account of the high recall rate . regarding the opinions generated by the system , we can see that the majority of opinions ( 50 . 7 % ) were strongly agree with the answers provided , however those that were strongly disagree were less than agree . overall , the results show that the ability to obtain answers in reasonable time was a high priority for the task .
table 1 presents the results of unsupervised and supervised ir baselines on the four domains . the results are presented in table 1 . semantic similarity methods outperform [ empty , 35 . 00 / 61 . 71 % ] . we observe that tf * idf and bm25 derive remarkably similar output : of the 2000 example pairs in the dataset , the two have completely opposing predictions ( i . e . name a vs . name b ) on only 325 examples . in comparison , the supervised ir baseline by bm25 yields a comparable result : 35 . 9 / 71 . 25 % and 35 . 71 / 59 . 38 respectively .
table 3 shows the rnd and ub scores of the syntactic and thematic rankers . the best result over syntactic rankers is achieved by our model , which shows that the semantic rankers are more useful for the task at hand .
induced hierarchies are shown in table 4 . they consist of four components : the source , target , domain , target and time . the first group contains the source and target , while the other two consist of the target and the location . topic and sentiment are the most interesting ones , providing a rich diversity of semantic information that can be easily extracted from supporting documents . qualitative information includes the presence and location of the relevant lecters , the number of tokens in question and the time taken to compute the result . mentions of time are specific of complaints , as they are typically accompanied by a timestamp . in addition , the presence of lecters and lecters gives a distinctive flavor , as the focus is on the self and the learner , and the delivery mechanism is specific of the domain .
the results of the second study are shown in table 5 . the ub and rnd scores are significantly lower than those of en - test , indicating that the training context is more important . also , the difference between the rnd and ub scores drops significantly for both languages .
the results are shown in table 1 . we observe that , let alone a reduction in performance , map outperforms egfr , ndcg and kras when trained and tested on the standard metric set of tf - idf , and similarly when trained on the additional metric of feat . moreover , the results are slightly superior when trained with the additional feature set of cosine and bm25 , indicating that the reliance on these features induces the model to rely less on syntactic shortcuts .
we report the macro - f1 scores of the bilstm and ted models ( see table 1 ) as well as the human upper bound on each of the epistemic activities in schulz et al . ( 2019a ) . in both cases , the ub model shows considerable improvement over the strong baselines by using fasttext embeddings . in fact , ub is comparable to flair ( hochreiter et al . , 2017 ) and closely matches its performance with that of eg ( gillick and favre , 2009 ) .
the results are shown in table 5 . we observe that context and context - aware bert models perform comparably to state - of - the - art models , but do not exceed the performance threshold set by logreg because context information is considered in the development step . moreover , context and tagged span overlap , which results in a performance drop of bert logits when trained and tested on the same dataset ( around 5 % on average ) . when trained on a single dataset without tagged spans , the performance gap between bert and all other models becomes narrower , with bert achieving 0 . 71 and 0 . 83 lr scores on the development and test sets , respectively , compared to 0 . 95 and 098 by the comparison of using tagged spans and handcrafted * * features . regarding development , we observe that the use of tagged spans improves performance by 4 % on the standard bert model , but to a smaller extent than on the standalone model . further , the handcrafted features contribute significantly less than the tagged spans .
the results of experiment 1 are shown in table 2 . we observe that the mostchallenging aspects of disinformation are caused by semantic over - representation and semantic minimisation . further , we see that falsehoods are aggravated by syntactic and syntactic features such as the black - and - white fallacy , causal oversimplification and founta fallacy ( see x4 ) . we see that the flag - waving fallacy is particularly difficult to detect , as it requires much more data than available in the test set . in addition , automatic metrics such as repetition and grammatical gender - parity ( p < 0 . 001 ) seem to have little effect on the results , however they do contribute significantly to the performance of question answering .
table 4 shows the rocchio and relevance scores for the kras and pik3ca datasets . the summaries generated by our approach outperform all state - of - the - art methods on both datasets by a significant margin .
table 1 compares three approaches that use word embeddings similar to our linspector web model . originally , all the features described in table 1 were described in terms of similarity tasks , but nayak et al . ( 2014 ) and dyer ( 2014a ) add additional tasks to the mix to improve the results for offline evaluation . adding dt tasks improves results for both languages , but does not improve results for web . we find that the number and type of tasks used in our model ( 28 vs 28 ) are comparable to the previous state - of - the - art .
we show the five most frequently used refinishing terms for this query in table 6 . most notably , we show the three most common ones for different cancers and their variants . among the many variants , the most prevalent ones are lung cancer , pancreatic cancer and stomach cancer .
table 1 shows the results of the first set of experiments on the difficulty prediction approaches . our svm model ( original ) shows considerable performance improvement over the strong baselines on k ∈ . 50 , while the new bilstm model shows a considerable performance drop .
table 2 shows the rmse scores for both strategies for each corpora with randomly sampled target difficulties and the number of targets for each target . for the reuters corpora , we see that brown ( p = 0 . 0088 ) significantly outperforms gutenberg ( p < 0 . 01 ) while sel ( p ( cid : 27 ) leads to a significant over - fitting by a significant margin .
table 3 presents the mean error rates e ( t ) per text and strategy for the standard and hard subset of sel . as expected , the average error rates for both sets are significantly lower than the standard def ( p < 0 . 05 ) .
we observe that without lexicon , the f1 - m model does not generalize well compared to the data - lexicon baseline . however , when lexical features are added , the results are comparable , with the exception of accambamb . with lexicon features , the model performs better than the baseline on both datasets with two tasks .
our model outperforms all the base methods with a gap of 10 . 2 bleu points from the last published results ( lei et al . , 2017 ) on three of the four similarity test sets . the difference is most prevalent in relation to boundary attentions , specifically , the gap between soft - attention ( soft - attention ) and automatic ( adaptive ) attention is 15 . 6 blesu points on average , while on the other hand , group - aware attention is 7 . 2 points higher . attention is selective , with automatic attention achieving high recall ( 83 . 4 % ) and low recall ( 59 . 8 % ) compared to soft attention ( 60 . 4 % ) . selective attention mechanisms reduce repetition , but do not improve generalization .
in table 1 , we report the fraction of incorrect summaries produced by recent summarization systems on the cnn - dm test set , evaluated on a subset of 100 summaries . our model ( rouge - l ) outperforms all the other methods with a large margin . the error reduction over the best baseline is 15 . 7 % on average .
table 1 shows the results for english and german captions . valuation results are presented in bold . the results show that the original model , when trained and tested on the same dataset , is significantly better than the best performing random model ( 50 . 7 % vs . 50 . 1 % ) on average . in addition , the difference between the average val and average bert score is smaller than that between random and infersent , showing the advantage of finetuning word embeddings during training . when trained on the unlabelled dataset , the results are slightly less clear , with val showing a drop of 0 . 8 % compared to the previous state of the art . sentiment prediction using esim shows a larger performance gap with random , with respect to english captions , the smaller difference between sse and random is less pronounced , but still suggests that there is a need to improve the model performance in this direction . selective attention mechanisms like bert and esim reduce repetition , but do not improve accuracy .
table 2 shows the french contraction rules . as can be seen , the most striking thing about the difference in pronunciation is that the vois ci → voici distinguish between male and female pronouns , both for the original and the debiased word embeddings .
table 1 provides the setup and setup information for the disjoint dbless and wbless . the setup setup information presented in table 1 shows that the sg and sg word embeddings are completely separate , with the exception of the case of the learner . using the disjoint setup setup proves beneficial for both datasets , the difference between sg and gl is minimal , still , we see significant difference in performance between the sets with respect to the two sets table 1 presents the setup setups for the two types of words . when using the full dbless / wbless setup , both sets perform identically , but the difference is much larger with respect to sg , the gap between the sg / sg and the gl scores is much smaller with the sg grouping being much more pronounced for wbi contexts . using only the postle adv part of the setup setup benefits the wbiess and bibless performance , as shown in fig . 3 , the use of postle dffn improves the joint sg / gl scores . however , it does not improve the joint wbiels performance . adding the word " learner " improves the vulic ( 2018 ) dataset by 9 %
table 2 shows the average precision ( ap ) of both postle and adv models in cross - lingual transfer using the distributional method . distributional embeddings give a significant ( 7 % ) boost in precision , which is expected in a single shot framework .
the performance of our model on the conll test set is presented in table 4 . we compare our deep coreference model with the best performing state - of - the - art models on three out of the four test sets . our model achieves the best results with a marginal improvement over the strong baselines across all three metrics .
table 1 shows the quality of the regression model ’ s predictions on the test set . experiment 1 shows that the model can make reasonable predictions with a r2 score of 0 . 03448 on average , r2 = 0 . 12238 on experiment 2 and 0 . 17576 on experiment 3 . however , the difference between the r2 scores of experiment 1 and experiment 3 is much larger ( p < 0 . 01 ) .
the pos - han model shows lower performance than the st - cnn model , indicating that the semantic information encoded in the pos - cnn embeddings are more useful for semantic information extraction . the accuracy of the ccat10 and blogs50 datasets is also lower than those for scat50 , indicating the syntactic representations discussed in table iii hold high semantic information value .
we observe that the syntactic and lexical models are comparable in difficulty to the original embeddings ( see table iv ) . however , the accuracy gap is much larger with respect to the style - han model , indicating that the syntactic model is more useful for production use . the accuracy gap between the original and the combined model is narrower than that between syntactic and lexical , however it is still significant . we observe the same trends in the semantic and syntactic baselines ( table iv ) . the syntactic model achieves the best performance with 86 . 59 % accuracy on the ccat10 and 86 . 76 % on the blogs50 test set .
table v compares the accuracies of the different fusion approaches . our approach ( ccat10 ) and blog10 achieve remarkably similar results ( 83 . 36 % and 90 . 58 % accuracies , respectively ) on the parallel test set , compared to the strong baselines of ccat10 ( 88 . 36 % ) and 82 . 83 % accuracy ( 71 . 35 % ) by blog50 ( 59 . 83 % ) on the combined test set .
we show the test results on the ccat10 , blogs50 and ccat50 datasets in table vi . the syntax - cnn model shows marked performance improvement over the strong baselines on both datasets . the n - gram cnn model achieves the best results with an accuracy of 86 . 08 % , slightly outperforming the svm - affix - punctuation model ( 83 . 08 % ) . however , it is inferior to the style - han model , showing the syntactic patterns discussed in section iv hold high predictive value for the task . the model with the worst performance is the cnn - char model , which shows severe over - fitting since the size and type of tokens used in the prefixing are small . domain - aware cnn models show lower performance than the automatic neural network models . due to the small size of the datasets , automatic neural networks perform better on the smaller scale , but still perform worse than automatic networks . we notice that the automatic network approaches that rely on syntactic appendages , such as cnn - cnn , strongly resemble the strong bias of prefixing .
we observe that the lstm [ ital ] c + lstm ras baseline outperforms all the base models apart from the svm baseline on ns r and svm f1 scores . moreover , the results are slightly superior on svm rbl , indicating that the training set size and type of training data contribute differently to the model performance . the results are presented in table 4 . svm performance is relatively consistent across all models with two exceptions ( ' c + c ' , c = c ' , rbl ) showing that it is better to rely on superficial cues .
we observe that the lstm c + lstm rasm baseline outperforms the svm baseline on all test sets except for the one where it is trained with svm rbl . moreover , it achieves state - of - the - art results on ns r and svm f1 scores as well . the results are shown in table 4 . in all but one of the comparisons , svm c + ras models perform better than the baseline model with a gap of 10 . 08 svm ras points from the last published results ( gillick et al . 2008 ) while on the other hand , the gap is much narrower .
table 3 shows that the tn and v models perform comparably to each other when only a and v are considered , with respect to precision , recall and error reduction , the svm performs better than the random model with a f - score of 71 . 9 % and 71 . 2 % respectively . however , with the v model , precision is only 67 . 6 % and error rate reduction is only 4 . 2 % . svm performance with t + a and v achieves gains of 3 . 6 and 4 . 7 % over random models , respectively , improving the recall and accuracy scores by nearly it achieves the best multi - model performance since the 1960s .
table 3 shows that the tn and v models perform comparably to each other when only a and v are considered , with respect to precision , recall and error reduction , the svm performs better than the random model with a f - score of 62 . 9 % on average compared to 62 . 2 % by random . svm with t + a and v achieves the best multi - model performance , outperforming both random and tn modes .
the results are shown in table 4 . the best results are obtained by the speaker dependent and the speaker independent sets . the speaker dependent set achieves the best results with a f - score of 71 . 9 % on the precision and recall measures , while the best performer is 71 . 6 % on recall . the context - based set - up improves performance for both sets , but the results are less pronounced for the speakerindependent set . further , the presence of the speaker in the context information fragments the performance of the set , leading to a drop of more than 2 % in precision and a 4 % drop in recall .
