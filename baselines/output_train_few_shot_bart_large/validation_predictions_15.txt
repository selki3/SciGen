table 2 shows the throughput and training time for our treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as the table 2 shows , both approaches yield comparable or better performance when training and inference are set to multiheaded attention .
the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization . as shown in table 1 , the balanced dataset also exhibits the greatest performance improvement , since the size of the batch increases linearly with the growth of the training set size .
as shown in table 2 , the max pooling strategy consistently performs better across all model variations , even when the model has the same representation as ud v1 . 3 . the dropout probability also decreases as the model grows , indicating that the dropout rate of the model is lessened with the growth of the feature set . finally , the activation func func . has the best performance with a f1 of 1 . 83 , which shows that the model can be further improved with the optimization of its dropout probabilities with the correct representation .
table 1 shows the effect of using the shortest dependency path on each relation type . the model - feature model significantly outperforms the macro - averaged approach , showing that the model can be further improved with the help of a shorter dependency path .
the results of the best performing models are shown in table 3 . the first group of results show that the y - 3 model achieves the best performance on the f1 and r - f1 measures , while the second group shows the worst performance . as can be seen , the difference in performance between the two sets of models is mostly due to the smaller size of the training set ( y - 3 : y = 25 . 45 % vs . 35 . 45 % ) and the high percentage of f1 participation on which these models are trained .
the results of paragraph accuracy are shown in table 1 . as can be seen , mst - parser achieves an accuracy of 100 % on average , while mate achieves 50 % or better on average .
as shown in table 4 , the average c - f1 score for the two systems is 60 . 40 ± 13 . 57 % on the essay and paragraph level , respectively . note that the mean performances are lower than the majority performances over the runs given in table 2 . the paragraph level system also shows lower performance than the essay level .
the results are shown in table 3 . the original and the cleanup tgen models outperform the original tgen model when trained and tested on the same training set . as the results show , once the training set has been cleaned , the model performs better on both test sets . however , the difference between original and cleaned tgen is less pronounced for the sc - lstm model than for the original .
as shown in table 1 , the original e2e data and our cleaned version have similar statistics ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . however , the difference in the ser percentage between the original and the cleaned version is much larger ( 17 . 5 % vs . 11 . 5 % ) for the train dataset .
the results are shown in table 3 . the original and original tgen models outperform the original sc - lstm model on every metric by a significant margin . in particular , the difference between the bleu , meteor and nist scores is much larger than those for tgen − and tgen + ( which gives a performance improvement of 2 . 83 points over the original model , though still performing substantially worse than the original . as the results of experiment 1 show , the ability to add items after training the model is crucial to the success of the model , as it helps the model to learn the task well .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . the errors we found in our system are mostly caused by errors caused by the presence of errors in the training data ( added instances ) , but there are also cases where the correct values for some instances were found to be missing or slightly disfluencies ( disfluencies ) which can be seen in table 4 . adding instances to the training set reduces the overall number of errors , however it does not improve the performance .
the performance of our model compared to previous approaches on the hidden test set is presented in table 3 . the first group of models shown in the table shows that our multi - headed approach outperforms the previous state - of - the - art on all metrics . on the other hand , our joint model performs slightly better than either the previous ensemble or singleheaded approach . graphlstm ( song et al . , 2018 ) achieves the best overall performance with a combined score of 25 . 9 % and a joint all - achievement ( all ) of 28 . 2 % compared to the previous best state of the art , which shows the advantage of finetuning word embeddings during development .
as shown in table 2 , our model achieves 24 . 5 bleu points on amr17 , a slight improvement over the previous state - of - the - art seq2seq model by 3 points . the ensemble model , however , achieves a slight performance improvement of 2 points over the single - model setup .
table 3 presents the results for english - german , czech and slovakian for both languages . our model outperforms the previous state - of - the - art models on both languages when trained and tested on the hidden test set of bow + gcn . as the results show , the difference between the single - and multi - model setups is less pronounced for german , while for czech , our model obtains a significant improvement .
the effect of the number of layers inside our model is shown in table 5 . we find that , for example , when we add 6 layers to our model , the performance improves by 3 . 5 points over the previous state of the art model . however , this is only true for the second iteration of our model .
comparisons with baselines are shown in table 6 . the first group of results show that the rcn with residual connections performs better than the gcn with no residual connections . moreover , the relative improvements over rc + la are larger than those for gcn + la ( decreased connections ) . the second group shows that the combination of rc and residual connections improves the generalization ability of the gcns , improving the bias metric by 3 points .
the performance of our model compared to previous models on the unsupervised test set is shown in table 4 . the first group of results show that our model performs significantly better than the previous state - of - the - art models on all metrics with a minimum of a drop of 10 % on average . on the other hand , our model achieves a performance improvement of more than 5 % on the supervised test set .
the results of an ablation study on the dev set of amr15 are shown in table 8 . the results show that removing the dense connections in the i - th block significantly decreases the density of connections , and consequently , the model performs better than the original dcgcn4 model .
the results of an ablation study for modules used in the graph encoder and the lstm decoder are shown in table 9 . the results show that , when the coverage mechanism is combined with the hierarchical attention mechanism , the model achieves competitive or better results than the original dcgcn4 model . however , the difference between the results of " - global node " and " - linear combination " is less pronounced than that of - global node .
table 7 shows the performance of our initialization strategies on various probing tasks . glorot obtains high scores for most aspects of the task , but is significantly worse than our approach for subjnum and topconst .
the results are shown in table 3 . the h - cmow model outperforms the cbow / 400 and the h - cbow variants on every metric by a significant margin . it achieves state - of - the - art results on three of the four metrics ( differences are statistically significant with t - test , p < 0 . 001 ) and on all metrics except length . subjnum and topconst are the only two metrics that consistently show significant performance improvement under the current set of constraints .
the results are shown in table 3 . the first group of results show that the cbow model outperforms the cmow model with a margin of 3 . 6 % over the best previous state - of - the - art method . hybrid method outperforms both cmp and cbow , showing the advantage of finetuning word embeddings during the development of the model .
table 3 shows the relative improvements on the unsupervised downstream tasks that our models achieve with respect to hybrid . the cbow model achieves a relative improvement of more than 20 % over the cmow model over the best previous state - of - the - art approach . hybrid also shows a relative increase in performance relative to the original approach , but this is less pronounced than the cbow gain .
table 8 shows the performance of our system for initialization and evaluation on the supervised downstream tasks . glorot ( 86 . 6 % ) and sick - r ( 71 . 4 % ) both receive high scores for initialization , but are significantly worse for evaluation of the downstream tasks , indicating that the performance gain comes from a better understanding of the task at hand and a better model design .
the performance of our approach on the unsupervised downstream tasks is shown in table 6 . our cbow - r model outperforms the cmow model in terms of all the three tasks .
the results are shown in table 3 . the first group of results show that the cbow - r model significantly outperforms the previous stateof - the - art methods on every metric by a significant margin . on the other hand , the difference is less pronounced for the wc model . subjnum and topconst are both close to the state - ofthe - art in terms of accuracy , while coordinv is closer to the mid - range . as the results show , the semantic information injected into the model by the additional cost term during training has a significant impact on the performance of the model , leading to a drop of performance on some metrics .
the results are shown in table 2 . the first group of results show that the cbow - r model significantly outperforms the cmow - c model on all subtasks except for the sub - category of mpqa . on the other hand , the difference between sub - categories is less pronounced for sst2 and sst5 , indicating that the performance gain comes from better training data .
the results are shown in table 3 . name matching and supervised learning perform similarly to the best previous approaches on loc and misc datasets . however , the performance gap between the two systems is much larger under λmil - nd , indicating that supervised learning relies on pre - trained knowledge more than on human judgement . moreover , the accuracy drop between loc / misc and all other datasets indicates that the model trained on the training data alone cannot learn the task well enough to solve all the subtasks in the full complement of the training set .
results on the test set under two settings are shown in table 2 . name matching has the best performance with a f1 score of 15 . 38 ± 1 . 03 and r = 0 . 59 , while supervised learning has the worst performance ( 15 . 03 ± 0 . 59 ) . the system performs well in both settings with a 95 % confidence interval of f1 scores showing that the model trained with the best feature set can learn the most useful features in the shortest time . in the more realistic setting , τmil - nd ( model 2 ) shows lower performance than our model ( 37 . 42 ± 0 . 53 ) on all metrics except for f1 . however , the improvement on the name matching metric is much larger , showing the effectiveness of our model in the low - resource settings .
table 6 shows that g2s - gat significantly outperforms the models that use gin and gin - gin , both when trained and tested on the single - headed attention ( s2s ) . as the results show , the model trained on the gat dataset is more than 4 . 5 times more likely to converge with the generalization ability of the original embeddings . also , when trained only on s2s data , the models trained on gat data tend to be less accurate than the original .
the performance of our model compared to previous work on the ldc datasets is presented in table 3 . the first group of results show that our g2s model outperforms the previous stateof - the - art models on three out of the four datasets . on the other hand , our model performs slightly better than the previous best state of the art models on two of the three datasets . we note that the performance gap between our model and previous work may be due to the smaller size of the dataset on which our model is trained ( ldc2015e86 , ldc2017t10 ) .
results on the test set of the ldc2015e86 test set are shown in table 3 . the models trained with additional gigaword data perform similarly to those trained with pre - g2s - ggnn ( konstas et al . , 2017 ) . however , the performance drop is less pronounced for the g2s model , which shows the advantage of pre - training with gigaword data .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model significantly outperforms the previous state - of - the - art models with a bleu score of 62 . 42 / 59 . 37 and meteor score of 59 . 37 / 59 , respectively .
the results are shown in table 3 . the first group of results show that g2s - ggnn model achieves a significant improvement over the baseline model in terms of sentence length and average number of words per sentence , with an absolute improvement of 3 . 51 % over the previous state - of - the - art model on all metrics .
table 8 shows that gold significantly outperforms the s2s model and the g2s - gat model in terms of the fraction of elements in the output that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . the model with the best performance is the gold model , which obtains a f1 of 50 . 35 on the input sentence and a miss of 33 . 67 on the output sentence .
table 4 shows the pos and sem accuracy for different target languages trained on a smaller parallel corpus ( 200k sentences ) . as expected , the pos tagging accuracy improves as the number of sentences in the corpus grows , but sem accuracy remains the same .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . word2tag significantly outperforms the unsupervised embeddings with a baseline of 87 . 95 % pos and 87 . 55 % sem . the difference in accuracy between the baselines is most prevalent for the most frequent tags ( mft ) , which shows the accuracy of the encoder - decoder is high .
table 4 presents the system ' s performance on the three metrics for the four languages . the system performs well on all metrics with the exception of pos tagging accuracy where it performs slightly better than the previous state - of - the - art model on all but one of the four metrics .
pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . as shown in table 5 , the bi - directional approach improves the pos accuracy over the strong bias of the original uni encoder , but does not improve the sem accuracy . finally , the res - based approach shows a drop in accuracy relative to the strong bidirectional approach .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . as shown in the table , the presence of gender and race features seem to have little effect on the attacker performance , however , they do contribute significantly to the overall accuracy . gender and age features have a significant impact on the performance of the attacker , as shown in table 8 . the presence of race and gender features also seem to affect the attacker ' s performance , as seen in fig . 3 .
accuracies when training directly towards a single task are shown in table 1 . the gender - neutral pan16 model outperforms all the baselines with a large margin . gender - neutral pronouns like " n * gga " and " b * tch " seem to have little effect on performance , however , the accuracy of " sentiment " is relatively high ( 83 . 2 % ) and " gender " is close to the level of pan16 .
as shown in table 2 , there is no significant difference in the performance between balanced and unbalanced data splits for gender - balanced task splits , and for task splits that are balanced or unbalanced . gender - aware features seem to have less impact on task accuracy , however , they do tend to have more impact on sentiment .
the performance of our model on different datasets with an adversarial training set is shown in table 3 . as the table shows , the gender - based features of the mentioned attributes have a significant impact on the performance , as do the race and age features . gender - based mentions have the least effect on the task performance , while age and race have the most significant effect .
the performance of the protected attribute with different encoders is shown in table 6 . the rnn encoder performs better than the guarded encoder for both leak and leaky embeddings .
the performance of these models on the multi - params test set is shown in table 3 . we note that the size of the training set used by yang et al . ( 2018 ) and parallelism is relatively small , but when combined with finetune fine - tuning , the model achieves state - of - the - art results . moreover , the number of parameters used to fine - tune the model is larger than either the base or the dynamic set , indicating that finetuning has a significant impact on the performance of the model . as shown in the second group of table 3 , lrn has the advantage of training on a larger training set , as compared to the lstm and gru , which have smaller training sets .
table 3 presents the results of the second study of rocktäschel et al . ( 2016 ) on the hidden test set of hotpotqa in the distractor and fullwiki setting , respectively . the results are presented in table 3 . we can see that the lstm model significantly outperforms the previous state - of - the - art models on every metric by a significant margin . on the other hand , the difference between the base acc and the average time taken to compute each parameter is less pronounced for gru , indicating that more training data may help the model to learn the task to a greater extent . table 3 shows the performance of the models with different combinations of training and test set time . as the results show , the training set size and the number of parameters used for parameter selection have a significant impact on the final performance , as shown in fig . 3 , the size and type of parameter selection vary depending on the training data distribution .
the results of zhang et al . ( 2015 ) are shown in table 3 . the first group of results show that this model significantly outperforms the previous state - of - the - art models on all metrics with a minimum of 3 . 57 points . on the other hand , the performance of this model considerably exceeds the previous best state of the art on three of the four metrics . table 3 shows that the performance gap between this and previous models is less pronounced for yelppolar than for amapolar , indicating that the training set size and the number of parameters used for parameter selection may vary depending on the underlying data distribution .
table 3 shows the bleu score of our model on the wmt14 english - german translation task . our model obtains a case - insensitive tokenized score of 26 . 67 on the training set of newstest2014 dataset and a corresponding 27 . 45 on the decoder set of tesla p100 . the difference between the two approaches is less pronounced for the german translation task , but the difference is larger for the english one .
table 4 shows the exact match / f1 - score of our model on the squad dataset . the model with the greatest f1 score ( + elmo ) obtains the best result with a match rate of 83 . 14 % on average . as the results show , the combination of elmo with the correct parameter number of base improves the model ' s performance . however , the model still performs substantially worse than the previous state - of - the - art models with the same parameter number . we conjecture that the performance drop is due to the high number of parameters in our model which the previous models do not have .
table 6 shows the f1 score of our model on the conll - 2003 english ner task . our lstm model obtains a significant improvement over the previous state - of - the - art model , giving a 4 . 56 f1 overall score . although the number of parameter numbers in our model is low , we find that it obtains the best performance with respect to most parameter categories . as shown in table 6 , the most important thing about parameter number is that it gives the model the best overall performance .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . the model with the best performance is elrn , while glrn has the worst performance . with the base setting of 62 . 06 % , both lrn models ( elrn + glrn ) have significantly worse performance than the model with base setting .
table 1 shows the system and sentence quality on the word analogy task for english and spanish . retrieving sentences from word analogy tasks is straightforward ; however , the quality of human sentences is considerably worse than that for oracle . sentence quality is relatively consistent across all systems , with the exception of human , which is better than both systems in terms of sentence quality . word analogy tasks generally yield comparable or better results than sentence analogy tasks . in particular , the similarity between human and oracle sentences is less pronounced for human , but not for any other language group .
table 4 presents the results of human evaluation . our system obtains the best overall performance on grammatical and appropriateness evaluations , and the highest standard deviation among all automatic systems . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) and table 4 shows that our model obtains a superior overall quality score . retrieval also receives a higher standard deviation than human evaluation , but is comparable in syntactic quality and contains fewer errors . among all the systems , our seq2seqaugaugaug model receives the highest percentage of evaluations , so it receives the most human evaluations .
the results are shown in table 3 . table 3 shows that , for the most part , all models trained on the ted talks corpus outperform their unsupervised counterparts on every metric by a significant margin . however , the difference is less pronounced for europarl , which shows that the training set is more stable and therefore requires less data to train . on the other hand , this small difference does not represent a significant performance drop for either language . as can be seen in the second group of results , the average p < 0 . 01 on all metrics for both languages is significantly higher than that for the other two .
the results are shown in table 3 . table 3 shows that , for the most part , all models trained on the ted talks corpus outperform their unsupervised counterparts on every metric by a significant margin . however , the difference is less pronounced for europarl , which shows that the training set is more suitable for the task at hand . on the other hand , the smaller size of the corpus and the smaller number of training instances mean that the p < 0 . 01 for all but one of the models ( lang , corpus , docsub , tf , tf - sqs and hclust ) shows a significant performance drop .
the results are shown in table 3 . table 3 shows that , for the most part , all models trained on the ted talks corpus outperform their unsupervised counterparts on every metric by a significant margin . however , the difference is less pronounced for europarl , which shows that the training set is more stable and therefore requires less data to train . on the other hand , this small difference does not represent a significant performance drop for either language .
in particular , we note that the average depth and the average number of roots per row are the most important metrics for the success of our system , followed by the quality of our feature set . europarl has the best overall performance , with an average of 11 . 05 % on both metrics and a maxdepth of 3 . 46 % .
as shown in table 1 , the average depth and the average number of roots for each relation are the most important metrics for brevity . europarl has the best overall performance with a weighted average of 9 . 73 % on both metrics . the difference between average and maxdepth is less pronounced for dsim and df , but larger for docsub and hclust .
in table 1 , we compare the performance of our approach against the baseline and the enhanced version of visdial v1 . 0 . the enhanced version ( lf ) model shows a significant improvement in the ndcg % compared to the baseline model , indicating that our approach can further improve the generalization ability of the model without sacrificing performance on the question type , answer score sampling and hidden dictionary learning , respectively .
as shown in table 2 , the model with the best performance is the one that applies p2 . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . the effectiveness of our approach is proved by the ndcg % ( normalized by the number of iterations ) of the ablative studies performed on the visdial v1 . 0 validation set . it can also be seen that the combination of p1 and p2 helps the model to learn the most effective vocabulary .
the hmd - f1 model significantly outperforms the hmd - recall model on both hard and soft alignments . the hmd pre - trained models perform similarly on both alignments , but do not receive the benefit of bert feature - values . note that hmd pre - training with bert features improves the recall scores for both sets of models , but does not improve the accuracy on the soft alignment . finally , the wmd - unigram model performs slightly better on both sets .
the results are shown in table 3 . the average score of bert score for each setting is reported in terms of ruse ( * ) and meteor + + . compared to the baselines , the sent - mover model achieves higher average score , while the bertscore - f1 score is slightly lower than the ruse baseline . as shown in the second group of table 3 , all the baseline methods give similar performance on the direct assessment task .
the performance of bertscore - f1 on the fixed - event dataset compared to the baselines is presented in table 3 . the system achieves the best results with a f1 score of 0 . 176 on the infrequent event set ( + 0 . 012 bleu - 1 ) . on the other hand , the sent - mover achieves the highest score of 1 . 078 on the regular event set and the best on the multi - event set ( + 1 . 07 ) . the set - factor evaluation performed by meteor + w2v shows lower performance than the baseline bert score , but higher on the multilingual set , indicating the effectiveness of the feature - rich clustering .
word - mover achieved the best results with a precision of 0 . 939 on the m1 and m2 metric , while the average bertscore - recall score was 0 . 866 . the accuracy of the word - mover is relatively consistent across all metrics with the exception of leic ( * ) where it gets a lower performance . sent - movers are consistently better than word - movement baseline on all metrics except for those that are set to spice .
the results are shown in table 6 . the first group of results show that the model with the most consistent performance is the shen - 1 model , while the other models with the least consistent performance are the ones with the best performance on all metrics . para - para models generally outperform the model without it , but do not do so well on sim or pp .
the results are shown in table 3 . we show that yelp significantly outperforms the strong baselines in terms of transfer quality and semantic preservation . semantic preservation is the most stable aspect of the model , while transfer quality is the weakest . the transfer quality drop of more than 10 % over the best baseline indicates that yelp has learned to rely less on superficial cues and rely more on semantic cues . overall , the model performance is better than the strong baseline on all three metrics .
table 5 shows the human evaluation results for each metric for validation of acc and pp . the summaries generated by our model match the human ratings of semantic preservation and fluency , but do not match the spearman ’ s [ italic ] ρ b / w sim metric , which shows the extent to which the semantic preservation information captured by the model can be improved with syntactic or syntactic analogy enhancement .
the results are shown in table 6 . the first group of results show that the model with the most consistent performance is shen - 1 , while the second group shows the best performance with the least consistent performance . para - para models significantly outperform the models with different syntactic and semantic features . as the results show , the syntactic features that contribute to the generalization of the model are crucial for the model to achieve the best results .
results on yelp sentiment transfer are shown in table 6 . our best models ( right table ) achieve higher bleu than prior work at similar levels of acc , but untransferred sentences achieve the highest accu . the best model , yang2018unsupervised , achieves 31 . 3 % bleu on average compared to the strong baselines of both fu - 1 and multi - decoder ( 7 . 6 % ) and the best unsupervised model ( 13 . 4 % ) in terms of acc . note that the definition of acc varies by row because of different classifiers in use . our model achieves acc of 22 . 4 % higher than the best previous work ( 22 . 3 % ) on the scale of 1000 sentences .
in table 2 , we report the percentage of reparandum tokens that were correctly predicted as disfluencies as well as the average number of repetition tokens for each type of disfluency . as a sanity check , we also included the number of tokens for repetition and non - rephrase tokens , as these are the only tokens that are considered to be disfluential in our system .
the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) or in neither . as shown in table 3 , the content - content tokens are the most frequently predicted to belong to each category , while the function - function tokens belong to the other two categories less frequently .
we find that the text - rich model performs best when trained and tested on the test set with the best average number of iterations , and when the model is tested on both the standard english and french word embeddings . as the results show , the combination of text and innovations improves the generalization ability of the model , and the dev mean improves over the single - input approach . moreover , the model becomes more interpretable as the output improves with each iteration .
performance comparison of our model with the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . word2vec embeddings achieve high accuracy on the discuss and agree tasks , while rnn - based sentences are accurate on the disagree task . our model achieves the best overall performance , with an accuracy of 82 . 43 % on the topic discuss and a full 82 % accuracy on relation extraction .
table 2 shows the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . the attentive neuraldater significantly improves the performance over the previous methods and the joint model outperforms ac - gcn and maxent - joint ,
the accuracy ( % ) of our model with and without attention is shown in table 3 . the difference in accuracy between word attention and graph attention shows that the word attention alone does not improve the performance for neuraldater . however , it does improve the accuracy of the s - gcn model with respect to word attention .
the performance of all models on the test set is shown in table 1 . embedding + t models significantly outperform the original dmcnn model in terms of all metrics , while the argument argument alone places it close to the performance of the original cnn model . as the table shows , the argument stage is the most important part of the model development process , as it sets the stage for future iterations to converge on a common output . the trigger stage is crucial for the success of any model that relies on word embeddings alone . when combining all the data from the previous stages , the model performs best on both 1 / 1 and 1 / n tasks . trigger and argument stage are the only ones that perform consistently worse than either the argument or the original .
table 3 presents the results for event identification and event classification . the system performs well across all three aspects with a gap of 10 . 5 points from the previous state of the art . as expected , the most interesting thing about cross - event inference is that it eliminates the role of gender bias and gender - parity in the identification and classification of event triggers . it also improves the recall for both event and argument identification .
the results are shown in table 3 . the first group shows that fine - tuned models perform better than the original ones on both dev perp and test wer . fine - tuning reduces repetition , but does not improve accuracy on the test set . as expected , the spanish - only model ( which relies on vocabulary word embeddings only , and has been shown to perform poorly in the past ) does not generalize well across all languages , the second group of results show that the lexical features of english - only and french - only models contribute similarly to accuracy on both test sets . finally , the third group shows the performance of all models except the original one when trained and tested on the same dataset . all models show lower precision on average compared to the original .
results on the dev set and on the test set using only subsets of the code - switched data are shown in table 4 . the fine - tuned model outperforms the original cs - only model with a large margin . fine - tuning reduces the noise in the model and improves the generalization ability for both training sets .
the accuracy on the dev set and on the test set is shown in table 5 . the fine - tuned approach shows marked improvement over the monolingual approach by a margin of 3 . 53 % on the gold sentence accuracy compared to the fixed - tailed approach . fine - tuned - disc also shows a significant improvement in accuracy over the original approach .
we show precision ( p ) and recall ( r ) on the conll - 2003 dataset as well as the f1 - score for using type - aggregated gaze features for training on all three eye - tracking datasets . the improvement in precision and recall is statistically significant ( t - test , p < 0 . 01 ) with respect to the baseline evaluation set , which shows that type combined gaze features give a significant performance improvement .
we show precision ( p ) and recall ( r ) on the conll - 2003 dataset for using type - aggregated gaze features as compared to the baseline model , and f1 - score ( f ) as a metric for evaluating the performance of our approach . the type combined approach shows a statistically significant improvement in precision and recall compared to baseline , indicating that the model can further improve the interpretability for the task at hand .
the experimental results on belinkov2014exploring ’ s test set are shown in table 1 . syntactic - sg embeddings generated by glove - extended refers to the synset of tokens obtained by running autoextend rothe and schütze ( 2015 ) on wordnet 3 . 1 and wordnet 4 . 1 , and gllove - retro refers to pre - trained wordnet vectors retrofitted by faruqui et al . ( 2015 ) . ontolstm - pp also uses syntactic and semantic tokens , but the type of tokens used in the original paper ( which uses skipgram ) is slightly different . the hpcd ( full ) and the extended form of our model outperforms both the original and the updated version .
results in table 2 show that the hpcd - based dependency parser significantly outperforms the original ontolstm - pp model with features coming from various pp attachment predictors and oracle attachments . also , the accuracy of the uas is slightly higher than the original lstm model with respect to all the pp predictors except oracle pp . further , the performance gain comes from a better dependency extraction algorithm .
table 3 shows the effect of removing the sense priors and context sensitivity ( attention ) from the model . the model achieves the best performance with a precision of 89 . 5 % on the ppa acc . score .
as can be seen in table 2 , the ensemble - of - 3 approach improves the bleu % score by 1 point over the plain en - de model and by 2 points over the multi30k model . the domain tuning also improves the generalization ability of the model , as the improvement is larger when the subtitle data is added .
the results of domain - tuned h + ms - coco are shown in table 3 . we find that the model with the best overall performance is the one with the most pronounced drop in performance for en - de and en - fr , while for mscoco17 , the drop is less pronounced .
table 4 shows bleu scores for en - de , en - fr and mscoco17 as well as marian amun . adding automatic image captions improves the general performance for all models except for those using multi30k dataset . the smaller size of the dataset ( micro - f1 ) and the brevity of the captions ( dual attn . ) seem to have little effect on performance , however , for larger datasets ( multi30k ) , the effect is less pronounced .
the results in table 5 show that enc - gate and dec - gate strategies achieve comparable or better bleu % scores than en - de and mscoco17 ( + subs3mlm ) , indicating that the decoding ability of the encoder is further enhanced by the presence of pre - trained knowledge of the target domain and the mask surface . however , the difference in accuracy between en - fr / en - de ( the standard encoder for flickr16 and 17 ) and the dec - gated encoder ( + dec - gate ) is less pronounced , indicating that encoder pre - training may help the model to learn more about the target and the visual information at once .
the results of en - fr and en - de models are shown in table 3 . we find that the ensemble - of - 3 approach significantly outperforms the monolingual approach , while the multi - lingual approach underperforms the text - only approach . the visual features feature - rich approach by itself improves the general performance of the model , but does not improve the fine - tuned approach significantly .
table 3 shows that en - fr - ht and en - es - ht receive significantly better performance than either mtld or ttr on the hidden test set of yule ’ s i and ii . moreover , the performance gap between the two sets is much narrower than that between mtld and ttr , indicating the advantage of finetuning the word embeddings during training .
the number of parallel sentences in the train , test and development splits for the language pairs we used is shown in table 1 . our model splits the sentences into 25 language pairs for each training and development set . the total number of sentences in our model is 1 , 467 , 489 , which is 559 , 633 sentences per language pair .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the average number of words per vocabulary pair for each language pair is 113 , 132 , which means that our models can learn to learn more than 200 words per sentence .
the system evaluation scores ( bleu and ter ) for the rev systems are shown in table 5 . the en - fr - rnn - rev and en - es - trans - rev systems receive good automatic evaluation scores . however , the ter evaluation scores are slightly lower than the bleu scores , indicating that the translation task is more difficult for the system to learn and maintain .
results on flickr8k are shown in table 2 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the audiovisual model from our second submission . the mean mfcc rank of our model is 0 . 2 , which means that our model significantly outperforms the previous state - of - the - art model on all metrics .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . it achieves the best recall @ 10 and median rank of 1 . 414 , while the average chance is 3 , 955 . the acoustic embeddings based on audio2vec - u outperform all the other approaches except for segmatch .
we report further examples in the appendix . the most interesting thing about these examples is that the rnn classifiers turn on sentences that are already in the screenplay but are at the edges , making it easier to turn them into sentences that the original embeddings can turn into . as the table 1 shows , the dan classifier is particularly useful for this task , as it turns on a sentence that is already in a screenplay and can easily be turned into a sentence . cnn also uses rnn as a classifier for the task at hand .
table 2 shows the percentage of occurrences for each part - of - speech that has increased , decreased or stayed the same through fine - tuning in sst - 2 . the numbers indicate the changes in percentage points with respect to the original sentence . for example , the number of instances for nouns and verbs has increased by about 3 % and by about 7 % respectively , while for adjectives has decreased by about 4 % . the presence of the word " good " in the sentence has not changed , indicating that the vocabulary used for the sentence is the same across all three clusters . punctuation marks the extent to which the grammatical entities in question have been expanded or subtracted . the last row indicates the overlap with the original sentences of the two clusters , as shown in fig . 3 .
as shown in table 3 , the sentiment score increases as a result of the flipped labels being flipped from negative to positive sentences . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the numbers indicate the changes in percentage points with respect to the original sentence .
the results of pubmed and sst - 2 are shown in table 2 . as can be seen , the difference in accuracy between good and bad pmi is less pronounced for pubmed , but still significant . sift achieves a higher accuracy rate than either pubmed or corr ( 98 % vs . 98 % ) .
