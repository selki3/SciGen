table 2 shows the throughput and training time for the treelstm model using our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as shown in the table , both approaches give good gains in terms of inference performance .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t .
the results shown in table 2 show that the max pooling strategy consistently performs better than softplus and sb , indicating that softplus is a better complement to the sigmoid model . however , we find that it is less useful to use softplus augmentation since it results in a lower f1 score . we find that using only one type of hyperparametrization parameter , the dropout probability is relatively high , which explains why softplus outperforms softplus . selecting only the three features that contribute the most to the model performance , the most important ones are the filter size , the activation func . and the number of frames per domain .
table 1 shows the effect of using the shortest dependency path on each relation type . our model significantly improves the f1 score over the best previous state - of - the - art model .
consistent with the observations by vaswani et al . ( 2017 ) , we observe that the performance gap between the best performing models on the three types of test sets is narrower when we only consider f1 and f1 scores , with the exception of r - f1 . note that for both groups , the drop of accuracy between 50 % and 100 % indicates that the model performing best on f1 is due to high accuracy in the low - supervision settings .
the results are shown in table 1 . we achieve 100 % accuracy on the basic paragraph prediction using the mst - parser model . it achieves close to 50 % on average , outperforming all the other approaches that do not use pre - trained word embeddings .
we observe lower performance on the paragraph level for the two indicated systems compared to the standard stagblcc embeddings . note that the mean performances are lower than the majority performances over the runs given in table 2 .
the results are shown in table 5 . the original and the cleaned tgen models perform comparably to each other when the training data are added and removed . however , the results are markedly worse when the original tgen is trained with sc - lstm data instead of tgen data , indicating that removing the semantic information from the original helps the model perform better . when trained with original and cleaned data , the bleu and nist scores are significantly higher than those of the original ( sign test , table 5 ) . finally , the cider score is slightly higher than the original but still lower than the cleaned baseline . table 5 shows that once semantic information is added , the model performs better than it did when it was trained using tgen alone .
table 1 compares the original e2e data with the cleaned version . the difference in number of distinct mrs and total number of textual references is minimal , however we see significant difference in ser ( p < 0 . 00 ) due to different training and test sets containing different training contexts . also , the difference in slot matching script performance is significant , with the original training set containing 42 , 061 instances and the test set containing 33 , 525 instances .
table 3 shows the bleu , nist , meteor and rouge - l scores of the original and the original models trained on the same dataset . the results show that the original model tgen + is significantly better than the original tgen model , and that it is better than sc - lstm when trained and tested on the original dataset . table 3 compares the results of adding and subtracting correct answers from the two sets of data when training and test sets are the same . adding correct answers after replacing the wrong ones gives a significant improvement ( 7 . 27 % ) over the performance of tgen − , but still performing significantly worse than tgen + . replacing the training data with the original one does not improve results significantly ( 6 . 03 % ) or improve results ( 4 . 59 % ) . adding and testing correct answers twice after training , however , gives a 0 . 9 % improvement over the original performance . selecting the correct answer once again proves beneficial , improving scores by 4 . 59 % over the baseline .
table 4 : results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) and the number of instances that were cleanly added ( significantly fewer errors ) as compared to the original set .
table 3 compares our model with previous state - of - the - art approaches on the hidden test set of gcnseq , tree2str and ps graphlstm . our results show that , let alone a reduction in performance , all the methods we consider give a comparable performance improvement on data without sacrificing too many features .
table 2 shows the performance of our model on amr17 . our dcgcn model achieves 43 . 5 bleu points , which marginally outperforms the previous state - of - the - art . by comparison , ggnn2seq ( beck et al . , 2018 ) achieves 27 . 6 bleu points , a performance gap of 3 . 5 points with the best performing ensemble model . gcnseq ( damonte and cohen , 2019 ) achieves 24 . 6 points , an absolute improvement of 2 . 4 points over previous work .
table 2 compares the performance of our model with previous models on the german and czech test sets . the first set of results in table 2 shows that , let alone a reduction in performance , our ggnn2seq model outperforms all the base lines with a gap of 10 . 8 % on average compared to the previous state of the art .
the effect of the number of layers inside our model is shown in table 5 . we observe that for each layer , our model performs better than the previous state of - the - art .
table 6 compares the results of rc and rc + la with baselines . adding rc information improves the generalization ability of gcns , but does not improve the performance for residual connections . rcn with residual connections show lower performance than those without . gcn with rc and la information do not generalize well compared to baselines , moreover , the performance gap between rcn and la is greater than that between b and r ( see x4 ) .
the results are shown in table 4 . our model ( dcgcn ) outperforms all the base the performance gap between dcgcn ( 1 ) and other models is modest , but significant with respect to gcn classification performance . our proposed model dkrn achieves state - of - the - art results , outperforming all the other base models apart from the exceptional case of eq . 10 . 4 % on average compared to the previous state of the art .
table 8 shows the ablation study results for the density of connections on the dev set of amr15 . the results show that removing the dense connections in the i - th block significantly reduces the noise in the decoder .
the results of an ablation study for the modules used in the graph encoder and the lstm decoder are shown in table 9 . the results show that , under the " - linear combination " and " - direction aggregation " models , the global nodes and the hierarchical clustering of the nodes perform best , while the global node and the linear grouping do not . also , under " - coverage mechanism " we get a performance drop of 2 . 8 % compared to the previous state of the art .
table 7 shows the performance of the initialization strategies for different probing tasks . our paper shows that our framework establishes a new state - of - the - art on all three high - level tasks , and on all subtasks except for subjnum . it improves upon the strong baselines by 4 . 8 points in the coefficient metric , and by 2 . 4 points in subjrank .
the results are shown in table 3 . our h - cmow model outperforms the cbow model and the h - cbow model . it achieves state - of - the - art results , improving upon the strong baselines by 3 . 8 points in the last analysis .
the results are shown in table 5 . our model outperforms all the base the first group shows that our cbow / 784 model significantly outperforms the best previous approaches across the five sub - categories . it achieves state - of - the - art results , outperforming all the alternatives except for sst2 .
table 3 shows the relative improvements on the four downstream tasks that our models performed on the unsupervised test set . cbow outperforms cmow and hybrid , showing that the cbow model can learn the tasks to a high degree . however , when trained with cmp . data , the difference between cbow and cmow becomes less pronounced , with a drop of 2 . 5 % in performance on the sts13 and sts16 test sets .
table 8 shows the performance of our system for initialization and supervised downstream tasks . our system establishes a new state - of - the - art on all three high - level tasks , and on all subtasks except mpqa . it improves upon the strong glorot baseline by 4 . 6 points in the sub - decoder performance .
table 6 shows the performance of our method compared to the state - of - the - art cbow model on the four downstream tasks . cbow - r shows lower performance on some of the tasks , but higher performance on others . on the sts13 and sts16 tasks , it achieves gains of 2 . 2 and 3 . 9 points over the best performing cmow model .
the results are shown in table 4 . we observe that the three methods give similar performances on the hidden test set . however , the difference in precision is most prevalent in the subtasks of subjnum , coordinv and length , where the cbow - r model consistently shows much better performance . topconst and topconst are the most difficult tasks to solve , and precision is only obtainable with a reduction of 0 . 5pp over the strong baselines .
we notice that the cbow - r model significantly outperforms the cmow - c model in all three sub - tests . in fact , it achieves state - of - the - art results on all subtasks , outperforming the original cmow model by a large margin .
finally , we give a brief overview of our system ' s performance on the loc and misc datasets . in table 3 , we report the results of all loc , all per and all misc tasks , the results are presented in bold . our system outperforms all supervised and unsupervised methods except for the case where it obtains the best results . in all but one of the comparisons , our system performs better than the best supervised oracle . the difference is most prevalent in the last category , name matching , which shows that the supervised learning approach can significantly improve the generalization ability of the model .
uncertain in low - supervision settings . name matching and named entity recognition are shown in table 2 , however , when trained and tested on a larger test set , the results are less clear , supervised learning ( model 1 ) and τmil - nd ( model 2 ) achieve higher f1 scores than both the naive and supervised learning methods . in fact , models trained using only one type of name matching algorithm , namely , the name matching method , achieve higher precision than those using only supervised learning . further , the precision increases with the growth of the training set , as shown in fig . 3 , the effectiveness of the supervised learning method decreases as the result of increasing the training size and the training epochs . since the training window size is small , small differences in precision are sometimes associated with small improvements in performance .
table 6 presents the results on paragraph selection . our model obtains the best results in terms of both ref and gen . it closely matches the performance of s2s and g2s - gin , while it is inferior to both the former and the latter on gen .
we compare our proposed approach against previous state - of - the - art models on the ldc2015e86 and ldc2017t10 datasets . the results , summarized in table 3 , are broken down in terms of performance on bleu , meteor and s2s measures , with the exception of konstas et al . ( 2015 ) . our proposed g2s model outperforms all the base lines with a gap of 10 . 32 ± 0 . 53 points on the two datasets on average . by further adding entity nodes , we get a performance gap of 3 . 36 ± 0 . , 18 and 6 . 59 points on bleeu and meteor , respectively , compared to the previous state of the art . our proposed approach outperforms both published and unpublished work on every metric by a significant margin .
table 3 shows the performance of our model on the ldc2015e86 test set when trained with additional gigaword data . our g2s - ggnn model improves upon the strong lemma baseline by 3 . 60 points in bleu score .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model ( get + bilstm ) obtains a performance improvement over the best previous state - of - the - art model by 3 . 8 % on the bleu metric , and by 2 . 3 % on meteor metric .
we observe that g2s - gat has outperformed all the base models with a gap of 3 . 51 % on average between the two baseline models in terms of sentence length .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( added ) and the fraction of elements of the input graph that are available in the missing sentence ( miss , for the test set of ldc2017t10 . the token lemmas are used in the comparison . s2s outperforms g2s - gat , indicating that the model can rely less on syntactic or semantic information . gold refers to the reference sentences . as shown in table 8 , the size and type of tokens in the gold - two - mention pairs are the most important factors in the generation of the goldtwo - sentence pairs .
table 4 shows the pos and sem accuracy scores for different target languages using the 4th nmt encoding layer . the pos features significantly improve over the original embeddings , showing that the semantic features extracted from the golbeck - keller - mikolov ( 18 ) model can be used to improve the model ' s performance in the semantic tasks . sem also shows a drop in accuracy compared to the original model . with 200k sentences , the model performs slightly better than expected .
table 2 compares the pos and sem tagging accuracy with baselines and an upper bound . word2tag embeddings significantly outperform the unsupervised embedding method ( 95 . 06 % vs . 87 . 41 % ) , but do not exceed the upper bound of 91 . 11 % on pos ( 87 . 95 % ) and 91 . 55 % on sem ( 83 . 55 % ) . unsupemb also outperforms word2tag ,
table 4 presents the pos tagging accuracy and sem tagging accuracy scores . we observe that , let alone a drop in performance , precision is relatively high across all three metrics , with the exception of the case of the last group .
pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we observe that the bi and res layers contribute similarly to the task , with the difference being that the res layer contributes more features . uni and bi layers both individually and as a group perform better than the res - based layer .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . gender and race features seem to be the mostchallenging for the attacker , as their accuracy is relatively low compared to other protected attributes . however , age and sentiment features are the most stable , lowering the overall performance to less than the level of the adversary . finally , the presence of the word " b * tch " in the sentence tags ( p < 0 . 01 ) indicates that the attacker is aware of the gender bias .
accuracies are shown in table 1 . the first set shows that the training set establishes a new state - of - the - art on the task prediction using only one type of data , namely , the dial and sentiment tasks . the second set shows the performance of the mention and dial tasks , both for genders .
table 2 shows the results for the balanced and unbalanced data splits . the results show that the gender and task accuracy are the mostchallenging aspects for the task prediction , followed by the age gap . gender and race are the only two areas where the accuracy is balanced , however , the unbalanced task accuracy is relatively high .
the performance of these models on the adversarial training set is shown in table 3 . in general terms , the results show that the gender and race features contribute similarly to the task , with the gender - based features having a bigger impact . however , the difference between the attacker score and the corresponding adversary ’ s accuracy is less pronounced , indicating that gender bias does not contribute significantly to the model performance . the presence of the word " b * tch " in the training data , along with the presence of " race " and " gender " seem to indicate that the features discussed in the previous section have a significant impact , however it is unclear whether this is reflected in the actual performance .
the results shown in table 6 show that when the protected attribute is encrypted , the rnn model can easily distinguish between the true and negative states . guarded rnn also exhibits a significant drop in performance .
we compare our model with previous state - of - the - art on the following three similarity test sets : the ptb base , wt2 base , ptb + finetune and wt2 + . the results are presented in table 2 . the difference in performance between the base and the finetune set is most prevalent on the ptb base , where our model obtains a final accuracy of 69 . 36 % compared to 62 . 63 % by yang et al . ( 2018 ) . further , on the wt2 base , our model achieves an accuracy of 73 . 37 % vs . 69 . 63 % , a marginal improvement over the previous state of the art . lrn also achieves competitive or better results than previous work , with an absolute improvement of 1 . 97 points over the lstm baseline . table 2 compares the performance of our model on the three comparison sets . our work shows that the dynamic part - ofspeech recognition approaches are comparable , but do not outperform our model , in which case we should note that the difference is narrower .
table 3 summarizes our results on the base and time metrics . the first set of results in table 3 shows that the lstm model is well - equipped to perform on the training data with a minimum of 80 % accuracy . the second set in the table shows that it is more than able to do the task in a reasonable amount of time . table 3 compares the performance of different approaches with the previous state - of - the - art on various base metrics . our model achieves the best results with a base time - averaged f1 score of 0 . 525 on the ln and a maximum of 3 . 43 on thebert metric , which shows the advantage of finetuning word embeddings during training .
table 3 presents the test set on the yelppolar err , amapolar time and yahoo time benchmarks . the results are presented in bold . as shown in the table , ama and yelp time are the most representative while atr is the less representative . table 3 compares the performance of different approaches on the three datasets . we observe that , let alone a reduction in performance , all the methods we considered had comparable performance with the original ones ( except for the one by zhang et al . ( 2015 ) .
table 3 shows the case - insensitive tokenized bleu score on the wmt14 english - german translation task . gnmt outperforms all the base systems except olrn , lrn and atr by a large margin . though the number of parameters used to decode one sentence is small , the time taken to train and decode each sentence is considerable , as measured by the average training time of 0 . 2k seconds on newstest2014 dataset . table 3 also highlights the performance gap between gnmt , atr and sru . although gnmt has the advantage of training with fewer parameters , it is unable to solve most cases , as shown in table 3 . as shown in the second group of table 3 , the smaller size of gnmt dataset makes it less suitable for this task , as it requires much more training time . also , lrn is comparable with atr in terms of training time , but it requires significantly more time to decode each sentence . finally , sru also requires a significant amount of time to solve each case .
table 4 shows the exact match / f1 - score of our model on the squad dataset . our model obtains a significant improvement over the strong baselines by 1 . 83 points over the previous state - of - the - art model by adding elmo ( elmo et al . , 2017 ) into the base set . further , our model improves its f1 score by 2 . 41 points compared to wang et al . ( 2017 ) by adding two parameters , namely , the parameter number of " elmo " and " params " . sru also improves its performance by 2 points .
table 6 shows the f1 score of our model ( lstm * ) on the conll - 2003 english ner task . the model obtains a significant improvement over the previous state - of - the - art results by 9 . 94 % on average compared to lample et al . ( 2016 ) .
table 7 shows the performance of our model on the snli and ptb tasks . with the base setting , our lrn model obtains 85 . 56 % accuracy on snli task and 169 . 81 % on ptb task , which shows significant gains in performance over the strong baselines .
table 2 shows the system and sentence recognition results for english and german captions . retrieving the most important features for each system is presented in table 2 . we observe that both systems are able to do each sub - step in real - time ; the difference is minimal , however system performance is still significantly worse than that for english . oracle retrieval is comparable to human , with the exception of sentence prediction . sentence prediction is very similar for both systems ; however , for english , it is much worse . system and sentence prediction are both more difficult to achieve than for german . table 2 compares system and word prediction using the current state of the art . we benchmark against the following set of features : r - 2 , r - 4 and mtr . word prediction using automatic metrics yields very similar results : on average , about 140 words per word , with an average of 35 . 08 per sentence .
table 4 presents the results of human evaluation . our system outperforms all the other systems with a large margin . the highest standard deviation among all is 1 . 8 points , which indicates that our model can easily distinguish between syntactic and semantic truth .
the results are shown in table 5 . we observe that , let alone a reduction in performance , the training set performance on the " ted talks " dataset has been consistently better than the " europarl " dataset on all datasets except for the one where it was slightly worse . on the " p " scale , the difference between en and pt is minimal , but significant with respect to " slqs " . table 5 shows that for the " docsub " dataset , training on the ted talks dataset , especially , results are significantly worse than those on the europarl dataset . finally , we see that rn and rn are the only two groups that are statistically significant ( paired ttest ) with " pt " as a lower - bound on the en / pt score .
the results are shown in table 3 . we observe that , let alone a reduction in performance , the training set performance on the " ted talks " dataset has been consistently better than the " europarl " dataset on all datasets except for the one where it was slightly worse . on the " p " scale , the difference between en and pt is minimal , but significant with respect to " slqs " . table 3 shows that for the " docsub " dataset , training on the ted talks dataset , especially , results in significantly better performance . table 5 shows that the clustering performance of the " hclust " and " sim " subsets is relatively consistent , but still significantly worse than those on " p " . when trained on " sqs " ,
the results are shown in table 5 . we observe that , let alone a reduction in performance , the training set performance on the " ted talks " dataset has been consistently better than the " europarl " dataset on all datasets except for the one that is tested on " patt " . specifically , all the models trained on europarl outperform the ted talks dataset except for those using df . table 5 shows that for the " patti " dataset , there is no significant difference in performance between the en and pt scores of the two sets , indicating that training on the same dataset leads to different training sets having different interpretability patterns . on the other hand , the " docsub " dataset outperforms the " slqs " and " tf " set , showing that the semantic information injected into the training data by the additional cost term is significant enough to result in a measurable improvement .
corpus are presented in table 4 . the averagedepth and maxdepth measures show that the dsim and docsub datasets are well - equipped to perform this task , with the exception of the case of slqs . according to the table , europarl achieves the best averagedepth of 11 . 05 % on the three datasets , which shows that the semantic relations in the dataset are strongly concentrated within a single dataset . on the other hand , docsub has the worst performance , showing that it is unable to distinguish between semantic and syntactic relations . at the same time , the number of tokens per row is relatively high , with 1 , 588 ( 1 , 588 vs 1 , 025 ) and 1 , 185 ( 1 . 184 ) compared to the average of 736 ( 788 vs 835 ) .
we also evaluated the relation extraction method proposed by peyrard and gurevych ( see table 1 ) . the first set of results in table 1 shows that , overall , all metrics we considered had low correlation with human judgement . europarl , in particular , had the lowest correlation with total terms . according to the table , total terms and roots are the most important factors in evaluating a relation extraction algorithm , followed by averagedepth .
in table 1 , we compare the performance of the enhanced version of our lf model with the original one ( lf ) on the validation set of visdial v1 . 0 . 7 . the enhanced version exhibits significant gains in loss function over the baseline model , and consequently , achieves the highest ndcg % ( 73 . 42 % ) compared to the original version ( 71 . 63 % ) . however , the difference between the two approaches is less pronounced for vqa , indicating that the enhanced model can rely less on superficial cues .
performance ( ndcg % ) of ablative studies on different models on the visdial v1 . 0 validation set . note that only applying p2 indicates the most effective one ( i . e . , the one that receives the most accurate dictionary learning ) , and that it is the most suitable for the current task .
we notice that the hmd - f1 model performs comparably to the pre - trained models on soft alignments as well as hard alignments , indicating that the recall function is useful for both datasets .
the results are shown in table 4 . the first set shows that the baselines set by meteor + + and ruse ( * ) are comparable to the best performing baselines on the direct assessment set . however , when we add smd and w2v , the results get worse than the baseline showing that the transfer learning method can significantly improve the results on the direct assessment set . table 4 shows the average score of bertscore - f1 scores on the selected set of metrics .
the results are shown in table 4 . the first set shows that the bertscore - f1 and the sent - mover scores are relatively stable , while the bleu scores are significantly lower . sent - mover performance is significantly worse than that of baselines , indicating that the semantic information injected into the model by the additional cost term is significant enough to result in a significant drop in performance . when we add w2v features , our model achieves the best results , showing that semantic information extracted from the smd dataset is a strong complement to the cost term .
word - mover performs similarly to the baseline models on both m1 and m2 while on m2 it performs slightly worse than the former best performing baseline . the semantic threshold set by meteor ( meteorological time complexity < 0 . 939 ) and spice ( 0 . 594 ) gives a significant performance boost which is expected in a production setting . we observe that the semantic threshold sets by spice and leic give a significant boost as well , which indicates that these features are useful for improving the recall of the word prediction . sentuation performance is relatively consistent across all metrics with the exception of leic ( * ) where it gets a slight drop . word - mover performance is comparable with the baseline set by word2vec ,
we observe that the models trained with only one type of wrapper , namely , the meta - para - aware wrapper , perform better than those trained with two types of wrapara : the combination of the two types result in better generalization . the results are shown in table 6 . more importantly , the results show that the performance reach the best when the model is trained with both meta - and syntactic layers of the wrapper , improving the generalization ability of the model .
table 3 shows the transfer quality and semantic preservation results on the yelp dataset . the results show that yelp significantly outperforms the best previous approaches across all three semantic and transfer quality measures . semantic preservation results are notably lower than those of google translate ( differences are statistically significant with t - test , p < 0 . 05 ) with respect to transfer quality , indicating that the semantic features discussed in section 3 . 3 are important for the model to perform well . finally , we see that the drop in transfer quality between m0 and m7 indicates that yelp has learned to rely less on word embeddings and more on syntactic features . we observe that , for the semantic preservation task , yelp has done exceptionally well , improving upon the strong semantic and syntactic baseline by 4 . 6 points in accuracy over the strong baselines .
table 5 presents the human evaluation results for each metric . it follows spearman ’ s [ italic ] ρ b / w sim and human ratings of semantic preservation . the results show that both summaries generated by the machine and the human are comparable in terms of syntactic and semantic recall ( see table 5 ) . however , the difference in acc score between human and machine evaluations is much larger , more importantly , it shows that the human judgments are significantly more accurate .
the results are shown in table 6 . our model achieves the best results with the para - para model combination . the difference in acc and gm between the baseline models is minimal , however we see significant difference in sim and pp scores due to different syntactic and semantic features in the different classifiers . adding the pre - trained word " para " and " - lang " improves sim performance by 3 . 8 points in the standard task formulation and by 4 . 3 points in gm . further , adding 2d syntactic features improves hern - 1 by 2 points .
results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best model ( right table ) achieve higher acc than prior work at similar levels of acc , but untransferred sentences achieve the highest blei . the results are slightly worse than those of simple - transfer , but still superior to the best previous work . we use the best model , yang2018 , unsupervised , and cues from xiao et al . ( 2018 ) , which achieves acc of 22 . 3 on the scale of human reference . note that the definition of acc varies by row because different classifiers in use . when trained and tested using only one classifier , the accuracy obtained by fu - 1 , the multi - decoder model achieves only 7 . 6 % acc , lower than the accuracy achieved by any other classifier . using the lm and classifier combination achieves 15 . 4 % acc higher than simple transfer .
in table 2 , we report the percent of reparandum tokens that were correctly predicted as disfluencies and the average length of repetition tokens . reparandum length is the average of the number of tokens in a sentence , not the length of the repetition tokens , which explains ∼ 9 % of the variation in the performance .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in either the reparandum or repair ( content - content ) . however , for those that consist entirely of function - function , the picture is less clear , showing that more than half of tokens belong to each category .
the results are shown in table 2 . we observe that the text model developed by vaswani et al . ( 2017 ) is comparable to the best previous state - of - the - art model ( hochreiter and schmidhuber , 1997 ) on both test sets , with the exception of the case where it performs slightly worse . moreover , the results are slightly superior when we add in the effects of domain - adaptive keyphrases such as " text + raw " and " icon " to the baseline model , improving the dev mean by 0 . 2 points . however , this analysis fails to account for the significant drop in performance between the early and late stages when text is combined with innovations .
performance comparison of our model with the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model shows marked performance improvement over the state of - art embeddings . the accuracy increase from 28 . 43 % on average to 83 . 43 % . however , the difference between the average and the final accuracy is much narrower , showing that our model can rely less on superficial cues .
table 2 compares the performance of the different methods on the apw and nyt datasets . our unified model significantly outperforms all previous models . the accuracy is higher than the previous state - of - the - art on both datasets , indicating that the neural models learned to reason more intuitively .
table 3 compares the performance of our method with and without word attention . our approach obtains a significant improvement in accuracy ( from 61 . 2 % t - gcn to 63 . 6 % ) on the graph attention task , which shows the effectiveness of both word attention and graph attention .
the results are shown in table 1 . our model establishes a new state - of - the - art on all three high - level tasks , and on all subtasks except event argument identification . it closely matches the performance of the best previous approaches : the dmcnn model ( 74 . 2 % ) and jrnn ( 71 . 8 % ) by a noticeable margin . by further adding entity nodes , the model achieves gains of 2 . 8 points on event identification and 3 . 6 points on sentence identification .
table 1 presents the results on event identification and classification . our method establishes a new state - of - the - art on all three high - level tasks , confirming the value of semantic thresholding . the most striking thing about the cross - event identification results is that it eliminates the effect of false negatives and allows more accurate identification . further , the classifiers trained on the same dataset perform better on events with different classifiers , indicating that the semantic information injected into the model by the additional cost term is significant enough to result in a better match . table 1 shows the performance of the method and the f1 scores of all the methods .
the results are shown in table 5 . we see that all models give similar results on the test set , with the exception of spanish - only , which gives a performance improvement of 2 . 36 points . however , when we switch to english - only - lm , we get a performance gain of 1 . 63 points . this indicates that the semantic features extracted by the shuffled - lm are quite useful , but only when they are considered in combination with other lexical features . finally , we see that fine - tuning gives a 0 . 9 / 4 . 18 performance gain over the baseline model , which shows the advantage of selective attention .
results on the dev set and the test set are shown in table 4 . the fine - tuned model improves upon the strong baselines by 3 . 8 points in the standard task formulation and by 4 . 2 points in fine - tuned model .
fine - tuned - disc improves the performance of the model for both gold sentences and monolingual sentences . the accuracy increases for both sets , with the improvement being larger on the test set . cs - only - disc shows marked performance improvement , and fine - tuned - lm shows a drop of less than 1 % .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) scores for using the type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvement from baseline to the current state - of - the - art is statistically significant ( p < 0 . 05 ) .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) scores for using type - aggregated gaze features for the conll - 2003 dataset . the improvement from the baseline model to the type combined model is statistically significant ( p < 0 . 05 ) and r > 0 . 03 , indicating that the ability to select compact regions with the best performing gaze features induces the generation of better captions .
we apply the hpcd model developed by faruqui et al . ( 2015 ) to wordnet 3 . 1 , and glove - extended refers to the synset embeddings obtained by running autoextend rothe and schüze ( 2015 ) . in the original paper , the type and type of tokens used for initialization are presented in table 1 . however , when using syntactic - sg instead of skipgram , the results on the belinkov2014exploring test set are only slightly better ( 83 . 3 % vs . 84 . 7 % ) . the difference between the accuracy of the original and the extended model is less pronounced for wordnet , but still suggests some advantage to using syntactic skipgram instead of prefixing with prefixes .
results shown in table 2 show that the hpcd system outperforms the original ontolstm - pp model and the best performs on the full uas dataset . additionally , it achieves a significant improvement on the ppa acc . score , showing that the dependency parsing approach can significantly improve the generalization ability of the model .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results show that the ppa acc . accuracy ( normalized by the number of tokens ) increases by 0 . 5pp over the pretrained baseline ,
in table 2 we report the bleu % scores of the models using subtitle data and domain tuning for image caption translation . the results show that the domain - tuned model performs better than the model using en - de and mscoco17 embeddings . additionally , the ensemble - of - 3 model achieves the best result , showing that domain tuning improves the translation ability of the subtitle data .
we observe that the domain - tuned model improves the results for both en - de and subs1m models . the results are shown in table 3 . sub - domain - tuning models perform better than the best - performing model ( h + ms - coco ) on all three datasets . the difference is most pronounced in en - fr , where the h + ms model performs best .
table 4 shows the bleu scores of the models using only one type of automatic image captions and marian amun . the results show that , in all but one case , adding automatic captions improves the model ' s performance . in most cases , the improvement is much larger than that of adding manual captions . in addition , the accuracy increases when using multi - attribute captions , as shown next .
the results in table 5 show that the enc - gate and dec - gate strategies achieve better results than en - de and mscoco17 . moreover , the accuracy increases when using transformer , multi30k + ms - coco + subs3mlm , detectron mask surface , and zsg vector .
the results are shown in table 3 . subsequently , we report the results of the best performing models using the three aspects of the visual features : the ensemble - of - 3 model , subs3m , subs6m and mscoco17 . the results , summarized in bold , show that the features that give the best performance are the multi - lingual features , namely , the fine - tuning ability of the detectrons and the semantic features .
the results are shown in table 5 . in general terms , en - fr - ht and en - es - ht achieve better performance on the three types of test sets compared to the models using mtld . the performance gap between the two sets is narrower with respect to ttr , mtld and chime - 4 , as seen in the second group of table 5 , the transition of the speech tags between genders is less pronounced in the standard set , however , it is still perceptible that the transition tags are markedly gender - neutral , improving the bias metric by 9 % in standard and to parity in the cross - domain test set .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . en – fr consists of 1 , 467 , 489 sentences and en – es consists of 5 , 734 sentences .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the average number of words per language pair is 113 , 132 for english , 131 , 104 for french and 168 , 195 for spanish .
automatic evaluation scores ( bleu and ter ) for the rev systems are shown in table 5 . the en - fr - rnn - rev and en - es - trans - rev systems generate strong baselines comparable to the best state - of - the - art systems ( cf . table 5 ) . however , their performance is significantly worse than those of the best - performing systems ( paired t - test ) on rev datasets . in ter mode , the system performance is much worse than in rnn - rev , indicating the syntactic differences between the two sets are less pronounced .
table 2 shows the test set on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the model supervised using the learned reward function . the mean mfcc score of the vgs model is 0 . 7 , which means that the model can easily distinguish between the true response and negative responses .
the results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the audiovisual model from rsaimage . in terms of recall , acoustic2vec - u outperforms the other approaches as the average recall is higher than that of rsaimage , indicating the advantage of finetuning word embeddings during training . also , the mean mfcc score of 1 . 414 is lower than the average of 0 . 5 , indicating that the model performs less well in synthetic contexts . when trained with random chance and recall , audio2vec achieves the best performance ,
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . originally , all the examples shown in table 1 were for use in unsupervised settings . since the dan model only works on word embeddings with edges , it has to learn to pick out the most interesting ones and discard the rest . we report further examples in the appendix . using the cnn model , we can see that it is very similar to rnn in that it turns in a screenplay that contains both edges and a lot of curves . however , the difference between the two is less pronounced for rnn ,
part - of - speech changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . however , the percentage of instances in the correct sentence has increased by 3 . 5 % , 4 . 5 % and 7 . 9 % over the original sentence , which indicates that the quality of the model has not changed .
the sentiment score changes in sst - 2 . and indicate that the score increases in positive and negative sentiment . the flipped labels cause the sentiment score to decrease , however , the positive sentiment score increases only by a few percentage points .
table 2 compares the results of pubmed ( pubmed ) and sst - 2 with other approaches . results show that pubmed outperforms all the other approaches except for sift .
