table 2 shows the performance of the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation .
the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t parallelization . table 1 shows the performance of the treernn model with varying tree balancedness . we observe that the balanced dataset shows the best performance , with the exception of the fact that it has the smallest number of instances per second .
the max pooling strategy consistently performs better in all model variations . in table 2 , we show the results for each model with different representation . conll08 and ud v1 . 3 outperform both softplus and softplus - based softplus models in all but one case ( table 2 ) . the best performance is obtained when the model is trained with the best representation .
table 1 shows the results of using the shortest dependency path on each relation type . the results are shown in table 1 . we observe that the best f1 ( in 5 - fold ) achieved by our model is significantly higher than those obtained by using the macro - averaged approach , indicating that our approach obtains the best results in terms of f1 .
the results are shown in table 3 . we observe that the y - 3 model outperforms the best state - of - the - art systems in terms of f1 score , r - f1 score and f1 50 % on average . the difference between the performance of the best and worst state of the art systems can be seen in the results presented in table 4 . as shown in the table , the performance gap between the best performing state - ofthe - art models and the worst performing ones is small , but still significant .
table 3 shows the results of using mst - parser with the best performance . the results are shown in table 3 . as can be seen , when using the best performing parser , it achieves the best results in terms of precision and accuracy . it achieves the highest precision on all three metrics .
table 4 shows the c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level and paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . the difference in performance between the two systems can be seen in table 4 , with the paragraph - level system performing better than the essay - level one .
the results are shown in table 3 . the results show that the original tgen + model outperforms both sc - lstm and tgen − models in terms of training and test set performance . moreover , the difference in performance between the original and the cleaned tgen model is significant , with the original model performing better on both test set and training set . further , the performance gap between the two models is less pronounced when training and testing set are combined .
table 1 compares the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . the results are shown in table 1 . the difference between the original and the cleaned version is significant , with the original having more than twice as many textual references ( 42 , 061 vs . 17 . 69 ) and more than three times the number of slot references ( 28 , 862 vs . 33 , 525 ) compared to the original .
table 3 shows the performance of our system compared to the best state - of - the - art systems , sc - lstm , tgen + and tgen − . the results are shown in table 3 . as can be seen , our system outperforms all the state of the art systems in terms of training and test set performance . we observe that our system is better than all the other systems except for tgen + , which shows that it has better performance on the test set . also , we observe that the performance gap between our system and other systems is less pronounced than that of the other approaches .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) . these results are shown in table 4 . as can be seen , adding and removing the missing and incorrect values leads to a drop in the overall number of errors , which indicates that the tgen system is performing better than expected .
table 3 shows the performance of our system on the external and internal test sets . our system outperforms all the previous state - of - the - art systems except for graphlstm ( song et al . , 2018 ) and pbmt ( pourdamghani et al . 2016 ) .
table 2 shows the results of our model on amr17 . the results are shown in table 2 . our model achieves a bleu point improvement over the previous state - of - the - art systems , and outperforms all the other models except for seq2seqb ( beck et al . , 2018 ) , which achieves a performance drop of 3 . 5 points . gcnseq ( damonte and cohen , 2019 ) achieves 24 . 3 points improvement over previous state of the art systems , but still performs worse than the best of the best .
table 3 presents the results of our model on the english - german and english - czech languages . we observe that our model outperforms all the other models except for birnn + gcn , which shows that it is able to perform better on both languages than all the others except english .
table 5 shows the effect of the number of layers inside the network on the performance of our model . we observe that our model outperforms the previous state - of - the - art models with a significant margin . our model obtains the best performance with a maximum of 6 layers , and has the best overall performance .
table 6 shows the performance of our model compared to the baselines in terms of rc and rc + la scores . our model outperforms both the previous state - of - the - art and the best - performing baselines by a significant margin . the difference in performance between the two baselines can be seen in table 6 , where the difference in rc scores between the best and worst - performing gcns is more pronounced for the dcgcn models .
table 3 shows the performance of our model compared to the best previous state - of - the - art systems . our model outperforms all the previous state of the art systems except for dcgcn ( 1 ) , which shows that it is able to improve upon the performance by a significant margin . we observe that the performance gap between our model and the best other systems is small , but it is significant enough to warrant further study .
table 8 shows the ablation study for density of connections on the dev set of amr15 . the results are shown in table 8 . the results show that removing the dense connections in the i - th and ii - th blocks significantly improves the performance of the dcgcn4 model . as shown in the table , removing these connections significantly reduces the number of connections .
table 9 shows the results of the ablation study for modules used in the graph encoder and the lstm decoder . the results are shown in table 9 . we observe that the coverage mechanism used in our model outperforms all the other coverage mechanisms except for direction aggregation , which shows that our coverage mechanism obtains the best results .
table 7 shows the performance of the initialization strategies on probing tasks . glorot outperforms all the other initialization strategies except for bshift and subjnum , which show that it is better at selecting the correct subset of objects for each task . as shown in table 7 , the best performing initialization strategies are somo and topconst , both of which are used in our paper ( table 7 ) .
table 3 shows the performance of the h - cbow and h - cmow systems compared to the best state - of - the - art approaches . the results are shown in table 3 . we observe that the best performance is obtained by using the best models with the highest precision . as shown in the table , the best performing models are the ones with the best precision and the highest tense scores .
table 3 shows the results of the best performing models for sub - jurisdiction selection . the best performing model is cmow / 784 , which outperforms all the other models except for sick - e . it also outperforms both sst2 and sts - b . as shown in table 3 , the performance gap between the best and worst performing models is small , but it is significant enough to warrant further study .
table 3 : scores on unsupervised downstream tasks attained by our models . the results are shown in table 3 . our model outperforms both the cbow and cmow methods on all the downstream tasks except for sts13 and sts14 , where it outperforms the hybrid model by a significant margin .
table 8 shows the performance of our system on supervised downstream tasks . glorot outperforms all the other systems except sick - e and sst - b in terms of the number of sub - second responses . we also observe that the performance gap between our system and the best other systems is small , indicating that our system is able to handle the task of initialization better than those of other systems .
table 6 shows the results for different training objectives on the unsupervised downstream tasks . the results are shown in table 6 . we observe that the cmow - r model outperforms the cbow - c model on all the downstream tasks except for sts13 and sts14 , where it outperforms both of the methods .
table 3 shows the performance of our approach compared to the best state - of - the - art approaches . our approach outperforms all the state of the art approaches except for the cmow - c model , which shows that it is able to achieve better performance on some of the most difficult features .
table 3 shows the performance of the best approaches for sub - jurisdiction selection . the results are shown in table 3 . as can be seen , the cmow - r model outperforms all the other approaches except for sick - r , which shows that it is better at selecting the correct sub - categories . it also outperforms both the best and worst - performing approaches .
table 3 shows the performance of our supervised learning system compared to the best state - of - the - art systems . the results are shown in table 3 . we observe that our system outperforms all the state of the art systems except for τmil - nd , which shows that it is more difficult to learn the correct name for a given entity . in addition , the performance gap between our system and those of the best supervised learning systems is much larger than that of our system , indicating that our approach may not be able to learn all the correct words .
table 2 shows the results on the test set under two settings . the results are shown in table 2 . in both settings , the supervised learning model ( mil - nd ) and the name matching model ( tmtmil ) outperform both the automatic and supervised learning systems . as shown in the table , the difference in f1 scores between the supervised and the automatic models is small , but it is significant enough to show that the difference between the performance of the two systems is due to the small difference in the f1 score between the trained and unsupervised models .
table 6 shows that g2s - gat outperforms all the other models in terms of the number of instances in which the model is trained on the gat dataset . it is clear that the model trained on gat has better performance than the other two models , as shown in table 6 . however , the difference between the performance of the two models is less pronounced in the case of gat .
table 3 presents the results of our model on the ldc2015e86 and ldc2017t10 datasets . our g2s - gat model outperforms all the other models in terms of bleu and meteor scores . it also outperforms the best - performing models of the previous state - of - the - art systems .
table 3 shows the results on the ldc2015e86 test set when models are trained with additional gigaword data . the g2s - ggnn model outperforms all the other models in terms of bleu scores . it also outperforms both the best - performing models when trained with external gigaword data , as shown in table 3 .
table 4 shows the results of the ablation study on the ldc2017t10 development set . the results are shown in table 4 . we found that the combination of bilstm and get improves the performance of the model , and the size of the models improves over the previous state - of - the - art .
table 3 presents the results of our model in terms of the number of sentences and the average length of sentences . the results are shown in table 3 . our model outperforms all the other models except for g2s - gat , which shows that it can handle longer sentences than other models .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . g2s - gin outperforms s2s and gat in terms of accuracy . as shown in table 8 , gold outperforms all the other models except gat , which shows that it is able to generate more than 50 % more tokens than the other two models .
table 4 shows that the pos and sem tagging accuracy are comparable across the different target languages on a smaller parallel corpus ( 200k sentences ) . moreover , the pos tagging accuracy is slightly better than the sem accuracy .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . the results are shown in table 2 . word2tag classifier outperforms both unsupemb embeddings in terms of most frequent and most frequently - tagged tags . as shown in fig . 2 , the best performance is obtained when using unsupervised word embedding .
table 4 shows the performance of our system in terms of pos tagging accuracy and sem tagging accuracy . the results are shown in table 4 . we observe that our system outperforms the previous state - of - the - art systems in both accuracy and precision . our system achieves the best performance on both the pos and sem metrics .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 . we observe that the uni encoder has the best overall performance , and the res encoder and the bi encoder have the worst performance . as shown in the table , the best performance is obtained by using only the best features from the four layers of the uni and bidirectional nmt encoding .
table 8 shows the performance of the attacker on different datasets . results are on a training set 10 % held - out . δ is the difference between the attacker score and the corresponding adversary ’ s accuracy . as shown in table 8 , the best performance is obtained on pan16 , where the attacker achieves the best overall performance .
table 1 shows the results of training pan16 on a single task . the results are shown in table 1 . as can be seen , pan16 outperforms pan16 with respect to accuracy and sentiment . it also outperforms both pan16 and pan16 - based systems in terms of recall .
table 2 shows the results of the balanced and unbalanced data splits in pan16 . the results are shown in table 2 . as can be seen , gender and age are the most frequently leaked attributes , followed by race and gender - neutral features . in pan16 , gender is the least frequently leaked , followed closely by age and sentiment .
table 3 shows the performance of our system on different datasets with an adversarial training . the results are shown in table 3 . as can be seen , our system outperforms the previous state - of - the - art approaches on all three datasets except for gender and age .
table 6 shows the performance of the protected attribute with different encoders . the results are shown in table 6 . rnn outperforms both guarded and leaky embeddings in terms of accuracy . the difference in performance between rnn and guarded is most pronounced when using the guarded encoder . it can be seen that the rnn encoder performs better than guarded in both cases .
table 3 presents the results of our work on the ptb and wt2 baselines . the results are shown in table 3 . our work shows that our approach outperforms the best state - of - the - art systems in terms of performance on all three baselines , with the exception of the wt2 baseline , which shows that it is unable to handle the large number of parameters that are required for the best performance .
table 3 presents the results of our work on the lstm and gru models . the results are shown in table 3 . as can be seen , our model outperforms both the previous state - of - the - art systems in terms of performance . we observe that our model performs better than both the best state of the art systems on both the time and the performance metrics .
table 3 presents the results of our model on the best performing time - of - speech ( err ) and yelppolar time ( yahootime ) datasets . the results are shown in table 3 . our model outperforms all the other approaches except for the work of zhang et al . ( 2015 ) , which shows that it is able to improve upon the performance of the previous state of the art in both err and time of speech . as shown in the table , the performance gap between our model and other approaches is small , but still significant .
table 3 shows the bleu score on the wmt14 english - german translation task on tesla p100 . the gnmt model outperforms all the other models except for olrn , which shows that it has better performance in case - insensitive tokenized tokens . as shown in table 3 , gnmt outperforms the other three models in terms of the number of tokens tokenized in the training and decoding steps .
table 4 shows the exact match / f1 - score on the squad dataset . the results are shown in table 4 . we observe that the best model , lrn , outperforms all the other models except atr and sru in terms of f1 score . lrn outperforms atr , sru , gru , and gru + elmo by a large margin . moreover , the performance gap between lrn and atr is less pronounced than that between gru and lrn + elmo .
table 6 shows the f1 score on conll - 2003 english ner task . as shown in table 6 , the lstm model outperforms all the other models except for sru and lrn , which have lower f1 scores . the difference between lrn and sru is due to the small number of parameter number in the ner tasks , which indicates that lrn is more sensitive to the number of parameters .
table 7 shows the test results on snli task with base + ln setting and test perplexity on ptb task with base setting . the results are shown in table 7 . lrn outperforms all the other models except elrn and glrn in both cases .
table 3 compares the performance of human and machine learning models trained on the b - 2 and b - 4 systems with those trained on oracle retrieval and mtr systems . the results are shown in table 3 . our system outperforms all the other systems except for human , which shows that human learning models are better at predicting sentence length and sentence quality .
table 4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that the human evaluation performed better than the automatic systems on these three metrics . table 4 also shows the percentage of evaluations a system being ranked in top 1 or 2 for overall quality . seq2seq outperforms candela and h & w hua and wang ( 2018 ) on all three metrics , with the exception of appr , which is slightly higher than human evaluation .
table 3 shows the performance of our system compared to the best state - of - the - art systems . our system outperforms all the other systems in terms of both performance on the test set and on the training set . the results are shown in table 3 .
table 3 shows the performance of our system compared to the best state - of - the - art systems . our system outperforms all the state of the art systems in terms of performance . the results are shown in table 3 . it can be seen that our system performs better than all the other systems except for ted talks .
table 3 shows the performance of our system compared to the best state - of - the - art systems . our system outperforms all the other systems in terms of both performance on the test set and on the training set . the results are shown in table 3 . our model outperforms both the best and worst state of the art systems .
table 3 shows the performance of our system in terms of the number of roots , total terms , maxdepth and maxdepth . our system outperforms all the other systems that we consider . we observe that our system has the best performance on all metrics except for the maxdepth metric .
table 3 shows the performance of our system in terms of the number of roots , maxdepth and maxdepth . our system outperforms all the other systems except for the maxdepth metric , which shows that our system is more sensitive to depth cohesion than other systems . we also observe that the average depth of our model is higher than that of other systems , indicating that our model has better coverage .
in table 1 , we show the results of applying our principles on the validation set of visdial v1 . 0 . the enhanced version of our system outperforms the baseline model in terms of both qt and ncdg % ( table 1 ) . moreover , the enhanced model outperforms both the baseline and the enhanced version by a significant margin . this indicates that the enhanced system is more sensitive to the features of the question type and answer score sampling .
table 2 shows the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . adding p1 and p2 improves the performance of all the models except for coatt , which shows that adding p1 alone does not improve the performance .
table 5 shows the performance of our system on hard and soft alignments . the hmd - f1 + bert model outperforms all the other models except for wmd - unigram , which shows that it has better recall . also , it outperforms ruse and wmd - bigram , indicating that it is more sensitive to recall .
table 3 presents the results of our model on the direct assessment and sent - mover datasets . the results are shown in table 3 . our model outperforms all the baselines except bertscore - f1 by a significant margin . as shown in the table , the average score of our system is significantly higher than those of the previous state - of - the - art systems .
table 3 shows the performance of the bleu - 2 system with respect to bertscore - f1 and bert score . the results are shown in table 3 . we observe that bert scores are significantly lower than those of the previous state - of - the - art systems . further , bertscore scores are much lower than the previous best state of the art systems .
table 3 shows the performance of the word - mover and the sent - mover models with respect to m1 and m2 scores . the results are shown in table 3 . word - movement models outperform all the other models except for bertscore - recall , which shows that it is possible to improve upon the performance with the help of additional features .
table 3 shows the performance of our model on the sim and gm test sets . we observe that our model outperforms the previous state - of - the - art models on all metrics except acc and sim - p < 0 . 01 .
table 3 shows the results of our model on the transfer quality , transfer quality tie and fluency test set . the results are shown in table 3 . our model outperforms all the other models in all three test sets except for transfer quality . we observe that our model has better transfer quality and better fluency than all the others . it also outperforms most other models .
table 5 shows the results of human sentence - level validation of the metrics for each dataset for validation of acc and gm . the results are shown in table 5 . as can be seen , both the human and machine judgments that match the acc metric are high , indicating that the system is performing well . moreover , the human ratings of semantic preservation and fluency are high .
table 3 shows the performance of the models trained on the shen - 1 dataset . the results are shown in table 3 . we observe that the performance gap between the best and worst models is less pronounced when using the " para + para " and " 2d + lang " models , but it is still significant . as shown in the table , the difference in performance between the two sets of models is significant .
table 6 shows the results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . the best models ( fu - 1 ) and the best unsupervised models ( yang2018unsupervised ) achieve higher acc than prior work at similar levels of acc , but the untransferred sentences achieve the highest acc . we also show the results of our best model , which outperforms all the other models except for the multi - decoder , in terms of accuracy . the results are shown in table 6 . as shown in the table , our best models outperform all the others except the multidecoder .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . as can be seen in table 2 , nested disfluencies are more difficult to predict than those that are not , which indicates that the training set is not well - equipped to handle these types of problems .
table 3 shows the relative frequency of rephrases that are correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either in the same sentence , or in neither . as shown in table 3 , content - content and function - function tokens are the most common types of tokens in the disfluency , followed by repair tokens , which are the ones that are most frequently used in reparandums and repair - disfluencies .
the results are shown in table 3 . we observe that the best performance is obtained by using the best model with the best test set , followed by the best models with the worst test set . moreover , we observe that when using only text and innovations , the model performs better than both the best single and the best combination of innovations and text .
table 2 shows the performance of our model on the fnc - 1 test dataset . our model outperforms all the state - of - the - art algorithms except for self - attention , which shows that it is able to distinguish between topics that are unrelated and those that are not .
table 2 shows the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . as shown in table 2 , ac - gcn and ac - gnc outperform all the other methods except for the attentive neuraldater , which shows that the attention - based neuraldater has better performance .
table 3 shows the performance of our neuraldater model with and without graph attention . the results show that both word attention and graph attention are effective in improving the accuracy of our neuraldater system . moreover , both attention and word attention improve the model ' s overall performance .
as shown in table 3 , the performance gap between cnn and dmcnn is small , but it is still significant . the difference in performance between the two approaches can be seen in terms of the number of instances in which the model is trained on different stages of the network . in particular , the difference between the performance of cnn and jrnn is significant in the 1 / 1 and 1 / n stages , but not in the 2 / n stage , indicating that the model trained on all stages is better than the other two .
table 3 presents the results of our cross - event event detection system . the results are shown in table 3 . our system outperforms the best state - of - the - art event detection systems in terms of f1 score , f1 p and f1 r . we observe that our system is able to distinguish between events with different classifiers and those with the same classifiers , which indicates that the system is well - adapted to different event types .
the results are shown in table 3 . we observe that the best performance is obtained by using the best - performing variant of english - only - lm , followed by the best performing variant of spanish - only and french - only languages . the best performance obtained by all models is achieved by applying the best features of both languages in the same training set .
table 4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the fine - tuned model outperforms both the best - trained and worst - trained models on both sets of training data .
as shown in table 5 , fine - tuned - disc outperforms both the best - trained and worst - trained models on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) , and test cs vs . test mono .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . the results are shown in table 7 . the type combined approach shows that the recall and precision are significantly better than the baseline model , and the precision is higher than the type combined model .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset ( table 5 ) as well as the f1 score ( f1 ) of the type combined model . the type combined approach shows significant improvements over the baseline model in precision and recall . it also shows significant improvement in f1 scores , indicating that the model is more sensitive to the type of gaze features in question .
table 1 shows the results on belinkov2014exploring ’ s ppa test set . glove - extended embeddings outperform lstm - pp in terms of type and type - of - speech . the difference between the two systems can be seen in table 1 . in particular , the difference in the type of speech generated by the two approaches is significant . as shown in the table 1 , both the type and the number of tokens generated by hpcd outperform both the original and the second set of models . moreover , both systems outperform the first set of model .
results are shown in table 2 . the results show that hpcd and ontolstm - pp ( full ) outperform all the other approaches except for rbg , which shows that it is better at predicting pp attachments and oracle attachments .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . as can be seen , the ppa acc . scores are significantly higher when the context sensitivity is removed .
table 2 shows the results of adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun . the results are shown in table 2 . as can be seen , the domain tuning improves the bleu % score for both en - de and multi30k models , but it does not improve the performance for en - fr and mscoco17 models . adding subtitle data improves the performance of both models , as shown in the table 2 .
table 3 shows that domain - tuned h + ms - coco outperforms both en - de and en - fr models in terms of performance on all three datasets . the results are shown in table 3 . as can be seen in the table , domain - domain - tuning improves the performance of all three models , but it does not improve the performance on the en - fran and flickr datasets .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . adding only the best 5 captions improves the performance of our model by 3 . 7 points over the best en - de and mscoco17 models , and by 2 . 9 points over multi30k models , respectively .
table 5 shows the bleu % scores of the different strategies for integrating visual information ( see table 5 ) . all results using transformer , multi30k + ms - coco + subs3mlm , detectron mask surface , en - de and mscoco17 are shown in table 5 . as can be seen , the enc - gate + dec - gate approach outperforms all the other approaches except for en - fr , which shows that it is more effective at integrating the visual information . moreover , the performance gap between en - def and dec - gate is small , indicating that it can be used to improve the overall performance of the system .
the results are shown in table 3 . we observe that the multi - lingual approach outperforms all the other approaches except for ms - coco , which shows that it is more sensitive to visual features . moreover , the performance gap between en - fr and en - de models is much larger than that between mscoco17 and flickr17 , indicating that the visual features are more important than the linguistic ones .
table 3 shows the performance of the different approaches for yule ' s i and yule ’ s i . the results are shown in table 3 . as can be seen , the best performing approaches are en - fr - ht and en - es - ht , while the worst performing ones are those that use the " fr - smt - ff " embeddings . in addition , the performance gap between the best and worst performing approaches is small , but still significant .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the results are shown in table 1 . our model outperforms the previous state - of - the - art models in terms of parallel sentence splits .
table 2 shows the training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 . our model outperforms the previous state - of - the - art models in all three languages .
table 5 : automatic evaluation scores ( bleu and ter ) for the rev systems . the results are shown in table 5 . our system outperforms all the other systems except for en - fr - rnn - rev , which has higher bleu scores than all the others except en - es - trans - rev .
table 2 shows the results on flickr . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled rsaimage is the one trained on the rsaimage dataset . rsaimage outperforms both vgs and segmatch in terms of recall and recall percentage .
table 1 : results on synthetically spoken coco . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled rsaimage is the rsaimage - trained model . the results are shown in table 1 . as can be seen , the vgs model has significantly better recall than the other approaches . moreover , the recall is higher than that of rsaimage .
we report further examples in the appendix . in table 1 , we show that the rnn turns in a screenplay that is at the edges of the original on sst - 2 . in addition , the dan classifier turns in the screenplay of the same sentence as the cnn classifier , but turns on a different set of classifiers . the cnn classifiers turn in the same sentences as the original , but turn on different classifiers for different sentences .
table 2 shows that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . as shown in table 2 , rnn outperforms all the other models except for cnn , which shows that it is able to fine - tune more frequently than any other model . rnp outperforms both cnn and dan in terms of accuracy .
table 3 shows the results for positive and negative sentiment in sst - 2 . the numbers indicate the changes in percentage points with respect to the original sentence . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the results are shown in table 3 . as can be seen , the positive sentiment score increases as the number of negative labels is flipped from negative to positive .
table 1 presents the results of our experiments on pubmed and sst - 2 . the results are shown in table 1 . we observe that our approach outperforms all the other approaches except for pubmed , which shows that it is unable to distinguish between positive and negative responses .
