table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . the iterative and recursive approaches yield similar performance on the training and inference tasks , but the latter has the advantage of using more instances and hence more data .
table 1 shows the throughput for the treernn model using different tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t .
the max pooling strategy consistently performs better in all model variations . in ud v1 . 3 and conll08 , it achieves the best performance with a f1 score of 75 . 83 and 75 . 57 % respectively . with the sb model , the f1score is slightly better than the softplus score , but still significantly worse than the sigmoid baseline . as shown in table 2 , selecting the best multi - domain representation helps the model to learn more about the task at hand .
table 1 shows the effect of using the shortest dependency path on each relation type . our model significantly outperforms the best f1 model without sdp and with sdp , showing that the dependency path choice has a significant impact .
the results are shown in table 1 . we observe that the best performances are obtained on the recurlink + y - 3 dataset , with an absolute improvement of 3 . 57 % over the previous best state - of - the - art model on all metrics .
the results are shown in table 1 . our model outperforms all the stateof - the - art parsers except mst - parser . the difference is most striking in paragraph prediction , where our model achieves 100 % accuracy .
table 4 shows the c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . however , the difference is less pronounced for paragraph level , indicating that the lstm - parser parser has better generalization ability .
the results are shown in table 1 . the original tgen model outperforms tgen + and sc - lstm when the training set is completely cleaned . also , the results are slightly better than tgen − when trained and tested with the same set of features , indicating that tgen is more than able to handle the additional training data . when the training data is cleaned , the improvement is less pronounced , but still significant .
table 1 shows the statistics for the original e2e dataset and the cleaned version . the difference in number of distinct mrs and total number of textual references is significant ( 17 . 5 % vs . 11 . 69 % ) , with the difference being less pronounced for the train dataset , which shows that the training data is more interpretable .
the results are shown in table 1 . the results show that the original tgen model outperforms tgen + and sc - lstm when trained and tested with the correct set of features . however , when trained with the wrong feature set , the results are slightly worse . adding features to the training set improves the bleu and nist scores , but does not improve the meteor scores .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . as can be seen , there are a significant number of errors ( around 22 % ) that can be attributed to missing instances , as well as slight disfluencies ( around 5 % ) caused by the addition of missing values .
table 3 shows the performance of our model with respect to entity clustering and multi - domain clustering . our approach outperforms all the stateof - the - art models that do not use tree2str and pbmt . it also outperforms the best previous approaches with a gap of 3 . 9 points from the last published results .
table 2 shows the performance of our model with respect to amr17 . our model achieves 24 . 5 bleu points , which is slightly better than the previous state - of - the - art seq2seq model by a margin of 2 . 6 points . gcnseq ( damonte and cohen , 2019a ) achieves a marginal improvement of 1 . 5 points over the previous best model , but still achieves a significant performance gain of 3 . 6 bleu points . table 2 also shows that our model is comparable in terms of performance with the best ensemble model , confirming the viability of our proposed method .
table 1 presents the results for english - german , czech and slovakian , compared to english - czech . our proposed method outperforms the previous state - of - the - art models in all three languages . the results show that our proposed method is comparable to the best previous approaches in terms of performance on both languages .
table 5 shows that the number of layers inside a network is the most important factor in the success of our model , with a drop of 3 . 5 % in performance compared to previous models .
table 6 shows that the rcn with residual connections outperforms the gcn with rc + la ( 18 . 6 % vs . 21 . 8 % ) and the + rc + la baseline on all gcns except for those that have residual connections ( 24 . 2 % vs 25 . 9 % ) .
the results are shown in table 4 . the most striking thing about our model is that it achieves state - of - the - art results in terms of generalization without sacrificing performance on specific gcn tasks . it closely matches the performance of the best previous models with only 0 . 2 % absolute difference .
table 8 shows the ablation study results for the dev set of amr15 . our model ( dcgcn4 ) shows that removing the dense connections in the i - th block significantly decreases the number of connections , and consequently , the overall performance of the model decreases .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . the results show that , under the best case scenario , the domain - aware decoding model ( dcgcn4 ) obtains the best performance with a gap of 3 . 5x in coverage from the previous state - of - the - art model .
table 7 shows the performance of our initialization strategies for different probing tasks . our proposed method outperforms glorot and topconst in terms of all metrics except tense , indicating that it is more suitable for production use .
we observe that the h - cmow variant outperforms the cbow variant in terms of concatenation and tense , and that it achieves state - of - the - art generalization on a single - domain task .
the results are shown in table 1 . hybrid models outperform monolingual models and outperform cbow / 784 in terms of all three sub - criteria . the best results are obtained by combining the best performing sub - categories , with the exception of sst2 , which is under - represented in the table . ame model outperforms both cbow and cmow , showing that ame has the advantage of training on a larger corpus .
table 3 shows the relative improvements on unsupervised downstream tasks that our models have achieved since we switched from monolingual to multi - headed attention . cmow shows a significant performance gain of 26 . 5 % over the cbow baseline , while hybrid achieves a gain of 44 . 6 % . with respect to sts13 and sts16 , the gains are less pronounced , but still represent a significant improvement of 4 . 6 % and 7 . 0 % over cbow , respectively .
table 8 shows the performance of our system with respect to initialization and supervised downstream tasks . glorot outperforms all the stateof - the - art systems with a gap of 3 . 6 points from the last published results .
table 6 shows the scores for different training objectives on the unsupervised downstream tasks . our proposed cbow - r method outperforms the cmow - based method in terms of all the three major tasks .
the results are shown in table 1 . we observe that the cbow - r classification scheme outperforms the cmow - c classification scheme in terms of all metrics , and that it achieves state - of - the - art results on both subtasks .
the results are shown in table 1 . we see that the best performing method is the cbow - r classification scheme , which achieves 90 . 6 % improvement over the previous state of the art cmow - c model .
the results of τmil - nd are shown in table 1 . in general terms , our model outperforms the previous state - of - the - art models in terms of all metrics , with the exception of name matching , where it performs slightly worse than the best previous model . supervised learning outperforms all the other models except for mil - nd ,
table 2 shows the results on the test set under two settings . the best results are obtained by τmil - nd ( model 2 ) with a f1 score of 73 . 57 % , while the best performance by supervised learning model ( model 1 ) is 71 . 38 % . the results are slightly worse under the automatic learning setting , but still show that the model can learn the task to a high degree even under the difficult task of name matching . further improving performance by high margins
table 6 shows that the g2s - gat model is comparable to the best previous state - of - the - art models in terms of both generalization and performance . however , it is significantly worse than the gggnn model in generalization . note that the difference is mostly due to the high performance of the gat - gin variant .
table 3 presents the results of our experiments on the ldc2015e86 , ldc2017t10 , and the full set of table 3 . our model outperforms the previous state - of - the - art models on all metrics except meteor by a noticeable margin . the g2s model by a margin of 3 . 42 ± 0 . 53 points on bleu metric and 3 . 55 ± 0 . 55 points on meteors metric . additionally , we see a significant performance drop on relis and s2s metric from the previous experiments , with the latter performing worse than the former .
table 3 shows the results on the ldc2015e86 test set when models are trained with additional gigaword data . our g2s - ggnn model outperforms the previous state - of - the - art models with a gap of 10 . 60 points from the last published results .
table 4 shows the results of the ablation study on the ldc2017t10 development set . our model significantly outperforms the previous state - of - the - art models in terms of bleu , meteor and size .
we observe that g2s - ggnn model has the best overall performance , with an absolute improvement of 3 . 51 % over the previous state - of - the - art model in terms of all metrics .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( added ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . the model with the highest fraction of missing elements is s2s , while g2s - gat has the smallest fraction ( gin ) .
table 4 shows the pos and sem tagging accuracy using different target languages on a smaller parallel corpus ( 200k sentences ) . the pos tagging accuracy is slightly better than sem , however it is still significantly worse than sem .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . word2tag significantly outperforms unsupemb in terms of most frequent tags and pos as well as sem .
table 4 presents the system ' s performance on the four metrics for pos , epm , tagging accuracy , semtagging accuracy and freelists . our model outperforms the previous state - of - the - art models on all metrics except for the pos tagging accuracy where it performs slightly better .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . our proposed res - based encoder outperforms the bi - based uni encoder with a gap of 3 . 5 points from last published results .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . the difference between the attacker score and the corresponding adversary is significant , indicating that the attacker is more likely to make mistakes .
table 1 shows the performance of our model with respect to training directly towards a single task . our model outperforms all the state - of - the - art models with a large margin .
table 2 shows the results for balanced and unbalanced data splits . our model outperforms the previous state - of - the - art models in terms of both task and named attribute leakage .
as shown in table 3 , there is a significant difference in the performance between the best performing and worst performing tasks with an adversarial training set . δ is the difference between the attacker score and the corresponding adversary ’ s accuracy . in our particular case , this is due to the high number of instances where the named entity has a missing attribute or has leaked information .
as shown in table 6 , the rnn encoders are more accurate at decoding the leaky and the guarded attributes than the ones that are already protected .
table 3 presents the results of our final model with respect to finetune and tuning . our model outperforms the previous state - of - the - art models in terms of all metrics with a gap of 3 . 36 points from the last published results ( ptb + finetune ) . the difference is most prevalent in relation to finetune tuning , where our model obtains an absolute improvement of 2 . 97 points over previous work .
table 3 presents the results of our final model evaluation on the lstm test set in the real - world setting . the results are presented in table 3 . our model significantly outperforms the previous state - of - the - art models in terms of all metrics , with the exception of base acc where it is slightly worse than the other two .
table 3 presents the results of our final model on the three domains . our proposed method outperforms the previous state - of - the - art models on all metrics except for yelppolar err . on the other hand , it performs slightly better on amapolar and yahoo time .
table 3 shows the case - insensitive tokenized bleu score of our model on the wmt14 english - german translation task . our model significantly outperforms all the base the difference is most striking when we consider the number of tokens required to encode one sentence , which shows that our model is more than able to handle the task in hand . additionally , the difference in decoding time between the training and the decode time is less striking , indicating that the model has better generalization ability .
table 4 shows the exact match / f1 - score on squad dataset . the best model is rnet * with a f1 score of 76 . 14 / [ bold ] 83 . 83 % and the worst model is lrn ( 71 . 45 / 83 . 83 % ) . table 4 also shows the results of adding elmo parameter to the base model to improve the model ' s performance . with the addition of elmo , the model improves from 71 . 41 / [ 79 . 41 % to 75 . 83 / [ 76 . 83 % ) and from 69 . 59 / 71 . 67 % to 69 . 67 / 71 % . the other models that do not have elmo as a parameter are sru , lrn , atr , gru and lstm ( table 4 ) .
table 6 shows the f1 score of our model on the conll - 2003 english ner task . our lstm model significantly outperforms the previous state - of - the - art models with a gap of 9 . 56 points from the last published results . the difference is most prevalent in relation to parameter number , where our model obtains 245 . 94 points more than the previous best performer .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . with the base setting , our model significantly outperforms the previous state - of - the - art models in both tasks .
table 1 shows the system performance on word analogy task . our system outperforms all the stateof - the - art systems on all metrics except sentence recognition . on average , our system is more than 4 . 5x faster than the best previous systems . on the other hand , our oracle system is nearly 4x faster .
table 4 presents the results of human evaluation on the selected automatic systems . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that the system performing best is likely to have the best generalization ability . the second best result is achieved by candela ( 30 . 2 % ) on the grammatical evaluation , followed by retrieval ( 50 . 6 % ) . the third best result by seq2seq ( 25 . 6 % ) shows that the performance gain comes from a better model design .
the results are shown in table 1 . we can see that the performance gap between en and europarl is relatively small , with p < 0 . 01 and r > 0 . 05 indicating that the former is better than the latter .
the results are shown in table 1 . we can see that the performance gap between en and europarl is relatively small , with p < 0 . 01 indicating that the former has better generalization ability . however , the gap is much larger with respect to ted talks .
the results are shown in table 1 . we can see that the performance gap between en and europarl is relatively small , with p < 0 . 01 indicating that the latter has better generalization ability .
as shown in table 1 , the average depth and the number of roots per row are the most important metrics for brevity . for europarl , average depth is 11 . 05 % lower than maxdepth , which means that more than half of the trees in question have less than 1 . 5 % depth .
as shown in table 1 , the average depth and the number of roots are the most important metrics for brevity . europarl has the worst performance in terms of both metrics , with an average of 9 . 43 % and 4 . 29 % lower than df .
table 1 shows the performance ( ndcg % ) and the enhanced version of our lf model compared to the baseline model in the experiments of applying our principles on the validation set of visdial v1 . 0 . the enhanced version shows that it is comparable to the original model in terms of both performance on the question type and answer score sampling , but is slightly worse on the ranking loss metric .
table 2 shows the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . the best performing model is p2 + p1 , which shows that p1 + p2 is the most effective one . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut .
table 5 shows that hmd - f1 and hmd - recall perform similarly on hard and soft alignments , while wmd - unigram and wmd - bigram do not . the hmd pre - trained models do not perform as well on hard alignments as does ruse , indicating that the bert - trained hmd models are more suitable for this task .
the results are shown in table 1 . the average score of all the baselines is slightly better than the bertscore - f1 baseline with the exception of ruse ( * ) where it is slightly worse . sent - mover achieves the best results with a weighted average of 0 . 719 .
the results are shown in table 1 . the first set of results shows that bertscore - f1 is relatively consistent with the bleu - 1 baseline , while the second set shows significant performance drop . sent - mover and w2v achieve the best results among all the baselines , with the exception of meteor .
word - mover achieved the best results with an f1 score of 0 . 939 on the m1 metric and a f1score of 1 . 087 on m2 metric . sent - movers obtained at the 90 % level outperform all the baseline models except meteor .
the results are shown in table 1 . the results show that the more parsergic approach leads to better generalization , with an absolute improvement of 1 . 81 points over the baseline model in all metrics .
table 3 shows the transfer quality , transfer quality and semantic preservation metrics for yelp and google translate . the results are statistically significant with respect to all metrics , with the exception of transfer quality where we see a drop of 0 . 9 % in δtransfer quality . this suggests that the model with the worst transfer quality may not be able to learn the best conversational features .
table 5 shows the human evaluation results for each metric . our approach verifies that the summaries generated by the machine and human are comparable in terms of syntactic and semantic recall . it also verifies the accuracy of the judgments that match the human judgments .
the results are shown in table 1 . the results of m1 and m2 show that the model with the best generalization performance is the one with the least variation in terms of error .
results on yelp sentiment transfer are shown in table 6 . our best models ( right table ) achieve higher bleu than prior work at similar levels of acc , but untransferred sentences achieve the highest accu . additionally , our model ( fu - 1 ) achieves higher acc than the best previous model ( yang2018unsupervised ) at the same level of transfer , indicating that the transfer quality of our model is high even under the difficult requirement of 1000 transferred sentences .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent in the nested disfluencies when we only included repetition tokens . additionally , we include the number of repetition tokens as well as the overall number of disfluency tokens in our model to account for the fact that the repetition tokens are rare in a single sentence .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) . similarly , the frequency of tokens containing a function word in the repair is slightly higher than those without . these tokens belong to the " disfluencies " category , which we define as those that are broken up into phrases with two or more parts .
the results are shown in table 1 . we observe that the best performing models are those that use the best combination of text + raw and innovations , with the exception of the " early " model ( which relies on single - domain embeddings ) . the " late " model , on the other hand , shows that incorporating all the features from the previous models helps the model to perform better .
performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . our model shows that it is comparable to the state of the art in terms of generalization , with the exception of the accuracy in discuss case , where our model is slightly better . additionally , our model shows a slight performance improvement over the previous state of - the - art cnn - based model on the discuss case .
table 2 shows the performance of the different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . neuraldater is the only one that performs significantly worse than the other methods . attention - aware neuraldater is comparable to the best performing oe - gcn model , while maxent - joint is slightly better than the attentive neuraldater . the best performing ad3 model is slightly higher than the previous best performing model , but still performs significantly better than any other method .
table 3 shows the performance of our method with and without word attention . our neuraldater model shows that it is comparable in terms of accuracy to the best performing oe - gcn model . however , the difference is less pronounced with respect to graph attention .
the results are shown in table 1 . the best performances are obtained by the jmee model , which achieves 75 . 3 % overall improvement over the previous state - of - the - art dmcnn model . however , the difference is less pronounced for cnn and jrnn , showing that these models are better at selecting the correct argument and answering questions from multiple states .
table 1 presents the results on event detection and event classification . our proposed method significantly outperforms state - of - the - art methods in terms of both event and event detection . the results show that our proposed method is comparable to state of the - art on both event - specific and cross - event detection . on the other hand , it performs significantly worse on event classification and classification .
for english - only and spanish - only learners , fine - tuned multi - task learning models outperform all the other models apart from the case of the case where fine - tuned - lm is used . the results are shown in table 1 . as the table shows , the best performances are obtained by the models using lexical encoder with the best recall . further , the results are slightly worse than those using lexically - shuffled - lm , indicating that the lexical features of the language chosen by the learner are important for the model to succeed .
results on the dev set and on the test set using only subsets of the code - switched data are shown in table 4 . the fine - tuned model outperforms the original cs - only model with a margin of 3 . 8 % on the training set and a gap of 4 . 5 % on test set .
as shown in table 5 , fine - tuning the decoding algorithm improves the performance on the dev set and on the test set , and upsampling has a generally positive effect ( p < 0 . 05 ) . fine - tuned - disc also improves performance over the monolingual approach , but only marginally .
table 7 shows the precision ( p ) and recall ( r ) numbers for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvement in precision is statistically significant ( p < 0 . 01 ) and f1 - score ( f1 ) shows that type combined gaze features are superior to the baseline model in terms of precision and recall . also , the f1 score shows a significant improvement ( p > 0 . 05 ) compared to baseline for the type combined model .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . the improvements are statistically significant ( p < 0 . 05 ) with respect to the baseline and the type - combined baseline , which shows that the ability to select the correct gaze features with precision is a significant advantage .
results on belinkov2014exploring ’ s test set are shown in table 1 . glove - extended refers to the synset embeddings obtained by running autoextend rothe and schütze ( 2015 ) on wordnet 3 . 1 and wordnet 4 . 0 , while ontolstm - pp refers to word - extend encodings obtained using the best performing hpcd model . the results are slightly worse than those on the original paper ( farrequi et al . , 2015 ) , indicating that the additional decoding effort required to extend wordnet is not worth the effort .
results shown in table 2 show that the hpcd dependency parser is comparable to the best ontolstm - pp model , while the performance gap is narrower with rbg . further , the performance gain is larger with respect to the full uas as compared to the lstm model .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . our model achieves the best performance with a ppa acc . of 89 . 7 % on the full test set and 89 . 5 % with respect to the attention metric .
as shown in table 2 , domain tuning improves the bleu % score for en - de and multi30k models , but does not improve the performance for flickr17 or mscoco17 . adding subtitle data and domain tuning also improves performance for both models , however , the improvement is less pronounced for the former .
the results of domain - tuned h + ms - coco are shown in table 1 . we find that the model with the best overall results is the one with the most pronounced drop in performance over the two domains .
table 4 shows the bleu scores of en - de , en - fr and mscoco17 models when adding automatic image captions ( only the best one or all 5 ) . the table shows that the combination of the best 5 captions gives a noticeable boost in performance , and that the best multi - attention captions give a significant boost as well .
table 5 shows the bleu % and multi30k + ms - coco + subs3mlm scores for en - de , en - fr , mscoco17 , and multi - domain multi - task learning models . the encoder and decoder use different gateways to encode and decode visual information , and their performance is strongly impacted by the type of encoder used . for example , encoder + dec - gate ( 68 . 38 % ) achieves the best performance , while encoder + dec - gate achieves the worst performance ( 45 . 40 % ) .
we observe that the multi - lingual approach outperforms the monolingual approach , but does not exceed the performance of subs3m and subs6m in terms of generalization . moreover , the results are slightly worse than those of en - de and mscoco17 when we switch from the word " visual features " to " language features " .
table 1 shows that en - fr - ht and en - es - ht are comparable in terms of ttr , mtld and word embeddings . however , the performance gap between the two is much larger when we switch from mtld to ttr and yule ’ s i .
table 1 shows the number of parallel sentences in the training , test and development splits for the language pairs we used . our model outperforms the previous state - of - the - art en model by a significant margin .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the results are statistically significant ( p < 0 . 05 ) with respect to the src and trg measures , which shows that the models have comparable performance in both languages .
table 5 shows the bleu and ter evaluation scores for the rev systems . the en - fr - rnn - rev and en - es - trans - rev embeddings show that their automatic evaluation scores are comparable to those of the best state - of - the - art systems ( i . e . , en - e . g . , gurevych et al . , 2013b ) . however , the performance gap is narrower between the two sets when en - rev is used as the reference , showing that the performance gain comes from a better model design .
table 2 shows the test set ' s performance on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the one supervised with the generated embeddings . our model outperforms both the supervised and unsupervised models with a large margin .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled u is the audiovisual model from the previous work ( chan et al . , 2017 ) . the acoustic embeddings based on audio2vec - u outperform all the other approaches except segmatch .
table 1 shows the comparison of the different classifiers for the original and the new embeddings on sst - 2 . we report further examples in the appendix . the dan classifier turns in a screenplay that is slightly off - kilter at the edges , but is otherwise correct . cnn classifiers are similarly accurate , but do not handle edges as well . rnn is able to distinguish between edges and curves , but cannot distinguish between the two .
table 2 shows the percentage of occurrences that have increased , decreased or stayed the same as a result of fine - tuning in sst - 2 . the last row indicates the overlap with the original sentence , while the others indicate that the number of occurrences has increased . the numbers indicate the changes in percentage points with respect to the word " speech " .
sentiment score changes in sst - 2 . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . in this case , the positive sentiment score increases by 9 . 3 % and the negative sentiment score by 27 . 3 % .
table 1 presents the results of our experiments on the pubmed and sst - 2 datasets . our proposed method outperforms the best previous approaches on both datasets with a margin of 98 . 5 % and 99 . 5 % .
