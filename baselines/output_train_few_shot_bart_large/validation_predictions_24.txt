table 2 shows the throughput and training time for our treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . table 2 compares the performance of our recursive and iterative approaches with the best - performing model configurations .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization .
the results in table 2 show that the max pooling strategy consistently performs better in all models with different representation configurations . specifically , ud v1 . 3 achieves the best f1 score with a f1 of 1 . 83 and a dropout probability of 9 . 57 % compared to the sigmoid model ( 7 . 66e - 03 ) . we find that the softplus representation also performs better than softplus , indicating that softplus is a better choice for this task .
table 1 shows that using the shortest dependency path on each relation type improves the f1 score over the best baseline model without sdp . further , when using sdp as dependency path , the model - feature model improves significantly over the macro - averaged model .
consistent with the observations by vaswani et al . ( 2017 ) , we observe that the three types of models perform comparably to each other when the true response is added . however , in the more realistic second case , when using only 50 % f1 score on the training data , the results are markedly worse . note that the r - f1 score of all models only applies to f1 50 % of the time , not even 50 % on average . the results are slightly better when using y - 3 : y , which shows marked improvement in performance over the baseline model .
the results of paragraph prediction on the test set are presented in table 1 . we report the results for paragraph prediction at the level of paragraph level acc . and f1 . from left to right , we have paragraph level acc . , f1 score ( from 0 . 03 - 2 . 59 ) and average number of frames predicted per paragraph ( from 1 . 59 - 6 . 69 ) . the results are statistically significant with respect to both accuracy and precision , with mst - parser achieving 100 % accuracy on average and 50 % on average .
table 4 shows the overall performances of our system compared to the best previous state - of - the - art parser lstm - parser on the essay and paragraph level . the difference in c - f1 score between the two systems is less pronounced for the paragraph level , but still suggests some advantage to using paragraph - based parsing .
the results are shown in table 1 . the first group shows that the original tgen model significantly outperforms the cleaned model and the sc - lstm model when it is trained and tested on the new dataset . next , we compare against the results of tgen + and tgen − . in both cases , the results show that the cleaner model performs better on the training data and the test set .
table 1 shows that our cleaned version of the e2e dataset is comparable in many respects to the original one , with the exception of the number of textual references . the difference in ser statistics between the original and the cleaned version is less pronounced , but still significant ( 17 . 5 % vs . 17 . 69 % ) on average .
table 1 shows that the original tgen model outperforms the original and the best - performing add - on model , sc - lstm , when trained and tested on the multi - news dataset . the results are slightly worse than those of tgen − but still superior to tgen + on bleu , nist and meteor ( see x4 ) . as can be seen in the results presented in table 1 , the difference between the two approaches is less pronounced for non - g & l models , with respect to training , the results are less clear , but still indicate that tgen models are more effective for production use .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . as expected , the majority of errors in our system ( 71 % ) are caused by errors caused by adding training data ( 23 % ) , which can be seen in table 4 . however , a few instances ( 14 % ) are slightly disfluencies ( 7 % ) which indicate that the system is able to handle the task well .
the results are shown in table 1 . our model achieves the best results with an all score of 28 . 2 % compared to the previous state - of - the - art .
table 2 shows the performance of our model on amr17 . our ensemble model achieves 24 . 5 bleu points , which marginally outperforms the previous state - of - the - art on par with seq2seqb and ggnn2seq models .
table 1 shows that the best performances for english - german and czech are achieved by our model , which verifies the effectiveness of our model . we also observe that our model outperforms the previous state - of - the - art on both languages .
table 5 shows that the number of layers inside our model is the most important factor in the success of our model . we find that , with a minimum of 6 layers , we can improve the bias metric by 9 % in the standard task formulation and to parity in the real - world test set .
table 6 shows that the rcn with residual connections outperforms the baselines and the gcn with no residual connections . gcns with rc and la connections perform better than those without . moreover , the results are slightly worse than those with rc + la connections , indicating that the model can rely on residual connections to improve performance .
the results are shown in table 4 . our model ( dcgcn ) outperforms all the stateof - the - art models on both datasets with two tasks . it improves upon the strong baselines by 4 . 4 points in the accuracy metric and achieves 4 . 2 % overall improvement over the best previous state of the art model .
table 8 shows the ablation study results for the density of connections on the dev set of amr15 . our model obtains 25 . 2 % higher density compared to the previous state - of - the - art model dcgcn4 .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . the results of " - linear combination " and " - direction aggregation " models show that the global node and the hierarchical clustering of the nodes can improve the results , but only when combined with domain - aware attention . also , the results show that - coverage mechanism reduces performance marginally , but helps the model to improve significantly over the plain linear encoder .
table 7 shows the performance of our initialization strategies for different probing tasks . our paper establishes that our framework establishes a new state - of - the - art on all three high - level tasks , and on all subtasks except concatenation . it improves upon the strong baselines by 4 . 8 points in the bshift metric and to parity in the length metric .
the results are shown in table 1 . we observe that the h - cmow model outperforms both the cbow and the wc models on every metric by a significant margin . in fact , it achieves state - of - the - art results on three of the four metrics , outperforming all the alternatives except coordinv by a noticeable margin .
the results are shown in table 1 . the first group shows that our method outperforms all the base the second group of results show that our cmp model significantly improves over the best previous state - of - the - art model . our model improves upon the results of both the sst2 and sst5 models by 3 - 4 points .
table 3 shows the performance of our models on the four downstream tasks as well as the improvements in overall performance over the best previous state - of - the - art method . hybrid mode outperforms both cbow and cmow , showing that it is better at selecting the relevant features and its output is more interpretable . with respect to sts13 and sts16 , the results show that cbow has the advantage of training on a larger corpus , since it has completed all the training tasks on the smaller scale . though the improvement over hybrid is slim , it is significant , and we note that it should not be dismissed as a result of lower performance on these tasks alone .
table 8 shows the performance of our system for initialization and supervised downstream tasks . our system establishes a new state - of - the - art on all three high - level tasks , and on all subtasks except mpqa . it improves upon the strong glorot baseline by 4 . 6 points in the three stages .
table 6 shows that our approach outperforms the best previous approaches across the four downstream tasks . our cbow - r model improves upon the strong baselines by 3 . 8 points in the unsupervised tasks . it achieves the best performances on three of the four tasks and achieves the highest average score on the fourth task .
the results are shown in table 1 . we observe that our method obtains the best generalization performance . it improves upon the strong baselines across the board , and outperforms all the alternatives except subjnum by a noticeable margin .
the results are shown in table 1 . the first group shows that cbow outperforms all the mod table 1 shows that the hierarchical clustering method cbow achieves the best results . it improves upon the strong monolingual model of sst2 and sst5 by 4 - 8 points in the sub - score .
the results are shown in table 1 . in all configurations , the supervised learning approach outperforms all the alternatives except name matching . however , it does not exceed the performance of the best previous model , which shows the advantage of finetuning word embeddings during training . name matching and entity prediction are the most difficult tasks to solve , both in terms of loc and misc . we find that our model , τmil - nd , can do both with ease . moreover , it improves upon the previous model with a noticeable margin .
uncertain in low - supervision settings . in table 2 we report the results of model 1 and model 2 under two settings , with different feature sets trained on the test set . the results are shown in table 2 . name matching and named entity recognition achieve high f1 scores , but cannot exceed the level of intuition in terms of e - 1 score , indicating the difficulty of the task . supervised learning , on the other hand , gives a significant improvement in f1 score over both the baseline model and the best performing model - adapted model ( tmtmil - nd ) . it also improves upon the name matching and entity recognition performance .
table 6 shows that the model with the best performance is the g2s - gat model . it considerably outperforms the models with the worse performance in terms of both ref and gen .
we compare our model against previous state - of - the - art on the ldc2015e86 and ldc2017t10 datasets . the results , summarized in table 1 , are broken down in terms of performance on bleu metric , meteor metric and s2s metric . our model improves upon the previous state of the art on all metrics by 3 - 4 points on average . note that g2s - gat has the worse performance on both datasets with a gap of 2 - 3 points from the last published results . on the other hand , our model improves on three of the four datasets by 4 - 6 points .
table 3 shows the performance of our model on the ldc2015e86 test set when models are trained with additional gigaword data . our g2s - ggnn model improves over the previous state - of - the - art on the test set , and outperforms all the previous models by a large margin .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model ( get + bilstm ) obtains a significant improvement in performance over the previous state - of - the - art model on three out of the four scenarios ( table 4 ) by a factor of 3 . 7m .
we also evaluated the models in terms of sentence length and average number of frames , comparing against the baseline models . the results are shown in table 1 . as can be seen , the smaller graph diameter and average sentence length seem to indicate , but the larger difference in performance between s2s and g2s - gin indicates that the former has superior generalization ability . with respect to sentence length , between the two baseline models , gat has the advantage of training on a larger corpus , with an average of 260 sentences per model , which explains the significant performance drop in sentence length .
table 8 shows that gold significantly outperforms the s2s model and the g2s - gat model in the fraction of elements in the output that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . as shown in table 8 , the smaller size of the output indicates that gold has better generalization ability .
table 4 shows the pos and sem tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . as expected , the pos features significantly improve over the sem feature , showing that the pos tagging accuracy is high even under the difficult requirement of a low false positive rate .
table 2 shows pos and sem tagging accuracy with baselines and an upper bound . word2tag embeddings significantly outperform the best previous approaches ( pos and sem ) in both languages . the difference is most prevalent in pos , the most frequent tag , which shows the advantage of using unsupervised word embeddings . it also outperforms unsupemb in semantic tagging accuracy .
table 1 shows the pos tagging accuracy and sem tagging accuracy on the test set of hotpotqa in the distractor and fullwiki setting , respectively . for pos , we report precision @ k = 0 , 1 , 2 , 3 , 4 and 7 . the results show that on average , our model performs better than all the other methods apart from the case of the last group where it has the advantage of training on more data .
table 5 shows pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . our model achieves high precision in both languages , with the exception of english , where it gets only 87 % accuracy . we find that the transfer learning method from one layer to the next improves the model ' s general performance .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . as shown in table 8 , the gender and race features seem to be the most difficult for the attacker to pick out , followed by age and sentiment . finally , the dial and sentiment features are the only ones that the adversary can pick out with high accuracy .
table 1 shows the performance of our system with respect to training directly towards a single task . our model establishes a new state - of - the - art on all metrics with a gap of 10 . 8 % in accuracy from the previous state of the art . gender - parity and age - based bias result in significantly worse performance , with gender - based biases leading to an overall drop of 15 % in performance .
table 2 shows the results for balanced & unbalanced data splits . our model outperforms all the baselines across all three attributes with a large margin . gender and race features cause the greatest imbalance in data leakage , followed by age and sentiment . the task accuracy is relatively balanced , but is unbalanced under gender - parity , indicating that there is a need to design more complicated models to prevent this from happening .
as shown in table 3 , an adversarial training set can reduce the error of the task prediction when training with a new classifier . as expected , gender and race features have the highest performance , followed by age and task prediction accuracy .
the results in table 6 show that the guarded encoders perform better than the original embeddings . the difference in performance between guarded and unencoders is less pronounced for rnn , but still significant .
the results are shown in table 1 . the first group of results show that adapting the finetune scheme to the task at hand , improves the results for both wt2 and ptb . the model achieves state - of - the - art results , outperforming both the original model and the best performing variant , lrn , by a margin of 3 . 59 points . further , the model achieves competitive or better results than the best - performing variant , lstm , by 3 . 97 points on average .
the results are shown in table 2 . we report the acc time and the average number of iterations for each parameter according to rocktäschel et al . ( 2016 ) . the results reconfirm that the lstm model is well - equipped to perform this task in the low - resource settings . table 2 also compares our model with previous state - of - the - art models on both acc and time metrics . as shown in the table , gru has 6 . 41m acc and 8 . 43m time to compute the base sentences , while sru has 2 . 87m and 7 . 45m acc . the difference in time between the two models is minimal , however we see significant in terms of acc time due to the large variation in training set size .
table 1 shows the test set for amapolar , yahoo time and yelppolar time on the training set of hotpotqa in the distractor and fullwiki setting , and zhang et al . ( 2015 ) . the results are presented in table 1 . in all but one case , this model obtains better results than the previous state - of - the - art on all metrics . for example , on average , it achieves 4 . 42 / 10 . 00 amafull err and 7 . 63 / 25 . 55 yelppol time , respectively , with an absolute improvement of 2 . 36 / 4 . 55 and 6 . 63 / 4 . 55 points over the previous best state of the art .
table 3 shows the case - insensitive tokenized bleu score of our model on the translation task of wmt14 english - german translation task . our model obtains a 27 . 19 % bleu score improvement over the previous state - of - the - art model on average .
table 4 shows the exact match / f1 - score on the squad dataset of wang et al . ( 2017 ) . it can be seen that the alternative approaches that aim to improve the interpretability of input - specific neural models , do not perform well in the low - supervision settings . lrn , on the other hand , obtains high f1 scores and achieves state - of - the - art results . we note particularly that the model with the most parameter number of elmo , rnet * ( 71 . 67 % ) , obtains the best match rate with 79 % f1 score . the other alternative models that perform in the same range as lrn but with less elmo as parameter number are glove , lstm and sru .
table 6 shows the f1 score of our model ( lstm * ) on the conll - 2003 english ner task . the model obtains a significant improvement in performance over the previous state - of - the - art on three out of the four parameter numbers . it closely matches the performance of lrn and atr with only 1 . 56 points difference . although lrn has the advantage of training on a larger corpus , it can still outperform the other two models with a large margin .
table 7 shows that our model improves the performance on the snli task with the use of base + ln setting and test perplexity on ptb task with base setting . the difference in test accuracy between snli and ptb is minimal , however we see that our glrn model significantly improves over the other lrn model .
table 2 presents the results of system and word prediction using the best performing feature set . our system dkrn outperforms all the state - of - the - art systems in terms of both r - 2 and mtr scores . word prediction using our system achieves the best results with an f1 - score of 21 . 08 on the b - 2 metric and 35 . 64 on the mtr metric . retrieving sentences from the database with a minimum of 66 words gives a 4 . 28 % f1 score improvement over the previous state of the art .
table 4 presents the results of human evaluation . our system outperforms all the other systems with a large margin . among all the systems , it achieves the highest average score on grammatical accuracy ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 8 points , which indicates that our system is well - equipped to perform this task .
the results are shown in table 1 . we report p and r scores for en and pt on the test set of hotpotqa using the best performing feature set . for both datasets , our model outperforms the previous state - of - the - art on every metric by a significant margin .
the results are shown in table 1 . we report p and r on the test set of en and europarl compared to those of ted talks . according to the table , en outperforms both ted talks and its variants on every metric by a significant margin .
the results are shown in table 1 . as can be seen , the smaller performance gap between en and pt on the test set " ted talks " and " europarl " indicates that the former has better generalization ability . on the other hand , for " docsub " the gap is much larger . table 1 shows that for both datasets , training on the europarl dataset results in significantly better performance . finally , the results are slightly less clear on " p " metric , indicating that the training set developed on the ted talks dataset is more suitable for production use .
from table 1 , we can see that all metrics we consider have low correlation with the human judgement . among all metrics , the average depth of the roots is 11 . 05 , which means that more than half of the trees in the dataset have more than one root . europarl , on the other hand , has 43 roots .
from table 1 , we report the five metrics for brevity . our system outperforms all the base the first group shows that it has the best generalization ability . among all metrics , total terms and roots are the most consistent , with a gap of 9 . 29 % from the last published results .
as shown in table 1 , the enhanced version of our model ( lf ) outperforms the baseline model by a significant margin . it achieves 73 . 42 % ndcg % compared to 62 . 63 % on the validation set of visdial v1 . 0 . the difference is most prevalent in question type , answer score sampling , and hidden dictionary learning , respectively .
as shown in table 2 , applying p2 improves the performance of all the models . the model with the best performance is the one using the history shortcut . it achieves 73 . 44 % ndcg % on the visdial v1 . 0 validation set . the difference between baseline and p2 indicates that p2 is the most effective one .
table 5 shows that the hmd - f1 model significantly outperforms the other approaches on both soft alignments and hard alignments . the results are broken down in terms of performance on cs - en , fi - en and lv - en . the hmd models perform similarly on both sets , with the exception of the case of ruse .
the results are shown in table 1 . first , we report the average score of bert score on the direct assessment metric and on the sent - mover metric . from left to right , we can see that meteor + + , ruse ( * ) and w2v ( which relies on word2vec embeddings ) perform comparably to the best previous state - of - the - art on both metrics , with the exception of ru - en , which obtains a lower average score . further , we compare against the following baselines : bertscore - f1 ( from table 1 ) , ruse ( from ruse + w2v ) and smd ( from smd + nsp ) .
the results are shown in table 1 . the first group shows that bertscore - f1 and w2v significantly outperform the baseline models on all metrics , while sfhotel achieves the best results on inf and qual scores . sent - mover also achieves competitive or better results than the baselines on two of the four metrics , with a gap of 2 . 5 points in the inf / qual score compared to the smd baseline .
the results are shown in table 1 . word - mover and sentence prediction using the best performing feature set outperforms all the baseline models on both m1 and m2 metric by a significant margin . the difference is most prevalent in the case of leic metric , which shows that the model can significantly improve upon the performance of the word prediction using only one baseline feature set . sentiment prediction using our bertscore - recall method achieves the best results with a f1 - score of 0 . 939 on m1 metric and a bertscore - recall of 1 . 083 on m2 .
the results are shown in table 1 . the first group shows that when only using shen - 1 , the model performs better than the best previous state - of - the - art on all metrics . moreover , the improvement is much larger when using para - para as well as 2d language features . we observe that m1 and m2 achieve the best results with both sets of features together , with the exception of sim .
table 1 shows the transfer quality and fluency scores of all models trained on the multi - domain dataset . the results are presented in table 1 . semantic preservation and transfer quality are high while fluency is low . we observe that yelp significantly improves over the best baseline model , and its average transfer quality is close to the best even under the difficult requirement of a drop of 5 % in f1 score .
table 5 presents the results of human evaluation . we use 200 examples for each metric for validation . the summaries generated by our system match the human ratings of semantic preservation and fluency , and match the spearman ’ s ρ b / w sim metric , which measures the degree to which a sentence has semantic preservation . however , it does not match the pp metric , and requires human evaluation to find out whether the sentence contains grammatical errors .
the results are shown in table 1 . the first group shows that when only using shen - 1 , the model performs better than it does with any other combination of language models except para - para . moreover , the improvement is much larger when using 2d as well . m1 and m2 achieve competitive or better results than the former state - of - the - art on all metrics with a gap of 10 points .
table 6 : results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best model ( lu et al . , 2018 ) achieves higher acc than prior work at similar levels of acc with a gap of 10 points from the best previous state - of - the - art . multi - decoder and template also achieve higher acc , but are worse than our model . we use the best model , yang2018 , unsupervised , because it has the best feature set and can transfer 1000 sentences with ease .
in table 2 we report the percent of reparandum tokens that were correctly predicted as disfluent . reparandum length is the average of the number of tokens in a sentence , and number of repetition tokens over the length of the original sentence . as the table shows , nested disfluencies tend to have higher repetition rates , but also higher overall performance , indicating that they are harder to solve for than rephrase .
table 3 shows the relative frequency of rephrases predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) . also , we report the percentage of tokens in each category that belong to each category as well as the fraction of tokens belonging to the repair category in table 3 . reparandum length and repair length are the most important factors in predicting whether a phrase will belong to either a repair or a repair phrase . content - content tokens alone account for 71 % of the tokens in the disfluency dataset , which shows that the ability to repair a phrase once it has been reparanged is important .
the results are shown in table 2 . we report the mean of the best performances of the single model , the best - performing model , and the average number of iterations among all models . the model performs best when trained and tested on the unsupervised dataset with the same training data , with a gap of 10 . 2 % between the best and worst performances .
performance comparison of our model to the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model improves upon the previous state of - art embeddings by 3 . 43 points in the f1 test set . it achieves the best average f1 score of 83 . 54 % , which indicates that it can easily distinguish between the features discussed and the unanticipated ones .
table 2 shows the performance of all the methods on the apw and nyt datasets for the document dating problem . our unified model significantly outperforms all previous models .
table 3 compares the performance of our method with and without graph attention in terms of word attention . our neuraldater model shows marked performance improvement , improving from 61 . 2 % accuracy to 63 . 6 % on average . with respect to graph attention , the ac - gcn model improves from 62 . 9 % to 65 . 2 % . however , it remains significantly worse than the oe model in both accuracy and graph attention .
the results are shown in table 1 . the first group shows that all models perform well on the one - to - n test set , with the exception of dmcnn . among all models , jrnn is the better performer on both 1 / 1 and 1 / n tests . it closely matches the performance of the best previous model , embedding + t , while surpassing all the other methods apart from argument argument detection .
table 1 presents the results of cross - event event detection on the training data . our method establishes a new state - of - the - art in the identification and event prediction tasks , and in the role prediction tasks . it significantly improves upon the previous state of the art on both event and argument prediction using the current set of features . the results are presented in table 1 . cross - event event detection is particularly beneficial for argument prediction , with an f1 of 68 . 7 and rn of 50 . 1 indicating that it is well - equipped to handle the task at hand .
consistent with what klinger et al . ( 2018 ) , all models give similar results on the test set , with the exception of spanish - only lm , which gets worse performance than all the other models apart from english - only - lm . moreover , fine - tuning gives only marginal improvement . the results are slightly worse than those of cs - only , but still superior to the models using shuffling and concatenation . the best results are obtained by using the best performing last model , cs - last - lm , which gives a performance gain of 2 . 72 points over the model by itself .
concerning training with only subsets of the data , we note that fine - tuning gives a significant improvement over cs - only training on the dev set , and on the test set as well . with 75 % train dev and 75 % test set coverage , fine - tuning gives a 4 . 2 % improvement over the model ' s performance on the train set .
the results in table 5 show that fine - tuning improves the model ' s performance on the test set and the dev set . the model achieves gains in accuracy over both language - adaptive and monolingual modes . it closely matches the performance of fine - tuned - lm with only 0 . 40 % absolute difference in performance .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvement from baseline to type combined gaze features is statistically significant ( p = 0 . 0088 , t - test , p < 0 . 01 ) which indicates that the ability to select compact gaze features induces the generation of better captions .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . our model improves upon the baseline performance by 9 % in terms of precision and r = 0 . 35pp , which indicates a statistically significant improvement in the performance of our model .
we apply the hpcd model from faruqui et al . ( 2015 ) to wordnet 3 . 1 , and glove - extended from rothe and schüze ( 2015 ) . the results on the test set of belinkov2014exploring ’ s ppa are shown in table 1 . the results reconfirm the effectiveness of our model , with the exception of the syntactic - sg embeddings . wordnet , verbnet and ontolstm - pp achieve gains over the type - based approach by a margin of 3 . 8 points . further improving performance by 3 . 7 points over the original lstm implementation .
table 2 shows the results for rbg with features coming from various pp attachment predictors and oracle attachments . our model obtains the best performance with a 94 . 97 % ppa acc . on the full uas test set .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the model achieves the best performance with a ppa acc . of 89 . 7 % when only using full context sensitivity .
in table 2 we report the bleu % scores of incorporating subtitle data and domain tuning for image caption translation . our model improves upon the strong baselines of en - de and mscoco17 by 3 points . by further adding subtitle data , the multi30k model improves by 4 points .
in the en - de and mscoco17 experiments , the domain - tuned models outperform the plain models and the subs1m model . the results are shown in table 1 . the first group shows that after domain adaptation , the h + ms model improves over the plain model on all three datasets . it achieves state - of - the - art results , outperforming all the alternatives except for the case of the flickr17 model by a margin of 3 . 8 points .
table 4 shows that incorporating automatic image captions improves the results for all models except for those using marian amun ( cf . table 4 ) . the model using multi30k embeddings achieves gains in bleu score over both en - de and multi - task learning models . it also improves its general performance . the largest gains are seen in en - fr and mscoco17 , both when using all 5 captions and when using the best one or all 5 .
the results in table 5 show that our encoder and dec - gate strategies outperform the en - de and mscoco17 strategies , indicating the advantage of finetuning word embeddings during development . further , our model improves upon the strong lemma baseline by 3 . 38 points in the bleu % score .
in the en - de et al . ( 2018 ) report , the results are slightly worse than those of en - fr and mscoco17 , but still superior to both ensemble - of - 3 and multi - lingual models . the results reconfirm that the linguistic features that give the best performance are the visual features , as measured by the average number of frames in the feed - forward lm detectron ( from table 2 ) .
table 1 shows that en - fr - ht and en - es - ht models perform comparably to the best previous state - of - the - art models on both mtld and ttr datasets . however , in the more realistic case of yule ’ s i , when using only translate - trans - f1 , the performance on mtld is significantly worse than those on ttr . as can be seen in the second group of table 1 , the models trained on the mtld dataset are markedly worse on cross - domain training data than those trained on yule ' s i .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . en – fr and en – es have 7 , 723 and 5 , 734 sentences , respectively , compared to 1 , 472 , 203 and 459 , 633 in en - es .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . as expected , the performance of our model improves with the growth of the src and trg scores .
in table 5 we report the automatic evaluation scores of our rev systems . the en - fr - rnn - rev and en - es - smt - rev systems achieve high performance , both in terms of bleu and ter ( table 5 ) . however , their performance is still significantly worse than that of the transformer system , indicating the advantage of finetuning word embeddings during training .
table 2 shows the performance of our model compared to the best previous model on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the model supervised using the best performing rsaimage embeddings . our model obtains the highest recall @ 10 and median rank , which shows that it is well - equipped to perform this task .
the results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . the second row labeled rsaimage is the model supervised using the best performing feature set . it achieves a mean recall @ 10 of 1 . 414 and a median rank of 0 . 9 , significantly higher than the previous best performing model , audio2vec - u .
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . as can be seen , all the classifiers turn in a screenplay that is easily distinguishable from the original ( hence , there is no need to reproduce the original ) . however , for rnn , it is harder to distinguish between edges edges edges and curves , as shown in table 1 . the difference between the effectiveness of the dan and rnn is less pronounced for unk , but still significant .
table 2 shows that part - of - speech changes in sst - 2 have occurred as a result of fine - tuning . the numbers indicate the changes in percentage points with respect to the original sentence . as the table shows , the number of occurrences has increased , decreased or stayed the same as the sentence grows . however , the percentage of instances in question has increased as well as the average number of words in the correct sentence , indicating that the effect has not been significant .
table 3 shows that sentiment scores in sst - 2 have increased as a result of the flipped labels being flipped from positive to negative sentiment . the numbers indicate the changes in percentage points with respect to the original sentence .
table 1 presents the results of experiment 1 . we empirically compare pubmed with two baselines , namely , sst - 2 and pubmed . from left to right , the results are presented in table 1 . positive and negative evaluations show that pubmed outperforms both the best and the worst performing baselines . in fact , pubmed achieves a positive f1 score of 98 % , which places it at 98 % on par with the best performing baseline . in comparison , the worst - performing baseline is 99 % on average .
