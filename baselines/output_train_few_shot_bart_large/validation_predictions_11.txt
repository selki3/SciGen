table 2 shows the performance of our treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as the table shows , the training and inference time increases as the number of instances grows , the performance decreases as well .
the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization . table 1 shows the performance of treernn models with different tree balancedness in the multi - batch setup when the size of the instances is increased to 25 . with a batch size of 1 , the balanced dataset exhibits the best performance , while the moderate dataset shows the worst performance . as the table shows , when the number of instances increases to 10 , the performance decreases as well .
the max pooling strategy consistently performs better in all model variations . for example , ud v1 . 3 shows a drop in f1 score of 0 . 63e - 04 compared to conll08 with the default values of 1 . 13e - 02 and 1 . 15e - 03 , and a drop of 9 . 66e - 00 compared to the softplus representation with a maximum of 716 parameters .
table 1 shows the effect of using the shortest dependency path on each relation type . our macro - averaged model obtains the best f1 score in 5 - fold test set in all relation types without sdp and with sdp .
the results are shown in table 3 . we observe that the performance of our model on f1 and f1 50 % is significantly better than the previous state - of - the - art models on all metrics except for r - f1 , where we observe that our model performs on par with the best previous models on both metrics .
the results are shown in table 1 . our model outperforms all the state - of - the - art models on every metric by a significant margin . our mst - parser model achieves 100 % accuracy on average on all three metrics .
table 4 shows the performance of our system compared to the majority system over the 10 runs in table 2 . our proposed lstm - parser parser outperforms both the majority and the original stagblcc parser in paragraph and sentence level performance .
the results are shown in table 1 . the results show that when tgen + is cleaned , it performs better than tgen − and sc - lstm when trained with the original tgen model .
table 1 compares the original e2e data with our cleaned version . the difference in number of distinct mrs between the original and the cleaned version is less than 0 . 5pt / 2pt , however the difference in ser is much larger ( 17 . 69 % vs . 11 . 42 % ) .
the results are shown in table 3 . the results show that original tgen + model outperforms tgen − and sc - lstm when trained and tested with the correct set of parameters . as can be seen in the table , the difference between the average bleu score of the original and the average of the two sets of parameters is much lower when trained with the wrong parameters .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . as can be seen , the number of errors we found in our tgen model is relatively low ( around 10 % ) , but we found a lot of errors in the missing and wrong values as well as slight disfluencies in the correct values .
the results are shown in table 1 . our joint model outperforms all the base models except tree2str and tsp by a large margin . the gap between our joint model and the previous state - of - the - art is almost entirely due to the small size of our data set , with the exception of seq2seqk ( konstas et al . , 2017 ) , which achieves a final all score of 25 . 9 % on average .
table 2 shows the performance of our model on amr17 . our model achieves 24 . 5 bleu points . the performance gap between our model and the previous state - of - the - art ensemble model is narrower than expected by a margin of 3 . 5 points . gcnseq ( damonte and cohen , 2019 ) , on the other hand , achieves a performance gap of 10 points .
table 3 shows the performance of our model with respect to english - german , czech and slovak language embeddings . our model outperforms all the base models except birnn and cnn + gcn in all but one case . the difference is most prevalent in the czech language , where our model achieves 43 . 8 % better performance compared to the previous state of the art .
table 5 shows the effect of the number of layers inside a dc network on performance . our model obtains the best results with n = 3 . 5 layers , and m = 23 . 3 .
comparisons with baselines are shown in table 6 . our model outperforms all the baselines except for the dcgcn1 baseline by a large margin . the difference between rc and rc + la is most prevalent in relation to residual connections , where our model obtains 25 . 5 % better performance than the previous state - of - the - art model .
the results are shown in table 4 . our model outperforms all the state - of - the - art models on every metric by a significant margin . on average , our model achieves d / p scores of 10 . 2 and 22 . 9 % better than the previous best state - ofthe - art model on all metrics .
table 8 shows the ablation study results for density of connections on the dev set of amr15 . the results show that removing the dense connections in the i - th and j - th blocks reduces the overall number of connections , but does not reduce the overall ablation rate .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . the results show that when domain - aware domain attention is used , the results are comparable to those obtained with domain coverage and linear combination . however , when domain coverage is used as the coverage mechanism , results are significantly worse .
table 7 shows the performance of our initialization strategies on probing tasks . we observe that our glorot - based system outperforms all the other approaches except our paper in terms of precision and tense scores .
the results are shown in table 3 . the h - cmow model outperforms the h - cbow model on every metric by a significant margin . it achieves state - of - the - art results on all metrics except tense with a gap of 3 . 6 points from the previous state - ofthe - art model .
the results are shown in table 3 . hybrid models outperform all the base we observe that the cbow / 784 model outperforms cmow and sick - r on all metrics except for sub - subj . subj and cmp are the only ones that show significant performance gains . as expected , the sub - jurisdiction gains are small but consistent across all metrics , with the exception of cmp .
table 3 shows the performance of our models on unsupervised downstream tasks . the results show that the cbow and cmow models have considerable performance gains over the strong hybrid baseline . hybrid models outperform cmp . models in all three tasks except for sts13 , where they perform slightly worse .
table 8 shows the performance of our initialization strategies on supervised downstream tasks . our system outperforms glorot and trec by a large margin . it achieves state - of - the - art performance on all three metrics . on sst5 and sts - b datasets , it achieves 87 . 6 % and 86 . 6 % , respectively , improvement over the previous state of the art on all metrics .
table 6 shows the scores for different training objectives on the unsupervised downstream tasks . our method outperforms the best approaches by a large margin . our cbow - r trained model achieves a final score of 43 . 2 % on the sts13 and sts16 tasks .
the results are shown in table 3 . we observe that our method outperforms the previous state - of - the - art models on every metric by a significant margin . our method obtains the best results on all metrics with a gap of 3 . 6 points from the last published results .
the results are shown in table 3 . we observe that the best performing method is the cbow - r variant , which achieves 90 . 6 % on average compared to the sick - e model on all metrics .
the results are shown in table 3 . supervised learning outperforms all the supervised learning approaches except for τmil - nd , which shows that it can learn the task to a high degree even under the difficult task of name matching . name matching is a state - of - the - art task , achieving accuracies comparable to the best supervised learning models on both loc and misc datasets . in particular , we observe that the performance on misc dataset is close to that on loc and e + loc , with a marginal improvement of 0 . 03 points over the best previous model .
table 2 shows the results on the test set under two settings . our model achieves the best results with an f1 score of 43 . 38 ± 1 . 03 on the name matching and 42 . 42 ± 0 . 87 on the f1 scores in in and out test set . the results are shown in black - aligned table 2 . supervised learning and τmil - nd ( model 2 ) achieve very high precision scores on both f1 and in test set , respectively , with accuracies of 42 . 57 ± 1 and 35 . 03 ± 15 . 03 respectively .
table 6 shows that the model with the best performance is the g2s - gat model , followed closely by the models with the worst performance . our model obtains the best results in terms of all three metrics .
the performance of our model compared to previous models on the ldc datasets is reported in table 3 . we observe that our g2s model outperforms all the previous models except konstas et al . ( 2017 ) in terms of bleu , meteor and s2s scores . moreover , it achieves a marginal improvement over the previous state - of - the - art on ldc2015e86 and ldc2017t10 scores . our model achieves a final score of 25 . 28 ± 0 . 03 on the three datasets , compared to the previous best state - ofthe - art models on all three datasets .
table 3 shows the results on the ldc2015e86 test set when models are trained with additional gigaword data . our g2s - ggnn model outperforms the previous state - of - the - art models in terms of bleu and external test set correlation .
table 4 shows the results of the ablation study on the ldc2017t10 development set . our model significantly outperforms the previous state - of - the - art models in terms of bleu , meteor and size .
we observe that g2s - ggnn model has the best performance on sentence length and average number of words per sentence , with an absolute improvement of 3 . 51 % over the baseline model in all metrics .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . g2s - gat outperforms s2s and gin in terms of both fraction of elements missing and miss . as the table shows , the model with the best performance is the one that obtains the most tokens .
table 4 shows the pos and sem accuracies for different target languages trained on a smaller parallel corpus ( 200k sentences ) . pos tagging accuracy improves as the training set grows , and sem accuracy decreases as the number of target languages grows .
table 2 shows pos and sem tagging accuracy with baselines and an upper bound . our embeddings outperform the best previous approaches on both metrics . our encoders encoder - decoder perform better than the unsupervised word encoder and word2tag classifier with a baseline of 91 . 55 % pos and 87 . 41 % sem .
table 4 presents the system ' s performance on the pos and sem metrics . our model outperforms all the state - of - the - art systems on all metrics except for pos tagging accuracy . it achieves the best results on three of the four metrics . on the pos metrics , our model obtains the best performance with an absolute improvement of 3 . 8 % on average compared to the previous state of the art model . on sem metrics , it achieves the highest performance with a 4 . 9 % absolute improvement .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . our model achieves the best results with a precision of 87 . 9 % on pos and 87 . 6 % on sem .
table 8 shows the performance of the attacker on different datasets . results are on a training set 10 % held - out . δ is the difference between the attacker score and the corresponding adversary ’ s accuracy . for pan16 , we observe that the attacker performs slightly worse than the adversary on all three datasets .
table 1 shows the performance of our model with respect to training directly towards a single task . our model outperforms all the state - of - the - art models on every metric by a large margin .
table 2 shows the results for balanced and unbalanced data splits . our model outperforms all the state - of - the - art models in terms of task accuracy and precision on all three attributes . the gender and racial features contribute significantly less than the age and gender - based features , however , their performance is comparable on task prediction .
the performance on different datasets with an adversarial training set is shown in table 3 . as can be seen , the difference between the attacker score and the corresponding adversary ’ s accuracy is small but significant , indicating that the training set can be further improved with further training .
table 6 shows the ablation accuracies of the protected attribute with different encoders . guarded embeddings perform slightly worse than leaky ones , but still perform better than rnn .
table 3 shows the performance of our model compared to previous work on the dynamic and finetune based baselines . our lstm achieves state - of - the - art results , outperforming all the previous models except for yang et al . ( 2018 ) by a large margin . our model achieves a final score of 62 . 86 % on the finetune test set compared to 62 . 45 % by the previous state of the art on the wt2 test set .
table 3 shows the performance of our model compared to previous work on the lstm and gru datasets . our model achieves state - of - the - art results with an absolute improvement over the previous state - ofthe - art on all metrics except for the base time metric .
table 3 presents the results of our final model on yelp and ama in real - time . our model outperforms both published and unpublished work on every metric by a significant margin . the results are shown in black - box table 3 shows that our proposed lstm outperforms the previous state - of - the - art models on all metrics except for yelp err .
table 3 shows the case - insensitive tokenized bleu score of our model on the wmt14 english - german translation task on tesla p100 . our model obtains the best performance with an absolute improvement of 2 . 67 points over the previous state - of - the - art model on this task .
table 4 shows the exact match / f1 score of our model on the squad dataset . our model obtains the best result with 71 . 41 % f1 score compared to the previous state - of - the - art model , rnet * ( 71 . 41 % ) on average .
table 6 shows the f1 score of our model on the conll - 2003 english ner task . our lstm model achieved 90 . 56 f1 on average , which is slightly better than the previous state - of - the - art model .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . with the base setting , our model exhibits the best performance . the difference in test accuracies between snli and ptb tasks with base and ln setting is less pronounced , but still indicates that our model can handle the task with high precision .
table 3 shows the system performance on word analogy task . our system outperforms all the other systems that do not use word analogy . word analogy task performance is very similar across all systems , with the exception of human . we observe that human is better than both system and human when trained with word analogy tasks .
table 4 shows the human evaluation results on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that human evaluation is a relatively accurate metric for selecting the best system . the average number of evaluations per system is less than the standard deviation , but still indicates a high quality . top - 1 / 2 : % of evaluations a system being ranked in top 1 or 2 for overall quality . retrieval is ranked in the top 10 % of all evaluations .
the results are shown in table 3 . table 3 shows that our model outperforms the best previous models on every metric by a significant margin . the most striking thing about our model is that it achieves state - of - the - art performance on all metrics with a gap of less than 0 . 5pp over previous models .
the results are shown in table 1 . table 1 shows that our model outperforms all the state - of - the - art models on every metric by a significant margin . the most striking thing about our model is the p < 0 . 01 score on the corpus and ted talks metrics , which show that our approach is comparable to the best previous models on all metrics except tf and docsub .
the results are shown in table 1 . table 1 shows that our model outperforms all the state - of - the - art models on every metric by a significant margin . the most striking thing about our model is that it obtains the best performance on all metrics with a gap of 0 . 5 points in the multi - model test set compared to the previous state of the art .
the results are shown in table 1 . we observe that the average depth and maxdepth metrics are relatively consistent across all metrics , with the exception of the depth cohesion metric , which is significantly lower for europarl . the maxdepth metric is the most important metric for our model , and it is the only metric that appears to have a significant effect on the overall performance .
the results are shown in table 1 . we observe that the average depth and maxdepth metrics are relatively consistent across all metrics , with the exception of the maxdepth metric , which is significantly lower than the other metrics . europarl has the best overall performance on all metrics .
table 1 shows the performance ( ndcg % ) on the validation set of visdial v1 . 0 . the enhanced version of our framework outperforms the baseline model in terms of question type , answer score sampling and hidden dictionary learning . moreover , it achieves a significant improvement in generalization performance over p1 by 10 . 42 % over the best baseline model ( 57 . 36 % vs . 62 . 63 % ) .
table 2 shows the performance of the ablative studies on different models trained on the visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 2 , while p1 indicates the best performing one . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut .
table 5 shows the results on hard and soft alignments . the hmd - f1 model outperforms all the other approaches except wmd - bigram and hmd - recall with a gap of 0 . 823 points on hard alignments and 0 . 866 points on soft alignings . wmd unigram and bert perform similarly on both alignments , but the gap is much larger on the soft alignment , where we get 0 . 864 points .
the results are shown in table 1 . the average score of our method is 0 . 685 on the direct assessment metric and 0 . 866 on the multi - factor test set compared to the baselines .
the results are shown in table 3 . sent - mover and w2v models outperform meteor and bertscore on all metrics except for f1 score . as the table shows , when using the baselines from bleu - 1 and 2 , the sfhotel model performs best on all but f1 scores . we observe that the quality of the final score obtained by bert score is relatively high when using all the metrics we set as baseline .
the results are shown in table 3 . sent - mover and word - mover metrics are the most interesting ones . we observe that the performance on m1 and m2 metric is significantly better than those on m2 when using only word - based metrics , indicating that the clustering ability of our word embeddings is high . on the other hand , when using word based metrics , the performance is lower .
the results are shown in table 7 . we observe that the performance of our model with only shen - 1 as input improves as the meta - para layer grows , and that the effect is less pronounced when the model is trained with 2d as input . our model achieves state - of - the - art results with accuracies on all metrics except pp .
the results are shown in table 3 . we observe that the transfer quality and transfer quality metrics are relatively consistent across all models , with the exception of yelp , where we observe that transfer quality is significantly worse than transfer quality . semantic preservation and fluency metrics are weakly correlated with transfer quality , indicating that the semantic preservation function is under - powered . finally , we observe a drop in performance between models a and b when we apply our transfer quality metric δsim compared to those without .
table 5 shows the results of human sentence - level validation on yelp dataset for validation of acc metric and pos metrics . our model verifies 94 % of machine and human judgments that match the acc metric , 94 % on average . pos metrics verifies 84 % , a significant improvement over the previous state of the art .
the results are shown in table 7 . we observe that the performance of our model with shen - 1 as the shen baseline improves as the meta - para layer grows , and that our model performs better on para - based models as well . our model achieves state - of - the - art results on all metrics .
table 6 shows the results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( right table ) achieve higher acc than prior work at similar levels of acc , but untransferred sentences achieve the highest bleu . acc ∗ : the definition of acc varies by row because of different classifiers in use . our model achieves the highest acc even when trained with only one classifier . the results are slightly worse than those obtained using simple transfer , but still comparable to the best unsupervised models .
table 2 shows the percent of reparandum tokens that were correctly predicted as disfluent . as expected , nested disfluencies are much less common than repetition tokens , but still represent a significant drop in performance compared to rephrase tokens . moreover , the average number of repetition tokens in a disfluency prediction is much higher than in a nested disfuncuation prediction .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) . the majority of tokens in each category belong to the repair category , but the fraction of tokens belong to each category as disfluency is small , so we observe that for all but one of these categories , our model can perform well in predicting the disfluences .
the results are shown in table 3 . we observe that the model performs best when trained with only one type of transformation : text + innovations or text + raw + innovations . as the results show , when we add in the innovations and the text transformation , the dev mean and the test best scores of our model are both significantly improved .
the performance comparison between our model and the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model achieves the best performance with an accuracy improvement of 3 . 43 % over the state of - the - art rnn - based embeddings .
table 2 shows the precision and accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . neuraldater achieves the best performance with an accuracy of 62 . 2 % on the nyt dataset and 62 . 9 % on apw dataset . ac - gcn performs similarly on both datasets , with a slight improvement over the performance of burstysimdater . attentive neuraldater shows lower performance , but is comparable to maxent - joint . finally , ad3 shows a slight performance improvement .
table 3 shows the performance of our neural models with and without word attention . our neural model outperforms all the state - of - the - art approaches that do not rely on word attention except ac - gcn .
the results are shown in table 1 . embedding + t models perform better than cnn and dmcnn on 1 / 1 and 1 / n tasks , but perform worse on 3 / n and 7 / 10 tasks . as shown in the table , when all models are trained on the same training data , the performance gap between the best performing model and the worst performing model becomes much narrower .
table 1 presents the results for cross - event event detection . our model outperforms all the state - of - the - art methods on every metric except for f1 score . the results show that cross - event event detection is beneficial , improving upon the performance of previous models on both event and argument detection . moreover , our model performs better on event detection as well .
the results are shown in table 3 . we observe that fine - tuned models perform better than all the alternatives except for spanish - only - lm , which is closer to the performance of the original model . however , fine - tuned models do not perform as well on the test set as well as shuffled models . finally , the results are slightly worse on the dev perp metric than on test wer metric , indicating that the tuning process may not be beneficial for all models .
results on the dev set and on the test set using only subsets of the code - switched data are shown in table 4 . the fine - tuned model outperforms cs - only on both sets with a large margin . fine - tuned models perform better on the train dev and test set , with a marginal drop in performance on the full train set when trained with 50 % train dev .
the results in table 5 show that fine - tuned disc - based learning outperforms fine - tuned monolingual learning on the dev set and on the test set , both when the gold sentence is in the set and when it is tested on the standard test set . the results are statistically significant even when using only one type of gold sentence in the training set .
table 7 shows precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . the type combined approach shows a significant improvement in precision and recall over the baseline model , with a f1 score of 3 . 61 points higher than the type combined baseline model . the improvement is statistically significant ( p < 0 . 05 ) and r = 0 . 61 , both for type combined and baseline models .
table 5 shows precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset as inputs . the type combined approach shows a statistically significant improvement in precision and recall compared to baseline , indicating that the model can further improve interpretability without sacrificing performance .
the results on belinkov2014exploring ’ s test set are shown in table 1 . syntactic - sg embeddings outperform lstm - pp and ontolstm on the wordnet test set . glove - extended refers to the fact that the word embedding obtained by autoextendrothe and schütze ( 2015 ) is already in the pre - trained word embedds , and therefore does not need to be extended . further , the type and type of tokens used in the hpcd setup are the same as in the original paper ( farrequi et al . , 2015 ) . however , the difference in performance between the two approaches is less pronounced with respect to the syntactic type , indicating that the type - based approach is more suitable for production use . finally , the performance gap between the original and the second iteration is narrower than expected by chance .
results shown in table 2 show that the hpcd dependency parser outperforms the ontolstm - pp model with features derived from various pp attachment predictors and oracle attachments . further improving upon the performance with additional features , the system achieves state - of - the - art results .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . our model achieves the best performance with a ppa acc . score of 89 . 7 % on the full test set .
the results in table 2 show that domain tuning and domain tuning improve the bleu % scores for image caption translation across all three models . the ensemble - of - 3 model achieves the best results , with marian amun ( marian amun et al . , 2017 ) reporting 66 . 7 % bleus % on multi30k word embeddings and 62 . 6 % on en - de . as the results show , domain tuning improves interpretability across all models .
we observe that domain - tuned models perform best in en - de and en - fr settings , but do not perform as well in flickr17 or mscoco17 . the results are slightly better than those reported in ( table 3 ) .
table 4 shows the bleu scores in % for en - de , en - fr and mscoco17 models compared to those using automatic captions with marian amun ( see § 2 ) . the results show that when automatic image captions are added to a multi - task context - aware framework , the performance of the model with the best 5 captions improves significantly . however , when only auto - captions are used , the improvement is less pronounced .
the results in table 5 show that enc - gate and dec - gate strategies outperform en - de and mscoco17 when using the multi30k + ms - coco + subs3mlm transformation with bleu % scores computed using the detectron mask surface as a metric of visual information integration . further , the improvement is much larger when using multi - 30k with sub - subs as the mask surface .
the results are shown in table 3 . multi - lingual models perform significantly worse than those using text - only and multilingual features . for example , subs3m ( gill2048 ) and subs6m ( hochreiter et al . , 2017 ) perform substantially worse than either ensemble - of - 3 or monolingual models with visual features . moreover , the performance drop is much worse when using ms - coco features , as shown in fig . 3 .
the results are shown in table 3 . we observe that en - fr - ht and en - es - ht produce better results than enfr - smt - back models . the average number of frames per second for each translation is slightly higher than that for those without . however , the difference is less pronounced when we consider ttr and mtld as inputs .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . our en – fr model outperforms en – es by a large margin .
table 2 shows the training vocabularies for the english , french and spanish data used for our models . the results are shown in black - aligned t - test . our model outperforms the best previous models on all three languages .
table 5 shows the automatic evaluation scores for the rev systems . the en - fr - rnn - rev and en - es - smt - rev systems perform similarly to each other in terms of bleu and ter scores . however , the performance gap between the two is much smaller when using ter as the reference , as shown in table 5 .
table 2 shows the results on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the trained model from flickr18k . the difference in recall between the two models is small , with vgs achieving a mean mfcc of 0 . 2 and a mean rank of 17 . 0 , which indicates that the model performs well in low - supervision settings .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled u denotes the audio2vec - u model trained on the generated rsaimage embeddings . the average number of recalls per recall row is 0 . 5 , the mean number of recall per chance row is 3 , 955 and the median rank is 647 .
table 1 shows the example sentences of the different classifiers compared to the original on sst - 2 . we report further examples in the appendix . the rnn turns in a screenplay that has edges at the edges and a curved center . cnn turns on a on ( in in the the the edges ’ s so clever “ want to hate it ” sentence . as can be seen in the table , the difference in the average number of turns per sentence between the original and the rnn is small , but the difference is significant . it can be observed that the dan classifier is much more useful for this task .
table 2 shows the pos changes in sst - 2 since fine - tuning . the numbers indicate the changes in percentage points with respect to the original sentence . as the table indicates , the number of occurrences has increased , decreased or stayed the same as a result of fine tuning . the last row indicates the overlap with the word embeddings in the sentence , which indicates that the extent to which the vocabulary has been expanded or decreased as a function of the fine tuning process .
table 3 shows the sentiment score changes in sst - 2 . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the last two rows correspond to the case where negative labels are flipped to positive and vice versa .
table 1 presents the results of our experiments on pubmed and sst - 2 . the results are shown in bold . our approach outperforms all the state - of - the - art models on every metric by a significant margin . on the negative metric , pubmed achieves an acc / f1 score of 98 % and 99 % on the positive metric .
