table 2 : throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . the iter and fold approaches yield comparable performance on inference and training , respectively , with a batch size improvement of 2 . 5 instances / s over the previous state of the art . on the training dataset , the fold technique shows a performance gain of 3 . 6 instances / s in inference and 5 . 6 examples / s on training , while it is comparable on inference .
table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t .
the max pooling strategy consistently performs better in all model variations . for example , ud v1 . 3 shows a f1 score of 75 . 83 on the test set with the best representation and a dropout probability of 69 . 57 % on the conll08 test set , which shows that the dropout prediction function can be further improved with the addition of a filtering function .
table 1 shows the effect of using the shortest dependency path on each relation type . our model obtains the best f1 score in 5 - fold test set when using sdp instead of macro - averaged dependency paths .
the performance of our model on the f1 test set is presented in table 3 . we observe that our model achieves 100 % f1 and 50 % r - f1 on average , with an absolute improvement of 3 . 45 % on average compared to the previous state of the art .
table 3 presents the results for english and german . our model outperforms all the alternatives except mst - parser by a large margin . we achieve 100 % accuracy on average with a gap of 2 . 59 points from the last published results .
table 4 shows the average c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph and paragraph vs . sentence level . note that the mean performances are lower than the majority performances over the runs given in table 2 . the difference between paragraph and sentence level is less pronounced for the essay system , indicating that the parser performs better at the paragraph level .
the results are shown in table 4 . the original and the original cleaned tgen + model outperform the original cleaned tgen model on every metric by a significant margin . on the test set , the original tgen − model is better than the original on all metrics except meteor and bleu , while the difference is narrower on nist and cider .
table 1 compares the original e2e dataset with the cleaned version . the difference in the number of distinct mrs and total number of textual references shows that our cleaned version is comparable in quality to the original one .
the results are shown in table 3 . the original tgen model outperforms the original and the original variation of sc - lstm on all metrics except bleu and nist . the difference is most pronounced on meteor , which shows that the tgen + model is better than the original on most metrics . on the nist metric , the difference is less pronounced , but still shows significant performance improvement over the original model .
table 4 : results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) and the number of instances for which the correct values were added and subtracted ( disfluencies ) .
the performance of our model compared to previous approaches on the hidden test set is presented in table 3 . our proposed graphlstm model outperforms all the previous approaches except for seq2seqk , which shows a slight improvement over the previous state - of - the - art . we observe that the performance gap between our model and previous approaches is narrower than the gap between the best previous state of the art .
table 2 shows the performance of our model on amr17 . our ensemble model achieves a bleu point improvement over the previous state - of - the - art seq2seq model by 3 . 5 bleus over the best single ensemble model . similarly , our dcgcn model achieves an increase of 4 . 5 points over previous state of the - art results . table 2 also shows that our ensemble model is comparable to the best performance of the previous ensemble models in terms of all parameters .
table 3 presents the results for english - german , german - czech and czech . our model outperforms all the base lines except the bow + gcn model , which shows the advantage of cross - linguistic similarity . we also observe that our proposed ggnn2seq model performs better than the previous state - of - the - art models on both languages .
table 5 : the effect of the number of layers inside our model on the performance of the final test set . our model obtains a significant improvement over the state of the art when only one layer is added . we observe that our model achieves a new state - of - the - art result when we add 6 layers to our model .
table 6 compares our model with previous state - of - the - art gcns with residual connections . our model outperforms all the baselines except for the united states department of agriculture ( usda ) gcn , which shows considerable performance improvement over the best previous state of the art model .
the results are shown in table 4 . our proposed method outperforms the previous state - of - the - art on all metrics by a significant margin . on the dcgcn test set , our model obtains the best results with an absolute improvement of 3 . 9 % over the previous best result . we observe that our proposed method is comparable to the best previous state of the art on most metrics .
table 8 shows the ablation study results for density of connections on the dev set of amr15 . the results show that removing the dense connections in the i - th and x - th blocks significantly reduces the overall density of the network .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . our proposed decoding scheme outperforms the previous state - of - the - art on both metrics by a significant margin .
table 7 shows the performance of our initialization strategies on various probing tasks . our glorot - based model obtains the best results . it obtains a score of 35 . 8 % on average compared to the previous state - of - the - art model on all metrics .
the results are shown in table 3 . we observe that the h - cmow variant outperforms the original cbow and h - cbow with a margin of 3 . 6 points over the best previous state - of - the - art model . it also shows a significant performance improvement over the strong baselines on the subtasks of concatenation .
the results are shown in table 3 . we observe that our method outperforms the previous state of the art on all metrics except sst2 and sst5 by a margin of 0 . 2 % and 0 . 6 % , respectively .
table 3 : scores on unsupervised downstream tasks attained by our models . we show the relative change in cbow and cmow scores with respect to hybrid when we switch from cbow to cmp . on the sts12 and sts15 datasets , the cbow model shows a marginal improvement of 2 . 5 % and 7 . 6 % over the hybrid model , respectively . the difference is less pronounced on sts16 , where our model achieves a marginal gain of 4 . 6 % . we also see a marginal increase of 1 . 8 % for cbow over the cmow model , which shows the diminishing returns from mixing cbow with cmow .
table 8 shows the performance of our initialization strategies on supervised downstream tasks . our glorot - based system outperforms all the state - of - the - art systems on all metrics except sst2 and sst5 by a noticeable margin . on average , it achieves a score of 87 . 6 / 87 . 7 and 86 . 4 / 86 . 2 over the previous state of the art on both datasets .
table 6 shows the scores for different training objectives on the unsupervised downstream tasks . our method outperforms the previous methods on all the three datasets . the cbow - r score improves from 43 . 2 % to 61 . 9 % on average . on the sts13 dataset , our method improves from 35 . 5 % to 62 . 9 % .
the results are shown in table 3 . we observe that our method outperforms the previous state - of - the - art methods on all metrics except subtasks except length . our method obtains the best results on subtasks with a gap of 3 . 6 points from the previous best state of the art . on the subtasks , it obtains an overall improvement of 2 . 9 points over previous methods .
the results are shown in table 3 . the best performances are obtained by the sub - subj and mrpc sub - categories , with the exception of sick - e , which shows considerable improvement under trec . our method outperforms all the other methods except cmow - r by a noticeable margin .
table 3 shows the performance of our system with respect to all loc , per and misc scores . our proposed method outperforms all the stateof - the - art methods except for supervised learning . our model obtains the best results with an absolute improvement of 3 . 36 points over the previous state of the art on all metrics .
results on the test set under two settings are shown in table 2 . our model improves upon the previous state - of - the - art model with a f1 score of 43 . 42 ± 0 . 59 compared to 35 . 38 ± 1 . 03 and 35 . 87 ± 0 . 59 for the original model . with the additional training data , our model improves further to a final score of 71 . 57 ± 0 . 59 . compared to the previous best state of the art model , this is a significant improvement .
table 6 presents the results of our experiments on the hidden test set of g2s - gat . our model outperforms the previous state - of - the - art models on every metric by a significant margin . on average , our model obtains 73 . 45 % better results on average compared to the previous best state of the art model .
table 3 compares the performance of our model with previous state - of - the - art models on the ldc2015e86 and ldc2017t10 datasets . our g2s model outperforms all the previous state of the art models except for konstas et al . ( 2017 ) . it achieves a marginal improvement over the s2s baseline by 0 . 3 points on the three ldc datasets , but still performs substantially worse than the previous best state - ofthe - art model . we also observe that our model is comparable to the best previous work on three of the four datasets .
table 3 : results on ldc2015e86 test set when models are trained with additional gigaword data . our g2s - ggnn model improves upon the previous state - of - the - art model by 3 . 5 bleu points .
table 4 shows the results of the ablation study on the ldc2017t10 development set . our model outperforms the previous state - of - the - art models on all metrics except meteor by a significant margin .
the results are shown in table 3 . we observe that g2s - gat has achieved a marginal improvement of 3 . 51 % over the previous state - of - the - art model on average across all metrics .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( added ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . our model outperforms s2s and g2s - ggnn by a significant margin .
table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the pos model shows that it is comparable to previous state - of - the - art nmt models on a single target language , with the exception of semantic features .
table 2 compares the pos and sem tagging accuracy with baselines and an upper bound . our unsupervised embeddings outperform the best previous approaches on both metrics . our encoder encoder decoder has the upper bound of 91 . 55 % on pos and 91 . 41 % on sem , which shows the accuracy of our encoder - decoder . on the other hand , our word2tag encoder has 95 % accuracy .
table 4 presents the system ' s performance on the pos and sem datasets . our model obtains the best results on all metrics with an absolute improvement of 3 . 9 % on average compared to the previous state of the art .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we observe that the res encoder has the best performance , while the uni encoder is close to the best .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . the difference between the attacker score and the corresponding adversary ' s accuracy is statistically significant ( p < 0 . 01 ) with respect to all the three protected attributes .
table 1 : accuracies when training directly towards a single task . our model outperforms all the state - of - the - art methods on every metric by a significant margin .
table 2 : protected attribute leakage : balanced & unbalanced data splits . we show the performance of our model with respect to the task accuracy and the unbalanced task accuracy in table 2 . our proposed model outperforms the previous state - of - the - art on all three datasets .
the performance of our model on different datasets with an adversarial training set is shown in table 3 . our model outperforms the previous state - of - the - art on all metrics except for the accuracy in the task prediction . δ is the difference between the attacker score and the corresponding adversary ' s accuracy .
the results are shown in table 6 . the rnn encoder outperforms the guarded encoder in terms of decoding the protected attribute with different encoders . it is comparable to the performance of the original embeddings .
table 3 presents the results of our final model on the hidden test set of wt2 . our model outperforms the previous state - of - the - art on all metrics except finetune by a noticeable margin . it achieves a final score of 69 . 36 % on the ptb + finetune metric , which implies that our model is more than 4 . 5 % better than previous state of the art models on average . on the wt2 dataset , our model achieves an absolute improvement of 3 . 45 % over the previous best state - ofthe - art model .
table 3 presents the results of our final model on the lstm test set . our model outperforms the previous state - of - the - art on every metric by a noticeable margin . the difference is most prevalent on the base acc metric , which shows that our proposed gru model is more than 4 . 5x better than the previous best state of the art model on average .
the results of zhang et al . ( 2015 ) are shown in table 3 . table 3 shows the results of our final model compared to the previous state - of - the - art on the three datasets . our proposed method outperforms all the previous methods except atr by a noticeable margin . our final model obtains the best results on the amapolar and yelppolar datasets . it also outperforms the previous best model on the yahoo dataset by a significant margin .
table 3 shows the bleu score of our model on the wmt14 english - german translation task on tesla p100 . our model obtains a case - insensitive t - test score of 26 . 67 / 71 . 36 on the training set and a corresponding 27 . 36 / 59 . 45 bleu score on the decode set .
table 4 shows the exact match / f1 - score of our model on the squad dataset . our model obtains a 79 . 41 % f1 score on average compared to the previous best state - of - the - art model by wang et al . ( 2017 ) . we also show the results of the model with the parameter number of elmo as well as the number of parameters for base . finally , we show the f1 scores of the models with the least and most parameters as well . the most interesting thing about our model is that it obtains the best performance even when using only elmo .
table 6 shows the f1 score of our model on the conll - 2003 english ner task . our lstm model outperforms all the stateof - the - art models with a gap of 3 . 56 points from the last published result ( lample et al . , 2016 ) .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . with the base setting , our model obtains 85 . 56 % accuracy . with the perplexity setting set at 62 % , our model gets a marginal improvement of 3 % .
table 3 presents the system performance on the word analogy task . our system outperforms all the state - of - the - art systems on all metrics except human . word analogy task is comparable on both systems , with the exception of human , where oracle retrieval leads to a performance gap of 2 . 5 points . retrieving the word analogies from word analogy tasks without error is comparable to human , but does not exceed the performance of oracle .
table 4 presents the results of human evaluation . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that our system is indeed comparable to the best previous state - of - the - art on most aspects ( except appropriateness ) . retrieval , on the other hand , obtains a lower standard deviation than candela and h & w hua and wang ( 2018 ) .
the results are shown in table 5 . table 5 shows the performance of our approach compared to the previous state - of - the - art methods on the test set of english , french , spanish , dutch , russian and turkish . our approach outperforms all the previous methods except docsub and wikipedia by a noticeable margin .
the results are shown in table 3 . table 3 shows the performance of our approach compared to the previous state of the art on the test set of english , french , spanish , dutch , russian and turkish . our proposed hclust outperforms all the previous methods except for the case of corpus , which is slightly better than the previous best state - of - the - art .
table 3 shows the performance of our approach compared to the previous state - of - the - art methods on the test set of english , french , spanish , dutch , russian and turkish . our proposed hclust model outperforms all the previous methods except for the case of ted talks , which is slightly better than the previous best state of the art .
the results are shown in table 3 . we show the averagedepth and maxdepth scores of our dsim and europarl datasets . our dsim dataset outperforms the previous state - of - the - art on every metric by a significant margin . the averagedepth score of 11 . 05 points is slightly higher than the previous best state of the art .
the results are shown in table 3 . we show the averagedepth , averageroots and totalterms . the averagedepth metric is the average of the number of roots divided by the average depth of the word " europarl " . we also show the maxdepth metric , which measures the extent to which a word has been divided into multiple entities . europarl has the best performance .
in table 1 , we compare the performance of our enhanced model with the original version of visdial v1 . 0 . the enhanced model outperforms the baseline model by a significant margin . the difference in ndcg % between the enhanced and the original model is more than 2 . 5 points .
the performance ( ndcg % ) of the ablative studies on different models on the visdial v1 . 0 validation set is shown in table 2 . the best performing model is p2 , which shows that hidden dictionary learning is indeed the most effective . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut .
table 5 compares the performance of our hmd - f1 model with the previous state - of - the - art recall and unigram based hmd models on the hard and soft alignments . our hmd model shows that it is comparable to previous state of the art on both alignments , with the exception of the recall metric , which is slightly worse than the hmd baseline .
the results are shown in table 3 . the average score of all the baselines is reported in terms of bertscore f1 . our proposed method outperforms the previous state - of - the - art method on all metrics except for direct assessment .
the results of bertscore - f1 on the hotpot test set are shown in table 3 . the proposed hgn outperforms the previous state - of - the - art on all metrics except for the sent - mover metric by a noticeable margin . on the hgn test set , our proposed sfhotel model outperforms all the baselines except the meteor baseline by a significant margin .
the results are shown in table 3 . word - mover accuracy on the m1 and m2 metric is set at 0 . 939 and 0 . 949 , respectively , while the bertscore score is 0 . 866 . we observe that word - mover accuracy is relatively consistent across all the metrics , with the exception of leic score , which is slightly lower than the other baseline . sent - movers accuracy is slightly higher than other methods , but still comparable to the best previous work .
the results are shown in table 7 . para - para model outperforms all the other classifiers except shen - 1 , indicating that it has superior generalization ability . we observe that , for example , m0 [ italic ] + cyc + para is better than m0 + 2d for all but one of the five classes , with the exception of m6 , which is slightly worse than m7 .
table 3 presents the results of our final model on the yelp dataset . our proposed transfer quality and semantic preservation metrics outperform the previous state - of - the - art on every metric by a significant margin . we observe that our transfer quality metric is comparable to the best previous state of the art on all metrics except for transfer quality . our semantic preservation metric outperforms all the previous methods except transfer quality , indicating that our proposed semantic preservation method is comparable with the state - ofthe - art .
table 5 shows the results of human evaluation . our model verifies 94 % of machine and human judgments that match the acc metric . it also verifies 84 % of human judgments . the accuracy increases with the growth of scalability .
the results are shown in table 6 . we show that our model outperforms the previous state - of - the - art on all metrics except sim . on the sim test set , our model obtains the best performance with a marginal improvement of 0 . 8 points over the previous best state of the art . para - para models outperform all the previous models except shen - 1 .
table 6 : results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( right table ) outperform all the previous work on the sentiment transfer dataset by a significant margin . acc ∗ : the definition of acc varies by row because of different classifiers in use . the best model achieves the highest acc , but is still worse than the best unsupervised model . we also show the performance of our model with respect to lexical features as well as the accuracy of our classifiers . we use the best multi - decoder model ( fu - 1 ) and the best classifier ( yang2018 ) for both transfer and retrieval .
in table 2 , we report the percentage of reparandum tokens that were correctly predicted as disfluent when we filtered out repetition tokens and nested disfluencies . reparandum length is the average of the length of the reparanda with the greatest overlap between repetition and disfluency . the average number of repetition tokens is slightly longer than the average length of disfluential ones .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) . similarly , it also shows the fraction of tokens that belong to each category when the content word is contained in the repair . reparandum length and repair length are the most important factors in predicting whether a disfluency will occur in a reparation or repair disfunction .
the results are shown in table 3 . we observe that the dev mean and average test scores of our model are both slightly higher than the best previous state - of - the - art models on average when we switch from single - input to multi - input learning models . our model achieves the best result with a precision of 87 . 53 % on average .
the performance comparison of our model with the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model obtains an accuracy improvement of 3 . 43 % over the previous state of - the - art model on average .
table 2 compares the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . our neural dater outperforms the previous state - of - the - art models on both datasets .
table 3 compares the performance of our neuraldater model with and without graph attention . our approach shows that the word attention is comparable to the effectiveness of graph attention , while the accuracy is superior to the approach using t - gcn ( 63 . 9 % vs 62 . 6 % ) .
the results are shown in table 3 . we show that the best performance is obtained by the jrnn model , which obtains 75 . 2 % overall improvement over the previous state - of - the - art dmcnn model . however , it is still inferior to all the other methods except for the argument stage , when it comes to accuracy . embedding + t also improves performance , but still lags significantly behind other methods .
table 3 presents the results of cross - event event coreference on the test set . our proposed method outperforms all the state - of - the - art methods except cross - event inference . it achieves a f1 score of 68 . 7 / 71 . 9 on the validation set , which implies that our proposed method is comparable to the best previous state of the art .
the results are shown in table 3 . we see that the best performance on english - only and spanish - only language is achieved by the fine - tuned variant of our model , while the worst performance is obtained by the original shuffled - lm variant .
results on the dev set and on the test set using only subsets of the code - switched data . the fine - tuned model outperforms the original cs - only model by a large margin . it achieves a final score of 75 . 2 % on the train dev and 75 . 9 % on test set , both on the 75 % and 75 % train test sets , respectively .
the accuracy on the dev set and on the test set , according to the type of the gold sentence in the set , is reported in table 5 . the fine - tuned variation outperforms the original variation in both accuracy on both sets . note , however , that fine - tuned variation has lower accuracy on test set compared to the original .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the type combined approach shows a significant improvement in precision over the baseline model , with f1 scores improving from 62 . 97 % on the baseline to 74 . 61 % .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . our proposed method shows a significant improvement in performance over the baseline model , indicating the effectiveness of our proposed method .
the results on the test set of belinkov2014exploring ’ s ppa are shown in table 1 . glove - extended refers to the synset embeddings obtained by running autoextend rothe and schütze ( 2015 ) on gllove . it outperforms the original hpcd model by a large margin . the difference is most pronounced in the syntactic - sg case , when we use the type - of tokens obtained from the original paper , which is 84 . 3 % on par with the original lstm - pp .
table 2 : results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . our hpcd system obtains the best performance with 94 . 51 % acc . on the full uas test set . the best performance is obtained when using ontolstm - pp as the dependency parser .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . our model achieves an acc . accuracy ( ppa ) of 89 . 5 on the full test set and 87 . 4 on the attention test set .
table 2 : adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun . the ensemble - of - 3 model outperforms the multi30k model with a bleu % score of 66 . 5 % compared to 62 . 0 % with en - de . as the results show , domain tuning improves the translation performance for both en - fr and mscoco17 models .
the results of domain - tuned h + ms - coco are shown in table 3 . our model outperforms all the baselines except en - de and mscoco17 by a margin of 3 . 8 points . the difference is most pronounced in en - fr and flickr17 , where the h + domain - tuning model leads to a performance drop of 3 points .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . our model obtains the best results with marian amun . adding only the best 5 captions improves our model ' s performance by 2 . 7 points . the performance of en - de model is slightly better than en - fr model , but still slightly worse than mscoco17 model . finally , the performance of our model with autocap 1 - 5 improves by 2 points .
in table 5 , we compare the bleu % scores of our enc - gate and dec - gate strategies for integrating visual information into the multi30k + ms - coco + subs3mlm task using the best performing transformer model . our enc - gate + dec - gate encoder outperforms all the en - de and mscoco - based encoder and decoder models except for en - fr , which shows the advantage of domain - aware decoding . we also observe that our encoder is more interpretable than our decoder , indicating that our decoding technique is more effective in the low - resource settings .
we show the results for en - de and en - fr for flickr16 , flickr17 , and mscoco17 in table 3 . the results are summarized in bold . we observe that the ensemble - of - 3 approach outperforms the single - lingual approach when we only consider visual features , and when we consider all the other aspects as well . for example , the ensemble of 3 approach by itself gives a performance comparable to the performance of subs3m [ italic ] and subs6m when using the text - only approach . however , when we add the visual features as well , our results are slightly worse than the previous approaches .
the results are shown in table 3 . we show that en - fr - ht and en - es - ht variants are comparable in performance to the best previous state - of - the - art models on both ttr and mtld . however , the difference in performance between the two sets is narrower than expected when we only consider ttr data . the difference is most pronounced when we consider mtld data , which shows the extent to which the semantic features of the original sentence can be improved with the addition of the required syntactic distance measures .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . our en – fr model outperforms the previous state - of - the - art on all three splits .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the results are statistically significant ( p < 0 . 01 ) with respect to the src and trg scores .
table 5 shows the evaluation scores for the rev systems . our en - fr - rnn - rev system obtains the highest bleu score and ter score , while en - es - trans - rev receives the highest ter score . we observe that our system is comparable to the state - of - the - art in terms of generalization .
results on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the trained model from the previous work ( chan et al . , 2017 ) . rsaimage has the highest recall @ 10 and the median rank of 0 . 8k , indicating that the model is more than 1 . 5 times more likely to match the original image .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled u denotes the audio2vec - u model . the acoustic2vec embeddings outperform all the alternatives except segmatch , indicating that the similarity between rsaimage and audio2vec is less pronounced for synthetic datasets . we also observe that the average recall @ 10 is higher for rsaimage than for vgs , indicating the advantage of cross - linguistic supervision .
table 1 compares the original on sst - 2 with the different classifiers used in our experiments . the dan classifier turns in a screenplay that is at the edges of the word " loathing " . it turns on a on ( in in the the the edges ) and gives the correct answer when asked to name the worst part of the movie . cnn classifiers turn in a sentence that is so clever you want to hate it . rnn classifiers also turn in sentences that are so clever , but give the wrong answer . we report further examples in the appendix .
table 2 shows the percentage points at which the number of occurrences have increased , decreased or stayed the same through fine - tuning of the original sentence in sst - 2 . the last row indicates the overlap with the original sentences . the numbers indicate the changes in percentage points with respect to the corresponding part - of - speech ( pos ) in table 2 . for example , the rnp model has increased by 3 . 5 % in pos since the last iteration ( from 69 . 0 % to 71 . 5 % . for cnn , the percentage point increase is only 2 . 7 % . for rnp , the change is 2 . 9 % .
the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . in the positive sentiment case , the rnn score increases by 9 . 6 points and the cnn score by 5 . 3 points . the corresponding dan scores increase by 7 . 9 points .
table 3 presents the results of our experiments on the pubmed dataset . our proposed method outperforms all the state - of - the - art methods except sift by a large margin . the difference between pubmed and sift is statistically significant ( p < 0 . 001 ) .
