table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as the table shows , both approaches benefit from more data in the training set and the improvement on inference is greater when the number of instances is increased from 10 to 25 .
table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . we find that the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t .
the max pooling strategy consistently performs better in all model variations . it achieves the best results when trained and tested on the conll08 dataset ( table 2 ) . as a sanity check , we also compare it to ud v1 . 3 ( which takes the best performing feature - mapping approach ) . we observe that when trained with the correct representation , the dropout probability is lower than when using softplus . as hard coreference problems are rare in a single model , we do not have significant performance improvement . this confirms our intuition that softplus is a better complement to sigmoid in the low - resource settings . we continue to investigate this by testing our model with different representation settings . we benchmark it against the following features : the hyperparametrization parameters ( which can be found in table 2 ) and the number of iterations of the training set . we compare the results of each approach ( i . e . in 5 - fold ) with the previous state of the art . the first set of results show that when using the feature - rich training set with a minimum of 5 iterations , the model performs better than the other two sets .
table 1 : effect of using the shortest dependency path on each relation type . we show that macro - averaged models result in significantly better f1 scores than those without . also , when we add sdp as dependency path , our model achieves the best f1 .
consistent with the observations by vaswani et al . ( 2017 ) , we observe that the three types of models perform comparably when trained and tested on the same dataset . however , in the more realistic second case , when trained on a larger corpus , performance on the y - 3 model drops significantly . this is mostly due to the lower performance of the second group on the f1 and r - f1 scores . on the other hand , on the larger scale , this bias is less pronounced for the smaller - scale models . we observe that on the large - scale training set , the accuracy remains the same , with the exception of on f1 50 % .
the results of paragraph prediction accuracy are presented in table 1 . we present results for the best performing model . for each paragraph prediction , we set a minimum accuracy level and a maximum number of spaces for each prediction . our model achieves accuracy of 100 % , which means it achieves more than 50 % on average . on the other hand , it gets close to 50 % .
we compare the performances of our system with the best performing parser ( lstm - parser ) on paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . the difference is most prevalent in paragraph level , where our system obtains the highest c - f1 score .
the results are shown in table 1 . the first group shows that the original and the cleaned tgen models perform comparably to each other when the original is trained and tested on the same set of features . as can be seen , the difference is most prevalent for the cleaned model , which shows that once tgen has been cleaned , it is still unable to perform as well as the original . moving onto meteor and rouge - l , we observe that the former performs better than the latter on both datasets when the training data are combined with the best performing feature set . table 1 shows that for both datasets , adding out - of - sample data helps predictive performance .
table 1 shows the comparison of the original e2e dataset with the cleaned version . the difference in mrs between the two sets is less pronounced for the train dataset , but still significant ( 17 . 5 % vs . 11 . 5 % ) . as expected , the number of distinct mrs in the train dataset is higher than the original , both because the training set is larger and because there are more references in the test dataset . ser also drops significantly for the dev dataset .
we compare against the original and the best performing add - on systems . the results are presented in table 3 . as can be seen , the difference between the two approaches is minimal , however we see significant difference in bleu score due to different feature sets being used on different test sets . adding all the features from one system to another achieves only marginal improvement . this is mostly due to small size of the data set ( low some models that perform poorly on add - ons are able to regain a lot of accuracy when trained and tested on a larger dataset ( e . g . , meteor , rouge - l , nist ) .
the results of manual error analysis of tgen are shown in table 4 . it can be seen that there are a considerable number of errors in our system that are caused by small errors in the training data . adding the correct values for some instances ( for example , 22 instances ) causes the system to misfire , and for others ( 14 instances ) it corrects the wrong values .
the performance of these models is presented in table 3 . all models show small improvements relative to the previous state of the art on both datasets . the largest gains are seen on the seq2seqk dataset , which shows that all models perform better when trained and tested on the same dataset ( table 3 ) .
as shown in table 2 , both ensemble and single - model models achieve better bleu scores than the best performing ensemble model on amr17 . though the number of parameters in our model is smaller than seq2seqb , it achieves competitive or better results than both ensemble models . we find that the smaller size of the model does not impact performance , as the results show , once all the parameters are in , the model performs optimally .
we compare our model with previous models on both english - and czech - language datasets . the results are presented in table 1 . as a baseline , we also compare against bow + gcn ( bastings et al . , 2017 ) and seq2seqb . the first set of results show that the former is more accurate on both languages than the latter . the second set shows that , when trained and tested on the hidden test set of bow + , both the average number of instances for each language is higher than the average of the single model .
the effect of the number of layers inside our model is shown in table 5 . we observe that for each layer , there is a drop in performance relative to the previous state of the art .
comparisons with baselines are shown in table 6 . the first group shows that gcns with residual connections perform better than those without . moreover , the performance gap between rc and rc + la is less pronounced for gcns that are more derived from residual connections . adding rc improves performance for both groups , we find that the relative improvements are larger for the gcn containing residual connections than those with unregarded connections .
the experimental results of all models are shown in table 4 . as can be seen , the smaller performance gap between dcgcn ( 1 ) and non - dcgcn models indicates that there is a need to design more sophisticated neural networks and to refine these models for future work .
we also performed an ablation study on the dev set of amr15 . in table 8 we show the results of removing the dense connections in the i - th and iii - th blocks . the results show that the model performs better than the dcgcn4 model with a reduction of 10 % in the density of connections .
we also performed an ablation study for modules used in the graph encoder and the lstm decoder . as table 9 shows , both approaches yield strong baselines comparable to the best previous approaches . the results of " - linear combination " and " - direction aggregation " achieve gains over both the naive approach and the best generalization scheme , we find that both approaches give a significant performance gain when using only one domain representation .
table 7 shows the performance of different initialization strategies for different probing tasks . our paper shows that glorot and topconst achieve high performance , both when combined with other sophisticated initialization schemes , and when used alone .
we observe that the h - cbow model outperforms both the cbow / 400 and h - cmow variants when trained and tested on the hidden test set of somo . it achieves state - of - the - art results , outperforming both published and unpublished work on every metric by a significant margin . as can be seen in the results presented in table 1 , the smaller size and type of dependency trees indicate that the dependency trees are more useful for production use .
the results are shown in table 5 . we observe that our method outperforms the best previous approaches across the five sub - criteria . it achieves state - of - the - art results , outperforming all the other methods except sst2 .
table 3 : scores on unsupervised downstream tasks attained by our models . as can be seen , the cbow and cmp . ow models have seen considerable performance improvement over the best performing method ( hochreiter and schmidhuber , 1997 ) . hybrid also achieves gains relative to cbow , but these are less significant than those on sts13 . though cmp . ow has seen a drop in performance over the years , it is still comparable with cbow on some of the most difficult tasks .
table 8 presents the scores for initialization strategies on supervised downstream tasks . our system outperforms glorot and trec with a large margin . it achieves state - of - the - art results across all three sub - phrases , outperforming all the alternatives except for sst2 .
table 6 shows the performance of each method for different training objectives on the unsupervised downstream tasks . the cbow - r approach achieves gains over the best performing method with a gap of 10 . 2 points from the last published results .
we compare our proposed method with the best performing baselines on the hidden test set of somo . in particular , we compare it against the following baselines : subjnum , topconst , and coordinv . the results are shown in table 3 . sub - jnum and length are the most important factors in the model ' s performance . they are predicted to result in significantly better precision than those of other baselines . topconst is the only one that performs consistently better than cbow - c .
we observe that the best performing method is the cbow - c variant . it achieves state - of - the - art results across all three sub - criteria , outperforming all the other methods except for sst2 .
the experimental results of all models are shown in table 3 . in all but one case , the system performs better when trained and tested on all loc and misc datasets . supervised learning outperforms all supervised learning methods except for name matching . it achieves the best results with a gap of 10 . 03 points from the last published results ( tmtmil - nd ) on the e + loc and per datasets , and is nearly 5 points better on the misc dataset . name matching is the only part of the model that performs much worse than supervised learning .
uncertain in low - supervision settings . results on the test set are shown in table 2 . in all settings , our system achieves high f1 scores , outperforming both supervised and unsupervised learning models . with respect to name matching , the system achieves the best f1 score of 15 . 03 ± 0 . 59 in inference . however , when trained with the best performing feature set , it gets only a marginal improvement of 2 . 42 ± 0 . 03 over the best baseline . supervised learning achieves the highest f1 and r scores , showing the extent to which the model can rely on superficial cues . we conjecture that this is due to the large variation in feature set between training and test set size .
table 6 presents the results on paragraph selection . our model obtains the best results . it significantly outperforms both the best previous models and performs on par with the best non - g2s model .
we compare our model against previous stateof - the - art on the ldc2015e86 and ldc2017t10 datasets . for the former , we note that g2s - gat is signifi cantly better than s2s on both datasets and outperforms both published and unpublished work on every metric by a significant margin . on the other hand , we observe that gat is less effective on the smaller scale , which indicates the non - triviality of gat . we compare against previous models on the larger scale , as these have been trained on larger datasets . the results are summarized in table 3 . our model achieves the best results , outperforming all the previous models by a noticeable margin . note , however , that the gat model is trained on a significantly larger corpus .
the results on the test set of ldc2015e86 are shown in table 3 . the g2s - ggnn model improves upon the baseline with additional gigaword data . it achieves state - of - the - art results , outperforming both the best previous models by a noticeable margin .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model ( get ) obtains substantial gains over the best previous state - of - the - art model on three of the four scenarios . on the fourth scenario , it achieves a marginal improvement of 1 . 42 points over the previous state of the art model .
we noticed that g2s - gat had the highest correlation with sentence length , sentence length and average number of letters per sentence , so we observed lower correlation with s2s .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( added ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . s2s outperforms g2s - gat when we add the missing elements from the input graph . as the table shows , the ability to edit the output once the tokens are added to the output is a major advantage . we also compare our model with other approaches that rely on syntactic or semantic information augmentation . we find that our model performs better than the other approaches when using only reference sentences . though the accuracy of our model is slightly higher than other approaches , it should be noted that it is trained on a larger corpus .
table 4 shows the pos and sem tagging accuracy for different target languages trained on a smaller parallel corpus . it can be observed that the pos features have high correlation with the semantic features extracted from the 4th nmt encoding layer , indicating that the features are of high quality . however , sem is only comparable with the pos feature , showing that it is less accurate than pos .
table 2 compares the pos and sem tagging accuracy with baselines and an upper bound . we use the best performing method , word2tag , which verifies the effectiveness of embeddings without supervision . word2tags also outperforms mft in terms of pos tagging accuracy .
the performance of our system on the four metrics is presented in table 4 . we present the results for pos tagging accuracy , pos accuracy and sem tagging accuracy . the system performs well on all metrics with two exceptions . it achieves the best results with the improvement of 3 . 8 % on average compared to previous models .
we investigate pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . table 5 shows that the first layer of our system achieves the best performance with respect to both tags . the second layer is closer to the expected level with 94 % accuracy .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . as shown in the table , the presence of gender and race features seem to have little effect on the task performance , however , age and sentiment are the only two groups that consistently show significant performance drop . the attacker also shows lower accuracy compared to the baseline on three of the four datasets . sentiment is the only group that shows significant performance improvement .
the performance of these models on a single task is shown in table 1 . as can be seen , all the features we consider have high accuracy . gender bias , age and classifiers have the least and the presence of race and gender have the most accurate features .
we show the results for tasks that are balanced and unbalanced , and those that are unbalanced . the results are presented in tables 2 and 3 . as these data splits show , there is a significant imbalance in the distribution of data that is caused by the presence of different protected attribute leakage . in general , the more balanced datasets tend to have less data leakage , but not always .
the performance of these models on different datasets is shown in table 3 . as can be seen , all the features cause a significant drop in performance when the attacker is trained with an adversarial training . gender bias also contributes significantly to the performance of the models , as it reduces the recall score for some features . finally , the presence of race and gender bias further decreases performance . since these features are relatively new to the model , they have not been tested before .
we investigate the impact of different encoders on the model in table 6 . we find that the guard - free embeddings perform better than the ones that are already protected . the difference is most prevalent for rnn ,
we compare our proposed approach against two baselines - ptb and wt2 - with the best performing model , yang et al . ( 2018 ) . the results are presented in table 2 . the first set of results show that the model performs better when trained and tested on a larger corpus . the second set shows that it is more stable and therefore requires less training data . we benchmark against the following baselines : ptb base , wt2 + finetune , and lstm . the results , summarized in the table 2 , are broken down in terms of parameter accuracy . as can be seen , the smaller size of the base set and the number of parameters suggests that there are not enough data to pretrain the model , hence leading to incorrect predictions . also , we observe that the training data size range is relatively consistent across all baselines , with the exception of wt2 .
the best results for this model are reported in table 2 , where we report both the base time and the average number of iterations for each parameter as reported in the previous work . the results are presented in bold . as a sanity check , we also include the time - averaged average of both base acc and time taken to compute each parameter for each iteration , as this is the average time weighted over multiple iterations of the model without adding redundancy . table 2 compares the performance of these models with previous work on various benchmarks . we observe that , let alone a drop in performance , our model obtains the best results . it can be observed that the model is more than 4 . 5x faster when trained and tested on the same dataset with different combinations of parameters .
the best results for yelppolar and amafull time are reported in table 3 . we benchmark against three baselines : amapolar err , yahoo time , and trip advisor time . the results are presented in bold . as can be seen , both the average time taken to compile and the number of tweets per second for each baseline are significantly higher than the previous state of the art . table 3 compares the performance of these models on different benchmarks . for yelp , we maintain performance at the level of the state - ofthe - art , with a marginal drop of 0 . 36 % compared to previous work . this is mostly due to small size of the dataset ( around 74k tweets ) and high correlation with human judgement . also , we note that the training set size is small , which indicates that there are not enough data to pretrained models to pretrain the model .
we show the bleu score of our model as well as the time to decode one sentence on the training set of newstest2014 dataset in table 3 . as these models use multi - params , each training step takes a significant amount of time to train . since the training time is relatively short , we used only seconds to decode each sentence . though the number of tokens in question is small , we managed to obtain a decent case - insensitive case - inference score of 27 . 57 % on the wmt14 english - german translation task . table 3 shows that our model can easily distinguish between the true meaning of the sentence and the negative connotation .
we also include the exact match / f1 - score of our model on squad dataset in table 4 . it can be seen that all the parameter combinations we considered had good match rates with a minimum of 75 % f1 score . however , when we added elmo as a parameter , our model had the worst performance . we observed that only the presence of elmo in the parameter set mattered , and only when it was considered in combination with other required parameters . finally , we included the score of " match rate " for each parameter as a metric for scalability table 4 shows that the combination of the parameter number of base . measure and elmo significantly improved the model ' s performance .
we present the f1 score of our model on the conll - 2003 english ner task in table 6 . as the table shows , all the parameter numbers had a significant impact on our model ' s performance . although the number of parameters in question was small , it did not harm the model , it outperforms other models which had larger parameter numbers . lrn also outperforms the other models with a large margin . table 6 shows that lrn significantly boosts performance for non - rln based models . it also improves for lrn - based models when using atr as a parameter . though the improvement is slim , it is encouraging to continue researching into this area
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . with the base - based setting , our model obtains 85 . 26 % accuracy . with the perplexity setting , it gets 169 . 81 % . the difference is less pronounced with glrn , but still shows significant difference .
table 1 shows the system performance of all systems trained and tested on the word analogy task . we benchmark against the following features : system retrieval , mtr and sentence prediction . for brevity we only show results for systems trained on b - 2 and b - 4 . tweets from one domain are considered while those from other domains are considered for both systems . the results are presented in table 1 . word analogy task is performed on both systems with two stages . the first stage consists of word analogy tasks . the second stage is word prediction . it consists of sentence prediction tasks . we show both of these stages in the unsupervised setting , with different combinations of words used for each stage . the goal is to achieve high precision with a minimum of 80 % on average . since the average number of words per sentence is relatively low , this means that there is a high chance of mis - matching . we do not have significant performance drop with respect to sentence prediction using the second stage .
the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that our system is well - equipped to perform in the low - resource settings . the second best result is achieved by candela ( 30 . 2 % ) on the k 500 test set , which shows the high quality of its output . although seq2seq achieves lower k 500 than human , it is still comparable with human on most aspects ( gram , appropriateness , and content richness ) . retrieval , on the other hand , is only comparable with humans on two of the four aspects ( appr and cont . ) it achieves the highest score on content richness , showing the extent to which the system can rely on syntactic or semantic information extracted from supporting documents . finally , the highest percentage of evaluations a system receives is 2 . 2 points , showing a system being ranked in top 1 or 2 for overall quality . table 4 presents the results of human evaluation . top - 1 / 2 : % of evaluations received by human .
the performance of these models on the test set is presented in table 6 . we observe that , let alone a reduction in performance , all models perform comparably to each other when trained and tested on the ted talks dataset . moreover , the gap between en and pt is less pronounced for ted talks than for other models , indicating that the training set is more suitable for production use . as can be seen in the second group of table 6 , the model trained on the europarl dataset outperforms all the other models except docsub .
the performance of these models on the test set is presented in table 6 . we observe that , let alone a reduction in performance , all models perform comparably to each other when trained and tested on the ted talks dataset . moreover , the gap between en and pt is less pronounced for ted talks than for other models , as can be seen in the second group of table 6 , both the average p and r scores of all models are slightly higher than the average of any other model except for docsub .
the performance of these models on the test set is presented in table 6 . we observe that , let alone a reduction in performance , all models perform comparably to each other when trained and tested on the same dataset . as can be seen , the smaller performance gap between en and pt shows that the model trained on the ted talks dataset is less likely to receive significant performance improvement . also , when tested on a larger corpus , such as df , docsub and tf , the gap is less pronounced . finally , we observe that for both datasets , our model performs slightly better than the other two .
we benchmark against five baselines : dsim , tf , docsub , df , df and hclust . the results are shown in table 1 . as can be seen , all the metrics we consider have low correlation with the average depth of the original embeddings . europarl , in particular , has the worst performance .
we benchmark against five baselines : dsim , tf , docsub , df , df and hclust . the results are shown in table 1 . as can be seen , all the metrics we consider have low correlation with the average depth of the roots . europarl , in particular , has the worst correlation with average depth .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . 3 are shown in table 1 . the enhanced version of our model ( lf ) outperforms the baseline model by a noticeable margin . although the improvement is slim , it is encouraging to continue researching into this direction as it shows that further improvements are possible with a reasonable selection of the right combination of loss functions . as hardmax loss is rare in the vsdial set , we do not have significant performance improvement . however , we managed to show a slight improvement with p1 as well , which shows that it is possible to improve the feature extraction procedure for future work .
performance ( ndcg % ) of ablative studies on different models on the visdial v1 . 0 validation set is shown in table 2 . the best performing model is coatt . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . further applying p1 and p2 shows a significant drop in performance , which indicates that the use of p2 has a high impact on the model performance .
we notice that the hmd - f1 model significantly outperforms the other baselines on both soft and hard alignments . the results are summarized in table 5 . hmd pre - trained with bert outperforms all the other models except wmd - bigram ,
the results are shown in table 1 . we compare against the baselines on the direct assessment and sent - mover tasks . the average score of bertscore - f1 is 0 . 685 while the average of smd and w2v are 0 . 86 . also , we compare against ruse ( * ) as a baseline because it has higher correlation with direct assessment . as the results of the second set of experiments show , when trained and tested on the same dataset , the bert score can vary significantly depending on the setting .
the experimental set of bertscore - f1 achieved the best results with a f1 - score of 0 . 178 on the " inf " and " qual " benchmarks , while the " sent - mover " set achieved the highest f1 score . from this group of metrics , we can observe that the three sets of metrics are comparable in difficulty when trained and tested on the same dataset . the difference is most prevalent for " inf " .
word - mover with different feature sets outperforms other approaches which allow bertscore - recall to exceed 0 . 9 . on the other hand , when using spice as a baseline , it achieves 0 . 723 and 0 . 949 better results . we find that the semantic features derived from spice are particularly useful for the task as they reduce recall and improve the recall scores of the moving targets . when using word2vec feature - values , we get 0 . 792 and 0 . 949 better scores on m1 and m2 scores , respectively , compared to the previous methods .
we further compare these models with each other on the hidden test set of simnet . the results are shown in table 6 . as can be seen , the models using both meta - and meta - para - based features achieve better results overall when the meta - based feature is added . moreover , the performance reach its peak when the feature - based model is combined with the lexical features of both shen - 1 and max - trained models .
the results are shown in table 3 . we observe that yelp significantly outperforms the best previous approaches across all three transfer quality measures . the transfer quality and semantic preservation scores are the most consistent , with yelp achieving the highest transfer quality . semantic preservation is the only part of the data that is consistently better than the other two . finally , we observe that the variation of the transfer quality between models is less pronounced for yelp , indicating that the model is more sensitive to semantic trends .
table 5 presents the results of human evaluation for each metric . it can be seen that both summaries generated by the machine and the human are comparable in terms of accuracy , with the exception of paragraph selection . however , human evaluation results are slightly higher than those by machine for three of the four metrics . this indicates that human evaluation methods are more effective in generation of accurate summaries .
we further compare our model with other models trained only on simnet . the results are shown in table 6 . we observe that the model performance obtained with the alias " cyc + para " is consistently better than the model with no alias at all . moreover , the improvement is much larger when the model is trained with both meta - and meta - para features . as can be seen , the average number of points scored for each model is slightly higher when the alias is added than when it is removed .
results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences , are shown in table 6 . as these results show , the transfer quality of the best models is achieved when acc is used as a weight for both transferred and untransferred sentences . note that the method used for this experiment relies on lexical features such as the style embeddings and the delete / retrieve function , and therefore requires significantly more data than is available in the standard corpus . further , the classifiers used in this experiment are significantly worse than those in the previous work . we find that combining all the features improves acc by about 2 points , though still performing substantially worse than simple - transfer . since this is a relatively small corpus , we include only results from transfer with human references as those results are not considered in this table .
in table 2 we report the percentage of reparandum tokens that were correctly predicted as disfluent . reparandum length is the average of the number of tokens in a sentence , and number of repetition tokens . as this table shows , both types of disfluencies have low correlation with repetition , indicating that they are difficult for the model to predict .
we note the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) . as shown in table 3 , more than half of tokens in each category belong to each category , and more than 50 % belong to the third category . however , for those that are in the repair category , only 39 % of tokens are predicted to belong to either category .
we show the results of models trained on tweets from one domain and tested on all tweets from other domains , with different features added for each domain . the results are presented in table 2 . we observe that the text transformation is beneficial for both domains with different feature sets contributing differently to the model ' s performance . for example , text transformation with innovations improves the dev mean of both test sets , but does not improve the best performing feature set . moreover , the average number of iterations for each feature is slightly higher than the average of the previous model , indicating that the feature - rich text transformation contributes differently to both domains .
performance comparison of our model with the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model shows marked performance improvement . it closely matches the performance of the best previous embeddings . in fact , it achieves close to the level of accuracy of the state of the art .
table 2 shows the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . in fact , it achieves higher accuracy than any previous approach .
we investigate the effectiveness of both word attention and graph attention for this task in table 3 . the first set of results show that both approaches have comparable performance , however the accuracy is higher for graph attention . the second set shows that neural dater is more accurate than oe - gcn both with and without attention .
the experimental results of all models are shown in table 1 . embedding + t achieves the best results with a gap of 10 . 5 points from the last published results ( hochreiter et al . 2008 ) . it closely matches the performance of dmcnn with only 0 . 3 points absolute difference . while the gap is modest , it is significant with respect to overall performance , with jrnn achieving gains of 2 . 6 points on average over the previous state of the art .
table 1 presents the results on event identification and event classification . we show that our method is comparable to state - of - the - art methods across all three domains , with the exception of argument identification . the system performs well on both event and classifier - based triggers , with a gap of 10 . 5 points from the last published results ( kutuzov et al . , 2016 ) on the threshold for each event . on the other hand , cross - event event identification is only slightly better than chance at predicting event events .
consistent with the observations by vaswani et al . ( 2017 ) , all models show lower precision on the test set when trained and tested with the same language subset . the best results are obtained by the spanish - only model , which achieves 60 . 72 % accuracy on average . however , when trained with english - only - lm , the accuracy is only 58 . 03 % . as the results of fine - tuning the model get worse , we notice that it is harder to fine - tune than before . we find that the best performance is obtained using the shuffled - lm variant , which shows the advantage of finetuning the model after training .
results on the dev set and on the test set using only subsets of the code - switched data are shown in table 4 . fine - tuned models perform better than cs - only on both sets . as hard coreference problems are rare in dfgn , we do not have significant performance improvement . however , fine - tuned models continue to improve over the best performing baseline .
we find that fine - tuning the feature - rich rewards for each gold sentence improves the model ' s performance on both test and dev set . as hard coreference problems are rare in the standard task formulation , we do not have significant performance improvement . however , fine - tuned - disc feature - values improve over both monolingual and code - switched sentences .
we show precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvement from baseline to the current state - of - the - art is statistically significant ( table 7 ) .
we show precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset as well as the corresponding embeddings for the grusky et al . ( 2018 ) model . the improvement in precision from baseline to type combined is statistically significant ( t - test , p < 0 . 01 ) .
results on the test set of belinkov2014exploring are shown in table 1 . the first set of results show that glove - extended refers to the synset embeddings obtained by running autoextend rothe and schüze ( 2015 ) on wordnet 3 . 1 , and the second set shows that it is comparable to wordnet , verbnet and syntactic - sg ( see x4 ) . the third set shows the results of applying hpcd ( full ) to the original paper , and it uses syntactic skipgram . as these tools use pascal - voc , they have somewhat higher performance on the original test set , but still do not exceed the upper boundary of the expected 80 % on the ppa test set .
results reported in table 2 show that the system performs well with all the features coming from various pp attachment predictors and oracle attachments . hpcd even outperforms rbg when using only ontolstm - pp as an attachment .
the results of removing sense priors and context sensitivity ( attention ) from the model are shown in table 3 . as these features remove context sensitivity , the model achieves a significant improvement in precision .
we show the bleu % scores of incorporating subtitle data and domain tuning for image caption translation in table 2 . as this table shows , the combination feature further improves the results for both en - de and mscoco17 models . the improvement is larger when the domain tuning is added than when it is subtracted . adding subtitle data improves both the generalization and the sub - domain performance of the model , as expected , the improvement is greater when the ensemble is added .
we find that the domain - tuned models perform better than the plain h + ms - coco model on three of the four datasets . the results are particularly striking for flickr16 , 17 , mscoco17 and subs1m . with the label - free model , t performs better than both en - de and en - frager ( a ) .
we also show bleu scores in % for adding automatic image captions ( only the best one or all 5 ) . the table shows that the combination feature improves the general performance of all models . adding only the best five captions improves performance slightly , but still puts the model slightly worse than the others .
we include the bleu % scores of all strategies for integrating visual information in table 5 . the first group shows that enc - gate and dec - gate achieve better results than en - de and mscoco17 on flickr16 and 17 . moreover , the improvement is larger when using transformer , multi30k + ms - coco + subs3mlm , detectron mask surface , and founta et al . ( 2018 ) . the second group of results show that both strategies achieve gains with the improvement of visual information with each layer of masking . finally , we compare our approach against the best previous approaches .
we observe that the ensemble - of - 3 approach achieves the best results . it verifies the effectiveness of the vsms - coco feature - rich model , as hard core visual features alone do not improve the results for any model , we do not consider them as a lower - bound on performance . rather , we consider their effect on generalization . the results are shown in table 3 . the first group of results show that incorporating all the visual features bridges the gap between the performance of " good " and " bad " . subsequently , we also consider the effect of multi - lingual features , again , we observe that for all models , the effect is less pronounced for those using ms - cocao features .
we compared these models on the test set of hotpotqa in the distractor and treehugger setting , respectively . for both sets , en - fr - ht and en - es - ht achieve better results than either mtld or ttr . as can be seen in table 1 , the smaller performance gap between the two sets is mostly due to the smaller size of the training set , as expected , once the training data are merged , the performance of both models drops significantly . in addition , the clustering quality drops significantly for both sets .
the number of parallel sentences in the train , test and development splits for the language pairs we used is shown in table 1 . as this table shows , there is a significant imbalance in the number of sentences that are in the training and development sets , which indicates that the model is unable to learn the task effectively .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . as the table shows , the difference in performance between the two sets is minimal , which indicates that the models are well - equipped to handle the task .
automatic evaluation scores for the rev systems are shown in table 5 . as can be seen , the en - fr - rnn - rev and en - es - smt - rev systems perform comparably to each other when trained and tested on the same dataset ( table 5 ) . however , when trained on a larger corpus , the performance gap is much larger . perhaps this indicates that the semantic information injected into the system by the additional cost term is significant enough to result in a significant improvement in performance .
results on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com . it achieves the best recall @ 10 and average recall @ 0 . 0 , which shows that the model is well - equipped to perform this task . the mean mfcc score of the vgs model is 0 . 7 , slightly higher than the average of 0 . 2 for rsaimage .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the audiovisually supervised model . the average recall @ 10 is 0 . 0 , which shows that the acoustic features captured by the vgs model have a significant impact on the performance .
we report further examples in the appendix . originally , all the classifiers used for this experiment were adapted from the original sst - 2 source . since the new dan model only works on unigram - based scripts , we had to adapt some of the examples . as a result , we have to add some examples for each classifier as shown in table 1 . in most cases , the difference between the average sentence of the different classifiers is less pronounced for dan than for cnn . still , for rnn , it is much more consistent . it turns in a screenplay that is slightly longer than the original , about 260 words .
part - of - speech changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . as can be seen in table 2 , the percentage of instances in the original sentence has increased , but remains the same as before . also , the average number of instances per sentence has decreased as well .
sentiment score changes in sst - 2 . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the flipped labels cause the sentiment score to increase as well . as the table 3 depicts , the flipped labels have a significant impact on sentiment score .
most importantly , we compare against the best performing method , pubmed , which verifies the effectiveness of our approach . results tabulated at table 1 shows that pubmed outperforms both published and unpublished work on every metric by a significant margin .
