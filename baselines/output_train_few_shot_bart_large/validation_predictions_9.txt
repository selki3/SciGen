table 2 : throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . the iterative approach outperforms the recursive framework on both inference and training , with a performance gain of 3 . 2 instances / s in inference and 4 . 6 examples / s on training .
table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w
the max pooling strategy consistently performs better in all model variations . for example , ud v1 . 3 , conll08 , sb and sigmoid models perform better than all the other models with different representation combinations .
table 1 shows the effect of using the shortest dependency path on each relation type . our macro - averaged model obtains the best f1 score in 5 - fold test set without sdp and with sdp .
the results are shown in table 3 . we observe that the performance gap between the best performing models on r - f1 and f1 50 % is narrower than that on y - 3 : y , with the former achieving 50 % better f1 score and the latter 50 % higher than the former .
table 3 shows the performance of our model compared to the best state - of - the - art parser , mst - parser . our model achieves the best overall performance on all three metrics , with an absolute improvement of 50 % on average .
table 4 shows the c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level and paragraph level . the average score for both systems is 60 . 40 ± 13 . 57 % on average . note that the mean performances are lower than the majority performances over the runs given in table 2 .
table 3 shows the bleu , meteor , rouge - l , and cider scores for the original , clean - up , and add model on the train and test set , respectively . the original model significantly outperforms the cleaned model on all metrics , while the difference between the two is less pronounced for the add model .
table 1 compares the original e2e dataset with our cleaned version . the difference in number of distinct mrs , total number of textual references , and ser as measured by our slot matching script is statistically significant ( p < 0 . 00 ) for both datasets .
table 3 shows the bleu , meteor , rouge - l , and cider scores for the original and the add model on the train and test set , respectively . the original model outperforms the original model on all metrics except for bneu , where it is slightly better than the original . the difference between original and add model is less pronounced , but still significant , with tgen + surpassing original model by 2 . 83 points .
table 4 : results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) . the number of errors per example is shown in table 4 . for each error we found , we show the number of instances with the correct values in the training set and the total number of tokens in the corrected set .
table 3 shows the performance of our model compared to all the stateof - the - art models on the single - domain and ensemble datasets . our model significantly outperforms all the other models on both datasets except for tree2str , which achieves the best performance on the ensemble dataset . on the other hand , our model performs significantly worse than the best previous state - ofthe - art model , pbmt .
table 2 shows the performance of our model on amr17 . our model achieves 24 . 5 bleu points . by comparison , the best performance by any ensemble model is achieved by seq2seqb ( beck et al . , 2018 ) , which achieves 28 . 5 points . the best performance achieved by our model is by damianonte and cohen , 2019 , which achieves 27 . 6 points . similarly , our ensemble model achieves 27 points .
table 1 shows the performance of all models on the english - german and czech test sets . the models trained on birnn + gcn ( bastings et al . , 2017 ) outperform all the other models except for seq2seqb , which achieves the best performance .
table 5 shows the effect of the number of layers inside the network on the performance of our model . our model obtains the best performance with n = 6 layers , and m = 3 . 5 .
table 6 compares the performance of our model with the baselines for the four gcns with residual connections . the results are shown in bold . our model outperforms all baselines except for the dcgcn3 model , which is comparable to the best state - of - the - art model .
table 3 shows the performance of our model with respect to all metrics . our model outperforms all the state - of - the - art models on all metrics except d and b .
table 8 : ablation study for density of connections on the dev set of amr15 . our model obtains the best result with an ablation score of 25 . 5 / 55 . 1 for the i - th dense block .
table 9 : ablation study for modules used in the graph encoder and the lstm decoder . the results are shown in table 9 . our model obtains the best performance with respect to coverage and attention on both datasets .
table 7 : scores for initialization strategies on probing tasks . our paper achieves the best performance , with a score of 35 . 8 / 71 . 9 on the depth and tense metric , while glorot achieves the highest score of 62 / 59 . 3 on the topconst metric . subjnum and topconst also receive high scores .
table 3 shows the results of our h - cmow model compared to the h - cbow model . our model outperforms both the previous state - of - the - art models on all metrics except bshift and subjnum by a significant margin .
table 3 shows the performance of our method compared to the state - of - the - art models on the three sub - test sets of sst2 , sst5 , and sts - b . our model outperforms all the other models except for sick - e , which achieves a marginal improvement of 0 . 2 % over the previous state - ofthe - art model .
table 3 : scores on unsupervised downstream tasks attained by our models . hybrid models outperform cbow and cmow on all but one of the four downstream tasks ( sts13 , 14 , 15 , and 16 ) . for sts16 , hybrid models perform slightly better than cbow , but still outperform cmow .
table 8 : scores for initialization strategies on supervised downstream tasks . our system outperforms glorot and trec by a large margin . it achieves the best performance among all the models on all three downstream tasks , with an absolute improvement of 3 . 6 points over the previous state - of - the - art model .
table 6 shows the scores for different training objectives on the unsupervised downstream tasks . the cbow - r model outperforms the cmow model on all the downstream tasks except for sts13 and sts16 .
table 3 shows the performance of our method compared to the state - of - the - art cbow - r model . our method outperforms all the other approaches except subjnum and topconst by a significant margin . the difference is most striking in the subtasks of bshift and tense , where our model obtains the best performance .
the results are shown in table 3 . the best performing method is the cmow - r model , which achieves 90 . 6 % accuracy on all three sub - criteria .
table 3 shows the performance of our model on all loc and misc datasets . our model outperforms all the state - of - the - art supervised and unsupervised models on all metrics except for name matching , where it performs slightly better .
table 2 : results on the test set under two settings . our model achieves the best performance with an f1 score of 42 . 42 ± 0 . 59 on the in - in test set and a full score of 69 . 38 ± 1 . 03 on the all r and f1 test set , respectively . the model with the highest f1 scores , τmil - nd ( model 2 ) , obtains the best overall performance with a final score of 71 . 59 ± 1 point .
table 6 : entailment ( ent and con ) for the models trained on the g2s - gat dataset compared to those trained only on the ggnn dataset . the results are shown in table 6 . our model outperforms both the previous state - of - the - art models on both ref and s2s with a large margin .
table 1 compares the performance of our model with previous models on the ldc2015e86 and ldc2017t10 datasets . our g2s model outperforms all the previous models except konstas et al . ( 2017 ) and cao et al . ( 2018 ) . it also outperforms the models by a significant margin on both datasets . we observe that our model performs slightly better than the previous state - of - the - art models on all datasets except ldc2016e86 , where it performs slightly worse .
table 3 : results on ldc2015e86 test set when models are trained with additional gigaword data . our model outperforms the previous state - of - the - art g2s - ggnn model by a large margin . the model achieves a final accuracy of 32 . 23 % on the test set .
table 4 shows the results of the ablation study on the ldc2017t10 development set . our model significantly outperforms the previous state - of - the - art models in terms of bleu , meteor and size .
we observe that g2s - gin outperforms all the other models with a margin of 2 . 51 % and 3 . 43 % , respectively , on average .
table 8 : fraction of elements in the output that are not present in the input ( added ) and the fraction of words missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . s2s and g2s - ggnn are the only models that use the gat model as the input , and the only ones that use gin as the output .
table 4 shows the pos and sem tagging accuracy using different target languages on a smaller parallel corpus ( 200k sentences ) . our model outperforms all the state - of - the - art models on all but one of the four target languages .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . word2tag significantly outperforms the unsupervised embeddings on both baselines , with an accuracy of 91 . 11 % and 91 . 41 % , respectively .
table 4 shows the performance of our model on the four types of pos datasets . our model achieves the best performance on all metrics , with an accuracy improvement of 3 . 9 % on average compared to the previous state of the art model .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . our model obtains the best results with a score of 87 . 9 % for pos and 87 . 6 % for sem .
table 8 : attacker ’ s performance on different datasets . results are on a training set 10 % held - out . as shown in the table , the difference between the attacker score and the corresponding adversary ' s accuracy is small , but still significant .
table 1 : accuracies when training directly towards a single task . our model outperforms all the state - of - the - art models with a large margin .
table 2 : protected attribute leakage : balanced & unbalanced data splits . our model outperforms all the baselines on all three attributes except gender and age .
table 3 shows the performance of our adversarial model on different datasets with an adversarial training set . our model outperforms all the state - of - the - art models on all three datasets except for pan16 .
the results are shown in table 6 . the rnn encoder significantly outperforms the guarded encoder in terms of decoding the protected attribute .
table 1 compares the performance of our model with previous work on the dynamic and finetuned variants of the wt2 dataset . our model achieves the best results on both datasets , outperforming the previous state - of - the - art models by a significant margin . the difference between our model and previous work is most striking when we consider the size of the training set and the number of parameters for each parameter , as shown in yang et al . ( 2018 ) . our model obtains the highest performance on the two datasets , surpassing the previous best state - ofthe - art model by 3 . 36 points .
table 1 compares the performance of our model with previous work on the lstm model by rocktäschel et al . ( 2016 ) and this model . the results are shown in table 1 . as the table shows , our model performs significantly better than the previous state - of - the - art model on all three metrics .
table 1 compares the performance of our model with previous work on the amapolar , yahoo time and yelppolar time datasets . the results are presented in bold . our model outperforms all the previous models except for zhang et al . ( 2015 ) by a large margin . the difference between our model and the previous state - of - the - art model can be seen in table 1 . our gru model achieves the best performance with an err of 4 . 836 on the three datasets .
table 3 shows the bleu score of our model on the wmt14 english - german translation task on the newstest2014 dataset . our model obtains a case - insensitive tokenized score of 26 . 67 on the german translation task and a corresponding 26 . 45 bleu score on the english translation task .
table 4 : exact match / f1 - score on squad dataset . our model obtains the best performance with a f1 score of 76 . 14 / [ 83 . 41 / [ 79 . 83 ] on average . as the results show , the combination of the parameter number of elmo and the number of parameters contributed by the model improves the model ' s performance . however , the model still performs significantly worse than the previous state - of - the - art model , with an f1score of only 74 . 83 / [ 78 . 45 / 83 . 50 ] on the squad dataset , indicating that the model is not well - equipped to handle large datasets .
table 6 shows the f1 score of our model on the conll - 2003 english ner task . lstm * denotes the reported result , and lrn denotes the unsupervised model . lrn has the best performance with 90 . 56 f1 . as shown in table 6 , lrn outperforms all the other models with a large margin .
table 7 : test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . lrn model significantly outperforms elrn and glrn on both snli and ptb tasks .
table 1 shows the performance of our system with and without oracle retrieval on the word embeddings for english , spanish , french , dutch , russian , turkish , russian and turkish . our system outperforms all the alternatives with a large margin . the average number of words per sentence is 140 , compared to 22 for the others .
table 4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that our system is comparable to the best previous work on all three metrics . the best performing automatic system is candela ( h & w hua and wang , 2018 ) , with a score of 38 . 6 % on the grammaticality and appr measures . the second - best performer is retrieval ( 28 . 2 % ) on the content richness measure . the third - best system is seq2seq ( 25 . 6 % ) .
table 3 shows the performance of all models trained on the en / tf / docsub embeddings . our model outperforms all the other models except for the df model , which is slightly better than the other two . the difference between r and p is less pronounced for df , but still significant .
table 3 shows the performance of our approach compared to the best state - of - the - art models on the test set of english , german , french , dutch , spanish , russian and dutch . our approach outperforms all the baselines except for english , where it performs slightly better than both the best - performing models .
table 3 shows the performance of all models trained on the en / tf / docsub embeddings . our model outperforms all the other models except for the df model , which is outperformed by a large margin . for example , our model beats all the models except df by 2 . 5 points .
table 1 shows the performance of our model on all metrics . europarl outperforms all the other models except for df and docsub . the average depth of our max - depth embeddings is 11 . 05 , and the average number of roots is 8 . 46 , which indicates that our model has a high degree of coverage .
table 3 shows the performance of our model on all metrics . europarl outperforms all the other models except for df and docsub . the average depth of our max - depth embeddings is 9 . 43 / 9 . 29 , and the average number of roots is 2 . 29 / 2 . 43 , which indicates that our model has good coverage .
in table 1 , we compare the performance of our enhanced model with the baseline model on the validation set of visdial v1 . 0 . the enhanced model ( lf ) achieves the best performance with a ndcg % of 73 . 42 % on the question type , answer score sampling , and hidden dictionary learning , compared to 62 . 63 % for baseline .
table 2 shows the performance of the ablative studies on different models on the visdial v1 . 0 validation set . the model with the best performance is the hidden dictionary learning model ( p2 ) , while the model using only p1 has the worst performance ( p1 + p2 ) .
table 5 compares the performance of different approaches on hard and soft alignments . the hmd - f1 model outperforms all the other approaches except for wmd - bigram , which achieves the best performance with 0 . 823 / 0 . 817 on hard alignments and 0 . 864 / 0 . 866 on soft . similarly , the hmd - recall model achieves the highest performance on both alignments , with a gap of 2 / 3 / 4 / 2 on both soft and hardalignments .
table 3 shows the performance of our method with respect to the three baselines and the average score of bertscore - f1 on the direct assessment metric . our method significantly outperforms all the baselines except ruse ( * ) and meteor + + .
the results of bertscore - f1 are shown in table 3 . we observe that bleu - 1 has the best performance with a f1 score of 0 . 176 , while belu - 2 has the worst performance . sent - mover achieves the highest score with 0 . 175 , while w2v achieves the lowest f1 .
the results are shown in table 3 . word - mover and bertscore - recall are the most accurate on m1 and m2 , respectively , while meteor and spice are the worst on m2 . the best performance on both metrics is obtained by word - 2vec , with a gap of 0 . 723 points from the baseline .
the results are shown in table 1 . we observe that the model with the best performance is the shen - 1 model , which achieves the best generalization on the sim and the gm datasets . further , the models with the worst performance on the gm dataset are the ones with the most complex lexical features .
table 3 shows the performance of our model on the transfer quality and semantic preservation metrics . our model obtains the best transfer quality with a transfer quality score of 75 . 7 % and a semantic preservation score of 73 . 4 % . we observe that our semantic preservation model outperforms all the other models with a large margin .
table 5 shows the results of human sentence - level validation on yelp . our model verifies the accuracy and fluency of all metrics , as well as the semantic preservation of the tokens in question . we use spearman ’ s [ italic ] ρ b / w sim and human ratings of semantic preservation for each metric , and negative pp for all tokens in the yelp dataset .
the results are shown in table 1 . we observe that the model with the best performance is the shen - 1 model , with m0 [ italic ] + cyc + para , followed by m1 , m2 , m3 , m4 , m5 , m6 and m7 .
table 6 shows the results for yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( right table ) outperform all the previous work on both datasets with similar amounts of acc , but the difference is less pronounced for the untransferred dataset , indicating that the training data are more suitable for sentiment transfer . the best model , yang2018unsupervised , achieves the highest acc with 22 . 4 bleus on the 300 - sentence dataset , which is comparable to previous work ( yang2018 , 2018 ) .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . for nested disfluencies , the average number of tokens per disfluency prediction is slightly higher than for rephrase tokens , but still comparable to the average for repetition tokens . for both types , the overall prediction accuracy is close to zero .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either the reparandum or repair ( both the content - function ) or in neither . as the table 3 shows , content - content and function - function tokens belong to the same category , but the fraction of tokens belonging to each category is slightly less than in the previous section .
the results are shown in table 1 . we observe that the model with the best performance is the single - sentence model , followed by the best - performing variants of the text + innovations model . the models with the worst performance are the ones with the most innovations , and those with the least .
performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . our model achieves the best performance , with an accuracy of 83 . 43 % on average compared to the state of the art .
table 2 : accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . the ac - gcn model , for example , has the best performance with 69 . 2 % accuracy compared to the previous state - of - the - art model , the attentive neuraldater model with 66 . 6 % accuracy and the maxent - joint model with 62 . 2 % .
table 3 compares the performance of our model with and without word attention for this task . neuraldater outperforms the oe - gcn model with both word attention and graph attention . the difference in accuracy between the two approaches can be seen in table 3 .
table 1 shows the performance of all models on the 1 / 1 , 1 / n and 3 / 10 tasks . the model with the best performance is the jrnn model , followed by the dmcnn model and the argument neural network . the best performing model is the cnn model , which achieves 75 . 3 % performance on both tasks .
table 1 shows the performance of our method compared to the best state - of - the - art cross - event event detection system . our method significantly outperforms all the baselines in terms of both event detection and event prediction .
for english - only and spanish - only languages , fine - tuned models outperform all the other models except for the case of " shuffled - lm " . fine - tuned models perform slightly better than all the others , but still lag significantly behind the original models .
table 4 shows the results on the dev set and on the test set using only subsets of the code - switched data . fine - tuned models outperform cs - only models with 75 % and 75 % train dev on both sets , respectively .
table 5 shows the performance on the dev set and the test set compared to the monolingual and code - switched subsets of the gold sentence in the set . the fine - tuned - disc model outperforms the fine - tuned model on both sets , with an absolute improvement of 3 . 33 points over the original model .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . as expected , the type combined approach significantly improves the recall and precision of our model , and the f1 score of our approach significantly increases as well .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset as inputs for our model . our model significantly improves upon the baseline model with a f1 score of 1 . 35 , a statistically significant improvement over the previous state of the art .
the results on the belinkov2014exploring ’ s ppa test set are shown in table 1 . syntactic - sg embeddings outperform both the original lstm - pp and the glove - extended model on both datasets . the difference between the two models is most pronounced on the wordnet 3 . 1 dataset , where syntacticsg embedding achieves an accuracy of 88 . 7 % compared to the previous best performance of 84 . 3 % . the performance gap between the original model and the model using the glosdk embedding is less pronounced on wordnet , but still significant .
table 2 shows the performance of our system with various pp attachment predictors and oracle attachments . our hpcd model obtains the best performance with 94 . 59 % acc . on the full uas dataset .
table 3 : effect of removing sense priors and context sensitivity ( attention ) from the model . our model achieves the best performance with a precision of 89 . 5 % on the ppa acc . metric .
table 2 : adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun . as can be seen in the table , the domain - tuned model outperforms the en - de model and the multi30k model , both in terms of bleu % and fmeu score .
table 3 shows that domain - tuned models outperform en - de models and mscoco17 models on all datasets except en - fr and flickr17 . the results are statistically significant across all datasets , with the exception of flickr16 .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . the results of en - de and multi - de captions are shown in bold , while en - fr and mscoco17 are in the black - box . adding only the best 5 captions improves the performance for both models , but only marginally for the former .
table 5 : comparison of strategies for integrating visual information ( bleu % scores ) . all results using transformer , multi30k + ms - coco + subs3mlm , detectron mask surface , and en - de encoder are presented in table 5 . our encoder achieves the best performance with a bleu % score of 62 . 38 % compared to the en - fr model ' s 62 . 45 % and the mscoco17 model ' s 69 . 45 % .
the results of en - fr and en - de models are shown in table 3 . multi - lingual models outperform all the other models except for those using ms - coco , which is outperformed by all but one of the three models .
the results are shown in table 3 . the en - fr - smt - back embedding model outperforms all the other models except for translate - f1 , which achieves the best performance with a marginal gain of 0 . 45 points over the enfr - rnn - back model . on the other hand , trans - trans - ff achieves the worst performance with 0 . 59 points . trans - tf - ff achieves the highest performance with 1 . 03 points .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the en – fr and en – es language pairs have 1 , 472 , 203 and 459 , 633 sentences , respectively .
table 2 shows the training vocabularies for the english , french and spanish data used for our models . the models trained on the spanish and english datasets are shown in table 2 .
table 5 : automatic evaluation scores ( bleu and ter ) for the rev systems . the en - fr - rnn - rev and en - es - smt - rev systems receive the highest bleu scores and ter scores , respectively , while the en - e - trans - rev system receives the lowest ter score .
table 2 shows the results of our model on flickr . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled rsaimage is the model from flickr2016 . our model obtains the highest recall @ 10 and the highest average mfcc .
table 1 : results on synthetically spoken coco . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled rsaimage is the model trained on the generated rsaimage embeddings . the results are shown in table 1 . rsaimage has the highest recall @ 10 and the highest mean mfcc score ( 1 . 414 % ) , while vgs has the lowest .
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . we report further examples in the appendix . for example , rnn turns in a screenplay that has edges at the edges , while cnn turns on a on ( in the the the edges of the screenplay ) and dan turns in an on - screen screenplay of the same size with the same number of edges as the original , but without the edges .
table 2 : part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . the numbers indicate the changes in percentage points with respect to the original sentence . the last row indicates the overlap with the original sentences , and the last row shows the percentage of words in the sentence that are in the correct part of the sentence .
table 3 shows the sentiment score changes in sst - 2 . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the positive sentiment score increases by 9 . 6 % and 28 . 4 % compared to the negative sentiment score .
table 1 presents the results of our experiments on the pubmed and sst - 2 datasets . the results are summarized in table 1 . our approach outperforms all the other approaches except for the case of pubmed , where our approach obtains the best performance .
