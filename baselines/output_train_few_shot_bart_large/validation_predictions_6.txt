the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . the iterative approach , on the other hand , achieves the best results on training and inference , and is comparable to tensorflow in terms of both performance and size .
the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t parallelization .
the max pooling strategy consistently performs better in all model variations . hyperparametric optimization results for each model with different representation are shown in table 2 . conll08 and ud v1 . 3 consistently outperforms softplus and sb in terms of f1 score ( i . e . , the number of iterations per iteration is higher than the dropout rate ) . sigmoid and sb models also perform better than softplus models with different feature maps .
the results of using the shortest dependency path on each relation type are shown in table 1 . we observe that the macro - averaged approach achieves the best f1 score , and the model - feature approach obtains the second - best f1 .
the results are shown in table 3 . we observe that the y - 3 scores are significantly higher than the r - f1 and f1 scores of the previous state - of - the - art systems , indicating that the model is more suitable for the task at hand . however , the results are less clear on the f1 score , which indicates that it is easier for the model to achieve a higher f1 and r - score . finally , we observe that when the model was trained on f1 , it achieved a f1 / r - score of 50 % and 50 % respectively .
we observe that mst - parser achieves the best results on all three test sets , with an absolute improvement over the previous state - of - the - art in terms of accuracy . the results are shown in table 3 . we observe a significant drop in accuracy on all test sets with respect to paragraph level acc . and r - f1 scores compared to the original model .
the results are shown in table 4 . the average c - f1 score for the two systems is 60 . 40 ± 13 . 57 % compared to 56 . 24 ± 2 . 87 % for the lstm - parser system . note that the mean performances are lower than the majority performances over the runs given in table 2 . the difference between the average and the mean scores is due to the smaller sample size of paragraph level .
the results are shown in table 3 . we observe that the original tgen + model outperforms the tgen − model in terms of bleu , meteor , rouge - l and nist scores . the difference in performance between the original and the cleaned model is less pronounced , but is still significant . when the model is cleaned , it achieves the best results , surpassing sc - lstm by a significant margin .
table 1 compares the original e2e dataset with our cleaned version . the difference in the number of distinct mrs between the original and the cleaned version is statistically significant ( p < 0 . 001 ) and the difference in ser as measured by our slot matching script , see section 3 ) .
the results are shown in table 3 . we observe that tgen + outperforms both sc - lstm and tgen − by a significant margin . the difference in bleu and nist scores between the original and the final test set is less pronounced , but is still significant . as the results show , the difference between original and final test sets is much less pronounced . finally , we observe that the difference in accuracy between the two sets is less than that between the average of the two train sets , indicating that the original model is more likely to be more accurate .
results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) . these results are shown in table 4 . we found that adding the correct values to the training set significantly reduced the number of errors , however , it did not improve the overall performance .
the results are shown in table 3 . our model outperforms all the previous state - of - the - art approaches except for tree2str and pbmt in terms of average number of iterations . we observe that graphlstm ( song et al . , 2018 ) achieves the best performance with an average of 25 . 2 iterations , while seq2seqk achieves the highest performance with a total of 27 . 9 iterations .
as shown in table 2 , our model achieves the highest bleu score on amr17 , surpassing all the state - of - the - art ensemble models except seq2seqb ( beck et al . , 2018 ) , which achieves 25 . 5 points . gcnseq ( damonte and cohen , 2019 ) achieves 24 . 3 points , which is slightly better than seq - 2e , but still falls short of our model in terms of performance .
the results are shown in table 1 . we observe that the single - factor approach outperforms all the other approaches except for seq2seqb , which shows that it is more suitable for english - german and czech - language embeddings , and is comparable to birnn + gcn in terms of performance .
table 5 shows the effect of the number of layers inside each layer on the performance of our model . our model obtains the best results with m - score of 53 . 3 compared to 50 . 0 for the previous state - of - the - art model .
comparisons with baselines are shown in table 6 . our model outperforms all the baselines except for dcgcn1 , which is comparable to the best baselines in terms of roc scores . we observe that the rc scores of our model are significantly higher than those of baselines with residual connections , indicating that our model is better able to distinguish between connections and connections without residual connections .
the results are shown in table 3 . we observe that the best performing model is the dcgcn ( 1 ) with a d / b average of 10 . 2 and a b average of 12 . 4 . the best performing dcgcns are the ones with an f1 score of > 10 . 0 and a f2 average of > 2 . 0 .
ablation study for density of connections on the dev set of amr15 . the results are shown in table 8 . our model obtains the best results with an ablation rate of 25 . 8 % compared to 25 . 1 % for the dcgcn4 model .
ablation study for modules used in the graph encoder and the lstm decoder . the results are shown in table 9 . we observe that the coverage mechanism used by the decoder achieves the best results , with an absolute improvement over the previous state - of - the - art on both metrics .
the results for initialization and probing tasks are shown in table 7 . we observe that glorot and somo are comparable in terms of generalization scores , with somo achieving an overall improvement of 3 . 5 points over our paper on both metrics .
we observe that the h - cmow model outperforms both the previous state - of - the - art h - cbow model and the best state of the art cmow model in terms of concatenation scores . the results are shown in table 3 . we observe a significant drop in concatenated results compared to the previous model in both accuracy and tense scores , indicating that the method is more suitable for more complicated tasks . as expected , the best performance is obtained with the model trained on the baselines of somo and wc .
the results are shown in table 3 . hybrid models outperform all the other approaches except for cbow , which shows a slight drop in performance of 0 . 2 % compared to cmow . we observe that the sick - e model outperforms both cmow and cbow in terms of test set quality .
as shown in table 3 , the cbow scores on unsupervised downstream tasks are significantly higher than those on supervised downstream tasks , indicating that our models are able to learn new tasks more easily . hybrid outperforms both cbow and cmow in terms of the number of tasks , but is less effective at learning new ones .
table 8 shows the performance of our initialization strategies on supervised downstream tasks . glorot achieves the best results with an overall score of 87 . 6 % , while trec achieves 87 . 4 % . we observe that sick - e achieves the highest score with an absolute improvement of 3 . 6 % over the previous state - of - the - art score .
table 6 shows the results for different training objectives on the unsupervised downstream tasks . we observe that the cmow - r approach outperforms the cbow approach on all the tasks except for sts13 and sts14 , where the cmows - r achieves the best performance . the cbow - c approach achieves the highest performance on sts12 , sts15 , and the sts16 tasks .
the results are presented in table 3 . we observe that the cbow - r model outperforms the cmow model on all metrics except for length . the difference between the two models is most striking when we consider the bshift and subjnum metrics , which are the most important ones for our model . as a result , our model obtains the best results on both metrics , outperforming both the previous state - of - the - art and the state of the art in both categories .
the results are shown in table 3 . we observe that the cmow - r method outperforms all the other methods except mpqa and trec in terms of test set quality . it achieves a c - score of 90 . 6 % on average compared to 78 . 9 % on the sst2 test set , and 87 . 2 % on sst5 test set .
the results are shown in table 3 . we observe that the supervised learning approach outperforms both the supervised and unsupervised approaches in terms of both loc and misc scores . supervised learning outperforms the supervised approach in all but one of the three cases . in particular , it achieves a score of 96 . 38 % on loc and 96 . 57 % on misc , a significant improvement over the previous state - of - the - art .
results on the test set under two settings are shown in table 2 . name matching and supervised learning achieve the highest f1 scores , with τmil - nd ( model 2 ) achieving the best f1 score . supervised learning achieves the best overall score , with a f1 of 73 . 38 ± 1 . 59 and a rn of 15 . 03 ± 0 . 59 , respectively , compared to 15 . 42 ± 0 . 03 and 15 . 59 ± 1 . 15 , indicating that the supervised learning approach is more suitable for the task at hand . we observe that the model with the best performance is the one with the highest rn and the highest e + p score , which indicates that the approach is well suited to the task . finally , we observe that when the model is trained on a single test set , the results are comparable across all settings , with the exception of name matching , which shows that it is easier to generalize to multiple test sets .
the results are shown in table 6 . we observe that g2s - gat achieves the best results with an absolute improvement of 3 . 86 points over the previous state - of - the - art model in terms of both ref and average number of iterations . also , the model with the highest percentage of iterations is significantly more likely to achieve the best result .
the results are presented in table 3 . we observe that the g2s model outperforms all the other models except for song et al . ( 2018 ) in terms of bleu scores , meteor scores , and gat scores , indicating that the model with the best performance is the one with the highest correlation with the most recent state - of - the - art ldc datasets . the results of our model are comparable to those of the best previous work on the ldc and ldc2015e86 datasets .
results on ldc2015e86 test set when models are trained with additional gigaword data . we observe that g2s - ggnn outperforms both external and internal models in terms of bleu scores . the results are shown in table 3 .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model significantly outperforms the previous state - of - the - art bilstm model in terms of both bleu score and meteor score .
we observe that g2s - gat outperforms all the other models in terms of average number of sentences and average length of sentences , with a margin of 0 . 51 % and 0 . 43 % of error , respectively , compared to the performance of the previous state - of - the - art models . the model with the highest correlation with average sentence length is gat - ggnn , which shows that it is able to predict sentence length better than any other model .
the token lemmas are used in the comparison . gold refers to the reference sentences . as shown in table 8 , the fraction of elements in the output that are missing in the generated sentence that are not present in the input ( added ) is higher than that in the original sentence ( miss ) , indicating that gold has a better generalization ability . g2s - ggnn outperforms the s2s model in terms of fraction of missing elements .
as shown in table 4 , the pos and sem features are comparable in accuracy to those extracted from the 4th nmt encoding layer , with the difference being that the pos features are more useful for target languages .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . word2tag classifier outperforms both unsupemb embeddings in terms of most frequent tags and upper bound encoder - decoder accuracy . the results show that the unsupervised encoder encoder decoder is better at predicting most frequent and most frequently tagged words , and is more accurate overall .
the results are shown in table 3 . we observe that the average number of errors per second is significantly lower than the number of false positives , indicating that there is a need to further improve the accuracy of our system . our system achieves the best results on all three metrics , with the exception of pos tagging accuracy , which is slightly higher than the previous state - of - the - art .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we observe that the bi - level features are more accurate than the res - based features , indicating that bi - layer features are better at tagging the target language features , and are more sensitive to variations in the semantic information .
the results are shown in table 8 . the attacker achieves the best performance on all three datasets . as shown in the table , the difference between the attacker score and the corresponding adversary ’ s accuracy is less than 0 . 1 points , indicating that the attacker is more sensitive to gender stereotypes .
accuracies when training directly towards a single task . the results are shown in table 1 . our approach outperforms the previous state - of - the - art approach in terms of both accuracy and sentiment scores .
as shown in table 2 , the balanced and unbalanced data splits result in better performance for pan16 compared to the unbalanced dataset . the difference in performance between the two sets of data is most pronounced for gender - specific features , but not for age - specific ones .
performances on different datasets with an adversarial training . the results are shown in table 3 . we observe that gender - specific features have a significant impact on the performance of our model , with the gender - neutral features contributing the most to the model ’ s performance . as expected , the features of age and gender are the most important predictors of performance , followed by the importance of sentiment and task accuracy . finally , we observe that the presence of features of race and gender predict performance .
as shown in table 6 , the rnn encoders significantly outperform the embeddings with respect to the protected attribute . the difference in accuracy between rnn and rnn is statistically significant , with rnn achieving an accuracy of 64 . 5 % compared to 54 . 8 % with the guarded encoder . we observe that rnn significantly outperforms embedding guarded encoders in terms of accuracy .
the results are presented in table 3 . we observe that the work performed by yang et al . ( 2018 ) is comparable to previous work on the ptb and wt2 baselines , with the exception of finetune , which shows that the model performs better on both baselines . the results of the final model are also comparable to those of the previous work , but are slightly worse on the wt2 and ptb baselines than those of previous work .
the results of rocktäschel et al . ( 2016 ) are presented in table 3 . we observe that the work performed by gru and sru is comparable to that of lstm in terms of generalization , with the exception of the number of params , which is slightly larger . gru achieves the best generalization with a final score of 90 . 5 % compared to the 90 . 3 % achieved by lstms . as shown in fig . 3 , gru outperforms sru in all but one of the tasks . it is clear from the results that gru has a better generalization ability than sru .
the results of zhang et al . ( 2015 ) are presented in table 3 . the results are summarized in bold . our model outperforms all the previous state - of - the - art models in terms of both err and time . we observe that our gru model is comparable to the best state of the art lstm model on both datasets . however , we observe that the performance gap between our model and the best previous work is much smaller , with gru achieving an err of 4 . 836 and a time - to - error of 1 . 867 , respectively .
as shown in table 3 , gnmt outperforms all the other methods except olrn in the case of case - insensitive tokenized bleu score . lrn achieves the best performance in wmt14 english - german translation task . gnmt achieves the highest case - inflation score with an absolute improvement of 3 . 67 points over the previous state - of - the - art model , gru . the performance gap between gnmt and lrn is less pronounced when the model is trained on tesla p100 .
table 4 shows the exact match / f1 - score of our model on squad dataset . we observe that our lstm model outperforms all the state - of - the - art models in terms of match rate and f1 score . as the results show , our model is comparable to the best state of the art models on both datasets , with the exception of rnet * ( 71 . 41 % match rate , 75 . 27 % f1score ) and rnet * : rnet has the highest number of parameter number , which indicates that our model has the best generalization ability .
table 6 shows the f1 score of our lstm model on conll - 2003 english ner task . we observe that our model significantly outperforms the previous state - of - the - art models in terms of f1 scores . our model obtains the highest score with an f1 of 90 . 56 on the ner test set , compared to the previous best result of 89 . 61 .
table 7 shows the test results on snli and ptb task with base + ln setting and test perplexity on base setting . lrn outperforms elrn and glrn in both tasks . we observe that lrn has the better performance on the snli task , but is slightly worse on the perplexity task .
the results are presented in table 1 . we observe that the average number of words per sentence for human and human - trained systems is comparable , but significantly higher for oracle - based systems , indicating that the system embedding the word embedding feature is beneficial for the human learner . our system outperforms both human and trained systems in terms of average word embeddings .
the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that the human evaluation method is comparable to the best automatic system in terms of overall quality . top - 1 / 2 : % of evaluations a system receives that are ranked in the top 1 or 2 for overall quality , while the average number of evaluations is less than 1 . 0 . the best performance is achieved by candela ( 30 . 6 % ) , followed by h & w hua and wang ( 2018 ) ( 38 . 8 % ) and seq2seq ( 25 . 6 % ) .
the results are shown in table 3 . we observe that our approach outperforms all the other approaches except for ted talks in terms of p < 0 . 05 , which shows that it is comparable to the best state - of - the - art approach . in addition , we observe that the performance gap between our approach and those of other approaches is much smaller , with our approach achieving a p > 0 . 5 on average compared to those of the others .
the results are shown in table 3 . we observe that our approach outperforms all the other approaches except for dsim and tf , which are comparable to our approach in terms of performance . as expected , our approach is comparable to both the best and worst - performing approaches on the test set .
the results are shown in table 3 . we observe that the performance gap between the en and the pt scores of the two systems is small , with the former outperforming the latter by a significant margin . the difference in performance between en and pt is less pronounced for the ted talks dataset , but is still significant . europarl outperforms both systems in terms of p < 0 . 001 and r = 0 . 005 .
the results are presented in table 3 . we observe that the number of roots per row is higher than the average of the max and maxdepth metrics , and that the average depth is lower than the maxdepth metric . europarl outperforms all the other systems in terms of total roots and average depth . the average number of number of rows per row also exceeds the max depth metric .
the results are presented in table 3 . we observe that the average depth of our dataset is slightly larger than the max depth of most other systems . europarl outperforms all the other systems in terms of the number of roots and the average number of number of features . the average depth is larger than that of all other systems except for dsim , df and tf .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . 1 is shown in table 1 . the enhanced version of our model outperforms the baseline model in terms of both qt and d scores . we observe that the enhanced version , lf , has significantly higher ncdcg % compared to the baseline baseline model , indicating that our approach is more suitable for the task at hand .
performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 is shown in table 2 . the best performing model is p2 , which indicates that the hidden dictionary learning approach is the most effective one . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . the performance of p2 indicates that it is comparable to the best baseline model in terms of accuracy .
comparison on hard and soft alignments is shown in table 5 . the hmd - recall model outperforms both ruse and wmd - bigram in terms of recall and accuracy . it achieves a recall score of 0 . 821 / 0 . 823 and a accuracy score of 1 . 012 / 0 , respectively , compared to 0 . 681 and 0 . 658 for ruse . wmd - unigram achieves the highest recall score , 0 . 788 / 1 . 012 , and a performance improvement over hmd - f1 . we observe that the recall scores of both models are comparable on soft and hard alignments . finally , the accuracy scores of the two models is comparable on both sets .
the results are shown in table 3 . we observe that the baselines used in our model are comparable to those used in meteor + + and bertscore - f1 , with the exception of ruse , which is comparable to meteor + and w2v . the average number of iterations of our model is 0 . 719 , which indicates that the model is able to replicate the performance of the best baselines .
the results of bertscore - f1 are shown in table 3 . we observe that the bleu - 2 model outperforms all the baselines except meteor by a significant margin . the results are similar across all the metrics except for the number of iterations , which indicates that the model is more sensitive to small variations in the clustering scheme . in addition , we observe that it is easier for the model to adapt to different clustering schemes .
the results are shown in table 3 . word - mover is the most important metric in our model , followed by recall . we observe that word - mover has the highest correlation with recall with respect to both m1 and m2 scores . sent - movers have the lowest correlation with m1 scores , but are comparable with bertscore - recall , indicating that the bert scores are comparable to those of meteor and spice .
the results are shown in table 1 . we observe that the model with the highest accuracy is the one with the most para - para clustering . para - based models outperform the models with the lowest accuracy . with the exception of the case of sim , all the models that have the best performance are those with the best meta - level , i . e . , those that are the only ones that have both the most and the least amount of para .
the results are shown in table 3 . we observe that the transfer quality and semantic preservation scores of our models are comparable to those of the best state - of - the - art systems . the transfer quality scores of yelp and google + are slightly higher than those of other systems , indicating that the model with the highest transfer quality is more likely to have a better semantic preservation score . semantic preservation scores are higher than that of google + and facebook + but lower than both the average and the average transfer quality of the other models . finally , we observe that our model has the best transfer quality score .
table 5 shows the results of human sentence - level validation on yelp and lit . net for each dataset , as well as the results for gm . the results show that both human and machine judgments are comparable in terms of semantic preservation and fluency . also , the human ratings of linguistic preservation are comparable to those of the machine .
the results are shown in table 3 . we observe that the para - para model outperforms both the shen - 1 model and the standard para model in terms of accuracy and generalization . para + para models outperform the standard model in all but one of the three categories . as expected , the para model performs best in all categories except for sim , where it achieves the best performance .
results on yelp sentiment transfer are shown in table 6 . our best models ( right table ) achieve higher bleu than prior work at similar levels of acc , but untransferred sentences achieve the highest bleu . acc ∗ : the definition of acc varies by row because of different classifiers in use . for example , our multi - decoder model achieves acc of 22 . 4 % higher than the best unsupervised model ( fu - 1 ) and 22 . 6 % higher acc compared to the best trained model ( yang2018unsupervised ) . the difference between the best and worst results is due to the fact that our model uses lexical embeddings instead of classifiers , which reduces the accuracy of the model .
we show the percentage of reparandum tokens that were correctly predicted as disfluent as well as the overall number of disfluencies for each type in table 2 . reparandum length and number of repetition tokens are the most important factors in predicting disfluency , followed by the number of tokens that are actually disfluential . we note that the average number of repetitions is less than the average length of the disfluent tokens , indicating that the training set contains fewer repetition tokens . finally , we note that nested disfluencings are more difficult to predict than those without , which indicates that there is a need to design a better training set .
we show the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , or in neither . the average number of tokens for each category is shown in table 3 . percentages in parentheses show the fraction of tokens belong to each category . reparandum length and repair length are the most important factors in predicting disfluency , followed by function - function and content - function .
the results are shown in table 3 . we observe that the best model is the one with the highest average number of iterations , followed by those with the best dev mean and best test scores . as the results show , when the model is combined with innovations , it achieves the best results . moreover , the model with innovations achieves the highest dev average and the best test score .
performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . word2vec embedding achieves the highest accuracy on the micro f1 test set , surpassing self - attention and rnn - based embeddings by a significant margin . our model achieves the best overall performance , with an accuracy of 83 . 43 % compared to 78 . 53 % for the state of the art rnn embedding .
as shown in table 2 , the unified model significantly outperforms all previous methods on the apw and nyt datasets for the document dating problem ( higher is better ) . attentive neuraldater outperforms the previous state - of - the - art models on both datasets .
as shown in table 3 , both word attention and graph attention significantly improve the accuracy of neuraldater compared to oe - gcn with and without attention . we observe that neuraldater is more accurate with word attention than with graph attention .
the results are shown in table 3 . we observe that the best performance is obtained when the model is trained on all stages , with the exception of the argument stage , where the dmcnn model achieves the highest performance . the best performing model is the jrnn model , which achieves 75 . 3 % on average across all stages . as shown in fig . 3 , the best performing argument stage is the triggering stage , followed by the embedding stage .
the results are presented in table 1 . we observe that cross - event event prediction is the most effective method for cross - domain prediction , with an f1 score of 68 . 7 % and a p - value of 50 . 1 % compared to 44 . 9 % for the best state - of - the - art method . cross - event event prediction outperforms all the other methods except for the classification method , which achieves a f1 of 69 . 9 % .
the results are shown in table 3 . we observe that the best performing variant is the one with the highest percentage of concatenated tokens , followed closely by the worst performing variant with the second highest percentage . as expected , fine - tuned models outperform all the other variants except for spanish - only - lm , which has the highest concatenation rate .
results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . fine - tuned models outperform cs - only models with a significant margin . the results are shown in table 4 . we observe that fine - tuning improves the model ' s performance on both test set and dev set , and improves the generalization ability of the model .
as shown in table 5 , fine - tuning the gold sentences improves the accuracy on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) . fine - tuned - disc improves the performance of both gold sentences and test sentences as well .
in table 7 , we report the precision and recall scores for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . the type combined approach shows a significant improvement over the baseline approach .
we observe that type - aggregated gaze features significantly improve the recall and precision of our model compared to the baseline model ( table 5 ) , indicating that the precision gains are due to a larger number of types of gaze features . also , the f1 score improves from baseline to 94 . 38 compared to 93 . 03 .
results on the test set of belinkov2014exploring are shown in table 1 . syntactic - sg embeddings outperform wordnet , verbnet , and ontolstm - pp in terms of both type and number of tokens . glove - extended synsets outperform both wordnet and wordnet - retro , and it achieves the best overall performance on the ppa test set . the difference between the two systems is statistically significant , with syntactic sg embedding achieving an average of 89 . 8 % and 89 . 7 % respectively .
results are shown in table 2 . we observe that hpcd and ontolstm - pp are comparable in accuracy to rbg in terms of pp prediction accuracy . also , the accuracy of ontolpstm is comparable to that of rbg . finally , we observe that the accuracy obtained by onto lstm outperforms the accuracy achieved by rbg alone .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . we observe that the ppa acc . scores for both models are significantly higher when the context sensitivity is removed , indicating that the model is more sensitive to context cues .
as shown in table 2 , adding subtitle data and domain tuning for image caption translation improves the bleu % scores for both en - de and multi30k models , but does not improve the overall results for en - fr models . adding subtitle data improves the overall bleuc % scores by 0 . 7 points , but not the overall score . subsfolding the subtitle data also improves the translation scores , but only marginally .
we observe that the domain - tuned h + ms - coco model outperforms all the other models in terms of performance on both en - de and en - fr settings . the results are shown in table 3 . domain - tuning improves the performance of all models except for mscoco17 , which shows a slight drop in performance compared to the baseline . moreover , the results are comparable across all models , with the exception of flickr16 , where the domain tuning improves performance by 0 . 3 points .
as shown in table 4 , adding automatic image captions ( only the best one or all 5 captions ) improves the overall bleu scores for all models except for mscoco17 , which shows that the best captions are the ones with the most concatenated ones . adding only the best five captions , however , does not improve the overall performance . the results of en - de and en - frachde models show that the automatic captions do not have a significant effect on the overall score , but do improve the multi - task performance .
comparison of strategies for integrating visual information ( bleu % scores ) . all results using transformer , multi30k + ms - coco + subs3mlm , detectron mask surface , and mscoco17 are shown in table 5 . the encoder and decoder strategies significantly outperform the en - de and en - fr strategies in terms of bleu % score . encoding and dec - gate strategies significantly improve the overall performance of the encoder , as shown in fig . 5 . en - de strategies are significantly more effective than encoder + dec - gate , as seen in the results in table 6 . the decoder and encoder strategies are more effective at extracting visual information , as the results show . we observe that encoder + decoder significantly improves the performance of our encoder .
the results for en - de and en - fr are shown in table 3 . we observe that the multi - lingual approach outperforms all the other approaches except for ms - coco , which shows that it is able to distinguish between visual and linguistic features better than the others . in addition , the results show that the multilingual approach is superior to all the others , and that it can even outperform the visual features alone . finally , we see that the ensemble - of - 3 approach achieves the best results .
we observe that en - fr - ht and en - es - ht are comparable in terms of ttr and mtld scores with respect to yule ’ s i and ttr scores , but are less comparable with mtld score . the results are shown in table 3 . as expected , the translation quality of the two approaches is comparable , but the performance gap between the two is less pronounced . we see that the approach that relies on the back - propagation of the word embeddings is less effective than the one that relies only on the trans - trans - ff embedding .
the number of parallel sentences in the train , test and development splits for the language pairs we used is shown in table 1 . we found that en – fr and en – es had the highest parallel sentences , followed by en - es and en - fr .
training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 . we observe that the training vocabulary is comparable across all three languages , with the exception of spanish , which is slightly larger .
automatic evaluation scores ( bleu and ter ) for the rev systems . the results are shown in table 5 . we observe that the system evaluation scores obtained by en - fr - rnn - rev and en - es - smt - rev are comparable in terms of bleu score and ter score . however , the evaluation score obtained by ter is higher for rev system , indicating that it is more suitable for the task at hand . finally , we observe that ter scores are comparable to those obtained for en - erl - rev .
results on flickr are shown in table 2 . the row labeled vgs is the visually supervised model from chrupala2017representations . com and the row labeled rsaimage is the one supervised by chrupala et al . ( 2017 ) with the highest recall ( 15 . 2 % ) and the highest mean mfcc ( 17 . 0 % ) . the vgs model significantly outperforms both rsaimage and segmatch .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled u is the one supervised by audio2vec - u . we observe that the vgs model significantly outperforms the other approaches in terms of recall and chance . as expected , vgs has higher recall and higher chance to match rsaimage .
we report further examples in the appendix . the classifiers are shown in table 1 . all the classifiers turn in sentences that are similar to the original on sst - 2 , except for dan , which turns in a sentence that is slightly different from the original . rnn turns in sentences containing the word " want to hate it " and " hate it " . cnn turns on a sentence containing the words " hate hate hate hate " . as shown in the table , rnn is easier to classify than cnn and dan .
as shown in table 2 , the number of occurrences has increased , decreased or stayed the same through fine - tuning respectively . the last row indicates the overlap with the original sentence , which indicates that the improvement in the quality of the final sentence is due to the improvement of the semantic quality . the rnn model outperforms both cnn and dan in terms of overall accuracy . rnp achieves a final score of 69 . 5 % on the part - of - speech test set compared to 70 . 0 % for cnn . as expected , rnn achieves a higher percentage of correct answers than dan , but is still inferior to cnn .
the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the results are shown in table 3 .
the results are shown in table 1 . we observe that pubmed outperforms both pubmed and corr by a large margin . the results of pubmed are slightly better than corr ( p < 0 . 001 ) and slightly worse than sift ( p > 0 . 01 ) .
