table 2 shows the performance of the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as expected , the recursive approach outperforms both the iterative and the folding approach on training .
table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t parallelization .
table 2 shows the results for each model with different representation . the max pooling strategy consistently performs better in all model variations . conll08 and ud v1 . 3 outperform all the other models in terms of f1 and f1 score .
table 1 : effect of using the shortest dependency path on each relation type . the best f1 ( in 5 - fold ) with sdp is shown in table 1 . we show the results of our model with and without sdp and with macro - averaged models .
table 3 shows the results of our model on the f1 and r - f1 test sets . the results are shown in table 3 . our model achieves the best performance on all three test sets , with the exception of f1 , which achieves the highest f1 score .
table 3 shows the performance of the mst - parser compared to other approaches . the results are shown in table 3 . as expected , the performance gap between the two approaches is much larger than the previous state - of - the - art approaches . the difference in performance between the best and worst - performing approaches is significant .
table 4 shows the c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . stagblcc outperforms lstm - parser in terms of the percentage of sentences that contain at least one paragraph .
table 3 shows the performance of the original and the clean - up test set compared to sc - lstm . the results are shown in table 3 . tgen + and tgen − outperform the original by a significant margin .
table 1 compares the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . the results are shown in table 1 . as expected , the cleaned version has significantly higher mrs compared to the original data .
table 3 shows the results of the training set and the test set . the results are shown in table 3 . tgen + outperforms tgen − and sc - lstm in terms of performance on both train and test set , with the former outperforming the latter by a significant margin .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) . these results are shown in table 4 . the total number of errors in the training data is shown in parentheses .
table 3 shows the results of our model on the external and internal datasets . the results are shown in table 3 . our model outperforms the previous state - of - the - art models in terms of performance on all three datasets , except for graphlstm ( song et al . , 2018 ) , which outperforms all the other models except tree2str .
table 2 shows the main results on amr17 . the results are shown in table 2 . table 2 summarizes the model size in terms of parameters ; “ s ” and “ e ” denote single and ensemble models , respectively . table 2 also shows the number of bleu points that the model achieves for each parameter . seq2seqb ( beck et al . , 2018 ) achieves 24 . 5 bleus , while gcnseq ( damonte and cohen , 2019 ) achieves 27 . 3 bleus points and ggnnseq achieves 28 . 3 points . as expected , both models achieve high performance on both datasets .
table 3 presents the results of our experiments on the english - german and czech - czech datasets . the results are shown in table 3 . table 3 shows the performance of all the models compared to the previous state - of - the - art models . as expected , the best performing models are those that use birnn + gcn ( bastings et al . , 2017 ) and bow - gcn , while the worst performing are the ones that use bow + cnn and cnn + cnn + .
table 5 shows the effect of the number of layers inside dc on the performance of the model . the effect of layers on the overall performance is shown in table 5 . as shown in the table , adding layers significantly improves the performance , but the effect is less pronounced for the second and third layers .
table 6 shows the performance of the gcns with residual connections compared to baselines . the results are shown in table 6 . as expected , rc + la ( 2 ) and rc + rc ( 6 ) are the most important factors in terms of performance , as they significantly improve the performance on the baselines compared to the baseline .
table 3 shows the results of the experiments on the dcgcn ( 1 ) model . the results are shown in table 3 . the results show that the model outperforms the previous state - of - the - art models in terms of both d and # p . in particular , the model with the best performance is the one with the highest d / p and the highest number of d / b .
table 8 : ablation study for density of connections on the dev set of amr15 . the results are shown in table 8 . the results of the ablation study show that the dcgcn4 model outperforms all the other models in terms of the number of connections removed in the i - th block .
table 9 : ablation study for modules used in the graph encoder and the lstm decoder . the results are shown in table 9 . the results of the ablation study are summarized in table 8 . we found that the coverage mechanism and direction aggregation are the most important components of the decoder modules . the other components are the global node and linear combination .
table 7 shows the scores for initialization strategies on probing tasks . glorot is the best performing initialization strategy , with a score of 31 . 3 % compared to our paper ' s 35 . 7 % and 35 . 6 % for our paper . the other two best performing strategies are bshift and subjnum .
table 3 shows the performance of the h - cmow and h - cbow models compared to the previous state - of - the - art cmow models . the results are shown in table 3 . the results show that the cmow and cbow models outperform all the other models in terms of performance .
table 3 shows the results of the test set for cmow and cmow / 784 . the results are shown in table 3 . we can see that cmow achieves the best results in terms of performance on all three metrics .
table 3 shows the performance of our models on unsupervised downstream tasks attained by our models . the results are shown in table 3 . our models outperform the previous state - of - the - art approaches on all the downstream tasks except for sts13 and sts14 . we also show the relative change with respect to hybrid . as expected , our model achieves the best performance on all three downstream tasks .
table 8 shows the results for initialization strategies on supervised downstream tasks . glorot and trec both outperform our paper in terms of sick - e , sst2 , sts - b , and sst5 on all three tasks .
table 6 shows the results for different training objectives on the unsupervised downstream tasks . the cmow - r and cbow - c scores are shown in table 6 . as expected , the cbow scores are significantly higher than the cmow scores on the supervised downstream tasks , indicating that the training objectives are more difficult to achieve in the supervised tasks .
table 3 shows the performance of the cmow - r and cbow - c models compared to the previous state - of - the - art cmow models . the results are shown in table 3 . the cmow model outperforms all the other models in terms of accuracy .
table 3 shows the performance of the cmow - r and cbow - c models compared to the previous state - of - the - art cmow models . the results are shown in table 3 . the cmow model outperforms all the previous models except for cbow .
table 3 shows the results of our supervised and supervised learning experiments on the e + loc and e + misc datasets . the results are shown in table 3 . we found that the supervised learning approach outperforms all the other approaches in terms of performance on all three datasets .
table 2 shows the results on the test set under two settings . the results are shown in table 2 . we show the f1 scores of all the models in table 1 and table 2 as well as τmil - nd ( model 2 ) and supervised learning ( model 1 ) . in both settings , the model with the highest f1 score is the one with the best supervised learning . in the case of the second set of models , the best performing model is mil - nd , which achieves a f1 of 42 . 42 ± 0 . 38 and a τ - mil score of 38 . 38 ± 1 . 68 . the other two sets of models are trained on the same test set with different settings , and the results are reported in table 3 .
table 6 shows the results of our experiments on the g2s - gat model compared to other models . the results are shown in table 6 . the results show that the gat model outperforms all the other models in terms of ref and ref ⇒ ref . in particular , the model with the highest ref is the one with the best performance .
table 3 presents the results of our model on the ldc2017t10 dataset . the results are shown in table 3 . our model outperforms the previous state - of - the - art models in terms of bleu , meteor , and g2s - gat scores . we also note that our model achieves the best performance on all three datasets .
table 3 shows the results on the ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 . g2s - ggnn outperforms all the other models in terms of performance on the test set , with the exception of the bleu model , which is trained with only gigaword data .
table 4 shows the results of the ablation study on the ldc2017t10 development set . the results are shown in table 4 . we found that bilstm improves the performance of the model and the size of the data set by a significant margin .
table 3 shows the results of our model with respect to s2s , g2s - gin and gat . the results are shown in table 3 . our model outperforms the previous state - of - the - art models in all cases except for gat , where it outperforms all the other models except gat and ggnn .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . s2s and g2s - ggnn are shown in table 8 . the results are summarized in table 7 .
table 4 shows the pos and sem tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the results are shown in table 4 .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . the results are shown in table 2 . word2tag classifier achieves the best performance on all three baselines . unsupemb classifier has the highest accuracy on all baselines , and the best accuracy on the word embeddings .
table 4 presents the results of our test set . the results are shown in table 4 . our model achieves the best performance on all metrics except for the pos tagging accuracy and sem tagging accuracy . we show that our model outperforms the previous state - of - the - art models in all but one case .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 . as expected , the best performing layers are those with bidirectional embeddings , and the worst performing layers have the most negative features .
table 8 : attacker ’ s performance on different datasets . results are on a training set 10 % held - out . the results are shown in table 8 . in pan16 , the attacker performs significantly worse than the corresponding adversary on all three datasets .
table 1 : accuracies when training directly towards a single task . the results are shown in table 1 . pan16 outperforms all the other models in terms of accuracy when trained directly towards the task .
table 2 shows the results of our model with balanced and unbalanced data splits . the results are shown in table 2 . our model outperforms all the other models in terms of the number of data splits and the percentage of data leaks .
table 3 shows the performance on different datasets with adversarial training . the results are shown in table 3 . in pan16 and pan1616 , we show the performance of our adversarial model on all three datasets . we also show the difference between the attacker score and the corresponding adversary ’ s accuracy .
table 6 : accuracies of the protected attribute with different encoders . the results are shown in table 6 . the rnn encoder outperforms the guarded encoder in all cases except for leaky and guarded .
table 3 presents the results of our work in table 3 . the results are shown in table 1 . we show that our model outperforms all the other models in terms of the number of parameters in the evaluation set . in particular , our model performs better than all the others except for the lstm model , which achieves the best performance on all metrics .
table 3 presents the results of our work on the lstm and gru models in table 2 . the results are shown in table 1 . we note that the results obtained in table 3 are comparable to those reported in the previous work by rocktäschel et al . ( 2016 ) . the difference in performance between gru and sru is significant , with gru outperforming sru in all but one of the cases .
table 3 presents the results of our work on amapolar err and yelppolar time . the results are shown in table 3 . as expected , we found that the work performed by zhang et al . ( 2015 ) is significantly better than previous work on the same set of datasets .
table 3 shows the bleu score on the wmt14 english - german translation task on tesla p100 . the results are shown in table 3 . gnmt outperforms all the other models except olrn and sru in terms of case - insensitive tokenization score . lrn outperforms sru and gru on all but gnmt and gnmt . as shown in the previous section , lrn is more sensitive to the number of tokens in a sentence compared to other models .
table 4 shows the exact match / f1 - score on the squad dataset . the results are shown in table 4 . lrn outperforms all the other models in terms of f1 score . sru outperforms atr and gru , while atr outperforms gru and lrn by a large margin . as expected , lrn has the best performance on the test set .
table 6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the reported result lample et al . ( 2016 ) and lrn and sru denote the reported results . lrn is the parameter number of the parameterized set of ner tasks . gru is the number of parameterized sets of the ner questions . sru and gru are the parameter - based set of questions that are considered to be the most important for the model .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . the results are shown in table 7 . lrn outperforms elrn and glrn in both cases .
table 3 shows the performance of our system on the test set . the results are shown in table 3 . our system outperforms the previous state - of - the - art on all metrics except for human and human - sentenced sentences .
table 4 shows the results of the human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 0 . top - 1 / 2 : % of evaluations a system being ranked in top 1 or 2 for overall quality . table 4 also shows the percentage of evaluations that are ranked in the top 2 or 3 for the overall quality of the system . the best results are shown in table 4 .
table 3 shows the performance of the best - performing models compared to the worst - performing ones . the results are shown in table 3 . europarl outperforms all the other models except ted talks in terms of p < 0 . 05 , pt and en .
table 3 shows the performance of the best - performing models compared to the worst - performing ones . the results are shown in table 3 . europarl outperforms all the other models except for ted talks , which has the highest p < 0 . 01 compared to all the others .
table 3 shows the performance of the best - performing models compared to the worst - performing ones . the results are shown in table 3 . europarl outperforms all the other models except ted talks in terms of p < 0 . 01 and pt .
table 3 shows the performance of the europarl embeddings . the results are shown in table 3 . we note that the average depth of each embedding is slightly higher than the max depth of the embedding , but still lower than the maximum depth .
table 3 shows the performance of the europarl system on the numberrels test set . the results are shown in table 3 . the average depth is 9 . 43 , and the max depth is 10 . 29 .
table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . the results are shown in table 1 . as expected , the enhanced version of lf improves the performance significantly compared to the baseline model . in addition , the weighted softmax loss reduces the performance of the model by 3 . 7 % compared to baseline , and the weighted ranking loss by 2 . 7 % .
table 2 shows the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . p1 indicates the best performing model , and p1 + p2 indicates that the best model is the one with the best performance .
table 5 : comparison on hard and soft alignments . the results are shown in table 5 . hmd - f1 + bert and hmd - recall outperforms all other approaches except wmd - unigram and ruse in terms of accuracy and recall . ruse outperforms wmd - bigram and hmd prec in both cases .
table 3 shows the average performance of the baselines on all three sets . the results are shown in table 3 . bertscore - f1 and ruse ( * ) have the highest average performance on all the three sets , while meteor + + has the lowest average performance .
table 3 shows the results of the bertscore - f1 and bleu - 2 tests on the sfhotel and meteor datasets . the results are shown in table 3 . the results show that bert score is significantly higher than the baselines on all three datasets except for the smd dataset , which is significantly lower than the previous best baseline .
table 3 presents the results of our test set . the results are shown in table 3 . we show that our model outperforms all the baselines except leic ( * ) and meteor in terms of m1 and m2 .
table 3 shows the results of our model on the table . the results are shown in table 3 . we show that our model outperforms the previous state - of - the - art models in all but one case , with the exception of shen - 1 , where it performs worse than the previous best .
table 3 shows the results of our model on the yelp dataset . the results are shown in table 3 . our model outperforms all the other models in terms of transfer quality , transfer quality tie , and semantic preservation .
table 5 shows the results of human sentence - level validation of metrics ; 100 examples for each dataset for validating acc ; 150 each for sim and pp ; see text for validation of gm . the results are shown in table 5 . table 5 summarizes the results for each metric in table 4 .
table 3 shows the results of our model on the weighted average of m1 , m2 , m3 , m4 , m5 , m6 , m7 , m8 , m9 , m10 , m11 , m12 , m13 , m14 , m15 , m16 , m17 , m18 , m19 , m20 , m21 , m22 , m23 , m24 , m25 , m26 , m27 , m28 , m29 , m30 , m31 , m32 , m33 , m34 , m35 , m36 , m37 , m38 , m39 , m40 , m41 , m42 , m43 , m44 , m45 , m46 , m47 , m48 , m49 , m50 , m51 , m52 , m53 , m54 , m55 , m56 , m57 , m58 , m59 , m61 , m62 , m63 , m64 , m65 , m69 , m70 , m71 , m72 , m73 , m74 , m75 , m76 , m77 , m78 , m79 , m80 , m81 , m82 , m83 , m84 , m85 , m86 , m87 , m88 , m89 , m92 , m93 , m94 , m95 , m97 , m98 , m99 , m100 , m01 , m02 , m03 , m04 , m05 , m06 , m07 , m08 , m09 and m10 . the results are shown in table 3 . our model outperforms the previous state - of - the - art on all metrics .
table 6 shows the results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( right table ) achieve higher bleus than prior work at similar levels of acc , but untransferred sentences achieve the highest bleu . the results are shown in table 6 . as expected , our best model achieves higher acc than previous work on the same dataset .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . reparandum length and overall length are shown in table 2 . for nested disfluencies , the average length of the disfluency tokens is 0 . 66 and 0 . 39 , respectively , compared to 0 . 62 for nested - disfluencies . the average length is also slightly higher than for nesteddisfluency , but still lower than the average for disfluential tokens .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , or the content - function , or in neither . the average number of tokens in each category is shown in table 3 . percentages in parentheses show the fraction of tokens belong to each category .
table 3 shows the results of our test set . the results are shown in table 3 . our model outperforms the previous state - of - the - art models in terms of both test mean and test best . as expected , our model achieves the best performance on all three tests .
table 2 shows the performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . our model outperforms all other models except self - attention and rnn - based embeddings in terms of accuracy . it also outperforms the state of the art on the micro f1 dataset .
table 2 shows the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . as shown in table 2 , ac - gcn is the best performing method on both datasets , outperforming burstysimdater and maxent - joint .
table 3 shows the accuracy ( % ) comparisons of component models with and without attention . the results show that both word attention and graph attention are effective in improving the performance of neuraldater .
table 3 shows the performance of the model and the model - based embeddings compared to the previous state - of - the - art models . the results are shown in table 3 . as expected , the best performing models are the ones that embedding + t and dmcnn , while the worst performing ones are those that embedded - t and jmee . the best performing model is jrnn .
table 3 presents the results of our model on the cross - event test set . the results are shown in table 3 . we show that our model outperforms the previous state - of - the - art models in terms of f1 , f1 p , and f1 r . in particular , our model achieves the highest f1 score on the cross - event test set , with a f1 of 4 . 7 .
table 3 shows the performance of all the models in terms of dev perp , test acc , and test wer . the results are shown in table 3 . all models except for cs - only + vocab - lm and cs - last - lm outperform all the other models except english - only and spanish - only . as expected , the performance gap between the best - performing models and the worst - performing ones is small .
table 4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the results are shown in table 4 . as expected , fine - tuned training improves the performance of the model on both sets .
table 5 shows the accuracy on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) . the results are shown in table 5 . as expected , fine - tuned - disc improves the performance on both the dev and test sets .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . the type combined approach shows a significant improvement in precision and recall compared to baseline .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset ( table 5 ) compared to the baseline dataset . type combined gaze features significantly improve recall and precision compared to baseline , while type combined features improve f1 score .
table 1 : results on belinkov2014exploring ’ s ppa test set . hpcd ( full ) is from the original paper , and it uses syntactic skipgram . syntactic - sg embeddings are obtained by running autoextend rothe and schütze ( 2015 ) on glove vectors retrofitted to wordnet 3 . 1 . the results are shown in table 1 . as expected , the results are similar to those reported in the original study .
table 2 : results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 . hpcd and ontolstm - pp ( full ) outperform all the other models in terms of ppa acc . and ppa accuracy .
table 3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . ppa acc . = 89 . 7 and 89 . 5 , respectively .
table 2 : adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun . the results are shown in table 2 . as expected , adding subtitle data improves the bleu % score for all three models . however , the results for en - de and en - fr are slightly worse than those for mscoco17 and flickr17 .
table 3 shows the performance of domain - tuned h + ms - coco with respect to en - de , en - fr , flickr16 and flickr17 . the results are shown in table 3 . the results of domain tuning are summarized in table 1 . as expected , domain tuning improves performance on all three datasets .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . the results are shown in table 4 . as expected , the best captions are the ones with only the best 5 captions , while the best ones with all 5 are those with the best 1 , 2 , 3 , 4 , 5 and 6 .
table 5 : comparison of strategies for integrating visual information ( bleu % scores ) . all results using transformer , multi30k + ms - coco + subs3mlm , detectron mask surface , and mscoco17 are shown in table 5 . the best performing strategies are en - de and dec - gate , while en - fr and mscoco17 outperform all other strategies except enc - gate . as expected , the best performing strategy is enc - de , which achieves the highest bleu % score .
table 3 shows the performance of the three approaches compared to the previous state - of - the - art approaches . the results are shown in table 3 . we note that all three approaches outperform the previous approaches in terms of the number of features in each dataset . in particular , all three models outperform all the other approaches except for ms - coco .
table 3 shows the results of our approach on yule ’ s i , ttr , mtld , and mtld . the results are shown in table 3 . our approach achieves the best results on all three metrics , with the exception of the mtld score , which shows that our approach does not improve upon the previous state - of - the - art .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the number of sentences in each split is shown in table 1 . we used en – fr and en – es as our language pairs .
table 2 shows the training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 . our models are trained on both english and french data .
table 5 shows the automatic evaluation scores ( bleu and ter ) for the rev systems . the results are shown in table 5 . bleu is the average evaluation score of the system reference and ter is the evaluation score for the ter system . as can be seen , both rev and ter scores are significantly higher than the baseline rev scores .
table 2 shows the results on flickr . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled rsaimage is the rsaimage model from the previous study . the results are shown in table 2 . rsaimage achieves the highest recall @ 10 % and the highest median rank .
table 1 shows the results on synthetically spoken coco . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled rsaimage is the rsaimage - u model from the previous work . the results are shown in table 1 . as expected , the vgs model has the highest recall @ 10 and the highest median rank .
we report further examples in the appendix . table 1 shows the results of the different classifiers compared to the original on sst - 2 . the classifiers are shown in table 1 . as expected , all the classifiers turn in a screenplay that is at the edges of the screenplay .
table 2 shows the part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . the numbers indicate the changes in percentage points with respect to the original sentence . the last row indicates the overlap with the original sentences , and the last row shows the percentage of words that have been added or subtracted in the final sentence .
table 3 : sentiment score changes in sst - 2 . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the last two rows correspond to the case where negative labels are flipped to positive and vice versa .
table 2 presents the results of our experiment on the sst - 2 dataset . the results are shown in table 2 . the results of the experiment are summarized in table 1 . we note that the results are statistically significant for both pubmed and corr .
