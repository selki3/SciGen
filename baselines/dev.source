<R> <C> Batch size <C> Throughput (instances/s) Inference <C> Throughput (instances/s) Inference <C> Throughput (instances/s) Inference <C> Throughput (instances/s) Training <C> Throughput (instances/s) Training <C> Throughput (instances/s) Training <R> <C> Batch size <C> Iter <C> Recur <C> Fold <C> Iter <C> Recur <C> Fold <R> <C> 1 <C> 19.2 <C> 81.4 <C> 16.5 <C> 2.5 <C> 4.8 <C> 9.0 <R> <C> 10 <C> 49.3 <C> 217.9 <C> 52.2 <C> 4.0 <C> 4.2 <C> 37.5 <R> <C> 25 <C> 72.1 <C> 269.9 <C> 61.6 <C> 5.5 <C> 3.6 <C> 54.7 <CAP> Table 2: Throughput for processing the TreeLSTM model on our recursive framework, Fold’s folding technique, and TensorFlow’s iterative approach, with the Large Movie Review dataset. The recursive approach performs the best on inference with efficient parallel execution of tree nodes, while the folding technique shows better performance on training thanks to its GPU exploitation.
<R> <C> Batch size <C> Throughput (instances/s) Balanced <C> Throughput (instances/s) Moderate <C> Throughput (instances/s) Linear <R> <C> 1 <C> 46.7 <C> 27.3 <C> 7.6 <R> <C> 10 <C> 125.2 <C> 78.2 <C> 22.7 <R> <C> 25 <C> 129.7 <C> 83.1 <C> 45.4 <CAP> Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.
<R> <C> [BOLD] Representation <C> [BOLD] Hyper parameters Filter size <C> [BOLD] Hyper parameters Num. Feature maps <C> [BOLD] Hyper parameters Activation func. <C> [BOLD] Hyper parameters L2 Reg. <C> [BOLD] Hyper parameters Learning rate <C> [BOLD] Hyper parameters Dropout Prob. <C> [BOLD] F1.(avg. in 5-fold) with default values <C> [BOLD] F1.(avg. in 5-fold) with optimal values <R> <C> CoNLL08 <C> 4-5 <C> 1000 <C> Softplus <C> 1.15e+01 <C> 1.13e-03 <C> 1 <C> 73.34 <C> 74.49 <R> <C> SB <C> 4-5 <C> 806 <C> Sigmoid <C> 8.13e-02 <C> 1.79e-03 <C> 0.87 <C> 72.83 <C> [BOLD] 75.05 <R> <C> UD v1.3 <C> 5 <C> 716 <C> Softplus <C> 1.66e+00 <C> 9.63E-04 <C> 1 <C> 68.93 <C> 69.57 <CAP> Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.
<R> <C> [BOLD] Relation <C> [BOLD] best F1 (in 5-fold) without sdp <C> [BOLD] best F1 (in 5-fold) with sdp <C> [BOLD] Diff. <R> <C> USAGE <C> 60.34 <C> 80.24 <C> + 19.90 <R> <C> MODEL-FEATURE <C> 48.89 <C> 70.00 <C> + 21.11 <R> <C> PART_WHOLE <C> 29.51 <C> 70.27 <C> +40.76 <R> <C> TOPIC <C> 45.80 <C> 91.26 <C> +45.46 <R> <C> RESULT <C> 54.35 <C> 81.58 <C> +27.23 <R> <C> COMPARE <C> 20.00 <C> 61.82 <C> + 41.82 <R> <C> macro-averaged <C> 50.10 <C> 76.10 <C> +26.00 <CAP> Table 1: Effect of using the shortest dependency path on each relation type.
<R> <C> [EMPTY] <C> C-F1 100% <C> C-F1 50% <C> R-F1 100% <C> R-F1 50% <C> F1 100% <C> F1 50% <R> <C> Y-3 <C> 49.59 <C> 65.37 <C> 26.28 <C> 37.00 <C> 34.35 <C> 47.25 <R> <C> Y-3:Y<italic>C</italic>-1 <C> 54.71 <C> 66.84 <C> 28.44 <C> 37.35 <C> 37.40 <C> 47.92 <R> <C> Y-3:Y<italic>R</italic>-1 <C> 51.32 <C> 66.49 <C> 26.92 <C> 37.18 <C> 35.31 <C> 47.69 <R> <C> Y-3:Y<italic>C</italic>-3 <C> <bold>54.58</bold> <C> 67.66 <C> <bold>30.22</bold> <C> <bold>40.30</bold> <C> <bold>38.90</bold> <C> <bold>50.51</bold> <R> <C> Y-3:Y<italic>R</italic>-3 <C> 53.31 <C> 66.71 <C> 26.65 <C> 35.86 <C> 35.53 <C> 46.64 <R> <C> Y-3:Y<italic>C</italic>-1:Y<italic>R</italic>-2 <C> 52.95 <C> <bold>67.84</bold> <C> 27.90 <C> 39.71 <C> 36.54 <C> 50.09 <R> <C> Y-3:Y<italic>C</italic>-3:Y<italic>R</italic>-3 <C> 54.55 <C> 67.60 <C> 28.30 <C> 38.26 <C> 37.26 <C> 48.86 <CAP> Table 3: Performance of MTL sequence tagging approaches, essay level. Tasks separated by “:”. Layers from which tasks feed are indicated by respective numbers.
<R> <C> [EMPTY] <C> Paragraph level Acc. <C> Paragraph level C-F1 <C> Paragraph level C-F1 <C> Paragraph level R-F1 <C> Paragraph level R-F1 <C> Paragraph level F1 <C> Paragraph level F1 <C> Essay level Acc. <C> Essay level C-F1 <C> Essay level C-F1 <C> Essay level R-F1 <C> Essay level R-F1 <C> Essay level F1 <C> Essay level F1 <R> <C> [EMPTY] <C> [EMPTY] <C> 100% <C> 50% <C> 100% <C> 50% <C> 100% <C> 50% <C> [EMPTY] <C> 100% <C> 50% <C> 100% <C> 50% <C> 100% <C> 50% <R> <C> MST-Parser <C> 31.23 <C> 0 <C> 6.90 <C> 0 <C> 1.29 <C> 0 <C> 2.17 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Mate <C> 22.71 <C> 2.72 <C> 12.34 <C> 2.03 <C> 4.59 <C> 2.32 <C> 6.69 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Kiperwasser <C> 52.80 <C> 26.65 <C> 61.57 <C> 15.57 <C> 34.25 <C> 19.65 <C> 44.01 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> LSTM-Parser <C> 55.68 <C> 58.86 <C> 68.20 <C> 35.63 <C> 40.87 <C> 44.38 <C> 51.11 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> STagBLCC <C> 59.34 <C> 66.69 <C> 74.08 <C> 39.83 <C> 44.02 <C> 49.87 <C> 55.22 <C> <bold>60.46</bold> <C> 63.23 <C> 69.49 <C> <bold>34.82</bold> <C> <bold>39.68</bold> <C> <bold>44.90</bold> <C> <bold>50.51</bold> <R> <C> LSTM-ER <C> <bold>61.67</bold> <C> <bold>70.83</bold> <C> <bold>77.19</bold> <C> <bold>45.52</bold> <C> <bold>50.05</bold> <C> <bold>55.42</bold> <C> <bold>60.72</bold> <C> 54.17 <C> <bold>66.21</bold> <C> <bold>73.02</bold> <C> 29.56 <C> 32.72 <C> 40.87 <C> 45.19 <R> <C> ILP <C> 60.32 <C> 62.61 <C> 73.35 <C> 34.74 <C> 44.29 <C> 44.68 <C> 55.23 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 2: Performance of dependency parsers, STagBLCC, LSTM-ER and ILP (from top to bottom). The ILP model operates on both levels. Best scores in each column in bold (signific. at p<0.01; Two-sided Wilcoxon signed rank test, pairing F1 scores for documents). We also report token level accuracy.
<R> <C> [EMPTY] <C> STagBLCC <C> LSTM-Parser <R> <C> Essay <C> 60.62±3.54 <C> 9.40±13.57 <R> <C> Paragraph <C> 64.74±1.97 <C> 56.24±2.87 <CAP> Table 4: C-F1 (100%) in % for the two indicated systems; essay vs. paragraph level. Note that the mean performances are lower than the majority performances over the runs given in Table 2.
<R> <C> Train <C> Test <C> [BOLD] System <C> [BOLD] BLEU <C> [BOLD] NIST <C> [BOLD] METEOR <C> [BOLD] ROUGE-L <C> [BOLD] CIDEr <C> [BOLD] Add <C> [BOLD] Miss <C> [BOLD] Wrong <C> [BOLD] SER <R> <C> Original <C> [BOLD] Cleaned <C> TGen− <C> 36.85 <C> 5.3782 <C> 35.14 <C> 55.01 <C> 1.6016 <C> 00.34 <C> 09.81 <C> 00.15 <C> 10.31 <R> <C> Original <C> [BOLD] Cleaned <C> TGen <C> 39.23 <C> 6.0217 <C> 36.97 <C> 55.52 <C> 1.7623 <C> 00.40 <C> 03.59 <C> 00.07 <C> 04.05 <R> <C> Original <C> [BOLD] Cleaned <C> TGen+ <C> 40.25 <C> 6.1448 <C> 37.50 <C> 56.19 <C> 1.8181 <C> 00.21 <C> 01.99 <C> 00.05 <C> 02.24 <R> <C> Original <C> [BOLD] Cleaned <C> SC-LSTM <C> 23.88 <C> 3.9310 <C> 32.11 <C> 39.90 <C> 0.5036 <C> 07.73 <C> 17.76 <C> 09.52 <C> 35.03 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned <C> [BOLD] Cleaned <C> TGen− <C> 40.19 <C> 6.0543 <C> 37.38 <C> 55.88 <C> 1.8104 <C> 00.17 <C> 01.31 <C> 00.25 <C> 01.72 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned <C> [BOLD] Cleaned <C> TGen <C> 40.73 <C> 6.1711 <C> 37.76 <C> 56.09 <C> 1.8518 <C> 00.07 <C> 00.72 <C> 00.08 <C> 00.87 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned <C> [BOLD] Cleaned <C> TGen+ <C> 40.51 <C> 6.1226 <C> 37.61 <C> 55.98 <C> 1.8286 <C> 00.02 <C> 00.63 <C> 00.06 <C> 00.70 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned <C> [BOLD] Cleaned <C> SC-LSTM <C> 23.66 <C> 3.9511 <C> 32.93 <C> 39.29 <C> 0.3855 <C> 07.89 <C> 15.60 <C> 08.44 <C> 31.94 <R> <C> Cleaned missing <C> [BOLD] Cleaned <C> TGen− <C> 40.48 <C> 6.0269 <C> 37.26 <C> 56.19 <C> 1.7999 <C> 00.43 <C> 02.84 <C> 00.26 <C> 03.52 <R> <C> Cleaned missing <C> [BOLD] Cleaned <C> TGen <C> 41.57 <C> 6.2830 <C> 37.99 <C> 56.36 <C> 1.8849 <C> 00.37 <C> 01.40 <C> 00.09 <C> 01.86 <R> <C> Cleaned missing <C> [BOLD] Cleaned <C> TGen+ <C> 41.56 <C> 6.2700 <C> 37.94 <C> 56.38 <C> 1.8827 <C> 00.21 <C> 01.04 <C> 00.07 <C> 01.31 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added <C> [BOLD] Cleaned <C> TGen− <C> 35.99 <C> 5.0734 <C> 34.74 <C> 54.79 <C> 1.5259 <C> 00.02 <C> 11.58 <C> 00.02 <C> 11.62 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added <C> [BOLD] Cleaned <C> TGen <C> 40.07 <C> 6.1243 <C> 37.45 <C> 55.81 <C> 1.8026 <C> 00.05 <C> 03.23 <C> 00.01 <C> 03.29 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added <C> [BOLD] Cleaned <C> TGen+ <C> 40.80 <C> 6.2197 <C> 37.86 <C> 56.13 <C> 1.8422 <C> 00.01 <C> 01.87 <C> 00.01 <C> 01.88 <CAP> Table 3: Results evaluated on the cleaned test set (cf. Table 2 for column details; note that the numbers are not comparable to Table 2 as the test set is different).
<R> <C> [BOLD] Dataset <C> [BOLD] Part <C> [BOLD] MRs <C> [BOLD] Refs <C> [BOLD] SER(%) <R> <C> Original <C> Train <C> 4,862 <C> 42,061 <C> 17.69 <R> <C> Original <C> Dev <C> 547 <C> 4,672 <C> 11.42 <R> <C> Original <C> Test <C> 630 <C> 4,693 <C> 11.49 <R> <C> [0.5pt/2pt] Cleaned <C> Train <C> 8,362 <C> 33,525 <C> (0.00) <R> <C> [0.5pt/2pt] Cleaned <C> Dev <C> 1,132 <C> 4,299 <C> (0.00) <R> <C> [0.5pt/2pt] Cleaned <C> Test <C> 1,358 <C> 4,693 <C> (0.00) <CAP> Table 1: Data statistics comparison for the original E2E data and our cleaned version (number of distinct MRs, total number of textual references, SER as measured by our slot matching script, see Section 3).
<R> <C> Train <C> Test <C> [BOLD] System <C> [BOLD] BLEU <C> [BOLD] NIST <C> [BOLD] METEOR <C> [BOLD] ROUGE-L <C> [BOLD] CIDEr <C> [BOLD] Add <C> [BOLD] Miss <C> [BOLD] Wrong <C> [BOLD] SER <R> <C> Original <C> [BOLD] Original <C> TGen− <C> 63.37 <C> 7.7188 <C> 41.99 <C> 68.53 <C> 1.9355 <C> 00.06 <C> 15.77 <C> 00.11 <C> 15.94 <R> <C> Original <C> [BOLD] Original <C> TGen <C> 66.41 <C> 8.5565 <C> 45.07 <C> 69.17 <C> 2.2253 <C> 00.14 <C> 04.11 <C> 00.03 <C> 04.27 <R> <C> Original <C> [BOLD] Original <C> TGen+ <C> 67.06 <C> 8.5871 <C> 45.83 <C> 69.73 <C> 2.2681 <C> 00.04 <C> 01.75 <C> 00.01 <C> 01.80 <R> <C> Original <C> [BOLD] Original <C> SC-LSTM <C> 39.11 <C> 5.6704 <C> 36.83 <C> 50.02 <C> 0.6045 <C> 02.79 <C> 18.90 <C> 09.79 <C> 31.51 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned <C> [BOLD] Original <C> TGen− <C> 65.87 <C> 8.6400 <C> 44.20 <C> 67.51 <C> 2.1710 <C> 00.20 <C> 00.56 <C> 00.21 <C> 00.97 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned <C> [BOLD] Original <C> TGen <C> 66.24 <C> 8.6889 <C> 44.66 <C> 67.85 <C> 2.2181 <C> 00.10 <C> 00.02 <C> 00.00 <C> 00.12 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned <C> [BOLD] Original <C> TGen+ <C> 65.97 <C> 8.6630 <C> 44.45 <C> 67.59 <C> 2.1855 <C> 00.02 <C> 00.00 <C> 00.00 <C> 00.03 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned <C> [BOLD] Original <C> SC-LSTM <C> 38.52 <C> 5.7125 <C> 37.45 <C> 48.50 <C> 0.4343 <C> 03.85 <C> 17.39 <C> 08.12 <C> 29.37 <R> <C> Cleaned missing <C> [BOLD] Original <C> TGen− <C> 66.28 <C> 8.5202 <C> 43.96 <C> 67.83 <C> 2.1375 <C> 00.14 <C> 02.26 <C> 00.22 <C> 02.61 <R> <C> Cleaned missing <C> [BOLD] Original <C> TGen <C> 67.00 <C> 8.6889 <C> 44.97 <C> 68.19 <C> 2.2228 <C> 00.06 <C> 00.44 <C> 00.03 <C> 00.53 <R> <C> Cleaned missing <C> [BOLD] Original <C> TGen+ <C> 66.74 <C> 8.6649 <C> 44.84 <C> 67.95 <C> 2.2018 <C> 00.00 <C> 00.21 <C> 00.03 <C> 00.24 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added <C> [BOLD] Original <C> TGen− <C> 64.40 <C> 7.9692 <C> 42.81 <C> 68.87 <C> 2.0563 <C> 00.01 <C> 13.08 <C> 00.00 <C> 13.09 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added <C> [BOLD] Original <C> TGen <C> 66.23 <C> 8.5578 <C> 45.12 <C> 68.87 <C> 2.2548 <C> 00.04 <C> 03.04 <C> 00.00 <C> 03.09 <R> <C> 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added <C> [BOLD] Original <C> TGen+ <C> 65.96 <C> 8.5238 <C> 45.49 <C> 68.79 <C> 2.2456 <C> 00.00 <C> 01.44 <C> 00.00 <C> 01.45 <CAP> Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section 5.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table 3 as the test set is different.
<R> <C> [BOLD] Training data <C> [BOLD] Add <C> [BOLD] Miss <C> [BOLD] Wrong <C> [BOLD] Disfl <R> <C> Original <C> 0 <C> 22 <C> 0 <C> 14 <R> <C> Cleaned added <C> 0 <C> 23 <C> 0 <C> 14 <R> <C> Cleaned missing <C> 0 <C> 1 <C> 0 <C> 2 <R> <C> Cleaned <C> 0 <C> 0 <C> 0 <C> 5 <CAP> Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).
<R> <C> [BOLD] Model <C> [BOLD] External <C> B <R> <C> Seq2SeqK (Konstas et al.,  2017 ) <C> - <C> 22.0 <R> <C> GraphLSTM (Song et al.,  2018 ) <C> - <C> 23.3 <R> <C> GCNSEQ (Damonte and Cohen,  2019 ) <C> - <C> 24.4 <R> <C> DCGCN(single) <C> - <C> 25.9 <R> <C> DCGCN(ensemble) <C> - <C> [BOLD] 28.2 <R> <C> TSP (Song et al.,  2016 ) <C> ALL <C> 22.4 <R> <C> PBMT (Pourdamghani et al.,  2016 ) <C> ALL <C> 26.9 <R> <C> Tree2Str (Flanigan et al.,  2016 ) <C> ALL <C> 23.0 <R> <C> SNRG (Song et al.,  2017 ) <C> ALL <C> 25.6 <R> <C> Seq2SeqK (Konstas et al.,  2017 ) <C> 0.2M <C> 27.4 <R> <C> GraphLSTM (Song et al.,  2018 ) <C> 0.2M <C> 28.2 <R> <C> DCGCN(single) <C> 0.1M <C> 29.0 <R> <C> DCGCN(single) <C> 0.2M <C> [BOLD] 31.6 <R> <C> Seq2SeqK (Konstas et al.,  2017 ) <C> 2M <C> 32.3 <R> <C> GraphLSTM (Song et al.,  2018 ) <C> 2M <C> 33.6 <R> <C> Seq2SeqK (Konstas et al.,  2017 ) <C> 20M <C> 33.8 <R> <C> DCGCN(single) <C> 0.3M <C> 33.2 <R> <C> DCGCN(ensemble) <C> 0.3M <C> [BOLD] 35.3 <CAP> Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M
<R> <C> [BOLD] Model <C> [BOLD] T <C> #P <C> B <C> C <R> <C> Seq2SeqB (Beck et al.,  2018 ) <C> S <C> 28,4M <C> 21.7 <C> 49.1 <R> <C> GGNN2Seq (Beck et al.,  2018 ) <C> S <C> 28.3M <C> 23.3 <C> 50.4 <R> <C> Seq2SeqB (Beck et al.,  2018 ) <C> E <C> 142M <C> 26.6 <C> 52.5 <R> <C> GGNN2Seq (Beck et al.,  2018 ) <C> E <C> 141M <C> 27.5 <C> 53.5 <R> <C> DCGCN (ours) <C> S <C> [BOLD] 19.1M <C> 27.9 <C> 57.3 <R> <C> DCGCN (ours) <C> E <C> 92.5M <C> [BOLD] 30.4 <C> [BOLD] 59.6 <CAP> Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; “S” and “E” denote single and ensemble models, respectively.
<R> <C> [BOLD] Model <C> [BOLD] Type <C> [BOLD] English-German #P <C> [BOLD] English-German B <C> [BOLD] English-German C <C> [BOLD] English-Czech #P <C> [BOLD] English-Czech B <C> [BOLD] English-Czech C <R> <C> BoW+GCN (Bastings et al.,  2017 ) <C> Single <C> - <C> 12.2 <C> - <C> - <C> 7.5 <C> - <R> <C> CNN+GCN (Bastings et al.,  2017 ) <C> Single <C> - <C> 13.7 <C> - <C> - <C> 8.7 <C> - <R> <C> BiRNN+GCN (Bastings et al.,  2017 ) <C> Single <C> - <C> 16.1 <C> - <C> - <C> 9.6 <C> - <R> <C> PB-SMT (Beck et al.,  2018 ) <C> Single <C> - <C> 12.8 <C> 43.2 <C> - <C> 8.6 <C> 36.4 <R> <C> Seq2SeqB (Beck et al.,  2018 ) <C> Single <C> 41.4M <C> 15.5 <C> 40.8 <C> 39.1M <C> 8.9 <C> 33.8 <R> <C> GGNN2Seq (Beck et al.,  2018 ) <C> Single <C> 41.2M <C> 16.7 <C> 42.4 <C> 38.8M <C> 9.8 <C> 33.3 <R> <C> DCGCN (ours) <C> Single <C> [BOLD]  29.7M <C> [BOLD] 19.0 <C> [BOLD] 44.1 <C> [BOLD]  28.3M <C> [BOLD] 12.1 <C> [BOLD] 37.1 <R> <C> Seq2SeqB (Beck et al.,  2018 ) <C> Ensemble <C> 207M <C> 19.0 <C> 44.1 <C> 195M <C> 11.3 <C> 36.4 <R> <C> GGNN2Seq (Beck et al.,  2018 ) <C> Ensemble <C> 206M <C> 19.6 <C> 45.1 <C> 194M <C> 11.7 <C> 35.9 <R> <C> DCGCN (ours) <C> Ensemble <C> [BOLD]  149M <C> [BOLD] 20.5 <C> [BOLD] 45.8 <C> [BOLD]  142M <C> [BOLD] 13.1 <C> [BOLD] 37.8 <CAP> Table 4: Main results on English-German and English-Czech datasets.
<R> <C> [ITALIC] Block <C> [ITALIC] n <C> [ITALIC] m <C> B <C> C <R> <C> 1 <C> 1 <C> 1 <C> 17.6 <C> 48.3 <R> <C> 1 <C> 1 <C> 2 <C> 19.2 <C> 50.3 <R> <C> 1 <C> 2 <C> 1 <C> 18.4 <C> 49.1 <R> <C> 1 <C> 1 <C> 3 <C> 19.6 <C> 49.4 <R> <C> 1 <C> 3 <C> 1 <C> 20.0 <C> 50.5 <R> <C> 1 <C> 3 <C> 3 <C> 21.4 <C> 51.0 <R> <C> 1 <C> 3 <C> 6 <C> 21.8 <C> 51.7 <R> <C> 1 <C> 6 <C> 3 <C> 21.7 <C> 51.5 <R> <C> 1 <C> 6 <C> 6 <C> 22.0 <C> 52.1 <R> <C> 2 <C> 3 <C> 6 <C> [BOLD] 23.5 <C> 53.3 <R> <C> 2 <C> 6 <C> 3 <C> 23.3 <C> [BOLD] 53.4 <R> <C> 2 <C> 6 <C> 6 <C> 22.0 <C> 52.1 <CAP> Table 5: The effect of the number of layers inside DCGCN sub-blocks on the AMR15 development set.
<R> <C> [BOLD] GCN +RC (2) <C> B 16.8 <C> C 48.1 <C> [BOLD] GCN +RC+LA (2) <C> B 18.3 <C> C 47.9 <R> <C> +RC (4) <C> 18.4 <C> 49.6 <C> +RC+LA (4) <C> 18.0 <C> 51.1 <R> <C> +RC (6) <C> 19.9 <C> 49.7 <C> +RC+LA (6) <C> 21.3 <C> 50.8 <R> <C> +RC (9) <C> [BOLD] 21.1 <C> 50.5 <C> +RC+LA (9) <C> [BOLD] 22.0 <C> 52.6 <R> <C> +RC (10) <C> 20.7 <C> [BOLD] 50.7 <C> +RC+LA (10) <C> 21.2 <C> [BOLD] 52.9 <R> <C> DCGCN1 (9) <C> 22.9 <C> 53.0 <C> DCGCN3 (27) <C> 24.8 <C> 54.7 <R> <C> DCGCN2 (18) <C> 24.2 <C> 54.4 <C> DCGCN4 (36) <C> [BOLD] 25.5 <C> [BOLD] 55.4 <CAP> Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i×(n+m) layers. The number of layers for each model is shown in parenthesis.
<R> <C> [BOLD] Model <C> D <C> #P <C> B <C> C <R> <C> DCGCN(1) <C> 300 <C> 10.9M <C> 20.9 <C> 52.0 <R> <C> DCGCN(2) <C> 180 <C> 10.9M <C> [BOLD] 22.2 <C> [BOLD] 52.3 <R> <C> DCGCN(2) <C> 240 <C> 11.3M <C> 22.8 <C> 52.8 <R> <C> DCGCN(4) <C> 180 <C> 11.4M <C> [BOLD] 23.4 <C> [BOLD] 53.4 <R> <C> DCGCN(1) <C> 420 <C> 12.6M <C> 22.2 <C> 52.4 <R> <C> DCGCN(2) <C> 300 <C> 12.5M <C> 23.8 <C> 53.8 <R> <C> DCGCN(3) <C> 240 <C> 12.3M <C> [BOLD] 23.9 <C> [BOLD] 54.1 <R> <C> DCGCN(2) <C> 360 <C> 14.0M <C> 24.2 <C> [BOLD] 54.4 <R> <C> DCGCN(3) <C> 300 <C> 14.0M <C> [BOLD] 24.4 <C> 54.2 <R> <C> DCGCN(2) <C> 420 <C> 15.6M <C> 24.1 <C> 53.7 <R> <C> DCGCN(4) <C> 300 <C> 15.6M <C> [BOLD] 24.6 <C> [BOLD] 54.8 <R> <C> DCGCN(3) <C> 420 <C> 18.6M <C> 24.5 <C> 54.6 <R> <C> DCGCN(4) <C> 360 <C> 18.4M <C> [BOLD] 25.5 <C> [BOLD] 55.4 <CAP> Table 7: Comparisons of different DCGCN models under almost the same parameter budget.
<R> <C> [BOLD] Model <C> B <C> C <R> <C> DCGCN4 <C> 25.5 <C> 55.4 <R> <C> -{4} dense block <C> 24.8 <C> 54.9 <R> <C> -{3, 4} dense blocks <C> 23.8 <C> 54.1 <R> <C> -{2, 3, 4} dense blocks <C> 23.2 <C> 53.1 <CAP> Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.
<R> <C> [BOLD] Model <C> B <C> C <R> <C> DCGCN4 <C> 25.5 <C> 55.4 <R> <C> Encoder Modules <C> [EMPTY] <C> [EMPTY] <R> <C> -Linear Combination <C> 23.7 <C> 53.2 <R> <C> -Global Node <C> 24.2 <C> 54.6 <R> <C> -Direction Aggregation <C> 24.6 <C> 54.6 <R> <C> -Graph Attention <C> 24.9 <C> 54.7 <R> <C> -Global Node&Linear Combination <C> 22.9 <C> 52.4 <R> <C> Decoder Modules <C> [EMPTY] <C> [EMPTY] <R> <C> -Coverage Mechanism <C> 23.8 <C> 53.0 <CAP> Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder
<R> <C> Initialization <C> Depth <C> BShift <C> SubjNum <C> Tense <C> CoordInv <C> Length <C> ObjNum <C> TopConst <C> SOMO <C> WC <R> <C> N(0,0.1) <C> 29.7 <C> 71.5 <C> 82.0 <C> 78.5 <C> 60.1 <C> 80.5 <C> 76.3 <C> 74.7 <C> [BOLD] 51.3 <C> 52.5 <R> <C> Glorot <C> 31.3 <C> [BOLD] 72.3 <C> 81.8 <C> 78.7 <C> 59.4 <C> 81.3 <C> 76.6 <C> [BOLD] 74.6 <C> 50.4 <C> 57.0 <R> <C> Our paper <C> [BOLD] 35.1 <C> 70.8 <C> [BOLD] 82.0 <C> [BOLD] 80.2 <C> [BOLD] 61.8 <C> [BOLD] 82.8 <C> [BOLD] 79.7 <C> 74.2 <C> 50.7 <C> [BOLD] 72.9 <CAP> Table 7: Scores for initialization strategies on probing tasks.
<R> <C> Dim <C> Method <C> Depth <C> BShift <C> SubjNum <C> Tense <C> CoordInv <C> Length <C> ObjNum <C> TopConst <C> SOMO <C> WC <R> <C> 400 <C> CBOW/400 <C> 32.5 <C> 50.2 <C> 78.9 <C> 78.7 <C> 53.6 <C> 73.6 <C> 79.0 <C> 69.6 <C> 48.9 <C> 86.7 <R> <C> 400 <C> CMOW/400 <C> [BOLD] 34.4 <C> 68.8 <C> 80.1 <C> [BOLD] 79.9 <C> [BOLD] 59.8 <C> 81.9 <C> [BOLD] 79.2 <C> [BOLD] 70.7 <C> [BOLD] 50.3 <C> 70.7 <R> <C> 400 <C> H-CBOW <C> 31.2 <C> 50.2 <C> 77.2 <C> 78.8 <C> 52.6 <C> 77.5 <C> 76.1 <C> 66.1 <C> 49.2 <C> [BOLD] 87.2 <R> <C> 400 <C> H-CMOW <C> 32.3 <C> [BOLD] 70.8 <C> [BOLD] 81.3 <C> 76.0 <C> 59.6 <C> [BOLD] 82.3 <C> 77.4 <C> 70.0 <C> 50.2 <C> 38.2 <R> <C> 784 <C> CBOW/784 <C> 33.0 <C> 49.6 <C> 79.3 <C> 78.4 <C> 53.6 <C> 74.5 <C> 78.6 <C> 72.0 <C> 49.6 <C> [BOLD] 89.5 <R> <C> 784 <C> CMOW/784 <C> [BOLD] 35.1 <C> [BOLD] 70.8 <C> [BOLD] 82.0 <C> 80.2 <C> [BOLD] 61.8 <C> 82.8 <C> [BOLD] 79.7 <C> 74.2 <C> [BOLD] 50.7 <C> 72.9 <R> <C> 800 <C> Hybrid <C> 35.0 <C> [BOLD] 70.8 <C> 81.7 <C> [BOLD] 81.0 <C> 59.4 <C> [BOLD] 84.4 <C> 79.0 <C> [BOLD] 74.3 <C> 49.3 <C> 87.6 <R> <C> - <C> cmp. CBOW <C> +6.1% <C> +42.7% <C> +3% <C> +3.3% <C> +10.8% <C> +13.3% <C> +0.5% <C> +3.2% <C> -0.6% <C> -2.1% <R> <C> - <C> cmp. CMOW <C> -0.3% <C> +-0% <C> -0.4% <C> +1% <C> -3.9% <C> +1.9% <C> -0.9% <C> +0.1% <C> -2.8% <C> +20.9% <CAP> Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.
<R> <C> Method <C> SUBJ <C> CR <C> MR <C> MPQA <C> MRPC <C> TREC <C> SICK-E <C> SST2 <C> SST5 <C> STS-B <C> SICK-R <R> <C> CBOW/784 <C> 90.0 <C> [BOLD] 79.2 <C> [BOLD] 74.0 <C> 87.1 <C> 71.6 <C> 85.6 <C> 78.9 <C> 78.5 <C> 42.1 <C> 61.0 <C> [BOLD] 78.1 <R> <C> CMOW/784 <C> 87.5 <C> 73.4 <C> 70.6 <C> [BOLD] 87.3 <C> 69.6 <C> [BOLD] 88.0 <C> 77.2 <C> 74.7 <C> 37.9 <C> 56.5 <C> 76.2 <R> <C> Hybrid <C> [BOLD] 90.2 <C> 78.7 <C> 73.7 <C> [BOLD] 87.3 <C> [BOLD] 72.7 <C> 87.6 <C> [BOLD] 79.4 <C> [BOLD] 79.6 <C> [BOLD] 43.3 <C> [BOLD] 63.4 <C> 77.8 <R> <C> cmp. CBOW <C> +0.2% <C> -0.6% <C> -0.4% <C> +0.2% <C> +1.5% <C> +2.3% <C> +0.6% <C> +1.4% <C> +2.9% <C> +3.9% <C> -0.4% <R> <C> cmp. CMOW <C> +3.1% <C> +7.2% <C> +4.4% <C> +0% <C> +4.5% <C> -0.5% <C> +2.9% <C> +6.7% <C> +14.3 <C> +12.2% <C> +2.1% <CAP> Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.
<R> <C> Method <C> STS12 <C> STS13 <C> STS14 <C> STS15 <C> STS16 <R> <C> CBOW <C> 43.5 <C> [BOLD] 50.0 <C> [BOLD] 57.7 <C> [BOLD] 63.2 <C> 61.0 <R> <C> CMOW <C> 39.2 <C> 31.9 <C> 38.7 <C> 49.7 <C> 52.2 <R> <C> Hybrid <C> [BOLD] 49.6 <C> 46.0 <C> 55.1 <C> 62.4 <C> [BOLD] 62.1 <R> <C> cmp. CBOW <C> +14.6% <C> -8% <C> -4.5% <C> -1.5% <C> +1.8% <R> <C> cmp. CMOW <C> +26.5% <C> +44.2% <C> +42.4 <C> +25.6% <C> +19.0% <CAP> Table 3: Scores on unsupervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.
<R> <C> Initialization <C> SUBJ <C> CR <C> MR <C> MPQA <C> MRPC <C> TREC <C> SICK-E <C> SST2 <C> SST5 <C> STS-B <C> SICK-R <R> <C> N(0,0.1) <C> 85.6 <C> 71.5 <C> 68.4 <C> 86.2 <C> [BOLD] 71.6 <C> 86.4 <C> 73.7 <C> 72.3 <C> [BOLD] 38.2 <C> 53.7 <C> 72.7 <R> <C> Glorot <C> 86.2 <C> [BOLD] 74.4 <C> 69.5 <C> 86.5 <C> 71.4 <C> [BOLD] 88.4 <C> 75.4 <C> 73.2 <C> [BOLD] 38.2 <C> 54.1 <C> 73.6 <R> <C> Our paper <C> [BOLD] 87.5 <C> 73.4 <C> [BOLD] 70.6 <C> [BOLD] 87.3 <C> 69.6 <C> 88.0 <C> [BOLD] 77.2 <C> [BOLD] 74.7 <C> 37.9 <C> [BOLD] 56.5 <C> [BOLD] 76.2 <CAP> Table 8: Scores for initialization strategies on supervised downstream tasks.
<R> <C> Method <C> STS12 <C> STS13 <C> STS14 <C> STS15 <C> STS16 <R> <C> CMOW-C <C> 27.6 <C> 14.6 <C> 22.1 <C> 33.2 <C> 41.6 <R> <C> CMOW-R <C> [BOLD] 39.2 <C> [BOLD] 31.9 <C> [BOLD] 38.7 <C> [BOLD] 49.7 <C> [BOLD] 52.2 <R> <C> CBOW-C <C> [BOLD] 43.5 <C> 49.2 <C> [BOLD] 57.9 <C> [BOLD] 63.7 <C> [BOLD] 61.6 <R> <C> CBOW-R <C> [BOLD] 43.5 <C> [BOLD] 50.0 <C> 57.7 <C> 63.2 <C> 61.0 <CAP> Table 6: Scores for different training objectives on the unsupervised downstream tasks.
<R> <C> Method <C> Depth <C> BShift <C> SubjNum <C> Tense <C> CoordInv <C> Length <C> ObjNum <C> TopConst <C> SOMO <C> WC <R> <C> CMOW-C <C> [BOLD] 36.2 <C> 66.0 <C> 81.1 <C> 78.7 <C> 61.7 <C> [BOLD] 83.9 <C> 79.1 <C> 73.6 <C> 50.4 <C> 66.8 <R> <C> CMOW-R <C> 35.1 <C> [BOLD] 70.8 <C> [BOLD] 82.0 <C> [BOLD] 80.2 <C> [BOLD] 61.8 <C> 82.8 <C> [BOLD] 79.7 <C> [BOLD] 74.2 <C> [BOLD] 50.7 <C> [BOLD] 72.9 <R> <C> CBOW-C <C> [BOLD] 34.3 <C> [BOLD] 50.5 <C> [BOLD] 79.8 <C> [BOLD] 79.9 <C> 53.0 <C> [BOLD] 75.9 <C> [BOLD] 79.8 <C> [BOLD] 72.9 <C> 48.6 <C> 89.0 <R> <C> CBOW-R <C> 33.0 <C> 49.6 <C> 79.3 <C> 78.4 <C> [BOLD] 53.6 <C> 74.5 <C> 78.6 <C> 72.0 <C> [BOLD] 49.6 <C> [BOLD] 89.5 <CAP> Table 4: Scores for different training objectives on the linguistic probing tasks.
<R> <C> Method <C> SUBJ <C> CR <C> MR <C> MPQA <C> MRPC <C> TREC <C> SICK-E <C> SST2 <C> SST5 <C> STS-B <C> SICK-R <R> <C> CMOW-C <C> 85.9 <C> 72.1 <C> 69.4 <C> 87.0 <C> [BOLD] 71.9 <C> 85.4 <C> 74.2 <C> 73.8 <C> 37.6 <C> 54.6 <C> 71.3 <R> <C> CMOW-R <C> [BOLD] 87.5 <C> [BOLD] 73.4 <C> [BOLD] 70.6 <C> [BOLD] 87.3 <C> 69.6 <C> [BOLD] 88.0 <C> [BOLD] 77.2 <C> [BOLD] 74.7 <C> [BOLD] 37.9 <C> [BOLD] 56.5 <C> [BOLD] 76.2 <R> <C> CBOW-C <C> [BOLD] 90.0 <C> [BOLD] 79.3 <C> [BOLD] 74.6 <C> [BOLD] 87.5 <C> [BOLD] 72.9 <C> 85.0 <C> [BOLD] 80.0 <C> 78.4 <C> 41.0 <C> 60.5 <C> [BOLD] 79.2 <R> <C> CBOW-R <C> [BOLD] 90.0 <C> 79.2 <C> 74.0 <C> 87.1 <C> 71.6 <C> [BOLD] 85.6 <C> 78.9 <C> [BOLD] 78.5 <C> [BOLD] 42.1 <C> [BOLD] 61.0 <C> 78.1 <CAP> Table 5: Scores for different training objectives on the supervised downstream tasks.
<R> <C> System <C> All LOC <C> All ORG <C> All PER <C> All MISC <C> In  [ITALIC] E+ LOC <C> In  [ITALIC] E+ ORG <C> In  [ITALIC] E+ PER <C> In  [ITALIC] E+ MISC <R> <C> Name matching <C> 96.26 <C> 89.48 <C> 57.38 <C> 96.60 <C> 92.32 <C> 76.87 <C> 47.40 <C> 76.29 <R> <C> MIL <C> 57.09 <C> [BOLD] 76.30 <C> 41.35 <C> 93.35 <C> 11.90 <C> [BOLD] 47.90 <C> 27.60 <C> 53.61 <R> <C> MIL-ND <C> 57.15 <C> 77.15 <C> 35.95 <C> 92.47 <C> 12.02 <C> 49.77 <C> 20.94 <C> 47.42 <R> <C> [ITALIC] τMIL-ND <C> [BOLD] 55.15 <C> 76.56 <C> [BOLD] 34.03 <C> [BOLD] 92.15 <C> [BOLD] 11.14 <C> 51.18 <C> [BOLD] 20.59 <C> [BOLD] 40.00 <R> <C> Supervised learning <C> 55.58 <C> 61.32 <C> 24.98 <C> 89.96 <C> 8.80 <C> 14.95 <C> 7.40 <C> 29.90 <CAP> Table 3: % errors on the development set for different named entity types under two settings. (Smaller is better.)
<R> <C> System <C> All P <C> All R <C> All F1 <C> In  [ITALIC] E+ P <C> In  [ITALIC] E+ R <C> In  [ITALIC] E+ F1 <R> <C> Name matching <C> 15.03 <C> 15.03 <C> 15.03 <C> 29.13 <C> 29.13 <C> 29.13 <R> <C> MIL (model 1) <C> 35.87 <C> 35.87 <C> 35.87 ±0.72 <C> 69.38 <C> 69.38 <C> 69.38 ±1.29 <R> <C> MIL-ND (model 2) <C> 37.42 <C> [BOLD] 37.42 <C> 37.42 ±0.35 <C> 72.50 <C> [BOLD] 72.50 <C> [BOLD] 72.50 ±0.68 <R> <C> [ITALIC] τMIL-ND (model 2) <C> [BOLD] 38.91 <C> 36.73 <C> [BOLD] 37.78 ±0.26 <C> [BOLD] 73.19 <C> 71.15 <C> 72.16 ±0.48 <R> <C> Supervised learning <C> 42.90 <C> 42.90 <C> 42.90 ±0.59 <C> 83.12 <C> 83.12 <C> 83.12 ±1.15 <CAP> Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.
<R> <C> <bold>Model</bold> <C> REF ⇒ GEN <bold>ENT</bold> <C> REF ⇒ GEN <bold>CON</bold> <C> REF ⇒ GEN <bold>NEU</bold> <R> <C> S2S <C> 38.45 <C> 11.17 <C> 50.38 <R> <C> G2S-GIN <C> 49.78 <C> 9.80 <C> 40.42 <R> <C> G2S-GAT <C> 49.48 <C> 8.09 <C> 42.43 <R> <C> G2S-GGNN <C> 51.32 <C> 8.82 <C> 39.86 <R> <C> [EMPTY] <C> GEN ⇒ REF <C> GEN ⇒ REF <C> GEN ⇒ REF <R> <C> <bold>Model</bold> <C> <bold>ENT</bold> <C> <bold>CON</bold> <C> <bold>NEU</bold> <R> <C> S2S <C> 73.79 <C> 12.75 <C> 13.46 <R> <C> G2S-GIN <C> 76.27 <C> 10.65 <C> 13.08 <R> <C> G2S-GAT <C> 77.54 <C> 8.54 <C> 13.92 <R> <C> G2S-GGNN <C> 77.64 <C> 9.64 <C> 12.72 <CAP> Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.
<R> <C> <bold>Model</bold> <C> <bold>BLEU</bold> <C> <bold>METEOR</bold> <R> <C> LDC2015E86 <C> LDC2015E86 <C> LDC2015E86 <R> <C> Konstas et al. (2017) <C> 22.00 <C> - <R> <C> Song et al. (2018) <C> 23.28 <C> 30.10 <R> <C> Cao et al. (2019) <C> 23.50 <C> - <R> <C> Damonte et al.(2019) <C> 24.40 <C> 23.60 <R> <C> Guo et al. (2019) <C> <bold>25.70</bold> <C> - <R> <C> S2S <C> 22.55 ± 0.17 <C> 29.90 ± 0.31 <R> <C> G2S-GIN <C> 22.93 ± 0.20 <C> 29.72 ± 0.09 <R> <C> G2S-GAT <C> 23.42 ± 0.16 <C> 29.87 ± 0.14 <R> <C> G2S-GGNN <C> 24.32 ± 0.16 <C> <bold>30.53</bold> ± 0.30 <R> <C> LDC2017T10 <C> LDC2017T10 <C> LDC2017T10 <R> <C> Back et al. (2018) <C> 23.30 <C> - <R> <C> Song et al. (2018) <C> 24.86 <C> 31.56 <R> <C> Damonte et al.(2019) <C> 24.54 <C> 24.07 <R> <C> Cao et al. (2019) <C> 26.80 <C> - <R> <C> Guo et al. (2019) <C> 27.60 <C> - <R> <C> S2S <C> 22.73 ± 0.18 <C> 30.15 ± 0.14 <R> <C> G2S-GIN <C> 26.90 ± 0.19 <C> 32.62 ± 0.04 <R> <C> G2S-GAT <C> 26.72 ± 0.20 <C> 32.52 ± 0.02 <R> <C> G2S-GGNN <C> <bold>27.87</bold> ± 0.15 <C> <bold>33.21</bold> ± 0.15 <CAP> Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.
<R> <C> <bold>Model</bold> <C> <bold>External</bold> <C> <bold>BLEU</bold> <R> <C> Konstas et al. (2017) <C> 200K <C> 27.40 <R> <C> Song et al. (2018) <C> 200K <C> 28.20 <R> <C> Guo et al. (2019) <C> 200K <C> 31.60 <R> <C> G2S-GGNN <C> 200K <C> <bold>32.23</bold> <CAP> Table 3: Results on LDC2015E86 test set when models are trained with additional Gigaword data.
<R> <C> <bold>Model</bold> <C> <bold>BLEU</bold> <C> <bold>METEOR</bold> <C> <bold>Size</bold> <R> <C> biLSTM <C> 22.50 <C> 30.42 <C> 57.6M <R> <C> <italic>GEt</italic> + biLSTM <C> 26.33 <C> 32.62 <C> 59.6M <R> <C> <italic>GEb</italic> + biLSTM <C> 26.12 <C> 32.49 <C> 59.6M <R> <C> <italic>GEt</italic> + <italic>GEb</italic> + biLSTM <C> 27.37 <C> 33.30 <C> 61.7M <CAP> Table 4: Results of the ablation study on the LDC2017T10 development set.
<R> <C> <bold>Model</bold> <C> <bold>Graph Diameter</bold> 0-7 Δ <C> <bold>Graph Diameter</bold> 7-13 Δ <C> <bold>Graph Diameter</bold> 14-20 Δ <R> <C> S2S <C> 33.2 <C> 29.7 <C> 28.8 <R> <C> G2S-GIN <C> 35.2 +6.0% <C> 31.8 +7.4% <C> 31.5 +9.2% <R> <C> G2S-GAT <C> 35.1 +5.9% <C> 32.0 +7.8% <C> 31.5 +9.51% <R> <C> G2S-GGNN <C> 36.2 +9.0% <C> 33.0 +11.4% <C> 30.7 +6.7% <R> <C> [EMPTY] <C> <bold>Sentence Length</bold> <C> <bold>Sentence Length</bold> <C> <bold>Sentence Length</bold> <R> <C> [EMPTY] <C> 0-20 Δ <C> 20-50 Δ <C> 50-240 Δ <R> <C> S2S <C> 34.9 <C> 29.9 <C> 25.1 <R> <C> G2S-GIN <C> 36.7 +5.2% <C> 32.2 +7.8% <C> 26.5 +5.8% <R> <C> G2S-GAT <C> 36.9 +5.7% <C> 32.3 +7.9% <C> 26.6 +6.1% <R> <C> G2S-GGNN <C> 37.9 +8.5% <C> 33.3 +11.2% <C> 26.9 +6.8% <R> <C> [EMPTY] <C> <bold>Max Node Out-degree</bold> <C> <bold>Max Node Out-degree</bold> <C> <bold>Max Node Out-degree</bold> <R> <C> [EMPTY] <C> 0-3 Δ <C> 4-8 Δ <C> 9-18 Δ <R> <C> S2S <C> 31.7 <C> 30.0 <C> 23.9 <R> <C> G2S-GIN <C> 33.9 +6.9% <C> 32.1 +6.9% <C> 25.4 +6.2% <R> <C> G2S-GAT <C> 34.3 +8.0% <C> 32.0 +6.7% <C> 22.5 -6.0% <R> <C> G2S-GGNN <C> 35.0 +10.3% <C> 33.1 +10.4% <C> 22.2 -7.3% <CAP> Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.
<R> <C> <bold>Model</bold> <C> <bold>ADDED</bold> <C> <bold>MISS</bold> <R> <C> S2S <C> 47.34 <C> 37.14 <R> <C> G2S-GIN <C> 48.67 <C> 33.64 <R> <C> G2S-GAT <C> 48.24 <C> 33.73 <R> <C> G2S-GGNN <C> 48.66 <C> 34.06 <R> <C> GOLD <C> 50.77 <C> 28.35 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 8: Fraction of elements in the output that are not present in the input (ADDED) and the fraction of elements in the input graph that are missing in the generated sentence (MISS), for the test set of LDC2017T10. The token lemmas are used in the comparison. GOLD refers to the reference sentences.
<R> <C> [EMPTY] <C> Ar <C> Es <C> Fr <C> Ru <C> Zh <C> En <R> <C> POS <C> 88.7 <C> 90.0 <C> 89.6 <C> 88.6 <C> 87.4 <C> 85.2 <R> <C> SEM <C> 85.3 <C> 86.1 <C> 85.8 <C> 85.2 <C> 85.0 <C> 80.7 <CAP> Table 4: SEM and POS tagging accuracy using features extracted from the 4th NMT encoding layer, trained with different target languages on a smaller parallel corpus (200K sentences).
<R> <C> [EMPTY] <C> MFT <C> UnsupEmb <C> Word2Tag <R> <C> POS <C> 91.95 <C> 87.06 <C> 95.55 <R> <C> SEM <C> 82.00 <C> 81.11 <C> 91.41 <CAP> Table 2: POS and SEM tagging accuracy with baselines and an upper bound. MFT: most frequent tag; UnsupEmb: classifier using unsupervised word embeddings; Word2Tag: upper bound encoder-decoder.
<R> <C> [ITALIC] k <C> Ar <C> Es <C> Fr <C> Ru <C> Zh <C> En <R> <C> POS Tagging Accuracy <C> POS Tagging Accuracy <C> POS Tagging Accuracy <C> POS Tagging Accuracy <C> POS Tagging Accuracy <C> POS Tagging Accuracy <C> POS Tagging Accuracy <R> <C> 0 <C> 88.0 <C> 87.9 <C> 87.9 <C> 87.8 <C> 87.7 <C> 87.4 <R> <C> 1 <C> 92.4 <C> 91.9 <C> 92.1 <C> 92.1 <C> 91.5 <C> 89.4 <R> <C> 2 <C> 91.9 <C> 91.8 <C> 91.8 <C> 91.8 <C> 91.3 <C> 88.3 <R> <C> 3 <C> 92.0 <C> 92.3 <C> 92.1 <C> 91.6 <C> 91.2 <C> 87.9 <R> <C> 4 <C> 92.1 <C> 92.4 <C> 92.5 <C> 92.0 <C> 90.5 <C> 86.9 <R> <C> SEM Tagging Accuracy <C> SEM Tagging Accuracy <C> SEM Tagging Accuracy <C> SEM Tagging Accuracy <C> SEM Tagging Accuracy <C> SEM Tagging Accuracy <C> SEM Tagging Accuracy <R> <C> 0 <C> 81.9 <C> 81.9 <C> 81.8 <C> 81.8 <C> 81.8 <C> 81.2 <R> <C> 1 <C> 87.9 <C> 87.7 <C> 87.8 <C> 87.9 <C> 87.7 <C> 84.5 <R> <C> 2 <C> 87.4 <C> 87.5 <C> 87.4 <C> 87.3 <C> 87.2 <C> 83.2 <R> <C> 3 <C> 87.8 <C> 87.9 <C> 87.9 <C> 87.3 <C> 87.3 <C> 82.9 <R> <C> 4 <C> 88.3 <C> 88.6 <C> 88.4 <C> 88.1 <C> 87.7 <C> 82.1 <R> <C> BLEU <C> BLEU <C> BLEU <C> BLEU <C> BLEU <C> BLEU <C> BLEU <R> <C> [EMPTY] <C> 32.7 <C> 49.1 <C> 38.5 <C> 34.2 <C> 32.1 <C> 96.6 <CAP> Table 3: SEM and POS tagging accuracy using features extracted from the k-th encoding layer of 4-layered NMT models trained with different target languages. “En” column is an English autoencoder. BLEU scores are given for reference.
<R> <C> Uni <C> POS <C> 0 87.9 <C> 1 92.0 <C> 2 91.7 <C> 3 91.8 <C> 4 91.9 <R> <C> Uni <C> SEM <C> 81.8 <C> 87.8 <C> 87.4 <C> 87.6 <C> 88.2 <R> <C> Bi <C> POS <C> 87.9 <C> 93.3 <C> 92.9 <C> 93.2 <C> 92.8 <R> <C> Bi <C> SEM <C> 81.9 <C> 91.3 <C> 90.8 <C> 91.9 <C> 91.9 <R> <C> Res <C> POS <C> 87.9 <C> 92.5 <C> 91.9 <C> 92.0 <C> 92.4 <R> <C> Res <C> SEM <C> 81.9 <C> 88.2 <C> 87.5 <C> 87.6 <C> 88.5 <CAP> Table 5: POS and SEM tagging accuracy with features from different layers of 4-layer Uni/Bidirectional/Residual NMT encoders, averaged over all non-English target languages.
<R> <C> Data <C> Task <C> Protected Attribute <C> Δ <R> <C> Dial <C> Sentiment <C> Race <C> 12.2 <R> <C> [EMPTY] <C> Mention <C> Race <C> 14.3 <R> <C> PAN16 <C> Mention <C> Gender <C> 8.1 <R> <C> [EMPTY] <C> Mention <C> Age <C> 9.7 <CAP> Table 8: Attacker’s performance on different datasets. Results are on a training set 10% held-out. Δ is the difference between the attacker score and the corresponding adversary’s accuracy.
<R> <C> Data <C> Task <C> Accuracy <R> <C> Dial <C> Sentiment <C> 67.4 <R> <C> [EMPTY] <C> Mention <C> 81.2 <R> <C> [EMPTY] <C> [ITALIC] Race <C> 83.9 <R> <C> PAN16 <C> Mention <C> 77.5 <R> <C> [EMPTY] <C> [ITALIC] Gender <C> 67.7 <R> <C> [EMPTY] <C> [ITALIC] Age <C> 64.8 <CAP> Table 1: Accuracies when training directly towards a single task.
<R> <C> Data <C> Task <C> Protected Attribute <C> Balanced Task Acc <C> Balanced Leakage <C> Unbalanced Task Acc <C> Unbalanced Leakage <R> <C> Dial <C> Sentiment <C> Race <C> 67.4 <C> 64.5 <C> 79.5 <C> 73.5 <R> <C> [EMPTY] <C> Mention <C> Race <C> 81.2 <C> 71.5 <C> 86.0 <C> 73.8 <R> <C> PAN16 <C> Mention <C> Gender <C> 77.5 <C> 60.1 <C> 76.8 <C> 64.0 <R> <C> [EMPTY] <C> [EMPTY] <C> Age <C> 74.7 <C> 59.4 <C> 77.5 <C> 59.7 <CAP> Table 2: Protected attribute leakage: balanced & unbalanced data splits.
<R> <C> Data <C> Task <C> Protected Attribute <C> Task Acc <C> Leakage <C> Δ <R> <C> Dial <C> Sentiment <C> Race <C> 64.7 <C> 56.0 <C> 5.0 <R> <C> [EMPTY] <C> Mention <C> Race <C> 81.5 <C> 63.1 <C> 9.2 <R> <C> PAN16 <C> Mention <C> Gender <C> 75.6 <C> 58.5 <C> 8.0 <R> <C> [EMPTY] <C> Mention <C> Age <C> 72.5 <C> 57.3 <C> 6.9 <CAP> Table 3: Performances on different datasets with an adversarial training. Δ is the difference between the attacker score and the corresponding adversary’s accuracy.
<R> <C> [EMPTY] <C> [EMPTY] <C> Embedding Leaky <C> Embedding Guarded <R> <C> RNN <C> Leaky <C> 64.5 <C> 67.8 <R> <C> RNN <C> Guarded <C> 59.3 <C> 54.8 <CAP> Table 6: Accuracies of the protected attribute with different encoders.
<R> <C> Model <C> Model <C> #Params <C> PTB Base <C> PTB +Finetune <C> PTB +Dynamic <C> WT2 Base <C> WT2 +Finetune <C> WT2 +Dynamic <R> <C> Yang et al. ( 2018 ) <C> Yang et al. ( 2018 ) <C> 22M <C> 55.97 <C> 54.44 <C> 47.69 <C> 63.33 <C> 61.45 <C> 40.68 <R> <C> This <C> LSTM <C> 22M <C> 63.78 <C> 62.12 <C> [BOLD] 53.11 <C> [BOLD] 69.78 <C> [BOLD] 68.68 <C> [BOLD] 44.60 <R> <C> This <C> GRU <C> 17M <C> 69.09 <C> 67.61 <C> 60.21 <C> 73.37 <C> 73.05 <C> 49.77 <R> <C> This <C> ATR <C> 9M <C> 66.24 <C> 65.86 <C> 58.29 <C> 75.36 <C> 73.35 <C> 48.65 <R> <C> Work <C> SRU <C> 13M <C> 69.64 <C> 65.29 <C> 60.97 <C> 85.15 <C> 84.97 <C> 57.97 <R> <C> [EMPTY] <C> LRN <C> 11M <C> [BOLD] 61.26 <C> [BOLD] 61.00 <C> 54.45 <C> 69.91 <C> 68.86 <C> 46.97 <CAP> Table 5: Test perplexity on PTB and WT2 language modeling task. “#Params”: the parameter number in PTB task. Finetune: fintuning the model after convergence. Dynamic dynamic evaluation. Lower perplexity indicates better performance.
<R> <C> Model <C> Model <C> #Params <C> Base ACC <C> Base Time <C> +LN ACC <C> +LN Time <C> +BERT ACC <C> +BERT Time <C> +LN+BERT ACC <C> +LN+BERT Time <R> <C> Rocktäschel et al. ( 2016 ) <C> Rocktäschel et al. ( 2016 ) <C> 250K <C> 83.50 <C> - <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> This <C> LSTM <C> 8.36M <C> 84.27 <C> 0.262 <C> 86.03 <C> 0.432 <C> 89.95 <C> 0.544 <C> [BOLD] 90.49 <C> 0.696 <R> <C> This <C> GRU <C> 6.41M <C> [BOLD] 85.71 <C> 0.245 <C> [BOLD] 86.05 <C> 0.419 <C> [BOLD] 90.29 <C> 0.529 <C> 90.10 <C> 0.695 <R> <C> This <C> ATR <C> 2.87M <C> 84.88 <C> 0.210 <C> 85.81 <C> 0.307 <C> 90.00 <C> 0.494 <C> 90.28 <C> 0.580 <R> <C> Work <C> SRU <C> 5.48M <C> 84.28 <C> 0.258 <C> 85.32 <C> 0.283 <C> 89.98 <C> 0.543 <C> 90.09 <C> 0.555 <R> <C> [EMPTY] <C> LRN <C> 4.25M <C> 84.88 <C> [BOLD] 0.209 <C> 85.06 <C> [BOLD] 0.223 <C> 89.98 <C> [BOLD] 0.488 <C> 89.93 <C> [BOLD] 0.506 <CAP> Table 1: Test accuracy (ACC) on SNLI task. “#Params”: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.
<R> <C> Model <C> Model <C> #Params <C> AmaPolar ERR <C> AmaPolar Time <C> Yahoo ERR <C> Yahoo Time <C> AmaFull ERR <C> AmaFull Time <C> YelpPolar ERR <C> YelpPolar Time <R> <C> Zhang et al. ( 2015 ) <C> Zhang et al. ( 2015 ) <C> - <C> 6.10 <C> - <C> 29.16 <C> - <C> 40.57 <C> - <C> 5.26 <C> - <R> <C> This <C> LSTM <C> 227K <C> [BOLD] 4.37 <C> 0.947 <C> [BOLD] 24.62 <C> 1.332 <C> 37.22 <C> 1.003 <C> 3.58 <C> 1.362 <R> <C> This <C> GRU <C> 176K <C> 4.39 <C> 0.948 <C> 24.68 <C> 1.242 <C> [BOLD] 37.20 <C> 0.982 <C> [BOLD] 3.47 <C> 1.230 <R> <C> This <C> ATR <C> 74K <C> 4.78 <C> 0.867 <C> 25.33 <C> 1.117 <C> 38.54 <C> 0.836 <C> 4.00 <C> 1.124 <R> <C> Work <C> SRU <C> 194K <C> 4.95 <C> 0.919 <C> 24.78 <C> 1.394 <C> 38.23 <C> 0.907 <C> 3.99 <C> 1.310 <R> <C> [EMPTY] <C> LRN <C> 151K <C> 4.98 <C> [BOLD] 0.731 <C> 25.07 <C> [BOLD] 1.038 <C> 38.42 <C> [BOLD] 0.788 <C> 3.98 <C> [BOLD] 1.022 <CAP> Table 2: Test error (ERR) on document classification task. “#Params”: the parameter number in AmaPolar task. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti.
<R> <C> Model <C> #Params <C> BLEU <C> Train <C> Decode <R> <C> GNMT <C> - <C> 24.61 <C> - <C> - <R> <C> GRU <C> 206M <C> 26.28 <C> 2.67 <C> 45.35 <R> <C> ATR <C> 122M <C> 25.70 <C> 1.33 <C> [BOLD] 34.40 <R> <C> SRU <C> 170M <C> 25.91 <C> 1.34 <C> 42.84 <R> <C> LRN <C> 143M <C> 26.26 <C> [BOLD] 0.99 <C> 36.50 <R> <C> oLRN <C> 164M <C> [BOLD] 26.73 <C> 1.15 <C> 40.19 <CAP> Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.
<R> <C> Model <C> #Params <C> Base <C> +Elmo <R> <C> rnet* <C> - <C> 71.1/79.5 <C> -/- <R> <C> LSTM <C> 2.67M <C> [BOLD] 70.46/78.98 <C> 75.17/82.79 <R> <C> GRU <C> 2.31M <C> 70.41/ [BOLD] 79.15 <C> 75.81/83.12 <R> <C> ATR <C> 1.59M <C> 69.73/78.70 <C> 75.06/82.76 <R> <C> SRU <C> 2.44M <C> 69.27/78.41 <C> 74.56/82.50 <R> <C> LRN <C> 2.14M <C> 70.11/78.83 <C> [BOLD] 76.14/ [BOLD] 83.83 <CAP> Table 4: Exact match/F1-score on SQuad dataset. “#Params”: the parameter number of Base. rnet*: results published by Wang et al. (2017).
<R> <C> Model <C> #Params <C> NER <R> <C> LSTM* <C> - <C> 90.94 <R> <C> LSTM <C> 245K <C> [BOLD] 89.61 <R> <C> GRU <C> 192K <C> 89.35 <R> <C> ATR <C> 87K <C> 88.46 <R> <C> SRU <C> 161K <C> 88.89 <R> <C> LRN <C> 129K <C> 88.56 <CAP> Table 6: F1 score on CoNLL-2003 English NER task. “#Params”: the parameter number in NER task. LSTM* denotes the reported result Lample et al. (2016).
<R> <C> Model <C> SNLI <C> PTB <R> <C> LRN <C> [BOLD] 85.06 <C> [BOLD] 61.26 <R> <C> gLRN <C> 84.72 <C> 92.49 <R> <C> eLRN <C> 83.56 <C> 169.81 <CAP> Table 7: Test accuracy on SNLI task with Base+LN setting and test perplexity on PTB task with Base setting.
<R> <C> [EMPTY] <C> [ITALIC] w/ System Retrieval  [BOLD] B-2 <C> [ITALIC] w/ System Retrieval  [BOLD] B-4 <C> [ITALIC] w/ System Retrieval  [BOLD] R-2 <C> [ITALIC] w/ System Retrieval  [BOLD] MTR <C> [ITALIC] w/ System Retrieval  [BOLD] #Word <C> [ITALIC] w/ System Retrieval  [BOLD] #Sent <C> [ITALIC] w/ Oracle Retrieval  [BOLD] B-2 <C> [ITALIC] w/ Oracle Retrieval  [BOLD] B-4 <C> [ITALIC] w/ Oracle Retrieval  [BOLD] R-2 <C> [ITALIC] w/ Oracle Retrieval  [BOLD] MTR <C> [ITALIC] w/ Oracle Retrieval  [BOLD] #Word <C> [ITALIC] w/ Oracle Retrieval  [BOLD] #Sent <R> <C> Human <C> - <C> - <C> - <C> - <C> 66 <C> 22 <C> - <C> - <C> - <C> - <C> 66 <C> 22 <R> <C> Retrieval <C> 7.55 <C> 1.11 <C> 8.64 <C> 14.38 <C> 123 <C> 23 <C> 10.97 <C> 3.05 <C> 23.49 <C> 20.08 <C> 140 <C> 21 <R> <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [BOLD] Comparisons <C> [EMPTY] <C> [EMPTY] <R> <C> Seq2seq <C> 6.92 <C> 2.13 <C> 13.02 <C> 15.08 <C> 68 <C> 15 <C> 6.92 <C> 2.13 <C> 13.02 <C> 15.08 <C> 68 <C> 15 <R> <C> Seq2seqAug <C> 8.26 <C> 2.24 <C> 13.79 <C> 15.75 <C> 78 <C> 14 <C> 10.98 <C> 4.41 <C> 22.97 <C> 19.62 <C> 71 <C> 14 <R> <C> [ITALIC] w/o psg <C> 7.94 <C> 2.28 <C> 10.13 <C> 15.71 <C> 75 <C> 12 <C> 9.89 <C> 3.34 <C> 14.20 <C> 18.40 <C> 66 <C> 12 <R> <C> H&W Hua and Wang ( 2018 ) <C> 3.64 <C> 0.92 <C> 8.83 <C> 11.78 <C> 51 <C> 12 <C> 8.51 <C> 2.86 <C> 18.89 <C> 17.18 <C> 58 <C> 12 <R> <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [BOLD] Our Models <C> [EMPTY] <C> [EMPTY] <R> <C> CANDELA <C> 12.02∗ <C> [BOLD] 2.99∗ <C> [BOLD] 14.93∗ <C> [BOLD] 16.92∗ <C> 119 <C> 22 <C> 15.80∗ <C> [BOLD] 5.00∗ <C> [BOLD] 23.75 <C> [BOLD] 20.18 <C> 116 <C> 22 <R> <C> [ITALIC] w/o psg <C> [BOLD] 12.33∗ <C> 2.86∗ <C> 14.53∗ <C> 16.60∗ <C> 123 <C> 23 <C> [BOLD] 16.33∗ <C> 4.98∗ <C> 23.65 <C> 19.94 <C> 123 <C> 23 <CAP> Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. ∗: statistically significantly better than all comparisons (randomization approximation test Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.
<R> <C> [EMPTY] <C> [ITALIC] K 100 <C> [ITALIC] K 500 <C> [ITALIC] K 1000 <C> [ITALIC] K 2000 <R> <C> Human <C> 44.1 <C> 25.8 <C> 18.5 <C> 12.0 <R> <C> Retrieval <C> 50.6 <C> 33.3 <C> 26.0 <C> 18.6 <R> <C> Seq2seq <C> 25.0 <C> 7.5 <C> 3.2 <C> 1.2 <R> <C> Seq2seqAug <C> 28.2 <C> 9.2 <C> 4.6 <C> 1.8 <R> <C> H&W Hua and Wang ( 2018 ) <C> 38.6 <C> 24.0 <C> 19.5 <C> 16.2 <R> <C> CANDELA <C> 30.0 <C> 10.5 <C> 5.3 <C> 2.3 <CAP> Table 4: Human evaluation on grammaticality (Gram), appropriateness (Appr), and content richness (Cont.), on a scale of 1 to 5 (best). The best result among automatic systems is highlighted in bold, with statistical significance marked with ∗ (approximation randomization test, p<0.0005). The highest standard deviation among all is 1.0. Top-1/2: % of evaluations a system being ranked in top 1 or 2 for overall quality.
<R> <C> [EMPTY] <C> Lang <C> Corpus <C> Patt <C> DSim <C> SLQS <C> TF <C> DF <C> DocSub <C> HClust <R> <C> P <C> EN <C> Europarl <C> [BOLD] 0.1173 <C> 0.0366 <C> 0.0503 <C> 0.0554 <C> 0.0548 <C> 0.0443 <C> 0.0761 <R> <C> P <C> EN <C> Ted Talks <C> [BOLD] 0.1125 <C> 0.0301 <C> 0.0382 <C> 0.0425 <C> 0.0441 <C> 0.0710 <C> 0.0664 <R> <C> P <C> PT <C> Europarl <C> 0.5163 <C> 0.3330 <C> 0.5257 <C> 0.6109 <C> 0.5984 <C> [BOLD] 0.7311 <C> 0.5676 <R> <C> [EMPTY] <C> PT <C> Ted Talks <C> 0.5387 <C> 0.2907 <C> 0.5300 <C> 0.6117 <C> 0.6159 <C> [BOLD] 0.6533 <C> 0.5656 <R> <C> R <C> EN <C> Europarl <C> 0.0396 <C> 0.3999 <C> 0.5499 <C> [BOLD] 0.6045 <C> 0.5887 <C> 0.0023 <C> 0.0017 <R> <C> R <C> EN <C> Ted Talks <C> 0.0018 <C> 0.4442 <C> 0.5377 <C> 0.5657 <C> [BOLD] 0.6077 <C> 0.2666 <C> 0.0019 <R> <C> R <C> PT <C> Europarl <C> 0.0111 <C> 0.3554 <C> 0.5795 <C> [BOLD] 0.6727 <C> 0.5184 <C> 0.0053 <C> 0.0012 <R> <C> [EMPTY] <C> PT <C> Ted Talks <C> 0.0004 <C> 0.3142 <C> 0.5484 <C> [BOLD] 0.6877 <C> 0.5515 <C> 0.4706 <C> 0.0011 <R> <C> F <C> EN <C> Europarl <C> 0.0591 <C> 0.0671 <C> 0.0922 <C> [BOLD] 0.1015 <C> 0.1003 <C> 0.0044 <C> 0.0033 <R> <C> F <C> EN <C> Ted Talks <C> 0.0035 <C> 0.0564 <C> 0.0713 <C> 0.0791 <C> 0.0822 <C> [BOLD] 0.1121 <C> 0.0037 <R> <C> F <C> PT <C> Europarl <C> 0.0217 <C> 0.3438 <C> 0.5513 <C> [BOLD] 0.6403 <C> 0.5555 <C> 0.0105 <C> 0.0024 <R> <C> [EMPTY] <C> PT <C> Ted Talks <C> 0.0008 <C> 0.3020 <C> 0.5390 <C> [BOLD] 0.6475 <C> 0.5819 <C> 0.5471 <C> 0.0022 <CAP> Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.
<R> <C> [EMPTY] <C> Lang <C> Corpus <C> Patt <C> DSim <C> SLQS <C> TF <C> DF <C> DocSub <C> HClust <R> <C> P <C> EN <C> Europarl <C> [BOLD] 0.1192 <C> 0.0083 <C> 0.0137 <C> 0.0150 <C> 0.0150 <C> 0.0445 <C> 0.0326 <R> <C> P <C> EN <C> Ted Talks <C> [BOLD] 0.1022 <C> 0.0069 <C> 0.0060 <C> 0.0092 <C> 0.0090 <C> 0.0356 <C> 0.0162 <R> <C> P <C> PT <C> Europarl <C> 0.5710 <C> 0.1948 <C> 0.3855 <C> 0.5474 <C> 0.4485 <C> [BOLD] 0.8052 <C> 0.4058 <R> <C> [EMPTY] <C> PT <C> Ted Talks <C> [BOLD] 0.6304 <C> 0.1870 <C> 0.3250 <C> 0.5312 <C> 0.4576 <C> 0.6064 <C> 0.3698 <R> <C> R <C> EN <C> Europarl <C> 0.0037 <C> 0.3278 <C> 0.5941 <C> 0.6486 <C> [BOLD] 0.6490 <C> 0.0017 <C> 0.0003 <R> <C> R <C> EN <C> Ted Talks <C> 0.0002 <C> 0.1486 <C> 0.4332 <C> [BOLD] 0.6467 <C> 0.6332 <C> 0.0967 <C> 0.0003 <R> <C> R <C> PT <C> Europarl <C> 0.0002 <C> 0.1562 <C> 0.5157 <C> [BOLD] 0.7255 <C> 0.5932 <C> 0.0032 <C> 0.0001 <R> <C> [EMPTY] <C> PT <C> Ted Talks <C> 2.10-5 <C> 0.0507 <C> 0.4492 <C> [BOLD] 0.7000 <C> 0.5887 <C> 0.1390 <C> 0.0002 <R> <C> F <C> EN <C> Europarl <C> 0.0073 <C> 0.0162 <C> 0.0268 <C> [BOLD] 0.0293 <C> [BOLD] 0.0293 <C> 0.0033 <C> 0.0006 <R> <C> F <C> EN <C> Ted Talks <C> 0.0004 <C> 0.0132 <C> 0.0118 <C> 0.0181 <C> 0.0179 <C> [BOLD] 0.0520 <C> 0.0005 <R> <C> F <C> PT <C> Europarl <C> 0.0005 <C> 0.1733 <C> 0.4412 <C> [BOLD] 0.6240 <C> 0.5109 <C> 0.0064 <C> 0.0002 <R> <C> [EMPTY] <C> PT <C> Ted Talks <C> 4.10-5 <C> 0.0798 <C> 0.3771 <C> [BOLD] 0.6040 <C> 0.5149 <C> 0.2261 <C> 0.0004 <CAP> Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.
<R> <C> [EMPTY] <C> Lang <C> Corpus <C> Patt <C> DSim <C> SLQS <C> TF <C> DF <C> DocSub <C> HClust <R> <C> P <C> EN <C> Europarl <C> [BOLD] 0.1038 <C> 0.0170 <C> 0.0490 <C> 0.0641 <C> 0.0641 <C> 0.0613 <C> 0.0761 <R> <C> P <C> EN <C> Ted Talks <C> [BOLD] 0.1282 <C> 0.0291 <C> 0.0410 <C> 0.0270 <C> 0.0270 <C> 0.1154 <C> 0.0661 <R> <C> P <C> PT <C> Europarl <C> 0.6185 <C> 0.3744 <C> 0.4144 <C> 0.4394 <C> 0.4394 <C> [BOLD] 0.7553 <C> 0.5676 <R> <C> [EMPTY] <C> PT <C> Ted Talks <C> 0.6308 <C> 0.4124 <C> 0.4404 <C> 0.4515 <C> 0.4945 <C> [BOLD] 0.8609 <C> 0.5295 <R> <C> R <C> EN <C> Europarl <C> [BOLD] 0.0021 <C> 0.0004 <C> 0.0011 <C> 0.0014 <C> 0.0014 <C> 0.0013 <C> 0.0017 <R> <C> R <C> EN <C> Ted Talks <C> 0.0011 <C> 0.0008 <C> 0.0011 <C> 0.0008 <C> 0.0008 <C> [BOLD] 0.0030 <C> 0.0018 <R> <C> R <C> PT <C> Europarl <C> 0.0012 <C> 0.0008 <C> 0.0009 <C> 0.0010 <C> 0.0010 <C> [BOLD] 0.0016 <C> 0.0012 <R> <C> [EMPTY] <C> PT <C> Ted Talks <C> 0.0003 <C> 0.0009 <C> 0.0009 <C> 0.0010 <C> 0.0010 <C> [BOLD] 0.0017 <C> 0.0011 <R> <C> F <C> EN <C> Europarl <C> [BOLD] 0.0041 <C> 0.0007 <C> 0.0021 <C> 0.0027 <C> 0.0027 <C> 0.0026 <C> 0.0033 <R> <C> F <C> EN <C> Ted Talks <C> 0.0022 <C> 0.0016 <C> 0.0022 <C> 0.0015 <C> 0.0015 <C> [BOLD] 0.0058 <C> 0.0036 <R> <C> F <C> PT <C> Europarl <C> 0.0024 <C> 0.0016 <C> 0.0018 <C> 0.0019 <C> 0.0019 <C> [BOLD] 0.0031 <C> 0.0023 <R> <C> [EMPTY] <C> PT <C> Ted Talks <C> 0.0005 <C> 0.0018 <C> 0.0018 <C> 0.0020 <C> 0.0021 <C> [BOLD] 0.0034 <C> 0.0022 <CAP> Table 5: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts and selecting the best parent.
<R> <C> Corpus <C> Metric <C> Patt <C> DSim <C> SLQS <C> TF <C> DF <C> DocSub <C> HClust <R> <C> Europarl <C> TotalTerms: <C> 957 <C> 1,000 <C> 1,000 <C> 1,000 <C> 1,000 <C> 836 <C> 1,000 <R> <C> Europarl <C> TotalRoots: <C> 44 <C> 1 <C> 1 <C> 1 <C> 1 <C> 43 <C> 1 <R> <C> Europarl <C> NumberRels: <C> 1,588 <C> 1,025 <C> 1,028 <C> 1,185 <C> 1,103 <C> 1,184 <C> 999 <R> <C> Europarl <C> MaxDepth: <C> 21 <C> 921 <C> 901 <C> 788 <C> 835 <C> 8 <C> 15 <R> <C> Europarl <C> MinDepth: <C> 1 <C> 921 <C> 901 <C> 788 <C> 835 <C> 1 <C> 1 <R> <C> Europarl <C> AvgDepth: <C> 11.82 <C> 921 <C> 901 <C> 788 <C> 835 <C> 3.05 <C> 8.46 <R> <C> Europarl <C> DepthCohesion: <C> 1.78 <C> 1 <C> 1 <C> 1 <C> 1 <C> 2.62 <C> 1.77 <R> <C> Europarl <C> MaxWidth: <C> 20 <C> 2 <C> 3 <C> 4 <C> 3 <C> 88 <C> 41 <R> <C> Europarl <C> MinWidth: <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <R> <C> Europarl <C> AvgWidth: <C> 1.99 <C> 1.03 <C> 1.03 <C> 1.19 <C> 1.10 <C> 4.20 <C> 2.38 <R> <C> TED Talks <C> TotalTerms: <C> 476 <C> 1,000 <C> 1,000 <C> 1,000 <C> 1,000 <C> 1,000 <C> 1,000 <R> <C> TED Talks <C> TotalRoots: <C> 164 <C> 2 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <R> <C> TED Talks <C> NumberRels: <C> 521 <C> 1,029 <C> 1,331 <C> 3,025 <C> 3,438 <C> 3,802 <C> 1,009 <R> <C> TED Talks <C> MaxDepth: <C> 16 <C> 915 <C> 658 <C> 454 <C> 395 <C> 118 <C> 12 <R> <C> TED Talks <C> MinDepth: <C> 1 <C> 913 <C> 658 <C> 454 <C> 395 <C> 110 <C> 1 <R> <C> TED Talks <C> AvgDepth: <C> 5.82 <C> 914 <C> 658 <C> 454 <C> 395 <C> 112.24 <C> 5.95 <R> <C> TED Talks <C> DepthCohesion: <C> 2.75 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1.05 <C> 2.02 <R> <C> TED Talks <C> MaxWidth: <C> 25 <C> 2 <C> 77 <C> 13 <C> 12 <C> 66 <C> 98 <R> <C> TED Talks <C> MinWidth: <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <R> <C> TED Talks <C> AvgWidth: <C> 1.83 <C> 1.03 <C> 1.36 <C> 3.03 <C> 3.44 <C> 6.64 <C> 2.35 <CAP> Table 6: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in English.
<R> <C> Corpus <C> Metric <C> Patt <C> DSim <C> SLQS <C> TF <C> DF <C> DocSub <C> HClust <R> <C> Europarl <C> TotalTerms: <C> 980 <C> 1,000 <C> 1,000 <C> 1,000 <C> 1,000 <C> 996 <C> 1,000 <R> <C> Europarl <C> TotalRoots: <C> 79 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <R> <C> Europarl <C> NumberRels: <C> 1,527 <C> 1,031 <C> 1,049 <C> 1,185 <C> 1,093 <C> 1,644 <C> 999 <R> <C> Europarl <C> MaxDepth: <C> 19 <C> 902 <C> 894 <C> 784 <C> 849 <C> 6 <C> 10 <R> <C> Europarl <C> MinDepth: <C> 1 <C> 902 <C> 894 <C> 784 <C> 849 <C> 1 <C> 1 <R> <C> Europarl <C> AvgDepth: <C> 9.43 <C> 902 <C> 894 <C> 784 <C> 849 <C> 2.73 <C> 4.29 <R> <C> Europarl <C> DepthCohesion: <C> 2.02 <C> 1 <C> 1 <C> 1 <C> 1 <C> 2.19 <C> 2.33 <R> <C> Europarl <C> MaxWidth: <C> 27 <C> 3 <C> 3 <C> 4 <C> 3 <C> 201 <C> 58 <R> <C> Europarl <C> MinWidth: <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <R> <C> Europarl <C> AvgWidth: <C> 1.98 <C> 1.03 <C> 1.05 <C> 1.19 <C> 1.09 <C> 6.25 <C> 2.55 <R> <C> TED Talks <C> TotalTerms: <C> 296 <C> 1,000 <C> 1,000 <C> 1,000 <C> 1,000 <C> 1,000 <C> 1,000 <R> <C> TED Talks <C> TotalRoots: <C> 101 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <R> <C> TED Talks <C> NumberRels: <C> 291 <C> 1,045 <C> 1,229 <C> 3,637 <C> 4,284 <C> 2,875 <C> 999 <R> <C> TED Talks <C> MaxDepth: <C> 10 <C> 860 <C> 727 <C> 388 <C> 354 <C> 252 <C> 17 <R> <C> TED Talks <C> MinDepth: <C> 1 <C> 860 <C> 727 <C> 388 <C> 354 <C> 249 <C> 1 <R> <C> TED Talks <C> AvgDepth: <C> 3.94 <C> 860 <C> 727 <C> 388 <C> 354 <C> 250.43 <C> 6.16 <R> <C> TED Talks <C> DepthCohesion: <C> 2.54 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1.01 <C> 2.76 <R> <C> TED Talks <C> MaxWidth: <C> 37 <C> 3 <C> 79 <C> 18 <C> 13 <C> 9 <C> 41 <R> <C> TED Talks <C> MinWidth: <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <C> 1 <R> <C> TED Talks <C> AvgWidth: <C> 1.79 <C> 1.05 <C> 1.23 <C> 3.64 <C> 4.29 <C> 2.94 <C> 2.37 <CAP> Table 7: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in Portuguese.
<R> <C> Model <C> baseline <C> QT <C> S  [ITALIC] R0 <C> S  [ITALIC] R1 <C> S  [ITALIC] R2 <C> S  [ITALIC] R3 <C> D <R> <C> LF  <C> 57.21 <C> 58.97 <C> 67.82 <C> 71.27 <C> 72.04 <C> 72.36 <C> 72.65 <R> <C> LF +P1 <C> 61.88 <C> 62.87 <C> 69.47 <C> 72.16 <C> 72.85 <C> 73.42 <C> [BOLD] 73.63 <CAP> Table 1: Performance (NDCG%) comparison for the experiments of applying our principles on the validation set of VisDial v1.0. LF is the enhanced version as we mentioned. QT, S and D denote question type, answer score sampling, and hidden dictionary learning, respectively. R0, R1, R2, R3 denote regressive loss, weighted softmax loss, binary sigmoid loss ,and generalized ranking loss, respectively.
<R> <C> Model <C> LF  <C> HCIAE  <C> CoAtt  <C> RvA  <R> <C> baseline <C> 57.21 <C> 56.98 <C> 56.46 <C> 56.74 <R> <C> +P1 <C> 61.88 <C> 60.12 <C> 60.27 <C> 61.02 <R> <C> +P2 <C> 72.65 <C> 71.50 <C> 71.41 <C> 71.44 <R> <C> +P1+P2 <C> [BOLD] 73.63 <C> 71.99 <C> 71.87 <C> 72.88 <CAP> Table 2: Performance (NDCG%) of ablative studies on different models on VisDial v1.0 validation set. P2 indicates the most effective one (i.e., hidden dictionary learning) shown in Table 1. Note that only applying P2 is implemented by the implementations in Section 5 with the history shortcut.
<R> <C> Metrics <C> cs-en <C> de-en <C> fi-en <C> lv-en <R> <C> RUSE <C> 0.624 <C> 0.644 <C> 0.750 <C> 0.697 <R> <C> Hmd-F1 + BERT <C> 0.655 <C> 0.681 <C> 0.821 <C> 0.712 <R> <C> Hmd-Recall + BERT <C> 0.651 <C> 0.658 <C> 0.788 <C> 0.681 <R> <C> Hmd-Prec + BERT <C> 0.624 <C> 0.669 <C> 0.817 <C> 0.707 <R> <C> Wmd-unigram + BERT <C> 0.651 <C> 0.686 <C> <bold>0.823</bold> <C> 0.710 <R> <C> Wmd-bigram + BERT <C> <bold>0.665</bold> <C> <bold>0.688</bold> <C> 0.821 <C> <bold>0.712</bold> <CAP> Table 5: Comparison on hard and soft alignments.
<R> <C> Setting <C> Metrics <C> <bold>Direct Assessment</bold> cs-en <C> <bold>Direct Assessment</bold> de-en <C> <bold>Direct Assessment</bold> fi-en <C> <bold>Direct Assessment</bold> lv-en <C> <bold>Direct Assessment</bold> ru-en <C> <bold>Direct Assessment</bold> tr-en <C> <bold>Direct Assessment</bold> zh-en <C> <bold>Direct Assessment</bold> Average <R> <C> Baselines <C> METEOR++ <C> 0.552 <C> 0.538 <C> 0.720 <C> 0.563 <C> 0.627 <C> 0.626 <C> 0.646 <C> 0.610 <R> <C> Baselines <C> RUSE(*) <C> 0.624 <C> 0.644 <C> 0.750 <C> 0.697 <C> 0.673 <C> 0.716 <C> 0.691 <C> 0.685 <R> <C> Baselines <C> BERTScore-F1 <C> 0.670 <C> 0.686 <C> 0.820 <C> 0.710 <C> 0.729 <C> 0.714 <C> 0.704 <C> 0.719 <R> <C> Sent-Mover <C> Smd + W2V <C> 0.438 <C> 0.505 <C> 0.540 <C> 0.442 <C> 0.514 <C> 0.456 <C> 0.494 <C> 0.484 <R> <C> Sent-Mover <C> Smd + ELMO + PMeans <C> 0.569 <C> 0.558 <C> 0.732 <C> 0.525 <C> 0.581 <C> 0.620 <C> 0.584 <C> 0.595 <R> <C> Sent-Mover <C> Smd + BERT + PMeans <C> 0.607 <C> 0.623 <C> 0.770 <C> 0.639 <C> 0.667 <C> 0.641 <C> 0.619 <C> 0.652 <R> <C> Sent-Mover <C> Smd + BERT + MNLI + PMeans <C> 0.616 <C> 0.643 <C> 0.785 <C> 0.660 <C> 0.664 <C> 0.668 <C> 0.633 <C> 0.667 <R> <C> Word-Mover <C> Wmd-1 + W2V <C> 0.392 <C> 0.463 <C> 0.558 <C> 0.463 <C> 0.456 <C> 0.485 <C> 0.481 <C> 0.471 <R> <C> Word-Mover <C> Wmd-1 + ELMO + PMeans <C> 0.579 <C> 0.588 <C> 0.753 <C> 0.559 <C> 0.617 <C> 0.679 <C> 0.645 <C> 0.631 <R> <C> Word-Mover <C> Wmd-1 + BERT + PMeans <C> 0.662 <C> 0.687 <C> 0.823 <C> 0.714 <C> 0.735 <C> 0.734 <C> 0.719 <C> 0.725 <R> <C> Word-Mover <C> Wmd-1 + BERT + MNLI + PMeans <C> 0.670 <C> 0.708 <C> <bold>0.835</bold> <C> <bold>0.746</bold> <C> <bold>0.738</bold> <C> 0.762 <C> <bold>0.744</bold> <C> <bold>0.743</bold> <R> <C> Word-Mover <C> Wmd-2 + BERT + MNLI + PMeans <C> <bold>0.679</bold> <C> <bold>0.710</bold> <C> 0.832 <C> 0.745 <C> 0.736 <C> <bold>0.763</bold> <C> 0.740 <C> <bold>0.743</bold> <CAP> Table 1: Absolute Pearson correlations with segment-level human judgments in 7 language pairs on WMT17 dataset.
<R> <C> Setting <C> Metrics <C> BAGEL <bold>Inf</bold> <C> BAGEL <bold>Nat</bold> <C> BAGEL <bold>Qual</bold> <C> SFHOTEL <bold>Inf</bold> <C> SFHOTEL <bold>Nat</bold> <C> SFHOTEL <bold>Qual</bold> <R> <C> Baselines <C> BLEU-1 <C> 0.225 <C> 0.141 <C> 0.113 <C> 0.107 <C> 0.175 <C> 0.069 <R> <C> Baselines <C> BLEU-2 <C> 0.211 <C> 0.152 <C> 0.115 <C> 0.097 <C> 0.174 <C> 0.071 <R> <C> Baselines <C> METEOR <C> 0.251 <C> 0.127 <C> 0.116 <C> 0.111 <C> 0.148 <C> 0.082 <R> <C> Baselines <C> BERTScore-F1 <C> 0.267 <C> 0.210 <C> <bold>0.178</bold> <C> 0.163 <C> 0.193 <C> 0.118 <R> <C> Sent-Mover <C> SMD + W2V <C> 0.024 <C> 0.074 <C> 0.078 <C> 0.022 <C> 0.025 <C> 0.011 <R> <C> Sent-Mover <C> SMD + ELMO + PMeans <C> 0.251 <C> 0.171 <C> 0.147 <C> 0.130 <C> 0.176 <C> 0.096 <R> <C> Sent-Mover <C> SMD + BERT + PMeans <C> 0.290 <C> 0.163 <C> 0.121 <C> 0.192 <C> 0.223 <C> 0.134 <R> <C> Sent-Mover <C> SMD + BERT + MNLI + PMeans <C> 0.280 <C> 0.149 <C> 0.120 <C> 0.205 <C> 0.239 <C> 0.147 <R> <C> Word-Mover <C> Wmd-1 + W2V <C> 0.222 <C> 0.079 <C> 0.123 <C> 0.074 <C> 0.095 <C> 0.021 <R> <C> Word-Mover <C> Wmd-1 + ELMO + PMeans <C> 0.261 <C> 0.163 <C> 0.148 <C> 0.147 <C> 0.215 <C> 0.136 <R> <C> Word-Mover <C> Wmd-1 + BERT + PMeans <C> <bold>0.298</bold> <C> <bold>0.212</bold> <C> 0.163 <C> 0.203 <C> 0.261 <C> 0.182 <R> <C> Word-Mover <C> Wmd-1 + BERT + MNLI + PMeans <C> 0.285 <C> 0.195 <C> 0.158 <C> <bold>0.207</bold> <C> <bold>0.270</bold> <C> <bold>0.183</bold> <R> <C> Word-Mover <C> Wmd-2 + BERT + MNLI + PMeans <C> 0.284 <C> 0.194 <C> 0.156 <C> 0.204 <C> 0.270 <C> 0.182 <CAP> Table 3: Spearman correlation with utterance-level human judgments for BAGEL and SFHOTEL datasets.
<R> <C> Setting <C> Metric <C> M1 <C> M2 <R> <C> Baselines <C> LEIC(*) <C> <bold>0.939</bold> <C> <bold>0.949</bold> <R> <C> Baselines <C> METEOR <C> 0.606 <C> 0.594 <R> <C> Baselines <C> SPICE <C> 0.759 <C> 0.750 <R> <C> Baselines <C> BERTScore-Recall <C> 0.809 <C> 0.749 <R> <C> Sent-Mover <C> SMD + W2V <C> 0.683 <C> 0.668 <R> <C> Sent-Mover <C> SMD + ELMO + P <C> 0.709 <C> 0.712 <R> <C> Sent-Mover <C> SMD + BERT + P <C> 0.723 <C> 0.747 <R> <C> Sent-Mover <C> SMD + BERT + M + P <C> 0.789 <C> 0.784 <R> <C> Word-Mover <C> Wmd-1 + W2V <C> 0.728 <C> 0.764 <R> <C> Word-Mover <C> Wmd-1 + ELMO + P <C> 0.753 <C> 0.775 <R> <C> Word-Mover <C> Wmd-1 + BERT + P <C> 0.780 <C> 0.790 <R> <C> Word-Mover <C> Wmd-1 + BERT + M + P <C> <bold>0.813</bold> <C> <bold>0.810</bold> <R> <C> Word-Mover <C> Wmd-2 + BERT + M + P <C> 0.812 <C> 0.808 <CAP> Table 4: Pearson correlation with system-level human judgments on MSCOCO dataset. ’M’ and ’P’ are short names.
<R> <C> [EMPTY] <C> Acc <C> Sim <C> PP <C> GM <R> <C> M0: shen-1 <C> 0.694 <C> 0.728 <C> [BOLD] 22.3 <C> 8.81 <R> <C> M1: M0 [ITALIC] +para <C> 0.702 <C> 0.747 <C> 23.6 <C> 11.7 <R> <C> M2: M0 [ITALIC] +cyc <C> 0.692 <C> 0.781 <C> 49.9 <C> [BOLD] 12.8 <R> <C> M3: M0 [ITALIC] +cyc+lang <C> 0.698 <C> 0.754 <C> 39.2 <C> 12.0 <R> <C> M4: M0 [ITALIC] +cyc+para <C> 0.702 <C> 0.757 <C> 33.9 <C> [BOLD] 12.8 <R> <C> M5: M0 [ITALIC] +cyc+para+lang <C> 0.688 <C> 0.753 <C> 28.6 <C> 11.8 <R> <C> M6: M0 [ITALIC] +cyc+2d <C> 0.704 <C> [BOLD] 0.794 <C> 63.2 <C> [BOLD] 12.8 <R> <C> M7: M6+ [ITALIC] para+lang <C> 0.706 <C> 0.768 <C> 49.0 <C> [BOLD] 12.8 <CAP> Table 3: Literature results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.
<R> <C> Dataset <C> Models A <C> Models B <C> Transfer quality A>B <C> Transfer quality B>A <C> Transfer quality Tie <C> Semantic preservation A>B <C> Semantic preservation B>A <C> Semantic preservation Tie <C> Semantic preservation ΔSim <C> Fluency A>B <C> Fluency B>A <C> Fluency Tie <C> Fluency ΔPP <R> <C> [EMPTY] <C> M0 <C> M2 <C> 9.0 <C> 6.0 <C> 85.1 <C> 1.5 <C> [BOLD] 25.4 <C> 73.1 <C> -0.05 <C> 10.4 <C> [BOLD] 23.9 <C> 65.7 <C> 0.9 <R> <C> Yelp <C> M0 <C> M7 <C> 9.6 <C> 14.7 <C> 75.8 <C> 2.5 <C> [BOLD] 54.5 <C> 42.9 <C> -0.09 <C> 4.6 <C> [BOLD] 39.4 <C> 56.1 <C> 8.3 <R> <C> Yelp <C> M6 <C> M7 <C> 13.7 <C> 11.6 <C> 74.7 <C> 16.0 <C> 16.7 <C> 67.4 <C> 0.01 <C> 10.3 <C> 20.0 <C> 69.7 <C> 14.3 <R> <C> [EMPTY] <C> M2 <C> M7 <C> 5.8 <C> 9.3 <C> 84.9 <C> 8.1 <C> [BOLD] 25.6 <C> 66.3 <C> -0.04 <C> 14.0 <C> [BOLD] 26.7 <C> 59.3 <C> 7.4 <R> <C> Literature <C> M2 <C> M6 <C> 4.2 <C> 6.7 <C> 89.2 <C> 16.7 <C> 20.8 <C> 62.5 <C> 0.01 <C> [BOLD] 40.8 <C> 13.3 <C> 45.8 <C> -13.3 <R> <C> Literature <C> M6 <C> M7 <C> 15.8 <C> 13.3 <C> 70.8 <C> [BOLD] 25.0 <C> 9.2 <C> 65.8 <C> 0.03 <C> 14.2 <C> 20.8 <C> 65.0 <C> 14.2 <CAP> Table 4: Manual evaluation results (%) using models from Table 2 (i.e., with roughly fixed Acc). > means “better than”. ΔSim=Sim(A)−Sim(B), and ΔPP=PP(A)−PP(B) (note that lower PP generally means better fluency). Each row uses at least 120 sentence pairs. A cell is bold if it represents a model win of at least 10%.
<R> <C> Metric <C> Method of validation <C> Yelp <C> Lit. <R> <C> Acc <C> % of machine and human judgments that match <C> 94 <C> 84 <R> <C> Sim <C> Spearman’s  [ITALIC] ρ b/w Sim and human ratings of semantic preservation <C> 0.79 <C> 0.75 <R> <C> PP <C> Spearman’s  [ITALIC] ρ b/w negative PP and human ratings of fluency <C> 0.81 <C> 0.67 <CAP> Table 5: Human sentence-level validation of metrics; 100 examples for each dataset for validating Acc; 150 each for Sim and PP; see text for validation of GM.
<R> <C> [EMPTY] <C> Acc <C> Sim <C> PP <C> GM <R> <C> M0: shen-1 <C> 0.818 <C> 0.719 <C> 37.3 <C> 10.0 <R> <C> M1: M0 [ITALIC] +para <C> 0.819 <C> 0.734 <C> 26.3 <C> 14.2 <R> <C> M2: M0 [ITALIC] +cyc <C> 0.813 <C> 0.770 <C> 36.4 <C> 18.8 <R> <C> M3: M0 [ITALIC] +cyc+lang <C> 0.807 <C> 0.796 <C> 28.4 <C> 21.5 <R> <C> M4: M0 [ITALIC] +cyc+para <C> 0.798 <C> 0.783 <C> 39.7 <C> 19.2 <R> <C> M5: M0 [ITALIC] +cyc+para+lang <C> 0.804 <C> 0.785 <C> 27.1 <C> 20.3 <R> <C> M6: M0 [ITALIC] +cyc+2d <C> 0.805 <C> [BOLD] 0.817 <C> 43.3 <C> 21.6 <R> <C> M7: M6+ [ITALIC] para+lang <C> 0.818 <C> 0.805 <C> [BOLD] 29.0 <C> [BOLD] 22.8 <CAP> Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.
<R> <C> Model <C> BLEU <C> Acc∗ <R> <C> fu-1 <C> [EMPTY] <C> [EMPTY] <R> <C> Multi-decoder <C> 7.6 <C> 0.792 <R> <C> Style embed. <C> 15.4 <C> 0.095 <R> <C> simple-transfer <C> simple-transfer <C> simple-transfer <R> <C> Template <C> 18.0 <C> 0.867 <R> <C> Delete/Retrieve <C> 12.6 <C> 0.909 <R> <C> yang2018unsupervised <C> yang2018unsupervised <C> yang2018unsupervised <R> <C> LM <C> 13.4 <C> 0.854 <R> <C> LM + classifier <C> [BOLD] 22.3 <C> 0.900 <R> <C> Untransferred <C> [BOLD] 31.4 <C> 0.024 <CAP> Table 6: Results on Yelp sentiment transfer, where BLEU is between 1000 transferred sentences and human references, and Acc is restricted to the same 1000 sentences. Our best models (right table) achieve higher BLEU than prior work at similar levels of Acc, but untransferred sentences achieve the highest BLEU. Acc∗: the definition of Acc varies by row because of different classifiers in use. Other results from simple-transfer are not included as they are worse.
<R> <C> [BOLD] Type <C> [BOLD] Reparandum Length  [BOLD] 1-2 <C> [BOLD] Reparandum Length  [BOLD] 3-5 <C> [BOLD] Reparandum Length  [BOLD] 6-8 <C> [BOLD] Reparandum Length  [BOLD] 8+ <C> [BOLD] overall <R> <C> repetition <C> 0.99 <C> 0.99 <C> 1 <C> 1 <C> 0.99 <R> <C> rephrase <C> 0.75 <C> 0.66 <C> 0.44 <C> – <C> 0.70 <R> <C> restart <C> 0.41 <C> 0 <C> – <C> – <C> 0.39 <R> <C> nested∗ <C> 0.79 <C> 0.66 <C> 0.62 <C> 0.21 <C> 0.62 <CAP> Table 2: Percent of reparandum tokens that were correctly predicted as disfluent. *Statistics for nested disfluencies exclude repetition tokens.
<R> <C> [BOLD] Type <C> [BOLD] Reparandum Length  [BOLD] 1-2 <C> [BOLD] Reparandum Length  [BOLD] 3-5 <R> <C> content-content <C> 0.61 (30%) <C> 0.58 (52%) <R> <C> content-function <C> 0.77 (20%) <C> 0.66 (17%) <R> <C> function-function <C> 0.83 (50%) <C> 0.80 (32%) <CAP> Table 3: Relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair (content-content), either the reparandum or repair (content-function) or in neither. Percentages in parentheses show the fraction of tokens belong to each category.
<R> <C> [EMPTY] <C> [BOLD] Model <C> [BOLD] dev mean <C> [BOLD] dev best <C> [BOLD] test mean <C> [BOLD] test best <C> [ITALIC] α <R> <C> single <C> text <C> 86.54 <C> 86.80 <C> 86.47 <C> 86.96 <C> – <R> <C> single <C> raw <C> 35.00 <C> 37.33 <C> 35.78 <C> 37.70 <C> – <R> <C> single <C> innovations <C> 80.86 <C> 81.51 <C> 80.28 <C> 82.15 <C> – <R> <C> early <C> text + raw <C> 86.46 <C> 86.65 <C> 86.24 <C> 86.53 <C> – <R> <C> early <C> text + innovations <C> 86.53 <C> 86.77 <C> 86.54 <C> 87.00 <C> – <R> <C> early <C> text + raw + innovations <C> 86.35 <C> 86.69 <C> 86.55 <C> 86.44 <C> – <R> <C> late <C> text + raw <C> 86.71 <C> 87.05 <C> 86.35 <C> 86.71 <C> 0.2 <R> <C> late <C> text + innovations <C> [BOLD] 86.98 <C> [BOLD] 87.48 <C> [BOLD] 86.68 <C> [BOLD] 87.02 <C> 0.5 <R> <C> late <C> text + raw + innovations <C> 86.95 <C> 87.30 <C> 86.60 <C> 86.87 <C> 0.5 <CAP> Table 4: F1 scores on disfluency detection when using a single set of features (text-only, raw prosody features or innovation features), with early fusion and late fusion. “Raw” indicates the usage of original prosodic features (Section 3.2), while “innovations” indicate the usage of innovation features (Section 3.3).
<R> <C> Model <C> Accuracy (%) agree <C> Accuracy (%) disagree <C> Accuracy (%) discuss <C> Accuracy (%) unrelated <C> Micro F1(%) <R> <C> Average of Word2vec Embedding <C> 12.43 <C> 01.30 <C> 43.32 <C> 74.24 <C> 45.53 <R> <C> CNN-based Sentence Embedding <C> 24.54 <C> 05.06 <C> 53.24 <C> 79.53 <C> 81.72 <R> <C> RNN-based Sentence Embedding <C> 24.42 <C> 05.42 <C> 69.05 <C> 65.34 <C> 78.70 <R> <C> Self-attention Sentence Embedding <C> 23.53 <C> 04.63 <C> 63.59 <C> 80.34 <C> 80.11 <R> <C> Our model <C> 28.53 <C> 10.43 <C> 65.43 <C> 82.43 <C> [BOLD] 83.54 <CAP> Table 2: Performance comparison with the state-of-art algorithms on the FNC-1 test dataset.
<R> <C> Method <C> APW <C> NYT <R> <C> BurstySimDater <C> 45.9 <C> 38.5 <R> <C> MaxEnt-Joint <C> 52.5 <C> 42.5 <R> <C> NeuralDater <C> 64.1 <C> 58.9 <R> <C> Attentive NeuralDater  <C> 66.2 <C> 60.1 <R> <C> OE-GCN  <C> 63.9 <C> 58.3 <R> <C> AC-GCN  <C> 65.6 <C> 60.3 <R> <C> [BOLD] AD3  <C> [BOLD] 68.2 <C> [BOLD] 62.2 <CAP> Table 2: Accuracy (%) of different methods on the APW and NYT datasets for the document dating problem (higher is better). The unified model significantly outperforms all previous models.
<R> <C> Method <C> Accuracy <R> <C> T-GCN of NeuralDater <C> 61.8 <R> <C> OE-GCN <C> [BOLD] 63.9 <R> <C> S-GCN of NeuralDater <C> 63.2 <R> <C> AC-GCN <C> [BOLD] 65.6 <CAP> Table 3: Accuracy (%) comparisons of component models with and without Attention. This results show the effectiveness of both word attention and Graph Attention for this task. Please see Section 6.2 for more details.
<R> <C> [BOLD] Stage <C> [BOLD] Model <C> [BOLD] 1/1 <C> [BOLD] 1/N <C> [BOLD] all <R> <C> [EMPTY] <C> Embedding+T <C> 68.1 <C> 25.5 <C> 59.8 <R> <C> [EMPTY] <C> CNN <C> 72.5 <C> 43.1 <C> 66.3 <R> <C> Trigger <C> DMCNN <C> 74.3 <C> 50.9 <C> 69.1 <R> <C> [EMPTY] <C> JRNN <C> [BOLD] 75.6 <C> 64.8 <C> 69.3 <R> <C> [EMPTY] <C> [BOLD] JMEE <C> 75.2 <C> [BOLD] 72.7 <C> [BOLD] 73.7 <R> <C> [EMPTY] <C> Embedding+T <C> 37.4 <C> 15.5 <C> 32.6 <R> <C> [EMPTY] <C> CNN <C> 51.6 <C> 36.6 <C> 48.9 <R> <C> Argument <C> DMCNN <C> 54.6 <C> 48.7 <C> 53.5 <R> <C> [EMPTY] <C> JRNN <C> 50.0 <C> 55.2 <C> 55.4 <R> <C> [EMPTY] <C> [BOLD] JMEE <C> [BOLD] 59.3 <C> [BOLD] 57.6 <C> [BOLD] 60.3 <CAP> Table 2: System Performance on Single Event Sentences (1/1) and Multiple Event Sentences (1/N)
<R> <C> [BOLD] Method <C> [BOLD] Trigger  [BOLD] Identification (%) <C> [BOLD] Trigger  [BOLD] Identification (%) <C> [BOLD] Trigger  [BOLD] Identification (%) <C> [BOLD] Trigger  [BOLD] Classification (%) <C> [BOLD] Trigger  [BOLD] Classification (%) <C> [BOLD] Trigger  [BOLD] Classification (%) <C> [BOLD] Argument  [BOLD] Identification (%) <C> [BOLD] Argument  [BOLD] Identification (%) <C> [BOLD] Argument  [BOLD] Identification (%) <C> [BOLD] Argument  [BOLD] Role (%) <C> [BOLD] Argument  [BOLD] Role (%) <C> [BOLD] Argument  [BOLD] Role (%) <R> <C> [BOLD] Method <C> [ITALIC] P <C> [ITALIC] R <C> [ITALIC] F1 <C> [ITALIC] P <C> [ITALIC] R <C> [ITALIC] F1 <C> [ITALIC] P <C> [ITALIC] R <C> [ITALIC] F1 <C> [ITALIC] P <C> [ITALIC] R <C> [ITALIC] F1 <R> <C> Cross-Event <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 68.7 <C> 68.9 <C> 68.8 <C> 50.9 <C> 49.7 <C> 50.3 <C> 45.1 <C> 44.1 <C> 44.6 <R> <C> JointBeam <C> 76.9 <C> 65.0 <C> 70.4 <C> 73.7 <C> 62.3 <C> 67.5 <C> 69.8 <C> 47.9 <C> 56.8 <C> 64.7 <C> 44.4 <C> 52.7 <R> <C> DMCNN <C> [BOLD] 80.4 <C> 67.7 <C> 73.5 <C> 75.6 <C> 63.6 <C> 69.1 <C> 68.8 <C> 51.9 <C> 59.1 <C> 62.2 <C> 46.9 <C> 53.5 <R> <C> PSL <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 75.3 <C> 64.4 <C> 69.4 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> JRNN <C> 68.5 <C> [BOLD] 75.7 <C> 71.9 <C> 66.0 <C> [BOLD] 73.0 <C> 69.3 <C> 61.4 <C> 64.2 <C> 62.8 <C> 54.2 <C> 56.7 <C> 55.4 <R> <C> dbRNN <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 74.1 <C> 69.8 <C> 71.9 <C> 71.3 <C> 64.5 <C> 67.7 <C> 66.2 <C> 52.8 <C> 58.7 <R> <C> [BOLD] JMEE <C> 80.2 <C> 72.1 <C> [BOLD] 75.9 <C> [BOLD] 76.3 <C> 71.3 <C> [BOLD] 73.7 <C> [BOLD] 71.4 <C> [BOLD] 65.6 <C> [BOLD] 68.4 <C> [BOLD] 66.8 <C> [BOLD] 54.9 <C> [BOLD] 60.3 <CAP> Table 1: Overall performance comparing to the state-of-the-art methods with golden-standard entities.
<R> <C> [EMPTY] <C> dev perp ↓ <C> dev acc ↑ <C> dev wer ↓ <C> test perp ↓ <C> test acc ↑ <C> test wer ↓ <R> <C> Spanish-only-LM <C> 329.68 <C> 26.6 <C> 30.47 <C> 322.26 <C> 25.1 <C> 29.62 <R> <C> English-only-LM <C> 320.92 <C> 29.3 <C> 32.02 <C> 314.04 <C> 30.3 <C> 32.51 <R> <C> All:CS-last-LM <C> 76.64 <C> 47.8 <C> 14.56 <C> 76.97 <C> 49.2 <C> 14.13 <R> <C> All:Shuffled-LM <C> 68.00 <C> 51.8 <C> 13.64 <C> 68.72 <C> 51.4 <C> 13.89 <R> <C> CS-only-LM <C> 43.20 <C> 60.7 <C> 12.60 <C> 43.42 <C> 57.9 <C> 12.18 <R> <C> CS-only+vocab-LM <C> 45.61 <C> 61.0 <C> 12.56 <C> 45.79 <C> 58.8 <C> 12.49 <R> <C> Fine-Tuned-LM <C> 39.76 <C> 66.9 <C> 10.71 <C> 40.11 <C> 65.4 <C> 10.17 <R> <C> CS-only-disc <C> – <C> 72.0 <C> 6.35 <C> – <C> 70.5 <C> 6.70 <R> <C> Fine-Tuned-disc <C> – <C> [BOLD] 74.2 <C> [BOLD] 5.85 <C> – <C> [BOLD] 75.5 <C> [BOLD] 5.59 <CAP> Table 3: Results on the dev set and on the test set. “perp” stands for perplexity, “acc” stands for accuracy (in percents), and “wer” stands for word-error-rate.
<R> <C> [EMPTY] <C> 25% train dev <C> 25% train test <C> 50% train dev <C> 50% train test <C> 75% train dev <C> 75% train test <C> full train dev <C> full train test <R> <C> CS-only <C> 58.4 <C> 58.9 <C> 65.2 <C> 63.6 <C> 70.8 <C> 68.8 <C> 72.0 <C> 70.5 <R> <C> Fine-Tuned <C> [BOLD] 68.4 <C> [BOLD] 67.7 <C> [BOLD] 71.9 <C> [BOLD] 70.1 <C> [BOLD] 72.8 <C> [BOLD] 73.0 <C> [BOLD] 74.2 <C> [BOLD] 75.5 <CAP> Table 4: Results on the dev set and on the test set using discriminative training with only subsets of the code-switched data.
<R> <C> [EMPTY] <C> dev CS <C> dev mono <C> test CS <C> test mono <R> <C> CS-only-LM <C> 45.20 <C> 65.87 <C> 43.20 <C> 62.80 <R> <C> Fine-Tuned-LM <C> 49.60 <C> 72.67 <C> 47.60 <C> 71.33 <R> <C> CS-only-disc <C> [BOLD] 75.60 <C> 70.40 <C> 70.80 <C> 70.53 <R> <C> Fine-Tuned-disc <C> 70.80 <C> [BOLD] 74.40 <C> [BOLD] 75.33 <C> [BOLD] 75.87 <CAP> Table 5: Accuracy on the dev set and on the test set, according to the type of the gold sentence in the set: code-switched (CS) vs. monolingual (mono).
<R> <C> [BOLD] CoNLL-2003 <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F <R> <C> baseline <C> 72.80 <C> 56.97 <C> 63.92 <R> <C> type combined <C> [BOLD] 74.56 <C> [BOLD] 60.20 <C> [BOLD] 66.61* <CAP> Table 7: Precision (P), recall (R) and F1-score (F) for using type-aggregated gaze features trained on all three eye-tracking datasets and tested on the CoNLL-2003 dataset (* marks statistically significant improvement).
<R> <C> [BOLD] CoNLL-2003 <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F <R> <C> baseline <C> 93.89 <C> 94.16 <C> 94.03 <R> <C> type combined <C> [BOLD] 94.38 <C> [BOLD] 94.32 <C> [BOLD] 94.35* <CAP> Table 5: Precision (P), recall (R) and F1-score (F) for using type-aggregated gaze features on the CoNLL-2003 dataset (* marks statistically significant improvement).
<R> <C> [BOLD] System <C> [BOLD] Initialization <C> [BOLD] Embedding <C> [BOLD] Resources <C> [BOLD] Test Acc. <R> <C> HPCD (full) <C> Syntactic-SG <C> Type <C> WordNet, VerbNet <C> 88.7 <R> <C> LSTM-PP <C> GloVe <C> Type <C> - <C> 84.3 <R> <C> LSTM-PP <C> GloVe-retro <C> Type <C> WordNet <C> 84.8 <R> <C> OntoLSTM-PP <C> GloVe-extended <C> Token <C> WordNet <C> [BOLD] 89.7 <CAP> Table 1: Results on belinkov2014exploring’s PPA test set. HPCD (full) is from the original paper, and it uses syntactic SkipGram. GloVe-retro is GloVe vectors retrofitted Faruqui et al. (2015) to WordNet 3.1, and GloVe-extended refers to the synset embeddings obtained by running AutoExtend Rothe and Schütze (2015) on GloVe.
<R> <C> [BOLD] System <C> [BOLD] Full UAS <C> [BOLD] PPA Acc. <R> <C> RBG <C> 94.17 <C> 88.51 <R> <C> RBG + HPCD (full) <C> 94.19 <C> 89.59 <R> <C> RBG + LSTM-PP <C> 94.14 <C> 86.35 <R> <C> RBG + OntoLSTM-PP <C> 94.30 <C> 90.11 <R> <C> RBG + Oracle PP <C> 94.60 <C> 98.97 <CAP> Table 2: Results from RBG dependency parser with features coming from various PP attachment predictors and oracle attachments.
<R> <C> [BOLD] Model <C> [BOLD] PPA Acc. <R> <C> full <C> 89.7 <R> <C> - sense priors <C> 88.4 <R> <C> - attention <C> 87.5 <CAP> Table 3: Effect of removing sense priors and context sensitivity (attention) from the model.
<R> <C> en-fr <C> flickr16 <C> flickr17 <C> mscoco17 <R> <C> multi30k <C> 61.4 <C> 54.0 <C> 43.1 <R> <C> +subsfull <C> 53.7 <C> 48.9 <C> 47.0 <R> <C> +domain-tuned <C> 66.1 <C> 59.7 <C> [BOLD] 51.7 <R> <C> +ensemble-of-3 <C> [BOLD] 66.5 <C> [BOLD] 60.2 <C> 51.6 <R> <C> en-de <C> flickr16 <C> flickr17 <C> mscoco17 <R> <C> multi30k <C> 38.9 <C> 32.0 <C> 27.7 <R> <C> +subsfull <C> 41.3 <C> 34.1 <C> 31.3 <R> <C> +domain-tuned <C> 43.3 <C> 38.4 <C> 35.0 <R> <C> +ensemble-of-3 <C> [BOLD] 43.9 <C> [BOLD] 39.6 <C> [BOLD] 37.0 <CAP> Table 2: Adding subtitle data and domain tuning for image caption translation (BLEU% scores). All results with Marian Amun.
<R> <C> [EMPTY] <C> en-fr <C> flickr16 <C> flickr17 <C> mscoco17 <R> <C> A <C> subs1M [ITALIC]  [ITALIC] H+MS-COCO <C> 66.3 <C> 60.5 <C> 52.1 <R> <C> A <C> +domain-tuned <C> 66.8 <C> 60.6 <C> 52.0 <R> <C> A <C> +labels <C> [BOLD] 67.2 <C> 60.4 <C> 51.7 <R> <C> T <C> subs1M [ITALIC]  [ITALIC] LM+MS-COCO <C> 66.9 <C> 60.3 <C> [BOLD] 52.8 <R> <C> T <C> +labels <C> [BOLD] 67.2 <C> [BOLD] 60.9 <C> 52.7 <R> <C> [EMPTY] <C> en-de <C> flickr16 <C> flickr17 <C> mscoco17 <R> <C> A <C> subs1M [ITALIC]  [ITALIC] H+MS-COCO <C> 43.1 <C> 39.0 <C> 35.1 <R> <C> A <C> +domain-tuned <C> 43.9 <C> 39.4 <C> 35.8 <R> <C> A <C> +labels <C> 43.2 <C> 39.3 <C> 34.3 <R> <C> T <C> subs1M [ITALIC]  [ITALIC] LM+MS-COCO <C> [BOLD] 44.4 <C> 39.4 <C> 35.0 <R> <C> T <C> +labels <C> 44.1 <C> [BOLD] 39.8 <C> [BOLD] 36.5 <CAP> Table 3: Using automatically translated image captions and domain labels (BLEU% scores). A is short for Amun, T for Transformer.
<R> <C> en-fr <C> flickr16 <C> flickr17 <C> mscoco17 <R> <C> multi30k <C> 61.4 <C> 54.0 <C> 43.1 <R> <C> +autocap (dual attn.) <C> 60.9 <C> 52.9 <C> 43.3 <R> <C> +autocap 1 (concat) <C> 61.7 <C> 53.7 <C> 43.9 <R> <C> +autocap 1-5 (concat) <C> [BOLD] 62.2 <C> [BOLD] 54.4 <C> [BOLD] 44.1 <R> <C> en-de <C> flickr16 <C> flickr17 <C> mscoco17 <R> <C> multi30k <C> 38.9 <C> 32.0 <C> 27.7 <R> <C> +autocap (dual attn.) <C> 37.8 <C> 30.2 <C> 27.0 <R> <C> +autocap 1 (concat) <C> 39.7 <C> [BOLD] 32.2 <C> [BOLD] 28.8 <R> <C> +autocap 1-5 (concat) <C> [BOLD] 39.9 <C> 32.0 <C> 28.7 <CAP> Table 4: Adding automatic image captions (only the best one or all 5). The table shows BLEU scores in %. All results with Marian Amun.
<R> <C> en-fr <C> flickr16 <C> flickr17 <C> mscoco17 <R> <C> IMG [ITALIC] W <C> [ITALIC] 68.30 <C> [BOLD] 62.45 <C> 52.86 <R> <C> enc-gate <C> 68.01 <C> 61.38 <C> [BOLD] 53.40 <R> <C> dec-gate <C> 67.99 <C> 61.53 <C> 52.38 <R> <C> enc-gate + dec-gate <C> [BOLD] 68.58 <C> [ITALIC] 62.14 <C> [ITALIC] 52.98 <R> <C> en-de <C> flickr16 <C> flickr17 <C> mscoco17 <R> <C> IMG [ITALIC] W <C> [ITALIC] 45.09 <C> 40.81 <C> 36.94 <R> <C> enc-gate <C> 44.75 <C> [BOLD] 41.44 <C> [BOLD] 37.76 <R> <C> dec-gate <C> [BOLD] 45.21 <C> 40.79 <C> 36.47 <R> <C> enc-gate + dec-gate <C> 44.91 <C> [ITALIC] 41.06 <C> [ITALIC] 37.40 <CAP> Table 5: Comparison of strategies for integrating visual information (BLEU% scores). All results using Transformer, Multi30k+MS-COCO+subs3MLM, Detectron mask surface, and domain labeling.
<R> <C> en-fr <C> flickr16 <C> flickr17 <C> mscoco17 <R> <C> subs3M [ITALIC]  [ITALIC] LM detectron <C> 68.30 <C> 62.45 <C> 52.86 <R> <C> +ensemble-of-3 <C> 68.72 <C> 62.70 <C> 53.06 <R> <C> −visual features <C> [BOLD] 68.74 <C> [BOLD] 62.71 <C> 53.14 <R> <C> −MS-COCO <C> 67.13 <C> 61.17 <C> [BOLD] 53.34 <R> <C> −multi-lingual <C> 68.21 <C> 61.99 <C> 52.40 <R> <C> subs6M [ITALIC]  [ITALIC] LM detectron <C> 68.29 <C> 61.73 <C> 53.05 <R> <C> subs3M [ITALIC]  [ITALIC] LM gn2048 <C> 67.74 <C> 61.78 <C> 52.76 <R> <C> subs3M [ITALIC]  [ITALIC] LM text-only <C> 67.72 <C> 61.75 <C> 53.02 <R> <C> en-de <C> flickr16 <C> flickr17 <C> mscoco17 <R> <C> subs3M [ITALIC]  [ITALIC] LM detectron <C> 45.09 <C> 40.81 <C> 36.94 <R> <C> +ensemble-of-3 <C> 45.52 <C> [BOLD] 41.84 <C> [BOLD] 37.49 <R> <C> −visual features <C> [BOLD] 45.59 <C> 41.75 <C> 37.43 <R> <C> −MS-COCO <C> 45.11 <C> 40.52 <C> 36.47 <R> <C> −multi-lingual <C> 44.95 <C> 40.09 <C> 35.28 <R> <C> subs6M [ITALIC]  [ITALIC] LM detectron <C> 45.50 <C> 41.01 <C> 36.81 <R> <C> subs3M [ITALIC]  [ITALIC] LM gn2048 <C> 45.38 <C> 40.07 <C> 36.82 <R> <C> subs3M [ITALIC]  [ITALIC] LM text-only <C> 44.87 <C> 41.27 <C> 36.59 <R> <C> +multi-modal finetune <C> 44.56 <C> 41.61 <C> 36.93 <CAP> Table 6: Ablation experiments (BLEU% scores). The row subs3MLM detectron shows our best single model. Individual components or data choices are varied one by one. + stands for adding a component, and − for removing a component or data set. Multiple modifications are indicated by increasing the indentation.
<R> <C> Translation <C> Yule’s I <C> TTR <C> MTLD <R> <C> [EMPTY] <C> [EMPTY] <C> * 1000 <C> [EMPTY] <R> <C> en-fr-HT <C> [ITALIC] 9.2793 <C> [ITALIC] 2.9277 <C> [ITALIC] 127.1766 <R> <C> en-fr-rnn-ff <C> 0.7107 <C> 0.8656 <C> 109.4506 <R> <C> en-fr-smt-ff <C> 6.7492 <C> 2.6442 <C> 118.1239 <R> <C> en-fr-trans-ff <C> 1.1768 <C> 1.0925 <C> 120.5179 <R> <C> en-fr-rnn-back <C> 0.7587 <C> 0.8776 <C> 116.8942 <R> <C> en-fr-smt-back <C> [BOLD] 7.8738 <C> [BOLD] 2.7496 <C> 120.9909 <R> <C> en-fr-trans-back <C> 1.0325 <C> 1.0172 <C> [BOLD] 121.5801 <R> <C> en-es-HT <C> [ITALIC] 12.3065 <C> [ITALIC] 3.7037 <C> [ITALIC] 99.0850 <R> <C> en-es-rnn-ff <C> 0.6298 <C> 0.9394 <C> 89.3562 <R> <C> en-es-smt-ff <C> 7.3249 <C> 3.1170 <C> 95.1146 <R> <C> en-es-trans-ff <C> 1.0022 <C> 1.1581 <C> [BOLD] 96.2113 <R> <C> en-es-rnn-back <C> 0.7355 <C> 0.9829 <C> 95.7198 <R> <C> en-es-smt-back <C> [BOLD] 8.1325 <C> [BOLD] 3.2166 <C> 95.1479 <R> <C> en-es-trans-back <C> 0.9162 <C> 1.1014 <C> 95.0886 <CAP> Table 6: Lexical richness metrics (Train set).
<R> <C> Language pair <C> Train <C> Test <C> Dev <R> <C> EN–FR <C> 1,467,489 <C> 499,487 <C> 7,723 <R> <C> EN–ES <C> 1,472,203 <C> 459,633 <C> 5,734 <CAP> Table 1: Number of parallel sentences in the train, test and development splits for the language pairs we used.
<R> <C> Language pair <C> SRC <C> TRG <R> <C> EN–FR <C> 113,132 <C> 131,104 <R> <C> EN–ES <C> 113,692 <C> 168,195 <CAP> Table 2: Training vocabularies for the English, French and Spanish data used for our models.
<R> <C> System reference <C> BLEU↑ <C> TER↓ <R> <C> en-fr-rnn-rev <C> 33.3 <C> 50.2 <R> <C> en-fr-smt-rev <C> 36.5 <C> 47.1 <R> <C> en-fr-trans-rev <C> [BOLD] 36.8 <C> [BOLD] 46.8 <R> <C> en-es-rnn-rev <C> 37.8 <C> 45.0 <R> <C> en-es-smt-rev <C> 39.2 <C> 44.0 <R> <C> en-es-trans-rev <C> [BOLD] 40.4 <C> [BOLD] 42.7 <CAP> Table 5: Automatic evaluation scores (BLEU and TER) for the REV systems.
<R> <C> [EMPTY] <C> Recall@10 (%) <C> Median rank <C> RSAimage <R> <C> VGS <C> 15 <C> 17 <C> 0.2 <R> <C> SegMatch <C> 12 <C> 17 <C> 0.0 <R> <C> Mean MFCC <C> 0 <C> 711 <C> 0.0 <CAP> Table 2: Results on Flickr8K. The row labeled VGS is the visually supervised model from chrupala2017representations.
<R> <C> [EMPTY] <C> Recall@10 (%) <C> Median rank <C> RSAimage <R> <C> VGS <C> 27 <C> 6 <C> 0.4 <R> <C> SegMatch <C> [BOLD] 10 <C> [BOLD] 37 <C> [BOLD] 0.5 <R> <C> Audio2vec-U <C> 5 <C> 105 <C> 0.0 <R> <C> Audio2vec-C <C> 2 <C> 647 <C> 0.0 <R> <C> Mean MFCC <C> 1 <C> 1,414 <C> 0.0 <R> <C> Chance <C> 0 <C> 3,955 <C> 0.0 <CAP> Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from chrupala2017representations.
<R> <C> Orig <C> <u> turns in a <u> screenplay that <u> at the edges ; it ’s so clever you want to hate it . <R> <C> DAN <C> <u> turns in a <u> screenplay screenplay screenplay of <u> edges edges edges shapes so clever easy want hate hate hate hate hate hate hate hate hate hate <R> <C> CNN <C> she turns on a on ( ( in in the the the edges ’s so clever “ want to hate it ” <R> <C> RNN <C> <u> turns in a <u> screenplay was <u> <u> <u> edges edges edges curves <u> clever clever you want hate hate it . <CAP> Table 1: Example sentences of the different classifiers compared to the original on SST-2. We report further examples in the Appendix. <u> use for <UNK>.
<R> <C> [EMPTY] <C> <bold>RNN</bold> <C> <bold>CNN</bold> <C> <bold>DAN</bold> <R> <C> Nouns <C> +63 <C> −3 <C> +<bold>93</bold> <R> <C> DT <C> −29 <C> +32 <C> −<bold>38</bold> <R> <C> Verbs <C> +20 <C> −4 <C> +<bold>34</bold> <R> <C> Adj. <C> +25 <C> −1 <C> +<bold>66</bold> <R> <C> Prep. <C> +12 <C> +12 <C> −<bold>62</bold> <R> <C> Punct. <C> −<bold>53</bold> <C> −14 <C> −47 <R> <C> <U> <C> +<bold>82</bold> <C> −14 <C> +16 <R> <C> <bold>RNP</bold> <C> 69.0% <C> 70.5% <C> 81.5% <CAP> Table 2: Part-of-Speech (POS) changes in SST-2: , , and indicate that the number of occurrences have increased, decreased or stayed the same through fine-tuning respectively. The symbols are purely analytic without any notion of goodness. The numbers indicate the changes in percentage points with respect to the original sentence. A score of 0 thus means that fine-tuning has not changed the number of words. The last row indicates the overlap with the extractive RNP approach. We report results for PubMed in the Appendix.
<R> <C> [EMPTY] <C> <bold>RNN</bold> <C> <bold>CNN</bold> <C> <bold>DAN</bold> <R> <C> Positive <C> +9.7 <C> +4.3 <C> +<bold>23.6</bold> <R> <C> Negative <C> +6.9 <C> +5.5 <C> +<bold>16.1</bold> <R> <C> Flipped to Positive <C> +20.2 <C> +24.9 <C> +27.4 <R> <C> Flipped to Negative <C> +31.5 <C> +28.6 <C> +19.3 <CAP> Table 3: Sentiment score changes in SST-2. The numbers indicate the changes in percentage points with respect to the original sentence. The last two rows correspond to the case where negative labels are flipped to positive and vice versa. and indicate that the score increases in positive and negative sentiment.
<R> <C> [EMPTY] <C> <bold>SST-2</bold> Positive <C> <bold>SST-2</bold> Negative <C> <bold>PubMed</bold> Objective <C> <bold>PubMed</bold> Conclusion <R> <C> <bold>PMI</bold> <C> best <C> too <C> compare <C> should <R> <C> <bold>PMI</bold> <C> love <C> bad <C> investigate <C> suggest <R> <C> <bold>PMI</bold> <C> fun <C> n’t <C> evaluate <C> findings <R> <C> <bold>SIFT</bold> <C> but <C> nothing <C> to <C> larger <R> <C> <bold>SIFT</bold> <C> come <C> awkward <C> whether <C> confirm <R> <C> <bold>SIFT</bold> <C> it <C> lacking <C> clarify <C> concluded <R> <C> Acc <C> <italic>98%</italic> <C> <italic>98%</italic> <C> <italic>98%</italic> <C> <italic>99%</italic> <R> <C> <italic>Corr</italic> <C> <italic>0.486</italic> <C> <italic>0.5415</italic> <C> <italic>0.398</italic> <C> <italic>0.00089</italic> <CAP> Table 4: Top 3 PMI and SIFT terms for a subset of classes with SIFT on the RNN model. The second last row depicts the SIFT accuracy on the test set for the flipped label setting. The last row indicates the correlation of the PMI and SIFT list using weighted Kendall’s tau correlation Shieh1998 .
<R> <C> Approach <C> UU <C> UB <C> BhalfB <C> BB <R> <C> Prior <C> 27.38 <C> 24.04 <C> 24.04 <C> 24.04 <R> <C> Language-only <C> 48.21 <C> 41.40 <C> 41.47 <C> 43.01 <R> <C> d-LSTM+n-I  <C> 54.40 <C> 47.56 <C> 49.23 <C> 51.62 <R> <C> HieCoAtt  <C> 57.09 <C> 50.31 <C> 51.88 <C> 54.57 <R> <C> MCB  <C> 60.36 <C> 54.22 <C> 56.08 <C> 59.14 <CAP> Table 1: Performance of VQA models when trained/tested on unbalanced/balanced VQA datasets. UB stands for training on Unbalanced train and testing on Balanced val datasets. UU, BhalfB and BB are defined analogously.
<R> <C> [EMPTY] <C> [EMPTY] <C> [ITALIC]  [BOLD] μ [BOLD] r ’00-’13 <C> [ITALIC]  [BOLD] μ [BOLD] r avg±std <C> [BOLD] Rec@5 ’00-’13 <C> [BOLD] Rec@5 avg±std <C> [BOLD] Rec@10 ’00-’13 <C> [BOLD] Rec@10 avg±std <C> [BOLD] Rec@50 ’00-’13 <C> [BOLD] Rec@50 avg±std <R> <C> Past Work/Baselines <C> RAND <C> 49.97 <C> 50.01±0.04 <C> 5.00 <C> 4.99±0.03 <C> 10.01 <C> 9.98±0.04 <C> 50.02 <C> 49.97±0.08 <R> <C> Past Work/Baselines <C> PROCR <C> 30.63 <C> 28.51±2.68 <C> 18.46 <C> 14.32±5.00 <C> 27.69 <C> 29.94±4.64 <C> 78.46 <C> [BOLD] 80.47±3.79 <R> <C> Past Work/Baselines <C> PROCR [ITALIC] k <C> 31.47 <C> 28.71±2.65 <C> 20.00 <C> 14.67±3.85 <C> 29.23 <C> 28.76±4.32 <C> 72.31 <C> 79.64±4.49 <R> <C> Past Work/Baselines <C> PROCR [ITALIC] kt <C> 31.91 <C> 28.47±2.85 <C> 20.00 <C> 14.32±4.23 <C> 27.69 <C> 28.88±4.45 <C> 70.77 <C> 80.00±4.53 <R> <C> Past Work/Baselines <C> RF <C> 30.01 <C> 30.45±4.15 <C> 10.77 <C> 15.62±4.30 <C> 21.54 <C> 27.46±7.16 <C> 78.46 <C> 77.63±6.42 <R> <C> Past Work/Baselines <C> LSTM [ITALIC] r <C> 27.87 <C> [BOLD] 27.83±2.65 <C> 12.31 <C> 15.98±5.94 <C> 29.23 <C> 30.30±6.39 <C> 80.00 <C> 80.12±4.72 <R> <C> Past Work/Baselines <C> LSTM [ITALIC] f <C> 28.62 <C> 28.61±3.47 <C> 16.92 <C> [BOLD] 17.40±5.60 <C> 32.31 <C> [BOLD] 31.83±6.07 <C> 76.92 <C> 78.82±4.83 <R> <C> Past Work/Baselines <C> GT [ITALIC] c <C> 47.87 <C> 44.04±1.54 <C> 7.69 <C> 7.41±2.26 <C> 16.92 <C> 14.13±3.76 <C> 52.31 <C> 57.90±2.94 <R> <C> Past Work/Baselines <C> GT [ITALIC] β <C> 38.09 <C> 36.16±1.74 <C> 13.85 <C> 14.83±4.14 <C> 24.62 <C> 23.36±3.94 <C> 66.15 <C> 69.37±3.26 <R> <C> Past Work/Baselines <C> PROCR∗ <C> 25.01 <C> 27.99±3.03 <C> 21.54 <C> 15.15 ±4.52 <C> 32.31 <C> 28.40±3.75 <C> 81.54 <C> 80.24±3.49 <R> <C> Ours <C> seq2seq [ITALIC] r <C> 24.75 <C> 28.36±3.38 <C> 21.54 <C> 19.05±4.47 <C> 38.46 <C> 29.94±6.64 <C> [BOLD] 84.62 <C> 81.42±4.64 <R> <C> Ours <C> seq2seq [ITALIC] f <C> [BOLD] 23.86 <C> 27.17±4.16 <C> 26.15 <C> 22.01±6.72 <C> [BOLD] 46.15 <C> 34.32±10.13 <C> [BOLD] 84.62 <C> 81.18±5.07 <R> <C> Ours <C> seq2seq [ITALIC] rf <C> 24.28 <C> [BOLD] 24.29±0.67 <C> [BOLD] 29.23 <C> [BOLD] 25.77±2.28 <C> 36.92 <C> [BOLD] 39.49±2.11 <C> [BOLD] 84.62 <C> [BOLD] 85.00±1.16 <CAP> Table 1: Performance of our models and the baselines when operating on the entire time sequence (2000-2013) and averaged across time (2000-01, …, 2000-13). PROCR and PROCRk(t) are based on the methods employed in Hamilton et al. (2016) and Tsakalidis et al. (2019), respectively; GTc,β models are based on the work by Shoemark et al. (2019). The complete results in μr across all runs are provided in Appendix B.
<R> <C> [EMPTY] <C> 1 <C> 2 <C> 3 <C> 4 <C> 5 <C> 6 <C> 7 <C> 8 <R> <C> M1* <C> 0.0056 <C> 0.0024 <C> 0.0105 <C> 0.0061 <C> 0.0055 <C> 0.0119 <C> 0.0148 <C> 0.0093 <R> <C> M1+ <C> 0.0044 <C> 0.0029 <C> 0.0127 <C> 0.0043 <C> 0.0046 <C> 0.0133 <C> 0.0120 <C> 0.0108 <R> <C> M2* <C> 0.0054 <C> 0.0028 <C> 0.0147 <C> 0.0035 <C> 0.0115 <C> 0.0149 <C> 0.0132 <C> 0.0167 <R> <C> M2+ <C> 0.0026 <C> 0.0020 <C> 0.0125 <C> 0.0050 <C> 0.0096 <C> 0.0154 <C> 0.0156 <C> 0.0125 <R> <C> M2Max* <C> 0.0012 <C> 0.0007 <C> 0.0049 <C> -0.0003 <C> 0.0005 <C> 0.0069 <C> 0.0010 <C> 0.0011 <R> <C> M2Max+ <C> 0.0028 <C> 0.0013 <C> 0.0068 <C> 0.0024 <C> 0.0018 <C> 0.0071 <C> 0.0026 <C> 0.0038 <CAP> Table 3: Average difference between the AP of the document-specific and generic models with λ=0.01.
<R> <C> No. Non-Compositional <C> 43 (3.6%) <R> <C> No. Mostly Non-Compositional <C> 145 (12.1%) <R> <C> No. Ambiguos Phrases <C> 126 (10.5%) <R> <C> No. Mostly Compositional <C> 141 (12.0%) <R> <C> No. Compositional <C> 739 (61.8%) <R> <C> Unique number of Phrases <C> 1042 <R> <C> No. of context <C> 1194 <R> <C> Average number of context by Phrase <C> 1.146 <CAP> Table 1. Results of different compositionality detection methods; na denotes not applicable.
<R> <C> Subset <C> Feature <C> mean <C> Trust std <C> max <C> mean <C> Fake std <C> max <R> <C> [EMPTY] <C> Profile description <C> 0.19 <C> 0.39 <C> 1.0 <C> 0.06 <C> 0.24 <C> 1.0 <R> <C> Personal <C> Bookmark lists <C> 36.47 <C> 183.19 <C> 5842.0 <C> 2.09 <C> 27.74 <C> 1717.0 <R> <C> [EMPTY] <C> Lists <C> 1.45 <C> 15.67 <C> 712.0 <C> 0.04 <C> 0.58 <C> 30.0 <R> <C> [EMPTY] <C> Review updates <C> 4.22 <C> 20.80 <C> 562.0 <C> 0.34 <C> 2.52 <C> 85.0 <R> <C> [EMPTY] <C> Friends’ no. of friends <C> 231.75 <C> 417.09 <C> 5000.0 <C> 66.77 <C> 269.39 <C> 13699.0 <R> <C> [EMPTY] <C> Friends’ no. of reviews <C> 80.6 <C> 189.99 <C> 2603.0 <C> 26.70 <C> 121.96 <C> 2885.0 <R> <C> [EMPTY] <C> Profile has photo <C> 0.76 <C> 0.43 <C> 1.0 <C> 0.41 <C> 0.49 <C> 1.0 <R> <C> Social <C> No. of followers <C> 6.18 <C> 45.34 <C> 1782.0 <C> 0.38 <C> 5.02 <C> 263.0 <R> <C> [EMPTY] <C> No. of friends <C> 70.86 <C> 260.70 <C> 5000.0 <C> 13.90 <C> 106.14 <C> 5000.0 <R> <C> [EMPTY] <C> No. of votes ‘cool’ <C> 155.91 <C> 1169.02 <C> 35842.0 <C> 5.41 <C> 112.61 <C> 5440.0 <R> <C> [EMPTY] <C> No. of votes ‘useful’ <C> 231.35 <C> 1449.30 <C> 51012.0 <C> 8.58 <C> 128.43 <C> 6170..0 <R> <C> [EMPTY] <C> No. of votes ‘funny’ <C> 136.18 <C> 1010.25 <C> 32844.0 <C> 4.35 <C> 92.27 <C> 4184.0 <R> <C> [EMPTY] <C> No. of reviews <C> 77.71 <C> 328.41 <C> 11225.0 <C> 7.78 <C> 42.14 <C> 1404.0 <R> <C> [EMPTY] <C> Rating distribution (% 5 stars) <C> 0.37 <C> 0.31 <C> 1 <C> 0.14 <C> 0.27 <C> 1.0 <R> <C> [EMPTY] <C> Rating distribution (% 4 stars) <C> 0.13 <C> 0.16 <C> 0.83 <C> 0.05 <C> 0.13 <C> 1 <R> <C> Review Activity <C> Rating distribution (% 3 stars) <C> 0.06 <C> 0.09 <C> 1.0 <C> 0.02 <C> 0.07 <C> 0.8 <R> <C> [EMPTY] <C> Rating distribution (% 2 stars) <C> 0.05 <C> 0.08 <C> 0.8 <C> 0.02 <C> 0.06 <C> 0.6 <R> <C> [EMPTY] <C> Rating distribution (% 1 star) <C> 0.12 <C> 0.17 <C> 1.0 <C> 0.07 <C> 0.18 <C> 1.0 <R> <C> [EMPTY] <C> Average rating <C> 2.79 <C> 1.78 <C> 5.0 <C> 1.1 <C> 1.74 <C> 5.0 <R> <C> Trust <C> No. of photos <C> 127.39 <C> 1135.01 <C> 57761.0 <C> 5.60 <C> 141.04 <C> 7599.0 <R> <C> [EMPTY] <C> No. of tips <C> 24.29 <C> 269.99 <C> 16364.0 <C> 1.27 <C> 18.56 <C> 1040.0 <CAP> TABLE IV: Selected features distributions over the whole dataset
<R> <C> [BOLD] City <C> [BOLD] Features <C> LR <C> DT <C> RF <C> GNB <C> AB <R> <C> All cities <C> S + P + T + RA <C> 0.77 <C> 0.80 <C> 0.80 <C> 0.71 <C> [BOLD] 0.81 <R> <C> New York <C> S + P + T + RA <C> 0.79 <C> 0.81 <C> [BOLD] 0.82 <C> 0.72 <C> [BOLD] 0.82 <R> <C> Los Angeles <C> S + P + T + RA <C> 0.73 <C> 0.73 <C> 0.78 <C> 0.69 <C> [BOLD] 0.79 <R> <C> Miami <C> S + P + T + RA <C> 0.78 <C> 0.81 <C> 0.81 <C> 0.71 <C> [BOLD] 0.82 <R> <C> San Francisco <C> S + P + T + RA <C> 0.78 <C> 0.81 <C> 0.81 <C> 0.69 <C> [BOLD] 0.82 <R> <C> Friedman rank <C> Friedman rank <C> 3.87 <C> 2.87 <C> 2.12 <C> 5 <C> 1.12 <CAP> TABLE V: F-Score results
<R> <C> [BOLD] Model <C> [BOLD] Slot  [ITALIC] P <C> [BOLD] Slot  [ITALIC] R <C> [BOLD] Slot  [ITALIC] F1 <C> [BOLD] Intent  [ITALIC] Acc. <R> <C> Hakkani-Tur et al., 2016  <C> [EMPTY] <C> [EMPTY] <C> 94.3 <C> 92.6 <R> <C> Liu and Lane, 2016  <C> [EMPTY] <C> [EMPTY] <C> 94.2 <C> 91.1 <R> <C> Goo et al., 2018  <C> [EMPTY] <C> [EMPTY] <C> 95.2 <C> 94.1 <R> <C> Highway:W <C> 95.4 <C> 95.3 <C> 95.4 <C> 96.5 <R> <C> Highway:CNN <C> 94.5 <C> 94.1 <C> 94.3 <C> 95.8 <R> <C> Highway:W+CNN <C> 95.7 <C> 95.6 <C> [BOLD] 95.6 <C> 96.8 <R> <C> GRU:W+CNN <C> 95.2 <C> 95.3 <C> 95.2 <C> 96.8 <R> <C> MulHeadAtt:W+CNN <C> 93.7 <C> 94.3 <C> 94.0 <C> [BOLD] 97.0 <R> <C> Block-Dim. Att:W+CNN <C> 93.9 <C> 94.6 <C> 94.3 <C> 96.8 <CAP> Table 1: Different models in basic setting compared to the state-of-the-art results borrowed from [18]. W–Word embeddings, CNN – CNN character embeddings
<R> <C> [BOLD] Model <C> [BOLD] Slot  [ITALIC] P <C> [BOLD] Slot  [ITALIC] R <C> [BOLD] Slot  [ITALIC] F1 <C> [BOLD] Intent  [ITALIC] Acc. <R> <C> Monolingual <C> 90.6 <C> 88.6 <C> 89.6 <C> 87.8 <R> <C> All <C> 92.4 <C> 92.3 <C> [BOLD] 92.3 <C> 89.0 <R> <C> Full-Slot <C> 85.6 <C> 89.0 <C> 88.8 <C> 87.6 <R> <C> Full-Multidim <C> 90.4 <C> 90.2 <C> 90.3 <C> [BOLD] 89.5 <R> <C> Full-bi-LSTM <C> 87.8 <C> 88.3 <C> 88.0 <C> 85.8 <CAP> Table 4: Monolingual model vs. weight transfer results on German ATIS translations.
<R> <C> Dataset <C> Orthographic <C> PMI <C> Manhattan ConvNet <R> <C> Austronesian <C> 0.766 <C> 0.78 <C> 0.746 <R> <C> Indo-European <C> 0.815 <C> 0.804 <C> 0.804 <R> <C> Austronesian <C> 0.821 <C> 0.837 <C> 0.820 <R> <C> Austronesian <C> 0.661 <C> 0.656 <C> 0.570 <R> <C> Austronesian <C> 0.759 <C> 0.768 <C> 0.728 <R> <C> Indo-European <C> 0.876 <C> 0.873 <C> 0.871 <R> <C> Indo-European <C> 0.631 <C> 0.569 <C> 0.590 <R> <C> Indo-European <C> 0.806 <C> 0.786 <C> 0.791 <R> <C> Austronesian <C> 0.771 <C> 0.795 <C> 0.707 <R> <C> Indo-European <C> 0.731 <C> 0.692 <C> 0.691 <CAP> Table 5: Testing accuracies, class-wise and combined F-scores, average precision score of each system on Indo-European and Austronesian families.
<R> <C> [BOLD] Order <C> [BOLD] En → De  [BOLD] Serial <C> [BOLD] En → De  [BOLD] Parallel <C> [BOLD] En → Zh  [BOLD] Serial <C> [BOLD] En → Zh  [BOLD] Parallel <R> <C> [BOLD] Order <C> vaswani-nips-2017 <C> vaswani-nips-2017 <C> This Work <C> This Work <R> <C> Transformer <C> 27.3 <C> [EMPTY] <C> 35.8 <C> [EMPTY] <R> <C> [EMPTY] <C> stern-arxiv-2019 <C> stern-arxiv-2019 <C> This Work <C> This Work <R> <C> Uniform <C> 27.12 <C> 26.72 <C> 32.9 <C> 33.1 <R> <C> Binary Tree <C> 27.29 <C> 27.41 <C> 32.6 <C> 34.0 <R> <C> [EMPTY] <C> This Work <C> This Work <C> This Work <C> This Work <R> <C> Random <C> 26.15 <C> 26.10 <C> 32.6 <C> 32.4 <R> <C> Left-to-Right <C> 26.37 <C> 25.56 <C> 31.7 <C> 31.2 <R> <C> Right-to-Left <C> 26.60 <C> 24.49 <C> 32.4 <C> 30.8 <R> <C> Common First <C> 26.88 <C> 26.86 <C> 33.5 <C> 32.9 <R> <C> Rare First <C> 26.06 <C> 26.24 <C> 32.5 <C> 32.2 <R> <C> Shortest First <C> 27.05 <C> 27.15 <C> 33.0 <C> 32.7 <R> <C> Longest First <C> 26.45 <C> 26.41 <C> 32.8 <C> 33.2 <R> <C> Alphabetical (A → z) <C> 26.86 <C> 26.58 <C> 32.7 <C> 32.5 <R> <C> Alphabetical (z → A) <C> 27.22 <C> 26.37 <C> 33.1 <C> 33.0 <R> <C> Easy First <C> 26.95 <C> 27.05 <C> 32.5 <C> 32.5 <R> <C> Hard First <C> 25.85 <C> 26.30 <C> 32.4 <C> 32.9 <CAP> Table 3: Test BLEU results for WMT14 En-De newstest2014 and WMT18 En-Zh newstest2018 with serial and parallel decoding.
<R> <C> Model <C> [ITALIC] AdverSuc <C> [ITALIC] machine-vs-random <R> <C> MLE-BS <C> 0.037 <C> 0.942 <R> <C> MLE-Greedy <C> 0.049 <C> 0.945 <R> <C> MMI+ [ITALIC] p( [ITALIC] t| [ITALIC] s) <C> 0.073 <C> 0.953 <R> <C> MMI- [ITALIC] p( [ITALIC] t) <C> 0.090 <C> 0.880 <R> <C> Sampling <C> 0.372 <C> 0.679 <R> <C> Adver-Reinforce <C> 0.080 <C> 0.945 <R> <C> Adver-REGS <C> 0.098 <C> 0.952 <CAP> Table 3: AdverSuc and machine-vs-random scores achieved by different training/decoding strategies.
<R> <C> Setting <C> adver-win <C> adver-lose <C> tie <R> <C> single-turn <C> 0.62 <C> 0.18 <C> 0.20 <R> <C> multi-turn <C> 0.72 <C> 0.10 <C> 0.18 <CAP> Table 4: The gain from the proposed adversarial model over the mutual information system based on pairwise human judgments.
<R> <C> [EMPTY] <C> Flickr 8K  <C> Composite  <R> <C> Bleu-1 <C> 0.32 <C> 0.26 <R> <C> Bleu-4 <C> 0.14 <C> 0.18 <R> <C> ROUGE-L <C> 0.32 <C> 0.28 <R> <C> METEOR <C> 0.42 <C> 0.35 <R> <C> CIDEr <C> 0.44 <C> 0.36 <R> <C> [BOLD] SPICE <C> [BOLD] 0.45 <C> [BOLD] 0.39 <R> <C> Inter-human <C> 0.73 <C> - <CAP> Table 3: Caption-level Kendall’s τ correlation between evaluation metrics and graded human quality scores. At the caption-level SPICE modestly outperforms existing metrics. All p-values (not shown) are less than 0.001
<R> <C> [EMPTY] <C> HC <C> HI <C> HM <C> MM <C> All <R> <C> Bleu-1 <C> [BOLD] 64.9 <C> 95.2 <C> 90.7 <C> 60.1 <C> 77.7 <R> <C> Bleu-2 <C> 56.6 <C> 93.0 <C> 87.2 <C> 58.0 <C> 73.7 <R> <C> ROUGE-L <C> 61.7 <C> 95.3 <C> 91.7 <C> 60.3 <C> 77.3 <R> <C> METEOR <C> 64.0 <C> [BOLD] 98.1 <C> [BOLD] 94.2 <C> 66.8 <C> [BOLD] 80.8 <R> <C> CIDEr <C> 61.9 <C> 98.0 <C> 91.0 <C> 64.6 <C> 78.9 <R> <C> SPICE <C> 63.3 <C> 96.3 <C> 87.5 <C> [BOLD] 68.2 <C> 78.8 <CAP> Table 4: Caption-level classification accuracy of evaluation metrics at matching human judgment on PASCAL-50S with 5 reference captions. SPICE is best at matching human judgments on pairs of model-generated captions (MM). METEOR is best at differentiating human and model captions (HM) and human captions where one is incorrect (HI). Bleu-1 performs best given two correct human captions (HC)
<R> <C> [EMPTY] <C> MRR <C> HITS@10 <C> MAP (w/ type checking) <R> <C> DistMult <C> 0.36 <C> 58.5 <C> 64.5 <R> <C> DistMult-tanh <C> 0.39 <C> 63.3 <C> 76.0 <R> <C> DistMult-tanh-WV-init <C> 0.28 <C> 52.5 <C> 65.5 <R> <C> DistMult-tanh-EV-init <C> [BOLD] 0.42 <C> [BOLD] 73.2 <C> [BOLD] 88.2 <CAP> Table 4: Evaluation with pre-trained vectors
<R> <C> Random effect <C> Individual intercept <C> Individual slope <R> <C> sentenceID_15 <C> 39.64 <C> 10.67 <R> <C> sentenceID_16 <C> 49.86 <C> 19.52 <R> <C> sentenceID_17 <C> 53.12 <C> 23.73 <R> <C> sentenceID_18 <C> 53.39 <C> 11.75 <R> <C> sentenceID_19 <C> 66.55 <C> 20.39 <R> <C> user_A <C> 45.96 <C> 3.06 <R> <C> user_B <C> 53.66 <C> 10.63 <R> <C> user_C <C> 51.23 <C> 15.01 <R> <C> user_D <C> 37.77 <C> 5.06 <R> <C> user_E <C> 53.07 <C> 7.28 <R> <C> user_F <C> 54.10 <C> 8.97 <CAP> Table 1: Excerpt for the model coefficients for the used fixed effect of online NMT adaptation to the individual intercepts for response variable hBLEU; in the example, the global intercept has a value of 47.19 and the global slope lies at 6.73
<R> <C> Model <C> Precision <C> Recall <C> F_{1} <R> <C> [Utiyama_Isahara_ACL2003] <C> 54.1 <C> 50.0 <C> 51.9 <R> <C> QANet (Ja-En) <C> 56.3 <C> [BOLD] 67.3 <C> 61.3 <R> <C> QANet (En-Ja) <C> 57.2 <C> [BOLD] 67.3 <C> 61.8 <R> <C> QANet (Ja-En) + ILP <C> [BOLD] 72.5 <C> 65.8 <C> [BOLD] 69.0 <R> <C> QANet (En-Ja) + ILP <C> 64.8 <C> 59.6 <C> 62.1 <R> <C> QANet (Bidi) + ILP <C> 67.3 <C> [BOLD] 67.3 <C> 67.3 <R> <C> BERT (Ja-En) <C> 83.5 <C> 70.6 <C> 76.5 <R> <C> BERT (En-Ja) <C> 86.0 <C> 69.9 <C> 77.1 <R> <C> BERT (Bidi) <C> [BOLD] 86.4 <C> [BOLD] 74.6 <C> [BOLD] 80.1 <CAP> Table 3: Experimental results using actual newspaper articles. Bold indicates the highest value for QANet and BERT.
<R> <C> Model <C> StackExchange AUC <C> StackExchange MAP <C> StackExchange MRR <C> StackExchange P@1 <C> StackExchange # Align. <C> MultiNews AUC <C> MultiNews MAP <C> MultiNews MRR <C> MultiNews P@1 <C> MultiNews # Align. <R> <C> OT <C> 98.0 <C> 91.2 <C> 91.5 <C> 86.1 <C> 8 <C> 97.5 <C> 96.8 <C> 98.1 <C> 97.2 <C> 48 <R> <C> OT (1:1) <C> 97.7 <C> 89.7 <C> 90.0 <C> 83.9 <C> 4 <C> 97.8 <C> 96.7 <C> 97.9 <C> 96.8 <C> 19 <R> <C> OT (relaxed 1:1) <C> 97.8 <C> 88.5 <C> 88.9 <C> 81.8 <C> 3 <C> 93.1 <C> 93.2 <C> 96.0 <C> 94.1 <C> 19 <R> <C> OT (exact  [ITALIC] k) <C> 98.1 <C> 92.3 <C> 92.5 <C> 87.8 <C> 2 <C> 96.4 <C> 96.3 <C> 97.7 <C> 96.6 <C> 6 <R> <C> Attention <C> 98.2 <C> 92.4 <C> 92.5 <C> 88.0 <C> 23 <C> 97.8 <C> 96.4 <C> 97.6 <C> 96.3 <C> 637 <R> <C> Attention ( [ITALIC] T=0.1) <C> 98.2 <C> 92.4 <C> 92.5 <C> 87.7 <C> 22 <C> 98.0 <C> 97.0 <C> 98.1 <C> 97.1 <C> 634 <R> <C> Attention ( [ITALIC] T=0.01) <C> 97.9 <C> 89.7 <C> 89.9 <C> 83.5 <C> 8 <C> 97.9 <C> 96.9 <C> 98.0 <C> 97.0 <C> 594 <R> <C> Sparse Attention <C> 98.0 <C> 92.5 <C> 92.6 <C> 88.3 <C> 19 <C> 98.2 <C> 97.7 <C> 98.1 <C> 97.1 <C> 330 <CAP> Table 3: Performance of all models on the StackExchange and MultiNews datasets. We report ranking results and the average number of active alignments (# Align.) used. For our method with the exact k alignment constraint, we set k=2 for StackExchange and k=6 for MultiNews, respectively.
<R> <C> Dataset <C> Voc. <C> Images InVoc. <C> Images OutVoc. <C> Words InVoc. <C> Words OutVoc. <R> <C> ST <C> ST <C> 7266715 <C> - <C> 76222 <C> - <R> <C> IC13 <C> ST <C> 857 <C> 158 <C> 549 <C> 142 <R> <C> IC15 <C> ST <C> 1369 <C> 442 <C> 669 <C> 348 <R> <C> SVT <C> ST <C> 530 <C> 117 <C> 333 <C> 94 <R> <C> SVTP <C> ST <C> 536 <C> 109 <C> 300 <C> 80 <R> <C> CT <C> ST <C> 218 <C> 70 <C> 171 <C> 63 <R> <C> IIIT <C> ST <C> 2429 <C> 571 <C> 1277 <C> 495 <R> <C> IIIT <C> Test <C> 1354 <C> 1646 <C> 502 <C> 1270 <CAP> Table 2: The number of words and images in training and evaluation data. “Voc.” is the vocabulary of datasets. “Test” is the vocabulary collected from test images except IIIT.
<R> <C> Linking Method <C> P <C> R <R> <C> String2Article <C> 0.9 <C> 0.97 <R> <C> String2Category <C> 1.0 <C> 0.27 <R> <C> TagMe <C> 0.7 <C> 0.83 <R> <C> Wikipedia index <C> 0.81 <C> 0.98 <CAP> Table 3: Correctness of different entity linking methods for image and caption nodes.
<R> <C> [EMPTY] <C> Avg cands. <C> P <C> R <C> F1 <C> ΔF1% <R> <C> Seeds <C> 8.6 <C> 0.18 <C> 0.16 <C> 0.17 <C> 0.0 <R> <C> Intermediates <C> 11.4 <C> 0.19 <C> 0.22 <C> 0.21 <C> +19.0%* <R> <C> Top Borders <C> 31 <C> 0.09 <C> 0.30 <C> 0.14 <C> -21.4%* <CAP> Table 4: Quality of gist candidate selection method. Significance is indicated by * (paired t-test, p-value ≤0.05).
<R> <C> [BOLD] Model <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 (%) <R> <C> Patterns∗ Angeli et al. ( 2015 ) <C> 86.9 <C> 23.2 <C> 36.6 <R> <C> LR* Surdeanu et al. ( 2012 ) <C> 73.5 <C> 49.9 <C> 59.4 <R> <C> LR + Patterns∗ Angeli et al. ( 2015 ) <C> 72.9 <C> 51.8 <C> [BOLD] 60.5 <R> <C> SDP-LSTM∗ Xu et al. ( 2015 ) <C> 66.3 <C> 52.7 <C> 58.7 <R> <C> Tree-LSTM∗ Tai et al. ( 2015 ) <C> 66.0 <C> 59.2 <C> 62.4 <R> <C> C-GCN Zhang et al. ( 2018 ) <C> 69.9 <C> 63.3 <C> 66.4 <R> <C> S-GCN Wu et al. ( 2019 ) <C> - <C> - <C> 67.0 <R> <C> C-AGGCN Guo et al. ( 2019 ) <C> 69.6 <C> 66.0 <C> [BOLD] 67.8 <R> <C> CNN∗ Kim ( 2014 ) <C> 75.6 <C> 47.5 <C> 58.3 <R> <C> CNN-PE∗ Zeng et al. ( 2014 ) <C> 70.3 <C> 54.2 <C> 61.2 <R> <C> LSTM∗ Zhang and Wang ( 2015 ) <C> 65.7 <C> 59.9 <C> 62.7 <R> <C> PA-LSTM Zhang et al. ( 2017 ) <C> 65.7 <C> 64.5 <C> 65.1 <R> <C> [BOLD] C-SGCN-Softmax <C> 69.3 <C> 65.4 <C> 67.3 <R> <C> [BOLD] C-SGCN <C> 69.8 <C> 65.9 <C> [BOLD] 67.8 <CAP> Table 1: Performance comparison of SGCN models against baselines. ∗ refers the performance was reported by Zhang et al. (2017). The performances of the feature-based, sequence-based and graph-based model are separated in the first, second and third part of the table. The best F1 score in each section is highlighted.
<R> <C> [BOLD] Model <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 (%) <R> <C> [BOLD] C-SGCN <C> 69.8 <C> 65.9 <C> [BOLD] 67.8 <R> <C> [BOLD] No_SGCN <C> 71.7 <C> 61.6 <C> 66.3 <R> <C> [BOLD] No_LSTM <C> 68.5 <C> 43.9 <C> 53.5 <R> <C> [BOLD] No_LA <C> 75.2 <C> 55.9 <C> 64.1 <CAP> Table 2: Ablation analysis on the test set of TACRED dataset.
<R> <C> [BOLD] Train <C> [BOLD] Metric <C> [BOLD] Text-based SVM <C> [BOLD] Text-based LIWC <C> [BOLD] Network Embed LP <C> [BOLD] Network Embed DW <C> [BOLD] Network Embed LINE <C> [BOLD] GNNs GAT <C> [BOLD] GNNs HAN <C> [BOLD] GNNs HGAT <R> <C> 20% <C> Accuracy <C> 0.1967 <C> 0.1432 <C> 0.2218 <C> 0.1932 <C> 0.1532 <C> 0.2110 <C> 0.2181 <C> [BOLD] 0.2561 <R> <C> 20% <C> F1 <C> 0.1624 <C> 0.1225 <C> 0.1925 <C> 0.1562 <C> 0.0765 <C> 0.1054 <C> 0.1234 <C> [BOLD] 0.2141 <R> <C> 20% <C> Recall <C> 0.1801 <C> 0.0965 <C> 0.2153 <C> 0.1718 <C> 0.1433 <C> 0.1975 <C> 0.1884 <C> [BOLD] 0.2415 <R> <C> 20% <C> Precision <C> 0.1905 <C> 0.1409 <C> [BOLD] 0.2859 <C> 0.1742 <C> 0.0326 <C> 0.1687 <C> 0.2467 <C> 0.2397 <R> <C> 40% <C> Accuracy <C> 0.2042 <C> 0.1543 <C> 0.2278 <C> 0.1952 <C> 0.1567 <C> 0.2237 <C> 0.2240 <C> [BOLD] 0.2757 <R> <C> 40% <C> F1 <C> 0.1775 <C> 0.1314 <C> 0.1944 <C> 0.1646 <C> 0.0798 <C> 0.1103 <C> 0.1441 <C> [BOLD] 0.2484 <R> <C> 40% <C> Recall <C> 0.1892 <C> 0.0987 <C> 0.2183 <C> 0.1742 <C> 0.1505 <C> 0.1987 <C> 0.1853 <C> [BOLD] 0.2616 <R> <C> 40% <C> Precision <C> 0.2047 <C> 0.1491 <C> 0.3037 <C> 0.1745 <C> 0.0401 <C> 0.1815 <C> 0.2572 <C> [BOLD] 0.3649 <R> <C> 60% <C> Accuracy <C> 0.2061 <C> 0.1513 <C> 0.2373 <C> 0.1969 <C> 0.1453 <C> 0.2214 <C> 0.2256 <C> [BOLD] 0.2707 <R> <C> 60% <C> F1 <C> 0.1871 <C> 0.1321 <C> 0.2099 <C> 0.1647 <C> 0.0653 <C> 0.1162 <C> 0.1475 <C> [BOLD] 0.2445 <R> <C> 60% <C> Recall <C> 0.1976 <C> 0.1002 <C> 0.2222 <C> 0.1764 <C> 0.1410 <C> 0.1954 <C> 0.1852 <C> [BOLD] 0.2576 <R> <C> 60% <C> Precision <C> 0.2118 <C> 0.1561 <C> 0.2955 <C> 0.1966 <C> 0.0307 <C> 0.1870 <C> 0.2792 <C> [BOLD] 0.3767 <R> <C> 80% <C> Accuracy <C> 0.2186 <C> 0.1567 <C> 0.2407 <C> 0.2013 <C> 0.1623 <C> 0.2212 <C> 0.2207 <C> [BOLD] 0.2665 <R> <C> 80% <C> F1 <C> 0.1962 <C> 0.1305 <C> 0.2187 <C> 0.1669 <C> 0.0875 <C> 0.1037 <C> 0.1218 <C> [BOLD] 0.2393 <R> <C> 80% <C> Recall <C> 0.2081 <C> 0.0954 <C> 0.2341 <C> 0.1830 <C> 0.1512 <C> 0.1975 <C> 0.1840 <C> [BOLD] 0.2586 <R> <C> 80% <C> Precision <C> 0.2233 <C> 0.1553 <C> 0.3149 <C> 0.1896 <C> 0.0468 <C> 0.1819 <C> 0.2497 <C> [BOLD] 0.3757 <CAP> Table 2: The results of multi-class news articles classification
<R> <C> Train. AM <C> Adapt. AM <C> DNN <C> CNN <C> TFCNN <C> sFCNN <C> rFCNN <R> <C> Nor. NL <C> - <C> 17.1 <C> 15.8 <C> 15.9 <C> 15.8 <C> 16.3 <R> <C> Nor. NL <C> Dys. NL <C> 12.6 <C> 11.3 <C> 11.2 <C> 10.6 <C> [BOLD] 10.3 <CAP> Table 8: Word error rates in % obtained on the Dutch test set using adapted acoustic models trained on gammatone filterbank features
<R> <C> Train. BN <C> Adapt. BN <C> Train. AM <C> Adapt. AM <C> DNN <C> CNN <C> TFCNN <R> <C> Nor. NL <C> - <C> Dys. NL <C> - <C> 12.9 <C> 13.8 <C> 12.0 <R> <C> Nor. NL <C> Dys. NL <C> Dys. NL <C> - <C> 11.8 <C> 13.1 <C> 13.4 <R> <C> Nor. NL <C> - <C> Nor. NL <C> - <C> 21.9 <C> 17.6 <C> 19.6 <R> <C> Nor. NL <C> Dys. NL <C> Nor. NL <C> - <C> 21.0 <C> 15.2 <C> 15.5 <R> <C> Nor. NL <C> - <C> Nor. NL <C> Dys. NL <C> 11.8 <C> 10.4 <C> 10.3 <R> <C> Nor. NL <C> Dys. NL <C> Nor. NL <C> Dys. NL <C> 11.0 <C> 10.2 <C> [BOLD] 10.0 <CAP> Table 9: Word error rates in % obtained on the Dutch test set by applying model adaptation at bottleneck extraction and acoustic modeling stages
<R> <C> Human Motion  [BOLD] # Recordings <C> Human Motion 3911 <C> Natural Language  [BOLD] # Annotations <C> Natural Language 6278 <R> <C> [BOLD] # Subjects <C> 111 <C> [BOLD] # Annotators <C> 110 <R> <C> [BOLD] Total Duration <C> 11.23h <C> [BOLD] # Words <C> 52903 <R> <C> [BOLD] Mean Duration <C> 10.33 ± 13.38s <C> [BOLD] Vocabulary <C> 1623 <CAP> Table I: Overview of the dataset content.
<R> <C> [EMPTY] <C> [BOLD] Perplexity <C> [BOLD] Annotation <R> <C> [BOLD] 1 <C> 2.85 <C> a person walks 4 steps forward <R> <C> [BOLD] 2 <C> 5.51 <C> a person walks forward then turns right by 180 degrees and walks forward again <R> <C> [BOLD] 3 <C> 12.24 <C> a human walks a very slow 90 degree arc to the right <R> <C> [BOLD] 4 <C> 31.33 <C> a person walks without a hurry <R> <C> [BOLD] 5 <C> 54.68 <C> subject walks backwards slightly angled <R> <C> [BOLD] 6 <C> 154.34 <C> some is reverse walking <R> <C> [BOLD] 7 <C> 309.08 <C> walk turn and walk back <R> <C> [BOLD] 8 <C> 1160.68 <C> slow walking motion <CAP> Table IV: Different annotations of walking motions, sorted by their perplexities in ascending order.
<R> <C> [EMPTY] <C> hits@1 <C> hits@5 <C> hits@10 <R> <C> GloVe [ITALIC] train <C> 12.6 <C> 39.6 <C> 63.7 <R> <C> GloVe [ITALIC] emb <C> 18.0 <C> 44.6 <C> 66.9 <R> <C> BERT [ITALIC] emb <C> 15.4 <C> 41.0 <C> 62.9 <R> <C> Fasttext [ITALIC] emb <C> 17.8 <C> 44.9 <C> 67.2 <R> <C> PR-Embedding <C> [BOLD] 22.4 <C> [BOLD] 60.0 <C> [BOLD] 81.1 <R> <C> IR baseline† <C> 21.4 <C> - <C> - <R> <C> Starpace† <C> 31.8 <C> - <C> - <R> <C> Profile Memory† <C> 31.8 <C> - <C> - <R> <C> KVMemnn <C> 32.3 <C> 62.0 <C> 79.2 <R> <C> +PR-Embedding <C> 35.9 <C> 66.1 <C> 82.6 <R> <C> KVMemnn (GloVe) <C> 36.8 <C> 68.1 <C> 83.6 <R> <C> +PR-Embedding <C> [BOLD] 39.9 <C> [BOLD] 72.4 <C> [BOLD] 87.0 <CAP> Table 1: Experimental results on the test set of the PersonaChat dataset. The upper part compares the embeddings in the single-turn and the lower one is for the multi-turn task. train: train the embedding with the training set; emb: use the public embedding directly; †: take the results from the paper of the dataset.
<R> <C> [EMPTY] <C> NDCG <C> NDCG@5 <C> P@1 <C> P@1(s) <R> <C> GloVe [ITALIC] train <C> 69.97 <C> 48.87 <C> 51.23 <C> 33.48 <R> <C> DSG [ITALIC] emb <C> 70.82 <C> 50.45 <C> 52.19 <C> 35.61 <R> <C> BERT [ITALIC] emb <C> 70.06 <C> 48.45 <C> 51.66 <C> 35.08 <R> <C> PR-Emb <C> [BOLD] 74.79 <C> [BOLD] 58.16 <C> [BOLD] 62.03 <C> [BOLD] 45.99 <R> <C> w/o PR <C> 70.68 <C> 50.60 <C> 50.48 <C> 35.19 <R> <C> w/o SLL <C> 71.65 <C> 52.03 <C> 53.48 <C> 40.86 <CAP> Table 2: Experimental results on the Chinese test set. P@1(s) means only use the response with label ‘good’ as the right one and other metrics treat the label ‘middle’ and ‘good’ as right.
<R> <C> [EMPTY] <C> Relevance <C> Readability <C> Total <R> <C> See et al. ( 2017 ) <C> 120 <C> 128 <C> 248 <R> <C> rnn-ext + abs + RL + rerank <C> [BOLD] 137 <C> [BOLD] 133 <C> [BOLD] 270 <R> <C> Equally good/bad <C> 43 <C> 39 <C> 82 <CAP> Table 4: Human Evaluation: pairwise comparison between our final model and get_to_the_point.
<R> <C> Models <C> Speed total time (hr) <C> Speed words / sec <R> <C> See et al. ( 2017 ) <C> 12.9 <C> 14.8 <R> <C> rnn-ext + abs + RL <C> 0.68 <C> 361.3 <R> <C> rnn-ext + abs + RL + rerank <C> 2.00 (1.46 +0.54) <C> 109.8 <CAP> Table 5: Speed comparison with get_to_the_point.
<R> <C> Models <C> Novel  [ITALIC] N-gram (%) 1-gm <C> Novel  [ITALIC] N-gram (%) 2-gm <C> Novel  [ITALIC] N-gram (%) 3-gm <C> Novel  [ITALIC] N-gram (%) 4-gm <R> <C> See et al. ( 2017 ) <C> 0.1 <C> 2.2 <C> 6.0 <C> 9.7 <R> <C> rnn-ext + abs + RL + rerank <C> 0.3 <C> 10.0 <C> 21.7 <C> 31.6 <R> <C> reference summaries <C> 10.8 <C> 47.5 <C> 68.2 <C> 78.2 <CAP> Table 6: Abstractiveness: novel n-gram counts.
<R> <C> [BOLD] Data Size <C> [BOLD] RA-WER <C> [BOLD] BLEU <C> [BOLD] M2 <R> <C> LARGE (18.6M) <C> 18.96 <C> 74.90 <C> 71.05 <R> <C> MODERATE (16M) <C> [BOLD] 17.15 <C> [BOLD] 76.20 <C> [BOLD] 71.76 <R> <C> SMALL (1.1M) <C> 18.56 <C> 74.51 <C> 71.43 <R> <C> ORIGIN (1.1M) <C> 24.46 <C> 67.28 <C> 51.80 <CAP> Table 4: Evaluation of MASS model that is fine-tuned on different size training dataset is shown. The values are evaluated on the CoNLL test set. The numbers in the parentheses are the approximate number of sentence pairs. MASS trained on MODERATE data achieves the best scores on all metrics. MASS trained on SMALL data gets a comparable result to the highest scores with a significantly smaller dataset. ORIGIN is the original GEC sentence pairs, which are used as the seed corpus for the APR task.
<R> <C> [BOLD] Input <C> [BOLD] PTs RMSE <C> [BOLD] PTs  [ITALIC] r <C> [BOLD] VTVs RMSE <C> [BOLD] VTVs  [ITALIC] r <R> <C> MFCCs (S1) <C> 0.894 <C> 0.448 <C> 0.879 <C> 0.517 <R> <C> MFCCs <C> 0.685 <C> 0.721 <C> 0.646 <C> 0.777 <R> <C> Phonemes <C> 0.664 <C> 0.742 <C> 0.617 <C> 0.782 <R> <C> LFs <C> 0.672 <C> 0.732 <C> 0.611 <C> 0.797 <R> <C> SFs <C> 0.667 <C> 0.744 <C> 0.618 <C> 0.783 <R> <C> MFCCs + Phonemes <C> 0.654 <C> 0.757 <C> 0.606 <C> 0.797 <R> <C> MFCCs + LFs <C> 0.657 <C> 0.748 <C> 0.602 <C> 0.801 <R> <C> MFCCs + SFs <C> 0.655 <C> 0.752 <C> 0.606 <C> 0.798 <CAP> Table 1: Supervised methods results on the test set for PT and VTV reconstruction in the matched condition case. MFCCs (S1) refers to a BLSTM trained on 1 single speaker data (JW14).
<R> <C> [BOLD] Input <C> [BOLD] Test gender <C> RMSE <C> [ITALIC] r <R> <C> MFCCs <C> [ITALIC] Male <C> 0.848 <C> 0.592 <R> <C> SFs <C> [ITALIC] Male <C> 0.604 <C> 0.782 <R> <C> MFCCs + SFs <C> [ITALIC] Male <C> 0.685 <C> 0.743 <R> <C> MFCCs <C> [ITALIC] Female <C> 0.860 <C> 0.557 <R> <C> SFs <C> [ITALIC] Female <C> 0.625 <C> 0.787 <R> <C> MFCCs + SFs <C> [ITALIC] Female <C> 0.686 <C> 0.748 <CAP> Table 4: BLSTM cross-gender VTV reconstruction.
<R> <C> Corpus <C> Vocabulary Size <C> Max sentence length <C> Average sentence length <R> <C> France <C> 3312 <C> 157 <C> 103 <R> <C> UK <C> 1202 <C> 116 <C> 50 <CAP> Table 3: Descriptive analysis of the two corpora (after preprocessing)
<R> <C> [EMPTY] <C> [EMPTY] <C> Algorithm Word2Vec <C> Algorithm GloVe <C> Algorithm LSA <R> <C> Corpus <C> Basic <C> 29 <C> 30 <C> 26 <R> <C> Corpus <C> Infused <C> 29 <C> 28 <C> 27 <CAP> Table 5: The change in optimal dimensionality of the word embeddings from basic to semantically infused corpus.
<R> <C> [EMPTY] <C> FK <C> BLEU(O, R) <C> BLEU(O, I) <C> iBLEU <C> SARI <R> <C> [BOLD] English Wikipedia <C> 13.32 <C> 28.19 <C> 100.0 <C> 15.37 <C> 13.59 <R> <C> [BOLD] Simple English Wikipedia <C> 10.75 <C> 100.0 <C> 30.41 <C> 86.96 <C> 91.47 <R> <C> [BOLD] Moses <C> 13.31 <C> 28.28 <C> 99.62 <C> 15.49 <C> 14.87 <R> <C> [BOLD] SBMT <C> 13.15 <C> 27.81 <C> 97.07 <C> 15.33 <C> 20.62 <R> <C> [BOLD] Seq2Seq <C> 12.74 <C> 25.51 <C> 66.94 <C> 16.27 <C> 33.16 <R> <C> [BOLD] Lexical Substitution <C> 13.34 <C> [BOLD] 30.44 <C> 84.31 <C> 18.97 <C> [BOLD] 46.29 <R> <C> [BOLD] Constrained Seq2Seq <C> [BOLD] 11.33 <C> [BOLD] 29.44 <C> 62.30 <C> [BOLD] 20.26 <C> 43.44 <R> <C> [BOLD] Multi-Constrained Seq2Seq <C> [BOLD] 11.02 <C> 27.94 <C> 52.72 <C> [BOLD] 19.87 <C> [BOLD] 43.98 <CAP> Table 2: Automatic evaluation of several simplification systems.
<R> <C> [EMPTY] <C> Grammaticality <C> Meaning <C> Simplicity <C> Same Percentage <R> <C> [BOLD] English Wikipedia <C> 4.00 <C> 4.00 <C> 0.00 <C> 120/120 <R> <C> [BOLD] Simple English Wikipedia <C> 3.55 <C> 2.83 <C> 2.20 <C> 0/120 <R> <C> [BOLD] Moses <C> 3.99 <C> 3.99 <C> 0.02 <C> 116/120 <R> <C> [BOLD] SBMT <C> 3.97 <C> 3.94 <C> 0.19 <C> 99/120 <R> <C> [BOLD] Lexical Substitution <C> 3.01 <C> 3.36 <C> 1.24 <C> 2/120 <R> <C> [BOLD] Seq2Seq <C> 3.28 <C> 3.45 <C> 0.91 <C> 30/120 <R> <C> [BOLD] Constrained Seq2Seq <C> 3.16 <C> 2.81 <C> 1.50 <C> 0/120 <R> <C> [BOLD] Multi-Constrained Seq2Seq <C> 2.60 <C> 2.65 <C> 1.62 <C> 0/120 <CAP> Table 3: Human evaluation of several simplification systems dicussed in our paper.
<R> <C> [BOLD] Data Source <C> [BOLD] Model <C> [BOLD] Content Preservation (↑) BLEU <C> [BOLD] Content Preservation (↑) ROUGE-1 <C> [BOLD] Content Preservation (↑) ROUGE-2 <C> [BOLD] Content Preservation (↑) ROUGE-3 <C> [BOLD] Content Preservation (↑) ROUGE-L <C> [BOLD] Stylistic Alignment (↓) Lexical (MSE) <C> [BOLD] Stylistic Alignment (↓) Syntactic (JSD) <C> [BOLD] Stylistic Alignment (↓) Surface (MSE) <R> <C> [BOLD] Opinosis <C> GPT-2 <C> 18.3±2.3 <C> 0.51±0.06 <C> 0.29±0.09 <C> 0.20±0.06 <C> 0.36±0.08 <C> 0.48±0.06 <C> 0.27±0.09 <C> 0.45±0.01 <R> <C> [BOLD] Opinosis <C> GPT-2 (FT) <C> 24.3±1.6 <C> 0.58±0.07 <C> 0.36±0.08 <C> 0.27±0.11 <C> 0.42±0.09 <C> 0.32±0.08 <C> 0.23±0.02 <C> 0.40±0.03 <R> <C> [BOLD] Opinosis <C> LM + DAE <C> 41.1±1.3 <C> [BOLD] 0.77±0.11 <C> 0.49±0.05 <C> 0.39±0.07 <C> 0.61±0.08 <C> 0.33±0.05 <C> 0.23±0.01 <C> 0.38±0.02 <R> <C> [BOLD] Opinosis <C> StyleLM <C> [BOLD] 43.4±1.7 <C> 0.73±0.13 <C> [BOLD] 0.53±0.06 <C> [BOLD] 0.41±0.08 <C> [BOLD] 0.68±0.07 <C> [BOLD] 0.29±0.04 <C> [BOLD] 0.19±0.01 <C> [BOLD] 0.31±0.04 <R> <C> [BOLD] Mark Twain <C> GPT-2 <C> 16.7±2.4 <C> 0.43±0.03 <C> 0.26±0.07 <C> 0.16±0.04 <C> 0.29±0.09 <C> 0.41±0.08 <C> 0.29±0.03 <C> 0.42±0.05 <R> <C> [BOLD] Mark Twain <C> GPT-2 (FT) <C> 22.9±1.6 <C> 0.49±0.06 <C> 0.38±0.08 <C> 0.21±0.06 <C> 0.37±0.08 <C> 0.35±0.07 <C> 0.25±0.02 <C> 0.39±0.06 <R> <C> [BOLD] Mark Twain <C> LM + DAE <C> 31.7±1.5 <C> [BOLD] 0.68±0.14 <C> 0.44±0.07 <C> 0.27±0.07 <C> 0.45±0.10 <C> 0.37±0.03 <C> 0.24±0.01 <C> 0.37±0.03 <R> <C> [BOLD] Mark Twain <C> StyleLM <C> [BOLD] 34.4±1.8 <C> 0.61±0.16 <C> [BOLD] 0.48±0.06 <C> [BOLD] 0.31±0.06 <C> [BOLD] 0.53±0.08 <C> [BOLD] 0.32±0.03 <C> [BOLD] 0.21±0.02 <C> [BOLD] 0.33±0.03 <R> <C> [BOLD] AI Wiki <C> GPT-2 <C> 12.6±2.1 <C> 0.37±0.04 <C> 0.19±0.09 <C> 0.09±0.05 <C> 0.25±0.08 <C> 0.49±0.07 <C> 0.31±0.02 <C> 0.46±0.05 <R> <C> [BOLD] AI Wiki <C> GPT-2 (FT) <C> 15.4±1.5 <C> 0.43±0.09 <C> 0.23±0.06 <C> 0.13±0.04 <C> 0.29±0.07 <C> 0.40±0.03 <C> 0.28±0.03 <C> 0.42±0.05 <R> <C> [BOLD] AI Wiki <C> LM + DAE <C> 23.7±1.6 <C> [BOLD] 0.59±0.12 <C> 0.31±0.08 <C> 0.18±0.06 <C> 0.37±0.09 <C> 0.41±0.04 <C> 0.26±0.02 <C> 0.41±0.03 <R> <C> [BOLD] AI Wiki <C> StyleLM <C> [BOLD] 26.7±1.9 <C> 0.54±0.13 <C> [BOLD] 0.34±0.08 <C> [BOLD] 0.23±0.08 <C> [BOLD] 0.46±0.09 <C> [BOLD] 0.34±0.02 <C> [BOLD] 0.22±0.01 <C> [BOLD] 0.36±0.04 <CAP> Table 1: Evaluating content preservation and stylistic alignment. We evaluate the performance of StyleLM against three baselines and on three test sets across multiple content preservation and stylistic alignment metrics. The reported numbers are mean and standard deviations (μ±σ) across all the 10 target authors. FT denotes author-specific fine-tuning; ↑ / ↓ indicates that higher / lower is better, respectively.
<R> <C> [EMPTY] <C> Transformer <C> DialoGPT <C> BERT-GPT <C> Groundtruth <R> <C> [EMPTY] <C> Transformer <C> No MMI <C> BERT-GPT <C> Groundtruth <R> <C> Relevance <C> 2.24 <C> 1.82 <C> 2.65 <C> 3.42 <R> <C> Informativeness <C> 2.06 <C> 1.72 <C> 2.37 <C> 3.26 <R> <C> Doctor-like <C> 2.57 <C> 1.80 <C> 3.16 <C> 3.78 <CAP> Table 9: Human evaluation on the CovidDialog-Chinese test set.
<R> <C> Embedding Size <C> 100 <R> <C> Hidden State Size  [ITALIC] dmodel <C> 256 <R> <C> Transformer Layers <C> 6 <R> <C> Attention Heads <C> 4 <R> <C> Gradients Clip <C> 5 <R> <C> Batch Size <C> 128 <R> <C> Embedding Dropout Ratio <C> 0.33 <R> <C> Initial Learning Rate <C> 2e-3 <R> <C> Annealing Rate <C> .75 [ITALIC] t/5000 <R> <C> Max epochs <C> 100 <CAP> Table 3: Hyper-Parameter Settings
<R> <C> [BOLD] Single-Criterion Learning Models <C> [BOLD] Single-Criterion Learning <C> [BOLD] Single-Criterion Learning MSRA <C> [BOLD] Single-Criterion Learning AS <C> [BOLD] Single-Criterion Learning PKU <C> [BOLD] Single-Criterion Learning CTB <C> [BOLD] Single-Criterion Learning CKIP <C> [BOLD] Single-Criterion Learning CITYU <C> [BOLD] Single-Criterion Learning NCC <C> [BOLD] Single-Criterion Learning SXU <C> [BOLD] Single-Criterion Learning Avg. <R> <C> Bi-LSTMschen2017adversarial <C> P <C> 95.7 <C> 93.64 <C> 93.67 <C> 95.19 <C> 92.44 <C> 94 <C> 91.86 <C> 95.11 <C> 93.95 <R> <C> Bi-LSTMschen2017adversarial <C> R <C> 95.99 <C> 94.77 <C> 92.93 <C> 95.42 <C> 93.69 <C> 94.15 <C> 92.47 <C> 95.23 <C> 94.33 <R> <C> Bi-LSTMschen2017adversarial <C> F <C> 95.84 <C> 94.2 <C> 93.3 <C> 95.3 <C> 93.06 <C> 94.07 <C> 92.17 <C> 95.17 <C> 94.14 <R> <C> Bi-LSTMschen2017adversarial <C> OOV <C> 66.28 <C> 70.07 <C> 66.09 <C> 76.47 <C> 72.12 <C> 65.79 <C> 59.11 <C> 71.27 <C> 68.4 <R> <C> Stacked Bi-LSTMchen2017adversarial <C> P <C> 95.69 <C> 93.89 <C> 94.1 <C> 95.2 <C> 92.4 <C> 94.13 <C> 91.81 <C> 94.99 <C> 94.03 <R> <C> Stacked Bi-LSTMchen2017adversarial <C> R <C> 95.81 <C> 94.54 <C> 92.66 <C> 95.4 <C> 93.39 <C> 93.99 <C> 92.62 <C> 95.37 <C> 94.22 <R> <C> Stacked Bi-LSTMchen2017adversarial <C> F <C> 95.75 <C> 94.22 <C> 93.37 <C> 95.3 <C> 92.89 <C> 94.06 <C> 92.21 <C> 95.18 <C> 94.12 <R> <C> Stacked Bi-LSTMchen2017adversarial <C> OOV <C> 65.55 <C> 71.5 <C> 67.92 <C> 75.44 <C> 70.5 <C> 66.35 <C> 57.39 <C> 69.69 <C> 68.04 <R> <C> Switch-LSTMs gong2018switch <C> P <C> 96.07 <C> 93.83 <C> 95.92 <C> [BOLD] 97.13 <C> 92.02 <C> 93.69 <C> 91.81 <C> 95.02 <C> 94.44 <R> <C> Switch-LSTMs gong2018switch <C> R <C> 96.86 <C> 95.21 <C> 95.56 <C> [BOLD] 97.05 <C> 93.76 <C> 93.73 <C> 92.43 <C> 96.13 <C> 95.09 <R> <C> Switch-LSTMs gong2018switch <C> F <C> 96.46 <C> 94.51 <C> 95.74 <C> [BOLD] 97.09 <C> 92.88 <C> 93.71 <C> 92.12 <C> 95.57 <C> 94.76 <R> <C> Switch-LSTMs gong2018switch <C> OOV <C> 69.9 <C> [BOLD] 77.8 <C> 72.7 <C> 81.8 <C> 71.6 <C> 59.8 <C> 55.5 <C> 67.3 <C> 69.55 <R> <C> Transformer <C> P <C> [BOLD] 98.14 <C> [BOLD] 96.61 <C> [BOLD] 96.06 <C> 96.26 <C> [BOLD] 95.97 <C> [BOLD] 96.44 <C> [BOLD] 95.56 <C> [BOLD] 97.08 <C> [BOLD] 96.52 <R> <C> Transformer <C> R <C> [BOLD] 98 <C> [BOLD] 95.51 <C> [BOLD] 96.73 <C> 96.57 <C> [BOLD] 95.35 <C> [BOLD] 96.2 <C> [BOLD] 96.59 <C> [BOLD] 97.09 <C> [BOLD] 96.51 <R> <C> Transformer <C> F <C> [BOLD] 98.07 <C> [BOLD] 96.26 <C> [BOLD] 96.39 <C> 96.43 <C> [BOLD] 95.66 <C> [BOLD] 96.32 <C> [BOLD] 95.57 <C> [BOLD] 97.08 <C> [BOLD] 96.47 <R> <C> Transformer <C> OOV <C> [BOLD] 73.75 <C> 73.05 <C> [BOLD] 72.82 <C> [BOLD] 82.82 <C> [BOLD] 79.05 <C> [BOLD] 83.72 <C> [BOLD] 71.81 <C> [BOLD] 77.95 <C> [BOLD] 76.87 <R> <C> [BOLD] Multi-Criteria Learning <C> [BOLD] Multi-Criteria Learning <C> [BOLD] Multi-Criteria Learning <C> [BOLD] Multi-Criteria Learning <C> [BOLD] Multi-Criteria Learning <C> [BOLD] Multi-Criteria Learning <C> [BOLD] Multi-Criteria Learning <C> [BOLD] Multi-Criteria Learning <C> [BOLD] Multi-Criteria Learning <C> [BOLD] Multi-Criteria Learning <C> [BOLD] Multi-Criteria Learning <R> <C> Models <C> [EMPTY] <C> MSRA <C> AS <C> PKU <C> CTB <C> CKIP <C> CITYU <C> NCC <C> SXU <C> Avg. <R> <C> MTL chen2017adversarial <C> P <C> 95.95 <C> 94.17 <C> 94.86 <C> 96.02 <C> 93.82 <C> 95.39 <C> 92.46 <C> 96.07 <C> 94.84 <R> <C> MTL chen2017adversarial <C> R <C> 96.14 <C> 95.11 <C> 93.78 <C> 96.33 <C> 94.7 <C> 95.7 <C> 93.19 <C> 96.01 <C> 95.12 <R> <C> MTL chen2017adversarial <C> F <C> 96.04 <C> 94.64 <C> 94.32 <C> 96.18 <C> 94.26 <C> 95.55 <C> 92.83 <C> 96.04 <C> 94.98 <R> <C> MTL chen2017adversarial <C> OOV <C> 71.6 <C> 73.5 <C> 72.67 <C> 82.48 <C> 77.59 <C> 81.4 <C> 63.31 <C> 77.1 <C> 74.96 <R> <C> Switch-LSTMs gong2018switch <C> P <C> 97.69 <C> 94.42 <C> [BOLD] 96.24 <C> [BOLD] 97.09 <C> 94.53 <C> 95.85 <C> 94.07 <C> 96.88 <C> 95.85 <R> <C> Switch-LSTMs gong2018switch <C> R <C> 97.87 <C> 96.03 <C> 96.05 <C> [BOLD] 97.43 <C> 95.45 <C> 96.59 <C> 94.17 <C> 97.62 <C> 96.4 <R> <C> Switch-LSTMs gong2018switch <C> F <C> 97.78 <C> 95.22 <C> 96.15 <C> [BOLD] 97.26 <C> 94.99 <C> 96.22 <C> 94.12 <C> 97.25 <C> 96.12 <R> <C> Switch-LSTMs gong2018switch <C> OOV <C> 64.2 <C> 77.33 <C> 69.88 <C> 83.89 <C> 77.69 <C> 73.58 <C> 69.76 <C> 78.69 <C> 74.38 <R> <C> Transformer <C> P <C> [BOLD] 98.03 <C> [BOLD] 96.84 <C> 95.88 <C> 96.79 <C> [BOLD] 96.92 <C> [BOLD] 97.03 <C> [BOLD] 95.85 <C> [BOLD] 97.52 <C> [BOLD] 96.86 <R> <C> Transformer <C> R <C> [BOLD] 98.06 <C> [BOLD] 96.05 <C> [BOLD] 96.95 <C> 97.18 <C> [BOLD] 96.11 <C> [BOLD] 96.78 <C> [BOLD] 96.24 <C> [BOLD] 97.69 <C> [BOLD] 96.88 <R> <C> Transformer <C> F <C> [BOLD] 98.05 <C> [BOLD] 96.44 <C> [BOLD] 96.41 <C> 96.99 <C> [BOLD] 96.51 <C> [BOLD] 96.91 <C> [BOLD] 96.04 <C> [BOLD] 97.61 <C> [BOLD] 96.87 <R> <C> Transformer <C> OOV <C> [BOLD] 78.92 <C> [BOLD] 76.39 <C> [BOLD] 78.91 <C> [BOLD] 87 <C> [BOLD] 82.89 <C> [BOLD] 86.91 <C> [BOLD] 79.3 <C> [BOLD] 85.08 <C> [BOLD] 81.92 <CAP> Table 5: Results of the proposed model on the test sets of eight CWS datasets. Here, P, R, F, OOV indicate the precision, recall, F1 value, and OOV recall rate respectively. The maximum F1 value and OOV value are highlighted for each dataset. There are two blocks. The upper block consists of single-criterion learning models. Bi-LSTMs and stack-LSTMS are baselines and the results on them are reported in chen2017adversarial. The lower block consists of multi-criteria learning models.
<R> <C> Models <C> MSRA <C> AS <C> PKU <C> CTB <C> CKIP <C> CITYU <C> NCC <C> SXU <C> Avg. F1 <R> <C> 8Simp <C> [BOLD] 98.05 <C> 96.44 <C> 96.41 <C> [BOLD] 96.99 <C> 96.51 <C> [BOLD] 96.91 <C> 96.04 <C> [BOLD] 97.61 <C> [BOLD] 96.87 <R> <C> 8Trad <C> 97.98 <C> 96.39 <C> 96.49 <C> [BOLD] 96.99 <C> 96.49 <C> 96.86 <C> 95.98 <C> 97.48 <C> 96.83 <R> <C> 5Simp, 3Trad <C> 98.03 <C> [BOLD] 96.52 <C> [BOLD] 96.6 <C> 96.94 <C> 96.38 <C> 96.8 <C> 96.02 <C> 97.55 <C> 96.86 <R> <C> 8 Simp, 8 Trad <C> 98.04 <C> 96.41 <C> 96.43 <C> [BOLD] 96.99 <C> [BOLD] 96.54 <C> 96.85 <C> [BOLD] 96.08 <C> 97.52 <C> 96.86 <CAP> Table 7: This table presents the results of training simplified Chinese corpus and traditional Chinese corpus together. “8Simp”, “8Trad” means all corpus are converted into simplified Chinese or traditional Chinese respectively. “5Simp,3Trad” means 5 datasets are in simplified Chinese and 3 datasets(including AS, CITYU and CKIP, these datasets are given as traditional Chinese.) are in traditional Chinese.
<R> <C> [BOLD] Name <C> [BOLD] Activation function <C> [BOLD] #Parameters <C> [BOLD] Valid perplexity <C> [BOLD] Test perplexity <R> <C> LSTM - medium (Zaremba et al.,  2014 ) <C> tanh <C> 20M <C> 86.2 <C> 82.7 <R> <C> QRNN (Bradbury et al.,  2017 ) <C> tanh <C> 18M <C> 82.9 <C> 79.9 <R> <C> LSTM - medium (our implementation) <C> tanh <C> 20M <C> 85.8 <C> 82.6 <R> <C> QRNN (our implementation) <C> tanh <C> 18M <C> 84.9 <C> 80.0 <R> <C> QRNN (our implementation) <C> ReLU <C> 18M <C> 89.5 <C> 85.3 <R> <C> QRNN (our implementation) <C> DReLU <C> 20M <C> 82.6 <C> [BOLD] 78.4 <R> <C> QRNN (our implementation) <C> DELU <C> 20M <C> 83.1 <C> 78.5 <CAP> Table 2: Evaluation of different activation functions on the task of word-level language modeling of the PTB corpus.
<R> <C> [BOLD] Model name <C> [BOLD] Activation function <C> [BOLD] # Layers <C> [BOLD] Hidden state size <C> [BOLD] Test BPC <C> [BOLD] # Params <R> <C> HF-MRNN (Mikolov et al.,  2012 ) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 1.41 <C> [EMPTY] <R> <C> BatchNorm LSTM (Cooijmans et al.,  2017 ) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 1.32 <C> [EMPTY] <R> <C> LayerNorm HM-LSTM (Chung et al.,  2017 ) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 1.24 <C> [EMPTY] <R> <C> LSTM (Ha et al.,  2017 ) <C> tanh <C> 2 <C> 1000 <C> 1.28 <C> 12.26M <R> <C> Layer Norm HyperLSTM (Ha et al.,  2017 ) <C> tanh <C> 2 <C> 1000 <C> 1.22 <C> 14.41M <R> <C> QRNN (our implementation) <C> tanh <C> 4 <C> 581 <C> 1.26 <C> 6.66M <R> <C> QRNN (our implementation) <C> tanh <C> 8 <C> 579 <C> 1.22 <C> 14.69M <R> <C> QRNN (our implementation) <C> DELU <C> 4 <C> 500 <C> 1.25 <C> 6.66M <R> <C> QRNN (our implementation) <C> DELU <C> 8 <C> 500 <C> [BOLD] 1.21 <C> 14.69M <R> <C> QRNN (our implementation) <C> DReLU <C> 2 <C> 250 <C> 1.38 <C> 0.82M <R> <C> QRNN (our implementation) <C> DReLU <C> 4 <C> 250 <C> 1.30 <C> 1.83M <R> <C> QRNN (our implementation) <C> DReLU <C> 4 <C> 500 <C> 1.25 <C> 6.66M <R> <C> QRNN (our implementation) <C> DReLU <C> 8 <C> 250 <C> 1.25 <C> 3.85M <R> <C> QRNN (our implementation) <C> DReLU <C> 8 <C> 500 <C> [BOLD] 1.21 <C> 14.69M <CAP> Table 4: Comparison of DReLUs and DELUs with other neural network architectures on the Penn Treebank test set.
<R> <C> [BOLD] Lex Unit <C> [BOLD] Model <C> [BOLD] aze  [BOLD] bi <C> [BOLD] aze  [BOLD] all <C> [BOLD] bel  [BOLD] bi <C> [BOLD] bel  [BOLD] all <R> <C> Sub-sep <C> Lookup <C> 11.25 <C> 8.10 <C> 16.53 <C> 15.16 <R> <C> Word <C> SDE <C> 12.25 <C> 12.09 <C> 19.08 <C> 19.69 <CAP> Table 6: BLEU scores for training with all four high-resource languages.
<R> <C> [EMPTY] <C> F-1 scores (ted+ted) lis <C> F-1 scores (ted+ted) Fixed <C> F-1 scores (ted+gv) lis <C> F-1 scores (ted+gv) Fixed <C> cnpmi lis <C> cnpmi Fixed <R> <C> wiki-paco <C> 0.627 <C> 0.638 <C> 0.551 <C> 0.534 <C> 0.256 <C> 0.258 <R> <C> wiki-inco <C> 0.551 <C> 0.526 <C> 0.475 <C> 0.470 <C> 0.220 <C> 0.217 <CAP> (a) Training softlink model.
<R> <C> [EMPTY] <C> F-1 scores (ted+ted) lis <C> F-1 scores (ted+ted) Fixed <C> F-1 scores (ted+gv) lis <C> F-1 scores (ted+gv) Fixed <C> cnpmi lis <C> cnpmi Fixed <R> <C> wiki-paco <C> 0.640 <C> 0.647 <C> 0.557 <C> 0.543 <C> 0.261 <C> 0.266 <R> <C> wiki-inco <C> 0.546 <C> 0.517 <C> 0.459 <C> 0.465 <C> 0.242 <C> 0.233 <CAP> (b) Training softlink with voclink.
<R> <C> [BOLD] Methods <C> [BOLD] Accuracy <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1 <R> <C> rule-based approach <C> 62.1 <C> [BOLD] 93.7 <C> 59.3 <C> 72.6 <R> <C> Bert-based approach <C> [BOLD] 85.1 <C> 90.6 <C> [BOLD] 77.6 <C> [BOLD] 83.6 <CAP> Table 6: Causality extraction performance.
<R> <C> Language pair <C> DE-EN <C> DE-EN BPE <R> <C> Baseline <C> 22.37 <C> 23.20 <R> <C> PN Copy <C> 23.01 <C> 23.20 <R> <C> PG Copy <C> 23.07 <C> 22.86 <R> <C> LexPN <C> 23.08 <C> 23.20 <R> <C> LexPG <C> 23.02 <C> 23.57 <R> <C> LexPG+S <C> [BOLD] 24.72 <C> 23.68 <R> <C> LexPG+F <C> 24.38 <C> 24.16 <R> <C> LexPG+S+F <C> 24.57 <C> [BOLD] 24.67 <CAP> Table 3: BLEU scores on the IWSLT2017 German-English dataset.
<R> <C> Pass <C> Language Model <C> Size <C> PPL <C> WER (%) <R> <C> 1st <C> 15M 3-gram <C> — <C> 191 <C> 18.7 <R> <C> 1st <C> 1.6B 5-gram <C> LARGE, pruned <C> 112 <C> 16.9 <R> <C> 2nd <C> 15M 3-gram <C> — <C> 191 <C> 18.8 <R> <C> 2nd <C> 1.6B 5-gram <C> LARGE, pruned <C> 112 <C> 16.9 <R> <C> 2nd <C> 12.7B 5-gram <C> LARGE <C> 108 <C> 16.8 <CAP> TABLE I: Speech recognition language model performance when used in the 1-st pass or in the 2-nd pass—lattice rescoring.
<R> <C> Model <C> MPQA Include-∅ <C> MPQA Exclude-∅ <C> GFBF Include-∅ <C> GFBF Exclude-∅ <R> <C> AttnCNN <C> 20.3 <C> 38.7 <C> 34.5 <C> 53.6 <R> <C> TreeLSTM <C> 29.9 <C> 56.1 <C> 46.4 <C> 70.6 <R> <C> SDP <C> 34.2 <C> 60.7 <C> 60.1 <C> 81.0 <R> <C> AttnLSTM <C> 32.7 <C> 62.4 <C> 60.4 <C> 80.4 <R> <C> Pred-rationales <C> 34.9 <C> 63.0 <C> 61.4 <C> 85.1 <R> <C> Trained-attn <C> [BOLD] 37.6(+4.9) <C> [BOLD] 68.7(+6.3) <C> [BOLD] 65.4(+5.0) <C> [BOLD] 88.7(+8.3) <CAP> Table 2: F-score (for Include-∅ columns) and accuracy (for Exclude-∅ columns) for various baselines and our method. AttnCNN is the CNN model by Wang et al. (2016); TreeLSTM is a tree model with LSTM units by Tai et al. (2015); SDP is the Shortest Dependency Path model by Xu et al. (2015); AttnLSTM is our baseline model by Zhang et al. (2017). Pred-rationales is a popular multi-task learning method for leveraging rationales, and trained-attn is our novel method for training the attention. Both of them are applied to AttnLSTM, and are described in Section 4.2. Improvements of Trained-attn over non-trained attention, shown in parentheses, are statistically significant with p<0.002. Likewise, improvements over Pred-rationales have p<0.02.
<R> <C> Plausibility <C> Probes-needed <C> Mass-needed <R> <C> Non-trained <C> 8.00 <C> 0.80 <R> <C> 100 rationales <C> 1.73 <C> 0.128 <R> <C> Fully trained <C> [BOLD] 1.55 <C> [BOLD] 0.111 <R> <C> Faithfulness <C> Probes-needed <C> Mass-needed <R> <C> Non-trained <C> 8.46/8.71 <C> 0.78/0.73 <R> <C> 100 rationales <C> 2.06/3.24 <C> 0.12/0.20 <R> <C> Fully trained <C> [BOLD] 2.02/2.68 <C> [BOLD] 0.10/0.16 <CAP> Table 3: Attention metrics on models trained with varying numbers of rationales. The lower the Probes/Mass -needed, the better the attention faithfulness. We report the cases where the prediction is correct / wrong separately.
<R> <C> Data Methods <C> UDC R10@1 <C> UDC R10@2 <C> UDC R10@5 <C> UDC MAP <C> MSDialog R10@1 <C> MSDialog R10@2 <C> MSDialog R10@5 <C> MSDialog MAP <C> AliMe R10@1 <C> AliMe R10@2 <C> AliMe R10@5 <C> AliMe MAP <R> <C> BM25 (Robertson:1994:SEA:188490.188561) <C> 0.5138 <C> 0.6439 <C> 0.8206 <C> 0.6504 <C> 0.2626 <C> 0.3933 <C> 0.6329 <C> 0.4387 <C> 0.2371 <C> 0.4204 <C> 0.6407 <C> 0.6392 <R> <C> BM25-PRF (DBLP:conf/sigir/YangQQGZCHC18) <C> 0.5289 <C> 0.6554 <C> 0.8292 <C> 0.6620 <C> 0.2652 <C> 0.3970 <C> 0.6423 <C> 0.4419 <C> 0.2454 <C> 0.4209 <C> 0.6510 <C> 0.6412 <R> <C> MV-LSTM (DBLP:conf/aaai/WanLGXPC16) <C> 0.4973 <C> 0.6733 <C> 0.8936 <C> 0.6611 <C> 0.2768 <C> 0.5000 <C> 0.8516 <C> 0.5059 <C> 0.2480 <C> 0.4105 <C> 0.7017 <C> 0.7734 <R> <C> DRMM (Guo:2016:DRM:2983323.2983769) <C> 0.5287 <C> 0.6773 <C> 0.8776 <C> 0.6749 <C> 0.3507 <C> 0.5854 <C> 0.9003 <C> 0.5704 <C> 0.2212 <C> 0.3616 <C> 0.6575 <C> 0.7165 <R> <C> Duet (Mitra:2017:LMU:3038912.3052579) <C> 0.4756 <C> 0.5592 <C> 0.8272 <C> 0.5692 <C> 0.2934 <C> 0.5046 <C> 0.8481 <C> 0.5158 <C> 0.2433 <C> 0.4088 <C> 0.6870 <C> 0.7651 <R> <C> DMN-KD (DBLP:conf/sigir/YangQQGZCHC18) <C> 0.6443 <C> 0.7841 <C> 0.9351 <C> 0.7655 <C> 0.4908 <C> 0.7089 <C> 0.9304 <C> 0.6728 <C> 0.3596 <C> 0.5122 <C> 0.7631 <C> 0.8323 <R> <C> DMN-PRF (DBLP:conf/sigir/YangQQGZCHC18) <C> 0.6552 <C> 0.7893 <C> 0.9343 <C> 0.7719 <C> 0.5021 <C> 0.7122 <C> 0.9356 <C> 0.6792 <C> 0.3601 <C> 0.5323 <C> 0.7701 <C> 0.8435 <R> <C> DAM (DBLP:conf/acl/WuLCZDYZL18) <C> 0.7686 <C> 0.8739 <C> 0.9697 <C> 0.8527 <C> 0.7012 <C> 0.8527 <C> 0.9715 <C> 0.8150 <C> 0.3819 <C> 0.5567 <C> 0.7717 <C> 0.8452 <R> <C> IART [ITALIC] Dot <C> [BOLD] 0.7703 <C> [BOLD] 0.8746 <C> 0.9688 <C> [BOLD] 0.8535 <C> [BOLD] 0.7234‡ <C> [BOLD] 0.8650‡ <C> [BOLD] 0.9772‡ <C> [BOLD] 0.8300‡ <C> [BOLD] 0.3821 <C> 0.5547 <C> [BOLD] 0.7802† <C> [BOLD] 0.8454 <R> <C> IART [ITALIC] Outerproduct <C> [BOLD] 0.7717‡ <C> [BOLD] 0.8766‡ <C> 0.9691 <C> [BOLD] 0.8548‡ <C> [BOLD] 0.7212‡ <C> [BOLD] 0.8664‡ <C> [BOLD] 0.9749 <C> [BOLD] 0.8289‡ <C> [BOLD] 0.3901‡ <C> [BOLD] 0.5649‡ <C> [BOLD] 0.7812† <C> [BOLD] 0.8493† <R> <C> IART [ITALIC] Bilinear <C> [BOLD] 0.7713‡ <C> [BOLD] 0.8747 <C> 0.9688 <C> [BOLD] 0.8542† <C> [BOLD] 0.7317‡ <C> [BOLD] 0.8752‡ <C> [BOLD] 0.9792‡ <C> [BOLD] 0.8364‡ <C> [BOLD] 0.3892† <C> [BOLD] 0.5592† <C> [BOLD] 0.7801† <C> [BOLD] 0.8471 <CAP> Table 4. Comparison of different models over Ubuntu Dialog Corpus (UDC), MSDialog, and AliMe data sets. Numbers in bold font mean the result is better compared with the best baseline DAM. † and ‡ means statistically significant difference over the best baseline DAM with p<0.1 and p<0.05 measured by the Student’s paired t-test respectively.
<R> <C> Model Decoding <C> Bidirectional Greedy <C> Bidirectional Beam <C> Forward-only Beam <R> <C> [EMPTY] <C> No LM <C> + LM <C> + LM <R> <C> CTC <C> 15.73 <C> 10.08 <C> 13.78 <R> <C> RNN-Transducer <C> 15.29 <C> 14.05 <C> 22.38 <R> <C> Attention <C> 14.99 <C> 14.07 <C> 19.19 <CAP> Table 5: WER of baseline models on WSJ eval’92 set. On smaller datasets, RNN-Transducers and Attention models do not have enough data to learn a good implicit language model and therefore perform poorer compared to CTC even after rescoring with an external LM (RNN-Transducers and Attention models learn a better implicit language model at scale, as shown in Tables 1 and 3).
<R> <C> Top k <C> 1 <C> 2 <C> 3 <C> 4 <C> 5 <R> <C> GloVe w\ POLAR <C> 0.876 <C> 0.667 <C> 0.420 <C> 0.222 <C> 0.086 <R> <C> Random chance <C> 0.5 <C> 0.22 <C> 0.083 <C> 0.023 <C> 0.005 <CAP> Table 4. Evaluation of the interpretability. We report the conditional probability of the top k dimensions as selected by the annotators to be among the ones selected by POLAR. We also report random chance probabilities for the selected dimension to be among the POLAR dimensions for different values of k. The probabilities for POLAR are significantly higher than the random chance probabilities indicating alignment with human judgement.
<R> <C> [BOLD] feature / corpus <C> [BOLD] EUR <C> [BOLD] HAN <C> [BOLD] LIT <C> [BOLD] TED <R> <C> FW <C> 96.3 <C> 98.1 <C> 97.3 <C> 97.7 <R> <C> char-trigrams <C> 98.8 <C> 97.1 <C> 99.5 <C> 100.0 <R> <C> POS-trigrams <C> 98.5 <C> 97.2 <C> 98.7 <C> 92.0 <R> <C> contextual FW <C> 95.2 <C> 96.8 <C> 94.1 <C> 86.3 <R> <C> cohesive markers <C> 83.6 <C> 86.9 <C> 78.6 <C> 81.8 <CAP> Table 2: In-domain (cross-validation) classification accuracy using various feature sets
<R> <C> [BOLD] method / corpus <C> [BOLD] EUR + HAN <C> [BOLD] EUR + LIT <C> [BOLD] HAN + LIT <C> [BOLD] EUR + HAN + LIT <R> <C> [BOLD] KMeans <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> accuracy by domain <C> 93.7 <C> 99.5 <C> 99.8 <C> 92.2 <R> <C> accuracy by translation status <C> 50.3 <C> 50.0 <C> 50.0 <C> – <R> <C> [BOLD] XMeans <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> generated # of clusters <C> 2 <C> 2 <C> 3 <C> 3 <R> <C> accuracy by domain <C> 93.6 <C> 99.5 <C> 99.9 <C> 92.2 <R> <C> accuracy by translation status <C> 50.3 <C> 50.0 <C> – <C> – <CAP> Table 7: Clustering a chunk-level mix of Europarl, Hansard and Literature using function words; accuracy by translation status (O vs. T) is reported where applicable (i.e., the outcome constitutes two clusters)
<R> <C> [BOLD] Author <C> [BOLD] Algorithm <C> [BOLD] # Vectors <C> [BOLD] Dimensions <R> <C> bojanowski2017enriching <C> FastText <C> 985,667 <C> 300 <R> <C> jose_canete_2019_3255001 <C> FastText <C> 1,313,423 <C> 300 <R> <C> cardellinoSBWCE <C> word2vec <C> 1,000,653 <C> 300 <R> <C> grave2018learning <C> FastText <C> 2,000,001 <C> 300 <R> <C> honnibal2017spacy <C> word2vec <C> 534,000 <C> 50 <R> <C> perez_fasttext <C> FastText <C> 855,380 <C> 300 <R> <C> perez_glove <C> GloVe <C> 855,380 <C> 300 <CAP> Table 3: Types of embeddings tried.
<R> <C> [BOLD] Features <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1 score <C> [BOLD] F1 change <R> <C> All features <C> 97.84 <C> [BOLD] 82.65 <C> [BOLD] 89.60 <C> [EMPTY] <R> <C> − Bias <C> 96.76 <C> 81.74 <C> 88.61 <C> −0.99 <R> <C> − Token <C> 95.16 <C> 80.82 <C> 87.41 <C> −2.19 <R> <C> − Uppercase <C> 97.30 <C> 82.19 <C> 89.11 <C> −0.49 <R> <C> − Titlecase <C> 96.79 <C> [BOLD] 82.65 <C> 89.16 <C> −0.44 <R> <C> − Char trigram <C> 96.05 <C> 77.63 <C> 85.86 <C> −3.74 <R> <C> − Quotation <C> 97.31 <C> [BOLD] 82.65 <C> 89.38 <C> −0.22 <R> <C> − Suffix <C> 97.30 <C> 82.19 <C> 89.11 <C> −0.49 <R> <C> − POS tag <C> [BOLD] 98.35 <C> 81.74 <C> 89.28 <C> −0.32 <R> <C> − Word shape <C> 96.79 <C> [BOLD] 82.65 <C> 89.16 <C> −0.44 <R> <C> − Word embedding <C> 95.68 <C> 80.82 <C> 87.62 <C> −1.98 <CAP> Table 4: Ablation study results on the development test.
<R> <C> [BOLD] Set <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1 score <R> <C> Development set (− OTHER) <C> 97.84 <C> 82.65 <C> 89.60 <R> <C> Development set (+ OTHER) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> ENG <C> 96.79 <C> 82.65 <C> 89.16 <R> <C> OTHER <C> 100.0 <C> 28.57 <C> 44.44 <R> <C> BORROWING <C> 96.86 <C> 79.40 <C> 87.26 <R> <C> Test set (− OTHER) <C> 95.05 <C> 81.60 <C> 87.82 <R> <C> Test set (+ OTHER) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> ENG <C> 95.03 <C> 81.13 <C> 87.53 <R> <C> OTHER <C> 100.0 <C> 46.15 <C> 63.16 <R> <C> BORROWING <C> 95.19 <C> 79.11 <C> 86.41 <R> <C> Supplemental test set (− OTHER) <C> 83.16 <C> 62.70 <C> 71.49 <R> <C> Supplemental test set (+ OTHER) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> ENG <C> 82.65 <C> 64.29 <C> 72.32 <R> <C> OTHER <C> 100.0 <C> 20.0 <C> 33.33 <R> <C> BORROWING <C> 87.62 <C> 57.14 <C> 69.17 <CAP> Table 5: Results on test set and supplemental test set.
<R> <C> [BOLD] Dataset <C> [BOLD] Coh <C> [BOLD] D-1% <C> [BOLD] D-2% <C> [BOLD] D-Sent% <R> <C> OST <C> 0.390 <C> 14.3 <C> 57.9 <C> 83.8 <R> <C> fOST <C> 0.801 <C> 15.5 <C> 62.9 <C> 89.3 <CAP> Table 1: Coherence and diversity metrics777Note that Distinct-1 and Distinct-2 are computed on a randomly selected subsets of 4k responses. for the OST and fOST datasets (see Section 3 for the datasets and Section 4.2 for metrics definition).
<R> <C> Task Rel. category <C> Predicting head 1-to-1 <C> Predicting head 1-to-M. <C> Predicting head M.-to-1 <C> Predicting head M.-to-M. <C> Predicting tail 1-to-1 <C> Predicting tail 1-to-M. <C> Predicting tail M.-to-1 <C> Predicting tail M.-to-M. <R> <C> TransE -soft <C> 76.2 <C> 93.6 <C> 47.5 <C> 70.2 <C> 76.7 <C> 50.9 <C> 93.1 <C> 72.9 <R> <C> TransH <C> 66.8 <C> 87.6 <C> 28.7 <C> 64.5 <C> 65.5 <C> 39.8 <C> 83.3 <C> 67.2 <R> <C> TransR <C> 78.8 <C> 89.2 <C> 34.1 <C> 69.2 <C> 79.2 <C> 37.4 <C> 90.4 <C> 72.1 <R> <C> cTransR <C> 81.5 <C> 89 <C> 34.7 <C> 71.2 <C> 80.8 <C> 38.6 <C> 90.1 <C> 73.8 <R> <C> Bigrams -soft <C> 76.2 <C> 90.3 <C> 37.4 <C> 70.1 <C> 75.9 <C> 44.4 <C> 89.8 <C> 72.8 <R> <C> Trigram -soft <C> 56.4 <C> 79.6 <C> 30.2 <C> 57 <C> 53.1 <C> 28.8 <C> 81.6 <C> 60.8 <R> <C> Tatec-ft -soft <C> 79.3 <C> 93.2 <C> 42.3 <C> 77.2 <C> 78.5 <C> 51.5 <C> 92.7 <C> 80.7 <CAP> Table 5: Detailed results by category of relationship. We compare our Bigrams, Trigram and Tatec models in terms of Hits@10 (in %) on FB15k in the filtered setting against other models of the literature. (M. stands for Many).
<R> <C> Method <C> Dev <C> Test <R> <C>  <C> 48.6 <C> 48.0 <R> <C> BiDAF <C> 65.4 <C> 62.3 <R> <C> PCNet <C> 50.7 <C> 49.4 <R> <C> + NELL <C> 54.0 <C> 52.9 <R> <C> + Reverb <C> 54.1 <C> 52.6 <R> <C> + Probase <C> 54.8 <C> [BOLD] 53.5 <R> <C> + Freebase <C> 54.7 <C> 53.1 <R> <C> KVMemNet <C> 63.4 <C> 63.6 <R> <C> + NELL <C> 64.8 <C> 63.8 <R> <C> + Reverb <C> 64.4 <C> [BOLD] 64.3 <R> <C> + Probase <C> 64.0 <C> 63.7 <R> <C> + Freebase <C> 64.1 <C> 63.7 <CAP> Table 3: Performances (P@1) of different approaches on dev and test sets.
<R> <C> Method <C> BLEU <C> P@1 <R> <C> Seq2Seq <C> 11.2 <C> 41.3 <R> <C> QGNet <C> 16.3 <C> 41.5 <R> <C> + NELL <C> 16.5 <C> 42.2 <R> <C> + Reverb <C> 16.2 <C> 42.1 <R> <C> + Probase <C> 16.5 <C> 41.8 <R> <C> + Freebase <C> 16.1 <C> 41.3 <R> <C> QGNet + PCNet <C> 16.3 <C> 50.9 <R> <C> QGNet + KVMemNet <C> 16.3 <C> 64.3 <CAP> Table 4: Performances of different question generation based systems on the test set.
<R> <C> System <C> Max Commit. Latency <C> Max Word Latency <R> <C> Baseline-1 <C> 93.4 <C> 230 <R> <C> Baseline-2 <C> 2.7 <C> 145 <R> <C> Portion <C> 21.8 <C> 23.0 <R> <C> Update <C> 2.7 <C> 9.0 <R> <C> Update-NA <C> 19.3 <C> 23.5 <CAP> Table 3: Peak latency.
<R> <C> [BOLD] Feature Set <C> [BOLD] NLL <C> [BOLD] RMSE <C> [BOLD] Corr. <R> <C> No Features <C> 2.72 <C> 4.45 <C> 0.0 <R> <C> Weather <C> 2.71 <C> 4.41 <C> 12.1 <R> <C> Time <C> 2.71 <C> 4.41 <C> 16.3 <R> <C> Time + Weather <C> 2.70 <C> 4.40 <C> 18.0 <R> <C> All Onset Features <C> 2.66 <C> 4.25 <C> 32.0 <R> <C> Cause Only <C> 2.60 <C> 4.15 <C> 36.9 <R> <C> Cause + Onset Features <C> 2.57 <C> 4.02 <C> 45.1 <CAP> TABLE II: Performance of initial outage duration predictions with different feature sets, as measured using negative log likelihood (NLL), root mean squared error in hours (RMSE), and Pearson’s correlation.
<R> <C> [BOLD] Model <C> [BOLD] All <C> [BOLD]  Coarse-Grained  [BOLD] LS <C> [BOLD]  Coarse-Grained  [BOLD] PAS <C> [BOLD]  Coarse-Grained  [BOLD] L <C> [BOLD]  Coarse-Grained  [BOLD] K <C> [BOLD] Fine-Grained  [BOLD] UQuant <C> [BOLD] Fine-Grained  [BOLD] MNeg <C> [BOLD] Fine-Grained  [BOLD] 2Neg <C> [BOLD] Fine-Grained  [BOLD] Coref <C> [BOLD] Fine-Grained  [BOLD] Restr <C> [BOLD] Fine-Grained  [BOLD] Down <R> <C> Single-Task Training <C> Single-Task Training <C> Single-Task Training <C> Single-Task Training <C> Single-Task Training <C> Single-Task Training <C> Single-Task Training <C> Single-Task Training <C> Single-Task Training <C> Single-Task Training <C> Single-Task Training <C> Single-Task Training <R> <C> BiLSTM <C> 21 <C> 25 <C> 24 <C> 16 <C> 16 <C> 70 <C> 53 <C> 4 <C> 21 <C> -15 <C> [BOLD] 12 <R> <C> +ELMo <C> 20 <C> 20 <C> 21 <C> 14 <C> 17 <C> 70 <C> 20 <C> [BOLD] 42 <C> 33 <C> -26 <C> -3 <R> <C> +CoVe <C> 21 <C> 19 <C> 23 <C> 20 <C> 18 <C> 71 <C> 47 <C> -1 <C> 33 <C> -15 <C> 8 <R> <C> +Attn <C> 25 <C> 24 <C> 30 <C> 20 <C> 14 <C> 50 <C> 47 <C> 21 <C> 38 <C> -8 <C> -3 <R> <C> +Attn, ELMo <C> [BOLD] 28 <C> [BOLD] 30 <C> [BOLD] 35 <C> [BOLD] 23 <C> 14 <C> [BOLD] 85 <C> 20 <C> [BOLD] 42 <C> 33 <C> -26 <C> -3 <R> <C> +Attn, CoVe <C> 24 <C> 29 <C> 29 <C> 18 <C> 12 <C> 77 <C> 50 <C> 1 <C> 18 <C> -1 <C> [BOLD] 12 <R> <C> Multi-Task Training <C> Multi-Task Training <C> Multi-Task Training <C> Multi-Task Training <C> Multi-Task Training <C> Multi-Task Training <C> Multi-Task Training <C> Multi-Task Training <C> Multi-Task Training <C> Multi-Task Training <C> Multi-Task Training <C> Multi-Task Training <R> <C> BiLSTM <C> 20 <C> 13 <C> 24 <C> 14 <C> 22 <C> 71 <C> 17 <C> -8 <C> 31 <C> -15 <C> 8 <R> <C> +ELMo <C> 21 <C> 20 <C> 21 <C> 19 <C> 21 <C> 71 <C> [BOLD] 60 <C> 2 <C> 22 <C> 0 <C> [BOLD] 12 <R> <C> +CoVe <C> 18 <C> 15 <C> 11 <C> 18 <C> [BOLD] 27 <C> 71 <C> 40 <C> 7 <C> [BOLD] 40 <C> 0 <C> 8 <R> <C> +Attn <C> 18 <C> 13 <C> 24 <C> 11 <C> 16 <C> 71 <C> 1 <C> -12 <C> 31 <C> -15 <C> 8 <R> <C> +Attn, ELMo <C> 22 <C> 18 <C> 26 <C> 13 <C> 19 <C> 70 <C> 27 <C> 5 <C> 31 <C> -26 <C> -3 <R> <C> +Attn, CoVe <C> 18 <C> 16 <C> 25 <C> 16 <C> 13 <C> 71 <C> 26 <C> -8 <C> 33 <C> [BOLD] 9 <C> 8 <R> <C> Pre-Trained Sentence Representation Models <C> Pre-Trained Sentence Representation Models <C> Pre-Trained Sentence Representation Models <C> Pre-Trained Sentence Representation Models <C> Pre-Trained Sentence Representation Models <C> Pre-Trained Sentence Representation Models <C> Pre-Trained Sentence Representation Models <C> Pre-Trained Sentence Representation Models <C> Pre-Trained Sentence Representation Models <C> Pre-Trained Sentence Representation Models <C> Pre-Trained Sentence Representation Models <C> Pre-Trained Sentence Representation Models <R> <C> CBoW <C> 9 <C> 6 <C> 13 <C> 5 <C> 10 <C> 3 <C> 0 <C> 13 <C> 28 <C> -15 <C> -11 <R> <C> Skip-Thought <C> 12 <C> 2 <C> 23 <C> 11 <C> 9 <C> 61 <C> 6 <C> -2 <C> 30 <C> -15 <C> 0 <R> <C> InferSent <C> 18 <C> 20 <C> 20 <C> 15 <C> 14 <C> 77 <C> 50 <C> -20 <C> 15 <C> -15 <C> -9 <R> <C> DisSent <C> 16 <C> 16 <C> 19 <C> 13 <C> 15 <C> 70 <C> 43 <C> -11 <C> 20 <C> -36 <C> -09 <R> <C> GenSen <C> 20 <C> 28 <C> 26 <C> 14 <C> 12 <C> 78 <C> 57 <C> 2 <C> 21 <C> -15 <C> [BOLD] 12 <CAP> Table 5: Results on the diagnostic set. We report R3 coefficients between gold and predicted labels, scaled by 100. The coarse-grained categories are Lexical Semantics (LS), Predicate-Argument Structure (PAS), Logic (L), and Knowledge and Common Sense (K). Our example fine-grained categories are Universal Quantification (UQuant), Morphological Negation (MNeg), Double Negation (2Neg), Anaphora/Coreference (Coref), Restrictivity (Restr), and Downward Monotone (Down).
<R> <C> [BOLD] Model <C> [BOLD] Acc. <R> <C> Random <C> 3.4 <R> <C> Random-Q <C> 33.4 <R> <C> 3.0pt1-2.51.5 <C> 3.0pt1-2.51.5 <R> <C> plus1fil minus1fil <C> plus1fil minus1fil <R> <C> LF-Q <C> 40.3 <R> <C> LF-QI <C> 50.4 <R> <C> LF-QH <C> 44.1 <R> <C> LF-QIH <C> 55.9 <R> <C> 3.0pt1-2.51.5 <C> 3.0pt1-2.51.5 <R> <C> plus1fil minus1fil <C> plus1fil minus1fil <R> <C> HRE-QH <C> 45.9 <R> <C> HRE-QIH <C> 63.3 <R> <C> 3.0pt1-2.51.5 <C> 3.0pt1-2.51.5 <R> <C> plus1fil minus1fil <C> plus1fil minus1fil <R> <C> MN-QH <C> 44.2 <R> <C> MN-QIH <C> 59.6 <R> <C> 3.0pt1-2.51.5 <C> 3.0pt1-2.51.5 <R> <C> plus1fil minus1fil <C> plus1fil minus1fil <R> <C> NMN <C> 56.6 <R> <C> CorefNMN <C> [BOLD] 68.0 <CAP> Table 3: Accuracy (%) on CLEVR-Dialog (higher is better). See text for details.
<R> <C> [BOLD] DSL Architecture Description (encoder used in attentional encoder-decoder LSTM) <C> [BOLD] Multi30k Val Loss <C> [BOLD] Multi30k Test BLEU <C> [BOLD] IWSLT’16 Test BLEU <R> <C> LSTM <C> 6.05 <C> 34.05 <C> 24.39 <R> <C> BC3 <C> 5.98 <C> 34.69 <C> 23.32 <R> <C> [EMPTY] <C> 5.83 <C> 36.03 <C> 22.76 <R> <C> [ITALIC] Gate3( [ITALIC] MM( [ITALIC] xt), [ITALIC] Tanh( [ITALIC] xt), [ITALIC] Sigmoid( [ITALIC] MM( [ITALIC] ht−1))) <C> 5.82 <C> 36.20 <C> 22.99 <R> <C> 5-deep nested  [ITALIC] Gate3 with  [ITALIC] LayerNorm (see Appendix C2) <C> 5.66 <C> 36.35 <C> 22.53 <R> <C> Residual  [ITALIC] xt with positional encoding (see Appendix C2) <C> 5.65 <C> 35.63 <C> 22.48 <R> <C> [ITALIC] Gate3( [ITALIC] MM( [ITALIC] xt), [ITALIC] xt, [ITALIC] Sigmoid( [ITALIC] MM( [ITALIC] SeLU( [ITALIC] ht−1)))) <C> 5.64 <C> 36.53 <C> 23.04 <CAP> Table 3: Model loss and BLEU on the Multi30k and IWSLT’16 MT datasets. All architectures were generated on the Multi30k dataset other than the LSTM and BC3 from the LM architecture search. We did not perform any hyperparameter optimizations on either dataset to avoid unfair comparisons, though the initial OpenNMT hyperparameters likely favored the baseline LSTM model.
<R> <C> [BOLD] Series Title <C> [BOLD] Short Name <C> [BOLD] Original Release <C> [BOLD] No. of Seasons <C> [BOLD] No. of Episodes <R> <C> Star Trek: The Original Series <C> TOS <C> 1966–1969 <C> 3 <C> 79 <R> <C> Star Trek: The Animated Series <C> TAS <C> 1973–1974 <C> 2 <C> 22 <R> <C> Star Trek: The Next, Generation <C> TNG <C> 1987–1994 <C> 7 <C> 178 <R> <C> Star Trek: Deep Space Nine <C> DS9 <C> 1993–1999 <C> 7 <C> 177 <R> <C> Star Trek: Voyager <C> Voyager <C> 1995–2001 <C> 7 <C> 172 <R> <C> Star Trek: Enterprise <C> Enterprise <C> 2001–2005 <C> 4 <C> 99 <R> <C> Star Trek: Discovery <C> Discovery <C> 2017–present <C> 2 <C> 29 <R> <C> Star Trek: Shorts <C> Shorts <C> 2018–present <C> 1 <C> 4 <CAP> Table 1: The Star Trek television series overview.
<R> <C> [EMPTY] <C> Clean conditions Female <C> Clean conditions Female <C> Clean conditions Female <C> Clean conditions Female <C> Clean conditions Male <C> Clean conditions Male <C> Clean conditions Male <C> Clean conditions Male <C> Noisy conditions Female <C> Noisy conditions Female <C> Noisy conditions Female <C> Noisy conditions Female <C> Noisy conditions Male <C> Noisy conditions Male <C> Noisy conditions Male <C> Noisy conditions Male <R> <C> [EMPTY] <C> VDE <C> GPE <C> FPE <C> FFE <C> VDE <C> GPE <C> FPE <C> FFE <C> VDE <C> GPE <C> FPE <C> FFE <C> VDE <C> GPE <C> FPE <C> FFE <R> <C> Get_F0 <C> [BOLD] 3.74 <C> 2.78 <C> 2.95 <C> [BOLD] 4.92 <C> [BOLD] 5.34 <C> 1.79 <C> 3.06 <C> [BOLD] 6.11 <C> 20.8 <C> 14.8 <C> 2.4 <C> 24.9 <C> 27.7 <C> [BOLD] 2.7 <C> 2.7 <C> 28.3 <R> <C> SHRP <C> 7.01 <C> 2.03 <C> [BOLD] 2.52 <C> 7.83 <C> 10.2 <C> 2.74 <C> 3.17 <C> 11.4 <C> 27.0 <C> 11.5 <C> 1.9 <C> 29.3 <C> 30.1 <C> 6.8 <C> 2.8 <C> 31.5 <R> <C> TEMPO <C> 5.38 <C> 1.51 <C> 3.05 <C> 6.01 <C> 9.28 <C> [BOLD] 0.93 <C> 3.13 <C> 9.66 <C> 25.2 <C> 4.4 <C> 3.9 <C> 25.8 <C> 36.8 <C> 16.7 <C> 3.8 <C> 37.6 <R> <C> AC <C> 6.81 <C> 1.50 <C> 2.68 <C> 7.41 <C> 8.02 <C> 1.40 <C> [BOLD] 2.77 <C> 8.59 <C> 20.5 <C> 14.2 <C> 2.4 <C> 24.3 <C> 28.2 <C> 5.9 <C> [BOLD] 2.4 <C> 29.6 <R> <C> CC <C> 8.41 <C> 1.76 <C> 2.77 <C> 9.15 <C> 9.25 <C> 2.23 <C> 3.44 <C> 10.2 <C> 21.1 <C> 18.0 <C> 2.7 <C> 26.1 <C> 27.8 <C> 7.9 <C> 3.0 <C> 29.8 <R> <C> YIN <C> 7.29 <C> 1.88 <C> 2.95 <C> 8.06 <C> 8.34 <C> 2.47 <C> 2.93 <C> 9.38 <C> [BOLD] 15.1 <C> 19.0 <C> 3.0 <C> 21.2 <C> [BOLD] 22.1 <C> 11.9 <C> 2.8 <C> 25.2 <R> <C> SSH <C> 5.81 <C> 4.67 <C> 2.76 <C> 7.49 <C> 8.87 <C> 2.45 <C> 3.31 <C> 9.88 <C> 24.2 <C> 39.1 <C> [BOLD] 1.9 <C> 32.1 <C> 23.3 <C> 6.3 <C> 2.8 <C> 25.1 <R> <C> [BOLD] SRH <C> 7.29 <C> [BOLD] 1.29 <C> 3.10 <C> 7.81 <C> 8.34 <C> 1.95 <C> 3.46 <C> 9.15 <C> [BOLD] 15.1 <C> [BOLD] 2.7 <C> 2.6 <C> [BOLD] 16.0 <C> [BOLD] 22.1 <C> 4.0 <C> 2.7 <C> [BOLD] 23.1 <CAP> Table 1: Detailed pitch tracking results in clean and noisy conditions (averaged over all noise types at 0 dB of SNR), for both male and female speakers.
<R> <C> Method Criterion <C> StackGAN++ IS <C> StackGAN++ FID <C> HDGAN IS <C> HDGAN FID <C> AttnGAN IS <C> AttnGAN FID <C> Obj-GAN IS <C> Obj-GAN FID <C> VHE-raster-scan-GAN IS <C> VHE-raster-scan-GAN FID <R> <C> Flower <C> 3.26 <C> 48.68 <C> 3.45 <C> 40.12∗ <C> – <C> – <C> - <C> - <C> [BOLD] 3.72 <C> [BOLD] 35.13 <R> <C> CUB <C> 3.84 <C> 15.30 <C> 4.15 <C> 13.48∗ <C> 4.36 <C> 13.02∗ <C> - <C> - <C> [BOLD] 4.41 <C> [BOLD] 12.02 <R> <C> COCO <C> 8.30 <C> 81.59 <C> 11.86 <C> 78.16∗ <C> 25.89 <C> 77.01∗ <C> 26.58∗ <C> [BOLD] 36.98∗ <C> [BOLD] 27.16 <C> 75.88 <CAP> Table 2: Ablation study for image-to-text learning, where the structures of different variations of raster-scan-GAN are illustrated in Figs. 1, 1, and 1.
<R> <C> Method Criterion <C> PGBN+StackGAN++ IS <C> PGBN+StackGAN++ FID <C> VHE-vanilla-GAN IS <C> VHE-vanilla-GAN FID <C> VHE-StackGAN++ IS <C> VHE-StackGAN++ FID <C> VHE-simple-raster-scan-GAN IS <C> VHE-simple-raster-scan-GAN FID <R> <C> Flower <C> 3.29 <C> 41.04 <C> 3.01 <C> 52.15 <C> 3.56 <C> 38.66 <C> 3.62 <C> 36.18 <R> <C> CUB <C> 3.92 <C> 13.79 <C> 3.52 <C> 21.24 <C> 4.20 <C> 12.93 <C> 4.31 <C> 12.35 <R> <C> COCO <C> 10.63 <C> 79.65 <C> 6.36 <C> 97.15 <C> 12.63 <C> 78.02 <C> 20.13 <C> 77.18 <CAP> Table 2: Ablation study for image-to-text learning, where the structures of different variations of raster-scan-GAN are illustrated in Figs. 1, 1, and 1.
<R> <C> Method <C> CNN-LSTM <C> AttnGAN <C> TA-GAN <C> VHE-StackGAN++ <C> VHE-raster-scan-GAN <R> <C> [EMPTY] <C> (Li et al.,  2017 ) <C> (Xu et al.,  2018 ) <C> (Nam et al.,  2018 ) <C> [EMPTY] <C> [EMPTY] <R> <C> Top1-ACC(%) <C> 61.5 <C> 55.1 <C> 61.3 <C> 60.2 <C> [BOLD] 61.7 <R> <C> AP@50(%) <C> 57.6 <C> 51.0 <C> [BOLD] 62.8 <C> 61.3 <C> 62.6 <CAP> Table 4: Comparison of the image-to-text retrieval performance, measured by Top-1 accuracy, and text-to-image retrieval performance, measured by AP@50, between different methods on CUB-E.
<R> <C> Encoder <C> [BOLD] Macro  [ITALIC] F1 across healthcare opertations cvs_aet <C> [BOLD] Macro  [ITALIC] F1 across healthcare opertations ci_esrx <C> [BOLD] Macro  [ITALIC] F1 across healthcare opertations antm_ci <C> [BOLD] Macro  [ITALIC] F1 across healthcare opertations aet_hum <C> [ITALIC] avgF1 <C> [ITALIC] avgwF1 <C> [BOLD] Average per-class accuracy  [ITALIC] sup <C> [BOLD] Average per-class accuracy  [ITALIC] ref <C> [BOLD] Average per-class accuracy  [ITALIC] com <C> [BOLD] Average per-class accuracy  [ITALIC] unr <R> <C> SVM <C> 51.0 <C> 51.0 <C> 65.7 <C> 65.0 <C> 58.1 <C> 58.5 <C> 54.5 <C> 43.9 <C> 41.2 <C> [BOLD] 88.4 <R> <C> MLP <C> 46.5 <C> 46.6 <C> 57.6 <C> 59.7 <C> 52.6 <C> 52.7 <C> 55.7 <C> 40.3 <C> 48.6 <C> 68.1 <R> <C> EmbAvg <C> 50.4 <C> 51.9 <C> 50.4 <C> 58.9 <C> 52.9 <C> 52.3 <C> 55.2 <C> 50.5 <C> 52.7 <C> 67.4 <R> <C> CharCNN <C> 49.6 <C> 48.3 <C> 65.6 <C> 60.9 <C> 56.1 <C> 56.8 <C> 55.5 <C> 44.2 <C> 41.6 <C> 82.1 <R> <C> WordCNN <C> 46.3 <C> 39.5 <C> 56.8 <C> 59.4 <C> 50.5 <C> 51.7 <C> 62.9 <C> 37.0 <C> 31.0 <C> 71.7 <R> <C> BiCE <C> 56.5 <C> 52.5 <C> 64.9 <C> 63.0 <C> 59.2 <C> 60.1 <C> 61.0 <C> 48.7 <C> 45.1 <C> 79.9 <R> <C> CrossNet <C> [BOLD] 59.1 <C> 54.5 <C> 65.1 <C> 62.3 <C> 60.2 <C> 61.1 <C> 63.8 <C> 48.9 <C> 50.5 <C> 75.8 <R> <C> SiamNet <C> 58.3 <C> 54.4 <C> [BOLD] 68.7 <C> [BOLD] 67.7 <C> [BOLD] 62.2 <C> [BOLD] 63.1 <C> 67.0 <C> 48.0 <C> 52.5 <C> 78.3 <R> <C> CoMatchAtt <C> 54.7 <C> 43.8 <C> 50.8 <C> 50.6 <C> 49.9 <C> 51.6 <C> [BOLD] 71.9 <C> 24.4 <C> 33.7 <C> 65.9 <R> <C> TAN <C> 56.0 <C> 55.9 <C> 66.2 <C> 66.7 <C> 61.2 <C> 61.3 <C> 66.1 <C> 49.0 <C> 51.7 <C> 74.1 <R> <C> HAN <C> 56.4 <C> [BOLD] 57.3 <C> 66.0 <C> 67.3 <C> 61.7 <C> 61.7 <C> 67.6 <C> [BOLD] 52.0 <C> [BOLD] 55.2 <C> 69.1 <R> <C> [ITALIC] mean <C> 53.1 <C> 50.5 <C> 61.6 <C> 62.0 <C> − <C> − <C> 61.9 <C> 44.2 <C> 45.8 <C> 74.6 <R> <C> [ITALIC] upperbound <C> 75.3 <C> 71.2 <C> 74.4 <C> 73.7 <C> 74.7 <C> 75.2 <C> 80.5 <C> 89.6 <C> 71.8 <C> 84.0 <CAP> Table 4: Results on the healthcare operations in the wt–wt dataset. Macro F1 scores are obtained by testing on the target operation while training on the other three. avgF1 and avgwF1 are, respectively, the unweighted and weighted (by operations size) average of all operations.
<R> <C> Dataset <C> Unde. Rate <C> Dis. Rate <C> F1 <R> <C> Political <C> 49.0 <C> 33.0 <C> 67.2 <R> <C> Gender <C> 88.0 <C> 14.0 <C> 53.6 <R> <C> Humorous <C> 66.0 <C> 31.0 <C> 79.3 <R> <C> Yelp <C> [BOLD] 11.0 <C> 16.0 <C> [BOLD] 93.2 <R> <C> Formality <C> [BOLD] 0.5 <C> 17.0 <C> [BOLD] 87.6 <CAP> Table 3: Comparison of dataset quality. “Unde. Rate” indicates the undecidable rate, “Dis. Rate” the disagreement rate between annotators.
<R> <C> [EMPTY] <C> image to recipe MedR <C> image to recipe R@1 <C> image to recipe R@5 <C> image to recipe R@10 <C> recipe to image MedR <C> recipe to image R@1 <C> recipe to image R@5 <C> recipe to image R@10 <R> <C> CCA  <C> 15.7 <C> 14.0 <C> 32.0 <C> 43.0 <C> 24.8 <C> 9.0 <C> 24.0 <C> 35.0 <R> <C> Im2Recipe  <C> 5.2 <C> 24.0 <C> 51.0 <C> 65.0 <C> 5.1 <C> 25.0 <C> 52.0 <C> 65.0 <R> <C> Ours <C> [BOLD] 1.0±0.1 <C> [BOLD] 39.8±1.8 <C> [BOLD] 69.0±1.8 <C> [BOLD] 77.4±1.1 <C> [BOLD] 1.0±0.1 <C> [BOLD] 40.2±1.6 <C> [BOLD] 68.1±1.2 <C> [BOLD] 78.7±1.3 <CAP> TABLE II: Comparison with the state of the art on image to recipe and recipe to image tasks. MedR stands for Median Rank. R@K means Recall at K in percent. Results over 10 bags of 1k pairs each.
<R> <C> [BOLD] Model <C> [BOLD] Aspect Extraction  [BOLD] P <C> [BOLD] Aspect Extraction  [BOLD] R <C> [BOLD] Aspect Extraction  [BOLD] F1 <R> <C> T <C> 0.532 <C> 0.543 <C> 0.533 <R> <C> T + CRF <C> 0.558 <C> 0.528 <C> 0.541 <R> <C> T + GV <C> 0.562 <C> 0.537 <C> 0.548 <R> <C> T + GV + CRF <C> 0.576* <C> 0.569 <C> 0.571** <R> <C> T + A + V <C> 0.587* <C> 0.578 <C> 0.580* <R> <C> T + CRF + A + V <C> 0.578 <C> 0.570 <C> 0.573* <R> <C> T + GV + CRF + A + V <C> 0.602** <C> 0.568 <C> 0.584*** <CAP> Table 4: Ablation study on aspect extraction on the simple setting. *** denotes differences against the only text model (T) results are statistically significant at 99% confidence, ** at 95% and * at 90%. (A + V) refers to the audio and video modalities, (GV) stands for GLoVe embeddings and (CRF) for the model trained using the Conditional Random Fields loss.
<R> <C> [BOLD] Method <C> [BOLD] Human Score <C> [BOLD] BLEU-1 <C> [BOLD] BLEU-2 <C> [BOLD] BLEU-3 <C> [BOLD] BLEU-4 <R> <C> Retrieval <C> 0.996 <C> 5.707 <C> 3.092 <C> 2.406 <C> [BOLD] 2.094 <R> <C> seq2seq <C> 0.907 <C> 3.676 <C> 1.228 <C> 0.564 <C> 0.286 <R> <C> biseq2seq <C> 0.966 <C> 7.762 <C> 3.293 <C> 2.056 <C> 1.487 <R> <C> Rerank(Retrieval,seq2seq) <C> 1.030 <C> 4.500 <C> 2.041 <C> 1.364 <C> 1.060 <R> <C> Rerank(Retrieval,biseq2seq) <C> [BOLD] 1.131 <C> [BOLD] 7.260 <C> [BOLD] 3.503 <C> [BOLD] 2.480 <C> 2.000 <CAP> Table 2: Results of our ensemble and competing methods in terms of average human scores and BLEUs. Inter-annotator agreement for human annotation: Fleiss’ κ=0.2824 [3], std =0.4031, indicating moderate agreement.
<R> <C> [BOLD] Method <C> [BOLD] Entropy <C> [BOLD] Length <R> <C> seq2seq <C> 7.420 <C> 7.362 <R> <C> biseq2seq <C> [BOLD] 8.302 <C> [BOLD] 8.185 <R> <C> Groundtruth <C> 8.625 <C> 12.62 <CAP> Table 3: Entropy and length of generated replies. We also include groundtruth for reference. A larger entropy value indicates that the replies are less common, and probably, more meaningful.
<R> <C> System <C> [ITALIC] evalman <C> [ITALIC] evalsge <R> <C> Best 4-gram <C> 22.57 <C> 32.42 <R> <C> N-best RNNLM rescoring <C> 21.14 <C> 30.15 <R> <C> Lattice RNNLM rescoring <C> 20.54 <C> 29.56 <CAP> Table 5: WER(%) results using lattice rescoring
<R> <C> [BOLD] Sim <C> [BOLD] Model <C> [BOLD] aze <C> [BOLD] bel <C> [BOLD] glg <C> [BOLD] slk <R> <C> - <C> Bi <C> 11.87 <C> 18.03 <C> 28.70 <C> 26.77 <R> <C> - <C> All <C> 10.87 <C> 17.77 <C> 25.49 <C> 26.28 <R> <C> - <C> copied <C> 10.74 <C> 17.19 <C> 29.75 <C> 27.81 <R> <C> LM-lang <C> TCS-D <C> 11.97 <C> 17.17 <C> 30.10 <C> 28.78 <R> <C> LM-lang <C> TCS-S <C> [BOLD] 12.55† <C> 17.23 <C> 30.69 <C> 28.95 <R> <C> Vocab-lang <C> TCS-D <C> 12.30 <C> 18.96 <C> [BOLD] 31.10∗ <C> [BOLD] 29.35∗ <R> <C> Vocab-lang <C> TCS-S <C> 12.37 <C> [BOLD] 19.83† <C> 30.94 <C> 29.00 <CAP> Table 3: BLEU scores using SDE as word encoding. Statistical significance is indicated with ∗ (p<0.001) and † (p<0.05), compared with the best baseline.
<R> <C> [EMPTY] <C> [BOLD] MedMentions <C> [EMPTY] <C> [BOLD] 3DNotes <C> [EMPTY] <R> <C> [BOLD] Model name <C> [BOLD] P@1 <C> [BOLD] MAP <C> [BOLD] P@1 <C> [BOLD] MAP <R> <C> TF-IDF <C> 61.39 <C> 67.74 <C> 56.89 <C> 69.45 <R> <C> ARC-I <C> 71.50 <C> 81.78 <C> 84.73 <C> 90.35 <R> <C> ARC-II <C> 72.56 <C> 82.36 <C> 86.12 <C> 91.38 <R> <C> KNRM <C> 74.92 <C> 83.47 <C> 84.32 <C> 90.04 <R> <C> Duet <C> 76.19 <C> 84.92 <C> 86.11 <C> 91.19 <R> <C> MatchPyramid <C> 78.15 <C> 86.31 <C> 85.97 <C> 91.32 <R> <C> MV-LSTM <C> 80.26 <C> 87.58 <C> 87.90 <C> 92.44 <R> <C> Conv-KNRM <C> 83.08 <C> 89.34 <C> 86.92 <C> 92.08 <R> <C> LATTE-NKT <C> 86.09 <C> 91.27 <C> 86.40 <C> 91.09 <R> <C> [BOLD] LATTE <C> [BOLD] 88.46 <C> [BOLD] 92.81 <C> [BOLD] 87.98 <C> [BOLD] 92.49 <CAP> Table 2: Comparison of LATTE with other baseline models on MedMentions and 3DNotes dataset. LATTE-NKT is trained without the supervision of known types classification. P@1 is short for Precision@1.
<R> <C> [EMPTY] <C> [BOLD] MedMentions <C> [EMPTY] <C> [BOLD] 3DNotes <C> [EMPTY] <R> <C> [BOLD] Model name <C> [BOLD] P@1 <C> [BOLD] MAP <C> [BOLD] P@1 <C> [BOLD] MAP <R> <C> LATTE_base <C> 80.02 <C> 86.94 <C> 84.08 <C> 90.15 <R> <C> LATTE_base+LT <C> 86.09 <C> 91.27 <C> 86.40 <C> 91.09 <R> <C> LATTE_base+KT <C> 87.73 <C> 92.33 <C> 87.80 <C> [BOLD] 92.66 <R> <C> LATTE <C> [BOLD] 88.46 <C> [BOLD] 92.81 <C> [BOLD] 87.98 <C> 92.49 <CAP> Table 3: Performance comparison of LATTE and its variants on MedMentions and 3DNotes Datasets.
<R> <C> Model <C> WS-240 <C> WS-296 <C> MC-30 <C> RG-65 <C> avg <C> Δ <R> <C> Skip-gram <C> 50.23 <C> 56.94 <C> 69.66 <C> 59.86 <C> 59.17 <C> - <R> <C> CBOW <C> 51.49 <C> 61.01 <C> 68.97 <C> 63.85 <C> 61.33 <C> +2.16 <R> <C> CWE <C> 52.63 <C> 58.98 <C> 68.82 <C> 59.60 <C> 60.01 <C> +0.84 <R> <C> GWE <C> 52.74 <C> 58.22 <C> 68.23 <C> 60.74 <C> 59.98 <C> +0.81 <R> <C> JWE <C> 51.92 <C> 59.84 <C> 70.27 <C> 62.83 <C> 61.22 <C> +2.05 <R> <C> VCWE <C> 57.81 <C> [BOLD] 61.29 <C> [BOLD] 72.77 <C> [BOLD] 70.62 <C> [BOLD] 65.62 <C> [BOLD] +6.45 <R> <C> -CNN <C> 55.82 <C> 59.60 <C> 66.87 <C> 68.53 <C> 62.71 <C> +3.54 <R> <C> -LSTM <C> [BOLD] 58.13 <C> 60.85 <C> 68.03 <C> 69.78 <C> 64.20 <C> +5.03 <CAP> Table 1: Spearman correlation for word similarity datasets, “-CNN” represents replacing the CNN and image information with randomly initialized character embedding, “-LSTM” represents replacing Bi-LSTM network and self-attention with the averaging operation. For each dataset, we boldface the score with the best performance across all models.
<R> <C> [BOLD] Model <C> [BOLD] P <C> [BOLD] R <C> [ITALIC] F1 <R> <C> MaxEnt  <C> 74.5 <C> 59.1 <C> 65.9 <R> <C> CrossEntity  <C> 72.9 <C> 64.3 <C> 68.3 <R> <C> DMCNN  <C> 75.6 <C> 63.6 <C> 69.1 <R> <C> JRNN  <C> 66.0 <C> 73.0 <C> 69.3 <R> <C> ANN-AugAtt  <C> 78.0 <C> 66.3 <C> 71.7 <R> <C> dbRNN†‡  <C> 74.1 <C> 69.8 <C> 71.9 <R> <C> HBTNGMA  <C> 77.9 <C> 69.1 <C> 73.3 <R> <C> GCN-ED†  <C> 77.9 <C> 68.8 <C> 73.1 <R> <C> JMEE†  <C> 76.3 <C> 71.3 <C> 73.7 <R> <C> MOGANED†  <C> [BOLD] 79.5 <C> 72.3 <C> 75.7 <R> <C> RGCN†‡  <C> 70.6 <C> [BOLD] 80.8 <C> 75.4 <R> <C> [BOLD] RA-GCN†‡ <C> 76.7 <C> 78.6 <C> [BOLD] 77.6 <CAP> Table 1: Performance on blind test data. † means models using syntactic dependency structure only and †‡ means models using syntactic dependency structure and relation simultaneously; Bold marks the highest score among all models.
<R> <C> [BOLD] Model <C> [ITALIC] F1 <R> <C> RA-GCN <C> 77.62 <R> <C> – RAAM <C> 75.51 <R> <C> – MdR <C> 73.51 <R> <C> – CARUM <C> 75.23 <R> <C> – RAAM & CARUM <C> 74.82 <R> <C> – BiLSTM <C> 65.84 <CAP> Table 2: Ablation study of RA-GCN, where “–” means “remove”, “RAAM” means “relation-aware aggregation module”, “MdR” means multi-dimensional representation of relation, “CARUM” means “context-aware relation update module”.
<R> <C> [BOLD] Method <C> [BOLD] Weibo Dataset  [BOLD] Avg. P <C> [BOLD] Weibo Dataset  [BOLD] Avg. R <C> [BOLD] Weibo Dataset  [BOLD] Avg. F1 <C> [BOLD] Weibo Dataset  [BOLD] Acc. <C> [BOLD] Reddit Dataset  [BOLD] Avg. P <C> [BOLD] Reddit Dataset  [BOLD] Avg. R <C> [BOLD] Reddit Dataset  [BOLD] Avg. F1 <C> [BOLD] Reddit Dataset  [BOLD] Acc. <R> <C> DTPC-GCN <C> [BOLD] 75.57 <C> [BOLD] 75.31 <C> [BOLD] 75.27 <C> [BOLD] 75.35 <C> [BOLD] 68.76 <C> [BOLD] 67.63 <C> [BOLD] 67.14 <C> [BOLD] 67.63 <R> <C> U branch only <C> 74.06 <C> 74.06 <C> 74.05 <C> 74.05 <C> 63.95 <C> 63.94 <C> 63.94 <C> 63.94 <R> <C> R branch only <C> 74.16 <C> 73.33 <C> 73.15 <C> 73.41 <C> 63.41 <C> 63.15 <C> 62.97 <C> 63.15 <CAP> Table 5: Ablation study of DTPC-GCN in the inter-topic experiments (%).
<R> <C> [BOLD] Model <C> [BOLD] BLEU-4 <C> [BOLD] METEOR <C> [BOLD] ROUGE-L <R> <C> SemQG [semiq] <C> 20.76 <C> 24.20 <C> 48.91 <R> <C> UNILM [ITALIC] LARGE [UNILM] <C> 22.12 <C> 25.06 <C> 51.07 <R> <C> ERNIE-GEN [ITALIC] BASE (beam size = 1) <C> 22.28 <C> 25.13 <C> 50.58 <R> <C> ERNIE-GEN [ITALIC] LARGE (beam size = 1) <C> 24.03 <C> 26.31 <C> 52.36 <R> <C> ERNIE-GEN [ITALIC] LARGE (beam size = 5) <C> [BOLD] 25.40 <C> [BOLD] 26.92 <C> [BOLD] 52.84 <R> <C> *  [ITALIC] Reversed test ↔  [ITALIC] dev split <C> *  [ITALIC] Reversed test ↔  [ITALIC] dev split <C> *  [ITALIC] Reversed test ↔  [ITALIC] dev split <C> *  [ITALIC] Reversed test ↔  [ITALIC] dev split <R> <C> MP-GSN [split] <C> 16.38 <C> 20.25 <C> 44.48 <R> <C> SemQG [semiq] <C> 20.76 <C> 24.20 <C> 48.91 <R> <C> UNILM [ITALIC] LARGE [UNILM] <C> 23.75 <C> 25.61 <C> 52.04 <R> <C> ERNIE-GEN [ITALIC] BASE (beam size = 1) <C> 23.52 <C> 25.61 <C> 51.45 <R> <C> ERNIE-GEN [ITALIC] LARGE (beam size = 1) <C> 25.57 <C> 26.89 <C> 53.31 <R> <C> ERNIE-GEN [ITALIC] LARGE (beam size = 5) <C> [BOLD] 26.95 <C> [BOLD] 27.57 <C> [BOLD] 53.77 <CAP> Table 4: Question generation results on SQuAD. Models in the upper block and the lower block use different test ↔ dev split method.
<R> <C> Model <C> PA <C> OG <C> FU <C> Macro <C> Micro <R> <C> Local Contexts <C> Local Contexts <C> Local Contexts <C> Local Contexts <C> Local Contexts <C> Local Contexts <R> <C> CNN Huang et al. ( 2016 ) <C> 91/83/87 <C> 46/57/51 <C> 49/67/57 <C> 62/69/65 <C> 77/77/76.9 <R> <C> LSTM <C> 88/83/85 <C> 47/54/51 <C> 52/62/57 <C> 63/66/64 <C> 75/75/75.5 <R> <C> Dependency Chains <C> Dependency Chains <C> Dependency Chains <C> Dependency Chains <C> Dependency Chains <C> Dependency Chains <R> <C> CNN <C> 91/84/87 <C> 49/63/55 <C> 60/65/62 <C> 67/71/68 <C> 79/79/78.6 <R> <C> LSTM <C> 92/85/ [BOLD] 88 <C> 49/63/ [BOLD] 55 <C> 63/71/ [BOLD] 67 <C> 68/73/ [BOLD] 70 <C> 80/80/ [BOLD] 79.6 <R> <C> Full Dependency Trees <C> Full Dependency Trees <C> Full Dependency Trees <C> Full Dependency Trees <C> Full Dependency Trees <C> Full Dependency Trees <R> <C> tree-LSTM <C> 92/80/86 <C> 47/59/53 <C> 30/58/40 <C> 56/66/60 <C> 75/75/75.1 <CAP> Table 1: Classification results on the test set. Each cell shows Recall/Precision/F1 score.
<R> <C> [BOLD] Model <C> [BOLD] Single-Oracle  [BOLD] EM <C> [BOLD] Single-Oracle  [BOLD] F1 <C> [BOLD] Ordered-Oracle  [BOLD] EM <C> [BOLD] Ordered-Oracle  [BOLD] F1 <R> <C> HotpotReader <C> 55.07 <C> 70.00 <C> 55.17 <C> 70.75 <R> <C> Bert <C> 64.08 <C> 77.86 <C> 65.03 <C> 79.15 <CAP> Table 1: Baseline results for HotpotReader and BERT
<R> <C> [BOLD] Method <C> [BOLD] Training <C> [BOLD] Test <R> <C> MOST FREQUENT <C> Wiki <C> 54.1 <R> <C> COSINE SIMILARITY <C> Wiki <C> 21.7 <R> <C> GRU+ATTN <C> [EMPTY] <C> [EMPTY] <R> <C> Mueller and Durrett ( 2018 ) <C> in-domain <C> 41.2 <R> <C> GRU+ATTN <C> Wiki <C> 43.4 <R> <C> CBoW+WORD2VEC <C> in-domain <C> 43.0 <R> <C> CBoW+WORD2VEC <C> Wiki <C> 38.0 <R> <C> Onoe and Durrett ( 2019 ) <C> Wiki <C> 62.2 <R> <C> Ours <C> Wiki <C> 69.4 <R> <C> Ours <C> in-domain <C> 71.7 <CAP> Table 6: Accuracy on the WikilinksNED Unseen-Mentions test set. The numbers of baseline models are from Onoe and Durrett (2019).
<R> <C> Model <C> BLEU <C> #Params <R> <C> Transformer (base) <C> 27.24 <C> 66.4M <R> <C> [BOLD] BBFNMT w/ Shared encoder <C> 28.35 <C> 78.6M <R> <C> [BOLD] BBFNMT w/ Independent encoders <C> 28.50 <C> 91.6M <CAP> TABLE V: The effect of encoder parameters.
<R> <C> [EMPTY] <C> Method <C> CoLA <C> RTE <C> QQP <C> MRPC <C> SST-2 <C> MNLI-m <C> MNLI-mm <C> QNLI <C> STS-B <C> IMDB <C> RACE <R> <C> Test Accuracy <C> BERTBASE <C> 52.5 <C> 68.1 <C> 71.2 <C> 88.7 <C> 93.0 <C> 84.6 <C> 84.0 <C> 91.0 <C> 85.8 <C> 93.5 <C> 66.9 <R> <C> Test Accuracy <C> PoWER-BERT <C> 52.3 <C> 67.4 <C> 70.2 <C> 88.1 <C> 92.1 <C> 83.8 <C> 83.1 <C> 90.1 <C> 85.1 <C> 92.5 <C> 66.0 <R> <C> Inference Time (ms) <C> BERTBASE <C> 898 <C> 3993 <C> 1833 <C> 1798 <C> 905 <C> 1867 <C> 1881 <C> 1848 <C> 881 <C> 9110 <C> 20040 <R> <C> Inference Time (ms) <C> PoWER-BERT <C> 201 <C> 1189 <C> 405 <C> 674 <C> 374 <C> 725 <C> 908 <C> 916 <C> 448 <C> 3419 <C> 10110 <R> <C> Speedup <C> [EMPTY] <C> [BOLD] (4.5x) <C> (3.4x) <C> [BOLD] (4.5x) <C> (2.7x) <C> (2.4x) <C> (2.6x) <C> (2.1x) <C> (2.0x) <C> (2.0x) <C> (2.7x) <C> (2.0x) <CAP> Table 2: Comparison between PoWER-BERT and BERTBASE. We limit the accuracy loss for PoWER-BERT to be within 1% by tuning the regularizer parameter λ. Inference done on a K80 GPU with batch size of 128 (averaged over 100 runs). Matthew’s Correlation reported for CoLA; F1-score for QQP and MRPC; Spearman Correlation for STS-B; Accuracy for the rest.
<R> <C> [EMPTY] <C> Method <C> CoLA <C> RTE <C> QQP <C> MRPC <C> SST-2 <C> MNLI-m <C> MNLI-mm <C> QNLI <C> STS-B <R> <C> Test Accuracy <C> ALBERT <C> 42.8 <C> 65.6 <C> 68.3 <C> 89.0 <C> 93.7 <C> 82.6 <C> 82.5 <C> 89.2 <C> 80.9 <R> <C> Test Accuracy <C> PoWER-BERT <C> 43.8 <C> 64.6 <C> 67.4 <C> 88.1 <C> 92.7 <C> 81.8 <C> 81.6 <C> 89.1 <C> 80.0 <R> <C> Inference Time (ms) <C> ALBERT <C> 940 <C> 4210 <C> 1950 <C> 1957 <C> 922 <C> 1960 <C> 1981 <C> 1964 <C> 956 <R> <C> Inference Time (ms) <C> PoWER-BERT <C> 165 <C> 1778 <C> 287 <C> 813 <C> 442 <C> 589 <C> 922 <C> 1049 <C> 604 <R> <C> Speedup <C> [EMPTY] <C> (5.7x) <C> (2.4x) <C> [BOLD] (6.8x) <C> (2.4x) <C> (2.1x) <C> (3.3x) <C> (2.1x) <C> (1.9x) <C> (1.6x) <CAP> Table 3: Comparison between PoWER-BERT and ALBERT. Here PoWER-BERT represents application of our scheme on ALBERT. The experimental setup is same as in Table 2
<R> <C> Method <C> VQA score on OK-VQA <R> <C> ResNet152 <C> 26.41 <R> <C> ResNet50 <C> 24.74 <R> <C> ResNet18 <C> 23.64 <R> <C> Q-Only <C> 14.93 <CAP> Table 3: Results on OK-VQA with different visual features.
<R> <C> Dataset <C> [ITALIC] α (CAWA) <C> [ITALIC] β (CAWA) <C> [ITALIC] k (ML-KNN) <C> [ITALIC] α (SEG-REFINE) <C> [ITALIC] m (MLTM) <R> <C> Movies <C> 0.2 <C> 0.1 <C> 100 <C> 0.50 <C> 120 <R> <C> Ohsumed <C> 0.1 <C> 0.1 <C> 20 <C> 0.40 <C> 90 <R> <C> TMC2007 <C> 0.1 <C> 0.3 <C> 50 <C> 0.40 <C> 90 <R> <C> Patents <C> 0.5 <C> 0.3 <C> 50 <C> 0.45 <C> 110 <R> <C> Delicious <C> 0.1 <C> 0.2 <C> 20 <C> 0.50 <C> 70 <CAP> Table 3: Hyperparameter values.
<R> <C> Dataset <C> Model* <C> \operatornamewithlimits  [ITALIC] SOV <C> \operatornamewithlimits  [ITALIC] PPPA <C> \operatornamewithlimits  [ITALIC] F1 <C> \operatornamewithlimits  [ITALIC] AUCμ <C> \operatornamewithlimits  [ITALIC] AUCM <R> <C> Movies <C> CAWA <C> [BOLD] 0.50 <C> 0.38 <C> [BOLD] 0.65 <C> 0.81 <C> 0.78 <R> <C> [EMPTY] <C> SEG-REF <C> 0.49 <C> 0.36 <C> 0.63 <C> 0.81 <C> 0.80 <R> <C> [EMPTY] <C> MLTM <C> [BOLD] 0.50 <C> [BOLD] 0.40 <C> [BOLD] 0.65 <C> 0.82 <C> 0.80 <R> <C> [EMPTY] <C> DNN+A <C> 0.33 <C> 0.27 <C> 0.62 <C> 0.84 <C> 0.82 <R> <C> [EMPTY] <C> DNN-A <C> 0.33 <C> 0.27 <C> 0.61 <C> [BOLD] 0.85 <C> 0.83 <R> <C> [EMPTY] <C> ML-KNN <C> 0.38 <C> 0.30 <C> 0.63 <C> 0.83 <C> 0.81 <R> <C> [EMPTY] <C> BR-MNB <C> 0.39 <C> 0.31 <C> 0.53 <C> 0.82 <C> [BOLD] 0.84 <R> <C> Ohsumed <C> CAWA <C> [BOLD] 0.65 <C> [BOLD] 0.55 <C> 0.64 <C> 0.93 <C> 0.89 <R> <C> [EMPTY] <C> SEG-REF <C> 0.63 <C> 0.47 <C> 0.65 <C> [BOLD] 0.94 <C> [BOLD] 0.92 <R> <C> [EMPTY] <C> MLTM <C> 0.56 <C> 0.47 <C> 0.60 <C> 0.93 <C> 0.91 <R> <C> [EMPTY] <C> DNN+A <C> 0.44 <C> 0.37 <C> [BOLD] 0.67 <C> [BOLD] 0.94 <C> [BOLD] 0.92 <R> <C> [EMPTY] <C> DNN-A <C> 0.33 <C> 0.31 <C> 0.58 <C> [BOLD] 0.94 <C> [BOLD] 0.92 <R> <C> [EMPTY] <C> ML-KNN <C> 0.48 <C> 0.38 <C> 0.59 <C> 0.90 <C> 0.87 <R> <C> [EMPTY] <C> BR-MNB <C> 0.29 <C> 0.30 <C> 0.31 <C> 0.82 <C> 0.71 <R> <C> TMC2007 <C> CAWA <C> 0.56 <C> [BOLD] 0.47 <C> 0.68 <C> 0.95 <C> 0.91 <R> <C> [EMPTY] <C> SEG-REF <C> [BOLD] 0.59 <C> 0.44 <C> 0.68 <C> 0.95 <C> 0.90 <R> <C> [EMPTY] <C> MLTM <C> 0.49 <C> 0.43 <C> 0.64 <C> [BOLD] 0.96 <C> [BOLD] 0.92 <R> <C> [EMPTY] <C> DNN+A <C> 0.43 <C> 0.37 <C> 0.68 <C> [BOLD] 0.96 <C> [BOLD] 0.92 <R> <C> [EMPTY] <C> DNN-A <C> 0.35 <C> 0.34 <C> 0.59 <C> [BOLD] 0.96 <C> [BOLD] 0.92 <R> <C> [EMPTY] <C> ML-KNN <C> 0.45 <C> 0.35 <C> [BOLD] 0.71 <C> 0.95 <C> 0.89 <R> <C> [EMPTY] <C> BR-MNB <C> 0.30 <C> 0.33 <C> 0.62 <C> 0.89 <C> 0.72 <R> <C> Patents <C> CAWA <C> [BOLD] 0.58 <C> [BOLD] 0.50 <C> 0.61 <C> 0.88 <C> 0.86 <R> <C> [EMPTY] <C> SEG-REF <C> 0.56 <C> 0.45 <C> 0.61 <C> 0.86 <C> 0.85 <R> <C> [EMPTY] <C> MLTM <C> 0.55 <C> 0.48 <C> 0.59 <C> 0.85 <C> 0.84 <R> <C> [EMPTY] <C> DNN+A <C> 0.53 <C> 0.43 <C> [BOLD] 0.64 <C> [BOLD] 0.89 <C> 0.87 <R> <C> [EMPTY] <C> DNN-A <C> 0.51 <C> 0.42 <C> 0.63 <C> [BOLD] 0.89 <C> [BOLD] 0.88 <R> <C> [EMPTY] <C> ML-KNN <C> 0.45 <C> 0.37 <C> 0.51 <C> 0.82 <C> 0.80 <R> <C> [EMPTY] <C> BR-MNB <C> 0.50 <C> 0.43 <C> 0.50 <C> 0.87 <C> 0.86 <R> <C> Delicious <C> CAWA <C> [BOLD] 0.50 <C> [BOLD] 0.39 <C> [BOLD] 0.52 <C> 0.85 <C> 0.84 <R> <C> [EMPTY] <C> SEG-REF <C> 0.48 <C> 0.36 <C> 0.49 <C> 0.85 <C> 0.85 <R> <C> [EMPTY] <C> MLTM <C> 0.49 <C> 0.37 <C> 0.50 <C> 0.84 <C> 0.83 <R> <C> [EMPTY] <C> DNN+A <C> 0.22 <C> 0.18 <C> 0.38 <C> 0.87 <C> 0.86 <R> <C> [EMPTY] <C> DNN-A <C> 0.21 <C> 0.17 <C> 0.36 <C> [BOLD] 0.88 <C> [BOLD] 0.87 <R> <C> [EMPTY] <C> ML-KNN <C> 0.24 <C> 0.19 <C> 0.35 <C> 0.82 <C> 0.80 <R> <C> [EMPTY] <C> BR-MNB <C> 0.25 <C> 0.19 <C> 0.05 <C> 0.76 <C> 0.73 <CAP> Table 4: Performance comparison results.
<R> <C> [EMPTY] <C> Accuracy <C> Acc@161 <C> Median <C> Mean <R> <C> HLPNN <C> 37.3 <C> 52.9 <C> 109.3 <C> 1289.4 <R> <C> w/o Char-CNN <C> 36.3 <C> 51.0 <C> 130.8 <C> 1429.9 <R> <C> w/o Word-Att <C> 36.4 <C> 51.5 <C> 130.2 <C> 1377.5 <R> <C> w/o Field-Att <C> 37.0 <C> 52.0 <C> 121.8 <C> 1337.5 <R> <C> w/o encoders <C> 36.8 <C> 52.5 <C> 117.4 <C> 1402.9 <R> <C> w/o country <C> 36.7 <C> 52.6 <C> 124.8 <C> 1399.2 <CAP> Table 4: An ablation study on WNUT dataset.
<R> <C> [BOLD] System <C> [BOLD] Task 1A: Sentence Overlap ( [ITALIC] F1) <C> [BOLD] Task 1A: ROUGE-SU4  [ITALIC] F1 <C> [BOLD] Task 1B <R> <C> system 3 Run 2 <C> 0.126 <C> 0.075 <C> 0.312 <R> <C> system 12 Run 1 <C> 0.124 <C> 0.090 <C> 0.221 <R> <C> system 3 Run 5 <C> 0.120 <C> 0.072 <C> 0.303 <R> <C> system 3 Run 6 <C> 0.118 <C> 0.079 <C> 0.292 <R> <C> system 12 Run 2 <C> 0.118 <C> 0.061 <C> 0.266 <R> <C> system 3 Run 10 <C> 0.110 <C> 0.073 <C> 0.276 <R> <C> system 3 Run 4 <C> 0.110 <C> 0.062 <C> 0.283 <R> <C> system 2 run15-Voting-1.1-SubtitleAndHfw-QD_method_1 <C> 0.106 <C> 0.034 <C> 0.389 <R> <C> system 2 run13-Voting-1.1-SubtitleAndHfw-LSA_method_3 <C> 0.106 <C> 0.034 <C> 0.389 <R> <C> system 2 run14-Voting-1.1-SubtitleAndHfw-LSA_method_4 <C> 0.106 <C> 0.034 <C> 0.389 <R> <C> system 2 run16-Voting-1.1-SubtitleAndHfw-SentenceVec_method_2 <C> 0.106 <C> 0.034 <C> 0.389 <R> <C> system 2 run23-Voting-2.0-Voting-QD_method_1 <C> 0.104 <C> 0.036 <C> 0.341 <R> <C> system 2 run24-Voting-2.0-Voting-SentenceVec_method_2 <C> 0.104 <C> 0.036 <C> 0.341 <R> <C> system 2 run20-Voting-2.0-TextCNN-SentenceVec_method_2 <C> 0.104 <C> 0.036 <C> 0.342 <R> <C> system 2 run21-Voting-2.0-Voting-LSA_method_3 <C> 0.104 <C> 0.036 <C> 0.341 <R> <C> system 2 run18-Voting-2.0-TextCNN-LSA_method_4 <C> 0.104 <C> 0.036 <C> 0.342 <R> <C> system 2 run22-Voting-2.0-Voting-LSA_method_4 <C> 0.104 <C> 0.036 <C> 0.341 <R> <C> system 2 run19-Voting-2.0-TextCNN-QD_method_1 <C> 0.104 <C> 0.036 <C> 0.342 <R> <C> system 2 run17-Voting-2.0-TextCNN-LSA_method_3 <C> 0.104 <C> 0.036 <C> 0.342 <R> <C> system 12 Run 3 <C> 0.104 <C> 0.041 <C> 0.286 <R> <C> system 2 run10-Jaccard-Focused-Voting-LSA_method_4 <C> 0.103 <C> 0.038 <C> 0.294 <R> <C> system 2 run7-Jaccard-Focused-SubtitleAndHfw-QD_method_1 <C> 0.103 <C> 0.038 <C> 0.385 <R> <C> system 2 run5-Jaccard-Focused-SubtitleAndHfw-LSA_method_3 <C> 0.103 <C> 0.038 <C> 0.385 <R> <C> system 2 run9-Jaccard-Focused-Voting-LSA_method_3 <C> 0.103 <C> 0.038 <C> 0.294 <R> <C> system 2 run12-Jaccard-Focused-Voting-SentenceVec_method_2 <C> 0.103 <C> 0.038 <C> 0.294 <R> <C> system 2 run6-Jaccard-Focused-SubtitleAndHfw-LSA_method_4 <C> 0.103 <C> 0.038 <C> 0.385 <R> <C> system 2 run11-Jaccard-Focused-Voting-QD_method_1 <C> 0.103 <C> 0.038 <C> 0.294 <R> <C> system 2 run8-Jaccard-Focused-SubtitleAndHfw-SentenceVec_method_2 <C> 0.103 <C> 0.038 <C> 0.385 <R> <C> system 12 Run 4 <C> 0.098 <C> 0.030 <C> 0.315 <R> <C> system 3 Run 3 <C> 0.097 <C> 0.062 <C> 0.251 <R> <C> system 4 WithoutEmb_Training20182019_Test2019_3_0.1 <C> 0.097 <C> 0.071 <C> 0.286 <R> <C> system 4 WithoutEmb_Training2018_Test2019_3_0.1 <C> 0.097 <C> 0.071 <C> 0.286 <R> <C> system 4 WithoutEmb_Training2019_Test2019_3_0.1 <C> 0.097 <C> 0.071 <C> 0.286 <R> <C> system 3 Run 1 <C> 0.093 <C> 0.060 <C> 0.255 <R> <C> system 9 Run 2 <C> 0.092 <C> 0.034 <C> 0.229 <R> <C> system 9 Run 3 <C> 0.092 <C> 0.034 <C> 0.229 <R> <C> system 9 Run 1 <C> 0.092 <C> 0.034 <C> 0.229 <R> <C> system 9 Run 4 <C> 0.092 <C> 0.034 <C> 0.229 <R> <C> system 4 WithoutEmbTopsim_Training20182019_Test2019_0.15_5_0.05 <C> 0.090 <C> 0.044 <C> 0.351 <R> <C> system 4 WithoutEmbTopsim_Training2019_Test2019_0.15_5_0.05 <C> 0.090 <C> 0.044 <C> 0.351 <R> <C> system 4 WithoutEmbTopsim_Training2018_Test2019_0.15_5_0.05 <C> 0.090 <C> 0.044 <C> 0.351 <R> <C> system 4 WithoutEmbPOS_Training20182019_Test2019_3_0.1 <C> 0.089 <C> 0.065 <C> 0.263 <R> <C> system 4 WithoutEmbPOS_Training2019_Test2019_3_0.1 <C> 0.089 <C> 0.065 <C> 0.263 <R> <C> system 4 WithoutEmbPOS_Training2018_Test2019_3_0.1 <C> 0.089 <C> 0.065 <C> 0.263 <R> <C> system 4 WithoutEmbTopsimPOS_Training2019_Test2019_0.15_5_0.05 <C> 0.088 <C> 0.044 <C> 0.346 <R> <C> system 4 WithoutEmbTopsimPOS_Training2018_Test2019_0.15_5_0.05 <C> 0.088 <C> 0.044 <C> 0.346 <R> <C> system 4 WithoutEmbTopsimPOS_Training20182019_Test2019_0.15_5_0.05 <C> 0.088 <C> 0.044 <C> 0.346 <R> <C> system 2 run1-Jaccard-Cascade-Voting-LSA_method_3 <C> 0.087 <C> 0.033 <C> 0.274 <R> <C> system 2 run3-Jaccard-Cascade-Voting-QD_method_1 <C> 0.087 <C> 0.033 <C> 0.274 <R> <C> system 2 run4-Jaccard-Cascade-Voting-SentenceVec_method_2 <C> 0.087 <C> 0.033 <C> 0.274 <R> <C> system 2 run2-Jaccard-Cascade-Voting-LSA_method_4 <C> 0.087 <C> 0.033 <C> 0.274 <R> <C> system 1 Run 26 <C> 0.086 <C> 0.041 <C> 0.245 <R> <C> system 1 Run 4 <C> 0.086 <C> 0.042 <C> 0.241 <R> <C> system 1 Run 30 <C> 0.081 <C> 0.036 <C> 0.242 <R> <C> system 1 Run 27 <C> 0.081 <C> 0.040 <C> 0.207 <R> <C> system 1 Run 8 <C> 0.081 <C> 0.036 <C> 0.242 <R> <C> system 1 Run 10 <C> 0.081 <C> 0.036 <C> 0.242 <R> <C> system 1 Run 23 <C> 0.081 <C> 0.036 <C> 0.242 <R> <C> system 1 Run 17 <C> 0.080 <C> 0.035 <C> 0.236 <R> <C> system 3 Run 7 <C> 0.078 <C> 0.048 <C> 0.218 <R> <C> system 1 Run 12 <C> 0.078 <C> 0.093 <C> 0.098 <R> <C> system 1 Run 15 <C> 0.078 <C> 0.093 <C> 0.110 <R> <C> system 1 Run 28 <C> 0.078 <C> 0.093 <C> 0.098 <R> <C> system 1 Run 2 <C> 0.078 <C> 0.093 <C> 0.110 <R> <C> system 1 Run 9 <C> 0.078 <C> 0.093 <C> 0.110 <R> <C> system 1 Run 25 <C> 0.078 <C> 0.093 <C> 0.098 <R> <C> system 1 Run 13 <C> 0.078 <C> 0.040 <C> 0.205 <R> <C> system 1 Run 24 <C> 0.078 <C> 0.093 <C> 0.110 <R> <C> system 1 Run 22 <C> 0.078 <C> 0.093 <C> 0.098 <R> <C> system 1 Run 3 <C> 0.078 <C> 0.093 <C> 0.098 <R> <C> system 1 Run 5 <C> 0.078 <C> 0.093 <C> 0.113 <R> <C> system 1 Run 6 <C> 0.078 <C> 0.093 <C> 0.110 <R> <C> system 1 Run 1 <C> 0.078 <C> 0.093 <C> 0.113 <R> <C> system 1 Run 14 <C> 0.078 <C> 0.093 <C> 0.113 <R> <C> system 1 Run 7 <C> 0.078 <C> 0.093 <C> 0.098 <R> <C> system 1 Run 16 <C> 0.078 <C> 0.093 <C> 0.098 <R> <C> system 1 Run 29 <C> 0.078 <C> 0.093 <C> 0.110 <R> <C> system 1 Run 18 <C> 0.077 <C> 0.033 <C> 0.232 <R> <C> system 4 unweightedPOS_W2v_Training2018_Test2019_3_0.05 <C> 0.076 <C> 0.045 <C> 0.201 <R> <C> system 4 unweightedPOS_W2v_Training20182019_Test2019_3_0.05 <C> 0.076 <C> 0.047 <C> 0.201 <R> <C> system 4 unweightedPOS_W2v_Training2019_Test2019_3_0.05 <C> 0.076 <C> 0.045 <C> 0.201 <R> <C> system 1 Run 11 <C> 0.075 <C> 0.091 <C> 0.106 <R> <C> system 3 Run 8 <C> 0.074 <C> 0.051 <C> 0.221 <R> <C> system 1 Run 19 <C> 0.073 <C> 0.031 <C> 0.218 <R> <C> system 8 Run 4 <C> 0.070 <C> 0.025 <C> 0.122 <R> <C> system 8 Run 2 <C> 0.066 <C> 0.026 <C> 0.277 <R> <C> system 3 Run 11 <C> 0.062 <C> 0.052 <C> 0.150 <R> <C> system 1 Run 20 <C> 0.061 <C> 0.032 <C> 0.178 <R> <C> system 1 Run 21 <C> 0.048 <C> 0.048 <C> 0.083 <R> <C> system 8 Run 3 <C> 0.031 <C> 0.021 <C> 0.078 <R> <C> system 8 Run 1 <C> 0.020 <C> 0.015 <C> 0.070 <R> <C> system 7 <C> 0.020 <C> 0.031 <C> 0.045 <R> <C> system 17 ntua-ilsp-RUN-NNT <C> 0.013 <C> 0.021 <C> 0.016 <R> <C> system 3 Run 9 <C> 0.012 <C> 0.018 <C> 0.039 <R> <C> system 2 run25-Word2vec-H-CNN-SubtitleAndHfw-QD_method_1 <C> 0.009 <C> 0.009 <C> 0.047 <R> <C> system 2 run26-Word2vec-H-CNN-SubtitleAndHfw-SentenceVec_method_2 <C> 0.009 <C> 0.009 <C> 0.047 <R> <C> system 17 ntua-ilsp-RUN_NNF <C> 0.007 <C> 0.013 <C> 0.013 <CAP> Table 1: Systems’ performance in Task 1A and 1B, ordered by their F1-scores for sentence overlap on Task 1A. Each system’s rank by their performance on ROUGE on Task 1A and 1B are shown in parentheses.
<R> <C> [BOLD] Method <C> [BOLD] Constraints  [ITALIC] e: Country;  [ITALIC] v: Person;  [ITALIC] t: year <C> [BOLD] Constraints  [ITALIC] e: Country;  [ITALIC] v: Person;  [ITALIC] t: year <C> [BOLD] Evaluation Setting On ( [ITALIC] e, [ITALIC] v, [ITALIC] t) <C> [BOLD] Evaluation Setting On ( [ITALIC] e, [ITALIC] v, [ITALIC] t) <C> [BOLD] Evaluation Setting On ( [ITALIC] e, [ITALIC] v,[ [ITALIC] tmin, [ITALIC] tmax]) <C> [BOLD] Evaluation Setting On ( [ITALIC] e, [ITALIC] v,[ [ITALIC] tmin, [ITALIC] tmax]) <R> <C> [BOLD] Method <C> C1 [ITALIC] v−1 [ITALIC] e <C> C1( [ITALIC] e, [ITALIC] t)−1 [ITALIC] v <C> [BOLD] AUC <C> [BOLD] F1 <C> [BOLD] AUC <C> [BOLD] F1 <R> <C> TruthFinder <C>  [BOLD] ✔ <C>  [BOLD] ✗ <C> 0.0006 <C> 0.0012 <C> 0.0006 <C> 0.0012 <R> <C> LTM <C>  [BOLD] ✗ <C>  [BOLD] ✗ <C> 0.1319 <C> 0.0199 <C> 0.2030 <C> 0.0218 <R> <C> LTM <C>  [BOLD] ✗ <C>  [BOLD] ✔ <C> 0.0212 <C> 0.0505 <C> 0.0407 <C> 0.0793 <R> <C> TruePIE <C>  [BOLD] ✔ <C>  [BOLD] ✗ <C> 0.0587 <C> 0.1430 <C> 0.0587 <C> 0.1430 <R> <C> MajVote <C>  [BOLD] ✗ <C>  [BOLD] ✔ <C> 0.3336 <C> 0.4318 <C> 0.4958 <C> 0.5927 <R> <C> TFWIN <C>  [BOLD] ✔ <C>  [BOLD] ✔ <C> 0.4746 <C> 0.6361 <C> 0.5523 <C> 0.6489 <R> <C> Ours (PGMCC) <C>  [BOLD] ✗ <C>  [BOLD] ✔ <C> 0.4840 <C> 0.6502 <C> 0.6006 <C> 0.7254 <R> <C> Ours (PGMCC) <C>  [BOLD] ✔ <C>  [BOLD] ✔ <C> [BOLD] 0.4987 <C> [BOLD] 0.6634 <C> [BOLD] 0.6075 <C> [BOLD] 0.7316 <CAP> Table 2: Our proposed model performs better than baseline methods on finding temporal facts.
<R> <C> [BOLD] Textual Pattern p <C> [ITALIC] rp( [ITALIC] post) <C> [ITALIC] rp( [ITALIC] tag) <R> <C> president Person of Country <C> 0.920 <C> 0.870 <R> <C> Country’s current president Person, <C> 0.978 <C> 0.250 <R> <C> Country’s newly elected president , Person , <C> 0.970 <C> 0.030 <R> <C> Person, now president of Country, <C> 0.750 <C> 0.110 <R> <C> Person, who has ruled Country <C> 0.438 <C> 0.994 <R> <C> $Country’s former president Person <C> 0.113 <C> 0.994 <R> <C> Person, who ruled Country <C> 0.607 <C> 0.758 <R> <C> Country president Person signed <C> 0.553 <C> 0.327 <R> <C> Country premier Person <C> 0.012 <C> 0.010 <R> <C> Country foreign minister Person <C> 0 <C> 0 <R> <C> Country golfer Person <C> 0 <C> 0 <CAP> Table 3: Pattern’s reliability for country’s presidency.
<R> <C> Model <C> Learning Rate <C> Batch Size <C> Epochs <C> Dropout <C> Recurrent Dropout <C> Filter Size <C> Kernel Size <C> Optimizer <C> Average Precision <C> Average Recall <C> Average  [ITALIC] F1 score <C> CPU Time (sec) <R> <C> CNN <C> 0.0079 <C> 10 <C> 1 <C> – <C> – <C> 16 <C> 2 <C> Adam <C> 0.75 <C> 0.73 <C> [BOLD] 0.74 <C> [BOLD] 503.82 <R> <C> CNN <C> 0.01 <C> 50 <C> 2 <C> – <C> – <C> 16 <C> 2 <C> Adagrad <C> 0.73 <C> 0.71 <C> 0.72 <C> 522.47 <R> <C> CNN <C> 0.0063 <C> 10 <C> 3 <C> – <C> – <C> 16 <C> 2 <C> Adam <C> 0.73 <C> 0.71 <C> 0.72 <C> 553.43 <R> <C> LSTM <C> 0.0002 <C> 10 <C> 10 <C> 0.4 <C> 0.2 <C> – <C> – <C> Adam <C> 0.7597 <C> 0.7475 <C> [BOLD] 0.7534 <C> 4241.97 <R> <C> LSTM <C> 0.0002 <C> 20 <C> 8 <C> 0.2 <C> 0.6 <C> – <C> – <C> Adam <C> 0.7597 <C> 0.7468 <C> 0.7530 <C> [BOLD] 4100.37 <R> <C> LSTM <C> 0.0006 <C> 100 <C> 12 <C> 0.6 <C> 0.6 <C> – <C> – <C> Adam <C> 0.7559 <C> 0.7431 <C> 0.7493 <C> 4209.37 <R> <C> RNN <C> 0.0001 <C> 10 <C> 7 <C> 0.0 <C> 0.2 <C> – <C> – <C> Adam <C> 0.7037 <C> 0.6957 <C> [BOLD] 0.6996 <C> 3069.81 <R> <C> RNN <C> 0.0001 <C> 20 <C> 5 <C> 0.0 <C> 0.0 <C> – <C> – <C> Adam <C> 0.7028 <C> 0.6921 <C> 0.6973 <C> [BOLD] 2805.52 <R> <C> RNN <C> 0.0001 <C> 100 <C> 12 <C> 0.0 <C> 0.2 <C> – <C> – <C> Adam <C> 0.70 <C> 0.69 <C> 0.69 <C> 3160.35 <CAP> Table 1: Average precision, recall, F1 score, and CPU time for the top three performing hyperparameter combinations on each of the CNN, LSTM, and RNN models. Bold numbers correspond to the highest F1 scores and lowest CPU times for each of the three model types. We report the recall, precision, and F1 score to four decimal places (when necessary) to distinguish the average F1 scores.
<R> <C> Model <C> Average Precision <C> Average Recall <C> Average  [ITALIC] F1 score <C> CPU Time (sec) <R> <C> CNN <C> 0.74 <C> 0.73 <C> 0.73 <C> [BOLD] 501.10 <R> <C> LSTM <C> 0.76 <C> 0.74 <C> [BOLD] 0.75 <C> 4211.01 <R> <C> RNN <C> 0.70 <C> 0.69 <C> 0.70 <C> 3085.59 <CAP> Table 2: Testing results with the optimal hyperparameter combinations for the CNN, LSTM, and RNN models. The bold numbers correspond to the highest F1 score and lowest CPU time among the three models.
<R> <C> [BOLD] NLG Model (a) <C> [BOLD] NLG Model Sequence-to-Sequence Model <C> [BOLD] BLEU 28.9 <C> [BOLD] ROUGE-1 40.7 <C> [BOLD] ROUGE-2 12.5 <C> [BOLD] ROUGE-L 32.1 <R> <C> (b) <C> + Hierarchical Decoder <C> 43.1 <C> 53.0 <C> 24.6 <C> 40.4 <R> <C> (c) <C> + Hierarchical Decoder, Repeat-Input <C> 42.3 <C> 52.9 <C> 24.0 <C> 40.1 <R> <C> (d) <C> + Hierarchical Decoder, Curriculum Learning <C> 58.4 <C> 60.4 <C> 30.6 <C> 44.6 <R> <C> (e) <C> + All <C> [BOLD] 58.7 <C> [BOLD] 62.3 <C> [BOLD] 31.6 <C> [BOLD] 45.4 <R> <C> (f) <C> (e) with High Inner-Layer TF Prob. <C> [BOLD] 62.1 <C> [BOLD] 64.0 <C> [BOLD] 32.8 <C> [BOLD] 46.0 <R> <C> (g) <C> (e) with High Inter-Layer TF Prob. <C> 56.7 <C> 61.3 <C> 30.9 <C> 44.6 <R> <C> (h) <C> (e) with High Inner- and Inter-Layer TF Prob. <C> 60.0 <C> 63.0 <C> 31.8 <C> 45.2 <CAP> Table 1: The NLG performance reported on BLEU, ROUGE-1, ROUGE-2, and ROUGE-L of models (%).
<R> <C> Model <C> Beam Size <C> BLEU NIST03 <C> BLEU NIST04 <C> BLEU NIST05 <C> BLEU Averaged <C> Degeneration <C> Lattency <C> Speedup <R> <C> Transformer <C> 4 <C> 40.74 <C> 40.54 <C> 40.48 <C> 40.59 <C> 0% <C> 410ms <C> 1.00× <R> <C> Transformer <C> 1 <C> 39.56 <C> 39.72 <C> 39.61 <C> 39.63 <C> 2% <C> 302ms <C> 1.36× <R> <C> Transformer,  [ITALIC] N=2 <C> 4 <C> 37.30 <C> 38.55 <C> 36.87 <C> 37.57 <C> 7% <C> 169ms <C> 2.43× <R> <C> Transformer,  [ITALIC] N=2 <C> 1 <C> 36.26 <C> 37.19 <C> 35.50 <C> 36.32 <C> 11% <C> 117ms <C> 3.50× <R> <C> [ITALIC] This Work <C> [ITALIC] This Work <C> [ITALIC] This Work <C> [ITALIC] This Work <C> [ITALIC] This Work <C> [ITALIC] This Work <C> [ITALIC] This Work <C> [ITALIC] This Work <C> [ITALIC] This Work <R> <C> SAT,  [ITALIC] K=2 <C> 4 <C> 39.13 <C> 40.04 <C> 39.55 <C> 39.57 <C> 3% <C> 243ms <C> 1.69× <R> <C> SAT,  [ITALIC] K=2 <C> 1 <C> 37.94 <C> 38.73 <C> 38.43 <C> 38.37 <C> 5% <C> 176ms <C> 2.33× <R> <C> SAT,  [ITALIC] K=4 <C> 4 <C> 37.08 <C> 38.06 <C> 37.12 <C> 37.42 <C> 8% <C> 152ms <C> 2.70× <R> <C> SAT,  [ITALIC] K=4 <C> 1 <C> 35.77 <C> 36.43 <C> 35.04 <C> 35.75 <C> 12% <C> 94ms <C> 4.36× <R> <C> SAT,  [ITALIC] K=6 <C> 4 <C> 34.61 <C> 36.29 <C> 35.06 <C> 35.32 <C> 13% <C> 129ms <C> 3.18× <R> <C> SAT,  [ITALIC] K=6 <C> 1 <C> 33.44 <C> 34.54 <C> 33.28 <C> 33.75 <C> 17% <C> 64ms <C> 6.41× <CAP> Table 6: Results on Chinese-English translation. Latency is calculated on NIST02.
<R> <C> [BOLD] Group <C> [BOLD] Layperson 1 <C> [BOLD] Layperson 2 <R> <C> Foreigners / Migrants <C> 159 <C> 112 <R> <C> Other <C> 103 <C> 137 <R> <C> Left wing/Green Party <C> 95 <C> 47 <R> <C> Muslims <C> 84 <C> 62 <R> <C> Other politicians <C> 77 <C> 35 <R> <C> Nationality / Origin <C> 48 <C> 57 <R> <C> Jews <C> 47 <C> 40 <R> <C> Women <C> 29 <C> 26 <R> <C> ‘not sure’ <C> 27 <C> 36 <R> <C> LGBTQ+ <C> 17 <C> 23 <R> <C> Black <C> 15 <C> 14 <R> <C> Disabled/sick <C> 6 <C> 5 <R> <C> Right wing <C> 0 <C> 2 <CAP> Table 2: Distribution for all groups.
<R> <C> Model <C> Accuracy (%) Dev <C> Accuracy (%) Test <R> <C> Random baseline <C> 33.3 <C> 33.3 <R> <C> GPT <C> 63.3 <C> 63.0 <R> <C> BERT-base <C> 63.3 <C> 63.1 <R> <C> BERT-large <C> [BOLD] 66.0 <C> [BOLD] 64.5 <R> <C> w/o context <C> 52.7 <C> – <R> <C> w/o question <C> 52.1 <C> – <R> <C> w/o context, question <C> 45.5 <C> – <R> <C> Human <C> 86.9* <C> 84.4* <CAP> Table 2: Experimental results. We additionally perform an ablation by removing contexts and questions, verifying that both are necessary for BERT-large’s performance. Human evaluation results are obtained using 900 randomly sampled examples.
<R> <C> Task <C> Model <C> Acc. (%) best <C> Acc. (%) mean <C> Acc. (%) std <R> <C> COPA <C> Sasaki2017HandlingME <C> 71.2 <C> – <C> – <R> <C> COPA <C> BERT-large <C> 80.8 <C> 75.0 <C> 3.0 <R> <C> COPA <C> BERT-Social IQa <C> [BOLD] 83.4 <C> 80.1 <C> 2.0 <R> <C> WSC <C> Kocijan2019ASR <C> 72.5 <C> – <C> – <R> <C> WSC <C> BERT-large <C> 67.0 <C> 65.5 <C> 1.0 <R> <C> WSC <C> BERT-Social IQa <C> [BOLD] 72.5 <C> 69.6 <C> 1.7 <R> <C> DPR <C> Peng2015SolvingHC <C> 76.4 <C> – <C> – <R> <C> DPR <C> BERT-large <C> 79.4 <C> 71.2 <C> 3.8 <R> <C> DPR <C> BERT-Social IQa <C> [BOLD] 84.0 <C> 81.7 <C> 1.2 <CAP> Table 4: Sequential finetuning of BERT-large on Social IQa before the task yields state of the art results (bolded) on COPA (Roemmele2011ChoiceOP), Winograd Schema Challenge (Levesque2011TheWS) and DPR (Rahman2012DPR). For comparison, we include previous published state of the art performance.
<R> <C> Sub-module <C> RNMT <C> Trans.(base) <C> Trans.(1layer) <R> <C> -Encoding <C> 72.3 <C> 63.2 <C> 10.6 <R> <C> -Decoding <C> 138.0 <C> 434.1 <C> 170.5 <R> <C> ~{}~{}-Attention <C> 43.3 <C> 99.3 <C> 24.9 <R> <C> ~{}~{}-SelfAtt/GRU <C> 42.9 <C> 152.0 <C> 41.5 <R> <C> ~{}~{}-FFN <C> - <C> 86.1 <C> 19.9 <R> <C> ~{}~{}-Softmax <C> 40.1 <C> 46.7 <C> 45.6 <R> <C> ~{}~{}-Others <C> 11.7 <C> 50.0 <C> 38.6 <R> <C> Total <C> 210.3 <C> 497.3 <C> 181.1 <CAP> Table 1: Time breakdown of RNMT decoding and Transformer decoding, both with beam size 8. Trans.(base) refers to the base Transformer with 6-layer encoder and decoder and Trans.(1layer) refers to the Transformer with 1-layer encoder and decoder. The time is measured on a single Tesla K40 GPU. The “Others” item in decoding contains beam expansion, data transmission etc..
<R> <C> System <C> N <C> Time <C> Speed (w/s) <C> NIST2005 <C> NIST2008 <C> NIST2012 <C> Avg. <R> <C> RNMT <C> 1-1 <C> 279s <C> 441.2 <C> 39.64 <C> 30.93 <C> 29.20 <C> 33.26 <R> <C> +KD <C> 1-1 <C> 278s <C> 441.8 <C> 41.25 <C> 32.90 <C> 31.24 <C> 35.13 <R> <C> Trans.(teacher) <C> 4-4 <C> 652s <C> 187.9 <C> 44.13 <C> 35.55 <C> 33.19 <C> 37.62 <R> <C> Hybrid+additive_attn <C> 4-1 <C> [BOLD] 233s <C> [BOLD] 524.2 <C> 42.10 <C> 33.87 <C> 31.80 <C> 35.92 <R> <C> +KD <C> 4-1 <C> [BOLD] 232s <C> [BOLD] 526.3 <C> 43.75 <C> 35.24 <C> 33.30 <C> 37.43 <R> <C> Hybrid+dot_attn <C> 4-1 <C> 234s <C> 522.8 <C> 42.25 <C> 32.81 <C> 30.66 <C> 35.24 <R> <C> +KD <C> 4-1 <C> 234s <C> 523.1 <C> 44.02 <C> 34.91 <C> 33.12 <C> 37.35 <R> <C> Hybrid+multi_attn <C> 4-1 <C> 235s <C> 523 <C> 42.82 <C> 33.73 <C> 31.95 <C> 36.17 <R> <C> +KD <C> 4-1 <C> 236s <C> 521.1 <C> 44.05 <C> 35.46 <C> 33.29 <C> 37.60 <R> <C> Trans.(teacher) <C> 6-6 <C> 1020s <C> 120.9 <C> 44.64 <C> 36.01 <C> 34.19 <C> 38.28 <R> <C> Hybrid+additive_attn <C> 6-1 <C> 250s <C> 495.8 <C> 42.65 <C> 33.93 <C> 31.75 <C> 36.11 <R> <C> +KD <C> 6-1 <C> 252s <C> 492.3 <C> 44.35 <C> 36.10 <C> 33.91 <C> 38.12 <R> <C> Hybrid+dot_attn <C> 6-1 <C> [BOLD] 249s <C> [BOLD] 491.8 <C> 43.08 <C> 32.83 <C> 31.23 <C> 35.71 <R> <C> +KD <C> 6-1 <C> [BOLD] 250s <C> [BOLD] 492 <C> 44.71 <C> 35.35 <C> 33.34 <C> 37.80 <R> <C> Hybrid+multi_attn <C> 6-1 <C> 253s <C> 486.4 <C> 44.08 <C> 34.31 <C> 31.98 <C> 36.79 <R> <C> +KD <C> 6-1 <C> 251s <C> 490.4 <C> 44.88 <C> 36.04 <C> 33.80 <C> 38.24 <CAP> Table 2: Decoding time and case-insensitive BLEU scores (%) for Chinese-English NIST datasets. “Trans.” is short for Transformer model. “N” refers to the depth of encoder and decoder. “Time” refers to the total decoding time on all the testsets and “Speed(w/s)” denotes the decoding speed measured by word per second. The “Avg.” is short for the average BLEU score.
<R> <C> System <C> N <C> Time <C> Speed(w/s) <C> test <R> <C> RNMT <C> 1-1 <C> 114s <C> 500.6 <C> 20.03 <R> <C> +KD <C> 1-1 <C> 113s <C> 505 <C> 21.03 <R> <C> Trans.(teacher) <C> 4-4 <C> 234s <C> 238.1 <C> 22.57 <R> <C> Hybrid+dot_attn <C> 4-1 <C> [BOLD] 100s <C> [BOLD] 554.8 <C> 21.43 <R> <C> +KD <C> 4-1 <C> [BOLD] 99s <C> [BOLD] 560.5 <C> 22.96 <R> <C> Hybrid+multi_attn <C> 4-1 <C> 105s <C> 540.5 <C> 22.05 <R> <C> +KD <C> 4-1 <C> 104s <C> 545.5 <C> 22.35 <R> <C> Trans.(teacher) <C> 6-6 <C> 420s <C> 130.2 <C> 23.08 <R> <C> Hybrid+dot_attn <C> 6-1 <C> [BOLD] 109s <C> [BOLD] 510.2 <C> 21.63 <R> <C> +KD <C> 6-1 <C> [BOLD] 107s <C> [BOLD] 519.9 <C> 22.50 <R> <C> Hybrid+multi_attn <C> 6-1 <C> 113s <C> 500.0 <C> 21.97 <R> <C> +KD <C> 6-1 <C> 114s <C> 495.7 <C> 22.93 <CAP> Table 3: Decoding time and case-sensitive BLEU scores (%) for WMT 2017 Chinese-English task.
<R> <C> [BOLD] Data <C> [BOLD] Model <C> [BOLD] MUC  [BOLD] P <C> [BOLD] MUC  [BOLD] R <C> [BOLD] MUC  [BOLD] F1 <C> [BOLD] B3  [BOLD] P <C> [BOLD] B3  [BOLD] R <C> [BOLD] B3  [BOLD] F1 <C> [BOLD] CEAF [ITALIC] ϕ4  [BOLD] P <C> [BOLD] CEAF [ITALIC] ϕ4  [BOLD] R <C> [BOLD] CEAF [ITALIC] ϕ4  [BOLD] F1 <C> [BOLD] Avg. F1 <R> <C> conll <C> lee2018higher <C> [BOLD] 81.4 <C> [BOLD] 79.5 <C> [BOLD] 80.4 <C> [BOLD] 72.2 <C> [BOLD] 69.5 <C> [BOLD] 70.8 <C> [BOLD] 68.2 <C> [BOLD] 67.1 <C> [BOLD] 67.6 <C> [BOLD] 73.0 <R> <C> conll <C> + high recall <C> 80.0 <C> [BOLD] 79.5 <C> 79.7 <C> 70.5 <C> [BOLD] 69.5 <C> 70.0 <C> 67.3 <C> 66.9 <C> 67.1 <C> 72.3 <R> <C> conll <C> + high recall + joint <C> 80.9 <C> 79.2 <C> 80.0 <C> 72.0 <C> 69.0 <C> 70.5 <C> 67.7 <C> 66.9 <C> 67.3 <C> 72.6 <R> <C> conll <C> lee2017end <C> 78.4 <C> 73.4 <C> 75.8 <C> 68.6 <C> 61.8 <C> 65.0 <C> 62.7 <C> 59.0 <C> 60.8 <C> 67.2 <R> <C> conll <C> + high recall <C> [BOLD] 78.6 <C> [BOLD] 74.0 <C> [BOLD] 76.2 <C> [BOLD] 68.9 <C> [BOLD] 62.2 <C> [BOLD] 65.4 <C> [BOLD] 63.2 <C> [BOLD] 59.6 <C> [BOLD] 61.4 <C> [BOLD] 67.7 <R> <C> conll <C> clark2016deep <C> 79.2 <C> 70.4 <C> 74.6 <C> 69.9 <C> 58.0 <C> 63.4 <C> 63.5 <C> 55.5 <C> 59.2 <C> 65.7 <R> <C> conll <C> + high recall <C> 78.7 <C> 72.4 <C> 75.4 <C> 69.4 <C> 59.7 <C> 64.2 <C> 62.2 <C> [BOLD] 57.7 <C> 59.9 <C> 66.5 <R> <C> conll <C> + balance <C> [BOLD] 80.3 <C> [BOLD] 72.5 <C> [BOLD] 76.2 <C> [BOLD] 71.2 <C> [BOLD] 60.4 <C> [BOLD] 65.3 <C> [BOLD] 64.6 <C> 57.1 <C> [BOLD] 60.6 <C> [BOLD] 67.4 <R> <C> crac <C> lee2018higher <C> [BOLD] 79.2 <C> 71.9 <C> 75.3 <C> [BOLD] 72.4 <C> 63.5 <C> 67.7 <C> 66.2 <C> 58.6 <C> 62.2 <C> 68.4 <R> <C> crac <C> + high recall <C> 76.2 <C> 73.1 <C> 74.6 <C> 68.4 <C> [BOLD] 65.5 <C> 66.9 <C> 65.1 <C> 61.8 <C> 63.4 <C> 68.3 <R> <C> crac <C> + high recall + joint <C> 77.6 <C> [BOLD] 73.4 <C> [BOLD] 75.4 <C> 70.4 <C> [BOLD] 65.5 <C> [BOLD] 67.9 <C> [BOLD] 66.4 <C> [BOLD] 61.9 <C> [BOLD] 64.1 <C> [BOLD] 69.1 <CAP> Table 5: Comparison between the baselines and the models enhanced by our Biaffine md on the coreference resolution task.
<R> <C> Vocabulary KN3† <C> 10k 90. <C> 10k 4 <C> 22k 125.3 <C> 36k 146.4 <C> 50k 159.9 <R> <C> LBL5 <C> 116. <C> 6 <C> 167.0 <C> 199.5 <C> 220.3 <R> <C> LSTM-s <C> 107. <C> 3 <C> 159.5 <C> 189.4 <C> 222.1 <R> <C> LSTM-z <C> 75. <C> 1 <C> 104.4 <C> 119.6 <C> 130.6 <R> <C> LSTM-z,wb <C> 73. <C> 7 <C> 103.4 <C> 122.9 <C> 138.2 <R> <C> LSTM-z,w <C> 72. <C> 9 <C> 101.9 <C> 119.3 <C> 129.2 <CAP> Table 3: Perplexity of our compressed language models and baselines. †Trained with the full corpus of 1.6 billion running words.
<R> <C> Vocabulary <C> 10k <C> 22k <C> 36k <C> 50k <R> <C> LSTM-z,w <C> 17.76 <C> 59.28 <C> 73.42 <C> 79.75 <R> <C> LSTM-z,wb <C> 17.80 <C> 59.44 <C> 73.61 <C> 79.95 <CAP> Table 4: Memory reduction (%) by our proposed methods in comparison with the uncompressed model LSTM-z. The memory of sparse codes are included.
<R> <C> [EMPTY] <C> People Daily Valid <C> People Daily Test <C> Children’s Fairy Tale Test-auto <C> Children’s Fairy Tale Test-human <R> <C> AS Reader <C> 64.1 <C> 67.2 <C> 40.9 <C> 33.1 <R> <C> CAS Reader (mode: avg) <C> [BOLD] 65.2 <C> [BOLD] 68.1 <C> 41.3 <C> [BOLD] 35.0 <R> <C> CAS Reader (mode: sum) <C> 64.7 <C> 66.8 <C> [BOLD] 43.0 <C> 34.7 <R> <C> CAS Reader (mode: max) <C> 63.3 <C> 65.4 <C> 38.3 <C> 32.0 <CAP> Table 5: Results on People Daily datasets and Children’s Fairy Tale (CFT) datasets.
<R> <C> [BOLD] Map <C> [BOLD] Full <C> [BOLD] Random <C> [BOLD] Majority vote <R> <C> 25% obstacles <C> [BOLD] 0.777 <C> 0.00 <C> 0.168 <R> <C> 50% obstacles <C> [BOLD] 0.687 <C> 0.00 <C> 0.204 <R> <C> 75% obstacles <C> [BOLD] 0.80 <C> 0.00 <C> 0.178 <CAP> Table 1: Accuracy values for Frogger environments with different obstacle densities. Accuracy values for sentences produced by the encoder-decoder network (full) significantly outperform those generated by a random model and a majority classifier as determined by a chi-square test.
<R> <C> [EMPTY] <C> BLEU <C> ROUGE <C> SemSim <C> ParaRank <C> Order Preserve <R> <C> Random <C> 3.1 <C> 6.9 <C> 75.1 <C> 50.0 <C> 50.0 <R> <C> VSE++ (Faghri et al.,  2018 ) <C> 11.0 <C> 9.5 <C> 84.6 <C> 59.1 <C> 55.2 <R> <C> VSE++ ILP <C> 12.56 <C> 11.23 <C> 83.98 <C> 58.08 <C> 47.93 <R> <C> SANDI-CV <C> 18.2 <C> 17.6 <C> 86.3 <C> 63.7 <C> 54.5 <R> <C> SANDI-MAN <C> [BOLD] 45.6 <C> [BOLD] 44.5 <C> [BOLD] 89.8 <C> 72.5 <C> [BOLD] 77.4 <R> <C> SANDI-BD <C> 26.6 <C> 25.1 <C> 84.7 <C> 61.3 <C> 61.2 <R> <C> SANDI∗ <C> 44.3 <C> 42.9 <C> 89.7 <C> [BOLD] 73.2 <C> 76.3 <CAP> Table 3. Complete Alignment on the Asia Exchange dataset.
<R> <C> [EMPTY] <C> BLEU <C> ROUGE <C> SemSim <C> ParaRank <C> Order Preserve <R> <C> Random <C> 6.8 <C> 8.9 <C> 70.8 <C> 50.0 <C> 50.0 <R> <C> VSE++ (Faghri et al.,  2018 ) <C> 19.4 <C> 17.7 <C> 85.7 <C> 51.9 <C> 48.0 <R> <C> VSE++ ILP <C> 23.5 <C> 20.11 <C> 85.98 <C> 52.55 <C> 46.13 <R> <C> SANDI-CV <C> 21.5 <C> 20.6 <C> 87.8 <C> 58.4 <C> 52.0 <R> <C> SANDI-MAN <C> [BOLD] 35.2 <C> [BOLD] 32.2 <C> 89.2 <C> 61.5 <C> 61.5 <R> <C> SANDI-BD <C> 24.1 <C> 22.3 <C> 86.7 <C> 56.0 <C> 53.6 <R> <C> SANDI∗ <C> 33.4 <C> 31.5 <C> [BOLD] 89.7 <C> [BOLD] 62.4 <C> [BOLD] 62.5 <CAP> Table 3. Complete Alignment on the Asia Exchange dataset.
<R> <C> SANDI-CV <C> [ITALIC] SemSim <C> Standard 86.2 <C> +CSK  [BOLD] 86.3 <R> <C> SANDI-CV <C> [ITALIC] ParaRank <C> 59.9 <C> 59.7 <R> <C> SANDI-MAN <C> [ITALIC] SemSim <C> 85.1 <C> [BOLD] 85.5 <R> <C> SANDI-MAN <C> [ITALIC] ParaRank <C> 53.8 <C> [BOLD] 55.0 <CAP> Table 4. Role of Commonsense Knowledge.
<R> <C> Tag Space <C> Precision <C> Random <C> NN <C> VSE++ <C> SANDI <R> <C> CV <C> [ITALIC] Strict <C> 0.4 <C> 2.0 <C> 1.14 <C> [BOLD] 4.18 <R> <C> CV <C> [ITALIC] Relaxed <C> 42.16 <C> 52.68 <C> 29.83 <C> [BOLD] 53.54 <R> <C> MAN <C> [ITALIC] Strict <C> 0.4 <C> 3.95 <C> - <C> [BOLD] 14.57 <R> <C> MAN <C> [ITALIC] Relaxed <C> 37.14 <C> 42.73 <C> [EMPTY] <C> [BOLD] 49.65 <R> <C> BD <C> [ITALIC] Strict <C> 0.4 <C> 1.75 <C> - <C> [BOLD] 2.71 <R> <C> BD <C> [ITALIC] Relaxed <C> 32.59 <C> 37.94 <C> [EMPTY] <C> [BOLD] 38.86 <R> <C> ∗ <C> [ITALIC] Strict <C> 0.4 <C> 4.8 <C> - <C> [BOLD] 11.28 <R> <C> ∗ <C> [ITALIC] relaxed <C> 43.84 <C> 50.06 <C> [EMPTY] <C> [BOLD] 54.34 <CAP> Table 6. Image Selection on the Asia Exchange dataset.
<R> <C> Tag Space <C> Precision <C> Random <C> NN <C> VSE++ <C> SANDI <R> <C> CV <C> [ITALIC] Strict <C> 0.45 <C> 0.65 <C> 0.44 <C> [BOLD] 0.79 <R> <C> CV <C> [ITALIC] Relaxed <C> 55.0 <C> [BOLD] 57.64 <C> 30.05 <C> 57.2 <R> <C> MAN <C> [ITALIC] Strict <C> 0.45 <C> 0.78 <C> - <C> [BOLD] 3.42 <R> <C> MAN <C> [ITALIC] Relaxed <C> 40.24 <C> 52.0 <C> [EMPTY] <C> [BOLD] 52.87 <R> <C> BD <C> [ITALIC] Strict <C> 0.45 <C> 0.82 <C> - <C> [BOLD] 0.87 <R> <C> BD <C> [ITALIC] Relaxed <C> 31.12 <C> [BOLD] 33.27 <C> [EMPTY] <C> 33.25 <R> <C> ∗ <C> [ITALIC] Strict <C> 0.45 <C> 1.04 <C> - <C> [BOLD] 1.7 <R> <C> ∗ <C> [ITALIC] relaxed <C> 55.68 <C> 58.1 <C> [EMPTY] <C> [BOLD] 58.2 <CAP> Table 6. Image Selection on the Asia Exchange dataset.
<R> <C> models <C> Random <C> CTC <C> CE <R> <C> 1600p800_4x6 <C> 10.55 <C> 10.40 <C> 9.33 <CAP> Table 2: WERs of initialization methods for RNN-T.
<R> <C> set <C> Lex <C> Lex+Brown <C> Lex+W2V <C> Lex+LDS <R> <C> dev <C> 93.90 <C> 93.79 <C> 94.14 <C> 94.21 <R> <C> test <C> 89.34 <C> 89.76 <C> 90.00 <C> 89.9 <CAP> Table 3: NER with various unsupervised token features
<R> <C> [EMPTY] <C> Baseline <C> LDS <R> <C> dev <C> 130.6 <C> 128.1 <R> <C> test <C> 124.0 <C> 122.8 <CAP> Table 4: Final perplexity for an RNN language model trained using random parameter initialization vs. LDS initialization.
<R> <C> ID <C> Model <C> stride <C> Size(MB) <C> WER (%) <R> <C> exp1 <C> DFSMN(6) <C> 1 <C> 104 <C> 10.7 <R> <C> exp2 <C> DFSMN(6) <C> 2 <C> 104 <C> 10.3 <R> <C> exp3 <C> DFSMN(8) <C> 2 <C> 120 <C> 9.6 <R> <C> exp4 <C> DFSMN(10) <C> 2 <C> 136 <C> 9.5 <R> <C> exp5 <C> DFSMN(12) <C> 2 <C> 152 <C> 9.4 <CAP> Table 2: Performance (WER%) of DFSMN based acoustic model with various architectures.
<R> <C> Model <C> [ITALIC] N2 <C> Delay Frame <C> CER% <C> Gain <R> <C> LFR-LCBLSTM <C> - <C> 40 <C> 16.05 <C> - <R> <C> [EMPTY] <C> 2 <C> 20 <C> 12.67 <C> +21.06% <R> <C> LFR-DFSMN(10) <C> 1 <C> 10 <C> 12.94 <C> +19.38% <R> <C> [EMPTY] <C> 1 and 0 <C> 5 <C> 13.38 <C> +16.64% <CAP> Table 5: Comparison of various acoustic models on 20000-hour-task.(“1 and 0” denotes the lookahead filter order of the odd layer and even layer is 1 and 0 respectively).
<R> <C> [ITALIC] Drepr [BOLD]  to  [ITALIC] Qrepr interaction <C> [BOLD] NE  [BOLD] Dev <C> [BOLD] NE  [BOLD] Test <C> [BOLD] CN  [BOLD] Dev <C> [BOLD] CN  [BOLD] Test <R> <C> [ITALIC] Dctx,  [ITALIC] Qctx (w/o know) <C> 75.50 <C> 70.30 <C> 68.20 <C> 64.80 <R> <C> [ITALIC] Dctx+ [ITALIC] kn,  [ITALIC] Qctx+ [ITALIC] kn <C> 76.45 <C> 69.68 <C> 70.85 <C> 66.32 <R> <C> [ITALIC] Dctx,  [ITALIC] Qctx+ [ITALIC] kn <C> [BOLD] 77.10 <C> 69.72 <C> 70.80 <C> 66.32 <R> <C> [ITALIC] Dctx+ [ITALIC] kn,  [ITALIC] Q \definecolor [ [ITALIC] named] [ITALIC] pgfstrokecolorrgb0,0,0 \pgfsys @ [ITALIC] color@ [ITALIC] gray@ [ITALIC] stroke0 \pgfsys @ [ITALIC] color@ [ITALIC] gray@ [ITALIC] fill0 [ITALIC] ctx <C> 75.65 <C> [BOLD] 70.88 <C> 71.20 <C> [BOLD] 67.96 <R> <C> Full model <C> 76.80 <C> 70.24 <C> [BOLD] 71.85 <C> 67.64 <R> <C> w/o  [ITALIC] Dctx,  [ITALIC] Qctx <C> 75.95 <C> 70.24 <C> 70.65 <C> 67.12 <R> <C> w/o  [ITALIC] Dctx+ [ITALIC] kn,  [ITALIC] Qctx+ [ITALIC] kn <C> 76.20 <C> 69.80 <C> 70.75 <C> 67.00 <R> <C> w/o  [ITALIC] Dctx,  [ITALIC] Qctx+ [ITALIC] kn <C> 76.55 <C> 70.52 <C> 71.75 <C> 66.32 <R> <C> w/o  [ITALIC] Dctx+ [ITALIC] kn,  [ITALIC] Q \definecolor [ [ITALIC] named] [ITALIC] pgfstrokecolorrgb0,0,0 \pgfsys @ [ITALIC] color@ [ITALIC] gray@ [ITALIC] stroke0 \pgfsys @ [ITALIC] color@ [ITALIC] gray@ [ITALIC] fill0 [ITALIC] ctx <C> 76.05 <C> 70.84 <C> 70.80 <C> 66.80 <CAP> Table 4: Results for different combinations of interactions between document (D) and question (Q) context (ctx) and context + knowledge (ctx+kn) representations. (CN5Sel, 50 facts)
<R> <C> [ITALIC] Drepr [BOLD]  to  [ITALIC] Qrepr interaction <C> [BOLD] NE  [BOLD] Dev <C> [BOLD] NE  [BOLD] Test <C> [BOLD] CN  [BOLD] Dev <C> [BOLD] CN  [BOLD] Test <R> <C> [ITALIC] Dctx,  [ITALIC] Qctx (w/o know) <C> 75.50 <C> 70.30 <C> 68.20 <C> 64.80 <R> <C> [ITALIC] Dctx+ [ITALIC] kn,  [ITALIC] Qctx+ [ITALIC] kn <C> 75.50 <C> 70.28 <C> 69.80 <C> 65.60 <R> <C> [ITALIC] Dctx,  [ITALIC] Qctx+ [ITALIC] kn <C> 74.20 <C> 69.88 <C> 70.40 <C> 66.56 <R> <C> [ITALIC] Dctx+ [ITALIC] kn,  [ITALIC] Qctx <C> [BOLD] 77.40 <C> 71.40 <C> 70.95 <C> 67.52 <R> <C> All <C> 76.65 <C> [BOLD] 71.52 <C> 70.80 <C> 67.08 <R> <C> w/o  [ITALIC] Dctx,  [ITALIC] Qctx <C> 76.70 <C> 70.68 <C> [BOLD] 71.10 <C> [BOLD] 67.68 <R> <C> w/o  [ITALIC] Dctx+ [ITALIC] kn,  [ITALIC] Qctx+ [ITALIC] kn <C> 76.35 <C> 70.88 <C> 70.95 <C> 67.44 <R> <C> w/o  [ITALIC] Dctx,  [ITALIC] Qctx+ [ITALIC] kn <C> 76.90 <C> 71.32 <C> 70.70 <C> 67.12 <R> <C> w/o  [ITALIC] Dctx+ [ITALIC] kn,  [ITALIC] Qctx <C> 76.50 <C> 70.64 <C> 70.75 <C> 66.88 <CAP> Table 7: Results for different combinations of interactions between document (D) and question (Q) context (ctx) and context + knowledge (ctx+kn) representations. (Subj/Obj, 100 facts)
<R> <C> Model <C> en → de <C> de → en <R> <C> Majority Class <C> 46.8 <C> 46.8 <R> <C> Glossed <C> 65.1 <C> 68.6 <R> <C> MT <C> 68.1 <C> 67.4 <R> <C> I-Matrix <C> 77.6 <C> 71.1 <R> <C> [ITALIC] dim=40 <C> [EMPTY] <C> [EMPTY] <R> <C> Add <C> 83.7 <C> 71.4 <R> <C> Add+ <C> 86.2 <C> 76.9 <R> <C> Bi <C> 83.4 <C> 69.2 <R> <C> Bi+ <C> 86.9 <C> 74.3 <R> <C> [ITALIC] dim=128 <C> [EMPTY] <C> [EMPTY] <R> <C> Add <C> 86.4 <C> 74.7 <R> <C> Add+ <C> 87.7 <C> 77.5 <R> <C> Bi <C> 86.1 <C> 79.0 <R> <C> Bi+ <C> [BOLD] 88.1 <C> [BOLD] 79.2 <CAP> Table 1: Classification accuracy for training on English and German with 1000 labeled examples on the RCV corpus. Cross-lingual compositional representations (Add, Bi and their multilingual extensions), I-Matrix [Klementiev et al.2012] translated (MT) and glossed (Glossed) word baselines, and the majority class baseline. The baseline results are from Klementiev:2012.
<R> <C> [EMPTY] <C> [BOLD] Model <C> [BOLD] Pearson ×100 @10 <C> [BOLD] Pearson ×100 @30 <C> [BOLD] Pearson ×100 @50 <C> [BOLD] Pearson ×100 all <C> [ITALIC] ρ×100 all <C> [BOLD] nDCG (proxy) @3 <C> [BOLD] nDCG (proxy) @10 <C> [BOLD] nDCG (proxy) @20 <C> [BOLD] nDCG (human) @3 <C> [BOLD] nDCG (human) @10 <C> [BOLD] nDCG (human) @20 <R> <C> [BOLD] Baselines <C> WLM <C> 27.6 <C> 28.3 <C> 24.0 <C> 19.4 <C> 12.1 <C> 0.63 <C> 0.59 <C> 0.62 <C> 0.50 <C> 0.46 <C> 0.52 <R> <C> [BOLD] Baselines <C> RankSVM <C> 28.5 <C> 34.7 <C> 31.4 <C> 20.7 <C> 27.5 <C> 0.65 <C> 0.61 <C> 0.64 <C> 0.52 <C> 0.61 <C> 0.65 <R> <C> [BOLD] Baselines <C> Entity2Vec <C> 18.6 <C> 22.0 <C> 21.8 <C> 20.5 <C> 18.7 <C> 0.62 <C> 0.60 <C> 0.61 <C> 0.54 <C> 0.53 <C> 0.54 <R> <C> [BOLD] Baselines <C> DeepWalk <C> 31.3 <C> 30.9 <C> 21.4 <C> 17.6 <C> 10.1 <C> 0.41 <C> 0.43 <C> 0.47 <C> 0.34 <C> 0.38 <C> 0.45 <R> <C> [BOLD] Baselines <C> ParaVecs-DBOW <C> 18.6 <C> 22.0 <C> 21.8 <C> 20.5 <C> 16.0 <C> 0.62 <C> 0.60 <C> 0.61 <C> 0.50 <C> 0.50 <C> 0.55 <R> <C> [BOLD] Baselines <C> ParaVecs-DM <C> 19.0 <C> 23.0 <C> 23.2 <C> 22.3 <C> 18.3 <C> 0.66 <C> 0.63 <C> 0.63 <C> 0.49 <C> 0.52 <C> 0.58 <R> <C> [BOLD] Model Ablation <C> TS-CNN <C> 51.9 <C> 51.0 <C> 43.0 <C> 35.8 <C> 26.5 <C> 0.41 <C> 0.43 <C> 0.47 <C> 0.40 <C> 0.43 <C> 0.48 <R> <C> [BOLD] Model Ablation <C> TS-CNN-Att ( [BOLD] Base) <C> 57.9 <C> 49.7 <C> 44.7 <C> 37.1 <C> 24.9 <C> 0.43 <C> 0.44 <C> 0.49 <C> 0.38 <C> 0.45 <C> 0.50 <R> <C> [BOLD] Model Ablation <C> Base+PV <C> 60.6 <C> 44.2 <C> 41.4 <C> 36.4 <C> 11.2 <C> 0.41 <C> 0.43 <C> 0.47 <C> 0.49 <C> 0.51 <C> 0.55 <R> <C> [BOLD] Model Ablation <C> Base+DW <C> 43.5 <C> 36.5 <C> 35.7 <C> 32.7 <C> 31.0 <C> 0.44 <C> 0.48 <C> 0.53 <C> 0.47 <C> 0.51 <C> 0.52 <R> <C> [BOLD] Model Ablation <C> Base+PV+DW <C> 56.9 <C> 46.1 <C> 43.4 <C> 32.9 <C> 28,4 <C> 0.41 <C> 0.44 <C> 0.48 <C> 0.49 <C> 0.54 <C> 0.57 <R> <C> [BOLD] Model Ablation <C> [ITALIC] ContentEmb+ [ITALIC] GraphEmb <C> 48.9 <C> 40.1 <C> 49.9 <C> 37.5 <C> 27.9 <C> 0.67 <C> 0.62 <C> 0.70 <C> 0.61 <C> 0.69 <C> 0.65 <R> <C> [BOLD] Model Ablation <C> Base+ [ITALIC] ContentEmb <C> [BOLD] 67.1 <C> 54.2 <C> [BOLD] 53.4 <C> 43.7 <C> 26.5 <C> 0.67 <C> 0.69 <C> 0.71 <C> 0.61 <C> 0.72 <C> 0.74 <R> <C> [BOLD] Model Ablation <C> Base+ [ITALIC] GraphEmb <C> 55.2 <C> 50.2 <C> 41.3 <C> 31.5 <C> 35.5 <C> 0.71 <C> 0.75 <C> 0.78 <C> 0.65∓ <C> 0.78∓ <C> 0.81∓ <R> <C> [EMPTY] <C> [BOLD] Trio <C> 58.6 <C> [BOLD] 54.3 <C> 50.2 <C> [BOLD] 45.4 <C> [BOLD] 43.5 <C> [BOLD] 0.75 <C> [BOLD] 0.78 <C> [BOLD] 0.83 <C> [BOLD] 0.74∓ <C> [BOLD] 0.82∓ <C> [BOLD] 0.85∓ <CAP> Table 3: Performance of different models on task (1) Pearson, Spearman’s ρ ranking correlation, and task (2) recommendation (measured by nDCG). Bold and underlined numbers indicate best and second-to-best results. ∓ shows statistical significant over WLM (p<0.05).
<R> <C> [EMPTY] <C> P@10 <C> P@50 <C> P@100 <R> <C> Brier score <C> 60 <C> 64 <C> 62 <R> <C> Text-based (LR) <C> 70 <C> 70 <C> 65 <R> <C> Text-based (CNN) <C> 90 <C> 68 <C> 64 <R> <C> Text-based (BERT-base) <C> 80 <C> 70 <C> 67 <CAP> Table 5: Precision@N of identifying skilled forecasters based on their first prediction.
<R> <C> [EMPTY] <C> [ITALIC] K <C> 1000 <C> 2000 <C> 3000 <C> 5000 <R> <C> LR <C> Bag-of-ngrams <C> 63.9 <C> 62.5 <C> 61.9 <C> 59.3 <R> <C> LR <C> Linguistic <C> 56.3 <C> 59.2 <C> 55.4 <C> 55.5 <R> <C> LR <C> All above <C> 64.3 <C> 64.1 <C> 61.5 <C> 59.7 <R> <C> Neural <C> CNN <C> 66.7 <C> 67.8 <C> 64.7 <C> 64.0 <R> <C> Neural <C> BERT-base <C> 70.8 <C> 66.7 <C> 65.8 <C> 64.4 <CAP> Table 6: Accuracy (%) for classifying accurate predictions when using top K and bottom K analysts’ predictions. We choose n-gram sizes to be 1 and 2. All reported results are on the test set.
<R> <C> [EMPTY] <C> NLM <C> GD <C> GD+NLM <C> TransE <C> TransE+NLM <C> NTN <C> NTN+NLM <R> <C> Knowledge Base <C> - <C> - <C> - <C> 82.87 <C> [BOLD] 83.10 <C> 80.95 <C> 81.27 <R> <C> Analogy Test <C> [BOLD] 42 <C> 41 <C> 41 <C> 37 <C> 38 <C> 36 <C> 41 <R> <C> Parsing <C> 76.03 <C> 75.90 <C> [BOLD] 76.18 <C> 75.86 <C> 76.01 <C> 75.85 <C> 76.14 <CAP> Table 1: Results summary: Accuracy on knowledge base completion, MaxDiff accuracy on Analogy Test, and Label Arc Score Accuracy on Dependency Parsing for single- and joint-objective models.
<R> <C> [ITALIC] Query: Britain’s biggest mortgage lender says that average house prices fell 3.6 percent in September, but analysts believe the market isn’t that weak.  [BOLD] Embedder <C> [ITALIC] Query: Britain’s biggest mortgage lender says that average house prices fell 3.6 percent in September, but analysts believe the market isn’t that weak.  [BOLD] Rank <C> [ITALIC] Query: Britain’s biggest mortgage lender says that average house prices fell 3.6 percent in September, but analysts believe the market isn’t that weak.  [BOLD] Sentence <R> <C> all embedders <C> ≤5 <C> Average house prices in Britain fell 3.6 percent in September from a month earlier, the country’s biggest mortgage lender said Thursday, although analysts believe the market isn’t that weak. <R> <C> bert-base-cls <C> 6 <C> Some analysts say that the December data indicate that consumer spending remains weak, making it harder for the economy to keep a sustained rebound. <R> <C> bert-ft-cls <C> 2 <C> Japanese consumer prices fell for 13th straight month in March, though the GDP data suggests that deflationary pressures are starting to ease. <R> <C> bert-ft-avg <C> 5 <C> An industry group says German machinery orders were down 3 percent on the year in January but foreign demand is improving. <R> <C> fasttext-cc-sub <C> 6 <C> It cautioned however that the economic situation abroad could still slow Sweden’s recovery, and said the country’s gross domestic product (GDP) would grow just 3.6 percent in 2011, down from its May estimate of 3.7 percent growth. <R> <C> glove-840b-300d <C> 12 <C> Meanwhile, Australia’s central bank left its key interest rate unchanged at 3.75 percent on Tuesday, surprising investors and analysts who had predicted the bank would continue raising the rate as the nation’s economy rebounds. <R> <C> gpt-last <C> 8 <C> The economy has since rebound and grew 8.9 percent year-on-year in the second quarter, the central bank said last month, with growth expected to exceed six percent in the full year. <CAP> Table 2: Popular and outlier near neighbors for the given query (top). The first sentence is in the 5-nearest neighborhood for all embedders; the remaining sentences are highly-ranked by the given embedder and outside the 50-nearest neighborhood for all other embedders.
<R> <C> [BOLD] Model <C> [BOLD] Training Set Size 500 <C> [BOLD] Training Set Size 2,000 <C> [BOLD] Training Set Size 5,000 <C> [BOLD] Training Set Size full set <R> <C> RNN <C> 75.3 <C> 83.7 <C> 86.1 <C> 87.4 <R> <C> +EDA <C> 79.1 <C> 84.4 <C> 87.3 <C> 88.3 <R> <C> CNN <C> 78.6 <C> 85.6 <C> 87.7 <C> 88.3 <R> <C> +EDA <C> 80.7 <C> 86.4 <C> 88.3 <C> 88.8 <R> <C> [ITALIC] Average <C> 76.9 <C> 84.6 <C> 86.9 <C> 87.8 <R> <C> +EDA <C> 79.9 <C> 85.4 <C> 87.8 <C> [BOLD] 88.6 <CAP> Table 2: Average performances (%) across five text classification tasks for models with and without EDA on different training set sizes.
<R> <C> [BOLD] System <C> [BOLD] ‘wh’ <C> [BOLD] ‘or’ <C> [BOLD] compound <C> [BOLD] Complex <R> <C> [EMPTY] <C> [BOLD] questions <C> [BOLD] questions <C> [BOLD] nouns <C> [EMPTY] <R> <C> Baseline <C> 64.35 <C> 56.89 <C> 70.05 <C> 68.84 <R> <C> Syntactic <C> 68.39 <C> 66.04 <C> 70.46 <C> 71.36 <R> <C> BERT <C> 71.26 <C> 73.15 <C> [BOLD] 71.25 <C> [BOLD] 75.05 <R> <C> BERT Syntactic <C> [BOLD] 72.25 <C> [BOLD] 78.15 <C> 70.13 <C> 73.69 <CAP> Table 2: Prosody evaluation breakdown by categories on CPE
<R> <C> [BOLD] Word <C> [BOLD] NonN <C> [BOLD] LN <C> [BOLD] MN <C> [BOLD] LMN <R> <C> , <C> 19 <C> [BOLD] 25 <C> 22 <C> 18 <R> <C> . <C> 44 <C> 39 <C> [BOLD] 46 <C> 41 <R> <C> a <C> 21 <C> 12 <C> 16 <C> [BOLD] 24 <R> <C> am <C> 3 <C> 5 <C> 4 <C> [BOLD] 6 <R> <C> at <C> 2 <C> 2 <C> 2 <C> [BOLD] 4 <R> <C> do <C> 9 <C> 9 <C> 7 <C> [BOLD] 11 <R> <C> for <C> [BOLD] 9 <C> 8 <C> 7 <C> [BOLD] 9 <R> <C> have <C> 12 <C> [BOLD] 15 <C> 12 <C> 12 <R> <C> I <C> 38 <C> 32 <C> [BOLD] 42 <C> 34 <R> <C> my <C> 4 <C> [BOLD] 6 <C> 2 <C> 3 <R> <C> so <C> 6 <C> 2 <C> 5 <C> [BOLD] 8 <R> <C> to <C> 26 <C> 30 <C> [BOLD] 34 <C> 31 <R> <C> what <C> 3 <C> 4 <C> 4 <C> [BOLD] 6 <R> <C> will <C> 5 <C> 4 <C> 5 <C> [BOLD] 6 <R> <C> with <C> 4 <C> [BOLD] 5 <C> 4 <C> 3 <R> <C> you <C> 10 <C> [BOLD] 13 <C> [BOLD] 13 <C> 12 <R> <C> [BOLD] SUM <C> 215 <C> 211 <C> 225 <C> [BOLD] 228 <CAP> TABLE VIII: Some examples about the distribution of bigrams on Lenovo dataset. Word indicates the first word of the bigram. The numbers in the table indicate the frequency of bigram generated from different normalization methods. The last row is the sum value of the above rows.
<R> <C> Method <C> P <C> R <C> [ITALIC] F1 <R> <C> Vectors (TransE) <C> 0.28 <C> 0.14 <C> 0.18 <R> <C> Vectors (DistMult) <C> 0.44 <C> 0.41 <C> 0.42 <R> <C> Angles <C> 0.48 <C> 0.43 <C> 0.45 <R> <C> Ours <C> [BOLD] 0.65 <C> [BOLD] 0.50 <C> [BOLD] 0.57 <CAP> Table 4: The experiment results on the toy dataset show that our metric based on probability distribution significantly outperforms other relation similarity metrics.
<R> <C> [EMPTY] <C> Model <C> P <C> R <C> [ITALIC] F1 <R> <C> Traditional <C> Patterns <C> [BOLD] 86.9 <C> 23.2 <C> 36.6 <R> <C> [EMPTY] <C> LR <C> 73.5 <C> 49.9 <C> 59.4 <R> <C> Neural <C> CNN <C> 75.6 <C> 47.5 <C> 58.3 <R> <C> [EMPTY] <C> CNN-PE <C> 70.3 <C> 54.2 <C> 61.2 <R> <C> [EMPTY] <C> SDP-LSTM Xu et al. ( 2015 ) <C> 66.3 <C> 52.7 <C> 58.7 <R> <C> [EMPTY] <C> LSTM <C> 65.7 <C> 59.9 <C> 62.7 <R> <C> [EMPTY] <C> PA-LSTM Zhang et al. ( 2017 ) <C> 65.7 <C> 64.5 <C> 65.1 <R> <C> Neural+Ours <C> PA-LSTM (Softmax-Margin Loss) <C> 68.5 <C> [BOLD] 64.7 <C> [BOLD] 66.6 <CAP> Table 5: Improvement of using similarity in softmax-margin loss.
<R> <C> system <C> BLEU <C> HTER <C> mTER <R> <C> PBSY <C> 25.3 <C> 28.0 <C> 21.8 <R> <C> HPB <C> 24.6 <C> 29.9 <C> 23.4 <R> <C> SPB <C> 25.8 <C> 29.0 <C> 22.7 <R> <C> NMT <C> 31.1∗ <C> 21.1∗ <C> 16.2∗ <CAP> Table 2: Overall results on the HE Set: BLEU, computed against the original reference translation, and TER, computed with respect to the targeted post-edit (HTER) and multiple post-edits (mTER).
<R> <C> system <C> HTERnoShft word <C> HTERnoShft lemma <C> %Δ <R> <C> PBSY <C> 27.1 <C> 22.5 <C> -16.9 <R> <C> HPB <C> 28.7 <C> 23.5 <C> -18.4 <R> <C> SPB <C> 28.3 <C> 23.2 <C> -18.0 <R> <C> NMT <C> 21.7∗ <C> 18.7∗ <C> -13.7 <CAP> Table 3: HTER ignoring shift operations computed on words and corresponding lemmas, and their % difference.
<R> <C> [EMPTY] <C> Q <C> S+Q <C> V+Q img <C> V+Q reg <C> V+Q cpt <C> S+V+Q img <C> S+V+Q reg <C> S+V+Q cpt <R> <C> what (55.62%) <C> 44.11 <C> 62.29 <C> 44.96 <C> 45.93 <C> 47.44 <C> 63.88 <C> 65.28 <C> 66.05 <R> <C> who (11.55%) <C> 36.55 <C> 68.33 <C> 35.75 <C> 34.85 <C> 34.68 <C> 67.76 <C> 67.20 <C> 67.99 <R> <C> where (11.67%) <C> 42.58 <C> 56.97 <C> 47.13 <C> 48.43 <C> 48.20 <C> 61.97 <C> 63.71 <C> 61.46 <R> <C> how (8.98%) <C> 41.17 <C> 71.97 <C> 41.17 <C> 42.41 <C> 40.95 <C> 71.17 <C> 70.80 <C> 71.53 <R> <C> why (10.38%) <C> 45.23 <C> 78.65 <C> 46.05 <C> 45.36 <C> 45.48 <C> 78.33 <C> 77.13 <C> 78.77 <R> <C> other (1.80%) <C> 36.50 <C> 74.45 <C> 37.23 <C> 36.50 <C> 33.58 <C> 73.72 <C> 72.63 <C> 74.09 <R> <C> all (100%) <C> 42.77 <C> 65.15 <C> 43.78 <C> 44.40 <C> 45.03 <C> 66.44 <C> 67.17 <C> 67.70 <CAP> Table 7: Accuracy of each question type using different models (w/ ts) on TVQA Validation set. Q = Question, S = Subtitle, V = Video, img = ImageNet features, reg = regional visual features, cpt = visual concept features. The percentage of each question type is shown in brackets.
<R> <C> Method <C> N.A. Src. <C> Video Feature <C> Val Accuracy w/o ts <C> Val Accuracy w/ ts <R> <C> V+Q <C> Rand <C> cpt <C> 84.64 <C> 85.01 <R> <C> S+Q <C> Rand <C> - <C> 90.94 <C> 90.72 <R> <C> S+V+Q <C> Rand <C> cpt <C> 91.55 <C> 92.00 <R> <C> V+Q <C> Human <C> cpt <C> 43.03 <C> 45.03 <R> <C> S+Q <C> Human <C> - <C> 62.99 <C> 65.15 <R> <C> S+V+Q <C> Human <C> cpt <C> 64.70 <C> 67.70 <CAP> Table 8: Accuracy on TVQA validation set with negative answers collected using different strategies. Negative Answer Source (N.A. Src.) indicates the collection method of the negative answers. Q = Question, S = Subtitle, V = Video, cpt = visual concept features, ts = timestamp annotation. All the experiments are conducted using the proposed multi-stream neural model.
<R> <C> Method <C> Method <C> dim <C> English across <C> English within <C> Tsonga across <C> Tsonga within <R> <C> (1) <C> Baseline <C> 13 <C> 28.1 <C> 15.6 <C> 33.8 <C> 19.1 <R> <C> (2) <C> MFCC <C> 39 <C> 28.6 <C> 15.9 <C> 30.8 <C> 16.3 <R> <C> (3) <C> DBM posterior <C> 39 <C> 26.0 <C> 15.7 <C> 29.2 <C> 16.2 <R> <C> (4) <C> BNF-1st, MR-0 <C> 39 <C> 26.8 <C> 16.0 <C> 26.5 <C> 15.5 <R> <C> (5) <C> BNF-1st, MR-1 <C> 39 <C> 23.9 <C> 14.6 <C> 22.0 <C> 13.4 <R> <C> (6) <C> BNF-1st, MR-2 <C> 39 <C> 24.5 <C> 14.9 <C> 22.1 <C> 13.3 <R> <C> (7) <C> BNF-2nd, MR-0 <C> 39 <C> 26.6 <C> 16.3 <C> 26.2 <C> 15.1 <R> <C> (8) <C> BNF-2nd, MR-1 <C> 39 <C> 24.5 <C> 15.1 <C> 23.3 <C> 13.9 <R> <C> (9) <C> BNF-2nd, MR-1* <C> 39 <C> 23.0 <C> 15.1 <C> 22.7 <C> 15.2 <R> <C> (10) <C> BNF-2nd, MR-2* <C> 39 <C> 23.2 <C> 14.7 <C> 23.3 <C> 14.3 <R> <C> (11) <C> BNF-1st, MR-1,64 <C> 64 <C> 23.2 <C> 14.5 <C> 21.9 <C> 13.3 <R> <C> (12) <C> BNF-1st, MR-1,128 <C> 128 <C> 23.0 <C> 14.5 <C> 21.9 <C> 13.3 <R> <C> (13) <C> BNF-1st, MR-1,256 <C> 256 <C> [BOLD] 21.9 <C> [BOLD] 14.0 <C> [BOLD] 21.4 <C> [BOLD] 12.8 <R> <C> (14) <C> LSTM autoencoder <C> 50 <C> 36.1 <C> 23.9 <C> 34.3 <C> 20.5 <R> <C> (15) <C> MAT-LSTM <C> 39 <C> 23.2 <C> 14.8 <C> 24.0 <C> 14.7 <R> <C> (16) <C> Thiollière et al.  <C> 100 <C> 17.9 <C> 12.0 <C> 16.6 <C> 11.7 <R> <C> (17) <C> Renshaw et al.  <C> 100 <C> 21.1 <C> 13.5 <C> 19.3 <C> 11.9 <R> <C> (18) <C> Badino et al.  <C> 64 <C> 26.3 <C> 17.3 <C> 23.6 <C> 14.1 <R> <C> (19) <C> Chen et al.  <C> 385 <C> 16.3 <C> 10.8 <C> 17.2 <C> 9.6 <R> <C> (20) <C> Baljekar et al.  <C> 26 <C> 29.8 <C> 18.4 <C> 29.7 <C> 18.1 <R> <C> (21) <C> Topline <C> [EMPTY] <C> 16.0 <C> 12.1 <C> 4.5 <C> 3.5 <CAP> Table I: Frame-level Speech feature quality in the metrics (ABX error percentages) of Track 1 of the Challenge for the languages English and Tsonga, both including across speaker and within speaker tasks. The best figure for each metric is shown in bold.
<R> <C> Lan <C> System <C> System <C> NED <C> Cov <C> Matching P <C> Matching R <C> Matching F <C> Grouping P <C> Grouping R <C> Grouping F <C> Type P <C> Type R <C> Type F <C> Token P <C> Token R <C> Token F <C> Boundary P <C> Boundary R <C> Boundary F <R> <C> [EMPTY] <C> (1) JHU Baseline <C> [EMPTY] <C> 21.9 <C> 16.3 <C> 39.4 <C> 1.6 <C> 3.1 <C> 21.4 <C> 84.6 <C> 33.3 <C> 6.2 <C> 1.9 <C> 2.9 <C> 5.5 <C> 0.4 <C> 8.0 <C> 44.1 <C> 4.7 <C> 8.6 <R> <C> [EMPTY] <C> (2) Proposed <C> (A) <C> 87.5 <C> [BOLD] 100.0 <C> 1.4 <C> 0.5 <C> 0.8 <C> 3.6 <C> 18.7 <C> 6.0 <C> 4.2 <C> [BOLD] 11.9 <C> [BOLD] 6.2 <C> [BOLD] 8.3 <C> [BOLD] 15.7 <C> [BOLD] 10.9 <C> 35.2 <C> 84.6 <C> 49.8 <R> <C> English <C> (3) Räsänen et al. <C> [EMPTY] <C> 70.8 <C> 42.4 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 13.4 <C> 15.7 <C> 14.2 <C> 14.1 <C> 12.9 <C> 13.5 <C> 22.6 <C> 6.1 <C> 9.6 <C> 75.7 <C> 33.7 <C> 46.7 <R> <C> [EMPTY] <C> (4) Lyzinski et al. <C> [EMPTY] <C> 61.2 <C> 80.2 <C> 6.5 <C> 3.5 <C> 4.6 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 3.1 <C> 9.2 <C> 4.6 <C> 2.4 <C> 3.5 <C> 2.8 <C> 18.8 <C> 64.0 <C> 29.0 <R> <C> [EMPTY] <C> (5) Topline <C> [EMPTY] <C> 0.0 <C> 100.0 <C> 98.3 <C> 18.5 <C> 31.1 <C> 99.5 <C> 100.0 <C> 99.7 <C> 50.3 <C> 56.2 <C> 53.1 <C> 68.2 <C> 60.8 <C> 64.3 <C> 88.4 <C> 86.7 <C> 87.5 <R> <C> [EMPTY] <C> (1) JHU Baseline <C> [EMPTY] <C> 12.0 <C> 16.2 <C> 69.1 <C> 0.3 <C> 0.5 <C> 52.1 <C> 77.4 <C> 62.2 <C> 3.2 <C> 1.4 <C> 2.0 <C> 2.6 <C> 0.5 <C> 0.8 <C> 22.3 <C> 5.6 <C> 8.9 <R> <C> [EMPTY] <C> (2) Proposed <C> (B) <C> 69.1 <C> [BOLD] 95.0 <C> 5.9 <C> [BOLD] 0.5 <C> [BOLD] 0.9 <C> 10.7 <C> 26.8 <C> 15.3 <C> 1.5 <C> [BOLD] 3.9 <C> [BOLD] 2.2 <C> 2.3 <C> [BOLD] 6.6 <C> [BOLD] 3.4 <C> 17.1 <C> [BOLD] 59.1 <C> [BOLD] 26.6 <R> <C> [EMPTY] <C> [EMPTY] <C> (C) <C> 60.2 <C> [BOLD] 96.1 <C> 9.7 <C> [BOLD] 0.4 <C> [BOLD] 0.8 <C> 13.5 <C> 12.7 <C> 13.1 <C> 1.8 <C> [BOLD] 4.7 <C> [BOLD] 2.5 <C> [BOLD] 3.9 <C> [BOLD] 9.1 <C> [BOLD] 5.4 <C> 21.2 <C> [BOLD] 62.1 <C> [BOLD] 31.6 <R> <C> Tsonga <C> (3) Räsänen et al. <C> [EMPTY] <C> 36.4 <C> 94.7 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 10.7 <C> 3.3 <C> 5.0 <C> 2.2 <C> 6.2 <C> 3.3 <C> 2.3 <C> 3.4 <C> 2.7 <C> 29.2 <C> 39.4 <C> 33.5 <R> <C> [EMPTY] <C> (4) Lyzinski et al. <C> [EMPTY] <C> 43.2 <C> 89.4 <C> 21.2 <C> 3.8 <C> 6.5 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 4.9 <C> 18.8 <C> 7.8 <C> 2.2 <C> 12.6 <C> 0.8 <C> 18.8 <C> 64.0 <C> 29.0 <R> <C> [EMPTY] <C> (5) Topline <C> [EMPTY] <C> 0.0 <C> 100.0 <C> 100.0 <C> 6.8 <C> 12.7 <C> 100.0 <C> 100.0 <C> 100.0 <C> 15.1 <C> 18.1 <C> 16.5 <C> 34.1 <C> 49.7 <C> 40.4 <C> 66.6 <C> 91.9 <C> 77.2 <CAP> Table II: Comparison of three typical example token sets (A)(B)(C) selected out of all shown in Fig. 6 with the JHU baseline offered by the Challenge, the systems proposed by Räsänen et al. [52], and Lyzinki et al. [53] in terms Precision (P), Recall (R), and F-score (F). Those better than JHU baseline are in bold.
<R> <C> [BOLD] System <C> [BOLD] Ppl <C> [BOLD] BLEU Before <C> [BOLD] BLEU After unk <R> <C> global (location) <C> 6.4 <C> 18.1 <C> 19.3 (+1.2) <R> <C> global (dot) <C> 6.1 <C> 18.6 <C> 20.5 (+1.9) <R> <C> global (general) <C> 6.1 <C> 17.3 <C> 19.1 (+1.8) <R> <C> local-m (dot) <C> >7.0 <C> x <C> x <R> <C> local-m (general) <C> 6.2 <C> 18.6 <C> 20.4 (+1.8) <R> <C> local-p (dot) <C> 6.6 <C> 18.0 <C> 19.6 (+1.9) <R> <C> local-p (general) <C> [BOLD] 5.9 <C> [BOLD] 19 <C> [BOLD] 20.9 (+1.9) <CAP> Table 4: Attentional Architectures – performances of different attentional models. We trained two local-m (dot) models; both have ppl >7.0.
<R> <C> DATASET <C> BBCSPORT <C> TWITTER <C> RECIPE <C> OHSUMED <C> CLASSIC <C> REUTERS <C> AMAZON <C> 20NEWS <R> <C> WMD <C> 4.6±0.7 <C> 28.7±0.6 <C> 42.6±0.3 <C> 44.5 <C> 2.8±0.1 <C> 3.5 <C> 7.4±0.3 <C> 28.3 <R> <C> S-WMD <C> 2.1±0.5 <C> 27.5±0.5 <C> 39.2±0.3 <C> [BOLD] 34.3 <C> 3.2±0.2 <C> 3.2 <C> 5.8±0.1 <C> 26.8 <R> <C> WFR <C> [BOLD] 0.8± [BOLD] 0.3 <C> [BOLD] 26.4± [BOLD] 0.2 <C> [BOLD] 38.9± [BOLD] 0.1 <C> 41.82 <C> [BOLD] 2.6± [BOLD] 0.2 <C> [BOLD] 3.2 <C> [BOLD] 4.8± [BOLD] 0.2 <C> [BOLD] 22.3 <R> <C> WME(512)+WMD <C> 3.5±0.7 <C> 26.8±2.3 <C> 48.0±0.6 <C> 42.1 <C> 4.8±0.3 <C> 4.0 <C> 7.4±0.4 <C> 30.7 <R> <C> WME(512)+WFR <C> 2.7±1.0 <C> 26.0±1.9 <C> 43.3±1.2 <C> 37.2 <C> 3.7±0.4 <C> 3.7 <C> 7.5±0.3 <C> 29.8 <CAP> Table 2: KNN classification error rate for WFR and other baselines and combine with WME(512).
<R> <C> [EMPTY] <C> Offset <C> Attn+CNN <C> NMF <C> SVD <C> HFT <C> DeepCoNN <C> TransRev <R> <C> Amazon Instant Video <C> 1.180 <C> 0.936 <C> 0.946 <C> 0.904 <C> 0.888 <C> 0.943 <C> [BOLD] 0.884 <R> <C> Automotive <C> 0.948 <C> 0.881 <C> 0.876 <C> 0.857 <C> 0.862 <C> [BOLD] 0.753 <C> 0.855 <R> <C> Baby <C> 1.262 <C> 1.176 <C> 1.171 <C> 1.108 <C> 1.104 <C> 1.154 <C> [BOLD] 1.100 <R> <C> Cds and Vinyl <C> 1.127 <C> 0.866 <C> 0.871 <C> 0.863 <C> [BOLD] 0.854 <C> 0.888 <C> [BOLD] 0.854 <R> <C> Grocery and Gourmet Food <C> 1.165 <C> 1.004 <C> 0.985 <C> 0.964 <C> 0.961 <C> 0.973 <C> [BOLD] 0.957 <R> <C> Health and Personal Care <C> 1.200 <C> 1.054 <C> 1.048 <C> 1.016 <C> 1.014 <C> 1.081 <C> [BOLD] 1.011 <R> <C> Kindle Store <C> 0.87 <C> 0.617 <C> 0.624 <C> 0.607 <C> [BOLD] 0.593 <C> 0.648 <C> 0.599 <R> <C> Musical Instruments <C> 0.733 <C> 0.703 <C> 0.725 <C> 0.694 <C> 0.692 <C> 0.723 <C> [BOLD] 0.690 <R> <C> Office Products <C> 0.876 <C> 0.726 <C> 0.742 <C> 0.727 <C> 0.727 <C> 0.738 <C> [BOLD] 0.724 <R> <C> Patio, Lawn and Garden <C> 1.156 <C> 0.999 <C> 0.958 <C> 0.950 <C> 0.956 <C> 1.070 <C> [BOLD] 0.941 <R> <C> Pet Supplies <C> 1.354 <C> 1.236 <C> 1.241 <C> 1.198 <C> 1.194 <C> 1.281 <C> [BOLD] 1.191 <R> <C> Tools and Home Improvement <C> 1.017 <C> 0.938 <C> 0.908 <C> 0.884 <C> 0.884 <C> 0.946 <C> [BOLD] 0.879 <R> <C> Toys and Games <C> 0.975 <C> - <C> 0.821 <C> 0.788 <C> [BOLD] 0.784 <C> 0.851 <C> [BOLD] 0.784 <R> <C> Beauty <C> 1.322 <C> - <C> 1.204 <C> 1.168 <C> 1.165 <C> 1.184 <C> [BOLD] 1.158 <R> <C> Digital Music <C> 1.137 <C> - <C> 0.805 <C> 0.797 <C> 0.793 <C> 0.835 <C> [BOLD] 0.782 <R> <C> Video Games <C> 1.401 <C> - <C> 1.138 <C> 1.093 <C> 1.086 <C> 1.133 <C> [BOLD] 1.082 <R> <C> Sports and Outdoors <C> 0.931 <C> - <C> 0.856 <C> 0.828 <C> 0.824 <C> 0.882 <C> [BOLD] 0.823 <R> <C> Cell Phones and Accesories <C> 1.455 <C> - <C> 1.357 <C> 1.290 <C> 1.285 <C> 1.365 <C> [BOLD] 1.279 <R> <C> [EMPTY] <C> 1.117∗ <C> - <C> 0.959∗ <C> 0.930∗ <C> 0.926∗ <C> 0.969∗ <C> [BOLD] 0.921∗ <R> <C> Yelp <C> 1.385 <C> 1.212 <C> 1.229 <C> 1.158 <C> 1.148 <C> 1.215 <C> [BOLD] 1.144 <CAP> Table 2: MSE for TransRev and state-of-the-art approaches. ∗ indicates the macro MSE across the Amazon data sets.
<R> <C> [ITALIC] k <C> Baby <C> Digital Music <C> Office Products <C> Tools&Home Improv. <R> <C> 4 <C> 1.100 <C> 0.782 <C> 0.724 <C> 0.880 <R> <C> 8 <C> 1.100 <C> 0.782 <C> 0.723 <C> 0.878 <R> <C> 16 <C> 1.100 <C> 0.782 <C> 0.724 <C> 0.879 <R> <C> 32 <C> 1.102 <C> 0.785 <C> 0.722 <C> 0.888 <R> <C> 64 <C> 1.099 <C> 0.787 <C> 0.726 <C> 0.888 <CAP> Table 3: Effect of the latent dimension k in TransRev.
<R> <C> Fr <C> Positive nurse <C> M:F 0:36 <C> Negative manager <C> M:F 685:1 <R> <C> [EMPTY] <C> secretary <C> 0:17 <C> employee <C> 406:0 <R> <C> [EMPTY] <C> teacher <C> 7:1 <C> employees <C> 364:0 <R> <C> [EMPTY] <C> assistant <C> 1:7 <C> parents <C> 353:0 <R> <C> [EMPTY] <C> manager <C> 8:0 <C> teacher <C> 337:0 <R> <C> De <C> secretary <C> 0:27 <C> manager <C> 594:0 <R> <C> [EMPTY] <C> nurse <C> 0:21 <C> employees <C> 409:1 <R> <C> [EMPTY] <C> teacher <C> 3:7 <C> friends <C> 359:0 <R> <C> [EMPTY] <C> receptionist <C> 0:9 <C> employee <C> 320:0 <R> <C> [EMPTY] <C> manager <C> 7:0 <C> students <C> 316:0 <R> <C> Es <C> teacher <C> 4:29 <C> manager <C> 691:0 <R> <C> [EMPTY] <C> nurse <C> 0:31 <C> employee <C> 446:0 <R> <C> [EMPTY] <C> secretary <C> 0:26 <C> friends <C> 380:0 <R> <C> [EMPTY] <C> writer <C> 8:0 <C> parents <C> 374:0 <R> <C> [EMPTY] <C> employee <C> 5:0 <C> supervisor <C> 345:0 <R> <C> Ru <C> nurse <C> 0:32 <C> manager <C> 713:0 <R> <C> [EMPTY] <C> babysitter <C> 0:13 <C> employees <C> 519:0 <R> <C> [EMPTY] <C> nurses <C> 0:5 <C> friends <C> 439:0 <R> <C> [EMPTY] <C> dishwasher <C> 0:4 <C> students <C> 417:0 <R> <C> [EMPTY] <C> technician <C> 3:0 <C> employee <C> 392:0 <CAP> Таблица 3: Top five human reference forms in our dataset, and their ratio of times they are translated as masculine compared to feminine. Positive indicates that the examples were taken from the at-risk group from our pipeline, and negative from the random sample among the not at-risk group.
<R> <C> [BOLD] Model <C> [BOLD] es % [BOLD]  Entities <C> [BOLD] es  [ITALIC] F1 <C> [BOLD] hi % [BOLD]  Entities <C> [BOLD] hi  [ITALIC] F1 <R> <C> Base model <C> 91.8 <C> 67.8 <C> 77.9 <C> 37.7 <R> <C> +phonetic <C> 93.7 <C> 71.4 <C> 83.0 <C> 35.0 <R> <C> +copy <C> 97.2 <C> 72.2 <C> 85.4 <C> 37.6 <R> <C> +gold <C> 98.3 <C> 73.3 <C> 88.5 <C> 42.0 <R> <C> +dist <C> 99.9 <C> 74.2 <C> 94.9 <C> 43.4 <CAP> Table 4: Ablation study for Spanish and Hindi
<R> <C> [EMPTY] <C> MT-Clinical BERT <C> Optimized Clinical BERT <C> Clinical Bert  <R> <C> n2c2-2019 <C> 86.7 (−0.5) <C> 87.2 <C> - <R> <C> MedNLI <C> 80.5 (−2.3) <C> 82.8 <C> 82.7 <R> <C> MedRQE <C> 76.5 (−3.6) <C> 80.1 <C> - <R> <C> n2c2-2018 <C> 87.4 (−0.7) <C> 88.1 <C> - <R> <C> i2b2-2014 <C> 91.9 (−3.6) <C> 95.5 <C> 92.7 <R> <C> i2b2-2012 <C> [BOLD] 84.1 (+0.2) <C> 83.9 <C> 78.9 <R> <C> i2b2-2010 <C> 89.5 (−0.3) <C> 89.8 <C> 87.8 <R> <C> quaero-2014 <C> 49.1 (−6.4) <C> 55.5 <C> - <CAP> Table 2: Clinical information extraction performance of MT Clinical BERT versus hyper parameter searched Clinical BERT fine-tuning runs. All span level metrics are exact match. Task performances showcased in the column MT-Clinical BERT represent a single multitask trained feature encoder with individual task-specific heads. All other reported results are generated from task-specific BERT models. Higher is better.
<R> <C> [EMPTY] <C> RQA AUC <C> RQA p@1/p@5 <C> DIY AUC <C> DIY p@1/p@5 <R> <C> Random <C> 49.4 <C> 17.8/16.7 <C> 49.8 <C> 6.3/6.8 <R> <C> Obj Detect <C> 58.7 <C> 25.1/21.5 <C> 53.4 <C> 17.9/11.8 <R> <C> NoStruct <C> 60.5 <C> 33.8/27.0 <C> 57.0 <C> 13.3/11.8 <R> <C> DC <C> 63.5 <C> 38.3/30.6 <C> 59.3 <C> 20.8/16.1 <R> <C> TK <C> 67.9 <C> 44.0/35.8 <C> 60.5 <C> 21.2/16.0 <R> <C> ↰ + 12 [ITALIC] k <C> 68.1 <C> 44.5/35.4 <C> 56.0 <C> 14.1/12.5 <R> <C> AP <C> [BOLD] 69.3 <C> [BOLD] 47.3/ [BOLD] 37.3 <C> [BOLD] 61.8 <C> [BOLD] 22.5/ [BOLD] 17.2 <R> <C> ↰ + 12 [ITALIC] k <C> [BOLD] 68.7 <C> [BOLD] 47.2/36.2 <C> 59.4 <C> [BOLD] 21.6/15.3 <CAP> Table 3: Performance on the organically-multimodal data; values within 1% of best-in-column are bolded.
<R> <C> [EMPTY] <C> Swedish <C> Danish <C> Bulgarian <C> Slovene <C> Chinese <C> Hungarian <C> Turkish <C> German <C> Czech <C> Dutch <R> <C> D-MST <C> 87.7/88.9 <C> 88.5/89.5 <C> 90.4/90.9 <C> 80.4/83.4 <C> 86.1/87.7 <C> 82.9/84.3 <C> 75.2/75.3 <C> 89.6/90.2 <C> 81.7/84.0 <C> 81.3/83.0 <R> <C> U-MST-uf-lep <C> 86.9/88.4 <C> 87.7/88.9 <C> 89.7/90.6 <C> 79.4/82.8 <C> 84.8/86.7 <C> 81.8/83.3 <C> 74.9/75.3 <C> 88.7/89.5 <C> 79.6/82.5 <C> 78.7/80.7 <R> <C> U-MST-uf <C> 84.3/87.8 <C> 85.1/89.0 <C> 87.0/90.2 <C> 76.1/82.4 <C> 81.1/86.4 <C> 79.9/82.9 <C> 73.1/75.0 <C> 86.9/89.0 <C> 76.1/81.9 <C> 73.4/80.5 <R> <C> U-MST-df <C> 72.0/79.2 <C> 74.3/82.9 <C> 69.5/81.4 <C> 66.8/75.8 <C> 65.9/76.5 <C> 68.2/72.1 <C> 57.4/62.6 <C> 77.7/82.5 <C> 57.3/70.9 <C> 59.0/71.3 <R> <C> [EMPTY] <C> Japanese <C> Spanish <C> Catalan <C> Greek <C> Basque <C> Portuguese <C> Italian <C> PTB <C> QBank <C> GENIA <R> <C> D-MST <C> 92.5/92.6 <C> 83.8/86.0 <C> 91.8/92.2 <C> 82.7/84.9 <C> 72.1/75.8 <C> 89.2/89.9 <C> 83.4/85.4 <C> 92.1/92.8 <C> 95.8/96.3 <C> 88.9/90.0 <R> <C> U-MST-uf-lep <C> 92.1/92.2 <C> 83.5/85.9 <C> 91.3/91.9 <C> 81.8/84.4 <C> 71.6/75.8 <C> 88.3/89.3 <C> 82.4/84.7 <C> 90.6/91.7 <C> 95.6/96.2 <C> 87.2/88.9 <R> <C> U-MST-uf <C> 91.4/92.4 <C> 80.4/85.4 <C> 89.7/91.7 <C> 78.7/84 <C> 68.8/75.4 <C> 85.8/89.3 <C> 79.4/84.4 <C> 88.5/91.8 <C> 94.8/96.0 <C> 85.0/89.0 <R> <C> U-MST-df <C> 74.4/85.2 <C> 73.1/81.3 <C> 73.1/83.5 <C> 71.3/78.7 <C> 62.8/71.4 <C> 67.9/79.7 <C> 65.2/77.2 <C> 77.2/85.4 <C> 89.1/92.9 <C> 72.4/81.6 <CAP> Table 1: Directed/undirected UAS for the various parsing models of this paper.
<R> <C> [EMPTY] <C> Swedish <C> Danish <C> Bulgarian <C> Slovene <C> Chinese <C> Hungarian <C> Turkish <C> German <C> Czech <C> Dutch <R> <C> D-MST <C> 20.6 <C> 20.8 <C> 15.1 <C> 25.4 <C> 15.5 <C> 26.4 <C> 22.3 <C> 21.3 <C> 29.7 <C> 27.7 <R> <C> U-MST-uf-lep <C> 18.0 <C> 24.5 <C> 22.1 <C> 29.6 <C> 16.7 <C> 27.2 <C> 19.3 <C> 17.9 <C> 26.2 <C> 24.4 <R> <C> Oracle <C> 88.9 <C> 89.7 <C> 91.6 <C> 81.9 <C> 87.8 <C> 83.9 <C> 77.1 <C> 90.6 <C> 82.8 <C> 82.8 <R> <C> [EMPTY] <C> (+1.2) <C> (+1.4) <C> (+1.2) <C> (+1.5) <C> (+1.7) <C> (+1) <C> (+1.9) <C> (+1) <C> (+1.1) <C> (+1.5) <R> <C> [EMPTY] <C> Japanese <C> Spanish <C> Catalan <C> Greek <C> Basque <C> Portuguese <C> Italian <C> PTB <C> QBank <C> GENIA <R> <C> D-MST <C> 5.7 <C> 26.7 <C> 23.4 <C> 28.9 <C> 23.4 <C> 22.6 <C> 22.5 <C> 27.8 <C> 5.3 <C> 33.7 <R> <C> U-MST-uf-lep <C> 4.0 <C> 30.1 <C> 26.3 <C> 30.5 <C> 30.8 <C> 21.9 <C> 24.9 <C> 20.9 <C> 6.0 <C> 23.8 <R> <C> Oracle <C> 93.1 <C> 84.8 <C> 92.6 <C> 83.9 <C> 74.1 <C> 89.9 <C> 84.4 <C> 92.8 <C> 96.4 <C> 89.7 <R> <C> [EMPTY] <C> (+0.6) <C> (+1) <C> (+0.8) <C> (+1.2) <C> (+2) <C> (+0.7) <C> (+1) <C> (+0.7) <C> (+0.8) <C> (+0.8) <CAP> Table 2: Top two lines (per language): percentage of sentences for which each of the models performs better than the other according to the directed UAS. Bottom line (Oracle): Directed UAS of an oracle model that selects the parse tree of the best performing model for each sentence. Improvement over the directed UAS score of D-MST is given in parenthesis.
<R> <C> Team Name <C> ar <C> bg <C> cs <C> nl <C> en <C> et <C> fi <C> fr <C> it <C> lv <C> lt <C> pl <C> ru <C> sl <C> sv <C> ta <C> uk <C> Avg. <R> <C> [EMPTY] <C> Official <C> Official <C> Official <C> Official <C> Official <C> Official <C> Official <C> Official <C> Official <C> Official <C> Official <C> Official <C> Official <C> Official <C> Official <C> Official <C> Official <C> [EMPTY] <R> <C> RobertNLP <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> [BOLD] 88.9 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 5.2 <R> <C> Koebsala <C> 60.8 <C> 68.9 <C> 61.1 <C> 62.9 <C> 65.4 <C> 59.1 <C> 67.5 <C> 67.9 <C> 69.1 <C> 64.8 <C> 56.3 <C> 61.3 <C> 64.2 <C> 64.1 <C> 64.5 <C> 47.4 <C> 64.2 <C> 62.9 <R> <C> ADAPT <C> 57.2 <C> 77.3 <C> 66.4 <C> 67.7 <C> 70.4 <C> 61.1 <C> 72.4 <C> 74.7 <C> 72.0 <C> 72.4 <C> 58.4 <C> 65.9 <C> 75.3 <C> 68.4 <C> 68.4 <C> 48.5 <C> 66.4 <C> 67.2 <R> <C> clasp <C> 51.3 <C> 84.9 <C> 67.1 <C> 78.9 <C> 82.9 <C> 60.4 <C> 66.0 <C> 72.8 <C> 87.1 <C> 66.0 <C> 52.6 <C> 71.2 <C> 70.4 <C> 65.2 <C> 71.4 <C> 42.2 <C> 63.2 <C> 67.9 <R> <C> Ours <C> 63.4 <C> 78.7 <C> 75.4 <C> 70.9 <C> 72.3 <C> 74.9 <C> 76.0 <C> 77.0 <C> 73.1 <C> 77.8 <C> 66.9 <C> 71.0 <C> 78.3 <C> 73.1 <C> 69.6 <C> 48.2 <C> 73.0 <C> 71.7 <R> <C> Unipi <C> 57.8 <C> 84.9 <C> 76.0 <C> 77.6 <C> 84.0 <C> 57.2 <C> 72.1 <C> 78.9 <C> 89.1 <C> 68.2 <C> 61.1 <C> 70.6 <C> 76.9 <C> 81.4 <C> 78.7 <C> 48.5 <C> 73.9 <C> 72.8 <R> <C> FASTPARSE <C> 66.9 <C> 84.9 <C> 77.2 <C> 77.4 <C> 78.5 <C> 74.1 <C> 75.7 <C> 77.8 <C> 84.8 <C> 75.6 <C> 61.4 <C> 74.5 <C> 80.4 <C> 73.5 <C> 75.2 <C> 47.0 <C> 74.0 <C> 74.0 <R> <C> EmoryNLP <C> 67.3 <C> 88.2 <C> 85.5 <C> 80.7 <C> 85.3 <C> 81.4 <C> 83.0 <C> [BOLD] 86.2 <C> 88.5 <C> 79.2 <C> 66.1 <C> 82.4 <C> 88.6 <C> 82.7 <C> 78.2 <C> 54.3 <C> 79.7 <C> 79.8 <R> <C> OrangeDeskin <C> 71.0 <C> 89.4 <C> 87.0 <C> 85.1 <C> 85.2 <C> 81.0 <C> 86.2 <C> 83.6 <C> 90.8 <C> 82.1 <C> 75.9 <C> 80.4 <C> 89.8 <C> 84.4 <C> 83.3 <C> [BOLD] 64.2 <C> 84.6 <C> 82.6 <R> <C> TurkuNLP <C> [BOLD] 77.8 <C> 90.7 <C> 87.5 <C> 84.7 <C> 87.2 <C> 84.5 <C> [BOLD] 89.5 <C> 85.9 <C> 91.5 <C> 84.9 <C> 77.6 <C> [BOLD] 84.6 <C> 90.7 <C> [BOLD] 88.6 <C> [BOLD] 85.6 <C> 57.8 <C> 87.2 <C> 84.5 <R> <C> [EMPTY] <C> Post-Evaluation <C> Post-Evaluation <C> Post-Evaluation <C> Post-Evaluation <C> Post-Evaluation <C> Post-Evaluation <C> Post-Evaluation <C> Post-Evaluation <C> Post-Evaluation <C> Post-Evaluation <C> Post-Evaluation <C> Post-Evaluation <C> Post-Evaluation <C> Post-Evaluation <C> Post-Evaluation <C> Post-Evaluation <C> Post-Evaluation <C> [EMPTY] <R> <C> Ours+en+MST <C> 77.7 <C> 91.5 <C> 90.1 <C> 86.2 <C> 87.1 <C> 86.0 <C> 89.0 <C> 85.3 <C> 91.5 <C> 87.6 <C> 78.9 <C> 84.0 <C> 92.3 <C> 87.6 <C> 84.7 <C> 56.7 <C> 88.0 <C> 85.0 <R> <C> Ours+cs+Eis <C> 77.8 <C> 91.1 <C> 89.5 <C> 86.3 <C> 87.2 <C> 85.7 <C> 88.5 <C> 85.3 <C> [BOLD] 91.5 <C> 87.3 <C> 78.6 <C> 83.7 <C> 92.3 <C> 87.1 <C> 84.8 <C> 58.4 <C> 88.0 <C> 84.9 <R> <C> Ours+cs+MST <C> 77.7 <C> [BOLD] 91.5 <C> [BOLD] 90.1 <C> [BOLD] 86.2 <C> 87.1 <C> [BOLD] 86.0 <C> 89.0 <C> 85.3 <C> 91.5 <C> [BOLD] 87.6 <C> [BOLD] 78.9 <C> 84.0 <C> [BOLD] 92.3 <C> 87.6 <C> 84.7 <C> 58.4 <C> [BOLD] 88.0 <C> [BOLD] 85.1 <CAP> Table 2: Official evaluations of all systems and post-evaluations of our team in ELAS. We use the ISO 639-1 language code to represent each language. MST and Eis means the MST and Eisner’s algorithm that we used for decoding. “en” and “cs” represents which dataset we mixed with the Tamil dataset for training the Tamil parser. Note that ‘Ours+en+MST‘ represent the parsed results of parsers that we used in the Official submission.
<R> <C> Approach <C> ar <C> bg <C> cs <C> nl <C> en <C> et <C> fi <C> fr <C> it <R> <C> XLMR+Flair+FastText+1st-Order <C> 81.66 <C> 89.29 <C> 91.04 <C> 92.55 <C> 89.74 <C> 88.33 <C> 89.40 <C> 90.64 <C> 91.94 <R> <C> XLMR+Flair+FastText+2nd-Order <C> 81.98 <C> 89.43 <C> [BOLD] 91.39 <C> [BOLD] 92.68 <C> 89.58 <C> [BOLD] 88.69 <C> 89.54 <C> 91.08 <C> [BOLD] 91.98 <R> <C> XLMR+1st-Order <C> 82.02 <C> 90.15 <C> 90.80 <C> 92.43 <C> 90.05 <C> 88.13 <C> 89.51 <C> 91.14 <C> 91.96 <R> <C> XLMR+2nd-Order <C> [BOLD] 82.42 <C> [BOLD] 90.37 <C> 91.21 <C> 92.66 <C> [BOLD] 90.26 <C> 88.60 <C> 90.35 <C> [BOLD] 91.69 <C> [BOLD] 91.98 <R> <C> [EMPTY] <C> lv <C> lt <C> pl <C> ru <C> sk <C> sv <C> ta <C> uk <C> Avg. <R> <C> XLMR+Flair+FastText+1st-Order <C> 88.21 <C> 80.21 <C> 86.91 <C> 92.88 <C> 87.28 <C> 85.52 <C> 66.17 <C> 88.26 <C> 89.40 <R> <C> XLMR+Flair+FastText+2nd-Order <C> 88.59 <C> 81.25 <C> 86.46 <C> [BOLD] 93.28 <C> 87.18 <C> 85.63 <C> [BOLD] 68.76 <C> 88.04 <C> 89.59 <R> <C> XLMR+1st-Order <C> 89.62 <C> 81.92 <C> 85.73 <C> 92.86 <C> 88.48 <C> 86.36 <C> 63.28 <C> 88.96 <C> 89.57 <R> <C> XLMR+2nd-Order <C> [BOLD] 89.97 <C> [BOLD] 83.24 <C> [BOLD] 87.49 <C> 93.21 <C> [BOLD] 89.07 <C> [BOLD] 86.85 <C> 64.84 <C> [BOLD] 89.99 <C> [BOLD] 89.95 <CAP> Table 3: A comparison of different word embedding concatenation and first-order and second-order inference approaches on the development set split by ourselves. We report Labeled F1 score (LF1) here.
<R> <C> [ITALIC] ser <C> [ITALIC] N <C> [ITALIC] M <C> [ITALIC] ccr semantic <C> [ITALIC] ccr semantic <C> [ITALIC] ccr retrieval <C> [ITALIC] ccr retrieval <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> MAP <C> MRR <C> MAP <C> MRR <R> <C> basic <C> 50 <C> 50 <C> [BOLD] 0.5195 <C> 0.6133 <C> 0.2758 <C> 0.4440 <R> <C> [EMPTY] <C> 50 <C> 100 <C> 0.5076 <C> 0.6138 <C> 0.2796 <C> 0.4453 <R> <C> [EMPTY] <C> 100 <C> 50 <C> 0.5133 <C> [BOLD] 0.6357 <C> 0.2927 <C> 0.4449 <R> <C> [EMPTY] <C> 100 <C> 100 <C> 0.5081 <C> 0.6341 <C> 0.2954 <C> 0.4489 <R> <C> pop <C> 50 <C> 50 <C> 0.4441‡ <C> 0.5738† <C> 0.2636 <C> 0.4030 <R> <C> [EMPTY] <C> 50 <C> 100 <C> 0.4499‡ <C> 0.5738 <C> 0.2695 <C> 0.3826† <R> <C> [EMPTY] <C> 100 <C> 50 <C> 0.4564‡ <C> 0.6061† <C> 0.2683† <C> 0.3988† <R> <C> [EMPTY] <C> 100 <C> 100 <C> [BOLD] 0.4633‡ <C> [BOLD] 0.6200 <C> 0.2706† <C> 0.4067† <R> <C> types <C> 50 <C> 50 <C> 0.4512‡ <C> 0.5818 <C> 0.2537 <C> 0.3942 <R> <C> [EMPTY] <C> 50 <C> 100 <C> 0.4475‡ <C> 0.5818 <C> 0.2558 <C> 0.3964 <R> <C> [EMPTY] <C> 100 <C> 50 <C> [BOLD] 0.4544‡ <C> 0.5866† <C> 0.2504† <C> 0.3828† <R> <C> [EMPTY] <C> 100 <C> 100 <C> 0.4477‡ <C> [BOLD] 0.5881† <C> 0.2572† <C> 0.3913† <CAP> Table 1. Context retrieval performance, measured in terms of MAP and MRR metrics. Best values per block are typeset in bold. Statistical significance is tested using a two-tailed paired t-test at p<0.05 (†) and p<0.001 (‡). For each line in the pop and types blocks, significance is tested against the corresponding row of the basic block.
<R> <C> Methods <C> [BOLD] Prec. <C> [BOLD] Recall <C> [BOLD] F1 Score <R> <C> Self-attentive biGRU <C> 0.683 <C> 0.649 <C> 0.665 <R> <C> CNN (Document) <C> 0.537 <C> 0.474 <C> 0.503 <R> <C> Standard Co-Training <C> 0.418 <C> 0.433 <C> 0.425 <R> <C> Performance Co-Training <C> 0.581 <C> 0.629 <C> 0.604 <R> <C> CoTrade Co-Training <C> 0.609 <C> 0.637 <C> 0.623 <R> <C> Sequence-SSL <C> 0.595 <C> 0.589 <C> 0.592 <R> <C> Region-SSL <C> 0.674 <C> 0.652 <C> 0.663 <R> <C> Adversarial-SSL <C> 0.698 <C> [BOLD] 0.691 <C> 0.694 <R> <C> Reinforced Co-Training <C> [BOLD] 0.709 <C> 0.684 <C> [BOLD] 0.696 <CAP> Table 2: The experimental results on clickbait dataset. Prec.: precision.
<R> <C> Methods <C> nDCG@1 <C> nDCG@5 <C> nDCG@10 <C> MAP <R> <C> M&W <C> 0.54 <C> 0.52 <C> 0.55 <C> 0.48 <R> <C> DSRM1 <C> 0.68 <C> 0.61 <C> 0.62 <C> 0.56 <R> <C> DSRM12 <C> 0.72 <C> 0.64 <C> 0.65 <C> 0.59 <R> <C> DSRM123 <C> 0.74 <C> 0.65 <C> 0.66 <C> 0.61 <R> <C> DSRM1234 <C> [BOLD] 0.81 <C> [BOLD] 0.73 <C> [BOLD] 0.74 <C> [BOLD] 0.68 <CAP> Table 3: Overall performance of entity semantic relatedness methods.
<R> <C> Dimension/Dataset <C> WS-353 <C> Euclidean Simlex <C> MEN <C> WS-353 <C> Hyperbolic Simlex <C> MEN <R> <C> 5 <C> 0.3508 <C> [BOLD] 0.1622 <C> 0.4152 <C> [BOLD] 0.3635 <C> 0.1460 <C> [BOLD] 0.4655 <R> <C> 20 <C> 0.5417 <C> 0.2291 <C> 0.6433 <C> [BOLD] 0.6156 <C> [BOLD] 0.2554 <C> [BOLD] 0.6694 <R> <C> 50 <C> 0.6628 <C> 0.2738 <C> [BOLD] 0.7217 <C> [BOLD] 0.6787 <C> [BOLD] 0.2784 <C> 0.7117 <R> <C> 100 <C> [BOLD] 0.6986 <C> [BOLD] 0.2923 <C> [BOLD] 0.7473 <C> 0.6846 <C> 0.2832 <C> 0.7217 <CAP> Table 1: Spearman rank correlation on 3 similarity datasets.
<R> <C> Dimension <C> 5 <C> 20 <C> 50 <C> 100 <R> <C> Euclidean <C> 0.0011 <C> 0.2089 <C> [BOLD] 0.3866 <C> [BOLD] 0.5513 <R> <C> Hyperbolic ( [ITALIC] Z) <C> 0.0020 <C> [BOLD] 0.2251 <C> 0.3536 <C> 0.3636 <R> <C> Hyperbolic ( [ITALIC] Z′) <C> 0.0008 <C> 0.0365 <C> 0.0439 <C> 0.0437 <CAP> Table 2: Accuracy on the Google word analogy dataset.
<R> <C> Model <C> Entity Detection Fusion <C> Entity Detection Fusion <C> Entity Detection Fusion <C> Entity Detection Separation <C> Entity Detection Separation <C> Entity Detection Separation <C> Coreference Recognition Fine-Grained <C> Coreference Recognition Fine-Grained <C> Coreference Recognition Fine-Grained <C> Coreference Recognition Triple-Level <C> Coreference Recognition Triple-Level <C> Coreference Recognition Triple-Level <R> <C> [EMPTY] <C> Entity Detection <C> Entity Detection <C> Entity Detection <C> Entity Detection <C> Entity Detection <C> Entity Detection <C> Coreference Recognition <C> Coreference Recognition <C> Coreference Recognition <C> Coreference Recognition <C> Coreference Recognition <C> Coreference Recognition <R> <C> [EMPTY] <C> P <C> R <C> F <C> P <C> R <C> F <C> P <C> R <C> F <C> P <C> R <C> F <R> <C> Pipeline <C> Pipeline <C> Pipeline <C> Pipeline <C> Pipeline <C> Pipeline <C> Pipeline <C> Pipeline <C> Pipeline <C> Pipeline <C> Pipeline <C> Pipeline <C> Pipeline <R> <C> basic <C> 83.10 <C> 83.88 <C> 83.49 <C> 82.84 <C> 84.46 <C> 83.64 <C> 76.09 <C> [BOLD] 76.66 <C> 76.37 <C> 74.26 <C> [BOLD] 75.23 <C> 74.74 <R> <C> GAT(word,Hownet) <C> 84.57 <C> 83.02 <C> 83.79 <C> 86.92 <C> 81.79 <C> 84.28 <C> 85.07 <C> 74.27 <C> 79.30 <C> 85.02 <C> 73.27 <C> 78.71 <R> <C> Joint <C> Joint <C> Joint <C> Joint <C> Joint <C> Joint <C> Joint <C> Joint <C> Joint <C> Joint <C> Joint <C> Joint <C> Joint <R> <C> basic <C> 82.85 <C> 81.40 <C> 82.12 <C> 85.79 <C> 82.46 <C> 84.09 <C> 81.65 <C> 74.32 <C> 77.81 <C> 80.32 <C> 72.94 <C> 76.45 <R> <C> GAT(char,pseudo) <C> 84.61 <C> 83.15 <C> 83.87 <C> 86.83 <C> 82.58 <C> 84.56 <C> 83.19 <C> 75.81 <C> 79.33 <C> 81.57 <C> 74.46 <C> 77.85 <R> <C> GAT(char,Hownet) <C> 84.79 <C> [BOLD] 84.27 <C> 84.53 <C> 88.07 <C> 81.75 <C> 84.79 <C> 84.71 <C> 75.15 <C> 79.64 <C> 83.72 <C> 73.87 <C> 78.49 <R> <C> GAT(word,pseudo) <C> 84.55 <C> 83.69 <C> 84.12 <C> 85.70 <C> [BOLD] 84.82 <C> 85.26 <C> [BOLD] 87.88 <C> 73.12 <C> 79.82 <C> [BOLD] 89.38 <C> 70.62 <C> 78.90 <R> <C> GAT(word,Hownet) <C> [BOLD] 85.18 <C> 84.16 <C> [BOLD] 84.67 <C> [BOLD] 88.64 <C> 83.98 <C> [BOLD] 86.25 <C> 85.63 <C> 75.60 <C> [BOLD] 80.30 <C> 86.19 <C> 74.02 <C> [BOLD] 79.64 <CAP> Table 3: Final results on the test dataset, where basic indicates no sememe information is used.
<R> <C> Model <C> IV AB <C> IV AA <C> OOV AB <C> OOV AA <C> Fine-Grained B <C> Fine-Grained A <R> <C> Joint <C> 86.4 <C> 94.5 <C> 30.4 <C> 81.8 <C> 28.1 <C> 82.4 <R> <C> GAT(word_real) <C> [BOLD] 89.5 <C> [BOLD] 94.7 <C> [BOLD] 41.1 <C> [BOLD] 82.9 <C> [BOLD] 36.5 <C> [BOLD] 83.7 <CAP> Table 4: F1 scores by different lexical fusion types.
<R> <C> Model <C> Aux. Info. <C> WN18 Hits@10 <C> WN18 MR <C> FB15k Hits@10 <C> FB15k MR <C> FB15k-237 Hits@10 <C> FB15k-237 MR <R> <C> TransE (Bordes et al.,  2013 ) <C> NO <C> 89.2 <C> 251 <C> 47.1 <C> 125 <C> - <C> - <R> <C> NTN (Socher et al.,  2013 ) <C> NO <C> 66.1 <C> - <C> 41.4 <C> - <C> - <C> - <R> <C> TransH (Wang et al.,  2014 ) <C> NO <C> 86.7 <C> 303 <C> 64.4 <C> 87 <C> - <C> - <R> <C> TransR (Lin et al.,  2015b ) <C> NO <C> 92.0 <C> 225 <C> 68.7 <C> 77 <C> - <C> - <R> <C> CTransR (Lin et al.,  2015b ) <C> NO <C> 92.3 <C> 218 <C> 70.2 <C> 75 <C> - <C> - <R> <C> KG2E (He et al.,  2015 ) <C> NO <C> 93.2 <C> 348 <C> 74.0 <C> 59 <C> - <C> - <R> <C> TransD (Ji et al.,  2015 ) <C> NO <C> 92.2 <C> 212 <C> 77.3 <C> 91 <C> - <C> - <R> <C> TATEC (García-Durán et al.,  2015b ) <C> NO <C> - <C> - <C> 76.7 <C> 58 <C> - <C> - <R> <C> DISTMULT (Yang et al.,  2015 ) <C> NO <C> 94.2 <C> - <C> 57.7 <C> - <C> 41.9 <C> 254 <R> <C> STransE (Nguyen et al.,  2016 ) <C> NO <C> 93.4 <C> [BOLD] 206 <C> 79.7 <C> 69 <C> - <C> - <R> <C> HOLE (Nickel et al.,  2016 ) <C> NO <C> 94.9 <C> - <C> 73.9 <C> - <C> - <C> - <R> <C> ComplEx (Trouillon et al.,  2016 ) <C> NO <C> 94.7 <C> - <C> 84.0 <C> - <C> - <C> - <R> <C> TransG (Xiao et al.,  2016 ) <C> NO <C> 94.9 <C> 345 <C> 88.2 <C> 50 <C> - <C> - <R> <C> ConvE (Dettmers et al.,  2017 ) <C> NO <C> 95.5 <C> 504 <C> 87.3 <C> 64 <C> 45.8 <C> 330 <R> <C> ProjE (Shi and Weninger,  2017 ) <C> NO <C> - <C> - <C> 88.4 <C> [BOLD] 34 <C> - <C> - <R> <C> RTransE (García-Durán et al.,  2015a ) <C> Path <C> - <C> - <C> 76.2 <C> 50 <C> - <C> - <R> <C> PTransE (Lin et al.,  2015a ) <C> Path <C> - <C> - <C> 84.6 <C> 58 <C> - <C> - <R> <C> NLFeat (Toutanova et al.,  2015 ) <C> Node + Link Features <C> 94.3 <C> - <C> 87.0 <C> - <C> - <C> - <R> <C> Random Walk (Wei et al.,  2016 ) <C> Path <C> 94.8 <C> - <C> 74.7 <C> - <C> - <C> - <R> <C> [BOLD] EKGN <C> NO <C> [BOLD] 95.3 <C> 249 <C> [BOLD] 92.7 <C> 38 <C> [BOLD] 46.4 <C> [BOLD] 211 <CAP> Table 1: The knowledge base completion (link prediction) results on WN18, FB15k, and FB15k-237.
<R> <C> documents <C> [ITALIC] CS <C> [ITALIC] SRCC <C> [ITALIC] PCC <R> <C> [ITALIC] d1, [ITALIC] d4 <C> [BOLD] 0.27 <C> 0.13 <C> 0.22 <R> <C> [ITALIC] d1, [ITALIC] d5 <C> 0.36 <C> 0.0018 <C> [BOLD] 0.05 <R> <C> [ITALIC] d1, [ITALIC] d8 <C> [BOLD] 0.012 <C> 0.0063 <C> 0.05 <R> <C> [ITALIC] d6, [ITALIC] d10 <C> 0.009 <C> [BOLD] 0.022 <C> 0.01 <R> <C> [ITALIC] d2, [ITALIC] d4 <C> [BOLD] 0.24 <C> 0.16 <C> 0.2 <R> <C> [ITALIC] d7, [ITALIC] d12 <C> 0.0073 <C> [BOLD] 0.0250 <C> 0.01 <R> <C> [ITALIC] d8, [ITALIC] d9 <C> 0.97 <C> 0.95 <C> 0.97 <CAP> TABLE II: A comparison between the measures CS, SRCC, PCC
<R> <C> Classifier <C> Fine-tuning <C> Speed (s) <C> BLEU <R> <C> Baseline (tied (6,6)) <C> [EMPTY] <C> 2,773 <C> 35.0 <R> <C> Oracle (tied) <C> [EMPTY] <C> 1,812 <C> 42.1 <R> <C> (#1) 8 layers, 8 heads <C> ✓ <C> 2,736 <C> 35.0 <R> <C> (#2) 2 layers, 4 heads <C> ✓ <C> 2,686 <C> 34.8 <R> <C> (#3) 2 layers, 4 heads <C> [EMPTY] <C> 2,645 <C> 34.7 <R> <C> (#4) 4 layers, 2 heads <C> [EMPTY] <C> 2,563 <C> 34.3 <CAP> Table 2: Dynamic layer combination selection results in decoding speed (in seconds, batch size of 1) and BLEU, including the baseline and oracle for the WMT newstest2018 using the tied-multi model architecture.
<R> <C> Inputs <C> Character <C> Phoneme <R> <C> Baseline <C> 3.77 <C> 4.12 <R> <C> [BOLD] Bi-Forward-Decoder <C> [BOLD] 4.07 <C> [BOLD] 4.26 <R> <C> Recording <C> 4.48 <C> 4.48 <CAP> Table 1: The MOS of different models in out-of-domain test, with confidence level of 95%.
<R> <C> Inputs <C> Character <C> Phoneme <R> <C> Baseline <C> 4.10 <C> 4.36 <R> <C> [BOLD] Bi-Forward-Decoder <C> [BOLD] 4.24 <C> [BOLD] 4.42 <R> <C> Recording <C> 4.49 <C> 4.49 <CAP> Table 4: The MOS of different models in in-domain test, with confidence level of 95%.
<R> <C> Language <C> [EMPTY] <C> Precision <C> Recall <C> Approximate <C> F1 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> match <C> Measure <R> <C> Hindi <C> Outer Level <C> 38.98 <C> 26.18 <C> 39.98 <C> 31.32 <R> <C> [EMPTY] <C> Inner Level <C> 08.30 <C> 25.00 <C> 08.47 <C> 12.46 <R> <C> Malayalam <C> Outer Level <C> 35.32 <C> 25.40 <C> 35.63 <C> 29.54 <R> <C> [EMPTY] <C> Inner Level <C> 39.05 <C> 17.83 <C> 39.05 <C> 24.48 <R> <C> Tamil <C> Outer Level <C> 54.32 <C> 28.19 <C> 55.04 <C> 37.11 <R> <C> [EMPTY] <C> Inner Level <C> 61.58 <C> 35.43 <C> 61.58 <C> 44.98 <R> <C> Average <C> Outer Level <C> 42.87 <C> 26.59 <C> 43.55 <C> 32.82 <R> <C> [EMPTY] <C> Inner Level <C> 36.31 <C> 26.08 <C> 36.36 <C> 30.36 <CAP> Table 3.6: Improved Results after FIRE-2014 Evaluation
<R> <C> [BOLD] Tools <C> [BOLD] 10 Fold-Cross <C> [BOLD] Development <C> [BOLD] Time <R> <C> [EMPTY] <C> [BOLD] Validation <C> [BOLD] Data <C> [BOLD] (mins) <R> <C> Mallet <C> 84.9 <C> 82.4 <C> 168.31 <R> <C> SVM <C> 79.8 <C> 76.3 <C> 20.15 <R> <C> CRFSuite <C> 88.9 <C> 85.2 <C> 4.12 <CAP> Table 3.7: Observations
<R> <C> [BOLD] Predication Pair <C> [BOLD] w2v <C> [BOLD] Apt <C> [BOLD] fT <C> [BOLD] ELMo <C> [BOLD] BERT <C> [BOLD] DNC <C> [BOLD] SNLI <R> <C> visit ⊨ leave <C> 0.36 <C> 0.09 <C> 0.53 <C> 0.59 <C> 0.69 <C> 0.69 <C> 0.28 <R> <C> is visiting ⊨ will leave <C> 0.57 <C> 0.02 <C> 0.60 <C> 0.60 <C> [BOLD] 0.77 <C> 0.26 <C> [BOLD] 0.26 <R> <C> is visiting ⊭ has left <C> 0.58 <C> 0.03 <C> 0.71 <C> 0.65 <C> 0.72 <C> 0.32 <C> 0.20 <R> <C> visit ⊨ arrive <C> 0.45 <C> 0.07 <C> 0.55 <C> 0.49 <C> 0.71 <C> 0.58 <C> 0.45 <R> <C> is visiting ⊨ has arrived <C> [BOLD] 0.62 <C> [BOLD] 0.04 <C> [BOLD] 0.69 <C> [BOLD] 0.51 <C> [BOLD] 0.84 <C> 0.25 <C> [BOLD] 0.51 <R> <C> is visiting ⊭ will arrive <C> 0.57 <C> 0.01 <C> 0.60 <C> 0.50 <C> 0.81 <C> 0.32 <C> 0.25 <R> <C> win ⊨ play <C> 0.52 <C> 0.14 <C> 0.54 <C> 0.59 <C> 0.73 <C> 0.39 <C> 0.32 <R> <C> has won ⊨ has played <C> [BOLD] 0.75 <C> [BOLD] 0.25 <C> [BOLD] 0.88 <C> [BOLD] 0.60 <C> [BOLD] 0.85 <C> [BOLD] 0.55 <C> 0.23 <R> <C> has won ⊭ will play <C> 0.60 <C> 0.11 <C> 0.64 <C> 0.55 <C> 0.78 <C> 0.31 <C> 0.36 <CAP> Table 4: Similarity scores between the example predicates. DNC and SNLI refer to the two biLSTMs pre-trained on DNC and SNLI, respectively.
<R> <C> [EMPTY] <C> LSTM <C> MN-S <C> MN <C> SDNC <C> WMN <C> WMN† <R> <C> 1: 1 supporting fact <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <R> <C> 2: 2 supporting facts <C> 81.9 <C> 0.0 <C> 1.0 <C> 0.6 <C> 0.7 <C> 0.3 <R> <C> 3: 3 supporting facts <C> 83.1 <C> 0.0 <C> 6.8 <C> 0.7 <C> 5.3 <C> 4.6 <R> <C> 4: 2 argument relations <C> 0.2 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <R> <C> 5: 3 argument relations <C> 1.2 <C> 0.3 <C> 6.1 <C> 0.3 <C> 0.6 <C> 0.4 <R> <C> 6: yes/no questions <C> 51.8 <C> 0.0 <C> 0.1 <C> 0.0 <C> 0.0 <C> 0.0 <R> <C> 7: counting <C> 24.9 <C> 3.3 <C> 6.6 <C> 0.2 <C> 0.6 <C> 0.5 <R> <C> 8: lists/sets <C> 34.1 <C> 1.0 <C> 2.7 <C> 0.2 <C> 0.2 <C> 0.3 <R> <C> 9: simple negation <C> 20.2 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <R> <C> 10: indefinite knowledge <C> 30.1 <C> 0.0 <C> 0.5 <C> 0.2 <C> 0.5 <C> 0.0 <R> <C> 11: basic coreference <C> 10.3 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.3 <C> 0.0 <R> <C> 12: conjunction <C> 23.4 <C> 0.0 <C> 0.1 <C> 0.1 <C> 0.0 <C> 0.0 <R> <C> 13: compound coreference <C> 6.1 <C> 0.0 <C> 0.0 <C> 0.1 <C> 0.0 <C> 0.0 <R> <C> 14: time reasoning <C> 81.0 <C> 0.0 <C> 0.0 <C> 0.1 <C> 0.0 <C> 0.0 <R> <C> 15: basic deduction <C> 78.7 <C> 0.0 <C> 0.2 <C> 0.0 <C> 0.0 <C> 0.0 <R> <C> 16: basic induction <C> 51.9 <C> 0.0 <C> 0.2 <C> 54.1 <C> 0.0 <C> 0.3 <R> <C> 17: positional reasoning <C> 50.1 <C> 24.6 <C> 41.8 <C> 0.3 <C> 0.3 <C> 0.1 <R> <C> 18: size reasoning <C> 6.8 <C> 2.1 <C> 8.0 <C> 0.1 <C> 0.1 <C> 0.4 <R> <C> 19: path finding <C> 90.3 <C> 31.9 <C> 75.7 <C> 1.2 <C> 0.6 <C> 0.0 <R> <C> 20: agent’s motivations <C> 2.1 <C> 0. <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <R> <C> Mean Error (%) <C> 36.4 <C> 3.2 <C> 7.5 <C> 2.8 <C> 0.4 <C> [BOLD] 0.3 <R> <C> Failed tasks (err. > 5%) <C> 16 <C> 2 <C> 6 <C> 1 <C> 1 <C> [BOLD] 0 <CAP> Table 1: Test accuracies on the jointly trained bAbI-10k dataset. MN-S stands for strongly supervised Memory Network, MN-U for end-to-end Memory Network without supervision, and WMN for Working Memory Network. Results for LSTM, MN-U, and MN-S are taken from Sukhbaatar et al. (2015). Results for SDNC are taken from Rae et al. (2016). WMN† is an ensemble of two Working Memory Networks.
<R> <C> [BOLD] Noise Type <C> [BOLD] SNR <C> [BOLD] SID EER (%) <C> [BOLD] SID DCF <C> [BOLD] VoiceID Loss [shon2019voiceid] EER (%) <C> [BOLD] VoiceID Loss [shon2019voiceid] DCF <C> [BOLD] SE+SID EER (%) <C> [BOLD] SE+SID DCF <C> [BOLD] SE-MS+SID EER (%) <C> [BOLD] SE-MS+SID DCF <C> [BOLD] SE+SID-MS EER (%) <C> [BOLD] SE+SID-MS DCF <R> <C> [BOLD] Noise <C> 0 <C> 16.94 <C> 0.933 <C> 16.56 <C> 0.938 <C> 16.20 <C> 0.912 <C> [BOLD] 15.95 <C> [BOLD] 0.901 <C> 16.13 <C> 0.908 <R> <C> [BOLD] Noise <C> 5 <C> 12.48 <C> 0.855 <C> 12.26 <C> 0.830 <C> 11.99 <C> 0.819 <C> [BOLD] 11.76 <C> [BOLD] 0.805 <C> 11.78 <C> 0.812 <R> <C> [BOLD] Noise <C> 10 <C> 10.03 <C> 0.760 <C> 9.86 <C> 0.747 <C> 9.54 <C> 0.732 <C> [BOLD] 9.17 <C> [BOLD] 0.717 <C> 9.29 <C> 0.727 <R> <C> [BOLD] Noise <C> 15 <C> 8.84 <C> 0.648 <C> 8.69 <C> 0.686 <C> 8.48 <C> 0.665 <C> [BOLD] 8.08 <C> [BOLD] 0.639 <C> 8.10 <C> 0.641 <R> <C> [BOLD] Noise <C> 20 <C> 7.96 <C> 0.594 <C> 7.83 <C> 0.639 <C> 7.52 <C> 0.629 <C> [BOLD] 7.07 <C> [BOLD] 0.615 <C> 7.09 <C> 0.623 <R> <C> [BOLD] Music <C> 0 <C> 17.04 <C> 0.940 <C> 16.24 <C> 0.913 <C> 15.96 <C> 0.901 <C> [BOLD] 15.58 <C> [BOLD] 0.899 <C> 15.89 <C> 0.904 <R> <C> [BOLD] Music <C> 5 <C> 11.54 <C> 0.828 <C> 11.44 <C> 0.818 <C> 11.15 <C> 0.805 <C> [BOLD] 10.93 <C> [BOLD] 0.791 <C> 11.04 <C> 0.801 <R> <C> [BOLD] Music <C> 10 <C> 9.69 <C> 0.749 <C> 9.13 <C> 0.733 <C> 9.12 <C> 0.731 <C> [BOLD] 8.87 <C> [BOLD] 0.714 <C> 8.97 <C> 0.725 <R> <C> [BOLD] Music <C> 15 <C> 8.40 <C> 0.689 <C> 8.10 <C> 0.677 <C> 8.08 <C> 0.643 <C> [BOLD] 7.62 <C> [BOLD] 0.621 <C> 7.77 <C> 0.629 <R> <C> [BOLD] Music <C> 20 <C> 7.70 <C> 0.665 <C> 7.48 <C> 0.635 <C> 7.39 <C> 0.619 <C> [BOLD] 7.13 <C> [BOLD] 0.607 <C> 7.26 <C> 0.614 <R> <C> [BOLD] Babble <C> 0 <C> 38.90 <C> 1.000 <C> 37.96 <C> 1.000 <C> 37.53 <C> 0.999 <C> 37.55 <C> 0.999 <C> [BOLD] 37.46 <C> [BOLD] 0.998 <R> <C> [BOLD] Babble <C> 5 <C> 28.04 <C> 0.998 <C> 27.12 <C> 0.996 <C> 26.97 <C> 0.979 <C> 26.42 <C> 0.981 <C> [BOLD] 26.35 <C> [BOLD] 0.977 <R> <C> [BOLD] Babble <C> 10 <C> 17.34 <C> 0.917 <C> 16.66 <C> 0.926 <C> 16.44 <C> 0.911 <C> [BOLD] 16.30 <C> [BOLD] 0.907 <C> 16.36 <C> 0.911 <R> <C> [BOLD] Babble <C> 15 <C> 11.31 <C> 0.795 <C> 11.25 <C> 0.807 <C> 11.24 <C> 0.801 <C> [BOLD] 10.89 <C> [BOLD] 0.795 <C> 10.94 <C> 0.801 <R> <C> [BOLD] Babble <C> 20 <C> 9.12 <C> 0.720 <C> 8.99 <C> 0.705 <C> 8.77 <C> 0.695 <C> [BOLD] 8.39 <C> [BOLD] 0.677 <C> 8.51 <C> 0.688 <R> <C> [BOLD] Original <C> [EMPTY] <C> 6.92 <C> 0.565 <C> 6.79 <C> 0.574 <C> 6.41 <C> 0.541 <C> [BOLD] 6.18 <C> [BOLD] 0.528 <C> 6.26 <C> 0.535 <CAP> Table 5: Speaker Verification Results on Voxceleb1 test data when it being corrupted by different types of noise (Noise, Music and Babble) at different SNR (0-20 dB). Four different scenarios are tested: only use SID-Net (SID); A joint system combining the SE-Net with the SID-Net without a multi-stage attention (SE+SID); A joint system using both SE-Net and SID-Net, but without being used in multi-stage attention (SE-MS+SID); A joint system consisting of SE-Net and SID-Net, with a multi-stage attention being used in SID-Net (SE+SID-MS). The results of VoiceID Loss [shon2019voiceid] is listed and works as a baseline.
<R> <C> [BOLD] Row Density <C> [BOLD] % Coverage <C> [BOLD] Avg Loss <R> <C> 0.1% <C> 90 <C> 14±5.6 <R> <C> 1% <C> 9.9 <C> 139±34.4 <R> <C> 10% <C> 0.1 <C> 39±1.4 <R> <C> 0.1% <C> 33.3 <C> 15±5.2 <R> <C> 1% <C> 33.3 <C> 92±12.4 <R> <C> 10% <C> 33.3 <C> 750±28.4 <R> <C> 0.1% <C> 0.1 <C> 16±5.0 <R> <C> 1% <C> 9.9 <C> 93±12.3 <R> <C> 10% <C> 90 <C> 754±28.5 <CAP> Table 1: The effect of sentence-label density on reconstruction loss using a low-rank approximation. See Section 4 for further details.
<R> <C> Metric <C> 20NG <C> Wiki <C> NYT <C> Mean <R> <C> [ITALIC] μ <C> 0.185 <C> 0.030 <C> 0.148 <C> 0.121 <R> <C> [ITALIC] σ <C> 0.480 <C> 0.295 <C> 0.600 <C> 0.458 <R> <C> [ITALIC] cv <C> [BOLD] 0.679 <C> [BOLD] 0.703 <C> [BOLD] 0.774 <C> [BOLD] 0.719 <CAP> Table 1: Pearson’s r of each potential metric of posterior variability with human judgments
<R> <C> [BOLD] Method <C> [BOLD] 20NG <C> [BOLD] Wiki <C> [BOLD] NYT <C> [BOLD] Mean <R> <C> CV Roder et al. ( 2015 ) <C> 0.129 <C> 0.385 <C> 0.248 <C> 0.254 <R> <C> CP Roder et al. ( 2015 ) <C> 0.378 <C> 0.403 <C> 0.061 <C> 0.280 <R> <C> DS Aletras and Stevenson ( 2013 ) <C> 0.461 <C> 0.423 <C> 0.365 <C> 0.416 <R> <C> NPMI Lau et al. ( 2014 ) <C> 0.639 <C> 0.568 <C> 0.639 <C> 0.615 <R> <C> PMI Newman et al. ( 2010 ) <C> 0.602 <C> 0.550 <C> 0.623 <C> 0.591 <R> <C> Coherence Mimno et al. ( 2011 ) <C> 0.280 <C> 0.102 <C> 0.535 <C> 0.305 <R> <C> Stability Xing and Paul ( 2018 ) <C> 0.230 <C> 0.137 <C> 0.322 <C> 0.230 <R> <C> Variability <C> [BOLD] 0.679 <C> [BOLD] 0.703 <C> [BOLD] 0.774 <C> [BOLD] 0.719 <CAP> Table 2: Pearson’s r correlation with human judgments for metrics.
<R> <C> [EMPTY] <C> BLEU <C> METEOR <C> CIDEr <R> <C> Top-50 Attention <C> 26.2 <C> 20.5 <C> 3.91 <R> <C> Entity Conditioning <C> 27.0 <C> 20.8 <C> 4.04 <R> <C> GRU-S2S <C> 27.7 <C> 21.6 <C> 4.43 <R> <C> VaTeX-S2S  <C> 28.5 <C> 21.6 <C> 4.51 <R> <C> Transformer-S2S† <C> 29.0 <C> 21.1 <C> 4.52 <CAP> Table 3: Overall test set results: † marks our submission.
<R> <C> [BOLD] Model <C> [BOLD] Movie Relevance <C> [BOLD] Movie Relevance <C> [BOLD] Movie Diversity <C> [BOLD] Movie Diversity <C> [BOLD] Ubuntu Relevance <C> [BOLD] Ubuntu Relevance <C> [BOLD] Ubuntu Diversity <C> [BOLD] Ubuntu Diversity <R> <C> [BOLD] Model <C> BLEU <C> ROUGE <C> DIST-1/2 <C> NASL <C> BLEU <C> ROUGE <C> DIST-1/2 <C> NASL <R> <C> HRED <C> 0.0474 <C> 0.0384 <C> 0.0026/0.0056 <C> 0.535 <C> 0.0177 <C> 0.0483 <C> 0.0203/0.0466 <C> 0.892 <R> <C> VHRED <C> 0.0606 <C> 0.1181 <C> 0.0048/0.0163 <C> 0.831 <C> 0.0171 <C> 0.0855 <C> 0.0297/0.0890 <C> 0.873 <R> <C> hredGAN_u <C> 0.0493 <C> 0.2416 <C> 0.0167/0.1306 <C> 0.884 <C> 0.0137 <C> 0.0716 <C> 0.0260/0.0847 <C> 1.379 <R> <C> hredGAN_w <C> 0.0613 <C> 0.3244 <C> 0.0179/0.1720 <C> 1.540 <C> 0.0216 <C> 0.1168 <C> 0.0516/0.1821 <C> 1.098 <R> <C> DAIM <C> 0.0155 <C> 0.0077 <C> 0.0005/0.0006 <C> 0.721 <C> 0.0015 <C> 0.0131 <C> 0.0013/0.0048 <C> [BOLD] 1.626 <R> <C> Transformer <C> 0.0360 <C> 0.0760 <C> 0.0107/0.0243 <C> [BOLD] 1.602 <C> 0.0030 <C> 0.0384 <C> 0.0465/0.0949 <C> 0.566 <R> <C> aBoots_u_gau <C> 0.0642 <C> 0.3326 <C> 0.0526/0.2475 <C> 0.764 <C> 0.0115 <C> 0.2064 <C> 0.1151/0.4188 <C> 0.819 <R> <C> aBoots_w_gau <C> 0.0749 <C> 0.3755 <C> 0.0621/0.3051 <C> 0.874 <C> 0.0107 <C> 0.1712 <C> [BOLD] 0.1695/0.7661 <C> 1.235 <R> <C> aBoots_u_uni <C> 0.0910 <C> 0.4015 <C> 0.0660/0.3677 <C> 0.975 <C> 0.0156 <C> 0.1851 <C> 0.0989/0.4181 <C> 0.970 <R> <C> aBoots_w_uni <C> 0.0902 <C> [BOLD] 0.4048 <C> [BOLD] 0.0672/0.3653 <C> 0.972 <C> 0.0143 <C> 0.1984 <C> 0.1214/0.5443 <C> 1.176 <R> <C> aBoots_u_cat <C> 0.0880 <C> 0.4063 <C> 0.0624/0.3417 <C> 0.918 <C> 0.0210 <C> 0.1491 <C> 0.0523/0.1795 <C> 1.040 <R> <C> [BOLD] aBoots_w_cat <C> [BOLD] 0.0940 <C> 0.3973 <C> 0.0613/0.3476 <C> 1.016 <C> [BOLD] 0.0233 <C> [BOLD] 0.2292 <C> 0.1288/0.5190 <C> 1.208 <CAP> Table 1: Automatic evaluation of generator performance
<R> <C> [BOLD] Model <C> [BOLD] Movie Relevance <C> [BOLD] Movie Relevance <C> [BOLD] Movie Diversity <C> [BOLD] Movie Diversity <C> [BOLD] Ubuntu Relevance <C> [BOLD] Ubuntu Relevance <C> [BOLD] Ubuntu Diversity <C> [BOLD] Ubuntu Diversity <R> <C> [BOLD] Model <C> BLEU <C> ROUGE <C> DIST-1/2 <C> NASL <C> BLEU <C> ROUGE <C> DIST-1/2 <C> NASL <R> <C> aBoots_g_u_gau <C> 0.0638 <C> 0.3193 <C> 0.0498/0.2286 <C> 0.778 <C> 0.0150 <C> 0.1298 <C> 0.0480/0.1985 <C> 0.960 <R> <C> aBoots_g_w_gau <C> 0.0729 <C> 0.3678 <C> 0.0562/0.3049 <C> 1.060 <C> 0.0123 <C> 0.1370 <C> 0.0646/0.1820 <C> 0.841 <R> <C> aBoots_g_u_uni <C> 0.0801 <C> 0.3972 <C> 0.0655/0.3414 <C> 0.869 <C> 0.0124 <C> 0.1424 <C> 0.0636/0.1853 <C> 0.870 <R> <C> aBoots_g_w_uni <C> 0.0860 <C> [BOLD] 0.4046 <C> [BOLD] 0.0671/0.3514 <C> 0.838 <C> 0.0170 <C> 0.2049 <C> 0.1074/0.4646 <C> 1.349 <R> <C> aBoots_g_u_cat <C> 0.0836 <C> 0.3887 <C> 0.0597/0.3276 <C> 0.917 <C> 0.0131 <C> 0.1214 <C> 0.0597/0.3276 <C> 1.060 <R> <C> aBoots_g_w_cat <C> 0.0928 <C> [BOLD] 0.4029 <C> [BOLD] 0.0613/0.3358 <C> 0.976 <C> 0.0202 <C> [BOLD] 0.2343 <C> [BOLD] 0.1254/0.4805 <C> 0.873 <CAP> Table 3: Automatic evaluation of aBoots models with the generator bootstrapping only
<R> <C> [BOLD] System <C> [BOLD] Prompt 1 <C> [BOLD] Prompt 2 <C> [BOLD] Prompt 7 <C> [BOLD] Prompt 8 <C> [BOLD] Mean QWK <R> <C> taghipour-ng-2016-neural <C> 0.775 <C> [BOLD] 0.687 <C> 0.805 <C> 0.594 <C> 0.715 <R> <C> dong-zhang-2016-automatic <C> 0.805 <C> 0.613 <C> 0.758 <C> 0.644 <C> 0.705 <R> <C> tay-2018-skipflow <C> 0.832 <C> 0.684 <C> 0.800 <C> 0.697 <C> 0.753 <R> <C> Only Prompt (dong-etal-2017-attention) <C> 0.816 <C> 0.667 <C> 0.792 <C> 0.678 <C> 0.738 <R> <C> Extra Essays † <C> 0.828 <C> 0.672 <C> 0.802 <C> 0.685 <C> 0.747 <R> <C> Extra Essays + Gaze * <C> [BOLD] 0.833 <C> 0.681 <C> [BOLD] 0.806 <C> [BOLD] 0.699 <C> [BOLD] 0.754 <CAP> Table 4: Results of our experiments on the unseen essay sets our dataset. The first 3 rows are results reported from other state-of-the-art deep learning systems. The next 2 rows are the results obtained without using gaze behaviour (without and with the extra essays). The last row is the results from our system. The result is statistically significant with p = 0.0041 († denotes the baseline system, and * denotes a statistically significant result).
<R> <C> [EMPTY] <C> FB15K-237 MRR(Filtered) <C> FB15K-237 Hit@1 <C> FB15K-237 Hit@3 <C> FB15K-237 Hit@10 <C> WN18RR MRR(Filtered) <C> WN18RR Hit@1 <C> WN18RR Hit@3 <C> WN18RR Hit@10 <R> <C> DistMult <C> 0.241 <C> 0.155 <C> 0.263 <C> 0.43 <C> 0.39 <C> 0.44 <C> 0.49 <C> 0.447 <R> <C> TransE <C> 0.294 <C> - <C> - <C> 0.465 <C> 0.226 <C> - <C> - <C> 0.501 <R> <C> TransE-GCN <C> 0.315 <C> 0.229 <C> 0.324 <C> 0.477 <C> 0.233 <C> 0.203 <C> 0.338 <C> 0.508 <R> <C> ComplEx <C> 0.247 <C> 0.158 <C> 0.275 <C> 0.428 <C> 0.44 <C> 0.41 <C> 0.46 <C> 0.51 <R> <C> R-GCN <C> 0.248 <C> 0.153 <C> 0.258 <C> 0.417 <C> - <C> - <C> - <C> - <R> <C> ConvE <C> 0.325 <C> 0.237 <C> 0.356 <C> 0.501 <C> 0.43 <C> 0.40 <C> 0.44 <C> 0.52 <R> <C> RotatE <C> 0.338 <C> 0.241 <C> 0.375 <C> 0.533 <C> 0.476 <C> 0.428 <C> 0.492 <C> 0.571 <R> <C> RotatE-GCN <C> [BOLD] 0.356 <C> [BOLD] 0.252 <C> [BOLD] 0.388 <C> [BOLD] 0.555 <C> [BOLD] 0.485 <C> [BOLD] 0.438 <C> [BOLD] 0.51 <C> [BOLD] 0.578 <CAP> Table 2. Prediction results of different models on FB15K-237 and WN18RR
<R> <C> [EMPTY] <C> MRR <C> Hit@10 <R> <C> [BOLD] TransE-GCN-1 <C> 0.315 <C> 0.474 <R> <C> TransE-GCN-2 <C> 0.297 <C> 0.453 <R> <C> TransE-GCN-3 <C> 0.273 <C> 0.421 <R> <C> RotatE-GCN-1 <C> 0.347 <C> 0.546 <R> <C> [BOLD] RotatE-GCN-2 <C> 0.356 <C> 0.555 <R> <C> RotatE-GCN-3 <C> 0.331 <C> 0.525 <CAP> Table 3. Prediction results of our models on FB15K-237 in terms of different hops
<R> <C> Model <C> ES <C> HA <C> HR <C> SV <C> TR <C> ZH <R> <C> [ITALIC] Unsupervised models: <C> [ITALIC] Unsupervised models: <C> [ITALIC] Unsupervised models: <C> [ITALIC] Unsupervised models: <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> DTW <C> 29.7 <C> 20.1 <C> 13.7 <C> 24.2 <C> 11.9 <C> 27.1 <R> <C> Downsample <C> 19.4 <C> 10.7 <C> 11.2 <C> 16.6 <C> 8.0 <C> 20.2 <R> <C> AE-RNN (UTD) <C> 18.1 <C> 6.5 <C> 10.4 <C> 12.0 <C> 6.8 <C> 18.5 <R> <C> CAE-RNN (UTD) <C> 39.7 <C> 17.8 <C> 21.4 <C> 25.2 <C> 10.7 <C> 21.3 <R> <C> [ITALIC] Multilingual models: <C> [ITALIC] Multilingual models: <C> [ITALIC] Multilingual models: <C> [ITALIC] Multilingual models: <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> CAE-RNN <C> [BOLD] 56.0 <C> [BOLD] 32.7 <C> 29.9 <C> [BOLD] 36.7 <C> 20.9 <C> 34.2 <R> <C> ClassifierRNN <C> 54.3 <C> 29.5 <C> [BOLD] 32.9 <C> 33.5 <C> [BOLD] 21.2 <C> [BOLD] 34.5 <CAP> Table 1: AP (%) on test data for the zero-resource languages. The unsupervised CAE-RNNs are trained separately for each zero-resource language on segments from a UTD system applied to unlabelled monolingual data. The multilingual models are trained on ground truth word segments obtained by pooling labelled training data from seven well-resourced languages.
<R> <C> [BOLD] Method <C> [BOLD] ne-en  [BOLD] 1M <C> [BOLD] ne-en  [BOLD] 5M <C> [BOLD] si-en  [BOLD] 1M <C> [BOLD] si-en  [BOLD] 5M <R> <C> Pre-trained LASER <C> 6.06 <C> 1.49 <C> [BOLD] 7.82 <C> [BOLD] 5.56 <R> <C> LASER  [ITALIC] local <C> [BOLD] 7.37 <C> [BOLD] 3.15 <C> 7.49 <C> 5.01 <CAP> Table 4: Comparison of results on the flores devtest set using the constrained and the pre-trained vesions of LASER.
<R> <C> [BOLD] Model <C> [BOLD] F1 - OLID <C> [BOLD] F1 - MOLID <R> <C> BERT (OLID) <C> 0.8203 <C> 0.9088 <R> <C> BERT (MOLID) <C> 0.7280 <C> 0.9060 <R> <C> BERT(Pre-trained with MSE loss) <C> 0.8138 <C> 0.9107 <R> <C> BERT + MTL <C> 0.8341 <C> 0.9139 <R> <C> BERT + MTL (Ensemble) <C> [BOLD] 0.8382 <C> [BOLD] 0.9151 <CAP> Table 2: Experimental results on sub-task A. The evaluation metric is macro F1 score, which is official in OffensEval-2020.
<R> <C> Corpus (ID) <C> Speakers <C> Emotion neutral <C> Emotion happy <C> Emotion sad <C> Emotion angry <C> Gender female <C> Gender male <C> Naturalness natural <C> Naturalness acted <C> Languages <R> <C> AIBO (A) <C> 51 <C> 10967 <C> 889 <C> 0 <C> 1492 <C> 7579 <C> 5769 <C> 13348 <C> 0 <C> German <R> <C> EMODB (E) <C> 10 <C> 77 <C> 61 <C> 58 <C> 97 <C> 160 <C> 133 <C> 0 <C> 293 <C> German <R> <C> ENTERFACE (F) <C> 43 <C> 0 <C> 208 <C> 422 <C> 211 <C> 200 <C> 641 <C> 0 <C> 841 <C> English <R> <C> LDC (L) <C> 7 <C> 80 <C> 180 <C> 161 <C> 139 <C> 320 <C> 240 <C> 0 <C> 560 <C> English <R> <C> IEMOCAP (I) <C> 10 <C> 1708 <C> 595 <C> 2168 <C> 2206 <C> 336 <C> 6341 <C> 3177 <C> 3500 <C> English <R> <C> total <C> 121 <C> 12832 <C> 1933 <C> 2809 <C> 4145 <C> 8595 <C> 13124 <C> 16525 <C> 5194 <C> [EMPTY] <CAP> Table 1: Overview of the selected corpora (the number of utterances)
<R> <C> Upto n-gram <C> Average Accuracy (%) <R> <C> Unigram <C> [BOLD] 86.0 <R> <C> Bigram <C> 84.72 <R> <C> Trigram <C> 83.83 <CAP> Table 2. Average accuracy over different n-gram ranges
<R> <C> Context Size (number of sentences) <C> Average Accuracy (%) <R> <C> 1 <C> [BOLD] 86.0 <R> <C> 2 <C> 84.82 <R> <C> 3 <C> 83.73 <R> <C> 4 <C> 83.92 <CAP> Table 3. Average accuracy over different context sizes
<R> <C> Model <C> Accuracy (%) <R> <C> Baseline <C> 78 <R> <C> +Removed Note and Information <C> 84 <R> <C> +Next step conditional overlap <C> 86 <R> <C> +Stop at sub-list item or paragraph <C> 90 <CAP> Table 6. Accuracy with different models for decision block extraction
<R> <C> [BOLD] Models <C> [BOLD] Perplexity ↓ <C> [BOLD] BLEU-4 ↑ <C> [BOLD] ROUGE-L↑ <C> [BOLD] Token-REP-4(%) ↓ <C> [BOLD] Sent-REP-4(%) ↓ <C> [BOLD] Unique 4-grams ↑ <R> <C> Seq2seq <C> 21.89 <C> 15.69 <C> 33.46 <C> 0.8206 <C> 21.33 <C> 12.98k <R> <C> Seq2seq + Adapation <C> 24.45 <C> 13.11 <C> 31.14 <C> 2.686 <C> 22.30 <C> 12.01k <R> <C> PAS <C> 7.616 <C> [BOLD] 21.46 <C> [BOLD] 39.47 <C> [BOLD] 0.3722 <C> [BOLD] 8.882 <C> [BOLD] 18.20k <R> <C> PAS - SIA <C> [BOLD] 7.588 <C> 21.13 <C> 39.27 <C> 0.4004 <C> 9.673 <C> 18.10k <R> <C> PAS - Adaption <C> 9.273 <C> 19.23 <C> 37.05 <C> 0.5398 <C> 14.61 <C> 15.58k <R> <C> PAS - SIA - Adaption <C> 9.272 <C> 18.95 <C> 36.82 <C> 0.5165 <C> 14.91 <C> 15.48k <R> <C> Human(Expert) <C> [EMPTY] <C> 100.0 <C> 100.0 <C> 0.0293 <C> 7.792 <C> 21.97k <R> <C> Human(Original) <C> [EMPTY] <C> 16.99 <C> 34.97 <C> 0.0745 <C> 0.8744 <C> 22.44k <CAP> Table 1: Performance results of all models. Across models, we compare fine-tuning at different stages with different loss functions. MLE stands for fine-tuning with maximum-likelihood estimation, while SIA stands for fine-tuning with Self Importance-Aware loss. Please refer to Automatic Evaluation Metrics section for each metrics explanation. * SIA is set with α=0.2 and β=40 here.
<R> <C> [BOLD] # <C> [BOLD] Layers <C> [BOLD] # Para. <C> [BOLD] Train <C> [BOLD] BLEU <R> <C> 1 <C> None <C> 88.0M <C> 1.92 <C> 27.31 <R> <C> 2 <C> [1-6] <C> 100.6M <C> 1.20 <C> 28.28 <R> <C> 3 <C> [4-6] <C> 94.3M <C> 1.54 <C> 28.26 <R> <C> 4 <C> [1-3] <C> 94.3M <C> 1.54 <C> 28.27 <R> <C> 5 <C> [1,2] <C> 92.2M <C> 1.67 <C> 28.26 <R> <C> 6 <C> [6] <C> 90.1M <C> 1.88 <C> 27.68 <R> <C> 7 <C> [1] <C> 90.1M <C> 1.88 <C> 27.75 <CAP> Table 3: Evaluation of different layers in the encoder, which are implemented as multi-head self-attention with the EM routing based information aggregation. “1” denotes the bottom layer, and “6” the top layer.
<R> <C> [EMPTY] <C> Tense Dev-ST <C> Tense Probing <C> SubjNum Dev-SS <C> SubjNum Probing <C> ObjNum Dev-SO <C> ObjNum Probing <R> <C> Majority <C> 37.90 <C> 50.00 <C> 36.88 <C> 50.0 <C> 39.52 <C> 50.0 <R> <C> CBOW-DS <C> 57.57 <C> 82.36 <C> 58.4 <C> 76.55 <C> 55.85 <C> 75.49 <R> <C> CBOW-PT <C> 60.31 <C> 82.2 <C> 58.2 <C> 75.69 <C> 59.15 <C> 74.38 <R> <C> BiLSTM-Av-DS <C> 63.53 <C> 82.93 <C> 64.24 <C> 79.53 <C> 66.23 <C> 76.11 <R> <C> BiLSTM-Av-PT <C> 65.08 <C> 82.79 <C> 66.76 <C> 78.81 <C> 67.08 <C> 75.48 <R> <C> BiLSTM-Max-DS <C> 63.35 <C> 81.14 <C> 65.91 <C> 78.56 <C> 65.94 <C> 74.79 <R> <C> BiLSTM-Max-PT <C> 64.6 <C> 81.04 <C> 66.87 <C> 79.51 <C> 66.98 <C> 72.44 <R> <C> BiLSTM-Last-DS <C> 61.08 <C> 80.43 <C> 64.2 <C> 81.52 <C> 62.26 <C> 72.65 <R> <C> BiLSTM-Last-PT <C> 63.89 <C> 78.44 <C> 66.18 <C> 78.9 <C> 66.04 <C> 72.82 <CAP> Table 2: Performance comparisons of task-specific and downsampled models. Dev-ST is MultiNLI development set controlled for tense, DEV-SS is MultiNLI development set controlled for subject number, Dev-SO is MultiNLI development set controlled for object number. PT is model trained on data partitioned by linguistic property. DS is models trained on downsampled data from MultiNLI to match the number of instances in PT.
<R> <C> Model <C> P <C> R <C> F1 <R> <C> CNN† <C> 50.7 <C> 43.0 <C> 46.5 <R> <C> GRU+Attention† <C> 53.0 <C> 46.3 <C> 49.5 <R> <C> BRAN <C> 48.0 <C> 54.1 <C> [BOLD] 50.8 ± .01 <CAP> Table 4: Precision, recall, and F1 results on the Biocreative VI Chem-Prot Dataset. † denotes results from Liu et al. (2017)
<R> <C> [EMPTY] <C> P <C> R <C> F1 <R> <C> [BOLD] Total <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Micro F1 <C> 44.8 <C> 50.2 <C> 47.3 <R> <C> Macro F1 <C> 34.0 <C> 29.8 <C> 31.7 <R> <C> [BOLD] Chemical / Disease <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> marker/mechanism <C> 46.2 <C> 57.9 <C> 51.3 <R> <C> therapeutic <C> 55.7 <C> 67.1 <C> 60.8 <R> <C> [BOLD] Gene / Disease <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> marker/mechanism <C> 42.2 <C> 44.4 <C> 43.0 <R> <C> therapeutic <C> 52.6 <C> 10.1 <C> 15.8 <R> <C> [BOLD] Chemical / Gene <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> increases_expression <C> 39.7 <C> 48.0 <C> 43.3 <R> <C> increases_metabolic_proc <C> 26.3 <C> 35.5 <C> 29.9 <R> <C> decreases_expression <C> 34.4 <C> 32.9 <C> 33.4 <R> <C> increases_activity <C> 24.5 <C> 24.7 <C> 24.4 <R> <C> affects_response <C> 40.9 <C> 35.5 <C> 37.4 <R> <C> decreases_activity <C> 30.8 <C> 19.4 <C> 23.5 <R> <C> affects_transport <C> 28.7 <C> 23.8 <C> 25.8 <R> <C> increases_reaction <C> 12.8 <C> 5.6 <C> 7.4 <R> <C> decreases_reaction <C> 12.3 <C> 5.7 <C> 7.4 <R> <C> decreases_metabolic_proc <C> 28.9 <C> 7.0 <C> 11.0 <CAP> Table 7: BRAN precision, recall and F1 results for the full CTD dataset by relation type. The model is optimized for micro F1 score across all types.
<R> <C> [EMPTY] <C> [ITALIC]  [BOLD] Obama care data set  [ITALIC] f1 <C> [ITALIC]  [BOLD] Obama care data set  [ITALIC] f2 <C> [ITALIC]  [BOLD] Obama care data set  [ITALIC] f3 <C> [ITALIC]  [BOLD] Obama care data set  [ITALIC] f4 <C> [ITALIC]  [BOLD] Obama care data set  [ITALIC] f5 <C> [ITALIC]  [BOLD] Death Penalty data set  [ITALIC] f1 <C> [ITALIC]  [BOLD] Death Penalty data set  [ITALIC] f2 <C> [ITALIC]  [BOLD] Death Penalty data set  [ITALIC] f3 <C> [ITALIC]  [BOLD] Death Penalty data set  [ITALIC] f4 <C> [ITALIC]  [BOLD] Death Penalty data set  [ITALIC] f5 <R> <C> KNN <C> 0.6245 <C> 0.6667 <C> 0.6017 <C> 0.6025 <C> 0.5845 <C> 0.7125 <C> 0.7500 <C> 0.6874 <C> 0.6850 <C> 0.6536 <R> <C> SVM <C> 0.5575 <C> 0.5708 <C> 0.5575 <C> 0.5520 <C> 0.5342 <C> 0.6542 <C> 0.6608 <C> 0.6542 <C> 0.6450 <C> 0.6347 <R> <C> RAkEL <C> 0.4675 <C> 0.4983 <C> 0.4483 <C> 0.4517 <C> 0.4452 <C> 0.5992 <C> 0.5750 <C> 0.5988 <C> 0.5842 <C> 0.5725 <R> <C> RAkEL+sum <C> 0.4583 <C> 0.5025 <C> 0.4325 <C> 0.4315 <C> 0.4220 <C> 0.5975 <C> 0.5733 <C> 0.6017 <C> 0.5788 <C> 0.5650 <R> <C> RAkEL+wsum <C> 0.4571 <C> 0.5025 <C> 0.4310 <C> 0.4235 <C> 0.4112 <C> 0.5967 <C> 0.5733 <C> 0.6000 <C> 0.5742 <C> 0.5554 <CAP> Table 6: The Hamming loss of different methods on Obama care and Death Penalty data set. The notations f1, f2, f3, f4, and f5 denote features 1-gram, 2-gram, (1+2)-gram, (1+2)-gram + POS and (1+2)-gram + POS + STAT. Smaller value denotes better performance.
<R> <C> [BOLD] Method <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1-measure <R> <C> [BOLD] CNN-image <C> 0.5387 <C> 0.4215 <C> 0.4729 <R> <C> [BOLD] LR-text-1000 <C> 0.5703 <C> 0.4114 <C> 0.4780 <R> <C> [BOLD] CNN-text-1000 <C> 0.8722 <C> 0.9079 <C> 0.8897 <R> <C> [BOLD] LSTM-text-400 <C> 0.9146 <C> 0.8704 <C> 0.8920 <R> <C> [BOLD] GRU-text-400 <C> 0.8875 <C> 0.8643 <C> 0.8758 <R> <C> [BOLD] TI-CNN-1000 <C> [BOLD]  0.9220 <C> [BOLD]  0.9277 <C> [BOLD]  0.9210 <CAP> TABLE IV: The experimental results on many baseline methods. The number after the name of the model is the maximum input length for textual information. For those news text less than 1,000 words, we padded the sequence with 0.
<R> <C> Method <C> [ITALIC] prec@0.5 <C> [ITALIC] prec@0.6 <C> [ITALIC] prec@0.7 <C> [ITALIC] prec@0.8 <C> [ITALIC] prec@0.9 <C> [ITALIC] IoU <R> <C> LingUNet (1x1) <C> 60.38 <C> 51.33 <C> 40.76 <C> 27.64 <C> 8.35 <C> 53.34 <R> <C> LingUNet (3x3) <C> 61.94 <C> 53.66 <C> 44.64 <C> 31.18 <C> 11.47 <C> 54.48 <R> <C> 1x1 Text Kernels <C> 67.58 <C> 59.42 <C> 49.79 <C> 36.16 <C> 12.57 <C> 57.57 <R> <C> No Dropout <C> 70.88 <C> 64.57 <C> 56.18 <C> 42.55 <C> 15.24 <C> 59.85 <R> <C> No Multi-Scale Loss <C> 71.37 <C> 64.20 <C> 55.77 <C> 42.30 <C> 15.64 <C> 60.09 <R> <C> Our Model <C> [BOLD] 71.51 <C> [BOLD] 65.04 <C> [BOLD] 56.79 <C> [BOLD] 43.23 <C> [BOLD] 16.18 <C> [BOLD] 60.50 <CAP> Table 3: Ablation results on the validation split of the UNC dataset with prec@X and mIoU metrics. In these experiments, the depths of architectures equal to 3.
<R> <C> [BOLD] Filtering <C> [BOLD] P <C> [BOLD] R <C> [BOLD] Multiple <R> <C> Unfiltered <C> 68.25 <C> 100 <C> 11x <R> <C> Iter =1 <C> 90.06 <C> 13.20 <C> 2x <R> <C> Iter ≤3 <C> 81.29 <C> 35.73 <C> 4x <R> <C> PBR score ≤0.6 <C> 90.14 <C> 5.48 <C> 1.42x <R> <C> PBR score ≤0.8 <C> 74.86 <C> 34.45 <C> 4.14x <R> <C> Aligner score ≥.99 <C> 85.01 <C> 32.56 <C> 3.61x <R> <C> Aligner score ≥.95 <C> 76.72 <C> 85.00 <C> 8.56x <R> <C> Lax conjunction <C> 87.73 <C> 20.82 <C> 2.62x <R> <C> Strict conjunction <C> 92.54 <C> 5.31 <C> 1.39x <R> <C> P-Classifier <C> [BOLD] 95.00 <C> 15.61 <C> 2.28x <R> <C> R-Classifier <C> 81.19 <C> [BOLD] 96.99 <C> 10.27x <CAP> Table 3: Human evaluation of system outputs across several filtering methods, with manually-judged Precision for the subset of outputs remaining after applying the given filter, Recall of sentences manually judged to be acceptable, and the Multiple (in terms of number of sentences) of the resulting dataset in relation to the original seed corpus. Filtering methods consider the iteration number, and scores from the paraphrase and aligner models for a given system output. The “lax” row applies a filter consisting of the conjunction of the criteria from rows 3, 5, and 7 (relatively lenient conditions) whereas the “strict” row conjoins the criteria from rows 2, 4, and 6 (which are stricter, and lead to higher precision but fewer lexical units).
<R> <C> [EMPTY] <C> Score Distribution textual↑ conceptual↑ <C> Score Distribution textual↓ conceptual↑ <R> <C> [ITALIC] SoulMateConcept <C> 0.07 <C> 0.30 <R> <C> [ITALIC] SoulMateContent <C> 0.43 <C> 0.05 <R> <C> [ITALIC] SoulMateJoint <C> [BOLD] 0.67 <C> [BOLD] 0.32 <R> <C> Temporal Collective <C> 0.63 <C> 0.01 <R> <C> CBOW Enriched <C> 0.48 <C> 0 <R> <C> Document Vector <C> 0.21 <C> 0 <R> <C> Exact Matching <C> 0.39 <C> 0.01 <CAP> TABLE V: Precision of author similarity in subgraph mining
<R> <C> [BOLD] Embedding <C> [BOLD] Author concept vector cluster type <C> [BOLD] Tweet vector combination type  [BOLD] Avg <C> [BOLD] Tweet vector combination type  [BOLD] Avg <C> [BOLD] Tweet vector combination type  [BOLD] Sum <C> [BOLD] Tweet vector combination type  [BOLD] Sum <R> <C> [BOLD] Embedding <C> [BOLD] Author concept vector cluster type <C> P [ITALIC] Textual <C> P [ITALIC] Conceptual <C> P [ITALIC] Textual <C> P [ITALIC] Conceptual <R> <C> CBOW <C> K-Medoids ( [ITALIC] K=22) <C> 0.10869 <C> 0.08405 <C> 0.10869 <C> 0.08405 <R> <C> CBOW <C> DBScan ( [ITALIC] ϵ=0.36) <C> 0.09166 <C> 0.06666 <C> 0.09166 <C> 0.06666 <R> <C> Collective <C> K-Medoids ( [ITALIC] K=22) <C> [BOLD] 0.17291 <C> [BOLD] 0.12638 <C> [BOLD] 0.17291 <C> [BOLD] 0.12638 <R> <C> Collective <C> DBScan ( [ITALIC] ϵ=0.36) <C> 0.16388 <C> 0.12592 <C> 0.16388 <C> 0.12592 <CAP> TABLE VII: Precision of user concept vectors
<R> <C> Category <C> Method <C> Accuracy <C> AUC-ROC <C> Fake News Precision <C> Fake News Recall <C> Fake News F1 <C> Real News Precision <C> Real News Recall <C> Real News F1 <R> <C> Supervised <C> LIWC-LR <C> 0.528 <C> 0.558 <C> 0.604 <C> 0.160 <C> 0.253 <C> 0.517 <C> 0.896 <C> 0.655 <R> <C> Supervised <C> LIWC-SVM <C> 0.568 <C> 0.598 <C> 0.574 <C> 0.521 <C> 0.546 <C> 0.563 <C> 0.614 <C> 0.587 <R> <C> Supervised <C> LIWC-RF <C> 0.590 <C> 0.616 <C> 0.613 <C> 0.483 <C> 0.541 <C> 0.574 <C> 0.696 <C> 0.629 <R> <C> Supervised <C> LSTM <C> 0.733 <C> 0.799 <C> 0.876 <C> 0.543 <C> 0.670 <C> 0.669 <C> [BOLD] 0.923 <C> 0.775 <R> <C> Supervised <C> CNN <C> 0.747 <C> 0.834 <C> 0.869 <C> 0.580 <C> 0.696 <C> 0.685 <C> 0.913 <C> 0.783 <R> <C> Supervised <C> EANN <C> 0.767 <C> 0.803 <C> 0.863 <C> 0.634 <C> 0.731 <C> 0.711 <C> 0.899 <C> 0.794 <R> <C> Semi-supervised <C> LSTM [ITALIC] semi <C> 0.753 <C> 0.841 <C> 0.854 <C> 0.611 <C> 0.713 <C> 0.697 <C> 0.895 <C> 0.784 <R> <C> Semi-supervised <C> CNN [ITALIC] semi <C> 0.759 <C> 0.848 <C> 0.850 <C> 0.630 <C> 0.723 <C> 0.706 <C> 0.889 <C> 0.787 <R> <C> Weakly supervised <C> LSTM [ITALIC] weak <C> 0.762 <C> 0.813 <C> 0.804 <C> 0.692 <C> 0.744 <C> 0.730 <C> 0.831 <C> 0.777 <R> <C> Weakly supervised <C> CNN [ITALIC] weak <C> 0.759 <C> 0.823 <C> 0.754 <C> 0.769 <C> 0.762 <C> 0.766 <C> 0.749 <C> 0.757 <R> <C> Automatically annotated <C> WeFEND− <C> 0.807 <C> 0.858 <C> 0.846 <C> [BOLD] 0.751 <C> 0.795 <C> 0.776 <C> 0.863 <C> 0.817 <R> <C> Automatically annotated <C> WeFEND <C> [BOLD] 0.824 <C> [BOLD] 0.873 <C> [BOLD] 0.880 <C> [BOLD] 0.751 <C> [BOLD] 0.810 <C> [BOLD] 0.783 <C> 0.898 <C> [BOLD] 0.836 <CAP> Table 2: The performance comparison of different methods on WeChat dataset.
<R> <C> [BOLD] Model <C> [BOLD] #param <C> [BOLD] Dev <C> [BOLD] Test <R> <C> 300D Residual-Stacked-Encoder <C> 9.7M <C> 86.4 <C> 85.7 <R> <C> 600D Residual-Stacked-Encoder <C> 28.9M <C> [BOLD] 87.0 <C> [BOLD] 86.0 <R> <C> 600D Shortcut-Stacked-Encoder <C> 34.7M <C> 86.8 <C> 85.9 <CAP> Table 6: Results on SNLI for the fewer-parameter Residual-Stacked Encoder models. Each model has 3 biLSTM-stacked layers and 1 MLP layer. The #param column denotes the number of parameters in millions.
<R> <C> [BOLD] LIWC Cat. <C> [BOLD] Empath <C> [BOLD] Empath+Crowd <C> [BOLD] Emolex <C> [BOLD] GI <R> <C> Positive <C> 0.944 <C> 0.950 <C> 0.955 <C> 0.971 <R> <C> Negative <C> 0.941 <C> 0.936 <C> 0.954 <C> 0.945 <R> <C> Sadness <C> 0.890 <C> 0.907 <C> 0.852 <C> [EMPTY] <R> <C> Anger <C> 0.889 <C> 0.894 <C> 0.837 <C> [EMPTY] <R> <C> Achievement <C> 0.915 <C> 0.903 <C> [EMPTY] <C> 0.817 <R> <C> Religion <C> 0.893 <C> 0.908 <C> [EMPTY] <C> 0.902 <R> <C> Work <C> 0.859 <C> 0.820 <C> [EMPTY] <C> 0.745 <R> <C> Home <C> 0.919 <C> 0.941 <C> [EMPTY] <C> [EMPTY] <R> <C> Money <C> 0.902 <C> 0.878 <C> [EMPTY] <C> [EMPTY] <R> <C> Health <C> 0.866 <C> 0.898 <C> [EMPTY] <C> [EMPTY] <R> <C> Sex <C> 0.928 <C> 0.935 <C> [EMPTY] <C> [EMPTY] <R> <C> Death <C> 0.856 <C> 0.901 <C> [EMPTY] <C> [EMPTY] <R> <C> Average <C> 0.900 <C> 0.906 <C> 0.899 <C> 0.876 <CAP> Table 3: We compared the classifications of LIWC, EmoLex and Empath across thirteen categories, finding strong correlation between tools. The first column represents comparisons between Empath’s unsupervised model against LIWC, the second after crowd filtering against LIWC, the third between EmoLex and LIWC, and the fourth between the General Inquirer and LIWC.
<R> <C> [EMPTY] <C> L2 distance in cm ↓ Align <C> L2 distance in cm ↓ Grasp <C> L2 distance in cm ↓ Lift <C> L2 distance in cm ↓ Move <C> L2 distance in cm ↓ Release <C> Success ↑ Rate <R> <C> Oracle <C> 0.04 <C> 0.03 <C> 0.04 <C> 0.04 <C> 0.04 <C> 98.4% <R> <C> GT Action <C> 0.32 <C> 0.31 <C> 0.48 <C> 0.63 <C> 0.63 <C> 90.4% <R> <C> Template <C> 0.32 <C> 0.39 <C> 0.47 <C> 0.65 <C> 0.65 <C> 87.8% <R> <C> Real Lang <C> 0.51 <C> 1.23 <C> 1.50 <C> 2.39 <C> 2.40 <C> 77.1% <CAP> TABLE I: L2 distances and accuracy when executing plans generated from either ambiguous natural language instructions or unambiguous template language.
<R> <C> Datasets <C> Classification (NNet) <C> Classification (SVM) <C> Classification (R.Forests) <C> Dictionary <R> <C> China (ICEWS, Protest) <C> 73 <C> 75 <C> 73 <C> 51 <R> <C> D. R. Congo (ICEWS, Fight) <C> 81 <C> 84 <C> 84 <C> 59 <R> <C> Syria (OEDA, Fight) <C> 74 <C> 73 <C> 75 <C> 57 <CAP> Table 7. Validation Results
<R> <C> [EMPTY] <C> Cortana <C> Conversation <R> <C> full <C> [BOLD] 9.28 <C> [BOLD] 17.47 <R> <C> factorized input <C> 9.62 <C> 18.31 <R> <C> factorized output <C> 9.50 <C> 18.11 <R> <C> factorized forget <C> 9.57 <C> 17.97 <CAP> Table 3: WERs of the 6-layer ltLSTM and its factorized gate versions on Cortana and Conversation test sets. Both test sets are mixed with close-talk and far-field utterances.
<R> <C> Models <C> BLEU-4 <R> <C> NQG++  <C> 13.29 <R> <C> NQG++  + our work <C> [BOLD] 14.97 <R> <C> Pointer-generator model  <C> 14.33 <R> <C> Pointer-generator model  + our work <C> [BOLD] 16.32 <CAP> Table 7: Performance Improvement on existing models on SQuAD dataset
<R> <C> Model / Size <C> GLUE <C> SuperGLUE <C> AVG <R> <C> T5 Base <C> 84.99 <C> 73.55 <C> 79.27 <R> <C> Ours Base <C> 85.22 (+0.27%) <C> 75.30 (+2.7%) <C> 80.26 (+1.3%) <R> <C> T5 Large <C> 88.22 <C> 80.04 <C> 84.13 <R> <C> Ours Large <C> 88.31 (+0.1%) <C> 81.56 (+1.9%) <C> 84.94 (+1.0%) <R> <C> T5 3B <C> 89.53 <C> 84.22 <C> 86.87 <R> <C> Ours 3B <C> [BOLD] 89.67 (+0.2%) <C> [BOLD] 85.75 (+1.8%) <C> [BOLD] 87.71 (+1.0%) <CAP> Table 4: Effect of HyperGrid on Multi-Task T5 on all model sizes. HyperGrid improves multi-task co-training consistently overly different model sizes. Improvement over SuperGLUE is greater than GLUE.
<R> <C> Model <C> Params <C> Train <C> Test <R> <C> [BOLD] Single Model (w/o Cross Sentence Attention) <C> [BOLD] Single Model (w/o Cross Sentence Attention) <C> [BOLD] Single Model (w/o Cross Sentence Attention) <C> [BOLD] Single Model (w/o Cross Sentence Attention) <R> <C> 300D Gumbel TreeLSTM Choi et al. ( 2017 ) <C> 2.9M <C> 91.2 <C> 85.6 <R> <C> 300D DISAN Shen et al. ( 2017 ) <C> 2.4M <C> 91.1 <C> 85.6 <R> <C> 300D Residual Stacked Encoders Nie and Bansal ( 2017 ) <C> 9.7M <C> 89.8 <C> 85.7 <R> <C> 600D Gumbel TreeLSTM Choi et al. ( 2017 ) <C> 10M <C> 93.1 <C> [BOLD] 86.0 <R> <C> 300D CAFE (w/o CA) <C> 3.7M <C> 87.3 <C> 85.9 <R> <C> [BOLD] Single Models <C> [BOLD] Single Models <C> [BOLD] Single Models <C> [BOLD] Single Models <R> <C> 100D LSTM with attention Rocktäschel et al. ( 2015 ) <C> 250K <C> 85.3 <C> 83.5 <R> <C> 300D mLSTM Wang and Jiang ( 2016b ) <C> 1.9M <C> 92.0 <C> 86.1 <R> <C> 450D LSTMN + deep att. fusion Cheng et al. ( 2016 ) <C> 3.4M <C> 88.5 <C> 86.3 <R> <C> 200D DecompAtt + Intra-Att Parikh et al. ( 2016 ) <C> 580K <C> 90.5 <C> 86.8 <R> <C> 300D NTI-SLSTM-LSTM Yu and Munkhdalai ( 2017b ) <C> 3.2M <C> 88.5 <C> 87.3 <R> <C> 300D re-read LSTM Sha et al. ( 2016 ) <C> 2.0M <C> 90.7 <C> 87.5 <R> <C> BiMPM Wang et al. ( 2017 ) <C> 1.6M <C> 90.9 <C> 87.5 <R> <C> 448D DIIN Gong et al. ( 2017 ) <C> 4.4M <C> 91.2 <C> 88.0 <R> <C> 600D ESIM Chen et al. ( 2017b ) <C> 4.3M <C> 92.6 <C> 88.0 <R> <C> 150D CAFE (SUM+2x200D MLP) <C> 750K <C> 88.2 <C> 87.7 <R> <C> 200D CAFE (SUM+2x400D MLP) <C> 1.4M <C> 89.4 <C> 88.1 <R> <C> 300D CAFE (SUM+2x600D MLP) <C> 3.5M <C> 89.2 <C> 88.3 <R> <C> 300D CAFE (AVGMAX+300D HN) <C> 4.7M <C> 89.8 <C> [BOLD] 88.5 <R> <C> [BOLD] Ensemble Models <C> [BOLD] Ensemble Models <C> [BOLD] Ensemble Models <C> [BOLD] Ensemble Models <R> <C> 600D ESIM + 300D Tree-LSTM Chen et al. ( 2017b ) <C> 7.7M <C> 93.5 <C> 88.6 <R> <C> BiMPM Wang et al. ( 2017 ) <C> 6.4M <C> 93.2 <C> 88.8 <R> <C> 448D DIIN Gong et al. ( 2017 ) <C> 17.0M <C> 92.3 <C> 88.9 <R> <C> 300D CAFE (Ensemble) <C> 17.5M <C> 92.5 <C> [BOLD] 89.3 <R> <C> [BOLD] External Resource Models <C> [BOLD] External Resource Models <C> [BOLD] External Resource Models <C> [BOLD] External Resource Models <R> <C> BiAttentive Classification + CoVe + Char McCann et al. ( 2017 ) <C> 22M <C> 88.5 <C> 88.1 <R> <C> KIM Chen et al. ( 2017a ) <C> 4.3M <C> 94.1 <C> 88.6 <R> <C> ESIM + ELMo Peters et al. ( 2018 ) <C> 8.0M <C> 91.6 <C> 88.7 <R> <C> 200D CAFE (AVGMAX + 200D MLP) + ELMo <C> 1.4M <C> 89.5 <C> [BOLD] 89.0 <CAP> Table 1: Performance comparison of all published models on the SNLI benchmark.
<R> <C> Model <C> MultiNLI Match <C> MultiNLI Mismatch <C> SciTail - <R> <C> Majority <C> 36.5 <C> 35.6 <C> 60.3 <R> <C> NGRAM# <C> - <C> - <C> 70.6 <R> <C> CBOW♭ <C> 65.2 <C> 64.8 <C> - <R> <C> BiLSTM♭ <C> 69.8 <C> 69.4 <C> - <R> <C> ESIM#,♭ <C> 72.4 <C> 72.1 <C> 70.6 <R> <C> DecompAtt# - <C> - <C> - <C> 72.3 <R> <C> DGEM# <C> - <C> - <C> 70.8 <R> <C> DGEM + Edge# <C> - <C> - <C> 77.3 <R> <C> ESIM† <C> 76.3 <C> 75.8 <C> - <R> <C> ESIM + Read† <C> 77.8 <C> 77.0 <C> - <R> <C> CAFE <C> 78.7 <C> 77.9 <C> [BOLD] 83.3 <R> <C> CAFE Ensemble <C> [BOLD] 80.2 <C> [BOLD] 79.0 <C> - <CAP> Table 2: Performance comparison (accuracy) on MultiNLI and SciTail. Models with †, # and ♭ are reported from Weissenborn (2017), Khot et al. (2018) and Williams et al. (2017) respectively.
<R> <C> [EMPTY] <C> Match <C> Mismatch <R> <C> Original Model <C> 79.0 <C> 78.9 <R> <C> (1a) Rm FM for 1L-FC <C> 77.7 <C> 77.9 <R> <C> (1b) Rm FM for 1L-FC (ReLU) <C> 77.3 <C> 77.5 <R> <C> (1c) Rm FM for 2L-FC (ReLU) <C> 76.6 <C> 76.4 <R> <C> (2) Remove Char Embed <C> 78.1 <C> 78.3 <R> <C> (3) Remove Syn Embed <C> 78.3 <C> 78.4 <R> <C> (4) Remove Inter Att <C> 75.2 <C> 75.6 <R> <C> (5) Replace HW Pred. with FC <C> 77.7 <C> 77.9 <R> <C> (6) Replace HW Enc. with FC <C> 78.7 <C> 78.7 <R> <C> (7) Remove Sub Feat <C> 77.9 <C> 78.3 <R> <C> (8) Remove Mul Feat <C> 78.7 <C> 78.6 <R> <C> (9) Remove Concat Feat <C> 77.9 <C> 77.6 <R> <C> (10) Add Bi-directional <C> 78.3 <C> 78.4 <CAP> Table 3: Ablation study on MultiNLI development sets. HW stands for Highway.
<R> <C> [EMPTY] <C> Matched ESIM <C> Matched CAFE <C> Mismatched ESIM <C> Mismatched CAFE <R> <C> Conditional <C> 100 <C> 70 <C> 60 <C> [BOLD] 85 <R> <C> Word overlap <C> 50 <C> [BOLD] 82 <C> 62 <C> [BOLD] 87 <R> <C> Negation <C> [BOLD] 76 <C> [BOLD] 76 <C> 71 <C> [BOLD] 80 <R> <C> Antonym <C> 67 <C> [BOLD] 82 <C> 58 <C> [BOLD] 80 <R> <C> Long Sentence <C> 75 <C> [BOLD] 79 <C> 69 <C> [BOLD] 77 <R> <C> Tense Difference <C> 73 <C> [BOLD] 82 <C> 79 <C> [BOLD] 89 <R> <C> Active/Passive <C> 88 <C> [BOLD] 100 <C> [BOLD] 91 <C> 90 <R> <C> Paraphrase <C> [BOLD] 89 <C> 88 <C> 84 <C> [BOLD] 95 <R> <C> Quantity/Time <C> 33 <C> [BOLD] 53 <C> 54 <C> [BOLD] 62 <R> <C> Coreference <C> [BOLD] 83 <C> 80 <C> 75 <C> [BOLD] 83 <R> <C> Quantifier <C> 69 <C> [BOLD] 75 <C> 72 <C> [BOLD] 80 <R> <C> Modal <C> 78 <C> [BOLD] 81 <C> 76 <C> [BOLD] 81 <R> <C> Belief <C> 65 <C> [BOLD] 77 <C> 67 <C> [BOLD] 83 <CAP> Table 4: Linguistic Error Analysis on MultiNLI dataset.
<R> <C> [EMPTY] <C> [BOLD] acc <R> <C> [ITALIC] GloVe_GigaWiki14 <C> 18.10 <R> <C> [ITALIC] GloVe_Giga <C> 19.00 <R> <C> [ITALIC] embeddings_wo_PPSuffix <C> 22.17 <R> <C> [ITALIC] embeddings_PP <C> [BOLD] 30.32 <CAP> Table 2: Results of embeddings_PP alone for bridging anaphora resolution compared to the baselines. Bold indicates statistically significant differences over the baselines using randomization test (p<0.01).
<R> <C> [EMPTY] <C> [BOLD] acc <R> <C> [ITALIC] MLN model II <C> 41.32 <R> <C> [ITALIC] MLN model II + GloVe_GigaWiki14 <C> 39.52 <R> <C> [ITALIC] MLN model II + embeddings_wo_PPSuffix <C> 40.42 <R> <C> [ITALIC] MLN model II + embeddings_PP <C> [BOLD] 45.85 <CAP> Table 3: Results of integrating embeddings_PP into MLN model II for bridging anaphora resolution compared to the baselines. Bold indicates statistically significant differences over the baselines using randomization test (p<0.01).
<R> <C> Model <C> Topics <C> Accuracy <C> Avg. score <R> <C> 205 <C> 3394 <C> 59.9 <C> 60.6 <R> <C> 319 <C> 4427 <C> 61.2 <C> 64.3 <R> <C> 693 <C> 6090 <C> 68.9 <C> 70.7 <R> <C> 1693 <C> 2868 <C> 53.1 <C> 55.8 <R> <C> 2427 <C> 3687 <C> 56.2 <C> 60.0 <R> <C> 6963 <C> 5510 <C> 64.1 <C> 66.4 <R> <C> Online LDA <C> 100 <C> 59.2 <C> 60.0 <R> <C> Online LDA <C> 400 <C> 65.4 <C> 65.9 <CAP> Table 2: Document classification for 20 Newsgroups corpus.
<R> <C> [EMPTY] <C> [BOLD] S <C> [BOLD] D <C> [BOLD] Q <C> [BOLD] C <R> <C> [BOLD] Support <C> [BOLD] 39 <C> 14 <C> 5 <C> 13 <R> <C> [BOLD] Deny <C> 8 <C> [BOLD] 28 <C> 5 <C> 30 <R> <C> [BOLD] Query <C> 2 <C> 3 <C> [BOLD] 62 <C> 4 <R> <C> [BOLD] Comment <C> 14 <C> 14 <C> 2 <C> [BOLD] 41 <CAP> Table 4. Confusion Matrix on Balanced Dataset
<R> <C> [BOLD] Ablation Test  [BOLD] Set <C> [BOLD] Overall  [BOLD] Features <C> [BOLD] Overall Acc <C> [BOLD] Overall Prec <C> [BOLD] Overall Rec <C> [BOLD] Support F1 <C> [BOLD] Support Acc <C> [BOLD] Support Prec <C> [BOLD] Support Rec <C> [BOLD] Query F1 <C> [BOLD] Query Acc <C> [BOLD] Query Prec <C> [BOLD] Query Rec <C> [BOLD] Comment F1 <C> [BOLD] Comment Acc <C> [BOLD] Comment Prec <C> [BOLD] Comment Rec <C> F1 <R> <C> A <C> Structural <C> 0.731 <C> 0.41 <C> 0.37 <C> 0.38 <C> 0.18 <C> [BOLD] 0.28 <C> 0.18 <C> 0.22 <C> 0.39 <C> 0.56 <C> 0.39 <C> 0.46 <C> 0.91 <C> 0.78 <C> 0.91 <C> 0.84 <R> <C> B <C> Conversational <C> 0.767 <C> 0.42 <C> 0.31 <C> 0.33 <C> 0.29 <C> 0.93 <C> 0.29 <C> 0.44 <C> 0 <C> 0 <C> 0 <C> 0 <C> 1 <C> 0.76 <C> 1 <C> 0.87 <R> <C> C <C> Affective <C> 0.742 <C> 0.19 <C> 0.25 <C> 0.21 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 1 <C> 0.74 <C> 1 <C> 0.85 <R> <C> D <C> Dialogue-Act <C> 0.742 <C> 0.19 <C> 0.25 <C> 0.21 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 1 <C> 0.74 <C> 1 <C> 0.85 <R> <C> E <C> A + B <C> 0.783 <C> 0.54 <C> 0.43 <C> 0.45 <C> 0.29 <C> [BOLD] 0.73 <C> 0.29 <C> 0.41 <C> 0.42 <C> 0.62 <C> 0.42 <C> [BOLD] 0.52 <C> 0.96 <C> 0.8 <C> 0.96 <C> 0.87 <R> <C> F <C> A + C <C> 0.741 <C> 0.42 <C> 0.36 <C> 0.38 <C> 0.14 <C> 0.27 <C> 0.14 <C> 0.18 <C> 0.39 <C> 0.62 <C> 0.39 <C> 0.48 <C> 0.93 <C> 0.77 <C> 0.93 <C> 0.84 <R> <C> G <C> A + D <C> 0.736 <C> 0.42 <C> 0.37 <C> 0.38 <C> 0.18 <C> 0.3 <C> 0.18 <C> 0.23 <C> 0.37 <C> 0.59 <C> 0.37 <C> 0.45 <C> 0.92 <C> 0.77 <C> 0.92 <C> 0.84 <R> <C> H <C> E + C <C> 0.788 <C> 0.56 <C> 0.42 <C> 0.46 <C> 0.28 <C> [BOLD] 0.74 <C> 0.28 <C> 0.4 <C> 0.44 <C> 0.7 <C> 0.44 <C> 0.54 <C> 0.97 <C> 0.8 <C> 0.97 <C> 0.87 <R> <C> I <C> E + D <C> 0.784 <C> 0.53 <C> 0.43 <C> 0.46 <C> 0.3 <C> [BOLD] 0.65 <C> 0.3 <C> 0.41 <C> 0.45 <C> 0.67 <C> 0.45 <C> 0.54 <C> 0.96 <C> 0.8 <C> 0.96 <C> 0.87 <R> <C> J <C> F + D <C> 0.749 <C> 0.43 <C> 0.36 <C> 0.38 <C> 0.14 <C> 0.33 <C> 0.14 <C> 0.19 <C> 0.38 <C> 0.63 <C> 0.38 <C> 0.47 <C> 0.94 <C> 0.77 <C> 0.94 <C> 0.85 <R> <C> K <C> All Features <C> [BOLD] 0.795 <C> 0.57 <C> 0.43 <C> 0.47 <C> 0.29 <C> [BOLD] 0.73 <C> 0.29 <C> 0.41 <C> 0.47 <C> 0.75 <C> 0.47 <C> [BOLD] 0.58 <C> 0.97 <C> 0.8 <C> 0.97 <C> 0.88 <CAP> Table 5. Ablation test on several feature sets.
<R> <C> Models <C> B-4 <C> R-1 <C> R-2 <C> R-4 <C> R-L <R> <C> GPT-2 <C> 31.49 <C> 64.47 <C> 39.59 <C> 17.34 <C> 53.04 <R> <C> -w/o caption <C> 21.06 <C> 53.53 <C> 28.74 <C> 9.93 <C> 45.73 <R> <C> -w/o header <C> 28.78 <C> 62.64 <C> 38.44 <C> 16.20 <C> 52.95 <CAP> Table 3: Ablation study: the importance of table caption and table header.
<R> <C> Model <C> Topic coherence <C> 5 <C> 10 <C> 20 <C> 30 <C> 50 <C> 100 <R> <C> word2vec(300) <C> NPMI <C> 0.32 <C> 0.30 <C> 0.23 <C> 0.21 <C> 0.19 <C> 0.17 <R> <C> [EMPTY] <C> PMI <C> 1.91 <C> 1.88 <C> 1.43 <C> 1.31 <C> 1.24 <C> 1.14 <R> <C> [EMPTY] <C> UMASS <C> 4.14 <C> 4.32 <C> 4.23 <C> 4.18 <C> 4.20 <C> 4.18 <R> <C> glove(300) <C> NPMI <C> 0.26 <C> 0.23 <C> 0.19 <C> 0.18 <C> 0.18 <C> 0.16 <R> <C> [EMPTY] <C> PMI <C> 1.66 <C> 1.51 <C> 1.20 <C> 1.18 <C> 1.15 <C> 1.07 <R> <C> [EMPTY] <C> UMASS <C> 4.23 <C> 4.28 <C> 4.20 <C> 4.15 <C> 4.12 <C> 4.11 <R> <C> glove(50) <C> NPMI <C> 0.27 <C> 0.22 <C> 0.15 <C> 0.13 <C> 0.13 <C> 0.13 <R> <C> [EMPTY] <C> PMI <C> 1.70 <C> 1.40 <C> 0.90 <C> 0.81 <C> 0.78 <C> 0.81 <R> <C> [EMPTY] <C> UMASS <C> 4.00 <C> 3.99 <C> 3.86 <C> 3.89 <C> 3.89 <C> 3.91 <CAP> Table 5: Comparison of topic coherence over the word representation models used as a initialization of word embedding matrix WEmb. The numbers in the parenthesis is the dimension of its embedding space.
<R> <C> [BOLD] Models <C> [BOLD] EmoBank Val <C> [BOLD] EmoBank Aro <C> [BOLD] EmoBank Dom <C> [BOLD] FB Post Val <C> [BOLD] FB Post Aro <R> <C> System [preoctiuc2016modelling] <C> – <C> – <C> – <C> 0.650 <C> [BOLD] 0.850 <R> <C> CNN (C) [akhtar2018multi] <C> 0.567 <C> 0.347 <C> 0.234 <C> 0.678 <C> 0.290 <R> <C> LSTM (L) <C> 0.601 <C> 0.337 <C> 0.245 <C> 0.671 <C> 0.324 <R> <C> GRU (G) <C> 0.569 <C> 0.315 <C> 0.243 <C> 0.668 <C> 0.313 <R> <C> Ensemble (CLG) <C> 0.618 <C> 0.365 <C> 0.263 <C> 0.695 <C> 0.336 <R> <C> ‡Ensemble (CLG + Old features) <C> 0.635 <C> 0.375 <C> 0.277 <C> 0.727 <C> 0.355 <R> <C> (+) lexical <C> +0.026–––––––– <C> +0.003–––––––– <C> +0.017∗ <C> +0.010–––––––– <C> +0.043–––––––– <R> <C> (+) surface <C> −0.003∗ <C> −0.005–––––––– <C> −0.001–––––––– <C> −0.006∗ <C> +0.008–––––––– <R> <C> (+) syntactic <C> +0.012–––––––– <C> +0.006–––––––– <C> +0.002∗ <C> +0.004∗ <C> +0.009–––––––– <R> <C> (+) all <C> +0.039–––––––– <C> +0.007–––––––– <C> +0.020∗ <C> +0.009–––––––– <C> +0.062–––––––– <R> <C> Ensemble (CLG + Old &  [BOLD] New features) <C> [BOLD] 0.674 <C> [BOLD] 0.382 <C> [BOLD] 0.297 <C> [BOLD] 0.736 <C> 0.417 <CAP> Table 5: Performance on the emotion prediction task. We concatenate multi-level stylistic features with handcrafted features and compare Pearson coefficient correlation. + denotes increase over baseline‡ due to addition of proposed stylistic features. Underlined values are statistically significant with p<0.001, while those with ∗ are significant with p<0.01.
<R> <C> Level <C> Features <C> P <C> R <C> F1 <R> <C> Entity <C> Unstructured <C> 50.0 <C> [BOLD] 67.2 <C> 52.9 <R> <C> [EMPTY] <C> + Pairs <C> 53.3 <C> 64.1 <C> 54.3 <R> <C> [EMPTY] <C> + Graph <C> [BOLD] 53.9 <C> 63.9 <C> [BOLD] 54.5 <R> <C> Sentence <C> Unstructured <C> 42.6 <C> [BOLD] 58.9 <C> 44.4 <R> <C> [EMPTY] <C> + Pairs <C> 46.5 <C> 54.1 <C> [BOLD] 45.6 <R> <C> [EMPTY] <C> + Graph <C> [BOLD] 47.0 <C> 53.6 <C> [BOLD] 45.6 <CAP> Table 3: Results on our corpus. All quantities are macro-averaged.
<R> <C> [BOLD] Dataset language <C> [BOLD] disease  [BOLD] en <C> [BOLD] disease  [BOLD] de <C> [BOLD] city  [BOLD] en <C> [BOLD] city  [BOLD] de <R> <C> total docs <C> 3.6k <C> 2.3k <C> 19.5k <C> 12.5k <R> <C> avg sents per doc <C> 58.5 <C> 45.7 <C> 56.5 <C> 39.9 <R> <C> avg sects per doc <C> 7.5 <C> 7.2 <C> 8.3 <C> 7.6 <R> <C> headings <C> 8.5k <C> 6.1k <C> 23.0k <C> 12.2k <R> <C> topics <C> 27 <C> 25 <C> 30 <C> 27 <R> <C> coverage <C> 94.6% <C> 89.5% <C> 96.6% <C> 96.1% <CAP> Table 1: Dataset characteristics for disease (German: Krankheit) and city (German: Stadt). Headings denotes the number of distinct section and subsection headings among the documents. Topics stands for the number of topic labels after synset clustering. Coverage denotes the proportion of headings covered by topics; the remaining headings are labeled as other.
<R> <C> Name <C> Type <C> Domain <C> Sent. <C> # Tuple <R> <C> NYT-222 <C> n-ary <C> News <C> 222 <C> 222 <R> <C> WEB-500 <C> binary <C> Web/News <C> 500 <C> 461 <R> <C> PENN-100 <C> binary <C> Mixed <C> 100 <C> 51 <R> <C> OIE2016 <C> n-ary <C> Wiki <C> 3200 <C> 10359 <CAP> Table 1: Data sets in RelVis
<R> <C> Method <C> Binary Task Mi-F <C> Binary Task Ma-F <C> Multi-Label Task Ma-P <C> Multi-Label Task Ma-R <C> Multi-Label Task Ma-F <R> <C> Source Only <C> 55.7 <C> 46.2 <C> 28.8 <C> 70.0 <C> 39.6 <R> <C> mSDA <C> 57.4 <C> 49.7 <C> 41.0 <C> 63.7 <C> 48.1 <R> <C> DANN <C> 68.2 <C> 65.8 <C> [BOLD] 50.8 <C> 36.3 <C> 42.2 <R> <C> SE <C> 64.0 <C> 59.5 <C> 42.7 <C> 64.1 <C> 51.0 <R> <C> + curriculum <C> 66.4 <C> 62.3 <C> 44.4 <C> 71.7 <C> 54.5 <R> <C> AE (ours) <C> 75.1 <C> 74.5 <C> 46.1 <C> 75.3 <C> 57.2 <R> <C> + curriculum <C> [BOLD] 77.4 <C> [BOLD] 77.1 <C> 48.2 <C> [BOLD] 83.5 <C> [BOLD] 61.1 <R> <C> In-Domain <C> 81.7 <C> 81.6 <C> 86.5 <C> 83.5 <C> 84.8 <CAP> Table 2: Framework results for the binary label task (left) and multi-label task (right). For the binary task, we show micro- and macro-averaged F1 scores. For the multi-label task, we show macro-averaged precision, recall, and F1 scores.
<R> <C> Model <C> Binary Task Mi-F <C> Binary Task Ma-F <C> Multi-Label Task Ma-P <C> Multi-Label Task Ma-R <C> Multi-Label Task Ma-F <R> <C> LR <C> 63.7 <C> 60.9 <C> 53.3 <C> 67.3 <C> 31.9 <R> <C> BiLSTM <C> 64.8 <C> 63.1 <C> 36.2 <C> 65.0 <C> 46.3 <R> <C> CNN (2D) <C> 73.1 <C> 72.1 <C> [BOLD] 49.0 <C> 73.8 <C> 58.9 <R> <C> CNN (ours) <C> 75.4 <C> 75.3 <C> 36.9 <C> [BOLD] 91.4 <C> 52.5 <R> <C> + seq squeeze <C> 75.1 <C> 74.6 <C> 45.8 <C> 79.6 <C> 58.2 <R> <C> + state conn <C> [BOLD] 80.2 <C> 76.3 <C> 45.3 <C> 85.5 <C> 59.2 <R> <C> + time emb <C> 77.4 <C> [BOLD] 77.1 <C> 48.2 <C> 83.5 <C> [BOLD] 61.1 <CAP> Table 3: Model results with adaptive ensembling for the binary label task (left) and multi-label task (right). For the binary task, we show micro- and macro-averaged F1 scores. For the multi-label task, we show macro-averaged precision, recall, and F1 scores.
<R> <C> Agent types <C> % agent A value correct <C> % agent B value correct <C> % proposal correct <R> <C> Selfish-Selfish <C> 21 <C> 25 <C> 94 (94) <R> <C> Prosocial-Prosocial <C> 26 <C> 81 <C> 80 (57) <R> <C> Random baseline <C> 20 <C> 20 <C> 17 <CAP> Table 4: Accuracy of predicting individual elements of the agents’ hidden utilities, as well as the final proposal that was accepted. Numbers in brackets indicate accuracy predicting the proposal from just the item pool. Random baseline numbers are obtained by predicting from a message transcript and item pool of all 0’s.
<R> <C> [BOLD] System <C> Min <C> Max <C> Avg Accuracy <R> <C> SRILM 1-gram <C> 43.79 <C> 60 <C> 46.53 <R> <C> SRILM 2-gram <C> 77.77 <C> 83.61 <C> 81.77 <R> <C> SRILM 3-gram <C> 84.07 <C> 88.88 <C> 85.45 <R> <C> SRILM 4-gram <C> 81.11 <C> 86.66 <C> 83.94 <R> <C> SRILM 5-gram <C> 77.77 <C> 86.66 <C> 83.15 <R> <C> SRILM 6-gram <C> 77.77 <C> 86.66 <C> 82.98 <R> <C> RNNLM 1-class <C> 83.13 <C> [BOLD] 93.33 <C> 84.42 <R> <C> RNNLM 6-classes <C> [BOLD] 84.44 <C> 90.58 <C> [BOLD] 87.69 <R> <C> RNNLM 100-classes <C> 82.22 <C> 89.44 <C> 85.38 <CAP> Table 1: Language identification accuracies for each language model (explicit approach)
<R> <C> Target Source <C> Croatian Slovenian <C> Slovak Czech <C> Norwegian Danish <C> Norwegian Swedish <R> <C> [ITALIC] UDPipe <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> supervised <C> 74.27 <C> 70.27 <C> 78.10 <C> 78.10 <R> <C> delex <C> 53.93 <C> 53.66 <C> 54.54 <C> 56.71 <R> <C> cross <C> 56.85 <C> 54.61 <C> 54.11 <C> 55.85 <R> <C> [ITALIC] mate-tools <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> supervised <C> 79.68 <C> 71.89 <C> 81.37 <C> 81.37 <R> <C> delex <C> 53.39 <C> 55.80 <C> 50.07 <C> 56.27 <R> <C> cross <C> 60.29 <C> 62.21 <C> 56.85 <C> 59.63 <R> <C> [ITALIC] Projected <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> 100,000 <C> 58.82 <C> 60.29 <C> 57.19 <C> 63.03 <R> <C> 500,000 <C> 59.86 <C> 62.23 <C> 57.58 <C> 64.61 <R> <C> 1,000,000 <C> [BOLD] 62.92 <C> 63.57 <C> 57.82 <C> 64.59 <R> <C> PBSMT <C> 60.81 <C> [BOLD] 65.97 <C> 57.87 <C> 65.96 <R> <C> SyntaxSMT <C> 58.57 <C> 63.13 <C> 58.36 <C> [BOLD] 66.31 <CAP> Table 1: Basic results of cross-lingual parsing models in terms of labeled attachment scores (LAS) on development data: Annotation projection on automatically parsed bitexts of varying sizes (projected: number of sentence pairs); treebank translation models (PBSMT and SyntaxSMT); compared to three baselines: delexicalized models (delex), source language models without adaptation (cross) and fully-supervised target language models (supervised).
<R> <C> LAS <C> hr <C> no <C> sk <R> <C> supervised <C> 73.37 <C> 81.77 <C> 71.41 <R> <C> delex <C> 50.05 <C> 58.13 <C> 53.87 <R> <C> cross <C> 56.91 <C> 60.22 <C> 61.17 <R> <C> CUNI <C> 60.70 <C> 70.21 <C> 78.12 <R> <C> our model <C> 57.98 <C> 68.60 <C> 73.14 <CAP> Table 3: Final results on the test set (our model) compared to baselines and fully supervised models. CUNI refers to a competing system – the winning team of VarDial. For the Norwegian baselines we report the results for Swedish as the source language, which is much better than using Danish.
<R> <C> UAS <C> hr <C> no <C> sk <R> <C> supervised <C> 80.16 <C> 85.59 <C> 78.73 <R> <C> delex <C> 63.29 <C> 67.86 <C> 64.55 <R> <C> cross <C> 68.52 <C> 69.31 <C> 70.60 <R> <C> CUNI <C> 69.73 <C> 77.13 <C> 84.92 <R> <C> our model <C> 69.57 <C> 76.77 <C> 82.87 <CAP> Table 3: Final results on the test set (our model) compared to baselines and fully supervised models. CUNI refers to a competing system – the winning team of VarDial. For the Norwegian baselines we report the results for Swedish as the source language, which is much better than using Danish.
<R> <C> [EMPTY] <C> MSE <C> # Epochs <R> <C> CNN <C> 0.186 (±0.023) <C> 81 <R> <C> RNN <C> 0.159 (±0.017) <C> 111 <CAP> Table 4: RNN and CNN comparison after cross-validation
<R> <C> [EMPTY] <C> ACC <C> AP <C> P@50 <R> <C> Random <C> 50.0 <C> 50.0 <C> 50.0 <R> <C> LSTM-COS <C> 68.2 <C> 71.6 <C> 81.0 <R> <C> + gating <C> 69.6 <C> 74.6 <C> 84.4 <R> <C> + cross-ent <C> 71.1 <C> 79.0 <C> [BOLD] 92.2 <R> <C> + dropout <C> [BOLD] 75.4 <C> [BOLD] 81.9 <C> 89.8 <CAP> Table 3: Results on the dataset of short answers written by language learners in response to visual prompts. Reporting accuracy, average precision, and precision at rank 50.
<R> <C> System <C> WPCS <R> <C> Phrase-based <C> 170 <R> <C> Syntax-based <C> 21.5 <R> <C> NMT 32-bit <C> 6.5 <R> <C> NMT 8-bit <C> 32.3 <CAP> Table 3: Running speed (in words per core second) of the phrase-based SMT system, syntax-based system, NMT with 32-bit decoding and NMT with 8-bit decoding.
<R> <C> Language <C> 32-bit <C> 8-bit <R> <C> En-Nl <C> 4.02 <C> 4.08 <R> <C> Nl-En <C> 4.03 <C> 4.03 <R> <C> Ru-En <C> 4.10 <C> 4.06 <R> <C> En-De 2x <C> 4.05 <C> 4.16 <R> <C> En-De 1x <C> 3.84 <C> 3.90 <CAP> Table 7: Human evaluation scores for 8-bit and 32-bit systems. All tests are news domain.
<R> <C> [BOLD] Frame <C> [BOLD] FQS <C> [BOLD] Definition <C> [BOLD] Example Sentences <C> [BOLD] FSS <R> <C> [ITALIC] killing <C> 0.954 <C> A Killer or Cause causes the death of the Victim. <C> [ITALIC] F1: Older kids left homeless after a recent murder- [ITALIC] suicide in Indianapolis claimed Mom and Dad. <C> 0.8 <R> <C> [ITALIC] killing <C> 0.954 <C> A Killer or Cause causes the death of the Victim. <C> [ITALIC] F2: The incident at Mayak was the third  [ITALIC] shooting in recent weeks involving nuclear weapons or facilities in Russia. <C> 0.75 <R> <C> [ITALIC] food <C> 0.838 <C> Words referring to items of food. <C> [ITALIC] F3: Lamma Island is perfect for sitting back to watch  [ITALIC] bananas grow. <C> 1.0 <R> <C> [ITALIC] food <C> 0.838 <C> Words referring to items of food. <C> [ITALIC] F4: Along with the usual  [ITALIC] chickens, you will see for sale snakes, dogs, and sometimes monkeys - all highly prized delicacies . <C> 0.838 <R> <C> [ITALIC] food <C> 0.838 <C> Words referring to items of food. <C> [ITALIC] F5: You can browse among antiques, flowers,  [ITALIC] herbs, and more. <C> 0.503 <R> <C> [ITALIC] assistance <C> 0.634 <C> A Helper benefits a Benefited party by enabling the culmination of a Goal of the Benefited party. <C> [ITALIC] F6: Your support  [ITALIC] helps provide real solutions. <C> 0.955 <R> <C> [ITALIC] assistance <C> 0.634 <C> A Helper benefits a Benefited party by enabling the culmination of a Goal of the Benefited party. <C> [ITALIC] F7: Unemployment  [ITALIC] provides benefits that many entry-level jobs don’t. <C> 0.467 <R> <C> [ITALIC] assistance <C> 0.634 <C> A Helper benefits a Benefited party by enabling the culmination of a Goal of the Benefited party. <C> [ITALIC] F8: Your support of Goodwill will  [ITALIC] provide job training. <C> 0.401 <R> <C> [ITALIC] purpose <C> 0.63 <C> An Agent wants to achieve a Goal. A Means is used to allow the Agent to achieve a Goal. <C> [ITALIC] F9: The  [ITALIC] objective of having kiosks is they serve as communication points between the guards <C> 0.94 <R> <C> [ITALIC] purpose <C> 0.63 <C> An Agent wants to achieve a Goal. A Means is used to allow the Agent to achieve a Goal. <C> [ITALIC] F10: They are antiviral drugs  [ITALIC] designed to shorten the flu. <C> 0.476 <R> <C> [ITALIC] purpose <C> 0.63 <C> An Agent wants to achieve a Goal. A Means is used to allow the Agent to achieve a Goal. <C> [ITALIC] F11: It seems that the city produced artists of this stature by accident, even against its  [ITALIC] will. <C> 0.241 <R> <C> [ITALIC] subjective <C> 0.366 <C> An Agent has influence on a Cognizer. The influence may be general, manifested in an Action as a consequence of the influence. <C> [ITALIC] F12: There have been changes, many of them due to economic progress, new construction, and other factors that  [ITALIC] influence cities. <C> 0.54 <R> <C> [ITALIC] influence <C> 0.366 <C> An Agent has influence on a Cognizer. The influence may be general, manifested in an Action as a consequence of the influence. <C> [ITALIC] F13: The Cycladic culture was  [ITALIC] influenced by societies in the east. <C> 0.46 <R> <C> [EMPTY] <C> 0.366 <C> An Agent has influence on a Cognizer. The influence may be general, manifested in an Action as a consequence of the influence. <C> [ITALIC] F14: Their complaint: the system  [ITALIC] discourages working. <C> 0.364 <R> <C> [ITALIC] undergo <C> 0.313 <C> An Entity changes, either in its category membership or in terms of the value of an Attribute. <C> [ITALIC] F15: The animosity between these two traditional enemies is beginning to  [ITALIC] diminish. <C> 0.805 <R> <C> [ITALIC] change <C> 0.313 <C> An Entity changes, either in its category membership or in terms of the value of an Attribute. <C> [ITALIC] F16: The  [ITALIC] shift in the image of Gates has been an interesting one for me to watch. <C> 0.351 <R> <C> [EMPTY] <C> 0.313 <C> An Entity changes, either in its category membership or in terms of the value of an Attribute. <C> [ITALIC] F17: The settlements of Thira and Akrotiri  [ITALIC] thrived at this time. <C> 0.256 <CAP> Table 4: Frame Quality Score Examples. The targeted word appears in italics font in the sentence.
<R> <C> [EMPTY] <C> [BOLD] MH370  [ITALIC] R-2 <C> [BOLD] MH370  [ITALIC] R-SU4 <C> [BOLD] Ukraine  [ITALIC] R-2 <C> [BOLD] Ukraine  [ITALIC] R-SU4 <C> [BOLD] Israel-Gaza  [ITALIC] R-2 <C> [BOLD] Israel-Gaza  [ITALIC] R-SU4 <C> [BOLD] NSA  [ITALIC] R-2 <C> [BOLD] NSA  [ITALIC] R-SU4 <R> <C> Chieu and Lee <C> 6.43 <C> 10.89 <C> 4.64 <C> 8.87 <C> 3.38 <C> [BOLD] 7.32 <C> 6.14 <C> 9.73 <R> <C> Yan et al. <C> 6.37 <C> 10.35 <C> 4.57 <C> 8.67 <C> 2.39 <C> 5.78 <C> 3.99 <C> 7.73 <R> <C> Abstract <C> 6.16 <C> 10.62 <C> 3.85 <C> 8.40 <C> 2.21 <C> 5.42 <C> 7.03 <C> 8.65 <R> <C> -  [ITALIC] Greedy Algorithm <C> -  [ITALIC] Greedy Algorithm <C> -  [ITALIC] Greedy Algorithm <C> -  [ITALIC] Greedy Algorithm <C> -  [ITALIC] Greedy Algorithm <C> -  [ITALIC] Greedy Algorithm <C> -  [ITALIC] Greedy Algorithm <C> -  [ITALIC] Greedy Algorithm <C> -  [ITALIC] Greedy Algorithm <R> <C> Basic <C> 6.59 <C> 9.80 <C> 5.31 <C> 9.23 <C> 3.15 <C> 6.20 <C> 3.81 <C> 7.58 <R> <C> Thread <C> 6.55 <C> 10.86 <C> 5.73 <C> 9.75 <C> 3.16 <C> 6.16 <C> 6.29 <C> 10.09 <R> <C> -  [ITALIC] Alternating Optimization (leveraging comments) <C> -  [ITALIC] Alternating Optimization (leveraging comments) <C> -  [ITALIC] Alternating Optimization (leveraging comments) <C> -  [ITALIC] Alternating Optimization (leveraging comments) <C> -  [ITALIC] Alternating Optimization (leveraging comments) <C> -  [ITALIC] Alternating Optimization (leveraging comments) <C> -  [ITALIC] Alternating Optimization (leveraging comments) <C> -  [ITALIC] Alternating Optimization (leveraging comments) <C> -  [ITALIC] Alternating Optimization (leveraging comments) <R> <C> Thread+OptTFIDF <C> [ITALIC] 8.74 <C> 11.63 <C> [ITALIC] 9.10 <C> [ITALIC] 12.59 <C> 3.78 <C> 6.45 <C> [ITALIC] 8.07 <C> [ITALIC] 10.31 <R> <C> Thread+OptWordNet <C> [ITALIC] 8.73 <C> [ITALIC]  [BOLD] 11.87 <C> [ITALIC] 8.67 <C> [ITALIC] 12.10 <C> [BOLD] 4.11 <C> 6.64 <C> [ITALIC]  [BOLD] 8.63 <C> [ITALIC]  [BOLD] 11.12 <R> <C> Thread+OptWordVec <C> [ITALIC]  [BOLD] 9.29 <C> 11.63 <C> [ITALIC]  [BOLD] 9.16 <C> [ITALIC]  [BOLD] 12.72 <C> 3.75 <C> 6.38 <C> [ITALIC] 8.29 <C> [ITALIC] 10.36 <CAP> Table 4: ROUGE-2 (R-2) and ROUGE-SU4 (R-SU4) scores (multiplied by 100) for different timeline generation approaches on four event datasets. Systems that statistically significantly outperform the three baselines (p<0.05, paired t−test) are in italics. Numbers in bold are the highest score for each column.
<R> <C> [ITALIC] k <C> [ITALIC] nocc <C> [BOLD] Most freq. selection <C> [BOLD] Most freq. selection  [ITALIC] μ avg <C> [BOLD] Random among top  [ITALIC] k <C> [BOLD] Random among top  [ITALIC] k μ avg <R> <C> 5 <C> >100 <C> 0.17 <C> 0.15 <C> 0.13 <C> 0.11 <R> <C> 10 <C> >100 <C> 0.19 <C> 0.14 <C> 0.08 <C> 0.07 <R> <C> 20 <C> >100 <C> 0.15 <C> 0.13 <C> 0.04 <C> 0.04 <R> <C> 5 <C> >200 <C> 0.16 <C> 0.13 <C> 0.13 <C> 0.11 <R> <C> 10 <C> >200 <C> 0.19 <C> 0.14 <C> 0.09 <C> 0.07 <R> <C> 20 <C> >200 <C> 0.14 <C> 0.11 <C> 0.04 <C> 0.04 <CAP> Table 6: System performances varying the number of candidates k and the number of occurrences of the names nocc.
<R> <C> [ITALIC] k <C> [ITALIC] nocc <C> [BOLD] Global acc. <C> [BOLD] Global acc.  [ITALIC] μ avg <C> [BOLD] CER acc. <C> [BOLD] CER acc.  [ITALIC] μ avg <R> <C> 5 <C> >100 <C> 0.61 <C> 0.49 <C> 0.39 <C> 0.26 <R> <C> 10 <C> >100 <C> 0.57 <C> 0.47 <C> 0.48 <C> 0.35 <R> <C> 20 <C> >100 <C> 0.48 <C> 0.39 <C> 0.39 <C> 0.29 <R> <C> 5 <C> >200 <C> 0.65 <C> 0.57 <C> 0.42 <C> 0.31 <R> <C> 10 <C> >200 <C> 0.62 <C> 0.56 <C> 0.54 <C> 0.44 <R> <C> 20 <C> >200 <C> 0.53 <C> 0.49 <C> 0.43 <C> 0.34 <CAP> Table 6: System performances varying the number of candidates k and the number of occurrences of the names nocc.
<R> <C> [BOLD] Algorithm <C> [BOLD] Features <C> [BOLD] Accuracy (%) <R> <C> Baseline <C> [EMPTY] <C> 43.5 <R> <C> MNB <C> Words + POS <C> 72.5 <R> <C> MNB <C> Words <C> 70.5 <R> <C> MNB <C> POS <C> 66.1 <R> <C> SVM <C> Words + POS <C> 74.1 <R> <C> SVM <C> Words <C> 72.5 <R> <C> SVM <C> POS <C> 67.4 <CAP> Table 2: Preliminary Experiments: Results
<R> <C> Baseline  [ITALIC] b <C> B1 <C> B4 <C> M <C> R <C> S <C> C <R> <C> [BOLD] w/o weight-init: <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> XE <C> 77.7 <C> 34.8 <C> 26.9 <C> 56.3 <C> 20.3 <C> 113.9 <R> <C> None <C> 65.6 <C> 19.4 <C> 22.7 <C> 48.9 <C> 15.8 <C> 91.4 <R> <C> MA <C> 75.6 <C> 28.7 <C> 24.4 <C> 53.6 <C> 17.9 <C> 103.3 <R> <C> SC <C> 79.0 <C> 34.6 <C> 26.9 <C> 56.2 <C> 20.6 <C> 118.1 <R> <C> CF <C> [BOLD] 79.9 <C> [BOLD] 36.5 <C> [BOLD] 27.7 <C> [BOLD] 57.4 <C> [BOLD] 21.4 <C> [BOLD] 122.1 <R> <C> [BOLD] w/ weight-init: <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> XE <C> 78.5 <C> 35.3 <C> 27.3 <C> 56.9 <C> 20.8 <C> 115.5 <R> <C> None <C> 78.6 <C> 33.7 <C> 26.5 <C> 56.1 <C> 20.2 <C> 115.2 <R> <C> MA <C> 79.0 <C> 34.1 <C> 26.6 <C> 56.3 <C> 20.2 <C> 116.1 <R> <C> SC <C> 79.6 <C> 36.5 <C> 27.6 <C> 57.4 <C> 21.4 <C> 121.2 <R> <C> CF <C> [BOLD] 80.3 <C> [BOLD] 37.3 <C> [BOLD] 28.1 <C> [BOLD] 58.0 <C> [BOLD] 21.8 <C> [BOLD] 124.0 <CAP> Table 3: Comparison of using various baselines b in Equation 6. XE: the performance after pre-training with cross-entropy loss.
<R> <C> #unlabel <C> stage <C> B1 <C> B4 <C> M <C> R <C> S <C> C <R> <C> 0 <C> XE <C> 78.5 <C> 35.3 <C> 27.3 <C> 56.9 <C> 20.8 <C> 115.5 <R> <C> 0 <C> CMAL <C> 80.3 <C> 37.3 <C> 28.1 <C> 58.0 <C> 21.8 <C> 124.0 <R> <C> 50k <C> XE <C> 78.8 <C> 36.2 <C> 27.6 <C> 57.2 <C> 21.1 <C> 118.1 <R> <C> 50k <C> CMAL <C> 80.2 <C> 37.6 <C> 28.1 <C> 58.1 <C> 21.9 <C> 124.8 <R> <C> 100k <C> XE <C> 79.0 <C> 36.2 <C> 27.7 <C> 57.3 <C> 21.2 <C> 118.3 <R> <C> 100k <C> CMAL <C> [BOLD] 80.5 <C> [BOLD] 38.0 <C> [BOLD] 28.3 <C> [BOLD] 58.2 <C> [BOLD] 22.0 <C> [BOLD] 125.5 <CAP> Table 5: The results after XE and CMAL training when using different numbers of unlabeled images.
<R> <C> [BOLD] Model <C> [BOLD] In-domain <C> [BOLD] Out-of-domain <C> [BOLD] Overall <R> <C> Bert-Large Baseline <C> 82.6 <C> 78.4 <C> 81.4 <R> <C> BERT with History Augmented Query <C> 82.7 <C> 78.6 <C> 81.5 <R> <C> Bert + Answer Verification <C> 83.8 <C> 81.9 <C> 82.8 <R> <C> BERT + MMFT + ADA <C> 86.4 <C> 81.9 <C> 85.0 <R> <C> ConvBERT <C> 87.7 <C> 85.4 <C> 86.8 <R> <C> Google SQuAD 2.0 + MMFT <C> 88.5 <C> 86.0 <C> 87.8 <R> <C> Our model <C> 90.9 <C> [BOLD] 89.2 <C> 90.4 <R> <C> Google SQuAD 2.0 + MMFT(Ensemble) <C> 89.9 <C> 88.0 <C> 89.4 <R> <C> Our model(Ensemble) <C> [BOLD] 91.4 <C> [BOLD] 89.2 <C> [BOLD] 90.7 <R> <C> human <C> 89.4 <C> 87.4 <C> 88.8 <CAP> Table 1: CoQA test set results, which are scored by the CoQA evaluation server. All the results are obtained from https://stanfordnlp.github.io/coqa/.
<R> <C> [BOLD] Model <C> [BOLD] In-domain <R> <C> Baseline Model <C> 89.5 <R> <C> + Rationale Tagging Task <C> 90.0 <R> <C> + Adversarial Training <C> 90.7 <R> <C> + Knowledge Distillation <C> 91.0 <R> <C> + Post-Processing <C> 91.3 <R> <C> Ensemble <C> 91.8 <CAP> Table 2: Ablation study on the CoQA dev-set
<R> <C> Model <C> MAP <C> MRR <R> <C> QLM <C> 0.678 <C> 0.726 <R> <C> NNQLM-II <C> 0.759 <C> 0.825 <R> <C> CNN (Yu et al.) (Yu et al.,  2014 ) <C> 0.711 <C> 0.785 <R> <C> CNN (Severyn) (Severyn and Moschitti,  2016 ) <C> 0.746 <C> 0.808 <R> <C> aNMM (Yang et al.) (Yin et al.,  2015 ) <C> 0.749 <C> 0.811 <R> <C> QMWF-LM-char <C> 0.715 [ITALIC] α <C> 0.758  [ITALIC] α <R> <C> QMWF-LM-word <C> [BOLD] 0.752  [ITALIC] α <C> [BOLD] 0.814  [ITALIC] α <CAP> Table 3. Experimental Result on TRECQA (raw). α denotes significant improvement over QLM.
<R> <C> Model <C> MAP <C> MRR <R> <C> QLM <C> 0.512 <C> 0.515 <R> <C> NNQLM-II <C> 0.650 <C> 0.659 <R> <C> QA-CNN (Santos et al.) (dos Santos et al.,  2016 ) <C> 0.670 <C> 0.682 <R> <C> AP-CNN (Santos et al.) (dos Santos et al.,  2016 ) <C> 0.688 <C> 0.696 <R> <C> QMWF-LM-char <C> 0.657  [ITALIC] α <C> 0.679  [ITALIC] α <R> <C> QMWF-LM-word <C> [BOLD] 0.695  [ITALIC] αβ <C> [BOLD] 0.710  [ITALIC] αβ <CAP> Table 4. Experimental Result on WIKIQA. α and β denote significant improvement over QLM and NNQLM-II, respectively.
<R> <C> Model <C> P@1 <C> MRR <R> <C> Random guess <C> 0.200 <C> 0.457 <R> <C> QLM <C> 0.395 <C> 0.604 <R> <C> NNQLM-II <C> 0.466 <C> 0.673 <R> <C> QA-CNN (Santos et al.)(dos Santos et al.,  2016 ) <C> 0.564 <C> 0.727 <R> <C> AP-CNN (Santos et al.)(dos Santos et al.,  2016 ) <C> 0.560 <C> 0.726 <R> <C> QMWF-LM-char <C> 0.513  [ITALIC] αβ <C> 0.696  [ITALIC] αβ <R> <C> QMWF-LM-word <C> [BOLD] 0.575  [ITALIC] αβ <C> [BOLD] 0.745  [ITALIC] αβ <CAP> Table 5. Experimental Result on YahooQA. α and β denote significant improvement over QLM and NNQLM-II, respectively.
<R> <C> [BOLD] Model <C> [BOLD] Units <C> [BOLD] NNLM <C> [BOLD] WER,%  [BOLD] calls <C> [BOLD] WER,%  [BOLD] YouTube <C> [BOLD] WER,%  [BOLD] books <R> <C> TDNN-F LF-MMI <C> phone <C> no <C> 34.2 <C> 22.2 <C> 19.8 <R> <C> TDNN-F LF-MMI <C> phone <C> yes <C> 33.5 <C> 20.9 <C> 18.6 <R> <C> Joint CTC-Attention <C> char <C> no <C> 38.9 <C> 23.2 <C> 21.0 <R> <C> Joint CTC-Attention <C> char <C> yes <C> 38.9 <C> 22.4 <C> 18.9 <R> <C> Joint CTC-Attention <C> subword <C> no <C> 39.6 <C> 23.0 <C> 21.5 <R> <C> Joint CTC-Attention <C> subword <C> yes <C> 40.4 <C> 23.3 <C> 19.6 <R> <C> RNN-transducer <C> char <C> no <C> 39.6 <C> 21.7 <C> 21.6 <R> <C> RNN-transducer <C> char <C> yes <C> 47.3 <C> 31.3 <C> 37.2 <R> <C> RNN-transducer <C> subword <C> no <C> 39.3 <C> 20.3 <C> 21.0 <R> <C> RNN-transducer <C> subword <C> yes <C> 45.9 <C> 24.8 <C> 23.2 <R> <C> Transformer <C> char <C> no <C> 35.1 <C> 21.3 <C> 18.5 <R> <C> Transformer <C> char <C> yes <C> 35.7 <C> 21.4 <C> 16.9 <R> <C> Transformer <C> subword <C> no <C> 34.8 <C> 19.1 <C> 18.1 <R> <C> Transformer <C> subword <C> yes <C> 36.8 <C> 20.9 <C> 16.8 <CAP> Table 2: Final results.
<R> <C> [BOLD] Source L. <C> [BOLD] ONTS <C> [BOLD] Google T. <R> <C> Arabic <C> 0.318 <C> 0.255 <R> <C> Czech <C> 0.218 <C> 0.226 <R> <C> Danish <C> 0.324 <C> 0.296 <R> <C> Farsi <C> 0.245 <C> 0.197 <R> <C> French <C> 0.26 <C> 0.286 <R> <C> German <C> 0.205 <C> 0.25 <R> <C> Italian <C> 0.236 <C> 0.31 <R> <C> Polish <C> 0.568 <C> 0.511 <R> <C> Portuguese <C> 0.579 <C> 0.424 <R> <C> Spanish <C> 0.283 <C> 0.334 <R> <C> Turkish <C> 0.238 <C> 0.395 <CAP> Table 1: Automatic evaluation.
<R> <C> Method <C> NLLoracle <C> NLLgen <R> <C> CoT <C> 8.19 <C> 7.54 <R> <C> Meta-CoTGAN <C> [BOLD] 7.69 <C> [BOLD] 6.86 <CAP> Table 1: Evaluation result on synthetic oracle with sequence length 20. For CoT, we present their best score for NLLgen.
<R> <C> Method <C> BLEU-2 <C> BLEU-3 <C> BLEU-4 <C> BLEU-5 <C> NLLgen <R> <C> MLE <C> 0.731 <C> 0.497 <C> 0.305 <C> 0.189 <C> 0.718 <R> <C> SeqGAN <C> 0.745 <C> 0.498 <C> 0.294 <C> 0.180 <C> 1.082 <R> <C> RankGAN <C> 0.743 <C> 0.467 <C> 0.264 <C> 0.156 <C> 1.344 <R> <C> LeakGAN <C> 0.746 <C> 0.528 <C> 0.355 <C> 0.230 <C> 0.679 <R> <C> RelGAN (100) <C> 0.849 ± 0.030 <C> 0.687 ± 0.047 <C> 0.502 ± 0.048 <C> 0.331 ± 0.044 <C> 0.756 ± 0.054 <R> <C> RelGAN (1000) <C> 0.814 ± 0.012 <C> 0.634 ± 0.020 <C> 0.455 ± 0.023 <C> 0.303 ± 0.020 <C> 0.655 ± 0.048 <R> <C> Meta-CoTGAN (100) <C> [BOLD] 0.858 ± 0.003 <C> [BOLD] 0.692 ± 0.005 <C> [BOLD] 0.518 ± 0.007 <C> [BOLD] 0.363 ± 0.009 <C> [BOLD] 0.578 ± 0.036 <R> <C> Meta-CoTGAN (1000) <C> 0.842 ± 0.011 <C> 0.675 ± 0.019 <C> 0.502 ± 0.026 <C> 0.349 ± 0.024 <C> 0.583 ± 0.028 <CAP> Table 2: Evaluations on COCO Image Captions dataset. For RelGAN and Meta-CoTGAN, the temperature (in parentheses) is set to be 100 and 1000, and results are averaged over 6 runs (random seeds). For NLLgen (last column), the smaller the better.
<R> <C> Method <C> BLEU-2 <C> BLEU-3 <C> BLEU-4 <C> BLEU-5 <C> NLLgen <R> <C> MLE <C> 0.768 <C> 0.473 <C> 0.240 <C> 0.126 <C> 2.382 <R> <C> SeqGAN <C> 0.777 <C> 0.491 <C> 0.261 <C> 0.138 <C> 2.773 <R> <C> RankGAN <C> 0.727 <C> 0.435 <C> 0.209 <C> 0.101 <C> 3.345 <R> <C> LeakGAN <C> 0.826 <C> 0.645 <C> 0.437 <C> 0.272 <C> 2.356 <R> <C> RelGAN (100) <C> 0.881± 0.013 <C> 0.705 ± 0.019 <C> 0.501 ± 0.023 <C> 0.319 ± 0.018 <C> 2.482 ± 0.031 <R> <C> RelGAN (1000) <C> 0.837 ± 0.012 <C> 0.654 ± 0.010 <C> 0.435 ± 0.011 <C> 0.265 ± 0.011 <C> 2.285 ± 0.025 <R> <C> Meta-CoTGAN (100) <C> [BOLD] 0.882 ± 0.014 <C> [BOLD] 0.734 ± 0.017 <C> [BOLD] 0.542 ± 0.016 <C> [BOLD] 0.358 ± 0.015 <C> 2.299 ± 0.011 <R> <C> Meta-CoTGAN (1000) <C> 0.868 ± 0.015 <C> 0.703 ± 0.014 <C> 0.500 ± 0.016 <C> 0.318 ± 0.016 <C> [BOLD] 2.205 ± 0.053 <CAP> Table 3: Evaluations on EMNLP2017 WMT News dataset. See the caption of Table 2 for more details.
<R> <C> Quora Model <C> Quora B-1 <C> Quora B-2 <C> Quora B-3 <C> Quora B-4 <C> Quora R-1 <C> Quora R-2 <C> Quora R-L <R> <C> Seq2seq(Prakash et al.,  2016 ) <C> 54.62 <C> 40.41 <C> 31.25 <C> 24.97 <C> 57.27 <C> 33.04 <C> 54.62 <R> <C> Residual Seq2seq-Attn (Prakash et al.,  2016 ) <C> 54.59 <C> 40.49 <C> 31.25 <C> 24.89 <C> 57.10 <C> 32.86 <C> 54.61 <R> <C> [ITALIC] β-VAE,  [ITALIC] β=10−3(Higgins et al.,  2017 ) <C> 43.02 <C> 28.60 <C> 20.98 <C> 16.29 <C> 41.81 <C> 21.17 <C> 40.09 <R> <C> [ITALIC] β-VAE,  [ITALIC] β=10−4(Higgins et al.,  2017 ) <C> 47.86 <C> 33.21 <C> 24.96 <C> 19.73 <C> 47.62 <C> 25.49 <C> 45.46 <R> <C> BOW-Hard (lower bound) <C> 33.40 <C> 21.18 <C> 14.43 <C> 10.36 <C> 36.08 <C> 16.23 <C> 33.77 <R> <C> LBOW-Topk (ours) <C> [BOLD] 55.79 <C> [BOLD] 42.03 <C> [BOLD] 32.71 <C> [BOLD] 26.17 <C> [BOLD] 58.79 <C> [BOLD] 34.57 <C> [BOLD] 56.43 <R> <C> LBOW-Gumbel (ours) <C> 55.75 <C> 41.96 <C> 32.66 <C> 26.14 <C> 58.60 <C> 34.47 <C> 56.23 <R> <C> RbM-SL(Li et al.,  2018 ) <C> - <C> 43.54 <C> - <C> - <C> 64.39 <C> 38.11 <C> - <R> <C> RbM-IRL(Li et al.,  2018 ) <C> - <C> 43.09 <C> - <C> - <C> 64.02 <C> 37.72 <C> - <R> <C> Cheating BOW (upper bound) <C> 72.96 <C> 61.78 <C> 54.40 <C> 49.47 <C> 72.15 <C> 52.61 <C> 68.53 <R> <C> MSCOCO <C> MSCOCO <C> MSCOCO <C> MSCOCO <C> MSCOCO <C> MSCOCO <C> MSCOCO <C> MSCOCO <R> <C> Model <C> B-1 <C> B-2 <C> B-3 <C> B-4 <C> R-1 <C> R-2 <C> R-L <R> <C> Seq2seq(Prakash et al.,  2016 ) <C> 69.61 <C> 47.14 <C> 31.64 <C> 21.65 <C> 40.11 <C> 14.31 <C> 36.28 <R> <C> Residual Seq2seq-Attn (Prakash et al.,  2016 ) <C> 71.24 <C> 49.65 <C> 34.04 <C> 23.66 <C> 41.07 <C> 15.26 <C> 37.35 <R> <C> [ITALIC] β-VAE,  [ITALIC] β=10−3(Higgins et al.,  2017 ) <C> 68.81 <C> 45.82 <C> 30.56 <C> 20.99 <C> 39.63 <C> 13.86 <C> 35.81 <R> <C> [ITALIC] β-VAE,  [ITALIC] β=10−4(Higgins et al.,  2017 ) <C> 70.04 <C> 47.59 <C> 32.29 <C> 22.54 <C> 40.72 <C> 14.75 <C> 36.75 <R> <C> BOW-Hard (lower bound) <C> 48.14 <C> 28.35 <C> 16.25 <C> 9.28 <C> 31.66 <C> 8.30 <C> 27.37 <R> <C> LBOW-Topk (ours) <C> [BOLD] 72.60 <C> [BOLD] 51.14 <C> [BOLD] 35.66 <C> [BOLD] 25.27 <C> 42.08 <C> [BOLD] 16.13 <C> [BOLD] 38.16 <R> <C> LBOW-Gumbel (ours) <C> 72.37 <C> 50.81 <C> 35.32 <C> 24.98 <C> [BOLD] 42.12 <C> 16.05 <C> 38.13 <R> <C> Cheating BOW (upper bound) <C> 80.87 <C> 75.09 <C> 62.24 <C> 52.64 <C> 49.95 <C> 23.94 <C> 43.77 <CAP> Table 1: Results on the Quora and MSCOCO dataset. B for BLEU and R for ROUGE.
<R> <C> [EMPTY] <C> Without AT (%) Args <C> Without AT (%) NonArgs <C> Without AT (%) Ratio <C> With AT (%) Args <C> With AT (%) NonArgs <C> With AT (%) Ratio <R> <C> Train <C> 7.65 <C> 92.35 <C> 1:13 <C> 43.81 <C> 56.19 <C> 1:1.3 <R> <C> Dev <C> 7.35 <C> 92.65 <C> 1:13 <C> 41.22 <C> 58.78 <C> 1:1.4 <CAP> Table 1: Label distribution of training and dev set. Arg is short for argument. AT denotes our introduced auxiliary tags.
<R> <C> System <C> P <C> R <C> F1 <R> <C> Ours <C> [BOLD] 89.7 <C> [BOLD] 88.3 <C> [BOLD] 89.0 <R> <C> -Auxiliary tags <C> 89.5 <C> 88.1 <C> 88.8 <R> <C> -Self-attention <C> 89.7 <C> 87.9 <C> 88.7 <R> <C> -Auxiliary tags -self-attention <C> 88.9 <C> 88.1 <C> 88.5 <R> <C> +Adaptive argument pruning <C> 88.6 <C> 85.5 <C> 87.0 <CAP> Table 5: Results on the English in-domain test set.
<R> <C> [BOLD] Model <C> [BOLD] Random batches <C> [BOLD] Document batches <R> <C> Baseline <C> 39.2 <C> 39.2 <R> <C> MLE <C> 41.2 <C> 40.0 <R> <C> Seq-MRT <C> 39.4 <C> 40.5 <R> <C> Doc-MRT (ordered) <C> [BOLD] 39.0 <C> [BOLD] 38.9 <CAP> Table 2: TER on en-de after MLE and MRT under sentence-TER (seq-MRT) and doc-TER (doc-MRT). Lower TER is better.
<R> <C> [BOLD] Model <C> [BOLD] JFLEG  [BOLD] P <C> [BOLD] JFLEG  [BOLD] R <C> [BOLD] JFLEG  [BOLD] M2 <C> [BOLD] JFLEG  [BOLD] GLEU <C> [BOLD] CONLL2014  [BOLD] P <C> [BOLD] CONLL2014  [BOLD] R <C> [BOLD] CONLL2014  [BOLD] M2 <C> [BOLD] CONLL2014  [BOLD] GLEU <R> <C> Baseline <C> [BOLD] 67.3 <C> 38.2 <C> [BOLD] 58.4 <C> 50.4 <C> [BOLD] 54.4 <C> 21.8 <C> 41.9 <C> 67.3 <R> <C> MLE <C> 64.7 <C> 37.7 <C> 56.6 <C> 50.1 <C> 51.4 <C> 20.9 <C> 39.8 <C> 67.1 <R> <C> Seq-MRT <C> 62.7 <C> 39.1 <C> 56.0 <C> 50.0 <C> 52.4 <C> 24.5 <C> 42.7 <C> 67.1 <R> <C> Doc-MRT (ordered) <C> 64.4 <C> [BOLD] 41.0 <C> 57.8 <C> [BOLD] 51.4 <C> 53.2 <C> [BOLD] 24.6 <C> [BOLD] 43.2 <C> [BOLD] 67.5 <CAP> Table 3: GEC Precision, Recall, M2, and GLEU after MLE and MRT. MRT is under 1−sentence-GLEU for seq-MRT and 1−doc-GLEU for doc-MRT. Both MRT schemes uses random batches and random sentence sampling. Higher scores are better for all metrics.
<R> <C> [BOLD] # <C> [BOLD] Class <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> 1 <C> RQ <C> 0.74 <C> 0.79 <C> 0.76 <R> <C> 2 <C> Fact <C> 0.77 <C> 0.72 <C> 0.74 <CAP> Table 4: Supervised Learning Results for RQs vs. Fact/Info-Seeking Questions in Debate Forums
<R> <C> [EMPTY] <C> MAE <C> RMSE⋆ <C> PCC ( [ITALIC] γ) <C> SRCC ( [ITALIC] ρ) <R> <C> BLSTM <C> 0.85 <C> 0.96 <C> 0.53 <C> 0.52 <R> <C> pBLSTM <C> 0.79 <C> 0.92 <C> 0.56 <C> 0.56 <R> <C> BLSTM + Attn <C> 0.68 <C> 0.74 <C> 0.80 <C> 0.79 <R> <C> pBLSTM + Attn <C> 0.51 <C> 0.57 <C> 0.89 <C> 0.88 <CAP> Table I: Performance comparison with baseline models. Results on two corpora are reported together.
<R> <C> [EMPTY] <C> Label (#) <C> Text Search <C> word2vec <C> SemanticILP <C> DecompAttn <C> DGEM <C> BiDAF <R> <C> Reasoning Types <C> qn logic (63) <C> 14.3 <C> 25.4 <C> 22.2 <C> 16.3 <C> 19.8 <C> 23.4 <R> <C> Reasoning Types <C> linguistic (38) <C> 26.3 <C> 21.1 <C> 28.9 <C> 26.3 <C> 29.6 <C> 34.9 <R> <C> Reasoning Types <C> hypothetical (27) <C> 18.5 <C> 29.6 <C> 18.5 <C> 29.4 <C> 33.3 <C> 22.0 <R> <C> Reasoning Types <C> explanation (25) <C> 12.0 <C> 28.0 <C> 12.0 <C> 25.0 <C> 29.3 <C> 26.3 <R> <C> Reasoning Types <C> multihop (19) <C> 21.1 <C> 15.8 <C> 15.8 <C> 22.4 <C> 32.9 <C> 25.9 <R> <C> Reasoning Types <C> comparison (11) <C> 18.2 <C> 9.1 <C> 18.2 <C> 29.5 <C> 13.6 <C> 47.0 <R> <C> Reasoning Types <C> algebraic (8) <C> 37.5 <C> 0.0 <C> 25.0 <C> 0.0 <C> 3.1 <C> 15.6 <R> <C> Reasoning Types <C> physical (7) <C> 14.3 <C> 14.3 <C> 14.3 <C> 28.6 <C> 35.7 <C> 17.9 <R> <C> Reasoning Types <C> analogy (1) <C> 0.0 <C> 0.0 <C> 100.0 <C> 0.0 <C> 0.0 <C> 0.0 <R> <C> Knowledge Types <C> basic facts (78) <C> 19.2 <C> 20.5 <C> 29.5 <C> 21.8 <C> 27.4 <C> 29.3 <R> <C> Knowledge Types <C> causes (39) <C> 23.1 <C> 25.6 <C> 12.8 <C> 23.7 <C> 25.0 <C> 32.7 <R> <C> Knowledge Types <C> experiments (25) <C> 4.0 <C> 16.0 <C> 8.0 <C> 21.8 <C> 22.0 <C> 24.1 <R> <C> Knowledge Types <C> definition (24) <C> 16.7 <C> 29.2 <C> 16.7 <C> 26.0 <C> 34.4 <C> 16.7 <R> <C> Knowledge Types <C> purpose (21) <C> 19.0 <C> 28.6 <C> 23.8 <C> 27.4 <C> 25.0 <C> 27.4 <R> <C> Knowledge Types <C> algebraic (6) <C> 50.0 <C> 0.0 <C> 50.0 <C> 0.0 <C> 0.0 <C> 16.7 <R> <C> Knowledge Types <C> physical (6) <C> 16.7 <C> 16.7 <C> 0.0 <C> 4.2 <C> 8.3 <C> 12.5 <R> <C> [EMPTY] <C> Overall acc. <C> 18.2 <C> 21.7 <C> 20.7 <C> 21.7 <C> 24.9 <C> 26.5 <CAP> Table 5: Accuracy on our subset of ARC Challenge Set questions, partitioned based on the first label from the Kemeny ordering of the reasoning and knowledge type annotations, respectively; \nicefrac1k partial credit is given when the correct answer is in the set of k selected answers. The number of questions assigned each primary label is indicated by (#).
<R> <C> [BOLD] Models <C> [BOLD] DLI <C> [BOLD] KVRET  [BOLD] Slot <C> [BOLD] KVRET  [BOLD] Slot <C> [BOLD] KVRET  [BOLD] Slot <C> [BOLD] KVRET  [BOLD] Intent <C> [BOLD] KVRET*  [BOLD] Slot <C> [BOLD] KVRET*  [BOLD] Slot <C> [BOLD] KVRET*  [BOLD] Slot <C> [BOLD] KVRET*  [BOLD] Intent <R> <C> [BOLD] Models <C> [BOLD] DLI <C> P <C> R <C> F1 <C> Acc. <C> P <C> R <C> F1 <C> Acc. <R> <C> NoMem <C> No <C> 54.8 <C> 80.0 <C> 56.7 <C> 93.4 <C> 48.9 <C> 81.0 <C> 54.7 <C> 93.8 <R> <C> MemNet <C> No <C> 75.8 <C> 81.1 <C> 75.8 <C> 93.9 <C> 73.1 <C> 81.8 <C> 74.5 <C> 92.8 <R> <C> MemNet <C> Yes <C> 76.0 <C> 82.3 <C> [BOLD] 77.4( [BOLD] +1.6) <C> 93.9(+0) <C> 75.8 <C> 81.3 <C> 76.3( [BOLD] +1.8) <C> [BOLD] 93.8( [BOLD] +1.0) <R> <C> SDEN <C> No <C> 70.5 <C> 80.9 <C> 70.1 <C> 93.6 <C> 56.9 <C> 81.3 <C> 59.4 <C> 93.0 <R> <C> SDEN <C> Yes <C> 64.9 <C> 80.9 <C> 70.8 ( [BOLD] +0.7) <C> 93.8( [BOLD] +0.2) <C> 56.5 <C> 81.4 <C> 60.2( [BOLD] +0.8) <C> 93.5 [BOLD] (+0.5) <R> <C> SDEN† <C> No <C> 71.9 <C> 82.2 <C> 74.0 <C> 93.7 <C> 72.7 <C> 80.8 <C> 74.9 <C> 93.2 <R> <C> SDEN† <C> Yes <C> 75.2 <C> 81.4 <C> 76.6( [BOLD] +2.6) <C> [BOLD] 94.3( [BOLD] +0.6) <C> 78.0 <C> 81.4 <C> [BOLD] 78.3( [BOLD] +3.4) <C> 93.2(+0) <CAP> Table 2: SLU results on original KVRET and multi-domain KVRET*, including accuracy of intent detection and average precision, recall and F1 score of slot filling.
<R> <C> [BOLD] Model <C> [BOLD] CoNLL03 <C> [BOLD] DocRED <C> [BOLD] Ontonote5 <C> [BOLD] Tweet <C> [BOLD] Webpage <C> [BOLD] Wikigold <C> [BOLD] Avg. <R> <C> [ITALIC] Public NER toolkits <C> [ITALIC] Public NER toolkits <C> [ITALIC] Public NER toolkits <C> [ITALIC] Public NER toolkits <C> [ITALIC] Public NER toolkits <C> [ITALIC] Public NER toolkits <C> [ITALIC] Public NER toolkits <C> [ITALIC] Public NER toolkits <R> <C> NLTK <C> 48.91 <C> 45.19 <C> 39.00 <C> 21.18 <C> 28.17 <C> 44.22 <C> 37.78 <R> <C> SpaCy <C> 65.14 <C> 51.32 <C> [BOLD] 76.66 <C> 31.87 <C> 36.39 <C> 58.86 <C> 53.37 <R> <C> StandordNER <C> 87.95 <C> 53.37 <C> 60.64 <C> 35.74 <C> 44.34 <C> 62.00 <C> 57.34 <R> <C> [ITALIC] State-of-the-art neural models <C> [ITALIC] State-of-the-art neural models <C> [ITALIC] State-of-the-art neural models <C> [ITALIC] State-of-the-art neural models <C> [ITALIC] State-of-the-art neural models <C> [ITALIC] State-of-the-art neural models <C> [ITALIC] State-of-the-art neural models <C> [ITALIC] State-of-the-art neural models <R> <C> Bi-LSTM-CRF <C> 87.00 <C> 52.30 <C> 56.33 <C> 33.66 <C> 46.45 <C> 61.81 <C> 56.26 <R> <C> CVT <C> 92.09 <C> 67.43 <C> 62.41 <C> 40.49 <C> 51.76 <C> 73.57 <C> 64.63 <R> <C> ELMo <C> 92.51 <C> 69.84 <C> 62.35 <C> 33.82 <C> [BOLD] 52.33 <C> 75.63 <C> 64.41 <R> <C> Flair <C> [BOLD] 92.90 <C> 67.45 <C> 64.76 <C> 38.87 <C> 51.56 <C> 74.99 <C> 65.09 <R> <C> BERTbase <C> 91.95 <C> 71.82 <C> 66.63 <C> 47.79 <C> 45.58 <C> 78.21 <C> 67.00 <R> <C> BERTlarge <C> 92.10 <C> 73.67 <C> 66.31 <C> 50.00 <C> 49.85 <C> 80.44 <C> 68.72 <R> <C> [ITALIC] Our models <C> [ITALIC] Our models <C> [ITALIC] Our models <C> [ITALIC] Our models <C> [ITALIC] Our models <C> [ITALIC] Our models <C> [ITALIC] Our models <C> [ITALIC] Our models <R> <C> OpenNERbase <C> 92.02 <C> 79.80 <C> 66.71 <C> 50.22 <C> 48.75 <C> [BOLD] 82.37 <C> 70.12 (+4.66%) <R> <C> OpenNERlarge <C> 92.15 <C> [BOLD] 80.20 <C> 68.19 <C> [BOLD] 51.45 <C> 50.81 <C> 82.16 <C> [BOLD] 70.83 (+3.07%) <CAP> Table 1: Comparing our methods with state-of-the-art methods on various open-domain NER datasets.
<R> <C> [BOLD] Method <C> [BOLD] NCC score <R> <C> USVC (Our, reconstruction) <C> 0.838 <R> <C> USVC (Our, conversion) <C> 0.821 <R> <C> PitchNet (reconstruction) <C> 0.882 <R> <C> PitchNet (conversion) <C> 0.855 <CAP> Table 1: Automatic quality scores
<R> <C> [BOLD] Method <C> [BOLD] Naturalness <C> [BOLD] Similarity <R> <C> USVC (Original) <C> 3.06 <C> 3.34 <R> <C> USVC (Our) <C> 2.92 <C> 3.14 <R> <C> PitchNet <C> [BOLD] 3.75 <C> [BOLD] 3.64 <CAP> Table 2: MOS scores
<R> <C> Test <C> Users <C> in-language Lexical <C> in-language Abstract <C> cross-language Lex Avg <C> cross-language Lex All <C> cross-language Embeds <C> cross-language Abs Avg <C> cross-language Abs All <R> <C> EN <C> 850 <C> 69.3 <C> 66.1 <C> 51.8 <C> 50.5 <C> 61.6 <C> 55.3 <C> 59.8 <R> <C> NL <C> 894 <C> 81.3 <C> 71.8 <C> 52.3 <C> 50.0 <C> 56.8 <C> 59.5 <C> 69.2 <R> <C> FR <C> 1,008 <C> 80.8 <C> 68.3 <C> 53.4 <C> 53.8 <C> 50.0 <C> 58.7 <C> 65.4 <R> <C> PT <C> 3,066 <C> 86.0 <C> 68.1 <C> 55.3 <C> 63.8 <C> 59.5 <C> 59.3 <C> 58.9 <R> <C> ES <C> 8,112 <C> 85.3 <C> 69.8 <C> 55.6 <C> 63.5 <C> 71.3 <C> 56.6 <C> 66.0 <CAP> Table 2: Number of users per language and results for gender prediction (accuracy). In-Language: 10-fold cross-validation. Cross-Language: Testing on all test data in two setups: averages over single source models (Avg) or training a single model on all languages except the target (All). Comparison of lexical n-gram models (Lex), bleached models (Abs) and multilingual embeddings model (Embeds).
<R> <C> [EMPTY] <C> [BOLD] Test → <C> EN <C> NL <C> FR <C> PT <C> ES <R> <C> [BOLD] Train <C> EN <C> [EMPTY] <C> 52.8 <C> 48.0 <C> 51.6 <C> 50.4 <R> <C> [BOLD] Train <C> NL <C> 51.1 <C> [EMPTY] <C> 50.3 <C> 50.0 <C> 50.2 <R> <C> [BOLD] Train <C> FR <C> 55.2 <C> 50.0 <C> [EMPTY] <C> 58.3 <C> 57.1 <R> <C> [BOLD] Train <C> PT <C> 50.2 <C> 56.4 <C> 59.6 <C> [EMPTY] <C> 64.8 <R> <C> [BOLD] Train <C> ES <C> 50.8 <C> 50.1 <C> 55.6 <C> 61.2 <C> [EMPTY] <R> <C> [EMPTY] <C> Avg <C> 51.8 <C> 52.3 <C> 53.4 <C> 55.3 <C> 55.6 <CAP> Table 3: Pair-wise results for lexicalized models.
<R> <C> [BOLD] Test Set <C> [BOLD] Method <C> [BOLD] Detection  [BOLD] Acc. <C> [BOLD] Detection  [BOLD] Prec. <C> [BOLD] Detection  [BOLD] Rec. <C> [BOLD] Detection  [BOLD] F1. <C> [BOLD] Correction  [BOLD] Acc. <C> [BOLD] Correction  [BOLD] Prec. <C> [BOLD] Correction  [BOLD] Rec. <C> [BOLD] Correction  [BOLD] F1. <R> <C> SIGHAN <C> NTOU tseng2015introduction <C> 42.2 <C> 42.2 <C> 41.8 <C> 42.0 <C> 39.0 <C> 38.1 <C> 35.2 <C> 36.6 <R> <C> [EMPTY] <C> NCTU-NTUT tseng2015introduction <C> 60.1 <C> 71.7 <C> 33.6 <C> 45.7 <C> 56.4 <C> 66.3 <C> 26.1 <C> 37.5 <R> <C> [EMPTY] <C> HanSpeller++ zhang2015hanspeller++ <C> 70.1 <C> [BOLD] 80.3 <C> 53.3 <C> 64.0 <C> 69.2 <C> [BOLD] 79.7 <C> 51.5 <C> 62.5 <R> <C> [EMPTY] <C> Hybird wang2018hybrid <C> - <C> 56.6 <C> 69.4 <C> 62.3 <C> - <C> - <C> - <C> 57.1 <R> <C> [EMPTY] <C> FASPell hong2019faspell <C> 74.2 <C> 67.6 <C> 60.0 <C> 63.5 <C> 73.7 <C> 66.6 <C> 59.1 <C> 62.6 <R> <C> [EMPTY] <C> Confusionset wang2019confusionset <C> - <C> 66.8 <C> 73.1 <C> 69.8 <C> - <C> 71.5 <C> 59.5 <C> 64.9 <R> <C> [EMPTY] <C> BERT-Pretrain <C> 6.8 <C> 3.6 <C> 7.0 <C> 4.7 <C> 5.2 <C> 2.0 <C> 3.8 <C> 2.6 <R> <C> [EMPTY] <C> BERT-Finetune <C> 80.0 <C> 73.0 <C> 70.8 <C> 71.9 <C> 76.6 <C> 65.9 <C> 64.0 <C> 64.9 <R> <C> [EMPTY] <C> Soft-Masked BERT <C> [BOLD] 80.9 <C> 73.7 <C> [BOLD] 73.2 <C> [BOLD] 73.5 <C> [BOLD] 77.4 <C> 66.7 <C> [BOLD] 66.2 <C> [BOLD] 66.4 <R> <C> News Title <C> BERT-Pretrain <C> 7.1 <C> 1.3 <C> 3.6 <C> 1.9 <C> 0.6 <C> 0.6 <C> 1.6 <C> 0.8 <R> <C> [EMPTY] <C> BERT-Finetune <C> 80.0 <C> 65.0 <C> 61.5 <C> 63.2 <C> 76.8 <C> 55.3 <C> 52.3 <C> 53.8 <R> <C> [EMPTY] <C> Soft-Masked BERT <C> [BOLD] 80.8 <C> [BOLD] 65.5 <C> [BOLD] 64.0 <C> [BOLD] 64.8 <C> [BOLD] 77.6 <C> [BOLD] 55.8 <C> [BOLD] 54.5 <C> [BOLD] 55.2 <CAP> Table 2: Performances of Different Methods on CSC
<R> <C> [BOLD] Train Set <C> [BOLD] Method <C> [BOLD] Detection  [BOLD] Acc. <C> [BOLD] Detection  [BOLD] Prec. <C> [BOLD] Detection  [BOLD] Rec. <C> [BOLD] Detection  [BOLD] F1. <C> [BOLD] Correction  [BOLD] Acc. <C> [BOLD] Correction  [BOLD] Prec. <C> [BOLD] Correction  [BOLD] Rec. <C> [BOLD] Correction  [BOLD] F1. <R> <C> 500,000 <C> BERT-Finetune <C> 71.8 <C> 49.6 <C> 48.2 <C> 48.9 <C> 67.4 <C> 36.5 <C> 35.5 <C> 36.0 <R> <C> 500,000 <C> Soft-Masked BERT <C> [BOLD] 72.3 <C> [BOLD] 50.3 <C> [BOLD] 49.6 <C> [BOLD] 50.0 <C> [BOLD] 68.2 <C> [BOLD] 37.9 <C> [BOLD] 37.4 <C> [BOLD] 37.6 <R> <C> 1,000,000 <C> BERT-Finetune <C> 74.2 <C> 54.7 <C> 51.3 <C> 52.9 <C> 70.0 <C> 41.6 <C> 39.0 <C> 40.3 <R> <C> 1,000,000 <C> Soft-Masked BERT <C> [BOLD] 75.3 <C> [BOLD] 56.3 <C> [BOLD] 54.2 <C> [BOLD] 55.2 <C> [BOLD] 71.1 <C> [BOLD] 43.6 <C> [BOLD] 41.9 <C> [BOLD] 42.7 <R> <C> 2,000,000 <C> BERT-Finetune <C> 77.0 <C> 59.7 <C> 57.0 <C> 58.3 <C> 73.1 <C> 48.0 <C> 45.8 <C> 46.9 <R> <C> 2,000,000 <C> Soft-Masked BERT <C> [BOLD] 77.6 <C> [BOLD] 60.0 <C> [BOLD] 58.5 <C> [BOLD] 59.2 <C> [BOLD] 73.7 <C> [BOLD] 48.4 <C> [BOLD] 47.3 <C> [BOLD] 47.8 <R> <C> 5,000,000 <C> BERT-Finetune <C> 80.0 <C> 65.0 <C> 61.5 <C> 63.2 <C> 76.8 <C> 55.3 <C> 52.3 <C> 53.8 <R> <C> 5,000,000 <C> Soft-Masked BERT <C> [BOLD] 80.8 <C> [BOLD] 65.5 <C> [BOLD] 64.0 <C> [BOLD] 64.8 <C> [BOLD] 77.6 <C> [BOLD] 55.8 <C> [BOLD] 54.5 <C> [BOLD] 55.2 <CAP> Table 3: Impact of Different Sizes of Training Data
<R> <C> [BOLD] Method <C> [BOLD] Detection  [BOLD] Acc. <C> [BOLD] Detection  [BOLD] Prec. <C> [BOLD] Detection  [BOLD] Rec. <C> [BOLD] Detection  [BOLD] F1. <C> [BOLD] Correction  [BOLD] Acc. <C> [BOLD] Correction  [BOLD] Prec. <C> [BOLD] Correction  [BOLD] Rec. <C> [BOLD] Correction  [BOLD] F1. <R> <C> BERT-Finetune +Force(Upper Bound) <C> 89.9 <C> 75.6 <C> 90.3 <C> 82.3 <C> 82.9 <C> 58.4 <C> 69.8 <C> 63.6 <R> <C> Soft-Masked BERT <C> 80.8 <C> 65.5 <C> 64.0 <C> 64.8 <C> 77.6 <C> 55.8 <C> 54.5 <C> 55.2 <R> <C> Soft-Masked BERT-R <C> 81.0 <C> 75.2 <C> 53.9 <C> 62.8 <C> 78.4 <C> 64.6 <C> 46.3 <C> 53.9 <R> <C> Rand-Masked BERT <C> 70.9 <C> 46.6 <C> 48.5 <C> 47.5 <C> 68.1 <C> 38.8 <C> 40.3 <C> 39.5 <R> <C> BERT-Finetune <C> 80.0 <C> 65.0 <C> 61.5 <C> 63.2 <C> 76.8 <C> 55.3 <C> 52.3 <C> 53.8 <R> <C> Hard-Masked BERT (0.95) <C> 80.6 <C> 65.3 <C> 63.2 <C> 64.2 <C> 76.7 <C> 53.6 <C> 51.8 <C> 52.7 <R> <C> Hard-Masked BERT (0.9) <C> 77.4 <C> 57.8 <C> 60.3 <C> 59.0 <C> 72.4 <C> 44.0 <C> 45.8 <C> 44.9 <R> <C> Hard-Masked BERT (0.7) <C> 65.3 <C> 38.0 <C> 50.9 <C> 43.5 <C> 58.9 <C> 24.2 <C> 32.5 <C> 27.7 <CAP> Table 4: Ablation Study of Soft-Masked BERT on News Title
<R> <C> Speaker <C> GSGW OK <C> GSGW KO <C> GSGW Acc. (%) <C> PC OK <C> PC KO <C> PC Acc. (%) <C> RPS OK <C> RPS KO <C> RPS Acc. (%) <C> OMPD OK <C> OMPD KO <C> OMPD Acc. (%) <R> <C> AWB <C> 1134 <C> 4 <C> 99.64 <C> 1138 <C> 0 <C> [BOLD] 100 <C> 1138 <C> 0 <C> [BOLD] 100 <C> 1138 <C> 0 <C> [BOLD] 100 <R> <C> BDL <C> 1112 <C> 19 <C> 98.32 <C> 1131 <C> 0 <C> [BOLD] 100 <C> 1131 <C> 0 <C> [BOLD] 100 <C> 1131 <C> 0 <C> [BOLD] 100 <R> <C> Berlin <C> 356 <C> 179 <C> 66.54 <C> 528 <C> 7 <C> 98.69 <C> 535 <C> 0 <C> [BOLD] 100 <C> 525 <C> 10 <C> 98.13 <R> <C> CLB <C> 1131 <C> 1 <C> 99.91 <C> 1132 <C> 0 <C> [BOLD] 100 <C> 1132 <C> 0 <C> [BOLD] 100 <C> 1132 <C> 0 <C> [BOLD] 100 <R> <C> JMK <C> 1096 <C> 18 <C> 98.38 <C> 1109 <C> 5 <C> 99.55 <C> 1114 <C> 0 <C> [BOLD] 100 <C> 1114 <C> 0 <C> [BOLD] 100 <R> <C> KSP <C> 1103 <C> 29 <C> 97.43 <C> 1132 <C> 0 <C> [BOLD] 100 <C> 1059 <C> 73 <C> 93.55 <C> 1132 <C> 0 <C> [BOLD] 100 <R> <C> RL <C> 50 <C> 0 <C> [BOLD] 100 <C> 50 <C> 0 <C> [BOLD] 100 <C> 50 <C> 0 <C> [BOLD] 100 <C> 50 <C> 0 <C> [BOLD] 100 <R> <C> RMS <C> 1082 <C> 50 <C> 95.58 <C> 1132 <C> 0 <C> [BOLD] 100 <C> 1129 <C> 3 <C> 99.73 <C> 1132 <C> 0 <C> [BOLD] 100 <R> <C> SB <C> 49 <C> 1 <C> [BOLD] 98 <C> 37 <C> 13 <C> 74 <C> 42 <C> 8 <C> 84 <C> 47 <C> 3 <C> 94 <R> <C> SLT <C> 1125 <C> 6 <C> 99.38 <C> 1101 <C> 30 <C> 97.35 <C> 1131 <C> 0 <C> [BOLD] 100 <C> 1131 <C> 0 <C> [BOLD] 100 <R> <C> [BOLD] TOTAL <C> 8238 <C> 307 <C> 96.41 <C> 8490 <C> 55 <C> 99.36 <C> 8461 <C> 84 <C> 99.02 <C> 8532 <C> 13 <C> [BOLD] 99.85 <CAP> Table 1: Results of polarity detection for 10 speech corpora using the four techniques. The number of sentences whose polarity is correctly (OK) or incorrectly (KO) determined are indicated, as well as the detection accuracy (in %).
<R> <C> [BOLD] Label Statement-Non-Opinion <C> [BOLD] Count 72,824 <C> [BOLD] % 36 <C> [BOLD] Label Collaborative Completion <C> [BOLD] Count 699 <C> [BOLD] % 0.4 <R> <C> Acknowledgement <C> 37,096 <C> 19 <C> Repeat-Phrase <C> 660 <C> 0.3 <R> <C> Statement-Opinion <C> 25,197 <C> 13 <C> Open-Question <C> 632 <C> 0.3 <R> <C> Agreement <C> 10,820 <C> 5 <C> Rhetorical-Question <C> 557 <C> 0.3 <R> <C> Abandoned <C> 10,569 <C> 5 <C> Hold <C> 540 <C> 0.2 <R> <C> Appreciation <C> 4,663 <C> 2 <C> Reject <C> 338 <C> 0.2 <R> <C> Yes-No-Question <C> 4,624 <C> 2 <C> Negative Non-No Answer <C> 292 <C> 0.1 <R> <C> Non-Verbal <C> 3,548 <C> 2 <C> Non-understanding <C> 288 <C> 0.1 <R> <C> Yes Answer <C> 2,934 <C> 1 <C> Other Answer <C> 279 <C> 0.1 <R> <C> Conventional Closing <C> 2,486 <C> 1 <C> Conventional Opening <C> 220 <C> 0.1 <R> <C> Uninterpretable <C> 2,158 <C> 1 <C> Or-Clause <C> 207 <C> 0.1 <R> <C> Wh-Question <C> 1,911 <C> 1 <C> Dispreferred Answers <C> 205 <C> 0.1 <R> <C> No Answer <C> 1,340 <C> 1 <C> 3rd-Party-Talk <C> 115 <C> 0.1 <R> <C> Response Acknowledgement <C> 1,277 <C> 1 <C> Offers / Options <C> 109 <C> 0.1 <R> <C> Hedge <C> 1,182 <C> 1 <C> Self-talk <C> 102 <C> 0.1 <R> <C> Declarative Yes-No-Question <C> 1,174 <C> 1 <C> Downplayer <C> 100 <C> 0.1 <R> <C> Other <C> 1,074 <C> 1 <C> Maybe <C> 98 <C> <0.1 <R> <C> Backchannel-Question <C> 1,019 <C> 1 <C> Tag-Question <C> 93 <C> <0.1 <R> <C> Quotation <C> 934 <C> 0.5 <C> Declarative Wh-Question <C> 80 <C> <0.1 <R> <C> Summarization <C> 919 <C> 0.5 <C> Apology <C> 76 <C> <0.1 <R> <C> Affirmative Non-Yes Answer <C> 836 <C> 0.4 <C> Thanking <C> 67 <C> <0.1 <R> <C> Action Directive <C> 719 <C> 0.4 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 1: Label distribution in the Switchboard Dialog Act Corpus (SwDA) Jurafsky1997.
<R> <C> [BOLD] Approach <C> [BOLD] SwDA  [ITALIC] μ <C> [BOLD] SwDA  [ITALIC] σ <C> [BOLD] MRDA  [ITALIC] μ <C> [BOLD] MRDA  [ITALIC] σ <R> <C> Random Initialization <C> 76.02 <C> 0.18 <C> 76.94 <C> 0.23 <R> <C> Word2Vec <C> 76.58 <C> 0.26 <C> 77.67 <C> 0.04 <R> <C> FastText <C> 76.38 <C> 0.25 <C> 78.10 <C> 0.14 <R> <C> Dependency-Based <C> 77.04 <C> 0.20 <C> 78.36 <C> 0.07 <R> <C> ELMo <C> 77.85 <C> 0.24 <C> 80.67 <C> 0.14 <R> <C> BERT <C> 79.19 <C> 0.16 <C> 88.62 <C> 0.03 <R> <C> Character-Level <C> 76.47 <C> 0.29 <C> 88.66 <C> 0.02 <R> <C> Coarse-Grained POS <C> 66.91 <C> 0.33 <C> 80.57 <C> 0.12 <R> <C> Fine-Grained POS <C> 69.47 <C> 0.15 <C> 82.12 <C> 0.05 <R> <C> Lemma <C> 74.46 <C> 0.24 <C> 77.05 <C> 0.13 <R> <C> BERT + Char-Level <C> 79.34 <C> 0.10 <C> 88.69 <C> 0.02 <R> <C> BERT + Fine-Grained POS <C> 79.09 <C> 0.30 <C> 88.62 <C> 0.05 <CAP> Table 3: Accuracy (%) results using different token representation approaches. The first block refers to word-level approaches, the second to the character-level approach, the third to functional-level approaches, and the last to the combination of multiple levels. The separation (dashed line) in the block for word-level approaches distinguishes between uncontextualized and contextualized word representations.
<R> <C> [BOLD] Approach <C> [BOLD] SwDA  [ITALIC] μ <C> [BOLD] SwDA  [ITALIC] σ <C> [BOLD] MRDA  [ITALIC] μ <C> [BOLD] MRDA  [ITALIC] σ <R> <C> Without Context <C> 79.34 <C> 0.10 <C> 88.69 <C> 0.02 <R> <C> 1 Preceding <C> 81.72 <C> 0.40 <C> 88.95 <C> 0.03 <R> <C> 3 Preceding Flat <C> 82.38 <C> 0.21 <C> 89.16 <C> 0.04 <R> <C> Whole History Flat <C> 81.88 <C> 0.25 <C> 79.01 <C> 0.04 <R> <C> 3 Preceding Summary <C> 82.49 <C> 0.22 <C> 89.16 <C> 0.03 <R> <C> Whole History Summary <C> 82.76 <C> 0.13 <C> 89.19 <C> 0.01 <R> <C> Future Summary <C> 81.53 <C> 0.31 <C> 88.86 <C> 0.04 <R> <C> Preceding + Future <C> 84.15 <C> 0.27 <C> 89.38 <C> 0.04 <CAP> Table 5: Accuracy (%) results using context information from the surrounding segments.
<R> <C> [BOLD] Approach <C> [BOLD] SwDA  [ITALIC] μ <C> [BOLD] SwDA  [ITALIC] σ <C> [BOLD] MRDA  [ITALIC] μ <C> [BOLD] MRDA  [ITALIC] σ <R> <C> Without Context <C> 79.34 <C> 0.10 <C> 88.69 <C> 0.02 <R> <C> 1 Preceding <C> 79.39 <C> 0.07 <C> 88.71 <C> 0.08 <R> <C> 3 Preceding Flat <C> 79.55 <C> 0.09 <C> 88.74 <C> 0.01 <R> <C> Whole History Flat <C> 79.05 <C> 0.32 <C> 88.63 <C> 0.08 <R> <C> 3 Preceding Summary <C> 79.45 <C> 0.10 <C> 88.72 <C> 0.01 <R> <C> Whole History Summary <C> 79.35 <C> 0.11 <C> 88.70 <C> 0.05 <CAP> Table 6: Accuracy (%) results using context information concerning turn taking.
<R> <C> [BOLD] Approach <C> [BOLD] SwDA  [ITALIC] μ <C> [BOLD] SwDA  [ITALIC] σ <C> [BOLD] MRDA  [ITALIC] μ <C> [BOLD] MRDA  [ITALIC] σ <R> <C> Gold Standard Preceding Annotations <C> 80.49 <C> 0.10 <C> 90.93 <C> 0.05 <R> <C> Automatic Preceding Classifications <C> 79.11 <C> 0.16 <C> 90.63 <C> 0.06 <R> <C> Preceding + Future Classifications <C> 82.34 <C> 0.20 <C> 91.27 <C> 0.05 <CAP> Table 8: Accuracy (%) results on the test sets of both datasets.
<R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> Devel fy <C> Devel nl <C> Devel fy-nl <C> Devel all <C> Test fy <C> Test nl <C> Test fy-nl <C> Test all <C> Total <R> <C> # of Frisian words <C> # of Frisian words <C> # of Frisian words <C> 9190 <C> 0 <C> 2381 <C> 11,571 <C> 10,753 <C> 0 <C> 1798 <C> 12,551 <C> 24,122 <R> <C> # of Dutch words <C> # of Dutch words <C> # of Dutch words <C> 0 <C> 4569 <C> 533 <C> 5102 <C> 0 <C> 3475 <C> 306 <C> 3781 <C> 8883 <R> <C> ASR System <C> AM train data <C> LM <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Baseline ASR <C> ( [BOLD] 1) <C> (1) <C> 36.4 <C> 43.7 <C> 48.2 <C> 40.3 <C> 31.5 <C> 39.5 <C> 47.9 <C> 35.2 <C> 37.8 <R> <C> ASR_AA <C> ( [BOLD] 1+ [BOLD] 2) <C> (1) <C> 31.1 <C> 35.2 <C> 42.4 <C> 34.1 <C> 28.6 <C> 31.8 <C> 44.0 <C> 31.2 <C> 32.7 <R> <C> ASR_AA_CGN-NL <C> ( [BOLD] 1+ [BOLD] 2+3) <C> (1) <C> 26.8 <C> 28.2 <C> 37.6 <C> 29.0 <C> 25.2 <C> 24.9 <C> 39.0 <C> 26.8 <C> 27.9 <R> <C> ASR_AA_CGN-NL-VL <C> ( [BOLD] 1+ [BOLD] 2+3+4) <C> (1) <C> 26.3 <C> 27.6 <C> 36.8 <C> 28.4 <C> 25.1 <C> 24.4 <C> 39.3 <C> 26.7 <C> 27.6 <R> <C> ASR_AA_CGN-NL++ <C> ( [BOLD] 1+ [BOLD] 2+ [BOLD] 3) <C> (1) <C> 25.8 <C> 27.4 <C> 36.9 <C> 28.1 <C> 24.8 <C> 24.6 <C> 37.6 <C> 26.4 <C> 27.2 <R> <C> ASR_AA_CGN-NL-VL++ <C> ( [BOLD] 1+ [BOLD] 2+ [BOLD] 3+ [BOLD] 4) <C> (1) <C> 26.4 <C> 26.9 <C> 36.2 <C> 28.1 <C> 24.5 <C> 23.2 <C> 38.5 <C> 26.0 <C> 27.1 <R> <C> ASR_AA_CGN-NL-VL++_RS <C> ( [BOLD] 1+ [BOLD] 2+ [BOLD] 3+ [BOLD] 4) <C> (1)+R <C> 24.3 <C> 25.2 <C> 36.2 <C> 26.5 <C> 22.9 <C> 21.8 <C> 36.5 <C> 24.3 <C> 25.4 <R> <C> ASR_AA_CGN-NL-VL++_CS-LM <C> ( [BOLD] 1+ [BOLD] 2+ [BOLD] 3+ [BOLD] 4) <C> (9) <C> 24.6 <C> 26.7 <C> 33.3 <C> 26.5 <C> 22.5 <C> 22.4 <C> 32.9 <C> 23.8 <C> 25.2 <R> <C> ASR_AA_CGN-NL-VL++_CS-LM_RS <C> ( [BOLD] 1+ [BOLD] 2+ [BOLD] 3+ [BOLD] 4) <C> (9)+R <C> [BOLD] 22.6 <C> [BOLD] 24.7 <C> [BOLD] 31.4 <C> [BOLD] 24.6 <C> [BOLD] 21.2 <C> [BOLD] 21.3 <C> [BOLD] 30.1 <C> [BOLD] 22.3 <C> [BOLD] 23.5 <CAP> Table 3: WER (%) obtained on the development and test set of the FAME! Corpus - Different AM training data is identified with the numbers which are defined in Table 1. The 3-fold data augmentation is applied for the numbers marked in bold. The numbers referring to different LM are defined in Table 2. “+R” indicates applying rescoring using an RNN-LM trained on the corresponding text corpus.
<R> <C> [BOLD] Dataset <C> [BOLD] Dev EM <C> [BOLD] Dev F{}_{1} <C> [BOLD] Test EM <C> [BOLD] Test F{}_{1} <R> <C> NewsQA <C> 29.34 <C> 45.40 <C> 29.69 <C> 46.19 <R> <C> Quoref <C> 34.49 <C> 42.65 <C> 30.13 <C> 38.39 <R> <C> DROP <C> 19.09 <C> 23.16 <C> 17.69 <C> 21.87 <R> <C> SQuAD 1.1 <C> 68.03 <C> 78.55 <C> - <C> - <R> <C> SQuAD 2.0 <C> 33.70 <C> 39.17 <C> - <C> - <R> <C> ROPES <C> 40.03 <C> 49.07 <C> 47.96 <C> 56.06 <R> <C> DuoRC <C> 25.65 <C> 34.28 <C> 23.44 <C> 31.73 <CAP> Table 5: Performance on baseline BERT model on different datasets
<R> <C> [BOLD] Narrative QA <C> BLEU-1 <C> BLEU-4 <C> METEOR <C> ROUGE-L (F1) <R> <C> Dev Set <C> 0.17 <C> 0.021 <C> 0.33 <C> 0.52 <R> <C> Test Set <C> 0.16 <C> 0.019 <C> 0.33 <C> 0.53 <CAP> Table 5: Performance on baseline BERT model on different datasets
<R> <C> [EMPTY] <C> [BOLD] Dev EM <C> [BOLD] Dev F{}_{1} <C> [BOLD] IC EM <C> [BOLD] IC F{}_{1} <C> [BOLD] Imp EM <C> [BOLD] Imp F{}_{1} <C> [BOLD] No-Ans EM <C> [BOLD] No-Ans F{}_{1} <C> [BOLD] SEARs EM <C> [BOLD] SEARs F{}_{1} <R> <C> NewsQA <C> 29.34 <C> 45.40 <C> - <C> - <C> 23.35 <C> 34.36 <C> 0.02 <C> 0.02 <C> 21.34 <C> 33.33 <R> <C> QuoRef <C> 34.49 <C> 42.65 <C> - <C> - <C> 32.91 <C> 44.84 <C> 0.0 <C> 0.0 <C> 34.84 <C> 42.11 <R> <C> DROP <C> 19.09 <C> 23.16 <C> 40.23 <C> 48.03 <C> - <C> - <C> 0.0 <C> 0.0 <C> 16.97 <C> 21.65 <R> <C> SQuAD <C> 68.03 <C> 78.55 <C> 56.25 <C> 64.58 <C> 46.74 <C> 57.97 <C> 0.0 <C> 0.0 <C> 56.53 <C> 71.25 <R> <C> ROPES <C> 40.03 <C> 49.07 <C> 24.08 <C> 31.74 <C> - <C> - <C> - <C> - <C> 14.05 <C> 19.12 <R> <C> DuoRC <C> 25.65 <C> 34.28 <C> 27.27 <C> 34.19 <C> 30.30 <C> 35.23 <C> - <C> - <C> 21.51 <C> 28.85 <CAP> Table 6: Quantitative and qualitative analysis of generated augmentations. We only show performance for high yield and high-quality augmentations.
<R> <C> [EMPTY] <C> [BOLD] StackPtr (code) UAS <C> [BOLD] StackPtr (code) LAS <C> [BOLD] H-PtrNet-PST (Gate) UAS <C> [BOLD] H-PtrNet-PST (Gate) LAS <C> [BOLD] H-PtrNet-PST (SGate) UAS <C> [BOLD] H-PtrNet-PST (SGate) LAS <R> <C> [BOLD] bg <C> 94.17±0.11 <C> 90.63±0.06 <C> 94.20±0.16 <C> 90.70±0.14 <C> [BOLD] 94.50±0.16 <C> [BOLD] 91.01±0.20 <R> <C> [BOLD] ca <C> [BOLD] 93.82±0.06 <C> [BOLD] 91.99±0.07 <C> 93.78±0.03 <C> 91.92±0.03 <C> 93.67±0.06 <C> 91.82±0.07 <R> <C> [BOLD] en <C> 90.97±0.07 <C> 89.06±0.08 <C> [BOLD] 91.03±0.19 <C> [BOLD] 89.07±0.16 <C> 90.94±0.12 <C> 89.01±0.12 <R> <C> [BOLD] de <C> 87.97±0.20 <C> 83.75±0.21 <C> [BOLD] 88.14±0.22 <C> [BOLD] 83.89±0.26 <C> 88.06±0.17 <C> 83.83±0.13 <R> <C> [BOLD] fr <C> 91.57±0.23 <C> 88.76±0.21 <C> 91.63±0.15 <C> 88.70±0.14 <C> [BOLD] 91.69±0.07 <C> [BOLD] 88.80±0.11 <R> <C> [BOLD] it <C> 93.76±0.13 <C> 92.00±0.08 <C> 93.73±0.08 <C> 91.90±0.10 <C> [BOLD] 93.88±0.05 <C> [BOLD] 92.09±0.02 <R> <C> [BOLD] ro <C> 91.15±0.12 <C> 85.54±0.13 <C> [BOLD] 91.34±0.18 <C> [BOLD] 85.73±0.22 <C> 91.09±0.09 <C> 85.36±0.10 <CAP> Table 1: Dependency parsing results on 7 UD Treebanks. StackPtr (code) denotes the experiments we rerun on our machine. H-PtrNet-PST (Gate) and H-PtrNet-PST (SGate) are H-PtrNet models with gating mechanism.
<R> <C> [BOLD] Approach <C> [BOLD] UAS <C> [BOLD] LAS <R> <C> [BOLD] Baselines <C> [EMPTY] <C> [EMPTY] <R> <C> StackPtr (paper) <C> 96.12±0.03 <C> 95.06±0.05 <R> <C> StackPtr (code) <C> 95.94±0.03 <C> 94.91±0.05 <R> <C> [BOLD] Proposed Model <C> [EMPTY] <C> [EMPTY] <R> <C> H-PtrNet-PST (Gate) <C> 96.03±0.02 <C> 94.99±0.02 <R> <C> H-PtrNet-PST (SGate) <C> 96.04±0.05 <C> 95.00±0.06 <R> <C> H-PtrNet-PS (Gate) <C> [BOLD] 96.09±0.05 <C> [BOLD] 95.03±0.03 <CAP> Table 2: Dependency parsing results on English Penn Treebank v3.0.
<R> <C> [BOLD] Approach <C> [BOLD] Span <C> [BOLD] Nuclearity <C> [BOLD] Relation <R> <C> [BOLD] Human Agreement <C> 95.7 <C> 90.4 <C> 83.0 <R> <C> [BOLD] Baselines <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Joty-2012 <C> 94.6 <C> 86.9 <C> 77.1 <R> <C> ji-eisenstein:2014:P14-1 <C> 93.5 <C> 81.3 <C> 70.5 <R> <C> Wang-acl-2017 <C> 95.6 <C> 87.8 <C> 77.6 <R> <C> Pointer Net† Xiang19 <C> 97.39±0.1 <C> 91.01±0.4 <C> 81.08±0.4 <R> <C> Pointer Net \lx @ [ITALIC] sectionsign Xiang19 <C> 97.14±0.1 <C> 91.00±0.3 <C> 81.29±0.2 <R> <C> [BOLD] Proposed Model <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> H-PtrNet-P† <C> [BOLD] 97.68±0.03 <C> 91.86±0.2 <C> 81.82±0.2 <R> <C> H-PtrNet-P \lx @ [ITALIC] sectionsign <C> 97.51±0.08 <C> 91.98±0.1 <C> 82.11±0.2 <R> <C> H-PtrNet-PS† <C> 97.56±0.1 <C> 91.52±0.3 <C> 82.05±0.5 <R> <C> H-PtrNet-PS \lx @ [ITALIC] sectionsign <C> 97.35±0.1 <C> 91.78±0.1 <C> 82.35±0.2 <R> <C> H-PtrNet-PST† <C> 97.56±0.06 <C> 91.97±0.3 <C> 82.37±0.4 <R> <C> H-PtrNet-PST \lx @ [ITALIC] sectionsign <C> 97.48±0.2 <C> [BOLD] 92.01±0.2 <C> [BOLD] 82.77±0.2 <CAP> Table 3: Discourse parsing results with gold segmentation. † denotes the models selected based on Span. \lx@sectionsign denotes the models selected based on Relation. We run all the experiments for three times and report the averages and the standard deviations.
<R> <C> [EMPTY] <C> MCD (dB) <C> BAP (dB) <C> [ITALIC] F0 RMSE (Hz) <C> V/UV (%) <R> <C> DNN <C> 4.19 <C> 1.95 <C> 9.13 <C> 4.24 <R> <C> LSTM <C> 4.05 <C> 1.94 <C> 8.76 <C> 3.97 <R> <C> BN-DNN <C> 4.00 <C> 1.92 <C> 8.90 <C> 3.97 <R> <C> BN-DNN-VB <C> 3.98 <C> 1.92 <C> [BOLD] 8.57 <C> 4.02 <R> <C> BN-DNN-MFC <C> 3.97 <C> [BOLD] 1.91 <C> 8.61 <C> [BOLD] 3.90 <R> <C> BN-DNN-WSJ <C> 3.97 <C> 1.92 <C> 8.59 <C> 3.96 <R> <C> MGE-DNN <C> 4.12 <C> 1.95 <C> 8.93 <C> 4.28 <R> <C> [BOLD] MGE-BN-DNN <C> [BOLD] 3.97 <C> 1.92 <C> 8.89 <C> 3.96 <CAP> TABLE I: Objective errors for all systems on the evaluation set. MCD and BAP are distortion measures for Mel-Cepstral Coefficients and Band Aperiodicities. RMSE indicates Root Mean Squared Error. V/UV indicates voiced/unvoiced error.
<R> <C> [EMPTY] <C> [BOLD] Model abbrev. <C> [BOLD] Layer form output <C> [BOLD] Dim #count <C> [BOLD] Seen labels RL <C> [BOLD] Seen labels AvgPr <C> [BOLD] Seen labels OneErr <C> [BOLD] Unseen labels RL <C> [BOLD] Unseen labels AvgPr <C> [BOLD] Unseen labels OneErr <C> [BOLD] Params #count <R> <C> [BOLD] [N16] <C> WSABIE+ <C> EW [ITALIC] ht <C> 100 <C> 5.21 <C> 36.64 <C> 41.72 <C> 48.81 <C> 0.37 <C> 99.94 <C> 722.10M <R> <C> [BOLD] [N16] <C> AiTextML avg <C> EW [ITALIC] ht <C> 100 <C> 3.54 <C> 32.78 <C> 25.99 <C> 52.89 <C> 0.39 <C> 99.94 <C> 724.47M <R> <C> [BOLD] [N16] <C> AiTextML inf <C> EW [ITALIC] ht <C> 100 <C> 3.54 <C> 32.78 <C> 25.99 <C> 21.62 <C> 2.66 <C> 98.61 <C> 724.47M <R> <C> 1-11  [BOLD] Baselines <C> WAN <C> [ITALIC] W⊤ [ITALIC] ht <C> – <C> 1.53 <C> 42.37 <C> [BOLD] 11.23 <C> – <C> – <C> – <C> 55.60M <R> <C> 1-11  [BOLD] Baselines <C> BIL-WAN [YH15] <C> [ITALIC] σ(EW)W [ITALIC] ht <C> 100 <C> 1.21 <C> 40.68 <C> 17.52 <C> 18.72 <C> 9.50 <C> 93.89 <C> 52.85M <R> <C> 1-11  [BOLD] Baselines <C> BIL-WAN [N16] <C> EW [ITALIC] ht <C> 100 <C> 1.12 <C> 41.91 <C> 16.94 <C> 16.26 <C> 10.55 <C> 93.23 <C> 52.84M <R> <C> [BOLD] Ours <C> GILE-WAN <C> [ITALIC] σ(E [ITALIC] U) [ITALIC] σ( [ITALIC] Vht) <C> 500 <C> [BOLD] 0.78 <C> [BOLD] 44.39 <C> 11.60 <C> [BOLD] 9.06 <C> [BOLD] 12.95 <C> [BOLD] 91.90 <C> 52.93M <R> <C> [BOLD] Ours <C> − constrained  [ITALIC] dj <C> [ITALIC] σ(EW) [ITALIC] σ(W [ITALIC] ht) <C> 100 <C> 1.01 <C> 37.71 <C> 16.16 <C> 10.34 <C> 11.21 <C> 93.38 <C> 52.85M <R> <C> [BOLD] Ours <C> − only label (Eq.  6 a) <C> [ITALIC] σ(EW) [ITALIC] ht <C> 100 <C> 1.06 <C> 40.81 <C> 13.77 <C> 9.77 <C> 14.71 <C> 90.56 <C> 52.84M <R> <C> [BOLD] Ours <C> − only input (Eq.  6 b) <C> E [ITALIC] σ(W [ITALIC] ht) <C> 100 <C> 1.07 <C> 39.78 <C> 15.67 <C> 19.28 <C> 7.18 <C> 95.91 <C> 52.84M <CAP> Table 2: Biomedical semantic indexing results computed over labels seen and unseen during training, i.e. the full-resource versus zero-resource settings. Best scores among the competing models are marked in bold.
<R> <C> [EMPTY] <C> TL;DR <C> CNN/Daily Mail <R> <C> zero-shot <C> 6/30 <C> 6/30 <R> <C> 60k fine-tuned <C> 26/30 <C> 29/30 <R> <C> supervised <C> 8/30 <C> 19/30 <R> <C> supervised + 60k fine-tuned <C> 11/30 <C> 20/30 <CAP> Table 8: Frequency with which generated summaries are accurate, in the sense of only making statements supported by the context, as judged by the authors on 30 articles from each dataset. The 60k fine-tuned model achieves high accuracy via copying; the supervised and supervised + 60k fine-tuned models are more abstractive but at significant cost to accuracy.
<R> <C> [ITALIC] β <C> 0 <C> 0.1 <C> 0.115 <C> 0.2 <C> 0.3 <C> 0.4 <R> <C> BLEU <C> 17.41 <C> 17.53 <C> 17.56 <C> 17.51 <C> 17.38 <C> 17.19 <CAP> Table 4: BLEU against β
<R> <C> [BOLD] Model <C> [BOLD] Validation R@1 <C> [BOLD] Validation R@10 <C> [BOLD] Validation R@50 <C> [BOLD] Validation MRR <C> [BOLD] Test R@1 <C> [BOLD] Test R@10 <C> [BOLD] Test R@50 <C> [BOLD] Test MRR <R> <C> [BOLD] subtask-1 <C> [BOLD] subtask-1 <C> [BOLD] subtask-1 <C> [BOLD] subtask-1 <C> [BOLD] subtask-1 <C> [BOLD] subtask-1 <C> [BOLD] subtask-1 <C> [BOLD] subtask-1 <C> [BOLD] subtask-1 <R> <C> ESIM <C> 43.76 <C> 71.70 <C> 95.84 <C> 53.24 <C> 50.1 <C> 78.3 <C> 95.4 <C> 59.34 <R> <C> T-ESIM <C> 52.62 <C> 76.46 <C> 96.08 <C> 60.57 <C> 61.9 <C> 82.2 <C> 96.6 <C> 69.09 <R> <C> T-ESIM-CR <C> 54.46 <C> 79.26 <C> 97.92 <C> 62.73 <C> 63.4 <C> 84.2 <C> 98.5 <C> 70.69 <R> <C> T-ESIM-Sampled <C> 53.16 <C> 78.5 <C> 96.46 <C> 61.54 <C> 62.8 <C> 83.4 <C> 96.6 <C> 69.7 <R> <C> T-ESIM-Sampled-CR <C> 55.46 <C> 81.98 <C> 98.2 <C> 64.12 <C> 64.3 <C> 84.7 <C> 97.3 <C> 71.25 <R> <C> [BOLD] subtask-2 <C> [BOLD] subtask-2 <C> [BOLD] subtask-2 <C> [BOLD] subtask-2 <C> [BOLD] subtask-2 <C> [BOLD] subtask-2 <C> [BOLD] subtask-2 <C> [BOLD] subtask-2 <C> [BOLD] subtask-2 <R> <C> ESIM <C> 11.12 <C> 31.5 <C> 58.6 <C> 18.59 <C> 12.8 <C> 28.5 <C> 36.5 <C> 18.43 <R> <C> T-ESIM <C> 18.72 <C> 35.9 <C> 61.26 <C> 25.13 <C> 21.6 <C> 36.0 <C> 44.1 <C> 26.68 <R> <C> [BOLD] subtask-4 <C> [BOLD] subtask-4 <C> [BOLD] subtask-4 <C> [BOLD] subtask-4 <C> [BOLD] subtask-4 <C> [BOLD] subtask-4 <C> [BOLD] subtask-4 <C> [BOLD] subtask-4 <C> [BOLD] subtask-4 <R> <C> ESIM <C> 40.16 <C> 76.18 <C> 96.36 <C> 53.43 <C> 43.5 <C> 82.1 <C> 96.2 <C> 57.96 <R> <C> T-ESIM <C> 47.76 <C> 77.22 <C> 96.4 <C> 58.43 <C> 52.5 <C> 82.3 <C> 97.1 <C> 63.6 <R> <C> [BOLD] subtask-5 <C> [BOLD] subtask-5 <C> [BOLD] subtask-5 <C> [BOLD] subtask-5 <C> [BOLD] subtask-5 <C> [BOLD] subtask-5 <C> [BOLD] subtask-5 <C> [BOLD] subtask-5 <C> [BOLD] subtask-5 <R> <C> K-ESIM <C> 44.82 <C> 72.74 <C> 96.4 <C> 54.52 <C> 50.1 <C> 78.3 <C> 96.3 <C> 60.2 <R> <C> TK-ESIM <C> 53.10 <C> 75.88 <C> 96.26 <C> 60.88 <C> 60.9 <C> 80.2 <C> 96.6 <C> 67.93 <R> <C> TK-ESIM-CR <C> 54.84 <C> 79.26 <C> 97.96 <C> 62.98 <C> 62.3 <C> 83.4 <C> 97.8 <C> 69.56 <CAP> Table 1: Performance of models on the Ubuntu validation and test datasets. R@k refers to Recall at position k in 100 candidates, denoted as R@1, R@10 and R@50. MRR refers to the Mean Reciprocal Rank.
<R> <C> Variable <C> Estimate <C> Std. Error <C> [ITALIC] z value <C> [ITALIC] Pr(>| [ITALIC] z|) <R> <C> Image: Scattered pairs (Intercept) <C> 16,20 <C> 4,15 <C> 3,91 <C> 9.42e-05 *** <R> <C> Image: Scattered random <C> -0,79 <C> 0,09 <C> -8,63 <C> <2e-16 *** <R> <C> Network: VGG9 <C> -0,73 <C> 3,47 <C> -0,21 <C> 0,83 <R> <C> Network: VGG7 <C> -12,22 <C> 2,45 <C> -4,98 <C> 6.37e-07 *** <R> <C> Dot ratio <C> -14,90 <C> 4,93 <C> -3,02 <C> 0.00253 ** <R> <C> Absolute difference <C> 0,17 <C> 0,37 <C> 0,46 <C> 0,64 <R> <C> Total dots <C> -0,03 <C> 0,04 <C> -0,87 <C> 0,39 <R> <C> Ratio * Network: VGG9 <C> 1,09 <C> 4,00 <C> 0,27 <C> 0,78 <R> <C> Ratio * Network: VGG7 <C> 11,81 <C> 2,83 <C> 4,18 <C> 2.97e-05 *** <CAP> Table 1: Multiple logistic regression of the CNN trials. Significance: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1.
<R> <C> Variable <C> Estimate <C> Std. Error <C> [ITALIC] z value <C> [ITALIC] Pr(>| [ITALIC] z|) <R> <C> Image: Column pairs sorted (Intercept) <C> 9,57 <C> 1,52 <C> 6,28 <C> 3.41e-10 *** <R> <C> Image: Column pairs mixed <C> -1,18 <C> 0,16 <C> -7,55 <C> 4.37e-14 *** <R> <C> Image: Scattered pairs <C> -3,54 <C> 0,14 <C> -25,15 <C> < 2e-16 *** <R> <C> Image: Scattered random <C> -3,75 <C> 0,14 <C> -26,73 <C> < 2e-16 *** <R> <C> Glimpses: RAM16 <C> -0,32 <C> 0,51 <C> -0,63 <C> 0,53 <R> <C> Glimpses: RAM8 <C> -0,97 <C> 0,51 <C> -1,91 <C> 0,06 <R> <C> Glimpses: RAM4 <C> -0,77 <C> 0,50 <C> -1,54 <C> 0,12 <R> <C> Dot ratio <C> -6,91 <C> 1,84 <C> -3,75 <C> 0.000179 *** <R> <C> Absolute difference <C> -0,25 <C> 0,15 <C> -1,64 <C> 0,10 <R> <C> Total dots <C> 0,04 <C> 0,02 <C> 2,24 <C> 0.025427 * <R> <C> Ratio * Glimpses: RAM16 <C> 0,33 <C> 0,63 <C> 0,52 <C> 0,60 <R> <C> Ratio * Glimpses: RAM8 <C> 1,34 <C> 0,62 <C> 2,14 <C> 0.032372 * <R> <C> Ratio * Glimpses: RAM4 <C> 0,81 <C> 0,62 <C> 1,31 <C> 0,19 <CAP> Table 2: Multiple logistic regression on RAM trials.
<R> <C> type <C> VGG7  [ITALIC] w <C> VGG7  [ITALIC] R2 <C> RAM24  [ITALIC] w <C> RAM24  [ITALIC] R2 <R> <C> scattered random <C> 0.363 <C> 0.843 <C> 0.524 <C> 0.801 <R> <C> scattered pairs <C> 0.256 <C> 0.581 <C> 0.340 <C> 0.913 <R> <C> column mixed <C> 0.047 <C> 0.979 <C> 0.078 <C> 0.975 <R> <C> column sorted <C> 0.012 <C> 1.0 <C> 0.051 <C> 0.984 <CAP> Table 3: Weber fractions and R2 for the ANS model.
<R> <C> [BOLD] Model <C> [BOLD] Laptop <C> [BOLD] Restaurant <R> <C> CRF <C> 74.01 <C> 69.56 <R> <C> IHS_RD <C> 74.55 <C> - <R> <C> NLANGP <C> - <C> 72.34 <R> <C> WDEmb <C> 75.16 <C> - <R> <C> LSTM <C> 75.25 <C> 71.26 <R> <C> BiLSTM-CNN-CRF <C> 77.8 <C> 72.5 <R> <C> RNCRF <C> 78.42 <C> - <R> <C> CMLA <C> 77.80 <C> - <R> <C> MIN <C> 77.58 <C> 73.44 <R> <C> GloVe-CNN <C> 77.67 <C> 72.08 <R> <C> Domain-CNN <C> 78.12 <C> 71.75 <R> <C> MaxPool-DE-CNN <C> 77.45 <C> 71.12 <R> <C> DE-LSTM <C> 78.73 <C> 72.94 <R> <C> DE-OOD-CNN <C> 80.21 <C> 74.2 <R> <C> DE-Google-CNN <C> 78.8 <C> 72.1 <R> <C> DE-CNN-CRF <C> 80.8 <C> 74.1 <R> <C> DE-CNN <C> [BOLD] 81.59* <C> [BOLD] 74.37* <CAP> Table 2: Comparison results in F1 score: numbers in the third group are averaged scores of 5 runs. * indicates the result is statistical significant at the level of 0.05.
<R> <C> [EMPTY] <C> Unique Params <C> # Streams DIRHA <C> # Streams DIRHA <C> # Streams AMI <R> <C> Model <C> (in million) <C> 2 <C> 3 <C> 2 <R> <C> [ITALIC] MEM-Array Model <C> [ITALIC] MEM-Array Model <C> [ITALIC] MEM-Array Model <C> [EMPTY] <C> [EMPTY] <R> <C> Baseline  <C> 21.8(2),32.1(3) <C> 33.0 <C> 32.1 <C> 59.5 <R> <C> Proposed Strategy <C> 11.6 <C> [BOLD] 26.8 <C> [BOLD] 21.7 <C> [BOLD] 54.6 <R> <C> [ITALIC] Other Fusion Methods <C> [ITALIC] Other Fusion Methods <C> [ITALIC] Other Fusion Methods <C> [EMPTY] <C> [EMPTY] <R> <C> WAV Align.& Avg. <C> 11.4 <C> 32.4 <C> 30.1 <C> 55.9 <R> <C> Frame Concat. <C> 16.9(2),23.8(3) <C> 33.7 <C> 33.8 <C> 59.4 <R> <C> ROVER <C> 11.4 <C> 34.2 <C> 23.6 <C> 58.0 <CAP> Table 4: WER(%) Comparson between proposed two-stage approach and alternative conventional methods.
<R> <C> [EMPTY] <C> DIHRA Fine-tune <C> DIHRA Freeze <C> [EMPTY] <R> <C> Model <C> PT Comp. <C> PT Comp. <C> AMI <R> <C> [ITALIC] Augmentation <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> no SpecAugment <C> 28.4 <C> 26.8 <C> 59.5 <R> <C> Stage-1 <C> 22.6 <C> [BOLD] 22.4 <C> 55.8 <R> <C> Stage-1, Stage-2 <C> 22.5 <C> 22.6 <C> [BOLD] 49.2 <CAP> Table 5: Perfermance (WER(%)) investigation of two-stage data augmentation using SpecAugment on 2-stream DIRHA and AMI.
<R> <C> [BOLD] Cross-Document Coreference Results <C> [BOLD] Cross-Document Coreference Results  [ITALIC] B3 <C> [BOLD] Cross-Document Coreference Results  [ITALIC] B3 <C> [BOLD] Cross-Document Coreference Results  [ITALIC] B3 <C> [BOLD] Cross-Document Coreference Results MUC <C> [BOLD] Cross-Document Coreference Results MUC <C> [BOLD] Cross-Document Coreference Results MUC <C> [BOLD] Cross-Document Coreference Results  [ITALIC] CEAFEe <C> [BOLD] Cross-Document Coreference Results  [ITALIC] CEAFEe <C> [BOLD] Cross-Document Coreference Results  [ITALIC] CEAFEe <C> [BOLD] Cross-Document Coreference Results CoNLL <R> <C> [EMPTY] <C> R <C> P <C> F1 <C> R <C> P <C> F1 <C> R <C> P <C> F1 <C> F1 <R> <C> LEMMA <C> 39.5 <C> 73.9 <C> 51.4 <C> 58.1 <C> 78.2 <C> 66.7 <C> 58.9 <C> 37.5 <C> 46.2 <C> 54.8 <R> <C> Common Classifier (WD) <C> 46 <C> 72.8 <C> 56.4 <C> 60.4 <C> 76.8 <C> 68.4 <C> 59.5 <C> 42.1 <C> 49.3 <C> 58 <R> <C> + 2nd Order Relations <C> 48.8 <C> 72.1 <C> 58.2 <C> 61.8 <C> 78.9 <C> 69.3 <C> 59.3 <C> 44.1 <C> 50.6 <C> 59.4 <R> <C> Common Classifier (CD) <C> 44.9 <C> 64.7 <C> 53 <C> 66.1 <C> 66.4 <C> 66.2 <C> 51.9 <C> 46.4 <C> 49 <C> 56.1 <R> <C> + 2nd Order Relations <C> 52.2 <C> 58.4 <C> 55.1 <C> [BOLD] 70.4 <C> 66.2 <C> 68.3 <C> 54.1 <C> 45.2 <C> 49.2 <C> 57.5 <R> <C> WD & CD Classifiers <C> 49 <C> 71.9 <C> 58.3 <C> 63.8 <C> 78.9 <C> 70.6 <C> 59.3 <C> 48.1 <C> 53.1 <C> 60.7 <R> <C> + 2nd Order Relations  [BOLD] (Full Model) <C> [BOLD] 56.2 <C> 66.6 <C> [BOLD] 61 <C> 67.5 <C> [BOLD] 80.4 <C> [BOLD] 73.4 <C> 59 <C> [BOLD] 54.2 <C> [BOLD] 56.5 <C> [BOLD] 63.6 <R> <C> HDDCRP Yang et al. ( 2015 ) <C> 40.6 <C> [BOLD] 78.5 <C> 53.5 <C> 67.1 <C> 80.3 <C> 73.1 <C> [BOLD] 68.9 <C> 38.6 <C> 49.5 <C> 58.7 <R> <C> HDP-LEX Bejan and Harabagiu ( 2010 ) <C> 43.7 <C> 65.6 <C> 52.5 <C> 63.5 <C> 75.5 <C> 69.0 <C> 60.2 <C> 34.8 <C> 44.1 <C> 55.2 <R> <C> Agglomerative Chen et al. ( 2009 ) <C> 40.2 <C> 73.2 <C> 51.9 <C> 59.2 <C> 78.3 <C> 67.4 <C> 65.6 <C> 30.2 <C> 41.1 <C> 53.6 <R> <C> [BOLD] Within-Document Coreference Results <C> [BOLD] Within-Document Coreference Results <C> [BOLD] Within-Document Coreference Results <C> [BOLD] Within-Document Coreference Results <C> [BOLD] Within-Document Coreference Results <C> [BOLD] Within-Document Coreference Results <C> [BOLD] Within-Document Coreference Results <C> [BOLD] Within-Document Coreference Results <C> [BOLD] Within-Document Coreference Results <C> [BOLD] Within-Document Coreference Results <C> [BOLD] Within-Document Coreference Results <R> <C> LEMMA <C> 56.8 <C> 80.9 <C> 66.7 <C> 35.9 <C> 76.2 <C> 48.8 <C> 67.4 <C> 62.9 <C> 65.1 <C> 60.2 <R> <C> Common Classifier (WD) <C> 59.7 <C> 80.5 <C> 68.6 <C> 44.6 <C> 75 <C> 55.9 <C> 68.2 <C> 67.7 <C> 67.9 <C> 64.2 <R> <C> + 2nd Order Relations <C> 62.7 <C> 79.4 <C> 70 <C> 50.3 <C> 75.2 <C> 60.3 <C> 68.6 <C> 70.5 <C> 69.5 <C> 66.6 <R> <C> Common Classifier (CD) <C> 65.2 <C> 67.1 <C> 66.1 <C> 47.6 <C> 53.9 <C> 50.5 <C> 69.2 <C> 62.1 <C> 65.5 <C> 60.7 <R> <C> + 2nd Order Relations <C> 66.9 <C> 69.1 <C> 68 <C> 56.7 <C> 55.1 <C> 55.9 <C> 70.4 <C> 63.6 <C> 66.8 <C> 62.8 <R> <C> WD & CD classifiers <C> 63.8 <C> 79.9 <C> 70.9 <C> 51.6 <C> [BOLD] 75.3 <C> 61.2 <C> 68.6 <C> 70.5 <C> 69.5 <C> 67.2 <R> <C> + 2nd Order Relations  [BOLD] (Full Model) <C> [BOLD] 69.2 <C> 76 <C> 72.4 <C> [BOLD] 58.5 <C> 67.3 <C> [BOLD] 62.6 <C> 67.9 <C> [BOLD] 76.1 <C> [BOLD] 71.8 <C> [BOLD] 68.9 <R> <C> HDDCRP Yang et al. ( 2015 ) <C> 67.3 <C> [BOLD] 85.6 <C> [BOLD] 75.4 <C> 41.7 <C> 74.3 <C> 53.4 <C> [BOLD] 79.8 <C> 65.1 <C> 71.7 <C> 66.8 <R> <C> HDP-LEX Bejan and Harabagiu ( 2010 ) <C> 67.6 <C> 74.7 <C> 71.0 <C> 39.1 <C> 50.0 <C> 43.9 <C> 71.4 <C> 66.2 <C> 68.7 <C> 61.2 <R> <C> Agglomerative Chen et al. ( 2009 ) <C> 67.6 <C> 80.7 <C> 73.5 <C> 39.2 <C> 61.9 <C> 48.0 <C> 76.0 <C> 65.6 <C> 70.4 <C> 63.9 <CAP> Table 3: Within- and cross-document event coreference result on ECB+ Corpus.
<R> <C> Model <C> Test NLL <C> Reconst. <C> KL <R> <C> [EMPTY] <C> (bpc) <C> (bpc) <C> (bpc) <R> <C> LSTM <C> 1.38 <C> - <C> - <R> <C> AWD-LSTM <C> 1.18 <C> - <C> - <R> <C> LSTM (sentence-wise) <C> 1.41 <C> - <C> - <R> <C> AF-only <C> 2.90 <C> 0.15 <C> 2.77 <R> <C> AF / AF <C> 1.42 <C> 0.10 <C> 1.37 <R> <C> AF / SCF <C> 1.46 <C> 0.10 <C> 1.43 <R> <C> IAF / SCF <C> 1.63 <C> 0.21 <C> 1.55 <CAP> Table 1: Character-level language modeling results on PTB. NLL for generative models is estimated with importance sampling using 50 samples, the reconstruction term and KL term refer to the two components of the ELBO. The LSTM from Cooijmans et al. (2017) and AWD-LSTM from Merity et al. (2018) use the standard character-setup which crosses sentence boundaries.
<R> <C> Model <C> Test NLL <C> Reconst. <C> KL <R> <C> [EMPTY] <C> (bpc) <C> (bpc) <C> (bpc) <R> <C> AF / AF <C> 1.42 <C> 0.10 <C> 1.37 <R> <C> - NLSq <C> 1.50 <C> 0.11 <C> 1.51 <R> <C> - AF hidden <C> 1.57 <C> 0.14 <C> 1.57 <R> <C> - AF hidden and NLSq <C> 1.56 <C> 0.29 <C> 1.56 <CAP> Table 2: Ablation experiments. AF / AF is the same result as in Table 1. -NLSq indicates the affine transformation is used instead of NLSq. -AF hidden indicates no dependencies across hidden (an independent vector affine transformation is used instead).
<R> <C> Feature Nickname <C> Description <C> Loss AUC <C> Example <R> <C> contains 1 <C> 1 occurs somewhere in the sequence <C> 0.50 <C> 2 4 11 1 4 <R> <C> prefix duplicate <C> Sequence begins with a duplicate <C> 0.52 <C> 2 2 11 12 4 <R> <C> first-last duplicate <C> First number equals last number <C> 0.55 <C> 2 4 11 12 2 <R> <C> adjacent duplicate <C> Adjacent duplicate is somewhere in the sequence <C> 1.43 <C> 11 12 2 2 4 <R> <C> contains first <C> First number is elsewhere in the sequence <C> 1.54 <C> 2 11 2 12 4 <CAP> Table 1: Features used to instantiate the strong feature in our experimental setup. Features are intended to differ in how hard they are for an LSTM to detect given sequential input. We use the the AUC of the validation loss curve as a heuristic measure of “hardness”, as described in Section 2.3.
<R> <C> [BOLD] Template <C> [BOLD] #Examples <C> [BOLD] Acc. <C> [BOLD] Top-2 Acc. <R> <C> 2 <C> 80 <C> 0.68 <C> 0.84 <R> <C> 1 <C> 18 <C> 0.66 <C> 0.94 <R> <C> 151 <C> 12 <C> 1.0 <C> 1.0 <R> <C> 3 <C> 12 <C> 0.25 <C> 0.42 <R> <C> 8 <C> 6 <C> 0.00 <C> 0.33 <R> <C> 5 <C> 1 <C> 0.00 <C> 0.00 <R> <C> 11 <C> 1 <C> 0.00 <C> 0.00 <CAP> Table 5: Template Level Model Accuracy on the QALD dataset.
<R> <C> [EMPTY] <C> MUC R <C> MUC P <C> MUC F1 <C> [ITALIC] B3 R <C> [ITALIC] B3 P <C> [ITALIC] B3 F1 <C> CEAF [ITALIC] e R <C> CEAF [ITALIC] e P <C> CEAF [ITALIC] e F1 <C> CoNLL Avg. F1 <C> LEA R <C> LEA P <C> LEA F1 <R> <C> baseline <C> 70.09 <C> 80.01 <C> 74.72 <C> 57.64 <C> 70.09 <C> 63.26 <C> 54.47 <C> 63.92 <C> 58.82 <C> 65.60 <C> 54.02 <C> 66.45 <C> 59.59 <R> <C> −gov <C> 70.10 <C> 79.96 <C> 74.71 <C> 57.51 <C> 70.31 <C> 63.27 <C> 54.41 <C> 64.08 <C> 58.85 <C> 65.61 <C> 53.93 <C> 66.76 <C> 59.66 <R> <C> +SP <C> 70.85 <C> 79.31 <C> 74.85 <C> 58.93 <C> 69.16 <C> 63.64 <C> 55.25 <C> 63.78 <C> 59.21 <C> 65.90 <C> 55.29 <C> 65.53 <C> 59.98 <R> <C> Reinforce <C> 70.98 <C> 78.81 <C> 74.69 <C> 58.97 <C> 69.05 <C> 63.61 <C> 55.66 <C> 63.28 <C> 59.23 <C> 65.84 <C> 55.31 <C> 65.32 <C> 59.90 <CAP> Table 1: Results on the CoNLL 2012 test set.
<R> <C> ER <C> TA <C> [ITALIC] Lb <C> [ITALIC] L <C> Size <C> ppl [ITALIC] l <R> <C> ✗ <C> ✓ <C> 1 <C> 4 <C> 13.2M <C> 28.10 <R> <C> ✗ <C> ✗ <C> 2 <C> 4 <C> 14.7M <C> 151.98 <CAP> Table 4: Comparative Tests on temporal attention layer and convolutional layer.
<R> <C> Token category <C> Number of vectors <C> Spearman’s  [ITALIC] ρ <R> <C> [CLS] <C> 17,443,296 <C> -0.34 <R> <C> [SEP] <C> 34,886,592 <C> -0.69 <R> <C> comma & period <C> 182,838,528 <C> -0.25 <R> <C> Others <C> 1,944,928,224 <C> -0.06 <CAP> Table 2: Spearman rank correlation coefficient between α and ∥f(x)∥ in each token category.
<R> <C> [BOLD] Relation <C> [BOLD] Head <C> [BOLD] Accuracy <C> [BOLD] Baseline <R> <C> All <C> 7-60 <C> 34.5 <C> 26.3 (1) <R> <C> prep <C> 7-40 <C> 66.7 <C> 61.8 (-1) <R> <C> pobj <C> 9-60 <C> [BOLD] 76.3 <C> 34.6 (-2) <R> <C> det <C> 8-11 <C> [BOLD] 94.3 <C> 51.7 (1) <R> <C> nn <C> 4-10 <C> 70.4 <C> 70.2 (1) <R> <C> nsubj <C> 8-20 <C> 58.5 <C> 45.5 (1) <R> <C> amod <C> 4-10 <C> 75.6 <C> 68.3 (1) <R> <C> dobj <C> 8-10 <C> [BOLD] 86.8 <C> 40.0 (-2) <R> <C> advmod <C> 7-60 <C> 48.8 <C> 40.2 (1) <R> <C> aux <C> 4-10 <C> 81.1 <C> 71.5 (1) <R> <C> poss <C> 7-60 <C> [BOLD] 80.5 <C> 47.7 (1) <R> <C> auxpass <C> 4-100 <C> [BOLD] 82.5 <C> 40.5 (1) <R> <C> ccomp <C> 8-10 <C> [BOLD] 48.8 <C> 12.4 (-2) <R> <C> mark <C> 8-20 <C> [BOLD] 50.7 <C> 14.5 (2) <R> <C> prt <C> 6-70 <C> [BOLD] 99.1 <C> 91.4 (-1) <CAP> Table 1: The best performing attentions heads of BERT on WSJ dependency parsing by dependency type. Numbers after baseline accuracies show the best offset found (e.g., (1) means the word to the right is predicted as the head). We show the 10 most common relations as well as 5 other ones attention heads do well on. Bold highlights particularly effective heads.
<R> <C> [BOLD] Model <C> [BOLD] All <C> [BOLD] Pronoun <C> [BOLD] Proper <C> [BOLD] Nominal <R> <C> Nearest <C> 27 <C> 29 <C> 29 <C> 19 <R> <C> Head match <C> 52 <C> 47 <C> 67 <C> 40 <R> <C> Rule-based <C> 69 <C> 70 <C> 77 <C> 60 <R> <C> Neural coref <C> 83* <C> – <C> – <C> – <R> <C> Head 5-4 <C> 65 <C> 64 <C> 73 <C> 58 <CAP> Table 2: Accuracies (%) for systems at selecting a correct antecedent given a coreferent mention in the CoNLL-2012 data. One of BERT’s attention heads performs fairly well at coreference.
<R> <C> [BOLD] Model <C> [BOLD] UAS <R> <C> Structural probe <C> 80 UUAS* <R> <C> Right-branching <C> 26 <R> <C> Distances + GloVe <C> 58 <R> <C> Random Init Attn + GloVe <C> 30 <R> <C> Attn <C> 61 <R> <C> Attn + GloVe <C> 77 <CAP> Table 3: Results of attention-based probing classifiers on dependency parsing. A simple model taking BERT attention maps and GloVe embeddings as input performs quite well. *Not directly comparable to our numbers; see text.
<R> <C> Model <C> BLEU 5-char <C> BLEU 7-char <R> <C> Basic model <C> 0.259 <C> 0.464 <R> <C> + All poem training <C> 0.267 <C> 0.467 <R> <C> + Input Reconstruction <C> 0.268 <C> 0.500 <R> <C> + Input Vector Attention <C> 0.290 <C> 0.501 <R> <C> + Hybrid training <C> [BOLD] 0.330 <C> [BOLD] 0.630 <CAP> Table 2: BLEU scores with various enhancement techniques.
<R> <C> Model <C> Compliance char-5 <C> Compliance char-7 <C> Fluency char-5 <C> Fluency char-7 <C> Consistence char-5 <C> Consistence char-7 <C> Aesthesis char-5 <C> Aesthesis char-7 <C> Overall char-5 <C> Overall char-7 <R> <C> SMT <C> 3.04 <C> 2.83 <C> 2.28 <C> 1.92 <C> 2.15 <C> 2.00 <C> 1.93 <C> 1.67 <C> 2.35 <C> 2.10 <R> <C> LSTMLM <C> 3.00 <C> 3.71 <C> 2.39 <C> 3.10 <C> 2.19 <C> 2.88 <C> 2.00 <C> 2.66 <C> 2.39 <C> 3.08 <R> <C> RNNPG <C> 2.90 <C> 2.60 <C> 2.05 <C> 1.70 <C> 1.97 <C> 1.70 <C> 1.70 <C> 1.45 <C> 2.15 <C> 1.86 <R> <C> Attention <C> [BOLD] 3.44 <C> [BOLD] 3.73 <C> 2.85 <C> 3.13 <C> 2.77 <C> 2.98 <C> 2.38 <C> 2.87 <C> 2.86 <C> 3.17 <R> <C> Human <C> 3.33 <C> 3.54 <C> [BOLD] 3.37 <C> [BOLD] 3.33 <C> [BOLD] 3.45 <C> [BOLD] 3.26 <C> [BOLD] 3.05 <C> [BOLD] 2.96 <C> [BOLD] 3.30 <C> [BOLD] 3.27 <CAP> Table 3: Averaged ratings for Chinese quatrain generation with different methods. ‘char-5’ and ‘char-7’ represent 5-char and 7-char characters quatrains respectively in the evaluation.
<R> <C> Method <C> De→En <C> Fr→En <R> <C> [ITALIC]  [BOLD] Seq2Seq [BOLD] NMT <C> 27.83 <C> 29.63 <R> <C> w/ n-gram <C> 28.41 <C> 30.04 <R> <C> w/ BERT <C> [BOLD] 29.31 <C> [BOLD] 30.52 <R> <C> w/ uniLM <C> 28.80 <C> 30.21 <R> <C> w/ biLM <C> 28.76 <C> 30.32 <R> <C> w/ T-TA <C> 28.83 <C> 30.20 <CAP> Table 2: BLEU scores after reranking with each language model on WMT13. Bolds are for the best performance on each sub-task. Underlines are for the best in our implementations.
<R> <C> [BOLD] Type of Uncertainty <C> [BOLD] Variance <R> <C> Aleatoric (with VE) <C> 3.277 ×10−3 <R> <C> Aleatoric (with UDL) <C> 5.743×10−3 <R> <C> Aleatoric (with AUL) <C> 2.561×10−3 <R> <C> Aleatoric (with PUL) <C> 1.841×10−3 <R> <C> Epistemic (50% training) <C> 4.371 ×10−4 <R> <C> Epistemic (75% training) <C> 3.369×10−4 <R> <C> Epistemic (100% training) <C> 1.972×10−4 <CAP> TABLE V: Aleatoric & Epistemic uncertainty measurement score.
<R> <C> Lookup/Visual <C> 100% <C> 50% <C> 12.5% <R> <C> zh_trad <C> 0.55/0.54 <C> 0.53/0.50 <C> 0.48/0.47 <R> <C> zh_simp <C> 0.55/0.54 <C> 0.53/0.52 <C> 0.48/0.46 <R> <C> ja <C> 0.42/0.39 <C> 0.47/0.45 <C> 0.44/0.41 <R> <C> ko <C> 0.47/0.42 <C> 0.44/0.39 <C> 0.37/0.36 <CAP> Table 3: The classification results of the Lookup / Visual models for different percentages of full training size.
<R> <C> [EMPTY] <C> zh_trad <C> zh_simp <C> ja <C> ko <R> <C> Lookup <C> 0.5503 <C> 0.5543 <C> 0.4914 <C> 0.4765 <R> <C> Visual <C> 0.5434 <C> 0.5403 <C> 0.4775 <C> 0.4207 <R> <C> early <C> 0.5520 <C> 0.5546 <C> 0.4896 <C> 0.4796 <R> <C> late <C> [BOLD] 0.5658 <C> [BOLD] 0.5685 <C> [BOLD] 0.5029 <C> [BOLD] 0.4869 <R> <C> fall <C> 0.5507 <C> 0.5547 <C> 0.4914 <C> 0.4766 <CAP> Table 5: Experiment results for three different fusion methods across 4 datasets. The late fusion model was better (p-value < 0.001) across four datasets.
<R> <C> Time step <C> [BOLD] Val-Seen 0 <C> [BOLD] Val-Seen 1 <C> [BOLD] Val-Seen 2 <C> [BOLD] Val-Seen 3 <C> [BOLD] Val-Seen 4 <C> [BOLD] Val-Seen 5 <C> [BOLD] Val-Seen 6 <C> [BOLD] Val-Seen 7 <C> [BOLD] Val-Seen Avg <C> [BOLD] Val-Unseen 0 <C> [BOLD] Val-Unseen 1 <C> [BOLD] Val-Unseen 2 <C> [BOLD] Val-Unseen 3 <C> [BOLD] Val-Unseen 4 <C> [BOLD] Val-Unseen 5 <C> [BOLD] Val-Unseen 6 <C> [BOLD] Val-Unseen 7 <C> [BOLD] Val-Unseen Avg <R> <C> Map Seen (m2) <C> 47.2 <C> 62.5 <C> 73.3 <C> 82.1 <C> 90.7 <C> 98.3 <C> 105 <C> 112 <C> 83.9 <C> 45.6 <C> 60.3 <C> 69.8 <C> 78.0 <C> 84.9 <C> 91.1 <C> 96.7 <C> 102 <C> 78.6 <R> <C> Goal Seen (%) <C> 8.82 <C> 17.2 <C> 25.9 <C> 33.7 <C> 41.2 <C> 48.8 <C> 54.5 <C> 60.2 <C> 36.3 <C> 16.0 <C> 25.2 <C> 34.6 <C> 43.2 <C> 50.5 <C> 57.0 <C> 62.8 <C> 67.6 <C> 44.6 <R> <C> [BOLD] Prediction Error (m) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Hand-coded baseline <C> 7.42 <C> 7.33 <C> 7.19 <C> 7.18 <C> 7.15 <C> 7.13 <C> 7.09 <C> 7.11 <C> 7.20 <C> 6.75 <C> 6.53 <C> 6.40 <C> 6.37 <C> 6.29 <C> 6.20 <C> 6.15 <C> 6.12 <C> 6.35 <R> <C> LingUNet baseline <C> 7.17 <C> 6.66 <C> 6.17 <C> 5.75 <C> 5.42 <C> 5.15 <C> 4.89 <C> 4.69 <C> 5.74 <C> 6.18 <C> 5.80 <C> 5.40 <C> 5.17 <C> 4.90 <C> 4.65 <C> 4.44 <C> 4.27 <C> 5.10 <R> <C> Filter,  [ITALIC]  [BOLD] s=( [ITALIC] x, [ITALIC] y) (ours) <C> 6.45 <C> 5.94 <C> 5.66 <C> 5.25 <C> 5.00 <C> 4.86 <C> 4.67 <C> 4.62 <C> 5.31 <C> 5.92 <C> 5.50 <C> 5.14 <C> 4.88 <C> 4.67 <C> 4.45 <C> 4.41 <C> 4.30 <C> 4.91 <R> <C> Filter,  [ITALIC]  [BOLD] s=( [ITALIC] x, [ITALIC] y, [ITALIC] θ) (ours) <C> [BOLD] 6.10 <C> [BOLD] 5.75 <C> [BOLD] 5.30 <C> [BOLD] 5.06 <C> [BOLD] 4.81 <C> [BOLD] 4.71 <C> [BOLD] 4.59 <C> [BOLD] 4.46 <C> [BOLD] 5.09 <C> [BOLD] 5.69 <C> [BOLD] 5.28 <C> [BOLD] 4.90 <C> [BOLD] 4.60 <C> [BOLD] 4.40 <C> [BOLD] 4.26 <C> [BOLD] 4.14 <C> [BOLD] 4.05 <C> [BOLD] 4.67 <R> <C> [BOLD] Success Rate (<3m error) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Hand-coded baseline <C> 17.3 <C> 17.8 <C> 18.5 <C> 18.2 <C> 18.0 <C> 19.1 <C> 18.8 <C> 18.6 <C> 18.3 <C> 18.9 <C> 20.1 <C> 21.1 <C> 21.3 <C> 21.8 <C> 22.2 <C> 22.6 <C> 22.9 <C> 21.4 <R> <C> LingUNet baseline <C> 10.7 <C> 16.7 <C> 21.2 <C> 25.8 <C> 29.7 <C> 33.6 <C> 36.9 <C> 39.1 <C> 26.7 <C> 16.9 <C> 22.3 <C> 27.7 <C> 31.6 <C> 35.2 <C> 38.4 <C> 41.1 <C> 44.5 <C> 32.2 <R> <C> Filter,  [ITALIC]  [BOLD] s=( [ITALIC] x, [ITALIC] y) (ours) <C> 24.6 <C> 29.3 <C> 31.9 <C> 35.9 <C> 39.7 <C> 41.0 <C> 42.1 <C> 41.2 <C> 35.7 <C> 29.1 <C> 32.5 <C> 36.1 <C> 39.2 <C> 41.9 <C> 44.5 <C> 45.7 <C> 46.2 <C> 39.4 <R> <C> Filter,  [ITALIC]  [BOLD] s=( [ITALIC] x, [ITALIC] y, [ITALIC] θ) (ours) <C> [BOLD] 30.9 <C> [BOLD] 34.3 <C> [BOLD] 38.4 <C> [BOLD] 41.6 <C> [BOLD] 43.7 <C> [BOLD] 44.9 <C> [BOLD] 44.3 <C> [BOLD] 46.2 <C> [BOLD] 40.6 <C> [BOLD] 34.2 <C> [BOLD] 38.7 <C> [BOLD] 42.7 <C> [BOLD] 46.1 <C> [BOLD] 48.2 <C> [BOLD] 48.4 <C> [BOLD] 49.9 <C> [BOLD] 51.2 <C> [BOLD] 44.9 <CAP> Table 1: Goal prediction results given a natural language navigation instruction and a fixed trajectory that either moves towards the goal, or randomly, with 50:50 probability. We evaluate predictions at each time step, although on average the goal is not seen until later time steps. Our filtering approach that explicitly models trajectories outperforms LingUNet Blukis et al. (2018); Misra et al. (2018) across all time steps (i.e., regardless of map sparsity). We confirm that add heading θ to the filter state provides a robust boost.
<R> <C> [BOLD] Datasets <C> [BOLD] #Plot types <C> [BOLD] #Plot images <C> [BOLD] #QA pairs <C> [BOLD] Vocabulary <C> [BOLD] Avg. question length <C> [BOLD] #Templates <C> [BOLD] #Unique answers <C> [BOLD] Open vocab. <R> <C> FigureQA <C> 4 <C> 180,000 <C> 2,388,698 <C> 100 colours from X11 colour set <C> 7.5 <C> 15 (no variations) <C> 2 <C> Not present <R> <C> DVQA <C> 1 <C> 300,000 <C> 3,487,194 <C> 1K nouns from Brown corpus <C> 12.30 <C> 26 (without paraphrasing) <C> 1576 <C> Not present <R> <C> PlotQA <C> 3 <C> 224,377 <C> 28,952,641 <C> Real-world axes variables and floating point numbers <C> 43.54 <C> 74 (with paraphrasing) <C> 5,701,618 <C> Present <CAP> Table 2: blackComparison between the existing datasets (FigureQA and DVQA) and our proposed dataset (PlotQA).
<R> <C> [EMPTY] <C> Perplexity <C> word types <R> <C> End-to-end model <C> 54.9 <C> 13.85 <R> <C> Pipeline model <C> [BOLD] 46.9 <C> [BOLD] 14.44 <CAP> Table 2: Perplexity and the average of word types for each generator model.
<R> <C> [BOLD] Row <C> [BOLD] Experiment <C> Model <C> [BOLD] Accuracy <C> [BOLD] Kappa <C> [BOLD] Macro F <R> <C> 1 <C> Argument Move <C> Individual <C> 0.669 <C> 0.343 <C> 0.502 <R> <C> 2 <C> Argument Move <C> Joint <C> 0.673 <C> 0.365 <C> 0.516 <R> <C> 3 <C> Specificity <C> Individual <C> 0.703 <C> 0.750 <C> 0.695 <R> <C> 4 <C> Specificity <C> Joint <C> 0.706 <C> 0.751 <C> 0.698 <CAP> Table 5: Neural classification results (both individual and joint models) for argument discourse unit prediction tasks.
<R> <C> History <C> Seq2seq <C> PGNet <C> DrQA <C> Augmt. <C> DrQA+ <R> <C> size <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> DrQA <C> PGNet <R> <C> 0 <C> 24.0 <C> 41.3 <C> 50.4 <C> 62.7 <C> 61.5 <R> <C> 1 <C> [BOLD] 27.5 <C> 43.9 <C> [BOLD] 54.7 <C> 66.8 <C> [BOLD] 66.2 <R> <C> 2 <C> 21.4 <C> 44.6 <C> 54.6 <C> [BOLD] 67.2 <C> 66.0 <R> <C> all <C> 21.0 <C> [BOLD] 45.4 <C> 52.3 <C> 64.5 <C> 64.3 <CAP> Table 9: Results on the development set with different history sizes. History size indicates the number of previous turns prepended to the current question. Each turn contains a question and its answer.
<R> <C> [EMPTY] <C> Augmt. <C> DrQA+ <C> Human <R> <C> [EMPTY] <C> DrQA <C> PGNet <C> [EMPTY] <R> <C> Yes <C> [BOLD] 76.2 <C> 72.5 <C> 97.7 <R> <C> No <C> [BOLD] 64.0 <C> 57.5 <C> 96.8 <R> <C> Fluency <C> [BOLD] 37.6 <C> 32.3 <C> 77.2 <R> <C> Counting <C> 8.8 <C> [BOLD] 24.8 <C> 88.3 <R> <C> Multiple choice <C> 0.0 <C> [BOLD] 46.4 <C> 94.3 <CAP> Table 10: Error analysis of questions with answers which do not overlap with the text passage.
<R> <C> [BOLD] Modalities <C> [BOLD] Features <C> [BOLD] F1(%) <R> <C> Text <C> Word2Vec (37.6K vocab) <C> 85.63 <R> <C> Text <C> GloVe (400K vocab) <C> 89.02 <R> <C> Text & Audio <C> GloVe & Acoustic (openSMILE/IS10) <C> 89.53 <R> <C> Text & Visual <C> GloVe & Video_cabin (CNN/Inception-ResNet) <C> 89.40 <R> <C> Text & Visual <C> GloVe & Video_road (CNN/Inception-ResNet) <C> 89.37 <R> <C> Text & Visual <C> GloVe & Video_cabin+road (CNN/Inception-ResNet) <C> 89.68 <R> <C> Audio <C> Speech2Vec (37.6K vocab) <C> 84.47 <R> <C> Text & Audio <C> Word2Vec+Speech2Vec <C> 88.08 <R> <C> Text & Audio <C> GloVe+Speech2Vec <C> 90.85 <R> <C> Text & Audio <C> GloVe+Word2Vec+Speech2Vec <C> 91.29 <R> <C> Text & Audio <C> GloVe+Word2Vec+Speech2Vec & Acoustic (IS10) <C> 91.68 <R> <C> Text & Audio & Visual <C> GloVe+Word2Vec+Speech2Vec & Video_cabin (CNN) <C> 91.50 <R> <C> Text & Audio & Visual <C> GloVe+Word2Vec+Speech2Vec & Video_cabin+road (CNN) <C> 91.55 <CAP> Table 3: F1-scores of Intent Recognition with Multimodal Features
<R> <C> Model <C> Person P <C> Person R <C> Person F <C> Organization P <C> Organization R <C> Organization F <C> Overall P <C> Overall R <C> Overall F <R> <C> Bi-LSTM <C> 67.11 <C> 78.46 <C> 72.34 <C> 76.56 <C> 72.59 <C> 74.52 <C> 73.04 <C> 74.50 <C> 73.76 <R> <C> Bi-LSTM CRF <C> 92.93 <C> 86.79 <C> 89.76 <C> 85.24 <C> [BOLD] 81.91 <C> 83.54 <C> 87.30 <C> 83.25 <C> 85.22 <R> <C> Bi-LSTM CRF +  [ITALIC] news word emb. <C> 95.05 <C> 90.57 <C> 92.75 <C> 85.13 <C> 81.21 <C> 83.12 <C> 87.84 <C> 83.76 <C> 85.75 <R> <C> Bi-LSTM CRF +  [ITALIC] Lenta word emb. <C> [BOLD] 95.60 <C> [BOLD] 94.57 <C> [BOLD] 95.08 <C> [BOLD] 87.40 <C> 81.62 <C> [BOLD] 84.41 <C> [BOLD] 89.57 <C> [BOLD] 84.89 <C> [BOLD] 87.17 <CAP> Table 2: Tagging results on Gareev’s dataset
<R> <C> Models <C> Gareev’s dataset P <C> Gareev’s dataset R <C> Gareev’s dataset F <C> Persons-1000 P <C> Persons-1000 R <C> Persons-1000 F <C> FactRuEval 2016 P <C> FactRuEval 2016 R <C> FactRuEval 2016 F <R> <C> Gareev et al.  <C> 84.10 <C> 67.98 <C> 75.05 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> Malykh et al.  <C> 59.65 <C> 65.70 <C> 62.49 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> Trofimov  <C> - <C> - <C> - <C> 97.26 <C> 93.92 <C> 95.57 <C> - <C> - <C> - <R> <C> Rubaylo et al.  <C> - <C> - <C> - <C> - <C> - <C> - <C> 77.70 <C> 78.50 <C> 78.13 <R> <C> Sysoev et al. <C> - <C> - <C> - <C> - <C> - <C> - <C> [BOLD] 88.19 <C> 64.75 <C> 74.67 <R> <C> Ivanitsky et al.  <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> [BOLD] 87.88 <R> <C> Mozharova et al.  <C> - <C> - <C> - <C> - <C> - <C> 97.21 <C> - <C> - <C> - <R> <C> NeuroNER <C> 88.19 <C> 82.73 <C> 85.37 <C> 96.38 <C> 96.83 <C> 96.60 <C> 80.49 <C> 79.23 <C> 79.86 <R> <C> NeuroNER + Highway char <C> 85.75 <C> [BOLD] 88.40 <C> 87.06 <C> 96.56 <C> 97.11 <C> 96.83 <C> 80.59 <C> 80.72 <C> 80.66 <R> <C> NeuroNER + Highway LSTM <C> 84.35 <C> 81.96 <C> 83.14 <C> 96.49 <C> 97.19 <C> 96.84 <C> 81.09 <C> 79.31 <C> 80.19 <R> <C> NeuroNER + Highway char + Highway LSTM <C> 83.33 <C> 85.05 <C> 84.18 <C> 96.74 <C> 96.83 <C> 96.78 <C> 79.13 <C> 78.76 <C> 78.95 <R> <C> Bi-LSTM + CRF +  [ITALIC] Lenta <C> [BOLD] 89.57 <C> 84.89 <C> [BOLD] 87.17 <C> [BOLD] 99.43 <C> [BOLD] 99.09 <C> [BOLD] 99.26 <C> 83.88 <C> [BOLD] 80.84 <C> 82.10 <CAP> Table 5: Performance of different models across datasets
<R> <C> [BOLD] Model/margin <C> [BOLD] Tok. accuracy <C> [BOLD] Unk. accuracy <C> [BOLD] Feat. templates <C> [BOLD] Speedup <R> <C> Baseline <C> 97.22 <C> 88.63 <C> 46 <C> 1x <R> <C> Stagewise <C> 96.54 <C> 83.63 <C> 9.50 <C> 2.74 <R> <C> Fixed <C> 89.88 <C> 56.25 <C> 1 <C> 16.16x <R> <C> Fixed <C> 94.66 <C> 60.59 <C> 3 <C> 9.54x <R> <C> Fixed <C> 96.16 <C> 87.09 <C> 5 <C> 7.02x <R> <C> Fixed <C> 96.88 <C> 88.81 <C> 10 <C> 3.82x <R> <C> Dynamic/15 <C> 96.09 <C> 83.12 <C> 1.92 <C> [BOLD] 10.36x <R> <C> Dynamic/35 <C> 97.02 <C> 88.26 <C> 4.33 <C> [BOLD] 5.22x <R> <C> Dynamic/45 <C> 97.16 <C> 88.84 <C> 5.87 <C> 3.97x <R> <C> Dynamic/50 <C> [BOLD] 97.21 <C> 88.95 <C> 6.89 <C> 3.41x <CAP> Table 1: Comparison of our models using different margins m, with speeds measured relative to the baseline. We train a model as accurate as the baseline while tagging 3.4x tokens/sec, and in another model maintain >97% accuracy while tagging 5.2x, and >96% accuracy with a speedup of 10.3x.
<R> <C> Pattern <C> Instances <C> Clues <R> <C> Argument from Consequence <C> 44 <C> Value Judgment (35/44), Causality (13/44) <R> <C> Argument from Analogy <C> 1 <C> - <R> <C> Presupposition <C> 1 <C> - <R> <C> Proposition <C> 2 <C> Value Judgment (2/2) <R> <C> Quantifier <C> 2 <C> - <CAP> Table 1: Distribution of clues required for capturing deep argumentative structure.
<R> <C> [BOLD] Model <C> [BOLD] Present MAE <C> [BOLD] Present Avg.# <C> [BOLD] Absent MAE <C> [BOLD] Absent Avg.# <R> <C> Oracle <C> 0.000 <C> 2.837 <C> 0.000 <C> 2.432 <R> <C> catSeq <C> 2.271 <C> 3.781 <C> 1.943 <C> 0.659 <R> <C> catSeqD <C> 2.225 <C> 3.694 <C> 1.961 <C> 0.629 <R> <C> catSeqCorr <C> 2.292 <C> 3.790 <C> 1.914 <C> 0.703 <R> <C> catSeqTG <C> 2.276 <C> 3.780 <C> 1.956 <C> 0.638 <R> <C> catSeq-RL <C> 2.118 <C> 3.733 <C> 1.494 <C> 1.574 <R> <C> catSeqD-RL <C> 2.087 <C> 3.666 <C> 1.541 <C> 1.455 <R> <C> catSeqCorr-RL <C> 2.107 <C> 3.696 <C> 1.557 <C> 1.409 <R> <C> catSeqTG-RL <C> 2.204 <C> 3.865 <C> 1.439 <C> 1.749 <R> <C> Transformer <C> 2.477 <C> 3.766 <C> 1.798 <C> 1.125 <R> <C> Trans+copy <C> 2.335 <C> 3.696 <C> 1.667 <C> 1.247 <R> <C> Trans+cd <C> 2.233 <C> 3.689 <C> 1.543 <C> 1.453 <R> <C> CDKGen <C> [BOLD] 2.004 <C> [BOLD] 3.655 <C> [BOLD] 1.411 <C> [BOLD] 1.797 <CAP> Table 6: Evaluations of predicting the correct number of keyphrases on the KP20k validation set. MAE denotes the mean absolute error and Avg. # the average number of generated keyphrases. For both MAE and Avg. #, the closer a model is to Oracle the better it performs.
<R> <C> [EMPTY] <C> [BOLD] A <C> [BOLD] A  [ITALIC] dC <C> [BOLD] A  [ITALIC] sC <C> [BOLD] A  [ITALIC] sL <C> [BOLD] A  [ITALIC] dL <R> <C> [EMPTY] <C> [ITALIC] dC <C> -3.5 <C> -6.1 <C> -9.7 <C> -3.4 <R> <C> [BOLD] B <C> [ITALIC] sC <C> 0.7 <C> 1.4 <C> 0.1 <C> 1.0 <R> <C> [EMPTY] <C> [ITALIC] sL <C> 1.1 <C> 5.6 <C> 6.1 <C> 3.5 <R> <C> [EMPTY] <C> [ITALIC] dL <C> 2.1 <C> -3.4 <C> 2.4 <C> -3.0 <CAP> Table 4: Surprise, SG(B|A), measures how much more likely category B outlets are to cite quotes reported by A outlets, compared to a hypothetical scenario where quoting is random.
<R> <C> [BOLD] Method <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <C> [BOLD] MCC <R> <C> quote popularity <C> 0.07 <C> 0.29 <C> 0.11 <C> 0.12 <R> <C> + outlet propensity <C> 0.08 <C> 0.34 <C> 0.13 <C> 0.14 <R> <C> matrix completion <C> [BOLD] 0.25 <C> 0.33 <C> [BOLD] 0.28 <C> [BOLD] 0.27 <CAP> Table 5: Classification performance of matrix completion, compared to the baselines in terms of precision, recall, F1 score and Matthew’s correlation coefficient. Bold scores are significantly better (based on 99% bootstrapped confidence intervals).
<R> <C> Model <C> Entity <C> Relation <C> Overall <C> Δ <R> <C> Full model <C> 86.320.2 <C> [BOLD] 60.290.2 <C> [BOLD] 73.30 <C> – <R> <C> (a) w/o Entity pre-training <C> 85.910.1 <C> 59.440.2 <C> 72.67 <C> -0.63 <R> <C> (b) w/o Entity embeddings <C> 86.070.1 <C> 59.520.2 <C> 72.80 <C> -0.51 <R> <C> (c) Single FFNN <C> 86.020.1 <C> 60.130.0 <C> 73.07 <C> -0.23 <R> <C> (d) w/o Head/Tail <C> 83.930.2 <C> 54.670.8 <C> 69.30 <C> -4.00 <R> <C> (e) w/o Bilinear <C> [BOLD] 86.350.1 <C> 59.600.0 <C> 72.98 <C> -0.33 <CAP> Table 2: Ablation experiment results on the CoNLL04 corpus. Scores are reported as a micro-averaged F1 score on the validation set, averaged across three runs of 5-fold cross-validation. (a) Without entity pre-training (section 2.1). (b) Without entity embeddings (eq. 3). (c) Using a single FFNN in place of FFNNhead and FFNNtail (eq. 4 and 5) (d) Without FFNNhead and FFNNtail (e) Without the bilinear operation (eq. 7). Bold: best scores. Subscripts denote standard deviation across three runs. Δ: difference to the full models score.
<R> <C> System <C> Dev <C> Test <R> <C> PBMT <C> 13.13 <C> 16.94 <R> <C> OnlyConceptRule <C> 13.15 <C> 14.93 <R> <C> OnlyInducedRule <C> 17.68 <C> 18.09 <R> <C> OnlyBigramLM <C> 17.19 <C> 17.75 <R> <C> All <C> 21.12 <C> 22.44 <R> <C> JAMR-gen <C> [BOLD] 23.00 <C> [BOLD] 23.00 <CAP> Table 1: Main results.
<R> <C> Mapping <C> CBOW→CBOW P@1 (%) <C> CBOW→CBOW P@5 (%) <C> CBOW→GloVe P@1 (%) <C> CBOW→GloVe P@5 (%) <C> GloVe→GloVe P@1 (%) <C> GloVe→GloVe P@5 (%) <R> <C> ES→EN <C> 35.6 <C> 55.0 <C> 48.9 <C> 66.7 <C> 45.6 <C> 67.8 <R> <C> IT→EN <C> 28.4 <C> 53.1 <C> 35.5 <C> 57.9 <C> 32.0 <C> 55.5 <R> <C> HR→EN <C> 29.9 <C> 52.9 <C> 32.2 <C> 57.5 <C> 32.3 <C> 48.7 <CAP> Table 2: Evaluation of translation matrices.
<R> <C> Model <C> R@1 <C> R@5 <C> R@10 <C> R@20 <R> <C> CL-VSM <C> 0.791 <C> 0.880 <C> 0.905 <C> 0.924 <R> <C> CWASA (S2Net) <C> 0.859 <C> 0.909 <C> 0.921 <C> 0.936 <R> <C> KBSim (CL-VSM) <C> 0.927 <C> 0.955 <C> 0.961 <C> 0.965 <R> <C> CL-STS-OptAlign <C> 0.895 <C> 0.930 <C> 0.940 <C> 0.948 <CAP> Table 7: Performance analysis on the task of cross-lingual plagiarism detection (in terms of R@k, where k = {1, 5, 10, 20}).
<R> <C> [EMPTY] <C> [BOLD] MUC Prec. <C> [BOLD] MUC Rec. <C> [BOLD] MUC F1 <C> [BOLD] B3 Prec. <C> [BOLD] B3 Rec. <C> [BOLD] B3 F1 <C> [BOLD] CEAF [ITALIC] ϕ4 Prec. <C> [BOLD] CEAF [ITALIC] ϕ4 Rec. <C> [BOLD] CEAF [ITALIC] ϕ4 F1 <C> Avg. F1 <R> <C> Triad <C> [BOLD] 90.38 <C> 80.19 <C> 84.98 <C> [BOLD] 83.86 <C> 64.53 <C> 72.94 <C> 72.01 <C> 73.36 <C> 72.68 <C> 76.87 <R> <C> Dyad <C> 88.60 <C> 77.82 <C> 82.86 <C> 82.62 <C> 61.86 <C> 70.75 <C> 69.19 <C> 71.17 <C> 70.17 <C> 74.59 <R> <C> Triad + post <C> 88.78 <C> [BOLD] 81.68 <C> [BOLD] 85.54 <C> 83.09 <C> [BOLD] 67.48 <C> [BOLD] 74.47 <C> [BOLD] 73.65 <C> [BOLD] 73.91 <C> [BOLD] 73.78 <C> [BOLD] 77.93 <R> <C> Dyad + post <C> 87.72 <C> 78.94 <C> 83.10 <C> 81.67 <C> 64.42 <C> 72.03 <C> 70.20 <C> 71.57 <C> 70.88 <C> 75.34 <CAP> Table 3: Results of the dyad model compared to the triad model. Results with postprocessing are represented with “+ post”.
<R> <C> [BOLD] Models <C> [BOLD] Rec(KL) <C> [BOLD] Rec-PPL <C> [BOLD] Time <R> <C> [ITALIC] Yelp corpus <C> [ITALIC] Yelp corpus <C> [ITALIC] Yelp corpus <C> [ITALIC] Yelp corpus <R> <C> LSTM-LM <C> 358.1 <C> 40.64 <C> - <R> <C> VAE <C> 357.9 <C> 40.56 <C> 5.4 <R> <C> SA-VAE <C> 357.5 <C> 40.39 <C> 56.3 <R> <C> Lag-VAE <C> 351.4 <C> 37.92 <C> 20.3 <R> <C> DB-VAE <C> [BOLD] 349.7 <C> [BOLD] 37.26 <C> 5.4 <R> <C> [ITALIC] Yahoo corpus <C> [ITALIC] Yahoo corpus <C> [ITALIC] Yahoo corpus <C> [ITALIC] Yahoo corpus <R> <C> LSTM-LM <C> 328.0 <C> 60.75 <C> - <R> <C> VAE <C> 328.6 <C> 61.21 <C> 6.9 <R> <C> SA-VAE <C> 329.1 <C> 61.59 <C> 69.2 <R> <C> Lag-VAE <C> 322.6 <C> 56.78 <C> 15.30 <R> <C> DB-VAE <C> [BOLD] 320.4 <C> [BOLD] 55.24 <C> 7.0 <CAP> Table 1: Performance comparisons on language modeling on the Yelp and Yahoo corpus.
<R> <C> Model <C> BLEU↑ R <C> BLEU↑ P <C> BLEU↑ F1 <C> BOW Embedding↑ A <C> BOW Embedding↑ E <C> BOW Embedding↑ G <C> intra-dist↑ dist-1 <C> intra-dist↑ dist-2 <C> inter-dist↑ dist-1 <C> inter-dist↑ dist-2 <R> <C> SeqGAN <C> 0.282 <C> 0.282 <C> 0.282 <C> 0.817 <C> 0.515 <C> 0.748 <C> 0.705 <C> 0.521 <C> 0.070 <C> 0.052 <R> <C> CVAE <C> 0.295 <C> 0.258 <C> 0.275 <C> 0.836 <C> 0.572 <C> 0.846 <C> 0.803 <C> 0.415 <C> 0.112 <C> 0.102 <R> <C> CVAE-BOW <C> 0.298 <C> 0.272 <C> 0.284 <C> 0.828 <C> 0.555 <C> 0.840 <C> 0.819 <C> 0.493 <C> 0.107 <C> 0.099 <R> <C> VHRED <C> 0.253 <C> 0.231 <C> 0.242 <C> 0.810 <C> 0.531 <C> 0.844 <C> 0.881 <C> 0.522 <C> 0.110 <C> 0.092 <R> <C> WAE-GMP <C> 0.420 <C> 0.258 <C> 0.319 <C> 0.925 <C> 0.661 <C> 0.894 <C> 0.713 <C> 0.671 <C> 0.333 <C> 0.555 <R> <C> DI-VAE <C> 0.310 <C> 0.175 <C> 0.224 <C> 0.802 <C> 0.583 <C> 0.862 <C> 0.891 <C> 0.779 <C> 0.489 <C> 0.767 <R> <C> DB-VAE <C> 0.386 <C> [BOLD] 0.274 <C> [BOLD] 0.320 <C> [BOLD] 0.925 <C> [BOLD] 0.668 <C> [BOLD] 0.906 <C> [BOLD] 0.905 <C> [BOLD] 0.836 <C> [BOLD] 0.553 <C> [BOLD] 0.808 <CAP> Table 5: Performance comparison on dialog response generation, Switchboard Dataset
<R> <C> [BOLD] Model <C> [BOLD] PPL <C> [BOLD] BLEU <R> <C> Variational Attention <C> 6.13 <C> 33.41 <R> <C> RNNsearch <C> 5.72 <C> 33.29 <R> <C> RNNsearch w/ top-k (ours) <C> [BOLD] 5.63 <C> [BOLD] 33.59 <CAP> Table 6: Evaluation on NMT, IWLST14
<R> <C> Model <C> BLEU↑ R <C> BLEU↑ P <C> BLEU↑ F1 <C> BOW Embedding↑ A <C> BOW Embedding↑ E <C> BOW Embedding↑ G <C> intra-dist↑ dist-1 <C> intra-dist↑ dist-2 <C> inter-dist↑ dist-1 <C> inter-dist↑ dist-2 <R> <C> SeqGAN <C> 0.270 <C> 0.270 <C> 0.270 <C> 0.907 <C> 0.495 <C> 0.774 <C> 0.747 <C> 0.806 <C> 0.075 <C> 0.081 <R> <C> CVAE <C> 0.265 <C> 0.222 <C> 0.242 <C> 0.923 <C> 0.543 <C> 0.811 <C> 0.938 <C> 0.973 <C> 0.177 <C> 0.222 <R> <C> CVAE-BOW <C> 0.256 <C> 0.224 <C> 0.239 <C> 0.923 <C> 0.540 <C> 0.812 <C> 0.949 <C> 0.976 <C> 0.165 <C> 0.206 <R> <C> VHRED <C> 0.271 <C> 0.260 <C> 0.265 <C> 0.892 <C> 0.507 <C> 0.786 <C> 0.633 <C> 0.711 <C> 0.071 <C> 0.089 <R> <C> WAE-GMP <C> 0.372 <C> 0.286 <C> 0.323 <C> 0.952 <C> 0.591 <C> 0.853 <C> 0.754 <C> 0.892 <C> 0.313 <C> 0.597 <R> <C> DI-VAE <C> 0.323 <C> 0.190 <C> 0.239 <C> 0.874 <C> 0.600 <C> 0.814 <C> 0.947 <C> 0.963 <C> 0.500 <C> 0.718 <R> <C> DB-VAE <C> [BOLD] 0.373 <C> 0.276 <C> 0.317 <C> 0.944 <C> [BOLD] 0.615 <C> 0.839 <C> [BOLD] 0.954 <C> [BOLD] 0.997 <C> 0.467 <C> [BOLD] 0.787 <CAP> Table 9: Performance comparison on dialog response generation, DailyDialog Dataset
<R> <C> [EMPTY] <C> Yes-No <C> Entity <C> Overall <R> <C> [BOLD] Adapted ROUGE-L <C> [BOLD] 0.540 <C> [BOLD] 0.620 <C> [BOLD] 0.570 <R> <C> [BOLD] ROUGE-L <C> 0.493 <C> 0.491 <C> 0.504 <R> <C> [BOLD] Adapted BLEU-4 <C> 0.478 <C> 0.469 <C> 0.481 <R> <C> [BOLD] BLEU-4 <C> 0.459 <C> 0.397 <C> 0.450 <CAP> Table 2: PCCs between various automatic metrics and human judgment for different question types on single question level.
<R> <C> [EMPTY] <C> Yes-No <C> Entity <C> Overall <R> <C> [BOLD] Adapted ROUGE-L <C> [BOLD] 0.702 <C> [BOLD] 0.884 <C> [BOLD] 0.792 <R> <C> [BOLD] ROUGE-L <C> 0.664 <C> 0.839 <C> 0.760 <R> <C> [BOLD] Adapted BLEU-4 <C> 0.536 <C> 0.686 <C> 0.646 <R> <C> [BOLD] BLEU-4 <C> 0.571 <C> 0.668 <C> 0.681 <CAP> Table 3: PCCs between various automatic metrics and human judgment for different question types on overall score level.
<R> <C> Model(train) <C> CER(valid) <C> CER(eval) <R> <C> WSJ-train_si284 (80hrs) <C> dev93 <C> eval92 <R> <C> CTC <C> 11.48 <C> 8.97 <R> <C> Attention(content-based) <C> 13.68 <C> 11.08 <R> <C> Attention(location-based) <C> 11.98 <C> 8.17 <R> <C> MTL( [ITALIC] λ=0.2) <C> [BOLD] 11.27 <C> [BOLD] 7.36 <R> <C> MTL( [ITALIC] λ=0.5) <C> 12.00 <C> 8.31 <R> <C> MTL( [ITALIC] λ=0.8) <C> 11.71 <C> 8.45 <R> <C> WSJ-train_si84 (15hrs) <C> dev93 <C> eval92 <R> <C> CTC <C> 27.41 <C> 20.34 <R> <C> Attention(content-based) <C> 28.02 <C> 20.06 <R> <C> Attention(location-based) <C> 24.98 <C> 17.01 <R> <C> MTL( [ITALIC] λ=0.2) <C> [BOLD] 23.03 <C> [BOLD] 14.53 <R> <C> MTL( [ITALIC] λ=0.5) <C> 26.28 <C> 16.24 <R> <C> MTL( [ITALIC] λ=0.8) <C> 32.21 <C> 21.30 <R> <C> CHiME-4-tr05_multi (18hrs) <C> dt05_real <C> et05_real <R> <C> CTC <C> 37.56 <C> 48.79 <R> <C> Attention(content-based) <C> 43.45 <C> 54.25 <R> <C> Attention(location-based) <C> 35.01 <C> 47.58 <R> <C> MTL( [ITALIC] λ=0.2) <C> [BOLD] 32.08 <C> [BOLD] 44.99 <R> <C> MTL( [ITALIC] λ=0.5) <C> 34.56 <C> 46.49 <R> <C> MTL( [ITALIC] λ=0.8) <C> 35.41 <C> 48.34 <CAP> Table 1: Character Error Rate (CER) on clean corpora WSJ1 (80hours) and WSJ0 (15hours), and a noisy corpus CHiME-4 (18hours). None of our experiments used any language model or lexicon information. (Word Error Rate (WER) of our model MTL(λ=0.2) was 18.2% and WER of [7] was 18.6% on WSJ1. Note that this is not an exact comparison because the hyper parameters were not completely same as [7].)
<R> <C> [BOLD] Method <C> [BOLD] Anneal <C> [BOLD] de-en <C> [BOLD] en-de <C> [BOLD] vi-en <R> <C> Baseline <C> No <C> 27.41±0.26 <C> 22.64±0.13 <C> 23.59±0.13 <R> <C> +SS <C> Yes <C> 27.47±0.28 <C> 22.56±0.17 <C> 23.97±0.39 <R> <C> +DSS <C> Yes <C> 27.30±0.24 <C> 22.47±0.20 <C> 23.68±0.35 <R> <C> +SS <C> No <C> 22.91±0.21 <C> 17.78±0.20 <C> 19.57±0.19 <R> <C> +SAML <C> No <C> [BOLD] 27.94±0.12 <C> [BOLD] 23.30±0.19 <C> [BOLD] 24.60±0.35 <CAP> Table 2: BLEU scores of our approach (SAML) and three baselines including the maximum likelihood (ML) baseline, scheduled sampling (SS), and differentiable scheduled sampling (DSS). The Anneal column indicates whether the sampling rate is annealed. For each task, we report the mean and standard deviation over 5 runs with different random seeds. SAML achieves the best BLEU scores and is simpler to train than SS and DSS, as it requires no annealing schedule.
<R> <C> [BOLD] Simulation <C> [BOLD] Accuracy (%) <C> [BOLD] KLD <R> <C> Utterance-level <C> 77.98 <C> 0.2338 <R> <C> Act-level <C> [BOLD] 84.96 <C> [BOLD] 0.188 <CAP> Table 2: Evaluation of The User Simulation on both Utterance and Act levels
<R> <C> [BOLD] Toys & Games <C> [BOLD] RG-1 <C> [BOLD] RG-2 <C> [BOLD] RG-L <R> <C> S2S  <C> 14.05 <C> 2.47 <C> 15.75 <R> <C> S2S-att  <C> 16.23 <C> 4.27 <C> 16.01 <R> <C> S2S-att + BiLSTM <C> 16.32 <C> 4.43 <C> 16.27 <R> <C> [BOLD] HSSC (this work) <C> [BOLD] 18.44 <C> [BOLD] 5.00 <C> [BOLD] 17.69 <R> <C> [BOLD] Sports & Outdoors <C> [BOLD] RG-1 <C> [BOLD] RG-2 <C> [BOLD] RG-L <R> <C> S2S  <C> 13.38 <C> 2.59 <C> 13.18 <R> <C> S2S-att  <C> 15.70 <C> 3.61 <C> 15.53 <R> <C> S2S-att + BiLSTM <C> 15.75 <C> 3.64 <C> 15.68 <R> <C> [BOLD] HSSC (this work) <C> [BOLD] 17.85 <C> [BOLD] 4.77 <C> [BOLD] 17.59 <R> <C> [BOLD] Movie & TV <C> [BOLD] RG-1 <C> [BOLD] RG-2 <C> [BOLD] RG-L <R> <C> S2S  <C> 10.98 <C> 2.34 <C> 10.77 <R> <C> S2S-att  <C> 12.17 <C> 3.08 <C> 11.77 <R> <C> S2S-att + BiLSTM <C> 12.33 <C> 3.22 <C> 11.92 <R> <C> [BOLD] HSSC (this work) <C> [BOLD] 14.52 <C> [BOLD] 4.84 <C> [BOLD] 13.42 <CAP> Table 1: Comparison between our model and the sequence-to-sequence baseline for abstractive summarization on the Amazon SNAP test sets. The test sets include three domains: Toys & Gamse, Sports & Outdoors, and Movie & TV. RG-1, RG-2, and RG-L denote ROUGE-1, ROUGE-2, and ROUGE-L, respectively.
<R> <C> [BOLD] Toys & Games <C> [BOLD] 5-class <C> [BOLD] 2-class <R> <C> CNN <C> 70.5 <C> 90.2 <R> <C> BiLSTM <C> 70.7 <C> 90.9 <R> <C> BiLSTM + S2S-att <C> 70.9 <C> 90.9 <R> <C> [BOLD] HSSC (this work) <C> [BOLD] 71.9 <C> [BOLD] 91.8 <R> <C> [BOLD] Sports & Outdoors <C> [BOLD] 5-class <C> [BOLD] 2-class <R> <C> CNN <C> 72.0 <C> 91.5 <R> <C> BiLSTM <C> 71.9 <C> 91.6 <R> <C> BiLSTM + S2S-att <C> 72.1 <C> 91.9 <R> <C> [BOLD] HSSC (this work) <C> [BOLD] 73.2 <C> [BOLD] 92.1 <R> <C> [BOLD] Movie & TV <C> [BOLD] 5-class <C> [BOLD] 2-class <R> <C> CNN <C> 66.9 <C> 86.0 <R> <C> BiLSTM <C> 67.8 <C> 86.2 <R> <C> BiLSTM + S2S-att <C> 68.0 <C> 86.6 <R> <C> [BOLD] HSSC (this work) <C> [BOLD] 68.9 <C> [BOLD] 88.4 <CAP> Table 2: Comparison between our model and the sequence-to-sequence baselines for sentiment classification on the Amazon SNAP test sets. The test sets include three domains: Toys & Games, Sports & Outdoors, and Movie & TV. 5-class and 2-class denote the accuracy of five-class sentiment and two-class sentiment, respectively.
<R> <C> [BOLD] Toys & Games <C> [BOLD] 5-class <C> [BOLD] RG-L <R> <C> w/o Multi-View <C> 70.9 <C> 16.47 <R> <C> w/o Highway <C> 70.1 <C> 16.06 <R> <C> HSSC (Full Model) <C> 71.9 <C> 17.69 <R> <C> [BOLD] Sports & Outdoors <C> [BOLD] 5-class <C> [BOLD] RG-L <R> <C> w/o Multi-View <C> 72.0 <C> 16.36 <R> <C> w/o Highway <C> 71.5 <C> 15.73 <R> <C> HSSC (Full Model) <C> 73.2 <C> 17.59 <R> <C> [BOLD] Movie & TV <C> [BOLD] 5-class <C> [BOLD] RG-L <R> <C> w/o Multi-View <C> 68.1 <C> 12.34 <R> <C> w/o Highway <C> 67.7 <C> 12.01 <R> <C> HSSC (Full Model) <C> 68.9 <C> 13.42 <CAP> Table 3: Ablation study. 5-class denotes the accuracy of five-grained sentiment, and RG-L denotes ROUGE-L for summarization.
<R> <C> [EMPTY] <C> 10 words Split error <C> 10 words JS divergence <C> 20 words Split error <C> 20 words JS divergence <C> 30 words Split error <C> 30 words JS divergence <R> <C> Tf-idf <C> 36.15% <C> 0.11946 <C> 20.09% <C> 0.35991 <C> 12.55% <C> 0.54468 <R> <C> Mean <C> 30.67% <C> 0.16186 <C> 21.05% <C> 0.34109 <C> 16.33% <C> 0.45534 <R> <C> Max <C> 28.27% <C> 0.20831 <C> 19.06% <C> 0.40030 <C> 15.18% <C> 0.49882 <R> <C> Min <C> 28.89% <C> 0.19694 <C> 19.54% <C> 0.38696 <C> 15.69% <C> 0.48592 <R> <C> Min/max <C> 26.89% <C> 0.22946 <C> 16.86% <C> 0.45004 <C> 12.76% <C> 0.56253 <R> <C> Mean, top 30% idf <C> 23.69% <C> 0.30044 <C> 15.86% <C> [BOLD] 0.48260 <C> [BOLD] 12.42% <C> [BOLD] 0.56456 <R> <C> Max, top 30% idf <C> 26.56% <C> 0.24028 <C> 20.63% <C> 0.36809 <C> 16.66% <C> 0.45522 <R> <C> Min/max, top 30% idf <C> 25.43% <C> 0.25761 <C> 19.13% <C> 0.39775 <C> 14.91% <C> 0.49927 <R> <C> Mean, idf weighed <C> [BOLD] 23.35% <C> [BOLD] 0.30400 <C> [BOLD] 15.77% <C> 0.48028 <C> 12.43% <C> 0.56028 <CAP> Table I: Comparison of different word vector aggregation techniques with cosine similarity.
<R> <C> [EMPTY] <C> Split error <C> JS divergence <R> <C> Mean, cosine <C> 21.05% <C> 0.34109 <R> <C> Mean, Euclidean <C> [BOLD] 19.55% <C> [BOLD] 0.37788 <R> <C> Mean,  [ITALIC] L3 <C> 19.62% <C> 0.37511 <R> <C> Mean,  [ITALIC] L4 <C> 19.77% <C> 0.37061 <R> <C> Mean, Bray-Curtis <C> 21.22% <C> 0.33775 <CAP> Table II: Comparison of different distance metrics for texts of 20 words long.
<R> <C> [BOLD] Form <C> [BOLD] Features <C> [BOLD] Class <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F <R> <C> [BOLD] Gen <C> N-Grams <C> [ITALIC] S <C> 0.73 <C> 0.70 <C> 0.72 <R> <C> [EMPTY] <C> [EMPTY] <C> [ITALIC] NS <C> 0.71 <C> 0.75 <C> [BOLD] 0.73 <R> <C> [EMPTY] <C> W2V <C> [ITALIC] S <C> 0.71 <C> 0.77 <C> [BOLD] 0.74 <R> <C> [EMPTY] <C> [EMPTY] <C> [ITALIC] NS <C> 0.75 <C> 0.69 <C> 0.72 <R> <C> [BOLD] RQ <C> N-Grams <C> [ITALIC] S <C> 0.71 <C> 0.68 <C> [BOLD] 0.70 <R> <C> [EMPTY] <C> [EMPTY] <C> [ITALIC] NS <C> 0.70 <C> 0.73 <C> [BOLD] 0.71 <R> <C> [EMPTY] <C> W2V <C> [ITALIC] S <C> 0.67 <C> 0.72 <C> 0.69 <R> <C> [EMPTY] <C> [EMPTY] <C> [ITALIC] NS <C> 0.70 <C> 0.64 <C> 0.67 <R> <C> [BOLD] Hyp <C> N-Grams <C> [ITALIC] S <C> 0.68 <C> 0.63 <C> [BOLD] 0.65 <R> <C> [EMPTY] <C> [EMPTY] <C> [ITALIC] NS <C> 0.66 <C> 0.71 <C> [BOLD] 0.68 <R> <C> [EMPTY] <C> W2V <C> [ITALIC] S <C> 0.57 <C> 0.56 <C> 0.57 <R> <C> [EMPTY] <C> [EMPTY] <C> [ITALIC] NS <C> 0.57 <C> 0.59 <C> 0.58 <CAP> Table 7: Supervised Learning Results for Generic (Gen: 3,260 posts per class), Rhetorical Questions (RQ: 851 posts per class) and Hyperbole (Hyp: 582 posts per class)
<R> <C> Recipe <C> LM Type <C> Mono <C> Tri <C> DNN <R> <C> Standard Kaldi s5 <C> Bigram LM <C> 36.4 <C> 23.2 <C> 20.1 <R> <C> Standard Kaldi s5 <C> Phone-loop <C> 39.4 <C> 26.3 <C> 22.4 <R> <C> Proposed Evaluation <C> Bigram LM <C> 42.7 <C> 28.6 <C> 24.6 <R> <C> Proposed Evaluation <C> Phone-loop <C> 46.7 <C> 32.5 <C> [BOLD] 27.5 <CAP> Table 1: Phone Error Rate (PER%) performance obtained applying different Kaldi recipes to the phonetically-rich sentence (dev-set) acquired in the FBK recording studio.
<R> <C> Mic. Sel. <C> Simulated Data Mono <C> Simulated Data Tri <C> Simulated Data DNN <C> Real Data Mono <C> Real Data Tri <C> Real Data DNN <R> <C> Random <C> 67.6 <C> 57.7 <C> 52.4 <C> 70.3 <C> 61.0 <C> 55.4 <R> <C> Oracle <C> 56.6 <C> 47.1 <C> [BOLD] 42.0 <C> 60.3 <C> 49.6 <C> [BOLD] 44.0 <CAP> Table 4: PER(%) performance obtained with a random and an oracle microphone selection.
<R> <C> Corpora <C> noun <C> verb <C> ad <C> pnoun <R> <C> Wikipedia <C> 69.0 <C> 57.9 <C> 66.4 <C> 83.0 <R> <C> Mixed* <C> 64.0 <C> 55.5 <C> 59.4 <C> 37.6 <CAP> Table 1: Comparison in performance by POS category with two different embedding sets. * The out-of-vocabulary rate for items in the dictionaries is negligible: 2, 0, and 1 for noun, verb , and ad , respectively.
<R> <C> [EMPTY] <C> es →en <C> es en→ <C> de →en <C> de en→ <C> da →en <C> da en→ <C> bg →en <C> bg en→ <C> hi →en <C> hi en→ <C> ar →en <C> ar en→ <R> <C> Source words <C> 1500 <C> 1500 <C> 1500 <C> 1500 <C> 1500 <C> 1500 <C> 1500 <C> 1500 <C> 1500 <C> 1500 <C> 1500 <C> 1500 <R> <C> Source words <C> 1145 <C> 1171 <C> 1111 <C> 1188 <C> 974 <C> 1158 <C> 1124 <C> 1125 <C> 963 <C> 1104 <C> 1212 <C> 1080 <R> <C> MUSE-S <C> 83.47 <C> 81.66 <C> 72.67 <C> 73.93 <C> 67.07 <C> 56.80 <C> 56.93 <C> 43.93 <C> 44.07 <C> 33.60 <C> 49.93 <C> 34.13 <R> <C> MUSE-S <C> 79.56 <C> 73.36 <C> 66.79 <C> 64.47 <C> 68.79 <C> 55.44 <C> 60.63 <C> 45.33 <C> 46.73 <C> 37.68 <C> 50.83 <C> 34.63 <R> <C> MUSE-U <C> 83.67 <C> 82.07 <C> 72.60 <C> 74.20 <C> 64.00 <C> 55.40 <C> 56.80 <C> 39.93 <C> 0.00 <C> 28.27 <C> 0.00 <C> 34.60 <R> <C> MUSE-U <C> 80.09 <C> 73.78 <C> 67.60 <C> 64.31 <C> 69.82 <C> 54.40 <C> 62.39 <C> 41.51 <C> 0.00 <C> 34.87 <C> 0.00 <C> 36.39 <R> <C> VM-S <C> 85.47 <C> 81.40 <C> 74.93 <C> 74.67 <C> 70.47 <C> 64.60 <C> 63.20 <C> 48.80 <C> 48.96 <C> 41.07 <C> 53.95 <C> 43.53 <R> <C> VM-S <C> 81.48 <C> 72.50 <C> 68.68 <C> 65.49 <C> 71.46 <C> 62.52 <C> 66.61 <C> 49.78 <C> 50.57 <C> 45.74 <C> 54.62 <C> 44.07 <R> <C> VM-U <C> 84.53 <C> 82.33 <C> 74.00 <C> 75.20 <C> 68.07 <C> 64.87 <C> 58.40 <C> 44.73 <C> 38.71 <C> 36.93 <C> 48.73 <C> 35.73 <R> <C> VM-U <C> 80.70 <C> 73.53 <C> 67.51 <C> 65.66 <C> 70.64 <C> 63.04 <C> 64.76 <C> 48.44 <C> 47.77 <C> 44.02 <C> 51.90 <C> 39.54 <R> <C> RCSLS <C> 86.40 <C> 84.46 <C> 76.00 <C> 79.00 <C> 70.07 <C> 61.93 <C> 63.60 <C> 51.73 <C> 47.15 <C> 38.27 <C> 55.56 <C> 42.20 <R> <C> RCSLS <C> 82.79 <C> 76.17 <C> 71.38 <C> 71.97 <C> 75.36 <C> 62.69 <C> 69.24 <C> 56.44 <C> 50.78 <C> 44.57 <C> 57.92 <C> 45.83 <CAP> Table 4: Cyan rows correspond to the original test data and white rows to the clean test data. The top rows report the sizes of the dictionaries, measured in terms of source words. For unstable models, e.g. MUSE-U, we train ten models and report results from one random successful model. For a fair comparison of MUSE-U and MUSE-S, we run Procrustes for 5 iterations in both cases, and use the same model selection criterion, mean cosine similarity, in both cases. All systems are evaluated using CSLS for retrieval. * Instead of full annotation for Spanish, we only mark proper nouns and remove them from the test dictionaries to and from English.
<R> <C> [BOLD] Algorithm <C> [BOLD] MNLI-m <C> [BOLD] MNLI-mm <C> [BOLD] SST-2 <C> [BOLD] QQP <C> [BOLD] QNLI <C> [BOLD] #Params <C> [BOLD] Model Size <R> <C> [BOLD] BERT-Base <C> [BOLD] 84.6 <C> [BOLD] 83.4 <C> [BOLD] 93.5 <C> [BOLD] 71.2/- <C> [BOLD] 90.5 <C> 110M <C> ×1.0 <R> <C> [BOLD] LadaBERT-1 <C> [BOLD] 83.5 <C> [BOLD] 82.5 <C> [BOLD] 92.8 <C> [BOLD] 70.7/88.9 <C> [BOLD] 89.6 <C> 44M <C> ×2.5 <R> <C> BERT-FT <C> 74.8 <C> 74.3 <C> 86.4 <C> 65.8/86.9 <C> 84.3 <C> 44M <C> ×2.5 <R> <C> BERT-KD <C> 75.4 <C> 74.8 <C> 86.9 <C> 67.3/87.6 <C> 84.0 <C> 44M <C> ×2.5 <R> <C> BERT-PKD <C> 76.7 <C> 76.3 <C> 87.5 <C> 68.1/87.8 <C> 84.7 <C> 44M <C> ×2.5 <R> <C> Weight pruning <C> 82.8 <C> 81.6 <C> 92.3 <C> 70.1/88.5 <C> 88.9 <C> 44M <C> ×2.5 <R> <C> matrix factorization <C> 77.7 <C> 77.4 <C> 87.6 <C> 65.7/87.2 <C> 84.3 <C> 44M <C> ×2.5 <R> <C> Hybrid pruning <C> 81.2 <C> 80.0 <C> 90.0 <C> 68.0/87.5 <C> 83.3 <C> 44M <C> ×2.5 <R> <C> [BOLD] LadaBERT-2 <C> [BOLD] 83.1 <C> [BOLD] 82.2 <C> [BOLD] 91.8 <C> [BOLD] 69.9/87.9 <C> [BOLD] 88.2 <C> 22M <C> ×5.0 <R> <C> Weight pruning <C> 75.9 <C> 75.6 <C> 84.8 <C> 60.3/83.5 <C> 81.7 <C> 22M <C> ×5.0 <R> <C> matrix factorization <C> 71.8 <C> 71.8 <C> 82.8 <C> 60.3/83.5 <C> 75.4 <C> 22M <C> ×5.0 <R> <C> Hybrid pruning <C> 76.1 <C> 75.3 <C> 85.4 <C> 64.9/85.8 <C> 80.6 <C> 22M <C> ×5.0 <R> <C> [BOLD] LadaBERT-3 <C> [BOLD] 82.1 <C> [BOLD] 81.8 <C> [BOLD] 89.9 <C> [BOLD] 69.4/87.8 <C> 84.5 <C> 15M <C> ×7.5 <R> <C> TinyBERT <C> 80.9 <C> 79.5 <C> 89.5 <C> 65.4/87.5 <C> 77.9 <C> 15M <C> ×7.5 <R> <C> BERT-Small <C> 75.4 <C> 74.9 <C> 87.6 <C> 66.5/- <C> [BOLD] 84.8 <C> 15M <C> ×7.5 <R> <C> Weight pruning <C> 69.1 <C> 68.8 <C> 81.8 <C> 59.7/82.9 <C> 76.4 <C> 15M <C> ×7.5 <R> <C> matrix factorization <C> 60.2 <C> 60.0 <C> 81.3 <C> 58.5/82.0 <C> 62.2 <C> 15M <C> ×7.5 <R> <C> Hybrid pruning <C> 71.9 <C> 71.0 <C> 83.5 <C> 62.3/84.7 <C> 73.8 <C> 15M <C> ×7.5 <R> <C> [BOLD] LadaBERT-4 <C> [BOLD] 75.8 <C> [BOLD] 76.1 <C> 84.0 <C> 67.4/86.6 <C> [BOLD] 75.1 <C> 11M <C> ×10.0 <R> <C> Distilled-BiLSTM <C> 73.0 <C> 72.6 <C> [BOLD] 90.7 <C> [BOLD] 68.2/88.1 <C> - <C> 10M <C> ×10.8 <R> <C> Weight pruning <C> 64.9 <C> 65.1 <C> 80.4 <C> 56.9/80.5 <C> 62.7 <C> 11M <C> ×10.0 <R> <C> matrix factorization <C> 59.9 <C> 59.6 <C> 79.2 <C> 57.8/81.9 <C> 62.2 <C> 11M <C> ×10.0 <R> <C> Hybrid pruning <C> 68.4 <C> 67.9 <C> 81.5 <C> 58.6/83.5 <C> 63.2 <C> 11M <C> ×10.0 <CAP> Table 3: Performance comparison on various model sizes
<R> <C> Models <C> WMT14 En→De <C> WMT14 De→En <C> WMT16 En→Ro <C> WMT16 Ro→En <C> IWSLT16 En→De <C> Speedup <R> <C> Transformer Vaswani et al. ( 2017 ) <C> 27.41 <C> 31.29 <C> / <C> / <C> 30.90 <C> 1.00× <R> <C> AT Demonstrator <C> 27.80 <C> 31.25 <C> 33.70 <C> 32.59 <C> 30.85 <C> 1.05× <R> <C> NAT-FT(Gu et al.,  2017 ) <C> 17.69 <C> 21.47 <C> 27.29 <C> 29.06 <C> 26.52 <C> 15.60× <R> <C> NAT-FT(+NPD s=10) <C> 18.66 <C> 22.41 <C> 29.02 <C> 30.76 <C> 27.44 <C> 7.68× <R> <C> NAT-FT(+NPD s=100) <C> 19.17 <C> 23.20 <C> 29.79 <C> 31.44 <C> 28.16 <C> 2.36× <R> <C> NAT-IR( [ITALIC] idec=1) <C> 13.91 <C> 16.77 <C> 24.45 <C> 25.73 <C> 22.20 <C> 8.90× <R> <C> NAT-IR( [ITALIC] idec=10) <C> 21.61 <C> 25.48 <C> 29.32 <C> 30.19 <C> 27.11 <C> 1.50× <R> <C> LT <C> 19.80 <C> / <C> / <C> / <C> / <C> 5.78× <R> <C> LT(rescoring 10) <C> 21.0 <C> / <C> / <C> / <C> / <C> / <R> <C> LT(rescoring 100) <C> 22.5 <C> / <C> / <C> / <C> / <C> / <R> <C> [BOLD] NAT without imitation <C> 19.69 <C> 22.71 <C> / <C> / <C> 25.34 <C> 18.6× <R> <C> [BOLD] imitate-NAT <C> 22.44 <C> 25.67 <C> 28.61 <C> 28.90 <C> 28.41 <C> 18.6× <R> <C> [BOLD] imitate-NAT (+LPD,Δ [ITALIC] T=3) <C> 24.15 <C> 27.28 <C> 31.45 <C> 31.81 <C> 30.68 <C> 9.70× <CAP> Table 1: The test set performances of AT and NAT models in BLEU score. NAT-FT, NAT-IR and LT denotes the competitor method in Gu et al. (2017), Lee et al. (2018) and Kaiser et al. (2018) respectively. imitate-NAT is our proposed NAT with imitation learning.
<R> <C> [BOLD] Method <C> [BOLD] Racism  [BOLD] p <C> [BOLD] Racism  [BOLD] r <C> [BOLD] Racism  [BOLD] f1 <C> [BOLD] Sexism  [BOLD] p <C> [BOLD] Sexism  [BOLD] r <C> [BOLD] Sexism  [BOLD] f1 <C> [BOLD] Overall  [BOLD] p <C> [BOLD] Overall  [BOLD] r <C> [BOLD] Overall  [BOLD] f1 <R> <C> lr <C> [BOLD] 80.59 <C> 70.62 <C> 75.28 <C> 83.12 <C> 62.54 <C> 71.38 <C> 83.18 <C> 75.62 <C> 78.75 <R> <C> lr + auth <C> 77.95 <C> 78.35 <C> 78.15 <C> 87.28 <C> 78.41 <C> 82.61 <C> 85.26 <C> 83.28 <C> 84.18 <R> <C> lr + extd <C> 77.95 <C> 78.35 <C> 78.15 <C> 87.02 <C> 78.73 <C> 82.67 <C> 85.17 <C> 83.33 <C> 84.17 <R> <C> gcn† <C> 74.12 <C> 64.95 <C> 69.23 <C> 82.48 <C> [BOLD] 82.22 <C> 82.35 <C> 81.90 <C> 79.42 <C> 80.56 <R> <C> lr + gcn† <C> 79.08 <C> [BOLD] 79.90 <C> [BOLD] 79.49 <C> [BOLD] 88.24 <C> 80.95 <C> [BOLD] 84.44 <C> [BOLD] 86.23 <C> [BOLD] 84.73 <C> [BOLD] 85.42 <CAP> Table 1: The baselines (lr, lr + auth/extd) vs. our gcn approaches (†) on the racism and sexism classes. Overall shows the macro-averaged metrics computed over the 3 classes: sexism, racism, and clean.
<R> <C> Sentence <C> Toxicity <C> Sentiment <R> <C> I hate Justin Timberlake. <C> 0.90 <C> -0.30 <R> <C> I hate Katy Perry. <C> 0.80 <C> -0.10 <R> <C> I hate Taylor Swift. <C> 0.74 <C> -0.40 <R> <C> I hate Rihanna. <C> 0.69 <C> -0.60 <CAP> Table 1: Sensitivity of NLP models to named entities in text. Toxicity score range: 0 to 1; Sentiment score range: -1 to +1.
<R> <C> Corpus <C> Toxicity  [ITALIC] ScoreDev <C> Toxicity  [ITALIC] ScoreRange <C> Sentiment  [ITALIC] ScoreDev <C> Sentiment  [ITALIC] ScoreRange <R> <C> FB-Pol. <C> 0.022 <C> 0.107 <C> 0.070 <C> 0.360 <R> <C> FB-Pub. <C> 0.025 <C> 0.118 <C> 0.083 <C> 0.420 <R> <C> Reddit <C> 0.022 <C> 0.107 <C> 0.072 <C> 0.376 <R> <C> Fitocracy <C> 0.022 <C> 0.103 <C> 0.071 <C> 0.364 <CAP> Table 2: ScoreDev is the per-sentence standard deviation of scores upon name perturbation, averaged across all sentences. ScoreRange is the per-sentence range of scores (i.e., max - min) upon name perturbation, averaged across all sentences.
<R> <C> [ITALIC] dt <C> [ITALIC] de <C> BLEU-4 <C> METEOR <C> [ITALIC] DKL <C> | [ITALIC] V| <C> [ITALIC] p( [ITALIC] len>20) <R> <C> 0.0 <C> 0.0 <C> 20.1 <C> [BOLD] 19.8 <C> 0.453 <C> 733 <C> 0.00 <R> <C> 0.0 <C> 0.2 <C> 16.5 <C> 18.4 <C> 0.298 <C> 2158 <C> 0.02 <R> <C> 0.0 <C> 0.4 <C> 7.4 <C> 14.0 <C> 0.270 <C> 7500 <C> 0.35 <R> <C> 0.0 <C> 0.6 <C> 2.2 <C> 9.3 <C> 0.823 <C> 9303 <C> 0.78 <R> <C> 0.0 <C> 0.8 <C> 0.2 <C> 5.4 <C> 1.496 <C> 9585 <C> 0.95 <R> <C> 0.2 <C> 0.0 <C> [BOLD] 20.3 <C> 19.5 <C> 0.497 <C> 630 <C> 0.00 <R> <C> 0.2 <C> 0.2 <C> 19.0 <C> 19.0 <C> 0.409 <C> 1312 <C> 0.00 <R> <C> 0.2 <C> 0.4 <C> 15.4 <C> 17.3 <C> [BOLD] 0.260 <C> 7007 <C> 0.03 <R> <C> 0.2 <C> 0.6 <C> 3.3 <C> 9.6 <C> 1.837 <C> 9841 <C> 0.83 <R> <C> 0.2 <C> 0.8 <C> 0.1 <C> 3.0 <C> 3.106 <C> 9840 <C> 0.99 <CAP> Table 1: Effects of dt and de on caption accuracy and vocabulary diversity.
<R> <C> Exponent <C> [ITALIC] ϕ <C> [ITALIC] ζ <C> [ITALIC] α <C> [ITALIC] β <R> <C> Basque <C> 1.13±0.04 <C> 1.77±0.14 <C> 0.90±0.03 <C> 3.1±0.3 <R> <C> Catalan <C> 1.17±0.05 <C> 1.89±0.14 <C> 0.92±0.03 <C> 2.8±0.4 <R> <C> English <C> 1.16±0.05 <C> 1.85±0.14 <C> 0.91±0.01 <C> 2.9±0.3 <R> <C> Galician <C> 1.18±0.04 <C> 1.80±0.14 <C> 0.89±0.03 <C> 2.9±0.4 <R> <C> Portuguese <C> 1.16±0.05 <C> 1.77±0.14 <C> 0.91±0.01 <C> 3.0±0.3 <R> <C> Spanish <C> 1.15±0.04 <C> 1.79±0.14 <C> 0.91±0.03 <C> 2.8±0.4 <CAP> Table 1: Summary of scaling exponents associated to the energy release distribution (ϕ), Zipf’s law (ζ), Heaps’ law (α) and Brevity law (β) for the six different languages. Power law fits are performed using maximum likelihood estimation (MLE) following Clauset clauset2009power and goodness-of-fit test and confidence interval are based on Kolmogorov-Smirnov (KS) tests. In all cases, KS are greater than 0.99. Exponents associated to energy release are compatible with those found in rainfall rainfall . Results are compatible with the hypothesis of language-independence.
<R> <C> EN categories Astronomy <C> # doc 151 <C> FR categories Astronomie <C> # doc 123 <C> EN categories Movie <C> # doc 151 <C> FR categories Film <C> # doc 151 <R> <C> Biology <C> 151 <C> Biologie <C> 115 <C> Music <C> 151 <C> Musique <C> 151 <R> <C> Economy <C> 144 <C> Economie <C> 151 <C> Skating <C> 151 <C> Patinage <C> 151 <R> <C> Food <C> 147 <C> Nourriture <C> 4 <C> Heritage <C> 151 <C> Patrimoine <C> 151 <R> <C> Football <C> 151 <C> Football <C> 151 <C> Politics <C> 151 <C> Politique <C> 151 <R> <C> Genetics <C> 82 <C> Génétique <C> 151 <C> Religion <C> 150 <C> Religion <C> 133 <R> <C> Geograpphy <C> 139 <C> Geographie <C> 151 <C> Rugby <C> 151 <C> Rugby <C> 151 <R> <C> Computer <C> 151 <C> Ordinateur <C> 151 <C> Health <C> 151 <C> Santé <C> 63 <R> <C> Literature <C> 150 <C> Littérature <C> 151 <C> Sculpture <C> 151 <C> Sculpture <C> 151 <R> <C> Mathematics <C> 151 <C> Mathématique <C> 63 <C> Tennis <C> 151 <C> Tennis <C> 151 <R> <C> Medicine <C> 151 <C> Médecine <C> 130 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 2: Composition of the comparable bilingual corpus extracted from Wikipedia (EN: English, FR: French)
<R> <C> Retrieval Methods <C> Django <C> Hearthstone <C> CoNaLa <R> <C> BM25 (fine tuned baseline) <C> 43.1 <C> 59.5 <C> 13.2 <R> <C> ReCode sequence similarity <C> 43.4 <C> 65.1 <C> 11.2 <R> <C> Oracle retrieval similarity <C> 58.1 <C> 74.2 <C> 38.0 <R> <C> Generative Methods <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Seq2Seq LSTM <C> 58.9 <C> 60.4 <C> [ITALIC] 10.6 <R> <C> Latent predictor networks (Ling et al.,  2016 ) <C> [ITALIC] 77.6 <C> 67.1 <C> — <R> <C> Retrieve and Edit LSTM (Hashimoto et al.,  2018 ) <C> — <C> [ITALIC] 70.0 <C> — <R> <C> Transformer baseline (Vaswani et al.,  2017 ) <C> 79.2 <C> 72.5 <C> 17.5 <R> <C> Transformer + Copy <C> 81.8 <C> 74.0 <C> 20.8 <R> <C> Transformer + Copy + Naïve Retrieval <C> 80.7 <C> 60.1 <C> 19.0 <R> <C> Relevance Transformer <C> [BOLD] 82.3 <C> [BOLD] 74.5 <C> [BOLD] 22.3 <CAP> Table 1. Analysis of performance on various test collections using BLEU. In italic we show the previous state-of-the-art non-AST methods. In bold we outline the best scores for each dataset.
<R> <C> [BOLD] Corpus <C> [BOLD] Words <C> [BOLD] Sentences <C> [BOLD] W/S <R> <C> Law (Acquis) <C> 18,128,173 <C> 715,372 <C> 25.3 <R> <C> Medical (EMEA) <C> 14,301,472 <C> 1,104,752 <C> 12.9 <R> <C> IT <C> 3,041,677 <C> 337,817 <C> 9.0 <R> <C> Koran (Tanzil) <C> 9,848,539 <C> 480,421 <C> 20.5 <R> <C> Subtitles <C> 114,371,754 <C> 13,873,398 <C> 8.2 <CAP> Table 1: Corpora used to train domain-specific systems, taken from the OPUS repository. IT corpora are GNOME, KDE, PHP, Ubuntu, and OpenOffice.
<R> <C> [BOLD] Label <C> [BOLD] Unobserved <C> [BOLD] Observed Once <R> <C> Adjective <C> 4 <C> 10 <R> <C> Named Entity <C> 40 <C> 42 <R> <C> Noun <C> 35 <C> 35 <R> <C> Number <C> 12 <C> 4 <R> <C> Verb <C> 3 <C> 6 <R> <C> Other <C> 6 <C> 3 <CAP> Table 2: Breakdown of the first 100 tokens that were unobserved in training or observed once in training, by hand-annotated category.
<R> <C> [BOLD] Method <C> [BOLD] Viol. <C> [BOLD] Amp. bias <C> [BOLD] Perf. (%) <R> <C> vSRL: Development Set <C> vSRL: Development Set <C> vSRL: Development Set <C> vSRL: Development Set <R> <C> CRF <C> 154 <C> 0.050 <C> 24.07 <R> <C> CRF + RBA <C> 107 <C> 0.024 <C> 23.97 <R> <C> vSRL: Test Set <C> vSRL: Test Set <C> vSRL: Test Set <C> vSRL: Test Set <R> <C> CRF <C> 149 <C> 0.042 <C> 24.14 <R> <C> CRF + RBA <C> 102 <C> 0.025 <C> 24.01 <R> <C> MLC: Development Set <C> MLC: Development Set <C> MLC: Development Set <C> MLC: Development Set <R> <C> CRF <C> 40 <C> 0.032 <C> 45.27 <R> <C> CRF + RBA <C> 24 <C> 0.022 <C> 45.19 <R> <C> MLC: Test Set <C> MLC: Test Set <C> MLC: Test Set <C> MLC: Test Set <R> <C> CRF <C> 38 <C> 0.040 <C> 45.40 <R> <C> CRF + RBA <C> 16 <C> 0.021 <C> 45.38 <CAP> Table 2: Number of violated constraints, mean amplified bias, and test performance before and after calibration using RBA. The test performances of vSRL and MLC are measured by top-1 semantic role accuracy and top-1 mean average precision, respectively.
<R> <C> [BOLD] Method <C> [BOLD] Result  [BOLD] Val Seen <C> [BOLD] Result  [BOLD] Val Unseen <C> [BOLD] Result  [BOLD] Gap |Δ| <R> <C> Room-to-Room  <C> Room-to-Room  <C> Room-to-Room  <C> Room-to-Room  <R> <C> R2R <C> 38.6 <C> 21.8 <C> 16.8 <R> <C> RPA <C> 42.9 <C> 24.6 <C> 18.3 <R> <C> S-Follower <C> 66.4 <C> 35.5 <C> 30.9 <R> <C> RCM <C> 66.7 <C> 42.8 <C> 23.9 <R> <C> SMNA <C> 67 <C> 45 <C> 22 <R> <C> Regretful <C> 69 <C> 50 <C> 19 <R> <C> EnvDrop <C> 62.1 <C> 52.2 <C> 9.9 <R> <C> ALTR <C> 55.8 <C> 46.1 <C> 9.7 <R> <C> RN+Obj <C> 59.2 <C> 39.5 <C> 19.7 <R> <C> CG <C> 31 <C> 31 <C> [BOLD] 0 <R> <C> Our baseline <C> 56.1 <C> 47.5 <C> 8.6 <R> <C> [BOLD] Our Learned-Seg <C> 52.6 <C> 53.3 <C> [BOLD] 0.7 <R> <C> Room-for-Room  <C> Room-for-Room  <C> Room-for-Room  <C> Room-for-Room  <R> <C> S-Follower <C> 51.9 <C> 23.8 <C> 28.1 <R> <C> RCM <C> 55.5 <C> 28.6 <C> 26.9 <R> <C> Our baseline <C> 54.6 <C> 30.7 <C> 23.9 <R> <C> [BOLD] Our Learned-Seg <C> 38.0 <C> 34.3 <C> [BOLD] 3.7 <R> <C> CVDN  <C> CVDN  <C> CVDN  <C> CVDN  <R> <C> NDH <C> 5.92 <C> 2.10 <C> 3.82 <R> <C> Our baseline <C> 6.60 <C> 3.05 <C> 3.55 <R> <C> [BOLD] Our Learned-Seg <C> 5.82 <C> 4.41 <C> [BOLD] 1.41 <R> <C> Touchdown  <C> Touchdown  <C> Touchdown  <C> Touchdown  <R> <C> GA (original) <C> 7.9 (dev) <C> 5.5 (test) <C> – <R> <C> RCONCAT (original) <C> 9.8 (dev) <C> 10.7 (test) <C> – <R> <C> Our baseline (original) <C> 15.0 (dev) <C> 14.2 (test) <C> – <R> <C> Our baseline (re-split) <C> 17.5 <C> 5.3 <C> 12.2 <CAP> Table 1: Results show the performance gaps between seen (‘Val Seen’) and unseen (‘Val Unseen’) environments in several VLN tasks. Room-to-Room and Room-for-Room are evaluated with ‘Success Rate’, CVDN is evaluated with ‘Goal Progress’, Touchdown is evaluated with ‘Task Completion’.
<R> <C> [BOLD] X-split <C> [BOLD] PD (meters) <C> 5-13 <C> 14-16 <C> 17-21 <C> 22-57 <R> <C> [BOLD] X-split <C> [BOLD] SR (%) <C> 56.3 <C> 56.2 <C> 50.5 <C> 43.6 <R> <C> [BOLD] Z-split <C> [BOLD] PD (meters) <C> 5-10 <C> 11-13 <C> 14-17 <C> 18-52 <R> <C> [BOLD] Z-split <C> [BOLD] SR (%) <C> 58.5 <C> 47.9 <C> 42.9 <C> 44.1 <CAP> Table 3: The success rate declines as the path moves further from training regions. Path distance is denoted as ‘PD’.
<R> <C> [BOLD] Type <C> [BOLD] Inter-rater  [ITALIC] α <C> [BOLD] Intra-rater  [BOLD] Mean  [ITALIC] α <C> [BOLD] Intra-rater  [BOLD] Stdev.  [ITALIC] α <R> <C> 5-point <C> 0.2308 <C> 0.4014 <C> 0.1907 <R> <C> 5-point norm. <C> 0.2820 <C> 0.4014 <C> 0.1907 <R> <C> 5-point norm. part. <C> 0.5059 <C> 0.5527 <C> 0.0470 <R> <C> 5-point norm. trans. <C> 0.3236 <C> 0.3845 <C> 0.1545 <R> <C> Pairwise <C> 0.2385 <C> 0.5085 <C> 0.2096 <R> <C> Pairwise filt. part. <C> 0.3912 <C> 0.7264 <C> 0.0533 <R> <C> Pairwise filt. trans. <C> 0.3519 <C> 0.5718 <C> 0.2591 <CAP> Table 1: Inter- and intra-reliability measured by Krippendorff’s α for 5-point and pairwise ratings of 1,000 translations of which 200 translations are repeated twice. The filtered variants are restricted to either a subset of participants (part.) or a subset of translations (trans.).
<R> <C> [BOLD] Model <C> [BOLD] WMT BLEU <C> [BOLD] WMT METEOR <C> [BOLD] WMT BEER <C> [BOLD] TED BLEU <C> [BOLD] TED METEOR <C> [BOLD] TED BEER <R> <C> WMT <C> 27.2 <C> 31.8 <C> 60.08 <C> 27.0 <C> 30.7 <C> 59.48 <R> <C> TED <C> 26.3 <C> 31.3 <C> 59.49 <C> 34.3 <C> 34.6 <C> 64.94 <CAP> Table 3: Results on test data for in- and out-of-domain fully-supervised models. Both are trained with MLE, the TED model is obtained by fine-tuning the WMT model on TED data.
<R> <C> [EMPTY] <C> Correct <C> Low confidence <C> Incorrect <R> <C> N <C> 4426 <C> 1142 <C> 375 <R> <C> M(complexity) <C> 0.786 <C> 0.725 <C> 0.637 <CAP> Table 5: Numbers and average complexity of utterances with different HRIRS outcomes
<R> <C> [EMPTY] <C> highest complexity <C> median complexity <C> lowest complexity <R> <C> M(requests) <C> 4.20 <C> 1.45 <C> 1.05 <CAP> Table 6: Average number of restaurant-types in different dialog complexity groups.
<R> <C> [EMPTY] <C> [ITALIC] ω3( [ITALIC] a1) <C> [ITALIC] ω3( [ITALIC] a2) <C> [ITALIC] ω3( [ITALIC] a3) <R> <C> Random allocation to agents <C> Random allocation to agents <C> Random allocation to agents <C> Random allocation to agents <R> <C> Ubuntu <C> 0.450 <C> 0.444 <C> 0.454 <R> <C> Insurance <C> 0.894 <C> 0.894 <C> 0.896 <R> <C> HR <C> 0.439 <C> 0.428 <C> 0.453 <R> <C> Restaurant <C> 0.542 <C> 0.536 <C> 0.537 <R> <C> Allocation by ascending dialog complexity <C> Allocation by ascending dialog complexity <C> Allocation by ascending dialog complexity <C> Allocation by ascending dialog complexity <R> <C> Ubuntu <C> 0.370 <C> 0.403 <C> 0.483 <R> <C> Insurance <C> 0.873 <C> 0.894 <C> 0.916 <R> <C> HR <C> 0.378 <C> 0.425 <C> 0.496 <R> <C> Restaurant <C> 0.460 <C> 0.502 <C> 0.601 <CAP> Table 7: Results of simulated experiment to distinguish agents with dialog complexity
<R> <C> Agent <C> Epoch = 100 Success <C> Epoch = 100 Reward <C> Epoch = 100 Turns <C> Epoch = 200 Success <C> Epoch = 200 Reward <C> Epoch = 200 Turns <C> Epoch = 300 Success <C> Epoch = 300 Reward <C> Epoch = 300 Turns <R> <C> DQN <C> .4260 <C> -3.84 <C> 31.93 <C> .5308 <C> 10.78 <C> 22.72 <C> .6480 <C> 27.66 <C> 22.21 <R> <C> DDQ(5) <C> .6056 <C> 20.35 <C> 26.65 <C> .7128 <C> 36.76 <C> 19.55 <C> .7372 <C> 39.97 <C> 18.99 <R> <C> DDQ(5, rand-init  [ITALIC] θM) <C> .5904 <C> 18.75 <C> 26.21 <C> .6888 <C> 33.47 <C> 20.36 <C> .7032 <C> 36.06 <C> 18.64 <R> <C> DDQ(5, fixed  [ITALIC] θM) <C> .5540 <C> 14.54 <C> 25.89 <C> .6660 <C> 29.72 <C> 22.39 <C> .6860 <C> 33.58 <C> 19.49 <R> <C> DQN(5) <C> [ITALIC] .6560 <C> [ITALIC] 29.38 <C> [ITALIC] 21.76 <C> [ITALIC] .7344 <C> [ITALIC] 41.09 <C> [ITALIC] 16.07 <C> [ITALIC] .7576 <C> [ITALIC] 43.97 <C> [ITALIC] 15.88 <R> <C> DDQ(10) <C>  [BOLD] .6624 <C> 28.18 <C> 24.62 <C>  [BOLD] .7664 <C> 42.46 <C> 21.01 <C>  [BOLD] .7840 <C> 45.11 <C> 19.94 <R> <C> DDQ(10, rand-init  [ITALIC] θM) <C> .6132 <C> 21.50 <C> 26.16 <C> .6864 <C> 32.43 <C> 21.86 <C> .7628 <C> 42.37 <C> 20.32 <R> <C> DDQ(10, fixed  [ITALIC] θM) <C> .5884 <C> 18.41 <C> 26.41 <C> .6196 <C> 24.17 <C> 22.36 <C> .6412 <C> 26.70 <C> 22.49 <R> <C> DQN(10) <C> [ITALIC] .7944 <C> [ITALIC] 48.61 <C> [ITALIC] 15.43 <C> [ITALIC] .8296 <C> [ITALIC] 54.00 <C> [ITALIC] 13.09 <C> [ITALIC] .8356 <C> [ITALIC] 54.89 <C> [ITALIC] 12.77 <CAP> Table 1: Results of different agents at training epoch = {100, 200, 300}. Each number is averaged over 5 runs, each run tested on 2000 dialogues. Excluding DQN(5) and DQN(10) which serve as the upper bounds, any two groups of success rate (except three groups: at epoch 100, DDQ(5, rand-init θM) and DDQ(10, fixed θM), at epoch 200, DDQ(5, rand-init θM) and DDQ(10, rand-init θM), at epoch 300, DQN and DDQ(10, fixed θM)) evaluated at the same epoch is statistically significant in mean with p<0.01. (Success: success rate)
<R> <C> Train/Test Class <C> Task 1 angle=90,lap=0pt-(1em)Name <C> Task 1 angle=90,lap=0pt-(1em)Surname <C> Task 1 angle=90,lap=0pt-(1em)State <C> Task 1 angle=90,lap=0pt-(1em)Location <C> Task 1 angle=90,lap=0pt-(1em)Occupation <C> Task 1 angle=90,lap=0pt-(1em) [ITALIC] Other# <C> Task 1 angle=90,lap=0pt-(1em) [ITALIC] F1 <C> Task 2 angle=90,lap=0pt-(1em)Husband <C> Task 2 angle=90,lap=0pt-(1em)Husband’s Father <C> Task 2 angle=90,lap=0pt-(1em)Husband’s Mother <C> Task 2 angle=90,lap=0pt-(1em)Other Person <C> Task 2 angle=90,lap=0pt-(1em)Wife <C> Task 2 angle=90,lap=0pt-(1em)Wife’s Father <C> Task 2 angle=90,lap=0pt-(1em)Wife’s Mother <C> Task 2 angle=90,lap=0pt-(1em) [ITALIC] Other# <C> Task 2 angle=90,lap=0pt-(1em) [ITALIC] F1 <R> <C> CCRF <C> CCRF <C> CCRF <C> CCRF <C> CCRF <C> CCRF <C> CCRF <C> CCRF <C> CCRF <C> CCRF <C> CCRF <C> CCRF <C> CCRF <C> CCRF <C> CCRF <C> CCRF <C> CCRF <R> <C> True/True <C> 0.9831 <C> 0.9534 <C> 0.9668 <C> 0.9546 <C> 0.9826 <C> 0.9851 <C> [BOLD] 0.9844 <C> 0.8089 <C> 0.5733 <C> 0.2959 <C> 0.6704 <C> 0.1992 <C> 0.6621 <C> 0.1119 <C> 0.9569 <C> [BOLD] 0.7333 <R> <C> True/Noisy <C> 0.9921 <C> 0.9742 <C> 0.9794 <C> 0.9703 <C> 0.9854 <C> 0.9887 <C> [BOLD] 0.9759 <C> 0.8367 <C> 0.653 <C> 0.4575 <C> 0.7234 <C> 0.2942 <C> 0.7014 <C> 0.3533 <C> 0.9581 <C> [BOLD] 0.7797 <R> <C> GCRF <C> GCRF <C> GCRF <C> GCRF <C> GCRF <C> GCRF <C> GCRF <C> GCRF <C> GCRF <C> GCRF <C> GCRF <C> GCRF <C> GCRF <C> GCRF <C> GCRF <C> GCRF <C> GCRF <R> <C> True/True <C> 0.9384 <C> 0.8523 <C> 0.8979 <C> 0.835 <C> 0.9721 <C> 0.9534 <C> [BOLD] 0.9218 <C> 0.8759 <C> 0.8668 <C> 0.912 <C> 0.9244 <C> 0.7586 <C> 0.8739 <C> 0.6793 <C> 0.9536 <C> [BOLD] 0.9072 <R> <C> True/Noisy <C> 0.9554 <C> 0.895 <C> 0.9174 <C> 0.8536 <C> 0.9816 <C> 0.9568 <C> [BOLD] 0.9345 <C> 0.9116 <C> 0.8894 <C> 0.9363 <C> 0.9479 <C> 0.8289 <C> 0.92 <C> 0.815 <C> 0.9557 <C> [BOLD] 0.9260 <R> <C> EFGCRF <C> EFGCRF <C> EFGCRF <C> EFGCRF <C> EFGCRF <C> EFGCRF <C> EFGCRF <C> EFGCRF <C> EFGCRF <C> EFGCRF <C> EFGCRF <C> EFGCRF <C> EFGCRF <C> EFGCRF <C> EFGCRF <C> EFGCRF <C> EFGCRF <R> <C> True/True <C> 0.9876 <C> 0.9611 <C> 0.9699 <C> 0.9638 <C> 0.9844 <C> 0.9868 <C> [BOLD] 0.9798 <C> 0.9334 <C> 0.922 <C> 0.9403 <C> 0.9607 <C> 0.9032 <C> 0.945 <C> 0.9008 <C> 0.9595 <C> [BOLD] 0.9430 <R> <C> True/Noisy <C> 0.9937 <C> 0.9758 <C> 0.982 <C> 0.9731 <C> 0.9864 <C> 0.9896 <C> [BOLD] 0.9859 <C> 0.9469 <C> 0.9341 <C> 0.9521 <C> 0.981 <C> 0.9352 <C> 0.957 <C> 0.9562 <C> 0.9623 <C> [BOLD] 0.9529 <R> <C> BLSTM <C> BLSTM <C> BLSTM <C> BLSTM <C> BLSTM <C> BLSTM <C> BLSTM <C> BLSTM <C> BLSTM <C> BLSTM <C> BLSTM <C> BLSTM <C> BLSTM <C> BLSTM <C> BLSTM <C> BLSTM <C> BLSTM <R> <C> True/True <C> 0.9914 <C> 0.9667 <C> 0.9795 <C> 0.9757 <C> 0.987 <C> 0.9893 <C> [BOLD] 0.9848 <C> 0.9714 <C> 0.9864 <C> 0.9841 <C> 0.9874 <C> 0.9857 <C> 0.9832 <C> 0.9672 <C> 0.9873 <C> [BOLD] 0.9849 <R> <C> True/Noisy <C> 0.9984 <C> 0.9935 <C> 0.996 <C> 0.9905 <C> 0.9916 <C> 0.9964 <C> [BOLD] 0.9954 <C> 0.9912 <C> 0.9951 <C> 0.9958 <C> 0.9966 <C> 0.9944 <C> 0.994 <C> 0.9957 <C> 0.9959 <C> [BOLD] 0.9952 <CAP> TABLE I: Class-wise F1 scores for both semantic categories (only best performing models or models with interesting observations are shown, # classes are not considered for IHHER evaluation)
<R> <C> [BOLD] Method <C> Geo <R> <C> [ITALIC]  [BOLD] Copying Entities <C> [EMPTY] <R> <C> GNN w/ edge vectors + BERT <C> [BOLD] 92.5 <R> <C> GNN w/ edge vectors <C> 89.3 <R> <C> [ITALIC]  [BOLD] Copying Tokens <C> [EMPTY] <R> <C> GNN w/ edge vectors <C> 87.9 <R> <C> − entity candidates,  [BOLD] e <C> 84.3 <R> <C> BERT <C> 89.6 <CAP> Table 3: Experimental results for copying tokens instead of entities when decoding, with and without conditioning on the set of entity candidates, e.
<R> <C> [BOLD] Edge Ablations <C> Geo <C> Atis <R> <C> GNN w/ edge vectors <C> 89.3 <C> 87.1 <R> <C> − entity span edges <C> 88.6 <C> 34.2 <CAP> Table 4: Results for ablating information about entity candidate spans for Geo and Atis.
<R> <C> Signer <C> Tandem HMM 1 <C> Tandem HMM 2 <C> Tandem HMM 3 <C> Tandem HMM 4 <C> Tandem HMM  [BOLD] Mean <C> Rescoring SCRF 1 <C> Rescoring SCRF 2 <C> Rescoring SCRF 3 <C> Rescoring SCRF 4 <C> Rescoring SCRF  [BOLD] Mean <C> First-pass SCRF 1 <C> First-pass SCRF 2 <C> First-pass SCRF 3 <C> First-pass SCRF 4 <C> First-pass SCRF  [BOLD] Mean <R> <C> No adapt. <C> 54.1 <C> 54.7 <C> 62.6 <C> 57.5 <C> [BOLD] 57.2 <C> 52.6 <C> 51.2 <C> 61.1 <C> 56.3 <C> [BOLD] 55.3 <C> 55.3 <C> 53.3 <C> 72.5 <C> 61.4 <C> [BOLD] 60.6 <R> <C> Forced align. <C> 30.2 <C> 38.5 <C> 39.6 <C> 36.1 <C> [BOLD] 33.6 <C> 39.5 <C> 36.0 <C> 38.2 <C> 34.5 <C> [BOLD] 32.0 <C> 24.4 <C> 24.9 <C> 36.5 <C> 35.5 <C> [BOLD] 30.3 <R> <C> Ground truth <C> 22.0 <C> 13.0 <C> 31.6 <C> 21.4 <C> [BOLD] 22.0 <C> 22.4 <C> 13.5 <C> 29.5 <C> 21.4 <C> [BOLD] 21.7 <C> 15.2 <C> 10.6 <C> 24.9 <C> 18.4 <C> [BOLD] 17.3 <R> <C> Signer-dep. <C> 13.8 <C> 7.1 <C> 26.1 <C> 11.5 <C> [BOLD] 14.6 <C> 10.2 <C> 7.0 <C> 19.1 <C> 10.0 <C> [BOLD] 11.5 <C> 8.1 <C> 7.7 <C> 9.3 <C> 10.1 <C> [BOLD] 8.8 <CAP> Table 3: Letter error rates (%) on four test signers.
<R> <C> Signer <C> SCRF + DNNs trained from scratch 1 <C> SCRF + DNNs trained from scratch 2 <C> SCRF + DNNs trained from scratch 3 <C> SCRF + DNNs trained from scratch 4 <C> SCRF + DNNs trained from scratch  [BOLD] Mean <C> Sig.-indep. SCRF, DNNs from scratch 1 <C> Sig.-indep. SCRF, DNNs from scratch 2 <C> Sig.-indep. SCRF, DNNs from scratch 3 <C> Sig.-indep. SCRF, DNNs from scratch 4 <C> Sig.-indep. SCRF, DNNs from scratch  [BOLD] Mean <C> Sig.-indep. SCRF, fine-tuned DNN 1 <C> Sig.-indep. SCRF, fine-tuned DNN 2 <C> Sig.-indep. SCRF, fine-tuned DNN 3 <C> Sig.-indep. SCRF, fine-tuned DNN 4 <C> Sig.-indep. SCRF, fine-tuned DNN  [BOLD] Mean <R> <C> Accuracy <C> 23.2 <C> 18.2 <C> 28.9 <C> 30.1 <C> [BOLD] 25.1 <C> 18.4 <C> 12.6 <C> 27.0 <C> 20.7 <C> [BOLD] 19.7 <C> 15.2 <C> 10.6 <C> 24.9 <C> 18.4 <C> [BOLD] 17.3 <CAP> Table 4: Letter error rates (%) for different settings of SCRF and DNN training in the signer-adapted case. Details are given in Section 5.3.1.
<R> <C> Fine-tuning with FA Signer 1 <C> Fine-tuning with FA Signer 2 <C> Fine-tuning with FA Signer 3 <C> Fine-tuning with FA Signer 4 <C> Fine-tuning with FA  [BOLD] Mean <C> Fine-tuning with FA + realignment Signer 1 <C> Fine-tuning with FA + realignment Signer 2 <C> Fine-tuning with FA + realignment Signer 3 <C> Fine-tuning with FA + realignment Signer 4 <C> Fine-tuning with FA + realignment  [BOLD] Mean <R> <C> 22.7 <C> 26.0 <C> 33.4 <C> 34.8 <C> [BOLD] 29.2 <C> 21.9 <C> 25.3 <C> 30.5 <C> 34.0 <C> [BOLD] 27.9 <CAP> Table 5: Letter error rates (%) with iterated forced-alignment (FA) adaptation.
<R> <C> Signer-dependent Signer 1 <C> Signer-dependent Signer 2 <C> Signer-dependent Signer 3 <C> Signer-dependent Signer 4 <C> Signer-dependent  [BOLD] Mean <C> Signer-independent Signer 1 <C> Signer-independent Signer 2 <C> Signer-independent Signer 3 <C> Signer-independent Signer 4 <C> Signer-independent  [BOLD] Mean <R> <C> 7.2 <C> 6.5 <C> 8.1 <C> 8.6 <C> [BOLD] 7.6 <C> 13.0 <C> 11.2 <C> 21.7 <C> 18.8 <C> [BOLD] 16.2 <CAP> Table 6: Letter error rates (%) obtained with a two-pass segmental cascade.
<R> <C> [BOLD] Model <C> [BOLD] ROUGE 1 <C> [BOLD] ROUGE 2 <C> [BOLD] ROUGE L <R> <C> Pointer-Generator See et al. ( 2017 ) <C> 36.44 <C> 15.66 <C> 33.42 <R> <C> Pointer-Generator + Coverage See et al. ( 2017 ) <C> 39.53 <C> [BOLD] 17.28 <C> 36.38 <R> <C> Graph Attention Tan et al. ( 2017 ) <C> 38.1 <C> 13.9 <C> 34.0 <R> <C> Pointer-Generator + DiffMask Gehrmann et al. ( 2018 ) <C> 38.45 <C> 16.88 <C> 35.81 <R> <C> Pointer-Generator (Re-Implementation) <C> 35.55 <C> 15.29 <C> 32.05 <R> <C> Pointer-Generator + Coverage (Re-Implementation) <C> 39.07 <C> 16.97 <C> 35.87 <R> <C> Latent-Structure (LS) Attention <C> 39.52 <C> 16.94 <C> 36.71 <R> <C> Explicit-Structure (ES) Attention <C> 39.63 <C> 16.98 <C> 36.72 <R> <C> LS + ES Attention <C> [BOLD] 39.62 <C> 17.00 <C> [BOLD] 36.95 <CAP> Table 1: Results of abstractive summarizers on the CNN/DM dataset. The top part shows abstractive summarization baselines. The second section are re-implementations of See et al. (2017) 333 https://github.com/atulkum/pointer_summarizer and results from StructSum.
<R> <C> Dutch <C> monolingual <C> Inefficiency 0.16 <C> Dissimilarity 0.11 <R> <C> [EMPTY] <C> bilingual <C> 0.17 <C> 0.12 <R> <C> [EMPTY] <C> hypothetical <C> 0.29 (±0.02) <C> 0.59 (±0.05) <R> <C> French <C> monolingual <C> 0.18 <C> 0.11 <R> <C> [EMPTY] <C> bilingual <C> 0.17 <C> 0.09 <R> <C> [EMPTY] <C> hypothetical <C> 0.31 (±0.01) <C> 0.56 (±0.06) <CAP> Table 1: Evaluation of the IB container-naming model. Lower values indicate a better fit of the model. Values for hypothetical systems are averages ±SD over 10,000 systems.
<R> <C> Model <C> Fluency <C> Relevance <R> <C> No fine tuning <C> [BOLD] 3.34 <C> [BOLD] 3.12 <R> <C> +QA, LM rewards <C> 3.05 <C> 2.75 <R> <C> +QA, LM, discriminator rewards +Adversarial discriminator <C> 2.89 <C> 2.82 <R> <C> Ground Truth <C> 4.67 <C> 4.72 <CAP> Table 3: Summary of human evaluation of selected models
<R> <C> Model <C> Acc <C> BLEU <C> G2 <C> H2 <R> <C> Our Model <C> 94.0 <C> 60.4 <C> [BOLD] 75.4 <C> [BOLD] 73.6 <R> <C> [ITALIC] -NSC <C> 88.3 <C> 55.7 <C> 70.1 <C> 68.3 <R> <C> [ITALIC] NSC- [ITALIC] λyj <C> [BOLD] 98.1 <C> 10.1 <C> 31.5 <C> 18.3 <R> <C> [ITALIC] - [ITALIC] Lxλ <C> 95.2 <C> 57.7 <C> 74.1 <C> 71.8 <R> <C> [ITALIC] Lcp→ [ITALIC] L′ [ITALIC] cp <C> 78.2 <C> [BOLD] 61.6 <C> 69.4 <C> 68.9 <R> <C> [ITALIC] - [ITALIC] Lyλ <C> 94.3 <C> 59.4 <C> 74.8 <C> 72.8 <R> <C> [ITALIC] - [ITALIC] Llm <C> 96.8 <C> 56.5 <C> 73.9 <C> 71.3 <R> <C> [ITALIC] Finetuning- <C> 91.5 <C> 55.8 <C> 71.4 <C> 68.9 <CAP> Table 3: Ablation study results.
<R> <C> Models <C> Level-1 Classification Dev <C> Level-1 Classification Test <C> Level-2 Classification Dev <C> Level-2 Classification Test <R> <C> Bi-LSTM <C> 55.10 <C> 56.88 <C> 35.02 <C> 42.44 <R> <C> Bi-GRU <C> 55.21 <C> 57.01 <C> 35.34 <C> 42.46 <R> <C> Tree-LSTM <C> 56.04 <C> 58.89 <C> 35.76 <C> 43.02 <R> <C> Tree-GRU <C> 55.36 <C> 58.98 <C> 36.09 <C> 43.78 <R> <C> Tag-Enhanced Tree-LSTM <C> [BOLD] 56.97 <C> [BOLD] 59.85 <C> 35.92 <C> [BOLD] 45.21 <R> <C> Tag-Enhanced Tree-GRU <C> 56.63 <C> 59.75 <C> [BOLD] 36.93 <C> 44.55 <CAP> Table 1: The accuracy score of multi-class classification
<R> <C> Datasets <C> [ITALIC] dhead <C> [ITALIC] dff <C> h <C> L <C> [ITALIC] dk <C> [ITALIC] dv <C> Test PPL <R> <C> PTB <C> 256 <C> 2100 <C> 2 <C> 3 <C> 40 <C> 40 <C> 49.8 <R> <C> WikiText-103 <C> 256 <C> 2100 <C> 2 <C> 6 <C> 40 <C> 40 <C> 18.9 <R> <C> One-Billion <C> 1024 <C> 2100 <C> 2 <C> 6 <C> 40 <C> 40 <C> 19.5 <CAP> Table 4: The hyperparameters in the Tensorized Transformers model
<R> <C> Word <C> Min. rank <C> Count rank <C> tf-idf rank <C> Word Ratio rank <C> Word Ratio odds <C> Word Ratio prob. <C> Pair Ratio rank <C> Pair Ratio odds <C> Pair Ratio prob. <R> <C> Trump <C> 1 <C> [BOLD] 1 <C> [BOLD] 1 <C> [BOLD] 1 <C> 139.4 <C> 99.3 <C> 5 <C> 139.4 <C> 99.3 <R> <C> casino <C> 2 <C> 14 <C> [BOLD] 2 <C> 41 <C> 2.7 <C> 73.1 <C> 13 <C> 74.4 <C> 98.7 <R> <C> say <C> 2 <C> [BOLD] 2 <C> 6 <C> 213 <C> 0.3 <C> 22.8 <C> 6 <C> 120.2 <C> 99.2 <R> <C> York <C> 2 <C> 9 <C> 11 <C> 344 <C> 0.1 <C> 11.4 <C> [BOLD] 2 <C> 272.5 <C> 99.6 <R> <C> Mar-a-Lago <C> 2 <C> 303 <C> 31 <C> [BOLD] 2 <C> 45.2 <C> 97.8 <C> 35 <C> 45.2 <C> 97.8 <R> <C> New <C> 2 <C> 4 <C> 25 <C> 607 <C> 0.0 <C> 4.4 <C> [BOLD] 2 <C> 272.5 <C> 99.6 <R> <C> campaign <C> 3 <C> 5 <C> [BOLD] 3 <C> 133 <C> 0.6 <C> 38.5 <C> 9 <C> 97.9 <C> 99.0 <R> <C> goproud <C> 3 <C> 454 <C> 55 <C> [BOLD] 3 <C> 33.2 <C> 97.1 <C> 63 <C> 33.2 <C> 97.1 <R> <C> republican <C> 4 <C> 10 <C> [BOLD] 4 <C> 149 <C> 0.6 <C> 35.5 <C> 34 <C> 46.2 <C> 97.9 <R> <C> CPAC <C> 4 <C> 303 <C> 47 <C> [BOLD] 4 <C> 25.9 <C> 96.3 <C> 79 <C> 25.9 <C> 96.3 <R> <C> States <C> 4 <C> 9 <C> 18 <C> 431 <C> 0.1 <C> 7.8 <C> [BOLD] 4 <C> 249.8 <C> 99.6 <R> <C> United <C> 4 <C> 6 <C> 17 <C> 476 <C> 0.1 <C> 6.5 <C> [BOLD] 4 <C> 249.8 <C> 99.6 <R> <C> state <C> 4 <C> [BOLD] 4 <C> 14 <C> 499 <C> 0.1 <C> 6.1 <C> 17 <C> 61.4 <C> 98.4 <R> <C> presidential <C> 5 <C> 13 <C> [BOLD] 5 <C> 97 <C> 0.9 <C> 48.4 <C> 48 <C> 41.9 <C> 97.7 <R> <C> Ivanka <C> 5 <C> 454 <C> 77 <C> [BOLD] 5 <C> 15.1 <C> 93.8 <C> 144 <C> 15.1 <C> 93.8 <R> <C> deferment <C> 6 <C> 606 <C> 149 <C> [BOLD] 6 <C> 14.6 <C> 93.6 <C> 154 <C> 14.6 <C> 93.6 <R> <C> Clinton <C> 7 <C> 18 <C> [BOLD] 7 <C> 58 <C> 1.8 <C> 63.9 <C> 56 <C> 36.9 <C> 97.4 <R> <C> estate <C> 7 <C> 18 <C> 12 <C> 170 <C> 0.4 <C> 29.9 <C> [BOLD] 7 <C> 98.9 <C> 99.0 <R> <C> real <C> 7 <C> 43 <C> 36 <C> 249 <C> 0.2 <C> 18.6 <C> [BOLD] 7 <C> 98.9 <C> 99.0 <R> <C> President <C> 7 <C> [BOLD] 7 <C> 9 <C> 287 <C> 0.2 <C> 14.8 <C> 51 <C> 39.1 <C> 97.5 <R> <C> Trans-Pacific <C> 7 <C> 454 <C> 95 <C> [BOLD] 7 <C> 14.3 <C> 93.5 <C> 98 <C> 21.9 <C> 95.6 <R> <C> tax <C> 8 <C> 15 <C> [BOLD] 8 <C> 101 <C> 0.9 <C> 47.1 <C> 24 <C> 55.1 <C> 98.2 <R> <C> trump-branded <C> 8 <C> 909 <C> 158 <C> [BOLD] 8 <C> 13.0 <C> 92.8 <C> 184 <C> 13.0 <C> 92.8 <R> <C> NYMA <C> 9 <C> 909 <C> 186 <C> [BOLD] 9 <C> 12.8 <C> 92.7 <C> 187 <C> 12.8 <C> 92.7 <R> <C> sue <C> 10 <C> 29 <C> [BOLD] 10 <C> 59 <C> 1.8 <C> 63.8 <C> 11 <C> 93.2 <C> 98.9 <R> <C> Organization <C> 10 <C> 37 <C> 45 <C> 360 <C> 0.1 <C> 10.4 <C> [BOLD] 10 <C> 93.8 <C> 98.9 <R> <C> Bethesda-by-the-Sea <C> 10 <C> 909 <C> 173 <C> [BOLD] 10 <C> 12.4 <C> 92.5 <C> 193 <C> 12.4 <C> 92.5 <R> <C> business <C> 11 <C> [BOLD] 11 <C> 15 <C> 279 <C> 0.2 <C> 15.4 <C> 27 <C> 51.8 <C> 98.1 <R> <C> make <C> 11 <C> [BOLD] 11 <C> 65 <C> 619 <C> 0.0 <C> 4.2 <C> 12 <C> 75.1 <C> 98.7 <R> <C> alt-right <C> 11 <C> 909 <C> 214 <C> [BOLD] 11 <C> 10.9 <C> 91.6 <C> 230 <C> 10.9 <C> 91.6 <R> <C> WrestleMania <C> 12 <C> 189 <C> 35 <C> [BOLD] 12 <C> 10.7 <C> 91.4 <C> 209 <C> 11.8 <C> 92.2 <R> <C> Hotel <C> 13 <C> 22 <C> [BOLD] 13 <C> 197 <C> 0.3 <C> 24.5 <C> 16 <C> 63.2 <C> 98.4 <R> <C> non-interventionist <C> 13 <C> 909 <C> 284 <C> [BOLD] 13 <C> 10.3 <C> 91.1 <C> 240 <C> 10.3 <C> 91.1 <R> <C> hotel/casino <C> 14 <C> 909 <C> 247 <C> [BOLD] 14 <C> 10.3 <C> 91.1 <C> 241 <C> 10.3 <C> 91.1 <R> <C> golf <C> 15 <C> 46 <C> 19 <C> 172 <C> 0.4 <C> 29.7 <C> [BOLD] 15 <C> 63.3 <C> 98.4 <R> <C> course <C> 15 <C> 94 <C> 86 <C> 349 <C> 0.1 <C> 11.0 <C> [BOLD] 15 <C> 63.3 <C> 98.4 <R> <C> hyperbole <C> 15 <C> 606 <C> 175 <C> [BOLD] 15 <C> 9.6 <C> 90.6 <C> 260 <C> 9.6 <C> 90.6 <R> <C> Fred <C> 16 <C> 37 <C> [BOLD] 16 <C> 127 <C> 0.7 <C> 40.0 <C> 32 <C> 49.3 <C> 98.0 <R> <C> other <C> 16 <C> [BOLD] 16 <C> 109 <C> 693 <C> 0.0 <C> 3.3 <C> 62 <C> 34.1 <C> 97.2 <R> <C> Reince <C> 16 <C> 909 <C> 250 <C> [BOLD] 16 <C> 9.3 <C> 90.3 <C> 270 <C> 9.3 <C> 90.3 <R> <C> Trumped <C> 17 <C> 909 <C> 230 <C> [BOLD] 17 <C> 9.2 <C> 90.2 <C> 274 <C> 9.2 <C> 90.2 <R> <C> Party <C> 18 <C> [BOLD] 18 <C> 26 <C> 492 <C> 0.1 <C> 6.2 <C> 34 <C> 46.2 <C> 97.9 <R> <C> city <C> 18 <C> 27 <C> 79 <C> 731 <C> 0.0 <C> 2.9 <C> [BOLD] 18 <C> 58.8 <C> 98.3 <R> <C> first <C> 18 <C> [BOLD] 18 <C> 170 <C> 851 <C> 0.0 <C> 2.0 <C> 53 <C> 39.0 <C> 97.5 <R> <C> Priebus <C> 18 <C> 909 <C> 250 <C> [BOLD] 18 <C> 9.1 <C> 90.1 <C> 275 <C> 9.1 <C> 90.1 <R> <C> become <C> 19 <C> 26 <C> 112 <C> 671 <C> 0.0 <C> 3.5 <C> [BOLD] 19 <C> 57.9 <C> 98.3 <R> <C> Lashley <C> 19 <C> 606 <C> 150 <C> [BOLD] 19 <C> 7.7 <C> 88.4 <C> 305 <C> 7.7 <C> 88.4 <R> <C> Ivana <C> 20 <C> 151 <C> [BOLD] 20 <C> [BOLD] 20 <C> 6.9 <C> 87.3 <C> 244 <C> 10.1 <C> 91.0 <R> <C> name <C> 20 <C> 29 <C> 141 <C> 859 <C> 0.0 <C> 2.0 <C> [BOLD] 20 <C> 57.4 <C> 98.3 <R> <C> bankruptcy <C> 21 <C> 75 <C> [BOLD] 21 <C> 60 <C> 1.8 <C> 63.8 <C> 462 <C> 5.6 <C> 84.9 <R> <C> usfl <C> 21 <C> 303 <C> 63 <C> [BOLD] 21 <C> 6.8 <C> 87.2 <C> 377 <C> 6.8 <C> 87.2 <R> <C> Obama <C> 22 <C> 75 <C> [BOLD] 22 <C> 56 <C> 1.8 <C> 64.3 <C> 123 <C> 17.6 <C> 94.6 <R> <C> Miss <C> 22 <C> 94 <C> 71 <C> 331 <C> 0.1 <C> 11.9 <C> [BOLD] 22 <C> 55.9 <C> 98.2 <R> <C> show <C> 22 <C> [BOLD] 22 <C> 52 <C> 518 <C> 0.1 <C> 5.8 <C> 23 <C> 55.2 <C> 98.2 <R> <C> Universe <C> 22 <C> 227 <C> 143 <C> 244 <C> 0.2 <C> 19.0 <C> [BOLD] 22 <C> 55.9 <C> 98.2 <R> <C> release <C> 22 <C> [BOLD] 22 <C> 51 <C> 647 <C> 0.0 <C> 3.8 <C> 50 <C> 41.4 <C> 97.6 <R> <C> time <C> 22 <C> [BOLD] 22 <C> 115 <C> 711 <C> 0.0 <C> 3.1 <C> 203 <C> 12.0 <C> 92.3 <R> <C> year <C> 22 <C> [BOLD] 22 <C> 148 <C> 831 <C> 0.0 <C> 2.1 <C> 219 <C> 11.3 <C> 91.9 <R> <C> Obamacare <C> 22 <C> 909 <C> 325 <C> [BOLD] 22 <C> 6.7 <C> 87.0 <C> 385 <C> 6.7 <C> 87.0 <R> <C> Donald <C> 23 <C> 51 <C> [BOLD] 23 <C> 131 <C> 0.6 <C> 38.9 <C> 26 <C> 53.1 <C> 98.2 <R> <C> Melania <C> 23 <C> 606 <C> 136 <C> [BOLD] 23 <C> 6.4 <C> 86.6 <C> 403 <C> 6.4 <C> 86.6 <R> <C> election <C> 24 <C> 27 <C> [BOLD] 24 <C> 503 <C> 0.1 <C> 6.1 <C> 44 <C> 42.2 <C> 97.7 <R> <C> return <C> 24 <C> 75 <C> 147 <C> 556 <C> 0.1 <C> 5.3 <C> [BOLD] 24 <C> 55.1 <C> 98.2 <R> <C> nbcuniversal <C> 24 <C> 606 <C> 181 <C> [BOLD] 24 <C> 6.1 <C> 86.0 <C> 427 <C> 6.1 <C> 86.0 <R> <C> Maples <C> 25 <C> 454 <C> 97 <C> [BOLD] 25 <C> 6.1 <C> 85.9 <C> 428 <C> 6.1 <C> 85.9 <R> <C> bondholder <C> 26 <C> 909 <C> 333 <C> [BOLD] 26 <C> 5.8 <C> 85.2 <C> 449 <C> 5.8 <C> 85.2 <R> <C> poll <C> 27 <C> 75 <C> [BOLD] 27 <C> 104 <C> 0.9 <C> 46.2 <C> 85 <C> 24.8 <C> 96.1 <R> <C> Trumps <C> 27 <C> 909 <C> 307 <C> [BOLD] 27 <C> 5.6 <C> 84.9 <C> 465 <C> 5.6 <C> 84.9 <R> <C> NBC <C> 28 <C> 94 <C> [BOLD] 28 <C> 106 <C> 0.8 <C> 45.3 <C> 347 <C> 7.1 <C> 87.6 <R> <C> Atlantic <C> 28 <C> 170 <C> 105 <C> 303 <C> 0.2 <C> 13.8 <C> [BOLD] 28 <C> 51.2 <C> 98.1 <R> <C> grope <C> 28 <C> 909 <C> 349 <C> [BOLD] 28 <C> 4.8 <C> 82.8 <C> 502 <C> 4.8 <C> 82.8 <CAP> Table 2. Words from the “Donald Trump” Wikipedia article. Sorted by the minimum rank. Bold indicates which method had the minimum rank, i.e., the strongest preference.
<R> <C> Evaluation metric <C> FilterComp <C> Super <R> <C> Mean rank in 21,098 candidates <C> 3611 <C> 3515 <R> <C> Median rank in 21,098 candidates <C> 52 <C> 13 <R> <C> Percent in top 1 <C> 10.0 <C> 14.0 <R> <C> Percent in top 10 <C> 32.4 <C> 47.7 <R> <C> Percent in top 100 <C> 57.1 <C> 70.6 <R> <C> Percent in 2000 candidates <C> 83.7 <C> 83.7 <R> <C> Number of candidates considered <C> 21,098 <C> 2000 <CAP> Table 19: Performance of FilterComp and Super on the adjective-noun dataset.
<R> <C> Persian (FA) <C> Persian (FA) MAP <C> Persian (FA) %Mono <C> Persian (FA) P@5 <C> Persian (FA) P@10 <C> French (FR) <C> French (FR) MAP <C> French (FR) %Mono <C> French (FR) P@5 <C> French (FR) P@10 <R> <C> Mono <C> 0.3659 <C> - <C> 0.588 <C> 0.562 <C> Mono <C> 0.407 <C> - <C> 0.5253 <C> 0.4697 <R> <C> TOP-1 <C> 0.2135 <C> 58.34 <C> 0.3480 <C> 0.3460 <C> TOP-1 <C> 0.2854 <C> 70.12 <C> 0.3515 <C> 0.3121 <R> <C> UNIFORM <C> 0.1977 <C> 54.03 <C> 0.3240 <C> 0.3120 <C> UNIFORM <C> 0.2708 <C> 66.54 <C> 0.3596 <C> 0.3091 <R> <C> PARALLEL <C> 0.1679 <C> 45.89 <C> 0.292 <C> 0.282 <C> PARALLEL <C> 0.1873 <C> 51.19 <C> 0.2485 <C> 0.2212 <R> <C> BiCTM <C> 0.2547 <C> 69.61 <C> 0.406 <C> 0.406 <C> BiCTM <C> 0.3042 <C> 74.74 <C> 0.3899 <C> 0.3515 <R> <C> JCLTRLM <C> 0.2523 <C> 68.95 <C> 0.400 <C> 0.391 <C> JCLTRLM <C> 0.2266 <C> 55.67 <C> 0.3414 <C> 0.299 <R> <C> CLWETM123 [ITALIC] α=0 <C> 0.2347 <C> 64.14 <C> 0.3740 <C> 0.3600 <C> CLWETM1235 [ITALIC] α=0 <C> 0.2929 <C> 71.96 <C> 0.3879 <C> 0.3444 <R> <C> MIXWETM123456 [ITALIC] α∗ <C> 0.2695 <C> 73.65 <C> 0.4280 <C> 0.4190 <C> MIXWETM1235 [ITALIC] α∗ <C> 0.2983 <C> 73.92 <C> 0.3919 <C> 0.3485 <R> <C> CLWETM1234567 [ITALIC] α∗ <C> [BOLD] 0.2774 <C> [BOLD] 75.81 <C> [BOLD] 0.4540 <C> [BOLD] 0.4250 <C> CLWETM1234567 [ITALIC] α∗ <C> [BOLD] 0.3183 <C> [BOLD] 78.21 <C> [BOLD] 0.4202 <C> [BOLD] 0.3707 <R> <C> Spanish (SP) <C> Spanish (SP) <C> Spanish (SP) <C> Spanish (SP) <C> Spanish (SP) <C> German (DE) <C> German (DE) <C> German (DE) <C> German (DE) <C> German (DE) <R> <C> [EMPTY] <C> MAP <C> %Mono <C> P@5 <C> P@10 <C> [EMPTY] <C> MAP <C> %M <C> P@5 <C> P@10 <R> <C> Mono <C> 0.5067 <C> - <C> 0.668 <C> 0.598 <C> Mono <C> 0.3912 <C> - <C> 0.524 <C> 0.4840 <R> <C> TOP-1 <C> 0.3655 <C> 72.13 <C> 0.4440 <C> 0.4220 <C> TOP-1 <C> 0.2244 <C> 57.36 <C> 0.3080 <C> 0.2500 <R> <C> UNIFORM <C> 0.3280 <C> 64.73 <C> 0.3720 <C> 0.3400 <C> UNIFORM <C> 0.2367 <C> 60.51 <C> 0.2558 <C> 0.2349 <R> <C> PARALLEL <C> 0.2006 <C> 39.59 <C> 0.292 <C> 0.2640 <C> PARALLEL <C> 0.2042 <C> 52.20 <C> 0.2680 <C> 0.2420 <R> <C> BiCTM <C> 0.3709 <C> 73.20 <C> [BOLD] 0.4840 <C> [BOLD] 0.4440 <C> BiCTM <C> 0.2351 <C> 60.10 <C> 0.3160 <C> 0.2760 <R> <C> JCLTRLM <C> 0.2734 <C> 53.96 <C> 0.4040 <C> 0.3500 <C> JCLTRLM <C> 0.1520 <C> 38.85 <C> 0.2160 <C> 0.1880 <R> <C> CLWETM23 [ITALIC] α=0 <C> 0.3598 <C> 71.01 <C> 0.4520 <C> 0.3980 <C> CLWETM12345 [ITALIC] α=0 <C> 0.2524 <C> 64.52 <C> 0.3240 <C> 0.2800 <R> <C> MIXWETM235 [ITALIC] α∗ <C> 0.3677 <C> 72.57 <C> 0.4280 <C> 0.4080 <C> MIXWETM12345 [ITALIC] α∗ <C> [BOLD] 0.2652 <C> [BOLD] 67.79 <C> 0.3400 <C> [BOLD] 0.3040 <R> <C> CLWETM123567 [ITALIC] α∗ <C> [BOLD] 0.3866 <C> [BOLD] 76.30 <C> 0.4800 <C> 0.4120 <C> CLWETM12345 [ITALIC] α∗ <C> 0.2608 <C> 66.67 <C> [BOLD] 0.3440 <C> 0.2960 <CAP> Table 2: Comparison of different CLIR methods. Superscripts indicate that the MAP improvements are statistically significant.
<R> <C> [BOLD] DEPREL <C> [BOLD] Error <C> [BOLD] Percentage <R> <C> obl <C> 1,294 <C> 30.15% <R> <C> acl <C> 961 <C> 22.39% <R> <C> nummod <C> 777 <C> 18.1% <R> <C> advcl <C> 462 <C> 10.76% <R> <C> goeswith <C> 203 <C> 4.73% <R> <C> conj <C> 99 <C> 2.31% <R> <C> compound <C> 96 <C> 2.24% <R> <C> flat <C> 91 <C> 2.12% <R> <C> ccomp <C> 77 <C> 1.79% <R> <C> etc <C> 232 <C> 5.41% <R> <C> Total <C> 4,292 <C> 100% <CAP> Table 6: DEPREL error of PKT-UD v2018.
<R> <C> [ITALIC] Data Split: <C> F0M0 % gend. <C> F0M0 % male <C> F0M0 F1 <C> F0M+ % gend. <C> F0M+ % male <C> F0M+ F1 <C> F+M0 % gend. <C> F+M0 % male <C> F+M0 F1 <C> F+M+ % gend. <C> F+M+ % male <C> F+M+ F1 <C> All F1 <R> <C> [ITALIC] Model <C> words <C> bias <C> score <C> words <C> bias <C> score <C> words <C> bias <C> score <C> words <C> bias <C> score <C> score <R> <C> Gold Lbl <C> 0 <C> 0 <C> - <C> 4.11 <C> 100 <C> - <C> 4.03 <C> 0 <C> - <C> 6.67 <C> 50.71 <C> - <C> - <R> <C> Baseline <C> 2.37 <C> 88.39 <C> 11.24 <C> 3.66 <C> 90.26 <C> 11.77 <C> 2.44 <C> 77.99 <C> 11.54 <C> 3.05 <C> 80.05 <C> 11.43 <C> 11.42 <R> <C> [ITALIC] ALL F0M0 <C> 0.14 <C> 64.19 <C> 11.72 <C> 0.24 <C> 80.11 <C> 11.51 <C> 0.22 <C> 25.0 <C> 11.63 <C> 0.23 <C> 81.58 <C> 10.72 <C> 11.61 <R> <C> [ITALIC] ALL F0M+ <C> 6.47 <C> 97.97 <C> 9.58 <C> 6.59 <C> 97.94 <C> 12.77 <C> 7.22 <C> 96.33 <C> 10.0 <C> 6.27 <C> 97.52 <C> 12.21 <C> 10.6 <R> <C> [ITALIC] ALL F+M0 <C> 4.77 <C> 11.66 <C> 10.27 <C> 5.12 <C> 15.84 <C> 10.94 <C> 5.84 <C> 7.13 <C> 11.28 <C> 5.03 <C> 13.64 <C> 11.23 <C> 10.57 <R> <C> [ITALIC] ALL F+M+ <C> 9.53 <C> 53.34 <C> 8.89 <C> 9.6 <C> 55.35 <C> 11.19 <C> 9.42 <C> 48.65 <C> 10.5 <C> 8.81 <C> 50.94 <C> 12.22 <C> 9.79 <CAP> Table 10: Performance of the ALL debiasing model controlled by indicating specific bins for all examples at test time. We report results for each possible conditioning bin choice. Across bins, the model maintains performance as measured by F1 whilst radically changing the genderedness of the language generated.
<R> <C> POS <C> Vanilla <C> Rand <C> Amnesic <C> Δ <R> <C> verb <C> 46.72 <C> 44.85 <C> 34.99 <C> 11.73 <R> <C> noun <C> 42.91 <C> 38.94 <C> 34.26 <C> 8.65 <R> <C> adposition <C> 73.80 <C> 72.21 <C> 37.86 <C> 35.93 <R> <C> determiner <C> 82.29 <C> 83.53 <C> 16.64 <C> 65.66 <R> <C> numeral <C> 40.32 <C> 40.19 <C> 33.41 <C> 6.91 <R> <C> punctuation <C> 80.71 <C> 81.02 <C> 47.03 <C> 33.68 <R> <C> particle <C> 96.40 <C> 95.71 <C> 18.74 <C> 77.66 <R> <C> conjunction <C> 78.01 <C> 72.94 <C> 4.28 <C> 73.73 <R> <C> adverb <C> 39.84 <C> 34.11 <C> 23.71 <C> 16.14 <R> <C> pronoun <C> 70.29 <C> 61.93 <C> 33.23 <C> 37.06 <R> <C> adjective <C> 46.41 <C> 42.63 <C> 34.56 <C> 11.85 <R> <C> other <C> 70.59 <C> 76.47 <C> 52.94 <C> 17.65 <CAP> Table 3: Masked, Tag removal, fine-grained lm analysis. Removing POS (tag) information and testing how specific words, accumulating by their label. Δ is the difference in performance between the Vanilla and Amnesic scores.
<R> <C> [BOLD] Model <C> [BOLD] Movie Relevance <C> [BOLD] Movie Relevance <C> [BOLD] Movie Diversity <C> [BOLD] Movie Diversity <C> [BOLD] Ubuntu Relevance <C> [BOLD] Ubuntu Relevance <C> [BOLD] Ubuntu Diversity <C> [BOLD] Ubuntu Diversity <R> <C> [BOLD] Model <C> BLEU <C> ROUGE <C> DIST-1/2 <C> NASL <C> BLEU <C> ROUGE <C> DIST-1/2 <C> NASL <R> <C> [BOLD] DLGNet-117M <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Single-turn Joint with BPE <C> ∼0.0 <C> ∼0.0 <C> 0.0400/0.1502 <C> 0.072 <C> ∼0.0 <C> 0.0004 <C> 0.1946/0.4636 <C> 0.064 <R> <C> Single-turn Conditional with BPE <C> 0.0013 <C> 0.0296 <C> 0.0134/0.0482 <C> 3.582 <C> ∼0.0 <C> 0.0083 <C> 0.0723/0.1470 <C> 0.890 <R> <C> Multi-turn Joint with BPE <C> 0.1825 <C> 0.1321 <C> 0.0346/0.0838 <C> 0.610 <C> 0.0012 <C> 0.1172 <C> 0.1719/0.3482 <C> 0.2937 <R> <C> Multi-turn Conditional with BPE <C> 0.0096 <C> 0.0628 <C> 0.0088/0.0394 <C> 3.425 <C> 0.0048 <C> 0.0766 <C> 0.0500/0.1454 <C> 2.372 <R> <C> Multi-turn Joint with Basic Tokenizer <C> 0.0518 <C> 0.0630 <C> 0.0176/0.0540 <C> 1.101 <C> 0.0030 <C> 0.0384 <C> 0.0465/0.0949 <C> 0.566 <R> <C> Multi-turn Conditional with Basic Tokenizer <C> 0.0149 <C> 0.1628 <C> 0.0394/0.1770 <C> 1.472 <C> ∼0.0 <C> 0.0136 <C> 0.2211/0.4192 <C> 0.281 <R> <C> [BOLD] DLGNet-345M <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Single-turn Joint with BPE <C> ∼0.0 <C> ∼0.0 <C> ∼0.0/∼0.0 <C> 0.072 <C> ∼0.0 <C> 0.0006 <C> 0.4741/0.9760 <C> 0.061 <R> <C> Single-turn Conditional with BPE <C> 0.0006 <C> 0.0212 <C> 0.0010/0.0419 <C> 3.582 <C> 0.0004 <C> 0.0158 <C> 0.0721/0.1671 <C> 3.437 <R> <C> Multi-turn Joint with BPE <C> 0.0449 <C> 0.1931 <C> 0.0460/0.1273 <C> 0.531 <C> ∼0.0 <C> 0.0121 <C> 0.3323/0.4406 <C> 0.227 <R> <C> Multi-turn Conditional with BPE <C> 0.0010 <C> 0.0125 <C> 0.0091/0.0422 <C> 3.918 <C> 0.0004 <C> 0.0158 <C> 0.0721/0.1671 <C> 4.108 <R> <C> Multi-turn Joint with Basic Tokenizer <C> 0.0376 <C> 0.1389 <C> 0.0232/0.0654 <C> 0.543 <C> 0.0042 <C> 0.0341 <C> 0.0568/0.1299 <C> 0.552 <R> <C> Multi-turn Conditional with Basic Tokenizer <C> 0.0057 <C> 0.0970 <C> 0.1568/0.3785 <C> 0.331 <C> 0.0015 <C> 0.0345 <C> 0.1555/0.3990 <C> 0.470 <CAP> Table 3: Ablation Performance of DLGNet Models with Static Padding
<R> <C> [BOLD] Train/Test <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1 <R> <C> WWW/KDD <C> 0.2551 <C> 0.1921 <C> 0.2012 <R> <C> KDD/WWW <C> 0.1956 <C> 0.1553 <C> 0.1583 <CAP> Table 7: Performance across datasets
<R> <C> [BOLD] Model <C> [BOLD] MR <C> [BOLD] CR <C> [BOLD] SUBJ <C> [BOLD] MPQA <C> [BOLD] SST <C> [BOLD] TREC <C> [BOLD] MRPC <C> [BOLD] SK-E <C> [BOLD] SK-R <C> [BOLD] STSB <C> [BOLD] AVG <R> <C> [ITALIC] Unsupervised Methods <C> [ITALIC] Unsupervised Methods <C> [ITALIC] Unsupervised Methods <C> [ITALIC] Unsupervised Methods <C> [ITALIC] Unsupervised Methods <C> [ITALIC] Unsupervised Methods <C> [ITALIC] Unsupervised Methods <C> [ITALIC] Unsupervised Methods <C> [ITALIC] Unsupervised Methods <C> [ITALIC] Unsupervised Methods <C> [ITALIC] Unsupervised Methods <C> [EMPTY] <R> <C> LangMod (Ours) <C> 72.1 <C> 72.0 <C> 87.8 <C> 88.1 <C> 77.4 <C> 75.0 <C> 75.4 <C> 77.7 <C> 70.3 <C> 54.4 <C> 75.0 <R> <C> Untrained LSTM <C> 77.1 <C> 79.3 <C> 91.2 <C> 89.1 <C> 81.8 <C> 82.8 <C> 71.6 <C> 85.3 <C> 82.0 <C> 71.0 <C> 81.1 <R> <C> FastText BoW <C> 78.2 <C> 80.2 <C> 91.8 <C> 88.0 <C> 82.3 <C> 83.4 <C> 74.4 <C> 82.0 <C> 78.9 <C> 70.2 <C> 80.9 <R> <C> SkipThought-LN <C> 79.4 <C> 83.1 <C> 93.7 <C> 89.3 <C> 82.9 <C> 88.4 <C> 72.4 <C> 85.8 <C> 79.5 <C> 72.1 <C> 82.7 <R> <C> QuickThoughts <C> 82.4 <C> 86.0 <C> 94.8 <C> 90.2 <C> 87.6 <C> 92.4 <C> 76.9 <C> - <C> 87.4 <C> - <C> - <R> <C> [ITALIC] Our Methods <C> [ITALIC] Our Methods <C> [ITALIC] Our Methods <C> [ITALIC] Our Methods <C> [ITALIC] Our Methods <C> [ITALIC] Our Methods <C> [ITALIC] Our Methods <C> [ITALIC] Our Methods <C> [ITALIC] Our Methods <C> [ITALIC] Our Methods <C> [ITALIC] Our Methods <C> [EMPTY] <R> <C> ConsSent-C(4) <C> 80.1 <C> 83.7 <C> 93.6 <C> 89.5 <C> 83.1 <C> 90.0 <C> 75.9 <C> [BOLD] 86.0 <C> 83.2 <C> 74.4 <C> 84.0 <R> <C> ConsSent-N(3) <C> 80.1 <C> 84.2 <C> 93.8 <C> 89.5 <C> [BOLD] 83.4 <C> 90.8 <C> [BOLD] 77.3 <C> 86.1 <C> 83.8 <C> [BOLD] 75.8 <C> [BOLD] 84.4 <R> <C> ConsSent-D(5) <C> 79.8 <C> 83.9 <C> 93.3 <C> 90.1 <C> 82.5 <C> 91.2 <C> 74.6 <C> 83.2 <C> 83.4 <C> 66.1 <C> 82.8 <R> <C> ConsSent-P(3) <C> 80.0 <C> 83.2 <C> 93.4 <C> 89.9 <C> 82.8 <C> 92.2 <C> 75.4 <C> 84.0 <C> 83.1 <C> 68.6 <C> 83.3 <R> <C> ConsSent-I(3) <C> [BOLD] 80.4 <C> 83.4 <C> 93.4 <C> 90.1 <C> 83.0 <C> 92.2 <C> 75.5 <C> 83.4 <C> 85.0 <C> 70.9 <C> 83.7 <R> <C> ConsSent-R(2) <C> 79.9 <C> [BOLD] 84.3 <C> 93.5 <C> 90.2 <C> [BOLD] 83.4 <C> 92.6 <C> 75.9 <C> 84.2 <C> 85.2 <C> 72.0 <C> 84.1 <R> <C> ConsSent-MT(3) <C> 80.2 <C> [BOLD] 84.3 <C> [BOLD] 94.4 <C> [BOLD] 90.4 <C> 83.1 <C> [BOLD] 93.1 <C> 76.7 <C> 83.4 <C> [BOLD] 86.5 <C> 72.2 <C> [BOLD] 84.4 <R> <C> Ensemble <C> 81.6 <C> 85.1 <C> 94.4 <C> 90.6 <C> 85.2 <C> 93.8 <C> 77.7 <C> 86.8 <C> 87.2 <C> 77.3 <C> 86.0 <R> <C> [ITALIC] Supervised Methods <C> [ITALIC] Supervised Methods <C> [ITALIC] Supervised Methods <C> [ITALIC] Supervised Methods <C> [ITALIC] Supervised Methods <C> [ITALIC] Supervised Methods <C> [ITALIC] Supervised Methods <C> [ITALIC] Supervised Methods <C> [ITALIC] Supervised Methods <C> [ITALIC] Supervised Methods <C> [ITALIC] Supervised Methods <C> [EMPTY] <R> <C> InferSent <C> 81.1 <C> 86.3 <C> 92.4 <C> 90.2 <C> 84.6 <C> 88.2 <C> 76.2 <C> 86.3 <C> 88.4 <C> 75.8 <C> 85.0 <R> <C> MultiTask <C> 82.5 <C> 87.7 <C> 94.0 <C> 90.9 <C> 83.2 <C> 93.0 <C> 78.6 <C> 87.8 <C> 88.8 <C> 78.9 <C> 86.5 <CAP> Table 2: Performance of ConsSent on the transfer tasks in the SentEval benchmark. SkipThought is described in (Kiros et al., 2015), QuickThoughts in (Logeswaran & Lee, 2018) and MultiTask in Subramanian et al. (2018) and InferSent in Conneau et al. (2017). The other numbers (except LangMod) have been taken from Conneau et al. (2018). SK-R and SK-E stand for SICK-R and SICK-E respectively. AVG is a simple average over all the tasks. Bold indicates best result among our non-ensemble models and underline indicates best overall for unsupervised methods.
<R> <C> [BOLD] Classifier <C> [BOLD] Accuracy (%) <C> [BOLD] F1-Score (%) <R> <C> Stochastic Gradient Descent <C> 80.7 <C> 80.0 <R> <C> Naive Bayes <C> 81.5 <C> 81.2 <R> <C> Decision Tree <C> 91.8 <C> 91.4 <R> <C> [BOLD] Random Forest <C> [BOLD] 92.0 <C> [BOLD] 91.9 <CAP> Table 1: Average classifier performance during 10-fold cross-validation on the training/validation set. Results shown are for the best performing parameters obtained using a grid search.
<R> <C> [EMPTY] <C> BERT  [BOLD] MRPC <C> BERT  [BOLD] MNLI <C> BERT  [BOLD] SST <C> RoBERTa  [BOLD] MRPC <C> RoBERTa  [BOLD] MNLI <C> RoBERTa  [BOLD] SST <R> <C> [BOLD] Prep <C> 16 <C> 178 <C> 36 <C> 15 <C> 103 <C> 43 <R> <C> [BOLD] Art/Det <C> 5 <C> 270 <C> 20 <C> 7 <C> 228 <C> 28 <R> <C> [BOLD] Wchoice <C> 93 <C> 1129 <C> 233 <C> 64 <C> 772 <C> 195 <R> <C> [BOLD] Vform <C> 8 <C> 231 <C> 26 <C> 9 <C> 314 <C> 37 <R> <C> [BOLD] SVA <C> 57 <C> 538 <C> 83 <C> 31 <C> 388 <C> 83 <R> <C> [BOLD] Nn <C> 14 <C> 128 <C> 13 <C> 3 <C> 84 <C> 13 <R> <C> [BOLD] Worder <C> 0 <C> 62 <C> 28 <C> 0 <C> 43 <C> 28 <R> <C> [BOLD] Trans <C> 5 <C> 70 <C> 25 <C> 5 <C> 31 <C> 25 <CAP> Table 4: Numbers of times each error type is chosen in successful attacks. We find that Wchoice and SVA are more harmful.
<R> <C> [EMPTY] <C> [BOLD] Prep <C> [BOLD] Artordet <C> [BOLD] Vform <C> [BOLD] Nn <C> [BOLD] Wchoice <C> [BOLD] Trans <C> [BOLD] SVA <C> [BOLD] Worder <R> <C> ELMo, layer 0 <C> 62.6 <C> 65.0 <C> 69.6 <C> 67.7 <C> 74.5 <C> 67.5 <C> 72.1 <C> 47.6 <R> <C> ELMo, layer 1 <C> [BOLD] 90.6 <C> [BOLD] 84.7 <C> [BOLD] 87.2 <C> [BOLD] 82.9 <C> [BOLD] 83.9 <C> [BOLD] 80.6 <C> [BOLD] 93.1 <C> [BOLD] 71.2 <R> <C> ELMo, layer 2 <C> 84.7 <C> 77.0 <C> 79.4 <C> 79.7 <C> 82.6 <C> 74.4 <C> 89.9 <C> 68.5 <R> <C> BERT, layer 0 <C> 62.5 <C> 60.8 <C> 67.4 <C> 64.6 <C> 73.9 <C> 69.5 <C> 70.3 <C> 48.2 <R> <C> BERT, layer 1 <C> 68.0 <C> 63.4 <C> 69.3 <C> 70.3 <C> 75.0 <C> 71.5 <C> 78.4 <C> 52.2 <R> <C> BERT, layer 2 <C> 74.4 <C> 67.0 <C> 75.3 <C> 74.8 <C> 76.7 <C> 73.1 <C> 84.4 <C> 62.0 <R> <C> BERT, layer 3 <C> 80.5 <C> 75.0 <C> 83.4 <C> 73.7 <C> 78.5 <C> 76.3 <C> 89.2 <C> 69.8 <R> <C> BERT, layer 4 <C> 82.7 <C> 80.7 <C> 83.6 <C> 77.7 <C> 82.6 <C> 79.6 <C> 90.6 <C> 72.4 <R> <C> BERT, layer 5 <C> 85.2 <C> 83.8 <C> 85.4 <C> 84.3 <C> 84.5 <C> 81.8 <C> 91.7 <C> 71.9 <R> <C> BERT, layer 6 <C> 88.2 <C> 86.6 <C> 85.8 <C> 86.7 <C> 84.5 <C> 82.6 <C> 90.9 <C> 73.4 <R> <C> BERT, layer 7 <C> 91.3 <C> 88.1 <C> 90.2 <C> 86.5 <C> [BOLD] 86.9 <C> 83.9 <C> [BOLD] 95.3 <C> 73.4 <R> <C> BERT, layer 8 <C> [BOLD] 92.5 <C> [BOLD] 88.3 <C> [BOLD] 91.4 <C> [BOLD] 88.4 <C> 86.3 <C> [BOLD] 85.5 <C> 94.5 <C> [BOLD] 73.8 <R> <C> BERT, layer 9 <C> 91.4 <C> 86.3 <C> 89.9 <C> 87.4 <C> 85.6 <C> 84.9 <C> 94.4 <C> 72.4 <R> <C> BERT, layer 10 <C> 90.8 <C> 87.4 <C> 88.2 <C> 87.0 <C> 86.1 <C> 84.8 <C> 94.9 <C> 71.8 <R> <C> BERT, layer 11 <C> 90.0 <C> 84.9 <C> 88.1 <C> 86.6 <C> 85.6 <C> 84.3 <C> 94.2 <C> 69.5 <R> <C> BERT, layer 12 <C> 88.4 <C> 85.6 <C> 88.1 <C> 84.3 <C> 84.0 <C> 82.6 <C> 93.3 <C> 68.1 <R> <C> RoBERTa, layer 0 <C> 61.9 <C> 65.9 <C> 69.7 <C> 67.1 <C> 75.1 <C> 69.1 <C> 68.3 <C> 50.9 <R> <C> RoBERTa, layer 1 <C> 78.3 <C> 74.7 <C> 84.6 <C> 77.6 <C> 80.2 <C> 75.9 <C> 88.4 <C> 67.8 <R> <C> RoBERTa, layer 2 <C> 85.2 <C> 79.4 <C> 88.7 <C> 83.0 <C> 83.3 <C> 78.8 <C> 90.9 <C> 71.8 <R> <C> RoBERTa, layer 3 <C> 89.3 <C> 85.7 <C> 90.6 <C> 86.9 <C> 87.0 <C> 84.1 <C> 94.3 <C> 72.6 <R> <C> RoBERTa, layer 4 <C> 90.2 <C> 88.7 <C> 91.8 <C> 88.7 <C> 86.2 <C> 86.4 <C> 94.5 <C> 74.5 <R> <C> RoBERTa, layer 5 <C> 91.4 <C> 89.1 <C> 92.9 <C> 90.5 <C> 89.0 <C> 87.1 <C> 95.5 <C> 74.5 <R> <C> RoBERTa, layer 6 <C> 93.4 <C> 91.3 <C> 91.9 <C> 91.4 <C> 88.9 <C> 86.8 <C> 95.0 <C> 75.3 <R> <C> RoBERTa, layer 7 <C> 93.9 <C> 90.5 <C> 91.8 <C> 90.4 <C> 88.2 <C> 86.9 <C> 94.6 <C> 74.7 <R> <C> RoBERTa, layer 8 <C> 93.9 <C> 91.1 <C> [BOLD] 93.4 <C> 92.3 <C> 88.0 <C> 87.2 <C> 94.4 <C> 75.9 <R> <C> RoBERTa, layer 9 <C> 94.3 <C> 90.6 <C> 92.5 <C> 92.1 <C> 89.4 <C> 88.0 <C> [BOLD] 95.7 <C> 74.7 <R> <C> RoBERTa, layer 10 <C> 94.4 <C> [BOLD] 92.0 <C> 93.3 <C> [BOLD] 92.3 <C> [BOLD] 89.9 <C> 88.1 <C> 95.0 <C> 75.1 <R> <C> RoBERTa, layer 11 <C> [BOLD] 95.3 <C> 91.5 <C> 93.3 <C> 89.4 <C> 88.8 <C> [BOLD] 88.2 <C> 95.2 <C> [BOLD] 76.0 <R> <C> RoBERTa, layer 12 <C> 94.5 <C> 91.1 <C> 92.7 <C> 88.3 <C> 87.3 <C> 87.9 <C> 95.3 <C> 74.8 <CAP> Table 7: Results of the accuracy on the binary linguistic acceptability probing task for individual layers of ELMo, BERT, and RoBERTa.
<R> <C> Strategy <C> Completion Rate <C> Num. of Turns <R> <C> Norm. Inputs <C> 0.94 <C> 16.06 <R> <C> Rand. Inputs <C> 0.82 <C> 22.87 <R> <C> Rand. w/ Entity <C> 0.95 <C> 17.19 <R> <C> Confusing Entity <C> 0.77 <C> 24.11 <CAP> Table 8: Adversarial Results on DynoNet.
<R> <C> [BOLD] Embeddings <C> [BOLD] Acc. <C> [BOLD] TPRgap <C> [BOLD] TNRgap <R> <C> GloVe <C> 0.818 <C> 0.091 <C> 0.0031 <R> <C> Strongly debias <C> 0.817 <C> 0.069 <C> 0.0023 <R> <C> Project only <C> 0.815 <C> 0.103 <C> 0.0032 <R> <C> Equalize only <C> 0.817 <C> 0.080 <C> 0.0029 <CAP> Table 4: Ablation study: Metrics for projection and equalization step
<R> <C> Dataset <C> MLE <C> SeqGAN <C> MaliGAN <C> RankGAN <C> SAL <R> <C> COCO <C> 2.96±0.51 <C> 3.26±0.56 <C> 3.14±0.57 <C> 2.91±0.62 <C> [BOLD] 3.84±0.56 (p<=0.01) <R> <C> WMT NEWS <C> 2.35±0.86 <C> 2.19±0.88 <C> 2.24±0.87 <C> 2.05±0.91 <C> [BOLD] 2.65±0.89 (p<=0.01) <CAP> Table 5: Human evaluation results of different models in both datasets. Scores are between 1-5, higher score indicates better quality.
<R> <C> [BOLD] Class <C> [BOLD] Training set <C> [BOLD] Test set <C> [BOLD] Total <R> <C> Positive <C> 5.745  [ITALIC] (44%) <C> 903 (45%) <C> 6.648 <R> <C> Neutral <C> 3.414  [ITALIC] (26%) <C> 512 (25%) <C> 3.926 <R> <C> Negative <C> 3.840  [ITALIC] (29%) <C> 586 (29%) <C> 4.426 <R> <C> [BOLD] Total <C> 12.999 <C> 2.001 <C> 15.000 <CAP> Table 1: Amount of documents in the corpus in each class.
<R> <C> [EMPTY] <C> #Sentences <C> IAA <C> UniqueExpressions <C> Train/Test <C> #Animate <R> <C> [ITALIC] Stories <C> 5835 <C> Cohen’s  [ITALIC] κ 0.99 <C> 4277 <C> 4084/1751 <C> 2080 <R> <C> [ITALIC] 19thC Machines <C> 393 <C> Krippendorff’s  [ITALIC] α 0.74 <C> 13 <C> 99/294 <C> 183 <CAP> Table 3: Comparison between the Stories and 19thC Machines animacy datasets.
<R> <C> [EMPTY] <C> [ITALIC] Stories Precision <C> [ITALIC] Stories Recall <C> [ITALIC] Stories F-Score <C> [ITALIC] Stories Map <C> [ITALIC] 19thC Machines Precision <C> [ITALIC] 19thC Machines Recall <C> [ITALIC] 19thC Machines F-Score <C> [ITALIC] 19thC Machines Map <R> <C> Most frequent class <C> 0.317 <C> 0.5 <C> 0.388 <C> 0.418 <C> 0.268 <C> 0.5 <C> 0.349 <C> 0.505 <R> <C> SVM TFIDF: targetExp <C> 0.878 <C> 0.867 <C> 0.873 <C> 0.896 <C> 0.268 <C> 0.5 <C> 0.349 <C> 0.505 <R> <C> SVM WordEmb: targetExp <C> 0.9 <C> 0.907 <C> 0.903 <C> 0.932 <C> 0.428 <C> 0.442 <C> 0.435 <C> 0.462 <R> <C> BERTClassifier: targetExp <C> [BOLD] 0.931 <C> [BOLD] 0.94 <C> [BOLD] 0.935 <C> [BOLD] 0.944 <C> 0.453 <C> 0.49 <C> 0.471 <C> 0.648 <R> <C> SVM TFIDF: targetExp + ctxt <C> 0.776 <C> 0.772 <C> 0.774 <C> 0.781 <C> 0.504 <C> 0.502 <C> 0.503 <C> 0.504 <R> <C> SVM WordEmb: targetExp + ctxt <C> 0.781 <C> 0.783 <C> 0.782 <C> 0.79 <C> 0.58 <C> 0.557 <C> 0.569 <C> 0.579 <R> <C> BERTClassifier: targetExp + ctxt <C> 0.91 <C> 0.918 <C> 0.914 <C> 0.929 <C> 0.559 <C> 0.538 <C> 0.549 <C> 0.559 <R> <C> SVM TFIDF: maskedExp + ctxt <C> 0.671 <C> 0.66 <C> 0.666 <C> 0.642 <C> 0.548 <C> 0.548 <C> 0.548 <C> 0.511 <R> <C> SVM WordEmb: maskedExp + ctxt <C> 0.651 <C> 0.655 <C> 0.653 <C> 0.616 <C> 0.561 <C> 0.526 <C> 0.543 <C> 0.528 <R> <C> BERTClassifier: maskedExp + ctxt <C> 0.76 <C> 0.766 <C> 0.763 <C> 0.77 <C> 0.583 <C> 0.583 <C> 0.583 <C> 0.539 <R> <C> MaskPredict: WordEmb <C> 0.709 <C> 0.709 <C> 0.709 <C> 0.689 <C> 0.441 <C> 0.487 <C> 0.463 <C> 0.407 <R> <C> MaskPredict: BERT-base <C> 0.767 <C> 0.767 <C> 0.767 <C> 0.742 <C> 0.741 <C> 0.74 <C> 0.741 <C> 0.764 <R> <C> MaskPredict: BERT-base +ctxt <C> 0.834 <C> 0.845 <C> 0.839 <C> 0.845 <C> 0.762 <C> 0.76 <C> 0.761 <C> 0.841 <R> <C> MaskPredict: 19thcBERT +ctxt <C> – <C> – <C> – <C> – <C> [BOLD] 0.783 <C> [BOLD] 0.774 <C> [BOLD] 0.779 <C> [BOLD] 0.853 <CAP> Table 5: Evaluation results on the Stories and 19thC Machines dataset.
<R> <C> Model <C> AliMe MRR <C> AliMe R10@1 <C> AliMe R10@2 <C> AliMe R10@5 <C> AliMe R2@1 <C> Quora MRR <C> Quora R10@1 <C> Quora R10@2 <C> Quora R10@5 <C> Quora R2@1 <R> <C> Q-Q Mean <C> 0.6122 <C> 0.5050 <C> 0.5623 <C> 0.7287 <C> 0.8473 <C> 0.8350 <C> 0.7847 <C> 0.8133 <C> 0.8973 <C> 0.9480 <R> <C> Q-Q Max <C> 0.6470 <C> 0.5477 <C> 0.6000 <C> 0.7590 <C> 0.8523 <C> 0.8438 <C> 0.7980 <C> 0.8227 <C> 0.8980 <C> 0.9493 <R> <C> Bag-Con <C> 0.6552 <C> 0.5610 <C> 0.6087 <C> 0.7607 <C> 0.8553 <C> 0.8026 <C> 0.7420 <C> 0.7800 <C> 0.8740 <C> 0.9287 <R> <C> Base <C> 0.6845 <C> 0.6027 <C> 0.6397 <C> 0.7700 <C> 0.8707 <C> 0.8184 <C> 0.7643 <C> 0.7973 <C> 0.8800 <C> 0.9337 <R> <C> Base+MC <C> 0.6936 <C> 0.6137 <C> 0.6497 <C> 0.7807 <C> [BOLD] 0.8823 <C> 0.8640 <C> 0.8247 <C> 0.8480 <C> 0.9083 <C> [BOLD] 0.9587 <R> <C> Base+BR <C> 0.6913 <C> 0.6103 <C> 0.6443 <C> 0.7833 <C> 0.8783 <C> 0.8628 <C> 0.8213 <C> 0.8477 <C> 0.9123 <C> 0.9497 <R> <C> Base+(BR w/o Cov) <C> 0.6849 <C> 0.6013 <C> 0.6410 <C> 0.7810 <C> 0.8727 <C> 0.8280 <C> 0.7763 <C> 0.8093 <C> 0.8833 <C> 0.9430 <R> <C> QBM (Base+BR+MC) <C> [BOLD] 0.7007‡ \lx @ [ITALIC] sectionsign <C> [BOLD] 0.6197‡ \lx @ [ITALIC] sectionsign <C> 0. [BOLD] 6600‡ \lx @ [ITALIC] sectionsign <C> [BOLD] 0.7923‡ \lx @ [ITALIC] sectionsign <C> [BOLD] 0.8823‡ <C> [BOLD] 0.8656‡ \lx @ [ITALIC] sectionsign <C> [BOLD] 0.8253‡ \lx @ [ITALIC] sectionsign <C> [BOLD] 0.8510‡ \lx @ [ITALIC] sectionsign <C> [BOLD] 0.9137 \lx @ [ITALIC] sectionsign <C> 0.9520‡ \lx @ [ITALIC] sectionsign <CAP> Table 1. Results of models and baselines with ablation study. MC and BR denote Mutual Coverage and Bag Representation respectively. “BR w/o Cov” denotes Bag Representation component without coverage module. ‡ and § means the results are significant with p-value<0.05 measured by the Student’s paired t-test over the best baseline and the base model respectively.
<R> <C> [EMPTY] <C> [EMPTY] <C> Term Classification Laptops <C> Term Classification Laptops <C> Term Classification Restaurants <C> Term Classification Restaurants <C> Category Classification Restaurants <C> Category Classification Restaurants <C> Category Classification SemEval 14+15 <C> Category Classification SemEval 14+15 <C> [EMPTY] <R> <C> [BOLD] Model <C> Aspect <C> 3-way <C> Binary <C> 3-way <C> Binary <C> 3-way <C> Binary <C> 3-way <C> Binary <C> Avg <R> <C> Majority <C> No <C> 53.45 <C> 72.71 <C> 65.00 <C> 78.79 <C> 67.52 <C> 74.40 <C> 64.16 <C> 75.12 <C> 68.89 <R> <C> NBOW <C> No <C> 58.62 <C> 73.34 <C> 67.49 <C> 82.47 <C> 70.81 <C> 78.61 <C> 70.92 <C> 77.18 <C> 72.43 <R> <C> LSTM <C> No <C> 61.75 <C> 78.25 <C> 67.94 <C> 82.03 <C> 73.38 <C> 79.97 <C> 75.96 <C> 79.92 <C> 74.90 <R> <C> TD-LSTM <C> Yes <C> 62.38 <C> 79.31 <C> 69.73 <C> 84.41 <C> 79.97 <C> 75.96 <C> 79.92 <C> 74.90 <C> 75.63 <R> <C> AT-LSTM <C> Yes <C> 65.83 <C> 78.25 <C> 74.37 <C> 84.74 <C> 77.90 <C> 84.87 <C> 76.16 <C> 81.28 <C> 77.93 <R> <C> ATAE-LSTM <C> Yes <C> 60.34 <C> 74.20 <C> 70.71 <C> 84.52 <C> 77.80 <C> 83.85 <C> 74.08 <C> 78.96 <C> 75.56 <R> <C> AF-LSTM (MUL) <C> Yes <C> 66.14 <C> 83.37 <C> 75.35 <C> 86.47 <C> 79.96 <C> 81.71 <C> 77.44 <C> 80.44 <C> 78.86 <R> <C> AF-LSTM (CORR) <C> Yes <C> 64.89 <C> 79.96 <C> 74.76 <C> 86.91 <C> 80.47 <C> 86.58 <C> 74.68 <C> [BOLD] 81.60 <C> 78.73 <R> <C> AF-LSTM (CONV) <C> Yes <C> [BOLD] 68.81 <C> [BOLD] 83.58 <C> [BOLD] 75.44 <C> [BOLD] 87.78 <C> [BOLD] 81.29 <C> [BOLD] 87.26 <C> [BOLD] 78.44 <C> 81.49 <C> [BOLD] 80.51 <CAP> Table 2: Comparisons of all deep learning architectures on all datasets. Avg column reports macro-averaged results across all datasets. Best performance is in bold face. Our proposed AF-LSTM (CONV) achieves state-of-the-art performance across all four datasets and settings.
<R> <C> Models <C> Grammaticality Win (%) <C> Grammaticality Lose (%) <C> Grammaticality Tie (%) <C> [ITALIC] κ <C> Coherence Win (%) <C> Coherence Lose (%) <C> Coherence Tie (%) <C> [ITALIC] κ <R> <C> PHVM vs. Checklist <C> 59.0** <C> 23.5 <C> 17.5 <C> 0.484 <C> 54.5* <C> 42.5 <C> 3.0 <C> 0.425 <R> <C> PHVM vs. CVAE <C> 69.5** <C> 13.5 <C> 17.0 <C> 0.534 <C> 60.0** <C> 37.0 <C> 3.0 <C> 0.426 <R> <C> PHVM vs. Pointer-S2S <C> 76.5** <C> 17.0 <C> 6.5 <C> 0.544 <C> 56.5** <C> 39.0 <C> 4.5 <C> 0.414 <R> <C> PHVM vs. Link-S2S <C> 66.0** <C> 28.5 <C> 5.5 <C> 0.462 <C> 62.5** <C> 31.5 <C> 6.0 <C> 0.415 <CAP> Table 4: Manual pair-wise evaluation for advertising text generation. We conducted Sign Test for significance test. Scores marked with * mean p-value < 0.05 and ** for p-value < 0.01. κ denotes Fleiss’ kappa, all indicating moderate agreement.
<R> <C> [EMPTY] <C> [BOLD] train <C> [BOLD] dev <C> [BOLD] test <R> <C> PTB <C> 2-21 <C> 22 <C> 23 <R> <C> CTB5 <C> 001-815, <C> 886-931, <C> 816-885, <R> <C> [EMPTY] <C> 1001-1136 <C> 1148-1151 <C> 1137-1147 <CAP> Table 2: Our data splits for English and Chinese
<R> <C> [ITALIC] m <C> [BOLD] 0 <C> [BOLD] 1 <C> [BOLD] 2 <C> [BOLD] 3 <C> [BOLD] 1-2 <C> [BOLD] 1-3 <C> [BOLD] 1-4 <R> <C> English <C> 91.05 <C> [BOLD] 91.43 <C> 91.14 <C> 91.22 <C> 91.27 <C> 91.26 <C> [EMPTY] <R> <C> Chinese <C> 78.95 <C> 79.85 <C> 79.42 <C> 79.06 <C> 79.97 <C> [BOLD] 80.11 <C> 79.73 <CAP> Table 3: Effects (LAS) of different number of DLMs for English and Chinese. m = 0 refers the baseline.
<R> <C> [BOLD] Size <C> [BOLD] 0 <C> [BOLD] 5 <C> [BOLD] 10 <C> [BOLD] 20 <C> [BOLD] 30 <R> <C> English <C> 91.05 <C> [BOLD] 91.43 <C> 91.38 <C> 91.13 <C> 91.28 <R> <C> Chinese <C> 78.95 <C> 80.11 <C> [BOLD] 80.15 <C> 79.72 <C> [EMPTY] <CAP> Table 4: Effects (LAS) of DLMs extracted from different size (in million sentences) of corpus. Size = 0 refers the baseline.
<R> <C> [BOLD] System <C> [BOLD] Beam <C> [BOLD] POS <C> [BOLD] LAS <C> [BOLD] UAS <R> <C> zhang11 <C> 32 <C> 97.44 <C> 90.95 <C> 93.00 <R> <C> bohnet2012:EACL2012 <C> 80 <C> 97.44 <C> 91.19 <C> 93.27 <R> <C> martins2013tubo <C> [EMPTY] <C> 97.44 <C> 90.55 <C> 92.89 <R> <C> zhang-mcdonald:2014:P14-2 <C> [EMPTY] <C> 97.44 <C> 91.02 <C> 93.22 <R> <C> chen2014fast† <C> 1 <C> [EMPTY] <C> 89.60 <C> 91.80 <R> <C> dyer-EtAl:2015:ACL-IJCNLP† <C> 1 <C> 97.30 <C> 90.90 <C> 93.10 <R> <C> weiss-etAl:2015:ACL† <C> 8 <C> 97.44 <C> 92.05 <C> 93.99 <R> <C> andor-EtAl:2016:ACL† <C> 32 <C> 97.44 <C> 92.79 <C> 94.61 <R> <C> dozat2017deep† <C> [EMPTY] <C> [EMPTY] <C> 94.08 <C> 95.74 <R> <C> 2017arXiv170705000L† <C> [EMPTY] <C> [EMPTY] <C> [BOLD] 95.20 <C> [BOLD] 96.20 <R> <C> chen2012utilizing Baseline * <C> 8 <C> [EMPTY] <C> [EMPTY] <C> 92.10 <R> <C> chen2012utilizing DLM * <C> 8 <C> [EMPTY] <C> [EMPTY] <C> 92.76 <R> <C> Our Baseline * <C> 40 <C> 97.33 <C> 92.44 <C> 93.38 <R> <C> Our Baseline <C> 40 <C> 97.36 <C> 90.95 <C> 93.08 <R> <C> [EMPTY] <C> 80 <C> 97.34 <C> 91.05 <C> 93.28 <R> <C> [EMPTY] <C> 150 <C> 97.34 <C> 91.05 <C> 93.29 <R> <C> Our DLM <C> 40 <C> 97.38 <C> 91.41 <C> 93.59 <R> <C> [EMPTY] <C> 80 <C> 97.39 <C> 91.47 <C> 93.65 <R> <C> [EMPTY] <C> 150 <C> 97.42 <C> 91.56 <C> 93.74 <CAP> Table 5: Comparing with top performing parsers on English. (* means results that are evaluated on yamada03 conversion. † means neural network-based parsers)
<R> <C> [BOLD] System <C> [BOLD] Beam <C> [BOLD] POS <C> [BOLD] LAS <C> [BOLD] UAS <R> <C> hatori-EtAl:2011:IJCNLP-2011 <C> 64 <C> 93.94 <C> [EMPTY] <C> 81.33 <R> <C> li-EtAl:2012:PAPERS4 <C> [EMPTY] <C> 94.60 <C> 79.01 <C> 81.67 <R> <C> chen2013feature <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 83.08 <R> <C> chen2015feature <C> [EMPTY] <C> 93.61 <C> [EMPTY] <C> 82.94 <R> <C> Our Baseline <C> 40 <C> 93.99 <C> 78.49 <C> 81.52 <R> <C> [EMPTY] <C> 80 <C> 94.02 <C> 78.48 <C> 81.58 <R> <C> [EMPTY] <C> 150 <C> 93.98 <C> 78.96 <C> 82.11 <R> <C> Our DLM <C> 40 <C> 94.27 <C> 79.42 <C> 82.51 <R> <C> [EMPTY] <C> 80 <C> 94.39 <C> 79.79 <C> 82.79 <R> <C> [EMPTY] <C> 150 <C> 94.40 <C> [BOLD] 80.21 <C> [BOLD] 83.28 <CAP> Table 6: Comparing with top performing parsers on Chinese.
<R> <C> [EMPTY] <C> [BOLD] Happiness <C> [BOLD] Sadness <R> <C> Jockers <C> 28.94 <C> 21.81 <R> <C> NRC <C> 15.52 <C> 10.63 <R> <C> Bing <C> 26.83 <C> 22.94 <R> <C> Afinn <C> 28.72 <C> 23.72 <R> <C> SenticNet 4 <C> 27.65 <C> 15.44 <R> <C> LIWC “Tone” <C> 30.76 <C> 25.91 <CAP> Table 3: R2 (in %) between reported emotion values and linguistic measurement
<R> <C> Model <C> 1K <C> 10K <C> 20K <C> 100K <C> 1M <R> <C> Transformer-big <C> 0.40 <C> 2.56 <C> 9.53 <C> 22.72 <C> 29.50 <R> <C> Transformer-base <C> 0.33 <C> 1.79 <C> 8.21 <C> 21.34 <C> 29.06 <CAP> Table 2: BLEU on Ja-En (ASPEC-JE)
<R> <C> Setting <C> En+Ja <C> Ru+Ja <R> <C> MASS <C> 71.18 <C> 72.35 <R> <C> BMASS <C> 73.76 <C> 73.98 <R> <C> BRSS.F <C> 84.82 <C> 84.89 <R> <C> JASS(BMASS+BRSS.F) <C> 81.53 <C> 81.63 <R> <C> MASS+BMASS <C> 72.33 <C> - <R> <C> MASS+BRSS.F <C> 79.56 <C> - <R> <C> MASS+JASS <C> 78.62 <C> 78.85 <CAP> Table 3: Pre-training accuracy, which is the 1-gram accuracy of the pre-trained model
<R> <C> [BOLD] Settings  [BOLD] Initialization <C> [BOLD] Settings  [BOLD] Joint Training <C> [BOLD] Annotated Evidence  [BOLD] P <C> [BOLD] Annotated Evidence  [BOLD] R <C> [BOLD] Annotated Evidence  [BOLD] F1 <C> [BOLD] Retrieved Evidence (Voting)  [BOLD] P <C> [BOLD] Retrieved Evidence (Voting)  [BOLD] R <C> [BOLD] Retrieved Evidence (Voting)  [BOLD] F1 <R> <C> LM embedding <C> False <C> [BOLD] 67.53 <C> [BOLD] 80.63 <C> [BOLD] 73.50 <C> [BOLD] 72.66 <C> [BOLD] 76.83 <C> [BOLD] 74.69 <R> <C> LM embedding <C> True <C> 65.16 <C> 76.76 <C> 70.48 <C> 70.15 <C> 74.09 <C> 72.06 <R> <C> Random <C> True <C> 63.37 <C> 71.52 <C> 67.19 <C> 66.74 <C> 70.11 <C> 68.38 <CAP> Table 4: Effect of embedding initialization and training. Only fuzzy matching results are shown.
<R> <C> [BOLD] Team, Runs <C> [BOLD] EN/ES <C> [BOLD] EN/PT <C> [BOLD] ES/EN <C> [BOLD] PT/EN <R> <C> UFRGS run1 (NMT) <C> 39.62 <C> [BOLD] 39.43 <C> 43.31 <C> [BOLD] 42.58 <R> <C> UFRGS run2 (SMT) <C> [BOLD] 39.77 <C> [BOLD] 39.43 <C> [BOLD] 43.41 <C> [BOLD] 42.58 <R> <C> TGF TALP UPC run1 <C> - <C> - <C> 40.49 <C> 39.49 <R> <C> TGF TALP UPC run2 <C> - <C> - <C> 39.06 <C> 38.54 <R> <C> UHH-DS run1 <C> 31.32 <C> 34.92 <C> 36.16 <C> 41.84 <R> <C> UHH-DS run2 <C> 31.05 <C> 34.19 <C> 35.17 <C> 41.80 <R> <C> UHH-DS run3 <C> 31.33 <C> 34.49 <C> 36.05 <C> 41.79 <CAP> Table 4: Official BLEU scores for the English/Spanish and English/Portuguese language pairs in both translation directions. Bold numbers indicate the best result for each direction.
<R> <C> [BOLD] In Domain <C> ZSDG <C> Transfer <C> DAML <C> Transfer-oneshot <C> DAML-oneshot <R> <C> BLEU <C> 70.1 <C> 51.8 <C> 51.8 <C> 51.1 <C> 53.7 <R> <C> Entity F1 <C> 79.9 <C> 88.5 <C> [BOLD] 91.4 <C> 87.6 <C> 91.2 <R> <C> Epoch <C> - <C> 2.7 <C> 1.4 <C> 2.2 <C> 1.0 <R> <C> [BOLD] Unseen Slot <C> ZSDG <C> Transfer <C> DAML <C> Transfer-oneshot <C> DAML-oneshot <R> <C> BLEU <C> 68.5 <C> 43.3 (46.3) <C> 41.7 (46.3) <C> 40.8 (43.9) <C> 40.0 (41.8) <R> <C> Entity F1 <C> 74.6 <C> 78.7 (78.5) <C> 75 ( [BOLD] 79.2) <C> 70.1 (67.7) <C> 72.0 (73.0) <R> <C> Epoch <C> - <C> 2.6 (2.4) <C> 4.8 (3.4) <C> 3.2 (2.6) <C> 5.0 (3.0) <R> <C> [BOLD] Unseen NLG <C> ZSDG <C> Transfer <C> DAML <C> Transfer-oneshot <C> DAML-oneshot <R> <C> BLEU <C> 70.1 <C> 30.6 (32.4) <C> 21.5 (26.0) <C> 20.0 (21.5) <C> 19.1 (19.1) <R> <C> Entity F1 <C> 72.9 <C> 82.2 (85.0) <C> 77.5 (82.4) <C> 82.8 (86.2) <C> 69.0 ( [BOLD] 86.4) <R> <C> Epoch <C> - <C> 3.2 (3.0) <C> 3.2 (2.1) <C> 12.3 (20.3) <C> 4.7 (5.7) <R> <C> [BOLD] New Domain <C> ZSDG <C> Transfer <C> DAML <C> Transfer-oneshot <C> DAML-oneshot <R> <C> BLEU <C> 54.6 <C> 30.1 <C> 32.7 <C> 21.5 <C> 22.4 <R> <C> Entity F1 <C> 52.6 <C> 64.0 <C> [BOLD] 66.2 <C> 55.9 <C> 59.5 <R> <C> Epoch <C> - <C> 5.6 <C> 4.5 <C> 14.2 <C> 5.8 <CAP> Table 1: DAML outperforms both ZSDG and transfer learning when given similar target domain data. Even the one-shot DAML method achieves better results than ZSDG. Values in parenthesis are the results of the model with an extra step of fine-tuning on the restaurant domain in training. “In Domain” uses all three source domains (restaurant, weather and bus), while “New Domain” refers to the movie domain. “Unseen Slot” and “Unseen NLG” correspond to restaurant-slot and restaurant-style separately.
<R> <C> Description of parameter <C> Value chosen <R> <C> Dimensionality of embedding <C> 200 <R> <C> Negative Sampling Loss (n) <C> 25 <R> <C> Subsampling frequency threshold <C> 10−4 <R> <C> Simultaneous threads running <C> 16 <R> <C> Number of training iterations <C> 30 <CAP> Table 1: Chosen values for different parameters used to implement the Continuous-Bag-of Words training in Word2Vec.
<R> <C> Model <C> Performance (%MAP) <R> <C> Path-RNN <C> 22.06 <R> <C> Single-Model <C> 63.33 <R> <C> Single-Model + MTL <C> 64.81 <CAP> Table 4: Model performance when trained with a small fraction of the data.
<R> <C> Model <C> MQ <R> <C> Comp. Bilinear Diag <C> 90.4 <R> <C> Comp. Trans-E <C> 93.3 <R> <C> Our Model <C> [BOLD] 98.94 <CAP> Table 6: Performance on path queries in WordNet.
<R> <C> Models <C> En-De <C> En-Fi <R> <C> Baseline+AttY <C> 2,546 <C> 1,631 <R> <C> Baseline+AttY2D <C> 2,902 (+14.0%) <C> 1,786 (+9.5%) <R> <C> Baseline+C+AttY <C> 2,758 <C> 1,626 <R> <C> Baseline+C+AttY2D <C> 2,894 (+4.5%) <C> 1,718 (+5.7%) <CAP> Table 2: Elapsed time (in seconds) for translation of test files. The test file ‘newstest2015’ for En-De has 2,169 sentences and ‘newstest2015’ for En-Fi has 1,370 sentences. The numbers in the parenthesis indicate the additional times for AttY2D compared to the corresponding AttY models.
<R> <C> [EMPTY] <C> 95% w <C> 95% g <C> 98% w <C> 98% g <R> <C> De bello Gallico <C> 8415 <C> 1669 <C> 9954 <C> 2714 <R> <C> Philipicae <C> 9172 <C> 1709 <C> 10711 <C> 2850 <R> <C> Vulgate <C> 6937 <C> 1996 <C> 8476 <C> 3261 <R> <C> Gesta Romanorum <C> 6174 <C> 1378 <C> 7707 <C> 2212 <R> <C> Encyclicals <C> 10682 <C> 2405 <C> 12221 <C> 3689 <CAP> TABLE I: Table of number of words (w) and groups (g) required to obtain 95% and 98% coverage.
<R> <C> [EMPTY] <C> [ITALIC] α <C> [ITALIC] β <C> [ITALIC] γ <C> [ITALIC] δ <R> <C> De bello Gallico <C> 0.3688 <C> 1.3583 <C> 0.3445 <C> 0.5877 <R> <C> Philipicae <C> 0.3491 <C> 1.3707 <C> 0.3249 <C> 0.5537 <R> <C> Vulgate <C> 0.3045 <C> 1.3188 <C> 0.2781 <C> 0.4206 <R> <C> Gesta Romanorum <C> 0.3433 <C> 1.3134 <C> 0.3196 <C> 0.5199 <R> <C> Encyclicals <C> 0.3615 <C> 1.2899 <C> 0.3407 <C> 0.5047 <CAP> TABLE II: Fit parameters
<R> <C> Sets <C> Models <C> Fluency <C> Coherence <C> Meaning <C> Aesthetics <C> Relevance <C> Overall Quality <R> <C> Set 1 <C> Basic <C> 3.00 <C> 2.54 <C> 2.30 <C> 2.71 <C> 2.54 <C> 2.35 <R> <C> Set 1 <C> USPG <C> 3.09 <C> 2.65 <C> 2.61 <C> 2.98 <C> 2.73 <C> 2.63 <R> <C> Set 1 <C> CVAE <C> 3.34 <C> 2.78 <C> 2.64 <C> 3.13 <C> 2.70 <C> 2.81 <R> <C> Set 1 <C> MRL <C> 3.91 <C> 3.66 <C> 3.36 <C> 3.73 <C> 3.19 <C> 3.55 <R> <C> Set 1 <C> MixPoet <C> [BOLD] 4.18∗∗ <C> [BOLD] 4.10∗∗ <C> [BOLD] 3.75∗∗ <C> [BOLD] 4.10∗∗ <C> [BOLD] 3.39 <C> [BOLD] 3.98∗∗ <R> <C> Set 1 <C> GT <C> 4.25 <C> 4.36+ <C> 4.19++ <C> 4.20 <C> 3.99++ <C> 4.25+ <R> <C> Set 2 <C> fBasic <C> 3.26 <C> 3.28 <C> 2.75 <C> 3.25 <C> 2.36 <C> 2.96 <R> <C> Set 2 <C> MixPoet <C> [BOLD] 4.08∗∗ <C> [BOLD] 4.28∗∗ <C> [BOLD] 3.85∗∗ <C> [BOLD] 4.12∗∗ <C> [BOLD] 2.92∗∗ <C> [BOLD] 3.96∗∗ <CAP> Table 3: Human evaluation results of quality. Set 1: poems generated without manually specified mixtures. USPG and MixPoet infer appropriate labels by themselves in terms of different keywords; Set 2: the ones generated with the six mixtures (we present the average scores of all mixtures). Diacritic ** (p<0.01) indicates MixPoet significantly outperforms baseline models; + (p<0.05) and ++ (p<0.01) indicate GT significantly outperforms all models. The Quadratic Weighted Kappa of human annotations is 0.67, which indicates acceptable inter-annotator agreement.
<R> <C> [ITALIC] Sentiment <C> [ITALIC] Government/Opposition Government <C> [ITALIC] Government/Opposition Neutral <C> [ITALIC] Government/Opposition Opposition <R> <C> Positive <C> 11 <C> 377 <C> 2 <R> <C> [EMPTY] <C> partners, progress, balance, achieved, legitimate, best, forward, better, improvement, improvements <C> confidence, like, great, well, ensure, hope good, opportunity, normal, responsible <C> wealth, creation <R> <C> Neutral <C> 66 <C> 2,329 <C> 54 <R> <C> [EMPTY] <C> public, now, economic, per, economy, cent, growth, new, way, community <C> government, country, business, irish, made, many, us, can, years, must <C> people, political, house, mr, one, taoiseach, minister, deputy, time, questions <R> <C> Negative <C> 8 <C> 346 <C> 0 <R> <C> [EMPTY] <C> problems, ireland’s, debt, difficulties, deficit, deterioration, opposite, implications <C> scandals, ireland, difficult, allegations, failed, concern, scandal, unfortunately, innuendo, loss <C> — <CAP> Table 2: Comparing government and opposition words to Lexicoder sentiment dictionary matches.
<R> <C> [BOLD] # <C> [BOLD] Input features <C> [BOLD] Model <C> [BOLD] Embedding <C> [BOLD] Validation Acc. <C> [BOLD] Validation P <C> [BOLD] Validation R <C> [BOLD] Validation F1 <C> [BOLD] Test Acc. <C> [BOLD] Test P <C> [BOLD] Test R <C> [BOLD] Test F1 <R> <C> [ITALIC] Ground-truth dataset <C> [ITALIC] Ground-truth dataset <C> [ITALIC] Ground-truth dataset <C> [ITALIC] Ground-truth dataset <C> [ITALIC] Ground-truth dataset <C> [ITALIC] Ground-truth dataset <C> [ITALIC] Ground-truth dataset <C> [ITALIC] Ground-truth dataset <C> [ITALIC] Ground-truth dataset <C> [ITALIC] Ground-truth dataset <C> [ITALIC] Ground-truth dataset <C> [ITALIC] Ground-truth dataset <R> <C> 1 <C> Diff Tokens <C> LR <C> One-hot <C> 0.644 <C> 0.648 <C> 0.913 <C> 0.758 <C> 0.630 <C> 0.645 <C> 0.877 <C> 0.743 <R> <C> 2 <C> [EMPTY] <C> H-CNN <C> Random <C> 0.636 <C> 0.635 <C> 0.950 <C> 0.761 <C> 0.657 <C> 0.645 <C> 0.975 <C> 0.776 <R> <C> 3 <C> [EMPTY] <C> [EMPTY] <C> Pre-trained <C> 0.682 <C> 0.702 <C> 0.832 <C> 0.761 <C> 0.600 <C> 0.649 <C> 0.753 <C> 0.697 <R> <C> 4 <C> [EMPTY] <C> HR-CNN <C> Random <C> 0.674 <C> 0.676 <C> 0.894 <C> 0.770 <C> 0.645 <C> 0.660 <C> 0.864 <C> 0.749 <R> <C> 5 <C> [EMPTY] <C> [EMPTY] <C> Pre-trained <C> 0.633 <C> 0.629 <C> 0.969 <C> 0.763 <C> 0.653 <C> 0.641 <C> [BOLD] 0.981 <C> 0.776 <R> <C> 6 <C> Paired-code Tokens <C> H-CNN <C> Random <C> 0.633 <C> 0.633 <C> 0.944 <C> 0.758 <C> 0.649 <C> 0.643 <C> 0.957 <C> 0.769 <R> <C> 7 <C> [EMPTY] <C> [EMPTY] <C> Pre-trained <C> 0.663 <C> 0.651 <C> 0.963 <C> 0.777 <C> 0.630 <C> 0.632 <C> 0.944 <C> 0.757 <R> <C> 8 <C> [EMPTY] <C> HR-CNN <C> Random <C> 0.674 <C> 0.671 <C> 0.913 <C> 0.774 <C> 0.668 <C> 0.673 <C> 0.889 <C> 0.766 <R> <C> 9 <C> [EMPTY] <C> [EMPTY] <C> Pre-trained <C> [BOLD] 0.746 <C> [BOLD] 0.761 <C> 0.851 <C> [BOLD] 0.804 <C> [BOLD] 0.725 <C> [BOLD] 0.726 <C> 0.883 <C> [BOLD] 0.797 <R> <C> 10 <C> Paired-AST Paths <C> Code2Vec <C> Random <C> 0.622 <C> 0.619 <C> [BOLD] 1.000 <C> 0.764 <C> 0.613 <C> 0.612 <C> 0.987 <C> 0.756 <R> <C> [ITALIC] Augmented dataset <C> [ITALIC] Augmented dataset <C> [ITALIC] Augmented dataset <C> [ITALIC] Augmented dataset <C> [ITALIC] Augmented dataset <C> [ITALIC] Augmented dataset <C> [ITALIC] Augmented dataset <C> [ITALIC] Augmented dataset <C> [ITALIC] Augmented dataset <C> [ITALIC] Augmented dataset <C> [ITALIC] Augmented dataset <C> [ITALIC] Augmented dataset <R> <C> 11 <C> Diff Tokens <C> LR <C> One-hot <C> 0.697 <C> 0.731 <C> 0.795 <C> 0.762 <C> 0.653 <C> 0.716 <C> 0.716 <C> 0.716 <R> <C> 12 <C> [EMPTY] <C> H-CNN <C> Random <C> 0.663 <C> 0.658 <C> 0.932 <C> 0.771 <C> 0.608 <C> 0.622 <C> 0.914 <C> 0.740 <R> <C> 13 <C> [EMPTY] <C> [EMPTY] <C> Pre-trained <C> 0.659 <C> 0.732 <C> 0.696 <C> 0.713 <C> 0.623 <C> [BOLD] 0.704 <C> 0.660 <C> 0.682 <R> <C> 14 <C> [EMPTY] <C> HR-CNN <C> Random <C> 0.663 <C> 0.658 <C> 0.932 <C> 0.771 <C> 0.608 <C> 0.622 <C> 0.914 <C> 0.740 <R> <C> 15 <C> [EMPTY] <C> [EMPTY] <C> Pre-trained <C> 0.648 <C> 0.739 <C> 0.652 <C> 0.693 <C> 0.596 <C> 0.692 <C> 0.611 <C> 0.649 <R> <C> 16 <C> Paired-code Tokens <C> H-CNN <C> Random <C> 0.610 <C> 0.610 <C> [BOLD] 1.000 <C> 0.758 <C> 0.611 <C> 0.611 <C> [BOLD] 1.000 <C> 0.759 <R> <C> 17 <C> [EMPTY] <C> [EMPTY] <C> Pre-trained <C> 0.610 <C> 0.610 <C> [BOLD] 1.000 <C> 0.758 <C> 0.623 <C> 0.618 <C> [BOLD] 1.000 <C> [BOLD] 0.764 <R> <C> 18 <C> [EMPTY] <C> HR-CNN <C> Random <C> 0.610 <C> 0.610 <C> [BOLD] 1.000 <C> 0.758 <C> 0.611 <C> 0.611 <C> [BOLD] 1.000 <C> 0.759 <R> <C> 19 <C> [EMPTY] <C> [EMPTY] <C> Pre-trained <C> [BOLD] 0.742 <C> [BOLD] 0.736 <C> 0.901 <C> [BOLD] 0.810 <C> [BOLD] 0.668 <C> 0.683 <C> 0.852 <C> 0.758 <R> <C> 20 <C> Paired-AST Paths <C> Code2Vec <C> Random <C> 0.629 <C> 0.624 <C> 1.000 <C> 0.768 <C> 0.625 <C> 0.621 <C> 0.974 <C> 0.759 <CAP> Table 1: Results for each model on the validation and test splits; best values are bolded.
<R> <C> Node dimensions <C> 100 <C> 200 <C> 500 <R> <C> w/o preordering <C> 22.73 <C> 22.73 <C> 22.73 <R> <C> w/o tags and categories <C> 24.63 <C> 24.95 <C> 25.02 <R> <C> w/ tags and categories <C> 25.22 <C> 25.41 <C> 25.38 <CAP> Table 1: BLEU scores with preordering by our model and without preordering under different λ settings (trained on a 500k subset of the training data).
<R> <C> Setting Dialog <C> Setting Seq2Seq <C> BLEU 1.84 <C> BLEU-1 15.1 <C> BLEU-2 2.40 <C> BLEU-3 1.02 <C> BLEU-4 0.66 <R> <C> Translation <C> Seq2Seq <C> 27.2 <C> 60.2 <C> 33.4 <C> 20.9 <C> 13.6 <R> <C> Translation <C> +shuffle 25% <C> 24.4 <C> 56.2 <C> 30.3 <C> 18.8 <C> 12.0 <R> <C> Translation <C> +shuffle 50% <C> 21.1 <C> 52.8 <C> 26.8 <C> 16.0 <C> 10.0 <R> <C> Translation <C> +shuffle 75% <C> 17.2 <C> 48.2 <C> 23.2 <C> 13.4 <C> 8.10 <R> <C> Translation <C> +shuffle 100% <C> .024 <C> 12.5 <C> .189 <C> 0.00 <C> 0.00 <CAP> Table 1: BLEU scores of dialog and translation systems.
<R> <C> [BOLD] Label <C> [BOLD] Count <C> [BOLD] Percentage <R> <C> Strictly implies <C> 42 <C> 28% <R> <C> Strictly implied by <C> 49 <C> 33% <R> <C> Equivalent to <C> 37 <C> 25% <R> <C> Not implicational <C> 22 <C> 15% <R> <C> Total <C> 150 <C> 100% <CAP> Table 4.1: Aggregated expert annotation on 150 implication rules identified by crowd workers in the Levy et al. (2016) dataset [2].
<R> <C> 1c <C> Overall 2 <C> Overall 0.445 <C> Overall ±0.186 <C> Male 2 <C> Male 0.451 <C> Male ±0.182 <C> Female 4 <C> Female 0.439 <C> Female ±0.185 <R> <C> 1d <C> 10 <C> 0.391 <C> ±0.191 <C> 9 <C> 0.399 <C> ±0.182 <C> 10 <C> 0.380 <C> ±0.200 <R> <C> 1e <C> 4 <C> 0.429 <C> ±0.178 <C> 3 <C> 0.440 <C> ±0.167 <C> 2 <C> 0.444 <C> ±0.171 <R> <C> 2a <C> 8 <C> 0.406 <C> ±0.182 <C> 10 <C> 0.396 <C> ±0.185 <C> 8 <C> 0.413 <C> ±0.188 <R> <C> 2b <C> 1 <C> 0.480 <C> ±0.165 <C> 1 <C> 0.485 <C> ±0.162 <C> 1 <C> 0.490 <C> ±0.170 <R> <C> 2c <C> 6 <C> 0.414 <C> ±0.184 <C> 6 <C> 0.414 <C> ±0.179 <C> 9 <C> 0.401 <C> ±0.191 <R> <C> 2d <C> 5 <C> 0.423 <C> ±0.186 <C> 4 <C> 0.432 <C> ±0.179 <C> 3 <C> 0.441 <C> ±0.179 <R> <C> 2e <C> 12 <C> 0.341 <C> ±0.219 <C> 12 <C> 0.342 <C> ±0.214 <C> 11 <C> 0.348 <C> ±0.222 <R> <C> 2f <C> 9 <C> 0.401 <C> ±0.197 <C> 7 <C> 0.413 <C> ±0.188 <C> 6 <C> 0.422 <C> ±0.175 <R> <C> 3a <C> 7 <C> 0.408 <C> ±0.187 <C> 8 <C> 0.409 <C> ±0.183 <C> 7 <C> 0.416 <C> ±0.188 <R> <C> 3b <C> 3 <C> 0.429 <C> ±0.174 <C> 5 <C> 0.418 <C> ±0.170 <C> 5 <C> 0.429 <C> ±0.187 <R> <C> 3c <C> 11 <C> 0.344 <C> ±0.211 <C> 11 <C> 0.342 <C> ±0.205 <C> 11 <C> 0.340 <C> ±0.217 <CAP> Table 2: Response ranking, mean and standard deviation for demographic groups with (*) p < .05, (**) p < .01 wrt. other groups.
<R> <C> 1c <C> A 4 <C> A 0.422 <C> B 2 <C> B 0.470 <C> C 2* <C> C 0.465 <C> D 7 <C> D 0.420 <R> <C> 1d <C> 9 <C> 0.378 <C> 11 <C> 0.385 <C> 8 <C> 0.382 <C> 9* <C> 0.407 <R> <C> 1e <C> 3 <C> 0.438 <C> 3 <C> 0.421 <C> 4 <C> 0.427 <C> 6 <C> 0.430 <R> <C> 2a <C> 7 <C> 0.410 <C> 10 <C> 0.390 <C> 6 <C> 0.424 <C> 8 <C> 0.409 <R> <C> 2b <C> 1 <C> 0.478 <C> 1 <C> 0.493 <C> 1 <C> 0.491 <C> 2* <C> 0.465 <R> <C> 2c <C> 6 <C> 0.410 <C> 4 <C> 0.415 <C> 9 <C> 0.380 <C> 5* <C> 0.432 <R> <C> 2d <C> 8** <C> 0.404 <C> 7 <C> 0.407 <C> 3** <C> 0.453 <C> 3 <C> 0.434 <R> <C> 2e <C> 12 <C> 0.345 <C> 9** <C> 0.393 <C> 10 <C> 0.327 <C> 12 <C> 0.333 <R> <C> 2f <C> 10** <C> 0.376 <C> 5 <C> 0.414 <C> 7 <C> 0.417 <C> 1** <C> 0.483 <R> <C> 3a <C> 5** <C> 0.421 <C> 6 <C> 0.409 <C> 5 <C> 0.426 <C> 10** <C> 0.382 <R> <C> 3b <C> 2 <C> 0.440 <C> 8 <C> 0.396 <C> - <C> - <C> 4 <C> 0.432 <R> <C> 3c <C> 11** <C> 0.360 <C> 12 <C> 0.340 <C> 11** <C> 0.322 <C> 11 <C> 0.345 <CAP> Table 4: Ranks and mean scores per prompt contexts (A) Gender and Sexuality, (B) Sexualised Comments, (C) Sexualised Insults and (D) Sexualised Requests and Demands.
<R> <C> Cluster <C> Bot <C> Avg <R> <C> 1 <C> Alley <C> 0.452 <R> <C> 2 <C> Alexa <C> 0.426 <R> <C> [EMPTY] <C> Alice <C> 0.425 <R> <C> [EMPTY] <C> Siri <C> 0.431 <R> <C> [EMPTY] <C> Parry <C> 0.423 <R> <C> [EMPTY] <C> Google Home <C> 0.420 <R> <C> [EMPTY] <C> Cortana <C> 0.418 <R> <C> [EMPTY] <C> Cleverbot <C> 0.414 <R> <C> [EMPTY] <C> Neuralconvo <C> 0.401 <R> <C> [EMPTY] <C> Eliza <C> 0.405 <R> <C> 3 <C> Annabelle Lee <C> 0.379 <R> <C> [EMPTY] <C> Laurel Sweet <C> 0.379 <R> <C> [EMPTY] <C> Clean Seq2Seq <C> 0.379 <R> <C> 4 <C> IR system <C> 0.355 <R> <C> [EMPTY] <C> Capt Howdy <C> 0.343 <R> <C> 5 <C> Dr Love <C> 0.330 <R> <C> 6 <C> Sophia69 <C> 0.287 <CAP> Table 5: System clusters according to Trueskill and “appropriateness” average score. Note that systems within a cluster are not significantly different.
<R> <C> [BOLD] Auxiliary Tasks <C> [BOLD] Main Task angle=45,lap=0pt-(.01em)anxiety <C> [BOLD] Main Task angle=45,lap=0pt-(.01em)bipolar <C> [BOLD] Main Task angle=45,lap=0pt-(.01em)depression <C> [BOLD] Main Task angle=45,lap=0pt-(.01em)suicide attempt <R> <C> [ITALIC] all <C> 0.813∗† <C> 0.752∗† <C> 0.769† <C> 0.835∗† <R> <C> [ITALIC] all conds <C> 0.786 <C> 0.743† <C> 0.772† <C> 0.833∗† <R> <C> [ITALIC] neuro <C> 0.763 <C> 0.740† <C> 0.759 <C> 0.797 <R> <C> [ITALIC] neuro+mood <C> 0.756 <C> 0.742† <C> 0.761 <C> 0.804 <R> <C> [ITALIC] neuro+anx <C> 0.770 <C> 0.744† <C> 0.746 <C> 0.792 <R> <C> [ITALIC] neuro+targets <C> 0.750 <C> 0.747† <C> 0.764 <C> 0.817 <R> <C> [ITALIC] none (STL) <C> 0.777 <C> 0.552 <C> 0.749 <C> 0.810 <R> <C> [ITALIC] LR <C> 0.791 <C> 0.723† <C> 0.763 <C> 0.817 <CAP> Table 2: Test AUC when predicting Main Task after training to predict a subset of auxiliary tasks. Significant improvement over LR baseline at p=0.05 is denoted by ∗, and over no auxiliary tasks (STL) by †.
<R> <C> [EMPTY] <C> SST-2 <C> SNIPS <C> TREC <R> <C> Dev <C> 91.91 <C> 98.86 <C> 94.69 <CAP> Table 3: Classifier performance on dev set for each corpus. Classifiers are used for intrinsic evaluation.
<R> <C> Target language <C> Cebuano <C> Kurmanji <C> Swahili <R> <C> Closest language <C> Tagalog <C> Turkish <C> Zulu <R> <C> SBN models <C> SBN models <C> SBN models <C> SBN models <R> <C> Monolingual <C> 73.5 <C> 86.2 <C> 65.8 <R> <C> Adapted multilingual <C> 65.0 <C> 75.5 <C> 54.9 <R> <C> Closest language <C> 63.7 <C> 75.0 <C> 54.2 <R> <C> Hybrid models <C> Hybrid models <C> Hybrid models <C> Hybrid models <R> <C> DNN <C> 63.9 <C> 74.9 <C> 54.0 <R> <C> LSTM <C> 63.0 <C> 74.0 <C> 53.0 <R> <C> PACRNN-DNN <C> 62.1 <C> 72.9 <C> 52.1 <R> <C> PACRNN-LSTM <C> 60.6 <C> 72.5 <C> 51.4 <R> <C> Hybrid models with closest language initialization <C> Hybrid models with closest language initialization <C> Hybrid models with closest language initialization <C> Hybrid models with closest language initialization <R> <C> DNN <C> 62.7 <C> 73.1 <C> 52.4 <R> <C> LSTM <C> 61.3 <C> 72.5 <C> 52.2 <R> <C> PAC-RNN-DNN <C> 60.8 <C> 71.8 <C> 51.6 <R> <C> PAC-RNN-LSTM <C> 59.7 <C> 71.4 <C> 50.4 <CAP> Table 1: WER (%) results for each ASR system.
<R> <C> With L1 norm <C> [BOLD] 0.0298 <R> <C> Without L1 norm <C> 0.0335 <CAP> Table 3: MSE of the TTE model for validation set.
<R> <C> [BOLD] Methods <C> [BOLD] MT02 <C> [BOLD] MT04 <C> [BOLD] MT05 <C> [BOLD] AVG <R> <C> Net [ITALIC] m <C> 40.36 <C> 38.30 <C> 37.93 <C> 38.86(+0.00) <R> <C> ListNet <C> 40.75 <C> 38.69 <C> 38.31 <C> 39.25(+0.39) <R> <C> MLE [ITALIC] m <C> 39.82 <C> 37.88 <C> 37.65 <C> 38.45(+0.00) <R> <C> ListMLE <C> 40.40 <C> 38.21 <C> 38.04 <C> 38.88(+0.43) <CAP> Table 2: The comparison of instances aggregating and k-best merging on the extended feature set.(Netm and MLEm denote ListNet and ListMLE with k-best merging respectively.)
<R> <C> [BOLD] Methods <C> [BOLD] MT02 <C> [BOLD] MT04 <C> [BOLD] MT05 <C> [BOLD] AVG <R> <C> MERT <C> 37.72 <C> 37.13 <C> 36.77 <C> 37.21(+0.00) <R> <C> PRO <C> 37.85 <C> 37.21 <C> 36.68 <C> 37.24(+0.03) <R> <C> KB-MIRA <C> 37.97 <C> 37.28 <C> 36.58 <C> 37.28(+0.07) <R> <C> ListNet <C> 37.71 <C> 37.47∗ <C> 36.78 <C> 37.32(+0.11) <R> <C> ListMLE <C> 37.54 <C> [BOLD] 37.54 <C> 36.65 <C> 37.24(+0.03) <R> <C> ListMLE-T5 <C> 37.90 <C> 37.32 <C> 36.84 <C> 37.35(+0.14) <R> <C> ListMLE-TE <C> [BOLD] 38.03 <C> 37.49∗ <C> [BOLD] 36.85 <C> [BOLD] 37.46(+0.25) <CAP> Table 5: Comparison of baseline and liswise approaches on basic feature set.
<R> <C> [BOLD] Models <C> [BOLD] Insert <C> [BOLD] Original <C> [BOLD] Both <R> <C> RNN <C> 78.4 <C> [BOLD] 73.4 <C> [BOLD] 68.2 <R> <C> SAN <C> 73.2 <C> 66.0 <C> 60.1 <R> <C> DiSAN <C> [BOLD] 79.6 <C> 70.1 <C> 68.0 <CAP> Table 1: Accuracy on the WRD task. “Insert” and “Original” denotes the accuracies of detecting the inserted and original positions respectively, and “Both” denotes detecting both positions.
<R> <C> [BOLD] Model <C> [BOLD] Translation En⇒De <C> [BOLD] Translation En⇒Ja <C> [BOLD] Detection En⇒De Enc. <C> [BOLD] Detection En⇒Ja Enc. <C> [BOLD] Detection WRD Enc. <R> <C> RNN <C> 26.8 <C> 42.9 <C> 33.9 <C> 29.0 <C> [BOLD] 68.2 <R> <C> SAN <C> 27.3 <C> 43.6 <C> [BOLD] 41.6 <C> [BOLD] 32.8 <C> 60.1 <R> <C> - Pos_Emb <C> 11.5 <C> – <C> 0.3 <C> – <C> 0.3 <R> <C> DiSAN <C> [BOLD] 27.6 <C> [BOLD] 43.7 <C> 39.7 <C> 31.2 <C> 68.0 <R> <C> - Pos_Emb <C> 27.0 <C> 43.1 <C> 40.1 <C> 31.0 <C> 62.8 <CAP> Table 2: Performances of NMT encoders pre-trained on WMT14 En⇒De and WAT17 En⇒Ja data. “Translation” denotes translation quality measured in BLEU scores, while “Detection” denotes the accuracies on WRD task. “En⇒De Enc.” denotes NMT encoder trained with translation objective on the En⇒De data. We also list the detection accuracies of WRD encoders (“WRD Enc.”) for comparison. “- Pos_Emb” indicates removing positional embeddings from SAN- or DiSAN-based encoder. Surprisingly, SAN-based NMT encoder achieves the best accuracy on the WRD task, which contrasts with the performances of WRD encoders (the last column).
<R> <C> Model <C> Accuracy <R> <C> Encoder-Decoder <C> 79.08 <R> <C> Encoder-Decoder Attention <C> 95.64 <R> <C> Ours W/O Encoder <C> 84.04 <R> <C> Ours <C> [BOLD] 96.20 <CAP> Table 7: Avg. accuracy across datasets of the encoder-decoder, attentional encoder-decoder & our model without encoder.
<R> <C> [BOLD] Dataset Rule <C> [BOLD] TripAdvisor DEV <C> [BOLD] TripAdvisor TEST <C> [BOLD] BeerAdvocate DEV <C> [BOLD] BeerAdvocate TEST <R> <C> R1 <C> 0.7215 <C> 0.7174 <C> 0.7220 <C> 0.7216 <R> <C> R2 <C> 0.7172 <C> 0.7180 <C> 0.6864 <C> 0.6936 <R> <C> R3 <C> 0.6263 <C> 0.6187 <C> 0.6731 <C> 0.6725 <R> <C> R4 <C> 0.6248 <C> 0.6279 <C> 0.6724 <C> 0.6717 <R> <C> R5 <C> 0.5902 <C> 0.5856 <C> 0.7095 <C> 0.7066 <R> <C> - R1 <C> 0.7538 <C> 0.7481 <C> 0.7458 <C> 0.7474 <R> <C> - R2 <C> 0.7342 <C> 0.7368 <C> 0.7504 <C> 0.7529 <R> <C> - R3 <C> 0.7418 <C> 0.7397 <C> 0.7565 <C> 0.7558 <R> <C> - R4 <C> 0.7424 <C> 0.7368 <C> 0.7518 <C> 0.7507 <R> <C> - R5 <C> 0.7448 <C> 0.7440 <C> 0.7550 <C> 0.7548 <R> <C> All <C> 0.7577 <C> 0.7561 <C> 0.7502 <C> 0.7538 <CAP> Table 3: Averaged accuracies on DMSC. “R1 – R5” means only using a rule while “-R1 –-R5” means removing a rule from all the rules.
<R> <C> Dataset <C> Acoustic Features AUD is based on <C> # units <C> % NMI <C> Average Precision <C> Document Classification Accuracy <C> Document Clustering Purity <C> Document Clustering B-Cubed F1 <R> <C> [EMPTY] <C> MFCC <C> 145 <C> 21.59 <C> 0.247 <C> 0.3083 ± 0.0908 <C> 0.2268 ± 0.0015 <C> 0.1817 ± 0.0008 <R> <C> Development <C> MFCC w/ LDA <C> 145 <C> 24.55 <C> 0.251 <C> 0.4361 ± 0.0692 <C> 0.2354 ± 0.0026 <C> 0.1855 ± 0.0006 <R> <C> 2-8[2.0pt/0.5pt] Data <C> BN <C> 184 <C> 28.20 <C> 0.343 <C> 0.7028 ± 0.0796 <C> 0.2446 ± 0.0018 <C> 0.1949 ± 0.0008 <R> <C> [EMPTY] <C> BN w/ LDA <C> 184 <C> 29.13 <C> 0.359 <C> 0.7167 ± 0.0733 <C> 0.2553 ± 0.0102 <C> 0.2023 ± 0.0047 <R> <C> [EMPTY] <C> MFCC <C> 144 <C> 21.20 <C> 0.224 <C> 0.4633 ± 0.0702 <C> 0.2388 ± 0.0010 <C> 0.1899 ± 0.0001 <R> <C> Evaluation <C> MFCC w/ LDA <C> 144 <C> 24.07 <C> 0.219 <C> 0.4833 ± 0.0477 <C> 0.2426 ± 0.0031 <C> 0.1893 ± 0.0005 <R> <C> 2-8[2.0pt/0.5pt] Data <C> BN <C> 184 <C> 28.01 <C> 0.303 <C> 0.7167 ± 0.0350 <C> 0.2398 ± 0.0069 <C> 0.1983 ± 0.0032 <R> <C> [EMPTY] <C> BN w/ LDA <C> 184 <C> 28.84 <C> 0.329 <C> 0.7300 ± 0.0567 <C> 0.2373 ± 0.0037 <C> 0.2140 ± 0.0035 <CAP> Table 1: AUD Performance evaluated by NMI, same-different task, document classification and clustering on Switchboard
<R> <C> [EMPTY] <C> Open-Ended test-dev <C> Open-Ended test-dev <C> Open-Ended test-dev <C> Open-Ended test-dev <C> Open-Ended test-std <C> Multiple-Choice test-dev <C> Multiple-Choice test-dev <C> Multiple-Choice test-dev <C> Multiple-Choice test-dev <C> Multiple-Choice test-std <R> <C> Method <C> Y/N <C> Num <C> Other <C> All <C> All <C> Y/N <C> Num <C> Other <C> All <C> All <R> <C> LSTM Q+I  antol2015vqa  <C> 80.5 <C> 36.8 <C> 43.0 <C> 57.8 <C> 58.2 <C> 80.5 <C> 38.2 <C> 53.0 <C> 62.7 <C> 63.1 <R> <C> Region Sel.  shih2015look  <C> - <C> - <C> - <C> - <C> - <C> 77.6 <C> 34.3 <C> 55.8 <C> 62.4 <C> - <R> <C> SMem  xu2015ask  <C> 80.9 <C> 37.3 <C> 43.1 <C> 58.0 <C> 58.2 <C> - <C> - <C> - <C> - <C> - <R> <C> SAN  yang2015stacked  <C> 79.3 <C> 36.6 <C> 46.1 <C> 58.7 <C> 58.9 <C> - <C> - <C> - <C> - <C> - <R> <C> FDA  Ilievski2016  <C> [BOLD] 81.1 <C> 36.2 <C> 45.8 <C> 59.2 <C> 59.5 <C> [BOLD] 81.5 <C> 39.0 <C> 54.7 <C> 64.0 <C> 64.2 <R> <C> DMN+  xiong2016dynamic  <C> 80.5 <C> 36.8 <C> 48.3 <C> 60.3 <C> 60.4 <C> - <C> - <C> - <C> - <C> - <R> <C> Ours [ITALIC] p+VGG <C> 79.5 <C> [BOLD] 38.7 <C> 48.3 <C> 60.1 <C> - <C> 79.5 <C> 39.8 <C> 57.4 <C> 64.6 <C> - <R> <C> Ours [ITALIC] a+VGG <C> 79.6 <C> 38.4 <C> 49.1 <C> 60.5 <C> - <C> 79.7 <C> [BOLD] 40.1 <C> 57.9 <C> 64.9 <C> - <R> <C> Ours [ITALIC] a+ResNet <C> 79.7 <C> [BOLD] 38.7 <C> [BOLD] 51.7 <C> [BOLD] 61.8 <C> [BOLD] 62.1 <C> 79.7 <C> 40.0 <C> [BOLD] 59.8 <C> [BOLD] 65.8 <C> [BOLD] 66.1 <CAP> Table 1: Results on the VQA dataset. “-” indicates the results is not available.
<R> <C> Method <C> Object <C> Number <C> Color <C> Location <C> Accuracy <C> WUPS0.9 <C> WUPS0.0 <R> <C> 2-VIS+BLSTM  ren2015exploring  <C> 58.2 <C> 44.8 <C> 49.5 <C> 47.3 <C> 55.1 <C> 65.3 <C> 88.6 <R> <C> IMG-CNN  ma2015learning  <C> - <C> - <C> - <C> - <C> 58.4 <C> 68.5 <C> 89.7 <R> <C> SAN(2, CNN)  yang2015stacked  <C> 64.5 <C> 48.6 <C> 57.9 <C> 54.0 <C> 61.6 <C> 71.6 <C> 90.9 <R> <C> Ours [ITALIC] p+VGG <C> 65.6 <C> 49.6 <C> 61.5 <C> 56.8 <C> 63.3 <C> 73.0 <C> 91.3 <R> <C> Ours [ITALIC] a+VGG <C> 65.6 <C> 48.9 <C> 59.8 <C> 56.7 <C> 62.9 <C> 72.8 <C> 91.3 <R> <C> Ours [ITALIC] a+ResNet <C> [BOLD] 68.0 <C> [BOLD] 51.0 <C> [BOLD] 62.9 <C> [BOLD] 58.8 <C> [BOLD] 65.4 <C> [BOLD] 75.1 <C> [BOLD] 92.0 <CAP> Table 2: Results on the COCO-QA dataset. “-” indicates the results is not available.
<R> <C> [EMPTY] <C> [EMPTY] <C> [BOLD] Predicted  [BOLD] E <C> [BOLD] Predicted  [BOLD] N <C> [BOLD] Predicted  [BOLD] C <R> <C> [BOLD] Actual <C> [BOLD] E <C> 1.0 <C> 0.36 <C> 0.0 <R> <C> [BOLD] Actual <C> [BOLD] N <C> 0.0 <C> 0.256 <C> 0.0 <R> <C> [BOLD] Actual <C> [BOLD] C <C> 0.0 <C> 0.384 <C> 0.0 <CAP> Table 1: Confusion matrix without semantic transformations
<R> <C> Confident Trigram <C> Confident Count <C> Hesitant Trigram <C> Hesitant Coune <R> <C> what be the <C> 171 <C> what be the <C> 190 <R> <C> in what year <C> 98 <C> what do the <C> 55 <R> <C> who be the <C> 70 <C> what type of <C> 51 <R> <C> when be the <C> 69 <C> what do luther <C> 31 <R> <C> when do the <C> 40 <C> how do the <C> 24 <R> <C> in which year <C> 29 <C> what be a <C> 21 <R> <C> what year do <C> 27 <C> what do tesla <C> 19 <R> <C> what year be <C> 16 <C> what be one <C> 18 <R> <C> what be another <C> 15 <C> where be the <C> 17 <R> <C> where be the <C> 14 <C> where do the <C> 14 <R> <C> how much do <C> 11 <C> why be the <C> 13 <R> <C> who design the <C> 11 <C> who be the <C> 11 <R> <C> approximately how many <C> 9 <C> what happen to <C> 11 <R> <C> on what date <C> 9 <C> when be the <C> 11 <R> <C> when do tesla <C> 8 <C> what be tesla <C> 10 <R> <C> what percentage of <C> 8 <C> how do luther <C> 10 <R> <C> what do the <C> 8 <C> what kind of <C> 10 <R> <C> how old be <C> 8 <C> what be another <C> 10 <R> <C> when do luther <C> 7 <C> what have the <C> 7 <R> <C> who lead the <C> 7 <C> how be the <C> 7 <CAP> Table 4: Common trigrams to start confident or hesitant questions of the Triage Module.
<R> <C> [BOLD] Training on SLMRD validation accuracy (%) <C> [BOLD] Training on SLMRD validation binary cross-entropy <C> [BOLD] Predicting on KID Prediction transfer accuracy (%) <R> <C> 89.02 <C> 0.2791 <C> 91.56 <R> <C> 89.12 <C> 0.2815 <C> 91.63 <R> <C> 89.17 <C> 0.2772 <C> 91.76 <CAP> Table 4: Scenarios 3 and 4 results. Data from experiments with training a model on SLMRD until it attains some validation accuracy grater than 89 % and then using that same model to predict the category for each labeled review in KID. Note that the transfer accuracy is computed over the entire set of 50 000 reviews in KID.
<R> <C> [EMPTY] <C> Development set Goal <C> Development set Goal <C> Development set Method <C> Development set Method <C> Development set Requested <C> Development set Requested <C> Test set Goal <C> Test set Goal <C> Test set Method <C> Test set Method <C> Test set Requested <C> Test set Requested <R> <C> model <C> Acc. <C> L2 <C> Acc. <C> L2 <C> Acc. <C> L2 <C> Acc. <C> L2 <C> Acc. <C> L2 <C> Acc. <C> L2 <R> <C> baseline <C> 0.61 <C> 0.63 <C> 0.83 <C> 0.27 <C> 0.89 <C> 0.17 <C> 0.72 <C> 0.46 <C> 0.90 <C> 0.16 <C> 0.88 <C> 0.20 <R> <C> LecTrack (base) <C> 0.63 <C> 0.74 <C> 0.90 <C> 0.19 <C> 0.96 <C> 0.08 <C> 0.62 <C> 0.75 <C> 0.92 <C> 0.15 <C> 0.96 <C> 0.07 <R> <C> LecTrack (score) <C> 0.63 <C> 0.73 <C> 0.89 <C> 0.20 <C> 0.96 <C> 0.07 <C> 0.64 <C> 0.73 <C> 0.92 <C> 0.16 <C> 0.96 <C> 0.07 <R> <C> LecTrack (transcr) <C> 0.66 <C> 0.69 <C> 0.90 <C> 0.20 <C> [BOLD] 0.97 <C> 0.07 <C> 0.67 <C> 0.65 <C> 0.92 <C> 0.15 <C> 0.97 <C> 0.07 <R> <C> LecTrack (abstract) <C> 0.67 <C> 0.65 <C> 0.90 <C> 0.20 <C> [BOLD] 0.97 <C> 0.07 <C> 0.68 <C> 0.64 <C> 0.93 <C> 0.14 <C> 0.97 <C> 0.06 <R> <C> LecTrack (model avg) <C> 0.69 <C> 0.71 <C> 0.90 <C> 0.19 <C> [BOLD] 0.97 <C> 0.07 <C> 0.72 <C> 0.64 <C> 0.93 <C> 0.14 <C> 0.97 <C> 0.06 <R> <C> LecTrack (dontcare oracle) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 0.75 <C> 0.50 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> turn-based RNN  <C> 0.70 <C> [BOLD] 0.46 <C> [BOLD] 0.92 <C> 0.14 <C> [BOLD] 0.97 <C> 0.06 <C> 0.77 <C> [BOLD] 0.35 <C> 0.94 <C> 0.10 <C> [BOLD] 0.98 <C> [BOLD] 0.04 <R> <C> state-of-the-art  <C> [BOLD] 0.71 <C> 0.74 <C> 0.91 <C> [BOLD] 0.13 <C> [BOLD] 0.97 <C> [BOLD] 0.05 <C> [BOLD] 0.78 <C> [BOLD] 0.35 <C> [BOLD] 0.95 <C> [BOLD] 0.08 <C> [BOLD] 0.98 <C> [BOLD] 0.04 <CAP> Table 1: Performance on the DSTC2 data.
<R> <C> Model | R@: <C> Image 1 <C> Image 10 <C> Caption 1 <C> Caption 10 <R> <C> VSE++ <C> 32.3 <C> 72.1 <C> 43.7 <C> 82.1 <R> <C> FastText (15M) <C> 35.6 <C> 74.7 <C> 47.1 <C> 82.7 <R> <C> ImageNet (29M) <C> 25.6 <C> 63.1 <C> 36.6 <C> 72.2 <R> <C> Naive (32M) <C> 34.4 <C> 73.9 <C> 46.4 <C> 82.2 <R> <C> Unweighted DME (15M) <C> 35.9 <C> 75.0 <C> 48.9 <C> 83.7 <R> <C> DME (15M) <C> 36.5 <C> 75.5 <C> 49.7 <C> 83.6 <R> <C> CDME (15M) <C> 36.5 <C> 75.6 <C> 49.0 <C> 83.8 <CAP> Table 3: Image and caption retrieval results (R@1 and R@10) on Flickr30k dataset, compared to VSE++ baseline Faghri et al. (2017). VSE++ numbers in the table are with ResNet features and random cropping, but no fine-tuning. Number of parameters included in parenthesis; averaged over five runs with std omitted for brevity.
<R> <C> Model <C> Levy <C> LEAR <C> SNLI <R> <C> CDME <C> 0.33 <C> 0.67 <C> 85.3±.9 <R> <C> Model <C> GloVe <C> Refined <C> SST <R> <C> CDME <C> 0.59 <C> 0.41 <C> 89.0±.4 <CAP> Table 4: Accuracy and learned weights on SNLI using LEAR Vulić and Mrkšić (2017) or SST using sentiment-refined embeddings using the specialization method from Yu:2017emnlp.
<R> <C> [BOLD] Methods <C> [ITALIC] T <C> [ITALIC] P <C> [ITALIC] R <C> [ITALIC] F1 <R> <C> Random baseline <C> 0.003 <C> 0.501 <C> 0.999 <C> 0.668 <R> <C> Length Model <C> 0.203 <C> 0.566 <C> 0.970 <C> 0.714 <R> <C> CL-C3G <C> 0.087 <C> 0.972 <C> 0.953 <C> 0.962 <R> <C> CL-CTS <C> 0.010 <C> 0.986 <C> 0.808 <C> 0.888 <R> <C> CL-ASA <C> 0.762 <C> 0.937 <C> 0.772 <C> 0.847 <R> <C> T+MA <C> 0.157 <C> 0.928 <C> 0.646 <C> 0.762 <CAP> Table 9: Precision (P), Recall (R) and F1 score, reached at a certain threshold (T), of some state-of-the-art methods for a data subset made with 1000 positives and 1000 negatives (mis)matches – 10 folds validation.
<R> <C> [BOLD] Model <C> [BOLD] Task 6 A <C> [BOLD] Task 6 C <R> <C> T + CO <C> 0.73 <C> 0.44 <R> <C> T + CO + U <C> 0.71 <C> 0.52 <R> <C> T + CO + F <C> [BOLD] 0.75 <C> 0.45 <R> <C> T + CO + U + F <C> 0.74 <C> 0.47 <R> <C> RF <C> 0.7 <C> [BOLD] 0.54 <R> <C> RF + F <C> 0.68 <C> 0.43 <R> <C> RF + U <C> 0.72 <C> 0.48 <R> <C> RF + U + F <C> 0.69 <C> 0.38 <CAP> Table 5: Macro F1 for selected Transformer models with different combinations of features
<R> <C> [BOLD] En-Zh <C> dev <C> test <C> average <R> <C> In <C> 32.45 <C> 30.42 <C> 31.44 <R> <C> Out + In <C> 30.37 <C> 28.76 <C> 29.57 <R> <C> Sampler <C> 35.06 <C> 32.97 <C> 34.02 <R> <C> Fine Tune <C> 35.02 <C> 33.36 <C> 34.19 <R> <C> DC <C> 31.08 <C> 29.59 <C> 30.34 <R> <C> DM <C> 30.98 <C> 29.73 <C> 30.36 <R> <C> TTM <C> 31.77 <C> 30.11 <C> 30.94 <R> <C> ADM <C> 31.23 <C> 29.88 <C> 30.56 <R> <C> our method <C> 36.55** <C> 34.84** <C> 35.70 <CAP> Table 1: Results of the en-zh translation experiments. The marks indicate whether the proposed methods were significantly better than the best performed contrast models(**: better at significance levelα=0.01, *:α=0.05)Collins et al. (2005)
<R> <C> DCN <C> Private <C> dev <C> test <C> average <R> <C> √ <C> √ <C> 36.55 <C> 34.84 <C> 35.70 <R> <C> √ <C> × <C> 35.73 <C> 34.09 <C> 34.91 <R> <C> × <C> √ <C> 35.67 <C> 34.22 <C> 34.94 <R> <C> × <C> × <C> 35.13 <C> 33.36 <C> 34.25 <CAP> Table 4: Results of the ablation study. "DCN" means the discriminator and "Private" means the private encoder-decoder.
<R> <C> [BOLD] En-Zh <C> test1 <C> test2 <C> test3 <R> <C> Out + In <C> 22.31 <C> 18.82 <C> 17.59 <R> <C> Sampler <C> 21.60 <C> 18.64 <C> 16.93 <R> <C> Fine Tune <C> 13.18 <C> 11.94 <C> 11.55 <R> <C> our method <C> 22.61 <C> 19.36 <C> 17.78 <CAP> Table 6: Results of the out-of-domain translation task. The test sets are from the NIST test sets but we exchange the translation directions.
<R> <C> [BOLD] En-Zh <C> dev <C> test <C> average <R> <C> In <C> 32.61 <C> 30.33 <C> 31.47 <R> <C> Sampler <C> 35.84 <C> 33.68 <C> 34.76 <R> <C> Fine Tune <C> 36.01 <C> 34.03 <C> 35.02 <R> <C> our method <C> 37.26** <C> 35.39** <C> 36.33 <CAP> Table 7: Results of the En-Zh experiments based on the transformer model.
<R> <C> # <C> [BOLD] Model <C> [BOLD] Label Granularity: Large \rightarrow Small Voice <C> [BOLD] Label Granularity: Large \rightarrow Small Tense <C> [BOLD] Label Granularity: Large \rightarrow Small TSS <C> [BOLD] Label Granularity: Large \rightarrow Small SPC <C> [BOLD] Label Granularity: Large \rightarrow Small POS <C> [BOLD] Label Granularity: Large \rightarrow Small Avg <R> <C> [ITALIC] Pre-Trained NMT Encoder <C> [ITALIC] Pre-Trained NMT Encoder <C> [ITALIC] Pre-Trained NMT Encoder <C> [ITALIC] Pre-Trained NMT Encoder <C> [ITALIC] Pre-Trained NMT Encoder <C> [ITALIC] Pre-Trained NMT Encoder <C> [ITALIC] Pre-Trained NMT Encoder <C> [ITALIC] Pre-Trained NMT Encoder <R> <C> 1 <C> Base <C> 73.38 <C> 73.73 <C> 72.72 <C> 92.81 <C> 93.73 <C> 81.27 <R> <C> 2 <C> N-Gram Phrase <C> 73.06 <C> 72.83 <C> 72.11 <C> 96.42 <C> 96.34 <C> 82.15 <R> <C> 3 <C> Syntactic Phrase <C> 73.37 <C> 73.62 <C> 75.60 <C> 96.72 <C> 96.68 <C> 83.19 <R> <C> 4 <C> Syntactic Phrase + Interaction <C> 73.20 <C> 74.78 <C> 75.24 <C> 96.78 <C> 96.56 <C> 83.31 <R> <C> [ITALIC] Train From Scratch <C> [ITALIC] Train From Scratch <C> [ITALIC] Train From Scratch <C> [ITALIC] Train From Scratch <C> [ITALIC] Train From Scratch <C> [ITALIC] Train From Scratch <C> [ITALIC] Train From Scratch <C> [ITALIC] Train From Scratch <R> <C> 5 <C> Base <C> 83.46 <C> 85.39 <C> 83.44 <C> 96.35 <C> 96.12 <C> 88.95 <R> <C> 6 <C> N-Gram Phrase <C> 83.55 <C> 85.62 <C> 85.21 <C> 96.23 <C> 96.17 <C> 89.36 <R> <C> 7 <C> Syntactic Phrase <C> 84.70 <C> 87.52 <C> 97.42 <C> 96.95 <C> 96.24 <C> 92.57 <R> <C> 8 <C> Syntactic Phrase + Interaction <C> 86.45 <C> 87.65 <C> 99.07 <C> 96.99 <C> 96.40 <C> 93.31 <CAP> Table 5: Accuracies on multi-granularity label prediction tasks. “Pre-Trained NMT Encoder” denotes using the pre-trained NMT encoders of model variations in Table 3. “Train From Scratch” denotes using three encoder layers with proposed Mg-Sa variants, which are trained from scratch. For syntactic phrase based models, we only apply syntactic boundary of phrases and do not use any tag supervision for fair comparison.
<R> <C> [EMPTY] <C> NEAT <C> Rand. Att. <C> Word Len. <C> Word Freq. <C> Full Surp. <C> Human <C> Full Att. <R> <C> Language Modeling <C> 180 <C> 333 <C> 230 <C> 219 <C> 211 <C> 218/170 <C> 107 <R> <C> Reconstruction <C> 4.5 <C> 56 <C> 40 <C> 39 <C> 34 <C> 39/31 <C> 1.6 <R> <C> Fixation Rate <C> 60.4% <C> 62.1% <C> 62.1% <C> 62.1% <C> 62.1% <C> 61.3%/72.0% <C> 100% <CAP> Table 1: Performance on language modeling and reconstruction as measured by perplexity. Random attention is an upper bound on perplexity, while full attention is a lower bound. For the human baseline, we give two figures, which differ in the treatment of missing data. The first figure is obtained when replacing missing values with a random variable ω∼Binom(n,0.61); the second results from replacing missing values with 1.
<R> <C> [EMPTY] <C> Acc <C> F1fix <C> F1skip <R> <C> NEAT <C> 63.7 <C> 70.4 <C> 53.0 <R> <C> Supervised Models <C> Supervised Models <C> Supervised Models <C> Supervised Models <R> <C> Nilsson and Nivre ( 2009 ) <C> 69.5 <C> 75.2 <C> 62.6 <R> <C> Matthies and Søgaard ( 2013 ) <C> 69.9 <C> 72.3 <C> 66.1 <R> <C> Human Performance and Baselines <C> Human Performance and Baselines <C> Human Performance and Baselines <C> Human Performance and Baselines <R> <C> Random Baseline <C> 52.6 <C> 62.1 <C> 37.9 <R> <C> Full Surprisal <C> 64.1 <C> 70.7 <C> 53.6 <R> <C> Word Frequency <C> 67.9 <C> 74.0 <C> 58.3 <R> <C> Word Length <C> 68.4 <C> 77.1 <C> 49.0 <R> <C> Human <C> 69.5 <C> 76.6 <C> 53.6 <CAP> Table 2: Evaluation of fixation sequence predictions against human data. For the human baseline, we predicted the n-th reader’s fixations by taking the fixations of the n+1-th reader (with missing values replaced by reader average), averaging the resulting scores over the ten readers.
<R> <C> (Intercept) <C> [ITALIC] β 247.43 <C> SE 7.14 <C> [ITALIC] t 34.68 <C> * <R> <C> Word Length <C> 12.92 <C> 0.21 <C> 60.62 <C> * <R> <C> Previous Word Freq. <C> −5.28 <C> 0.28 <C> −18.34 <C> * <R> <C> Prev. Word Fixated <C> −24.67 <C> 0.81 <C> −30.55 <C> * <R> <C> Launch Distance <C> -0.01 <C> 0.01 <C> −0.37 <C> [EMPTY] <R> <C> Obj. Landing Pos. <C> −8.07 <C> 0.20 <C> −41.25 <C> * <R> <C> Word Pos. in Sent. <C> −0.10 <C> 0.03 <C> −2.98 <C> * <R> <C> Log Word Freq. <C> −1.59 <C> 0.21 <C> −7.73 <C> * <R> <C> Resid. Random Surprisal <C> 2.69 <C> 0.10 <C> 29.27 <C> * <R> <C> Resid. Restr. Surprisal <C> 2.75 <C> 0.12 <C> 23.66 <C> * <R> <C> Resid. Full Surprisal <C> 2.99 <C> 0.12 <C> 25.23 <C> * <CAP> Table 3: Linear mixed effects models for first pass duration. The first part of the table shows the coefficients, standard errors, and t values for the predictors in the baseline model. The second part of the table gives the corresponding values for random surprisal, restricted surprisal computed by NEAT, and full surprisal, residualized against the baseline predictors, in three models obtained by adding these predictors.
<R> <C> Model <C> Bleu-4 <C> Meteor <C> Rouge-L <R> <C> Original (2-stage attention) <C> [BOLD] 17.76 <C> [BOLD] 21.56 <C> [BOLD] 45.89 <R> <C> - without attention <C> 3.06 <C> 10.83 <C> 28.75 <R> <C> - without masking <C> 5.19 <C> 13.08 <C> 31.14 <R> <C> - with 1-stage attention <C> 14.52 <C> 18.28 <C> 40.10 <R> <C> - with 3-stage attention <C> 12.87 <C> 16.05 <C> 38.33 <CAP> Table 7: Ablation study on SQuAD split 1.
<R> <C> [BOLD] Used <C> [BOLD] Valid <C> [BOLD] Test <C> [BOLD] METEOR <R> <C> [BOLD] Attention <C> [BOLD] Perplexity <C> [BOLD] Perplexity <C> [BOLD] Score <R> <C> Baselines (Beam=10) <C> - <C> - <C> 0.2313 <R> <C> Baselines (Greedy) <C> - <C> - <C> 0.2776 <R> <C> Baselines (-Dups) <C> - <C> - <C> 0.3011 <R> <C> Baselines (+Grounded) <C> - <C> - <C> 0.3142 <R> <C> LSTM Seq2Seq <C> 21.89 <C> 22.18 <C> 0.2721 <R> <C> GLAC Net (-Cascading) <C> 20.24 <C> 20.54 <C> [BOLD] 0.3063 <R> <C> GLAC Net (-Global) <C> 18.32 <C> 18.47 <C> 0.2913 <R> <C> GLAC Net (-Local) <C> 18.21 <C> 18.33 <C> 0.2996 <R> <C> GLAC Net (-Count) <C> 18.13 <C> 18.28 <C> 0.2823 <R> <C> GLAC Net <C> [BOLD] 18.13 <C> [BOLD] 18.28 <C> 0.3014 <CAP> Table 1: Results from experiment settings. Baselines are reported in Huang et al. (2016).
<R> <C> Explainer <C> SST CSR <C> SST ACC <C> IMDB CSR <C> IMDB ACC <C> AgNews CSR <C> AgNews ACC <C> Yelp CSR <C> Yelp ACC <C> SNLI CSR <C> SNLI ACC <R> <C> BoW <C> - <C> 82.54 <C> - <C> 88.96 <C> - <C> 95.62 <C> - <C> 68.78 <C> - <C> 69.81 <R> <C> RNN <C> - <C> [BOLD] 86.16 <C> - <C> [BOLD] 91.79 <C> - <C> [BOLD] 96.28 <C> - <C> [BOLD] 75.80 <C> - <C> [BOLD] 78.34 <R> <C> Random <C> 69.41 <C> 70.07 <C> 67.30 <C> 66.67 <C> 92.38 <C> 91.14 <C> 58.27 <C> 53.06 <C> 75.83 <C> 68.74 <R> <C> Erasure <C> 80.12 <C> 81.22 <C> 92.17 <C> 88.72 <C> 97.31 <C> 95.41 <C> 78.72 <C> 68.90 <C> 77.88 <C> 70.04 <R> <C> Top- [ITALIC] k gradient <C> 79.35 <C> 79.24 <C> 86.30 <C> 83.93 <C> 96.49 <C> 94.86 <C> 70.54 <C> 62.86 <C> 76.74 <C> 69.40 <R> <C> Top- [ITALIC] k softmax <C> 84.18 <C> 82.43 <C> 93.06 <C> 89.46 <C> [BOLD] 97.59 <C> 95.61 <C> 81.00 <C> 70.18 <C> 78.66 <C> 71.00 <R> <C> Top- [ITALIC] k 1.5-entmax <C> [BOLD] 85.23 <C> [BOLD] 83.31 <C> 93.32 <C> 89.60 <C> 97.29 <C> [BOLD] 95.67 <C> 82.20 <C> 70.78 <C> 80.23 <C> 73.39 <R> <C> Top- [ITALIC] k sparsemax <C> [BOLD] 85.23 <C> 81.93 <C> [BOLD] 93.34 <C> 89.57 <C> 95.92 <C> 94.48 <C> 82.50 <C> 70.99 <C> [BOLD] 82.89 <C> [BOLD] 74.76 <R> <C> Selec. 1.5-entmax <C> 83.96 <C> 82.15 <C> 92.55 <C> [BOLD] 89.96 <C> 97.30 <C> 95.66 <C> 81.38 <C> 70.41 <C> 77.25 <C> 71.44 <R> <C> Selec. sparsemax <C> [BOLD] 85.23 <C> 81.93 <C> 93.24 <C> 89.66 <C> 95.92 <C> 94.48 <C> [BOLD] 83.55 <C> [BOLD] 71.60 <C> 82.04 <C> 73.46 <CAP> Table 3: Results for text classification and SNLI datasets. CSR stands for the communication success rate, and ACC is the accuracy of the classifier/layperson. The top rows are the original classifiers, which access all words. The middle rows report performance for random, wrapper and filter explainers, for fixed k-word messages (the values of k for the several datasets are {5, 10, 10, 10, 4}, respectively). The bottom rows correspond to embedded methods where k is given automatically via sparsity. The average k obtained by 1.5-entmax and sparsemax are: SST: {4.65, 2.59}; IMDB: {28.23, 12.94}; AgNews {5.65, 4.14}; Yelp: {60.61, 23.86}; SNLI: {12.96, 8.27}.
<R> <C> Explainer <C> [ITALIC] k=0 <C> [ITALIC] k=1 <C> [ITALIC] k=3 <C> [ITALIC] k=5 <R> <C> Top- [ITALIC] k gradient <C> 21.99 <C> 35.21 <C> 38.33 <C> 40.30 <R> <C> Top- [ITALIC] k softmax <C> 21.99 <C> [BOLD] 62.58 <C> 62.82 <C> 62.64 <R> <C> Top- [ITALIC] k 1.5-entmax <C> [BOLD] 22.31 <C> 62.53 <C> [BOLD] 63.48 <C> [BOLD] 62.69 <R> <C> Top- [ITALIC] k sparsemax <C> 22.14 <C> 62.21 <C> 61.94 <C> 61.92 <CAP> Table 4: Results for IWSLT. Reported are CSR scores.
<R> <C> [BOLD] Type <C> [BOLD] Vector*Vector [ms] <C> [BOLD] Matrix*Vector [ms] <C> [BOLD] Dot product [ms] <R> <C> double <C> 0,66 <C> 3,04 <C> 0,64 <R> <C> float <C> 0,59 <C> 1,95 <C> 0,58 <R> <C> half <C> 0,52 <C> 1,12 <C> 0,51 <CAP> Table 3: Time result of particular matrix operations
<R> <C> [BOLD] Method <C> [BOLD] NN <C> [BOLD] 3NN <C> [BOLD] Tree <R> <C> WALS  [ITALIC] shared-all <C> 71.6 <C> 71.4 <C> 69.1 <R> <C> WALS  [ITALIC] shared-pairwise <C> 73.1 <C> 74.1 <C> [BOLD] 74.2 <R> <C> ESL <C> 71.4 <C> 70.7 <C> [BOLD] 72.2 <CAP> Table 3: Typology reconstruction results. Three types of predictions are compared, nearest neighbor (NN), 3 nearest neighbors (3NN) and nearest tree neighbors (Tree). WALS shared-all are WALS based predictions, where only the 32 features that have known values in all 14 languages are used for computing language similarities. In the WALS shared-pairwise predictions the language similarities are computed using the WALS features shared by each language pair. ESL results are obtained by projection of WALS features from the closest languages according to the ESL language similarities.
<R> <C> NE <C> NE <C> FRE 0.22 <C> GER 0.18 <C> CZE 0.09 <C> ESP  [BOLD] 0.17 <R> <C> SA No Transfer <C> SA No Transfer <C> 0.03 <C> 0.01 <C> 0.00 <C> 0.00 <R> <C> SA <C> 0 <C> [BOLD] 0.26 <C> 0.24 <C> 0.06 <C> 0.04 <R> <C> SA <C> 1K <C> 0.24 <C> 0.20 <C> 0.09 <C> 0.13 <R> <C> SA <C> 2K <C> [BOLD] 0.26 <C> [BOLD] 0.25 <C> [BOLD] 0.10 <C> 0.12 <R> <C> SA <C> 3K <C> 0.22 <C> 0.19 <C> 0.08 <C> 0.11 <R> <C> SA <C> 4K <C> [BOLD] 0.26 <C> 0.20 <C> 0.09 <C> 0.13 <CAP> Table 1: The retrieval performance of NE, SA trained by the target language only (denoted as SA No Transfer), and SA of the source language tuning with different amounts of data. The numbers (0, 1K, 2K, 3K, 4K) are the amount of target language segments used to tune the original SA trained by the source language. For example, SA 2K means that the SA is first trained by the source language and then tuned by 2K target language segments.
<R> <C> Model <C> Precision <C> Recall <C> F1 <R> <C> Top Candidate <C> 79.0 <C> 86.8 <C> 82.7 <R> <C> [BOLD] SNERL <C> [BOLD] 83.3 <C> [BOLD] 90.2 <C> [BOLD] 86.6 <CAP> Table 4: Results for entity linking on the CDR dataset. Bold values are statistically significant (p-value <0.05 using Wilcoxon signed-rank test).
<R> <C> System <C> CER % Dev. <C> CER % Test <C> RTF/Latency <R> <C> KALDI (nnet3) * † ‡ <C> - <C> 8.6 <C> - <R> <C> KALDI (chain) * † ‡ <C> - <C> 7.4 <C> - <R> <C> ESPNet (Transformer) † [karita2019comparative] <C> 6.0 <C> 6.7 <C> - <R> <C> A-FMLM [chen2019non] <C> 6.2 <C> 6.7 <C> - <R> <C> Fan et al. [fan2019unsupervised] <C> - <C> 6.7 <C> - <R> <C> Transformer (ours) <C> 6.1 <C> 6.6 <C> 0.19 / 961ms <R> <C> LASO1 <C> 6.4 <C> 7.3 <C> 0.0034 / 17ms <R> <C> LASO1 + speed perturb <C> 6.0 <C> 6.8 <C> 0.0034 / 17ms <R> <C> LASO2 <C> 6.2 <C> 7.0 <C> 0.0043 / 21ms <R> <C> LASO2 + speed perturb <C> [BOLD] 5.8 <C> [BOLD] 6.4 <C> 0.0043 / 21ms <CAP> Table 3: The character error rates (CERs) of the systems on AISHELL-1. Latency is inference time per utterance on test set (including time of feature extraction). Real-time factor (RTF) is computed as the ratio of the total inference time to the total duration of the test set. Inference is done utterance by utterance without batching, on an NVIDIA RTX 2080Ti GPU.
<R> <C> Methods <C> MRR (↑) <C> WMRR (↑) <C> ARP (↓) <C> WARP (↓) <C> DCG (↑) <R> <C> Dense-only method <C> 0.650 <C> 0.563 <C> 2.143 <C> 2.474 <C> 0.759 <R> <C> Sparse-only method <C> 0.665 <C> 0.578 <C> 2.067 <C> 2.412 <C> 0.767 <R> <C> Concatenation method <C> 0.682 <C> 0.589 <C> 2.002 <C> 2.366 <C> 0.770 <R> <C> [BOLD] SepAttn <C> [BOLD] 0.686 <C> [BOLD] 0.595 <C> [BOLD] 1.992 <C> [BOLD] 2.350 <C> [BOLD] 0.781 <R> <C> Δ(%) <C> +0.59∗ <C> +1.02∗ <C> −0.50∗ <C> −0.68∗ <C> +1.43∗ <CAP> Table 2. Evaluations on testing set with percentage of improvements over the best baseline. Metrics with an upper arrow (↑) indicate higher is better; metrics with a down arrow (↓) indicate lower is better. Improvements with an asterisk (*) denotes statistical significance relative to the best performing baseline (Concatenation), according to the two-tailed paired t-test.
<R> <C> Model <C> SWB <C> CH <C> EV <R> <C> 5/320 LSTM + no LM <C> [BOLD] 27.7 <C> [BOLD] 47.5 <C> 37.6 <R> <C> 5/320 LSTM + 7-g <C> 20.0 <C> 38.5 <C> 29.3 <R> <C> 5/320 LSTM + 9-g <C> [BOLD] 19.7 <C> [BOLD] 38.2 <C> 29.0 <R> <C> 5*1 28 RBs, CNN + no LM <C> [BOLD] 27.9 <C> [BOLD] 48.6 <C> 38.3 <R> <C> 5*1 28 RBs, CNN + 7-g <C> 21.7 <C> 40.4 <C> 31.1 <R> <C> 5*1 28 RBs, CNN + 9-g <C> [BOLD] 21.3 <C> [BOLD] 40.0 <C> 30.7 <R> <C> Maas  + no LM <C> 38.0 <C> 56.1 <C> 47.1 <R> <C> Maas  + 7-g <C> 27.8 <C> 43.8 <C> 35.9 <R> <C> Maas  + RNN <C> 21.4 <C> 40.2 <C> 30.8 <R> <C> Zenkel  + no LM <C> 30.4 <C> 44.0 <C> 37.2 <R> <C> Zenkel  + RNN <C> 18.6 <C> 31.6 <C> 25.1 <R> <C> Zweig  + no LM <C> 25.9 <C> 38.8 <C> - <R> <C> Zweig  +  [ITALIC] n-g <C> 19.8 <C> 32.1 <C> - <CAP> Table 4: Final test set results on Eval2000.
<R> <C> [BOLD] Corpus <C> [BOLD] Team <C> [BOLD] Binary <C> [BOLD] Gap <C> [BOLD] Full <R> <C> AGRR <C> Winner <C> 0.96 <C> 0.90 <C> 0.89 <R> <C> AGRR <C> 2nd best <C> 0.95 <C> 0.86 <C> 0.84 <R> <C> SynTagRus <C> Winner <C> 0.91 <C> 0.76 <C> 0.77 <R> <C> SynTagRus <C> 2nd best <C> 0.88 <C> 0.67 <C> 0.64 <CAP> Таблица 3: Top systems F1 scores on AGRR-2019 and SynTagRus test set. Binary: binary classification; Gap: gap resolution; Full: full annotation.
<R> <C> [BOLD] model <C> [ITALIC] N <C> [BOLD] accuracy <C> [BOLD] F1 score <R> <C> TED end-to-end <C> 10 <C> 0.64 <C> 0.28 <R> <C> [EMPTY] <C> 2 <C> 0.62 <C> 0.24 <R> <C> TED modular <C> 10 <C> 0.73 <C> 0.63 <R> <C> [EMPTY] <C> 2 <C> 0.69 <C> 0.55 <R> <C> LSTM end-to-end <C> 10 <C> 0.51 <C> 0.23 <R> <C> [EMPTY] <C> 2 <C> 0.57 <C> 0.24 <R> <C> LSTM modular <C> 10 <C> 0.68 <C> 0.60 <R> <C> [EMPTY] <C> 2 <C> 0.61 <C> 0.54 <CAP> Table 1: Accuracy and F1 scores of the TED policy in end-to-end and modular mode, as well as the TED policy with the transformer replaced by an LSTM. Models are evaluated on the MultiWOZ 2.1 dataset using max_history N. All scores concern prediction at the action level on the test set.
<R> <C> #Mic <C> Session ID I <C> Session ID II <C> Session ID III <C> Session ID IV <C> Session ID V <C> Session ID VI <C> Session ID VII <C> Session ID VIII <C> Avg. <R> <C> 1 <C> 31.2 <C> 30.1 <C> 37.1 <C> 37.6 <C> 28.2 <C> 48.4 <C> 50.4 <C> 52.5 <C> 38.2 <R> <C> 2 <C> 22.9 <C> 25.3 <C> 30.5 <C> 37.0 <C> 21.8 <C> 41.7 <C> 36.8 <C> 45.7 <C> 31.4 <R> <C> 3 <C> 26.8 <C> 24.4 <C> 35.9 <C> 37.2 <C> 23.2 <C> 43.1 <C> 41.9 <C> 46.6 <C> 33.7 <R> <C> 6 <C> 22.3 <C> 22.2 <C> 36.0 <C> 32.1 <C> 21.0 <C> 38.1 <C> 35.1 <C> 44.3 <C> 30.2 <R> <C> 11 <C> 21.2 <C> 21.1 <C> 32.5 <C> 30.9 <C> 19.6 <C> 37.6 <C> 34.0 <C> 41.0 <C> 28.7 <R> <C> 11* <C> 17.0 <C> 16.3 <C> 21.7 <C> 21.2 <C> 17.7 <C> 27.0 <C> 27.0 <C> 32.8 <C> 21.8 <R> <C> Headset <C> 18.3 <C> 15.8 <C> 21.0 <C> 20.1 <C> 13.6 <C> 21.3 <C> 24.9 <C> 25.8 <C> 19.7 <CAP> Table 3: CERs (%) obtained with various scaling factors λ in Equation 6.
<R> <C> #Mic <C> [ITALIC] λ 2−3 <C> [ITALIC] λ 2−2 <C> [ITALIC] λ 2−1 <C> [ITALIC] λ 20 <C> [ITALIC] λ 21 <C> [ITALIC] λ 22 <C> [ITALIC] λ 23 <R> <C> 2 <C> 33.7 <C> 31.7 <C> 32.2 <C> [BOLD] 31.4 <C> 31.8 <C> 33.3 <C> 35.5 <R> <C> 3 <C> 34.2 <C> 34.3 <C> 34.0 <C> 33.7 <C> [BOLD] 33.0 <C> 34.8 <C> 35.2 <R> <C> 6 <C> 33.5 <C> 34.1 <C> 33.4 <C> [BOLD] 30.2 <C> 31.4 <C> 31.9 <C> 32.4 <R> <C> 11 <C> 33.5 <C> 32.5 <C> 31.1 <C> 28.7 <C> 28.9 <C> [BOLD] 28.4 <C> 28.9 <CAP> Table 3: CERs (%) obtained with various scaling factors λ in Equation 6.
<R> <C> Method <C> CER (%) <R> <C> Baseline (11 mics) <C> 28.7 <R> <C> w/o binary closing <C> 30.6 <R> <C> w/o speech enhancement <C> 37.8 <R> <C> w/o duplication reduction <C> 31.9 <CAP> Table 3: CERs (%) obtained with various scaling factors λ in Equation 6.
<R> <C> Metric Name <C> PvN ROC <C> HCvN ROC <R> <C> PolyMultiple <C> 0.834 <C> 0.874 <R> <C> [ITALIC] L2* <C> 0.783 <C> 0.809 <R> <C> CSim <C> 0.709 <C> 0.703 <R> <C> BestCenterL2 <C> 0.578 <C> 0.587 <R> <C> BestCenterCSim <C> 0.719 <C> 0.742 <R> <C> BestTopicPerWord <C> 0.686 <C> 0.731 <R> <C> TopicCorr <C> 0.609 <C> 0.496 <R> <C> TopicWalkLength* <C> 0.740 <C> 0.778 <R> <C> TopicWalkBtwn* <C> 0.659 <C> 0.658 <R> <C> TopicWalkEigen <C> 0.585 <C> 0.582 <R> <C> TopicNetCCoef* <C> 0.651 <C> 0.638 <R> <C> TopicNetMod* <C> 0.659 <C> 0.628 <CAP> TABLE I: The above summarizes all ROC area results for all considered metrics on the set of published vs. noise pairs (PvN) and highly-cited vs. noise pairs (HCvN). Metrics marked with a (*) have been sorted in reverse order for the ROC calculations.
<R> <C> Version <C> GCC 4.9.2  [ITALIC] f0( [ITALIC] x) <C> GCC 4.9.2  [ITALIC] f1( [ITALIC] x) <C> GCC 4.9.2  [ITALIC] f2( [ITALIC] x) <C> LLVM 3.6  [ITALIC] f0( [ITALIC] x) <C> LLVM 3.6  [ITALIC] f1( [ITALIC] x) <C> LLVM 3.6  [ITALIC] f2( [ITALIC] x) <R> <C> Cycles <C> 1364 <C> 1000 <C> [BOLD] 868 <C> 976 <C> 960 <C> [BOLD] 852 <CAP> Table 5: Cycles needed to resolve 128 polynomal functions of degree 4 – less cycles is better
<R> <C> [BOLD] Rank <C> [BOLD] Arabic <C> [BOLD] English <C> [BOLD] Portuguese <C> [BOLD] Spanish <C> [BOLD] Average <R> <C> 11th of 22 <C> 0.7569 <C> 0.7746 <C> 0.9788 <C> 0.8993 <C> 0.8524 <R> <C> BOW-baseline <C> 0.3394 <C> 0.6592 <C> 0.9712 <C> 0.7929 <C> 0.6907 <R> <C> STAT-baseline <C> 0.2500 <C> 0.1667 <C> 0.5000 <C> 0.1429 <C> 0.2649 <CAP> Table 2: Test set accuracy results for language variety identification.
<R> <C> Model <C> CoLA <C> SST-2 <C> MRPC <C> STS-B <C> QQP <C> MNLI <C> QNLI <C> RTE <C> WNLI <R> <C> BERTBASE <C> 57.82 <C> 92.09 <C> 86.74 <C> 88.13 <C> 87.49 <C> 84.01 <C> 90.79 <C> 64.98 <C> 53.52 <R> <C> BioBERT <C> 37.78 <C> 89.68 <C> 88.44 <C> 87.40 <C> 86.96 <C> 83.19 <C> 89.79 <C> 60.29 <C> 28.17 <R> <C> Delta <C> 20.04 <C> 2.41 <C> -1.69 <C> 0.73 <C> 0.53 <C> 0.82 <C> 1.01 <C> 4.69 <C> 25.35 <CAP> Table 1: Performance drop of BioBERT after further pre-training on Pubmed articles. The last row shows a positive value indicating the degree to which performance has dropped, and a negative value when it has increased.
<R> <C> [BOLD] Model <C> [BOLD] Inputs <C> [BOLD] F <C> [BOLD] AUC <C> [BOLD] ACC <R> <C> Random <C> - <C> 0.666 <C> 0.499 <C> 50.2 <R> <C> Davison [Davidson2017] <C> [ITALIC] TT <C> 0.703 <C> 0.732 <C> 68.4 <R> <C> LSTM <C> [ITALIC] TT <C> 0.703 <C> 0.732 <C> 68.3 <R> <C> FCM <C> [ITALIC] TT <C> 0.697 <C> 0.727 <C> 67.8 <R> <C> FCM <C> [ITALIC] TT,  [ITALIC] IT <C> 0.697 <C> 0.722 <C> 67.9 <R> <C> FCM <C> [ITALIC] I <C> 0.667 <C> 0.589 <C> 56.8 <R> <C> FCM <C> [ITALIC] TT,  [ITALIC] IT,  [ITALIC] I <C> 0.704 <C> 0.734 <C> 68.4 <R> <C> SCM <C> [ITALIC] TT,  [ITALIC] IT,  [ITALIC] I <C> 0.702 <C> 0.732 <C> 68.5 <R> <C> TKM <C> [ITALIC] TT,  [ITALIC] IT,  [ITALIC] I <C> 0.701 <C> 0.731 <C> 68.2 <CAP> Table 1: Performance of the proposed models, the LSTM and random scores. The Inputs column indicate which inputs are available at training and testing time.
<R> <C> Metrics <C> SNLI <C> MR <C> SST1 <C> SST2 <C> TREC <R> <C> [ITALIC] V>0.6 <C> 0.14 <C> 1.38 <C> 1.31 <C> 1.71 <C> 1.38 <R> <C> [ITALIC] V<0.05 <C> 99.68 <C> 97.39 <C> 97.21 <C> 96.36 <C> 97.16 <CAP> Table 4: Distribution of values in the sparse representations (%). V>0.6 (V<0.05) shows the frequency of the values greater (less) than 0.6 (0.05)
<R> <C> [BOLD] Comparison <C> Adversarial - Better <C> Adversarial - Worse <R> <C> Beamsearch <C> 36.9 <C> 34.8 <R> <C> Sampling <C> 35.7 <C> 33.2 <CAP> Table 3: Human evaluation comparing adversarial model vs the baseline model on 482 random samples. Correctness of captions. With agreement of at least 3 out of 5 judges in %. Humans agreed in 89.2% and 86.7% of images in beamsearch and sampling cases respectively.
<R> <C> Parameter <C> Value <R> <C> Number of encoder layers <C> 2 <R> <C> Encoder forward cell size <C> 128 <R> <C> Encoder backward cell size <C> 128 <R> <C> Number of decoder layers <C> 1 <R> <C> Decoder cell size <C> 512 <R> <C> Input BPE vocab size <C> 40000 <R> <C> BPE embedding size <C> 100 <R> <C> UPOS embedding size <C> 100 <R> <C> Language embedding size <C> 20 <R> <C> Dropout rate <C> 0.2 <R> <C> Learning rate <C> 1e-4 <R> <C> Batch size <C> 32 <CAP> Table 1: Hyperparameters
<R> <C> [EMPTY] <C> ISO <C> 1-NN/5-NN <C> Total/Groups <R> <C> English <C> en <C> 97.27/93.36 <C> 2784/160 <R> <C> German <C> de <C> 93.45/86.77 <C> 1282/91 <R> <C> Spanish <C> es <C> 93.81/86.24 <C> 1503/81 <R> <C> Chinese <C> zh <C> 71.26/61.44 <C> 167/22 <R> <C> Korean <C> ko <C> 28.27/18.40 <C> 527/40 <R> <C> Dutch <C> nl <C> 74.17/51.71 <C> 3171/452 <CAP> Table 3: Syntactic Nearest-Neighbour Accuracy (%)
<R> <C> [EMPTY] <C> English <C> German <C> Spanish <C> Chinese <C> Korean <C> Dutch <R> <C> Model <C> 1-NN/5-NN <C> 1-NN/5-NN <C> 1-NN/5-NN <C> 1-NN/5-NN <C> 1-NN/5-NN <C> 1-NN/5-NN <R> <C> USE <C> 71.83/55.68 <C> 59.87/44.26 <C> 53.05/38.06 <C> 39.23/30.18 <C> 21.22/12.43 <C> 28.66/12.77 <R> <C> BERT [ITALIC] max <C> [BOLD] 90.19/86.36 <C> [BOLD] 83.66/77.63 <C> [BOLD] 83.89/79.92 <C> [BOLD] 67.96/68.40 <C> 20.30/11.92 <C> 37.67/19.51 <R> <C> BERT [ITALIC] avg <C> 89.06/84.70 <C> 79.54/74.82 <C> 78.24/75.61 <C> 65.75/67.07 <C> 20.30/11.47 <C> 37.04/19.46 <R> <C> BERT [ITALIC] output <C> 77.75/63.44 <C> 66.20/51.89 <C> 65.21/50.41 <C> 52.49/46.34 <C> 16.39/10.98 <C> 24.27/10.67 <R> <C> LASER <C> 86.33/76.66 <C> 76.56/62.88 <C> 72.49/59.72 <C> 56.89/45.15 <C> [BOLD] 26.63/15.90 <C> [BOLD] 50.75/31.00 <CAP> Table 4: Syntactic Nearest-Neighbour for Language Models (%)
<R> <C> [EMPTY] <C> Model <C> Sub-Goal Ablations -  [BOLD] Validation Goto <C> Sub-Goal Ablations -  [BOLD] Validation Pickup <C> Sub-Goal Ablations -  [BOLD] Validation Put <C> Sub-Goal Ablations -  [BOLD] Validation Cool <C> Sub-Goal Ablations -  [BOLD] Validation Heat <C> Sub-Goal Ablations -  [BOLD] Validation Clean <C> Sub-Goal Ablations -  [BOLD] Validation Slice <C> Sub-Goal Ablations -  [BOLD] Validation Toggle <C> Avg. <R> <C> [ITALIC] Seen <C> No Lang <C> 28 <C> 22 <C> 71 <C>  [BOLD] 89 <C>  [BOLD] 87 <C> 64 <C> 19 <C> 90 <C> 59 <R> <C> [ITALIC] Seen <C> S2S <C> 49 <C> 32 <C> 80 <C> 87 <C> 85 <C>  [BOLD] 82 <C> 23 <C> 97 <C> 67 <R> <C> [ITALIC] Seen <C> S2S + PM <C>  [BOLD] 51 <C>  [BOLD] 32 <C>  [BOLD] 81 <C> 88 <C> 85 <C> 81 <C>  [BOLD] 25 <C>  [BOLD] 100 <C>  [BOLD] 68 <R> <C> [ITALIC] Unseen <C> No Lang <C> 17 <C> 9 <C> 31 <C> 75 <C> 86 <C> 13 <C> 8 <C> 4 <C> 30 <R> <C> [ITALIC] Unseen <C> S2S <C> 21 <C> 20 <C>  [BOLD] 51 <C>  [BOLD] 94 <C> 88 <C> 21 <C>  [BOLD] 14 <C>  [BOLD] 54 <C> 45 <R> <C> [ITALIC] Unseen <C> S2S + PM <C>  [BOLD] 22 <C>  [BOLD] 21 <C> 46 <C> 92 <C>  [BOLD] 89 <C>  [BOLD] 57 <C> 12 <C> 32 <C>  [BOLD] 46 <CAP> Table 4: Evaluations by path weighted sub-goal success. All values are percentages. The highest values per fold and task are shown in blue. We note that the No Vision model achieves less than 2% on all sub-goals. See supplemental material for more.
<R> <C> [EMPTY] <C> Inception-v4 <C> BOW <C> master-T <C> master-IT <R> <C> top-1 acc <C> 69.4 <C> 83.1 <C> 92.6 <C> 94.7 <R> <C> top-3 acc <C> 85.5 <C> 90.7 <C> 97.8 <C> 98.2 <R> <C> top-5 acc <C> 90.3 <C> 93.1 <C> 98.5 <C> 99.1 <CAP> Table 1. Master model classification accuracy (percentile).
<R> <C> # utterances <C> Model <C> Navigation <C> Scheduling <C> Weather <R> <C> 100 <C> Baseline <C> 59.93 <C> 68.29 <C> 82.43 <R> <C> [EMPTY] <C> Ours <C> 72.91 <C> 77.30 <C> 90.55 <R> <C> 500 <C> Baseline <C> 78.99 <C> 86.05 <C> 93.68 <R> <C> [EMPTY] <C> Ours <C> 78.46 <C> 87.67 <C> 94.01 <CAP> Table 3: The results on Stanford dialogue dataset.
<R> <C> Model Ours <C> F-score 88. <C> F-score 72 <C> # new 301 <C> max. ED 3.18 <R> <C> - seq2seq generation <C> -0. <C> 84** <C> 0 <C> 0 <R> <C> - diversity ranks <C> -0. <C> 40* <C> 163 <C> 2.42 <R> <C> - filtering <C> -0. <C> 38 <C> 870 <C> 2.86 <CAP> Table 4: The result of the ablation test. # new marks the number of newly generated delexicalised utterances. max. ED marks the averaged maximum edit distances. Here we use * to indicate that the result is statistically significant under t-test (** for p-value threshold as 0.05 and * for threshold as 0.1) By removing the seq2seq generation from our method, no delexicalised utterance will be generated so the max. ED cell is 0.
<R> <C> [EMPTY] <C> E&M BLEU <C> E&M GLEU <C> F&R BLEU <C> F&R GLEU <R> <C> RuleBased <C> 60.37 <C> 16.48 <C> 66.4 <C> 18.79 <R> <C> PBMT <C> 66.88 <C> 24.38 <C> 72.4 <C> 26.96 <R> <C> NMT <C> 58.27 <C> 22.87 <C> 68.26 <C> 26.3 <R> <C> NMT-Copy <C> 58.67 <C> 22.93 <C> 68.09 <C> 26.05 <R> <C> NMT-Combine <C> 67.51 <C> 24.05 <C> 73.78 <C> 26.74 <R> <C> SimpleCopy <C> 50.28 <C> 7.42 <C> 51.66 <C> 6.8 <R> <C> Transformer <C> 61.86 <C> 21.61 <C> 66.69 <C> 24.94 <R> <C> Transformer-Combine <C> 65.5 <C> 23.94 <C> 70.63 <C> 25.88 <R> <C> MultiTask* <C> 72.01 <C> 25.92 <C> 75.35 <C> 27.15 <R> <C> Ablt. w/o self-recon <C> 64.53 <C> 22.81 <C> 70.43 <C> 22.92 <R> <C> Ablt. w/o cyc-recon <C> 66.39 <C> 23.53 <C> 71.71 <C> 25.50 <R> <C> Ablt. w/o class-guided <C> 67.90 <C> 24.13 <C> 72.00 <C> 24.64 <R> <C> Ours <C> 69.08 <C> 24.37 <C> 72.90 <C> 24.78 <R> <C> Ours w/ class-filter <C> 68.71 <C> 24.64 <C> 73.16 <C> 25.73 <R> <C> Ours w/ gec <C> [BOLD] 69.63 <C> [BOLD] 25.78 <C> [BOLD] 74.43 <C> [BOLD] 27.35 <CAP> Table 3: BLEU and GLEU scores on GYAFC dataset. The dataset has two domains: Entertainment & Music (E&M) and Family & Relationship (F&R). The best single model score under each metric is marked bold. MultiTask* is not comparable to other models in the table since it uses more supervised data and ensemble decoding.
<R> <C> [EMPTY] <C> #occurrence <C> #M-biased occs. <C> #F-biased occs. <R> <C> M <C> 5,300,000 <C> 170,000 <C> 81,000 <R> <C> F <C> 1,600,000 <C> 33,000 <C> 36,000 <CAP> Table 1: Training corpus for ELMo. We show total counts for male (M) and female (F) pronouns in the corpus, and counts corresponding to their co-occurrence with occupation words where the occupations are stereotypically male (M-biased) or female (F-biased).
<R> <C> Systems <C> 1st <C> 2nd <C> 3rd <C> 4th <C> 5th <C> MR <R> <C> BERTAbs <C> 0.08 <C> 0.10 <C> 0.18 <C> 0.26 <C> 0.38 <C> 3.76 <R> <C> UniLM <C> 0.06 <C> 0.22 <C> 0.21 <C> 0.26 <C> 0.25 <C> 3.42 <R> <C> RoBERTa-S2S <C> 0.13 <C> 0.15 <C> 0.24 <C> 0.29 <C> 0.19 <C> 3.26 <R> <C> Step <C> 0.25 <C> 0.36 <C> 0.18 <C> 0.10 <C> 0.11 <C> 2.46 <R> <C> Gold <C> 0.48 <C> 0.17 <C> 0.19 <C> 0.09 <C> 0.07 <C> 2.10 <CAP> Table 3: Human evaluation results: proportions of system rankings. MR: mean rank (the lower the better).
<R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> Top 200 Ter-improved Sentences Common Input <C> Top 200 Ter-improved Sentences Same Translation <R> <C> Model <C> Classes <C> #vocab <C> [%] <C> [%] <R> <C> map-each <C> optimized <C> 100 <C> - <C> - <R> <C> [EMPTY] <C> non-optimized <C> 100 <C> 89.5 <C> 89.9 <R> <C> [EMPTY] <C> random <C> 100 <C> 88.5 <C> 89.8 <R> <C> [EMPTY] <C> lemma <C> 26744 <C> 87.0 <C> 92.6 <R> <C> map-all <C> optimized <C> 100 <C> 56.0 <C> 54.5 <CAP> Table 5: Comparison of translation outputs for the smoothing models with different vocabularies. “optimized” denotes 30 iterations of the clustering algorithm, whereas “non-optimized” means the initial (default) clustering.
<R> <C> [BOLD] Approach <C> F-score <C> F-score-micro <C> F-score-macro <C> Accuracy <C> ARI <C> AMI <C> NMI <R> <C> 20 categories <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> TF-IDF+k-Means <C> 33.0 <C> 33.1 <C> 32.0 <C> 33.1 <C> 14.2 <C> 33.7 <C> 37.0 <R> <C> NMF <C> 35.1 <C> 34.2 <C> 34.2 <C> 34.2 <C> 18.3 <C> 33.4 <C> 34.5 <R> <C> LDA <C> 37.5 <C> 31.6 <C> 30.3 <C> 31.6 <C> 13.8 <C> 33.8 <C> 37.1 <R> <C> LSA+k-Means <C> 39.4 <C> 37.5 <C> 38.1 <C> 37.5 <C> 18.1 <C> 37.0 <C> 38.7 <R> <C> Ours <C> [BOLD] 42.2 <C> [BOLD] 50.8 <C> [BOLD] 40.6 <C> [BOLD] 50.8 <C> [BOLD] 41.6 <C> [BOLD] 53.1 <C> [BOLD] 57.1 <R> <C> 4 categories <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> TF-IDF+k-Means <C> 59.8 <C> 58.4 <C> 59.3 <C> 58.4 <C> 25.6 <C> 29.1 <C> 29.2 <R> <C> NMF <C> - <C> - <C> - <C> 64.3 <C> - <C> - <C> 44.3 <R> <C> LDA <C> 54.7 <C> 61.8 <C> 53.2 <C> 61.8 <C> 35.3 <C> 34.3 <C> 37.7 <R> <C> LSA+k-Means <C> 63.7 <C> 63.1 <C> 63.3 <C> 63.1 <C> 28.7 <C> 33.5 <C> 34.7 <R> <C> GMM <C> - <C> - <C> - <C> 51.9 <C> - <C> - <C> 20.5 <R> <C> PLSA <C> - <C> - <C> - <C> 66.5 <C> - <C> - <C> 47.6 <R> <C> M2DCU <C> - <C> - <C> - <C> 69.0 <C> - <C> - <C> 40.8 <R> <C> Ours <C> [BOLD] 78.9 <C> [BOLD] 79.1 <C> [BOLD] 78.6 <C> [BOLD] 79.1 <C> [BOLD] 55.3 <C> [BOLD] 52.9 <C> [BOLD] 53.0 <CAP> Table 3: The clustering results on 20 categories and 4 categories. We compare our method with TF-IDF+k-Means, NMF, LDA, LSA+k-Means, GMM, PLSA and M2DCU Zhang et al. (2011). We obtain the best performance on both category partitions with all evaluation metrics.
<R> <C> System <C> IWSLT05 <C> NIST03 <R> <C> Moses <C> 52.5 <C> 30.6 <R> <C> NMT <C> 43.9 <C> 31.3 <R> <C> NMT-L <C> 45.9 <C> 31.7 <R> <C> Arthur et al. <C> [EMPTY] <C> [EMPTY] <R> <C> M-NMT( [ITALIC] s, [ITALIC] uy) <C> 49.8 <C> 32.3 <R> <C> M-NMT( [ITALIC] sy, [ITALIC] uy) <C> 50.7 <C> 32.5 <R> <C> M-NMT( [ITALIC] s, [ITALIC] uxy) <C> 51.4 <C> 32.8 <R> <C> M-NMT( [ITALIC] sy, [ITALIC] uxy) <C> [BOLD] 52.9 <C> [BOLD] 34.0 <CAP> Table 3: BLEU scores with different translation systems on the two Chinese-English translation datasets.
<R> <C> [BOLD] Model <C> [BOLD] R5@1 <C> [BOLD] R5@2 <C> [BOLD] R11@1 <C> [BOLD] R11@2 <R> <C> PT-CoDE [ITALIC] small <C> 70.8 <C> 88.0 <C> 56.2 <C> 72.7 <R> <C> PT-CoDE [ITALIC] mid <C> 73.8 <C> 89.7 <C> 60.4 <C> 76.4 <R> <C> PT-CoDE [ITALIC] large <C> 77.2 <C> 91.3 <C> 64.2 <C> 79.1 <CAP> Table 2: Testing results of CoDE on the CoCo task.
<R> <C> Parameter <C> Definition <C> Default value <R> <C> [ITALIC] sg <C> Choice of training algorithm  [ITALIC] sg= 1 skip-gram  [ITALIC] sg= 0 CBOW <C> 1 <R> <C> [ITALIC] size <C> Dimension of the obtained vectors <C> 200 <R> <C> [ITALIC] min_ [ITALIC] count <C> Words with frequency lower than this value will be ignored <C> 1 <R> <C> [ITALIC] window <C> Maximum distance between the current and the predicted word <C> 5 <R> <C> [ITALIC] iter <C> Number of iterations <C> 5 <R> <C> [ITALIC] negative <C> Whether negative sampling will be used and how many ‘‘noise words’’ would be drawn <C> 5 <CAP> Table 1: Parameters used for training the Word2Vec model.
<R> <C> Root CAMEO <C> Description <C> Quad Class <R> <C> 01 <C> Make Public Statement <C> 0 <R> <C> 02 <C> Appeal <C> 0 <R> <C> 03 <C> Express Intent to Coop <C> 1 <R> <C> 04 <C> Consult <C> 1 <R> <C> 05 <C> Engage in Dip Coop <C> 1 <R> <C> 06 <C> Engage in Material Coop <C> 2 <R> <C> 07 <C> Provide Aid <C> 2 <R> <C> 08 <C> Yield <C> 2 <R> <C> 09 <C> Investigate <C> 3 <R> <C> 10 <C> Demand <C> 3 <R> <C> 11 <C> Disapprove <C> 3 <R> <C> 12 <C> Reject <C> 3 <R> <C> 13 <C> Threaten <C> 3 <R> <C> 14 <C> Protest <C> 4 <R> <C> 15 <C> Exhibit Force Posture <C> 4 <R> <C> 16 <C> Reduce Relations <C> 3 <R> <C> 17 <C> Coerce <C> 4 <R> <C> 18 <C> Assault <C> 4 <R> <C> 19 <C> Fight <C> 4 <R> <C> 20 <C> Use Unconventional Mass Violence <C> 4 <CAP> Table 1: CAMEO-to-Quad Class Conversions
<R> <C> [BOLD] Length <C> [BOLD] GloVe <C> [BOLD] Word2vec <C> [BOLD] FastText <R> <C> 1,000 <C> 1.026 <C> 1.026 <C> 1.079 <R> <C> 1,500 <C> 1.122 <C> 1.093 <C> 1.019 <R> <C> 2,000 <C> 1.068 <C> 1.091 <C> 1.091 <R> <C> 2,500 <C> 1.020 <C> 1.061 <C> 1.082 <R> <C> 5,000 <C> 1.036 <C> 1.054 <C> 1.071 <R> <C> 10,000 <C> 1.045 <C> 1.030 <C> 1.015 <CAP> Table 4: Comparison between the best results obtained via global and local thresholding. For each text length and embedding method, we show maxΓ(L)+/maxΓ(G)+, where Γ(L)+ and Γ(G)+ are the accuracy obtained with local and global thresholding strategy, respectively. We only show the results obtained with the SVM, since it turned out to be the classifier yielding the highest accuracy rates. These results point that the use of this local strategy in the filtering process does not improve the performance of the classification.
<R> <C> Model <C> Model parameters (M) <C> Test Perplexity <R> <C> LR LSTM 200-200 <C> 0.928 <C> 136.115 <R> <C> LSTM-SparseVD-VOC <C> 1.672 <C> 120.2 <R> <C> KN5 + cache <C> 2 <C> 125.7 <R> <C> LR LSTM 400-400 <C> 3.28 <C> 106.623 <R> <C> LSTM-SparseVD <C> 3.312 <C> 109.2 <R> <C> RNN-LDA + KN-5 + cache <C> 9 <C> 92 <R> <C> AWD-LSTM <C> 22 <C> 55.97 <R> <C> [BOLD] RLSTM-Tied-Dropout (r=0.5) <C> [BOLD] 2 (Embedding) +  [BOLD] 0.553 (RNN) <C> [BOLD] 103.5 <CAP> TABLE I: Comparison with state-of-the-art architectures in terms of Test Perplexity on Penn Treebank dataset
<R> <C> Dataset Models <C> Douban MAP <C> Douban MRR <C> Douban P@1 <C> ECD MAP <C> ECD MRR <C> ECD P@1 <R> <C> MV-LSTM <C> 0.498 <C> 0.538 <C> 0.348 <C> 0.613 <C> 0.684 <C> 0.525 <R> <C> Match-LSTM <C> 0.500 <C> 0.537 <C> 0.345 <C> - <C> - <C> - <R> <C> Multiview <C> 0.505 <C> 0.543 <C> 0.342 <C> - <C> - <C> - <R> <C> DL2R <C> 0.488 <C> 0.527 <C> 0.330 <C> 0.604 <C> 0.661 <C> 0.489 <R> <C> SMN <C> 0.530 <C> 0.569 <C> 0.378 <C> 0.666 <C> 0.739 <C> 0.591 <R> <C> SMN-WM <C> 0.550* <C> 0.589* <C> 0.397* <C> 0.670 <C> 0.749* <C> 0.612* <R> <C> DAM <C> 0.551 <C> 0.598 <C> 0.423 <C> 0.683 <C> 0.756 <C> 0.621 <R> <C> DAM-WM <C> [BOLD] 0.584* <C> [BOLD] 0.636* <C> [BOLD] 0.459* <C> [BOLD] 0.686 <C> [BOLD] 0.771* <C> [BOLD] 0.647* <CAP> Table 1: Results on two datasets. Numbers marked with * indicate that the improvement is statistically significant compared with the pre-trained baseline (t-test with p-value < 0.05). We copy the numbers from [16] for the baseline models. Because the first four baselines obtain similar results in Douban dataset, we only implement two of them in ECD dataset.
<R> <C> Method <C> Models <C> MAP <C> MRR <C> P@1 <R> <C> Original <C> DAM <C> 0.551 <C> 0.598 <C> 0.423 <R> <C> Heuristic <C> DAM-uniform <C> 0.577 <C> 0.623 <C> 0.433 <R> <C> Heuristic <C> DAM-random <C> 0.549 <C> 0.594 <C> 0.399 <R> <C> Heuristic <C> DAM-jaccard <C> 0.572 <C> 0.622 <C> 0.438 <R> <C> Heuristic <C> DAM-embedding <C> 0.573 <C> 0.615 <C> 0.426 <R> <C> Model-based <C> DAM-DAM <C> 0.580 <C> 0.627 <C> 0.438 <R> <C> Model-based <C> DAM-last-WM <C> 0.578 <C> 0.625 <C> 0.439 <R> <C> Model-based <C> DAM-dual <C> 0.579 <C> 0.621 <C> 0.430 <R> <C> Ours <C> DAM-WM <C> 0.584 <C> 0.636 <C> 0.459 <CAP> Table 2: Evaluation of DAM with different weighting strategies on Douban dataset.
<R> <C> [EMPTY] <C> Alternating Languages  [ITALIC] λ=10k <C> Alternating Languages  [ITALIC] λ=10k <C> Alternating Languages  [ITALIC] λ=10k <C> Alternating Languages  [ITALIC] λ=100k <C> Alternating Languages  [ITALIC] λ=100k <C> Alternating Languages  [ITALIC] λ=100k <C> Alternating Domains  [ITALIC] λ=10k <C> Alternating Domains  [ITALIC] λ=10k <C> Alternating Domains  [ITALIC] λ=10k <C> Alternating Domains  [ITALIC] λ=20k <C> Alternating Domains  [ITALIC] λ=20k <C> Alternating Domains  [ITALIC] λ=20k <R> <C> [EMPTY] <C> ppl <C> ppl@sw <C> rec <C> ppl <C> ppl@sw <C> rec <C> ppl <C> ppl@sw <C> rec <C> ppl <C> ppl@sw <C> rec <R> <C> Ind. LSTM <C> 7.1 <C> 7.16 <C> 1.15 <C> 4.7 <C> 4.73 <C> 1.18 <C> 356 <C> 349 <C> 1.11 <C> 295 <C> 292 <C> 1.15 <R> <C> Large LSTM <C> 7.78 <C> 10.4 <C> 6.82 <C> [BOLD] 4.86 <C> 8.58 <C> 18.9 <C> 352 <C> 406 <C> 3.61 <C> 457 <C> 619 <C> 6.56 <R> <C> Transformer <C> 14.4 <C> 15.7 <C> 2.37 <C> 8.7 <C> 11.7 <C> 11.5 <C> 455 <C> 497 <C> [BOLD] 2.4 <C> 352 <C> 369 <C> [BOLD] 2.56 <R> <C> PoE 5 <C> 7.68 <C> 10.1 <C> 7.06 <C> 5.32 <C> 9.79 <C> 25.5 <C> 297 <C> 389 <C> 5.18 <C> 404 <C> 505 <C> 4.47 <R> <C> PoE 30 <C> 7.96 <C> 10.7 <C> 7.33 <C> 5.17 <C> 9.9 <C> 24.8 <C> 315 <C> 375 <C> 3.89 <C> 297 <C> 389 <C> 5.18 <R> <C> FF-PoE 5 <C> [BOLD] 7.2 <C> [BOLD] 8.46 <C> [BOLD] 3.67 <C> 5.02 <C> 7.54 <C> 14.9 <C> 320 <C> 361 <C> 2.82 <C> 270 <C> 322 <C> 3.35 <R> <C> FF-PoE 30 <C> 7.41 <C> 9.17 <C> 4.76 <C> 5.04 <C> [BOLD] 7 <C> [BOLD] 9.03 <C> [BOLD] 285 <C> [BOLD] 316 <C> 2.68 <C> [BOLD] 241 <C> [BOLD] 287 <C> 3.54 <CAP> Table 2: Average perplexity (ppl), perplexity for 10 batches after a switch (ppl@sw) and recovery time after a switch in batches (rec) for the multilingual (left) and multi-domain (right) datasets per mean sequence length (λ).
<R> <C> LID <C> Len <C> CC <C> Att <C> FR <C> Hallu. <C> BLEU <R> <C> [EMPTY] <C> [EMPTY] <C> ✓ <C> [EMPTY] <C> 126 <C> 46 <C> 34.4 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 0 <C> 12 <C> 34.8 <R> <C> ✓ <C> ✓ <C> [EMPTY] <C> [EMPTY] <C> 0 <C> 0 <C> 35.2 <R> <C> ✓ <C> [EMPTY] <C> ✓ <C> [EMPTY] <C> 0 <C> 29 <C> 37.7 <R> <C> ✓ <C> ✓ <C> ✓ <C> ✓ <C> 0 <C> 0 <C> 38.7 <R> <C> ✓ <C> ✓ <C> ✓ <C> [EMPTY] <C> 0 <C> 10 <C> 39.6 <CAP> Table 4: Number of hallucinations and French-language outputs (according to langid.py) when translating MTNT-test (FR→EN). LID: language identifier, Len: length filtering, CC: training data includes CommonCrawl, Att: attention-based filtering.
<R> <C> [EMPTY] <C> [ITALIC] μ=1 F1 <C> [ITALIC] μ=1 #Q <C> [ITALIC] μ=1 #L <C> [ITALIC] μ=5 F1 <C> [ITALIC] μ=5 #Q <C> [ITALIC] μ=5 #L <C> [ITALIC] μ=10 F1 <C> [ITALIC] μ=10 #Q <C> [ITALIC] μ=10 #L <C> [ITALIC] μ=20 F1 <C> [ITALIC] μ=20 #Q <C> [ITALIC] μ=20 #L <R> <C> IIMB <C> 96.7% <C> 8 <C> 8 <C> 96.7% <C> 10 <C> 2 <C> 96.7% <C> 20 <C> 2 <C> 96.9% <C> 40 <C> 2 <R> <C> D-A <C> 97.8% <C> 52 <C> 52 <C> 97.8% <C> 60 <C> 12 <C> 97.7% <C> 60 <C> 6 <C> 97.3% <C> 80 <C> 4 <R> <C> I-Y <C> 71.4% <C> 102 <C> 102 <C> 71.3% <C> 105 <C> 21 <C> 71.3% <C> 110 <C> 11 <C> 71.4% <C> 120 <C> 6 <R> <C> D-Y <C> 87.3% <C> 127 <C> 127 <C> 87.2% <C> 135 <C> 27 <C> 87.3% <C> 140 <C> 14 <C> 87.2% <C> 160 <C> 8 <CAP> TABLE VII: F1-score and number of questions with different question number thresholds per round
<R> <C> Model <C> Macro F1 <C> Macro Precision <C> Macro Recall <R> <C> SVM <C> 0.748±0.007 <C> 0.795±0.007 <C> 0.731±0.006 <R> <C> TextCNN <C> 0.754±0.020 <C> 0.772±0.031 <C> 0.749±0.031 <R> <C> Bi-LSTM Attention <C> 0.761±0.016 <C> 0.758±0.016 <C> 0.769±0.021 <R> <C> BERT <C> 0.761±0.021 <C> 0.762±0.019 <C> 0.761±0.024 <R> <C> BioBERT <C> 0.764±0.010 <C> 0.774±0.015 <C> 0.758±0.009 <R> <C> LESA-BERT <C> 0.789±0.011 <C> 0.784±0.010 <C> 0.797±0.014 <R> <C> Distil-LESA-BERT-6 <C> 0.807±0.009 <C> 0.816±0.004 <C> 0.798±0.024 <R> <C> Distil-LESA-BERT-3 <C> 0.780±0.017 <C> 0.768±0.016 <C> 0.816±0.015 <CAP> Table 3: Performance metrics of different classifiers on the patient messages data.
<R> <C> [EMPTY] <C> Laptop <C> Restaurant <R> <C> Majority <C> 53.45 <C> 65.00 <R> <C> Feature+SVM <C> [BOLD] 72.10 <C> [BOLD] 80.89 <R> <C> LSTM <C> 66.45 <C> 74.28 <R> <C> TDLSTM <C> 68.13 <C> 75.63 <R> <C> TDLSTM+ATT <C> 66.24 <C> 74.31 <R> <C> ContextAVG <C> 61.22 <C> 71.33 <R> <C> MemNet (1) <C> 67.66 <C> 76.10 <R> <C> MemNet (2) <C> 71.14 <C> 78.61 <R> <C> MemNet (3) <C> 71.74 <C> 79.06 <R> <C> MemNet (4) <C> 72.21 <C> 79.87 <R> <C> MemNet (5) <C> 71.89 <C> 80.14 <R> <C> MemNet (6) <C> 72.21 <C> 80.05 <R> <C> MemNet (7) <C> [BOLD] 72.37 <C> 80.32 <R> <C> MemNet (8) <C> 72.05 <C> 80.14 <R> <C> MemNet (9) <C> 72.21 <C> [BOLD] 80.95 <CAP> Table 2: Classification accuracy of different methods on laptop and restaurant datasets. Best scores in each group are in bold.
<R> <C> Methods <C> APR MAP@10 <C> APR MAP@20 <C> APR MAP@50 <C> Wiki MAP@10 <C> Wiki MAP@20 <C> Wiki MAP@50 <C> PubMed-CVD MAP@10 <C> PubMed-CVD MAP@20 <C> PubMed-CVD MAP@50 <R> <C> EgoSet <C> 0.3949 <C> 0.3942 <C> 0.3706 <C> 0.5899 <C> 0.5754 <C> 0.5622 <C> 0.0511 <C> 0.0410 <C> 0.0441 <R> <C> SEISA <C> 0.7423 <C> 0.6090 <C> 0.3892 <C> 0.7643 <C> 0.6606 <C> 0.4998 <C> - <C> - <C> - <R> <C> word2vec <C> 0.6054 <C> 0.5385 <C> 0.4180 <C> 0.7193 <C> 0.6289 <C> 0.4510 <C> 0.8427 <C> 0.7701 <C> 0.6895 <R> <C> PTE <C> 0.3144 <C> 0.2777 <C> 0.1996 <C> 0.6817 <C> 0.5596 <C> 0.3839 <C> 0.9071 <C> 0.7654 <C> 0.5641 <R> <C> SetExpan− [ITALIC] cs <C> 0.8240 <C> 0.7997 <C> 0.7674 <C> 0.9540 <C> 0.8955 <C> 0.7439 <C> [BOLD] 1.000 <C> [BOLD] 1.000 <C> 0.5991 <R> <C> SetExpan− [ITALIC] re <C> 0.8509 <C> 0.7792 <C> 0.7681 <C> 0.9392 <C> 0.8680 <C> 0.7291 <C> [BOLD] 1.000 <C> 0.9605 <C> 0.7371 <R> <C> SetExpan [ITALIC] full <C> [BOLD] 0.8967 <C> [BOLD] 0.8621 <C> [BOLD] 0.7885 <C> [BOLD] 0.9571 <C> [BOLD] 0.9010 <C> [BOLD] 0.7457 <C> [BOLD] 1.000 <C> [BOLD] 1.000 <C> [BOLD] 0.7454 <CAP> Table 2: Overall end-to-end performance evaluation on 3 datasets over all queries.
<R> <C> Data Methods <C> UDC MAP <C> UDC R@5 <C> UDC R@2 <C> UDC R@1 <C> UDC Time <C> AliMeData MAP <C> AliMeData R@5 <C> AliMeData R@2 <C> AliMeData R@1 <C> AliMeData Time <R> <C> ARC-I <C> 0.2810 <C> 0.4887 <C> 0.1840 <C> 0.0873 <C> 16 <C> 0.7314 <C> 0.6383 <C> 0.3733 <C> 0.2171 <C> 23 <R> <C> ARC-II <C> 0.5451 <C> 0.8197 <C> 0.5349 <C> 0.3498 <C> 17 <C> 0.7306 <C> 0.6595 <C> 0.3671 <C> 0.2236 <C> 24 <R> <C> Pyramid <C> 0.6418 <C> 0.8324 <C> 0.6298 <C> 0.4986 <C> 17 <C> 0.8389 <C> 0.7604 <C> 0.4778 <C> 0.3114 <C> 27 <R> <C> Duet <C> 0.5692 <C> 0.8272 <C> 0.5592 <C> 0.4756 <C> 20 <C> 0.7651 <C> 0.6870 <C> 0.4088 <C> 0.2433 <C> 30 <R> <C> MV-LSTM <C> 0.6918 <C> 0.8982 <C> 0.7005 <C> 0.5457 <C> 1632 <C> 0.7734 <C> 0.7017 <C> 0.4105 <C> 0.2480 <C> 2495 <R> <C> SMN <C> [BOLD] 0.7327 <C> [BOLD] 0.9273 <C> 0.7523 <C> 0.5948 <C> 64 <C> 0.8145 <C> 0.7271 <C> 0.4680 <C> 0.2881 <C> 91 <R> <C> MT-hCNN-d <C> 0.7027 <C> 0.8992 <C> 0.7512 <C> 0.5838 <C> 20 <C> 0.8401 <C> 0.7712 <C> 0.4788 <C> 0.3238 <C> 31 <R> <C> MT-hCNN <C> 0.7323 <C> 0.9172 <C> [BOLD] 0.7525 <C> [BOLD] 0.5978 <C> 24 <C> [BOLD] 0.8418 <C> [BOLD] 0.7810 <C> [BOLD] 0.4796 <C> [BOLD] 0.3241 <C> 36 <CAP> Table 1: Comparison of base models on Ubuntu Dialog Corpus (UDC) and an E-commerce data (AliMe).
<R> <C> Data Methods <C> E-commerce data (AliMeData) MAP <C> E-commerce data (AliMeData) R@5 <C> E-commerce data (AliMeData) R@2 <C> E-commerce data (AliMeData) R@1 <R> <C> Src-only <C> 0.7012 <C> 0.7123 <C> 0.4343 <C> 0.2846 <R> <C> Tgt-only <C> 0.8418 <C> 0.7810 <C> 0.4796 <C> 0.3241 <R> <C> TL-S <C> 0.8521 <C> 0.8022 <C> 0.4812 <C> 0.3255 <R> <C> Ours <C> [BOLD] 0.8523 <C> [BOLD] 0.8125 <C> [BOLD] 0.4881 <C> [BOLD] 0.3291 <CAP> Table 2: Transferablity of our model.
<R> <C> method mapping <C> src (en) tr <C> tgt tr <C> en→fr 0.670 <C> en→fr 0.612 <C> fr→en 0.650 <C> fr→en 0.614 <C> en→de 0.579 <C> en→de 0.484 <C> de→en 0.587 <C> de→en 0.488 <C> en→ja 0.471 <C> en→ja 0.378 <C> ja→en 0.364 <C> ja→en 0.242 <R> <C> BLI from phrase table <C> tr <C> ps <C> - <C> 0.673 <C> - <C> 0.524 <C> - <C> 0.551 <C> - <C> 0.486 <C> - <C> 0.311 <C> - <C> 0.226 <R> <C> BLI from phrase table <C> ps <C> tr <C> - <C> 0.509 <C> - <C> 0.697 <C> - <C> 0.302 <C> - <C> 0.542 <C> - <C> 0.198 <C> - <C> 0.259 <R> <C> [EMPTY] <C> tr + ps <C> tr + ps <C> - <C> 0.673 <C> - <C> 0.522 <C> - <C> 0.551 <C> - <C> 0.486 <C> - <C> 0.311 <C> - <C> 0.226 <R> <C> joint learning <C> tr <C> ps <C> 0.640 <C> 0.636 <C> 0.615 <C> 0.634 <C> 0.552 <C> 0.509 <C> 0.545 <C> 0.520 <C> 0.347 <C> 0.295 <C> 0.272 <C> 0.227 <R> <C> joint learning <C> ps <C> tr <C> 0.587 <C> 0.579 <C> 0.643 <C> 0.685 <C> 0.535 <C> 0.491 <C> 0.577 <C> 0.549 <C> 0.279 <C> 0.226 <C> 0.305 <C> 0.249 <R> <C> joint learning <C> tr + ps <C> tr + ps <C> 0.654 <C> 0.642 <C> 0.642 <C> 0.650 <C> 0.585 <C> 0.532 <C> 0.520 <C> 0.518 <C> 0.325 <C> 0.267 <C> 0.295 <C> 0.234 <R> <C> mapping (ours) <C> tr <C> tr + ps <C> 0.709 <C> 0.666 <C> 0.687 <C> 0.688 <C> [BOLD] 0.656 <C> [BOLD] 0.582 <C> 0.635 <C> [BOLD] 0.563 <C> [BOLD] 0.514 <C> [BOLD] 0.405 <C> [BOLD] 0.436 <C> [BOLD] 0.304 <R> <C> mapping (ours) <C> tr + ps <C> tr <C> [BOLD] 0.728 <C> [BOLD] 0.684 <C> [BOLD] 0.703 <C> [BOLD] 0.700 <C> 0.647 <C> 0.566 <C> 0.636 <C> 0.562 <C> 0.486 <C> 0.392 <C> 0.407 <C> 0.297 <R> <C> mapping (ours) <C> tr + ps <C> tr + ps <C> 0.721 <C> 0.677 <C> 0.696 <C> 0.700 <C> 0.652 <C> 0.574 <C> [BOLD] 0.637 <C> 0.563 <C> 0.497 <C> 0.387 <C> 0.426 <C> 0.300 <CAP> Table 2: Comparison with previous approaches in BLI. “tr” and “ps” indicat training data and pseudo data. In each cell, the left cell shows the result of MRR, and the right cell shows the result of p@1.
<R> <C> corpus <C> simverb-3500 <C> men <R> <C> en <C> 0.259±0.006 <C> 0.763±0.001 <R> <C> en + pseudo(fr) <C> 0.260±0.004 <C> [BOLD] 0.767±0.002 <R> <C> en + pseudo(de) <C> 0.253±0.003 <C> [BOLD] 0.768±0.002 <R> <C> en + pseudo(ja) <C> 0.220±0.001 <C> 0.760±0.002 <CAP> Table 9: Results of Word Similarity. The scores indicate averages and standard deviations of 3 experiments with different seeds.
<R> <C> Context type <C> M <C> WER, (%) <R> <C> CI phones <C> 1 <C> 4.5 <R> <C> CI phones <C> 5 <C> 1.5 <R> <C> + word boundary <C> 1 <C> 1.8 <R> <C> + word boundary <C> 5 <C> 0.6 <CAP> TABLE VII: Word-Error-Rates in Validation Setup, Using Various Context Types as Well as Model Orders M
<R> <C> text-only <C> AIF-conv4 <C> AIF-emb <R> <C> 0.75 <C> 0.73 <C> 0.81 <CAP> Table 3: Human ranking results for ACT: micro-averaged rank over four annotators.
<R> <C> Flickr30k <C> SPICE <C> CIDEr <C> METEOR <C> ROUGE-L <C> BLEU-4 <R> <C> HardAtt Xu et al. ( 2015 ) <C> - <C> - <C> 0.185 <C> - <C> 0.199 <R> <C> SCA-CNN Chen et al. ( 2017 ) <C> - <C> - <C> 0.195 <C> - <C> 0.223 <R> <C> ATT-FCN You et al. ( 2016 ) <C> - <C> - <C> 0.189 <C> - <C> 0.230 <R> <C> SCN-LSTM Gan et al. ( 2017 ) <C> - <C> - <C> 0.210 <C> - <C> 0.257 <R> <C> AdaAtt Lu et al. ( 2017 ) <C> 0.145 <C> 0.531 <C> 0.204 <C> 0.467 <C> 0.251 <R> <C> NBT Lu et al. ( 2018 ) <C> 0.156 <C> 0.575 <C> 0.217 <C> - <C> 0.271 <R> <C> SR-PL Liu et al. ( 2018 )∗† <C> 0.158 <C> [BOLD] 0.650 <C> 0.218 <C> [BOLD] 0.499 <C> [BOLD] 0.293 <R> <C> simNet <C> [BOLD] 0.160 <C> 0.585 <C> [BOLD] 0.221 <C> 0.489 <C> 0.251 <CAP> Table 1: Performance on the Flickr30k Karpathy test split. The symbol ∗ denotes directly optimizing CIDEr. The symbol † denotes using extra data for training, thus not directly comparable. Nonetheless, our model supersedes all existing models in SPICE, which correlates the best with human judgments.
<R> <C> COCO <C> BLEU-1 c5 <C> BLEU-1 c40 <C> BLEU-2 c5 <C> BLEU-2 c40 <C> BLEU-3 c5 <C> BLEU-3 c40 <C> BLEU-4 c5 <C> BLEU-4 c40 <C> METEOR c5 <C> METEOR c40 <C> ROUGE-L c5 <C> ROUGE-L c40 <C> CIDEr c5 <C> CIDEr c40 <R> <C> HardAtt Xu et al. ( 2015 ) <C> 0.705 <C> 0.881 <C> 0.528 <C> 0.779 <C> 0.383 <C> 0.658 <C> 0.277 <C> 0.537 <C> 0.241 <C> 0.322 <C> 0.516 <C> 0.654 <C> 0.865 <C> 0.893 <R> <C> ATT-FCN You et al. ( 2016 ) <C> 0.731 <C> 0.900 <C> 0.565 <C> 0.815 <C> 0.424 <C> 0.709 <C> 0.316 <C> 0.599 <C> 0.250 <C> 0.335 <C> 0.535 <C> 0.682 <C> 0.943 <C> 0.958 <R> <C> SCA-CNN Chen et al. ( 2017 ) <C> 0.712 <C> 0.894 <C> 0.542 <C> 0.802 <C> 0.404 <C> 0.691 <C> 0.302 <C> 0.579 <C> 0.244 <C> 0.331 <C> 0.524 <C> 0.674 <C> 0.912 <C> 0.921 <R> <C> LSTM-A Yao et al. ( 2017 ) <C> 0.739 <C> 0.919 <C> 0.575 <C> 0.842 <C> 0.436 <C> 0.740 <C> 0.330 <C> 0.632 <C> 0.256 <C> 0.350 <C> 0.542 <C> 0.700 <C> 0.984 <C> 1.003 <R> <C> SCN-LSTM Gan et al. ( 2017 ) <C> 0.740 <C> 0.917 <C> 0.575 <C> 0.839 <C> 0.436 <C> 0.739 <C> 0.331 <C> 0.631 <C> 0.257 <C> 0.348 <C> 0.543 <C> 0.696 <C> 1.003 <C> 1.013 <R> <C> AdaAtt Lu et al. ( 2017 )† <C> 0.748 <C> 0.920 <C> 0.584 <C> 0.845 <C> 0.444 <C> 0.744 <C> 0.336 <C> 0.637 <C> 0.264 <C> 0.359 <C> 0.550 <C> 0.705 <C> 1.042 <C> 1.059 <R> <C> TD-M-ATT Chen et al. ( 2018 )∗† <C> 0.757 <C> 0.913 <C> 0.591 <C> 0.836 <C> 0.441 <C> 0.726 <C> 0.324 <C> 0.609 <C> 0.259 <C> 0.342 <C> 0.547 <C> 0.689 <C> 1.059 <C> 1.090 <R> <C> SCST Rennie et al. ( 2017 )∗† <C> 0.781 <C> 0.937 <C> 0.619 <C> 0.860 <C> 0.470 <C> 0.759 <C> 0.352 <C> 0.645 <C> 0.270 <C> 0.355 <C> 0.563 <C> 0.707 <C> 1.147 <C> 1.167 <R> <C> Up-Down Anderson et al. ( 2018 )∗†‡ <C> [BOLD] 0.802 <C> [BOLD] 0.952 <C> [BOLD] 0.641 <C> [BOLD] 0.888 <C> [BOLD] 0.491 <C> [BOLD] 0.794 <C> [BOLD] 0.369 <C> [BOLD] 0.685 <C> [BOLD] 0.276 <C> [BOLD] 0.367 <C> [BOLD] 0.571 <C> [BOLD] 0.724 <C> [BOLD] 1.179 <C> [BOLD] 1.205 <R> <C> simNet <C> 0.766 <C> 0.941 <C> 0.605 <C> 0.874 <C> 0.462 <C> 0.778 <C> 0.350 <C> 0.671 <C> 0.267 <C> 0.362 <C> 0.558 <C> 0.716 <C> 1.087 <C> 1.111 <CAP> Table 6: Performance on the online COCO evaluation server. The SPICE metric is unavailable for our model, thus not reported. c5 means evaluating against 5 references, and c40 means evaluating against 40 references. The symbol ∗ denotes directly optimizing CIDEr. The symbol † denotes model ensemble. The symbol ‡ denotes using extra data for training, thus not directly comparable. Our submission does not use the three aforementioned techniques. Nonetheless, our model is second only to Up-Down and surpasses almost all the other models in published work, especially when 40 references are considered.
<R> <C> Data Set L&G v. Texas <C> Tied -5507.11± <C> Tied 0.15 <C> Untied  [BOLD] -5502.87± <C> Untied 0.15 <R> <C> DC v. Heller <C> -6321.30± <C> 0.16 <C> [BOLD] -6303.55± <C> 0.15 <R> <C> Citizens United v. FEC <C> -4795.24± <C> 0.18 <C> [BOLD] -4777.96± <C> 0.17 <R> <C> “12 Angry Men” <C> -4014.56± <C> 0.24 <C> [BOLD] -3987.20± <C> 0.23 <CAP> Table 8: Log Probabilities of Held-Out Data for the Combined Model with Tied and Untied Parameters.
<R> <C> Model <C> Cornell Movies B1 <C> Cornell Movies B4 <C> Cornell Movies AG <C> OpenSubtitle B1 <C> OpenSubtitle B4 <C> OpenSubtitle AG <C> LJ users B1 <C> LJ users B4 <C> LJ users AG <C> Reddit comments B1 <C> Reddit comments B4 <C> Reddit comments AG <R> <C> Seq2Seq <C> 18.4 <C> 9.5 <C> 0.52 <C> 11.4 <C> 5.4 <C> 0.29 <C> 13.1 <C> 6.4 <C> 0.45 <C> 7.5 <C> 3.3 <C> 0.31 <R> <C> Seq2Seq-att <C> 17.7 <C> 9.2 <C> 0.54 <C> 13.2 <C> 6.5 <C> 0.42 <C> 11.4 <C> 5.6 <C> 0.49 <C> 5.5 <C> 2.4 <C> 0.25 <R> <C> DNC <C> 17.6 <C> 9.0 <C> 0.51 <C> 14.3 <C> 7.2 <C> 0.47 <C> 12.4 <C> 6.1 <C> 0.47 <C> 7.5 <C> 3.4 <C> 0.28 <R> <C> CVAE <C> 16.5 <C> 8.5 <C> 0.56 <C> 13.5 <C> 6.6 <C> 0.45 <C> 12.2 <C> 6.0 <C> 0.48 <C> 5.3 <C> 2.8 <C> 0.39 <R> <C> VLSTM <C> 18.6 <C> 9.7 <C> 0.59 <C> 16.4 <C> 8.1 <C> 0.43 <C> 11.5 <C> 5.6 <C> 0.46 <C> 6.9 <C> 3.1 <C> 0.27 <R> <C> VMED (K=1) <C> 20.7 <C> 10.8 <C> 0.57 <C> 12.9 <C> 6.2 <C> 0.44 <C> 13.7 <C> 6.9 <C> 0.47 <C> 9.1 <C> 4.3 <C> 0.39 <R> <C> VMED (K=2) <C> 22.3 <C> 11.9 <C> [BOLD] 0.64 <C> 15.3 <C> 8.8 <C> 0.49 <C> 15.4 <C> 7.9 <C> [BOLD] 0.51 <C> 9.2 <C> 4.4 <C> 0.38 <R> <C> VMED (K=3) <C> 19.4 <C> 10.4 <C> 0.63 <C> [BOLD] 24.8 <C> [BOLD] 12.9 <C> [BOLD] 0.54 <C> [BOLD] 18.1 <C> [BOLD] 9.8 <C> 0.49 <C> [BOLD] 12.3 <C> [BOLD] 6.4 <C> [BOLD] 0.46 <R> <C> VMED (K=4) <C> [BOLD] 23.1 <C> [BOLD] 12.3 <C> 0.61 <C> 17.9 <C> 9.3 <C> 0.52 <C> 14.4 <C> 7.5 <C> 0.47 <C> 8.6 <C> 4.6 <C> 0.41 <CAP> Table 1: BLEU-1, 4 and A-Glove on testing datasets. B1, B4, AG are acronyms for BLEU-1, BLEU-4, A-Glove metrics, respectively (higher is better).
<R> <C> [BOLD] Model <C> [BOLD] Num. Params  [BOLD] [billions] <C> [BOLD] Training Time  [BOLD] [hours] <C> [BOLD] Training Time  [BOLD] [CPUs] <C> [BOLD] Perplexity <R> <C> Interpolated KN 5-gram, 1.1B n-grams (KN) <C> 1.76 <C> 3 <C> 100 <C> 67.6 <R> <C> Katz 5-gram, 1.1B n-grams <C> 1.74 <C> 2 <C> 100 <C> 79.9 <R> <C> Stupid Backoff 5-gram (SBO) <C> 1.13 <C> 0.4 <C> 200 <C> 87.9 <R> <C> Interpolated KN 5-gram, 15M n-grams <C> 0.03 <C> 3 <C> 100 <C> 243.2 <R> <C> Katz 5-gram, 15M n-grams <C> 0.03 <C> 2 <C> 100 <C> 127.5 <R> <C> Binary MaxEnt 5-gram (n-gram features) <C> 1.13 <C> 1 <C> 5000 <C> 115.4 <R> <C> Binary MaxEnt 5-gram (n-gram + skip-1 features) <C> 1.8 <C> 1.25 <C> 5000 <C> 107.1 <R> <C> Hierarchical Softmax MaxEnt 4-gram (HME) <C> 6 <C> 3 <C> 1 <C> 101.3 <R> <C> Recurrent NN-256 + MaxEnt 9-gram <C> 20 <C> 60 <C> 24 <C> 58.3 <R> <C> Recurrent NN-512 + MaxEnt 9-gram <C> 20 <C> 120 <C> 24 <C> 54.5 <R> <C> Recurrent NN-1024 + MaxEnt 9-gram <C> 20 <C> 240 <C> 24 <C> 51.3 <CAP> Table 1: Results on the 1B Word Benchmark test set with various types of language models.
<R> <C> [BOLD] Model <C> [BOLD] Perplexity <R> <C> Interpolated KN 5-gram, 1.1B n-grams <C> 67.6 <R> <C> [BOLD] All models <C> [BOLD] 43.8 <CAP> Table 2: Model combination on the 1B Word Benchmark test set. The weights were tuned to minimize perplexity on held-out data. The optimal interpolation weights for the KN, rnn1024, rnn512, rnn256, SBO, HME were, respectively: 0.06, 0.61, 0.13, 0.00, 0.20, 0.00.
<R> <C> [BOLD] experiment <C> [BOLD] accuracy <R> <C> [ITALIC] baseline crf <C> 0.831 <R> <C> [ITALIC] baseline neural model <C> 0.634 <R> <C> [ITALIC] neural model <C> [EMPTY] <R> <C> [ITALIC] +features <C> 0.768 <R> <C> [ITALIC] +character embeddings <C> 0.796 <R> <C> [ITALIC] +pretrained word vectors <C> 0.845 <R> <C> [ITALIC] +l2 domain adaptation <C> 0.896 <R> <C> [ITALIC] +dropout <C> [BOLD] 0.903 <R> <C> [ITALIC] neural model joint training <C> 0.894 <R> <C> [ITALIC] final CRF of Rehbein 2013 <C> 0.888 <R> <C> [ITALIC] NCRF++ system <C> 0.887 <CAP> Table 1: Results on the test set using the time-distributed layer.
<R> <C> System B <C> CMOS (p-value) <C> Preference (%) TF <C> Preference (%) Neutral <C> Preference (%) System B <R> <C> SS <C> -0.04 (0.22) <C> 42.80 <C> 16.60 <C> 40.60 <R> <C> TF-GAN <C> 0.10 (0.02) <C> 22.73 <C> 49.21 <C> 28.06 <R> <C> SS-GAN <C> 0.01 (0.40) <C> 28.60 <C> 39.00 <C> 32.40 <CAP> Table 1: The results of the CMOS tests
<R> <C> System <C> V-Measure(%) All <C> V-Measure(%) Noun <C> V-Measure(%) Verb <C> Paired F-score(%) All <C> Paired F-score(%) Noun <C> Paired F-score(%) Verb <C> 80-20 SR(%) All <C> 80-20 SR(%) Noun <C> 80-20 SR(%) Verb <C> FS All <C> #CI <R> <C> UoY (2010) <C> 15.7 <C> 20.6 <C> 8.5 <C> 49.8 <C> 38.2 <C> 66.6 <C> 62.4 <C> 59.4 <C> 66.8 <C> - <C> 11.5 <R> <C> NMF [ITALIC] lib (2011) <C> 11.8 <C> 13.5 <C> 9.4 <C> 45.3 <C> 42.2 <C> 49.8 <C> 62.6 <C> 57.3 <C> 70.2 <C> - <C> 4.80 <R> <C> NB (2013) <C> [BOLD] 18.0 <C> [BOLD] 23.7 <C> [BOLD] 9.9 <C> 52.9 <C> 52.5 <C> 53.5 <C> 65.4 <C> 62.6 <C> 69.5 <C> - <C> 3.42 <R> <C> Spectral (2014) <C> 4.5 <C> 4.6 <C> 4.2 <C> [BOLD] 61.5 <C> [BOLD] 54.5 <C> [BOLD] 71.6 <C> - <C> - <C> - <C> 60.7 <C> 1.87 <R> <C> SE-WSI-fix-cmp <C> 16.3 <C> 20.8 <C> 9.7 <C> 54.3 <C> 54.2 <C> 54.3 <C> [BOLD] 66.3 <C> [BOLD] 63.6 <C> [BOLD] 70.2 <C> [BOLD] 66.4 <C> 2.61 <R> <C> SE-WSI-fix <C> 9.8 <C> 13.5 <C> 4.3 <C> 55.1 <C> 50.7 <C> 61.6 <C> 62.9 <C> 58.5 <C> 69.2 <C> 63.0 <C> 2.50 <R> <C> SE-WSI-CRP <C> 5.7 <C> 7.4 <C> 3.2 <C> 55.3 <C> 49.4 <C> 63.8 <C> 61.2 <C> 56.3 <C> 67.9 <C> 61.3 <C> 2.09 <R> <C> CRP-PPMI <C> 2.9 <C> 3.5 <C> 2.0 <C> 57.7 <C> 53.3 <C> 64.0 <C> 59.2 <C> 53.6 <C> 67.4 <C> 59.2 <C> 1.76 <R> <C> WE-Kmeans <C> 4.6 <C> 5.0 <C> 4.1 <C> 51.2 <C> 46.5 <C> 57.6 <C> 58.6 <C> 53.3 <C> 66.4 <C> 58.6 <C> 2.54 <CAP> Table 1: Result on SemEval-2010 WSI task. 80-20 SR is the supervised recall of 80-20 split supervised evaluation. FS is the F-Score of 80-20 split supervised evaluation. #CI is the average number of clusters (senses)
<R> <C> [EMPTY] <C> [BOLD] NUS SMS Dataset Logistic - Regression <C> [BOLD] NUS SMS Dataset Logistic - Regression <C> [BOLD] NUS SMS Dataset SGDC <C> [BOLD] NUS SMS Dataset SGDC <C> [BOLD] NUS SMS Dataset SVC <C> [BOLD] NUS SMS Dataset SVC <C> [BOLD] NUS SMS Dataset Multinomial- NB <C> [BOLD] NUS SMS Dataset Multinomial- NB <C> [BOLD] Twitter Dataset Logistic - Regression <C> [BOLD] Twitter Dataset Logistic - Regression <C> [BOLD] Twitter Dataset SGDC <C> [BOLD] Twitter Dataset SGDC <C> [BOLD] Twitter Dataset SVC <C> [BOLD] Twitter Dataset SVC <C> [BOLD] Twitter Dataset Multinomial- NB <C> [BOLD] Twitter Dataset Multinomial- NB <R> <C> [EMPTY] <C> IV <C> OOV <C> IV <C> OOV <C> IV <C> OOV <C> IV <C> OOV <C> IV <C> OOV <C> IV <C> OOV <C> IV <C> OOV <C> IV <C> OOV <R> <C> Precision <C> 0.91 <C> 0.95 <C> 0.84 <C> 0.98 <C> 0.87 <C> 0.97 <C> 0.89 <C> 0.97 <C> 0.71 <C> 0.69 <C> 0.63 <C> 0.72 <C> 0.74 <C> 0.67 <C> 0.81 <C> 0.68 <R> <C> Recall <C> 0.95 <C> 0.90 <C> 0.99 <C> 0.81 <C> 0.98 <C> 0.85 <C> 0.97 <C> 0.87 <C> 0.68 <C> 0.71 <C> 0.80 <C> 0.52 <C> 0.64 <C> 0.77 <C> 0.61 <C> 0.85 <R> <C> F-measure <C> 0.93 <C> 0.92 <C> 0.91 <C> 0.89 <C> 0.92 <C> 0.91 <C> 0.93 <C> 0.92 <C> 0.69 <C> 0.70 <C> 0.70 <C> 0.60 <C> 0.68 <C> 0.72 <C> 0.69 <C> 0.76 <R> <C> Accuracy <C> [BOLD] 0.9275 <C> [BOLD] 0.9275 <C> 0.89875 <C> 0.89875 <C> 0.915 <C> 0.915 <C> 0.9225 <C> 0.9225 <C> 0.6962 <C> 0.6962 <C> 0.6605 <C> 0.6605 <C> 0.7013 <C> 0.7013 <C> [BOLD] 0.7288 <C> [BOLD] 0.7288 <CAP> Table 2: Precision, Recall, F1 and Accuracy for each algorithm on different datasets
<R> <C> [BOLD] Database <C> [BOLD] Test set labels <C> [BOLD] WER (%) <C> [BOLD] Test sentences <C> [BOLD] Entities (Ref) <C> [BOLD] Entities (Test) <C> [BOLD] Difference (%) <R> <C> [ITALIC] WSJ <C> Eval92 <C> 5.64 <C> 333 <C> 1,802 <C> 1,836 <C> 1.02 <R> <C> [ITALIC] WSJ <C> Eval93 <C> 9.02 <C> 213 <C> 1,129 <C> 1,139 <C> 1.00 <R> <C> [ITALIC] Tedlium <C> Test set <C> 31.85 <C> 1,155 <C> 5,804 <C> 6,743 <C> 1.16 <CAP> Table 1: ASR performance and entity annotation for reference and test sentences in the WSJ and the TED-LIUM corpus
<R> <C> [EMPTY] <C> TableA original <C> TableA deduplicated <C> TableB original <C> TableB deduplicated <C> #Candidates Basic blocking <R> <C> Size <C> 789,409 <C> 788,094 <C> 412,418 <C> 62,511 <C> 10,652,249 <CAP> Table 9: Sizes of the two employer datasets to be matched.
<R> <C> [EMPTY] <C> sent2vec <C> skip-gram <C> skip-thoughts <R> <C> BLEU-1 <C> 0.479 <C> 0.514 <C> 0.520 <R> <C> BLEU-2 <C> 0.342 <C> 0.378 <C> 0.392 <R> <C> BLEU-3 <C> 0.213 <C> 0.245 <C> 0.276 <R> <C> BLEU-4 <C> 0.144 <C> 0.173 <C> 0.206 <R> <C> METEOR <C> 0.237 <C> 0.250 <C> 0.253 <R> <C> ROUGE-L <C> 0.48 <C> 0.509 <C> 0.522 <R> <C> CIDEr <C> 1.129 <C> 1.430 <C> 1.562 <CAP> Table 3: Evaluation of short to single sentence summarization on TACoS Multi-Level Corpus using vectors from sent2vec, skip-thoughts, and skip-gram respectively.
<R> <C> Model <C> Advising ? <C> Advising Q <C> ATIS ? <C> ATIS Q <C> GeoQuery <C> GeoQuery ? <C> GeoQuery Q <C> Restaurants <C> Restaurants ? <C> Restaurants Q <C> Scholar ? <C> Scholar Q <C> Academic <C> Academic ? <C> Academic Q <C> IMDB ? <C> IMDB Q <C> Yelp ? <C> Yelp Q <R> <C> No Variable Anonymization <C> No Variable Anonymization <C> No Variable Anonymization <C> No Variable Anonymization <C> No Variable Anonymization <C> No Variable Anonymization <C> No Variable Anonymization <C> No Variable Anonymization <C> No Variable Anonymization <C> No Variable Anonymization <C> No Variable Anonymization <C> No Variable Anonymization <C> No Variable Anonymization <C> No Variable Anonymization <C> No Variable Anonymization <C> No Variable Anonymization <C> No Variable Anonymization <C> No Variable Anonymization <C> No Variable Anonymization <C> No Variable Anonymization <R> <C> Baseline <C> [BOLD] 80 <C> 0 <C> 46 <C> 0 <C> [EMPTY] <C> 57 <C> 0 <C> [EMPTY] <C> 95 <C> 0 <C> 52 <C> 0 <C> [EMPTY] <C> 0 <C> 0 <C> 0 <C> 0 <C> 1 <C> 0 <R> <C> seq2seq <C> 6 <C> 0 <C> 8 <C> 0 <C> [EMPTY] <C> 27 <C> 7 <C> [EMPTY] <C> 47 <C> 0 <C> 19 <C> 0 <C> [EMPTY] <C> 6 <C> 7 <C> 1 <C> 0 <C> 0 <C> 0 <R> <C> + Attention <C> 29 <C> 0 <C> 46 <C> 18 <C> [EMPTY] <C> 63 <C> 21 <C> [EMPTY] <C> [BOLD] 100 <C> 2 <C> 33 <C> 0 <C> [EMPTY] <C> 71 <C> 64 <C> 7 <C> 3 <C> 2 <C> 2 <R> <C> + Copying <C> 70 <C> 0 <C> [BOLD] 51 <C> [BOLD] 32 <C> [EMPTY] <C> [BOLD] 71 <C> 20 <C> [EMPTY] <C> [BOLD] 100 <C> 4 <C> [BOLD] 59 <C> 5 <C> [EMPTY] <C> [BOLD] 81 <C> [BOLD] 74 <C> [BOLD] 26 <C> [BOLD] 9 <C> [BOLD] 12 <C> 4 <R> <C> D&L seq2tree <C> 46 <C> [BOLD] 2 <C> 46 <C> 23 <C> [EMPTY] <C> 62 <C> 31 <C> [EMPTY] <C> [BOLD] 100 <C> [BOLD] 11 <C> 44 <C> [BOLD] 6 <C> [EMPTY] <C> 63 <C> 54 <C> 6 <C> 2 <C> 1 <C> 2 <R> <C> Iyer et al. <C> 41 <C> 1 <C> 45 <C> 17 <C> [EMPTY] <C> 66 <C> [BOLD] 40 <C> [EMPTY] <C> [BOLD] 100 <C> 8 <C> 44 <C> 3 <C> [EMPTY] <C> 76 <C> 70 <C> 10 <C> 4 <C> 6 <C> [BOLD] 6 <R> <C> With Oracle Entities <C> With Oracle Entities <C> With Oracle Entities <C> With Oracle Entities <C> With Oracle Entities <C> With Oracle Entities <C> With Oracle Entities <C> With Oracle Entities <C> With Oracle Entities <C> With Oracle Entities <C> With Oracle Entities <C> With Oracle Entities <C> With Oracle Entities <C> With Oracle Entities <C> With Oracle Entities <C> With Oracle Entities <C> With Oracle Entities <C> With Oracle Entities <C> With Oracle Entities <C> With Oracle Entities <R> <C> Baseline <C> 89 <C> 0 <C> 56 <C> 0 <C> [EMPTY] <C> 56 <C> 0 <C> [EMPTY] <C> 95 <C> 0 <C> 66 <C> 0 <C> [EMPTY] <C> 0 <C> 0 <C> 7 <C> 0 <C> 8 <C> 0 <R> <C> seq2seq <C> 21 <C> 0 <C> 14 <C> 0 <C> [EMPTY] <C> 49 <C> 14 <C> [EMPTY] <C> 71 <C> 6 <C> 23 <C> 0 <C> [EMPTY] <C> 10 <C> 9 <C> 6 <C> 0 <C> 12 <C> 9 <R> <C> + Attention <C> 88 <C> 0 <C> 57 <C> 23 <C> [EMPTY] <C> 73 <C> 31 <C> [EMPTY] <C> 100 <C> 32 <C> 71 <C> 4 <C> [EMPTY] <C> 77 <C> 74 <C> 44 <C> 17 <C> 33 <C> 28 <R> <C> D&L seq2tree <C> 88 <C> 8 <C> 56 <C> 34 <C> [EMPTY] <C> 68 <C> 23 <C> [EMPTY] <C> 100 <C> 21 <C> 68 <C> 6 <C> [EMPTY] <C> 65 <C> 61 <C> 36 <C> 10 <C> 26 <C> 23 <R> <C> Iyer et al. <C> 88 <C> 6 <C> 58 <C> 32 <C> [EMPTY] <C> 71 <C> 49 <C> [EMPTY] <C> 100 <C> 33 <C> 71 <C> 1 <C> [EMPTY] <C> 77 <C> 75 <C> 52 <C> 24 <C> 44 <C> 32 <R> <C> Baseline-Oracle <C> 100 <C> 0 <C> 69 <C> 0 <C> [EMPTY] <C> 78 <C> 0 <C> [EMPTY] <C> 100 <C> 0 <C> 84 <C> 0 <C> [EMPTY] <C> 11 <C> 0 <C> 47 <C> 0 <C> 25 <C> 0 <CAP> Table 3: Accuracy of neural text-to-SQL systems on English question splits (‘?’ columns) and SQL query splits (‘Q’ columns). The vertical line separates datasets from the NLP (left) and DB (right) communities. Results for Iyer2017 are slightly lower here than in the original paper because we evaluate on SQL output, not the database response.
<R> <C> Configuration <C> WER (%) ReLU-DNN <C> WER (%) ReLU-DNN <C> WER (%) BLSTM <C> WER (%) BLSTM <C> WER (%) LACE <C> WER (%) LACE <R> <C> Configuration <C> CH <C> SWB <C> CH <C> SWB <C> CH <C> SWB <R> <C> Baseline <C> 21.9 <C> 13.4 <C> 17.3 <C> 10.3 <C> 16.9 <C> 10.4 <R> <C> i-vector <C> 20.1 <C> 11.5 <C> 17.6 <C> 9.9 <C> 16.4 <C> 9.3 <R> <C> i-vector+LFMMI <C> 17.9 <C> 10.2 <C> 16.3 <C> 8.9 <C> 15.2 <C> 8.5 <CAP> Table 4: Performance improvements from i-vector and LFMMI training on the NIST 2000 CTS set
<R> <C> [BOLD] Model <C> |{\bm{\theta}}| <C> v_{\rm{train}} <C> v_{\rm{test}} <C> Dev <C> MT04 <C> MT05 <C> MT06 <C> Tests Avg. <R> <C> Transformer <C> 66.1m <C> 1.00\times <C> 1.00\times <C> 45.83 <C> 46.66 <C> 43.36 <C> 42.17 <C> 44.06 <R> <C> Gdr <C> 68.9m <C> 0.77\times <C> 0.94\times <C> 46.50 <C> 47.03 <C> 45.50 <C> 42.21 <C> 44.91 (+0.75) <R> <C> + \mathcal{L}_{\textsc{BoW}} <C> 69.2m <C> 0.70\times <C> 0.94\times <C> 47.12 <C> 48.09 <C> 45.98 <C> 42.68 <C> 45.58 (+1.42) <R> <C> + \mathcal{L}_{\textsc{Bca}} <C> 69.4m <C> 0.75\times <C> 0.94\times <C> 46.86 <C> 48.00 <C> 45.67 <C> 42.62 <C> 45.43 (+1.37) <R> <C> + \mathcal{L}_{\textsc{BoW}} + \mathcal{L}_{\textsc{Bca}} [Ours] <C> 69.7m <C> 0.67\times <C> 0.94\times <C> [BOLD] 47.52 <C> [BOLD] 48.13 <C> [BOLD] 45.98 <C> [BOLD] 42.85 <C> [BOLD] 45.65 (+1.59) <R> <C> Ours -  [ITALIC] redundant capsules <C> 68.7m <C> 0.69\times <C> 0.94\times <C> 47.20 <C> 47.82 <C> 45.59 <C> 42.51 <C> 45.30 (+1.24) <R> <C> Rnmt <C> 50.2m <C> 1.00\times <C> 1.00\times <C> 35.98 <C> 37.85 <C> 36.12 <C> 35.86 <C> 36.61 <R> <C> +PFRnn zheng2018modeling <C> [EMPTY] <C> 0.54\times <C> 0.74\times <C> 37.90 <C> 40.37 <C> 36.75 <C> 36.44 <C> 37.85 (+1.24) <R> <C> +Aol kong2018neural <C> [EMPTY] <C> 0.57\times <C> 1.00\times <C> 37.61 <C> 40.05 <C> 37.58 <C> 36.87 <C> 38.16 (+1.55) <R> <C> Ours <C> 53.9m <C> 0.62\times <C> 0.90\times <C> [BOLD] 38.10 <C> [BOLD] 40.87 <C> [BOLD] 37.50 <C> [BOLD] 37.00 <C> [BOLD] 38.45 (+1.84) <CAP> Table 1: Experiment ressuts on NIST Zh-En task, including number of parameters (|{\bm{\theta}}|, excluding word embeddings), training/testing speeds (v_{\rm{train}}/v_{\rm{test}}), and translation results in case-insensitive BLEU.
<R> <C> [BOLD] Model <C> En-De <C> En-Ro <R> <C> GNMT+RL wu2016google <C> 24.6 <C> [EMPTY] <R> <C> ConvS2S Gehring2017ConSeq <C> 25.2 <C> 29.88 <R> <C> Transformer Vaswani2017Attention <C> 27.3 <C> [EMPTY] <R> <C> +Aol kong2018neural <C> 28.01 <C> [EMPTY] <R> <C> Transformer Gu2017NAT <C> [EMPTY] <C> 31.91 <R> <C> Transformer <C> 27.14 <C> 32.10 <R> <C> Ours <C> [BOLD] 28.10 <C> [BOLD] 32.96 <CAP> Table 2: Case-sensitive BLEU on WMT14 En-De and WMT16 En-Ro tasks.
<R> <C> [BOLD] Model <C> R-1 <C> R-2 <C> R-L <R> <C> RNN+context Hu2015LCSTSAL <C> 29.9 <C> 17.4 <C> 27.2 <R> <C> CopyNet P16-1154 <C> 34.4 <C> 21.6 <C> 31.3 <R> <C> Distraction chen2016distraction <C> 35.2 <C> 22.6 <C> 32.5 <R> <C> DGRD li2017deep <C> 36.99 <C> 24.15 <C> 34.21 <R> <C> MRT ayana2016mrt <C> 37.87 <C> 25.43 <C> 35.33 <R> <C> WEAN ma2018query <C> 37.80 <C> 25.60 <C> 35.20 <R> <C> AC-ABS li2018actor <C> 37.51 <C> 24.68 <C> 35.02 <R> <C> Transformer chang2018ahw <C> 40.49 <C> 26.83 <C> 37.32 <R> <C> +HWC chang2018ahw <C> 44.38 <C> 32.26 <C> 41.35 <R> <C> Transformer <C> 40.18 <C> 25.76 <C> 35.69 <R> <C> Ours <C> [BOLD] 43.85 <C> [BOLD] 29.57 <C> [BOLD] 39.10 <R> <C> -  [ITALIC] redundant capsules <C> 42.43 <C> 28.17 <C> 37.66 <CAP> Table 5: ROUGE scores on LCSTS abstract summarization task.
<R> <C> Method <C> Greedy <C> DP <R> <C> BOW (Boolean) <C> 0.213 <C> 0.217 <R> <C> BOW (Frequency) <C> 0.194 <C> 0.209 <R> <C> Embedding (Avg) <C> 0.233 <C> 0.238 <R> <C> Recursive NN <C> 0.241 <C> 0.257 <CAP> Table 5: Results of different algorithms for sentence organizing on youth (青春 in Chinese). DP means dynamic programming.
<R> <C> [BOLD] % of training set <C> [BOLD] 1% <C> [BOLD] 10% <C> [BOLD] 20% <C> [BOLD] 50% <C> [BOLD] 100% <R> <C> [BOLD] SemEval 2010 Task 8 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> bert \textsc  [ITALIC] EM <C> 28.6 <C> 66.9 <C> 75.5 <C> 80.3 <C> 82.1 <R> <C> bert \textsc  [ITALIC] EM+mtb <C> 31.2 <C> 70.8 <C> 76.2 <C> 80.4 <C> 82.7 <R> <C> [BOLD] KBP-37 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> bert \textsc  [ITALIC] EM <C> 40.1 <C> 63.6 <C> 65.4 <C> 67.8 <C> 69.5 <R> <C> bert \textsc  [ITALIC] EM+mtb <C> 44.2 <C> 66.3 <C> 67.2 <C> 68.8 <C> 70.3 <R> <C> [BOLD] TACRED <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> bert \textsc  [ITALIC] EM <C> 32.8 <C> 59.6 <C> 65.6 <C> 69.0 <C> 70.1 <R> <C> bert \textsc  [ITALIC] EM+mtb <C> 43.4 <C> 64.8 <C> 67.2 <C> 69.9 <C> 70.6 <CAP> Table 5: F1 scores on development sets for supervised relation extraction tasks while varying the amount of tuning data available to our bert\textscEM and bert\textscEM+mtb models.
<R> <C> [BOLD] Emoji <C> 1F609 <C> 1F606 <C> 1F605 <C> 1F64F <C> 1F604 <C> 1F634 <C> 1F602 <C> 1F633 <C> 1F64C <C> 1F601 <C> [BOLD] Avg. Accuracy <R> <C> [BOLD] BabelNet-based <C> 24.48 <C> 20.93 <C> 16.27 <C> 12.00 <C> 16.66 <C> 18.75 <C> 15.21 <C> 20.45 <C> 12.00 <C> 27.08 <C> 18.38 <R> <C> [BOLD] Twitter-based <C> [BOLD] 61.22 <C> [BOLD] 60.00 <C> [BOLD] 56.41 <C> [BOLD] 56.00 <C> 43.58 <C> [BOLD] 51.21 <C> [BOLD] 48.57 <C> [BOLD] 47.72 <C> [BOLD] 46.51 <C> [BOLD] 44.18 <C> [BOLD] 51.54 <R> <C> [BOLD] News-based <C> 32.65 <C> 59.45 <C> 41.46 <C> 29.16 <C> [BOLD] 52.17 <C> 41.17 <C> 43.24 <C> 13.63 <C> 37.50 <C> 38.63 <C> 38.91 <CAP> Table 3: Top 10 Emoji based on the Emoji Sense Disambiguation Accuracy (in % values)
<R> <C> [BOLD] Emoji Pair <C> 1F603 1F600 <C> 1F49E 1F499 <C> 1F49E 1F495 <C> 1F49A 1F495 <C> 1F49E 1F49A <C> 1F62D 1F622 <C> 1F389 1F388 <C> 1F604 1F603 <C> 1F61D 1F61C <C> 1F606 1F603 <R> <C> [BOLD] Similarity <C> 0.60 <C> 0.57 <C> 0.56 <C> 0.52 <C> 0.52 <C> 0.50 <C> 0.50 <C> 0.50 <C> 0.48 <C> 0.47 <CAP> Table 5: Ten Most Similar Emoji Pairs Based on Jaccard Similarity
<R> <C> Model <C> P <C> R <C> F <R> <C> CausalNet <C> 0.6211 <C> 0.5372 <C> 0.5761 <R> <C> Rules-Bayesian <C> 0.6042 <C> 0.5878 <C> 0.5959 <R> <C> IDCNN-softmax <C> 0.7455±0.0142 <C> 0.7074±0.0168 <C> 0.7258±0.0105 <R> <C> IDCNN-CRF <C> 0.7442±0.0225 <C> 0.7142±0.0122 <C> 0.7288±0.0160 <R> <C> BiLSTM-softmax <C> 0.7744±0.0183 <C> 0.7622±0.0114 <C> 0.7682±0.0138 <R> <C> CLSTM-BiLSTM-CRF <C> 0.8144±0.0284 <C> 0.7412±0.0073 <C> 0.7757±0.0107 <R> <C> CCNN-BiLSTM-CRF <C> 0.8069±0.0199 <C> 0.7520±0.0227 <C> 0.7780±0.0075 <R> <C> BiLSTM-CRF <C> 0.7837±0.0061 <C> 0.7932±0.0087 <C> 0.7884±0.0072 <R> <C> BERT-BiLSTM-CRF <C> 0.8277±0.0058 <C> 0.8209±0.0093 <C> 0.8243±0.0049 <R> <C> Flair+CLSTM-BiLSTM-CRF <C> 0.8403±0.0090 <C> 0.8284±0.0125 <C> 0.8343±0.0106 <R> <C> ELMo-BiLSTM-CRF <C> 0.8361±0.0135 <C> 0.8399±0.0063 <C> 0.8379±0.0092 <R> <C> Flair-BiLSTM-CRF <C> [BOLD] 0.8414±0.0079 <C> 0.8351±0.0141 <C> 0.8382±0.0092 <R> <C> [BOLD] SCITE (Flair+CCNN-BiLSTM-MHSA-CRF) <C> 0.8333±0.0042 <C> [BOLD] 0.8581±0.0021 <C> [BOLD] 0.8455±0.0028 <R> <C> SCITE (based on general tagging scheme) <C> 0.7609±0.0170 <C> 0.7757±0.0136 <C> 0.7682±0.0145 <CAP> Table 3: Comparison in precision (P), recall (R), and F1-score (F) on the test set with baselines.
<R> <C> Model <C> C-P <C> C-R <C> C-F <C> E-P <C> E-R <C> E-F <C> Emb-P <C> Emb-R <C> Emb-F <R> <C> BiLSTM-CRF <C> 0.8810 <C> 0.8628 <C> 0.8718 <C> 0.8928 <C> 0.8897 <C> 0.8913 <C> 0.4343 <C> 0.0960 <C> 0.1567 <R> <C> Flair-BiLSTM-CRF <C> 0.8995 <C> 0.8843 <C> 0.8917 <C> [BOLD] 0.9294 <C> 0.8885 <C> 0.9084 <C> [BOLD] 0.8556 <C> 0.1360 <C> 0.2197 <R> <C> [BOLD] SCITE <C> [BOLD] 0.8999 <C> [BOLD] 0.8998 <C> [BOLD] 0.8998 <C> 0.9272 <C> [BOLD] 0.9021 <C> [BOLD] 0.9144 <C> 0.8489 <C> [BOLD] 0.1920 <C> [BOLD] 0.2947 <CAP> Table 4: Comparison of predicted tags concerning “C” (cause), “E” (effect) and “Emb” (embedded causality) in precision (P), recall (R), and F1-score (F) on the test set.
<R> <C> [EMPTY] <C> [BOLD] WORD2VEC  [BOLD] Orig. <C> [BOLD] WORD2VEC  [BOLD] ABTT <C> [BOLD] WORD2VEC  [BOLD] CN <C> [BOLD] WORD2VEC  [BOLD] SB <C> [BOLD] WORD2VEC  [BOLD] HSR-RR <C> [BOLD] GLOVE  [BOLD] Orig. <C> [BOLD] GLOVE  [BOLD] ABTT <C> [BOLD] GLOVE  [BOLD] CN <C> [BOLD] GLOVE  [BOLD] SB <C> [BOLD] GLOVE  [BOLD] HSR-RR <C> [BOLD] PARAGRAM  [BOLD] Orig. <C> [BOLD] PARAGRAM  [BOLD] ABTT <C> [BOLD] PARAGRAM  [BOLD] CN <C> [BOLD] PARAGRAM  [BOLD] SB <C> [BOLD] PARAGRAM  [BOLD] HSR-RR <R> <C> [BOLD] STS-2012-MSRpar <C> [BOLD] 41.78 <C> 38.70 <C> 39.42 <C> 40.77 <C> 34.42 <C> [BOLD] 42.06 <C> 41.41 <C> 41.27 <C> 41.15 <C> 32.49 <C> 39.32 <C> 38.84 <C> 39.84 <C> 37.72 <C> [BOLD] 41.44 <R> <C> [BOLD] STS-2012-MSRvid <C> 76.27 <C> 75.60 <C> 75.32 <C> 74.98 <C> [BOLD] 79.63 <C> 65.85 <C> 67.84 <C> 62.50 <C> 64.71 <C> [BOLD] 80.03 <C> 56.34 <C> 57.65 <C> 56.78 <C> 55.55 <C> [BOLD] 62.31 <R> <C> [BOLD] STS-2012-surprise.OnWN <C> 70.62 <C> 70.89 <C> 70.73 <C> 69.99 <C> [BOLD] 71.27 <C> 60.74 <C> 69.48 <C> 67.87 <C> 57.02 <C> [BOLD] 72.24 <C> 62.60 <C> 64.61 <C> 63.21 <C> 60.68 <C> [BOLD] 67.91 <R> <C> [BOLD] STS-2012-SMTeuroparl <C> 31.20 <C> 35.71 <C> 35.29 <C> 33.88 <C> [BOLD] 40.32 <C> 51.97 <C> [BOLD] 54.36 <C> 52.58 <C> 50.06 <C> 51.60 <C> 50.64 <C> 51.64 <C> 50.63 <C> 51.34 <C> [BOLD] 51.92 <R> <C> [BOLD] STS-2012-surprise.SMTnews <C> [BOLD] 51.07 <C> 46.24 <C> 47.34 <C> 47.10 <C> 50.09 <C> 46.35 <C> 48.19 <C> 47.69 <C> 45.18 <C> [BOLD] 54.41 <C> 52.94 <C> 50.18 <C> 52.66 <C> [BOLD] 54.16 <C> 53.87 <R> <C> [BOLD] STS-2012 <C> 54.19 <C> 53.43 <C> 53.62 <C> 53.34 <C> [BOLD] 55.15 <C> 53.39 <C> 56.26 <C> 54.38 <C> 51.62 <C> [BOLD] 58.15 <C> 52.37 <C> 52.58 <C> 52.62 <C> 51.89 <C> [BOLD] 55.49 <R> <C> [BOLD] STS-2013-FNWN <C> 39.68 <C> 43.51 <C> 43.40 <C> 42.95 <C> [BOLD] 49.09 <C> 39.48 <C> 45.81 <C> 42.03 <C> 39.15 <C> [BOLD] 46.47 <C> 35.79 <C> 36.05 <C> 35.93 <C> 34.35 <C> [BOLD] 38.00 <R> <C> [BOLD] STS-2013-OnWN <C> 67.98 <C> 70.56 <C> 69.29 <C> 69.12 <C> [BOLD] 75.57 <C> 53.75 <C> 63.86 <C> 57.45 <C> 52.36 <C> [BOLD] 74.91 <C> 48.07 <C> 48.18 <C> 48.23 <C> 48.28 <C> [BOLD] 56.57 <R> <C> [BOLD] STS-2013-headlines <C> 63.29 <C> 63.24 <C> 63.62 <C> 63.22 <C> [BOLD] 63.65 <C> 63.54 <C> 66.70 <C> 67.00 <C> 60.65 <C> [BOLD] 68.56 <C> 64.43 <C> 65.13 <C> 64.69 <C> 62.99 <C> [BOLD] 66.90 <R> <C> [BOLD] STS-2013 <C> 56.98 <C> 59.10 <C> 58.77 <C> 58.43 <C> [BOLD] 62.77 <C> 52.26 <C> 58.79 <C> 55.49 <C> 50.72 <C> [BOLD] 63.31 <C> 49.43 <C> 49.79 <C> 49.62 <C> 48.54 <C> [BOLD] 53.82 <R> <C> [BOLD] STS-2014-OnWN <C> 74.85 <C> 75.92 <C> 75.27 <C> 74.43 <C> [BOLD] 81.40 <C> 61.91 <C> 70.93 <C> 66.43 <C> 60.36 <C> [BOLD] 81.39 <C> 60.29 <C> 61.95 <C> 60.75 <C> 59.45 <C> [BOLD] 68.30 <R> <C> [BOLD] STS-2014-deft-forum <C> 41.30 <C> 42.25 <C> 42.74 <C> 42.03 <C> [BOLD] 46.73 <C> 28.82 <C> 38.90 <C> 37.57 <C> 25.91 <C> [BOLD] 45.85 <C> 35.17 <C> 37.60 <C> 35.75 <C> 33.59 <C> [BOLD] 40.84 <R> <C> [BOLD] STS-2014-deft-news <C> 66.76 <C> 64.87 <C> 65.45 <C> 64.97 <C> [BOLD] 67.88 <C> 63.41 <C> 68.72 <C> 69.08 <C> 61.27 <C> [BOLD] 70.60 <C> 62.19 <C> 63.73 <C> 62.75 <C> 61.09 <C> [BOLD] 66.66 <R> <C> [BOLD] STS-2014-headlines <C> 60.87 <C> 60.61 <C> [BOLD] 61.09 <C> 60.66 <C> 60.93 <C> 59.28 <C> 61.34 <C> 61.71 <C> 56.25 <C> [BOLD] 64.01 <C> 60.84 <C> 60.72 <C> 60.97 <C> 60.21 <C> [BOLD] 62.83 <R> <C> [BOLD] STS-2014-tweet-news <C> 73.33 <C> 75.13 <C> 74.87 <C> 73.66 <C> [BOLD] 76.00 <C> 62.43 <C> 74.62 <C> [BOLD] 75.38 <C> 58.70 <C> 75.09 <C> 69.29 <C> 72.43 <C> 70.14 <C> 66.75 <C> [BOLD] 75.16 <R> <C> [BOLD] STS-2014-images <C> 77.44 <C> 77.81 <C> 78.42 <C> 77.11 <C> [BOLD] 80.55 <C> 61.89 <C> 69.40 <C> 65.81 <C> 59.03 <C> [BOLD] 78.45 <C> 53.67 <C> 58.29 <C> 54.86 <C> 51.58 <C> [BOLD] 65.10 <R> <C> [BOLD] STS-2014 <C> 65.76 <C> 66.10 <C> 66.31 <C> 65.48 <C> [BOLD] 68.92 <C> 56.29 <C> 63.99 <C> 62.66 <C> 53.59 <C> [BOLD] 69.23 <C> 56.91 <C> 59.12 <C> 57.54 <C> 55.45 <C> [BOLD] 63.15 <R> <C> [BOLD] STS-2015-answers-forums <C> 52.65 <C> 54.01 <C> 53.99 <C> 50.51 <C> [BOLD] 66.77 <C> 36.86 <C> 49.58 <C> 48.62 <C> 36.76 <C> [BOLD] 65.46 <C> 38.79 <C> 41.19 <C> 39.25 <C> 38.35 <C> [BOLD] 48.37 <R> <C> [BOLD] STS-2015-answers-students <C> 70.82 <C> 70.92 <C> 71.65 <C> 69.74 <C> [BOLD] 72.16 <C> 62.77 <C> 69.46 <C> [BOLD] 69.68 <C> 61.84 <C> 67.38 <C> 67.52 <C> 69.46 <C> 67.96 <C> 66.80 <C> [BOLD] 71.98 <R> <C> [BOLD] STS-2015-belief <C> 60.11 <C> 61.91 <C> 61.62 <C> 58.10 <C> [BOLD] 77.08 <C> 44.20 <C> 61.43 <C> 59.77 <C> 41.19 <C> [BOLD] 76.12 <C> 49.77 <C> 55.57 <C> 50.79 <C> 46.98 <C> [BOLD] 61.32 <R> <C> [BOLD] STS-2015-headlines <C> 68.11 <C> 68.28 <C> 68.65 <C> 68.19 <C> [BOLD] 69.02 <C> 65.42 <C> 68.90 <C> 69.20 <C> 63.25 <C> [BOLD] 71.41 <C> 67.85 <C> 68.40 <C> 68.09 <C> 66.92 <C> [BOLD] 70.38 <R> <C> [BOLD] STS-2015-images <C> 80.07 <C> 80.18 <C> 80.74 <C> 79.48 <C> [BOLD] 83.08 <C> 69.14 <C> 73.53 <C> 71.43 <C> 67.81 <C> [BOLD] 80.58 <C> 66.55 <C> 68.29 <C> 67.08 <C> 65.55 <C> [BOLD] 73.17 <R> <C> [BOLD] STS-2015 <C> 66.35 <C> 67.06 <C> 67.33 <C> 65.20 <C> [BOLD] 73.62 <C> 55.68 <C> 64.58 <C> 63.74 <C> 54.17 <C> [BOLD] 72.19 <C> 58.10 <C> 60.58 <C> 58.63 <C> 56.92 <C> [BOLD] 65.04 <R> <C> [BOLD] SICK <C> 72.25 <C> [BOLD] 72.49 <C> 72.40 <C> 72.32 <C> 72.02 <C> 66.64 <C> 68.12 <C> 66.42 <C> 66.03 <C> [BOLD] 71.62 <C> 64.55 <C> 64.89 <C> 64.78 <C> 64.05 <C> [BOLD] 67.07 <CAP> Table 2: Pearson correlation coefficient of 20 semantic textual similarity tasks
<R> <C> [EMPTY] <C> [BOLD] WORD2VEC  [BOLD] Orig. <C> [BOLD] WORD2VEC  [BOLD] CN <C> [BOLD] WORD2VEC  [BOLD] ABTT <C> [BOLD] WORD2VEC  [BOLD] SB <C> [BOLD] WORD2VEC  [BOLD] HSR-RR <C> [BOLD] GLOVE  [BOLD] Orig. <C> [BOLD] GLOVE  [BOLD] CN <C> [BOLD] GLOVE  [BOLD] ABTT <C> [BOLD] GLOVE  [BOLD] SB <C> [BOLD] GLOVE  [BOLD] HSR-RR <C> [BOLD] PARAGRAM  [BOLD] Orig. <C> [BOLD] PARAGRAM  [BOLD] CN <C> [BOLD] PARAGRAM  [BOLD] ABTT <C> [BOLD] PARAGRAM  [BOLD] SB <C> [BOLD] PARAGRAM  [BOLD] HSR-RR <R> <C> [BOLD] AR <C> 0.8375 <C> 0.8338 <C> 0.8329 <C> 0.8302 <C> [BOLD] 0.8377 <C> 0.8441 <C> 0.8431 <C> 0.8444 <C> 0.8426 <C> [BOLD] 0.8454 <C> 0.8124 <C> 0.8129 <C> 0.8113 <C> 0.8124 <C> [BOLD] 0.8152 <R> <C> [BOLD] CR <C> 0.7800 <C> 0.7792 <C> 0.7718 <C> 0.7726 <C> [BOLD] 0.7824 <C> [BOLD] 0.7829 <C> 0.7800 <C> 0.7808 <C> 0.7819 <C> 0.7792 <C> 0.7657 <C> 0.7649 <C> 0.7628 <C> 0.7644 <C> [BOLD] 0.7673 <R> <C> [BOLD] IMDB <C> 0.8392 <C> 0.8369 <C> 0.8370 <C> 0.8281 <C> [BOLD] 0.8434 <C> 0.8491 <C> 0.8453 <C> [BOLD] 0.8493 <C> 0.8459 <C> [BOLD] 0.8493 <C> 0.7957 <C> 0.7960 <C> 0.7953 <C> 0.7938 <C> [BOLD] 0.7999 <R> <C> [BOLD] STS-B <C> [BOLD] 0.8071 <C> 0.8062 <C> 0.8048 <C> 0.8052 <C> 0.8056 <C> 0.8044 <C> 0.8045 <C> 0.8049 <C> 0.8031 <C> [BOLD] 0.8053 <C> 0.7818 <C> 0.7819 <C> 0.7778 <C> 0.7813 <C> [BOLD] 0.7846 <CAP> Table 3: Five-fold cross-validation accuracy of four sentiment analysis tasks
<R> <C> [ITALIC] Method <C> [ITALIC] Plain <C> [ITALIC] Sim <C> [ITALIC] SimYu <R> <C> Tf.idf <C> 0.00354 <C> [EMPTY] <C> [EMPTY] <R> <C> SVD <C> 0.00345 <C> 0.00334 <C> 0.00342 <R> <C> Mean <C> 0.00341 <C> 0.00330 <C> 0.00331 <R> <C> CNN <C> 0.00350 <C> 0.00348 <C> 0.00349 <R> <C> LSTM <C> 0.00344 <C> 0.00335 <C> 0.00336 <CAP> Table 3: Average MSE of 10-fold cross-validation.
<R> <C> Model <C> Pretraining <C> Frozen <C> du <C> es <C> fr <C> it <C> ky <C> ru <C> sv <C> tr <C> tt <C> zh <C> Avg <R> <C> From scratch <C> - <C> No <C> 84.7 <C> 95.9 <C> 95.1 <C> 95.0 <C> 81.5 <C> 97.7 <C> 86.1 <C> 83.1 <C> 72.9 <C> 84.3 <C> 87.6 <R> <C> Bottleneck [fer2017multilingually] <C> Babel-1070h <C> Yes <C> 47.9 <C> 36.6 <C> 48.3 <C> 39.0 <C> 38.7 <C> 45.2 <C> 52.6 <C> 43.4 <C> 42.5 <C> 54.3 <C> 44.9 <R> <C> Supervised <C> LS-100h <C> Yes <C> 42.4 <C> 36.4 <C> 47.0 <C> 40.5 <C> 41.0 <C> 43.6 <C> 47.0 <C> 48.5 <C> 41.5 <C> 56.8 <C> [BOLD] 44.5 <R> <C> CPC [DBLP:journals/corr/abs-1807-03748] <C> LS-100h <C> Yes <C> 51.5 <C> 44.2 <C> 54.5 <C> 47.0 <C> 44.8 <C> 49.0 <C> 54.0 <C> 54.7 <C> 48.9 <C> 60.1 <C> 50.9 <R> <C> Modified CPC <C> LS-100h <C> Yes <C> 44.4 <C> 38.7 <C> 49.3 <C> 42.1 <C> 40.7 <C> 45.2 <C> 48.8 <C> 49.7 <C> 44.0 <C> 55.5 <C> 45.8 <R> <C> Modified CPC <C> LS-360h <C> Yes <C> 42.5 <C> 38.0 <C> 47.1 <C> 40.5 <C> 41.2 <C> 43.7 <C> 47.5 <C> 47.3 <C> 42.0 <C> 55.0 <C> [BOLD] 44.5 <CAP> Table 3: Transfer of pre-trained phoneme features across languages. We pre-train the features on 100h and 360h of Librispeech with supervision (“Supervised”) or not (“CPC” and “Modified CPC”). We also include multilingual bottleneck features (“Bottleneck”) pre-trained on 1070h from the Babel dataset. We train a linear classifier on the frozen features using 1h of speech from the Common Voice database in different languages. We also report a supervised model trained entirely from scratch on the 1h of speech. We report Phone Error Rate. The languages are: Dutch (du), Spanish (es), French (fr), Italian (it), Kyrgyz (ky), Russian (ru), Sweedish (sv), Turkish (tr), Tatar (tt) and Mandarin (zh).
<R> <C> Model <C> pretraining <C> frozen <C> finetune <R> <C> From scratch <C> - <C> - <C> 38.3 <R> <C> Supervised <C> LS-100 <C> 37.6 <C> [BOLD] 29.2 <R> <C> CPC [DBLP:journals/corr/abs-1807-03748] <C> LS-100 <C> 43.5 <C> 33.3 <R> <C> Mod. CPC <C> LS-100 <C> 38.8 <C> 31.0 <R> <C> Mod. CPC <C> LS-360 <C> [BOLD] 37.2 <C> 30.7 <CAP> Table 5: Comparison between frozen and fine-tuned features. PER averaged over 5 languages (Spanish, French, Italian, Russian and Tatar). The training set for each language contains 5 hours extracted from the Common Voice database.
<R> <C> [EMPTY] <C> Across <C> Within <R> <C> [ITALIC] Trained on Librispeech-100 <C> [ITALIC] Trained on Librispeech-100 <C> [ITALIC] Trained on Librispeech-100 <R> <C> CPC + LN <C> 12.0 <C> 8.7 <R> <C> CPC + LN + Conv8 <C> 13.4 <C> 9.2 <R> <C> CPC + LN + FFD <C> 11.7 <C> 8.56 <R> <C> CPC + LN + transformer <C> 9.5 <C> 7.3 <R> <C> CPC + LN + transformer + dropout <C> [BOLD] 9.3 <C> [BOLD] 6.8 <CAP> Table S2: Phoneme discriminability for various predictors design. Within- and across-speakers ABX scores for the English Zerospeech2017 test set.
<R> <C> [BOLD] MODEL <C> [BOLD] DRCD-dev EM <C> [BOLD] DRCD-dev F1 <C> [BOLD] ODSQA-test EM <C> [BOLD] ODSQA-test F1 <R> <C> BiDAF-word(a) <C> 56.45 <C> 70.57 <C> 39.38 <C> 55.1 <R> <C> BiDAF-char(b) <C> 70.23 <C> 81.65 <C> 55.29 <C> 67.16 <R> <C> R-NET-word <C> 70.38 <C> 79.25 <C> 36.68 <C> 46.55 <R> <C> R-NET-char <C> 69.90 <C> 79.49 <C> 43.44 <C> 55.83 <R> <C> QAnet-word <C> 69.83 <C> 78.33 <C> 49.80 <C> 59.35 <R> <C> QAnet-char <C> 70.78 <C> 80.83 <C> 46.52 <C> 59.11 <R> <C> Dr.QA-word <C> 63.21 <C> 74.11 <C> 41.39 <C> 54.28 <R> <C> Dr.QA-char <C> 70.24 <C> 81.19 <C> 56.22 <C> 68.99 <R> <C> F-Net-word <C> 57.54 <C> 70.86 <C> 45.39 <C> 57.40 <R> <C> F-Net-char <C> 71.33 <C> 82.12 <C> 47.98 <C> 67.26 <R> <C> [BOLD] Average-word <C> 63.48 <C> 74.62 <C> 42.52 <C> 54.53 <R> <C> [BOLD] Average-char <C> 70.49 <C> 81.05 <C> 49.89 <C> 63.67 <CAP> Table 3: Experiment results for state-of-the-art QA models demonstrating degrading performance under spoken data. All models were trained on the full DRCD training set. FusionNet is denoted by F-NET. DRCD dev set and ODSQA testing set are denoted by DRCD-dev and ODSQA-test, respectively.
<R> <C> [BOLD] MODEL <C> [BOLD] DRCD-dev EM <C> [BOLD] DRCD-dev F1 <C> [BOLD] ODSQA-test EM <C> [BOLD] ODSQA-test F1 <R> <C> DRCD (a) <C> 70.23 <C> 81.65 <C> 55.29 <C> 67.16 <R> <C> +pingyin (b) <C> 71.05 <C> 81.82 <C> 55.49 <C> 68.79 <R> <C> DRCD-TTS (c) <C> 59.24 <C> 72.64 <C> 50.64 <C> 63.65 <R> <C> +pingyin (d) <C> 61.36 <C> 74.22 <C> 51.74 <C> 64.59 <R> <C> DRCD-back (e) <C> 58.56 <C> 72.31 <C> 46.55 <C> 61.52 <R> <C> +pingyin (f) <C> 58.63 <C> 72.97 <C> 48.2 <C> 62.82 <R> <C> DRCD+TTS (i) <C> 70.51 <C> 81.85 <C> 55.97 <C> 69.31 <R> <C> +pingyin (j) <C> 71.53 <C> 82.42 <C> 56.65 <C> 69.45 <R> <C> DRCD+back (g) <C> 71.39 <C> 82.28 <C> 55.29 <C> 68.49 <R> <C> +pingyin (h) <C> 71.8 <C> 82.4 <C> 57.6 <C> 69.26 <R> <C> DRCD+TTS+back (k) <C> 72.21 <C> 82.8 <C> 57.61 <C> 70.29 <R> <C> +pingyin (l) <C> [BOLD] 72.76 <C> [BOLD] 83.15 <C> [BOLD] 59.52 <C> [BOLD] 71.01 <R> <C> [BOLD] Average (m) <C> 67.02 <C> 78.92 <C> 53.55 <C> 66.73 <R> <C> [BOLD] Average-pingyin (n) <C> 67.85 <C> 79.49 <C> 54.86 <C> 67.65 <CAP> Table 4: Comparison experiments demonstrating that the proposed sub-word units improved EM/F1 scores over both DRCD-dev and ODSQA-test. We use BiDAF as our base model in all experiments. Furthermore, augmenting DRCD with DRCD-TTS and DRCD-backtrans also gain improvements. Training with the combination of DRCD and DRCD-backtrans, the combination of DRCD and DRCD-TTS and the combination of DRCD, DRCD-TTS and DRCD-bakctrans are denoted as DRCD+back, DRCD+TTS and DRCD+TTS+back respectively.
<R> <C> [BOLD] MODEL <C> [BOLD] Text-Q EM <C> [BOLD] Text-Q F1 <C> [BOLD] Spoken-Q EM <C> [BOLD] Spoken-Q F1 <R> <C> DRCD (a) <C> 59.63 <C> 72.02 <C> 55.29 <C> 67.16 <R> <C> +pingyin (b) <C> 61.47 <C> 72.93 <C> 55.49 <C> 68.79 <R> <C> DRCD-TTS (c) <C> 54.43 <C> 67.18 <C> 50.64 <C> 63.65 <R> <C> +pingyin (d) <C> 55.39 <C> 68.12 <C> 51.74 <C> 64.59 <R> <C> DRCD-back (a) <C> 52.45 <C> 67.13 <C> 46.55 <C> 61.52 <R> <C> +pingyin (b) <C> 53.41 <C> 68.57 <C> 48.2 <C> 62.82 <R> <C> DRCD+TTS (i) <C> 61.95 <C> 73.78 <C> 55.97 <C> 69.31 <R> <C> +pingyin (j) <C> 62.43 <C> 74.3 <C> 56.65 <C> 69.45 <R> <C> DRCD+back (c) <C> 62.22 <C> 74.33 <C> 55.29 <C> 68.49 <R> <C> +pingyin (d) <C> 62.7 <C> 74.81 <C> 57.6 <C> 69.26 <R> <C> DRCD+TTS+back (e) <C> 63.11 <C> 75.27 <C> 58.29 <C> 69.94 <R> <C> +pingyin (f) <C> 64.54 <C> 75.63 <C> 59.52 <C> 70.95 <R> <C> [BOLD] Average (g) <C> 58.96 <C> 71.61 <C> 53.55 <C> 66.73 <R> <C> [BOLD] Average-pingyin (h) <C> 59.99 <C> 72.39 <C> 54.86 <C> 67.65 <CAP> Table 5: Comparison experiments between input with text question and input with transcribed question. We use BiDAF as our base model in all experiments.
<R> <C> Model <C> 2-R <C> 2-F <C> 3-F <C> SU4-F <R> <C> TalkSumm-Hybrid <C> [BOLD] 35.05 <C> [BOLD] 34.11 <C> [BOLD] 27.19 <C> [BOLD] 24.13 <R> <C> TalkSumm-only <C> 22.77 <C> 21.94 <C> 15.94 <C> 12.55 <R> <C> GCN Hybrid 2* <C> 32.44 <C> 30.08 <C> 23.43 <C> 23.77 <R> <C> GCN Cited text spans* <C> 25.16 <C> 24.26 <C> 18.79 <C> 17.67 <R> <C> Abstract* <C> 29.52 <C> 29.4 <C> 23.16 <C> 23.34 <CAP> Table 2: ROUGE scores on the CL-SciSumm 2016 test benchmark. *: results from aaai19.scisumm.
<R> <C> [BOLD] System <C> Syntactically correct (%) <C> Semantically correct (%) <C> Relevant (%) <R> <C> QG  <C> 51.6 <C> 48 <C> 52.3 <R> <C> QG+ F <C> 59.6 <C> 57 <C> 64.6 <R> <C> QG+ F + NE <C> 57 <C> 52.6 <C> 67 <R> <C> QG+ GAE <C> 44 <C> 35.3 <C> 50.6 <R> <C> QG+ F + AES <C> 51 <C> 47.3 <C> 55.3 <R> <C> [BOLD] QG+ F + AEB <C> 61 <C> 60.6 <C> [BOLD] 71.3 <R> <C> [BOLD] QG+ F + GAE <C> [BOLD] 63 <C> [BOLD] 61 <C> 67 <CAP> Table 1: Human evaluation results on Ste. Parameters are, p1: percentage of syntactically correct questions, p2: percentage of semantically correct questions, p3: percentage of relevant questions.
<R> <C> [BOLD] Model <C> [BOLD] BLEU-1 <C> [BOLD] BLEU-2 <C> [BOLD] BLEU-3 <C> [BOLD] BLEU-4 <C> [BOLD] METEOR <C> [BOLD] ROUGE-L <R> <C> QG  <C> 39.97 <C> 22.39 <C> 14.39 <C> 9.64 <C> 14.34 <C> 37.04 <R> <C> QG+ F <C> 41.89 <C> 24.37 <C> 15.92 <C> 10.74 <C> 15.854 <C> 37.762 <R> <C> QG+ F + NE <C> 41.54 <C> 23.77 <C> 15.32 <C> 10.24 <C> 15.906 <C> 36.465 <R> <C> QG+ GAE <C> 43.35 <C> 24.06 <C> 14.85 <C> 9.40 <C> 15.65 <C> 37.84 <R> <C> [BOLD] QG+ F + AES <C> [BOLD] 43.54 <C> [BOLD] 25.69 <C> [BOLD] 17.07 <C> [BOLD] 11.83 <C> [BOLD] 16.71 <C> [BOLD] 38.22 <R> <C> [BOLD] QG+ F + AEB <C> [BOLD] 42.98 <C> [BOLD] 25.65 <C> [BOLD] 17.19 <C> [BOLD] 12.07 <C> [BOLD] 16.72 <C> [BOLD] 38.50 <R> <C> [BOLD] QG+ F + GAE <C> [BOLD] 46.32 <C> [BOLD] 28.81 <C> [BOLD] 19.67 <C> [BOLD] 13.85 <C> [BOLD] 18.51 <C> [BOLD] 41.75 <CAP> Table 2: Automatic evaluation results on Ste. BLEU, METEOR and ROUGE-L scores vary between 0 and 100, with the upper bound of 100 attainable on the ground truth. QG[7]:Result obtained using latest version of Torch.
<R> <C> [BOLD] Model <C> MR <C> SST-1 <C> SST-2 <C> Subj <C> TREC <C> CR <C> MPQA <R> <C> CNN-non-static <C> 81.5 <C> 48.0 <C> 87.2 <C> 93.4 <C> 93.6 <C> 84.3 <C> 89.5 <R> <C> CNN-reproduce <C> 81.4 <C> 47.8 <C> 87.5 <C> 93.0 <C> 92.4 <C> 84.3 <C> 89.6 <R> <C> CNN-Dropout-same (p) <C> 81.5(0.1) <C> 48.5(0.1) <C> 87.6(0.1) <C> 93.5(0.2) <C> 92.9(0.1) <C> 84.5(0.5) <C> 87.4(0.1) <R> <C> CNN-GI-Dropout ( [ITALIC] β) <C> [BOLD] 81.9(0.87) <C> [BOLD] 49.0(0.95) <C> [BOLD] 88.1(0.98) <C> [BOLD] 93.4(0.91) <C> [BOLD] 93.2(0.83) <C> [BOLD] 85.1(0.87) <C> [BOLD] 89.8(0.98) <R> <C> RNN-baseline <C> 82.1 <C> 49.7 <C> 89.7 <C> 93.6 <C> 92.6 <C> 84.1 <C> 89.6 <R> <C> RNN-Dropout-same (p) <C> 82.2(0.2) <C> 51.9(0.1) <C> 90.1(0.1) <C> 93.9(0.1) <C> 93.4(0.2) <C> 84.2(0.1) <C> [BOLD] 89.7(0.1) <R> <C> RNN-GI-Dropout ( [ITALIC] β) <C> [BOLD] 82.5(0.87) <C> [BOLD] 54.1(0.95) <C> [BOLD] 90.4(0.95) <C> [BOLD] 94.2(0.98) <C> [BOLD] 94.8(0.95) <C> [BOLD] 84.7(0.91) <C> [BOLD] 89.7(0.98) <R> <C> MVCNN <C> - <C> 49.6 <C> 89.4 <C> 93.9 <C> - <C> - <C> - <R> <C> MGNC-CNN <C> - <C> 48.7 <C> 88.3 <C> 94.1 <C> 95.5 <C> - <C> - <R> <C> CNN-Rule <C> 81.7 <C> - <C> 89.3 <C> - <C> - <C> 85.3 <C> - <R> <C> Semantic-CNN <C> 82.1 <C> 50.8 <C> 89.0 <C> 93.7 <C> 94.4 <C> 86.0 <C> 89.3 <R> <C> combine-skip <C> 76.5 <C> - <C> - <C> 93.6 <C> 92.2 <C> 80.1 <C> 87.1 <R> <C> DSCNN <C> 82.2 <C> 50.6 <C> 88.7 <C> 93.9 <C> 95.6 <C> - <C> - <R> <C> Paragraph Vector <C> 74.8 <C> 48.7 <C> 87.8 <C> 90.5 <C> 91.8 <C> 78.1 <C> 74.2 <R> <C> NBSVM <C> 79.4 <C> - <C> - <C> 93.2 <C> - <C> 81.8 <C> 86.3 <R> <C> Tree LSTM <C> - <C> 51.0 <C> 88.0 <C> - <C> - <C> - <C> - <CAP> Table 4: Effectiveness of GI-Dropout. Dropout-same means dropping units with the same probability. Results also include: MVCNN Yin and Schütze (2015), MGNC-CNN Zhang et al. (2016b), CNN-Rule Hu et al. (2016), Semantic-CNN Li et al. (2017), combine-skip Kiros et al. (2015), combine-skip Kiros et al. (2015), DSCNN Zhang et al. (2016a), Paragraph Vector Le and Mikolov (2014), NBSVM Wang and Manning (2012) and Tree LSTM Tai et al. (2015).
<R> <C> Methods <C> How many VQA 1.0 <C> How many VQA 2.0 <C> What animal VQA 1.0 <C> What animal VQA 2.0 <R> <C> Question-only <C> 50.37 <C> 49.80 <C> 54.49 <C> 53.55 <R> <C> Strong-baseline <C> 49.89 <C> - <C> 33.84 <C> - <R> <C> Strong-baseline-SR <C> 49.81 <C> - <C> 33.85 <C> - <R> <C> Up-down <C> - <C> 48.01 <C> - <C> 33.81 <R> <C> Up-down-SR <C> - <C> 47.90 <C> - <C> 33.69 <R> <C> Counter <C> - <C> 46.30 <C> - <C> 31.09 <R> <C> Counter-SR <C> - <C> 46.26 <C> - <C> 31.04 <CAP> Table 3. LP scores of four baselines and three regularized methods on two typical question types.
<R> <C> Method <C> Generated corpus A <C> Generated corpus F1 <C> Human eval read <C> Human eval info <R> <C> filippova-emnlp15 <C> [BOLD] 35.36 <C> [BOLD] 82.83 <C> 4.66 <C> 4.03 <R> <C> Automatic <C> - <C> - <C> 4.31 <C> 3.77 <R> <C> Our Local (B=1) <C> 30.51 <C> 78.72 <C> 4.58 <C> 4.03 <R> <C> Our Local (B=8) <C> 31.19 <C> 75.69 <C> - <C> - <R> <C> Our Global (B=8) <C> 35.16 <C> 81.41 <C> [BOLD] 4.67 <C> [BOLD] 4.07 <CAP> Table 4: Sentence compression results on News data. Automatic refers to application of the same automatic extraction rules used to generate the News training corpus.
<R> <C> Method <C> UAS <C> LAS <R> <C> Local (B=1) <C> 92.85 <C> 90.59 <R> <C> Local (B=16) <C> 93.32 <C> 91.09 <R> <C> Global (B=16) { [ITALIC] θ( [ITALIC] d)} <C> 93.45 <C> 91.21 <R> <C> Global (B=16) { [ITALIC] W2, [ITALIC] θ( [ITALIC] d)} <C> 94.01 <C> 91.77 <R> <C> Global (B=16) { [ITALIC] W1, [ITALIC] W2, [ITALIC] θ( [ITALIC] d)} <C> 94.09 <C> 91.81 <R> <C> Global (B=16) (full) <C> 94.38 <C> 92.17 <CAP> Table 5: WSJ dev set scores for successively deeper levels of backpropagation. The full parameter set corresponds to backpropagation all the way to the embeddings. Wi: hidden layer i weights.
<R> <C> Method <C> Predicted compression <C> [ITALIC] pL <C> [ITALIC] pG <R> <C> Local (B=1) <C> In Pakistan, former leader Pervez Musharraf has appeared in court for the first time, on treason charges. <C> 0.13 <C> 0.05 <R> <C> Local (B=8) <C> In Pakistan, former leader Pervez Musharraf has appeared in court for the first time, on treason charges. <C> 0.16 <C> <10−4 <R> <C> Global (B=8) <C> In Pakistan, former leader Pervez Musharraf has appeared in court for the first time, on treason charges. <C> 0.06 <C> 0.07 <CAP> Table 6: Example sentence compressions where the label bias of the locally normalized model leads to a breakdown during beam search. The probability of each compression under the local (pL) and global (pG) models shows that only the global model can properly represent zero probability for the empty compression.
<R> <C> [BOLD] Models <C> [ITALIC] Accuracy <R> <C> Majority Class <C> 25.69 <R> <C> Baseline I <C> 41.97 <R> <C> Unidirectional LSTM: only POS <C> 34.90 <R> <C> only Word <C> 35.12 <R> <C> only Dependency <C> 34.48 <R> <C> Bidirectional LSTMs: only POS <C> 39.19 <R> <C> only Word <C> 37.69 <R> <C> only Dependency <C> 40.04 <R> <C> 2 Sequences: POS + Word <C> 44.54 <R> <C> Dependency + Word <C> 45.18 <R> <C> Dependency + POS <C> 47.75 <R> <C> [BOLD] Full Model <C> [BOLD] 53.32 <R> <C> Direct dependency path <C> 49.25 <R> <C> Baseline II <C> 43.90 <R> <C> Baseline III <C> 44.75 <CAP> Table 2: Temporal relation classification result on TimeBank corpus.
<R> <C> Relations <C> [ITALIC] OurSystem P <C> [ITALIC] OurSystem R <C> [ITALIC] OurSystem F <C> [ITALIC] BaselineI P <C> [ITALIC] BaselineI R <C> [ITALIC] BaselineI F <R> <C> After <C> [BOLD] 0.62 <C> [BOLD] 0.68 <C> [BOLD] 0.65 <C> 0.56 <C> 0.48 <C> 0.45 <R> <C> Before <C> [BOLD] 0.56 <C> [BOLD] 0.52 <C> [BOLD] 0.53 <C> 0.37 <C> 0.45 <C> 0.41 <R> <C> Simultan. <C> [BOLD] 0.44 <C> [BOLD] 0.51 <C> [BOLD] 0.47 <C> 0.32 <C> 0.43 <C> 0.37 <R> <C> Identity <C> [BOLD] 0.47 <C> [BOLD] 0.56 <C> [BOLD] 0.51 <C> 0.45 <C> 0.53 <C> 0.49 <R> <C> Includes <C> [BOLD] 0.59 <C> [BOLD] 0.39 <C> [BOLD] 0.47 <C> 0.43 <C> 0.30 <C> 0.35 <R> <C> IS_includ. <C> 0.5 <C> [BOLD] 0.56 <C> 0.53 <C> [BOLD] 0.61 <C> 0.51 <C> [BOLD] 0.56 <R> <C> Ended_by <C> [BOLD] 0.48 <C> [BOLD] 0.63 <C> [BOLD] 0.55 <C> 0.41 <C> 0.47 <C> 0.44 <R> <C> During_in. <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Begun_by <C> [BOLD] 0.75 <C> [BOLD] 0.43 <C> [BOLD] 0.55 <C> 0 <C> 0 <C> 0 <R> <C> Begins <C> [BOLD] 1.0 <C> [BOLD] 0.29 <C> [BOLD] 0.44 <C> 0 <C> 0 <C> 0 <R> <C> IBefore <C> [BOLD] 0.4 <C> [BOLD] 0.4 <C> [BOLD] 0.4 <C> 0 <C> 0 <C> 0 <R> <C> IAfter <C> [BOLD] 0.33 <C> [BOLD] 0.25 <C> [BOLD] 0.29 <C> 0 <C> 0 <C> 0 <R> <C> During <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Ends <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Macro Av. <C> [BOLD] 0.44 <C> [BOLD] 0.37 <C> [BOLD] 0.40 <C> 0.23 <C> 0.22 <C> 0.22 <CAP> Table 3: Per-class results of our best system and the baseline I.
<R> <C> Model <C> ROUGE-1 <C> ROUGE-2 <C> ROUGE-L <R> <C> 2-agent <C> 40.94 <C> 19.16 <C> 37.54 <R> <C> 3-agent <C> [BOLD] 41.69 <C> [BOLD] 19.47 <C> 37.92 <R> <C> 5-agent <C> 40.99 <C> 19.02 <C> [BOLD] 38.21 <CAP> Table 3: Comparison of multi-agent models varying the number of agents using ROUGE results of model (m7) from Table 1 on CNN/Daily Maily Dataset.
<R> <C> AM <C> Features <C> Train. Data <C> SentDys <C> SentNor <R> <C> DNN <C> FB <C> Nor. VL <C> 36.4 <C> 6.0 <R> <C> CNN <C> FB <C> Nor. VL <C> 33.5 <C> 5.3 <R> <C> TFCNN <C> FB <C> Nor. VL <C> 33.8 <C> 5.3 <R> <C> fCNN <C> FB + TV <C> Nor. VL <C> [BOLD] 32.2 <C> 5.0 <R> <C> DNN <C> FB <C> Nor. VL + Nor. NL <C> 32.1 <C> 5.5 <R> <C> CNN <C> FB <C> Nor. VL + Nor. NL <C> 30.1 <C> 4.9 <R> <C> TFCNN <C> FB <C> Nor. VL + Nor. NL <C> 30.1 <C> 4.9 <R> <C> fCNN <C> FB + TV <C> Nor. VL + Nor. NL <C> [BOLD] 29.0 <C> 4.9 <CAP> Table 2: Word error rates in % obtained on the Flemish test sets using different acoustic models
<R> <C> [BOLD] Approaches <C> [BOLD] Precision  [BOLD] P(1) <C> [BOLD] Precision  [BOLD] P(0) <C> [BOLD] Precision  [BOLD] P(avg) <C> [BOLD] Recall  [BOLD] R(1) <C> [BOLD] Recall  [BOLD] R(0) <C> [BOLD] Recall  [BOLD] R(avg) <C> [BOLD] F-score  [BOLD] F(1) <C> [BOLD] F-score  [BOLD] F(0) <C> [BOLD] F-score  [BOLD] F(avg) <R> <C> [BOLD] Past Approaches <C> [BOLD] Past Approaches <C> [BOLD] Past Approaches <C> [BOLD] Past Approaches <C> [BOLD] Past Approaches <C> [BOLD] Past Approaches <C> [BOLD] Past Approaches <C> [BOLD] Past Approaches <C> [BOLD] Past Approaches <C> [BOLD] Past Approaches <R> <C> Buschmeier et.al. <C> 0.19 <C> 0.98 <C> 0.84 <C> 0.99 <C> 0.07 <C> 0.24 <C> 0.32 <C> 0.13 <C> 0.16 <R> <C> Liebrecht et.al. <C> 0.19 <C> 1.00 <C> 0.85 <C> 1.00 <C> 0.07 <C> 0.24 <C> 0.32 <C> 0.13 <C> 0.17 <R> <C> Gonzalez et.al. <C> 0.19 <C> 0.96 <C> 0.83 <C> 0.99 <C> 0.06 <C> 0.23 <C> 0.32 <C> 0.12 <C> 0.15 <R> <C> Joshi et.al. <C> 0.20 <C> 1.00 <C> 0.86 <C> 1.00 <C> 0.13 <C> 0.29 <C> 0.33 <C> 0.23 <C> [BOLD] 0.25 <R> <C> [BOLD] Rule-Based Approaches <C> [BOLD] Rule-Based Approaches <C> [BOLD] Rule-Based Approaches <C> [BOLD] Rule-Based Approaches <C> [BOLD] Rule-Based Approaches <C> [BOLD] Rule-Based Approaches <C> [BOLD] Rule-Based Approaches <C> [BOLD] Rule-Based Approaches <C> [BOLD] Rule-Based Approaches <C> [BOLD] Rule-Based Approaches <R> <C> Approach-1 <C> 0.53 <C> 0.87 <C> 0.81 <C> 0.39 <C> 0.92 <C> 0.83 <C> 0.45 <C> 0.90 <C> [BOLD] 0.82 <R> <C> Approach-2 <C> 0.44 <C> 0.85 <C> 0.78 <C> 0.28 <C> 0.92 <C> 0.81 <C> 0.34 <C> 0.89 <C> 0.79 <R> <C> [BOLD] Machine-Learning Approaches <C> [BOLD] Machine-Learning Approaches <C> [BOLD] Machine-Learning Approaches <C> [BOLD] Machine-Learning Approaches <C> [BOLD] Machine-Learning Approaches <C> [BOLD] Machine-Learning Approaches <C> [BOLD] Machine-Learning Approaches <C> [BOLD] Machine-Learning Approaches <C> [BOLD] Machine-Learning Approaches <C> [BOLD] Machine-Learning Approaches <R> <C> SVM <C> 0.50 <C> 0.95 <C> 0.87 <C> 0.80 <C> 0.82 <C> 0.82 <C> 0.61 <C> 0.88 <C> [BOLD] 0.83 <R> <C> KNN <C> 0.36 <C> 0.94 <C> 0.84 <C> 0.81 <C> 0.68 <C> 0.70 <C> 0.50 <C> 0.79 <C> 0.74 <R> <C> Random Forest <C> 0.47 <C> 0.93 <C> 0.85 <C> 0.74 <C> 0.81 <C> 0.80 <C> 0.57 <C> 0.87 <C> 0.82 <R> <C> [BOLD] Deep-Learning Approaches <C> [BOLD] Deep-Learning Approaches <C> [BOLD] Deep-Learning Approaches <C> [BOLD] Deep-Learning Approaches <C> [BOLD] Deep-Learning Approaches <C> [BOLD] Deep-Learning Approaches <C> [BOLD] Deep-Learning Approaches <C> [BOLD] Deep-Learning Approaches <C> [BOLD] Deep-Learning Approaches <C> [BOLD] Deep-Learning Approaches <R> <C> [BOLD] CNN-FF <C> [BOLD] 0.88 <C> [BOLD] 0.94 <C> [BOLD] 0.93 <C> [BOLD] 0.71 <C> [BOLD] 0.98 <C> [BOLD] 0.93 <C> [BOLD] 0.79 <C> [BOLD] 0.96 <C> [BOLD] 0.93 <R> <C> CNN-LSTM-FF <C> 0.82 <C> 0.94 <C> 0.92 <C> 0.72 <C> 0.96 <C> 0.92 <C> 0.77 <C> 0.95 <C> 0.92 <R> <C> LSTM-FF <C> 0.76 <C> 0.93 <C> 0.90 <C> 0.68 <C> 0.95 <C> 0.90 <C> 0.72 <C> 0.94 <C> 0.90 <CAP> Table 6: Comparison Table for Classification Results of different approaches on Test Dataset. Subscripts 0 and 1 denotes Non-Sarcastic and Numeric-Sarcastic Class respectively
<R> <C> language <C> Vocab. size <C> OOVR <C> large models CRNN500 <C> large models Mixed <C> light models CRNN200 <C> light models Cond. <R> <C> Bulgarian <C> 109 k <C> 1.87 <C> 1.28 <C> 1.26 <C> 1.52 <C> 1.27 <R> <C> Czech <C> 144 k <C> 3.02 <C> 1.54 <C> 1.53 <C> 1.79 <C> 1.52 <R> <C> Danish <C> 128 k <C> 2.78 <C> 1.37 <C> 1.36 <C> 1.62 <C> 1.37 <R> <C> German <C> 136 k <C> 2.78 <C> 1.32 <C> 1.31 <C> 1.53 <C> 1.30 <R> <C> Greek <C> 132 k <C> 2.24 <C> 1.28 <C> 1.27 <C> 1.55 <C> 1.27 <R> <C> Spanish <C> 105 k <C> 1.71 <C> 1.28 <C> 1.26 <C> 1.50 <C> 1.27 <R> <C> Estonian <C> 190 k <C> 5.29 <C> 1.48 <C> 1.50 <C> 1.72 <C> 1.47 <R> <C> Finnish <C> 227 k <C> 6.91 <C> 1.39 <C> 1.43 <C> 1.63 <C> 1.38 <R> <C> French <C> 105 k <C> 1.67 <C> 1.24 <C> 1.23 <C> 1.48 <C> 1.24 <R> <C> Hungarian <C> 208 k <C> 5.05 <C> 1.39 <C> 1.42 <C> 1.65 <C> 1.36 <R> <C> Italian <C> 115 k <C> 1.95 <C> 1.30 <C> 1.29 <C> 1.52 <C> 1.29 <R> <C> Lithuanian <C> 163 k <C> 4.25 <C> 1.45 <C> 1.46 <C> 1.69 <C> 1.45 <R> <C> Latvian <C> 138 k <C> 3.06 <C> 1.42 <C> 1.41 <C> 1.65 <C> 1.42 <R> <C> Dutch <C> 109 k <C> 2.09 <C> 1.33 <C> 1.32 <C> 1.56 <C> 1.31 <R> <C> Polish <C> 153 k <C> 3.13 <C> 1.41 <C> 1.39 <C> 1.66 <C> 1.39 <R> <C> Portuguese <C> 110 k <C> 1.88 <C> 1.33 <C> 1.30 <C> 1.54 <C> 1.32 <R> <C> Romanian <C> 104 k <C> 1.66 <C> 1.29 <C> 1.26 <C> 1.54 <C> 1.27 <R> <C> Slovak <C> 145 k <C> 2.95 <C> 1.47 <C> 1.45 <C> 1.74 <C> 1.46 <R> <C> Slovene <C> 132 k <C> 2.66 <C> 1.47 <C> 1.44 <C> 1.70 <C> 1.47 <R> <C> Swedish <C> 130 k <C> 2.85 <C> 1.40 <C> 1.39 <C> 1.64 <C> 1.38 <R> <C> Average <C> [EMPTY] <C> [EMPTY] <C> 1.37 <C> 1.36 <C> 1.61 <C> 1.36 <CAP> Table 3: Results on the Europarl dataset. For all languages, we report the word vocabulary size and the out-of-vocabulary rate on the validation set. We report the performance of the mixed model (Mixed) and the conditional model (Cond.). We compare to a character-level RNN (CRNN) with hidden representations of size 200 and 500.
<R> <C> [BOLD] Method <C> [BOLD] Mean search time (ms) <C> [BOLD] Wikinews R@100 <R> <C> Brute force <C> 291.9 <C> 97.88 <R> <C> AH <C> 22.6 <C> 97.22 <R> <C> AH+Tree <C> 3.3 <C> 94.73 <CAP> Table 3: Comparison of nearest-neighbor search methods using the DEER model. The benchmark was conducted on a single machine. AH indicates quantization-based asymmetric hashing; AH+Tree adds an initial tree search to further reduce the search space.
<R> <C> [BOLD] Attack <C> [BOLD] LM Test Perplexity <C> [BOLD] Avg. Perplexity Gap <R> <C> [ITALIC] None (ground-truth corpus) <C> [ITALIC] 117.93 <C> [ITALIC] 0.0 <R> <C> Vanilla AdvT-Text <C> 122.53 <C> 4.60 <R> <C> iAdvT-Text <C> 123.86 <C> 5.93 <R> <C> SPGD <C> [BOLD] 119.02 <C> [BOLD] 1.09 <CAP> Table 1: Adversarial Quality
<R> <C> [BOLD] Model <C> [BOLD] Test Accuracy <C> [BOLD] Test Error Rate <R> <C> Baseline <C> 89.83 <C> 10.17 <R> <C> Pretrained <C> 92.69 <C> 7.31 <R> <C> Pretrained w/AdvT-Text <C> 93.58 <C> [BOLD] 6.42 <R> <C> Pretrained w/iAdvT-Text <C> 93.58 <C> [BOLD] 6.42 <R> <C> Pretrained w/SPGD (1 iter, 75% sparsity) <C> 93.54 <C> [ITALIC] 6.46 ( [ITALIC] ours) <CAP> Table 3: Test Results
<R> <C> [BOLD] Model <C> [BOLD] Afrikaans <C> [BOLD] isiZulu <C> [BOLD] Northern Sotho <C> [BOLD] Setswana <C> [BOLD] Xitsonga <R> <C> ConvS2S <C> 12.30 <C> 0.52 <C> 7.41 <C> 10.31 <C> 10.73 <R> <C> Transformer <C> [BOLD] 20.60 <C> [BOLD] 1.34 <C> [BOLD] 10.94 <C> [BOLD] 15.60 <C> [BOLD] 17.98 <CAP> Table 2: BLEU Scores for English-to-Target language on evaluation dataset
<R> <C> lang. <C> embeddings <C> #dim <C> acc <C> [ITALIC] F1 <C> ex./sec <R> <C> en <C> original <C> 400 <C> .666 <C> .623 <C> 4 <R> <C> en <C> Densifier <C> 40 <C> .662 <C> .620 <C> 85 <R> <C> en <C> Densifier <C> 4 <C> .646 <C> .608 <C> 178 <R> <C> cz <C> original <C> 400 <C> .803 <C> .802 <C> 1 <R> <C> cz <C> Densifier <C> 40 <C> .803 <C> .801 <C> 24 <R> <C> cz <C> Densifier <C> 4 <C> .771 <C> .769 <C> 83 <CAP> Table 4: Performance on Text Polarity Classification
<R> <C> Model <C> Test1 <C> Test2 <R> <C> Bag-of-words <C> 32.1 <C> 32.2 <R> <C> Metzler-Bendersky <C> 55.1 <C> 50.8 <R> <C> Arch-II  <C> 62.8 <C> 59.2 <R> <C> Arch-II GSED  <C> 65.3 <C> 61.0 <R> <C> Attention LSTM  <C> 69.0 <C> 64.8 <R> <C> TF-LSTM Concatenation <C> 62.1 <C> 61.5 <R> <C> Local-Global Attention <C> [BOLD] 70.1 <C> [BOLD] 67.4 <CAP> TABLE I: Performance of various models on InsuranceQA
<R> <C> Dataset Method <C> CamRest Inf P <C> CamRest Inf R <C> CamRest Inf F1 <C> CamRest Req P <C> CamRest Req R <C> CamRest Req F1 <C> KVRET Inf P <C> KVRET Inf R <C> KVRET Inf F1 <C> KVRET Req P <C> KVRET Req R <C> KVRET Req F1 <R> <C> TSCP/RL† <C> 0.970 <C> 0.971 <C> 0.971 <C> 0.983 <C> 0.935 <C> 0.959 <C> [BOLD] 0.936 <C> 0.874 <C> 0.904 <C> 0.725 <C> 0.485 <C> 0.581 <R> <C> TSCP† <C> 0.970 <C> 0.971 <C> 0.971 <C> 0.983 <C> 0.938 <C> 0.960 <C> 0.934 <C> 0.890 <C> 0.912 <C> 0.701 <C> 0.435 <C> 0.526 <R> <C> FSDM/Res <C> 0.979 <C> 0.984 <C> 0.978 <C> 0.994 <C> 0.947 <C> 0.967 <C> 0.918 <C> 0.930 <C> 0.925 <C> 0.812 <C> 0.993 <C> 0.893 <R> <C> FSDM <C> [BOLD] 0.983* <C> [BOLD] 0.986* <C> [BOLD] 0.984* <C> [BOLD] 0.997* <C> [BOLD] 0.952 <C> [BOLD] 0.974* <C> 0.92 <C> [BOLD] 0.935* <C> [BOLD] 0.927* <C> [BOLD] 0.819* <C> [BOLD] 1.000* <C> [BOLD] 0.900* <CAP> Table 2: Turn-level performance results. Inf: Informable, Req: Requestable, P: Precision, R: Recall. Results marked with † are computed using available code, and all the other ones are reported from the original papers. ∗ indicates the improvement is statistically significant with p=0.05.
<R> <C> Dataset Method <C> CamRest BLEU <C> CamRest EMR <C> CamRest SuccF1 <C> KVRET BLEU <C> KVRET EMR <C> KVRET SuccF1 <R> <C> NDM <C> 0.212 <C> 0.904 <C> 0.832 <C> 0.186 <C> 0.724 <C> 0.741 <R> <C> LIDM <C> 0.246 <C> 0.912 <C> 0.840 <C> 0.173 <C> 0.721 <C> 0.762 <R> <C> KVRN <C> 0.134 <C> - <C> - <C> 0.184 <C> 0.459 <C> 0.540 <R> <C> TSCP <C> 0.253 <C> 0.927 <C> 0.854 <C> [BOLD] 0.219 <C> 0.845 <C> 0.811 <R> <C> TSCP/RL † <C> 0.237 <C> 0.915 <C> 0.826 <C> 0.195 <C> 0.809 <C> 0.814 <R> <C> TSCP† <C> 0.237 <C> 0.913 <C> 0.841 <C> 0.189 <C> 0.833 <C> 0.81 <R> <C> FSDM/St <C> 0.245 <C> - <C> 0.847 <C> 0.204 <C> - <C> 0.809 <R> <C> FSDM/Res <C> 0.251 <C> 0.924 <C> 0.855 <C> 0.209 <C> 0.834 <C> 0.815 <R> <C> FSDM <C> [BOLD] 0.258* <C> [BOLD] 0.935* <C> [BOLD] 0.862* <C> 0.215 <C> [BOLD] 0.848* <C> [BOLD] 0.821* <CAP> Table 3: Dialogue level performance results. SuccF1: Success F1 score, EMR: Entity Match Rate. Results marked with † are computed using available code, and all the other ones are reported from the original papers. ∗ indicates the improvement is statistically significant with p=0.05.
<R> <C> [EMPTY] <C> PPMIAL <C> PPMITR <C> SGNSAL <C> SGNSTR <R> <C> Stable <C> 0.52 <C> 0.54 <C> 0.37 <C> [BOLD] 0.57 <R> <C> Unrelated <C> 0.83 <C> 0.83 <C> 0.86 <C> [BOLD] 0.91 <R> <C> Related <C> 0.73 <C> 0.73 <C> 0.78 <C> [BOLD] 0.78 <R> <C> Mean acc. <C> 0.65 <C> 0.66 <C> 0.59 <C> [BOLD] 0.70 <R> <C> F1-score <C> 0.69 <C> 0.69 <C> 0.67 <C> [BOLD] 0.74 <CAP> Table 2: Accuracy (averaged, and split into individual classes) and F1-scores for semantic change detection. For stable words (control words), peaks at 1 and 6 steps are correct. For change words, peaks at steps 2–5 are correct. We see that all methods find unrelated change better than related change, and that SGNSTR outperforms the other methods.
<R> <C> [BOLD] System <C> [BOLD] m <C> [BOLD] EER <C> [BOLD] minDCF0.01 <C> [BOLD] minDCF0.001 <R> <C> Softmax (Kaldi) <C> - <C> 3.208 <C> 0.3481 <C> 0.5753 <R> <C> Softmax <C> - <C> 3.271 <C> 0.3646 <C> 0.5018 <R> <C> A-Softmax <C> 2 <C> 2.434 <C> 0.2774 <C> 0.4536 <R> <C> AM-Softmax <C> 0.2 <C> 2.264 <C> 0.2537 <C> [BOLD] 0.3293 <R> <C> AAM-Softmax <C> 0.3 <C> [BOLD] 2.238 <C> [BOLD] 0.2433 <C> 0.4119 <CAP> Table 3: Comparison of systems under the VoxCeleb1 test set. All systems are trained on VoxCeleb1 trainining set and the whole VoxCeleb2 set with data augmentation.
<R> <C> [BOLD] System <C> [BOLD] m <C> [BOLD] EER <C> [BOLD] minDCF0.01 <C> [BOLD] minDCF0.001 <R> <C> Softmax (Kaldi) <C> - <C> 3.581 <C> 0.3456 <C> 0.5165 <R> <C> Softmax <C> - <C> 3.718 <C> 0.3491 <C> 0.5195 <R> <C> A-Softmax <C> 2 <C> 2.980 <C> 0.3045 <C> 0.5048 <R> <C> AM-Softmax <C> 0.2 <C> 3.089 <C> [BOLD] 0.2931 <C> [BOLD] 0.4496 <R> <C> AAM-Softmax <C> 0.2 <C> [BOLD] 2.761 <C> 0.3002 <C> 0.4712 <CAP> Table 4: Comparison of systems under the SITW test set. All systems are trained on the whole VoxCeleb1 set and VoxCeleb2 development set with data augmentation. 60 speakers in VoxCeleb1 that overlap with the test set are removed.
<R> <C> [BOLD] OOC score <C> [BOLD] Number of elements <C> [BOLD] OOC Mean <C> [BOLD] OOC Std <C> [BOLD] IC Mean <C> [BOLD] IC Std <R> <C> [BOLD] 0.0-0.5 <C> 112 <C> 0.42 <C> 0.09 <C> 0.54 <C> 0.1 <R> <C> [BOLD] 0.5-1.0 <C> 88 <C> 0.67 <C> 0.07 <C> 0.64 <C> 0.07 <CAP> Table 2: We show the number of pairs that received a low score out of context (first row) and the number of pairs that received a high score out of context (second row). We report the mean score and standard deviation (Std) of the two groups when judged out of context (OOC) and when judged in context (IC) by our model. The model’s scores range between 0 and 1. As can be seen, the mean of the low-scoring group rises in context, and the mean of the high-scoring group decreases in context.
<R> <C> [EMPTY] <C> NDCG.20 <C> MAP <C> Prec.5 <R> <C> rev DocID <C> 0.141 <C> 0.455 <C> 0.344 <R> <C> BM25-Title1 <C> 0.325 <C> 0.567 <C> 0.591 <R> <C> UQLM-Title <C> 0.314 <C> 0.560 <C> 0.574 <R> <C> WMD-Title2 <C> 0.329=1 <C> 0.579+1 <C> 0.603+1 <R> <C> DRMM <C> 0.300−1 <C> 0.545−1 <C> 0.549−1 <R> <C> SevMos-C13 <C> 0.352+2 <C> 0.597+2 <C> 0.625+2 <R> <C> SevMos-C34 <C> 0.373+3 <C> 0.594=3,+2 <C> 0.626=3,+2 <R> <C> Delta-325 <C> 0.365+3,−4 <C> 0.601+3,4 <C> 0.634+3,4 <R> <C> Delta-32-Lex3 <C> [BOLD] 0.394+4,5 <C> [BOLD] 0.609+4,5 <C> [BOLD] 0.646+4,5 <CAP> Table 2. Ranking metrics on the Full test data. The superscripts indicate statistical comparisons: ‘+’ for increase, ‘-’ for decrease, ‘=’ for equivalent, to a 99% confidence using a paired t-test. The comparison baselines are indicated with numbers 1 through 5, as marked in the first column. Highest values are in bold.
<R> <C> [EMPTY] <C> NDCG.20 <C> MAP <C> Prec.5 <R> <C> rev DocID <C> 0.081 <C> 0.413 <C> 0.310 <R> <C> BM25-Title1 <C> 0.233 <C> 0.474 <C> 0.490 <R> <C> WMD-Title2 <C> 0.243+1 <C> 0.483+1 <C> 0.496=1 <R> <C> DRMM <C> 0.242+1,=2 <C> 0.461−1,2 <C> 0.462−1,2 <R> <C> SevMos-C13 <C> 0.290+2 <C> 0.510+2 <C> 0.538+2 <R> <C> SevMos-C34 <C> 0.304+2,3 <C> 0.502+2,−3 <C> 0.535+2,=3 <R> <C> Delta-325 <C> 0.296+2,=3 <C> 0.513+2,=3 <C> 0.550+2,3 <R> <C> Delta-32-Lex3 <C> [BOLD] 0.326+4,5 <C> [BOLD] 0.522+4,5 <C> [BOLD] 0.560+4,5 <CAP> Table 3. Ranking metrics on the ‘Neg20+’ test data
<R> <C> [EMPTY] <C> NDCG.20 <C> MAP <R> <C> Delta-32, no Difference vectors <C> 0.323 <C> 0.574 <R> <C> Delta-32, no Delta features <C> 0.333 <C> 0.584 <R> <C> Delta-32 <C> 0.365 <C> 0.601 <R> <C> Delta-32-Lex3, (top 3 Lex features) <C> 0.394 <C> 0.609 <CAP> Table 6. Comparing the impact of different features on the ranking metrics for the Delta model
<R> <C> Models <C> Year <C> Features Appearence <C> Features Motion <C> Features Object <C> MSVD B@4 <C> MSVD M <C> MSVD R <C> MSVD C <C> MSR-VTT B@4 <C> MSR-VTT M <C> MSR-VTT R <C> MSR-VTT C <R> <C> SA-LSTM  <C> 2018 <C> Inception-V4 <C> - <C> - <C> 45.3 <C> 31.9 <C> 64.2 <C> 76.2 <C> 36.3 <C> 25.5 <C> 58.3 <C> 39.9 <R> <C> M3  <C> 2018 <C> VGG <C> C3D <C> - <C> 52.8 <C> 33.3 <C> - <C> - <C> 38.1 <C> 26.6 <C> - <C> - <R> <C> RecNet  <C> 2018 <C> Inception-V4 <C> - <C> - <C> 52.3 <C> 34.1 <C> 69.8 <C> 80.3 <C> 39.1 <C> 26.6 <C> 59.3 <C> 42.7 <R> <C> PickNet∗  <C> 2018 <C> ResNet-152 <C> - <C> - <C> 52.3 <C> 33.3 <C> 69.6 <C> 76.5 <C> 41.3 <C> 27.7 <C> 59.8 <C> 44.1 <R> <C> MARN  <C> 2019 <C> ResNet-101 <C> C3D <C> - <C> 48.6 <C> 35.1 <C> 71.9 <C> 92.2 <C> 40.4 <C> 28.1 <C> 60.7 <C> 47.1 <R> <C> SibNet  <C> 2019 <C> GoogleNet <C> - <C> - <C> 54.2 <C> 34.8 <C> 71.7 <C> 88.2 <C> 40.9 <C> 27.5 <C> 60.2 <C> 47.5 <R> <C> OA-BTG  <C> 2019 <C> ResNet-200 <C> - <C> Mask-RCNN <C> [BOLD] 56.9 <C> 36.2 <C> - <C> 90.6 <C> 41.4 <C> 28.2 <C> - <C> 46.9 <R> <C> GRU-EVE  <C> 2019 <C> InceptionResnetV2 <C> C3D <C> YOLO <C> 47.9 <C> 35.0 <C> 71.5 <C> 78.1 <C> 38.3 <C> 28.4 <C> 60.7 <C> 48.1 <R> <C> MGSA  <C> 2019 <C> InceptionResnetV2 <C> C3D <C> - <C> 53.4 <C> 35.0 <C> - <C> 86.7 <C> 42.4 <C> 27.6 <C> - <C> 47.5 <R> <C> POS+CG  <C> 2019 <C> InceptionResnetV2 <C> OpticalFlow <C> - <C> 52.5 <C> 34.1 <C> 71.3 <C> 88.7 <C> 42.0 <C> 28.2 <C> 61.6 <C> 48.7 <R> <C> POS+VCT  <C> 2019 <C> InceptionResnetV2 <C> C3D <C> - <C> 52.8 <C> 36.1 <C> 71.8 <C> 87.8 <C> 42.3 <C> [BOLD] 29.7 <C> [BOLD] 62.8 <C> 49.1 <R> <C> ORG-TRL <C> Ours <C> InceptionResnetV2 <C> C3D <C> FasterRCNN <C> 54.3 <C> [BOLD] 36.4 <C> [BOLD] 73.9 <C> [BOLD] 95.2 <C> [BOLD] 43.6 <C> 28.8 <C> 62.1 <C> [BOLD] 50.9 <CAP> Table 2: Performance comparisons on MSVD and MSR-VTT benchmarks. The best results and corresponding features are listed.
<R> <C> Methods ORG <C> Methods TRL <C> MSVD B@4 <C> MSVD M <C> MSVD R <C> MSVD C <C> MSR-VTT B@4 <C> MSR-VTT M <C> MSR-VTT R <C> MSR-VTT C <R> <C> × <C> × <C> 53.3 <C> 35.2 <C> 72.4 <C> 91.7 <C> 41.9 <C> 27.5 <C> 61.0 <C> 47.9 <R> <C> ✓ <C> × <C> 54.0 <C> 36.0 <C> 73.2 <C> 94.1 <C> 43.3 <C> 28.4 <C> 61.5 <C> 50.1 <R> <C> × <C> ✓ <C> 54.0 <C> 36.0 <C> 73.7 <C> 93.3 <C> 43.2 <C> 28.6 <C> 61.7 <C> 50.4 <R> <C> ✓ <C> ✓ <C> [BOLD] 54.3 <C> [BOLD] 36.4 <C> [BOLD] 73.9 <C> [BOLD] 95.2 <C> [BOLD] 43.6 <C> [BOLD] 28.8 <C> [BOLD] 62.1 <C> [BOLD] 50.9 <CAP> Table 4: Ablation Studies of the ORG and the TRL on MSVD and MSR-VTT benchmarks.
<R> <C> [BOLD] Model <C> [BOLD] Val <C> [BOLD] Test <R> <C> Wang ( 2017 ), all metadata <C> 0.247 <C> 0.274 <R> <C> Veracity@RulOracles <C> 0.308 <C> 0.300 <R> <C> Veracity@Rul <C> 0.313 <C> 0.313 <R> <C> MT-Veracity@Rul <C> [BOLD] 0.321 <C> [BOLD] 0.323 <R> <C> Alhindi et al. ( 2018 )@Just <C> 0.37 <C> 0.37 <R> <C> Veracity@Just <C> [BOLD] 0.443 <C> [BOLD] 0.443 <CAP> Table 2: Results (Macro F1 scores) of the veracity prediction task on all of the six classes. The models are trained using the text from the ruling oracles (@RulOracles), ruling comment (@Rul), or the gold justification (@Just).
<R> <C> [BOLD] Model <C> [BOLD] Validation  [BOLD] ROUGE-1 <C> [BOLD] Validation  [BOLD] ROUGE-2 <C> [BOLD] Validation  [BOLD] ROUGE-L <C> [BOLD] Test  [BOLD] ROUGE-1 <C> [BOLD] Test  [BOLD] ROUGE-2 <C> [BOLD] Test  [BOLD] ROUGE-L <R> <C> Lead-4 <C> 27.92 <C> 6.94 <C> 24.26 <C> 28.11 <C> 6.96 <C> 24.38 <R> <C> Oracle <C> 43.27 <C> 22.01 <C> 38.89 <C> 43.57 <C> 22.23 <C> 39.26 <R> <C> Explain-Extractive <C> [BOLD] 35.64 <C> [BOLD] 13.50 <C> [BOLD] 31.44 <C> [BOLD] 35.70 <C> [BOLD] 13.51 <C> [BOLD] 31.58 <R> <C> Explain-MT <C> 35.18 <C> 12.94 <C> 30.95 <C> 35.13 <C> 12.90 <C> 30.93 <CAP> Table 3: Results of the veracity explanation generation task. The results are ROUGE-N F1 scores of the gene- rated explanation w.r.t. the gold justification.
<R> <C> ↖ <C> Agree-C <C> [BOLD] Just  [BOLD] 0.403 <C> [BOLD] Explain-Extr 0.237 <C> [BOLD] Explain-MT 0.300 <R> <C> ↘ <C> Agree-NS <C> [BOLD] 0.065 <C> 0.250 <C> 0.188 <R> <C> ↘ <C> Agree-NC <C> [BOLD] 0.064 <C> 0.113 <C> 0.088 <R> <C> ↘ <C> Disagree <C> 0.468 <C> [BOLD] 0.400 <C> 0.425 <CAP> Table 6: Manual veracity labelling, given a particular explanation from the gold justification (Just), the generated explanation (Explain-Extr), and the explanation learned jointly (Explain-MT) with the veracity prediction model. Percentages of the dis/agreeing annotator predictions are shown, with agreement percentages split into: correct according to the gold label (Agree-C), incorrect (Agree-NC) or insufficient information (Agree-NS). The first column indicates whether higher (↖) or lower (↘) values are better. For each row, the best results are in bold, and the best results with automatically generated explanations are in blue.
<R> <C> [BOLD] Evidence Source <C> [BOLD] ROUGE-1 P <C> [BOLD] ROUGE-1 R <C> [BOLD] ROUGE-1 F1 <C> [BOLD] ROUGE-2 P <C> [BOLD] ROUGE-2 R <C> [BOLD] ROUGE-2 F1 <C> [BOLD] ROUGE-L P <C> [BOLD] ROUGE-L R <C> [BOLD] ROUGE-L F1 <R> <C> Ruling <C> 8.65 <C> 78.65 <C> 14.84 <C> 3.53 <C> 33.76 <C> 6.16 <C> 8.10 <C> 74.14 <C> 13.92 <R> <C> Ruling Oracle <C> 43.97 <C> 49.24 <C> 43.79 <C> 22.45 <C> 24.50 <C> 22.03 <C> 39.70 <C> 44.10 <C> 39.37 <CAP> Table 7: Comparison of sources of evidence - Ruling Comments and Ruling Oracles comapred to the target justification summary.
<R> <C> length <C> outermost entities P <C> outermost entities R <C> outermost entities F <C> outermost entities Num. <C> inner entities P <C> inner entities R <C> inner entities F <C> inner entities Num. <R> <C> 1 <C> 75.9 <C> 80.6 <C> 78.2 <C> 1,260 <C> - <C> - <C> - <C> - <R> <C> 2 <C> 72.1 <C> 74.8 <C> 73.4 <C> 488 <C> 76.6 <C> 63.6 <C> 69.5 <C> 77 <R> <C> 3 <C> 67.8 <C> 72.2 <C> 69.9 <C> 198 <C> 67.6 <C> 56.5 <C> 61.5 <C> 85 <R> <C> 4 <C> 62.5 <C> 60.9 <C> 61.7 <C> 112 <C> 68.1 <C> 42.3 <C> 52.2 <C> 111 <R> <C> 5 <C> 60.7 <C> 48.7 <C> 54.0 <C> 76 <C> 56.0 <C> 37.8 <C> 45.1 <C> 74 <R> <C> 6 <C> 46.3 <C> 46.3 <C> 46.3 <C> 41 <C> 28.0 <C> 25.9 <C> 26.9 <C> 54 <R> <C> 7 <C> 44.4 <C> 30.8 <C> 36.4 <C> 26 <C> 21.7 <C> 16.7 <C> 18.9 <C> 30 <R> <C> 8 <C> 64.3 <C> 40.9 <C> 50.0 <C> 22 <C> 31.8 <C> 21.2 <C> 25.5 <C> 33 <R> <C> 9 <C> 35.7 <C> 31.3 <C> 33.3 <C> 16 <C> 23.1 <C> 19.4 <C> 21.1 <C> 31 <R> <C> 10 <C> 57.1 <C> 22.2 <C> 32.0 <C> 18 <C> 20.0 <C> 15.4 <C> 17.4 <C> 26 <CAP> Table 6: Length-wise results on ACE2005 test dataset.
<R> <C> [BOLD] Model <C> [BOLD] NST15 <C> [BOLD] NST13 <C> [BOLD] NST14 <C> [BOLD] NST16 <C> [BOLD] Average <C> Δ <R> <C> Transformer <C> 30.33 <C> 29.14 <C> 28.87 <C> 35.74 <C> 31.25 <C> − <R> <C> + Weighted-fusion ( [ITALIC] deep) <C> 31.43 <C> 29.72 <C> 29.46 <C> 36.48 <C> 31.89 <C> +0.64 <R> <C> + Knowledge Transfer ( [ITALIC] deep) <C> 31.32 <C> 29.61 <C> 29.73 <C> 36.60 <C> 31.98 <C> +0.73 <R> <C> + Our Approach <C> [BOLD] 31.72 <C> [BOLD] 30.32 <C> [BOLD] 30.29 <C> [BOLD] 36.91 <C> [BOLD] 32.51 <C> [BOLD] +1.26 <CAP> Table 2: Translation qualities on the DE→EN experiments.
<R> <C> [BOLD] Model <C> [BOLD] NSD16 <C> [BOLD] NST16 <C> Δ <R> <C> Transformer <C> 15.19 <C> 13.72 <C> − <R> <C> + Our Approach <C> 15.55 <C> 14.43 <C> +0.71 <R> <C> + BT ( [ITALIC] small) <C> 15.58 <C> 14.64 <C> +0.92 <R> <C> + Our Approach <C> 15.84 <C> 14.99 <C> +0.95 <R> <C> + BT ( [ITALIC] medium) <C> 15.70 <C> 14.86 <C> +1.14 <R> <C> + Our Approach <C> 15.89 <C> 15.01 <C> +1.29 <R> <C> + BT ( [ITALIC] big) <C> 16.04 <C> 15.19 <C> +1.47 <R> <C> + Our Approach <C> [BOLD] 16.36 <C> [BOLD] 15.49 <C> [BOLD] +1.77 <CAP> Table 4: Translation qualities on the EN→TR experiments. (BT: back-translation; small, medium and big: the different size of pseudo corpus.)
<R> <C> [BOLD] Model <C> [BOLD] Side <C> [BOLD] BLEU <R> <C> Transformer <C> [EMPTY] <C> 44.59 <R> <C> + Weighted-fusion ( [ITALIC] deep) <C> [ITALIC] src. <C> 45.47 <R> <C> + Weighted-fusion ( [ITALIC] first) <C> [ITALIC] trg. <C> 44.66 <R> <C> + Weighted-fusion ( [ITALIC] deep) <C> [ITALIC] trg. <C> 43.93 <R> <C> + Knowledge Transfer ( [ITALIC] deep) <C> [ITALIC] trg. <C> 45.45 <R> <C> + Knowledge Transfer ( [ITALIC] first) <C> [ITALIC] src. <C> 44.92 <R> <C> + Knowledge Transfer ( [ITALIC] deep) <C> [ITALIC] src. <C> 45.06 <CAP> Table 7: The comparison of translation qualities for using the weighted-fusion and knowledge transfer methods in different sides (src.: source; trg.: target) on ZH-EN task.
<R> <C> [BOLD] System <C> [BOLD] F1 (macro) <C> [BOLD] Accuracy <R> <C> All NOT baseline <C> 0.4189 <C> 0.7209 <R> <C> All OFF baseline <C> 0.2182 <C> 0.2790 <R> <C> Convolutional Neural Network (on training data) <C> 0.8020 <C> 0.8387 <R> <C> Bidirectional LSTM with Attention (on training data) <C> 0.7851 <C> 0.8246 <R> <C> Bidirectional LSTM + Bidirectional GRU (on training data) <C> 0.7893 <C> 0.8301 <R> <C> MIDAS Submission 1 on test data (CNN) <C> 0.7964 <C> 0.8395 <R> <C> MIDAS Submission 2 on test data (Ensemble of CNN, BLSTM with Attention, BLSTM + BGRU) <C> 0.8066 <C> 0.8407 <CAP> Table 1: Results for Sub-task A.
<R> <C> [BOLD] Model <C> [BOLD] Surface  [BOLD] SeLen <C> [BOLD] Surface  [BOLD] WC <C> [BOLD] Surface  [BOLD] Avg <C> [BOLD] Syntactic  [BOLD] TrDep <C> [BOLD] Syntactic  [BOLD] ToCo <C> [BOLD] Syntactic  [BOLD] BShif <C> [BOLD] Syntactic  [BOLD] Avg <C> [BOLD] Semantic  [BOLD] Tense <C> [BOLD] Semantic  [BOLD] SubN <C> [BOLD] Semantic  [BOLD] ObjN <C> [BOLD] Semantic  [BOLD] SoMo <C> [BOLD] Semantic  [BOLD] CoIn <C> [BOLD] Semantic  [BOLD] Avg <R> <C> Base <C> 92.20 <C> 63.00 <C> 77.60 <C> 44.74 <C> 79.02 <C> 71.24 <C> 65.00 <C> 89.24 <C> 84.69 <C> 84.53 <C> 52.13 <C> 62.47 <C> 74.61 <R> <C> +  [ITALIC] Rel. Seq. PE <C> 89.82 <C> 63.17 <C> 76.50 <C> 45.09 <C> 78.45 <C> 71.40 <C> 64.98 <C> 88.74 <C> 87.00 <C> 85.53 <C> 51.68 <C> 62.21 <C> 75.03 <R> <C> +  [ITALIC] Stru. PE <C> 89.54 <C> 62.90 <C> 76.22 <C> 46.12 <C> 79.12 <C> 72.36 <C> 65.87 <C> 89.30 <C> 85.47 <C> 84.94 <C> 52.90 <C> 62.99 <C> 75.12 <CAP> Table 3: Performance on linguistic probing tasks. The probing tasks were conducted by evaluating linguistics embedded in the Transformer-Base encoder outputs. “Base”, “+ Rel. Seq. PE”, “+ Stru. PE” denote Transformer-Base, Transformer-Base with relative sequential PE, Transformer-Base with relative sequential PE and structural PE models respectively.
<R> <C> Model <C> Train <C> Validation <C> Test <R> <C> odd first <C> 39.925 <C> 45.377 <C> 45.196 <R> <C> rare first <C> 38.283 <C> 43.293 <C> 43.077 <R> <C> content first <C> 38.321 <C> 42.564 <C> 42.394 <R> <C> common first <C> 36.525 <C> 41.018 <C> 40.895 <R> <C> function first <C> 36.126 <C> 40.246 <C> 40.085 <R> <C> baseline <C> 38.668 <C> 41.888 <C> 41.721 <R> <C> enhanced baseline <C> 35.945 <C> 39.845 <C> 39.726 <CAP> Table 2: The perplexities achieved by the best version of each of our models.
<R> <C> Team <C> F-1 <R> <C> [BOLD] CMLA <C> [BOLD] 62.84 <R> <C> IIT-T <C> 56.99* <R> <C> TGB <C> 51.78 <R> <C> Baseline <C> 50.64 <CAP> Table 1: Results for the SE-16 Dutch aspect extraction task Pontiki et al. (2016), compared with our CMLA results that are highlighted in bold. * indicate unconstrained systems.
<R> <C> Models <C> R-1 <C> R-2 <C> R-L <R> <C> LEAD (See et al.,  2017 ) <C> 39.6 <C> 17.7 <C> 36.2 <R> <C> NeuralSum (Cheng & Lapata,  2016 ) <C> 35.5 <C> 14.7 <C> 32.2 <R> <C> SummaRuNNer (Nallapati et al.,  2017 ) <C> 39.6 <C> 16.2 <C> 35.3 <R> <C> REFRESH (Narayan et al.,  2018 ) <C> 40.0 <C> 18.2 <C> 36.6 <R> <C> Bottom-up Summarization (Gehrmann et al.,  2018 ) [Extractive] <C> 40.7 <C> 18.0 <C> 37.0 <R> <C> {\sc mask-}LMglobal ( [BOLD] Ours) <C> 41.2 <C> [BOLD] 19.1 <C> 37.6 <R> <C> LSTM+Elmopool <C> 41.0 <C> 18.9 <C> 37.4 <R> <C> LSTM+Skip-Thought <C> 40.8 <C> 18.7 <C> 37.2 <R> <C> NeuSum (Zhou et al.,  2018 ) <C> [BOLD] 41.6 <C> 19.0 <C> [BOLD] 38.0 <CAP> Table 3: Comparative evaluation of our summarization models with respect to other extractive summarization systems. ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L) F1 scores are reported. The first part of the table indicates system results with non-autoregressive sentence selection models, including prior work and our best proposed approach. The second part of the table shows the performance of (non-autoregressive) models using pre-trained sentence embeddings input to a document-level Bi-LSTM. NeuSum (Zhou et al., 2018) is included in the third part as it is specifically trained to score combinations of sentences with auto-regressive selection models.
<R> <C> Query <C> Groundtruth Reply <C> Candidate Replies <C> Human Score <C> Bleu-2 <C> Rouge <C> [ITALIC] sU <C> [ITALIC] sR <C> Ruber <R> <C> [EMPTY] <C> [EMPTY] <C> R1: 我也觉得很近 <C> 1.7778 <C> 0.0000 <C> 0.0000 <C> 1.8867 <C> 1.5290 <C> 1.7078 <R> <C> 貌似离得挺近的 <C> 你在哪里的嘞～ <C> I also think it’s near. <C> 1.7778 <C> 0.0000 <C> 0.0000 <C> 1.8867 <C> 1.5290 <C> 1.7078 <R> <C> It seems very near. <C> Where are you? <C> R2: 你哪的？ <C> 1.7778 <C> 0.0000 <C> 0.7722 <C> 1.1537 <C> 1.7769 <C> 1.4653 <R> <C> [EMPTY] <C> [EMPTY] <C> Where are you from? <C> 1.7778 <C> 0.0000 <C> 0.7722 <C> 1.1537 <C> 1.7769 <C> 1.4653 <CAP> Table 3: Case study. In the third column, R1 and R2 are obtained by the generative and retrieval systems, resp. Ruber here uses arithmetic mean. For comparison, we normalize all scores to the range of human annotation, i.e., [0,2]. Note that the normalization does not change the degree of correlation.
<R> <C> Sample <C> [BOLD] kNN <C> [BOLD] Bayes <C> [BOLD] C4.5 <C> [BOLD] SVM <R> <C> length <C> (%) <C> (%) <C> (%) <C> (%) <R> <C> 500 <C> 49.60 <C> 46.70 <C> 48.10 <C> 56.90 <R> <C> 1,000 <C> 53.33 <C> 53.30 <C> 57.10 <C> 68.09 <R> <C> 1,500 <C> 63.57 <C> 62.10 <C> 61.07 <C> 78.92 <R> <C> 2,000 <C> 72.50 <C> 63.50 <C> 63.50 <C> 78.50 <R> <C> 2,500 <C> 71.25 <C> 66.87 <C> 68.75 <C> 81.25 <R> <C> 3,000 <C> 68.57 <C> 67.14 <C> 59.28 <C> 83.57 <R> <C> 5,350 <C> 75.00 <C> 77.50 <C> 80.00 <C> 86.25 <R> <C> 7,130 <C> 81.60 <C> 73.30 <C> 81.60 <C> 86.67 <R> <C> 21,400 <C> 82.00 <C> 75.00 <C> 65.00 <C> 80.00 <CAP> Table 5: Accuracy rate for the authorship recognition task.
<R> <C> ↓ [ITALIC] trainblablablaeval→ <C> FQUAD <C> [ITALIC] PIAFv1.0 <R> <C> SQuAD-Fr <C> 78.39 <C> 68.90 <R> <C> FQUAD <C> 78.96 <C> 66.30 <R> <C> SQuAD-Fr + FQUAD <C> 81.09 <C> 71.11 <R> <C> SQuAD-Fr + FQUADsub <C> 79.12 <C> 69 <R> <C> SQuAD-Fr +  [ITALIC] PIAFv1.0 <C> 79.48 <C> - <R> <C> FQUAD +  [ITALIC] PIAFv1.0 <C> 79.64 <C> - <R> <C> [ITALIC] all <C> 82.01 <C> - <CAP> Table 4: F1 scores obtained after fine-tuning CamemBERT for QA on different training datasets. Models are evaluated on FQUAD (dev) and PIAFv1.0 (when applicable). all refers to the union (shuffled) of FQUAD (train), SQuAD-Fr (train), and PIAFv1.0.
<R> <C> [BOLD] Dataset <C> [BOLD] Model <C> [BOLD] R-1 <C> [BOLD] R-2 <C> [BOLD] R-L <R> <C> Newsroom-S <C> multi-task <C> 39.85 <C> 28.37 <C> 36.91 <R> <C> Newsroom-H <C> multi-task <C> 28.31 <C> 13.40 <C> 26.64 <R> <C> CNN/DM <C> transfer <C> 35.55 <C> 15.19 <C> 33.00 <R> <C> CNN/DM <C> +coverage <C> 38.49 <C> 16.78 <C> 35.68 <R> <C> Bytecup <C> transfer <C> 40.92 <C> 24.51 <C> 38.01 <CAP> Table 3: Performance of our model.
<R> <C> [BOLD] Method <C> [BOLD] WN11 <C> [BOLD] FB13 <C> [BOLD] Avg. <R> <C> NTN (Socher et al.,  2013 ) <C> 86.2 <C> 87.2 <C> 86.7 <R> <C> TransH (Wang et al.,  2014 ) <C> 78.8 <C> 83.3 <C> 81.1 <R> <C> TransR (Lin et al.,  2015 ) <C> 85.9 <C> 82.5 <C> 84.2 <R> <C> TransD (Ji et al.,  2015 ) <C> 86.4 <C> [BOLD] 89.1 <C> 87.8 <R> <C> TransR-FT (Feng et al.,  2016 ) <C> 86.6 <C> 82.9 <C> 84.8 <R> <C> TranSparse-S (Ji et al.,  2016 ) <C> 86.4 <C> 88.2 <C> 87.3 <R> <C> TranSparse-US (Ji et al.,  2016 ) <C> 86.8 <C> 87.5 <C> 87.2 <R> <C> ManifoldE (Xiao et al.,  2016a ) <C> 87.5 <C> 87.2 <C> 87.4 <R> <C> TransG (Xiao et al.,  2016b ) <C> 87.4 <C> 87.3 <C> 87.4 <R> <C> lppTransD (Yoon et al.,  2016 ) <C> 86.2 <C> 88.6 <C> 87.4 <R> <C> ConvKB (Nguyen et al.,  2018a ) <C> 87.6 <C> 88.8 <C> 88.2 <R> <C> TransE (Bordes et al.,  2013 ) (ours) <C> 89.2 <C> 88.1 <C> 88.7 <R> <C> Our R-MeN model <C> [BOLD] 90.5 <C> 88.9 <C> [BOLD] 89.7 <R> <C> TransE-NMM (Nguyen et al.,  2016 ) <C> 86.8 <C> 88.6 <C> 87.7 <R> <C> TEKE_H (Wang and Li,  2016 ) <C> 84.8 <C> 84.2 <C> 84.5 <R> <C> Bilinear-comp (Guu et al.,  2015 ) <C> 87.6 <C> 86.1 <C> 86.9 <R> <C> TransE-comp (Guu et al.,  2015 ) <C> 84.9 <C> 87.6 <C> 86.3 <CAP> Table 2: Accuracy results (in %) on the WN11 and FB13 test sets. The last 4 rows report accuracies of the models that use relation paths or incorporate with a large external corpus. The best score is in bold while the second best score is in underline. “Avg.” denotes the averaged accuracy over two datasets.
<R> <C> Hyperparameter <C> Value <R> <C> Dependency tag dimension <C> 256 <R> <C> Dependency arc dimension <C> 768 <R> <C> Optimizer <C> Adam <R> <C> [ITALIC] β1, [ITALIC] β2 <C> 0.9, 0.99 <R> <C> Weight decay <C> 0.01 <R> <C> Label Smoothing <C> 0.03 <R> <C> Dropout <C> 0.5 <R> <C> BERT dropout <C> 0.2 <R> <C> Mask probability <C> 0.2 <R> <C> Layer dropout <C> 0.1 <R> <C> Batch size <C> 32 <R> <C> Epochs <C> 80 <R> <C> Base learning rate <C> 1 [ITALIC] e−3 <R> <C> BERT learning rate <C> 5 [ITALIC] e−5 <R> <C> Learning rate warmup steps <C> 8000 <R> <C> Gradient clipping <C> 5.0 <CAP> Table 6: A summary of model hyperparameters.
<R> <C> [BOLD] Anchors <C> [ITALIC] F1 [ITALIC] Anch <C> [ITALIC] F1 [ITALIC] Unsup <R> <C> Jesus <C> 0.42 <C> 0.45 <R> <C> God <C> 0.49 <C> 0.43 <R> <C> Jesus,Christian <C> [BOLD] 0.55 <C> 0.45 <R> <C> Naive Bayes <C> 0.75 <C> 0.75 <CAP> Table 2: F1 scores on soc.religion.christianity.
<R> <C> [BOLD] Classifier <C> [BOLD] Macro-AUC <C> [BOLD] Macro-F1 <R> <C> Naive Bayes <C> 0.7120 <C> 0.4638 <R> <C> Anchored CorEx <C> 0.7445 <C> 0.5328 <CAP> Table 3: Classification performance on Obesity 2008.
<R> <C> [ITALIC] Target <C> [ITALIC] St. <C> [ITALIC] Precision <C> [ITALIC] Recall <C> [ITALIC] F1 <R> <C> Abortion <C> F <C> 0.44 <C> 0.35 <C> 0.39 <R> <C> Abortion <C> A <C> 0.73 <C> 0.85 <C> 0.79 <R> <C> Atheism <C> F <C> 0.34 <C> 0.28 <C> 0.31 <R> <C> Atheism <C> A <C> 0.79 <C> 0.86 <C> 0.82 <R> <C> Clinton <C> F <C> 0.42 <C> 0.22 <C> 0.29 <R> <C> Clinton <C> A <C> 0.64 <C> 0.90 <C> 0.75 <R> <C> Climate <C> F <C> 0.80 <C> 0.73 <C> 0.77 <R> <C> Climate <C> A <C> [EMPTY] <C> 0 <C> [EMPTY] <R> <C> Feminism <C> F <C> 0.24 <C> 0.64 <C> 0.35 <R> <C> Feminism <C> A <C> 0.72 <C> 0.43 <C> 0.54 <R> <C> [ITALIC] All <C> F <C> 0.44 <C> 0.35 <C> 0.39 <R> <C> [ITALIC] All <C> A <C> 0.73 <C> 0.85 <C> 0.79 <R> <C> [ITALIC] Macro-Avg <C> - <C> 0.59 <C> 0.64 <C> 0.61 <CAP> Table 3: Baseline performance (Naive Bayes classifiers, test data), St. - Stance
<R> <C> [ITALIC] Target ( [ITALIC] rank) <C> [ITALIC] St. <C> [ITALIC] P <C> [ITALIC] R <C> [ITALIC] F1 <R> <C> Abn. (1) <C> F <C> 0.54 <C> 0.67 <C> 0.60 <R> <C> Abn. (1) <C> A <C> 0.86 <C> 0.54 <C> 0.66 <R> <C> Ath. (12) <C> F <C> 0.33 <C> 0.25 <C> 0.29 <R> <C> Ath. (12) <C> A <C> 0.82 <C> 0.73 <C> 0.77 <R> <C> H. Cl (9) <C> F <C> 0.37 <C> 0.53 <C> 0.43 <R> <C> H. Cl (9) <C> A <C> 0.68 <C> 0.66 <C> 0.67 <R> <C> Cl. Ch. (12) <C> F <C> 0.80 <C> 0.82 <C> 0.81 <R> <C> Cl. Ch. (12) <C> A <C> [EMPTY] <C> 0 <C> [EMPTY] <R> <C> Fem. (8) <C> F <C> 0.37 <C> 0.40 <C> 0.38 <R> <C> Fem. (8) <C> A <C> 0.79 <C> 0.58 <C> 0.67 <R> <C> All (8) <C> F <C> 0.56 <C> 0.61 <C> 0.58 <R> <C> All (8) <C> A <C> 0.78 <C> 0.61 <C> 0.68 <R> <C> [ITALIC] Macro-Avg <C> - <C> 0.67 <C> 0.61 <C> 0.635 <CAP> Table 5: Results on test data, with rank out of the 19 teams.
<R> <C> [BOLD] Training <C> [BOLD] Retriever <C> [BOLD] Top-20 NQ <C> [BOLD] Top-20 TriviaQA <C> [BOLD] Top-20 WQ <C> [BOLD] Top-20 TREC <C> [BOLD] Top-20 SQuAD <C> [BOLD] Top-100 NQ <C> [BOLD] Top-100 TriviaQA <C> [BOLD] Top-100 WQ <C> [BOLD] Top-100 TREC <C> [BOLD] Top-100 SQuAD <R> <C> None <C> BM25 <C> 59.1 <C> 66.9 <C> 55.0 <C> 70.9 <C> 68.8 <C> 73.7 <C> 76.7 <C> 71.1 <C> 84.1 <C> 80.0 <R> <C> Single <C> DPR <C> 78.4 <C> 79.4 <C> 73.2 <C> 79.8 <C> 63.2 <C> 85.4 <C> [BOLD] 85.0 <C> 81.4 <C> 89.1 <C> 77.2 <R> <C> Single <C> BM25 + DPR <C> 76.6 <C> 79.8 <C> 71.0 <C> 85.2 <C> [BOLD] 71.5 <C> 83.8 <C> 84.5 <C> 80.5 <C> 92.7 <C> [BOLD] 81.3 <R> <C> Multi <C> DPR <C> [BOLD] 79.4 <C> 78.8 <C> [BOLD] 75.0 <C> [BOLD] 89.1 <C> 51.6 <C> [BOLD] 86.0 <C> 84.7 <C> [BOLD] 82.9 <C> 93.9 <C> 67.6 <R> <C> Multi <C> BM25 + DPR <C> 78.0 <C> [BOLD] 79.9 <C> 74.7 <C> 88.5 <C> 66.2 <C> 83.9 <C> 84.4 <C> 82.3 <C> [BOLD] 94.1 <C> 78.6 <CAP> Table 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20/100 retrieved passages that contain the answer. Single and Multi denote that our Dense Passage Retriever (DPR) was trained using individial or combined training datasets (all the datasets excluding SQuAD). See text for more details.
<R> <C> Training <C> Model <C> NQ <C> TriviaQA <C> WQ <C> TREC <C> SQuAD <R> <C> Single <C> BM25+BERT Lee et al. ( 2019 ) <C> 26.5 <C> 47.1 <C> 17.7 <C> 21.3 <C> 33.2 <R> <C> Single <C> ORQA Lee et al. ( 2019 ) <C> 33.3 <C> 45.0 <C> 36.4 <C> 30.1 <C> 20.2 <R> <C> Single <C> HardEM Min et al. ( 2019a ) <C> 28.1 <C> 50.9 <C> - <C> - <C> - <R> <C> Single <C> GraphRetriever Min et al. ( 2019b ) <C> 34.5 <C> 56.0 <C> 36.4 <C> - <C> - <R> <C> Single <C> PathRetriever Asai et al. ( 2020 ) <C> 32.6 <C> - <C> - <C> - <C> [BOLD] 56.5 <R> <C> Single <C> REALMWiki Guu et al. ( 2020 ) <C> 39.2 <C> - <C> 40.2 <C> 46.8 <C> - <R> <C> Single <C> REALMNews Guu et al. ( 2020 ) <C> 40.4 <C> - <C> 40.7 <C> 42.9 <C> - <R> <C> Single <C> BM25 <C> 32.6 <C> 52.4 <C> 29.9 <C> 24.9 <C> 38.1 <R> <C> Single <C> DPR <C> [BOLD] 41.5 <C> 56.8 <C> 34.6 <C> 25.9 <C> 29.8 <R> <C> Single <C> BM25+DPR <C> 39.0 <C> 57.0 <C> 35.2 <C> 28.0 <C> 36.7 <R> <C> Multi <C> DPR <C> [BOLD] 41.5 <C> 56.8 <C> [BOLD] 42.4 <C> 49.4 <C> 24.1 <R> <C> Multi <C> BM25+DPR <C> 38.8 <C> [BOLD] 57.9 <C> 41.1 <C> [BOLD] 50.6 <C> 35.8 <CAP> Table 5: End-to-end QA (Exact Match) Accuracy. REALMWiki and REALMNews are the same model but pretrained on Wikipedia and CC-News, respectively. Single and Multi denote that our Dense Passage Retriever (DPR) was trained using individual or combined training datasets (all except SQuAD). For WQ and TREC in the Multi setting, we finetune the reader trained on NQ.
<R> <C> POS Tag <C> noun <C> verb <C> adj <C> adv <R> <C> #synset <C> 10,360 <C> 2,240 <C> 2,419 <C> 442 <R> <C> #triplet <C> 210,127 <C> 20,657 <C> 23,490 <C> 4,952 <R> <C> avg. #triplet <C> 20.28 <C> 9.22 <C> 9.71 <C> 11.20 <CAP> Table 3: Numbers of POS tag-specific synsets and triplets and their average triplet numbers.
<R> <C> training data <C> [ITALIC] λ <C> dev Std <C> dev FJ <C> dev JS <C> dev JX <C> dev SC <C> dev GD <C> dev HN <C> dev Avg. <C> test Std <C> test FJ <C> test JS <C> test JX <C> test SC <C> test GD <C> test HN <C> test Avg. <R> <C> Std <C> - <C> 15.70 <C> 20.25 <C> 16.88 <C> 18.25 <C> 20.72 <C> 19.75 <C> 23.34 <C> 19.86 <C> 15.55 <C> 23.58 <C> 15.75 <C> 14.08 <C> 15.62 <C> 15.32 <C> 19.34 <C> 17.28 <R> <C> Std + (600hrs with trans) <C> - <C> 14.82 <C> 10.80 <C> 10.51 <C> 11.02 <C> 11.14 <C> 13.18 <C> 15.35 <C> 12.00 <C> 14.22 <C> 14.84 <C> 9.41 <C> 8.68 <C> 9.13 <C> 9.62 <C> 11.89 <C> 10.60 <R> <C> Std + (600hrs no trans) <C> 0.03 <C> 15.79 <C> 19.69 <C> 16.01 <C> 17.47 <C> 20.06 <C> 19.48 <C> 21.88 <C> 19.10 <C> 15.37 <C> 22.96 <C> 14.48 <C> 13.79 <C> 15.35 <C> 14.86 <C> 18.24 <C> 16.61 <CAP> Table 1: Character error rates (CER) of various trainings. The baseline system is trained on 360 hours of standard Mandarin (Std). There are 100 hours of training data from each accent. With no transcription available on the accented data, we show DAT is effective in learning features invariant to domain differences.
<R> <C> [EMPTY] <C> DC Normal <C> DC Comp. <C> DC Comp.* <C> IC Normal <C> IC Comp. <C> IC Comp.* <C> NER Normal <C> NER Comp. <C> NER Comp.* <C> All Normal <C> All Comp. <C> All Comp* <R> <C> Domain 1 <C> 27.2 <C> 2.2 <C> 0.9 <C> 42. <C> 1.3 <C> 0.4 <C> 14.0 <C> 1.5 <C> 1.2 <C> 83.2 <C> 5.0 <C> 2.5 <R> <C> Domain 2 <C> 25.8 <C> 2.1 <C> 0.9 <C> 65.4 <C> 1.4 <C> 0.5 <C> 86.9 <C> 7.6 <C> 5.5 <C> 178.1 <C> 11.1 <C> 6.9 <R> <C> Domain 3 <C> 13.1 <C> 1.1 <C> 0.4 <C> 0.6 <C> 0.1 <C> 0.002 <C> 1.0 <C> 0.2 <C> 0.1 <C> 14.7 <C> 1.4 <C> 0.5 <R> <C> Domain 4 <C> 10.9 <C> 0.9 <C> 0.4 <C> 9.5 <C> 0.6 <C> 0.2 <C> 2.5 <C> 0.4 <C> 0.3 <C> 22.9 <C> 1.9 <C> 0.9 <R> <C> Domain 5 <C> 25.3 <C> 2.0 <C> 0.9 <C> 25.7 <C> 1.3 <C> 0.5 <C> 84.7 <C> 7.4 <C> 5.6 <C> 135.7 <C> 10.7 <C> 7.0 <R> <C> Domain 6 <C> 51.5 <C> 4.1 <C> 1.7 <C> 64.8 <C> 3.7 <C> 1.3 <C> 16.4 <C> 1.9 <C> 1.6 <C> 132.6 <C> 9.7 <C> 4.6 <R> <C> Total <C> 153.7 <C> 12.4 <C> 5.2 <C> 208.0 <C> 8.4 <C> 2.9 <C> 205.5 <C> 19.0 <C> 14.3 <C> [BOLD] 567.2 <C> [BOLD] 39.8 <C> [BOLD] 22.4 <CAP> Table 2: NLU statistical models sizes in megabytes. Normal vs. compressed (ϵ=0.0001) vs. compressed* (ϵ=1)
<R> <C> Candidate hypernym <C> Frequency <R> <C> company <C> 5536 <R> <C> fruit <C> 3898 <R> <C> apple <C> 2119 <R> <C> vegetable <C> 928 <R> <C> orange <C> 797 <R> <C> tech company <C> 619 <R> <C> brand <C> 463 <R> <C> hardware company <C> 460 <R> <C> technology company <C> 427 <R> <C> food <C> 370 <CAP> Table 1. Candidate hypernyms for the term apple.
<R> <C> [EMPTY] <C> TAXI P <C> TAXI R <C> TAXI F1 <C> SubSeq P <C> SubSeq R <C> SubSeq F1 <R> <C> EN <C> 33.2 <C> 31.7 <C> 32.2 <C> [BOLD] 44.9 <C> [BOLD] 31.9 <C> [BOLD] 37.2 <R> <C> NL <C> [BOLD] 48.0 <C> 19.7 <C> 27.6 <C> 42.3 <C> [BOLD] 20.7 <C> [BOLD] 27.9 <R> <C> FR <C> 33.4 <C> 24.1 <C> 27.7 <C> [BOLD] 41.0 <C> [BOLD] 24.4 <C> [BOLD] 30.5 <R> <C> IT <C> [BOLD] 53.7 <C> 20.7 <C> 29.1 <C> 49.0 <C> [BOLD] 21.8 <C> [BOLD] 29.9 <CAP> Table 3. Precision (P), Recall (R) and F1 Metrics for TAXI vs. SubSeq across different languages. Results are aggregated over all domains per language.
<R> <C> [EMPTY] <C> [BOLD] UXTD <C> [BOLD] UXSSD <C> [BOLD] UPX <R> <C> Number of participants <C> 58 <C> 8 <C> 20 <R> <C> Female <C> 31 <C> 2 <C> 4 <R> <C> Male <C> 27 <C> 6 <C> 16 <R> <C> Mean age <C> 9y 3m <C> 7y 7m <C> 8y 4m <R> <C> SD age <C> 1y 10m <C> 1y 6m <C> 2y 2m <R> <C> Min age <C> 5y 8m <C> 5y 11m <C> 6y 1m <R> <C> Max age <C> 12y 10m <C> 10y 1m <C> 13y 4m <CAP> Table 1: The number of participants, their gender and ages. We report ages in years (y) and months (m). We recorded the ages of participants on their single visit in UXTD, and on their first baseline in UXSSD and UPX.
<R> <C> [BOLD] No. of cases <C> [BOLD] Class labels  [BOLD] ‘norisk’ <C> [BOLD] Class labels  [BOLD] ‘medium risk’ <C> [BOLD] Class labels  [BOLD] ‘high risk’ <R> <C> [BOLD] No. of cases <C> 946 <C> 43 <C> 199 <CAP> Table 1: Number of examples per category in our dataset
<R> <C> [EMPTY] <C> P <C> Disease R <C> F1 <C> P <C> Drug R <C> F1 <R> <C> DS_Struct <C> 0.300 <C> 0.300 <C> 0.300 <C> 0.232 <C> 0.072 <C> 0.110 <R> <C> DS_Target <C> 0.228 <C> 0.335 <C> 0.271 <C> 0.170 <C> 0.188 <C> 0.178 <R> <C> DS_Both <C> 0.233 <C> 0.353 <C> 0.281 <C> 0.154 <C> 0.175 <C> 0.164 <R> <C> DIEBOLDS <C> 0.143 <C> 0.372 <C> 0.209 <C> 0.050 <C> 0.435 <C> 0.090 <R> <C> MultiR* <C> 0.198 <C> 0.333 <C> 0.249 <C> 0.156 <C> 0.138 <C> 0.146 <R> <C> Mintz++* <C> 0.192 <C> 0.353 <C> 0.249 <C> 0.177 <C> 0.178 <C> 0.178 <R> <C> MIML-RE* <C> 0.211 <C> 0.360 <C> 0.266 <C> 0.167 <C> 0.160 <C> 0.163 <R> <C> DIEJOB_Target <C> 0.231 <C> 0.337 <C> 0.275 <C> 0.299 <C> 0.300 <C> 0.300 <R> <C> DIEJOB_Both <C> 0.317 <C> 0.333 <C> [BOLD] 0.324 <C> 0.327 <C> 0.288 <C> [BOLD] 0.306 <R> <C> DIEJOB_Target* <C> 0.235 <C> 0.339 <C> 0.277 <C> 0.289 <C> 0.425 <C> 0.344 <R> <C> DIEJOB_Both* <C> 0.317 <C> 0.333 <C> 0.324 <C> 0.282 <C> 0.422 <C> 0.338 <CAP> Table 1: Extraction results on the labeled pages.
<R> <C> [EMPTY] <C> [BOLD] A CTC <C> [BOLD] A+V CTC <C> [BOLD] A S2S <C> [BOLD] A+V S2S <R> <C> TER dev <C> 15.2 <C> 14.1 <C> 18.4 <C> 16.8 <R> <C> TER test <C> 13.6 <C> 13.1 <C> 16.3 <C> 15.7 <R> <C> PPL* dev <C> 113.6 <C> 80.6 <C> 1.38 <C> 1.37 <R> <C> PPL* test <C> 112.0 <C> 72.0 <C> 1.05 <C> 1.05 <CAP> Table 1: Results for Audio(A) and Audio-Visual(A+V) adaptation with the How-To data
<R> <C> # Semantic <C> Learner <C> PAN (Age) <C> PAN (Gender) <C> MBTI <C> BBC News <C> Drugs (effect) <C> Drugs (side) <R> <C> 0 <C> HILSTM <C> 0.422 <C> 0.752 <C> 0.407 <C> 0.833 <C> 0.443 <C> 0.514 <R> <C> 0 <C> SVM (Martinc et al.) <C> 0.417 <C> 0.814 <C> 0.682 <C> 0.983 <C> 0.468 <C> 0.503 <R> <C> 0 <C> SVM (generic) <C> 0.424 <C> 0.751 <C> 0.556 <C> 0.967 <C> 0.445 <C> 0.462 <R> <C> 256 (doc2vec) <C> SVM (Martinc et al.) <C> 0.422 <C> 0.817 <C> 0.675 <C> 0.979 <C> 0.416 <C> 0.523 <R> <C> 30 (tax2vec) <C> DNN <C> 0.400 <C> 0.511 <C> 0.182 <C> 0.353 <C> 0.400 <C> 0.321 <R> <C> 10 (tax2vec) <C> SVM (Martinc et al.) <C> 0.445 <C> 0.815 <C> 0.679 <C> 0.996 <C> 0.47 <C> 0.506 <R> <C> [EMPTY] <C> SVM (generic) <C> 0.502 <C> 0.781 <C> 0.556 <C> 0.972 <C> 0.445 <C> 0.469 <R> <C> 25 (tax2vec) <C> SVM (Martinc et al.) <C> 0.454 <C> 0.814 <C> 0.681 <C> 0.984 <C> 0.468 <C> 0.500 <R> <C> [EMPTY] <C> SVM (generic) <C> 0.484 <C> 0.755 <C> 0.554 <C> 0.967 <C> 0.449 <C> 0.466 <R> <C> 50 (tax2vec) <C> SVM (Martinc et al.) <C> 0.439 <C> 0.814 <C> 0.681 <C> 0.983 <C> 0.462 <C> 0.499 <R> <C> [EMPTY] <C> SVM (generic) <C> 0.444 <C> 0.751 <C> 0.554 <C> 0.963 <C> 0.446 <C> 0.463 <R> <C> 100 (tax2vec) <C> SVM (Martinc et al.) <C> 0.424 <C> 0.816 <C> 0.678 <C> 0.984 <C> 0.466 <C> 0.496 <R> <C> [EMPTY] <C> SVM (generic) <C> 0.422 <C> 0.749 <C> 0.551 <C> 0.958 <C> 0.443 <C> 0.46 <R> <C> 500 (tax2vec) <C> SVM (Martinc et al.) <C> 0.383 <C> 0.797 <C> 0.662 <C> 0.975 <C> 0.45 <C> 0.477 <R> <C> [EMPTY] <C> SVM (generic) <C> 0.400 <C> 0.724 <C> 0.532 <C> 0.909 <C> 0.424 <C> 0.438 <R> <C> 1000 (tax2vec) <C> SVM (Martinc et al.) <C> 0.368 <C> 0.783 <C> 0.647 <C> 0.964 <C> 0.436 <C> 0.466 <R> <C> [EMPTY] <C> SVM (generic) <C> 0.373 <C> 0.701 <C> 0.512 <C> 0.851 <C> 0.407 <C> 0.420 <CAP> Table 2: Effect of the added semantic features to classification performance, where all text segments (tweets/comments per user or segments per news article) are used. The best performing feature selection heuristic for the majority of top performing classifiers was “rarest terms” or “Closeness centrality”, indicating that only a handful of hypernyms carry added value, relevant for classification. Note that the results in the table correspond to the best performing combination of a classifier and a given heuristic.
<R> <C> [BOLD] Rule <C> [BOLD] Adversarial Rerun <C> [BOLD] Adversarial Inc. <C> [BOLD] Adversarial × <C> [BOLD] News Rerun <C> [BOLD] News Inc. <C> [BOLD] News × <C> [BOLD] Genomics Rerun <C> [BOLD] Genomics Inc. <C> [BOLD] Genomics × <C> [BOLD] Pharma. Rerun <C> [BOLD] Pharma. Inc. <C> [BOLD] Pharma. × <C> [BOLD] Paleontology Rerun <C> [BOLD] Paleontology Inc. <C> [BOLD] Paleontology × <R> <C> [BOLD] A1 <C> 1.0 <C> 0.03 <C> 33× <C> 2.2 <C> 0.02 <C> 112× <C> 0.3 <C> 0.01 <C> 30× <C> 3.6 <C> 0.11 <C> 33× <C> 2.8 <C> 0.3 <C> 10× <R> <C> [BOLD] FE1 <C> 1.1 <C> 0.2 <C> 7× <C> 2.7 <C> 0.3 <C> 10× <C> 0.4 <C> 0.07 <C> 6× <C> 3.8 <C> 0.3 <C> 12× <C> 3.0 <C> 0.4 <C> 7× <R> <C> [BOLD] FE2 <C> 1.2 <C> 0.2 <C> 6× <C> 3.0 <C> 0.3 <C> 10× <C> 0.4 <C> 0.07 <C> 6× <C> 4.2 <C> 0.3 <C> 12× <C> 3.3 <C> 0.4 <C> 8× <R> <C> [BOLD] I1 <C> 1.3 <C> 0.2 <C> 6× <C> 3.6 <C> 0.3 <C> 10× <C> 0.5 <C> 0.09 <C> 6× <C> 4.4 <C> 1.4 <C> 3× <C> 3.8 <C> 0.5 <C> 8× <R> <C> [BOLD] S1 <C> 1.3 <C> 0.2 <C> 6× <C> 3.6 <C> 0.4 <C> 8× <C> 0.6 <C> 0.1 <C> 6× <C> 4.7 <C> 1.7 <C> 3× <C> 4.0 <C> 0.5 <C> 7× <R> <C> [BOLD] S2 <C> 1.3 <C> 0.3 <C> 5× <C> 3.6 <C> 0.5 <C> 7× <C> 0.7 <C> 0.1 <C> 7× <C> 4.8 <C> 2.3 <C> 3× <C> 4.1 <C> 0.6 <C> 7× <CAP> Figure 9: End-to-end efficiency of incremental inference and learning. All execution times are in hours. The column × refers to the speedup of Incremental (Inc.) over Rerun.
<R> <C> [EMPTY] <C> [BOLD] ar <C> [BOLD] de <C> [BOLD] hi <C> [BOLD] ru <C> [BOLD] es <R> <C> Arabic <C> [BOLD] 0.59 <C> 0.56 <C> 0.62 <C> 0.59 <C> 0.61 <R> <C> Turkish <C> 0.5 <C> 0.58 <C> 0.65 <C> 0.57 <C> 0.57 <R> <C> German <C> 0.42 <C> [BOLD] 0.77 <C> 0.67 <C> 0.57 <C> 0.57 <R> <C> English <C> 0.47 <C> 0.61 <C> 0.67 <C> 0.61 <C> 0.63 <R> <C> Hindi <C> 0.51 <C> 0.51 <C> [BOLD] 0.75 <C> 0.64 <C> 0.62 <R> <C> Indo-Aryan <C> 0.48 <C> 0.54 <C> [BOLD] 0.74 <C> 0.56 <C> 0.58 <R> <C> Russian <C> 0.56 <C> 0.61 <C> 0.59 <C> [BOLD] 0.78 <C> 0.61 <R> <C> Slavic <C> 0.48 <C> 0.64 <C> 0.63 <C> [BOLD] 0.65 <C> 0.64 <R> <C> Spanish <C> 0.48 <C> 0.59 <C> 0.63 <C> 0.62 <C> [BOLD] 0.76 <R> <C> Romance <C> 0.51 <C> 0.64 <C> 0.66 <C> 0.62 <C> [BOLD] 0.68 <R> <C> Tonal <C> 0.40 <C> 0.49 <C> 0.62 <C> 0.65 <C> 0.46 <CAP> Table 2: Impact of worker’s language on Accuracies
<R> <C> [EMPTY] <C> Turns <C> Inform <C> Match <C> Success <R> <C> Rule <C> 5.25 <C> 94.00 <C> 100 <C> 100 <R> <C> FFN <C> 11.67 <C> 81.00 <C> 52.63 <C> 61.00 <R> <C> DQN <C> 18.79 <C> 28.50 <C> 11.07 <C> 11.85 <R> <C> PPO <C> 5.79 <C> 65.67 <C> 72.51 <C> 63.27 <R> <C> RE <C> 5.33 <C> 92.33 <C> 97.07 <C> 98.33 <R> <C> FLE <C> 6.81 <C> 89.67 <C> 94.12 <C> 91.67 <R> <C> RLE <C> 7.64 <C> 81.33 <C> 89.34 <C> 85.03 <R> <C> NLE <C> 7.20 <C> 84.67 <C> 85.31 <C> 86.83 <R> <C> FFN-ft <C> 9.62 <C> 83.00 <C> 90.79 <C> 76.00 <R> <C> FLE+R <C> 6.75 <C> [BOLD] 90.00 <C> [BOLD] 94.57 <C> 92.47 <R> <C> RLE+R <C> [BOLD] 6.38 <C> 88.67 <C> 90.62 <C> [BOLD] 92.93 <R> <C> NLE+R <C> 6.89 <C> 89.00 <C> 92.68 <C> 91.00 <CAP> Table 1: Evaluation results of baseline systems (top) as well as DQfD with rule-based and our weak expert approaches trained in-domain. The middle section denotes DQfD agents trained without RoFL; the bottom section shows results for agents trained with RoFL. Evaluation is conducted using an agenda-based user-simulator for 1000 dialogs. Reported scores are average number of Turns, Inform F1, Match Rate, and Success Rate. Best performing weak expert agents are in bold.
<R> <C> Model <C> EM <C> F1 <R> <C> RaSoR (base model) <C> 70.6 <C> 78.7 <R> <C> RaSoR + TR(MLP) <C> 72.5 <C> 79.9 <R> <C> RaSoR + TR <C> 75.0 <C> 82.5 <R> <C> RaSoR + TR + LM(emb) <C> 75.8 <C> 83.0 <R> <C> RaSoR + TR + LM(L1) <C> [BOLD] 77.0 <C> [BOLD] 84.0 <R> <C> RaSoR + TR + LM(L2) <C> 76.1 <C> 83.3 <CAP> Table 1: Results on SQuAD’s development set. The EM metric measures an exact-match between a predicted answer and a correct one and the F1 metric measures the overlap between their bag of words.
<R> <C> Model <C> AddSent <C> AddOneSent <R> <C> RaSoR + TR + LM(L1) [1] <C> 47.0 <C> 57.0 <R> <C> Mnemonic Reader [2] <C> 46.6 <C> 56.0 <R> <C> RaSoR + TR [3] <C> 44.5 <C> 53.9 <R> <C> MPCM [4] <C> 40.3 <C> 50.0 <R> <C> RaSoR (base model) [5] <C> 39.5 <C> 49.5 <R> <C> ReasoNet [6] <C> 39.4 <C> 50.3 <R> <C> jNet [7] <C> 37.9 <C> 47.0 <CAP> Table 3: x Single-model F1 on adversarial SQuAD. [1,3] This work. [2] Hu et al. (2017) [4] Wang et al. (2016) [5] Lee et al. (2016) [6] Shen et al. (2017) [7] Zhang et al. (2017)
<R> <C> dataset <C> model <C> Precision <C> Recall <C> F1 <R> <C> [EMPTY] <C> SpaceFusion <C> [BOLD] 1.22 <C> [BOLD] 0.66 <C> [BOLD] 0.86 <R> <C> Switchboard <C> CVAE+BOW <C> 0.76 <C> 0.57 <C> 0.65 <R> <C> [EMPTY] <C> MTask <C> 0.75 <C> 0.43 <C> 0.54 <R> <C> [EMPTY] <C> S2S+Sampling <C> 0.57 <C> 0.48 <C> 0.52 <R> <C> [EMPTY] <C> SpaceFusion <C> [BOLD] 0.40 <C> [BOLD] 0.26 <C> [BOLD] 0.31 <R> <C> Reddit <C> CVAE+BOW <C> 0.16 <C> 0.18 <C> 0.17 <R> <C> [EMPTY] <C> MTask <C> 0.31 <C> 0.18 <C> 0.23 <R> <C> [EMPTY] <C> S2S+Sampling <C> 0.10 <C> 0.11 <C> 0.11 <CAP> Table 4: Performance of each model on automatic measures. The highest score in each row is in bold for each dataset. Note that our BLEU scores are normalized to [0,100].
<R> <C> Method <C> Snap S2R P <C> Snap S2R R <C> Snap S2R F1 <C> Eclipse Platform P <C> Eclipse Platform R <C> Eclipse Platform F1 <C> FireFox P <C> FireFox R <C> FireFox F1 <C> Eclipse JDT P <C> Eclipse JDT R <C> Eclipse JDT F1 <R> <C> Logistic Regression <C> 0.67 <C> 0.63 <C> 0.65 <C> 0.71 <C> 0.88 <C> 0.78 <C> 0.92 <C> 0.95 <C> 0.94 <C> 0.76 <C> 0.88 <C> 0.81 <R> <C> Siamese CNN Severyn and Moschitti ( 2015 ) <C> 0.67 <C> 0.63 <C> 0.65 <C> 0.81 <C> 0.81 <C> 0.81 <C> 0.93 <C> 0.94 <C> 0.93 <C> 0.79 <C> 0.79 <C> 0.79 <R> <C> Siamese Bi-GRU <C> [BOLD] 0.76 <C> 0.61 <C> 0.68 <C> [BOLD] 0.86 <C> 0.84 <C> 0.85 <C> [BOLD] 0.95 <C> 0.93 <C> 0.94 <C> 0.85 <C> 0.84 <C> 0.84 <R> <C> Siamese Bi-GRU w Att <C> [BOLD] 0.76 <C> 0.62 <C> 0.68 <C> [BOLD] 0.86 <C> 0.86 <C> 0.86 <C> [BOLD] 0.95 <C> 0.94 <C> 0.94 <C> 0.84 <C> 0.84 <C> 0.84 <R> <C> DWEN Budhiraja et al. ( 2018a ) <C> 0.69 <C> 0.55 <C> 0.62 <C> 0.83 <C> 0.74 <C> 0.78 <C> 0.93 <C> 0.92 <C> 0.93 <C> 0.81 <C> 0.71 <C> 0.76 <R> <C> BiMPM Wang et al. ( 2017 ) <C> 0.73 <C> 0.65 <C> 0.69 <C> [BOLD] 0.86 <C> 0.82 <C> 0.84 <C> 0.94 <C> 0.92 <C> 0.93 <C> 0.85 <C> 0.79 <C> 0.82 <R> <C> Our Method <C> 0.73 <C> [BOLD] 0.67* <C> [BOLD] 0.70 <C> 0.84 <C> [BOLD] 0.91* <C> [BOLD] 0.87* <C> 0.94 <C> [BOLD] 0.96* <C> [BOLD] 0.95* <C> [BOLD] 0.86 <C> [BOLD] 0.90* <C> [BOLD] 0.88* <CAP> Table 2: Comparison of different methods for duplicate classification task on multiple datasets. * denotes statistical significance with the runner up for p-value<0.01
<R> <C> Group <C> Training set max. <C> Training set mean <C> Test set max. <C> Test set mean <C> rule <R> <C> 4 <C> 84.21 <C> 82.74 <C> 83.66 <C> 82.77 <C> 77.70 <R> <C> 5 <C> 24.67 <C> 21.62 <C> 26.46 <C> 21.31 <C> 16.16 <R> <C> 6 <C> 49.34 <C> 41.86 <C> 50.98 <C> 41.11 <C> 24.92 <R> <C> 7 <C> 67.09 <C> 66.00 <C> 61.27 <C> 58.62 <C> 60.44 <R> <C> 8 <C> 46.83 <C> 44.96 <C> 41.11 <C> 38.72 <C> 40.40 <R> <C> 9 <C> 48.08 <C> 45.09 <C> 44.56 <C> 39.89 <C> 36.83 <R> <C> 11 <C> 45.71 <C> 41.97 <C> 46.70 <C> 44.47 <C> 37.94 <R> <C> 12 <C> 76.16 <C> 73.13 <C> 72.38 <C> 69.41 <C> 66.50 <R> <C> 13 <C> 49.69 <C> 40.42 <C> 38.77 <C> 32.38 <C> 34.21 <CAP> Table 5: Results of the GP algorithm in terms of the maximum and mean accuracy of the best program. Training and test sets of questions are used to evaluate the accuracy of the best program in each of the 30 runs. The last column corresponds to the algebraic rule →d=→c−→a+→b.
<R> <C> Metrics <C> MLE <C> SeqGAN <C> RankGAN <C> LeakGAN <C> IRL <C> Ground Truth <R> <C> BLEUF-2 <C> 0.798 <C> 0.821 <C> 0.850∗ <C> [BOLD] 0.914 <C> 0.829 <C> 0.836 <R> <C> BLEUF-3 <C> 0.631 <C> 0.632 <C> 0.672∗ <C> [BOLD] 0.816 <C> 0.662 <C> 0.672 <R> <C> BLEUF-4 <C> 0.498 <C> 0.511 <C> 0.557∗ <C> [BOLD] 0.699 <C> 0.586 <C> 0.598 <R> <C> BLEUF-5 <C> 0.434 <C> 0.439 <C> 0.544∗ <C> [BOLD] 0.632 <C> 0.542 <C> 0.557 <R> <C> BLEUB-2 <C> 0.801 <C> 0.682 <C> - <C> 0.790 <C> [BOLD] 0.868 <C> 0.869 <R> <C> BLEUB-3 <C> 0.622 <C> 0.542 <C> - <C> 0.605 <C> [BOLD] 0.718 <C> 0.710 <R> <C> BLEUB-4 <C> 0.551 <C> 0.513 <C> - <C> 0.549 <C> [BOLD] 0.660 <C> 0.649 <R> <C> BLEUB-5 <C> 0.508 <C> 0.469 <C> - <C> 0.506 <C> [BOLD] 0.609 <C> 0.601 <R> <C> BLEUHA-2 <C> 0.799 <C> 0.745 <C> - <C> 0.847 <C> [BOLD] 0.848 <C> 0.852 <R> <C> BLEUHA-3 <C> 0.626 <C> 0.584 <C> - <C> [BOLD] 0.695 <C> 0.689 <C> 0.690 <R> <C> BLEUHA-4 <C> 0.523 <C> 0.512 <C> - <C> 0.615 <C> [BOLD] 0.621 <C> 0.622 <R> <C> BLEUHA-5 <C> 0.468 <C> 0.454 <C> - <C> 0.562 <C> [BOLD] 0.574 <C> 0.578 <CAP> Table 4: Results on COCO image caption dataset. Results of RankGAN with * are reported in [Guo et al.2017]. Results of MLE, SeqGAN and LeakGAN are based on their published implementations.
<R> <C> [BOLD] Description <C> [BOLD] Size <R> <C> User star ratings <C> 69,412 <R> <C> … with 5 stars <C> 40,064 <R> <C> Expert star ratings <C> 1,000 <R> <C> Expert judgments <C> 2,000 <R> <C> Query-title pairs <C> 164,065 <R> <C> … with  [ITALIC] recall=1 <C> 61,965 <R> <C> Title translations <C> 62,162 <CAP> Table 8: Data set sizes for collected feedback in number of sentences. The in-domain title translations are only used for simulation experiments.
<R> <C> [BOLD] Model <C> [BOLD] SMT <C> [BOLD] NMT (beam search) <C> [BOLD] NMT (greedy) <R> <C> EP BL <C> 25.27 <C> 27.55 <C> 26.32 <R> <C> NC BL <C> – <C> 22.35 <C> 19.63 <R> <C> MLE <C> 28.08 <C> 32.48 <C> 31.04 <R> <C> EL <C> – <C> 28.02 <C> 27.93 <R> <C> DPM <C> 26.24 <C> 27.54 <C> 26.36 <R> <C> DC <C> 26.33 <C> 28.20 <C> 27.39 <CAP> Table 11: BLEU results for simulation models evaluated on the News Commentary test set (nc-test2007) with beam search and greedy decoding. SMT results are from Lawrence et al. (2017b).
<R> <C> [BOLD] Methods <C> [BOLD] BLEU <C> [BOLD] Improvement <R> <C> Baseline (Pt-En) <C> 49.12 <C> [EMPTY] <R> <C> + Decoder Fusion (Pt-En) <C> 49.68 <C> [BOLD] +0.56 <R> <C> + Multimodal Attention (Pt-En) <C> 49.49 <C> [BOLD] +0.37 <R> <C> + VS Regularization (Pt-En) <C> 49.31 <C> [BOLD] +0.19 <CAP> Table 2: BLEU Score Comparison of the proposed methods
<R> <C> Train <C> GC <C> GM <C> EV <C> DP <C> Avg <R> <C> Test <C> GM <C> GC <C> DP <C> EV <C> [EMPTY] <R> <C> Swanson <C> 0.84 <C> 0.81 <C> 1 <C> [BOLD] 0.97 <C> 0.89 <R> <C> BERT-FT <C> 0.8 <C> 0.82 <C> 1.1 <C> 0.98 <C> 0.91 <R> <C> BERT-FTTOPIC <C> [BOLD] 0.65 <C> [BOLD] 0.71 <C> [BOLD] 0.96 <C> 1.01 <C> [BOLD] 0.81 <CAP> Table 5: Weighted-average RRSE on the 4 topic pairs of the SwanRank dataset (a lower score is better). Swanson row: the result by averaging the best train-test pairs cross-domain results published in swanson2015argument swanson2015argument.
<R> <C> [BOLD] Model <C> [ITALIC] r <C> [ITALIC] ρ <R> <C> [ITALIC] Unsupervised methods <C> [ITALIC] Unsupervised methods <C> [ITALIC] Unsupervised methods <R> <C> tf-idf <C> 46.77 <C> 42.95 <R> <C> Avg. GloVe embeddings <C> 32.40 <C> 34.00 <R> <C> InferSent - GloVe <C> 27.08 <C> 26.63 <R> <C> [ITALIC] 10-fold Cross-Validation <C> [ITALIC] 10-fold Cross-Validation <C> [ITALIC] 10-fold Cross-Validation <R> <C> SVR Misra et al. ( 2016 ) <C> 63.33 <C> - <R> <C> BERT-AFS-base <C> 77.20 <C> 74.84 <R> <C> SBERT-AFS-base <C> 76.57 <C> 74.13 <R> <C> BERT-AFS-large <C> 78.68 <C> 76.38 <R> <C> SBERT-AFS-large <C> 77.85 <C> 75.93 <R> <C> [ITALIC] Cross-Topic Evaluation <C> [ITALIC] Cross-Topic Evaluation <C> [ITALIC] Cross-Topic Evaluation <R> <C> BERT-AFS-base <C> 58.49 <C> 57.23 <R> <C> SBERT-AFS-base <C> 52.34 <C> 50.65 <R> <C> BERT-AFS-large <C> 62.02 <C> 60.34 <R> <C> SBERT-AFS-large <C> 53.82 <C> 53.10 <CAP> Table 3: Average Pearson correlation r and average Spearman’s rank correlation ρ on the Argument Facet Similarity (AFS) corpus Misra et al. (2016). Misra et al. proposes 10-fold cross-validation. We additionally evaluate in a cross-topic scenario: Methods are trained on two topics, and are evaluated on the third topic.
<R> <C> [BOLD] Model <C> [BOLD] Accuracy <R> <C> mean-vectors <C> 0.65 <R> <C> skip-thoughts-CS <C> 0.62 <R> <C> Dor et al. <C> 0.74 <R> <C> SBERT-WikiSec-base <C> 0.8042 <R> <C> SBERT-WikiSec-large <C> [BOLD] 0.8078 <R> <C> SRoBERTa-WikiSec-base <C> 0.7945 <R> <C> SRoBERTa-WikiSec-large <C> 0.7973 <CAP> Table 4: Evaluation on the Wikipedia section triplets dataset Dor et al. (2018). SBERT trained with triplet loss for one epoch.
<R> <C> [BOLD] Condition <C> [BOLD] DE <C> [BOLD] EN <C> [BOLD] FR <C> [BOLD] TR <R> <C> ML Baseline <C> 30.8 <C> 38.0 <C> 29.4 <C> 30.9 <R> <C> LFV app <C> 22.9 <C> 33.3 <C> 27.3 <C> 21.3 <R> <C> LFV mod <C> 20.7 <C> 32.7 <C> 25.4 <C> 19.6 <CAP> Table 1: TER of grapheme based system trained on 8h per language, 420 LSTM cells per layer
<R> <C> [BOLD] Condition <C> [BOLD] DE <C> [BOLD] EN <C> [BOLD] FR <C> [BOLD] TR <R> <C> Baseline <C> 10.6 <C> 18.2 <C> 15.9 <C> 9.1 <R> <C> LFV app <C> 9.5 <C> 16.1 <C> 14.3 <C> 8.1 <R> <C> LFV mod <C> 9.1 <C> 15.5 <C> 13.6 <C> 8.0 <CAP> Table 2: TER of grapheme based system trained on 45h per language, 420 LSTM cells per layer
<R> <C> Iteration <C> Bert Recall 1 <C> Bert Recall 2 <C> Bert Recall 3 <C> Bert Recall 4 <C> Bert Recall 5 <C> Bert Precision 1 <C> Bert Precision 2 <C> Bert Precision 3 <C> Bert Precision 4 <C> Bert Precision 5 <C> METEOR 1 <C> METEOR 2 <C> METEOR 3 <C> METEOR 4 <C> METEOR 5 <C> CIDEr 1 <C> CIDEr 2 <C> CIDEr 3 <C> CIDEr 4 <C> CIDEr 5 <R> <C> Dataset Size (%) <C> 10 <C> 15 <C> 20 <C> 25 <C> 30 <C> 10 <C> 15 <C> 20 <C> 25 <C> 30 <C> 10 <C> 15 <C> 20 <C> 25 <C> 30 <C> 10 <C> 15 <C> 20 <C> 25 <C> 30 <R> <C> Random <C> 66.94 <C> 67.61 <C> 68.02 <C> 68.39 <C> 68.64 <C> 65.28 <C> 65.86 <C> 66.22 <C> 66.54 <C> 66.89 <C> 12.96 <C> 13.57 <C> 14.00 <C> 14.37 <C> 14.68 <C> 87.01 <C> 91.75 <C> 94.90 <C> 97.63 <C> 100.09 <R> <C> Margin <C> 66.90 <C> 67.70 <C> 68.16 <C> 68.55 <C> 68.81 <C> 65.16 <C> 66.02 <C> 66.4 <C> 66.75 <C> 66.95 <C> 12.82 <C> 13.70 <C> 14.11 <C> 14.52 <C> 14.70 <C> 86.10 <C> 92.28 <C> 95.66 <C> 98.70 <C> 100.30 <R> <C> LC <C> 67.05 <C> 67.72 <C> 68.18 <C> 68.55 <C> 68.79 <C> 65.28 <C> 65.95 <C> 66.45 <C> 66.65 <C> 66.99 <C> 12.91 <C> 13.62 <C> 14.16 <C> 14.43 <C> 14.76 <C> 86.76 <C> 91.80 <C> 95.91 <C> 97.94 <C> 100.22 <R> <C> Entropy <C> 67.00 <C> 67.69 <C> 68.26 <C> 68.62 <C> 68.74 <C> 65.29 <C> 66.04 <C> 66.54 <C> 66.76 <C> 67.00 <C> 12.92 <C> 13.66 <C> 14.22 <C> 14.49 <C> 14.76 <C> 87.05 <C> 92.26 <C> 96.36 <C> 98.47 <C> 100.45 <R> <C> Random + VS <C> 69.57 <C> 70.35 <C> 70.72 <C> 71.10 <C> 71.35 <C> 66.39 <C> 67.24 <C> 67.81 <C> 68.09 <C> 68.41 <C> 14.52 <C> 15.34 <C> 15.91 <C> 16.23 <C> 16.55 <C> 101.68 <C> 107.76 <C> 111.88 <C> 114.27 <C> 116.72 <R> <C> Margin + VS <C> 69.56 <C> 70.36 <C> 70.75 <C> 71.08 <C> 71.42 <C> 66.29 <C> 67.24 <C> 67.64 <C> 68.11 <C> 68.49 <C> 14.43 <C> 15.40 <C> 15.86 <C> 16.30 <C> 16.70 <C> 100.73 <C> 108.20 <C> 111.61 <C> 115.14 <C> 117.97 <R> <C> LC + VS <C> 69.53 <C> 70.26 <C> 70.56 <C> 71.00 <C> 71.27 <C> 66.25 <C> 67.13 <C> 67.42 <C> 67.96 <C> 68.34 <C> 14.39 <C> 15.28 <C> 15.68 <C> 16.18 <C> 16.56 <C> 100.29 <C> 107.00 <C> 110.44 <C> 113.68 <C> 116.56 <R> <C> Entropy + VS <C> 69.47 <C> 70.31 <C> 70.68 <C> 71.07 <C> 71.37 <C> 66.17 <C> 67.22 <C> 67.67 <C> 68.10 <C> 68.38 <C> 14.31 <C> 15.33 <C> 15.83 <C> 16.27 <C> 16.56 <C> 100.26 <C> 108.02 <C> 111.49 <C> 114.66 <C> 116.98 <R> <C> Ours (Baye) <C> 67.15 <C> 67.76 <C> 68.14 <C> 68.46 <C> 68.73 <C> 65.47 <C> 65.99 <C> 66.35 <C> 66.70 <C> 67.05 <C> 13.04 <C> 13.67 <C> 14.03 <C> 14.42 <C> 14.75 <C> 87.92 <C> 92.24 <C> 95.33 <C> 97.71 <C> 100.15 <R> <C> Ours (Baye + Deno) <C> 67.13 <C> 67.86 <C> 68.38 <C> 68.68 <C> 68.98 <C> 65.49 <C> 66.17 <C> 66.63 <C> 66.93 <C> 67.22 <C> 13.05 <C> 13.83 <C> 14.31 <C> 14.67 <C> 14.96 <C> 87.59 <C> 93.01 <C> 96.81 <C> 99.34 <C> 101.44 <R> <C> Ours (Baye + VS) <C> 69.48 <C> 70.30 <C> 70.59 <C> 70.88 <C> 71.27 <C> 66.34 <C> 67.27 <C> 67.62 <C> 68.01 <C> 68.47 <C> 14.53 <C> 15.38 <C> 15.74 <C> 16.18 <C> 16.59 <C> 101.47 <C> 107.97 <C> 110.73 <C> 113.91 <C> 117.06 <R> <C> Ours (Baye + VS + Deno) <C> [BOLD] 69.74 <C> [BOLD] 70.51 <C> [BOLD] 70.99 <C> [BOLD] 71.35 <C> [BOLD] 71.63 <C> [BOLD] 66.54 <C> [BOLD] 67.44 <C> [BOLD] 68.01 <C> [BOLD] 68.34 <C> [BOLD] 68.70 <C> [BOLD] 14.66 <C> [BOLD] 15.53 <C> [BOLD] 16.12 <C> [BOLD] 16.47 <C> [BOLD] 16.91 <C> [BOLD] 102.19 <C> [BOLD] 108.87 <C> [BOLD] 112.80 <C> [BOLD] 115.67 <C> [BOLD] 118.67 <CAP> Table 2: We report our strategy’s performance and find that it outperforms all baselines. We also notice that the baselines improve considerably if we add visual-semantic information (+VS). (+Deno refers to the denoiser.
<R> <C> Dataset <C> Bag of Words <C> BaseMethods <C> BoW + BaseMethods <R> <C> english_dailabor <C> 68.4 <C> 67.1 <C> 72.4 <R> <C> aisopos_ntua <C> 72.3 <C> 62.0 <C> 69.9 <R> <C> tweet_semevaltest <C> 58.3 <C> 62.8 <C> 65.2 <R> <C> sentistrength_twitter <C> 58.8 <C> 59.1 <C> 61.2 <R> <C> sentistrength_youtube <C> 56.6 <C> 56.1 <C> 58.7 <R> <C> sanders <C> 61.5 <C> 54.1 <C> 56.4 <R> <C> sentistrength_myspace <C> 50.2 <C> 52.3 <C> 52.2 <R> <C> sentistrength_digg <C> 45.4 <C> 50.1 <C> 50.6 <R> <C> nikolaos_ted <C> 51.3 <C> 45.9 <C> 49.0 <R> <C> debate <C> 57.1 <C> 45.9 <C> 47.1 <R> <C> sentistrength_rw <C> 48.3 <C> 48.5 <C> 45.5 <R> <C> sentistrength_bbc <C> 34.8 <C> 45.5 <C> 43.8 <R> <C> vader_nyt <C> 28.0 <C> 38.9 <C> 39.2 <CAP> Table 3: Results of 10SENT using different set of features for Random Forest
<R> <C> [EMPTY] <C> 10Sent <C> Emoticons <C> 10Sent + Emoticons <R> <C> english_dailabor <C> 70.62 <C> 25.57 <C> 72.02 <R> <C> aisopos_ntua <C> 69.91 <C> 35.48 <C> 73.61 <R> <C> tweet_semevaltest <C> 64.78 <C> 18.06 <C> 65.13 <R> <C> sentistrength_twitter <C> 62.17 <C> 22.93 <C> 62.87 <R> <C> sentistrength_youtube <C> 57.06 <C> - <C> 59.36 <R> <C> sanders <C> 56.19 <C> 11.83 <C> 56.78 <R> <C> sentistrength_digg <C> 51.91 <C> - <C> 52.22 <R> <C> sentistrength_myspace <C> 50.22 <C> - <C> 53.20 <R> <C> nikolaos_ted <C> 47.97 <C> - <C> 48.97 <R> <C> debate <C> 47.18 <C> - <C> 47.37 <R> <C> sentistrength_rw <C> 47.15 <C> - <C> 45.25 <R> <C> sentistrength_bbc <C> 43.76 <C> - <C> 43.18 <R> <C> vader_nyt <C> 39.81 <C> - <C> 39.01 <CAP> Table 5: Macro-F1 results for experiments on 10SENT using Transfer Learning
<R> <C> [EMPTY] <C> Weights pattern.en <C> Weights sentiment140 <C> Weights emolex <C> Weights opinionfinder <C> Weights sentistrength <R> <C> Avg. Weight <C> 0.28 <C> 0.37 <C> 0.26 <C> 0.40 <C> 0.85 <R> <C> Std. Deviation <C> 0.25 <C> 0.28 <C> 0.29 <C> 0.31 <C> 0.28 <CAP> Table 7: Average and deviation for weights found during Exhaustive Weighted Vote step
<R> <C> [BOLD] Model <C> [BOLD] BookCorpus BLEU <C> [BOLD] BookCorpus Meteor <C> [BOLD] BookCorpus Length <C> [BOLD] BookCorpus Vocab <C> [BOLD] BookCorpus Trigrams <C> [BOLD] TripAdvisor BLEU <C> [BOLD] TripAdvisor Meteor <C> [BOLD] TripAdvisor Length <C> [BOLD] TripAdvisor Vocab % <C> [BOLD] TripAdvisor Trigrams <R> <C> L2W <C> [BOLD] 0.52 <C> [BOLD] 6.8 <C> [BOLD] 43.6 <C> [BOLD] 73.8 <C> 98.9 <C> 1.7 <C> 11.0 <C> 83.8 <C> [BOLD] 64.1 <C> [BOLD] 96.2 <R> <C> AdaptiveLM <C> [BOLD] 0.52 <C> 6.3 <C> 43.5 <C> 59.0 <C> 92.7 <C> [BOLD] 1.94 <C> [BOLD] 11.2 <C> [BOLD] 94.1 <C> 52.6 <C> 92.5 <R> <C> CacheLM <C> 0.33 <C> 4.6 <C> 37.9 <C> 31.0 <C> 44.9 <C> 1.36 <C> 7.2 <C> 52.1 <C> 39.2 <C> 57.0 <R> <C> Seq2Seq <C> 0.32 <C> 4.0 <C> 36.7 <C> 23.0 <C> 33.7 <C> 1.84 <C> 8.0 <C> 59.2 <C> 33.9 <C> 57.0 <R> <C> SeqGAN <C> 0.18 <C> 5.0 <C> 28.4 <C> 73.4 <C> [BOLD] 99.3 <C> 0.73 <C> 6.7 <C> 47.0 <C> 57.6 <C> 93.4 <R> <C> Reference <C> 100.0 <C> 100.0 <C> 65.9 <C> 73.3 <C> 99.7 <C> 100.0 <C> 100.0 <C> 92.8 <C> 69.4 <C> 99.4 <CAP> Table 1: Results for automatic evaluation metrics for all systems and domains, using the original continuation as the reference. The metrics are: Length - Average total length per example; Trigrams - % unique trigrams per example; Vocab - % unique words per example.
<R> <C> System <C> BUS <C> CAF <C> PED <C> STR <C> Avg. <R> <C> Unadapted <C> 43.47 <C> 45.93 <C> 30.43 <C> 36.13 <C> 38.96 <R> <C> Hard Label <C> 24.92 <C> 20.63 <C> 15.96 <C> 18.01 <C> 19.84 <R> <C> Soft T/S <C> 22.46 <C> 19.10 <C> 14.88 <C> 16.47 <C> 18.20 <R> <C> IT/S ( [ITALIC] λ=0.2) <C> 24.84 <C> 19.79 <C> 15.55 <C> 18.36 <C> 19.60 <R> <C> IT/S ( [ITALIC] λ=0.5) <C> 22.61 <C> 18.94 <C> 14.52 <C> 18.43 <C> 18.59 <R> <C> IT/S ( [ITALIC] λ=0.8) <C> 23.51 <C> 19.10 <C> 14.49 <C> 16.56 <C> 18.37 <R> <C> Conditional T/S <C> 20.72 <C> 17.46 <C> 12.52 <C> 15.09 <C> [BOLD] 16.42 <CAP> Table 1: The WER (%) performance of environment adaptation using one-hot hard label, T/S, interpolated T/S (IT/S) and conditional T/S learning on the real noisy test set of CHiME-3.
<R> <C> System <C> Supervised <C> Unsupervised <R> <C> SI <C> 13.95 <C> 13.95 <R> <C> Hard Label <C> 13.20 <C> 13.77 <R> <C> KLD ( [ITALIC] λ=0.2) <C> 12.61 <C> 13.65 <R> <C> KLD ( [ITALIC] λ=0.5) <C> 12.54 <C> 13.55 <R> <C> KLD ( [ITALIC] λ=0.8) <C> 13.17 <C> 13.72 <R> <C> Conditional T/S <C> [BOLD] 12.17 <C> [BOLD] 13.21 <CAP> Table 2: The WER (%) performance of speaker adaptation using one-hot hard label, KLD and conditional T/S learning on Microsoft SMD task. The SI LSTM model is trained with 2600 hours Microsoft live US English data.
<R> <C> Dataset <C> S. <C> TPR [ITALIC] a0 <C> TPR [ITALIC] b1 <C> ΔTPR [ITALIC] c <R> <C> ACE <C> No <C> 77.5 <C> 52.2 <C> 32.6 <R> <C> ACE <C> Yes <C> 76.1 <C> 45.0 <C> [BOLD] 40.9 <R> <C> ERE <C> No <C> 86.6 <C> 73.2 <C> 15.4 <R> <C> ERE <C> Yes <C> 87.3 <C> 70.6 <C> [BOLD] 19.1 <R> <C> CBT-NE <C> No <C> 76.3 <C> 30.2 <C> 60.4 <R> <C> CBT-NE <C> Yes <C> 74.5 <C> 28.5 <C> [BOLD] 61.8 <R> <C> CBT-CN <C> No <C> 39.0 <C> 16.6 <C> 57.4 <R> <C> CBT-CN <C> Yes <C> 38.9 <C> 15.4 <C> [BOLD] 60.4 <R> <C> [ITALIC] aTrue Positive Rate (before removal). <C> [ITALIC] aTrue Positive Rate (before removal). <C> [ITALIC] aTrue Positive Rate (before removal). <C> [ITALIC] aTrue Positive Rate (before removal). <C> [ITALIC] aTrue Positive Rate (before removal). <R> <C> [ITALIC] bTPR after removing the critical word(s). <C> [ITALIC] bTPR after removing the critical word(s). <C> [ITALIC] bTPR after removing the critical word(s). <C> [ITALIC] bTPR after removing the critical word(s). <C> [ITALIC] bTPR after removing the critical word(s). <R> <C> [ITALIC] cTPR change rate. <C> [ITALIC] cTPR change rate. <C> [ITALIC] cTPR change rate. <C> [ITALIC] cTPR change rate. <C> [ITALIC] cTPR change rate. <CAP> Table 4: True positive rate and true positive rate change of the trained models before and after removing the contributory word(s).
<R> <C> [BOLD] Chapters <C> [BOLD] 1 <C> [BOLD] 20 <C> [BOLD] 40 <C> [BOLD] 60 <C> [BOLD] 80 <R> <C> Density (×10−3) <C> 2.53 <C> 0.76 <C> 0.69 <C> 0.58 <C> 0.51 <R> <C> Max Degree <C> 8 <C> 42 <C> 61 <C> 81 <C> 112 <CAP> Table 3: Coherency Graph scalability with input size.
<R> <C> [BOLD] Method <C> [BOLD] Number of <C> [BOLD] Total <C> [BOLD] Average <C> [BOLD] Precision <C> [BOLD] Recall <C> [ITALIC] F [BOLD] -measure <R> <C> [EMPTY] <C> [BOLD] features <C> [BOLD] Runtime <C> [BOLD] Runtime <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] Content-Based <C> 29 <C> 0:52 <C> 0.02s <C> 78.59 <C> 83.61 <C> 81.02 <R> <C> [BOLD] Content-Based TF <C> 3 <C> 0:21 <C> 0.01s <C> 75.82 <C> 82.57 <C> 79.05 <R> <C> [BOLD] Graph-Based <C> 459 <C> 8:19:10 <C> 7.56s <C> 90.21 <C> 87.63 <C> 88.90 <R> <C> [BOLD] Graph-Based TF <C> 10 <C> 14:22 <C> 0.03s <C> 88.72 <C> 84.87 <C> 86.75 <R> <C> [BOLD] Early Fusion <C> 488 <C> 8:26:41 <C> 7.68s <C> 91.25 <C> 89.45 <C> 90.34 <R> <C> [BOLD] Early Fusion TF <C> 4 <C> 11:29 <C> 0.17s <C> 89.09 <C> 87.12 <C> 88.09 <R> <C> [BOLD] Late Fusion <C> 488 (2) <C> 8:23:57 <C> 7.64s <C> 94.10 <C> 92.43 <C> 93.26 <R> <C> [BOLD] Late Fusion TF <C> 13 <C> 15:42 <C> 0.24s <C> 91.64 <C> 89.97 <C> 90.80 <R> <C> [BOLD] Hybrid Fusion <C> 490 <C> 8:27:01 <C> 7.68s <C> 91.96 <C> 90.48 <C> 91.22 <R> <C> [BOLD] Hybrid Fusion TF <C> 4 <C> 16:57 <C> 0.26s <C> 90.74 <C> 89.00 <C> 89.86 <CAP> Table 1: Comparison of the performances obtained with the methods (Content-based, Graph-based, Fusion) and their subsets of Top Features (TF). The total runtime is expressed as h:min:s. See text for details.
<R> <C> [BOLD] Model <C> [BOLD] Dev <C> [BOLD] Test <R> <C> AM +  [ITALIC] n-gram LM <C> 0.5931 <C> 0.5859 <R> <C> L2RS(AM+ [ITALIC] n-gram LM) <C> 0.5531 <C> 0.5082 <R> <C> L2RS(AM+RNNLM) <C> 0.6400 <C> 0.6108 <R> <C> L2RS(AM+BERT-LM) <C> 0.5148 <C> 0.5360 <R> <C> L2RS(AM+TM-LM) <C> 0.5333 <C> 0.5340 <R> <C> L2RS(BERT-WE) <C> 0.7181 <C> 0.6713 <R> <C> L2RS(Topic-Vec) <C> 0.4864 <C> 0.4760 <R> <C> L2RS(opt) <C> [BOLD] 0.7430 <C> [BOLD] 0.7070 <CAP> Table 2: NDCG@10 of Different N-best Rescoring Methods
<R> <C> # <C> Model <C> [ITALIC] LBCE <C> [ITALIC] LKL <C> MAE <C> Details <R> <C> 1 <C> Full Model <C> [BOLD] 0.1094 <C> – <C> [BOLD] 0.4539 <C> No VAE <R> <C> 2 <C> Fixed Probability <C> 0.1295 <C> – <C> 1.4546 <C> Fixed Probability <R> <C> 3 <C> No Encoder <C> 0.1183 <C> – <C> 0.4934 <C> Encoder Ablation <R> <C> 4 <C> Only Acoustic <C> 0.1114 <C> – <C> 0.4627 <C> Encoder Ablation <R> <C> 5 <C> Only Linguistic <C> 0.1144 <C> – <C> 0.4817 <C> Encoder Ablation <R> <C> 6 <C> Only Acoustic <C> 0.1112 <C> – <C> 0.5053 <C> Inference Ablation <R> <C> 7 <C> Only Linguistic <C> 0.1167 <C> – <C> 0.4923 <C> Inference Ablation <R> <C> 8 <C> [ITALIC] wKL=0.0 <C> 0.1114 <C> 3.3879 <C> 0.4601 <C> Inclusion of VAE <R> <C> 9 <C> [ITALIC] wKL=10−4 <C> 0.1122 <C> 1.5057 <C> 0.4689 <C> Inclusion of VAE <R> <C> 10 <C> [ITALIC] wKL=10−3 <C> 0.1125 <C> 0.8015 <C> 0.4697 <C> Inclusion of VAE <R> <C> 11 <C> [ITALIC] wKL=10−2 <C> 0.1181 <C> 0.0000 <C> 0.5035 <C> Inclusion of VAE <R> <C> 12 <C> [ITALIC] wKL=10−1 <C> 0.1189 <C> [BOLD] 0.0000 <C> 0.5052 <C> Inclusion of VAE <CAP> Table 1: Experimental results on our test set. Lower is better in all cases. Best results shown in bold.
<R> <C> Group <C> Model <C> Appropriateness pre <C> Appropriateness post <C> Appropriateness t-test <C> Local grammar pre <C> Local grammar post <C> Local grammar t-test <C> Weighted sum pre <C> Weighted sum post <C> Weighted sum t-test <C> Global grammar pre <C> Global grammar post <C> Global grammar t-test <C> Structure pre <C> Structure post <C> Structure t-test <C> Meaning pre <C> Meaning post <C> Meaning t-test <R> <C> H <C> Vocabulary <C> [BOLD] 0.714 <C> 0.571 <C> 0.302 <C> [BOLD] 3.429 <C> 3.143 <C> 0.178 <C> [BOLD] 7.000 <C> 6.000 <C> 0.237 <C> [BOLD] 2.429 <C> 2.143 <C> 0.229 <C> 0.714 <C> [BOLD] 1.000 <C> 0.229 <C> 1.000 <C> 1.000 <C> 0.500 <R> <C> H <C> GMM <C> 0.444 <C> 0.444 <C> 0.500 <C> 2.444 <C> [BOLD] 3.000 <C> 0.123 <C> 4.667 <C> [BOLD] 5.222 <C> 0.347 <C> [BOLD] 1.667 <C> 1.556 <C> 0.364 <C> 0.667 <C> [BOLD] 1.222 <C> 0.025* <C> 0.333 <C> [BOLD] 1.111 <C> 0.004* <R> <C> H <C> BiLSTM <C> 0.273 <C> [BOLD] 0.545 <C> 0.041* <C> 2.364 <C> [BOLD] 3.273 <C> 0.008* <C> 3.727 <C> [BOLD] 6.000 <C> 0.011* <C> [BOLD] 1.545 <C> 1.364 <C> 0.276 <C> 0.818 <C> [BOLD] 1.182 <C> 0.052 <C> 0.545 <C> [BOLD] 0.909 <C> 0.052 <R> <C> L <C> Vocabulary <C> 0.182 <C> [BOLD] 0.364 <C> 0.170 <C> 2.182 <C> [BOLD] 2.909 <C> 0.098 <C> 3.091 <C> [BOLD] 4.727 <C> 0.056 <C> 0.818 <C> [BOLD] 1.091 <C> 0.247 <C> 0.364 <C> [BOLD] 1.000 <C> 0.013* <C> 0.455 <C> [BOLD] 0.636 <C> 0.220 <R> <C> L <C> GMM <C> 0.417 <C> [BOLD] 0.583 <C> 0.169 <C> 2.333 <C> [BOLD] 2.917 <C> 0.066 <C> 4.417 <C> [BOLD] 5.833 <C> 0.072 <C> 0.750 <C> [BOLD] 1.500 <C> 0.028* <C> 0.500 <C> [BOLD] 1.167 <C> 0.012* <C> 0.333 <C> [BOLD] 1.083 <C> 0.010* <R> <C> L <C> BiLSTM <C> 0.429 <C> [BOLD] 0.524 <C> 0.165 <C> 2.667 <C> [BOLD] 2.714 <C> 0.443 <C> 4.810 <C> [BOLD] 5.333 <C> 0.169 <C> 1.238 <C> [BOLD] 1.571 <C> 0.116 <C> 0.762 <C> [BOLD] 1.143 <C> 0.004* <C> 0.524 <C> [BOLD] 0.857 <C> 0.025* <CAP> Table 3: Result of translation experiment. The number of translated questions for each model ranges from 7 to 21, with the average number 11.8, depending on the number of early leave and absence we encountered in the experiment day. The pre- and post- numbers correspond to the average score for pre-test and post-test respectively and the t-test stars represent significance. The participants were separated into highly proficient (H) and less proficient (L) groups.
<R> <C> [EMPTY] <C> Overall <C> Count <C> Exist <C> Compare Numbers <C> Query Attributes <C> Compare Attributes <R> <C> ∘ early+batch+SFF <C> [BOLD] 95.5 <C> [BOLD] 91.0 <C> [BOLD] 98.5 <C> [BOLD] 84.7 <C> [BOLD] 98.4 <C> [BOLD] 98.7 <R> <C> ⋅ early+SFF <C> 94.4 <C> 88.6 <C> 97.7 <C> 82.9 <C> 98.0 <C> 97.6 <R> <C> ∘ late+batch+SFF <C> 58.0 <C> 51.9 <C> 71.4 <C> 72.2 <C> 54.4 <C> 55.8 <R> <C> ⋅ late+SFF <C> 56.3 <C> 51.0 <C> 72.6 <C> 71.3 <C> 50.9 <C> 55.0 <R> <C> SAN  <C> 68.5 <C> 52.2 <C> 71.1 <C> 73.5 <C> 85.3 <C> 52.3 <R> <C> early+batch+HAN+RN  <C> 98.8 <C> 97.2 <C> 99.6 <C> 96.9 <C> 99.6 <C> 99.6 <R> <C> late+batch+HAN+RN <C> 57.2 <C> 50.3 <C> 70.7 <C> 73.1 <C> 53.9 <C> 54.5 <CAP> Table 1: CLEVR. ∘ and ⋅ denote corresponding (early vs late) experiments.
<R> <C> 4 Model <C> [ITALIC] R2mod <C> 95% CI <R> <C> 4 Frequency <C> 19.5 <C> [18.5,20.7] <R> <C> F + Surp <C> 37.4 <C> [36.5,38.3] <R> <C> F + SemDis <C> 36.1 <C> [32.3,38.4] <R> <C> F + GLoVE <C> 35.0 <C> [31.8,38.2] <R> <C> F + ELMo <C> 35.2 <C> [34.3,36.2] <R> <C> F + S + SD <C> 46.6 <C> [43.5,49.7] <R> <C> F + S + SD + GloVe <C> 47.1 <C> [43.2,49.4] <R> <C> F + S + SD + ELMo <C> 49.5 <C> [48.9,50.1] <R> <C> 4 <C> [EMPTY] <C> [EMPTY] <CAP> Table 2: Proportion variance explained by each model (×100) and confidence interval across folds computed by a nonparametric bootstrap. F = frequency, S(urp) = surprisal, S(em)D(is) = semantic distance.
<R> <C> [EMPTY] <C> original <C> score <C> [ITALIC] f0 based <R> <C> original <C> 1.0000 <C> 0.1276 <C> 0.0700 <R> <C> score <C> 0.1276 <C> 1.0000 <C> -0.0240 <R> <C> [ITALIC] f0 based <C> 0.0700 <C> -0.0240 <C> 1.0000 <CAP> Table 1: The Pearson correlation coefficients resulted from pitch contours extracted from the synthesised singing voice by the proposed system (“score”) and the reference system (“f0 based”) compared with the pitch contours extracted from human singing voice
<R> <C> [EMPTY] <C> Seed <C> Data <C> Window <C> Algorithm <R> <C> Distance <C> 6.090 <C> 7.872 <C> 11.151 <C> 16.008 <R> <C> Relaxation <C> 0.39 <C> 0.23 <C> 0.00 <C> 0.00 <R> <C> Relaxation∗ <C> 0.99 <C> 0.97 <C> 0.22 <C> 0.00 <R> <C> Random init <C> 0.70 <C> 0.62 <C> 0.39 <C> 0.59 <R> <C> Convex init <C> 1.00 <C> 1.00 <C> 0.987 <C> 0.985 <CAP> Table 1: We report the accuracy of different methods for words with rank in the range 5,000-10,000. Relaxation indicates is the convex formulation applied to 2k random points, while relaxation∗ indicates the convex formulation applied to the 2k first vectors from each set.
<R> <C> [EMPTY] <C> 100 <C> 200 <C> 400 <C> 800 <C> 1600 <R> <C> Time <C> 1m47s <C> 2m07s <C> 2m54s <C> 5m34s <C> 22m13s <R> <C> en-es <C> 68.5 <C> 73.8 <C> 74.9 <C> 75.0 <C> 76.3 <R> <C> en-fr <C> 67.4 <C> 71.9 <C> 74.5 <C> 75.6 <C> 75.7 <R> <C> en-de <C> 59.1 <C> 63.0 <C> 64.4 <C> 65.8 <C> 66.4 <R> <C> en-ru <C> 23.7 <C> 27.9 <C> 29.9 <C> 32.3 <C> 33.2 <CAP> Table 4: Influence of the batch size: we report the precision at 1 after 4,000 iterations as a function of the batch size. We use the nearest neighbor (NN) approach to retrieve the translation of a given query.
<R> <C> [EMPTY] <C> [EMPTY] <C> Development RG <C> Development RG <C> Development CS <C> Development CS <C> Development CO <C> Development PPL <C> Development BLEU <R> <C> Beam <C> Model <C> P% <C> # <C> P% <C> R% <C> DLD% <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> Gold <C> 91.77 <C> 12.84 <C> 100 <C> 100 <C> 100 <C> 1.00 <C> 100 <R> <C> [EMPTY] <C> Template <C> 99.35 <C> 49.7 <C> 18.28 <C> 65.52 <C> 12.2 <C> [EMPTY] <C> 6.87 <R> <C> B=1 <C> Joint Copy <C> 47.55 <C> 7.53 <C> 20.53 <C> 22.49 <C> 8.28 <C> 7.46 <C> 10.41 <R> <C> B=1 <C> Joint Copy + Rec <C> 57.81 <C> 8.31 <C> 23.65 <C> 23.30 <C> 9.02 <C> 7.25 <C> 10.00 <R> <C> B=1 <C> Joint Copy + Rec + TVD <C> 60.69 <C> 8.95 <C> 23.63 <C> 24.10 <C> 8.84 <C> 7.22 <C> 12.78 <R> <C> B=1 <C> Conditional Copy <C> 68.94 <C> 9.09 <C> 25.15 <C> 22.94 <C> 9.00 <C> 7.44 <C> 13.31 <R> <C> B=5 <C> Joint Copy <C> 47.00 <C> 10.67 <C> 16.52 <C> 26.08 <C> 7.28 <C> 7.46 <C> 10.23 <R> <C> B=5 <C> Joint Copy + Rec <C> 62.11 <C> 10.90 <C> 21.36 <C> 26.26 <C> 9.07 <C> 7.25 <C> 10.85 <R> <C> B=5 <C> Joint Copy + Rec + TVD <C> 57.51 <C> 11.41 <C> 18.28 <C> 25.27 <C> 8.05 <C> 7.22 <C> 12.04 <R> <C> B=5 <C> Conditional Copy <C> 71.07 <C> 12.61 <C> 21.90 <C> 27.27 <C> 8.70 <C> 7.44 <C> 14.46 <R> <C> [EMPTY] <C> [EMPTY] <C> Test <C> Test <C> Test <C> Test <C> Test <C> Test <C> Test <R> <C> [EMPTY] <C> Template <C> 99.30 <C> 49.61 <C> 18.50 <C> 64.70 <C> 8.04 <C> [EMPTY] <C> 6.78 <R> <C> [EMPTY] <C> Joint Copy + Rec (B=5) <C> 61.23 <C> 11.02 <C> 21.56 <C> 26.45 <C> 9.06 <C> 7.47 <C> 10.88 <R> <C> [EMPTY] <C> Joint Copy + Rec + TVD (B=1) <C> 60.27 <C> 9.18 <C> 23.11 <C> 23.69 <C> 8.48 <C> 7.42 <C> 12.96 <R> <C> [EMPTY] <C> Conditional Copy (B=5) <C> 71.82 <C> 12.82 <C> 22.17 <C> 27.16 <C> 8.68 <C> 7.67 <C> 14.49 <CAP> Table 2: Performance of induced metrics on gold and system outputs of RotoWire development and test data. Columns indicate Record Generation (RG) precision and count, Content Selection (CS) precision and recall, Count Ordering (CO) in normalized Damerau-Levenshtein distance, perplexity, and BLEU. These first three metrics are described in Section 3.2. Models compare Joint and Conditional Copy also with addition Reconstruction loss and Total Variation Distance extensions (described in Section 4).
<R> <C> [EMPTY] <C> [EMPTY] <C> DUC-05  [ITALIC] ρ <C> DUC-05  [ITALIC] τ <C> DUC-05  [ITALIC] r <C> DUC-06  [ITALIC] ρ <C> DUC-06  [ITALIC] τ <C> DUC-06  [ITALIC] r <C> DUC-07  [ITALIC] ρ <C> DUC-07  [ITALIC] τ <C> DUC-07  [ITALIC] r <R> <C> \mathbcal  [ITALIC] Q [BOLD] 1 Grammaticality <C> best-rouge <C> 0.213 <C> 0.128 <C> 0.033 <C> -0.049 <C> -0.044 <C> 0.331 <C> 0.387 <C> 0.283 <C> 0.506 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 1 Grammaticality <C> gpt-2 <C> 0.678 <C> 0.511 <C> 0.637 <C> 0.391 <C> 0.280 <C> 0.593 <C> 0.780 <C> 0.586 <C> 0.675 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 1 Grammaticality <C> bert-fr-lm <C> 0.437 <C> 0.319 <C> 0.025 <C> 0.524 <C> 0.354 <C> 0.667 <C> 0.598 <C> 0.453 <C> 0.566 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 1 Grammaticality <C> bigru-att-s-1 <C> 0.119 <C> 0.079 <C> 0.116 <C> 0.263 <C> 0.182 <C> 0.459 <C> 0.119 <C> 0.085 <C> 0.494 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 1 Grammaticality <C> bigru-att-m-1 <C> 0.190 <C> 0.144 <C> 0.091 <C> 0.619 <C> 0.462 <C> 0.757 <C> 0.332 <C> 0.235 <C> 0.662 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 1 Grammaticality <C> bigru-att-m-5 <C> 0.156 <C> 0.160 <C> 0.040 <C> 0.613 <C> 0.466 <C> 0.771 <C> 0.315 <C> 0.215 <C> 0.584 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 1 Grammaticality <C> bert-ft-s-1 <C> 0.681 <C> 0.543 <C> [BOLD] 0.817 <C> [BOLD] 0.907 <C> [BOLD] 0.760 <C> [BOLD] 0.929 <C> 0.845 <C> 0.672 <C> [BOLD] 0.930 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 1 Grammaticality <C> bert-ft-m-1 <C> 0.675 <C> 0.543 <C> 0.805 <C> 0.889 <C> 0.749 <C> 0.902 <C> [BOLD] 0.851 <C> [BOLD] 0.684 <C> 0.896 <R> <C> [EMPTY] <C> bert-ft-m-5 <C> [BOLD] 0.712 <C> [BOLD] 0.564 <C> 0.802 <C> 0.883 <C> 0.732 <C> 0.925 <C> 0.840 <C> 0.680 <C> 0.902 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 2 Non redundancy <C> best-rouge <C> -0.121 <C> -0.081 <C> 0.064 <C> -0.401 <C> -0.301 <C> -0.408 <C> -0.299 <C> -0.222 <C> -0.486 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 2 Non redundancy <C> bigru-att-s-1 <C> -0.063 <C> -0.049 <C> -0.101 <C> 0.511 <C> 0.358 <C> 0.514 <C> 0.468 <C> 0.352 <C> 0.457 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 2 Non redundancy <C> bigru-att-m-1 <C> -0.197 <C> -0.143 <C> -0.094 <C> 0.478 <C> 0.478 <C> 0.524 <C> 0.478 <C> 0.340 <C> 0.565 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 2 Non redundancy <C> bigru-att-m-5 <C> -0.226 <C> -0.167 <C> -0.124 <C> 0.414 <C> 0.304 <C> 0.399 <C> 0.283 <C> 0.201 <C> 0.238 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 2 Non redundancy <C> bert-ft-s-1 <C> 0.330 <C> 0.232 <C> [BOLD] 0.499 <C> 0.677 <C> 0.517 <C> 0.679 <C> 0.756 <C> 0.576 <C> 0.689 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 2 Non redundancy <C> bert-ft-m-1 <C> 0.333 <C> 0.232 <C> 0.494 <C> [BOLD] 0.791 <C> [BOLD] 0.615 <C> [BOLD] 0.789 <C> [BOLD] 0.761 <C> [BOLD] 0.596 <C> [BOLD] 0.799 <R> <C> [EMPTY] <C> bert-ft-m-5 <C> [BOLD] 0.377 <C> [BOLD] 0.310 <C> 0.471 <C> 0.632 <C> 0.460 <C> 0.674 <C> 0.754 <C> 0.572 <C> 0.740 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 3 Referential clarity <C> best-rouge <C> 0.381 <C> 0.284 <C> 0.166 <C> 0.411 <C> 0.329 <C> 0.372 <C> 0.449 <C> 0.347 <C> 0.407 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 3 Referential clarity <C> bert-fr-ns <C> 0.185 <C> 0.130 <C> -0.138 <C> 0.462 <C> 0.315 <C> 0.494 <C> 0.478 <C> 0.322 <C> 0.085 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 3 Referential clarity <C> bigru-att-s-1 <C> 0.662 <C> 0.479 <C> 0.468 <C> 0.493 <C> 0.342 <C> 0.647 <C> 0.664 <C> 0.476 <C> 0.677 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 3 Referential clarity <C> bigru-att-m-1 <C> 0.702 <C> 0.540 <C> 0.492 <C> 0.527 <C> 0.396 <C> 0.681 <C> 0.732 <C> 0.533 <C> 0.681 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 3 Referential clarity <C> bigru-att-m-5 <C> 0.694 <C> 0.519 <C> 0.492 <C> 0.579 <C> 0.427 <C> 0.719 <C> 0.659 <C> 0.472 <C> 0.655 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 3 Referential clarity <C> bert-ft-s-1 <C> [BOLD] 0.913 <C> [BOLD] 0.759 <C> [BOLD] 0.796 <C> 0.872 <C> 0.732 <C> 0.901 <C> [BOLD] 0.934 <C> [BOLD] 0.796 <C> [BOLD] 0.936 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 3 Referential clarity <C> bert-ft-m-1 <C> 0.889 <C> 0.714 <C> 0.761 <C> [BOLD] 0.881 <C> [BOLD] 0.735 <C> 0.882 <C> 0.879 <C> 0.699 <C> 0.891 <R> <C> [EMPTY] <C> bert-ft-m-5 <C> 0.810 <C> 0.617 <C> 0.732 <C> 0.860 <C> 0.718 <C> [BOLD] 0.919 <C> 0.889 <C> 0.723 <C> 0.895 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 4 Focus <C> best-rouge <C> 0.440 <C> 0.373 <C> 0.270 <C> 0.440 <C> 0.331 <C> 0.475 <C> 0.495 <C> 0.360 <C> 0.563 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 4 Focus <C> bert-fr-ns <C> 0.458 <C> 0.337 <C> -0.106 <C> 0.522 <C> 0.354 <C> 0.508 <C> 0.547 <C> 0.364 <C> 0.089 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 4 Focus <C> bigru-att-s-1 <C> 0.150 <C> 0.110 <C> 0.153 <C> 0.355 <C> 0.242 <C> 0.644 <C> 0.433 <C> 0.321 <C> 0.533 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 4 Focus <C> bigru-att-m-1 <C> 0.199 <C> 0.118 <C> 0.194 <C> 0.366 <C> 0.259 <C> 0.653 <C> 0.533 <C> 0.372 <C> 0.553 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 4 Focus <C> bigru-att-m-5 <C> 0.154 <C> 0.097 <C> 0.160 <C> 0.493 <C> 0.371 <C> 0.691 <C> 0.645 <C> 0.462 <C> 0.657 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 4 Focus <C> bert-ft-s-1 <C> 0.645 <C> 0.471 <C> 0.578 <C> 0.814 <C> 0.636 <C> 0.853 <C> 0.873 <C> 0.704 <C> 0.902 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 4 Focus <C> bert-ft-m-1 <C> 0.664 <C> 0.491 <C> 0.642 <C> 0.776 <C> 0.608 <C> 0.842 <C> [BOLD] 0.893 <C> [BOLD] 0.745 <C> [BOLD] 0.905 <R> <C> [EMPTY] <C> bert-ft-m-5 <C> [BOLD] 0.791 <C> [BOLD] 0.621 <C> [BOLD] 0.739 <C> [BOLD] 0.875 <C> [BOLD] 0.710 <C> [BOLD] 0.911 <C> 0.818 <C> 0.636 <C> 0.867 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 5 Structure & Coherence <C> best-rouge <C> 0.391 <C> 0.300 <C> 0.039 <C> 0.080 <C> 0.056 <C> 0.023 <C> 0.370 <C> 0.292 <C> 0.293 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 5 Structure & Coherence <C> bert-fr-ns <C> 0.200 <C> 0.153 <C> -0.140 <C> 0.171 <C> 0.120 <C> 0.285 <C> 0.418 <C> 0.280 <C> 0.015 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 5 Structure & Coherence <C> bigru-att-s-1 <C> 0.223 <C> 0.153 <C> 0.040 <C> 0.458 <C> 0.326 <C> 0.526 <C> 0.606 <C> 0.442 <C> 0.534 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 5 Structure & Coherence <C> bigru-att-m-1 <C> 0.404 <C> 0.264 <C> 0.067 <C> 0.479 <C> 0.350 <C> 0.599 <C> 0.664 <C> 0.499 <C> 0.576 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 5 Structure & Coherence <C> bigru-att-m-5 <C> 0.244 <C> 0.157 <C> -0.113 <C> 0.435 <C> 0.296 <C> 0.540 <C> 0.522 <C> 0.389 <C> 0.506 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 5 Structure & Coherence <C> bert-ft-s-1 <C> 0.536 <C> 0.415 <C> 0.477 <C> 0.681 <C> 0.522 <C> 0.810 <C> 0.862 <C> 0.690 <C> [BOLD] 0.857 <R> <C> \mathbcal  [ITALIC] Q [BOLD] 5 Structure & Coherence <C> bert-ft-m-1 <C> 0.566 <C> 0.419 <C> 0.512 <C> 0.684 <C> 0.515 <C> 0.726 <C> 0.864 <C> 0.690 <C> 0.803 <R> <C> [EMPTY] <C> bert-ft-m-5 <C> [BOLD] 0.634 <C> [BOLD] 0.472 <C> [BOLD] 0.586 <C> [BOLD] 0.796 <C> [BOLD] 0.620 <C> [BOLD] 0.892 <C> [BOLD] 0.921 <C> [BOLD] 0.787 <C> 0.843 <CAP> Table 1: Spearman’s ρ, Kendall’s τ and Pearson’s r correlations on DUC-05, DUC-06 and DUC-07 for \mathbcalQ1–\mathbcalQ5. BEST-ROUGE refers to the version that achieved best correlations and is different across years.
<R> <C> Model <C> Strategy <C> PD Valid <C> PD Test <C> CFT Test-human <R> <C> AS Reader <C> - <C> 64.1 <C> 67.2 <C> 33.1 <R> <C> GA Reader <C> - <C> 64.1 <C> 65.2 <C> 35.7 <R> <C> CAS Reader <C> - <C> 65.2 <C> 68.1 <C> 35.0 <R> <C> CAW Reader <C> concat <C> 64.2 <C> 65.3 <C> 37.2 <R> <C> CAW Reader <C> sum <C> 65.0 <C> 68.1 <C> 38.7 <R> <C> CAW Reader <C> mul <C> [BOLD] 69.4 <C> [BOLD] 70.5 <C> [BOLD] 39.7 <CAP> Table 3: Accuracy on PD and CFT datasets. All the results except ours are from [7].
<R> <C> Model <C> CMRC-2017 Valid <C> CMRC-2017 Test <R> <C> Random Guess † <C> 1.65 <C> 1.67 <R> <C> Top Frequency † <C> 14.85 <C> 14.07 <R> <C> AS Reader † <C> 69.75 <C> 71.23 <R> <C> GA Reader <C> 72.90 <C> 74.10 <R> <C> SJTU BCMI-NLP † <C> 76.15 <C> 77.73 <R> <C> 6ESTATES PTE LTD † <C> 75.85 <C> 74.73 <R> <C> Xinktech † <C> 77.15 <C> 77.53 <R> <C> Ludong University † <C> 74.75 <C> 75.07 <R> <C> ECNU † <C> 77.95 <C> 77.40 <R> <C> WHU † <C> [BOLD] 78.20 <C> 76.53 <R> <C> CAW Reader (WE only) <C> 69.70 <C> 70.13 <R> <C> CAW Reader (concat) <C> 71.55 <C> 72.03 <R> <C> CAW Reader (sum) <C> 72.90 <C> 74.07 <R> <C> CAW Reader (mul) <C> 77.95 <C> [BOLD] 78.50 <CAP> Table 4: Accuracy on CMRC-2017 dataset. Results marked with † are from the latest official CMRC Leaderboard 555http://www.hfl-tek.com/cmrc2017/leaderboard.html. The best results are in bold face. WE is short for word embedding.
<R> <C> Model <C> CBT-NE Valid <C> CBT-NE Test <C> CBT-CN Valid <C> CBT-CN Test <R> <C> Human ‡ <C> - <C> 81.6 <C> - <C> 81.6 <R> <C> LSTMs ‡ <C> 51.2 <C> 41.8 <C> 62.6 <C> 56.0 <R> <C> MemNets ‡ <C> 70.4 <C> 66.6 <C> 64.2 <C> 63.0 <R> <C> AS Reader ‡ <C> 73.8 <C> 68.6 <C> 68.8 <C> 63.4 <R> <C> Iterative Attentive Reader ‡ <C> 75.2 <C> 68.2 <C> 72.1 <C> 69.2 <R> <C> EpiReader ‡ <C> 75.3 <C> 69.7 <C> 71.5 <C> 67.4 <R> <C> AoA Reader ‡ <C> 77.8 <C> 72.0 <C> 72.2 <C> 69.4 <R> <C> NSE ‡ <C> [BOLD] 78.2 <C> [BOLD] 73.2 <C> [BOLD] 74.3 <C> [BOLD] 71.9 <R> <C> GA Reader ‡ <C> 74.9 <C> 69.0 <C> 69.0 <C> 63.9 <R> <C> GA word char concat ‡ <C> 76.8 <C> 72.5 <C> 73.1 <C> 69.6 <R> <C> GA scalar gate ‡ <C> 78.1 <C> 72.6 <C> 72.4 <C> 69.1 <R> <C> GA fine-grained gate ‡ <C> 78.9 <C> 74.6 <C> 72.3 <C> 70.8 <R> <C> FG Reader ‡ <C> [BOLD] 79.1 <C> [BOLD] 75.0 <C> [BOLD] 75.3 <C> [BOLD] 72.0 <R> <C> CAW Reader <C> 78.4 <C> 74.9 <C> 74.8 <C> 71.5 <CAP> Table 5: Accuracy on CBT dataset. Results marked with ‡ are of previously published works [8, 7, 32].
<R> <C> Design dimensions #ℓ <C> Design dimensions # [ITALIC] h <C> Design dimensions # [ITALIC] a <C> Design dimensions #M <C> Dev Set Accuracy MNLI <C> Dev Set Accuracy MRPC <C> Dev Set Accuracy SST-2 <R> <C> 3 <C> 768 <C> 12 <C> 45 <C> 77.9 <C> 79.8 <C> 88.4 <R> <C> 6 <C> 768 <C> 3 <C> 55 <C> 80.6 <C> 82.2 <C> 90.7 <R> <C> 6 <C> 768 <C> 12 <C> 66 <C> 81.9 <C> 84.8 <C> 91.3 <R> <C> BERT base <C> BERT base <C> BERT base <C> BERT base <C> BERT base <C> BERT base <C> BERT base <R> <C> 12 <C> 768 <C> 12 <C> 108 <C> 84.4 <C> 86.7 <C> 92.9 <CAP> Table 1: Ablation study over BERT model size, Table 6 in Devlin et al. (2018). #M denotes number of model parameters in millions.
<R> <C> [EMPTY] <C> Successes <C> Failures <R> <C> 1st Epoch <C> 378 <C> 22 <R> <C> 2nd Epoch <C> 369 <C> 31 <CAP> Table 3: Success vs Failures over Test Set when following the learned policy π. The minor difference in the number of successes is because of imprecision in simulation.
<R> <C> [BOLD] Model <C> [BOLD] PPL↓ <C> [BOLD] PPL↓ <C> [BOLD] Diversity (Distinct) ↑ <C> [BOLD] Diversity (Distinct) ↑ <C> [BOLD] Diversity (Distinct) ↑ <C> [BOLD] Diversity (Distinct) ↑ <R> <C> [BOLD] Model <C> Val <C> Test <C> Ma-D-1 <C> Mi-D-1 <C> Ma-D-2 <C> Mi-D-2 <R> <C> SongNet <C> [BOLD] 12.75 <C> [BOLD] 14.73 <C> 75.96 <C> 2.69 <C> 97.59 <C> 37.26 <R> <C> SongNet-GRU <C> 16.52 <C> 20.49 <C> 74.73 <C> 1.77 <C> 98.30 <C> 28.98 <R> <C> SongNet w/o C <C> 13.51 <C> 15.38 <C> 75.42 <C> 2.48 <C> 97.36 <C> 34.85 <R> <C> SongNet w/o P <C> 14.16 <C> 17.16 <C> 73.73 <C> 2.56 <C> 97.52 <C> 34.82 <R> <C> SongNet w/ inverse-P <C> 13.40 <C> 15.13 <C> 74.95 <C> 2.54 <C> 97.76 <C> 35.65 <R> <C> SongNet w/o S <C> 13.23 <C> 15.44 <C> 75.38 <C> 2.74 <C> 97.31 <C> 37.50 <CAP> Table 4: Ablation analysis on SongCi
<R> <C> [BOLD] Model <C> [BOLD] Format↑ <C> [BOLD] Format↑ <C> [BOLD] Rhyme↑ <C> [BOLD] Rhyme↑ <C> [BOLD] Integrity↓ <R> <C> [BOLD] Model <C> Ma-F1 <C> Mi-F1 <C> Ma-F1 <C> Mi-F1 <C> [BOLD] Integrity↓ <R> <C> SongNet <C> 99.81 <C> 99.83 <C> 79.23 <C> 78.63 <C> 2.14±0.10 <R> <C> SongNet-GRU <C> 98.99 <C> 98.99 <C> 52.13 <C> 50.93 <C> 3.28±1.67 <R> <C> SongNet w/o C <C> 84.73 <C> 85.39 <C> 78.59 <C> 78.24 <C> 1.77±0.53 <R> <C> SongNet w/o P <C> 99.61 <C> 99.59 <C> 67.85 <C> 67.29 <C> 3.33±0.18 <R> <C> SongNet w/ inverse-P <C> 99.68 <C> 99.69 <C> 65.89 <C> 65.43 <C> 2.24±0.21 <R> <C> SongNet w/o S <C> 99.84 <C> 99.86 <C> 80.43 <C> 80.13 <C> 1.99±0.10 <CAP> Table 4: Ablation analysis on SongCi
<R> <C> [BOLD] Dataset  [BOLD] Method <C> [BOLD] Dataset Commercial vs. Non-commercial <C> [BOLD] Dataset Commercial vs. Non-commercial <C> [BOLD] Dataset Product Category Mapping <C> [BOLD] Dataset Product Category Mapping <R> <C> [EMPTY] <C> Macro-F1 <C> Micro-F1 <C> Macro-F1 <C> Micro-F1 <R> <C> tf*idf+SVM <C> 90.71 <C> 90.26 <C> 48.75 <C> 76.84 <R> <C> VDCNN (Conneau et al.,  2016 ) <C> 91.28 <C> 91.34 <C> 51.41 <C> 79.34 <R> <C> FastText (Bojanowski et al.,  2017 ) <C> 92.18 <C> 92.15 <C> 60.06 <C> 79.69 <R> <C> XML-CNN (Liu et al.,  2017 ) <C> 93.11 <C> 93.01 <C> 58.40 <C> 81.61 <R> <C> LEAM (Wang et al.,  2018a ) <C> 93.96 <C> 93.66 <C> 58.90 <C> 81.31 <R> <C> JointMap <C> 94.80  [BOLD] (+1.1%) <C> 94.63  [BOLD] (+1.0%) <C> 62.60  [BOLD] (+6.3%) <C> 83.01  [BOLD] (2.1%) <CAP> Table 2. Macro- and Micro- averaged F1 for different models. The improvements reported against LEAM.
<R> <C> Story <C> Intro-Agent <C> Extra-Agent <R> <C> Garden <C> 4.2 <C> 5.4 <R> <C> Pet <C> 4.7 <C> 5.0 <R> <C> Protest <C> 4.2 <C> 5.3 <R> <C> Storm <C> 3.7 <C> 5.7 <CAP> Table 2: Experiment results: participant evaluated extraversion scores (range from 1 - 7, with 1 being the most introverted and 7 being the most extraverted).
<R> <C> [BOLD] Systems <C> [BOLD] DUC 2006 R-1 <C> [BOLD] DUC 2006 R-2 <C> [BOLD] DUC 2006 R-SU4 <C> [BOLD] DUC 2007 R-1 <C> [BOLD] DUC 2007 R-2 <C> [BOLD] DUC 2007 R-SU4 <R> <C> Gold <C> 45.7 <C> 11.2 <C> 17.0 <C> 47.9 <C> 14.1 <C> 19.1 <R> <C> Oracle <C> 40.6 <C> 9.1 <C> 14.8 <C> 41.8 <C> 10.4 <C> 16.0 <R> <C> Lead <C> 32.1 <C> 5.3 <C> 10.4 <C> 33.4 <C> 6.5 <C> 11.3 <R> <C> Graph-based <C> Graph-based <C> Graph-based <C> Graph-based <C> Graph-based <C> Graph-based <C> Graph-based <R> <C> LexRank <C> 34.2 <C> 6.4 <C> 11.4 <C> 35.8 <C> 7.7 <C> 12.7 <R> <C> GRSum <C> 38.4∗ <C> 7.0∗ <C> 12.8∗ <C> 42.0 <C> 10.3 <C> 15.6 <R> <C> CTSum <C> — <C> — <C> — <C> 42.6 <C> 10.8 <C> 16.2 <R> <C> Autoencoder-based <C> Autoencoder-based <C> Autoencoder-based <C> Autoencoder-based <C> Autoencoder-based <C> Autoencoder-based <C> Autoencoder-based <R> <C> C-attention <C> 39.3 <C> 8.7 <C> 14.1 <C> 42.3 <C> 10.7 <C> 16.1 <R> <C> VaeSum <C> 39.6 <C> 8.9 <C> 14.3 <C> 42.1 <C> 11.0 <C> 16.4 <R> <C> Distantly supervised <C> Distantly supervised <C> Distantly supervised <C> Distantly supervised <C> Distantly supervised <C> Distantly supervised <C> Distantly supervised <R> <C> QuerySumS <C> 41.1 <C> 9.6 <C> 15.1 <C> 42.9 <C> 11.6 <C> 16.7 <R> <C> QuerySumP <C> 41.3 <C> 9.1 <C> 15.0 <C> 43.4 <C> 11.2 <C> 16.5 <R> <C> QuerySumS+P <C> 41.6 <C> 9.5 <C> 15.3 <C> 43.3 <C> 11.6 <C> 16.8 <CAP> Table 3: System performance on DUC 2006 and 2007. R-1, R-2 and R-SU4 stand for the F1 score of ROUGE 1, 2, and SU4, respectively. Results with ∗ were obtained based on our own implementation.
<R> <C> Category <C> Average size <R> <C> Capitals and countries <C> 5701 <R> <C> Family <C> 482 <R> <C> City in country <C> 2880 <R> <C> Animals <C> 1440 <R> <C> City with river <C> 701 <R> <C> Adjective to adverb <C> 873 <R> <C> Opposite adjective <C> 498 <R> <C> Comparative adjective <C> 866 <R> <C> Superlative adjective <C> 823 <R> <C> Verb to verbal noun <C> 415 <R> <C> Country to nationality <C> 924 <R> <C> Singular to plural <C> 1519 <R> <C> Genitive to dative <C> 1356 <R> <C> Present to past <C> 607 <R> <C> Present to other tense <C> 601 <CAP> Table 5: Average size in number of pairs for each category in the monolingual word analogy datasets.
<R> <C> Measurement <C> Method <C> Training ratio 10 % <C> Training ratio 30 % <C> Training ratio 50% <C> Training ratio 70% <R> <C> Micro-F1 <C> node2vec <C> 0.6641 <C> 0.6550 <C> 0.6688 <C> 0.6691 <R> <C> Micro-F1 <C> GCN <C> 0.7005 <C> 0.7093 <C> 0.7110 <C> 0.7180 <R> <C> Micro-F1 <C> RNN <C> 0.7686 <C> 0.7980 <C> 0.7978 <C> 0.8025 <R> <C> Micro-F1 <C> RNN-node2vec <C> 0.7940 <C> 0.8031 <C> 0.7933 <C> 0.8114 <R> <C> [EMPTY] <C> RNN-GCN <C> 0.7912 <C> 0.8230 <C> 0.8255 <C> 0.8284 <R> <C> [EMPTY] <C> LinkedRNN <C> 0.8146 <C> 0.8399 <C> 0.8463 <C> 0.8531 <R> <C> Macro-F1 <C> node2vec <C> 0.6514 <C> 0.6523 <C> 0.6513 <C> 0.6565 <R> <C> Macro-F1 <C> GCN <C> 0.6874 <C> 0.6992 <C> 0.7004 <C> 0.7095 <R> <C> [EMPTY] <C> RNN <C> 0.7452 <C> 0.7751 <C> 0.7754 <C> 0.7824 <R> <C> [EMPTY] <C> RNN+node2vec <C> 0.7734 <C> 0.7797 <C> 0.7702 <C> 0.7912 <R> <C> [EMPTY] <C> RNN+GCN <C> 0.7642 <C> 0.8014 <C> 0.8069 <C> 0.8104 <R> <C> [EMPTY] <C> LinkedRNN <C> 0.7970 <C> 0.8249 <C> 0.8331 <C> 0.8365 <CAP> Table 2. Performance Comparison in the DBLP dataset
<R> <C> Method <C> Training ratio 10 % <C> Training ratio 30 % <C> Training ratio 50% <C> 70% <R> <C> node2vec <C> 8.8702 <C> 8.8517 <C> 7.4744 <C> 7.0390 <R> <C> GCN <C> 8.9347 <C> 8.6830 <C> 6.7949 <C> 6.7278 <R> <C> RNN <C> 8.6600 <C> 8.6048 <C> 7.0466 <C> 6.8033 <R> <C> RNN-node2vec <C> 8.4653 <C> 8.5944 <C> 7.0173 <C> 6.7796 <R> <C> RNN-GCN <C> 8.6286 <C> 8.5662 <C> 6.9967 <C> 6.7945 <R> <C> LinkedRnn <C> 7.1822 <C> 6.3882 <C> 6.8416 <C> 6.3517 <CAP> Table 3. Performance Comparision in the BOOHEE dataset
<R> <C> Model <C> ppx <C> lowerbound <C> kl <C> unique(%) <C> zipf <R> <C> S2S; greedy <C> 46.26 <C> 69.20 <C> [EMPTY] <C> 2.65 <C> 1.14 <R> <C> S2S; sample <C> 46.26 <C> 69.20 <C> [EMPTY] <C> 96.73 <C> 1.07 <R> <C> LV-S2S,  [ITALIC] p( [ITALIC] ν) <C> 46.10 <C> 69.14 <C> 39.11 <C> 3.27 <C> 1.14 <R> <C> LV-S2S,  [ITALIC] p( [ITALIC] ν| [ITALIC] u) <C> 45.99 <C> [BOLD] 69.10 <C> [BOLD] 39.09 <C> 3.07 <C> 1.14 <R> <C> LV-S2S,  [ITALIC] p( [ITALIC] ν| [ITALIC] u), +A <C> 47.54 <C> 78.01 <C> 47.74 <C> 42.62 <C> 1.13 <R> <C> LTCM,  [ITALIC] p( [ITALIC] θ) <C> 95.19 <C> 91.18 <C> 55.47 <C> 50.34 <C> 1.11 <R> <C> LTCM,  [ITALIC] p( [ITALIC] θ| [ITALIC] u) <C> [BOLD] 45.24 <C> 89.17 <C> 59.29 <C> [BOLD] 54.08 <C> 1.11 <R> <C> LTCM,  [ITALIC] p( [ITALIC] θ| [ITALIC] u), +V <C> 45.47 <C> 85.89 <C> 55.97 <C> 48.83 <C> 1.12 <CAP> Table 1: Result of the corpus-based evaluation. p(ν) or p(θ) means the model samples from a gaussian prior, while p(ν|u) or p(θ|u) means the model samples from a Gaussian conditional distribution. +A indicates the model is trained with KL annealing, while +V means the model has a larger stop-word vocabulary (500).
<R> <C> [BOLD] Model <C> [BOLD] WMT14 En⇒De Speed <C> [BOLD] WMT14 En⇒De BLEU <C> [BOLD] WMT17 Zh⇒En Speed <C> [BOLD] WMT17 Zh⇒En BLEU <C> [BOLD] WAT17 Ja⇒En Speed <C> [BOLD] WAT17 Ja⇒En BLEU <R> <C> Transformer-Base <C> 1.28 <C> 27.31 <C> 1.21 <C> 24.13 <C> 1.33 <C> 28.10 <R> <C> + CSans <C> 1.22 <C> 28.18⇑ <C> 1.16 <C> 24.80⇑ <C> 1.28 <C> 28.50↑ <R> <C> Transformer-Big <C> 0.61 <C> 28.58 <C> 0.58 <C> 24.56 <C> 0.65 <C> 28.41 <R> <C> + CSans <C> 0.50 <C> 28.74 <C> 0.48 <C> 25.01↑ <C> 0.55 <C> 28.73↑ <CAP> Table 2: Experimental results on WMT14 En⇒De, WMT17 Zh⇒En and WAT17 Ja⇒En test sets. “Speed” denotes the training speed (steps/second). “↑/⇑” indicates statistically significant difference from the vanilla self-attention counterpart (p<0.05/0.01), tested by bootstrap resampling Koehn (2004).
<R> <C> [EMPTY] <C> Count <C> Part <R> <C> Persian text-dependent <C> 5 <C> 1 <R> <C> English text-dependent <C> 5 <C> 1 <R> <C> Persian months-name, 3-months <C> 1,320 <C> 2 <R> <C> Persian months-name, 12-months <C> 11,620 <C> 2 <R> <C> English digits, 4-digits <C> 5,040 <C> 2 <R> <C> English digits, 10-digits <C> 6,448 <C> 2 <R> <C> Persian name and family <C> 8,305 <C> 3 <R> <C> Persian transcribed phrases <C> 166,838 <C> 3 <CAP> Table 2: Numbers of unique phrases in each phrase type. “3-months/4-digits” means a random subset of months/digits used as test phrase. “Persian name and family” means a sequence of several Persian full names.
<R> <C> Baseline <C> TREC Robust(2004) MAP <C> TREC Robust(2004) % [ITALIC] Chg <C> TREC Web (2000-2001) MAP <C> TREC Web (2000-2001) % [ITALIC] Chg <R> <C> NL <C> 0.08925 <C> +15.25% *** <C> 0.15913 <C> +12.88% * <R> <C> Q <C> 0.09804 <C> +4.92% <C> 0.16543 <C> +8.58% <R> <C> Q bin <C> 0.08847 <C> +16.26% * <C> 0.17402 <C> +3.22% <R> <C> Random <C> 0.01808 <C> +468.91% *** <C> 0.04060 <C> +342.44% *** <R> <C> SMT <C> 0.06845 <C> +50.27% *** <C> 0.08891 <C> +102.04% *** <R> <C> RL <C> 0.08983 <C> +14.51% *** <C> 0.16474 <C> +9.04% <R> <C> [BOLD] SMT+RL <C> [BOLD] 0.10286 <C> [EMPTY] <C> [BOLD] 0.17963 <C> [EMPTY] <CAP> Table 2: Comparative effectiveness analysis of our approach. %Chg: improvement of SMT+RL over corresponding baselines. Paired t-test significance *: 0.01
<R> <C> Id <C> Experiment <C> Smatch <C> Unlabeled <C> No WSD <C> Named Entities <C> Wikification <C> Negations <C> Concepts <C> Rentrancies <C> SRL <R> <C> 0 <C> BO (JAMR) <C> 65.9 <C> 71 <C> 66 <C> 80 <C> 0 <C> 45 <C> 82 <C> 46 <C> 59 <R> <C> 1 <C> BO + Label (JAMR) <C> 67.0 <C> 72 <C> 68 <C> 81 <C> 79 <C> 46 <C> 82 <C> 48 <C> 64 <R> <C> 2 <C> BO + Label <C> 68.3 <C> 73 <C> 69 <C> 79 <C> 78 <C> 62 <C> 82 <C> 51 <C> 66 <R> <C> 3 <C> 2 + POS <C> 69.0 <C> 74 <C> 70 <C> 80 <C> 79 <C> 62 <C> 83 <C> 51 <C> 67 <R> <C> 4 <C> 3 + DEP <C> 69.4 <C> 75 <C> 70 <C> 81 <C> 79 <C> 65 <C> 83 <C> 52 <C> 67 <R> <C> 5 <C> 4 + NER <C> 69.8 <C> 75 <C> 70 <C> 83 <C> 79 <C> 62 <C> 83 <C> 52 <C> 67 <R> <C> 6 <C> 5 + Concepts <C> 70.9 <C> 76 <C> 71 <C> 83 <C> 79 <C> 66 <C> 84 <C> 54 <C> 69 <R> <C> 7 <C> 6 + BERT <C> 72.9 <C> 78 <C> 73 <C> 83 <C> 78 <C> 67 <C> 84 <C> 58 <C> [BOLD] 72 <R> <C> 8 <C> 1 + Attention <C> 69.8 <C> 75 <C> 70 <C> 80 <C> 78 <C> 63 <C> 83 <C> 53 <C> 68 <R> <C> 9 <C> 8 + POS <C> 70.4 <C> 75 <C> 71 <C> 80 <C> 79 <C> 64 <C> 83 <C> 53 <C> 68 <R> <C> 10 <C> 9 + DEP <C> 70.7 <C> 75 <C> 71 <C> 80 <C> 79 <C> 62 <C> 83 <C> 53 <C> 68 <R> <C> 11 <C> 10 + NER <C> 70.8 <C> 76 <C> 71 <C> 83 <C> 79 <C> 64 <C> 8 <C> 53 <C> 68 <R> <C> 12 <C> 11 + Concepts <C> 71.8 <C> 77 <C> 72 <C> 82 <C> 78 <C> 66 <C> 84 <C> 56 <C> 70 <R> <C> 13 <C> 12 + BERT1111footnotemark: 11 <C> 73.1 <C> 78 <C> 74 <C> 82 <C> 79 <C> 66 <C> 84 <C> 58 <C> [BOLD] 72 <R> <C> 14 <C> 13 + Smatch <C> 73.6 <C> 78 <C> 74 <C> 84 <C> 79 <C> 64 <C> 85 <C> 59 <C> [BOLD] 72 <R> <C> 15 <C> 8 + BERT <C> 73.4 <C> 78 <C> 74 <C> 83 <C> 79 <C> 64 <C> 84 <C> 57 <C> 71 <R> <C> 16 <C> 14 + RL <C> 75.5 <C> [BOLD] 80 <C> 76 <C> 83 <C> 80 <C> 67 <C> [BOLD] 86 <C> 56 <C> [BOLD] 72 <R> <C> [EMPTY] <C> zhang2019amr <C> [BOLD] 76.3 <C> 79 <C> [BOLD] 77 <C> 78 <C> [BOLD] 86 <C> [BOLD] 75 <C> 85 <C> [BOLD] 60 <C> 70 <R> <C> [EMPTY] <C> lyu2018amr <C> 74.4 <C> 77 <C> 76 <C> [BOLD] 86 <C> 76 <C> 58 <C> [BOLD] 86 <C> 52 <C> 70 <R> <C> [EMPTY] <C> NoordB17a <C> 71.0 <C> 74 <C> 72 <C> 79 <C> 65 <C> 62 <C> 82 <C> 52 <C> 66 <R> <C> [EMPTY] <C> D18-1198 <C> 69.8 <C> 74 <C> 72 <C> 78 <C> 71 <C> 57 <C> 84 <C> 49 <C> 64 <CAP> Table 1: Results, including comparison with the best systems, in the LDC2017T10 test set (aka AMR 2.0). Results highlighted in bold are the best in each metric. BO is Ballesteros and Al-Onaizan (2017) (which did not produce wikification). (JAMR) means that the model uses JAMR alignments, the rest use our alignments. Metrics by cai2013smatch and damonte2016incremental.
<R> <C> [BOLD] Model <C> [BOLD] Title-to-Abstract  [BOLD] Perplexity <C> [BOLD] Title-to-Abstract  [BOLD] METEOR <C> [BOLD] Abstract-to-Conclusion and Future Work  [BOLD] Perplexity <C> [BOLD] Abstract-to-Conclusion and Future Work  [BOLD] METEOR <C> [BOLD] Conclusion and Future Work-to-Title  [BOLD] Perplexity <C> [BOLD] Conclusion and Future Work-to-Title  [BOLD] METEOR <R> <C> Seq2seq Bahdanau et al. ( 2015 ) <C> 19.6 <C> 9.1 <C> 44.4 <C> 8.6 <C> 49.7 <C> 6.0 <R> <C> Editing Network (Wang et al.,  2018b ) <C> 18.8 <C> 9.2 <C> 30.5 <C> 8.7 <C> 55.7 <C> 5.5 <R> <C> Pointer Network See et al. ( 2017 ) <C> 146.7 <C> 8.5 <C> 74.0 <C> 8.1 <C> 47.1 <C> 6.6 <R> <C> Our Approach (-Repetition Removal) <C> 13.4 <C> 12.4 <C> 24.9 <C> [BOLD] 12.3 <C> 31.8 <C> 7.4 <R> <C> Our Approach <C> [BOLD] 11.5 <C> [BOLD] 13.0 <C> [BOLD] 18.3 <C> 11.2 <C> [BOLD] 14.8 <C> [BOLD] 8.9 <CAP> Table 3: Automatic Evaluation on Paper Writing for Diagnostic Tasks (%). The Pointer Network can be viewed as removing memory network part from our approach without repetition removal.
<R> <C> Model <C> Time (in hours) <R> <C> Huang et al <C> 168 <R> <C> MSSG 50d <C> 1 <R> <C> MSSG-300d <C> 6 <R> <C> NP-MSSG-50d <C> 1.83 <R> <C> NP-MSSG-300d <C> 5 <R> <C> Skip-gram-50d <C> 0.33 <R> <C> Skip-gram-300d <C> 1.5 <CAP> Table 1: Training Time Results. First five model reported in the table are capable of learning multiple embeddings for each word and Skip-gram is capable of learning only single embedding for each word.
<R> <C> Model <C> First-order dataset <C> Second-order dataset <C> QRPE dataset <R> <C> VQA-Bin (Baseline 1) <C> 0.6736 <C> 0.53 <C> 0.665 <R> <C> QPC-Sim (Baseline 2) <C> 0.7667 <C> 0.5595 <C> [BOLD] 0.7531 <R> <C> Logistic Regression <C> 0.6784 <C> 0.4293 <C> 0.5746 <R> <C> XGBoost <C> 0.8622 <C> 0.7466 <C> 0.6206 <R> <C> MLP <C> 0.8886 <C> 0.7507 <C> 0.6083 <R> <C> RelNet1 <C> [BOLD] 0.8889 <C> - <C> 0.6573 <R> <C> RelNet2 <C> 0.8825 <C> - <C> 0.6606 <R> <C> RelNet3 <C> 0.8878 <C> 0.7893 <C> 0.6564 <R> <C> RelNet4 <C> 0.8848 <C> [BOLD] 0.7945 <C> 0.6623 <CAP> Table 4. Results of true premise versus false premise question relevance detection using different models.
<R> <C> Model <C> Per-Thread <C> Grouped <R> <C> SVM Baseline <C> 73.0 <C> 70.0 <R> <C> Batched-CNN <C> 78.7 <C> 82.0 <R> <C> Separated-CNN <C> 79.8 <C> [BOLD] 83.0 <R> <C> Sequential-CNN-LSTM <C> [BOLD] 80.4 <C> 82.4 <CAP> Table 2: Accuracy obtained using different models
<R> <C> [EMPTY] <C> #T=2 <C> #T=3 <C> #T=4 <C> #T=5 <C> #T> 6 <R> <C> #Instances <C> 290 <C> 143 <C> 115 <C> 51 <C> 287 <R> <C> RoBERTa <C> 0.731 <C> 0.657 <C> 0.635 <C> 0.804 <C> 0.712 <R> <C> RoBERTa-MC <C> 0.681 <C> 0.622 <C> 0.609 <C> 0.725 <C> 0.750 <CAP> Table 5: Performance comparison (R@1) of different number of turns on the test set. #T denotes number of turns. #Instances is the number of instances
<R> <C> [EMPTY] <C> BERT-Un <C> TR.XL <C> BERT-ft <R> <C> RemoveDt <C> 47.4/17.2 <C> 39.7/9.1 <C> 80.9/62.0 <R> <C> RandomDt <C> 44.6/12.4 <C> 36.0/6.8 <C> 77.9/50.9 <R> <C> Unablated <C> 40.2/4.7 <C> 32.3/2.6 <C> 71.7/29.9 <CAP> Table 7: Test BA/PA with distractor ablations on test set. RemoveDt and RandomDt represent removing and sampling distractors respectively. BERT-Un and BERT-ft denotes pre-trained and finetuned BERT.
<R> <C> Diffculty <C> BA <C> PA <R> <C> Very Easy <C> 0.75 <C> 0.63 <R> <C> Easy <C> 0.78 <C> 0.45 <R> <C> Moderate <C> 0.71 <C> 0.29 <R> <C> Hard <C> 0.72 <C> 0.35 <R> <C> Very Hard <C> 0.68 <C> 0.20 <CAP> Table 8: BERT-ft performance in terms of human judgement of diffculty.
<R> <C> Training Strategy <C> PA <C> BA <C> DE <R> <C> [ITALIC] QA <C> 65.2 <C> 26.1 <C> 0.792 <R> <C> [ITALIC] QH <C> 71.7 <C> 29.9 <C> 0.661 <R> <C> [ITALIC] QA ;  [ITALIC] QH <C> 74.2 <C> 33.9 <C> [BOLD] 0.624 <R> <C> [ITALIC] QA +  [ITALIC] QH <C> [BOLD] 74.5 <C> [BOLD] 34.3 <C> 0.637 <CAP> Table 10: Test performance of models with QA and QH.
<R> <C> [EMPTY] <C> [BOLD] Joint <R> <C> Arabic <C> 0.630 <R> <C> English <C> 0.645 <R> <C> Spanish <C> 0.686 <R> <C> Portuguese <C> 0.792 <CAP> Table 6: Results (accuracy) for the joint prediction of Gender and Variety.
<R> <C> [EMPTY] <C> [BOLD] Gender DT <C> [BOLD] Gender MLP <C> [BOLD] Gender NB <C> [BOLD] Variety DT <C> [BOLD] Variety MLP <C> [BOLD] Variety NB <R> <C> Arabic <C> 0.619 <C> 0.619 <C> 0.699 <C> 0.685 <C> 0.813 <C> 0.729 <R> <C> English <C> 0.635 <C> 0.798 <C> 0.745 <C> 0.689 <C> 0.845 <C> 0.696 <R> <C> Spanish <C> 0.608 <C> 0.783 <C> 0.677 <C> 0.782 <C> 0.944 <C> 0.829 <R> <C> Portuguese <C> 0.714 <C> 0.813 <C> 0.676 <C> 0.955 <C> 0.986 <C> 0.983 <CAP> Table 7: Performances per classifier: DT: Decision Tree; MLP: Multi-Layer Perceptron, NB: Naive Bayes.
<R> <C> [BOLD] Task <C> [BOLD] System <C> [BOLD] Arabic <C> [BOLD] English <C> [BOLD] Portuguese <C> [BOLD] Spanish <C> [BOLD] Average <C> + 2nd <R> <C> Variety <C> N-GrAM <C> [BOLD] 0.8313 <C> 0.8988 <C> 0.9813 <C> 0.9621 <C> 0.9184 <C> 0.0013 <R> <C> [EMPTY] <C> LDR <C> 0.8250 <C> [BOLD] 0.8996 <C> [BOLD] 0.9875 <C> [BOLD] 0.9625 <C> [BOLD] 0.9187 <C> [EMPTY] <R> <C> Gender <C> N-GrAM <C> [BOLD] 0.8006 <C> [BOLD] 0.8233 <C> [BOLD] 0.8450 <C> [BOLD] 0.8321 <C> [BOLD] 0.8253 <C> 0.0029 <R> <C> [EMPTY] <C> LDR <C> 0.7044 <C> 0.7220 <C> 0.7863 <C> 0.7171 <C> 0.7325 <C> [EMPTY] <R> <C> Joint <C> N-GrAM <C> [BOLD] 0.6831 <C> [BOLD] 0.7429 <C> [BOLD] 0.8288 <C> [BOLD] 0.8036 <C> [BOLD] 0.7646 <C> 0.0101 <R> <C> [EMPTY] <C> LDR <C> 0.5888 <C> 0.6357 <C> 0.7763 <C> 0.6943 <C> 0.6738 <C> [EMPTY] <CAP> Table 8: Results (accuracy) on the test set for variety, gender and their joint prediction.
<R> <C> [EMPTY] <C> 168 speakers <C> 630 speakers <R> <C> TDGF  <C> 28.65 <C> / <R> <C> MCGF  <C> 4.70 <C> / <R> <C> VSCC  <C> 5.06 <C> 12.95 <R> <C> Using only the eigenresidual <C> 5.88 <C> 11.43 <R> <C> Using only the energy envelope <C> 8.76 <C> 17.14 <R> <C> Using both glottal signatures <C> [BOLD] 1.98 <C> [BOLD] 3.65 <CAP> TABLE V: Misidentification rate (%) on the TIMIT database obtained using state-of-the-art glottal approaches or the proposed DSM-based signatures.
<R> <C> Supervision <C> Approach <C> EN-IT \rightarrow <C> EN-IT \leftarrow <C> EN-DE \rightarrow <C> EN-DE \leftarrow <C> EN-FI \rightarrow <C> EN-FI \leftarrow <C> EN-ES \rightarrow <C> EN-ES \leftarrow <R> <C> Supervised Methods <C> Procrustes <C> 45.33 <C> 39.05 <C> 47.27 <C> 41.13 <C> 32.16 <C> 30.01 <C> 36.67 <C> 30.94 <R> <C> Supervised Methods <C> GPA\dagger <C> 45.33 <C> - <C> 48.46 <C> - <C> 31.39 <C> - <C> - <C> - <R> <C> Supervised Methods <C> GeoMM <C> 48.17 <C> 41.10 <C> 49.40 <C> 44.73 <C> 36.03 <C> 38.24 <C> 39.27 <C> 34.58 <R> <C> Supervised Methods <C> GeoMM{}_{semi} <C> [BOLD] 50.00 <C> [BOLD] 42.67 <C> 51.47 <C> 46.96 <C> [BOLD] 36.24 <C> 39.57 <C> 39.30 <C> 36.06 <R> <C> Unsupervised Methods <C> Adv-C-Procrustes <C> 45.40 <C> 38.78 <C> 46.40 <C> 00.00 <C> 25.21 <C> 00.15 <C> 35.47 <C> 0.05 <R> <C> Unsupervised Methods <C> Unsup-SL <C> 48.01 <C> 42.10 <C> 48.22 <C> 44.09 <C> 32.95 <C> 33.45 <C> 37.47 <C> 31.59 <R> <C> Unsupervised Methods <C> Sinkhorn-BT <C> 44.67 <C> 38.77 <C> 44.53 <C> 41.93 <C> 23.53 <C> 23.42 <C> 32.13 <C> 27.62 <R> <C> Unsupervised Methods <C> Ours-Procrustes <C> 45.60 <C> 38.29 <C> 46.58 <C> 42.50 <C> 28.08 <C> 26.48 <C> 35.20 <C> 28.94 <R> <C> Unsupervised Methods <C> Ours-GeoMM{}_{semi} <C> [BOLD] 50.00 <C> [BOLD] 42.67 <C> [BOLD] 51.60 <C> [BOLD] 47.22 <C> 35.88 <C> [BOLD] 39.62 <C> [BOLD] 39.47 <C> [BOLD] 36.43 <CAP> Table 4: Accuracy (P@1) on Vecmap. The best results are bolded. \daggerResults as reported in the original paper. For unsupervised methods, we report the average accuracy across 10 runs.
<R> <C> Model <C> Test Acc. <R> <C> UpDn  † <C> 40.4 <R> <C> + RUBi  <C> 44.23 <R> <C> + RUBi  + Semantic Loss (ours) <C> 47.5 ± 0.3 <CAP> Table 2: Complementary of gains: the semantic loss can be combined with SOTA methods decreasing language biases like RUBi [5], showing combined gains on VQAv2-CP [4]. The semantic space is Glove [24]. Baselines with † have been trained by us.
<R> <C> train/test ratio <C> Pre-training task <C> R@1 <C> R@5 <C> R@10 <C> R@50 <C> R@100 <R> <C> 1%/99% <C> BM-25 <C> 3.70 <C> 9.58 <C> 12.69 <C> 20.27 <C> 23.83 <R> <C> 1%/99% <C> ICT <C> [BOLD] 14.18 <C> 37.36 <C> 48.08 <C> 69.23 <C> 76.01 <R> <C> 1%/99% <C> ICT+BFS+WLP <C> 13.19 <C> [BOLD] 37.61 <C> [BOLD] 48.77 <C> [BOLD] 70.43 <C> [BOLD] 77.20 <R> <C> 5%/95% <C> BM-25 <C> 3.21 <C> 8.62 <C> 11.50 <C> 18.59 <C> 21.78 <R> <C> 5%/95% <C> ICT <C> [BOLD] 17.94 <C> 45.65 <C> 57.11 <C> 76.87 <C> 82.60 <R> <C> 5%/95% <C> ICT+BFS+WLP <C> 17.62 <C> [BOLD] 45.92 <C> [BOLD] 57.75 <C> [BOLD] 78.14 <C> [BOLD] 83.78 <R> <C> 80%/20% <C> BM-25 <C> 3.12 <C> 8.45 <C> 11.18 <C> 18.05 <C> 21.30 <R> <C> 80%/20% <C> ICT <C> 24.89 <C> 57.89 <C> 69.86 <C> 87.67 <C> 91.29 <R> <C> 80%/20% <C> ICT+BFS+WLP <C> [BOLD] 25.41 <C> [BOLD] 59.36 <C> [BOLD] 71.12 <C> [BOLD] 88.25 <C> [BOLD] 91.71 <CAP> Table 6: Open-domain retrieval results of Natural Questions dataset, where existing candidates are augmented with additional 1M retrieval candidates (i.e., 1M of (s,p) candidate pairs) extracted from open-domain Wikipedia articles.
<R> <C> [BOLD] Input <C> [BOLD] Vectoriz. <C> [BOLD] Econ. <C> [BOLD] Polit. <C> [BOLD] RCV1 <C> [BOLD] NYT <R> <C> Full-text <C> TF-IDF <C> 0.406 <C> 0.269 <C> 0.758 <C> 0.394 <R> <C> Full-text <C> BM25 <C> 0.370 <C> 0.230 <C> 0.740 <C> 0.370 <R> <C> Full-text <C> CF-IDF <C> 0.402 <C> 0.266 <C> 0.451 <C> 0.367 <R> <C> Full-text <C> BM25C <C> 0.296 <C> 0.161 <C> 0.423 <C> 0.236 <R> <C> Full-text <C> CTF-IDF <C> [BOLD] 0.411 <C> [BOLD] 0.272 <C> [BOLD] 0.761 <C> [BOLD] 0.406 <R> <C> Full-text <C> BM25CT <C> 0.377 <C> 0.231 <C> 0.742 <C> 0.379 <R> <C> Titles <C> TF-IDF <C> 0.351 <C> 0.201 <C> 0.709 <C> 0.238 <R> <C> Titles <C> BM25 <C> 0.349 <C> 0.196 <C> 0.687 <C> 0.230 <R> <C> Titles <C> CF-IDF <C> 0.303 <C> 0.183 <C> 0.275 <C> 0.105 <R> <C> Titles <C> BM25C <C> 0.304 <C> 0.172 <C> 0.193 <C> 0.073 <R> <C> Titles <C> CTF-IDF <C> [BOLD] 0.368 <C> [BOLD] 0.212 <C> [BOLD] 0.717 <C> [BOLD] 0.242 <R> <C> Titles <C> BM25CT <C> 0.364 <C> 0.208 <C> 0.693 <C> 0.239 <CAP> Table 2. Sample-averaged F-scores of the text vectorization methods with using kNN as common classifier
<R> <C> [BOLD] Input <C> [BOLD] Classifier <C> [BOLD] Econ. <C> [BOLD] Polit. <C> [BOLD] RCV1 <C> [BOLD] NYT <R> <C> Full-text <C> kNN ( [ITALIC] baseline) <C> 0.411 <C> 0.272 <C> 0.761 <C> 0.406 <R> <C> Full-text <C> Bayes (Bernoulli) <C> 0.318 <C> 0.191 <C> 0.657 <C> 0.281 <R> <C> Full-text <C> Bayes (Multinom.) <C> 0.235 <C> 0.207 <C> 0.703 <C> 0.349 <R> <C> Full-text <C> SVM <C> 0.481 <C> 0.319 <C> 0.852 <C> 0.554 <R> <C> Full-text <C> LR <C> 0.485 <C> 0.322 <C> 0.851 <C> 0.556 <R> <C> Full-text <C> L2R <C> 0.431 <C> 0.328 <C> 0.727 <C> 0.435 <R> <C> Full-text <C> MLP <C> [BOLD] 0.519 <C> [BOLD] 0.373 <C> [BOLD] 0.857 <C> 0.569 <R> <C> Full-text <C> RocchioDT <C> 0.291 <C> 0.225 <C> 0.645 <C> 0.393 <R> <C> Full-text <C> LRDT <C> 0.498 <C> 0.339 <C> 0.843 <C> 0.562 <R> <C> Full-text <C> L2RDT <C> 0.415 <C> 0.280 <C> 0.751 <C> 0.421 <R> <C> Full-text <C> MLPDT <C> 0.492 <C> 0.340 <C> [BOLD] 0.857 <C> [BOLD] 0.578 <R> <C> Titles <C> kNN <C> 0.368 <C> 0.212 <C> 0.717 <C> 0.242 <R> <C> Titles <C> Bayes (Bernoulli) <C> 0.301 <C> 0.179 <C> 0.708 <C> 0.233 <R> <C> Titles <C> Bayes (Multinom.) <C> 0.254 <C> 0.178 <C> 0.699 <C> 0.214 <R> <C> Titles <C> SVM <C> 0.426 <C> 0.272 <C> 0.804 <C> 0.325 <R> <C> Titles <C> LR <C> 0.429 <C> 0.274 <C> 0.803 <C> 0.326 <R> <C> Titles <C> L2R <C> 0.419 <C> 0.296 <C> 0.699 <C> 0.296 <R> <C> Titles <C> MLP <C> [BOLD] 0.472 <C> [BOLD] 0.309 <C> [BOLD] 0.812 <C> 0.332 <R> <C> Titles <C> RocchioDT <C> 0.335 <C> 0.219 <C> 0.584 <C> 0.252 <R> <C> Titles <C> LRDT <C> 0.451 <C> 0.279 <C> 0.796 <C> [BOLD] 0.353 <R> <C> Titles <C> L2RDT <C> 0.428 <C> 0.261 <C> 0.730 <C> 0.25 <R> <C> Titles <C> MLPDT <C> 0.457 <C> 0.277 <C> 0.808 <C> 0.340 <CAP> Table 3. Sample-averaged F-scores for classification methods with using the best vectorization method CTF-IDF
<R> <C> [EMPTY] <C> Encoder-Decoder clean <C> Encoder-Decoder noisy <C> Encoder-Decoder extreme <C> CTC clean <C> CTC noisy <C> CTC extreme <R> <C> Supervised baseline <C> Supervised baseline <C> Supervised baseline <C> Supervised baseline <C> Supervised baseline <C> Supervised baseline <C> Supervised baseline <R> <C> 1000h <C> 22.8 <C> 30.2 <C> 42.1 <C> 21.6 <C> 28.5 <C> 37.6 <R> <C> Weakly supervised models <C> Weakly supervised models <C> Weakly supervised models <C> Weakly supervised models <C> Weakly supervised models <C> Weakly supervised models <C> Weakly supervised models <R> <C> 2300h <C> 20.9 <C> 27.5 <C> 38.2 <C> 19.2 <C> 25.8 <C> 35.2 <R> <C> 12,800h <C> 18.6 <C> 25.5 <C> 34.8 <C> 18.7 <C> 25.2 <C> 34.2 <R> <C> 50,000h <C> 18.3 <C> 25 <C> 34.6 <C> 18.6 <C> [BOLD] 24.5 <C> 34.2 <R> <C> Weakly supervised filtered by relevance <C> Weakly supervised filtered by relevance <C> Weakly supervised filtered by relevance <C> Weakly supervised filtered by relevance <C> Weakly supervised filtered by relevance <C> Weakly supervised filtered by relevance <C> Weakly supervised filtered by relevance <R> <C> 2300h <C> 19.3 <C> 26.3 <C> 37 <C> 18.7 <C> 25.2 <C> 34.6 <R> <C> 12,800h <C> [BOLD] 17.6 <C> [BOLD] 24.3 <C> [BOLD] 33.7 <C> [BOLD] 18.2 <C> 24.8 <C> [BOLD] 33.7 <R> <C> Extra true labels instead of weak supervision <C> Extra true labels instead of weak supervision <C> Extra true labels instead of weak supervision <C> Extra true labels instead of weak supervision <C> Extra true labels instead of weak supervision <C> Extra true labels instead of weak supervision <C> Extra true labels instead of weak supervision <R> <C> 2000h <C> 18.8 <C> 25.8 <C> 35 <C> 18.7 <C> 25.1 <C> 34 <CAP> Table 1: WERs of the enc-dec and CTC fine-tuned models on the test sets clean, noisy and extreme for different train data sizes
<R> <C> Burn-in phase Mixing ratio <C> N 0 <C> N 0.3 <C> Y 0 <C> Y 0.3 <R> <C> 50,000h <C> 27.6 <C> 27.3 <C> 24.7 <C> 25.4 <R> <C> Filtered 12,800h <C> 28.7 <C> 26.1 <C> 24.4 <C> 25 <CAP> Table 2: WERs of fine-tuned CTC model on dev-noisy under different conditions of burn-in and mixing ratio
<R> <C> [BOLD] Foundation <C> [BOLD] 4th Coder <C> [BOLD] Our Model <R> <C> Care/Harm <C> 76.0 <C> 76.3 <R> <C> Fairness/Cheating <C> 76.6 <C> 72.3 <R> <C> Loyalty/Betrayal <C> 62.2 <C> 69.5 <R> <C> Authority/Subversion <C> 68.5 <C> 67.8 <R> <C> Purity/Degradation <C> 61.8 <C> 54.8 <R> <C> Non-moral <C> 77.9 <C> 69.2 <CAP> Table 5: A comparison of performance between human and our method (%, F-score).
<R> <C> Word dimension <C> Field dimension <C> Position dimension <C> Hidden size <C> Batch size <C> Learning rate <C> Optimizer <R> <C> 400 <C> 50 <C> 5 <C> 500 <C> 32 <C> 0.0005 <C> Adam <CAP> Table 2: Parameter settings of our experiments.
<R> <C> [EMPTY] <C> [BOLD] CoNLL’12 English development data MUC <C> [BOLD] CoNLL’12 English development data B3 <C> [BOLD] CoNLL’12 English development data CEAF [ITALIC] m <C> [BOLD] CoNLL’12 English development data CEAF [ITALIC] e <C> [BOLD] CoNLL’12 English development data Blanc <C> [BOLD] CoNLL’12 English development data CoNLL <C> [BOLD] CoNLL’12 English test data MUC <C> [BOLD] CoNLL’12 English test data B3 <C> [BOLD] CoNLL’12 English test data CEAF [ITALIC] m <C> [BOLD] CoNLL’12 English test data CEAF [ITALIC] e <C> [BOLD] CoNLL’12 English test data Blanc <C> [BOLD] CoNLL’12 English test data CoNLL <R> <C> MIR <C> 65.39 <C> 54.89 <C> – <C> 51.36 <C> – <C> 57.21 <C> 64.64 <C> 52.52 <C> – <C> 49.11 <C> – <C> 55.42 <R> <C> Stanford <C> 64.96 <C> 54.49 <C> 59.39 <C> 51.24 <C> 56.03 <C> 56.90 <C> 64.71 <C> 52.26 <C> 56.01 <C> 49.32 <C> 53.92 <C> 55.43 <R> <C> Multigraph <C> 66.22 <C> 56.41 <C> 60.87 <C> 52.61 <C> 58.15 <C> 58.41 <C> 65.41 <C> 54.38 <C> 58.60 <C> 50.21 <C> 56.03 <C> 56.67 <R> <C> [BOLD] Our Model <C> [BOLD] 67.89 <C> [BOLD] 57.83 <C> [BOLD] 62.11 <C> [BOLD] 53.76 <C> [BOLD] 60.58 <C> [BOLD] 59.83 <C> [BOLD] 67.69 <C> [BOLD] 55.86 <C> [BOLD] 59.66 <C> [BOLD] 51.75 <C> [BOLD] 57.78 <C> [BOLD] 58.44 <R> <C> IMS <C> 67.15 <C> 55.19 <C> 58.86 <C> 50.94 <C> 56.22 <C> 57.76 <C> 67.58 <C> 54.47 <C> 58.17 <C> 50.21 <C> 55.41 <C> 57.42 <R> <C> Latent-Tree <C> 69.46 <C> 57.83 <C> – <C> 54.43 <C> – <C> 60.57 <C> 70.51 <C> 57.58 <C> – <C> 53.86 <C> – <C> 60.65 <R> <C> Berkeley <C> 70.44 <C> 59.10 <C> – <C> 55.57 <C> – <C> 61.71 <C> 70.62 <C> 58.20 <C> – <C> 54.80 <C> – <C> 61.21 <R> <C> LaSO <C> 70.74 <C> 60.03 <C> 65.01 <C> 56.80 <C> – <C> 62.52 <C> 70.72 <C> 58.58 <C> 63.45 <C> 59.40 <C> – <C> 61.63 <R> <C> Latent-Strc <C> 72.11 <C> 60.74 <C> – <C> 57.72 <C> – <C> 63.52 <C> 72.17 <C> 59.58 <C> – <C> 55.67 <C> – <C> 62.47 <R> <C> Model-Stack <C> 72.59 <C> 61.98 <C> – <C> 57.58 <C> – <C> 64.05 <C> 72.59 <C> 60.44 <C> – <C> 56.02 <C> – <C> 63.02 <R> <C> Non-Linear <C> 72.74 <C> 61.77 <C> – <C> 58.63 <C> – <C> 64.38 <C> 72.60 <C> 60.52 <C> – <C> 57.05 <C> – <C> 63.39 <CAP> Table 3: F1 scores of different evaluation metrics for our model, together with two deterministic systems and one unsupervised system as baseline (above the dashed line) and seven supervised systems (below the dashed line) for comparison on CoNLL 2012 development and test datasets.
<R> <C> [BOLD] Method <C> [BOLD] Restaurant Precision <C> [BOLD] Restaurant Recall <C> [BOLD] Restaurant F1 <C> [BOLD] Laptop Precision <C> [BOLD] Laptop Recall <C> [BOLD] Laptop F1 <R> <C> CosSim <C> 0.5455 <C> 0.4782 <C> 0.4985 <C> 0.6055 <C> 0.5437 <C> 0.5083 <R> <C> ABAE <C> 0.5494 <C> 0.4904 <C> 0.5112 <C> 0.6127 <C> 0.6168 <C> 0.5950 <R> <C> MATE <C> 0.5613 <C> 0.5127 <C> 0.5177 <C> 0.6418 <C> 0.6550 <C> 0.6474 <R> <C> WeSTClass <C> 0.6153 <C> 0.5259 <C> 0.5461 <C> 0.6688 <C> 0.6848 <C> 0.6523 <R> <C> Dataless <C> 0.5225 <C> 0.4467 <C> 0.4265 <C> 0.5601 <C> 0.5693 <C> 0.5569 <R> <C> BERT <C> 0.5955 <C> 0.5285 <C> 0.5404 <C> 0.5949 <C> 0.5672 <C> 0.5632 <R> <C> Best+OurMisc <C> 0.5864 <C> 0.5373 <C> 0.5256 <C> 0.6724 <C> 0.6996 <C> 0.6685 <R> <C> ARYA <C> [BOLD] 0.7410 <C> [BOLD] 0.6913 <C> [BOLD] 0.7067 <C> [BOLD] 0.7849 <C> [BOLD] 0.7321 <C> [BOLD] 0.7447 <R> <C> ARYA-NoIter <C> 0.6934 <C> 0.6740 <C> 0.6749 <C> 0.7508 <C> 0.7037 <C> 0.7027 <R> <C> ARYA-NoTuning <C> 0.7019 <C> 0.6620 <C> 0.6729 <C> 0.7349 <C> 0.6874 <C> 0.6822 <R> <C> ARYA-NoFilter <C> 0.7145 <C> 0.6706 <C> 0.6836 <C> 0.7619 <C> 0.7158 <C> 0.7306 <CAP> Table 3: Evaluation Results on the Restaurant and Laptop Datasets. All precision, recall, and F1 scores are averaged in the macro-weighted manner. Underlines highlight the best compared models.
<R> <C> Feature extraction <C> # Learnable weights <R> <C> CNN <C> [BOLD] 2,153 <R> <C> Doc2Vec <C> 15,813 <R> <C> TFIDF <C> 243,813 <CAP> TABLE V: Quantity of learnable weights by approach
<R> <C> Filter <C> Tokens <C> cos( [ITALIC] θ) <R> <C> 1 <C> [ITALIC] final storage of docket <C> 0.46 <R> <C> 1 <C> [ITALIC] final remittance to origin <C> 0.45 <R> <C> 1 <C> [ITALIC] form registered in book <C> 0.42 <R> <C> 2 <C> [ITALIC] clerk <C> 0.51 <R> <C> 2 <C> [ITALIC] interlocutory appeal <C> 0.48 <R> <C> 2 <C> [ITALIC] são paulo clerk∗ <C> 0.48 <R> <C> 3 <C> [ITALIC] non-legal - name of a certain clerk* <C> 0.54 <R> <C> 3 <C> [ITALIC] non-legal - name of a certain clerk* <C> 0.51 <R> <C> 3 <C> [ITALIC] non-legal - name of a certain clerk* <C> 0.51 <R> <C> 4 <C> [ITALIC] temporarily stored docket <C> 0.47 <R> <C> 4 <C> [ITALIC] suspended <C> 0.42 <R> <C> 4 <C> [ITALIC] docket received from storage <C> 0.42 <R> <C> 5 <C> [ITALIC] originals <C> 0.56 <R> <C> 5 <C> [ITALIC] docket <C> 0.52 <R> <C> 5 <C> [ITALIC] submitted <C> 0.51 <R> <C> 6 <C> [ITALIC] Itaquaquecetuba County <C> 0.42 <R> <C> 6 <C> [ITALIC] wvpv∗ <C> 0.41 <R> <C> 6 <C> [ITALIC] original clerk <C> 0.4 <R> <C> 7 <C> [ITALIC] interlocutory appeal <C> 0.45 <R> <C> 7 <C> [ITALIC] non-legal∗ <C> 0.43 <R> <C> 7 <C> [ITALIC] non-legal - name of a certain clerk* <C> 0.42 <R> <C> 8 <C> [ITALIC] final remittance to origin <C> 0.45 <R> <C> 8 <C> [ITALIC] final storage of docket <C> 0.44 <R> <C> 8 <C> [ITALIC] remittance to origin <C> 0.39 <R> <C> 9 <C> [ITALIC] emitted <C> 0.47 <R> <C> 9 <C> [ITALIC] certificate <C> 0.43 <R> <C> 9 <C> [ITALIC] granted injunctions <C> 0.42 <R> <C> 10 <C> [ITALIC] small claims courts <C> 0.44 <R> <C> 10 <C> [ITALIC] defense entered <C> 0.41 <R> <C> 10 <C> [ITALIC] lack of standing from this point of view <C> 0.41 <R> <C> 11 <C> [ITALIC] temporarily stored docket <C> 0.55 <R> <C> 11 <C> [ITALIC] docket remain in clerk <C> 0.5 <R> <C> 11 <C> [ITALIC] return after granted period <C> 0.45 <R> <C> 12 <C> [ITALIC] final storage central storage <C> 0.51 <R> <C> 12 <C> [ITALIC] final storage of docket <C> 0.51 <R> <C> 12 <C> [ITALIC] non-reactivated proceeding <C> 0.51 <CAP> TABLE VII: Similarity between most similar tokens and filters
<R> <C> method <C> method <C> MCTest-150 acc <C> MCTest-150 acc <C> MCTest-150 acc <C> MCTest-150 NDCG4 <C> MCTest-150 NDCG4 <C> MCTest-150 NDCG4 <C> MCTest-500 acc <C> MCTest-500 acc <C> MCTest-500 acc <C> MCTest-500 NDCG4 <C> MCTest-500 NDCG4 <C> MCTest-500 NDCG4 <R> <C> [EMPTY] <C> [EMPTY] <C> one <C> mul <C> all <C> one <C> mul <C> all <C> one <C> mul <C> all <C> one <C> mul <C> all <R> <C> Baselines <C> Addition <C> 39.3 <C> 32.4 <C> 35.7 <C> 60.4 <C> 50.3 <C> 54.6 <C> 35.7 <C> 30.2 <C> 32.9 <C> 56.6 <C> 55.2 <C> 55.8 <R> <C> Baselines <C> Addition-proj <C> 42.1 <C> 38.7 <C> 40.3 <C> 65.3 <C> 61.3 <C> 63.2 <C> 39.4 <C> 36.7 <C> 38.0 <C> 63.3 <C> 60.1 <C> 61.7 <R> <C> Baselines <C> AR <C> 48.1 <C> 44.7 <C> 46.3 <C> 70.5 <C> 68.9 <C> 69.6 <C> 44.4 <C> 39.5 <C> 41.9 <C> 66.7 <C> 64.2 <C> 65.4 <R> <C> Baselines <C> NR <C> 48.4 <C> 46.8 <C> 47.6 <C> 70.7 <C> 68.2 <C> 69.7 <C> 45.7 <C> 45.6 <C> 45.6 <C> 71.9 <C> 69.5 <C> 70.6 <R> <C> Variant-I <C> Variant-I <C> 50.4 <C> 47.7 <C> 49.0 <C> 73.9 <C> 71.2 <C> 72.5 <C> 45.4 <C> 42.1 <C> 43.7 <C> 68.8 <C> 64.5 <C> 66.6 <R> <C> Variant-II <C> Variant-II <C> 53.6 <C> 51.0 <C> 52.2 <C> 74.6 <C> 70.1 <C> 72.3 <C> 47.1 <C> 46.0 <C> 46.5 <C> 70.2 <C> 63.8 <C> 66.9 <R> <C> HABCNN-QP <C> HABCNN-QP <C> 57.9 <C> 53.7 <C> 55.7 <C> 80.4 <C> 80.0 <C> 80.2 <C> 53.7 <C> 46.7 <C> 50.1 <C> 75.4 <C> 72.7 <C> 74.0 <R> <C> HABCNN-QAP <C> HABCNN-QAP <C> 59.0 <C> 57.9 <C> 58.4 <C> 81.5 <C> 79.9 <C> 80.6 <C> 54.0 <C> 47.2 <C> 50.6 <C> 75.7 <C> 72.6 <C> 74.1 <R> <C> HABCNN-TE <C> HABCNN-TE <C> [BOLD] 63.3 <C> [BOLD] 62.9 <C> [BOLD] 63.1 <C> [BOLD] 86.6 <C> [BOLD] 85.9 <C> [BOLD] 86.2 <C> [BOLD] 54.2 <C> [BOLD] 51.7 <C> [BOLD] 52.9 <C> [BOLD] 76.1 <C> [BOLD] 74.4 <C> [BOLD] 75.2 <CAP> Table 2: Experimental results for one-sentence (one), multiple-sentence (mul) and all cases.
<R> <C> [EMPTY] <C> Perplexity <R> <C> [ITALIC] n-gram <C> 123 <R> <C> model-M  <C> 121 <R> <C> Baseline LSTM <C> 114 <R> <C> HW-LSTM-C <C> 115 <R> <C> HW-LSTM-H <C> 102 <R> <C> HW-LSTM-CH <C> 102 <CAP> Table 1: Perplexity on broadcast news with various LMs.
<R> <C> [EMPTY] <C> [ITALIC] F1 [BOLD] -score <R> <C> RNN  <C> 94.11 <R> <C> CNN-CRF  <C> 94.35 <R> <C> Bi-directional RNN  <C> 94.73 <R> <C> LSTM  <C> 94.85 <R> <C> RNN-SOP  <C> 94.89 <R> <C> Hybrid RNN  <C> 95.06 <R> <C> Deep LSTM  <C> 95.08 <R> <C> RNN-EM  <C> 95.25 <R> <C> R-biRNN  <C> 95.47 <R> <C> 5×R-biRNN  <C> 95.56 <R> <C> [BOLD] Encoder-labeler LSTM(W) <C> [BOLD] 95.40 <R> <C> [BOLD] Encoder-labeler Deep LSTM(W) <C> [BOLD] 95.66 <CAP> Table 2: Comparison with published results on ATIS slot filling task. F1-scores by proposed method are improved from Table 1 due to sophisticated hyper-parameters. [%]
<R> <C> [BOLD] ExtraTrees Feature <C> [BOLD] ExtraTrees Score <C> [BOLD] SVM Feature <C> [BOLD] SVM Score <R> <C> SYN <C> 23.97 <C> F1S <C> 8.36 <R> <C> F1S <C> 18.02 <C> SYN <C> 7.67 <R> <C> F4S <C> 9.55 <C> F3WC <C> 5.29 <R> <C> F3WC <C> 7.14 <C> F4S <C> 4.25 <R> <C> F1C <C> 6.07 <C> F4SC <C> 3.99 <R> <C> F3SC <C> 5.07 <C> F3C <C> 3.92 <R> <C> F3C <C> 4.94 <C> F3SC <C> 2.79 <R> <C> F3S <C> 3.96 <C> F2C <C> 2.61 <R> <C> F2C <C> 3.58 <C> F3S <C> 2.54 <R> <C> F4C <C> 3.56 <C> F4C <C> 2.20 <R> <C> F4SC <C> 2.97 <C> F2S <C> 2.13 <R> <C> F2WC <C> 2.65 <C> BD <C> 2.08 <R> <C> BD <C> 2.11 <C> F2WC <C> 2.04 <R> <C> F2S <C> 1.87 <C> F4WC <C> 1.87 <R> <C> F4WC <C> 1.81 <C> F1C <C> 1.82 <R> <C> F2SC <C> 1.70 <C> F2SC <C> 1.72 <CAP> Table 1: Ranked Feature Importances, as determined by ExtraTrees and SVM
<R> <C> Model  [ITALIC] Character CRF <C> Eng  [ITALIC] Character CRF <C> Deu  [ITALIC] Character CRF <C> Nld  [ITALIC] Character CRF <C> Spa  [ITALIC] Character CRF <C> [BOLD] Avg  [ITALIC] Character CRF <C> Amh  [ITALIC] Character CRF <C> Ara  [ITALIC] Character CRF <C> Fas  [ITALIC] Character CRF <C> Hin  [ITALIC] Character CRF <C> Hun  [ITALIC] Character CRF <C> Ind  [ITALIC] Character CRF <C> Som  [ITALIC] Character CRF <C> Swa  [ITALIC] Character CRF <C> Tgl  [ITALIC] Character CRF <C> Vie <C> [BOLD] Avg <R> <C> Monolingual <C> 84.91 <C> 71.39 <C> 78.96 <C> 82.60 <C> 79.45 <C> 60.62 <C> 43.22 <C> 45.11 <C> 62.12 <C> 60.47 <C> 62.14 <C> 61.75 <C> 68.04 <C> 84.13 <C> 47.31 <C> 59.49 <R> <C> Polyglot <C> 83.38 <C> 70.86 <C> 79.38 <C> 81.64 <C> 77.85 <C> 59.39 <C> 43.25 <C> 43.20 <C> 62.88 <C> 60.86 <C> 64.59 <C> 65.45 <C> 68.32 <C> 84.80 <C> 49.71 <C> 59.87 <R> <C> Finetuned <C> 86.49 <C> 72.95 <C> 80.91 <C> 82.72 <C> [BOLD] 80.82 <C> 59.86 <C> 44.69 <C> 46.85 <C> 68.30 <C> 65.21 <C> 67.15 <C> 66.11 <C> 70.07 <C> 87.03 <C> 51.80 <C> [BOLD] 62.71 <R> <C> [ITALIC] Byte CRF <C> [ITALIC] Byte CRF <C> [ITALIC] Byte CRF <C> [ITALIC] Byte CRF <C> [ITALIC] Byte CRF <C> [ITALIC] Byte CRF <C> [ITALIC] Byte CRF <C> [ITALIC] Byte CRF <C> [ITALIC] Byte CRF <C> [ITALIC] Byte CRF <C> [ITALIC] Byte CRF <C> [ITALIC] Byte CRF <C> [ITALIC] Byte CRF <C> [ITALIC] Byte CRF <C> [ITALIC] Byte CRF <C> [EMPTY] <C> [EMPTY] <R> <C> Monolingual <C> 85.75 <C> 71.42 <C> 78.36 <C> 81.19 <C> 79.18 <C> 59.13 <C> 44.95 <C> 44.76 <C> 65.89 <C> 57.91 <C> 61.46 <C> 61.05 <C> 67.09 <C> 84.46 <C> 48.73 <C> 59.54 <R> <C> Polyglot <C> 83.79 <C> 71.54 <C> 79.43 <C> 80.25 <C> 78.75 <C> 57.03 <C> 42.88 <C> 41.88 <C> 65.10 <C> 60.46 <C> 61.07 <C> 62.22 <C> 68.40 <C> 82.75 <C> 47.27 <C> 58.90 <R> <C> Finetuned <C> 86.68 <C> 73.02 <C> 80.09 <C> 82.95 <C> [BOLD] 80.69 <C> 59.37 <C> 42.69 <C> 45.25 <C> 67.68 <C> 63.91 <C> 64.38 <C> 64.92 <C> 70.78 <C> 86.25 <C> 51.14 <C> [BOLD] 61.64 <R> <C> [ITALIC] CharNER <C> [ITALIC] CharNER <C> [ITALIC] CharNER <C> [ITALIC] CharNER <C> [ITALIC] CharNER <C> [ITALIC] CharNER <C> [ITALIC] CharNER <C> [ITALIC] CharNER <C> [ITALIC] CharNER <C> [ITALIC] CharNER <C> [ITALIC] CharNER <C> [ITALIC] CharNER <C> [ITALIC] CharNER <C> [ITALIC] CharNER <C> [ITALIC] CharNER <C> [EMPTY] <C> [EMPTY] <R> <C> Monolingual <C> 83.83 <C> 69.30 <C> 79.60 <C> 79.46 <C> 78.05 <C> 54.33 <C> 36.31 <C> 40.68 <C> 62.03 <C> 53.04 <C> 58.05 <C> 56.88 <C> 63.70 <C> 81.04 <C> 39.64 <C> 54.53 <R> <C> Polyglot <C> 84.14 <C> 69.19 <C> 78.94 <C> 79.39 <C> 77.92 <C> 49.64 <C> 36.98 <C> 37.41 <C> 60.02 <C> 49.37 <C> 55.51 <C> 58.56 <C> 63.49 <C> 79.36 <C> 44.50 <C> 53.48 <R> <C> Finetuned <C> 85.23 <C> 70.60 <C> 81.00 <C> 82.00 <C> [BOLD] 79.70 <C> 53.46 <C> 40.15 <C> 39.20 <C> 65.57 <C> 59.84 <C> 60.70 <C> 59.09 <C> 68.85 <C> 84.61 <C> 45.47 <C> [BOLD] 57.70 <R> <C> [ITALIC] Byte To Span <C> [ITALIC] Byte To Span <C> [ITALIC] Byte To Span <C> [ITALIC] Byte To Span <C> [ITALIC] Byte To Span <C> [ITALIC] Byte To Span <C> [ITALIC] Byte To Span <C> [ITALIC] Byte To Span <C> [ITALIC] Byte To Span <C> [ITALIC] Byte To Span <C> [ITALIC] Byte To Span <C> [ITALIC] Byte To Span <C> [ITALIC] Byte To Span <C> [ITALIC] Byte To Span <C> [ITALIC] Byte To Span <C> [EMPTY] <C> [EMPTY] <R> <C> Monolingual <C> 87.91 <C> 63.92 <C> 71.34 <C> 73.07 <C> 74.06 <C> 48.23 <C> 39.41 <C> 26.76 <C> 19.01 <C> 44.51 <C> 54.32 <C> 58.81 <C> 54.27 <C> 71.76 <C> 26.90 <C> 44.50 <R> <C> Polyglot <C> 86.43 <C> 71.10 <C> 76.11 <C> 74.26 <C> [BOLD] 76.98 <C> 46.41 <C> 41.59 <C> 40.09 <C> 55.69 <C> 60.53 <C> 57.58 <C> 62.30 <C> 54.78 <C> 74.52 <C> 43.95 <C> [BOLD] 53.64 <R> <C> [ITALIC] Multilingual BERT <C> [ITALIC] Multilingual BERT <C> [ITALIC] Multilingual BERT <C> [ITALIC] Multilingual BERT <C> [ITALIC] Multilingual BERT <C> [ITALIC] Multilingual BERT <C> [ITALIC] Multilingual BERT <C> [ITALIC] Multilingual BERT <C> [ITALIC] Multilingual BERT <C> [ITALIC] Multilingual BERT <C> [ITALIC] Multilingual BERT <C> [ITALIC] Multilingual BERT <C> [ITALIC] Multilingual BERT <C> [ITALIC] Multilingual BERT <C> [ITALIC] Multilingual BERT <C> [EMPTY] <C> [EMPTY] <R> <C> Monolingual <C> 90.94 <C> 81.50 <C> 88.62 <C> 88.16 <C> [BOLD] 87.31 <C> - <C> 48.36 <C> 56.42 <C> 72.52 <C> 66.99 <C> 78.32 <C> 62.69 <C> 72.18 <C> 86.13 <C> 54.18 <C> 66.75 <R> <C> Polyglot <C> 90.67 <C> 80.96 <C> 87.48 <C> 87.04 <C> 86.53 <C> - <C> 48.33 <C> 56.92 <C> 74.81 <C> 68.16 <C> 77.56 <C> 59.29 <C> 71.92 <C> 87.59 <C> 57.06 <C> 66.84 <R> <C> Finetuned <C> 91.08 <C> 81.27 <C> 88.74 <C> 86.87 <C> 86.99 <C> - <C> 49.94 <C> 54.67 <C> 76.83 <C> 69.52 <C> 80.14 <C> 62.70 <C> 73.16 <C> 88.05 <C> 56.74 <C> [BOLD] 69.97 <CAP> Table 1: Performance for monolingual, multilingual, and finetuned models trained on either CoNLL (left) or LORELEI (right) data sets. The results are taken from the best model out of 5 random seeds, as measured by dev performance. Almost every model achieves the best performance in the finetuned setting, indicating that multilingual pretraining is learning transferable parameters, but multilingual models are not able to use them effectively across all languages simultaneously. Note that we do not evaluate Amharic with mBERT, because the Amharic script is not a part of mBERT’s vocabulary.
<R> <C> Language <C> Monoling. <C> Poly. (Zero-shot) <C> Poly. (Fine-tuned) <R> <C> Russian <C> [BOLD] 43.97 <C> 1.61 <C> 41.55 <R> <C> Bengali <C> 76.10 <C> 2.08 <C> [BOLD] 76.63 <R> <C> Uzbek <C> [BOLD] 65.39 <C> 14.54 <C> 61.10 <R> <C> Yoruba <C> 62.66 <C> 29.02 <C> [BOLD] 64.95 <CAP> Table 3: F1 of a Byte-level CRF on 4 different lorelei language datasets, compared to the performance of the multilingual model which was not trained on any of these 4 languages, as well as the multilingual model after finetuning. The results are mixed - moreover, zero-shot performance does not seem to be a good indicator of transferability.
<R> <C> System <C> Alchemy Inst <C> Alchemy 3utts <C> Alchemy 5utts <C> Scene Inst <C> Scene 3utts <C> Scene 5utts <C> Tangrams Inst <C> Tangrams 3utts <C> Tangrams 5utts <R> <C> Long et al. ( 2016 ) <C> – <C> 56.8 <C> 52.3 <C> – <C> 23.2 <C> 14.7 <C> – <C> 64.9 <C> 27.6 <R> <C> [0.5pt/1pt]Guu et al. ( 2017 ) <C> – <C> 66.9 <C> 52.9 <C> – <C> 64.8 <C> 46.2 <C> – <C> 65.8 <C> 37.1 <R> <C> Fried et al. ( 2018 ) <C> – <C> – <C> 72.0 <C> – <C> – <C> 72.7 <C> – <C> – <C> 69.6 <R> <C> Supervised <C> 89.4 <C> 73.3 <C> 62.3 <C> 88.8 <C> 78.9 <C> 66.4 <C> 86.6 <C> 81.4 <C> 60.1 <R> <C> PolicyGradient <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 1.3 <C> 0.2 <C> 84.1 <C> 77.4 <C> 54.9 <R> <C> [0.5pt/1pt]ContextualBandit <C> 73.8 <C> 36.0 <C> 25.7 <C> 15.1 <C> 2.9 <C> 4.4 <C> 84.8 <C> 76.9 <C> 57.9 <R> <C> [0.5pt/1pt]Our approach <C> 89.1 <C> 74.2 <C> 62.7 <C> 87.1 <C> 73.9 <C> 62.0 <C> 86.6 <C> 80.8 <C> 62.4 <CAP> Table 3: Test accuracies for single instructions (Inst), first-three instructions (3utts), and full interactions (5utts).
<R> <C> System <C> Alchemy Inst <C> Alchemy 3utts <C> Alchemy 5utts <C> Scene Inst <C> Scene 3utts <C> Scene 5utts <C> Tangrams Inst <C> Tangrams 3utts <C> Tangrams 5utts <R> <C> Supervised <C> 92.0 <C> 83.3 <C> 71.4 <C> 85.3 <C> 72.7 <C> 60.6 <C> 86.1 <C> 81.9 <C> 58.3 <R> <C> PolicyGradient <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.9 <C> 1.0 <C> 0.5 <C> 85.2 <C> 74.9 <C> 52.3 <R> <C> [0.5pt/1pt]ContextualBandit <C> 58.8 <C> 6.9 <C> 5.7 <C> 12.0 <C> 0.5 <C> 1.5 <C> 85.6 <C> 78.4 <C> 52.6 <R> <C> Our approach <C> [BOLD] 92.1 <C> [BOLD] 82.9 <C> [BOLD] 71.8 <C> [BOLD] 83.9 <C> [BOLD] 68.7 <C> 56.1 <C> 88.5 <C> 82.4 <C> 60.3 <R> <C> [0.5pt/1pt]– previous instructions <C> 90.1 <C> 77.1 <C> 66.1 <C> 79.3 <C> 60.6 <C> 45.5 <C> 76.4 <C> 55.8 <C> 27.6 <R> <C> [0.5pt/1pt]– current and initial state <C> 25.7 <C> 4.5 <C> 3.3 <C> 17.5 <C> 0.0 <C> 0.0 <C> 45.4 <C> 15.1 <C> 3.5 <R> <C> [0.5pt/1pt]– current state <C> 89.8 <C> 78.0 <C> 62.9 <C> 83.0 <C> [BOLD] 68.7 <C> 54.0 <C> 87.6 <C> 78.4 <C> 60.8 <R> <C> [0.5pt/1pt]– initial state <C> 81.1 <C> 68.6 <C> 42.9 <C> 82.7 <C> 67.7 <C> [BOLD] 57.1 <C> [BOLD] 88.6 <C> [BOLD] 82.9 <C> [BOLD] 63.3 <R> <C> Our approach ( [ITALIC] μ± [ITALIC] σ) <C> 91.5 ±1.4 <C> 80.4 ±2.6 <C> 69.5 ±5.0 <C> 62.9 ±17.7 <C> 37.8 ±23.5 <C> 29.0 ±21.1 <C> 88.2 ±0.6 <C> 80.8 ±2.8 <C> 59.2 ±2.3 <CAP> Table 4: Development results, including model ablations. We also report mean μ and standard deviation σ for all metrics for our approach across five experiments. We bold the best performing variations of our model.
<R> <C> Class <C> Alc <C> Sce <C> Tan <R> <C> State reference <C> 23 <C> 13 <C> 7 <R> <C> [0.5pt/1pt]Multi-turn reference <C> 12 <C> 5 <C> 13 <R> <C> [0.5pt/1pt]Impossible multi-turn reference <C> 2 <C> 5 <C> 13 <R> <C> [0.5pt/1pt]Ambiguous or incorrect label <C> 2 <C> 19 <C> 12 <CAP> Table 5: Common error counts in the three domains.
<R> <C> [BOLD] Dataset <C> [BOLD] L/U <C> [BOLD] 100 <C> [BOLD] 500 <C> [BOLD] 1000 <C> [BOLD] 2000 <C> [BOLD] All L <R> <C> [BOLD] Nepal Earthquake <C> L <C> 47.11 <C> 52.63 <C> 55.95 <C> 58.26 <C> 60.89 <R> <C> [BOLD] Nepal Earthquake <C> L+U(50k) <C> 52.32 <C> 59.95 <C> 61.89 <C> 64.05 <C> 66.63 <R> <C> [BOLD] Queensland Flood <C> L <C> 58.52 <C> 60.14 <C> 62.22 <C> 73.92 <C> 78.92 <R> <C> [BOLD] Queensland Flood <C> L+U(∼21k) <C> 75.08 <C> 85.54 <C> 89.08 <C> 91.54 <C> 93.54 <CAP> Table 2: F-measure for different experimental settings. L refers to labeled data, U refers to unlabeled data, All L refers to all labeled instances for that particular dataset.
<R> <C> Wikipedia <C> MLP <C> SGNS 0.576 <C> GloVe 0.522 <C> PPMI 0.483 <R> <C> [EMPTY] <C> LT <C> 0.549 <C> 0.450 <C> 0.429 <R> <C> COCA <C> MLP <C> 0.634 <C> 0.554 <C> 0.440 <R> <C> [EMPTY] <C> LT <C> 0.598 <C> 0.494 <C> 0.454 <CAP> Table 3: Mean correlations over all attributes
<R> <C> [BOLD] Language <C> [BOLD] This Work  [BOLD] UAS <C> [BOLD] This Work  [BOLD] LAS <C> [BOLD] This Work  [BOLD] System <C> [BOLD] Best Greedy Result  [BOLD] UAS <C> [BOLD] Best Greedy Result  [BOLD] LAS <C> [BOLD] Best Greedy Result  [BOLD] System <C> [BOLD] Best Published Result  [BOLD] UAS <C> [BOLD] Best Published Result  [BOLD] LAS <C> [BOLD] Best Published Result  [BOLD] System <R> <C> Arabic <C> 86.08 <C> 83.41 <C> [BOLD] Chars <C> 84.57 <C> 81.90 <C> B’13 <C> 88.32 <C> 86.21 <C> B+’13 <R> <C> Basque <C> 85.22 <C> 78.61 <C> [BOLD] Chars + POS <C> 84.33 <C> 78.58 <C> B’13 <C> 89.96 <C> 85.70 <C> B+’14 <R> <C> French <C> 86.15 <C> 82.03 <C> [BOLD] Words + POS <C> 83.35 <C> 77.98 <C> B’13 <C> 89.02 <C> 85.66 <C> B+’14 <R> <C> German <C> 87.33 <C> 84.62 <C> [BOLD] Words + POS <C> 85.38 <C> 82.75 <C> B’13 <C> 91.64 <C> 89.65 <C> B+’13 <R> <C> Hebrew <C> 80.68 <C> 72.70 <C> [BOLD] Words + POS <C> 79.89 <C> 73.01 <C> B’13 <C> 87.41 <C> 81.65 <C> B+’14 <R> <C> Hungarian <C> 80.92 <C> 76.34 <C> [BOLD] Chars + POS <C> 83.71 <C> 79.63 <C> B’13 <C> 89.81 <C> 86.13 <C> B+’13 <R> <C> Korean <C> 88.39 <C> 86.27 <C> [BOLD] Chars <C> 85.72 <C> 82.06 <C> B’13 <C> 89.10 <C> 87.27 <C> B+’14 <R> <C> Polish <C> 87.06 <C> 79.83 <C> [BOLD] Words + POS <C> 85.80 <C> 79.89 <C> B’13 <C> 91.75 <C> 87.07 <C> B+’13 <R> <C> Swedish <C> 83.43 <C> 76.40 <C> [BOLD] Words + POS <C> 83.20 <C> 75.82 <C> B’13 <C> 88.48 <C> 82.75 <C> B+’14 <R> <C> Turkish <C> 76.32 <C> 64.34 <C> [BOLD] Chars <C> 75.82 <C> 65.68 <C> N+’06a <C> 77.55 <C> [EMPTY] <C> K+’10 <R> <C> Chinese <C> 85.96 <C> 84.40 <C> [BOLD] Words + POS <C> 87.20 <C> 85.70 <C> D+’15 <C> 87.20 <C> 85.70 <C> D+’15 <R> <C> English <C> 92.57 <C> 90.31 <C> [BOLD] Words + POS <C> 93.10 <C> 90.90 <C> D+’15 <C> 94.08 <C> 92.19 <C> W+’15 <CAP> Table 3: Test-set performance of our best results (according to UAS or LAS, whichever has the larger difference), compared to state-of-the-art greedy transition-based parsers (“Best Greedy Result”) and best results reported (“Best Published Result”). All of the systems we compare against use explicit morphological features and/or one of the following: pretrained word embeddings, unlabeled data and a combination of parsers; our models do not. B’13 is W13-4907; N+’06a is nivre06conll; D+’15 is lstmacl15; B+’13 is bjorkelund-EtAl:2013:SPMRL; B+’14 is bjorkelund-EtAl:2014:SPMRL-SANCL; K+’10 is Koo:2010:DDP:1870658.1870783; W+’15 is weiss:2015.
<R> <C> [BOLD] System <C> [BOLD] R-1 <C> [BOLD] R-2 <C> [BOLD] R-L <R> <C> RNN <C> 21.50 <C> 8.90 <C> 18.60 <R> <C> RNN-context <C> 29.90 <C> 17.40 <C> 27.20 <R> <C> CopyNet <C> 34.40 <C> 21.60 <C> 31.30 <R> <C> RNN-distract <C> 35.20 <C> 22.60 <C> 32.50 <R> <C> [BOLD] DRGD <C> [BOLD] 36.99 <C> [BOLD] 24.15 <C> [BOLD] 34.21 <CAP> Table 4: ROUGE-F1 on LCSTS
<R> <C> [ITALIC] X <C> [ITALIC] r <C> [ITALIC] L <C> [ITALIC] L∗ <C> [ITALIC] C <C> [ITALIC] C∗ <C> [ITALIC] I <C> [ITALIC] I∗ <C> [ITALIC] B <C> [ITALIC] s∗ <C> [ITALIC] γs <R> <C> [ITALIC] c <C> 0.14 <C> 0.62 <C> 0.99 <C> 0.96 <C> 0.05 <C> 0.39 <C> 0.00 <C> 0.00 <C> 0.09 <C> 0.12 <CAP> Table 4: Compatibility of VMS with natural languages. Except for I∗ and B, the measurements computed for VMS are consistent with those expected for texts written in natural languages.
<R> <C> [BOLD] Model  [BOLD] Metrics <C> [BOLD] Avg <C> [BOLD] BoolQ  [BOLD] Acc. <C> [BOLD] CB  [BOLD] F1/Acc. <C> [BOLD] CB  [BOLD] F1/Acc. <C> [BOLD] COPA  [BOLD] Acc. <C> [BOLD] MultiRC  [BOLD] F1 [ITALIC] a/EM <C> [BOLD] MultiRC  [BOLD] F1 [ITALIC] a/EM <C> [BOLD] ReCoRD  [BOLD] F1/EM <C> [BOLD] ReCoRD  [BOLD] F1/EM <C> [BOLD] RTE  [BOLD] Acc. <C> [BOLD] WiC  [BOLD] Acc. <C> [BOLD] WSC  [BOLD] Acc. <C> [BOLD] AX [ITALIC] b  [BOLD] MCC <C> [BOLD] AX [ITALIC] g  [BOLD] GPS <C> [BOLD] AX [ITALIC] g  [BOLD] Acc. <R> <C> Most Frequent <C> 47.1 <C> 62.3 <C> 21.7 / <C> 48.4 <C> 50.0 <C> 61.1 / <C> 0.3 <C> 33.4 / <C> 32.5 <C> 50.3 <C> 50.0 <C> 65.1 <C> 0.0 <C> 100.0 / <C> 50.0 <R> <C> CBoW <C> 44.3 <C> 62.1 <C> 49.0 / <C> 71.2 <C> 51.6 <C> 0.0 / <C> 0.4 <C> 14.0 / <C> 13.6 <C> 49.7 <C> 53.0 <C> 65.1 <C> -0.4 <C> 100.0 / <C> 50.0 <R> <C> BERT <C> 69.0 <C> 77.4 <C> 75.7 / <C> 83.6 <C> 70.6 <C> 70.0 / <C> 24.0 <C> 72.0 / <C> 71.3 <C> 71.6 <C> [BOLD] 69.5 <C> [BOLD] 64.3 <C> 23.0 <C> 97.8 / <C> 51.7 <R> <C> BERT++ <C> [BOLD] 71.5 <C> 79.0 <C> [BOLD] 84.7 / <C> [BOLD] 90.4 <C> 73.8 <C> 70.0 / <C> 24.1 <C> 72.0 / <C> 71.3 <C> 79.0 <C> [BOLD] 69.5 <C> [BOLD] 64.3 <C> 38.0 <C> 99.4 / <C> 51.4 <R> <C> Outside Best <C> - <C> [BOLD] 80.4 <C> - / <C> - <C> [BOLD] 84.4 <C> [BOLD] 70.4* / <C> [BOLD] 24.5* <C> [BOLD] 74.8 / <C> [BOLD] 73.0 <C> [BOLD] 82.7 <C> - <C> - <C> - <C> - / <C> - <R> <C> Human (est.) <C> 89.8 <C> 89.0 <C> 95.8 / <C> 98.9 <C> 100.0 <C> 81.8* / <C> 51.9* <C> 91.7 / <C> 91.3 <C> 93.6 <C> 80.0 <C> 100.0 <C> 77.0 <C> 99.3 / <C> 99.7 <CAP> Table 3: Baseline performance on the SuperGLUE test sets and diagnostics. For CB we report accuracy and macro-average F1. For MultiRC we report F1 on all answer-options and exact match of each question’s set of correct answers. AXb is the broad-coverage diagnostic task, scored using Matthews’ correlation (MCC). AXg is the Winogender diagnostic, scored using accuracy and the gender parity score (GPS). All values are scaled by 100. The Avg column is the overall benchmark score on non-AX∗ tasks. The bolded numbers reflect the best machine performance on task. *MultiRC has multiple test sets released on a staggered schedule, and these results evaluate on an installation of the test set that is a subset of ours.
<R> <C> Method <C> BLEU-4 <C> METEOR <C> CIDEr <R> <C> MMVD \lx @ [ITALIC] sectionsign <C> 40.7 <C> 28.6 <C> 46.5 <R> <C> M-to-M‡ <C> 40.8 <C> 28.8 <C> 47.1 <R> <C> Aalto† <C> 41.1 <C> 27.7 <C> 46.4 <R> <C> TGM \mathsection <C> 44.33 <C> 29.37 <C> 49.26 <R> <C> [ITALIC] Baselines <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> ResNet + LSTM <C> 39.54 <C> 26.59 <C> 45.22 <R> <C> C3D fc7 + LSTM <C> 40.17 <C> 26.86 <C> 45.95 <R> <C> C3D fc7 + pool2 <C> 40.43 <C> 26.93 <C> 47.15 <R> <C> C3D fc7 + pool3 <C> 42.04 <C> 27.18 <C> 48.93 <R> <C> C3D fc7 + pool4 <C> 41.98 <C> 27.42 <C> 48.21 <R> <C> C3D fc7 + pool5 <C> 40.83 <C> 27.01 <C> 47.86 <R> <C> [ITALIC] dualAFR <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Soft Attention (Single) <C> 43.72 <C> 29.67 <C> 50.21 <R> <C> Hard Attention (Single) <C> 43.89 <C> 28.71 <C> 50.29 <R> <C> Soft Attention (Ensemble) <C> 44.99 <C> [BOLD] 30.16 <C> 51.13 <R> <C> Hard Attention (Ensemble) <C> [BOLD] 45.01 <C> 29.98 <C> [BOLD] 51.41 <CAP> Table 2: Results on BLEU-4, METEOR and CIDEr metrics compared to other models and baselines on MSR-VTT. \lx@sectionsign(Ramanishka et al., 2016); †(Shetty and Laaksonenl, 2016); \mathsection(Chen et al., 2017); ‡(Pasunuru and Bansal, 2017).
<R> <C> [BOLD] Method dataset <C> [BOLD] Method word overlap  [ITALIC] ow <C> [BOLD] MT 28.6 <C> [BOLD] EL 30.7 <C> [BOLD] POS 13.4 <C> [BOLD] DEP 52.3 <R> <C> dataset <C> subword overlap  [ITALIC] osw <C> 29.2 <C> – <C> – <C> – <R> <C> dataset <C> size ratio  [ITALIC] stf/ [ITALIC] stk <C> 3.7 <C> 0.3 <C> 9.5 <C> 24.8 <R> <C> dataset <C> type-token ratio  [ITALIC] dttr <C> 2.5 <C> – <C> 7.4 <C> 6.4 <R> <C> ling. distance <C> genetic  [ITALIC] dgen <C> 24.2 <C> 50.9 <C> 14.8 <C> 32.0 <R> <C> ling. distance <C> syntactic  [ITALIC] dsyn <C> 14.8 <C> 46.4 <C> 4.1 <C> 22.9 <R> <C> ling. distance <C> featural  [ITALIC] dfea <C> 10.1 <C> 47.5 <C> 5.7 <C> 13.9 <R> <C> ling. distance <C> phonological  [ITALIC] dpho <C> 3.0 <C> 4.0 <C> 9.8 <C> 43.4 <R> <C> ling. distance <C> inventory  [ITALIC] dinv <C> 8.5 <C> 41.3 <C> 2.4 <C> 23.5 <R> <C> ling. distance <C> geographic  [ITALIC] dgeo <C> 15.1 <C> 49.5 <C> 15.7 <C> 46.4 <R> <C> LangRank (all) <C> LangRank (all) <C> 51.1 <C> [BOLD] 63.0 <C> [BOLD] 28.9 <C> [BOLD] 65.0 <R> <C> LangRank (dataset) <C> LangRank (dataset) <C> [BOLD] 53.7 <C> 17.0 <C> 26.5 <C> [BOLD] 65.0 <R> <C> LangRank (URIEL) <C> LangRank (URIEL) <C> 32.6 <C> 58.1 <C> 16.6 <C> 59.6 <CAP> Table 1: Our LangRank model leads to higher average NDCG@3 over the baselines on all four tasks: machine translation (MT), entity linking (EL), part-of-speech tagging (POS) and dependency parsing (DEP).
<R> <C> [EMPTY] <C> [EMPTY] <C> [BOLD] Development UP <C> [BOLD] Development UR <C> [BOLD] Development LP <C> [BOLD] Development LR <C> [BOLD] Development SAcc. <C> [BOLD] Test UP <C> [BOLD] Test UR <C> [BOLD] Test LP <C> [BOLD] Test LR <C> [BOLD] Test SAcc. <R> <C> oracle <C> composite <C> 91.32 <C> 88.20 <C> [BOLD] 91.32 <C> [BOLD] 88.20 <C> [BOLD] 91.14** <C> 90.71 <C> 86.81 <C> [BOLD] 90.71 <C> 86.81 <C> [BOLD] 86.08* <R> <C> oracle <C> orphan <C> [BOLD] 94.08 <C> [BOLD] 93.79 <C> 87.54 <C> 87.27 <C> 72.15 <C> [BOLD] 92.02 <C> [BOLD] 92.02 <C> 87.12 <C> [BOLD] 87.12 <C> 72.15 <R> <C> end-to-end <C> composite <C> 70.48 <C> [BOLD] 49.69* <C> [BOLD] 65.64 <C> [BOLD] 46.27* <C> [BOLD] 31.65 <C> 67.39 <C> [BOLD] 47.55 <C> 61.74 <C> [BOLD] 43.56 <C> 31.65 <R> <C> end-to-end <C> orphan <C> [BOLD] 71.73 <C> 42.55 <C> 65.45 <C> 38.82 <C> 30.38 <C> [BOLD] 78.92** <C> 44.78 <C> [BOLD] 68.11 <C> 38.65 <C> [BOLD] 34.18 <R> <C> [EMPTY] <C> K&K 2017 <C> - <C> - <C> - <C> - <C> 00.00 <C> - <C> - <C> - <C> - <C> 00.00 <CAP> Table 5: Labeled and unlabeled precision and recall as well as sentence-level accuracy of the two gapping reconstructions methods and the K&K parser on the development and test set of the combined treebank. Results that differ significantly from the other result within the same section are marked with * (p<0.05) or ** (p<0.01).
<R> <C> parser <C> UAS <R> <C> left chain baseline <C> 6.8 <R> <C> right chain baseline <C> 29.5 <R> <C> algorithm D <C> 31.1 <R> <C> algorithm D, red. punct. <C> 33.1 <R> <C> algorithm R <C> 37.0 <R> <C> algorithm R, red. punct. <C> 40.6 <CAP> Table 1: Parsing results, Unlabelled Attachment Score
<R> <C> [EMPTY] <C> Model <C> Success (%) <C> BLEU <R> <C> [BOLD] Ground Truth <C> [BOLD] Ground Truth <C> [BOLD] Ground Truth <C> [BOLD] Ground Truth <R> <C> [EMPTY] <C> Ground Truth <C> 91.6 <C> 1.000 <R> <C> [BOLD] Published Models (Wen et al.,  2016 ) <C> [BOLD] Published Models (Wen et al.,  2016 ) <C> [BOLD] Published Models (Wen et al.,  2016 ) <C> [BOLD] Published Models (Wen et al.,  2016 ) <R> <C> [EMPTY] <C> NDM <C> 76.1 <C> 0.212 <R> <C> [EMPTY] <C> NDM+Att <C> 79.0 <C> 0.224 <R> <C> [EMPTY] <C> NDM+Att+SS <C> 81.8 <C> 0.240 <R> <C> [BOLD] LIDM Models <C> [BOLD] LIDM Models <C> [BOLD] LIDM Models <C> [BOLD] LIDM Models <R> <C> [EMPTY] <C> LIDM,  [ITALIC] I=50 <C> 66.9 <C> 0.238 <R> <C> [EMPTY] <C> LIDM,  [ITALIC] I=70 <C> 61.0 <C> [BOLD] 0.246 <R> <C> [EMPTY] <C> LIDM,  [ITALIC] I=100 <C> 63.2 <C> 0.242 <R> <C> [BOLD] LIDM Models + RL <C> [BOLD] LIDM Models + RL <C> [BOLD] LIDM Models + RL <C> [BOLD] LIDM Models + RL <R> <C> [EMPTY] <C> LIDM,  [ITALIC] I=50, +RL <C> 82.4 <C> 0.231 <R> <C> [EMPTY] <C> LIDM,  [ITALIC] I=70, +RL <C> 81.6 <C> 0.230 <R> <C> [EMPTY] <C> LIDM,  [ITALIC] I=100, +RL <C> [BOLD] 84.6 <C> 0.240 <CAP> Table 2: Corpus-based Evaluation.
<R> <C> [BOLD] Task <C> [BOLD] Previous SOTA <C> [EMPTY] <C> [BOLD] Our baseline <C> [BOLD] ELMo + baseline <C> [BOLD] Increase (absolute/ relative) <R> <C> SQuAD <C> Liu et al. ( 2017 ) <C> 84.4 <C> 81.1 <C> 85.8 <C> 4.7 / 24.9% <R> <C> SNLI <C> Chen et al. ( 2017 ) <C> 88.6 <C> 88.0 <C> 88.7 ± 0.17 <C> 0.7 / 5.8% <R> <C> SRL <C> He et al. ( 2017 ) <C> 81.7 <C> 81.4 <C> 84.6 <C> 3.2 / 17.2% <R> <C> Coref <C> Lee et al. ( 2017 ) <C> 67.2 <C> 67.2 <C> 70.4 <C> 3.2 / 9.8% <R> <C> NER <C> Peters et al. ( 2017 ) <C> 91.93 ± 0.19 <C> 90.15 <C> 92.22 ± 0.10 <C> 2.06 / 21% <R> <C> SST-5 <C> McCann et al. ( 2017 ) <C> 53.7 <C> 51.4 <C> 54.7 ± 0.5 <C> 3.3 / 6.8% <CAP> Table 1: Test set comparison of ELMo enhanced neural models with state-of-the-art single model baselines across six benchmark NLP tasks. The performance metric varies across tasks – accuracy for SNLI and SST-5; F1 for SQuAD, SRL and NER; average F1 for Coref. Due to the small test sizes for NER and SST-5, we report the mean and standard deviation across five runs with different random seeds. The “increase” column lists both the absolute and relative improvements over our baseline.
<R> <C> Task <C> Baseline <C> Last Only <C> All layers  [ITALIC] λ=1 <C> All layers  [ITALIC] λ=0.001 <R> <C> SQuAD <C> 80.8 <C> 84.7 <C> 85.0 <C> [BOLD] 85.2 <R> <C> SNLI <C> 88.1 <C> 89.1 <C> 89.3 <C> [BOLD] 89.5 <R> <C> SRL <C> 81.6 <C> 84.1 <C> 84.6 <C> [BOLD] 84.8 <CAP> Table 2: Development set performance for SQuAD, SNLI and SRL comparing using all layers of the biLM (with different choices of regularization strength λ) to just the top layer.
<R> <C> Task <C> Input Only <C> Input & Output <C> Output Only <R> <C> SQuAD <C> 85.1 <C> [BOLD] 85.6 <C> 84.8 <R> <C> SNLI <C> 88.9 <C> [BOLD] 89.5 <C> 88.7 <R> <C> SRL <C> [BOLD] 84.7 <C> 84.3 <C> 80.9 <CAP> Table 3: Development set performance for SQuAD, SNLI and SRL when including ELMo at different locations in the supervised model.
<R> <C> [BOLD] Model <C> [BOLD] F1 <R> <C> WordNet 1st Sense Baseline <C> 65.9 <R> <C> Raganato et al. ( 2017a ) <C> 69.9 <R> <C> Iacobacci et al. ( 2016 ) <C> [BOLD] 70.1 <R> <C> CoVe, First Layer <C> 59.4 <R> <C> CoVe, Second Layer <C> [ITALIC] 64.7 <R> <C> biLM, First layer <C> 67.4 <R> <C> biLM, Second layer <C> [ITALIC] 69.0 <CAP> Table 5: All-words fine grained WSD F1. For CoVe and the biLM, we report scores for both the first and second layer biLSTMs.
<R> <C> [BOLD] Dataset SNLI <C> [BOLD] Dataset SNLI <C> [BOLD] Before tuning 72.1 <C> [BOLD] After tuning 16.8 <R> <C> CoNLL 2012 (coref/SRL) <C> CoNLL 2012 (coref/SRL) <C> 92.3 <C> - <R> <C> CoNLL 2003 (NER) <C> CoNLL 2003 (NER) <C> 103.2 <C> 46.3 <R> <C> SQuAD <C> Context <C> 99.1 <C> 43.5 <R> <C> SQuAD <C> Questions <C> 158.2 <C> 52.0 <R> <C> SST <C> SST <C> 131.5 <C> 78.6 <CAP> Table 7: Development set perplexity before and after fine tuning for one epoch on the training set for various datasets (lower is better). Reported values are the average of the forward and backward perplexities.
<R> <C> [BOLD] Model <C> [BOLD] EM <C> [BOLD] F1 <R> <C> BiDAF (Seo et al.,  2017 ) <C> 68.0 <C> 77.3 <R> <C> BiDAF + Self Attention <C> 72.1 <C> 81.1 <R> <C> DCN+ <C> 75.1 <C> 83.1 <R> <C> Reg-RaSoR <C> 75.8 <C> 83.3 <R> <C> FusionNet <C> 76.0 <C> 83.9 <R> <C> r-net (Wang et al.,  2017 ) <C> 76.5 <C> 84.3 <R> <C> SAN (Liu et al.,  2017 ) <C> 76.8 <C> 84.4 <R> <C> BiDAF + Self Attention + ELMo <C> [BOLD] 78.6 <C> [BOLD] 85.8 <R> <C> DCN+ Ensemble <C> 78.9 <C> 86.0 <R> <C> FusionNet Ensemble <C> 79.0 <C> 86.0 <R> <C> Interactive AoA Reader+ Ensemble <C> 79.1 <C> 86.5 <R> <C> BiDAF + Self Attention + ELMo Ensemble <C> [BOLD] 81.0 <C> [BOLD] 87.4 <CAP> Table 9: Test set results for SQuAD, showing both Exact Match (EM) and F1. The top half of the table contains single model results with ensembles at the bottom. References provided where available.
<R> <C> [BOLD] Model <C> [BOLD] F1 <R> <C> Pradhan et al. ( 2013 ) <C> 77.5 <R> <C> Zhou and Xu ( 2015 ) <C> 81.3 <R> <C> He et al. ( 2017 ), single <C> 81.7 <R> <C> He et al. ( 2017 ), ensemble <C> 83.4 <R> <C> He et al. ( 2017 ), our impl. <C> 81.4 <R> <C> He et al. ( 2017 ) + ELMo <C> [BOLD] 84.6 <CAP> Table 10: SRL CoNLL 2012 test set F1.
<R> <C> [BOLD] Model <C> [BOLD] Average F1 <R> <C> Durrett and Klein ( 2013 ) <C> 60.3 <R> <C> Wiseman et al. ( 2016 ) <C> 64.2 <R> <C> Clark and Manning ( 2016 ) <C> 65.7 <R> <C> Lee et al. ( 2017 ) (single) <C> 67.2 <R> <C> Lee et al. ( 2017 ) (ensemble) <C> 68.8 <R> <C> Lee et al. ( 2017 ) + ELMo <C> [BOLD] 70.4 <CAP> Table 11: Coreference resolution average F1 on the test set from the CoNLL 2012 shared task.
<R> <C> Model <C> R1 <C> R2 <C> RL <C> R-mean <C> BLEU <R> <C> PGN <C> 39.6 <C> 20.8 <C> 35.2 <C> 31.9 <C> 52.1 <R> <C> mBART <C> 41.7 <C> 22.7 <C> 37.2 <C> 33.9 <C> 53.2 <R> <C> BertSumAbs <C> 41.9 <C> 22.5 <C> 37.3 <C> 33.9 <C> 54.2 <CAP> Table 3: Evaluation on RIA articles from 2020
<R> <C> [BOLD] Algorithm <C> [BOLD] Test scores <R> <C> Full <C> 0.745 <R> <C> No embeddings <C> 0.660 <R> <C> No pre-processing <C> 0.678 <CAP> Table 2: Final results
<R> <C> [BOLD] Model <C> [BOLD] Validation  [BOLD] P <C> [BOLD] Validation  [BOLD] WR <C> [BOLD] Validation  [BOLD] WF <C> [BOLD] Dev  [BOLD] P <C> [BOLD] Dev  [BOLD] WR <C> [BOLD] Dev  [BOLD] WF <C> [BOLD] Test  [BOLD] P <C> [BOLD] Test  [BOLD] WR <C> [BOLD] Test  [BOLD] WF <R> <C> Fairseq Baseline (en-hu) <C> - <C> - <C> - <C> 19.35 <C> 12.47 <C> 13.02 <C> 18.3 <C> 11.8 <C> 12.17 <R> <C> AWS Baseline (en-hu) <C> - <C> - <C> - <C> [BOLD] 84.6 <C> 19.9 <C> 29.85 <C> [BOLD] 86.8 <C> 18.9 <C> 28.1 <R> <C> Fine-tuned Transformer (en-hu) <C> 75.14 <C> 50.34 <C> 56.72 <C> 75.2 <C> [BOLD] 55.2 <C> [BOLD] 59.8 <C> 75.5 <C> [BOLD] 49.2 <C> [BOLD] 55.08 <R> <C> Fairseq Baseline (en-pt) <C> - <C> - <C> - <C> 29.86 <C> 13.3 <C> 15.14 <C> 28.2 <C> 11.7 <C> 13.57 <R> <C> AWS Baseline (en-pt) <C> - <C> - <C> - <C> [BOLD] 86.8 <C> 14.09 <C> 21.15 <C> [BOLD] 87.8 <C> 13.9 <C> 21.3 <R> <C> Fine-tuned Transformer (en-pt) <C> 72.14 <C> 49.22 <C> 54.25 <C> 69.96 <C> [BOLD] 52.55 <C> [BOLD] 55.03 <C> 72.06 <C> [BOLD] 50.11 <C> [BOLD] 54.39 <CAP> Table 2: Final submission results. Bold indicates best performance. P=Precision. WR=Weighted Recall. WF=Weighted Macro F1.
<R> <C> [EMPTY] <C> [ITALIC] ρ <C> [ITALIC] p <R> <C> federal <C> 0.131 <C> 0.001 <R> <C> spine <C> 0.195 <C> 0.032 <R> <C> optical <C> 0.227 <C> 0.003 <R> <C> compact <C> 0.229 <C> 0.002 <R> <C> signal <C> 0.233 <C> 0.008 <R> <C> leaf <C> 0.252 <C> 0.001 <R> <C> net <C> 0.361 <C> 0.001 <R> <C> coach <C> 0.433 <C> 0.007 <R> <C> sphere <C> 0.446 <C> 0.002 <R> <C> mirror <C> 0.454 <C> 0.027 <R> <C> card <C> 0.358 <C> 0.055 <R> <C> virus <C> 0.271 <C> 0.159 <R> <C> disk <C> 0.183 <C> 0.211 <R> <C> brick <C> 0.203 <C> 0.263 <R> <C> virtual <C> -0.085 <C> 0.561 <R> <C> energy <C> 0.002 <C> 0.990 <CAP> Table 2: Correlation results per word.
<R> <C> [EMPTY] <C> Many-to-many <C> One-to-many <C> Many-to-one <C> One-to-one <C> HNMT <C> Improvement <R> <C> am <C> 1.51 <C> 1.65 <C> 1.32 <C> 1.15 <C> [BOLD] 2.02 <C> 0.51 <R> <C> ar <C> 6.08 <C> 6.43 <C> 5.41 <C> 4.90 <C> [BOLD] 7.15 <C> 1.06 <R> <C> aym <C> 4.08 <C> 4.28 <C> 3.43 <C> 3.04 <C> [BOLD] 5.07 <C> 0.98 <R> <C> bg <C> 4.22 <C> 4.38 <C> 3.55 <C> 3.23 <C> [BOLD] 5.59 <C> 1.36 <R> <C> bn <C> 8.92 <C> 9.11 <C> 7.85 <C> 7.53 <C> [BOLD] 10.28 <C> 1.36 <R> <C> ca <C> 5.62 <C> 5.96 <C> 4.73 <C> 4.43 <C> [BOLD] 7.33 <C> 1.71 <R> <C> cs <C> 4.76 <C> 4.90 <C> 3.99 <C> 3.89 <C> [BOLD] 6.04 <C> 1.28 <R> <C> da <C> 4.49 <C> 4.74 <C> 3.79 <C> 3.64 <C> [BOLD] 5.57 <C> 1.08 <R> <C> de <C> 7.57 <C> 7.76 <C> 6.08 <C> 6.20 <C> [BOLD] 8.98 <C> 1.42 <R> <C> el <C> 7.54 <C> 7.78 <C> 6.43 <C> 6.06 <C> [BOLD] 8.78 <C> 1.25 <R> <C> en <C> 11.26 <C> 11.43 <C> 9.92 <C> 9.73 <C> [BOLD] 12.51 <C> 1.25 <R> <C> eo <C> 2.33 <C> 2.66 <C> 1.75 <C> 1.75 <C> [BOLD] 3.12 <C> 0.78 <R> <C> es <C> 11.61 <C> 11.82 <C> 10.45 <C> 10.22 <C> [BOLD] 13.35 <C> 1.74 <R> <C> fa <C> 3.74 <C> 3.99 <C> 3.14 <C> 2.85 <C> [BOLD] 4.64 <C> 0.90 <R> <C> fil <C> 2.60 <C> 2.91 <C> 2.09 <C> 1.97 <C> [BOLD] 3.31 <C> 0.71 <R> <C> fr <C> 8.72 <C> 8.90 <C> 7.70 <C> 7.29 <C> [BOLD] 10.27 <C> 1.55 <R> <C> he <C> 1.21 <C> 1.36 <C> 0.92 <C> 0.95 <C> [BOLD] 1.65 <C> 0.43 <R> <C> hi <C> 1.71 <C> 1.75 <C> 1.44 <C> 1.30 <C> [BOLD] 2.32 <C> 0.61 <R> <C> hu <C> 4.50 <C> 4.79 <C> 3.61 <C> 3.40 <C> [BOLD] 5.46 <C> 0.97 <R> <C> id <C> 4.05 <C> 4.21 <C> 3.04 <C> 3.02 <C> [BOLD] 4.87 <C> 0.82 <R> <C> it <C> 7.84 <C> 8.04 <C> 6.73 <C> 6.47 <C> [BOLD] 9.35 <C> 1.51 <R> <C> jp <C> 6.53 <C> 6.65 <C> 5.48 <C> 5.12 <C> [BOLD] 7.47 <C> 0.95 <R> <C> km <C> 0.81 <C> 0.96 <C> 0.64 <C> 0.57 <C> [BOLD] 1.22 <C> 0.41 <R> <C> ko <C> 3.67 <C> 4.05 <C> 2.99 <C> 2.79 <C> [BOLD] 4.58 <C> 0.90 <R> <C> mg <C> 8.67 <C> 8.90 <C> 7.64 <C> 7.24 <C> [BOLD] 9.55 <C> 0.88 <R> <C> mk <C> 6.14 <C> 6.37 <C> 5.45 <C> 4.87 <C> [BOLD] 7.67 <C> 1.52 <R> <C> my <C> 1.51 <C> 1.74 <C> 1.31 <C> 1.09 <C> [BOLD] 2.15 <C> 0.64 <R> <C> nl <C> 6.21 <C> 6.42 <C> 5.28 <C> 5.05 <C> [BOLD] 7.57 <C> 1.35 <R> <C> or <C> 0.40 <C> 0.43 <C> 0.29 <C> 0.29 <C> [BOLD] 0.48 <C> 0.09 <R> <C> pl <C> 6.57 <C> 6.77 <C> 5.39 <C> 5.28 <C> [BOLD] 7.86 <C> 1.29 <R> <C> pt <C> 6.95 <C> 7.18 <C> 5.86 <C> 5.46 <C> [BOLD] 8.71 <C> 1.77 <R> <C> ro <C> 3.33 <C> 3.53 <C> 2.78 <C> 2.58 <C> [BOLD] 4.33 <C> 1.00 <R> <C> ru <C> 8.16 <C> 8.34 <C> 7.00 <C> 6.81 <C> [BOLD] 9.61 <C> 1.45 <R> <C> sq <C> 4.03 <C> 4.31 <C> 3.54 <C> 3.25 <C> [BOLD] 5.26 <C> 1.22 <R> <C> sr <C> 5.39 <C> 5.62 <C> 4.38 <C> 4.22 <C> [BOLD] 6.63 <C> 1.24 <R> <C> sv <C> 4.88 <C> 5.06 <C> 3.93 <C> 3.75 <C> [BOLD] 6.07 <C> 1.19 <R> <C> sw <C> 5.22 <C> 5.47 <C> 4.43 <C> 3.98 <C> [BOLD] 6.17 <C> 0.95 <R> <C> tr <C> 3.54 <C> 3.68 <C> 2.99 <C> 2.66 <C> [BOLD] 4.29 <C> 0.74 <R> <C> ur <C> 3.61 <C> 3.76 <C> 2.98 <C> 2.77 <C> [BOLD] 4.48 <C> 0.87 <R> <C> zhs <C> 6.22 <C> 6.43 <C> 5.10 <C> 4.90 <C> [BOLD] 7.19 <C> 0.97 <R> <C> zht <C> 6.41 <C> 6.58 <C> 5.38 <C> 5.20 <C> [BOLD] 7.39 <C> 0.98 <R> <C> Average <C> 5.19 <C> 5.39 <C> 4.40 <C> 4.17 <C> [BOLD] 6.25 <C> 1.07 <CAP> Table 1: BLEU scores obtained by each of the baselines and the proposed model for each language considered in the study. Improvement denotes the difference with respect to the bilingual machine translation baseline (Many-to-many). Note that values shown for each language correspond to the average BLEU obtained using the language as source with respect to all other languages used as target. Language names are described using ISO-639 notation.
<R> <C> [BOLD] System <C> [BOLD] 3WT <C> [BOLD] nt2016 <C> [BOLD] nt2017 <R> <C> [BOLD] TR→EN ( [ITALIC] Pdrop=0.3) <C> [BOLD] TR→EN ( [ITALIC] Pdrop=0.3) <C> [BOLD] TR→EN ( [ITALIC] Pdrop=0.3) <C> [BOLD] TR→EN ( [ITALIC] Pdrop=0.3) <R> <C> (E1) Baseline (200K) <C> × <C> 14.2 <C> 14.2 <R> <C> (E2) E1 + 150K-BT <C> × <C> 16.6 <C> 16.1 <R> <C> (E3) E1 + 150K-BT ( [ITALIC] Pdrop=0.2) <C> × <C> 16.4 <C> 16.3 <R> <C> Ensemble (2xE2 + 2xE3) <C> × <C> 18.1 <C> [BOLD] 17.9 <R> <C> [BOLD] EN→TR ( [ITALIC] Pdrop=0.2) <C> [BOLD] EN→TR ( [ITALIC] Pdrop=0.2) <C> [BOLD] EN→TR ( [ITALIC] Pdrop=0.2) <C> [BOLD] EN→TR ( [ITALIC] Pdrop=0.2) <R> <C> (T1) Baseline (200K) <C> × <C> 10.9 <C> 11.1 <R> <C> (T2) T1 + 150K-BT <C> × <C> 12.7 <C> 13.6 <R> <C> (T3) T1 + 150K-BT + Init0 <C> × <C> 12.8 <C> 13.5 <R> <C> (T4) Baseline (200K) <C> ✓ <C> 11.5 <C> 11.6 <R> <C> (T5) T4 + 150K-BT <C> ✓ <C> 13.4 <C> 14.2 <R> <C> (T6) T4 + 150K-BT + Init0 <C> ✓ <C> 13.3 <C> 14.0 <R> <C> Ensemble (2xT5 + 2xT6) <C> ✓ <C> 14.7 <C> [BOLD] 16.0 <CAP> Table 2: EN↔TR: Underlined and bold scores represent contrastive and primary submissions respectively.
<R> <C> [BOLD] System <C> [BOLD] # Params <C> [BOLD] nt2016 <C> [BOLD] nt2017 <R> <C> EN→DE Baseline <C> 35.0M <C> 29.11 <C> 23.26 <R> <C> + synthetic <C> [EMPTY] <C> 31.08 <C> 24.94 <R> <C> + primary ensemble <C> [EMPTY] <C> 33.89 <C> [BOLD] 26.60 <R> <C> DE→EN Baseline <C> 52.9M <C> 33.13 <C> 29.42 <R> <C> primary ensemble (No-BT) <C> [EMPTY] <C> 33.63 <C> [ITALIC] 30.10 <R> <C> + synthetic <C> [EMPTY] <C> 37.36 <C> 32.20 <R> <C> post-deadline ensemble (BT) <C> [EMPTY] <C> [BOLD] 39.07 <C> [BOLD] 33.90 <CAP> Table 5: BLEU scores computed with mteval-v13a.pl for EN↔DE systems on newstest2016 and newstest2017.
<R> <C> model <C> LM <C> label unit <C> WER[%] SWB <C> WER[%] CH <C> WER[%] Hub5’01 <R> <C> LF MMI, 2016  <C> 4-gram <C> CDp <C> 9.6 <C> 19.3 <C> [EMPTY] <R> <C> hybrid <C> 4-gram <C> CDp <C> 9.8 <C> 19.0 <C> 14.7 <R> <C> hybrid <C> LSTM <C> CDp <C> [BOLD] 8.3 <C> [BOLD] 17.3 <C> 12.9 <R> <C> CTC1, 2014  <C> RNN <C> chars <C> 20.0 <C> 31.8 <C> [EMPTY] <R> <C> CTC, 2015  <C> none <C> chars <C> 38.0 <C> 56.1 <C> [EMPTY] <R> <C> CTC, 2015  <C> RNN <C> chars <C> 21.4 <C> 40.2 <C> [EMPTY] <R> <C> attention, 2016  <C> none <C> chars <C> 32.8 <C> 52.7 <C> [EMPTY] <R> <C> attention, 2016  <C> 5-gram <C> chars <C> 30.5 <C> 50.4 <C> [EMPTY] <R> <C> attention, 2016  <C> none <C> words <C> 26.8 <C> 48.2 <C> [EMPTY] <R> <C> attention, 2016  <C> 3-gram <C> words <C> 25.8 <C> 46.0 <C> [EMPTY] <R> <C> CTC, 2017  <C> none <C> chars <C> 24.7 <C> 37.1 <C> [EMPTY] <R> <C> CTC, 2017  <C> [ITALIC] n-gram <C> chars <C> 19.8 <C> 32.1 <C> [EMPTY] <R> <C> CTC2, 2017  <C> word RNN <C> chars <C> 14.0 <C> [BOLD] 25.3 <C> [EMPTY] <R> <C> attention, 2017  <C> none <C> chars <C> 23.1 <C> 40.8 <C> [EMPTY] <R> <C> attention <C> none <C> BPE 10K <C> 13.5 <C> 27.1 <C> 19.9 <R> <C> attention <C> none <C> BPE 1K <C> 13.1 <C> 26.1 <C> 19.7 <R> <C> attention <C> LSTM <C> BPE 1K <C> [BOLD] 11.8 <C> 25.7 <C> 18.1 <CAP> Table 2: Comparisons on Switchboard 300h. The hybrid HMM/NN model is a 6 layer deep bidirectional LSTM. The attention model has a 6 layer deep bidirectional LSTM encoder and a 1 layer LSTM decoder. CDp are (clustered) context-dependend phones. Byte-pair encoding (BPE) are sub-word units. SWB and CH are from Hub5’00. 1added noise from external data. 2added the lexicon, i.e. also additional data.
<R> <C> [EMPTY] <C> CaTeRS <C> RED <R> <C> [BOLD] Token Sentence Distance (%) <C> [BOLD] Token Sentence Distance (%) <C> [BOLD] Token Sentence Distance (%) <R> <C> 0 <C> 39.20 <C> 88.24 <R> <C> 1 <C> 46.67 <C> 5.75 <R> <C> 2 <C> 7.02 <C> 2.00 <R> <C> 3 <C> 3.53 <C> 0.83 <R> <C> ≥ 4 <C> 3.57 <C> 3.41 <CAP> Table 2: Token sentence distance breakdown. 0: a pair of events in the same sentence; 1: a pair of events in the neighboring sentence (2 sentence span); 2: a pair of events in 3 sentence span, etc.
<R> <C> Pair <C> Human <C> LRA <C> NLRA <R> <C> laugh:happiness <C> 50 <C> 0.217 <C> 0.578 <R> <C> nod:agreement <C> 46 <C> 0.245 <C> 0.347 <R> <C> tears:sadness <C> 44 <C> 0.381 <C> 0.483 <R> <C> ⋯ <C> ⋯ <C> ⋯ <C> ⋯ <R> <C> scream:terror <C> 26 <C> 0.396 <C> 0.417 <R> <C> handshake:cordiality <C> 24 <C> [BOLD] 0 (no pattern) <C> 0.34 <R> <C> lie:dishonesty <C> 16 <C> 0.206 <C> 0.394 <R> <C> ⋯ <C> ⋯ <C> ⋯ <C> ⋯ <R> <C> discourse:relationship <C> -60 <C> 0.331 <C> 0.275 <R> <C> friendliness:wink <C> -68 <C> [BOLD] 0 (no pattern) <C> 0.26 <CAP> Table 2: The scores assigned by humans, LRA, and NLRA for the Reference-Express relation. The pairs are sorted in descending order according to the human score.
<R> <C> Model <C> Accuracy <C> Correlation <R> <C> rink2012 <C> 0.394 <C> 0.229 <R> <C> mikolov2013 <C> 0.418 <C> 0.275 <R> <C> levy2014 <C> 0.452 <C> – <R> <C> zhila2013 <C> 0.452 <C> 0.353 <R> <C> iacobacci2015 <C> 0.459 <C> 0.358 <R> <C> turney2013 <C> 0.472 <C> [BOLD] 0.408 <R> <C> VecOff <C> 0.443 <C> 0.321 <R> <C> LRA <C> 0.415 <C> 0.264 <R> <C> NLRA <C> 0.453 <C> 0.36 <R> <C> NLRA+VecOff <C> [BOLD] 0.475 <C> 0.391 <CAP> Table 3: Published results of other models on the SemEval2012 Task 2 dataset.
<R> <C> Model <C> PLDA Train Dataset <C> SITW Core-Core EER (%) <C> SITW Core-Core minDCF <C> VOiCES Dev EER (%) <C> VOiCES Dev minDCF <C> VOiCES Eval EER (%) <C> VOiCES Eval minDCF <R> <C> GPLDA <C> VoxCeleb <C> 2.79 <C> 0.29 <C> 2.79 <C> 0.31 <C> 7.35 <C> 0.57 <R> <C> GPLDA <C> VoxCeleb Augmented <C> 2.79 <C> 0.29 <C> 2.79 <C> 0.30 <C> 6.38 <C> 0.53 <R> <C> Gaussian Backend <C> VoxCeleb <C> 3.19 <C> 0.31 <C> 3.14 <C> 0.33 <C> 7.58 <C> 0.60 <R> <C> Gaussian Backend <C> VoxCeleb Augmented <C> 3.06 <C> 0.31 <C> 2.89 <C> 0.30 <C> 6.63 <C> 0.53 <R> <C> DPLDA <C> VoxCeleb Augmented <C> 2.98 <C> 0.32 <C> 3.05 <C> 0.36 <C> 6.65 <C> 0.56 <R> <C> NPLDA <C> VoxCeleb <C> 2.14 <C> 0.23 <C> 2.20 <C> 0.26 <C> 6.72 <C> 0.51 <R> <C> NPLDA <C> VoxCeleb Augmented <C> [BOLD] 2.05 <C> [BOLD] 0.20 <C> [BOLD] 1.91 <C> [BOLD] 0.23 <C> [BOLD] 6.01 <C> [BOLD] 0.49 <R> <C> NPLDA (BCE Loss) <C> VoxCeleb Augmented <C> 2.10 <C> 0.22 <C> 2.32 <C> 0.26 <C> 6.34 <C> 0.53 <CAP> Table 1: Performance of systems on SITW Eval Core-Core, VOiCES Dev and VOiCES Eval using the GPLDA baseline model, Gaussian backend, Discriminative PLDA (DPLDA) and the proposed NPLDA model. We also report the use of binary cross-entropy (BCE) Loss in the NPLDA model place of the soft detection cost. The best scores are highlighted.
<R> <C> System <C> Domain <C> EER [%] <C> minDCF08 <C> minDCF10 <R> <C> [EMPTY] <C> out <C> 1.80 <C> 0.085 <C> 0.256 <R> <C> DNN-LDA <C> in <C> 0.78 <C> 0.034 <C> 0.149 <R> <C> [EMPTY] <C> in+out <C> 0.75 <C> 0.032 <C> 0.125 <R> <C> [EMPTY] <C> out <C> 1.55 <C> 0.071 <C> 0.226 <R> <C> DNN-NDA <C> in <C> 0.75 <C> 0.033 <C> 0.119 <R> <C> [EMPTY] <C> in+out <C> [BOLD] 0.59 <C> [BOLD] 0.025 <C> [BOLD] 0.095 <CAP> Table 2: Comparison of LDA vs NDA for in-domain and out-of-domain training on C5, with fMLLR features and 10k senones.
<R> <C> Attack <C> Flip white <C> Flip black <C> Insert white <C> Insert black <C> Delete white <C> Delete black <C> Swap white <C> Swap black <R> <C> FR <C> [BOLD] 4.27 <C> 6.98 <C> [BOLD] 4.74 <C> 4.85 <C> [BOLD] 4.99 <C> 5.86 <C> [BOLD] 4.87 <C> 5.20 <R> <C> DE <C> [BOLD] 4.50 <C> 6.87 <C> [BOLD] 3.91 <C> 4.31 <C> [BOLD] 5.63 <C> 5.73 <C> 4.94 <C> [BOLD] 4.74 <R> <C> CS <C> [BOLD] 4.31 <C> 6.09 <C> [BOLD] 4.66 <C> 5.86 <C> [BOLD] 6.30 <C> 6.62 <C> 6.05 <C> [BOLD] 5.82 <CAP> Table 3: BLEU score after greedy decoding in the existence of different types of untargeted attacks.
<R> <C> TrainingTest French <C> TrainingTest Vanilla <C> Clean 37.54 <C> Nat 19.17 <C> Key 12.12 <C> Rand 4.75 <C> FIDS-B 6.85 <C> FIDS-W 5.36 <C> Avg. 15.95 <R> <C> French <C> Nat <C> 26.35 <C> [BOLD] 33.23 <C> 11.16 <C> 5.32 <C> 8.28 <C> 6.65 <C> 15.16 <R> <C> French <C> Key <C> 33.02 <C> 17.30 <C> [BOLD] 35.97 <C> 4.63 <C> 7.00 <C> 5.17 <C> 17.17 <R> <C> French <C> Rand <C> 36.06 <C> 18.54 <C> 8.31 <C> [BOLD] 36.10 <C> 8.76 <C> 7.14 <C> 19.14 <R> <C> French <C> FIDS-B <C> 34.48 <C> 21.59 <C> 28.48 <C> 6.82 <C> 32.62 <C> 13.60 <C> 22.92 <R> <C> French <C> FIDS-W <C> 37.15 <C> 23.65 <C> 31.18 <C> 7.78 <C> [BOLD] 32.72 <C> [BOLD] 31.94 <C> 27.40 <R> <C> French <C> Rand+Key+Nat <C> 34.55 <C> 30.74 <C> 32.82 <C> 34.01 <C> 12.05 <C> 7.08 <C> 25.20 <R> <C> French <C> Ensemble <C> [BOLD] 37.81 <C> 30.27 <C> 29.36 <C> 34.42 <C> 32.00 <C> 30.01 <C> [BOLD] 32.30 <R> <C> German <C> Vanilla <C> 31.81 <C> 17.24 <C> 10.36 <C> 4.20 <C> 6.78 <C> 5.50 <C> 12.64 <R> <C> German <C> Nat <C> 24.89 <C> [BOLD] 32.14 <C> 10.22 <C> 4.61 <C> 7.53 <C> 5.99 <C> 14.23 <R> <C> German <C> Key <C> 27.20 <C> 15.98 <C> [BOLD] 30.62 <C> 4.64 <C> 7.68 <C> 4.74 <C> 15.13 <R> <C> German <C> Rand <C> 31.01 <C> 17.90 <C> 6.59 <C> [BOLD] 30.70 <C> 9.19 <C> 5.83 <C> 16.86 <R> <C> German <C> FIDS-B <C> 28.27 <C> 20.22 <C> 23.84 <C> 6.29 <C> 27.35 <C> 10.79 <C> 19.45 <R> <C> German <C> FIDS-W <C> [BOLD] 31.81 <C> 21.72 <C> 26.23 <C> 7.75 <C> [BOLD] 27.38 <C> [BOLD] 26.51 <C> 23.56 <R> <C> German <C> Rand+Key+Nat <C> 29.22 <C> 29.78 <C> 27.83 <C> 28.88 <C> 10.30 <C> 6.14 <C> 22.01 <R> <C> German <C> Ensemble <C> 31.54 <C> 31.11 <C> 23.91 <C> 28.95 <C> 26.38 <C> 25.06 <C> [BOLD] 27.82 <R> <C> Czech <C> Vanilla <C> [BOLD] 26.44 <C> 13.55 <C> 9.49 <C> 4.78 <C> 7.30 <C> 5.93 <C> 11.24 <R> <C> Czech <C> Nat <C> 18.73 <C> [BOLD] 23.06 <C> 9.07 <C> 4.45 <C> 7.36 <C> 5.42 <C> 11.34 <R> <C> Czech <C> Key <C> 22.76 <C> 13.09 <C> [BOLD] 23.79 <C> 4.83 <C> 7.93 <C> 5.82 <C> 13.03 <R> <C> Czech <C> Rand <C> 24.23 <C> 12.00 <C> 7.26 <C> [BOLD] 24.53 <C> 7.24 <C> 5.47 <C> 13.45 <R> <C> Czech <C> FIDS-B <C> 22.31 <C> 14.15 <C> 17.91 <C> 6.48 <C> 19.67 <C> 8.60 <C> 14.84 <R> <C> Czech <C> FIDS-W <C> 25.53 <C> 15.57 <C> 19.74 <C> 7.18 <C> [BOLD] 20.02 <C> [BOLD] 19.42 <C> 17.90 <R> <C> Czech <C> Rand+Key+Nat <C> 22.21 <C> 20.59 <C> 20.60 <C> 21.33 <C> 10.06 <C> 5.89 <C> 16.77 <R> <C> Czech <C> Ensemble <C> 25.45 <C> 20.46 <C> 17.15 <C> 21.39 <C> 18.52 <C> 17.03 <C> [BOLD] 19.99 <CAP> Table 6: BLEU score of models on clean and adversarial examples, using a decoder with beam size of 4. The best result on each test set is shown in bold. FIDS-W performs best on all noisy test sets compared with models which have not been trained on that particular noise (shown in red). FIDS-B performs best on white-box adversarial examples compared with other black-box trained models (shown in blue).
<R> <C> Dataset <C> Type <C> Points <C> Features <C> Clusters <C> Balance <C> Mean Clust <C> Min Cl Size <C> Max Cl Size <R> <C> NIPS★ <C> text <C> 424 <C> 17522 <C> 9 <C> 0.105 <C> 47.1 <C> 15 <C> 143 <R> <C> NIPS <C> text <C> 451 <C> 17522 <C> 13 <C> 0.028 <C> 34.7 <C> 4 <C> 143 <R> <C> Reuters★ <C> text <C> 8095 <C> 14143 <C> 20 <C> 0.011 <C> 404.8 <C> 42 <C> 3735 <R> <C> Reuters <C> text <C> 8654 <C> 14333 <C> 65 <C> 0 <C> 133.1 <C> 1 <C> 3735 <R> <C> RCV1★ <C> text <C> 13254 <C> 20478 <C> 40 <C> 0.028 <C> 331.4 <C> 45 <C> 1587 <R> <C> RCV1 <C> text <C> 13732 <C> 20478 <C> 75 <C> 0.001 <C> 183.1 <C> 1 <C> 1587 <R> <C> PIE-Expr <C> image <C> 232 <C> 4096 <C> 68 <C> 0.75 <C> 3.4 <C> 3 <C> 4 <R> <C> ORL <C> image <C> 400 <C> 5796 <C> 40 <C> 1 <C> 10 <C> 10 <C> 10 <R> <C> COIL-20 <C> image <C> 1440 <C> 4096 <C> 20 <C> 1 <C> 72 <C> 72 <C> 72 <R> <C> ExtYaleB <C> image <C> 2447 <C> 3584 <C> 38 <C> 0.678 <C> 64.4 <C> 59 <C> 87 <CAP> TABLE I: Datasets description. ★ = the dataset has been pruned as in [13].
<R> <C> Approach <C> UK Petitions MAE <C> UK Petitions MAPE <C> US Petitions MAE <C> US Petitions MAPE <R> <C> Mean <C> 4.37 <C> 159.7 <C> 2.82 <C> 44.61 <R> <C> LinearBoW <C> 1.75 <C> 57.56 <C> 2.51 <C> 37.01 <R> <C> LinearGI <C> 1.77 <C> 58.22 <C> 1.84 <C> 27.71 <R> <C> SVRBoW <C> 1.53 <C> 45.35 <C> 1.39 <C> 20.37 <R> <C> SVRfeat <C> 1.54 <C> 46.96 <C> 1.40 <C> 20.48 <R> <C> SVRBoW+feat <C> 1.52 <C> 44.71 <C> 1.39 <C> 20.38 <R> <C> CNNregress <C> 1.44 <C> 36.72 <C> 1.24 <C> 14.98 <R> <C> CNNregress+ord <C> 1.42 <C> 33.86 <C> 1.22 <C> 14.68 <R> <C> CNNregress+ord+feat <C> 1.41 <C> 32.92 <C> 1.20 <C> 14.47 <R> <C> CNNregress+feat <C> 1.43 <C> 35.84 <C> 1.23 <C> 14.75 <R> <C> CNNregress+ord+feat + Additional hidden layer <C> [BOLD] 1.40 <C> [BOLD] 31.68 <C> [BOLD] 1.16 <C> [BOLD] 14.38 <CAP> Table 1: Results over UK and US Government petition datasets. Best scores are given in bold.
<R> <C> [EMPTY] <C> [BOLD] DE→EN in-domain <C> [BOLD] DE→EN average OOD <C> [BOLD] DE→RM in-domain <C> [BOLD] DE→RM average OOD <R> <C> [BOLD] Baselines <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> (1) SMT <C> 58.4 <C> 11.8 <C> 45.2 <C> 15.5 <R> <C> (2) NMT <C> 61.5 <C> 11.7 <C> 52.5 <C> 18.9 <R> <C> (3) NMT + SR <C> 61.4 <C> 11.2 <C> [BOLD] 53.7 <C> 20.1 <R> <C> (4) NMT + D <C> 61.1 <C> 13.1 <C> 52.5 <C> 19.3 <R> <C> (5) Multilingual <C> 61.4 <C> 11.7 <C> 52.8 <C> 19.6 <R> <C> (6) Reconstruction <C> 61.5 <C> 12.5 <C> 53.4 <C> 21.2 <R> <C> (7) Multilingual + SR <C> 60.3 <C> 12.8 <C> 52.4 <C> 20.1 <R> <C> (8) Reconstruction + SR <C> 60.3 <C> [BOLD] 13.2 <C> 52.4 <C> 20.3 <R> <C> (9) Multilingual + NC <C> 62.7 <C> 11.8 <C> 53.1 <C> 21.4 <R> <C> (10) Reconstruction + NC <C> [BOLD] 62.8 <C> 13.0 <C> 53.3 <C> [BOLD] 21.6 <R> <C> (11) Multilingual + SR + NC <C> 60.7 <C> 12.3 <C> 53.1 <C> 21.4 <R> <C> (12) Reconstruction + SR + NC <C> 60.8 <C> 13.1 <C> 52.4 <C> 20.7 <CAP> Table 7: BLEU scores (higher is better) of all systems on test data. SR=Subword Regularization, D=Distillation, NC=Noisy Channel Model, average OOD=average BLEU score over out-of-domain test sets.
<R> <C> [EMPTY] <C> [BOLD] DE→EN  [ITALIC] λtf <C> [BOLD] DE→EN  [ITALIC] λtb <C> [BOLD] DE→EN  [ITALIC] λlm <C> [BOLD] DE→RM  [ITALIC] λtf <C> [BOLD] DE→RM  [ITALIC] λtb <C> [BOLD] DE→RM  [ITALIC] λlm <R> <C> (5) Multilingual <C> 0.7 <C> 0.26 <C> 0.04 <C> 0.9 <C> 0.09 <C> 0.01 <R> <C> (6) Reconstruction <C> 0.6 <C> 0.32 <C> 0.08 <C> 0.9 <C> 0.09 <C> 0.01 <R> <C> (7) Multilingual + SR <C> 0.5 <C> 0.42 <C> 0.08 <C> 0.9 <C> 0.09 <C> 0.01 <R> <C> (8) Reconstruction + SR <C> 0.5 <C> 0.46 <C> 0.04 <C> 0.9 <C> 0.09 <C> 0.01 <CAP> Table 10: Best weights for noisy channel reranking found with grid search on in-domain development set. Row numbers correspond to the ones in Table 7. λtf=forward translation weight, λtb=backward translation weight, λlm= language model weight
<R> <C> [BOLD] Stages in Pipeline  [BOLD] Region Proposal and Text Region Extraction <C> [BOLD] Output Classes  [BOLD] Full Text regions <C> [BOLD] Output Classes  [BOLD] Full Text regions <C> [BOLD] Output Classes  [BOLD] Full Text regions <C> [BOLD] Output Classes  [BOLD] Partial Text regions <C> [BOLD] Output Classes  [BOLD] Partial Text regions <C> [BOLD] Output Classes  [BOLD] Partial Text regions <C> [BOLD] Indicative Accuracies (completely perfect cases only)  [BOLD] Full Text Regions (43/50) <R> <C> [BOLD] Region Proposal and Text Region Extraction <C> 43 <C> 43 <C> 43 <C> 7 <C> 7 <C> 7 <C> 86% <R> <C> [BOLD] Symbol Segmentation <C> [BOLD] Full Symbols <C> [BOLD] Partial/ Combined Symbols <C> [BOLD] NoSymbols <C> [BOLD] Full Symbols <C> [BOLD] Partial/ Combined Symbols <C> [BOLD] No Symbols <C> [BOLD] Full Symbols ((29+5)/50) <R> <C> [BOLD] Symbol Segmentation <C> 29 <C> 11 <C> 3 <C> 5 <C> 2 <C> 0 <C> 68% <CAP> Table 2: The Indus script deep learning pipeline’s empirical analysis results on 50 sampled test seal images
<R> <C> [EMPTY] <C> Model <C> Quora iBLEU <C> Quora BLEU <C> Quora Rouge1 <C> Quora Rouge2 <C> Wikianswers iBLEU <C> Wikianswers BLEU <C> Wikianswers Rouge1 <C> Wikianswers Rouge2 <R> <C> Supervised <C> ResidualLSTM <C> 12.67 <C> 17.57 <C> 59.22 <C> 32.40 <C> 22.94 <C> 27.36 <C> 48.52 <C> 18.71 <R> <C> Supervised <C> VAE-SVG-eq <C> 15.17 <C> 20.04 <C> 59.98 <C> 33.30 <C> 26.35 <C> 32.98 <C> 50.93 <C> 19.11 <R> <C> Supervised <C> Pointer-generator <C> 16.79 <C> 22.65 <C> 61.96 <C> 36.07 <C> 31.98 <C> 39.36 <C> 57.19 <C> 25.38 <R> <C> Supervised <C> Transformer <C> 16.25 <C> 21.73 <C> 60.25 <C> 33.45 <C> 27.70 <C> 33.01 <C> 51.85 <C> 20.70 <R> <C> Supervised <C> Transformer+Copy <C> 17.98 <C> 24.77 <C> 63.34 <C> 37.31 <C> 31.43 <C> 37.88 <C> 55.88 <C> 23.37 <R> <C> Supervised <C> DNPG <C> [BOLD] 18.01 <C> [BOLD] 25.03 <C> [BOLD] 63.73 <C> [BOLD] 37.75 <C> [BOLD] 34.15 <C> [BOLD] 41.64 <C> [BOLD] 57.32 <C> [BOLD] 25.88 <R> <C> Supervised <C> Pointer-generator <C> 5.04 <C> 6.96 <C> 41.89 <C> 12.77 <C> 21.87 <C> 27.94 <C> 53.99 <C> 20.85 <R> <C> Supervised <C> Transformer+Copy <C> 6.17 <C> 8.15 <C> 44.89 <C> 14.79 <C> 23.25 <C> 29.22 <C> 53.33 <C> 21.02 <R> <C> Supervised <C> Shallow fusion <C> 6.04 <C> 7.95 <C> 44.87 <C> 14.79 <C> 22.57 <C> 29.76 <C> 53.54 <C> 20.68 <R> <C> + Domain-adapted <C> MTL <C> 4.90 <C> 6.37 <C> 37.64 <C> 11.83 <C> 18.34 <C> 23.65 <C> 48.19 <C> 17.53 <R> <C> [EMPTY] <C> MTL+Copy <C> 7.22 <C> 9.83 <C> 47.08 <C> 19.03 <C> 21.87 <C> 30.78 <C> 54.10 <C> 21.08 <R> <C> [EMPTY] <C> DNPG <C> 10.39 <C> 16.98 <C> 56.01 <C> 28.61 <C> 25.60 <C> 35.12 <C> 56.17 <C> 23.65 <R> <C> Unsupervised <C> VAE <C> 8.16 <C> 13.96 <C> 44.55 <C> 22.64 <C> 17.92 <C> 24.13 <C> 31.87 <C> 12.08 <R> <C> Unsupervised <C> CGMH <C> 9.94 <C> 15.73 <C> 48.73 <C> 26.12 <C> 20.05 <C> 26.45 <C> 43.31 <C> 16.53 <R> <C> Unsupervised <C> UPSA <C> 12.02 <C> 18.18 <C> 56.51 <C> 30.69 <C> 24.84 <C> 32.39 <C> 54.12 <C> 21.45 <CAP> Table 1: Performance on the Quora and Wikianswers datasets. The results of supervised learning and domain-adapted supervised methods are quoted from zichao2019 zichao2019. We run experiments for all unsupervised methods and use the same evaluation script with zichao2019 zichao2019 for a fair comparison. The results of CGMH in this table is slightly different from miao2018cgmh miao2018cgmh, because miao2018cgmh miao2018cgmh use corpus-level BLEU, while zichao2019 zichao2019 and we use sentence-level BLEU.
<R> <C> Line # <C> UPSA Variant <C> iBLEU <C> BLEU <C> Rouge1 <C> Rouge2 <R> <C> 1 <C> UPSA <C> [BOLD] 12.41 <C> 18.48 <C> 57.06 <C> 31.39 <R> <C> 2 <C> w/o  [ITALIC] fsim,key <C> 10.28 <C> 15.34 <C> 50.85 <C> 26.42 <R> <C> 3 <C> w/o  [ITALIC] fsim,sen <C> 11.78 <C> 17.95 <C> 57.04 <C> 30.80 <R> <C> 4 <C> w/o  [ITALIC] fexp <C> 11.93 <C> 21.17 <C> 59.75 <C> 34.91 <R> <C> 5 <C> w/o copy <C> 11.42 <C> 17.25 <C> 56.09 <C> 29.73 <R> <C> 6 <C> w/o annealing <C> 10.56 <C> 16.52 <C> 56.02 <C> 29.25 <CAP> Table 5: Ablation study.
<R> <C> Accuracy <C> Top 100 <C> Top 200 <C> Top 500 <C> Average <R> <C> Mintz <C> 0.77 <C> 0.71 <C> 0.55 <C> 0.676 <R> <C> MultiR <C> 0.83 <C> 0.74 <C> 0.59 <C> 0.720 <R> <C> MIML <C> 0.85 <C> 0.75 <C> 0.61 <C> 0.737 <R> <C> PCNN+MIL <C> 0.84 <C> 0.77 <C> 0.64 <C> 0.750 <R> <C> PCNN+ATT <C> 0.86 <C> 0.83 <C> 0.73 <C> 0.807 <R> <C> APCNN+D <C> 0.87 <C> 0.83 <C> 0.74 <C> 0.813 <R> <C> Baseline <C> 0.86 <C> 0.84 <C> 0.73 <C> 0.810 <R> <C> SEE-TRANS <C> [BOLD] 0.91 <C> [BOLD] 0.87 <C> [BOLD] 0.77 <C> [BOLD] 0.850 <CAP> Table 1: Manual evaluation results.
<R> <C> [BOLD] Model <C> [BOLD] Accuracy (%) <R> <C> Sun et al. ( 2015 ) <C> 83.90 <R> <C> Raiman and Raiman ( 2018 ) <C> 90.85 <R> <C> Gillick et al. ( 2019 ) <C> 88.62 <R> <C> This work <C> 92.05 <R> <C> Yamada et al. ( 2017 ) <C> 85.20 <R> <C> Nie et al. ( 2018 ) <C> 89.10 <R> <C> Gillick et al. ( 2019 ) <C> 86.86 <R> <C> This work <C> 88.42 <CAP> Table 1: Mention-level accuracy on the TACKBP-2010 test set. Wikipedia is growing immensely with time and the difficulty of the task depends on the number of entities in the knowledge base. The first set of rows use 800k-1M entities and the second set of rows use 5-5.7M entities. This allows for a relatively fair comparison, if not perfectly fair as the number of entities is still not exactly the same.
<R> <C> [ITALIC] n-best listModel <C> SmallVoc <C> SmallVoc.rev <C> BigVoc <C> PrePBMT <C> All <C> Oracle <R> <C> SmallVoc <C> [BOLD] 37.90 <C> 39.50 <C> 39.35 <C> 39.05 <C> 40.30 <C> 56.41 <R> <C> SmallVoc.rev <C> 39.74 <C> [BOLD] 38.72 <C> 39.92 <C> 39.94 <C> 40.80 <C> 56.82 <R> <C> BigVoc <C> 38.73 <C> 39.61 <C> [BOLD] 38.80 <C> 39.51 <C> 40.25 <C> 56.46 <R> <C> PrePBMT <C> 38.91 <C> 39.68 <C> 39.36 <C> [BOLD] 38.33 <C> 40.24 <C> 54.44 <R> <C> Union <C> [BOLD] 37.92 <C> [BOLD] 38.65 <C> [BOLD] 38.81 <C> [BOLD] 38.33 <C> 40.66 <C> 63.09 <CAP> Table 5: Text representation systems: MSLT German→English
<R> <C> [BOLD] Dataset Name <C> [BOLD] Word/Sentence pairs <C> [BOLD] Similarity score range <C> [BOLD] Year <C> [BOLD] Reference <R> <C> R&G <C> 65 <C> 0-4 <C> 1965 <C> (Rubenstein and Goodenough,  1965 ) <R> <C> M&C <C> 30 <C> 0-4 <C> 1991 <C> (Miller and Charles,  1991 ) <R> <C> WS353 <C> 353 <C> 0-10 <C> 2002 <C> (Finkelstein et al.,  2001 ) <R> <C> LiSent <C> 65 <C> 0-4 <C> 2007 <C> (Li et al.,  2006 ) <R> <C> SRS <C> 30 <C> 0-4 <C> 2007 <C> (Pedersen et al.,  2007 ) <R> <C> WS353-Sim <C> 203 <C> 0-10 <C> 2009 <C> (Agirre et al.,  2009 ) <R> <C> STS2012 <C> 5250 <C> 0-5 <C> 2012 <C> (Agirre et al.,  2012 ) <R> <C> STS2013 <C> 2250 <C> 0-5 <C> 2013 <C> (Agirre et al.,  2013 ) <R> <C> WP300 <C> 300 <C> 0-1 <C> 2013 <C> (Li et al.,  2013 ) <R> <C> STS2014 <C> 3750 <C> 0-5 <C> 2014 <C> (Agirre et al.,  2014 ) <R> <C> SL7576 <C> 7576 <C> 1-5 <C> 2014 <C> (Silberer and Lapata,  2014 ) <R> <C> SimLex-999 <C> 999 <C> 0-10 <C> 2014 <C> (Hill et al.,  2015 ) <R> <C> SICK <C> 10000 <C> 1-5 <C> 2014 <C> (Marelli et al.,  2014 ) <R> <C> STS2015 <C> 3000 <C> 0-5 <C> 2015 <C> (Agirre et al.,  2015 ) <R> <C> SimVerb <C> 3500 <C> 0-10 <C> 2016 <C> (Gerz et al.,  2016 ) <R> <C> STS2016 <C> 1186 <C> 0-5 <C> 2016 <C> (Agirre et al.,  2016 ) <R> <C> WiC <C> 5428 <C> [EMPTY] <C> 2019 <C> (Pilehvar and Camacho-Collados,  2019 ) <CAP> Table 1. Popular Benchmark datasets for Semantic similarity
<R> <C> [BOLD] Language Arabic <C> [BOLD] BiAtt-DP 80.34 <C> [BOLD] BiAtt-DP [68.58] <C> [BOLD] RBGParser 79.95 <C> [BOLD] Best Published  [BOLD] 81.12 <C> [BOLD] Best Published (Ma11) <C> [BOLD] Crossed 17.24 <C> [BOLD] Uncrossed 80.71 <C> [BOLD] %Crossed 0.58 <R> <C> Bulgarian <C> 93.96 <C> [89.55] <C> 93.50 <C> [BOLD] 94.02 <C> (Zh14) <C> 79.59 <C> 94.10 <C> 0.98 <R> <C> Czech <C> [BOLD] 91.16 <C> [85.14] <C> 90.50 <C> 90.32 <C> (Ma13) <C> 81.62 <C> 91.63 <C> 4.68 <R> <C> Danish <C> 91.56 <C> [85.53] <C> 91.39 <C> [BOLD] 92.00 <C> (Zh13) <C> 73.33 <C> 91.89 <C> 1.80 <R> <C> Dutch <C> [BOLD] 87.15 <C> [82.41] <C> 86.41 <C> 86.19 <C> (Ma13) <C> 82.82 <C> 87.66 <C> 10.48 <R> <C> German <C> [BOLD] 92.71 <C> [89.80] <C> 91.97 <C> 92.41 <C> (Ma13) <C> 85.93 <C> 92.90 <C> 2.70 <R> <C> Japanese <C> 93.44 <C> [90.67] <C> 93.71 <C> [BOLD] 93.72 <C> (Ma11) <C> 48.67 <C> 94.48 <C> 2.26 <R> <C> Portuguese <C> 92.77 <C> [88.44] <C> 91.92 <C> [BOLD] 93.03 <C> (Ko10) <C> 73.02 <C> 93.28 <C> 2.52 <R> <C> Slovene <C> 86.01 <C> [75.90] <C> 86.24 <C> [BOLD] 86.95 <C> (Ma11) <C> 60.11 <C> 86.99 <C> 3.66 <R> <C> Spanish <C> [BOLD] 88.74 <C> [84.03] <C> 88.00 <C> 87.98 <C> (Zh14) <C> 50.00 <C> 88.77 <C> 0.08 <R> <C> Swedish <C> 90.50 <C> [84.05] <C> 91.00 <C> [BOLD] 91.85 <C> (Zh14) <C> 45.16 <C> 90.78 <C> 0.62 <R> <C> Turkish <C> [BOLD] 78.43 <C> [66.16] <C> 76.84 <C> 77.55 <C> (Ko10) <C> 38.85 <C> 79.71 <C> 3.13 <CAP> Table 3: UAS on 12 languages in the CoNLL 2006 shared task [Buchholz and Marsi2006]. We also report corresponding LAS in squared brackets. The results of the 3rd-order RBGParser are reported in [Lei et al.2014]. Best published results on the same dataset in terms of UAS among [Pitler and McDonald2015], [Zhang and McDonald2014], [Zhang et al.2013], [Zhang and McDonald2012], [Rush and Petrov2012], [Martins et al.2013], [Martins et al.2010], and [Koo et al.2010]. To study the effectiveness of the parser in dealing with non-projectivity, we follow [Pitler and McDonald2015], to compute the recall of crossed and uncrossed arcs in the gold tree, as well as the percentage of crossed arcs.
<R> <C> # <C> Model <C> NL Question P@1 <C> NL Question R@3 <C> NL Question MRR <C> Keyword Query P@1 <C> Keyword Query R@3 <C> Keyword Query MRR <R> <C> 1 <C> Random <C> 0.012 <C> 0.034 <C> – <C> 0.012 <C> 0.034 <C> – <R> <C> 2 <C> BM25 <C> 0.150 <C> 0.216 <C> 0.243 <C> 0.150 <C> 0.216 <C> 0.243 <R> <C> 3 <C> BERT (unsupervised) <C> 0.081 <C> 0.117 <C> 0.159 <C> 0.073 <C> 0.164 <C> 0.187 <R> <C> 4 <C> SciBERT (unsupervised) <C> 0.040 <C> 0.056 <C> 0.099 <C> 0.024 <C> 0.064 <C> 0.094 <R> <C> 5 <C> BioBERT (unsupervised) <C> 0.097 <C> 0.142 <C> 0.170 <C> 0.129 <C> 0.145 <C> 0.185 <R> <C> 6 <C> BERT (fine-tuned on MS MARCO) <C> 0.194 <C> 0.315 <C> 0.329 <C> [BOLD] 0.234 <C> 0.306 <C> 0.342 <R> <C> 7 <C> BioBERT (fine-tuned on SQuAD) <C> 0.161 <C> 0.403 <C> 0.336 <C> 0.056 <C> 0.093 <C> 0.135 <R> <C> 8 <C> BioBERT (fine-tuned on MS MARCO) <C> 0.194 <C> 0.313 <C> 0.312 <C> 0.185 <C> 0.330 <C> 0.322 <R> <C> 9 <C> T5 (fine-tuned on MS MARCO) <C> [BOLD] 0.282 <C> [BOLD] 0.404 <C> [BOLD] 0.415 <C> 0.210 <C> [BOLD] 0.376 <C> [BOLD] 0.360 <CAP> Table 1: Effectiveness of the models examined in this paper.
<R> <C> Sentiment vs. <C> Gov. Member <C> Seats <R> <C> 17th Bundestag <C> 0.84 <C> 0.70 <R> <C> 18th Bundestag <C> 0.98 <C> 0.89 <CAP> Table 9: Correlation coefficient between average sentiment of political speeches of a party in the german Bundestag with two indicators of political power, a) membership in the government and b) the number of seats a party occupies in the parliament.
<R> <C> Methods <C> Hits@1 <C> Hits@10 <C> MRR <R> <C> TransE‡ <C> 13.3 <C> 40.9 <C> 0.22 <R> <C> TransR‡ <C> 10.9 <C> 38.2 <C> 0.20 <R> <C> TransD‡ <C> 17.8 <C> 44.7 <C> 0.27 <R> <C> ComplEx <C> 15.2 <C> 41.9 <C> 0.24 <R> <C> ConvE <C> 23.9 <C> 49.1 <C> 0.31 <R> <C> RotatE <C> [BOLD] 24.1 <C> [BOLD] 53.3 <C> [BOLD] 0.34 <R> <C> RSNs (w/o cross-KG bias) <C> 20.2 <C> 45.3 <C> 0.28 <CAP> Table 8: KG completion results on FB15K-237
<R> <C> Model <C> Laptop source <C> Laptop augmentation <C> Restaurant source <C> Restaurant augmentation <R> <C> BiLSTM-CRF <C> 73.42 <C> [BOLD] 74.28 <C> 69.16 <C> [BOLD] 71.44 <R> <C> Seq2Seq for ATE <C> 76.68 <C> [BOLD] 78.68 <C> 73.71 <C> [BOLD] 74.01 <R> <C> BERT-FTC <C> 79.39 <C> [BOLD] 81.14 <C> 74.75 <C> [BOLD] 75.89 <R> <C> DE-CNN <C> 81.08 <C> [BOLD] 81.58 <C> 74.52 <C> [BOLD] 75.19 <R> <C> BERT-PT <C> 84.59 <C> [BOLD] 85.33 <C> 77.49 <C> [BOLD] 80.29 <CAP> Table 2: F1-score(%) obtained on the tests for various models, where source denotes the original datasets.
<R> <C> [BOLD] Dataset <C> [BOLD] Laptop  [BOLD] Precision <C> [BOLD] Laptop  [BOLD] Recall <C> [BOLD] Laptop  [BOLD] F1 <C> [BOLD] Restaurant  [BOLD] Precision <C> [BOLD] Restaurant  [BOLD] Recall <C> [BOLD] Restaurant  [BOLD] F1 <R> <C> Source <C> 81.24 <C> 80.91 <C> 81.08 <C> 70.62 <C> 78.88 <C> 74.52 <R> <C> Ours w/o LEM <C> 80.75 <C> 79.66 <C> 80.20 <C> 70.63 <C> 78.23 <C> 74.24 <R> <C> Ours <C> [BOLD] 81.88 <C> [BOLD] 81.29 <C> [BOLD] 81.58 <C> [BOLD] 70.86 <C> [BOLD] 80.08 <C> [BOLD] 75.19 <CAP> Table 3: Results of ablation study on whether label embeddings are used, where Source denotes the original dataset, and Ours w/o LEM denotes our augmentation model without label embeddings.
<R> <C> [BOLD] Model <C> [BOLD] Category Classification Accuracy (%) <C> [BOLD] Expert Retrieval: MRR <R> <C> Random <C> 2.61 <C> 0.0092 <R> <C> Weighted Random <C> 12.16 <C> 0.0605 <R> <C> Image-only <C> 29.88 <C> 0.0849 <R> <C> Text-only <C> 68.32 <C> 0.2071 <R> <C> Dual-net <C> 68.04 <C> 0.2022 <R> <C> Embedding Concatenation <C> 70.52 <C> 0.2310 <R> <C> Sum-Prod-Concat <C> 71.35 <C> 0.2369 <R> <C> SAN 1-layer <C> 72.08 <C> 0.2375 <R> <C> SAN 2-layer <C> 72.05 <C> 0.2375 <R> <C> Hie-Co-Att <C> 71.87 <C> 0.2365 <R> <C> MCB <C> 72.01 <C> 0.2370 <R> <C> CQA Augmented Model <C> [BOLD] 76.14 <C> [BOLD] 0.2529 <CAP> Table 2. Baseline model performances vs. CQA Augmented Model on YC-CQA test split.
<R> <C> [BOLD] Model <C> [BOLD] Category Classification Accuracy (%) <C> [BOLD] Expert Retrieval: MRR <R> <C> SAN Big Att <C> 72.48 <C> 0.2379 <R> <C> SAN Big FC <C> 72.37 <C> 0.2376 <R> <C> W/O Image Weight <C> 74.84 <C> 0.2499 <R> <C> W/O Auxiliary Tasks <C> 74.17 <C> 0.2474 <R> <C> W/O Image-to-Texts <C> 75.23 <C> 0.2510 <R> <C> W/O Text-to-Images <C> 75.06 <C> 0.2505 <R> <C> W/O Attention <C> 75.14 <C> 0.2504 <R> <C> W/O Fine-tuning <C> 75.82 <C> 0.2518 <R> <C> Full Model <C> 76.14 <C> 0.2529 <CAP> Table 3. Results for bigger SAN and ablations
<R> <C> [EMPTY] <C> EER% C5 <C> EER% C10 <C> EER% C15 <R> <C> Cosine <C> 4.72 <C> 2.91 <C> 2.67 <R> <C> GT <C> [BOLD] 2.56 <C> [BOLD] 1.86 <C> [BOLD] 1.82 <R> <C> LT <C> 3.67 <C> 2.47 <C> 2.27 <R> <C> Pool <C> 2.72 <C> 1.90 <C> 1.90 <CAP> TABLE III: EER(%) results of various recognition systems
<R> <C> [ITALIC] System (DUC 2006 SysID) <C> [ITALIC] AutoSummENG Score <R> <C> Baseline (1) <C> 0.1437 <R> <C> Top Peer (23) <C> 0.2050 <R> <C> Last Peer (11) <C> 0.1260 <R> <C> Peer Average (All Peers) <C> 0.1842 (Std. Dev. 0.0170) <R> <C> [BOLD] Proposed System (-) <C> [BOLD] 0.1783 <CAP> Table 5: AutoSummENG performance data for DUC 2006. NOTE: The top and last peers are based on the AutoSummENG measure performance of the systems.
<R> <C> [BOLD] Method <C> WAN <C> PCA (4 pc’s) <C> PCA (16 pc’s) <C> Delta (Manhattan) <C> Delta (Euclidean) <C> Delta (Cosine) <R> <C> [BOLD] Accuracy <C> 92.6 <C> 72.8 <C> 81.5 <C> 91.3 <C> 79.3 <C> 81.5 <CAP> Table 2: Accuracies of various attribution methods on full plays between six candidate authors.
<R> <C> [BOLD] Method <C> WAN <C> PCA (4 pc’s) <C> PCA (16 pc’s) <C> Delta (Manhattan) <C> Delta (Euclidean) <C> Delta (Cosine) <R> <C> [BOLD] Accuracy (Act) <C> 93.4 <C> 62.6 <C> 71.4 <C> 74.3 <C> 67.0 <C> 68.1 <R> <C> [BOLD] Accuracy (Scene) <C> 91.5 <C> 69.1 <C> 71.5 <C> 70.1 <C> 69.3 <C> 69.8 <CAP> Table 3: Accuracies of various attribution methods on acts and scenes among eight and two authors, respectively.
<R> <C> clusters <C> 100 <C> 250 <C> 500 <C> 1000 <C> 2000 <R> <C> no clusters <C> 0.228 [0.243] <C> 0.228 [0.243] <C> 0.228 [0.243] <C> 0.228 [0.243] <C> 0.228 [0.243] <R> <C> skipgram40, [ITALIC] w5 <C> 0.223 <C> 0.223 <C> [BOLD] 0.221 <C> [BOLD] 0.221 <C> 0.222 <R> <C> skipgram100, [ITALIC] w5 <C> 0.223 <C> 0.224 <C> [BOLD] 0.221 <C> 0.223 <C> 0.222 <R> <C> skipgram100, [ITALIC] w10 <C> 0.225 <C> 0.223 <C> [BOLD] 0.220 <C> 0.221 <C> 0.222 <R> <C> cbow40, [ITALIC] w5 <C> 0.227 <C> [BOLD] 0.221 <C> 0.228 <C> 0.222 <C> 0.223 <R> <C> cbow100, [ITALIC] w5 <C> 0.225 <C> 0.225 <C> 0.227 <C> [BOLD] 0.224 <C> 0.225 <R> <C> cbow100, [ITALIC] w10 <C> 0.226 <C> 0.226 <C> 0.231 <C> [BOLD] 0.224 <C> [BOLD] 0.224 <R> <C> glove40, [ITALIC] w5 <C> 0.222 <C> 0.222 <C> [BOLD] 0.220 <C> 0.221 <C> 0.222 <R> <C> glove100, [ITALIC] w5 <C> 0.223 <C> 0.221 <C> 0.221 <C> [BOLD] 0.220 <C> 0.221 <R> <C> glove100, [ITALIC] w10 <C> [BOLD] 0.220 <C> 0.222 <C> 0.224 <C> 0.224 <C> 0.223 <R> <C> glove50, [ITALIC] wiki <C> 0.222 <C> 0.221 <C> [BOLD] 0.219 <C> 0.220 <C> 0.221 <R> <C> glove100, [ITALIC] wiki <C> 0.221 <C> 0.221 <C> 0.222 <C> [BOLD] 0.220 <C> 0.222 <CAP> Table 5: Earth Movers Distance for fine-grained sentiment quantification across different types of word embeddings and number of clusters. The score in brackets denotes the best performance achieved in the challenge.
<R> <C> [BOLD] Method <C> [BOLD] SR <C> [BOLD] HATE <C> [BOLD] HAR <R> <C> LR(Char-gram + Unigram) <C> 0.79 <C> 0.85 <C> 0.68 <R> <C> LR(Waseem and Hovy,  2016 ) <C> 0.74 <C> - <C> - <R> <C> LR (Davidson et al.,  2017 ) <C> - <C> 0.90 <C> - <R> <C> CNN (Park and Fung,  2017 ) <C> 0.83 <C> - <C> - <R> <C> GRU Text (Founta et al.,  2018 ) <C> 0.83 <C> 0.89 <C> - <R> <C> GRU Text + Metadata <C> [BOLD] 0.87 <C> 0.89 <C> - <R> <C> TWEM (Ours) <C> 0.86 <C> [BOLD] 0.924 <C> [BOLD] 0.71 <CAP> Table 2: F1 Results444SR: Sexist/Racist (Waseem and Hovy, 2016), HATE: Hate (Davidson et al., 2017) HAR: Harassment (Golbeck et al., 2017)
<R> <C> [BOLD] Split/Dataset <C> [BOLD] Reddit <C> [BOLD] Twitter <R> <C> [BOLD] Training <C> 2.491 <C> 3.867 <R> <C> [BOLD] Testing <C> 4.254 <C> 3.164 <CAP> Table 1: The average number of contexts per post.
<R> <C> Model <C> Avg. <C> Std. <R> <C> LR Pre-trained-TF-IDF-LSI_300 <C> 0.682 <C> 0.012 <R> <C> LR Pre-trained-TF-IDF-LSI_500 <C> 0.679 <C> 0.009 <R> <C> LR TF-IDF-LSI_300 <C> 0.683 <C> 0.009 <R> <C> LR TF-IDF-LSI_500 <C> 0.681 <C> 0.009 <R> <C> LR Pre-trained-LDA_300 <C> 0.659 <C> 0.011 <R> <C> LR Pre-trained-LDA_500 <C> 0.667 <C> 0.015 <R> <C> LR LDA_300 <C> 0.661 <C> [BOLD] 0.008 <R> <C> LR LDA_500 <C> 0.669 <C> 0.015 <R> <C> LR Word2Vec_300 <C> 0.653 <C> 0.010 <R> <C> Proposed Model <C> [EMPTY] <C> [EMPTY] <R> <C> LR Author2Vec_768 <C> 0.702 <C> 0.015 <R> <C> MLP Author2Vec_768 <C> [BOLD] 0.720 <C> 0.015 <CAP> Table 4: Comparison of the baseline and proposed user embedding for depression classification task (best results in bold). [LR denotes logistic regression, MLP denotes multilayer perceptron.]
<R> <C> Testing systems <C> LSTM decoder <C> DQN <R> <C> Average SmoothedBLEU on sentences IN the training set <C> 0.425 <C> 0.494 <R> <C> Average SmoothedBLEU on sentences NOT in the training set <C> 0.107 <C> 0.228 <CAP> Table 1: Experimental results for decoding the seen and unseen sentences in testing.
<R> <C> [EMPTY] <C> TF-IDF <C> LDA <C> HDP-LDA <C> Author-Topic <C> Labeled LDA <R> <C> Agile Planning for Software Products <C> [BOLD] 0.824 <C> 0.388 <C> 0.400 <C> 0.072 <C> [ITALIC] 0.436 <R> <C> Client Needs and Software Requirements <C> [BOLD] 0.654 <C> 0.163 <C> [ITALIC] 0.478 <C> 0.362 <C> 0.246 <R> <C> Design Patterns <C> [BOLD] 0.654 <C> 0.266 <C> 0.173 <C> 0.066 <C> [ITALIC] 0.390 <R> <C> Introduction to Software Product Management <C> [ITALIC] 0.187 <C> 0.077 <C> 0.172 <C> 0.050 <C> [BOLD] 0.217 <R> <C> Object Oriented Design <C> [BOLD] 0.927 <C> 0.400 <C> 0.292 <C> 0.087 <C> [ITALIC] 0.432 <R> <C> [ITALIC] All Courses Combined <C> [BOLD] 0.649 <C> 0.259 <C> 0.303 <C> 0.127 <C> [ITALIC] 0.344 <CAP> Table 3. Bootstrapped mean reciprocal ranks for the baseline and topic models on our courses. Best values are bolded.
<R> <C> [EMPTY] <C> [BOLD] Models <C> [BOLD] Restaurant  [BOLD] Accuracy <C> [BOLD] Restaurant  [BOLD] Macro-F1 <C> [BOLD] Laptop  [BOLD] Accuracy <C> [BOLD] Laptop  [BOLD] Macro-F1 <C> [BOLD] Twitter  [BOLD] Accuracy <C> [BOLD] Twitter  [BOLD] Macro-F1 <R> <C> [BOLD] Baselines <C> ATAE-LSTM <C> 0.7720 <C> [EMPTY] <C> 0.6870 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] Baselines <C> TD-LSTM <C> 0.7560 <C> [EMPTY] <C> 0.6810 <C> [EMPTY] <C> 0.6662∗ <C> 0.6401∗ <R> <C> [BOLD] Baselines <C> IAN <C> 0.7860 <C> [EMPTY] <C> 0.7210 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] Baselines <C> MemNet(3) <C> 0.8032 <C> [EMPTY] <C> 0.7237 <C> [EMPTY] <C> 0.6850∗ <C> 0.6691∗ <R> <C> [BOLD] Baselines <C> RAM(3) <C> 0.8023 <C> 0.7080 <C> 0.7449 <C> 0.7135 <C> 0.6936 <C> 0.6730 <R> <C> [BOLD] Baselines <C> MGAN <C> 0.8125 <C> 0.7194 <C> 0.7539 <C> [BOLD] 0.7247 <C> 0.7254 <C> 0.7081 <R> <C> [BOLD] Baselines <C> ANTM <C> 0.8143 <C> 0.7120 <C> 0.7491 <C> 0.7142 <C> 0.7011 <C> 0.6814 <R> <C> [BOLD] Ablation Test <C> CAPSAR w/o R <C> 0.8185 <C> 0.7216 <C> 0.7484 <C> 0.7039 <C> 0.7255 <C> 0.7067 <R> <C> [BOLD] Ablation Test <C> CAPSAR w/o H <C> 0.8188 <C> 0.7226 <C> 0.7461 <C> 0.7054 <C> 0.7298 <C> 0.7080 <R> <C> [BOLD] Ablation Test <C> CAPSAR <C> [BOLD] 0.8286 <C> [BOLD] 0.7432 <C> [BOLD] 0.7593 <C> 0.7221 <C> [BOLD] 0.7368 <C> [BOLD] 0.7231 <R> <C> [BOLD] Combine BERT <C> BERT <C> 0.8476 <C> 0.7713 <C> 0.7787 <C> 0.7371 <C> 0.7537 <C> 0.7383 <R> <C> [BOLD] Combine BERT <C> CAPSAR-BERT <C> [BOLD] 0.8594 <C> [BOLD] 0.7867 <C> [BOLD] 0.7874 <C> [BOLD] 0.7479 <C> [BOLD] 0.7630 <C> [BOLD] 0.7536 <CAP> Table 2: The average accuracy and F1-score on standard ATSA tasks. The results with ‘*’ are retrieved from the papers of RAM, and other results of baselines are rerieved from corresponding papers.
<R> <C> [BOLD] Datasets <C> [BOLD] Avg. Aspect <C> [BOLD] Avg. SenLen <C> [BOLD] Pre.@1 <C> [BOLD] Rec.@5 <C> [BOLD] mAP <R> <C> [BOLD] Resturant <C> 2.76 <C> 16.25 <C> 0.8233 <C> 0.7884 <C> 0.7139 <R> <C> [BOLD] Laptop <C> 2.54 <C> 15.79 <C> 0.6408 <C> 0.7557 <C> 0.6173 <CAP> Table 4: The average Precision@1, Recall@5 and mAP on aspect term detection. The column “Avg. Aspect” and “Avg.SenLen” indicate average number of words on aspect terms in each sentence and average length of each sentence on the test set, respectively.
<R> <C> Language <C> Model <C> Over-segmentation STM  [BOLD] ↓ <C> Over-segmentation SUF  [BOLD] ↓ <C> Over-segmentation PRE  [BOLD] ↓ <C> Over-segmentation UNKNOWN  [BOLD] ↓ <C> Over-segmentation  [BOLD] PRE/TOT ↑ <C> Under-segmentation STM-SUF  [BOLD] ↓ <C> Under-segmentation STM-STM  [BOLD] ↓ <C> Under-segmentation SUF-SUF  [BOLD] ↓ <C> Under-segmentation SUF-STM  [BOLD] ↓ <C> Under-segmentation PRE-STM  [BOLD] ↓ <C> Under-segmentation UNKNOWN  [BOLD] ↓ <C> Under-segmentation  [BOLD] REC/TOT ↑ <R> <C> eng <C> Characters <C> 71.05 <C> 11.82 <C> 1.66 <C> 0.33 <C> 15.13 <C> 0.00 <C> 0.00 <C> 0.00 <C> 0.00 <C> 0.00 <C> [EMPTY] <C> 100.00 <R> <C> eng <C> Words <C> 0.00 <C> 0.00 <C> 0.00 <C> 0.00 <C> 100.00 <C> 55.07 <C> 5.90 <C> 8.56 <C> 0.14 <C> 4.38 <C> [EMPTY] <C> 23.06 <R> <C> eng <C> SentencePiece 38k <C> 17.60 <C> 10.25 <C> 0.18 <C> 0.24 <C> 71.74 <C> 26.40 <C> 2.48 <C> 2.74 <C> 0.05 <C> 2.78 <C> [EMPTY] <C> 65.26 <R> <C> eng <C> Morfessor Baseline <C> [BOLD] 10.17 <C> [BOLD] 2.32 <C> [BOLD] 0.03 <C> [BOLD] 0.07 <C> [BOLD] 87.42 <C> 22.46 <C> 2.10 <C> 4.75 <C> [BOLD] 0.04 <C> 1.65 <C> [EMPTY] <C> 67.37 <R> <C> eng <C> EM+Prune MDL <C> 15.46 <C> 2.75 <C> 0.05 <C> 0.13 <C> 81.61 <C> [BOLD] 19.93 <C> [BOLD] 1.82 <C> [BOLD] 4.32 <C> [BOLD] 0.04 <C> [BOLD] 1.46 <C> [EMPTY] <C> [BOLD] 70.84 <R> <C> fin <C> Characters <C> 65.23 <C> 13.80 <C> 0.67 <C> 0.57 <C> 19.73 <C> 0.00 <C> 0.00 <C> 0.00 <C> 0.00 <C> 0.00 <C> 0.00 <C> 100.00 <R> <C> fin <C> Words <C> 0.00 <C> 0.00 <C> 0.00 <C> 0.00 <C> 100.00 <C> 49.19 <C> 17.16 <C> 21.76 <C> 4.84 <C> 0.96 <C> 0.58 <C> 4.09 <R> <C> fin <C> SentencePiece 13k <C> 35.11 <C> 3.71 <C> 0.08 <C> 0.41 <C> 60.69 <C> 25.96 <C> 1.45 <C> 16.18 <C> 0.35 <C> 0.08 <C> [BOLD] 0.16 <C> [BOLD] 55.81 <R> <C> fin <C> Morfessor Baseline <C> 34.75 <C> 2.82 <C> [BOLD] 0.03 <C> 0.38 <C> 62.02 <C> [BOLD] 24.57 <C> [BOLD] 0.86 <C> 16.31 <C> [BOLD] 0.15 <C> [BOLD] 0.04 <C> 0.20 <C> 57.63 <R> <C> fin <C> EM+Prune MDL <C> [BOLD] 29.34 <C> [BOLD] 2.20 <C> [BOLD] 0.03 <C> [BOLD] 0.26 <C> [BOLD] 68.18 <C> 24.68 <C> 0.90 <C> [BOLD] 15.95 <C> 0.29 <C> 0.05 <C> 0.19 <C> 57.60 <R> <C> sme <C> Characters <C> 81.44 <C> 6.80 <C> [EMPTY] <C> [EMPTY] <C> 11.76 <C> 0.00 <C> 0.00 <C> 0.00 <C> 0.00 <C> [EMPTY] <C> [EMPTY] <C> 100.00 <R> <C> sme <C> Words <C> 0.00 <C> 0.00 <C> [EMPTY] <C> [EMPTY] <C> 100.00 <C> 52.92 <C> 13.15 <C> 4.43 <C> 0.61 <C> [EMPTY] <C> [EMPTY] <C> 28.64 <R> <C> sme <C> SentencePiece 64k <C> 30.10 <C> 4.52 <C> [EMPTY] <C> [EMPTY] <C> 65.38 <C> 31.35 <C> 3.96 <C> [BOLD] 3.09 <C> 0.20 <C> [EMPTY] <C> [EMPTY] <C> 61.40 <R> <C> sme <C> Morfessor Baseline <C> [BOLD] 23.27 <C> [BOLD] 3.02 <C> [EMPTY] <C> [EMPTY] <C> [BOLD] 73.71 <C> 33.16 <C> [BOLD] 2.22 <C> 3.40 <C> [BOLD] 0.10 <C> [EMPTY] <C> [EMPTY] <C> 60.99 <R> <C> sme <C> EM+Prune MDL <C> 23.35 <C> 4.41 <C> [EMPTY] <C> [EMPTY] <C> 72.25 <C> [BOLD] 30.48 <C> 3.10 <C> 3.23 <C> 0.17 <C> [EMPTY] <C> [EMPTY] <C> [BOLD] 62.84 <CAP> Table 10: Error analysis for English (eng, α=0.9), Finnish (fin, α=0.02), and North Sámi (sme, α=1.0). All results without forcesplit. Over-segmentation and under-segmentation errors reduce precision and recall, respectively.
<R> <C> Dataset <C> Method <C> K <C> Purity <C> NMI <C> RI <C> FM <R> <C> Toy_Apple <C> K-means <C> 40 <C> 0.96 <C> 0.71 <C> 0.98 <C> 0.41 <R> <C> [EMPTY] <C> WebSets <C> 25 <C> 0.99 <C> 0.99 <C> 1.00 <C> 0.99 <R> <C> Delicious_Sports <C> K-means <C> 50 <C> 0.72 <C> 0.68 <C> 0.98 <C> 0.47 <R> <C> [EMPTY] <C> WebSets <C> 32 <C> 0.83 <C> 0.64 <C> 1.00 <C> 0.85 <CAP> Table 8: Comparison of WebSets vs. K-means
<R> <C> Method <C> K <C> FM w/ Entity records <C> FM w/ Triplet records <R> <C> WebSets <C> [EMPTY] <C> 0.11 (K=25) <C> 0.85 (K=34) <R> <C> K-Means <C> 30 <C> 0.09 <C> 0.35 <R> <C> [EMPTY] <C> 25 <C> 0.08 <C> 0.38 <CAP> Table 9: Comparison of performance on Entity vs. Triplet record representation (Toy_Apple dataset)
<R> <C> [EMPTY] <C> [BOLD] SD <C> [BOLD] SeTD <C> [ITALIC] p [BOLD] -value <R> <C> [BOLD] Younger Users: Age 18 - 45 <C> [BOLD] Younger Users: Age 18 - 45 <C> [BOLD] Younger Users: Age 18 - 45 <C> [BOLD] Younger Users: Age 18 - 45 <R> <C> [BOLD] Question 2: Useful Directions <C> 79.41% <C> 87.18% <C> 0.3720 <R> <C> [BOLD] Question 3: User Understand System <C> 3.18 <C> 3.05 <C> 0.5473 <R> <C> [BOLD] Question 4: System Understand User <C> 2.97 <C> 3.36 <C> 0.0627 <R> <C> [BOLD] Open-Ended: Comments about difficulty <C> 5.71% <C> 2.56% <C> 0.4956 <R> <C> [BOLD] Older Users: Ages 45+ <C> [BOLD] Older Users: Ages 45+ <C> [BOLD] Older Users: Ages 45+ <C> [BOLD] Older Users: Ages 45+ <R> <C> [BOLD] Question 2: Useful Directions <C> 75.56% <C> 81.40% <C> 0.5064 <R> <C> [BOLD] Question 3: User Understand System <C> 3.04 <C> 3.07 <C> 0.9072 <R> <C> [BOLD] Question 4: System Understand User <C> 3.27 <C> 3.30 <C> 0.8536 <R> <C> [BOLD] Open-Ended: Comments about difficulty <C> 24.44% <C> 11.63% <C> 0.1178 <CAP> Table 3: Responses to the subjective questions in the survey. For question 2, the rate of yes answers is shown. For questions 3 and 4, the answers are converted to numeric values with 4 being the most positive answer (e.g., “system understood me perfectly”) with 1 being the most negative answer (e.g., “system could not understand me”). The last row shows the percentage of open-ended comments that concerned difficulty with the system, such as speed of delivery and the voice.
<R> <C> [BOLD] Model <C> [BOLD] SST-2 <C> [BOLD] SST-5 <R> <C> BiLSTM Cho et al. ( 2014 ) <C> 87.5 <C> 49.5 <R> <C> CNN-non-static Kim ( 2014 ) <C> 87.2 <C> 48.0 <R> <C> BiLSTM with self-attention <C> 88.2 <C> 50.4 <R> <C> CNN (Dense) with self-attention <C> 88.3 <C> [BOLD] 50.6 <R> <C> ours (Single DSA) <C> [BOLD] 88.5 <C> [BOLD] 50.6 <CAP> Table 2: Test accuracy with SST dataset.
<R> <C> Modalities <C> Method <C> Acc <C> F-score <C> MAE <R> <C> Text <C> RNTN (Socher et al.,  2013 ) <C> (73.7) <C> (73.4) <C> (0.990) <R> <C> Text <C> DAN (Iyyer et al.,  2015 ) <C> 70.0 <C> 69.4 <C> - <R> <C> Text <C> D-CNN (Kalchbrenner et al.,  2014 ) <C> 69.0 <C> 65.1 <C> - <R> <C> Text <C> SAL-CNN text (Wang et al.,  2016 ) <C> 73.5 <C> - <C> - <R> <C> Text <C> SVM-MD text (Zadeh et al.,  2016b ) <C> 73.3 <C> 72.1 <C> 1.186 <R> <C> Text <C> RF text (Zadeh et al.,  2016b ) <C> 57.6 <C> 57.5 <C> - <R> <C> Text <C> LSTM text (ours) <C> 67.8 <C> 51.2 <C> 1.234 <R> <C> Text <C> LSTM(A) text (ours) <C> 71.3 <C> 67.3 <C> 1.062 <R> <C> Multimodal <C> Random <C> 50.2 <C> 48.7 <C> 1.880 <R> <C> Multimodal <C> SAL-CNN (Wang et al.,  2016 ) <C> 73.0 <C> - <C> - <R> <C> Multimodal <C> SVM-MD (Zadeh et al.,  2016b ) <C> 71.6 <C> 72.3 <C> 1.100 <R> <C> Multimodal <C> C-MKL (Poria et al.,  2015b ) <C> 73.5 <C> - <C> - <R> <C> Multimodal <C> RF (Zadeh et al.,  2016b ) <C> 57.4 <C> 59.0 <C> - <R> <C> Multimodal <C> LSTM (ours) <C> 69.4 <C> 63.7 <C> 1.245 <R> <C> Multimodal <C> LSTM(A) (ours) <C> 75.7 <C> 72.1 <C> 1.019 <R> <C> Multimodal <C> GME-LSTM(A) (ours) <C> [BOLD] 76.5 <C> [BOLD] 73.4 <C> [BOLD] 0.955 <R> <C> Multimodal <C> Human <C> 85.7 <C> 87.5 <C> 0.710 <R> <C> Multimodal <C> Δ [ITALIC] SOTA <C> ↑3.0 <C> ↑1.1 <C> ↓0.145 <CAP> Table 1. Sentiment prediction results on test set using different text-based and multimodal methods. Numbers are reported in binary classification accuracy (Acc), F-score and MAE, and the best scores are highlighted in bold (excluding human performance). ΔSOTA shows improvement over the state-of-the-art. Results for RNTN are parenthesized because the model was trained on the Stanford Sentiment Treebank dataset (Socher et al., 2013) which is much larger than CMU-MOSI.
<R> <C> [BOLD] Name <C> [BOLD] Value <R> <C> batch size <C> 16 <R> <C> learning rate  [ITALIC] lr <C> 0.005 <R> <C> SGD momentum  [ITALIC] μ <C> 0.9 <R> <C> dropout rate  [ITALIC] r <C> 0.3 <R> <C> MSE loss weight  [ITALIC] θ <C> 0.4 <R> <C> regularization weight  [ITALIC] λ <C> 0.0005 <R> <C> news embedding dimension  [ITALIC] V <C> 256 <R> <C> recurrent state dimension  [ITALIC] D <C> 100 <R> <C> trading sequence length  [ITALIC] T <C> 7 <CAP> Table 2: Hyper-parameters setting.
<R> <C> [BOLD] Model <C> L [ITALIC] nll <C> L [ITALIC] nll+L [ITALIC] align <C> L [ITALIC] nll+L [ITALIC] align+L [ITALIC] hid <R> <C> [BOLD] BLEU <C> 23.08 <C> 24.76 <C> [BOLD] 25.55 <R> <C> [BOLD] Long-sentence BLEU <C> 17.48 <C> 19.24 <C> [BOLD] 20.63 <CAP> Table 2: Ablation studies on IWSLT14 De-En. Results are BLEU scores without teacher rescoring.
<R> <C> Task <C> Entity F1 <C> Entity P <C> Entity R <C> Entity MCC <C> Relation F1 <C> Relation P <C> Relation R <C> Relation MCC <R> <C> A <C> 0.74 <C> 0.82 <C> 0.69 <C> 0.55 <C> 0.60 <C> 0.67 <C> 0.54 <C> 0.30 <R> <C> B <C> 0.77 <C> 0.83 <C> 0.72 <C> 0.57 <C> 0.62 <C> 0.69 <C> 0.57 <C> 0.33 <R> <C> C <C> 0.68 <C> 0.76 <C> 0.64 <C> 0.39 <C> 0.51 <C> 0.55 <C> 0.48 <C> 0.28 <CAP> Table 6: Test set results for fuzzy inputs at inference time on the ER extraction task with the joint transformer based model (SciBERT).
<R> <C> [EMPTY] <C> WS <C> WSS <C> WSR <C> MEN <C> RG <R> <C> WN+NR1 <C> 0.55 <C> [BOLD] 0.67 <C> 0.46 <C> 0.53 <C> 0.40 <R> <C> WN+NR2 <C> 0.43 <C> 0.53 <C> 0.32 <C> 0.47 <C> 0.49 <R> <C> WE [ITALIC] Mikolov <C> 0.55 <C> 0.64 <C> [BOLD] 0.59 <C> 0.66 <C> 0.75 <R> <C> WE [ITALIC] Senna <C> 0.47 <C> 0.59 <C> 0.38 <C> 0.57 <C> 0.47 <R> <C> CE (Ours) <C> 0.55 <C> 0.63 <C> 0.49 <C> 0.64 <C> 0.78 <R> <C> HCE (Ours) <C> [BOLD] 0.57 <C> [BOLD] 0.67 <C> 0.53 <C> [BOLD] 0.69 <C> [BOLD] 0.83 <CAP> Table 4: Correlation with human judgements on semantic relatedness evaluation datasets.
<R> <C> [EMPTY] <C> [ITALIC] L=1 <C> [ITALIC] L=2 <C> [ITALIC] L=3 <R> <C> PLDA <C> 0.361 <C> 0.209 <C> 0.159 <R> <C> Norm. PLDA <C> 0.362 <C> 0.210 <C> 0.159 <R> <C> [EMPTY] <C> [ITALIC] L=4 <C> [ITALIC] L=5 <C> Mix. enroll. <R> <C> PLDA <C> 0.131 <C> 0.113 <C> 0.266 <R> <C> Norm. PLDA <C> 0.131 <C> 0.116 <C> [BOLD] 0.188 <CAP> Table 1: Results on the NIST SRE 2014 with various enrollment conditions (minDCF with β=100).
<R> <C> [BOLD] Testset(SNR/RT) <C> [BOLD] baseline <C> [BOLD] +N_DNN <C> [BOLD] MLT <R> <C> clean <C> 3.0 <C> [BOLD] 2.9 <C> 3.1 <R> <C> music(00) <C> 28.4 <C> [BOLD] 25.5 <C> 29.1 <R> <C> music(10) <C> 6.5 <C> [BOLD] 6.3 <C> 7.4 <R> <C> reverb(0.6) <C> 16.4 <C> [BOLD] 15.4 <C> 17.4 <R> <C> reverb(1.0) <C> 26.8 <C> [BOLD] 25.3 <C> 29.0 <R> <C> street(00) <C> 35.0 <C> [BOLD] 32.7 <C> 39.1 <R> <C> street(10) <C> 7.7 <C> [BOLD] 6.7 <C> 7.7 <R> <C> white(00) <C> 30.7 <C> [BOLD] 28.8 <C> 33.8 <R> <C> white(10) <C> 9.7 <C> [BOLD] 8.3 <C> 9.5 <R> <C> Average <C> 18.3 <C> [BOLD] 16.9 <C> 19.5 <CAP> Table 1: Comparison of WERs(%) between the baseline, N_DNN, and MTL model using 50-dimensional embeddings for 8 different noisy evaluation sets and one clean evaluation set.
<R> <C> [BOLD] Model (CHiME-3) <C> [BOLD] In-domain Noise (CHiME-3) Dev (%) <C> [BOLD] In-domain Noise (CHiME-3) Eval (%) <C> [BOLD] Unseen Noise (Aurora4) test_eval92 (%) <R> <C> Baseline <C> 8.9 <C> 15.6 <C> 11.7 <R> <C> +N_NAT <C> [BOLD] 8.8 <C> 15.9* <C> 12.6* <R> <C> +N_GMM <C> 8.8 <C> 15.7 <C> 12.4* <R> <C> +N_GMM_ON <C> 8.9 <C> 15.7 <C> 11.6* <R> <C> +N_DNN <C> 8.8 <C> [BOLD] 15.3* <C> [BOLD] 11.5* <CAP> Table 2: Comparison of WERs(%) on the CHiME-3 task (In-domain Noise 4.5hrs) and the Aurora4 task (Unseen Noise 9.4hrs) between the baseline, +N_NAT, +N_GMM, +N_GMM_ON, and +N_DNN. 40 dimensional noise embeddingss were augmented for noise adaptation. The models are trained on CHiME-3 training dataset (18hrs). (*) denotes the statistical significance (α=0.05) [23].
<R> <C> [EMPTY] <C> Perplexity <C> Sentiment Acc (%) <R> <C> SEQ2SEQ <C> 157.5 <C> 55.6 <R> <C> CVAE <C> 81.83 <C> 75.6 <R> <C> CGAN <C> 120.3 <C> 64.4 <R> <C> CGAN-CVAE <C> [BOLD] 69.54 <C> [BOLD] 78.8 <CAP> Table 1: Evaluation of various dialogue systems with perplexity and sentiment accuracy.
<R> <C> [EMPTY] <C> Quality <C> Sen-Acc(%) <R> <C> SEQ2SEQ <C> 2.1 <C> 54.4 <R> <C> CVAE <C> 3.6 <C> 73.3 <R> <C> CGAN <C> 2.9 <C> 66.7 <R> <C> CGAN-CVAE <C> [BOLD] 3.9 <C> [BOLD] 78.9 <CAP> Table 2: Dialogue response quality and sentiment accuracy (Sen-Acc) of different dialogue systems based on human evaluation.
<R> <C> [EMPTY] <C> R1* <C> R2* <C> R1 <C> R2 <R> <C> # Documents <C> 107 <C> 117 <C> 100 <C> 100 <R> <C> # Concepts <C> 351 <C> 344 <C> 317 <C> 317 <R> <C> # Historical <C> 67 <C> 80 <C> 79 <C> 65 <R> <C> # Not Historical <C> 276 <C> 264 <C> 238 <C> 252 <CAP> Table 1: Total labelled ‘seizure’ symptom concepts and for each human annotator (R1, R2) for the ‘temporality’ task of labelling concepts that have occurred the past relative to the hospital episode. * indicates raw numbers before taking into account the intersection of notes between annotators
<R> <C> source <C> langauge <C> [ITALIC] k mean <C> [ITALIC] k stdev <C> [ITALIC] α mean <C> [ITALIC] α stdev <C> [ITALIC] β mean <C> [ITALIC] β stdev <C> perplexity mean <C> perplexity stdev <R> <C> GitHub <C> C <C> 521.2 <C> 73.7 <C> 3.94 <C> 4.35 <C> 68.4 <C> 35.8 <C> 236.5 <C> 6.5 <R> <C> GitHub <C> C++ <C> 577.4 <C> 173.6 <C> 1.75 <C> 1.20 <C> 61.7 <C> 32.9 <C> 228.4 <C> 5.2 <R> <C> GitHub <C> CSS <C> 455.4 <C> 34.1 <C> 1.52 <C> 0.82 <C> 36.7 <C> 16.0 <C> 236.7 <C> 7.8 <R> <C> GitHub <C> HTML <C> 439.2 <C> 37.0 <C> 0.93 <C> 0.09 <C> 45.4 <C> 17.6 <C> 236.6 <C> 8.6 <R> <C> GitHub <C> Java <C> 480.2 <C> 76.0 <C> 1.81 <C> 0.89 <C> 44.6 <C> 37.1 <C> 226.0 <C> 3.1 <R> <C> GitHub <C> JavaScript <C> 484.0 <C> 19.9 <C> 1.59 <C> 0.57 <C> 23.4 <C> 18.2 <C> 238.1 <C> 2.7 <R> <C> GitHub <C> Python <C> 529.0 <C> 43.7 <C> 1.51 <C> 0.27 <C> 32.9 <C> 14.9 <C> 257.4 <C> 10.9 <R> <C> GitHub <C> Ruby <C> 505.4 <C> 28.0 <C> 2.41 <C> 1.49 <C> 89.1 <C> 37.0 <C> 213.9 <C> 6.0 <R> <C> GitHub <C> [ITALIC] all <C> 499.0 <C> 81.0 <C> 1.93 <C> 1.80 <C> 50.3 <C> 32.4 <C> 234.2 <C> 13.3 <R> <C> Stack Overflow <C> C <C> 377.0 <C> 34.3 <C> 0.95 <C> 0.35 <C> 51.8 <C> 55.1 <C> 202.9 <C> 4.5 <R> <C> Stack Overflow <C> C++ <C> 337.6 <C> 29.6 <C> 3.33 <C> 3.30 <C> 97.4 <C> 61.8 <C> 199.3 <C> 3.0 <R> <C> Stack Overflow <C> CSS <C> 196.2 <C> 24.2 <C> 1.01 <C> 0.96 <C> 18.1 <C> 15.3 <C> 184.1 <C> 2.7 <R> <C> Stack Overflow <C> HTML <C> 244.4 <C> 18.1 <C> 2.45 <C> 2.33 <C> 76.4 <C> 69.5 <C> 196.7 <C> 5.9 <R> <C> Stack Overflow <C> Java <C> 349.8 <C> 49.1 <C> 0.85 <C> 0.46 <C> 10.0 <C> 8.2 <C> 223.9 <C> 2.5 <R> <C> Stack Overflow <C> JavaScript <C> 252.8 <C> 34.5 <C> 4.24 <C> 3.66 <C> 50.9 <C> 44.0 <C> 213.6 <C> 2.0 <R> <C> Stack Overflow <C> Python <C> 295.8 <C> 47.3 <C> 1.10 <C> 0.18 <C> 67.6 <C> 78.6 <C> 229.0 <C> 4.0 <R> <C> Stack Overflow <C> Ruby <C> 269.3 <C> 33.1 <C> 2.11 <C> 2.72 <C> 64.0 <C> 52.4 <C> 215.9 <C> 7.3 <R> <C> Stack Overflow <C> [ITALIC] all <C> 283.7 <C> 61.9 <C> 2.06 <C> 2.37 <C> 57.6 <C> 57.4 <C> 207.8 <C> 14.2 <R> <C> [ITALIC] all <C> [ITALIC] all <C> 379.4 <C> 128.7 <C> 2.00 <C> 2.12 <C> 54.4 <C> 47.8 <C> 219.5 <C> 19.1 <CAP> TABLE II: Results of tuning (number of topics k, α, β) for eight programming languages from two sources, with the goal of minimising perplexity.
<R> <C> [BOLD] Family <C> [BOLD] Lang. <C> csls [BOLD] -10K Avg. <C> csls [BOLD] -10K Best <C> [BOLD] Mod-10K Avg. <C> [BOLD] Mod-10K Best <R> <C> Germanic <C> da <C> [BOLD] 52.62 <C> [BOLD] 60.27 <C> 52.18 <C> 60.13 <R> <C> Germanic <C> de <C> [BOLD] 75.27 <C> [BOLD] 75.60 <C> 75.16 <C> 75.53 <R> <C> Romance <C> es <C> [BOLD] 74.35 <C> 83.00 <C> 74.32 <C> 83.00 <R> <C> Romance <C> it <C> 78.41 <C> 78.80 <C> [BOLD] 78.43 <C> 78.80 <R> <C> Indo-Iranian <C> fa <C> [BOLD] 27.79 <C> 33.40 <C> 27.77 <C> 33.40 <R> <C> Indo-Iranian <C> hi <C> 25.71 <C> 33.73 <C> [BOLD] 26.39 <C> [BOLD] 34.20 <R> <C> Indo-Iranian <C> bn <C> 0.00 <C> 0.00 <C> [BOLD] 0.09 <C> [BOLD] 0.87 <R> <C> Others <C> fi <C> 4.71 <C> 47.07 <C> 4.71 <C> 47.07 <R> <C> Others <C> hu <C> [BOLD] 52.55 <C> 54.27 <C> 52.35 <C> [BOLD] 54.73 <R> <C> Others <C> ja <C> 18.13 <C> 49.69 <C> [BOLD] 36.13 <C> 49.69 <R> <C> Others <C> zh <C> 5.01 <C> 37.20 <C> [BOLD] 10.75 <C> 37.20 <R> <C> Others <C> ko <C> 16.98 <C> 20.68 <C> [BOLD] 17.34 <C> [BOLD] 22.53 <R> <C> Others <C> ar <C> 15.43 <C> 33.33 <C> [BOLD] 15.71 <C> [BOLD] 33.67 <R> <C> Others <C> id <C> 67.69 <C> 68.40 <C> [BOLD] 67.82 <C> 68.40 <R> <C> Others <C> vi <C> 0.01 <C> 0.07 <C> 0.01 <C> 0.07 <CAP> Table 6: bli results (P@1 ×100%) from en to each target language with different validation metrics for muse: default (csls-10K) and modularity (Mod-10K). We report the average (Avg.) and the best (Best) from ten runs with ten random seeds for each validation metric. Bold values are mappings that are not shared between the two validation metrics. Mod-10K improves the robustness of muse on distant language pairs.
<R> <C> Model <C> #Params [millions] <C> Test Perplexity Published1 <C> Test Perplexity BlackOut <C> Time to Solution Published1 <C> Time to Solution BlackOut <R> <C> KN 5-gram <C> 1,748 <C> 66.95 <C> 66.95 <C> 45m <C> 45m <R> <C> RNN-128 + KN 5-gram <C> 1,764 <C> 60.8 <C> 59.0 <C> 6h <C> 9h <R> <C> RNN-256 + KN 5-gram <C> 1,781 <C> 57.3 <C> 55.1 <C> 16h <C> 14h <R> <C> RNN-512 + KN 5-gram <C> 1,814 <C> 53.2 <C> 51.5 <C> 1d2h <C> 1d <R> <C> RNN-1024 + KN 5-gram <C> 1,880 <C> 48.9 <C> 47.6 <C> 2d2h <C> 1d14h <R> <C> RNN-2048 + KN 5-gram <C> 2,014 <C> 45.2 <C> 43.9 <C> 4d7h <C> 2d15h <R> <C> RNN-4096 + KN 5-gram <C> 2,289 <C> [BOLD] 42.4 <C> [BOLD] 42.0 <C> 14d5h <C> 10d <CAP> Table 1: Performance on the one billion word benchmark by interpolating RNNLM on a 64K word vocabulary with a full-size KN 5-gram LM.
<R> <C> [BOLD] Algorithm <C> [BOLD] Dev w.o. Shaping <C> [BOLD] Dev w. Shaping <C> [BOLD] Test w.o. Shaping <C> [BOLD] Test w. Shaping <R> <C> Maximum Margin Likelihood <C> 33.2 <C> 32.5 <C> 31.0 <C> 32.3 <R> <C> Meritocratic ( [ITALIC] β=0) <C> 27.1 <C> 28.1 <C> 31.3 <C> 30.1 <R> <C> Meritocratic ( [ITALIC] β=0.5) <C> 28.3 <C> 28.7 <C> 31.7 <C> 32.0 <R> <C> Meritocratic ( [ITALIC] β=∞) <C> 39.3 <C> 41.6 <C> 41.6 <C> 45.2 <R> <C> REINFORCE <C> 10.2 <C> 11.8 <C> 2.4 <C> 4.0 <R> <C> Off-Policy Policy Gradient <C> 36.6 <C> 38.6 <C> 42.6 <C> 44.1 <R> <C> MMR <C> 38.4 <C> 40.7 <C> 43.2 <C> 46.9 <R> <C> MAVER <C> 39.6 <C> 44.1 <C> 43.7 <C> [BOLD] 49.7 <CAP> Table 2: Experimental results on different model update algorithms, with and without policy shaping.
<R> <C> ID <C> System <C> [ITALIC] Dh <C> [ITALIC] Dp <C> tg <C> cn <R> <C> L275h1 <C> 1L LSTMP <C> 1000 <C> 500 <C> 26.5 <C> 26.0 <R> <C> S275h1 <C> 1L sigmoid HORNNP <C> 1000 <C> 500 <C> 26.4 <C> 25.8 <R> <C> R275h1 <C> 1L ReLU HORNNP <C> 1000 <C> 500 <C> 26.4 <C> 25.9 <R> <C> L275h3 <C> 2L LSTMP <C> 1000 <C> 500 <C> 25.7 <C> 25.2 <R> <C> S275h4 <C> 2L sigmoid HORNNP <C> 1000 <C> 500 <C> 25.6 <C> 25.2 <R> <C> R275h4 <C> 2L ReLU HORNNP <C> 1000 <C> 500 <C> 25.3 <C> 25.0 <R> <C> D275h1 <C> 7L sigmoid DNN <C> 1000 <C> [EMPTY] <C> 28.4 <C> 27.5 <CAP> Table 2: %WERs for a selection of 275h system on dev17b. Systems use a trigram LM with Viterbi decoding (tg) or CN decoding (cn).
<R> <C> [EMPTY] <C> # Supp. <C> # Cont. <C> Lan. Score <R> <C> Gold Reference <C> 4.25 <C> 0.84 <C> 1.85 <R> <C> Base + switch <C> 2.57 <C> 2.17 <C> 0.93 <R> <C> Base + switch + LM (ours) <C> [BOLD] 3.64 <C> [BOLD] 1.12 <C> 1.59 <R> <C> - w/o copy loss  [ITALIC] pcopy <C> 3.54 <C> 1.30 <C> [BOLD] 1.63 <CAP> Table 5: Human evaluation results: Average number of supporting facts (column 2, the larger the better), contradicting facts (column 3, the smaller the better), and language naturalness score (column 4, the larger the better).
<R> <C> [EMPTY] <C> WN18RR ( [ITALIC] h,?, [ITALIC] t) Uncalib. <C> WN18RR ( [ITALIC] h,?, [ITALIC] t) Iso. <C> WN18RR ( [ITALIC] h,?, [ITALIC] t) Platt 1D <C> WN18RR ( [ITALIC] h,?, [ITALIC] t) Platt 2D <C> FB15K ( [ITALIC] h, [ITALIC] r,?) Uncalib. <C> FB15K ( [ITALIC] h, [ITALIC] r,?) Iso. <C> FB15K ( [ITALIC] h, [ITALIC] r,?) Platt 1D <R> <C> [BOLD] TransE <C> 0.624 <C> 0.040 <C> 0.054 <C> [BOLD] 0.014 <C> 0.022 <C> [BOLD] 0.002 <C> [BOLD] 0.002 <R> <C> [BOLD] TransH <C> 0.054 <C> 0.044 <C> 0.057 <C> [BOLD] 0.018 <C> 0.496 <C> [BOLD] 0.002 <C> 0.003 <R> <C> [BOLD] DistMult <C> 0.046 <C> [BOLD] 0.029 <C> 0.040 <C> 0.044 <C> 0.894 <C> [BOLD] 0.002 <C> 0.004 <R> <C> [BOLD] ComplEx <C> 0.028 <C> [BOLD] 0.034 <C> 0.041 <C> 0.035 <C> 0.910 <C> [BOLD] 0.004 <C> 0.005 <CAP> Table 5: ECE (M=10 bins) on WN18RR and FB15K, averaged over five dataset splits (all stdevs <0.01.) Lower is better.
<R> <C> Model <C> F1 score <R> <C> WordNet 1st Sense Baseline <C> 65.9 <R> <C> Raganato et al. ( 2017 ) <C> 69.9 <R> <C> Iacobacci et al. ( 2016 ) <C> [BOLD] 70.1 <R> <C> ELMo <C> 69.0 <R> <C> ESuLMo <C> 69.6 <CAP> Table 3: Word Sense Disambiguation
<R> <C> [BOLD] System <C> [BOLD] Accuracy <C> [BOLD] Precision <C> [BOLD] Recall <R> <C> [BOLD] Word level <C> [BOLD] Word level <C> [BOLD] Word level <C> [BOLD] Word level <R> <C> Multitask <C> 0.853 (0.849 - 0.857) <C> 0.831 <C> 0.911 <R> <C> Classifier task <C> 0.820 (0.815 - 0.824) <C> 0.818 <C> 0.855 <R> <C> Gillespie et al. <C> 0.753 (na) <C> 0.823 <C> 0.728 <R> <C> [BOLD] Speaker level <C> [BOLD] Speaker level <C> [BOLD] Speaker level <C> [BOLD] Speaker level <R> <C> Multitask <C> 0.929 (0.790-0.984) <C> 1.000 <C> 0.867 <R> <C> Classifier task <C> 0.929 (0.790-0.984) <C> 0.933 <C> 0.933 <R> <C> Gillespie et al. <C> 0.929 (na) <C> na <C> na <CAP> Table 2: Accuracy of dysarthria detection including 95% CI. Classifier task - target mel-spectrogram (ML) is not observed during training. Multitask - both targets ML and dysarthric labels are observed
<R> <C> [BOLD] WER(%) <C> [BOLD] WER(%) <C> [ITALIC] λ 0 <C> [ITALIC] λ 0.5 <C> [ITALIC] λ 1 <C> [ITALIC] λ 1.5 <R> <C> [ITALIC] α <C> 0 <C> 15.41 <C> 13.42 <C> 12.20 <C> 11.54 <R> <C> [ITALIC] α <C> 2.5 <C> 14.63 <C> 12.64 <C> 11.39 <C> 10.74 <R> <C> [ITALIC] α <C> 5 <C> 14.01 <C> 12.01 <C> 10.77 <C> 10.13 <R> <C> [ITALIC] α <C> 0 <C> 14.67 <C> 13.34 <C> 12.66 <C> 12.39 <R> <C> [ITALIC] α <C> 2.5 <C> 13.54 <C> 12.22 <C> 11.53 <C> 11.26 <R> <C> [ITALIC] α <C> 5 <C> 12.38 <C> 11.07 <C> 10.37 <C> 10.12 <CAP> Table 3: Oracle test results on the with-error set with phrase expansion (top) and OOV (bottom) schemes.
<R> <C> [BOLD] WER (%) <C> [BOLD] WER (%) <C> # Distractors 0 <C> # Distractors 10000 <C> # Distractors 0 <C> # Distractors 10000 <R> <C> # Classes <C> 50 <C> 9.55 <C> 10.40 <C> 9.82 <C> 10.26 <R> <C> # Classes <C> 500 <C> 10.77 <C> 10.97 <C> 10.37 <C> 10.70 <R> <C> # Classes <C> 5000 <C> 12.40 <C> 12.43 <C> 11.16 <C> 11.44 <CAP> Table 4: WER results on different number of classes with phrase expansion (left) and OOV (right) schemes.
<R> <C> [BOLD] Approach <C> [BOLD] Supporting  [BOLD] Prec. <C> [BOLD] Supporting  [BOLD] Recall <C> [BOLD] Supporting  [BOLD] F1 <C> [BOLD] Opposing  [BOLD] Prec. <C> [BOLD] Opposing  [BOLD] Recall <C> [BOLD] Opposing  [BOLD] F1 <C> [BOLD] Overall (Macro)  [BOLD] Prec. <C> [BOLD] Overall (Macro)  [BOLD] Recall <C> [BOLD] Overall (Macro)  [BOLD] F1 <R> <C> LSTM <C> 63.42 <C> 58.80 <C> 61.02 <C> 56.99 <C> 61.67 <C> 59.24 <C> 60.20 <C> 60.24 <C> 60.13 <R> <C> ESIM <C> 64.38 <C> 61.32 <C> 62.81 <C> 58.53 <C> 61.67 <C> 60.06 <C> 61.46 <C> 61.50 <C> 61.44 <R> <C> MLP <C> 64.53 <C> 60.98 <C> 62.71 <C> 58.50 <C> 62.14 <C> 60.26 <C> 61.51 <C> 61.56 <C> 61.48 <R> <C> WordAttn <C> 64.43 <C> 63.43 <C> 63.93 <C> 59.40 <C> 60.45 <C> 59.92 <C> 62.07 <C> 62.03 <C> 62.04 <R> <C> LangFeat <C> 63.74 <C> 75.05 <C> 68.94 <C> 64.75 <C> 51.77 <C> 57.53 <C> 64.24 <C> 63.41 <C> 63.23 <R> <C> BERTBASE <C> 78.43 <C> 80.08 <C> 79.25 <C> 76.95 <C> [BOLD] 75.12 <C> 76.02 <C> 77.69 <C> 77.60 <C> 77.63 <R> <C> BERTCONS <C> [BOLD] 79.05 <C> [BOLD] 84.64 <C> [BOLD] 81.75 <C> [BOLD] 81.14 <C> 74.65 <C> [BOLD] 77.76 <C> [BOLD] 80.09 <C> [BOLD] 79.65 <C> [BOLD] 79.95 <R> <C> Human <C> - <C> - <C> - <C> - <C> - <C> - <C> 91.3 <C> 90.6 <C> 90.9 <CAP> Table 2: Comparison of our approach BERTCONS with different baseline models for stance classification.
<R> <C> Model <C> P@10 <C> MAP <C> MRR <R> <C> word2vec averaging <C> 0.221 <C> 0.176 <C> 0.500 <R> <C> (public release 300d) <C> 0.221 <C> 0.176 <C> 0.500 <R> <C> word2vec averaging <C> 0.223 <C> 0.193 <C> 0.546 <R> <C> (academic corpus) <C> 0.223 <C> 0.193 <C> 0.546 <R> <C> Paragraph Vector <C> 0.227 <C> 0.177 <C> 0.495 <R> <C> KeyVec <C> [BOLD] 0.279 <C> [BOLD] 0.232 <C> [BOLD] 0.619 <CAP> Table 1: Evaluation of document retrieval with different embedding models
<R> <C> Train <C> No clusters <C> Dist  [ITALIC] ρ <C> Task  [ITALIC] ζ <C> Dist+Task  [ITALIC] ρ ζ <C> All  [ITALIC] ρ ζ η τ <C> All (no  [ITALIC] w0) <R> <C> WSJ <C> 91.96 <C> 93.38 <C> 93.44 <C> 93.74 <C> 93.80 <C> [BOLD] 94.03 <R> <C> +FTBL <C> 92.31 <C> 93.47 <C> 93.50 <C> 93.92 <C> [BOLD] 93.94 <C> 93.83 <CAP> Table 6: Adaptation results to the Football domain when training on both datasets.
<R> <C> [BOLD] Measurement <C> [ITALIC] κ [BOLD]  score <R> <C> Overall <C> 0.63 <R> <C> Topicality <C> 0.57 <R> <C> Informativeness <C> 0.31 <R> <C> Background <C> 0.05 <CAP> Table 5: Median κ inter-annotator agreement scores for various questions asked in the survey.
<R> <C> [BOLD] Training data % 100 % of data <C> [BOLD] Spearman 0.414 <C> p-value < 0.001 <C> [BOLD] Pearson 0.395 <C> p-value < 0.001 <R> <C> 75 % of data <C> 0.408 <C> < 0.001 <C> 0.393 <C> < 0.001 <R> <C> 50 % of data <C> 0.400 <C> < 0.001 <C> 0.391 <C> < 0.001 <R> <C> 25 % of data <C> 0.330 <C> < 0.001 <C> 0.331 <C> < 0.001 <R> <C> 10 % of data <C> 0.245 <C> < 0.001 <C> 0.265 <C> < 0.001 <R> <C> 5 % of data <C> 0.098 <C> 0.015 <C> 0.161 <C> < 0.001 <CAP> Table 10: adem correlations when trained on different amounts of data.
<R> <C> System <C> Dev SE07 <C> Test Datasets SE2 <C> Test Datasets SE3 <C> Test Datasets SE13 <C> Test Datasets SE15 <C> Concatenation of Test Datasets Noun <C> Concatenation of Test Datasets Verb <C> Concatenation of Test Datasets Adj <C> Concatenation of Test Datasets Adv <C> Concatenation of Test Datasets  [BOLD] All <R> <C> MFS baseline <C> 54.5 <C> 65.6 <C> 66.0 <C> 63.8 <C> 67.1 <C> 67.7 <C> 49.8 <C> 73.1 <C> 80.5 <C> 65.5 <R> <C> Lesk [ITALIC] ext+ [ITALIC] emb <C> 56.7 <C> 63.0 <C> 63.7 <C> 66.2 <C> 64.6 <C> 70.0 <C> 51.1 <C> 51.7 <C> 80.6 <C> 64.2 <R> <C> Babelfy <C> 51.6 <C> 67.0 <C> 63.5 <C> 66.4 <C> 70.3 <C> 68.9 <C> 50.7 <C> 73.2 <C> 79.8 <C> 66.4 <R> <C> IMS <C> 61.3 <C> 70.9 <C> 69.3 <C> 65.3 <C> 69.5 <C> 70.5 <C> 55.8 <C> 75.6 <C> 82.9 <C> 68.9 <R> <C> IMS+ [ITALIC] emb <C> 62.6 <C> 72.2 <C> 70.4 <C> 65.9 <C> 71.5 <C> 71.9 <C> 56.6 <C> 75.9 <C> 84.7 <C> 70.1 <R> <C> Bi-LSTM <C> - <C> 71.1 <C> 68.4 <C> 64.8 <C> 68.3 <C> 69.5 <C> 55.9 <C> 76.2 <C> 82.4 <C> 68.4 <R> <C> Bi-LSTM+ [ITALIC] att.+ [ITALIC] LEX+ [ITALIC] POS <C> 64.8 <C> 72.0 <C> 69.1 <C> 66.9 <C> 71.5 <C> 71.5 <C> 57.5 <C> 75.0 <C> 83.8 <C> 69.9 <R> <C> GAS [ITALIC] ext (Linear) <C> - <C> 72.4 <C> 70.1 <C> 67.1 <C> 72.1 <C> 71.9 <C> 58.1 <C> 76.4 <C> 84.7 <C> 70.4 <R> <C> GAS [ITALIC] ext (Concatenation) <C> - <C> 72.2 <C> 70.5 <C> 67.2 <C> 72.6 <C> 72.2 <C> 57.7 <C> 76.6 <C> 85.0 <C> 70.6 <R> <C> CAN [ITALIC] s <C> - <C> 72.2 <C> 70.2 <C> 69.1 <C> 72.2 <C> 73.5 <C> 56.5 <C> 76.6 <C> 80.3 <C> 70.9 <R> <C> HCAN <C> - <C> 72.8 <C> 70.3 <C> 68.5 <C> 72.8 <C> 72.7 <C> 58.2 <C> 77.4 <C> 84.1 <C> 71.1 <R> <C> SemCor, hypernyms (single) <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> 75.6 <R> <C> SemCor, hypernyms (ensemble)† <C> 69.5 <C> 77.5 <C> 77.4 <C> 76.0 <C> 78.3 <C> 79.6 <C> 65.9 <C> 79.5 <C> 85.5 <C> 76.7 <R> <C> SemCor+WNGC, hypernyms (single)‡ <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> 77.1 <R> <C> SemCor+WNGC, hypernyms (ensemble)† ‡ <C> 73.4 <C> 79.7 <C> 77.8 <C> 78.7 <C> 82.6 <C> 81.4 <C> 68.7 <C> 83.7 <C> 85.5 <C> 79.0 <R> <C> BERT(Token-CLS) <C> 61.1 <C> 69.7 <C> 69.4 <C> 65.8 <C> 69.5 <C> 70.5 <C> 57.1 <C> 71.6 <C> 83.5 <C> 68.6 <R> <C> GlossBERT(Sent-CLS) <C> 69.2 <C> 76.5 <C> 73.4 <C> 75.1 <C> 79.5 <C> 78.3 <C> 64.8 <C> 77.6 <C> 83.8 <C> 75.8 <R> <C> GlossBERT(Token-CLS) <C> 71.9 <C> 77.0 <C> [BOLD] 75.4 <C> 74.6 <C> 79.3 <C> 78.3 <C> 66.5 <C> [BOLD] 78.6 <C> 84.4 <C> 76.3 <R> <C> GlossBERT(Sent-CLS-WS) <C> [BOLD] 72.5 <C> [BOLD] 77.7 <C> 75.2 <C> [BOLD] 76.1 <C> [BOLD] 80.4 <C> [BOLD] 79.3 <C> [BOLD] 66.9 <C> 78.2 <C> [BOLD] 86.4 <C> [BOLD] 77.0 <CAP> Table 3: F1-score (%) for fine-grained English all-words WSD on the test sets in the framework of Raganato et al. (2017b) (including the development set SE07). The six blocks list the MFS baseline, two knowledge-based systems, two traditional word expert supervised systems, six recent neural-based systems, one BERT feature-based system and our systems, respectively. Results in first three blocks come from Raganato et al. (2017b), and others from the corresponding papers. † values are ensemble systems and ‡ values are models trained on both SemCor and WNGC. Bold font indicates best single model system trained on SemCor, i.e. excludes † ‡ values since it is meaningless to compare ensemble systems and models trained on two training sets with our single model trained on SemCor training set only.
<R> <C> Our Dataset (Man (Urdu)) Models <C> Our Dataset (Man (Urdu)) BLEU <C> Our Dataset (Man (Urdu)) Optimizers with Inception <C> Our Dataset (Man (Urdu)) BLEU <C> Other Datasets Dataset <C> Other Datasets BLEU <R> <C> Resnet-101-V2 <C> 0.83 <C> SGD <C> 0.33 <C> Flickr (English) <C> 0.68 <R> <C> Inception-V3 <C> 0.81 <C> ADAM <C> 0.81 <C> Man (English) <C> 0.66 <R> <C> Xception <C> 0.8 <C> RMSprop <C> 0.61 <C> Dog (Urdu) <C> 0.68 <CAP> Table 1: Table showing the BLEU performance of our technique on the different datasets. This table shows that our model is able to achieve a substantial BLEU score.
<R> <C> [BOLD] Dimension <C> [BOLD] Algorithm <C> [BOLD] Precision  [BOLD] w/o Imp <C> [BOLD] Precision  [BOLD] w/ Imp <C> [BOLD] Recall  [BOLD] w/o Imp <C> [BOLD] Recall  [BOLD] w/ Imp <C> [BOLD] F1-Score  [BOLD] w/o Imp <C> [BOLD] F1-Score  [BOLD] w/ Imp <R> <C> [BOLD] Baseline <C> NB <C> 0.88 <C> – <C> 0.82 <C> – <C> 0.84 <C> – <R> <C> Ideology (I) <C> RF <C> 0.89 <C> 0.90 <C> 0.82 <C> 0.85 <C> 0.85 <C> 0.87 <R> <C> Religion (R) <C> RF <C> 0.79 <C> 0.81 <C> 0.80 <C> 0.82 <C> 0.80 <C> 0.81 <R> <C> Hate (H) <C> RF <C> 0.84 <C> 0.88 <C> 0.85 <C> 0.86 <C> 0.85 <C> 0.87 <R> <C> I <C> NB <C> 0.80 <C> 0.88 <C> 0.71 <C> 0.75 <C> 0.75 <C> 0.81 <R> <C> R <C> NB <C> 0.70 <C> 0.79 <C> 0.71 <C> 0.74 <C> 0.75 <C> 0.76 <R> <C> H <C> NB <C> 0.79 <C> 0.80 <C> 0.81 <C> 0.85 <C> 0.80 <C> 0.83 <R> <C> I+H <C> RF <C> 0.88 <C> 0.90 <C> 0.85 <C> 0.87 <C> 0.86 <C> 0.89 <R> <C> I+R <C> RF <C> 0.84 <C> 0.90 <C> [BOLD] 0.87 <C> 0.89 <C> 0.86 <C> 0.89 <R> <C> R+H <C> RF <C> 0.85 <C> 0.91 <C> [BOLD] 0.87 <C> [BOLD] 0.90 <C> 0.86 <C> 0.91 <R> <C> I+H <C> NB <C> 0.88 <C> 0.88 <C> 0.83 <C> 0.85 <C> 0.85 <C> 0.86 <R> <C> I+R <C> NB <C> 0.86 <C> 0.89 <C> 0.81 <C> 0.87 <C> 0.83 <C> 0.88 <R> <C> R+H <C> NB <C> 0.81 <C> 0.92 <C> 0.80 <C> 0.84 <C> 0.80 <C> 0.88 <R> <C> [BOLD] R+I+H <C> [BOLD] RF <C> [BOLD] 0.95 <C> [BOLD] 0.97 <C> 0.86 <C> 0.89 <C> [BOLD] 0.91 <C> [BOLD] 0.93 <R> <C> R+I+H <C> NB <C> 0.90 <C> 0.91 <C> 0.82 <C> 0.82 <C> 0.86 <C> 0.87 <CAP> Table 5. Results of the uni-bi-tri-dimensional models with and without imputation (Imp). The models without imputation were created based on 879 users after the removal of 49 identified outlier users and 148 users with sparse representations as described in section 5.1. The models with imputation were created based on the 1027 users after the removal of the 49 identified outlier users.
<R> <C> Model <C> R-1 <C> R-2 <C> R-L <R> <C> Lead3 <C> 40.34 <C> 17.70 <C> 36.57 <R> <C> Lead3 Nallapati et al. ( 2017 ) <C> 39.20 <C> 15.70 <C> 35.50 <R> <C> [ITALIC] abstract <C> 35.46 <C> 13.30 <C> 32.65 <R> <C> [ITALIC] pointer+coverage <C> 39.53 <C> 17.28 <C> 36.38 <R> <C> [ITALIC] abstract-RL <C> [BOLD] 41.16 <C> 15.75 <C> [BOLD] 39.08 <R> <C> [ITALIC] abstract-ML+RL <C> 39.87 <C> 15.82 <C> 36.90 <R> <C> [ITALIC] SummaRuNNer <C> 39.60 <C> 16.20 <C> 35.30 <R> <C> extract-cnn <C> 40.11 <C> 17.52 <C> 36.39 <R> <C> Refresh Narayan et al. ( 2018 ) <C> 40.00 <C> 18.20 <C> 36.60 <R> <C> extract <C> 40.62 <C> 18.45 <C> 37.14 <R> <C> latent <C> 41.05 <C> [BOLD] 18.77 <C> 37.54 <R> <C> latent+compress <C> 36.69 <C> 15.43 <C> 34.33 <CAP> Table 1: Results of different models on the CNN/Dailymail test set using full-length F1 Rouge-1 (R-1), Rouge-2 (R-2), and Rouge-L (R-L).
<R> <C> [BOLD] model <C> [BOLD] Rep. <C> [BOLD] Cor. <C> [BOLD] Res. <C> [BOLD] All <R> <C> CNN <C> 93.3 <C> 66.0 <C> 57.1 <C> 80.4 <R> <C> ACNN <C> 97.5 <C> 80.0 <C> 57.1 <C> 88.9 <CAP> Table 3: F-scores for different types of disfluencies on a subset of the Switchboard dev set containing 140 disfluent structures — including 85 repetitions (Rep.), 51 corrections (Cor.) and 4 restarts (Res.).
<R> <C> Model <C> EN <C> DE <C> FR <C> IT <C> ES <C> PT <C> FI <C> Avg <R> <C> [BOLD] SRC <C> [BOLD] SRC <C> [BOLD] SRC <C> [BOLD] SRC <C> [BOLD] SRC <C> [BOLD] SRC <C> [BOLD] SRC <C> [BOLD] SRC <C> [BOLD] SRC <R> <C> Emb <C> 50.3 <C> 49.2 <C> 52.4 <C> 44.9 <C> 46.7 <C> 51.0 <C> 36.4 <C> 47.3 <R> <C> BERT <C> 51.8 <C> 50.6 <C> 54.0 <C> 45.3 <C> 51.3 <C> 51.8 <C> 38.1 <C> 49.0 <R> <C> ELMo <C> 53.6 <C> 51.6 <C> 56.7 <C> 51.3 <C> 57.4 <C> 52.6 <C> 39.7 <C> 51.8 <R> <C> [BOLD] TGT <C> [BOLD] TGT <C> [BOLD] TGT <C> [BOLD] TGT <C> [BOLD] TGT <C> [BOLD] TGT <C> [BOLD] TGT <C> [BOLD] TGT <C> [BOLD] TGT <R> <C> Emb <C> 56.5 <C> 51.6 <C> 55.2 <C> 47.1 <C> 50.0 <C> 53.2 <C> 40.4 <C> 50.6 <R> <C> BERT <C> 59.8 <C> 55.5 <C> 57.0 <C> 52.6 <C> 54.3 <C> 56.6 <C> 44.0 <C> 54.3 <R> <C> ELMo <C> 60.7 <C> 57.8 <C> 59.9 <C> 54.8 <C> 56.7 <C> 58.8 <C> 46.9 <C> 56.5 <R> <C> [BOLD] SRC & TGT (ELMo) <C> [BOLD] SRC & TGT (ELMo) <C> [BOLD] SRC & TGT (ELMo) <C> [BOLD] SRC & TGT (ELMo) <C> [BOLD] SRC & TGT (ELMo) <C> [BOLD] SRC & TGT (ELMo) <C> [BOLD] SRC & TGT (ELMo) <C> [BOLD] SRC & TGT (ELMo) <C> [BOLD] SRC & TGT (ELMo) <R> <C> [ITALIC] BASIC <C> 61.9 <C> 64.8 <C> 60.3 <C> 56.4 <C> 61.1 <C> 63.1 <C> 50.7 <C> 59.8 <R> <C> [ITALIC] PGN <C> [BOLD] 65.7 <C> [BOLD] 68.8 <C> 66.1 <C> 64.8 <C> [BOLD] 68.7 <C> [BOLD] 69.2 <C> [BOLD] 58.6 <C> [BOLD] 66.0 <R> <C> [ITALIC] MoE <C> 63.2 <C> 67.8 <C> 63.1 <C> 62.6 <C> 65.2 <C> 67.5 <C> 54.2 <C> 63.4 <R> <C> [ITALIC] MAN-MoE <C> 64.0 <C> 68.5 <C> [BOLD] 67.2 <C> [BOLD] 65.7 <C> 67.5 <C> 69.0 <C> 57.5 <C> 65.6 <CAP> Table 3: Cross-lingual transfer with multiple sources.
<R> <C> Source <C> EN <C> DE <C> FR <C> IT <C> ES <C> PT <C> FI <R> <C> EN <C> [EMPTY] <C> [BOLD] 65.0 <C> 64.8 <C> 58.7 <C> 62.5 <C> 56.0 <C> [BOLD] 54.5 <R> <C> DE <C> [BOLD] 63.2 <C> [EMPTY] <C> 63.9 <C> 60.4 <C> 65.8 <C> 53.4 <C> 50.5 <R> <C> FR <C> 60.1 <C> 53.7 <C> [EMPTY] <C> 63.3 <C> 63.6 <C> 62.1 <C> 51.3 <R> <C> IT <C> 60.2 <C> 58.9 <C> [BOLD] 65.3 <C> [EMPTY] <C> 65.1 <C> 58.6 <C> 48.6 <R> <C> ES <C> 60.1 <C> 57.3 <C> 64.9 <C> [BOLD] 64.1 <C> [EMPTY] <C> [BOLD] 67.0 <C> 50.7 <R> <C> PT <C> 57.3 <C> 58.6 <C> 65.1 <C> 63.5 <C> [BOLD] 67.8 <C> [EMPTY] <C> 40.9 <R> <C> FI <C> 50.7 <C> 52.1 <C> 64.6 <C> 53.6 <C> 60.3 <C> 51.6 <C> [EMPTY] <R> <C> ALL <C> 65.7 <C> 68.8 <C> 66.1 <C> 64.8 <C> 68.7 <C> 69.2 <C> 58.6 <CAP> Table 4: The results of bilingual transferring.
<R> <C> Metrics <C> Vanilla <C> EI <C> EI +Attn <C> EI+Attn +Chat <R> <C> DA (p/r/f1) <C> 83.5 77.9 80.5 <C> 79.7 80.1 80.0 <C> 80.0 83.1 81.5 <C> 81.8 83.5 82.7 <R> <C> Slot (p/r/f1) <C> 42.0 30.3 35.2 <C> 60.6 63.6 62.1 <C> 63.7 64.7 64.2 <C> 64.6 69.1 66.8 <R> <C> KB (p/r/f1) <C> [EMPTY] <C> 48.9 55.3 51.9 <C> 55.4 70.8 62.2 <C> 58.2 71.9 64.4 <R> <C> BLEU <C> 36.9 <C> 54.6 <C> 59.3 <C> 60.5 <CAP> Table 1: Performance of each model on automatic measures.
<R> <C> Annotator <C> 1 <C> 2 <C> 3 <C> 4 <C> 5 <C> 6 <C> 7 <C> 8 <C> 9 <C> 10 <C> 11 <C> 12 <C> Avg. <R> <C> 1 <C> 1 <C> 0.73 <C> 0.9 <C> 0.88 <C> 0.55 <C> 0.77 <C> 1 <C> 0.94 <C> 0.9 <C> 0.28 <C> 0.95 <C> 0.67 <C> .80 <R> <C> 2 <C> 0.73 <C> 1 <C> 0.78 <C> 0.83 <C> 0.95 <C> 0.62 <C> 0.77 <C> 0.75 <C> 1 <C> 0.27 <C> 0.8 <C> 0.85 <C> .78 <R> <C> 3 <C> 0.9 <C> 0.78 <C> 1 <C> 0.85 <C> 0.64 <C> 0.8 <C> 0.9 <C> 0.82 <C> 0.96 <C> 0.3 <C> 0.85 <C> 0.72 <C> .79 <R> <C> 4 <C> 0.88 <C> 0.83 <C> 0.85 <C> 1 <C> 0.6 <C> 0.88 <C> 0.83 <C> 0.92 <C> 1 <C> 0.33 <C> 0.91 <C> 0.68 <C> .81 <R> <C> 5 <C> 0.55 <C> 0.95 <C> 0.64 <C> 0.6 <C> 1 <C> 0.46 <C> 0.64 <C> 0.55 <C> 0.83 <C> 0.23 <C> 0.59 <C> 0.85 <C> .66 <R> <C> 6 <C> 0.77 <C> 0.62 <C> 0.8 <C> 0.88 <C> 0.46 <C> 1 <C> 0.88 <C> 0.74 <C> 0.88 <C> 0.16 <C> 0.75 <C> 0.6 <C> .71 <R> <C> 7 <C> 1 <C> 0.77 <C> 0.9 <C> 0.83 <C> 0.64 <C> 0.88 <C> 1 <C> 0.94 <C> 1 <C> 0.2 <C> 1 <C> 0.67 <C> .82 <R> <C> 8 <C> 0.94 <C> 0.75 <C> 0.82 <C> 0.92 <C> 0.55 <C> 0.74 <C> 0.94 <C> 1 <C> 1 <C> 0.36 <C> 0.94 <C> 0.7 <C> .81 <R> <C> 9 <C> 0.9 <C> 1 <C> 0.96 <C> 1 <C> 0.83 <C> 0.88 <C> 1 <C> 1 <C> 1 <C> 0 <C> 1 <C> 0.81 <C> .87 <R> <C> 10 <C> 0.28 <C> 0.27 <C> 0.3 <C> 0.33 <C> 0.23 <C> 0.16 <C> 0.2 <C> 0.36 <C> 0 <C> 1 <C> 0.11 <C> 0.12 <C> .28 <R> <C> 11 <C> 0.95 <C> 0.8 <C> 0.85 <C> 0.91 <C> 0.59 <C> 0.75 <C> 1 <C> 0.94 <C> 1 <C> 0.11 <C> 1 <C> 0.68 <C> .80 <R> <C> 12 <C> 0.67 <C> 0.85 <C> 0.72 <C> 0.68 <C> 0.85 <C> 0.6 <C> 0.67 <C> 0.7 <C> 0.81 <C> 0.12 <C> 0.68 <C> 1 <C> .70 <R> <C> Total <C> .85 <C> .86 <C> .86 <C> .9 <C> .82 <C> .77 <C> .98 <C> .92 <C> .96 <C> .61 <C> .94 <C> .82 <C> [EMPTY] <CAP> Table 3: Pair-wise and total agreement by annotator. The ‘Total’ row shows agreement with the set of all other annotators and the ‘Avg.’ column is the average pairwise agreement.
<R> <C> Language <C> Degradation None <C> Degradation Light <C> Degradation Heavy <R> <C> DE <C> 69.6 <C> 69.5 <C> 68.2 <R> <C> EN <C> 79.5 <C> 77.5 <C> 72.8 <R> <C> ES <C> 78.0 <C> 76.5 <C> 71.5 <R> <C> FR <C> 82.6 <C> 82.3 <C> 75.7 <R> <C> IT <C> 82.0 <C> 81.8 <C> 77.4 <R> <C> PT-BR <C> 80.6 <C> 80.0 <C> 72.4 <R> <C> SV <C> 77.7 <C> 77.1 <C> 76.9 <R> <C> Average <C> 78.5 <C> 77.8 <C> 73.6 <CAP> Table 4: Simulated degradation results. Light has 40% of arcs removed and Heavy has 70% removed.
<R> <C> Parser <C> Features <C> Gold Tags EN <C> Gold Tags ES <C> Predicted Tags EN <C> Predicted Tags ES <R> <C> RB <C> [EMPTY] <C> 17.1 <C> 28.0 <C> 17.1 <C> 28.0 <R> <C> Gibbs <C> GFL <C> 60.2 <C> 65.3 <C> 55.8 <C> 52.7 <R> <C> ConvexMST <C> UG <C> 63.1 <C> 63.5 <C> 56.9 <C> 50.0 <R> <C> ConvexMST <C> GFL <C> 65.9 <C> 70.5 <C> 61.2 <C> 67.1 <R> <C> ConvexMST <C> UG+GFL <C> [BOLD] 68.2 <C> [BOLD] 71.3 <C> [BOLD] 63.2 <C> [BOLD] 67.3 <CAP> Table 5: Directed dependency accuracy on English and Spanish universal treebanks using annotator provided GFL annotations, 10 or fewer words.
<R> <C> Feature Set <C> Partial Annotations <C> Full Annotations <R> <C> UG <C> 56.9 <C> 58.8 <R> <C> GFL <C> 61.2 <C> 62.8 <R> <C> GFL+UG <C> [BOLD] 63.2 <C> [BOLD] 66.6 <CAP> Table 6: Comparison between full and partial annotations, 10 or fewer words, using predicted POS tags.
<R> <C> [BOLD] category <C> [BOLD] DeepSUN  [BOLD] dev <C> [BOLD] DeepSUN  [BOLD] test <C> [BOLD] DeepSUN  [BOLD] train <R> <C> pers <C> 6719 <C> 4766 <C> 22115 <R> <C> func <C> 1830 <C> 1425 <C> 6628 <R> <C> org <C> 5133 <C> 3506 <C> 15804 <R> <C> loc <C> 5195 <C> 3915 <C> 18159 <R> <C> prod <C> 652 <C> 606 <C> 2317 <R> <C> time <C> 3763 <C> 2769 <C> 12020 <R> <C> amount <C> 1591 <C> 1450 <C> 5959 <R> <C> event <C> 79 <C> 0 <C> 321 <R> <C> [BOLD] Sum <C> [BOLD] 24962 <C> [BOLD] 18437 <C> [BOLD] 83323 <CAP> Table 1: Distribution of named entities by categories in the DeepSUN corpus
<R> <C> [BOLD] Modeling Method <C> [ITALIC] R2 [BOLD]  Score <R> <C> Linear Regression <C> 0.31 <R> <C> Support Vector Regression <C> 0.25 <CAP> TABLE I: Evaluation results
<R> <C> [BOLD] Method <C> [BOLD] Target Speaker <C> [BOLD] Naturalness <C> [BOLD] Similarity <R> <C> f0 <C> singing <C> 3.23 <C> 3.00 <R> <C> [ITALIC] f0 <C> speech <C> 2.77 <C> 2.84 <R> <C> [ITALIC] f0 + RMSE <C> singing <C> 3.80 <C> 3.65 <R> <C> [ITALIC] f0 + RMSE <C> speech <C> 3.42 <C> 3.49 <CAP> Table 1: MOS for Singing Conversion Quality and Similarity. Target speaker type indicates what types of samples we used for singing voice conversion from target speaker. ’singing’ means the singing samples from target speaker are used for singing voice conversion, and ’speech’ means speech samples from target speaker are used for singing voice conversion.
<R> <C> [EMPTY] <C> BLEU-1 <C> BLEU-2 <C> BLEU-3 <C> BLEU-4 <C> METEOR <C> ROUGE-L <C> CIDEr <R> <C> Baseline  <C> 0.273 <C> 0.173 <C> 0.118 <C> 0.084 <C> 0.117 <C> 0.291 <C> 0.766 <R> <C> Basic Attention (D) <C> 0.317 <C> 0.185 <C> 0.121 <C> 0.083 <C> 0.132 <C> 0.330 <C> 0.760 <R> <C> Basic Attention (D, S, C) <C> 0.318 <C> 0.183 <C> 0.120 <C> 0.083 <C> 0.122 <C> 0.318 <C> 0.765 <R> <C> DMN (A, V, D, S, C) <C> 0.316 <C> 0.187 <C> 0.127 <C> 0.089 <C> 0.123 <C> 0.327 <C> 0.821 <R> <C> Entropy-Enhanced DMN (A, V, D) <C> 0.320 <C> 0.185 <C> 0.121 <C> 0.083 <C> 0.121 <C> 0.319 <C> 0.736 <R> <C> Entropy-Enhanced DMN (A, D, S, C) <C> 0.329 <C> 0.194 <C> 0.130 <C> 0.093 <C> 0.127 <C> 0.330 <C> 0.858 <R> <C> Entropy-Enhanced DMN (V, D, S, C) <C> 0.323 <C> 0.184 <C> 0.119 <C> 0.081 <C> 0.123 <C> 0.321 <C> 0.707 <R> <C> Entropy-Enhanced DMN (A, V, D, S, C, no d.m.) <C> [BOLD] 0.333 <C> 0.194 <C> 0.128 <C> 0.091 <C> 0.127 <C> 0.327 <C> 0.780 <R> <C> Entropy-Enhanced DMN (A, V, D, S, C) <C> 0.331 <C> [BOLD] 0.196 <C> 0.130 <C> 0.091 <C> 0.128 <C> 0.333 <C> 0.843 <R> <C> Entropy-Enhanced DMN (A, V, D, S, C,  [ITALIC] M=3) <C> 0.329 <C> 0.195 <C> [BOLD] 0.131 <C> [BOLD] 0.093 <C> [BOLD] 0.129 <C> [BOLD] 0.334 <C> [BOLD] 0.880 <CAP> Table 1: The first part is the official objective evaluation values. The second part contain two subsection and both of them don’t have audio or visual information. In the third part, each subsection will all have at least one video features. DMN represents the general DMN, and Entropy-Enhanced DMN will be our proposed model. Each DMN will have M=2: two updating times in a DMN. In the final row of the third part, M=3 means we increase to three times for DMN to update. The capital letter in the parentheses means the given modality. (D: dialogue history; S: summary; C: caption, A: audio, V: visual)
<R> <C> [EMPTY] <C> BLEU-1 <C> BLEU-2 <C> BLEU-3 <C> BLEU-4 <C> METEOR <C> ROUGE-L <C> CIDEr <C> Human <R> <C> Baseline  <C> 0.626 <C> 0.485 <C> 0.383 <C> 0.309 <C> 0.215 <C> 0.487 <C> 0.746 <C> 2.848 <R> <C> Entropy-Enhanced DMN (A, V, D, S, C) <C> [BOLD] 0.641 <C> [BOLD] 0.493 <C> [BOLD] 0.388 <C> [BOLD] 0.310 <C> [BOLD] 0.241 <C> [BOLD] 0.527 <C> [BOLD] 0.912 <C> [BOLD] 3.048 <CAP> Table 2: This table is the final result released by the official.
<R> <C> [EMPTY] <C> [EMPTY] <C> Baseline ASR <C> Baseline BiDAF <C> MI-SubQuery TopHyp <C> MI-SubQuery CNN <C> Base-NMT TopHyp <C> Base-NMT CNN <C> AQA TopHyp <C> AQA Voting <C> AQA MaxConf <C> AQA CNN <C> Human <R> <C> Dev <C> EM <C> - <C> 31.7 <C> 24.1 <C> 37.5 <C> 26.0 <C> 37.5 <C> 32.0 <C> 33.6 <C> 35.5 <C> [BOLD] 40.5 <C> - <R> <C> Dev <C> F1 <C> 24.2 <C> 37.9 <C> 29.9 <C> 44.5 <C> 32.2 <C> 44.8 <C> 38.2 <C> 40.5 <C> 42.0 <C> [BOLD] 47.4 <C> - <R> <C> Test <C> EM <C> - <C> 28.6 <C> 23.2 <C> 35.8 <C> 24.8 <C> 35.7 <C> 30.6 <C> 33.3 <C> 33.8 <C> [BOLD] 38.7 <C> 43.9 <R> <C> Test <C> F1 <C> 22.8 <C> 34.6 <C> 29.0 <C> 42.8 <C> 31.0 <C> 42.9 <C> 36.8 <C> 39.3 <C> 40.2 <C> [BOLD] 45.6 <C> - <CAP> Table 1: Results table for the experiments on SearchQA. Two-sample t-tests between the AQA results and either the Base-NMT or the MI-SubQuery results show that differences in F1 and Exact Match scores are statistically significant, p<10−4, for both Top Hypothesis and CNN predictions. The difference between Base-NMT and MI-SubQuery is also significant for Top Hypothesis predictions.
<R> <C> [EMPTY] <C> CA <C> WSC <C> SM <C> SMR <C> SWAG <C> HellaSwag <C> ARCT1 <C> ARCT2 <C> Average <R> <C> RANDOM <C> 0.500 <C> 0.500 <C> 0.500 <C> 0.333 <C> 0.250 <C> 0.250 <C> 0.500 <C> 0.500 <C> 0.416 <R> <C> GPT <C> 0.830 <C> 0.558 <C> 0.735 <C> 0.354 <C> 0.592 <C> 0.263 <C> 0.472 <C> 0.528 <C> 0.542 <R> <C> GPT2-base <C> 0.787 <C> 0.512 <C> 0.705 <C> 0.355 <C> 0.503 <C> 0.300 <C> 0.466 <C> 0.509 <C> 0.517 <R> <C> GPT2-medium <C> 0.885 <C> 0.568 <C> 0.746 <C> 0.385 <C> 0.591 <C> 0.338 <C> 0.462 <C> 0.527 <C> 0.563 <R> <C> BERT-base <C> 0.891 <C> 0.523 <C> 0.697 <C> 0.419 <C> 0.625 <C> 0.373 <C> 0.477 <C> 0.503 <C> 0.563 <R> <C> BERT-large <C> 0.934 <C> 0.625 <C> 0.694 <C> 0.444 <C> 0.696 <C> 0.393 <C> 0.468 <C> 0.517 <C> 0.596 <R> <C> XLNet-base <C> 0.809 <C> 0.544 <C> 0.662 <C> 0.374 <C> 0.494 <C> 0.381 <C> 0.516 <C> 0.526 <C> 0.543 <R> <C> XLNet-large <C> 0.891 <C> 0.636 <C> 0.583 <C> 0.394 <C> 0.662 <C> 0.435 <C> 0.563 <C> 0.570 <C> 0.591 <R> <C> RoBERTa-base <C> 0.901 <C> 0.623 <C> 0.750 <C> 0.423 <C> 0.712 <C> 0.414 <C> 0.501 <C> 0.537 <C> 0.565 <R> <C> RoBERTa-large <C> 0.962 <C> 0.694 <C> 0.792 <C> 0.512 <C> 0.769 <C> 0.5 <C> 0.606 <C> 0.599 <C> 0.679 <R> <C> HUMAN <C> 0.993 <C> 0.920 <C> 0.991 <C> 0.975 <C> 0.880 <C> 0.945 <C> 0.909 <C> 0.909 <C> 0.945 <CAP> Table 3: Accuracy for each pre-trained contextualizer on each test set. The rightmost column shows the average of accuracy score of each model.
<R> <C> Method <C> Austro-Asiatic <C> Austronesian <C> Indo-European <C> Pama-Nyungan <C> Sino-Tibetan <R> <C> CCM <C> 0.71 <C> 0.7 <C> 0.75 <C> 0.74 <C> 0.48 <R> <C> NED <C> 0.73 <C> 0.77 <C> 0.69 <C> 0.53 <C> 0.49 <R> <C> SCA <C> 0.76 <C> 0.78 <C> 0.81 <C> 0.71 <C> 0.56 <R> <C> LexStat <C> 0.76 <C> 0.84 <C> 0.83 <C> 0.84 <C> 0.6 <R> <C> OnlinePMI <C> 0.76 <C> 0.81 <C> 0.82 <C> 0.72 <C> 0.56 <R> <C> SVM <C> 0.82 <C> 0.81 <C> 0.79 <C> 0.86 <C> 0.5 <CAP> Table 2: B-cubed F-scores for different cognate detection methods across the language families.
<R> <C> Model <C> English/Spanish en-US <C> English/Spanish es-US <C> English/Hindi en-IN <C> English/Hindi hi-IN <R> <C> baseline acoustic-only <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <R> <C> + SSL <C> 48.4 <C> 14.71 <C> 20.0 <C> 17.6 <CAP> Table 4: %Relative Error Rate Reduction of the acoustic-only LID model retrained after data augmentation
<R> <C> [BOLD] Experimental Setup <C> [ITALIC]  [BOLD] Variant <C> [ITALIC]  [BOLD] Features <R> <C> [BOLD] Single Training <C> 1 <C> lower <R> <C> [BOLD] Joint Training <C> 1 <C> lower <R> <C> [EMPTY] <C> 2 <C> lemma <R> <C> [BOLD] Optimized <C> 3 <C> lemma_lower <R> <C> [BOLD] Training <C> 4 <C> lemmapos <R> <C> [EMPTY] <C> 5 <C> lemmapos_lower <CAP> TABLE III: Embedding Variants per Experimental Setups
<R> <C> [EMPTY] <C> [BOLD] Rare <C> [BOLD] Medium <C> [BOLD] Frequent <R> <C> training types <C> 12K <C> 1K <C> 386 <R> <C> training tokens <C> 30K <C> 56K <C> 200K <R> <C> [ITALIC] Precision (%): <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Weiss et al. <C> 81.1 <C> 76.3 <C> 79.6 <R> <C> 160hrs <C> 87.5 <C> 69 <C> 65.7 <R> <C> [ITALIC] Recall (%): <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Weiss et al. <C> 24.5 <C> 65.4 <C> 78.1 <R> <C> 160hrs <C> 1.1 <C> 36.9 <C> 64.4 <CAP> Table 3: Content word frequency vs. dev precision/recall. Rare words have ≤10 tokens per type in the training text; medium have 25–100 tokens; frequent have ≥150 tokens.
<R> <C> [BOLD] System <C> [BOLD] Top-1 (%) <C> [BOLD] Top-5 (%) <R> <C> CNN on spectrograms, trained on face-verified segments  <C> 80.5 <C> 92.1 <R> <C> DNN on i-vectors, weakly supervised <C> 94.6 <C> 98.1 <CAP> Table 2: Speaker identification accuracies on the VoxCeleb test set.
<R> <C> [BOLD] Model <C> [BOLD] WER % <C> [BOLD] CER % <R> <C> Random <C> 18.54 <C> 5.15 <R> <C> GloVe  <C> 18.54 <C> 6.94 <R> <C> Word2Vec  <C> 18.54 <C> 5.49 <R> <C> C2V (proposed) <C> 18.54 <C> [BOLD] 4.70 <R> <C> Schumann and Angkititrakul  <C> 10.55 <C> 5.04\getrefnumberfootnote\getrefnumberfootnotefootnotemark: footnote <CAP> Table 2: Results with Training and Testing on ASR transcripts.
<R> <C> City <C> Accuracy <C> Precision <C> Recall <C> F1 <C> AUC PR <C> AUC ROC <R> <C> Melbourne <C> 0.873 <C> 0.770 <C> 0.820 <C> 0.793 <C> 0.688 <C> 0.913 <R> <C> Sydney <C> 0.860 <C> 0.707 <C> 0.770 <C> 0.719 <C> 0.640 <C> 0.897 <R> <C> Brisbane <C> 0.855 <C> 0.514 <C> 0.612 <C> 0.528 <C> 0.449 <C> 0.791 <R> <C> Perth <C> 0.903 <C> 0.605 <C> 0.797 <C> 0.686 <C> 0.574 <C> 0.886 <R> <C> Jakarta <C> 0.762 <C> 0.816 <C> 0.706 <C> 0.705 <C> 0.735 <C> 0.860 <CAP> TABLE III: The average results of event detection in multiple cities using multiple metrics after cross validating the results on 10 folds
<R> <C> [EMPTY] <C> Setup 1 softmax <C> Setup 1 CRF <C> Setup 2 softmax <C> Setup 2 CRF <C> Setup 3 softmax <C> Setup 3 CRF <R> <C> Peop <C> [BOLD] 95.24 <C> 94.95 <C> 93.99 <C> [BOLD] 94.47 <C> 91.46 <C> [BOLD] 92.21 <R> <C> Org <C> [BOLD] 88.94 <C> 87.56 <C> 78.95 <C> [BOLD] 79.37 <C> 67.29 <C> [BOLD] 67.91 <R> <C> Loc <C> 93.25 <C> [BOLD] 93.63 <C> 90.69 <C> [BOLD] 90.80 <C> 85.99 <C> [BOLD] 86.20 <R> <C> Other <C> [BOLD] 90.38 <C> 89.54 <C> 73.78 <C> [BOLD] 73.97 <C> [BOLD] 62.67 <C> 61.19 <R> <C> Avg EC <C> [BOLD] 91.95 <C> 91.42 <C> 84.35 <C> [BOLD] 84.65 <C> 76.85 <C> [BOLD] 76.88 <R> <C> Located_in <C> 55.03 <C> [BOLD] 57.72 <C> 51.03 <C> [BOLD] 55.13 <C> 44.96 <C> [BOLD] 52.29 <R> <C> Work_for <C> [BOLD] 71.23 <C> 70.67 <C> 52.89 <C> [BOLD] 61.42 <C> 52.63 <C> [BOLD] 65.31 <R> <C> OrgBased_in <C> 53.25 <C> [BOLD] 59.38 <C> 56.96 <C> [BOLD] 59.12 <C> 46.15 <C> [BOLD] 57.65 <R> <C> Live_in <C> [BOLD] 59.57 <C> 58.94 <C> [BOLD] 64.29 <C> 60.12 <C> [BOLD] 64.09 <C> 61.45 <R> <C> Kill <C> 74.70 <C> [BOLD] 79.55 <C> 69.14 <C> [BOLD] 74.73 <C> [BOLD] 82.93 <C> 75.86 <R> <C> Avg RE <C> 62.76 <C> [BOLD] 65.25 <C> 58.86 <C> [BOLD] 62.10 <C> 58.15 <C> [BOLD] 62.51 <R> <C> Avg EC+RE <C> 77.36 <C> [BOLD] 78.33 <C> 71.61 <C> [BOLD] 73.38 <C> 67.50 <C> [BOLD] 69.69 <CAP> Table 1: F1 results for entity classification (EC) and relation extraction (RE) in the three setups
<R> <C> [EMPTY] <C> CIDEr <C> METEOR <C> SPICE <R> <C> FC <C> 0.258 <C> 0.240 <C> 0.318 <R> <C> Att2In <C> 0.228 <C> 0.210 <C> 0.284 <R> <C> TopDown <C> 0.185 <C> 0.168 <C> 0.215 <CAP> Table 4: Pearson correlation coefficients between 1-CHs and CIDEr, METEOR, and SPICE scores, see Section 3.4.
<R> <C> [EMPTY] <C> Metric <C> Metric +(1-CHs) <C> Metric +(1-CHi) <R> <C> METEOR <C> 0.269 <C> 0.299 <C> 0.304 <R> <C> CIDEr <C> 0.282 <C> 0.321 <C> 0.322 <R> <C> SPICE <C> 0.248 <C> 0.277 <C> 0.281 <CAP> Table 5: Pearson correlation coefficients between individual/combined metrics and human scores. See Section 3.4.
<R> <C> Datasets <C> [ITALIC] Respect <C> [ITALIC] Occ. <C> Both <R> <C> sentiment ann. vs.  [ITALIC] regard ann. <C> 0.95 <C> 0.70 <C> 0.82 <R> <C> VADER pred. vs. sentiment ann. <C> 0.78 <C> 0.71 <C> 0.74 <R> <C> VADER pred. vs.  [ITALIC] regard ann. <C> 0.69 <C> 0.54 <C> 0.61 <CAP> Table 5: Spearman’s correlation between sentiment vs. regard, and between predictions from an off-the-shelf VADER sentiment classifier vs. annotated scores. Occ. is occupation context.
<R> <C> [BOLD] Model <C> [BOLD] Accuracy dev <C> [BOLD] Accuracy test <C> [ITALIC] F1 dev <C> [ITALIC] F1 test <R> <C> Bi-GRU <C> 90.62 <C> 90.18 <C> 20.02 <C> 20.20 <R> <C> CNN <C> 92.41 <C> 91.64 <C> [BOLD] 30.99 <C> 29.05 <R> <C> Self-Attentive <C> 92.12 <C> 91.85 <C> 21.26 <C> 22.61 <R> <C> Hierarchical CNN-GRU <C> 91.56 <C> 91.30 <C> 24.80 <C> 24.44 <R> <C> Jumper <C> 92.43 <C> 92.42 <C> 26.57 <C> 29.60 <R> <C> Jumper-sharing <C> [BOLD] 92.71 <C> [BOLD] 92.65 <C> 27.57 <C> [BOLD] 30.52 <CAP> Table 6: The average accuracy and F1 on the OI dataset using the decision-sharing mechanism.
<R> <C> [BOLD] F1 <C> [BOLD] Bias (binary) <C> [BOLD] Bias (ternary) <C> [BOLD] Hate <R> <C> Term Matching <C> - <C> - <C> 0.195 <R> <C> CharCNN <C> 0.547 <C> 0.535 <C> 0.415 <R> <C> BiLSTM <C> 0.302 <C> 0.291 <C> 0.340 <R> <C> BERT <C> [BOLD] 0.681 <C> [BOLD] 0.633 <C> [BOLD] 0.525 <R> <C> BERT (+ bias) <C> - <C> - <C> [BOLD] 0.569 <CAP> Table 2: F1 score of benchmarks on the test set. Note that the term matching model checks the presence of hate or offensiveness. Therefore, in this case, we combine hate and offensive into a single category, turning the original ternary task into binary.
<R> <C> [BOLD] F1 <C> [BOLD] Gender <C> [BOLD] Others <C> [BOLD] None <C> [BOLD] Bias (ternary) <R> <C> CharCNN <C> 0.519 <C> 0.259 <C> 0.826 <C> 0.535 <R> <C> BiLSTM <C> 0.055 <C> 0.000 <C> 0.819 <C> 0.291 <R> <C> BERT <C> 0.693 <C> 0.326 <C> 0.880 <C> [BOLD] 0.633 <CAP> Table 3: Detailed results on macro-F1 of Bias (ternary)
<R> <C> [EMPTY] <C> [BOLD] Model <C> [BOLD] Goal Progress (m) ↑  [ITALIC] tO <C> [BOLD] Goal Progress (m) ↑  [ITALIC] QAi-1 <C> [BOLD] Goal Progress (m) ↑  [ITALIC] QA1: [ITALIC] i-1 <C> [BOLD] Goal Progress (m) ↑ + Oracle Stopping <C> [BOLD] BLEU ↑  [ITALIC] QAi-1 <C> [BOLD] BLEU ↑  [ITALIC] QA1: [ITALIC] i-1 <R> <C> Val Seen <C> Baseline <C> [BOLD] 20.1 <C> 10.5 <C>  [BOLD] 15.0 <C> [BOLD] 22.9 <C> 0.9 <C> 0.8 <R> <C> Val Seen <C> Data Aug. <C> 20.1 <C> 10.5 <C> 10.0 <C> 14.2 <C> 1.3 <C> 1.3 <R> <C> Val Seen <C> [BOLD] RMM [ITALIC] N=1 <C> 18.7 <C> 10.0 <C> 13.3 <C> 20.4 <C> 3.3 <C> 3.0 <R> <C> Val Seen <C> [BOLD] RMM [ITALIC] N=3 <C> 18.9 <C> [BOLD] 11.5 <C> 14.0 <C> 16.8 <C> [BOLD] 3.4 <C>  [BOLD] 3.6 <R> <C> Val Seen <C> Shortest Path <C> ———– 32.8 ———– <C> ———– 32.8 ———– <C> ———– 32.8 ———– <C> ———– 32.8 ———– <C> [EMPTY] <C> [EMPTY] <R> <C> Val Unseen <C> Baseline <C> 6.8 <C> 4.7 <C> 04.6 <C> 6.3 <C> 0.5 <C> 0.5 <R> <C> Val Unseen <C> Data Aug. <C> 6.8 <C> 5.6 <C> 04.4 <C> 6.5 <C> 1.3 <C> 1.1 <R> <C> Val Unseen <C> [BOLD] RMM [ITALIC] N=1 <C> 6.1 <C> [BOLD] 6.1 <C> 05.1 <C> 6.0 <C> 2.6 <C> 2.8 <R> <C> Val Unseen <C> [BOLD] RMM [ITALIC] N=3 <C> [BOLD] 7.3 <C> 5.5 <C> 0  [BOLD] 5.6 <C> [BOLD] 8.9 <C> [BOLD] 2.9 <C>  [BOLD] 2.9 <R> <C> Val Unseen <C> Shortest Path <C> ———– 29.3 ———— <C> ———– 29.3 ———— <C> ———– 29.3 ———— <C> ———– 29.3 ———— <C> [EMPTY] <C> [EMPTY] <CAP> Table 1: Gameplay results on CVDN evaluated when agent voluntarily stops or at 80 steps. Full evaluations are highlighted in gray with the best results in blue, remaining white columns are ablation results.
<R> <C> [BOLD] # conv. layer <C> [ITALIC] win <C> [ITALIC] kW1 <C> [ITALIC] kW2 <C> [ITALIC] kW3 <C> [ITALIC] kW4 <C> [ITALIC] kW5 <C> [ITALIC] dn <C> [ITALIC] kWmp <C> [BOLD] # output <R> <C> 1 <C> 310 <C> 3 <C> na <C> na <C> na <C> na <C> 39 <C> 50 <C> 351 <R> <C> 2 <C> 310 <C> 3 <C> 7 <C> na <C> na <C> na <C> 39 <C> 7 <C> 351 <R> <C> 3 <C> 430 <C> 3 <C> 5 <C> 5 <C> na <C> na <C> 39 <C> 4 <C> 351 <R> <C> 4 <C> 510 <C> 3 <C> 5 <C> 3 <C> 3 <C> na <C> 39 <C> 3 <C> 351 <R> <C> 5 <C> 310 <C> 3 <C> 5 <C> 7 <C> 7 <C> 7 <C> 39 <C> 2 <C> 351 <CAP> Table 2: Network hyper-parameters for a fixed output size
<R> <C> [BOLD] Models <C> [BOLD] DDIC <C> [BOLD] DDI <R> <C> FBK-IrstChowdhury and Lavelli ( 2013 ) <C> 0.398 <C> 0.530 <R> <C> SCAI Bobic et al.  <C> 0.420 <C> 0.47 <R> <C> WBIThomas et al. ( 2013 ) <C> 0.365 <C> 0.503 <R> <C> UTurkuBjörne et al. ( 2013 ) <C> 0.286 <C> 0.479 <R> <C> UMADRastegar-Mojarad et al. ( 2013 ) <C> 0.312 <C> 0.479 <R> <C> BLSTM-RE <C> 0.390 <C> 0.488 <R> <C> [ITALIC] T-BLSTM-Multi [ITALIC] EAE=>⨁ <C> [BOLD] 0.469 <C> [BOLD] 0.562 <R> <C> [ITALIC] T-BLSTM-Multi [ITALIC] CRE=>⨁ <C> 0.425 <C> 0.561 <R> <C> [ITALIC] T-BLSTM-Multi [ITALIC] ADE=>⨁ <C> 0.422 <C> 0.518 <CAP> Table 5: Performance comparison of existing methods for DDI and DDIC task. Here values indicate F1 Score of both task. ⨁ is MedDDI or MedDDIC
<R> <C> Summarizer <C> 2-R <C> 2-F <C> 3-F <C> SU4-F <R> <C> Abstract <C> 29.52 <C> 29.40 <C> 23.16 <C> 23.34 <R> <C> GCN Hybrid 2 w/ auth <C> [BOLD] 33.88 <C> [BOLD] 31.54 <C> [BOLD] 24.32 <C> [BOLD] 24.36 <R> <C> GCN Hybrid 2 <C> 32.44 <C> 30.08 <C> 23.43 <C> 23.77 <R> <C> GCN Hybrid 1 w/ auth <C> 29.65 <C> 28.05 <C> 21.83 <C> 20.22 <R> <C> GCN Hybrid 1 <C> 29.64 <C> 27.96 <C> 21.81 <C> 19.41 <R> <C> GCN Cited text spans w/ auth <C> 26.30 <C> 24.39 <C> 18.85 <C> 17.31 <R> <C> GCN Cited text spans <C> 25.16 <C> 24.26 <C> 18.79 <C> 17.67 <R> <C> w/ auth: using authority feature. <C> w/ auth: using authority feature. <C> w/ auth: using authority feature. <C> w/ auth: using authority feature. <C> [EMPTY] <CAP> Table 2: Results of Exp 2, showing ROUGE evaluations on the CL-SciSumm Test benchmark. All models are trained on our corpus. The hybrid models outperform abstracts and pure citation summaries.
<R> <C> Model <C> Size <C> Depth <C> Valid <C> Test <R> <C> VD LSTM, Merity et al. ( 2016 ) <C> 20M <C> 2 <C> 101.7 <C> 96.3 <R> <C> VD+Zoneout LSTM, Merity et al. ( 2016 ) <C> 20M <C> 2 <C> 108.7 <C> 100.9 <R> <C> VD LSTM, Inan et al. ( 2016 ) <C> 22M <C> 2 <C> 91.5 <C> 87.7 <R> <C> AWD-LSTM, Merity et al. ( 2017 ) † <C> 33M <C> 3 <C> 68.6 <C> 65.8 <R> <C> LSTM (tuned for PTB) <C> 10M <C> 1 <C> 88.4 <C> 83.2 <R> <C> LSTM <C> 10M <C> 1 <C> 72.7 <C> 69.1 <R> <C> LSTM <C> 10M <C> 2 <C> 73.8 <C> 70.7 <R> <C> LSTM <C> 10M <C> 4 <C> 78.3 <C> 74.3 <R> <C> RHN <C> 10M <C> 5 <C> 83.5 <C> 79.5 <R> <C> NAS <C> [EMPTY] <C> 1 <C> 79.6 <C> 75.9 <R> <C> LSTM (tuned for PTB) <C> 24M <C> 1 <C> 79.8 <C> 76.3 <R> <C> LSTM <C> 24M <C> 1 <C> 69.3 <C> 65.9 <R> <C> LSTM <C> 24M <C> 2 <C> 69.1 <C> 65.9 <R> <C> LSTM <C> 24M <C> 4 <C> 70.5 <C> 67.6 <R> <C> RHN <C> 24M <C> 5 <C> 78.1 <C> 75.6 <R> <C> NAS <C> [EMPTY] <C> 1 <C> 73.0 <C> 69.8 <CAP> Table 2: Validation and test set perplexities on Wikitext-2. All results are with shared input and output embeddings. †: parallel work.
<R> <C> [BOLD] Task <C> [BOLD] Emo_disc acc <C> [BOLD] Emo_disc F1 <C> [BOLD] Info_disc acc <C> [BOLD] Info_disc F1 <C> [BOLD] Support acc <C> [BOLD] Support F1 <C> [BOLD] Gen_supp acc <C> [BOLD] Gen_supp F1 <C> [BOLD] Info_supp acc <C> [BOLD] Info_supp F1 <C> [BOLD] Emo_supp acc <C> [BOLD] Emo_supp F1 <R> <C> BERTBASE <C> 71.3 <C> 65.7 <C> 71.1 <C> 68.7 <C> 81.9 <C> 75.6 <C> 90.6 <C> 63.9 <C> 88.9 <C> 69.8 <C> 92.9 <C> 73.8 <R> <C> XLNetBASE <C> 72.4 <C> 67.9 <C> 72.2 <C> 69.3 <C> 83.4 <C> 77.3 <C> [BOLD] 92.7 <C> [BOLD] 65.0 <C> 87.9 <C> 70.3 <C> 93.4 <C> 73.8 <R> <C> SMDA <C> [BOLD] 75.2 <C> [BOLD] 68.5 <C> [BOLD] 74.3 <C> [BOLD] 71.0 <C> [BOLD] 83.5 <C> [BOLD] 77.7 <C> 91.7 <C> 63.7 <C> [BOLD] 89.9 <C> [BOLD] 70.5 <C> [BOLD] 93.6 <C> [BOLD] 76.2 <CAP> Table 3: Results on test set. Our baseline is our implementation of XLNet-cased-base.
<R> <C> [BOLD] Test Domain <C> [BOLD] Training <C> [BOLD] Top-1 Avg.  [BOLD] LGNB <C> [BOLD] Top-1 Avg.  [BOLD] NETL <C> [BOLD] nDCG-1  [BOLD] LGNB <C> [BOLD] nDCG-1  [BOLD] NETL <C> [BOLD] nDCG-3  [BOLD] LGNB <C> [BOLD] nDCG-3  [BOLD] NETL <C> [BOLD] nDCG-5  [BOLD] LGNB <C> [BOLD] nDCG-5  [BOLD] NETL <R> <C> [BOLD] blogs <C> Baseline <C> 1.84 <C> [BOLD] 1.91 <C> 0.75 <C> [BOLD] 0.77 <C> 0.77 <C> [BOLD] 0.82 <C> 0.79 <C> [BOLD] 0.83 <R> <C> 2-10 <C> In-Domain <C> 1.98 <C> [BOLD] 2.00 <C> 0.81 <C> 0.81 <C> 0.82 <C> [BOLD] 0.85 <C> 0.83 <C> 0.84 <R> <C> [EMPTY] <C> Cross-domain: books <C> 1.88 <C> [BOLD] 1.91 <C> 0.77 <C> 0.78 <C> 0.81 <C> [BOLD] 0.83 <C> 0.83 <C> 0.83 <R> <C> [EMPTY] <C> Cross-domain: news <C> [BOLD] 1.97 <C> 1.92 <C> [BOLD] 0.80 <C> 0.78 <C> 0.83 <C> 0.84 <C> 0.83 <C> 0.84 <R> <C> [EMPTY] <C> Cross-domain: pubmed <C> [BOLD] 1.95 <C> 1.90 <C> [BOLD] 0.80 <C> 0.77 <C> 0.82 <C> 0.83 <C> 0.83 <C> 0.83 <R> <C> [EMPTY] <C> Cross-domain: All 3 <C> — <C> 1.92 <C> — <C> 0.78 <C> — <C> 0.84 <C> — <C> 0.84 <R> <C> 2-10 <C> Upper Bound <C> 2.45 <C> [BOLD] 2.48 <C> 1.00 <C> 1.00 <C> 1.00 <C> 1.00 <C> 1.00 <C> 1.00 <R> <C> [BOLD] books <C> Baseline <C> 1.75 <C> [BOLD] 1.97 <C> 0.77 <C> 0.78 <C> 0.77 <C> [BOLD] 0.82 <C> 0.79 <C> [BOLD] 0.83 <R> <C> 2-10 <C> In-Domain <C> 1.91 <C> [BOLD] 1.99 <C> [BOLD] 0.84 <C> 0.82 <C> 0.81 <C> 0.82 <C> 0.83 <C> 0.84 <R> <C> [EMPTY] <C> Cross-domain: blogs <C> 1.82 <C> [BOLD] 2.02 <C> 0.79 <C> [BOLD] 0.83 <C> 0.81 <C> 0.82 <C> 0.82 <C> [BOLD] 0.84 <R> <C> [EMPTY] <C> Cross-domain: news <C> 1.82 <C> [BOLD] 1.99 <C> 0.79 <C> [BOLD] 0.81 <C> 0.81 <C> 0.82 <C> 0.83 <C> 0.84 <R> <C> [EMPTY] <C> Cross-domain: pubmed <C> 1.87 <C> [BOLD] 1.97 <C> 0.81 <C> 0.80 <C> 0.82 <C> 0.82 <C> 0.83 <C> 0.84 <R> <C> [EMPTY] <C> Cross-domain: All 3 <C> — <C> 2.03 <C> — <C> 0.83 <C> — <C> 0.83 <C> — <C> 0.84 <R> <C> 2-10 <C> Upper Bound <C> 2.29 <C> [BOLD] 2.49 <C> 1.00 <C> 1.00 <C> 1.00 <C> 1.00 <C> 1.00 <C> 1.00 <R> <C> [BOLD] news <C> Baseline <C> 1.96 <C> [BOLD] 2.04 <C> 0.80 <C> [BOLD] 0.82 <C> 0.79 <C> [BOLD] 0.84 <C> 0.78 <C> [BOLD] 0.85 <R> <C> 2-10 <C> In-Domain <C> 2.02 <C> 2.02 <C> [BOLD] 0.82 <C> 0.80 <C> 0.82 <C> [BOLD] 0.84 <C> 0.84 <C> 0.85 <R> <C> [EMPTY] <C> Cross-domain: blogs <C> 2.03 <C> 2.03 <C> [BOLD] 0.83 <C> 0.81 <C> 0.82 <C> [BOLD] 0.84 <C> 0.84 <C> 0.85 <R> <C> [EMPTY] <C> Cross-domain: books <C> [BOLD] 2.01 <C> 1.98 <C> [BOLD] 0.82 <C> 0.79 <C> 0.82 <C> 0.83 <C> 0.83 <C> 0.84 <R> <C> [EMPTY] <C> Cross-domain: pubmed <C> 2.01 <C> 2.00 <C> [BOLD] 0.82 <C> 0.79 <C> 0.82 <C> 0.83 <C> 0.83 <C> 0.84 <R> <C> [EMPTY] <C> Cross-domain: All 3 <C> — <C> 1.99 <C> — <C> 0.79 <C> — <C> 0.84 <C> — <C> 0.84 <R> <C> 2-10 <C> Upper Bound <C> 2.45 <C> [BOLD] 2.56 <C> 1.00 <C> 1.00 <C> 1.00 <C> 1.00 <C> 1.00 <C> 1.00 <R> <C> [BOLD] pubmed <C> Baseline <C> 1.73 <C> [BOLD] 1.94 <C> 0.75 <C> [BOLD] 0.79 <C> 0.77 <C> [BOLD] 0.80 <C> 0.79 <C> [BOLD] 0.82 <R> <C> 2-10 <C> In-Domain <C> 1.79 <C> [BOLD] 1.99 <C> 0.77 <C> [BOLD] 0.81 <C> 0.82 <C> 0.81 <C> [BOLD] 0.84 <C> 0.82 <R> <C> [EMPTY] <C> Cross-domain: blogs <C> 1.80 <C> [BOLD] 1.98 <C> 0.78 <C> [BOLD] 0.80 <C> 0.82 <C> 0.81 <C> [BOLD] 0.84 <C> 0.82 <R> <C> [EMPTY] <C> Cross-domain: books <C> 1.77 <C> [BOLD] 1.98 <C> 0.77 <C> [BOLD] 0.80 <C> 0.82 <C> 0.81 <C> 0.83 <C> 0.82 <R> <C> [EMPTY] <C> Cross-domain: news <C> 1.79 <C> [BOLD] 1.98 <C> 0.77 <C> [BOLD] 0.80 <C> 0.82 <C> 0.81 <C> [BOLD] 0.84 <C> 0.82 <R> <C> [EMPTY] <C> Cross-domain: All 3 <C> — <C> 2.01 <C> — <C> 0.81 <C> — <C> 0.81 <C> — <C> 0.82 <R> <C> 2-10 <C> Upper Bound <C> 2.31 <C> [BOLD] 2.51 <C> 1.00 <C> 1.00 <C> 1.00 <C> 1.00 <C> 1.00 <C> 1.00 <CAP> Table 2: Results across the four domains. Boldface indicates the better system between NETL and LGNB (with an absolute difference >0.01).
<R> <C> TestSet Type <C> Ppl without Sentiment <C> Ppl with Sentiment <R> <C> MR1 <C> 423.89 <C> [BOLD] 406.74 <R> <C> MR2 <C> 2028.69 <C> [BOLD] 1871.57 <R> <C> MR3 <C> 5842.39 <C> [BOLD] 5824.97 <CAP> Table 2: Perplexity Estimation on Movie Review Dataset Using Proposed Model
<R> <C> [EMPTY] <C> Total Number <C> Numb of Positive Words <C> Num of Negative Words <R> <C> NG(2000) <C> 155 <C> 100 <C> 55 <R> <C> RCV(10000) <C> 950 <C> 447 <C> 503 <R> <C> MR(24916) <C> 3114 <C> 1242 <C> 1872 <CAP> Table 3: Number of words shared between each of the three dictionaries and the MPQA sentiment lexicon
<R> <C> Model <C> R-1 <C> R-2 <C> R-4 <R> <C> Centroid <C> 36.03 <C> 7.89 <C> 1.20 <R> <C> LexRank <C> 35.49 <C> 7.42 <C> 0.81 <R> <C> KLSum <C> 37.63 <C> 8.50 <C> 1.26 <R> <C> CLASSY04 <C> 37.23 <C> 8.89 <C> 1.46 <R> <C> ICSI <C> 38.02 <C> [BOLD] 9.72 <C> [BOLD] 1.72 <R> <C> Submodular <C> 38.62 <C> 9.19 <C> 1.34 <R> <C> DPP <C> [BOLD] 39.41 <C> 9.57 <C> 1.56 <R> <C> RegSum <C> 38.23 <C> 9.71 <C> 1.59 <R> <C> Centroid <C> 37.91 <C> 9.53 <C> 1.56 <R> <C> Centroid + N-first <C> 38.04 <C> 9.56 <C> 1.56 <R> <C> Centroid + N-best <C> 37.86 <C> 9.67 <C> [BOLD] 1.67 <R> <C> Centroid + new-tf-idf <C> 38.27 <C> 9.64 <C> 1.54 <R> <C> Centroid + G <C> 38.55 <C> 9.73 <C> 1.53 <R> <C> Centroid + G + N-first <C> 38.85 <C> [BOLD] 9.86 <C> 1.62 <R> <C> Centroid + G + N-best <C> 38.86 <C> 9.77 <C> 1.53 <R> <C> Centroid + G + new-tf-idf <C> [BOLD] 39.11 <C> 9.81 <C> 1.58 <R> <C> Centroid - R <C> 35.54 <C> 8.73 <C> 1.42 <R> <C> Centroid + G - R <C> 38.58 <C> 9.73 <C> 1.53 <CAP> Table 1: Rouge scores on DUC2004.
<R> <C> Class <C> train <C> cv <C> eval <R> <C> True Triggers <C> 42,675 <C> 4,742 <C> 11,646 <R> <C> False Triggers <C> 18,669 <C> 2,074 <C> 11,316 <CAP> Table 1: Dataset for false trigger mitigation task.
<R> <C> System <C> # of params <C> AUC <C> FAR <R> <C> ASR-Output <C> - <C> - <C> 0.868 <R> <C> BiLRNN <C> 15,041 <C> 0.9906 <C> 0.134 <R> <C> GCN 6 layers <C> 26,369 <C> 0.9888 <C> 0.185 <R> <C> ResGCN 8 layers <C> 70,209 <C> 0.9905 <C> 0.155 <R> <C> SAGNN 2 layers/4 heads <C> 39,105 <C> 0.9906 <C> 0.150 <R> <C> MaskedSAGNN 2 layers/4 heads <C> 39,105 <C> 0.9914 <C> 0.134 <CAP> Table 2: FTM results using various systems. FAR shown at TPR=0.99 in all systems (except ASR-Output where TPR=0.9994).
<R> <C> 2*Approaches <C> [BOLD] 2-way  [BOLD] Acc <C> [BOLD] 2-way  [BOLD] M-F1 <C> [BOLD] 2-way  [BOLD] W-F1 <C> [BOLD] 3-way  [BOLD] Acc <C> [BOLD] 3-way  [BOLD] M-F1 <C> [BOLD] 3-way  [BOLD] W-F1 <C> [BOLD] 5-way  [BOLD] Acc <C> [BOLD] 5-way  [BOLD] M-F1 <C> [BOLD] 5-way  [BOLD] W-F1 <R> <C> [BOLD] Non-Neural Approaches <C> [BOLD] Non-Neural Approaches <C> [BOLD] Non-Neural Approaches <C> [BOLD] Non-Neural Approaches <C> [BOLD] Non-Neural Approaches <C> [BOLD] Non-Neural Approaches <C> [BOLD] Non-Neural Approaches <C> [BOLD] Non-Neural Approaches <C> [BOLD] Non-Neural Approaches <C> [BOLD] Non-Neural Approaches <R> <C> CoMeT (ott2013comet) <C> 0.7740 <C> 0.7680 <C> 0.7730 <C> 0.7130 <C> 0.6400 <C> 0.7070 <C> 0.6000 <C> 0.5510 <C> 0.5980 <R> <C> ETS (heilman2013ets) <C> 0.7760 <C> 0.7620 <C> 0.7700 <C> 0.7200 <C> 0.6470 <C> 0.7080 <C> 0.6430 <C> 0.5980 <C> 0.6400 <R> <C> SOFTCAR (jimenez2013softcardinality) <C> 0.7240 <C> 0.7150 <C> 0.7220 <C> 0.6590 <C> 0.5550 <C> 0.6470 <C> 0.5440 <C> 0.4740 <C> 0.5370 <R> <C> SultanSS16 <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> 0.5820 <R> <C> [BOLD] Neural Approaches <C> [BOLD] Neural Approaches <C> [BOLD] Neural Approaches <C> [BOLD] Neural Approaches <C> [BOLD] Neural Approaches <C> [BOLD] Neural Approaches <C> [BOLD] Neural Approaches <C> [BOLD] Neural Approaches <C> [BOLD] Neural Approaches <C> [BOLD] Neural Approaches <R> <C> taghipour2016neural–Best† <C> - <C> - <C> 0.6700 <C> - <C> - <C> - <C> - <C> - <C> 0.5210 <R> <C> taghipour2016neural–Tuned† <C> - <C> - <C> 0.7120 <C> - <C> - <C> - <C> - <C> - <C> 0.5330 <R> <C> InferSent (conneau2017supervised) <C> 0.7463 <C> 0.7410 <C> 0.7461 <C> 0.6963 <C> 0.6428 <C> 0.6916 <C> 0.6018 <C> 0.5616 <C> 0.5996 <R> <C> Saha2018 <C> 0.7926 <C> 0.7858 <C> 0.7910 <C> 0.7185 <C> 0.6662 <C> 0.7143 <C> 0.6444 <C> 0.6010 <C> 0.6420 <R> <C> [BOLD] Joint Multi-Domain - ASAG <C> [BOLD] 0.8037 <C> [BOLD] 0.7986 <C> [BOLD] 0.8030 <C> [BOLD] 0.7462 <C> [BOLD] 0.7111 <C> [BOLD] 0.7442 <C> [BOLD] 0.6518 <C> [BOLD] 0.6252 <C> [BOLD] 0.6565 <CAP> Table 6. Comparison of JMD-ASAG with state-of-the-art non-neural and neural models on SemEval-2013 SciEntsBank dataset. JMD-ASAG outperforms all existing models on this dataset. †Results as reported by riordan2017investigating.
<R> <C> Models <C> Validation  [ITALIC] F1 [ITALIC] MS <C> Validation Prec. <C> Validation Rec. <C> Test  [ITALIC] F1 [ITALIC] MS <C> Test Prec. <C> Test Rec. <R> <C> [BOLD] SQuAD <C> [BOLD] SQuAD <C> [BOLD] SQuAD <C> [BOLD] SQuAD <C> [BOLD] SQuAD <C> [BOLD] SQuAD <C> [BOLD] SQuAD <R> <C> H&S <C> - <C> - <C> - <C> 0.292 <C> 0.252 <C> 0.403 <R> <C> ENT <C> 0.308 <C> 0.249 <C> [BOLD] 0.523 <C> 0.347 <C> 0.295 <C> [BOLD] 0.547 <R> <C> NES <C> 0.334 <C> 0.335 <C> 0.354 <C> 0.362 <C> 0.375 <C> 0.380 <R> <C> PtrNet <C> [BOLD] 0.352 <C> [BOLD] 0.387 <C> 0.337 <C> [BOLD] 0.404 <C> [BOLD] 0.448 <C> 0.387 <R> <C> [BOLD] NewsQA <C> [BOLD] NewsQA <C> [BOLD] NewsQA <C> [BOLD] NewsQA <C> [BOLD] NewsQA <C> [BOLD] NewsQA <C> [BOLD] NewsQA <R> <C> ENT <C> 0.187 <C> 0.127 <C> [BOLD] 0.491 <C> 0.183 <C> 0.125 <C> [BOLD] 0.479 <R> <C> PtrNet <C> [BOLD] 0.452 <C> [BOLD] 0.480 <C> 0.444 <C> [BOLD] 0.435 <C> [BOLD] 0.467 <C> 0.427 <CAP> Table 1: Model evaluation on key phrase extraction
<R> <C> [BOLD] Learning Scheme <C> [BOLD] Learning Scheme <C> [BOLD] NLU  [BOLD] Micro-F1 <C> [BOLD] NLG  [BOLD] BLEU <C> [BOLD] NLG  [BOLD] ROUGE-1 <C> [BOLD] NLG  [BOLD] ROUGE-2 <C> [BOLD] NLG  [BOLD] ROUGE-L <R> <C> (a) <C> Iterative training ( [BOLD] supervised) <C> 71.14 <C> 55.05 <C> 55.37 <C> 27.95 <C> 39.90 <R> <C> (b) <C> Dual supervised learning Su et al. ( 2019 ) <C> [BOLD] 72.32 <C> [BOLD] 57.16 <C> [BOLD] 56.37 <C> [BOLD] 29.19 <C> [BOLD] 40.44 <R> <C> (c) <C> Joint training (Straight-Through) <C> 71.73 <C> 55.19 <C> 55.16 <C> 27.45 <C> 39.33 <R> <C> (d) <C> (c) + (NLG w/ distribution) <C> 73.22 <C> 55.18 <C> 55.35 <C> 27.81 <C> [BOLD] 39.36 <R> <C> (e) <C> (c) + (NLU w/ distribution) <C> 79.19 <C> 51.47 <C> 53.62 <C> 26.17 <C> 37.90 <R> <C> (f) <C> (c) + (NLU and NLG w/ distribution) <C> [BOLD] 80.03 <C> [BOLD] 55.34 <C> [BOLD] 56.17 <C> [BOLD] 28.48 <C> 39.24 <R> <C> (g) <C> (f) +  [BOLD] RL [ITALIC] mid(reconstruction likelihood) <C> 80.07 <C> 55.32 <C> 56.12 <C> 28.07 <C> 39.59 <R> <C> (h) <C> (f) +  [BOLD] RL [ITALIC] end(reconstruction likelihood) <C> 79.97 <C> 55.21 <C> 56.15 <C> 28.50 <C> 39.42 <R> <C> (i) <C> (f) +  [BOLD] RL [ITALIC] mid(BLEU+ROUGE, F1) <C> 79.49 <C> 56.04 <C> 56.61 <C> 28.78 <C> 39.93 <R> <C> (j) <C> (f) +  [BOLD] RL [ITALIC] end(BLEU+ROUGE, F1) <C> 80.35 <C> [BOLD] 57.59 <C> [BOLD] 56.71 <C> [BOLD] 29.06 <C> [BOLD] 40.28 <R> <C> (k) <C> (f) +  [BOLD] RL [ITALIC] mid(LM, MADE) <C> [BOLD] 81.52 <C> 54.13 <C> 54.60 <C> 26.85 <C> 38.90 <R> <C> (l) <C> (f) +  [BOLD] RL [ITALIC] end(LM, MADE) <C> 79.52 <C> 55.61 <C> 55.97 <C> 28.57 <C> 39.97 <CAP> Table 1: The NLU performance reported on micro-F1 and the NLG performance reported on BLEU, ROUGE-1, ROUGE-2, and ROUGE-L of models (%).
<R> <C> [BOLD] Model <C> Google N-grams  [BOLD] Relevance <C> Google N-grams  [BOLD] Polarity <C> Google N-grams  [BOLD] Category <C> COHA  [BOLD] Relevance <C> COHA  [BOLD] Polarity <C> COHA  [BOLD] Category <R> <C> Random <C> 0.50 <C> 0.50 <C> 0.10 <C> 0.50 <C> 0.50 <C> 0.10 <R> <C> Centroid <C> [BOLD] 0.84 <C> 0.90 <C> [BOLD] 0.59 <C> 0.78 <C> [BOLD] 0.80 <C> [BOLD] 0.40 <R> <C> Naïve Bayes <C> [BOLD] 0.84 <C> 0.89 <C> 0.53 <C> 0.76 <C> 0.78 <C> 0.39 <R> <C> 1-NN <C> 0.80 <C> 0.88 <C> 0.53 <C> 0.74 <C> 0.76 <C> 0.32 <R> <C> 5-NN <C> 0.83 <C> [BOLD] 0.93 <C> 0.57 <C> 0.74 <C> 0.75 <C> 0.33 <R> <C> KDE <C> 0.82 <C> 0.90 <C> 0.57 <C> [BOLD] 0.80 <C> 0.76 <C> 0.33 <CAP> Table 2: Classification accuracy of moral seed words for moral relevance, moral polarity, and fine-grained moral categories based on 1990–1999 word embeddings for two independent corpora, Google N-grams and COHA.
<R> <C> [EMPTY] <C> [BOLD] metric <C> [BOLD] refs <C> [BOLD] train? <C> [BOLD] Pearson <C> [BOLD] MSE <R> <C> 1 <C> [BOLD] SVR: <C> 3−5 <C> yes <C> [BOLD] 0.594 <C> [BOLD] 0.217 <R> <C> [EMPTY] <C> ROUGE+WPSLOR <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> 2 <C> [BOLD] ROUGE-LM <C> 3−5 <C> no <C> 0.496 <C> 0.252 <R> <C> 3 <C> ROUGE-L-mult <C> 3−5 <C> no <C> 0.430 <C> 0.273 <R> <C> 4 <C> WPSLOR <C> 0 <C> no <C> 0.439 <C> 0.270 <CAP> Table 7: Combinations; all differences except for 3 and 4 are statistically significant; refs=number of references used to compute the metric; ROUGE=ROUGE-L-mult; best results in bold.
<R> <C> [EMPTY] <C> Model <C> MAP <C> MRR <C> P@5 <C> SR@1 <C> SR@5 <C> NDCG <R> <C> q-Q <C> TSUBAKI <C> 0.558 <C> 0.598 <C> 0.297 <C> 0.504 <C> 0.734 <C> 0.501 <R> <C> q-A <C> Bi-LSTM <C> 0.451 <C> 0.498 <C> 0.248 <C> 0.379 <C> 0.601 <C> 0.496 <R> <C> [EMPTY] <C> BERT [ITALIC] targetOnly <C> 0.559 <C> 0.610 <C> 0.285 <C> 0.504 <C> 0.751 <C> 0.526 <R> <C> [EMPTY] <C> BERT <C> 0.576 <C> 0.631 <C> 0.333 <C> 0.509 <C> 0.810 <C> 0.560 <R> <C> q-QA <C> Proposed <C> [BOLD] 0.647 <C> [BOLD] 0.705 <C> [BOLD] 0.357 <C> [BOLD] 0.612 <C> [BOLD] 0.841 <C> [BOLD] 0.621 <CAP> Table 1. Evaluation result on the localgovFAQ dataset.
<R> <C> [EMPTY] <C> Model <C> MAP <C> MRR <C> P@5 <R> <C> q-Q <C> CNN-rank <C> 0.79 <C> 0.77 <C> 0.63 <R> <C> [EMPTY] <C> TSUBAKI <C> 0.698 <C> 0.669 <C> 0.638 <R> <C> q-A <C> BERT (w/o query paraphrases) <C> 0.631 <C> 0.805 <C> 0.546 <R> <C> [EMPTY] <C> BERT <C> 0.887 <C> 0.936 <C> 0.770 <R> <C> q-QA <C> CNN-rank <C> 0.74 <C> 0.84 <C> 0.62 <R> <C> [EMPTY] <C> Proposed <C> [BOLD] 0.897 <C> [BOLD] 0.942 <C> [BOLD] 0.776 <CAP> Table 2. Evaluation result on the StackExchange dataset.
<R> <C> [EMPTY] <C> dev <C> test <R> <C> [EMPTY] <C> PER (%) <C> PER (%) <R> <C> HMM-DNN <C> [EMPTY] <C> 21.4 <R> <C> first first-pass SCRF  <C> [EMPTY] <C> 33.1 <R> <C> Boundary-factored SCRF  <C> [EMPTY] <C> 26.5 <R> <C> Deep segmental NN  <C> [EMPTY] <C> 21.87 <R> <C> our first-pass model ( [ITALIC] H1) <C> 22.15 <C> 21.73 <R> <C> DSC 2 [ITALIC] nd level with bigram LM <C> 19.80 <C> [EMPTY] <R> <C> + 2nd-order boundary features <C> 19.22 <C> [EMPTY] <R> <C> + 1st-order segment NN <C> 18.86 <C> [EMPTY] <R> <C> + 1st-order bi-phone NN bottleneck <C> 18.77 <C> 19.93 <CAP> Table 1: A summary of results in terms of phonetic error rate (%) on the TIMIT test set, for prior first-pass segmental models, a speaker-independent HMM-DNN system given by a standard Kaldi recipe [18], and our models.
<R> <C> label <C> precision <C> recall <C> f1-score <C> support <R> <C> B-person <C> 0.767 <C> 0.618 <C> 0.684 <C> 842 <R> <C> I-person <C> 0.795 <C> 0.833 <C> 0.814 <C> 294 <R> <C> B-geoloc <C> 0.757 <C> 0.697 <C> 0.726 <C> 699 <R> <C> B-transportLine <C> 0.978 <C> 0.926 <C> 0.951 <C> 517 <R> <C> B-musicartist <C> 0.667 <C> 0.178 <C> 0.281 <C> 90 <R> <C> B-other <C> 0.286 <C> 0.134 <C> 0.183 <C> 149 <R> <C> B-org <C> 0.712 <C> 0.277 <C> 0.399 <C> 545 <R> <C> B-product <C> 0.519 <C> 0.135 <C> 0.214 <C> 312 <R> <C> I-product <C> 0.320 <C> 0.113 <C> 0.167 <C> 364 <R> <C> B-media <C> 0.724 <C> 0.462 <C> 0.564 <C> 210 <R> <C> B-facility <C> 0.639 <C> 0.363 <C> 0.463 <C> 146 <R> <C> I-facility <C> 0.620 <C> 0.486 <C> 0.545 <C> 175 <R> <C> B-sportsteam <C> 0.514 <C> 0.277 <C> 0.360 <C> 65 <R> <C> I-sportsteam <C> 1.000 <C> 0.200 <C> 0.333 <C> 10 <R> <C> B-event <C> 0.436 <C> 0.185 <C> 0.260 <C> 92 <R> <C> I-event <C> 0.356 <C> 0.292 <C> 0.321 <C> 89 <R> <C> B-tvshow <C> 0.429 <C> 0.058 <C> 0.102 <C> 52 <R> <C> I-tvshow <C> 0.286 <C> 0.065 <C> 0.105 <C> 31 <R> <C> I-media <C> 0.200 <C> 0.019 <C> 0.035 <C> 52 <R> <C> B-movie <C> 0.333 <C> 0.045 <C> 0.080 <C> 44 <R> <C> I-other <C> 0.000 <C> 0.000 <C> 0.000 <C> 73 <R> <C> I-transportLine <C> 0.873 <C> 0.729 <C> 0.795 <C> 85 <R> <C> I-geoloc <C> 0.650 <C> 0.409 <C> 0.502 <C> 159 <R> <C> I-musicartist <C> 0.636 <C> 0.163 <C> 0.259 <C> 43 <R> <C> I-movie <C> 0.250 <C> 0.049 <C> 0.082 <C> 41 <CAP> Table 4: Fine grained score analysis
<R> <C> Model <C> annealing <C> z-dim <C> Rec. <C> KLD <C> Total <R> <C> [EMPTY] <C> epochs <C> [EMPTY] <C> error <C> term <C> [EMPTY] <R> <C> VoiceLoop (w/o) <C> [EMPTY] <C> [EMPTY] <C> 15.946 <C> [EMPTY] <C> [BOLD] 15.946 <R> <C> VoiceLoop (w/) <C> [EMPTY] <C> [EMPTY] <C> 15.759 <C> [EMPTY] <C> 15.759 <R> <C> VAE-Loop <C> 0 <C> 64 <C> 15.832 <C> 0.073 <C> 15.905 <R> <C> VAE-Loop <C> 15(10%) <C> 64 <C> [BOLD] 15.684 <C> [BOLD] 0.090 <C> [BOLD] 15.774 <R> <C> VAE-Loop <C> 30(20%) <C> 64 <C> 15.749 <C> 0.086 <C> 15.835 <R> <C> VAE-Loop <C> 15 <C> 32 <C> 15.839 <C> 0.082 <C> 15.921 <R> <C> VAE-Loop <C> 15 <C> 128 <C> 15.724 <C> 0.084 <C> 15.808 <CAP> Table 1: Test errors for different numbers of annealing epochs and z dimensions, on the VCTK dataset.
<R> <C> Model <C> Cellphone T <C> Cellphone G <C> Household Electrics T <C> Household Electrics G <R> <C> S2SA <C> 2,940 <C> 18 <C> 5,163 <C> 18 <R> <C> TA-S2S <C> 3,862 <C> 19 <C> 6,936 <C> 20 <R> <C> ConvS2S <C> 791 <C> 7 <C> 1,923 <C> 8 <R> <C> ConvS2S-RV <C> 791 <C> 13 <C> 1,923 <C> 11 <R> <C> RAGE/POS <C> 908 <C> 10 <C> 2,760 <C> 11 <R> <C> RAGE <C> 989 <C> 11 <C> 4,146 <C> 13 <CAP> Table 5. The time cost in seconds for one epoch model training (T) and answer generation (G) by different models.
<R> <C> MTL Weight Tuning for Audio and Visual Tasks Pretraining Dataset <C> MTL Weight Tuning for Audio and Visual Tasks Pretraining Dataset <C> MTL Weight Tuning for Audio and Visual Tasks Pretraining Dataset <C> MTL Weight Tuning for Audio and Visual Tasks Pretraining Dataset <C> [BOLD] Emotion Recognition LRW <C> [BOLD] Emotion Recognition LRW <C> [BOLD] Speech Recognition LRW <R> <C> Evaluation Dataset <C> Evaluation Dataset <C> Evaluation Dataset <C> Evaluation Dataset <C> [BOLD] CREMA-D <C> [BOLD] Ravdess <C> [BOLD] SPC <R> <C> Classifier for  [ITALIC] (t, dim) features <C> Classifier for  [ITALIC] (t, dim) features <C> Classifier for  [ITALIC] (t, dim) features <C> Classifier for  [ITALIC] (t, dim) features <C> LSTM <C> LSTM <C> LSTM <R> <C> Labels <C> Labels <C> Labels <C> Labels <C> 6 emotions <C> 8 emotions <C> 30 words <R> <C> Method <C> Video weight ( [ITALIC] α) <C> Audio weight (1− [ITALIC] α) <C> Dim. <C> Accuracy (↑) <C> Accuracy (↑) <C> Accuracy (↑) <R> <C> L1 + AoT <C> 0.17 <C> 0.83 <C> 512 <C> 46.22 <C> 38.61 <C> 88.74 <R> <C> L1 + AoT <C> 0.33 <C> 0.67 <C> 512 <C> 47.91 <C> 40.18 <C> 89.28 <R> <C> L1 + AoT <C> 0.50 <C> 0.50 <C> 512 <C> 51.50 <C> 43.03 <C> 90.36 <R> <C> L1 + AoT <C> [BOLD] 0.67 <C> [BOLD] 0.33 <C> 512 <C> [BOLD] 51.77 <C> [BOLD] 44.39 <C> [BOLD] 91.94 <R> <C> L1 + AoT <C> 0.83 <C> 0.17 <C> 512 <C> 48.93 <C> 40.40 <C> 90.79 <R> <C> L1 + Odd <C> 0.17 <C> 0.83 <C> 512 <C> 48.91 <C> 42.11 <C> 89.97 <R> <C> L1 + Odd <C> 0.33 <C> 0.67 <C> 512 <C> 47.48 <C> 39.81 <C> 88.39 <R> <C> L1 + Odd <C> 0.50 <C> 0.50 <C> 512 <C> 50.73 <C> 43.26 <C> 90.78 <R> <C> L1 + Odd <C> [BOLD] 0.67 <C> [BOLD] 0.33 <C> 512 <C> [BOLD] 52.81 <C> [BOLD] 44.32 <C> [BOLD] 92.17 <R> <C> L1 + Odd <C> 0.83 <C> 0.17 <C> 512 <C> 51.17 <C> 42.41 <C> 91.31 <CAP> Table III: Comparison of different MTL weights. All results are accuracies on the validation sets of the evaluation datasets.
<R> <C> [BOLD] Model <C> CoNLL 2003 <C> i2b2 2014 <R> <C> Best published <C> 90.9 <C> 97.9 <R> <C> NeuroNER <C> 90.5 <C> 97.7 <CAP> Table 1: F1-scores (%) on the test set comparing NeuroNER with the best published methods in the literature, viz. (Passos et al., 2014) for CoNLL 2003, (Dernoncourt et al., 2016) for i2b2 2014.
<R> <C> [EMPTY] <C> [BOLD] Image Relevance METEOR <C> [BOLD] Image Relevance CIDEr <C> [BOLD] Class Relevance Similarity <C> [BOLD] Class Relevance Rank <C> [BOLD] Best Explanation Bird Expert Rank <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> (1-200) <C> (1-5) <R> <C> Definition <C> 27.9 <C> 43.8 <C> 42.60 <C> 15.82 <C> 2.92 <R> <C> Description <C> 27.7 <C> 42.0 <C> 35.3 <C> 24.43 <C> 3.11 <R> <C> Explanation-Label <C> 28.1 <C> 44.7 <C> 40.86 <C> 17.69 <C> 2.97 <R> <C> Explanation-Dis. <C> 28.8 <C> 51.9 <C> 43.61 <C> 19.80 <C> 3.22 <R> <C> Explanation <C> [BOLD] 29.2 <C> [BOLD] 56.7 <C> [BOLD] 52.25 <C> [BOLD] 13.12 <C> [BOLD] 2.78 <CAP> Table 1: Comparison of our explanation model to our definition and description baseline, as well as the explanation-label and explanation-discriminative (explanation-dis. in the table) ablation models. We demonstrate that our generated explanations are image relevant by computing METEOR and CIDEr scores (higher is better). We demonstrate class relevance using a class similarity metric (higher is better) and class rank metric (lower is better) (see Section 4 for details). Finally, we ask experienced bird watchers to rank our explanations. On all metrics, our explanation model performs best.
<R> <C> [BOLD] Methods <C> [BOLD] Named Entity  [BOLD] Precision <C> [BOLD] Named Entity  [BOLD] Recall <C> [BOLD] Named Entity  [BOLD] F1 <C> [BOLD] Nominal Mention  [BOLD] Precision <C> [BOLD] Nominal Mention  [BOLD] Recall <C> [BOLD] Nominal Mention  [BOLD] F1 <R> <C> Character+Segmentation <C> 48.52 <C> 39.23 <C> 43.39 <C> 58.75 <C> [BOLD] 47.96 <C> 52.91 <R> <C> Character+Position <C> [BOLD] 65.87 <C> [BOLD] 39.71 <C> [BOLD] 49.55 <C> [BOLD] 68.12 <C> [BOLD] 47.96 <C> [BOLD] 56.29 <CAP> Table 2: Two methods to incorporate word segmentation information.
<R> <C> [BOLD] Models <C> [BOLD] Named Entity  [BOLD] Precision <C> [BOLD] Named Entity  [BOLD] Recall <C> [BOLD] Named Entity  [BOLD] F1 <C> [BOLD] Nominal Mention  [BOLD] Precision <C> [BOLD] Nominal Mention  [BOLD] Recall <C> [BOLD] Nominal Mention  [BOLD] F1 <C> [BOLD] Overall <C> [BOLD] OOV <R> <C>  <C> 57.98 <C> 35.57 <C> 44.09 <C> 63.84 <C> 29.45 <C> 40.38 <C> 42.70 <C> - <R> <C>  <C> 63.33 <C> 39.18 <C> 48.41 <C> 58.59 <C> 37.42 <C> 45.67 <C> 47.38 <C> - <R> <C> B-LSTM <C> 65.87 <C> 39.71 <C> 49.55 <C> 68.12 <C> 47.96 <C> 56.29 <C> 52.81 <C> 13.97 <R> <C> B-LSTM + MMNN <C> 65.29 <C> 37.80 <C> 47.88 <C> [BOLD] 73.53 <C> 51.02 <C> [BOLD] 60.24 <C> 53.86 <C> 17.90 <R> <C> F-Score Driven I (proposal) <C> 66.67 <C> 39.23 <C> 49.40 <C> 69.50 <C> 50.00 <C> 58.16 <C> 53.64 <C> 17.03 <R> <C> F-Score Driven II (proposal) <C> [BOLD] 66.93 <C> [BOLD] 40.67 <C> [BOLD] 50.60 <C> 66.46 <C> [BOLD] 53.57 <C> 59.32 <C> [BOLD] 54.82 <C> [BOLD] 20.96 <CAP> Table 3: NER results for named and nominal mentions on test data.
<R> <C> [EMPTY] <C> Dataset <C> no reg <C> lasso <C> ridge <C> elastic <C> OMP <C> Group lasso regularizers LDA <C> Group lasso regularizers LSI <C> Group lasso regularizers sen <C> Group lasso regularizers GoW <C> Group lasso regularizers w2v <C> GOMP <R> <C> 20NG <C> science <C> 0.946 <C> 0.916 <C> 0.954 <C> 0.954 <C> 0.964* <C> [BOLD] 0.968 <C> [BOLD] 0.968* <C> 0.942 <C> 0.967* <C> [BOLD] 0.968* <C> 0.953* <R> <C> 20NG <C> sports <C> 0.908 <C> 0.907 <C> 0.925 <C> 0.920 <C> 0.949* <C> 0.959 <C> 0.964* <C> [BOLD] 0.966 <C> 0.959* <C> 0.946* <C> 0.951* <R> <C> 20NG <C> religion <C> 0.894 <C> 0.876 <C> 0.895 <C> 0.890 <C> 0.902* <C> 0.918 <C> 0.907* <C> [BOLD] 0.934 <C> 0.911* <C> 0.916* <C> 0.902* <R> <C> 20NG <C> computer <C> 0.846 <C> 0.843 <C> 0.869 <C> 0.856 <C> 0.876* <C> 0.891 <C> 0.885* <C> 0.904 <C> 0.885* <C> [BOLD] 0.911* <C> 0.902* <R> <C> Sentiment <C> vote <C> 0.606 <C> 0.643 <C> 0.616 <C> 0.622 <C> 0.684* <C> 0.658 <C> 0.653 <C> 0.656 <C> 0.640 <C> 0.651 <C> [BOLD] 0.687* <R> <C> Sentiment <C> movie <C> 0.865 <C> 0.860 <C> 0.870 <C> 0.875 <C> 0.860* <C> [BOLD] 0.900 <C> 0.895 <C> 0.895 <C> 0.895 <C> 0.890 <C> 0.850 <R> <C> Sentiment <C> books <C> 0.750 <C> 0.770 <C> 0.760 <C> 0.780 <C> 0.800 <C> 0.790 <C> 0.795 <C> 0.785 <C> 0.790 <C> 0.800 <C> [BOLD]  0.805* <R> <C> Sentiment <C> dvd <C> 0.765 <C> 0.735 <C> 0.770 <C> 0.760 <C> 0.785 <C> 0.800 <C> 0.805* <C> 0.785 <C> 0.795* <C> 0.795* <C> [BOLD]  0.820* <R> <C> Sentiment <C> electr. <C> 0.790 <C> 0.800 <C> 0.800 <C> 0.825 <C> [BOLD] 0.830 <C> 0.800 <C> 0.815 <C> 0.805 <C> 0.820 <C> 0.815 <C> 0.800 <R> <C> Sentiment <C> kitch. <C> 0.760 <C> 0.800 <C> 0.775 <C> 0.800 <C> 0.825 <C> 0.845 <C> [BOLD] 0.860* <C> 0.855 <C> 0.840 <C> 0.855* <C> 0.830 <CAP> Table 2: Accuracy on the test sets. Bold font marks the best performance for a dataset, while * indicates statistical significance at p<0.05 using micro sign test against lasso. For GOMP, we use w2v clusters and add all unigram features as individual groups.
<R> <C> [EMPTY] <C> Dataset <C> no reg <C> lasso <C> ridge <C> elastic <C> OMP <C> Group lasso regularizers LDA <C> Group lasso regularizers LSI <C> Group lasso regularizers sen <C> Group lasso regularizers GoW <C> Group lasso regularizers w2v <C> GOMP <R> <C> 20NG <C> science <C> 100 <C> [BOLD] 1 <C> 100 <C> 63 <C> 2.7 <C> 19 <C> 20 <C> 86 <C> 19 <C> 21 <C> 5.8 <R> <C> 20NG <C> sports <C> 100 <C> [BOLD] 1 <C> 100 <C> 5 <C> 1.8 <C> 60 <C> 11 <C> 6.4 <C> 55 <C> 44 <C> 7.7 <R> <C> 20NG <C> religion <C> 100 <C> [BOLD] 1.1 <C> 100 <C> 3 <C> 1.5 <C> 94 <C> 31 <C> 99 <C> 10 <C> 85 <C> 1.5 <R> <C> 20NG <C> computer <C> 100 <C> 1.6 <C> 100 <C> 7 <C> [BOLD] 0.6 <C> 40 <C> 35 <C> 77 <C> 38 <C> 18 <C> 4.9 <R> <C> Sentiment <C> vote <C> 100 <C> [BOLD] 0.1 <C> 100 <C> 8 <C> 5 <C> 15 <C> 16 <C> 13 <C> 97 <C> 13 <C> 1.5 <R> <C> Sentiment <C> movie <C> 100 <C> 1.3 <C> 100 <C> 59 <C> [BOLD] 0.9 <C> 72 <C> 81 <C> 55 <C> 90 <C> 62 <C> 2.3 <R> <C> Sentiment <C> books <C> 100 <C> [BOLD] 3.3 <C> 100 <C> 14 <C> 4.6 <C> 41 <C> 74 <C> 72 <C> 90 <C> 99 <C> 8.3 <R> <C> Sentiment <C> dvd <C> 100 <C> [BOLD] 2 <C> 100 <C> 28 <C> 2.8 <C> 64 <C> 8 <C> 8 <C> 58 <C> 64 <C> 9 <R> <C> Sentiment <C> electr. <C> 100 <C> [BOLD] 4 <C> 100 <C> 6 <C> 6.3 <C> 10 <C> 8 <C> 43 <C> 8 <C> 9 <C> 12 <R> <C> Sentiment <C> kitch. <C> 100 <C> 4.5 <C> 100 <C> 79 <C> [BOLD] 4.3 <C> 73 <C> 44 <C> 27 <C> 75 <C> 46 <C> 6.5 <CAP> Table 3: Model sizes (percentages of non-zero features in the resulting models). Bold for best, blue for best group.
<R> <C> [EMPTY] <C> Dataset <C> CNN <C> FastText <C> Best OMP <C> Best <R> <C> [EMPTY] <C> Dataset <C> (20eps) <C> (100eps) <C> or GOMP <C> Lasso <R> <C> 20NG <C> science <C> 0.935 <C> 0.958 <C> 0.964 <C> [BOLD] 0.968 <R> <C> 20NG <C> sports <C> 0.924 <C> 0.935 <C> 0.951 <C> [BOLD] 0.966 <R> <C> 20NG <C> religion <C> [BOLD] 0.934 <C> 0.898 <C> 0.902 <C> [BOLD] 0.934 <R> <C> 20NG <C> computer <C> 0.885 <C> 0.867 <C> 0.902 <C> [BOLD] 0.911 <R> <C> Sentiment <C> vote <C> 0.651 <C> 0.643 <C> [BOLD] 0.687 <C> 0.658 <R> <C> Sentiment <C> movie <C> 0.780 <C> 0.875 <C> 0.860 <C> [BOLD] 0.900 <R> <C> Sentiment <C> books <C> 0.742 <C> 0.787 <C> [BOLD] 0.805 <C> 0.800 <R> <C> Sentiment <C> dvd <C> 0.732 <C> 0.757 <C> [BOLD] 0.820 <C> 0.805 <R> <C> Sentiment <C> electr. <C> 0.760 <C> 0.800 <C> [BOLD] 0.830 <C> 0.820 <R> <C> Sentiment <C> kitch. <C> 0.805 <C> 0.845 <C> 0.830 <C> [BOLD] 0.860 <CAP> Table 5: Comparison in test accuracy with state-of-the-art classifiers: CNN Kim (2014), FastText Joulin et al. (2017) with no pre-trained vectors. The proposed OMP and GOMP algorithms produce the highest accurate model in 4 out of 10 datasets.
<R> <C> Transfer method <C> SQuAD GloVe  [ITALIC] EM <C> SQuAD GloVe  [ITALIC] F1 <C> SQuAD ELMo  [ITALIC] EM <C> SQuAD ELMo  [ITALIC] F1 <C> IMDB GloVe  [ITALIC] Accuracy <C> MNLI GloVe  [ITALIC] matched <C> MNLI GloVe  [ITALIC] mism. <R> <C> transfer feature only (baseline) <C> 69.33 <C> 78.73 <C> 74.75 <C> 82.95 <C> 88.51 <C> 77.14 <C> 77.40 <R> <C> GLoMo on embeddings <C> 70.84 <C> 79.90 <C> [BOLD] 76.00 <C> [BOLD] 84.13 <C> [BOLD] 89.16 <C> [BOLD] 78.32 <C> [BOLD] 78.00 <R> <C> GLoMo on RNN states <C> [BOLD] 70.95 <C> [BOLD] 79.95 <C> 75.59 <C> 83.62 <C> - <C> - <C> - <CAP> Table 1: Main results on natural language datasets. Self-attention modules are included in all baseline models. All baseline methods are feature-based transfer learning methods, including ELMo and GloVe. Our methods combine graph-based transfer with feature-based transfer. Our graphs operate on various sets of features, including GloVe embeddings, ELMo embeddings, and RNN states. “mism.” refers to the “mismatched” setting.
<R> <C> Method / Base-model <C> ResNet-18 <C> ResNet-34 <R> <C> baseline <C> 90.93±0.33 <C> 91.42±0.17 <R> <C> GLoMo <C> [BOLD] 91.55±0.23 <C> [BOLD] 91.70±0.09 <R> <C> ablation: uniform graph <C> 91.07±0.24 <C> - <CAP> Table 3: CIFAR-10 classification results. We adopt a 42,000/8,000 train/validation split—once the best model is selected according to the validation error, we directly forward it to the test set without doing any validation set place-back retraining. We only used horizontal flipping for data augmentation. The results are averaged from 5 rounds of experiments.
<R> <C> MODEL <C> CONTROL AUC <C> CONTROL 95% CI <C> DEMENTIA AUC <C> DEMENTIA 95% CI <C> CONTROL-DEMENTIA AUC <C> CONTROL-DEMENTIA 95% CI <R> <C> RWTHLM [ITALIC] LSTM <C> 0.80 <C> – <C> 0.64 <C> – <C> 0.92 <C> – <R> <C> GluonNLP [ITALIC] LSTM <C> 0.80 <C> ± 0.002 <C> 0.65 <C> ± 0.002 <C> 0.91 <C> ± 0.004 <CAP> Table 1: Classification accuracy using individual models’ perplexities and their difference for various models.
<R> <C> System <C> CoNLL P <C> CoNLL R <C> CoNLL M2 <C> JFLEG GLEU <R> <C> NMT <C> 66.61 <C> 17.58 <C> 42.76 <C> 50.08 <R> <C> NMT + RNN-LM <C> 61.05 <C> [BOLD] 26.71 <C> 48.56 <C> 56.04 <R> <C> NMT×4 <C> [BOLD] 71.10 <C> 15.42 <C> 41.29 <C> 50.30 <R> <C> NMT×4 + RNN-LM <C> 60.27 <C> 30.08 <C> [BOLD] 50.19 <C> [BOLD] 56.74 <CAP> Table 3: Results for NMT systems on the CoNLL-2014 (M2) and JFLEG Test (GLEU) sets.
<R> <C> [EMPTY] <C> Relations <C> Agreement <R> <C> Prevents <C> 154 <C> 63.0 <R> <C> Treats <C> 4,425 <C> 67.3 <R> <C> Treats Outcomes <C> 2,268 <C> 67.1 <R> <C> Not Established <C> 241 <C> 35.1 <R> <C> Not Recommended <C> 262 <C> 49.5 <R> <C> other <C> 1,262 <C> 35.9 <CAP> Table 1: Drug–disease relations collected using crowdsourcing. The “Agreement” column gives the average agreement between workers and the labels inferred. Higher agreement correlates strongly with the degree to which the information is explicit in the label text.
<R> <C> [EMPTY] <C> PDTB5 coarse <C> PDTB16 fine <C> T <R> <C> InferSent <C> 46.7 <C> 34.2 <C> - <R> <C> DisSent <C> 48.9 <C> 36,9 <C> - <R> <C> DiscoveryBase <C> [BOLD] 52.5 <C> 40.0 <C> 20.6 <R> <C> DiscoveryHard <C> 50.7 <C> 39.8 <C> 9.3 <R> <C> Discovery10 <C> 48.3 <C> 37.7 <C> 51.9 <R> <C> DiscoveryAdv <C> 49.7 <C> 37.6 <C> 26.1 <R> <C> DiscoveryShuffled <C> 51.0 <C> 39.5 <C> 11.5 <R> <C> DiscoveryBig <C> 51.3 <C> [BOLD] 41.3 <C> 22.2 <CAP> Table 8: Test results (accuracy) on implicit discursive relation prediction task (PDTB relations level 1 and 2, i.e coarse-grained and fine-grained) and training tasks T. Note that scores for T are not comparable since the test set changes for each version of the dataset.
<R> <C> Model <C> Dev Acc (%) <C> Test Acc (%) <R> <C> BERT-FT (A|P, Q) <C> 66.2 <C> 67.1 <R> <C> BERT-FT (A|P) <C> 63.5 <C> 64.5 <R> <C> BERT-FT (A|Q) <C> 56.2 <C> 55.9 <R> <C> BERT-FT (A) <C> 40.3 <C> 40.3 <CAP> Table 4: Ablation of Paragraphs (P) or Questions (Q)
<R> <C> Model <C> Dev Acc <C> Test Acc <R> <C> BERT-FTSWAG <C> 28.9 <C> 28.5 <R> <C> BERT-FTRACE <C> 42.0 <C> 42.5 <R> <C> BERT-FTRACE+SWAG <C> 44.2 <C> 45.1 <R> <C> BERT-FTSWAG→Cosmos <C> 67.8 <C> 68.9 <R> <C> BERT-FTRACE→Cosmos <C> 67.4 <C> 68.2 <R> <C> BERT-FTRACE+SWAG→Cosmos <C> 67.1 <C> 68.7 <CAP> Table 5: Knowledge transfer through fine-tuning. (%)
<R> <C> Metrics <C> GPT2 <C> GPT2-FT <R> <C> BLEU Papineni et al. ( 2002 ) <C> 10.7 <C> 21.0 <R> <C> METEOR Banerjee and Lavie ( 2005 ) <C> 7.2 <C> 8.6 <R> <C> ROUGE-L Lin ( 2004 ) <C> 13.9 <C> 22.1 <R> <C> CIDEr Vedantam et al. ( 2015 ) <C> 0.05 <C> 0.17 <R> <C> BERTScore F1 Zhang et al. ( 2019b ) <C> 41.9 <C> 44.5 <R> <C> Human <C> 11.0% <C> 29.0% <CAP> Table 6: Generative performance of pre-trained GPT2 and GPT2-FT on Cosmos QA. All automatic metric scores are averaged from 10 sets of sample output.
<R> <C> English-Malayalam <C> S1 <C> BLEU 08.52 <C> 1-TER 13.63 <C> 1-PER 32.32 <C> 1-CDER 21.46 <R> <C> [EMPTY] <C> S2 <C> 08.15 <C> [BOLD] 14.37 <C> [BOLD] 32.74 <C> [BOLD] 21.57 <R> <C> [EMPTY] <C> S3 <C> 08.10 <C> 09.85 <C> 24.07 <C> 20.36 <R> <C> [EMPTY] <C> S4 <C> [BOLD] 08.25 <C> 10.03 <C> 24.38 <C> 20.52 <R> <C> English-Hindi <C> S1 <C> 16.75 <C> 27.05 <C> 51.73 <C> 33.95 <R> <C> [EMPTY] <C> S2 <C> 18.74 <C> 31.30 <C> 51.94 <C> 37.37 <R> <C> [EMPTY] <C> S3 <C> 19.30 <C> 33.38 <C> 52.35 <C> 37.61 <R> <C> [EMPTY] <C> S4 <C> [BOLD] 19.43 <C> [BOLD] 33.53 <C> [BOLD] 52.57 <C> [BOLD] 37.77 <R> <C> English-Punjabi <C> S1 <C> 21.71 <C> 38.26 <C> 56.13 <C> 41.44 <R> <C> [EMPTY] <C> S2 <C> [BOLD] 23.09 <C> [BOLD] 40.90 <C> [BOLD] 56.83 <C> [BOLD] 44.06 <R> <C> [EMPTY] <C> S3 <C> 22.17 <C> 39.20 <C> 56.25 <C> 42.77 <R> <C> [EMPTY] <C> S4 <C> 22.26 <C> 39.35 <C> 56.48 <C> 42.88 <R> <C> English-Tamil <C> S1 <C> 06.20 <C> 13.05 <C> 32.72 <C> 21.97 <R> <C> [EMPTY] <C> S2 <C> 07.44 <C> 16.35 <C> 32.29 <C> 24.43 <R> <C> [EMPTY] <C> [ITALIC] S3′ <C> 07.47 <C> 17.87 <C> 34.86 <C> 23.49 <R> <C> [EMPTY] <C> [ITALIC] S4′ <C> [BOLD] 07.56 <C> [BOLD] 18.01 <C> [BOLD] 35.06 <C> [BOLD] 23.62 <CAP> Table 5: Translation quality scores for different systems; S1: BL; S2: BL+RO; S3: BL+RO+FACT; S3′: BL+RO+SPLIT+FACT; S4: BL+RO+FACT+TR; S4′: BL+RO+SPLIT+FACT+TR; BL: Baseline; RO: Reordering; FACT: Factored models; TR: Transliteration
<R> <C> Languages <C> Team <C> Avg Adequacy (A) <C> Avg Fluency (F) <C> Avg Rating (R) <C> A&F% <C> R% <C> BLEU <R> <C> English-Malayalam <C> CDAC-M <C> 1.92 <C> 1.67 <C> 1.60 <C> [BOLD] 38.34 <C> [BOLD] 31.94 <C> 2.60 <R> <C> English-Hindi <C> CDAC-M <C> 3.82 <C> 3.63 <C> 3.43 <C> [BOLD] 74.53 <C> [BOLD] 68.64 <C> 20.64 <R> <C> [EMPTY] <C> NIT-M <C> 3.27 <C> 3.56 <C> 3.26 <C> 68.27 <C> 65.14 <C> 23.25 <R> <C> [EMPTY] <C> IIT-B <C> 2.55 <C> 3.23 <C> 2.59 <C> 57.81 <C> 51.87 <C> 21.01 <R> <C> [EMPTY] <C> JU <C> 1.81 <C> 1.72 <C> 1.58 <C> 35.28 <C> 31.50 <C> 3.57 <R> <C> English-Punjabi <C> NIT-M <C> 3.38 <C> 3.74 <C> 3.235 <C> [BOLD] 67.55 <C> [BOLD] 65.05 <C> 9.24 <R> <C> [EMPTY] <C> CDAC-M <C> 3.05 <C> 3.02 <C> 2.92 <C> 60.91 <C> 58.34 <C> 8.68 <R> <C> [EMPTY] <C> IIT-B <C> 2.65 <C> 2.71 <C> 2.62 <C> 52.93 <C> 52.4 <C> 11.38 <R> <C> English-Tamil <C> CDAC-M <C> 2.61 <C> 2.57 <C> 2.40 <C> [BOLD] 52.26 <C> [BOLD] 48.00 <C> 6.15 <R> <C> [EMPTY] <C> HANS <C> 2.16 <C> 2.12 <C> 2.17 <C> 43.22 <C> 43.50 <C> 1.93 <R> <C> [EMPTY] <C> NIT-M <C> 1.59 <C> 1.65 <C> 1.58 <C> 31.72 <C> 31.74 <C> 1.31 <CAP> Table 7: Submissions at MTIL2017; CDAC-M: Centre for Development of Advanced Computing, Mumbai, India; IIT-B: Indian Institute of Technology, Bombay, India; NIT-M: National Institute of Technology, Mizoram, India; JU: Jadavpur University, Kolkata, West Bengal, India; HANS: New York University, New York City, NY, United States; Avg: Average of three manual evaluation scores.
<R> <C> [BOLD] Language <C> [BOLD] Accuracy <C> [BOLD] Edit dist. <R> <C> Albanian <C> 97.9 <C> 0.07 <R> <C> Arabic <C> 89.8 <C> 0.39 <R> <C> Armenian <C> 95.6 <C> 0.08 <R> <C> Basque <C> 100.0 <C> 0.00 <R> <C> Bengali <C> 99.0 <C> 0.05 <R> <C> Bulgarian <C> 96.7 <C> 0.07 <R> <C> Catalan <C> 97.8 <C> 0.06 <R> <C> Czech <C> 92.0 <C> 0.15 <R> <C> Danish <C> 93.8 <C> 0.09 <R> <C> Dutch <C> 95.9 <C> 0.07 <R> <C> English <C> 96.6 <C> 0.07 <R> <C> Estonian <C> 96.8 <C> 0.08 <R> <C> Faroese <C> 84.6 <C> 0.31 <R> <C> Finnish <C> 91.0 <C> 0.17 <R> <C> French <C> 87.5 <C> 0.24 <R> <C> Georgian <C> 97.6 <C> 0.05 <R> <C> German <C> 89.5 <C> 0.21 <R> <C> Haida <C> 95.0 <C> 0.10 <R> <C> Hebrew <C> 99.0 <C> 0.01 <R> <C> Hindi <C> 99.8 <C> 0.00 <R> <C> Hungarian <C> 84.8 <C> 0.35 <R> <C> Icelandic <C> 86.3 <C> 0.25 <R> <C> Irish <C> 87.6 <C> 0.35 <R> <C> Italian <C> 96.8 <C> 0.09 <R> <C> Khaling <C> 98.3 <C> 0.03 <R> <C> Kurmanji <C> 93.8 <C> 0.10 <R> <C> Latin <C> 75.3 <C> 0.39 <R> <C> Latvian <C> 95.4 <C> 0.08 <R> <C> Lithuanian <C> 91.0 <C> 0.15 <R> <C> Lower Sorbian <C> 96.9 <C> 0.06 <R> <C> Macedonian <C> 96.6 <C> 0.06 <R> <C> Navajo <C> 88.9 <C> 0.28 <R> <C> Northern Sami <C> 94.5 <C> 0.12 <R> <C> Norwegian (Bokmål) <C> 92.4 <C> 0.13 <R> <C> Norwegian (Nynorsk) <C> 89.4 <C> 0.18 <R> <C> Persian <C> 99.3 <C> 0.01 <R> <C> Polish <C> 90.6 <C> 0.22 <R> <C> Portuguese <C> 98.8 <C> 0.02 <R> <C> Quechua <C> 100.0 <C> 0.00 <R> <C> Romanian <C> 86.4 <C> 0.42 <R> <C> Russian <C> 89.3 <C> 0.31 <R> <C> Serbo-Croatian <C> 90.1 <C> 0.24 <R> <C> Slovak <C> 93.1 <C> 0.13 <R> <C> Slovene <C> 96.6 <C> 0.07 <R> <C> Sorani <C> 88.6 <C> 0.14 <R> <C> Spanish <C> 93.5 <C> 0.15 <R> <C> Swedish <C> 91.8 <C> 0.13 <R> <C> Turkish <C> 96.6 <C> 0.11 <R> <C> Ukrainian <C> 94.2 <C> 0.11 <R> <C> Urdu <C> 99.7 <C> 0.01 <R> <C> Welsh <C> 99.0 <C> 0.03 <R> <C> [BOLD] Average <C> [BOLD] 93.6 <C> [BOLD] 0.14 <CAP> Table 1: Our system’s official results on the SIGMORPHON-2017 shared task-1 test set in the high setting.
<R> <C> [BOLD] Note Type <C> [BOLD] Count <C> Percentage <R> <C> Nursing/other <C> 83147 <C> 36.78% <R> <C> Radiology <C> 61096 <C> 27.02% <R> <C> Nursing <C> 43790 <C> 19.37% <R> <C> Physician <C> 27789 <C> 12.30% <R> <C> Respiratory <C> 5728 <C> 2.53% <R> <C> General <C> 1775 <C> 0.78% <R> <C> Nutrition <C> 1549 <C> 0.68% <R> <C> Rehab Services <C> 521 <C> 0.23% <R> <C> Social Work <C> 501 <C> 0.22% <R> <C> Case Management <C> 134 <C> 0.060% <R> <C> Consult <C> 40 <C> 0.018% <R> <C> Pharmacy <C> 12 <C> 0.0053% <R> <C> Overall <C> 226082 <C> 100% <CAP> Table 1: Distribution of free-text medical notes in our dataset.
<R> <C> [BOLD] Model <C> [BOLD] All <C> [BOLD] Excl. low <C> [BOLD] Transd. only <R> <C> [BOLD] Model <C> [BOLD] treebanks <C> [BOLD] resource <C> [BOLD] treebanks <R> <C> Basic <C> 92.22 <C> 95.37 <C> 92.03 <R> <C> Augm. autoencoder 4K <C> 92.89 <C> 95.42 <C> 93.11 <R> <C> Augm. transducer 4K <C> 93.15 <C> 95.45 <C> 93.55 <R> <C> Augm. mixed 2K + 2K <C> 93.12 <C> 95.47 <C> 93.45 <R> <C> Augm. mixed 4K + 4K <C> 93.17 <C> 95.48 <C> 93.56 <R> <C> Augm. mixed 8K + 8K <C> [BOLD] 93.24 <C> [BOLD] 95.51 <C> [BOLD] 93.61 <R> <C> Transd. Coverage <C> — <C> — <C> 86.76 <R> <C> Transd. Recall <C> — <C> — <C> 78.15 <CAP> Table 1: Evaluation of our two data augmentation methods, augmented with autoencoder and augmented with transducer as well as a mixed method, compared to our basic models. Additionally, we measure average percentage of words recognized by the transducer (Transducer Coverage) and average percentage of words having the correct lemma among the possible analyses (Transducer Recall), which represents an oracle accuracy achievable by transducers if all lemmas could be disambiguated correctly. All metrics are measured on token level.
<R> <C> [EMPTY] <C> [BOLD] IWSLT  [BOLD] De → En <C> [BOLD] IWSLT  [BOLD] Es → En <C> [BOLD] IWSLT  [BOLD] He → En <C> [BOLD] WMT  [BOLD] En → De <R> <C> [ITALIC] Base <C> 34.79 <C> 41.58 <C> 33.64 <C> 28.40 <R> <C> + [ITALIC] Swap <C> 34.70 <C> 41.60 <C> 34.25 <C> 28.13 <R> <C> + [ITALIC] Dropout <C> 35.13 <C> 41.62 <C> 34.29 <C> 28.29 <R> <C> + [ITALIC] Blank <C> 35.37 <C> 42.28 <C> 34.37 <C> 28.89 <R> <C> + [ITALIC] Smooth <C> 35.45 <C> 41.69 <C> 34.61 <C> 28.97 <R> <C> + [ITALIC] LMsample <C> 35.40 <C> 42.09 <C> 34.31 <C> 28.73 <R> <C> [BOLD] Ours <C> [BOLD] 35.78 <C> [BOLD] 42.61 <C> [BOLD] 34.91 <C> [BOLD] 29.70 <CAP> Table 1: BLEU scores on four translation tasks.
<R> <C> Task WMT14 En-De <C> Time Speedup 68.89 <C> Time Speedup 1.00× <C> Sparsity 0.00% <C> Quality 27.59 <R> <C> WMT14 En-De <C> 68.38 <C> 1.01× <C> 46.7% <C> 27.06 <R> <C> WMT18 Zh-En <C> 116.3 <C> 1.00× <C> 0.00% <C> 21.10 <R> <C> WMT18 Zh-En <C> 118.3 <C> 0.98× <C> 39.1% <C> 20.80 <R> <C> CNN/Daily Mail <C> 3909 <C> 1.00× <C> 0.00% <C> 36.88 <R> <C> CNN/Daily Mail <C> 3227 <C> 1.21× <C> 47.6% <C> 36.51 <R> <C> WikiSum <C> 70505 <C> 1.00× <C> 0.00% <C> 39.20 <R> <C> WikiSum <C> 42669 <C> 1.65× <C> 71.5% <C> 38.75 <CAP> Table 1: Decoding results for different tasks when finetuning with λ=0.3. “Time”: the decoding time (in seconds) of the whole test set. “Sparsity”: the sparsity rate, 0.00% indicates the Transformer baseline. “Speedup”: the decoding acceleration over the baseline. “Quality”: BLEU for WMT tasks and ROUGE-L for summarization tasks. We evaluate the decoding time on GeForce GTX 1080 Ti, with a batch size of 32 for WMT tasks and 10 for summarization tasks.
<R> <C> Pattern <C> WMT14 En-De Sparsity <C> WMT14 En-De BLEU <C> CNN/Daily Mail Sparsity <C> CNN/Daily Mail RL <R> <C> Baseline <C> 0.00% <C> 27.59 <C> 0.00% <C> 36.88 <R> <C> [ITALIC] L0Drop <C> 46.7% <C> 27.06 <C> 47.6% <C> 36.51 <R> <C> POS Pattern <C> 46.7% <C> 27.11 <C> 39.6% <C> 35.57 <R> <C> Freq Pattern <C> 42.1% <C> 26.98 <C> 47.8% <C> 35.67 <R> <C> Group Pattern <C> 50.0% <C> 26.82 <C> 50.0% <C> 30.69 <R> <C> Inv Freq Pattern <C> 44.7% <C> 26.42 <C> 39.0% <C> - <CAP> Table 2: Sparsity and generation quality for different models on the WMT14 En-De (measured by tokenized case-sensitive BLEU) and the CNN/Daily Mail (measured by ROUGE-L or RL) test set. The sparsity rate is evaluated on test set.
<R> <C> [BOLD] Type <C> [BOLD] ERRANT F0.5  [BOLD] No train <C> [BOLD] ERRANT F0.5  [BOLD] Train <R> <C> Missing <C> - <C> 51.96 <R> <C> Replacement <C> 45.53 <C> 55.63 <R> <C> Unnecessary <C> - <C> 50.38 <R> <C> ADJ <C> - <C> 27.03 <R> <C> ADV <C> - <C> 29.80 <R> <C> DET <C> 19.17 <C> 55.01 <R> <C> MORPH <C> 33.20 <C> 64.81 <R> <C> NOUN <C> 4.31 <C> 34.88 <R> <C> NOUN:NUM <C> 53.43 <C> 64.96 <R> <C> NOUN:POSS <C> - <C> 13.51 <R> <C> ORTH <C> 62.77 <C> 74.07 <R> <C> OTHER <C> 2.45 <C> 18.39 <R> <C> PREP <C> 34.39 <C> 56.58 <R> <C> PRON <C> - <C> 40.91 <R> <C> PUNCT <C> - <C> 46.08 <R> <C> SPELL <C> 67.91 <C> 75.21 <R> <C> VERB <C> - <C> 37.94 <R> <C> VERB:FORM <C> 48.03 <C> 63.33 <R> <C> VERB:SVA <C> 66.67 <C> 68.39 <R> <C> VERB:TENSE <C> 35.39 <C> 47.90 <CAP> Table 4: A selection of ERRANT F0.5 error type scores comparing the best CoNLL-2014 system with and without training data. A dash means the system did not attempt to correct the error type.
<R> <C> [BOLD] Model <C> MR <C> CR <C> SUBJ <C> MPQA <C> TREC <R> <C> NB-SVM <C> 79.4 <C> 81.8 <C> 93.2 <C> 86.3 <C> - <R> <C> MNB <C> 79.0 <C> 80.0 <C> 93.6 <C> 86.3 <C> - <R> <C> RAE <C> 77.7 <C> - <C> - <C> 86.4 <C> - <R> <C> MV-RecNN <C> 79.0 <C> - <C> - <C> - <C> - <R> <C> CNN <C> 81.5 <C> 85.0 <C> 93.4 <C> 89.6 <C> [BOLD] 93.6 <R> <C> DCNN <C> - <C> - <C> - <C> - <C> 93.0 <R> <C> P.V. <C> 74.8 <C> 78.1 <C> 90.5 <C> 74.2 <C> 91.8 <R> <C> cBoW <C> 77.2 <C> 79.9 <C> 91.3 <C> 86.4 <C> 87.3 <R> <C> RNN <C> 77.2 <C> 82.3 <C> 93.7 <C> 90.1 <C> 90.2 <R> <C> BRNN <C> 82.3 <C> 82.6 <C> 94.2 <C> 90.3 <C> 91.0 <R> <C> GrConv <C> 76.3 <C> 81.3 <C> 89.5 <C> 84.5 <C> 88.4 <R> <C> AdaSent <C> [BOLD] 83.1 <C> [BOLD] 86.3 <C> [BOLD] 95.5 <C> [BOLD] 93.3 <C> 92.4 <CAP> Table 2: Classification accuracy of AdaSent compared with other models. For NB-SVM, MNB, RAE, MV-RecNN, CNN and DCNN, we use the results reported in the corresponding paper. We use the public implementation of P.V. and we implement other methods.
<R> <C> [BOLD] Model <C> MR <C> CR <C> SUBJ <R> <C> P.V. <C> 71.11±0.80 <C> 71.22±1.04 <C> 90.22±0.21 <R> <C> cBoW <C> 72.74±1.03 <C> 71.86±2.00 <C> 90.58±0.52 <R> <C> RNN <C> 74.39±1.70 <C> 73.81±3.52 <C> 89.97±2.88 <R> <C> BRNN <C> 75.25±1.33 <C> 76.72±2.78 <C> 90.93±1.00 <R> <C> GrConv <C> 71.64±2.09 <C> 71.52±4.18 <C> 86.53±1.33 <R> <C> AdaSent <C> [BOLD] 79.84± [BOLD] 1.26 <C> [BOLD] 83.61± [BOLD] 1.60 <C> [BOLD] 92.19± [BOLD] 1.19 <R> <C> [BOLD] Model <C> MPQA <C> TREC <C> [EMPTY] <R> <C> P.V. <C> 67.93±0.57 <C> 86.30±1.10 <C> [EMPTY] <R> <C> cBoW <C> 84.04±1.20 <C> 85.16±1.76 <C> [EMPTY] <R> <C> RNN <C> 84.52±1.17 <C> 84.24±2.61 <C> [EMPTY] <R> <C> BRNN <C> 85.36±1.13 <C> 86.28±0.90 <C> [EMPTY] <R> <C> GrConv <C> 82.00±0.88 <C> 82.04±2.23 <C> [EMPTY] <R> <C> AdaSent <C> [BOLD] 90.42± [BOLD] 0.71 <C> [BOLD] 91.10± [BOLD] 1.04 <C> [EMPTY] <CAP> Table 3: Model variance.
<R> <C> Methods <C> nDCG@5 <C> nDCG@10 <C> P@5 <C> P@10 <R> <C> BM25 <C> 0.3251 <C> 0.3359 <C> 0.532 <C> 0.446 <R> <C> (A1.1) wBT+BM25 <C> 0.4088 <C> 0.4155 <C> [BOLD] 0.644 <C> 0.53 <R> <C> (A1.2) wQ+BM25 <C> 0.3942 <C> [BOLD] 0.4315 <C> 0.632 <C> [BOLD] 0.578 <R> <C> (A2.1) IR-RoBERTa <C> 0.394 <C> 0.3918 <C> 0.628 <C> 0.514 <R> <C> (A2.2) IR-BERT <C> [BOLD] 0.4199 <C> 0.4104 <C> 0.628 <C> 0.504 <CAP> Table 2. Comparison of nDCG and precision values between proposed methods and BM25
<R> <C> [ITALIC] η <C> [BOLD] Agreement <C> [BOLD] es <C> [BOLD] nl <C> [BOLD] de <C> [BOLD] ar <C> [BOLD] fi <R> <C> Distillation by clustering <C> Distillation by clustering <C> Distillation by clustering <C> Distillation by clustering <C> Distillation by clustering <C> Distillation by clustering <C> Distillation by clustering <R> <C> .7 <C> ∩ <C> 82.28 <C> 83.25 <C> 78.86 <C> 52.64 <C> 78.47 <R> <C> .5 <C> ∩ <C> 82.35 <C> 83.11 <C> 78.16 <C> 54.20 <C> 78.28 <R> <C> Distillation by model confidence <C> Distillation by model confidence <C> Distillation by model confidence <C> Distillation by model confidence <C> Distillation by model confidence <C> Distillation by model confidence <C> Distillation by model confidence <R> <C> 50% <C> ∩ <C> [BOLD] 82.52 <C> 82.46 <C> 75.95 <C> 52.00 <C> 77.51 <R> <C> 50% <C> [ITALIC] ϕ <C> 81.66 <C> 82.26 <C> 77.19 <C> 52.97 <C> 77.77 <R> <C> 80% <C> ∩ <C> 82.33 <C> [BOLD] 83.53 <C> 78.50 <C> [BOLD] 54.48 <C> 78.43 <R> <C> 80% <C> [ITALIC] ϕ <C> 81.61 <C> 83.03 <C> 77.08 <C> 53.31 <C> 78.34 <R> <C> 90% <C> ∩ <C> 81.90 <C> 82.80 <C> [BOLD] 79.03 <C> 52.41 <C> [BOLD] 78.66 <R> <C> 90% <C> [ITALIC] ϕ <C> 81.21 <C> 82.77 <C> 77.28 <C> 52.20 <C> 77.93 <R> <C> 100% <C> ∩ <C> 82.50 <C> 82.35 <C> 77.06 <C> 52.58 <C> 77.51 <R> <C> 100% <C> [ITALIC] ϕ <C> 81.89 <C> 82.15 <C> 76.97 <C> 52.68 <C> 78.01 <CAP> Figure 2: Validation F1 results in XNER for multi-epoch co-teaching training of MultiMix.
<R> <C> [BOLD] Context Type <C> Adj <C> Verbs <C> Nouns <C> All <R> <C> BOW (win=2) <C> 0.604 <C> 0.307 <C> 0.501 <C> 0.464 <R> <C> POSIT (win=2) <C> 0.585 <C> 0.400 <C> 0.471 <C> 0.469 <R> <C> COORD (conjlr) <C> 0.629 <C> 0.413 <C> 0.428 <C> 0.430 <R> <C> SP <C> 0.649 <C> [BOLD] 0.458 <C> 0.414 <C> 0.444 <R> <C> DEPS-All <C> 0.574 <C> 0.389 <C> 0.492 <C> 0.464 <R> <C> BEST-ADJ <C> [BOLD] 0.671 <C> 0.348 <C> 0.504 <C> 0.449 <R> <C> BEST-VERBS <C> 0.392 <C> 0.455 <C> 0.478 <C> 0.448 <R> <C> BEST-NOUNS <C> 0.581 <C> 0.327 <C> [BOLD] 0.535 <C> 0.489 <R> <C> BEST-ALL <C> 0.616 <C> 0.402 <C> 0.519 <C> [BOLD] 0.506 <CAP> Table 6: Results on the A/V/N SimLex-999 subsets, and on the entire set (All) in the setup from Schwartz:2016naacl. d=500. BEST-* are again the best class-specific configs returned by Alg. 1.
<R> <C> [BOLD] Number of Words in NAWL <C> [BOLD] Number of Words in NAWL (after stemming) <C> [BOLD] Coverage of NAWL by LScDC (#) <C> [BOLD] Coverage of NAWL by LScDC (%) <R> <C> 963 <C> 895 <C> 891 <C> 99.6% <CAP> Table 7.5. Coverage of the NAWL by the LScDC.
<R> <C> [BOLD] Number of interval 179 <C> [BOLD] Width of interval 5 <C> [BOLD] Percentage of overlapping 1.8% <C> [BOLD] Number of interval 13 <C> [BOLD] Width of interval 70 <C> [BOLD] Percentage of overlapping 16.2% <R> <C> 90 <C> 10 <C> 2.9% <C> 12 <C> 75 <C> 18.5% <R> <C> 60 <C> 15 <C> 4.5% <C> 11 <C> 85 <C> 20.0% <R> <C> 45 <C> 20 <C> 5.1% <C> 10 <C> 90 <C> 21.9% <R> <C> 36 <C> 257 <C> 6.2% <C> 9 <C> 100 <C> 22.9% <R> <C> 30 <C> 30 <C> 8.0% <C> 8 <C> 115 <C> 25.5% <R> <C> 26 <C> 35 <C> 8.6% <C> 7 <C> 130 <C> 27.9% <R> <C> 23 <C> 40 <C> 9.3% <C> 6 <C> 150 <C> 33.2% <R> <C> 20 <C> 45 <C> 12.6% <C> 5 <C> 180 <C> 37.9% <R> <C> 18 <C> 50 <C> 12.2% <C> 4 <C> 225 <C> 46.4% <R> <C> 17 <C> 55 <C> 13.2% <C> 3 <C> 300 <C> 55.7% <R> <C> 15 <C> 60 <C> 14.1% <C> 2 <C> 450 <C> 72.8% <R> <C> 14 <C> 65 <C> 15.8% <C> 1 <C> 891 <C> 100.0% <CAP> Table 7.10. The percentage of the overlapping of words in intervals with number of intervals and width of intervals.
<R> <C> [BOLD] Test <C> [BOLD] Test Statistics <R> <C> PCC <C> 0.30 <R> <C> SRC <C> 0.58 <R> <C> PCC-log <C> 0.61 <CAP> Table 7.11. Correlation coefficients that measure the relationship between ranks of words in NAWL and LScDC: Pearson’s Correlation Coefficient (PCC), Spearman’s Rank Correlation (SRC) and PCC for logarithmic scaled frequencies.
<R> <C> [EMPTY] <C> [BOLD] Words/Sec. <C> [BOLD] Speedup <R> <C> [BOLD] Type <C> [BOLD] (Single-Threaded) <C> [BOLD] Factor <R> <C> Baseline <C> 95 <C> 1.00x <R> <C> + 16-Bit Mult. <C> 248 <C> 2.59x <R> <C> + Pre-Comp. Emb. <C> 311 <C> 3.25x <R> <C> + Pre-Comp. Att. <C> 342 <C> 3.57x <R> <C> + SSE & Lookup <C> 386 <C> 4.06x <R> <C> + Merge Rec. <C> 418 <C> 4.37x <CAP> Table 1: Decoding speeds on an Intel E5-2660 CPU, processing each sentence independently.
<R> <C> [BOLD] System <C> [BOLD] BLEU <C> [BOLD] Words/Sec  [BOLD] (Single-Threaded) <R> <C> Basic Phrase-Based MT (Schwenk,  2014 ) <C> 33.1 <C> - <R> <C> SOTA Phrase-Based MT (Durrani et al.,  2014 ) <C> 37.0 <C> - <R> <C> 6-Layer Non-Attentional Seq-to-Seq LSTM (Luong et al.,  2014 ) <C> 33.1 <C> - <R> <C> RNN Search, 1-Layer Att. GRU, w/ Large Vocab (Jean et al.,  2015 ) <C> 34.6 <C> † <R> <C> Google NMT, 8-Layer Att. LSTM, Word-Based (Wu et al.,  2016 ) <C> 37.9 <C> ♭ <R> <C> Google NMT, 8-Layer Att. LSTM, WPM-32k (Wu et al.,  2016 ) <C> 39.0‡ <C> ♭ <R> <C> Baidu Deep Attention, 8-Layer Att. LSTM (Zhou et al.,  2016 ) <C> 39.2 <C> - <R> <C> (S1) Trg: 1024-AttGRU <C> 36.2 <C> 418 <R> <C> (S2) Trg: 1024-AttGRU + 1024-GRU <C> 36.8 <C> 242 <R> <C> (S3) Trg: 1024-AttGRU + 3-Layer 768-FC-Relu + 1024-FC-Tanh <C> 37.1 <C> 271 <R> <C> (S4) Trg: 1024-AttGRU + 7-Layer 768-FC-Relu + 1024-FC-Tanh <C> 37.4 <C> 229 <R> <C> (S5) Trg: 1024-AttGRU + 7-Layer 768-FC-Relu + 1024-GRU <C> 37.6 <C> 157 <R> <C> (S6) Trg: 1024-AttGRU + 15-Layer 768-FC-Relu + 1024-FC-Tanh <C> 37.3 <C> 163 <R> <C> (S7) Src: 8-Layer LSTM, Trg: 1024-AttLSTM + 7-Layer 1024-LSTM \lx @ [ITALIC] sectionsign <C> 37.8 <C> 28 <R> <C> [BOLD] (E1) Ensemble of 2x Model (S4) <C> [BOLD] 38.3 <C> [BOLD] 102 <R> <C> (E2) Ensemble of 3x Model (S4) <C> 38.5 <C> 65 <CAP> Table 2: Results on WMT English-French NewsTest2014. Models (S1)-(S6) use a 3-layer 512-dim bidirectional GRU for the source side. The CPU is an Intel Haswell E5-2660. † Reported as ~8 words/sec on one CPU core. ♭ Reported as ~100 words/sec, parallelized across 44 CPU cores. ‡ Uses word-piece tokenization, all others are word-based. \lx@sectionsign Reproduction of Google NMT, Word-Based.
<R> <C> [BOLD] Adaptation <C> [BOLD] Similarity <C> [BOLD] AUC(0.05) <C> [BOLD] AUC(0.1) <R> <C> — <C> Sigmoid <C> 0.431 <C> 0.557 <R> <C> — <C> Cosine <C> 0.692 <C> 0.782 <R> <C> Classification <C> Cosine <C> 0.791 <C> 0.862 <R> <C> Wasserstein <C> Cosine <C> 0.795 <C> 0.869 <CAP> Table 3: Duplicate question detection: direct transfer vs. adversarial domain adaptation from AskUbuntu to Android.
<R> <C> [EMPTY] <C> nolm <C> lm <C> maskedlm <C> lm+bpe <C> lm+pos <C> lm+rnn <C> lm+bpe+rnn <R> <C> [EMPTY] <C> [BOLD] KP20k <C> [BOLD] KP20k <C> [BOLD] KP20k <C> [BOLD] KP20k <C> [BOLD] KP20k <C> [BOLD] KP20k <C> [BOLD] KP20k <R> <C> F1@5 <C> 0.254 <C> 0.281 <C> 0.225 <C> 0.273 <C> 0.281 <C> [BOLD] 0.300 <C> 0.288 <R> <C> F1@10 <C> 0.230 <C> 0.277 <C> 0.214 <C> 0.270 <C> 0.277 <C> [BOLD] 0.299 <C> 0.287 <R> <C> [EMPTY] <C> [BOLD] Inspec <C> [BOLD] Inspec <C> [BOLD] Inspec <C> [BOLD] Inspec <C> [BOLD] Inspec <C> [BOLD] Inspec <C> [BOLD] Inspec <R> <C> F1@5 <C> 0.287 <C> 0.390 <C> 0.273 <C> 0.392 <C> 0.398 <C> [BOLD] 0.423 <C> 0. [BOLD] 423 <R> <C> F1@10 <C> 0.364 <C> 0.483 <C> 0.361 <C> 0.477 <C> 0.485 <C> [BOLD] 0.511 <C> 0.504 <R> <C> [EMPTY] <C> [BOLD] Krapivin <C> [BOLD] Krapivin <C> [BOLD] Krapivin <C> [BOLD] Krapivin <C> [BOLD] Krapivin <C> [BOLD] Krapivin <C> [BOLD] Krapivin <R> <C> F1@5 <C> 0.192 <C> 0.256 <C> 0.177 <C> [BOLD] 0.283 <C> 0.246 <C> 0.252 <C> 0.276 <R> <C> F1@10 <C> 0.190 <C> 0.247 <C> 0.187 <C> [BOLD] 0.280 <C> 0.252 <C> 0.256 <C> 0.269 <R> <C> [EMPTY] <C> [BOLD] NUS <C> [BOLD] NUS <C> [BOLD] NUS <C> [BOLD] NUS <C> [BOLD] NUS <C> [BOLD] NUS <C> [BOLD] NUS <R> <C> F1@5 <C> 0.191 <C> 0.313 <C> 0.215 <C> 0.331 <C> 0.327 <C> 0.326 <C> [BOLD] 0.331 <R> <C> F1@10 <C> 0.190 <C> 0.343 <C> 0.247 <C> 0.345 <C> 0.339 <C> 0.340 <C> [BOLD] 0.345 <R> <C> [EMPTY] <C> [BOLD] SemEval <C> [BOLD] SemEval <C> [BOLD] SemEval <C> [BOLD] SemEval <C> [BOLD] SemEval <C> [BOLD] SemEval <C> [BOLD] SemEval <R> <C> F1@5 <C> 0.167 <C> 0.295 <C> 0.173 <C> [BOLD] 0.312 <C> 0.289 <C> 0.292 <C> 0.301 <R> <C> F1@10 <C> 0.195 <C> 0.328 <C> 0.211 <C> 0.335 <C> 0.331 <C> 0.336 <C> [BOLD] 0.341 <R> <C> [EMPTY] <C> [BOLD] KPTimes <C> [BOLD] KPTimes <C> [BOLD] KPTimes <C> [BOLD] KPTimes <C> [BOLD] KPTimes <C> [BOLD] KPTimes <C> [BOLD] KPTimes <R> <C> F1@5 <C> 0.286 <C> 0.368 <C> 0.287 <C> 0.382 <C> 0.378 <C> 0.401 <C> [BOLD] 0.405 <R> <C> F1@10 <C> 0.276 <C> 0.362 <C> 0.277 <C> 0.382 <C> 0.372 <C> 0.398 <C> [BOLD] 0.404 <R> <C> [EMPTY] <C> [BOLD] JPTimes <C> [BOLD] JPTimes <C> [BOLD] JPTimes <C> [BOLD] JPTimes <C> [BOLD] JPTimes <C> [BOLD] JPTimes <C> [BOLD] JPTimes <R> <C> F1@5 <C> 0.249 <C> 0.307 <C> 0.244 <C> 0.318 <C> 0.323 <C> [BOLD] 0.348 <C> 0.335 <R> <C> F1@10 <C> 0.248 <C> 0.310 <C> 0.246 <C> 0.322 <C> 0.326 <C> [BOLD] 0.350 <C> 0.337 <R> <C> [EMPTY] <C> [BOLD] DUC <C> [BOLD] DUC <C> [BOLD] DUC <C> [BOLD] DUC <C> [BOLD] DUC <C> [BOLD] DUC <C> [BOLD] DUC <R> <C> F1@5 <C> 0.195 <C> [BOLD] 0.277 <C> 0.190 <C> 0.270 <C> 0.269 <C> 0.271 <C> [BOLD] 0.277 <R> <C> F1@10 <C> 0.227 <C> 0.326 <C> 0.222 <C> 0.312 <C> 0.317 <C> [BOLD] 0.334 <C> 0.320 <R> <C> [EMPTY] <C> [BOLD] Average <C> [BOLD] Average <C> [BOLD] Average <C> [BOLD] Average <C> [BOLD] Average <C> [BOLD] Average <C> [BOLD] Average <R> <C> F@5 <C> 0.228 <C> 0.311 <C> 0.223 <C> 0.320 <C> 0.314 <C> 0.327 <C> [BOLD] 0.330 <R> <C> F@10 <C> 0.240 <C> 0.334 <C> 0.246 <C> 0.340 <C> 0.337 <C> [BOLD] 0.353 <C> 0.351 <CAP> Table 3: Results of the ablation study.
<R> <C> Data <C> VggT <C> VggTLarge <R> <C> MuST-C + LS + 17607h LL <C> 23.8 <C> 23.7 <R> <C> MuST-C + LS + 35217h LL <C> 24.6 <C> 25.6 <CAP> Table 3: Results obtained on the En-De MuST-C dev set when increasing the model size. Results are reported after fine-tuning.
<R> <C> ID <C> Category <C> balAPinc <C> ConVecs <C> SimDiffs <C> Size <R> <C> 1 <C> class-inclusion <C> 79.1 <C> 76.1 <C> 88.1 <C> 67 <R> <C> 2 <C> part-whole <C> 61.0 <C> 69.5 <C> 66.9 <C> 118 <R> <C> 3 <C> similar <C> 46.5 <C> 44.2 <C> 54.7 <C> 86 <R> <C> 4 <C> contrast <C> 65.0 <C> 76.7 <C> 80.0 <C> 60 <R> <C> 5 <C> attribute <C> 44.2 <C> 68.4 <C> 70.5 <C> 95 <R> <C> 6 <C> non-attribute <C> 77.5 <C> 74.6 <C> 76.1 <C> 71 <R> <C> 7 <C> case relations <C> 43.9 <C> 64.9 <C> 71.9 <C> 57 <R> <C> 8 <C> cause-purpose <C> 49.2 <C> 73.8 <C> 75.4 <C> 65 <R> <C> 9 <C> space-time <C> 68.4 <C> 83.5 <C> 77.2 <C> 79 <R> <C> 10 <C> reference <C> 40.5 <C> 75.7 <C> 73.0 <C> 74 <R> <C> — <C> average <C> 57.3 <C> 70.2 <C> 72.4 <C> 77 <CAP> Table 7: Accuracy for each of the ten high-level categories in the Test set
<R> <C> Algorithm <C> Matrices <C> AP0 <C> AP1 <C> Pre <C> Rec <C> F <C> Acc <C> 95% C.I. <R> <C> balAPinc <C> [BOLD] Gen <C> 0.57 <C> 0.56 <C> 0.573 <C> 0.573 <C> 0.573 <C> 57.3 <C> 53.8–60.7 <R> <C> [EMPTY] <C> Dom <C> 0.53 <C> 0.54 <C> 0.532 <C> 0.532 <C> 0.532 <C> 53.2 <C> 49.7–56.7 <R> <C> [EMPTY] <C> Fun <C> 0.54 <C> 0.53 <C> 0.530 <C> 0.530 <C> 0.530 <C> 53.0 <C> 49.5–56.5 <R> <C> ConVecs <C> [BOLD] Gen <C> 0.76 <C> 0.77 <C> 0.703 <C> 0.702 <C> 0.702 <C> 70.2 <C> 66.9–73.3 <R> <C> [EMPTY] <C> Dom <C> 0.72 <C> 0.75 <C> 0.676 <C> 0.675 <C> 0.674 <C> 67.5 <C> 64.1–70.7 <R> <C> [EMPTY] <C> Fun <C> 0.79 <C> 0.78 <C> 0.719 <C> 0.719 <C> 0.719 <C> 71.9 <C> 68.6–75.0 <R> <C> SimDiffs <C> [BOLD] Dom, Fun <C> 0.80 <C> 0.79 <C> 0.724 <C> 0.724 <C> 0.724 <C> 72.4 <C> 69.1–75.4 <R> <C> [EMPTY] <C> Gen, Fun <C> 0.79 <C> 0.79 <C> 0.728 <C> 0.728 <C> 0.728 <C> 72.8 <C> 69.5–75.9 <R> <C> [EMPTY] <C> Dom, Gen <C> 0.77 <C> 0.77 <C> 0.702 <C> 0.702 <C> 0.702 <C> 70.2 <C> 66.9–73.3 <R> <C> [EMPTY] <C> Gen, Gen <C> 0.75 <C> 0.76 <C> 0.689 <C> 0.689 <C> 0.689 <C> 68.9 <C> 65.6–72.1 <CAP> Table 9: Effect of the matrices on the performance of the algorithms
<R> <C> [EMPTY] <C> Precision <C> Recall <C> F1 <R> <C> Ham <C> 0.95 <C> 1 <C> 0.98 <R> <C> Spam <C> 1 <C> 0.95 <C> 0.97 <R> <C> avg. <C> 0.98 <C> 0.97 <C> 0.97 <CAP> Table 8: MPA sender classification results for SMS spam.
<R> <C> Embeddings <C> OurNepali Raw <C> OurNepali Raw <C> OurNepali Raw <C> OurNepali Lemmatized <C> OurNepali Lemmatized <C> OurNepali Lemmatized <R> <C> Embeddings <C> Train <C> Test <C> Val <C> Train <C> Test <C> Val <R> <C> Random <C> 78.72 <C> 63.66 <C> 64.89 <C> 88.44 <C> 75.11 <C> 77.2 <R> <C> Word2Vec_CBOW <C> 82.33 <C> 74.59 <C> 75.15 <C> 88.05 <C> 81.96 <C> 83.82 <R> <C> Word2Vec_Skip Gram <C> 81.58 <C> 75.93 <C> 75.75 <C> 89.84 <C> 83.47 <C> 85.79 <R> <C> GloVe <C> 87.54 <C> 76.86 <C> 76.7 <C> 92.48 <C> 82.24 <C> 84.16 <R> <C> fastText_Pretrained <C> 81.57 <C> 75.06 <C> 71.96 <C> 85.76 <C> 77.6 <C> 79.78 <R> <C> fastText_CBOW <C> 86.01 <C> 81.4 <C> 80.52 <C> 89.23 <C> 81.58 <C> 83.51 <R> <C> fastText_Skip Gram <C> 88.31 <C> 80.6 <C> 78.85 <C> [BOLD] 94.01 <C> [BOLD] 84.74 <C> [BOLD] 85.11 <CAP> Table 3: Effect of different embedding with Bi-LSTM.
<R> <C> [EMPTY] <C> MayoSRS <C> MiniMaySRS: doctors <C> MiniMaySRS: coders <R> <C> Google News <C> 0.128 <C> 0.145 <C> 0.302 <R> <C> MIMIC w2v <C> [BOLD] 0.398 <C> 0.442 <C> [BOLD] 0.572 <R> <C> MIMIC W2VF: words <C> 0.324 <C> 0.495 <C> 0.489 <R> <C> AWE-CM <C> 0.365 <C> [BOLD] 0.508 <C> 0.514 <CAP> Table 1: Spearman coefficient of correlation with various experts.
<R> <C> Tool <C> included <C> not included <C> timeout <C> memory out <C> other failures <R> <C> SPOT <C> 1803 <C> 10177+53 <C> 1 780 <C> 670 <C> 1 517 <R> <C> ROLL [ITALIC] H <C> 2497(5) <C> 10177+3194 <C> 119 <C> 0 <C> 13 <R> <C> ROLL [ITALIC] B <C> 2501(45) <C> 12436+1054 <C> 0 <C> 0 <C> 9 <R> <C> RABIT <C> 2205 <C> 12436+45 <C> 306 <C> 1 008 <C> 0 <CAP> Table 1: Experiment results on random automata with fixed state space and alphabet.
<R> <C> Ch. <C> Method <C> Comp. time (RTF) <C> SRMR <R> <C> [EMPTY] <C> Orig <C> — <C> [ITALIC] 3.18 <R> <C> 8 <C> STFT 32ms/128ms <C> 2.54 / 2.60 <C> [BOLD] 6.90 / 5.31 <R> <C> 8 <C> STFChT 128ms <C> 4.32 <C> 6.33 <R> <C> 8 <C> STFChT i0.3 96ms <C> 6.59 <C> 6.78 <R> <C> 2 <C> STFT 32ms/128ms <C> 0.70 / 0.77 <C> [BOLD] 6.29 / 4.57 <R> <C> 2 <C> STFChT 128ms <C> 2.51 <C> 5.24 <R> <C> 2 <C> STFChT i0.3 96ms <C> 4.78 <C> 5.85 <R> <C> 1 <C> STFT 32ms/128ms <C> 0.50 / 0.56 <C> [BOLD] 5.80 / 4.21 <R> <C> 1 <C> STFChT 128ms <C> 2.27 <C> 4.85 <R> <C> 1 <C> STFChT i0.3 96ms <C> 4.54 <C> 5.45 <CAP> Table 3: Results for RealData evaluation set.
<R> <C> [EMPTY] <C> PN <C> PP <C> F <C> Model <R> <C> TN <C> 253 <C> 107 <C> 67.55 <C> Unigram <R> <C> TP <C> 112 <C> 228 <C> [EMPTY] <C> [EMPTY] <R> <C> TN <C> 267 <C> 93 <C> 70.46 <C> Unigram + WC <R> <C> TP <C> 103 <C> 237 <C> [EMPTY] <C> + Sentiment + MEQ <CAP> Table 3: Confusion Matrix for baseline and the best performing model. In the table, TN=True Negative, TP=True Positive, PN=Predicted Negative, PP=Predicted Positive and F=F-Score.
<R> <C> [BOLD] Feature sets <C> [BOLD] QWKappa <R> <C> Speciteller <C> 0.5758 <R> <C> Speciteller + Online dialogue <C> 0.6347* <R> <C> All: Speciteller + Online dialogue + Pronoun + NE + Book <C> 0.6360* <R> <C> Speciteller + Semantic + Embeddings <C> [BOLD] 0.6550* <R> <C> Pedagogical <C> 0.5886 <CAP> Table 4: Classification performance of different feature sets. * indicates statistically significant improvement over Speciteller features with p-value < 0.001. Statistical significance was tested using a two-tailed paired t-test. Bold font highlights best results.
<R> <C> [BOLD] Feature <C> [BOLD] Coefficient <R> <C> Number of connectives <C> 1.9168 <R> <C> [ITALIC] Cosine similarity – whole summary <C> 0.9293 <R> <C> MRC imageability <C> 0.8172 <R> <C> [ITALIC] Number of characters <C> 0.6931 <R> <C> MPQ subjectivity <C> -0.5440 <R> <C> Fraction of stopwords <C> -0.4087 <R> <C> MRC familiarity <C> 0.3986 <R> <C> [ITALIC] Number of possessive pronouns <C> 0.2035 <R> <C> [ITALIC] Number of named entities normalized <C> 0.1865 <R> <C> [ITALIC] Number of 3rd person pronouns <C> 0.1755 <R> <C> [ITALIC] Word overlap – whole summary <C> 0.1585 <R> <C> [ITALIC] Number of personal pronouns <C> 0.1476 <CAP> Table 5: Pedagogical feature set and respective logistic regression coefficients. Italic font shows features developed in this study (Section 4.4).
<R> <C> Datasets <C> HKUST #utts <C> HKUST Duration (hours) <C> IMDA #utts <C> IMDA Duration (hours) <R> <C> Labelled data (P) <C> 22,500 <C> 20.2 <C> 15,000 <C> 20.6 <R> <C> External text (T) <C> 158,605 <C> - <C> 1,547,399 <C> - <R> <C> Validation <C> 5,457 <C> 4.88 <C> 16,144 <C> 21.2 <R> <C> Test <C> 5,151 <C> 4.75 <C> 5,589 <C> 7.6 <CAP> Table 1: Data division for the HKUST and NSC corpora.
<R> <C> [BOLD] Model <C> Bags <C> Headsets <C> Boots <C> Keyb/s <C> TVs <C> Vaccums <C> Average <R> <C> ABAE  <C> 41.6 <C> 48.5 <C> 41.0 <C> 41.3 <C> 45.7 <C> 40.6 <C> 43.2 <R> <C> MATE  <C> 48.6 <C> 54.5 <C> 46.4 <C> 45.3 <C> 51.8 <C> 47.7 <C> 49.1 <R> <C> BERT  <C> [BOLD] 61.4 <C> [BOLD] 66.5 <C> 52.0 <C> 57.5 <C> [BOLD] 63.0 <C> 60.4 <C> [BOLD] 60.2 <R> <C> 1-8 AspMem <C> 52.4 <C> 58.1 <C> 54.5 <C> 51.4 <C> 53.9 <C> 54.6 <C> 54.2 <R> <C> w/ extra memory <C> 60.0 <C> 62.0 <C> [BOLD] 55.8 <C> [BOLD] 61.8 <C> 60.0 <C> [BOLD] 61.8 <C> [BOLD] 60.2 <CAP> Table 2: Evaluation of the aspect identification task via multi-class F1 measure. Our method outperforms MATE on all the categories and achieves a 5.1% increase on average. The extra latent aspect embeddings for the General aspects further boost the performance by 6.0%.
<R> <C> [BOLD] Methods <C> [BOLD] R-1 <C> [BOLD] R-2 <R> <C> Lead <C> 35.5 <C> 15.2 <R> <C> LexRank <C> 37.7 <C> 14.1 <R> <C> Opinosis <C> 36.8 <C> 14.3 <R> <C> MATE + MILNET <C> 44.1 <C> 21.8 <R> <C> 1-3 AspMemSum <C> 46.6 <C> 25.7 <R> <C> w/o filtering <C> [BOLD] 48.0 <C> [BOLD] 28.7 <R> <C> w/o Relevance <C> 41.5 <C> 20.5 <R> <C> w/o Sentiment <C> 40.5 <C> 18.2 <R> <C> w/o ILP <C> 46.2 <C> 25.1 <R> <C> 1-3 Inter-annotator Agreement <C> 54.7 <C> 36.6 <CAP> Table 4: Summarization results evaluated by Rouge. The proposed AspMemSum without redundancy filtering achieves the best performance on automatic metrics, and both two perform better than all the baselines.
<R> <C> [BOLD] System <C> [BOLD] Model <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1-score <R> <C> SVM <C> Word2Vec <C> 0.290 <C> 0.143 <C> 0.120 <R> <C> SVM <C> Wang2Vec <C> 0.297 <C> 0.144 <C> 0.121 <R> <C> SVM <C> FastText <C> 0.278 <C> 0.144 <C> 0.120 <R> <C> SVM <C> Glove <C> 0.296 <C> 0.144 <C> [BOLD] 0.124 <R> <C> Random Forest <C> Word2Vec <C> 0.388 <C> 0.197 <C> 0.212 <R> <C> Random Forest <C> Wang2Vec <C> 0.386 <C> 0.197 <C> 0.207 <R> <C> Random Forest <C> FastText <C> 0.404 <C> 0.203 <C> [BOLD] 0.215 <R> <C> Random Forest <C> Glove <C> 0.394 <C> 0.199 <C> 0.210 <R> <C> BLSTM <C> Word2Vec <C> 0.492 <C> 0.454 <C> 0.465 <R> <C> BLSTM <C> Wang2Vec <C> [BOLD] 0.515 <C> [BOLD] 0.460 <C> [BOLD] 0.477 <R> <C> BLSTM <C> FastText <C> 0.417 <C> 0.350 <C> 0.360 <R> <C> BLSTM <C> Glove <C> 0.492 <C> 0.460 <C> 0.470 <CAP> Table 2: Classification results for each classifier and word embeddings model combination
<R> <C> [BOLD] Genre <C> [BOLD] F1-score <R> <C> Gospel <C> 0.89 <R> <C> Funk-carioca <C> 0.70 <R> <C> Sertanejo <C> 0.69 <R> <C> Forró <C> 0.53 <R> <C> Axé <C> 0.49 <R> <C> MPB <C> 0.49 <R> <C> Pagode <C> 0.48 <R> <C> Infantil <C> 0.47 <R> <C> Rock <C> 0.46 <R> <C> Velha-guarda <C> 0.38 <R> <C> Samba <C> 0.35 <R> <C> Bossa-nova <C> 0.31 <R> <C> Pop <C> 0.26 <R> <C> Jovem-guarda <C> 0.19 <R> <C> [ITALIC] Average <C> [ITALIC] 0.481 <CAP> Table 3: Detailed result of BLSTM
<R> <C> Languages <C> Model FA <C> Model DWA <C> Model DWA <R> <C> [EMPTY] <C> [EMPTY] <C> [ITALIC] k=0 <C> [ITALIC] k=3 <R> <C> zh|en <C> 49.4 <C> 48.4 <C> 48.7 <R> <C> en|zh <C> 44.9 <C> 45.3 <C> 45.9 <R> <C> fr|en <C> 17.1 <C> 17.2 <C> 17.0 <R> <C> en|fr <C> 16.6 <C> 16.3 <C> 16.1 <CAP> Table 1: Alignment error rate (AER) comparison, in both directions, between the FastAlign (FA) alignment model and our model (DWA) with k context words (see Equation 1). Lower numbers indicate better performance.
<R> <C> Recall (%) <C> Top-1 <C> Top-3 <C> Top-5 <C> Top-10 <R> <C> Fill (Questions) <C> 68.50 <C> 88.01 <C> 94.66 <C> 98.93 <R> <C> Fill (Contexts) <C> 95.64 <C> 97.45 <C> 98.22 <C> 98.73 <R> <C> Find <C> 41.00 <C> - <C> - <C> - <CAP> Table 8: Performance of the Fill (evaluated on the testing data) and Find (manually evaluated on collected explanations) module.
<R> <C> [BOLD] Type <C> [BOLD] Size <C> [BOLD] DE <C> [BOLD] EN <C> [BOLD] FR <C> [BOLD] TR <R> <C> Phone <C> 105 <C> 9.0 <C> 14.3 <C> 11.7 <C> 7.0 <R> <C> Phone <C> 210 <C> 7.2 <C> 12.2 <C> 8.6 <C> 5.9 <R> <C> Grapheme <C> 105 <C> 8.3 <C> 16.5 <C> 13.3 <C> 7.2 <R> <C> Grapheme <C> 210 <C> 6.4 <C> 13.1 <C> 9.6 <C> 5.6 <CAP> Table 1: CER of monolingual subnets
<R> <C> Method (WER) <C> [ITALIC] P@10 <C> [ITALIC] P@ [ITALIC] N <C> EER <C> AP <C> Spear.  [ITALIC] ρ <R> <C> PerfectASR (0%) <C> [BOLD] 88.8 <C> [BOLD] 64.0 <C> [BOLD] 14.3 <C> 60.1 <C> 31.6 <R> <C> GoogleASR (8.6%) <C> 88.2 <C> 62.8 <C> 15.5 <C> 58.8 <C> 30.8 <R> <C> SimASR (8.6%) <C> 85.5 <C> 61.0 <C> 16.7 <C> 56.0 <C> 27.7 <R> <C> SimASR (20%) <C> 77.5 <C> 55.3 <C> 20.7 <C> 49.1 <C> 26.8 <R> <C> SimASR (50%) <C> 60.5 <C> 40.9 <C> 30.9 <C> 31.4 <C> 19.2 <R> <C> SimASR (80%) <C> 34.6 <C> 22.3 <C> 40.8 <C> 15.0 <C> 09.6 <R> <C> VisionSpeechCNN <C> 58.8 <C> 39.7 <C> 23.9 <C> 39.4 <C> [BOLD] 32.4 <CAP> TABLE V: Semantic speech retrieval performance (%) using a cascaded approach combining ASR with TextParagram. The word error rate of each ASR system is given in parentheses. PerfectASR reproduces the TextParagram results (last row) of Table III. GoogleASR uses an actual high-resource ASR system cascaded with TextParagram. SimASR uses a simulated ASR system with varying error rates cascaded with TextParagram.
<R> <C> [BOLD] TaskID <C> [BOLD] Baseline  [BOLD] ACC <C> [BOLD] Baseline  [BOLD] F-Score <C> [BOLD] Baseline  [BOLD] MRR <C> [BOLD] Baseline  [BOLD] MAP <C> [BOLD] Neural Machine Transliteration  [BOLD] ACC <C> [BOLD] Neural Machine Transliteration  [BOLD] F-Score <C> [BOLD] Neural Machine Transliteration  [BOLD] MRR <C> [BOLD] Neural Machine Transliteration  [BOLD] MAP <R> <C> En-Ch <C> 0.1935 <C> 0.5851 <C> 0.1935 <C> 0.1830 <C> 0.2659 <C> 0.6227 <C> 0.3185 <C> 0.2549 <R> <C> Ch-En <C> 0.0981 <C> 0.6459 <C> 0.0981 <C> 0.0953 <C> 0.0834 <C> 0.6564 <C> 0.1425 <C> 0.0830 <R> <C> En-Th <C> 0.0680 <C> 0.7070 <C> 0.0680 <C> 0.0680 <C> 0.1456 <C> 0.7514 <C> 0.2181 <C> 0.1456 <R> <C> Th-En <C> 0.0914 <C> 0.7397 <C> 0.0914 <C> 0.0914 <C> 0.1286 <C> 0.7624 <C> 0.1966 <C> 0.1286 <R> <C> En-Hi <C> 0.2700 <C> 0.7992 <C> 0.2700 <C> 0.2624 <C> 0.3480 <C> 0.8349 <C> 0.4745 <C> 0.3434 <R> <C> En-Ta <C> 0.2580 <C> 0.8117 <C> 0.2580 <C> 0.2573 <C> 0.3240 <C> 0.8369 <C> 0.4461 <C> 0.3235 <R> <C> En-Ka <C> 0.1960 <C> 0.7833 <C> 0.1960 <C> 0.1955 <C> 0.2860 <C> 0.8224 <C> 0.4019 <C> 0.2856 <R> <C> En-Ba <C> 0.2870 <C> 0.8360 <C> 0.2870 <C> 0.2837 <C> 0.3460 <C> 0.8600 <C> 0.4737 <C> 0.3438 <R> <C> En-He <C> 0.1091 <C> 0.7715 <C> 0.1091 <C> 0.1077 <C> 0.1591 <C> 0.7976 <C> 0.2377 <C> 0.1582 <R> <C> En-Pe <C> 0.4818 <C> 0.9060 <C> 0.4818 <C> 0.4482 <C> 0.5816 <C> 0.9267 <C> 0.7116 <C> 0.5673 <CAP> Table 2: The effectiveness of neural machine transliteration is compared with the robust baseline [Koehn et al.2007] provided by NEWS 2016 shared task on transliteration of named entities.
<R> <C> [EMPTY] <C> [BOLD] Book Snippets  [BOLD] EM <C> [BOLD] Book Snippets  [BOLD] DS <C> [BOLD] Tweets  [BOLD] EM <C> [BOLD] Tweets  [BOLD] DS <R> <C> Overall <C> 7.01 <C> 32.68 <C> 9.09 <C> 39.63 <R> <C> ‘Outside’ cases <C> 6.81 <C> 6.81 <C> 4.71 <C> 4.71 <CAP> Table 8: Comparison of performance of our approach in case of examples with target outside the text (indicated by ‘Outside’ cases), with complete dataset (indicated by ‘Overall’); EM: Exact Match, DS: Dice Score
<R> <C> [BOLD] Machine-generated misinformation  [BOLD] adaptive <C> [BOLD] Machine-generated misinformation QA extension (false vs. true) <C> precision 0.72 <C> recall 0.71 <C> F1 0.71 <C> accuracy 71% <R> <C> [BOLD] adaptive <C> modification ( [ITALIC] m=2) <C> 0.53 <C> 0.52 <C> 0.53 <C> 53% <R> <C> [BOLD] adaptive <C> modification ( [ITALIC] m=6) <C> 0.66 <C> 0.65 <C> 0.65 <C> 65% <R> <C> [BOLD] adaptive <C> modification ( [ITALIC] m=10) <C> 0.73 <C> 0.47 <C> 0.63 <C> 65% <CAP> Table 1: Results: Section 4. We report (macro) F1 score and overall accuracy, as well as precision and recall of the “fake” class. Zero-shot performance (not included) was very low in all cases.
<R> <C> [EMPTY] <C> [BOLD] method <C> [BOLD] AP@100 original <C> [BOLD] AP@100 switched <C> [BOLD] Δ <R> <C> [BOLD] supervised <C> concat, word2vec,  [ITALIC] L1 <C> 0.995 <C> 0.575 <C> -0.42 <R> <C> [BOLD] unsupervised <C> cosWeeds, win2d, ppmi <C> 0.818 <C> 0.882 <C> +0.064 <CAP> Table 5: Average Precision (AP) at k=100 of the best supervised and unsupervised methods for hypernym vs. random-n, on the original BLESS validation set and the validation set with the artificially added switched hypernym pairs.
<R> <C> [EMPTY] <C> BLEU with in-training factorization <C> BLEU without factorization <R> <C> German <C> [BOLD] 26.22 <C> 25.95 <R> <C> Portuguese <C> [BOLD] 22.63 <C> 21.56 <R> <C> Turkish <C> [BOLD] 12.05 <C> 11.95 <CAP> Table 3: Factorized and non-factorized transformer performance by language. The non-factorized model uses an embedding dimension of 512, and the factorized model uses an inner size of 256.
<R> <C> [EMPTY] <C> en-tr <C> en-bn <C> en-hi <C> et-fi <R> <C> Artetxe et al. ( 2017 ) <C> 28.93 <C> 0.87 <C> 2.07 <C> 30.18 <R> <C> Ours (1:1) <C> 38.73 <C> 2.33 <C> 10.47 <C> 33.79 <R> <C> Ours (1:1, rank constr.) <C> [BOLD] 42.40 <C> [BOLD] 11.93 <C> [BOLD] 31.80 <C> [BOLD] 34.78 <CAP> Table 4: Bilingual dictionary induction results for English-{Turkish, Bengali, Hindi} and Estonian-Finnish.
<R> <C> [EMPTY] <C> [BOLD] 1-grams <C> [BOLD] 2-grams <C> [BOLD] 3-grams <C> [BOLD] 1+2-grams <C> [BOLD] 1+2+3-grams <R> <C> [ITALIC] N-gram Representations <C> 0 <C> 43.00 <C> 164.34 <C> 43.00 <C> 207.34 <R> <C> [ITALIC] K-means <C> 14.18 <C> 291.62 <C> 747.90 <C> 302.34 <C> 1203.99 <R> <C> Document Representations <C> 36.45 <C> 173.48 <C> 494.06 <C> 343.29 <C> 949.01 <R> <C> Total <C> 50.63 <C> 508.10 <C> 1406.30 <C> 688.63 <C> 2360.34 <CAP> Table 2: Computation time for building movie review representations with K=300 semantic concepts. Time is reported in seconds.
<R> <C> Method <C> YelpNYC | [ITALIC] G| <C> YelpNYC  [ITALIC] GS <C> YelpNYC  [ITALIC] RCS <C> YelpNYC ND <C> YelpZIP | [ITALIC] G| <C> YelpZIP  [ITALIC] GS <C> YelpZIP  [ITALIC] RCS <C> YelpZIP ND <C> Amazon | [ITALIC] G| <C> Amazon  [ITALIC] GS <C> Amazon  [ITALIC] RCS <C> Amazon ND <C> Playstore | [ITALIC] G| <C> Playstore  [ITALIC] GS <C> Playstore  [ITALIC] RCS <C> Playstore ND <R> <C> GGSpam <C> 1218 <C> 0.574 <C> 0.218 <C> 0.567 <C> 1167 <C> 0.629 <C> 0.219 <C> 0.563 <C> 144 <C> 0.131 <C> 0.250 <C> 0.230 <C> 1213 <C> 0.749 <C> 0.010 <C> 0.464 <R> <C> GSBP <C> 809 <C> 0.562 <C> 0.173 <C> 0.521 <C> 807 <C> 0.478 <C> 0.265 <C> 0.520 <C> 115 <C> 0.416 <C> 0.260 <C> 0.689 <C> 250 <C> 0.744 <C> 0.016 <C> 0.474 <R> <C> GSRank <C> 998 <C> 0.102 <C> 0.313 <C> 0.569 <C> 1223 <C> 0.132 <C> 0.054 <C> [BOLD] 0.706 <C> 2922 <C> 0.293 <C> 0.309 <C> 0.144 <C> 994 <C> 0.577 <C> 0.018 <C> 0.476 <R> <C> DeFrauderR <C> 4399 <C> 0.124 <C> 0.021 <C> — <C> 6815 <C> 0.139 <C> 0.031 <C> — <C> 197 <C> 0.234 <C> 0.290 <C> — <C> 385 <C> 0.372 <C> 0.005 <C> — <R> <C> DeFrauderT <C> 152 <C> 0.237 <C> 0.069 <C> — <C> 3666 <C> 0.648 <C> 0.271 <C> — <C> 807 <C> 0.698 <C> 0.207 <C> — <C> 200 <C> 0.458 <C> 0.007 <C> — <R> <C> DeFrauder <C> 1118 <C> [BOLD] 0.731 <C> [BOLD] 0.348 <C> [BOLD] 0.603 <C> 4574 <C> [BOLD] 0.667 <C> [BOLD] 0.287 <C> 0.602 <C> 713 <C> [BOLD] 0.718 <C> [BOLD] 0.314 <C> [BOLD] 0.768 <C> 940 <C> [BOLD] 0.841 <C> [BOLD] 0.018 <C> [BOLD] 0.789 <CAP> Table 3: Performance of the competing methods: GGSpam Wang et al. (2018b), GSBP Wang et al. (2016), GSRank Mukherjee et al. (2012), DeFrauderR, DeFrauderT, and DeFrauder. Number of groups detected (|G|) are mentioned after removing groups of size less than 2 (cyan regions). Accuracy for the group detection (white regions) and ranking (gray regions) is reported in terms of EMD (the higher, the better), and NDCG@50 (ND) respectively. DeFrauderR and DeFrauderT are used only for group detection. The ranking methods of all baselines are run on the groups detection by DeFrauder.
<R> <C> Hyperparameter <C> Ridge <C> SVR <R> <C> c_ngmax <C> 5 <C> 6 <R> <C> w_ngmax <C> 3 <C> 2 <R> <C> min_df <C> 2 <C> 1 <R> <C> lowercase <C> word <C> word <R> <C> [ITALIC] αtotal <C> 5.0 <C> 5.0 <R> <C> [ITALIC] αanxiety <C> 5.0 <C> 10.0 <R> <C> [ITALIC] αdepression <C> 5.0 <C> 20.0 <R> <C> ctrl_weight <C> 0.5 <C> 0.5 <CAP> Table 1: Best hyperparameter values for ridge regression (Ridge) and support vector regression (SVR) models for Task A. The values are obtained through a random search from approximately 400 random parameter settings.
<R> <C> Hyperparameter <C> Ridge <C> SVR <R> <C> c_ngmax <C> 4 <C> 7 <R> <C> w_ngmax <C> 5 <C> 5 <R> <C> min_df <C> 1 <C> 1 <R> <C> lowercase <C> word <C> word <R> <C> [ITALIC] αtotal <C> 3.0 <C> 8.0 <R> <C> [ITALIC] αanxiety <C> 8.0 <C> 20.0 <R> <C> [ITALIC] αdepression <C> 10.0 <C> 20.0 <R> <C> ctrl_weight <C> 1.0 <C> 0.1 <R> <C> a11_weight <C> 0.5 <C> 0.1 <CAP> Table 2: Best hyperparameter values for ridge regression (Ridge) and support vector regression (SVR) models for Task B. The values are obtained through a random search from approximately 400 random parameter settings. a11_weight is based on predicted age-11 outcomes.
<R> <C> Model <C> Precision <C> Recall <C> F1 <R> <C> DAN <C> 0.66 <C> 0.49 <C> 0.56 <R> <C> attribute+sym. difference <C> 0.64 <C> 0.48 <C> 0.55 <R> <C> no attribute layer <C> 0.63 <C> 0.33 <C> 0.43 <R> <C> Random baseline <C> 0.16 <C> 0.16 <C> 0.16 <CAP> Table 2: Predicting discriminative features
<R> <C> CEFR <C> Document level Books <C> Document level Publ. <C> Document level Texts <C> Document level Mean nr. sent <C> Sentence level Books <C> Sentence level Sentences <R> <C> A1 <C> 4 <C> 3 <C> 49 <C> 14.0 <C> 4 <C> 505 <R> <C> A2 <C> 4 <C> 3 <C> 157 <C> 13.8 <C> 4 <C> 754 <R> <C> B1 <C> 5 <C> 3 <C> 258 <C> 17.9 <C> 4 <C> 408 <R> <C> B2 <C> 4 <C> 3 <C> 288 <C> 26.6 <C> 3 <C> 124 <R> <C> C1 <C> 2 <C> 2 <C> 115 <C> 42.1 <C> 1 <C> 83 <R> <C> Total <C> 12 <C> 4 <C> 867 <C> - <C> 4 <C> 1874 <CAP> Table 1: The distribution of items per CEFR level in the datasets.
<R> <C> Type <C> Nr <C> Acc (%) <C> F <C> RMSE <R> <C> Majority <C> - <C> 33.2 <C> 0.17 <C> 0.52 <R> <C> LIX <C> 1 <C> 34.9 <C> 0.22 <C> 0.38 <R> <C> Lex <C> 11 <C> [BOLD] 80.3 <C> [BOLD] 0.80 <C> 0.24 <R> <C> All <C> 61 <C> [BOLD] 81.3 <C> [BOLD] 0.81 <C> 0.27 <CAP> Table 3: Document-level classification results.
<R> <C> Target training e.g. Intent ↓ Model → <C> 0 CT <C> 0 ZAT <C> 0 +2Ex <C> 50 LSTM <C> 50 CT <C> 50 ZAT <C> 50 +10Ex <R> <C> AddToPlaylist <C> 53.3 <C> 46.8 <C> [BOLD] 55.2 <C> 59.4 <C> 74.4 <C> 73.4 <C> [BOLD] 76.2* <R> <C> BookRestaurant <C> 45.7 <C> 46.6 <C> [BOLD] 48.6* <C> 57.5 <C> [BOLD] 63.8 <C> 63.5 <C> 63.6 <R> <C> GetWeather <C> 63.5 <C> 60.7 <C> [BOLD] 66.0* <C> 75.7 <C> 72.1 <C> 71.1 <C> [BOLD] 77.5* <R> <C> PlayMusic <C> 28.7 <C> 30.1 <C> [BOLD] 33.8* <C> 49.3 <C> 56.4 <C> 56.0 <C> [BOLD] 58.8 <R> <C> RateBook <C> 24.5 <C> [BOLD] 31.0 <C> 28.5 <C> [BOLD] 85.1* <C> 82.9 <C> 83.8 <C> 82.2 <R> <C> SearchCreativeWork <C> 24.7 <C> [BOLD] 26.7 <C> 26.2 <C> 52.9 <C> 62.8 <C> 63.7 <C> [BOLD] 65.9 <R> <C> FindScreeningEvent <C> 23.7 <C> 19.7 <C> [BOLD] 25.5* <C> 60.8 <C> 64.9 <C> 64.6 <C> [BOLD] 67.0* <R> <C> Average <C> 37.7 <C> 37.4 <C> [BOLD] 40.6* <C> 62.8 <C> 68.2 <C> 68.0 <C> [BOLD] 70.1* <CAP> Table 2: Slot F1 scores for baselines (CT, ZAT, LSTM) and our best models (with 2 slot values for zero-shot and 10 values for 50 train instances) on SNIPS. Rows represent different train-test splits, defined in Section 5. Our model consistently outperforms the baselines, with ∼3% absolute gain in the zero-shot setting.444Asterisk (*) indicates a statistically significant gain over the second-best model as per McNemar’s test (p<0.05).
<R> <C> Models <C> Yelp <C> IMDB <C> CZ Movies <C> Debates <C> [ITALIC] θ <R> <C> Feature-based classifiers <C> 59.8 <C> 40.9 <C> 78.5 <C> 74.0 <C> — <R> <C> Paragraph vector (Tang et al.,  2015a ) <C> 57.7 <C> 34.1 <C> — <C> —- <C> — <R> <C> Convolutional neural network (Tang et al.,  2015a ) <C> 59.7 <C> — <C> — <C> — <C> — <R> <C> Convolutional gated RNN (Tang et al.,  2015a ) <C> 63.7 <C> 42.5 <C> — <C> — <C> — <R> <C> LSTM gated RNN (Tang et al.,  2015a ) <C> 65.1 <C> 45.3 <C> — <C> — <C> — <R> <C> RST-based recursive neural network (Ji and Smith,  2017 ) <C> — <C> — <C> — <C> 75.7 <C> — <R> <C> 75D Hierarchical attention networks (Yang et al.,  2016 ) <C> 68.2 <C> [BOLD] 49.4 <C> 80.8 <C> 74.0 <C> 273K <R> <C> 75D No Attention <C> 66.7 <C> 47.5 <C> 80.5 <C> 73.7 <C> 330K <R> <C> 100D Simple Attention <C> 67.7 <C> 48.2 <C> 81.4 <C> 75.3 <C> 860K <R> <C> 100D Structured Attention (sentence-level) <C> 68.0 <C> 48.8 <C> 81.5 <C> 74.6 <C> 842K <R> <C> 100D Structured Attention (document-level) <C> 67.8 <C> 48.6 <C> 81.1 <C> 75.2 <C> 842K <R> <C> 100D Structured Attention (both levels) <C> [BOLD] 68.6 <C> 49.2 <C> [BOLD] 82.1 <C> [BOLD] 76.5 <C> 860K <CAP> Table 4: Test accuracy on four datasets and number of parameters θ (excluding embeddings). Regarding feature-based classification methods, results on Yelp and IMDB are taken from Tang et al. (2015a), on CZ movies from Brychcın and Habernal (2013), and Debates from Yogatama and Smith (2014). Wherever available we also provide the size of the recurrent unit (LSTM or GRU).
<R> <C> [BOLD] Model Variant <C> [BOLD] Valid Acc. <C> [BOLD] Test Acc. <R> <C> TBCNN+∘ <C> 73.8 <C> 72.5 <R> <C> TBCNN+- <C> 79.9 <C> 79.3 <R> <C> TBCNN+cat <C> 80.8 <C> 79.3 <R> <C> TBCNN+cat,∘ <C> 81.6 <C> 80.7 <R> <C> TBCNN+cat,- <C> 81.7 <C> 81.6 <R> <C> TBCNN+cat,∘,- <C> [BOLD] 82.4 <C> [BOLD] 82.1 <CAP> Table 4: Validation and test accuracies of TBCNN-pair variants (in percentage).
<R> <C> [BOLD] Examples <C> [BOLD] Training Mode <C> [BOLD] Yelp <C> [BOLD] AG’s News <C> [BOLD] Yahoo <C> [BOLD] MNLI (m) <R> <C> |T|=0 <C> unsupervised (avg) <C> 33.8±9.6 <C> 69.5±7.2 <C> 44.0±9.1 <C> 39.1±4.3 <R> <C> |T|=0 <C> unsupervised (max) <C> 40.8±0.0 <C> 79.4±0.0 <C> 56.4±0.0 <C> 43.8±0.0 <R> <C> |T|=0 <C> iPet <C> [BOLD] 56.7± [BOLD] 0.2 <C> [BOLD] 87.5± [BOLD] 0.1 <C> [BOLD] 70.7± [BOLD] 0.1 <C> [BOLD] 53.6± [BOLD] 0.1 <R> <C> |T|=10 <C> supervised <C> 21.1±1.6 <C> 25.0±0.1 <C> 10.1±0.1 <C> 34.2±2.1 <R> <C> |T|=10 <C> Pet <C> 52.9±0.1 <C> 87.5±0.0 <C> 63.8±0.2 <C> 41.8±0.1 <R> <C> |T|=10 <C> iPet <C> [BOLD] 57.6± [BOLD] 0.0 <C> [BOLD] 89.3± [BOLD] 0.1 <C> [BOLD] 70.7± [BOLD] 0.1 <C> [BOLD] 43.2± [BOLD] 0.0 <R> <C> |T|=50 <C> supervised <C> 44.8±2.7 <C> 82.1±2.5 <C> 52.5±3.1 <C> 45.6±1.8 <R> <C> |T|=50 <C> Pet <C> 60.0±0.1 <C> 86.3±0.0 <C> 66.2±0.1 <C> 63.9±0.0 <R> <C> |T|=50 <C> iPet <C> [BOLD] 60.7± [BOLD] 0.1 <C> [BOLD] 88.4± [BOLD] 0.1 <C> [BOLD] 69.7± [BOLD] 0.0 <C> [BOLD] 67.4± [BOLD] 0.3 <R> <C> |T|=100 <C> supervised <C> 53.0±3.1 <C> 86.0±0.7 <C> 62.9±0.9 <C> 47.9±2.8 <R> <C> |T|=100 <C> Pet <C> 61.9±0.0 <C> 88.3±0.1 <C> 69.2±0.0 <C> 74.7±0.3 <R> <C> |T|=100 <C> iPet <C> [BOLD] 62.9± [BOLD] 0.0 <C> [BOLD] 89.6± [BOLD] 0.1 <C> [BOLD] 71.2± [BOLD] 0.1 <C> [BOLD] 78.4± [BOLD] 0.7 <R> <C> |T|=1000 <C> supervised <C> 63.0±0.5 <C> [BOLD] 86.9± [BOLD] 0.4 <C> 70.5±0.3 <C> 73.1±0.2 <R> <C> |T|=1000 <C> Pet <C> [BOLD] 64.8± [BOLD] 0.1 <C> [BOLD] 86.9± [BOLD] 0.2 <C> [BOLD] 72.7± [BOLD] 0.0 <C> [BOLD] 85.3± [BOLD] 0.2 <CAP> Table 1: Results for RoBERTa (large) on Yelp, AG’s News, Yahoo and MNLI (matched) for various training set sizes. Scores for Pet were obtained using the weighted variant with manually defined verbalizers.
<R> <C> [BOLD] Examples <C> [BOLD] Mode <C> [BOLD] De <C> [BOLD] Fr <C> [BOLD] It <R> <C> |T|=1000 <C> supervised <C> 43.3 <C> 49.5 <C> 41.0 <R> <C> |T|=1000 <C> Pet <C> [BOLD] 66.4 <C> [BOLD] 68.7 <C> [BOLD] 64.7 <R> <C> |T|=2000 <C> supervised <C> 57.4 <C> 62.1 <C> 52.8 <R> <C> |T|=2000 <C> Pet <C> [BOLD] 69.5 <C> [BOLD] 71.7 <C> [BOLD] 67.3 <R> <C> |T|=4000 <C> supervised <C> 63.2 <C> 66.7 <C> 58.7 <R> <C> |T|=4000 <C> Pet <C> [BOLD] 71.7 <C> [BOLD] 74.0 <C> [BOLD] 69.5 <R> <C> TDe , TFr <C> supervised <C> 76.6 <C> 76.0 <C> 71.0 <R> <C> TDe , TFr <C> Pet <C> [BOLD] 77.9 <C> [BOLD] 79.0 <C> [BOLD] 73.6 <R> <C> TDe+TFr <C> sup. (*) <C> 76.8 <C> 76.7 <C> 70.2 <R> <C> TDe+TFr <C> supervised <C> 77.6 <C> 79.1 <C> 75.9 <R> <C> TDe+TFr <C> Pet <C> [BOLD] 78.8 <C> [BOLD] 80.6 <C> [BOLD] 77.2 <CAP> Table 2: Results on x-stance for XLM-R (base) trained on subsets of TDe and TFr and for jointly training on all available data (TDe+TFr). (*): Best results reported in Vamvas and Sennrich (2020).
<R> <C> [EMPTY] <C> [BOLD] Yelp <C> [BOLD] AG’s <C> [BOLD] Yahoo <C> [BOLD] MNLI <R> <C> min <C> 39.6 <C> 82.1 <C> 50.2 <C> 36.4 <R> <C> max <C> 52.4 <C> 85.0 <C> 63.6 <C> 40.2 <R> <C> Pet uniform <C> 52.7 <C> 87.3 <C> [BOLD] 63.8 <C> [BOLD] 42.0 <R> <C> Pet weighted <C> [BOLD] 52.9 <C> [BOLD] 87.5 <C> [BOLD] 63.8 <C> 41.8 <CAP> Table 3: Minimum (min) and maximum (max) accuracy of models based on individual patterns as well as Pet after training on 10 examples
<R> <C> Model <C> Accuracy utt = 1 <C> Accuracy utt = 3 <C> Accuracy utt = 5 <C> Accuracy utt = 10 <R> <C> EOS <C> 92.19 <C> 92.02 <C> 91.88 <C> 91.82 <R> <C> Multi-task <C> 92.39 <C> 92.03 <C> 91.86 <C> 91.85 <R> <C> Multi-task FB <C> 92.42 <C> 91.95 <C> 91.90 <C> 91.78 <CAP> Table 1: End-Of-Sentence Detection Accuracy
<R> <C> [BOLD] Model <C> [BOLD] LT  [BOLD] B-ref↑ <C> [BOLD] LT  [BOLD] B-ori <C> [BOLD] LT  [BOLD] PPL↓ <C> [BOLD] LT  [BOLD] ACC↑ <C> [BOLD] LT  [BOLD] Human↑ <C> [BOLD] GSD  [BOLD] B-ref↑ <C> [BOLD] GSD  [BOLD] B-ori <C> [BOLD] GSD  [BOLD] PPL↓ <C> [BOLD] GSD  [BOLD] ACC↑ <C> [BOLD] GSD  [BOLD] Human↑ <R> <C> Template <C> [BOLD] 41.6 <C> 81.48 <C> [BOLD] 5.4 <C> 0.31 <C> [BOLD] 4.3 /  [BOLD] 4.2 <C> [BOLD] 81.7 <C> 88.8 <C> [BOLD] 5.3 <C> 0.42 <C> 4.2 /  [BOLD] 4.2 <R> <C> CrossAlign <C> 2.2 <C> 2.1 <C> 1895.6 <C> 0.45 <C> 1.2 / 1.1 <C> 2.7 <C> 2.2 <C> 1049.7 <C> 0.36 <C> 1.0 / 1.0 <R> <C> DeleteRetrieve <C> [BOLD] 35.9 <C> 41.6 <C> 63.3 <C> 0.33 <C> 1.0 / 1.0 <C> 20.5 <C> 21.4 <C> 28.8 <C> 0.41 <C> 2.1 / 1.3 <R> <C> DualRL <C> 4.1 <C> 3.9 <C> 1400.7 <C> 0.49 <C> 1.2 / 1.2 <C> 25.4 <C> 27.5 <C> 171.0 <C> 0.41 <C> 2.9 / 1.5 <R> <C> VAE <C> 13.5 <C> 16.3 <C> 8.5 <C> 0.49 <C> 3.5 / 1.7 <C> 12.4 <C> 26.4 <C> 21.5 <C> 0.45 <C> [BOLD] 4.3 / 2.1 <R> <C> ST2-CA (ours) <C> 6.3 <C> 6.8 <C> 54.8 <C> [BOLD] 0.65 <C> 3.1 /  [BOLD] 2.3 <C> [BOLD] 66.7 <C> 73.2 <C> 21.4 <C> 0.42 <C> 3.6 /  [BOLD] 3.8 <R> <C> ST2-VAE (ours) <C> 20.5 <C> 15.1 <C> [BOLD] 8.2 <C> 0.62 <C> [BOLD] 3.8 / 1.9 <C> 14.7 <C> 13.9 <C> [BOLD] 10.9 <C> [BOLD] 0.71 <C> [BOLD] 4.3 / 2.7 <CAP> Table 3: Results for multi-task style transfer. The larger↑/lower↓, the better. B-ref and B-ori means BLEU score and self-BLEU score, respectively. The human evaluation scores include language fluency/content preservation, respectively. Our base models are underlined.
<R> <C> Model <C> P [ITALIC] ma <C> R [ITALIC] ma <C> F [ITALIC] ma <C> F [ITALIC] mi <R> <C> Random Baseline <C> 0.196 <C> 0.198 <C> 0.189 <C> 0.196 <R> <C> Random Forest Baseline <C> 0.496 <C> 0.335 <C> 0.267 <C> 0.279 <R> <C> LSTM Baseline <C> 0.512 <C> 0.510 <C> 0.503 <C> 0.521 <R> <C> Multi-Task <C> 0.526 <C> 0.525 <C> 0.505 <C> 0.534 <R> <C> Adversarial <C> [BOLD] 0.533 <C> [BOLD] 0.534 <C> [BOLD] 0.515 <C> [BOLD] 0.548 <CAP> Table 5: Macro- (ma) and micro-averaged (mi) scores for the online discussion test data averaged over 3 runs. The multi-task model uses the Twitter and argument quality datasets as auxiliary tasks. The micro-average F of a baseline that predicts the majority class is 0.307.
<R> <C> [BOLD] Model <C> [BOLD] Number of attractors 0 <C> [BOLD] Number of attractors 1 <C> [BOLD] Number of attractors 2 <C> [BOLD] Number of attractors 3 <C> [BOLD] Number of attractors 4 <C> [BOLD] Number of attractors 5 <C> Total <R> <C> [BOLD] Previous best results (Yogatama et al.,  2018 ): <C> [BOLD] Previous best results (Yogatama et al.,  2018 ): <C> [BOLD] Previous best results (Yogatama et al.,  2018 ): <C> [BOLD] Previous best results (Yogatama et al.,  2018 ): <C> [BOLD] Previous best results (Yogatama et al.,  2018 ): <C> [BOLD] Previous best results (Yogatama et al.,  2018 ): <C> [BOLD] Previous best results (Yogatama et al.,  2018 ): <C> [BOLD] Previous best results (Yogatama et al.,  2018 ): <R> <C> Best Stack-RNN <C> [ITALIC] 0.994 <C> 0.979 <C> 0.965 <C> 0.935 <C> 0.916 <C> 0.880 <C> 0.992 <R> <C> Best LSTM <C> 0.993 <C> 0.972 <C> 0.950 <C> 0.922 <C> 0.900 <C> 0.842 <C> 0.991 <R> <C> Best Attention <C> [BOLD] 0.994 <C> [BOLD] 0.977 <C> 0.959 <C> 0.929 <C> 0.907 <C> 0.842 <C> [BOLD] 0.992 <R> <C> [BOLD] Our results: <C> [BOLD] Our results: <C> [BOLD] Our results: <C> [BOLD] Our results: <C> [BOLD] Our results: <C> [BOLD] Our results: <C> [BOLD] Our results: <C> [BOLD] Our results: <R> <C> Transformer <C> 0.973 <C> 0.941 <C> 0.932 <C> 0.917 <C> 0.901 <C> 0.883 <C> 0.962 <R> <C> Universal Transformer <C> 0.993 <C> 0.971 <C> [BOLD] 0.969 <C> 0.940 <C> 0.921 <C> 0.892 <C> [BOLD] 0.992 <R> <C> UT w/ ACT <C> [BOLD] 0.994 <C> 0.969 <C> 0.967 <C> [BOLD] 0.944 <C> [BOLD] 0.932 <C> [BOLD] 0.907 <C> [BOLD] 0.992 <R> <C> Δ (UT w/ ACT - Best) <C> 0 <C> -0.008 <C> 0.002 <C> 0.009 <C> 0.016 <C> 0.027 <C> - <CAP> Table 2: Accuracy on the subject-verb agreement number prediction task (higher is better).
<R> <C> [BOLD] Model <C> [BOLD] Copy char-acc <C> [BOLD] Copy seq-acc <C> [BOLD] Reverse char-acc <C> [BOLD] Reverse seq-acc <C> [BOLD] Addition char-acc <C> [BOLD] Addition seq-acc <R> <C> LSTM <C> 0.45 <C> 0.09 <C> 0.66 <C> 0.11 <C> 0.08 <C> 0.0 <R> <C> Transformer <C> 0.53 <C> 0.03 <C> 0.13 <C> 0.06 <C> 0.07 <C> 0.0 <R> <C> Universal Transformer <C> 0.91 <C> 0.35 <C> 0.96 <C> 0.46 <C> 0.34 <C> 0.02 <R> <C> Neural GPU∗ <C> [BOLD] 1.0 <C> [BOLD] 1.0 <C> [BOLD] 1.0 <C> [BOLD] 1.0 <C> [BOLD] 1.0 <C> [BOLD] 1.0 <CAP> Table 4: Accuracy (higher better) on the algorithmic tasks. ∗Note that the Neural GPU was trained with a special curriculum to obtain the perfect result, while other models are trained without any curriculum.
<R> <C> [EMPTY] <C> Bleu  [BOLD] scores 1st <C> Bleu  [BOLD] scores 2nd <C> Bleu  [BOLD] scores 3rd <C> Bleu  [BOLD] scores 4th <C> Bleu  [BOLD] scores 5th <R> <C> [BOLD] Train <C> 0.387 <C> 0.355 <C> 0.330 <C> 0.329 <C> 0.302 <R> <C> [BOLD] Test <C> 0.338 <C> 0.283 <C> 0.295 <C> 0.277 <C> 0.250 <CAP> Table 2: Corpus-level Bleu scores for the motion-to-language model.
<R> <C> [BOLD] System <C> [BOLD] F1 (weighted)  [BOLD] FB <C> [BOLD] F1 (weighted)  [BOLD] SM <R> <C> Random Baseline <C> 0.3535 <C> 0.3477 <R> <C> System 1 <C> 0.5673 <C> 0.5453 <R> <C> System 2 <C> 0.5847 <C> 0.5391 <R> <C> System 3 <C> [BOLD] 0.5921 <C> [BOLD] 0.5663 <CAP> Table 3: Results for the English test set. FB: Facebook and SM: Social Media.
<R> <C> Module <C> Depth <C> Classes <C> Types <C> Attributes <C> References <R> <C> ietf-interfaces - PYANG <C> 5 <C> 5 <C> 2 <C> 29 <C> 21 <R> <C> ietf-interfaces - YANG2UML <C> 1 <C> 3 <C> 2 <C> 33 <C> 25 <R> <C> ietf-netconf-monitoring - PYANG <C> 5 <C> 30 <C> 1 <C> 29 <C> 21 <R> <C> ietf-netconf-monitoring - YANG2UML <C> 1 <C> 22 <C> 3 <C> 51 <C> 36 <R> <C> ietf-routing - PYANG <C> 5 <C> 63 <C> 0 <C> 64 <C> 48 <R> <C> ietf-routing - YANG2UML <C> 1 <C> 25 <C> 16 <C> 90 <C> 74 <CAP> TABLE II: Quantitative evaluation of PYANG and YANG2UML
<R> <C> Model <C> PE attention <C> Data <C> dev TER <C> test 2016 TER <C> test 2017 TER <C> test 2017 BLEU <C> Steps <R> <C> Baseline <C> Baseline <C> none <C> 24.81 <C> 24.76 <C> 24.48 <C> 62.49 <C> [EMPTY] <R> <C> SPE <C> SPE <C> 12k <C> [EMPTY] <C> 24.64 <C> 24.69 <C> 62.97 <C> [EMPTY] <R> <C> Best 2016 (AMU) <C> Best 2016 (AMU) <C> 4M + 500k + 12k <C> 21.46 <C> 21.52 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Best 2017 (FBK) <C> Best 2017 (FBK) <C> 23k + ? <C> [EMPTY] <C> [EMPTY] <C> [BOLD] 19.60 <C> 70.07 <C> [EMPTY] <R> <C> Mono-source <C> global <C> 12k <C> 24.15 <C> 24.26 <C> [EMPTY] <C> [EMPTY] <C> 29000 <R> <C> Mono-source <C> forced (contr. 1) <C> 12k <C> 23.20 <C> 23.32 <C> 23.51 <C> 64.52 <C> 16600 <R> <C> Chained <C> forced (contr. 2) <C> 12k <C> 23.40 <C> 23.30 <C> 23.66 <C> 64.46 <C> 23600 <R> <C> Chained <C> forced (primary) <C> 500k + 12k <C> 22.77 <C> [BOLD] 22.94 <C> [BOLD] 23.22 <C> 65.12 <C> 119200 <R> <C> Mono-source <C> global <C> 23k <C> 23.60 <C> 23.55 <C> [EMPTY] <C> [EMPTY] <C> 47200 <R> <C> Mono-source <C> forced (contr. 1) <C> 23k <C> 23.07 <C> 22.89 <C> 23.08 <C> 65.57 <C> 38800 <R> <C> Chained <C> forced (contr. 2) <C> 23k <C> 22.61 <C> 22.76 <C> 23.15 <C> 64.94 <C> 50400 <R> <C> Chained <C> forced (primary) <C> 500k + 23k <C> 22.03 <C> [BOLD] 22.49 <C> [BOLD] 22.81 <C> 65.91 <C> 121200 <CAP> Table 3: Results on the en-de task. The SPE results are those provided by the organizers of the task (SMT system). The AMU system is the winner of the 2016 APE task (Junczys-Dowmunt and Grundkiewicz, 2016). FBK is the winner of this year’s edition. We evaluate our models on dev every 200 training steps, and take the model with the lowest TER. The steps column gives the corresponding training time (SGD updates). 500k + 12k is a concatenation of the 500k synthetic corpus with the 12k corpus oversampled 20 times. 500k + 23k is a concatenation of 500k with 23k oversampled 10 times.
<R> <C> Model <C> PE attention <C> Data <C> train-dev TER <C> dev TER <C> test 2017 TER <C> test 2017 BLEU <C> Steps <R> <C> Baseline <C> Baseline <C> none <C> 16.11 <C> 15.58 <C> 15.55 <C> 79.54 <C> [EMPTY] <R> <C> SPE <C> SPE <C> 24k <C> [EMPTY] <C> [EMPTY] <C> 15.74 <C> 79.28 <C> [EMPTY] <R> <C> Best 2017 (FBK) <C> Best 2017 (FBK) <C> 24k + ? <C> [EMPTY] <C> [EMPTY] <C> [BOLD] 15.29 <C> 79.82 <C> [EMPTY] <R> <C> Mono-source <C> global <C> 24k <C> 16.06 <C> 15.55 <C> [EMPTY] <C> [EMPTY] <C> 5200 <R> <C> Mono-source <C> forced (contr. 1) <C> 24k <C> 16.05 <C> 15.57 <C> 15.62 <C> 79.48 <C> 3400 <R> <C> Chained <C> forced (contr. 2) <C> 24k <C> 16.02 <C> 15.63 <C> 15.68 <C> 79.35 <C> 7000 <R> <C> Chained <C> forced (primary) <C> 500k + 24k <C> 15.98 <C> 15.67 <C> 15.53 <C> 79.46 <C> 27200 <CAP> Table 4: Results on the de-en task. Because the test set was not available before submission, we used a small part (1000 tuples) of the training set as a train-dev set. This set was used for selecting the best models, while the provided dev set was used for final evaluation of our models. The 500k + 24k corpus is a concatenation of our synthetic corpus with the 24k corpus oversampled 10 times.
<R> <C> Vectors <C> [ITALIC] ρ∗ <C> |H| <C> [ITALIC] σ <C> [ITALIC] λ1 <C> [ITALIC] λ2 <R> <C> GloVe <C> 0.15 <C> 1000 <C> 0.4 <C> 1 <C> 1 <R> <C> word2vec <C> 0.15 <C> 1000 <C> 0.2 <C> 1 <C> 1 <CAP> Table 3: Grid-search was performed to select values of the following hyperparamters: Sparsity fraction (ρ∗), hidden-dimension size (|H|), standard deviation of the additive isotropic zero-mean Gaussian noise (σ), and the coefficients for the ASL and PSL loss terms (λ1 and λ2).
<R> <C> Task <C> GloVe (original) <C> SPOWV (w/ GloVe) <C> SPINE (w/ GloVe) <C> Word2vec (original) <C> SPOWV (w/ word2vec) <C> SPINE (w/ word2vec) <R> <C> Sentiment Analysis (Accuracy) <C> 71.37 <C> 71.83 <C> [BOLD] 72.44 <C> 73.50 <C> [BOLD] 74.01 <C> 72.71 <R> <C> Question Clf. (Accuracy) <C> 82.80 <C> [BOLD] 89.20 <C> 88.20 <C> 88.40 <C> 91.80 <C> [BOLD] 92.40 <R> <C> Sports News Clf. (Accuracy) <C> 95.47 <C> 95.6 <C> [BOLD] 96.23 <C> 92.83 <C> [BOLD] 95.6 <C> 93.96 <R> <C> Religion News Clf. (Accuracy) <C> 79.35 <C> 81.72 <C> [BOLD] 83.4 <C> 83.12 <C> [BOLD] 84.79 <C> 82.56 <R> <C> Computers News Clf. (Accuracy) <C> 71.68 <C> [BOLD] 77.86 <C> 77.47 <C> 72.71 <C> [BOLD] 81.46 <C> 74.38 <R> <C> NP Bracketing (Accuracy) <C> 73.11 <C> 70.28 <C> [BOLD] 74.85 <C> [BOLD] 78.13 <C> 72.19 <C> 75.41 <R> <C> Word Similarity ( [ITALIC] ρ in %) <C> [BOLD] 66.82 <C> 66.77 <C> 64.77 <C> [BOLD] 68.42 <C> 63.73 <C> 62.79 <CAP> Table 6: Effectiveness comparison of the generated word embeddings (all accuracies are in %). We compare the embeddings generated by our SPINE model against the initial GloVe and word2vec vectors, and Sparse Overcomplete Word Vectors (SPOWV)  [Faruqui et al.2015] on a suite of benchmark downstream tasks.
<R> <C> [BOLD] Model <C> [BOLD] Wt. Acc. (%) <C> [BOLD] F1 (%) <C> [BOLD] Acc. (%) <C> [BOLD] Precision (%) <C> [BOLD] Recall (%) <R> <C> [BOLD] Random <C> 50.0 <C> - <C> 68.2 <C> - <C> - <R> <C> [BOLD] Keywords <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Random Forest <C> 67.0 <C> 55.2 <C> 78.1 <C> 78.2 <C> 42.6 <R> <C> Logistic Regression <C> 69.9 <C> 57.8 <C> 78.4 <C> 75.5 <C> 46.8 <R> <C> Linear SVM <C> 69.5 <C> 57.0 <C> 78.6 <C> 78.0 <C> 44.9 <R> <C> [BOLD] Average Trafficking Vectors <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Random Forest <C> 67.3 <C> 54.1 <C> 78.0 <C> 79.3 <C> 41.1 <R> <C> Logistic Regression <C> 72.2 <C> 61.7 <C> 80.2 <C> 79.2 <C> 50.6 <R> <C> Linear SVM <C> 70.3 <C> 57.7 <C> 79.2 <C> 80.7 <C> 44.9 <R> <C> [BOLD] 108 One-Hot <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Random Forest <C> 62.4 <C> 60.7 <C> 72.6 <C> 61.5 <C> 60.0 <R> <C> Logistic Regression <C> 62.5 <C> 45.1 <C> 72.2 <C> 60.0 <C> 36.1 <R> <C> Linear SVM <C> 61.7 <C> 45.1 <C> 71.8 <C> 58.6 <C> 36.7 <R> <C> [BOLD] Bag of Words <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Random Forest <C> 57.6 <C> 24.5 <C> 70.4 <C> 63.2 <C> 15.2 <R> <C> Logistic Regression <C> 71.1 <C> 24.5 <C> 70.4 <C> 63.2 <C> 15.2 <R> <C> Linear SVM <C> 71.2 <C> 24.5 <C> 70.4 <C> 63.2 <C> 15.2 <R> <C> [BOLD] HTDN Unimodal <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> F [ITALIC] l <C> 74.5 <C> 65.8 <C> 78.8 <C> 69.8 <C> 62.3 <R> <C> F [ITALIC] v [VGG] <C> 69.1 <C> 58.4 <C> 74.2 <C> 66.7 <C> 52.0 <R> <C> F [ITALIC] v [T-VGG] <C> 70.4 <C> 59.5 <C> 77.3 <C> 78.3 <C> 48.0 <R> <C> [BOLD] HTDN <C> [BOLD] 75.3 <C> [BOLD] 66.5 <C> 80.0 <C> 71.4 <C> 62.2 <R> <C> [BOLD] Human <C> 83.7 <C> 73.7 <C> 84.0 <C> 76.7 <C> 70.9 <CAP> Table 1: Results of our experiments. We compare our HTDN model to various baselines using different inputs. HTDN ourperforms other baselines in both weighted accuracy and F-score.
<R> <C> Google Analogy - micro <C> 3cosadd 0.74 <C> [BOLD] uncon. 0.21 <C> 3cosmul 0.75 <C> [BOLD] uncon. 0.47 <C> bolukbasi 0.04 <C> [BOLD] uncon. 0.11 <R> <C> Google Analogy - macro <C> 0.71 <C> 0.21 <C> 0.73 <C> 0.45 <C> 0.06 <C> 0.11 <CAP> Table 1: Accuracies of the three formulas on the Google Analogy test set (Mikolov et al., 2013), comparing the constrained version (original code), with the unconstrained version (uncon.). For all formulas, unconstrained means also taking the input vectors into account. For bolukbasi, more constraints were removed (see Section 4.3).
<R> <C> [BOLD] Model <C> [BOLD] Inspec <C> [BOLD] Krapivin <C> [BOLD] SemEval <C> [BOLD] KP20k <R> <C> Transformer <C> 0.28625 <C> 0.29746 <C> 0.22038 <C> 0.22341 <R> <C> catSeq <C> 0.30211 <C> 0.2778 <C> 0.2002 <C> 0.2174 <R> <C> catSeqD <C> 0.30414 <C> 0.2839 <C> 0.1991 <C> 0.2158 <R> <C> catSeqCorr <C> 0.35238 <C> 0.3544 <C> 0.24923 <C> 0.28214 <R> <C> ExHiRD-s <C> 0.21014 <C> 0.18212 <C> 0.1198 <C> 0.1376 <R> <C> ExHiRD-h <C> [BOLD] 0.0306 <C> [BOLD] 0.1406 <C> [BOLD] 0.09110 <C> [BOLD] 0.1101 <CAP> Table 4: The average DupRatios of predicted keyphrases on all datasets. The lower the score, the better the performance.
<R> <C> [BOLD] Model <C> [BOLD] Inspec #PK <C> [BOLD] Inspec #AK <C> [BOLD] Krapivin #PK <C> [BOLD] Krapivin #AK <C> [BOLD] SemEval #PK <C> [BOLD] SemEval #AK <C> [BOLD] KP20k #PK <C> [BOLD] KP20k #AK <R> <C> Oracle <C> 7.64 <C> 2.10 <C> 3.27 <C> 2.57 <C> 6.28 <C> 8.12 <C> 3.32 <C> 1.93 <R> <C> Transformer <C> 3.1710 <C> 0.704 <C> 3.5729 <C> 0.634 <C> 3.2420 <C> 0.673 <C> 3.4417 <C> 0.584 <R> <C> catSeq <C> 3.332 <C> 0.584 <C> 3.7010 <C> 0.635 <C> 3.455 <C> 0.643 <C> 3.704 <C> 0.512 <R> <C> catSeqD <C> 3.334 <C> 0.582 <C> 3.6610 <C> 0.611 <C> 3.475 <C> 0.637 <C> 3.743 <C> 0.502 <R> <C> catSeqCorr <C> 3.077 <C> 0.532 <C> [BOLD] 3.3914 <C> 0.561 <C> 3.153 <C> 0.621 <C> [BOLD] 3.364 <C> 0.501 <R> <C> ExHiRD-s <C> 3.565 <C> 0.812 <C> 4.337 <C> 0.863 <C> [BOLD] 3.6914 <C> 0.796 <C> 3.942 <C> 0.691 <R> <C> ExHiRD-h <C> [BOLD] 4.004 <C> [BOLD] 1.506 <C> 4.419 <C> [BOLD] 1.027 <C> 3.6513 <C> [BOLD] 0.994 <C> 3.973 <C> [BOLD] 0.811 <CAP> Table 5: Results of average numbers of predicted unique keyphrases per document. “#PK” and “#AK” are the number of present and absent keyphrases respectively. “Oracle” is the gold average keyphrase number. The closest values to the oracles are bold.
<R> <C> [BOLD] Model <C> [BOLD] Present  [ITALIC] F1@ [ITALIC] M <C> [BOLD] Present  [ITALIC] F1@5 <C> [BOLD] Present #PK <C> [BOLD] Absent  [ITALIC] F1@ [ITALIC] M <C> [BOLD] Absent  [ITALIC] F1@5 <C> [BOLD] Absent #AK <C> [BOLD] DupRatio <R> <C> ExHiRD-h <C> [BOLD] 0.335 <C> [BOLD] 0.284 <C> [BOLD] 3.65 <C> [BOLD] 0.025 <C> [BOLD] 0.017 <C> [BOLD] 0.99 <C> [BOLD] 0.091 <R> <C> w/o HRD <C> 0.320 <C> 0.274 <C> 3.58 <C> 0.018 <C> 0.013 <C> 0.97 <C> 0.093 <R> <C> w/o ES <C> 0.330 <C> 0.278 <C> 3.51 <C> 0.022 <C> 0.014 <C> 0.70 <C> 0.191 <CAP> Table 6: Ablation study of our ExHiRD-h model on SemEval dataset. “w/o HRD” means the hierarchical decoder is replaced with a sequential decoder and the exclusive search is still incorporated. “w/o ES” represents our hierarchical decoding model without utilizing exclusive search mechanism.
<R> <C> [BOLD] Model <C> [BOLD] Present  [ITALIC] F1@ [ITALIC] M <C> [BOLD] Present  [ITALIC] F1@5 <C> [BOLD] Present #PK <C> [BOLD] Absent  [ITALIC] F1@ [ITALIC] M w <C> [BOLD] Absent  [ITALIC] F1@5 <C> [BOLD] Absent #AK <C> [BOLD] DupRatio <R> <C> Oracle <C> - <C> - <C> 3.32 <C> - <C> - <C> 1.93 <C> - <R> <C> Transformer <C> 0.360 <C> 0.282 <C> 3.44 <C> 0.024 <C> 0.011 <C> 0.58 <C> 0.223 <R> <C> catSeq <C> 0.368 <C> 0.295 <C> 3.70 <C> 0.023 <C> 0.010 <C> 0.51 <C> 0.217 <R> <C> catSeqD <C> 0.368 <C> 0.296 <C> 3.74 <C> 0.023 <C> 0.010 <C> 0.50 <C> 0.215 <R> <C> catSeqCorr <C> 0.367 <C> 0.281 <C> [BOLD] 3.36 <C> 0.023 <C> 0.010 <C> 0.50 <C> 0.282 <R> <C> Transformer w/ ES <C> 0.359 <C> 0.294 <C> 3.75 <C> 0.027 <C> 0.013 <C> 0.79 <C> 0.114 <R> <C> catSeq w/ ES <C> 0.366 <C> 0.305 <C> 3.95 <C> 0.025 <C> 0.012 <C> 0.68 <C> 0.138 <R> <C> catSeqD w/ ES <C> 0.366 <C> 0.306 <C> 3.99 <C> 0.026 <C> 0.012 <C> 0.65 <C> 0.137 <R> <C> catSeqCorr w/ ES <C> 0.366 <C> 0.298 <C> 3.74 <C> 0.027 <C> 0.013 <C> 0.72 <C> 0.159 <R> <C> ExHiRD-h <C> [BOLD] 0.374 <C> [BOLD] 0.311 <C> 3.97 <C> [BOLD] 0.032 <C> [BOLD] 0.016 <C> [BOLD] 0.81 <C> [BOLD] 0.110 <CAP> Table 8: Results of applying our exclusive search to other baselines on KP20k. The “w/ ES” means our exclusive search is applied.
<R> <C> ∣Embedding∣=100 Data Split <C> ∣Embedding∣=100 Training <C> ∣Embedding∣=100 Testing on the <C> ∣Embedding∣=100 Testing on the <C> ∣Embedding∣=300 Data Split <C> ∣Embedding∣=300 Training <C> ∣Embedding∣=300 Testing on the <C> ∣Embedding∣=300 Testing on the <R> <C> (|dialogues|) <C> [EMPTY] <C> Training Set <C> Test Set <C> (|dialogues|) <C> [EMPTY] <C> Training Set <C> Test Set <R> <C> 0 (861) <C> 1.8778 <C> 3.7711 <C> -1.1708 <C> 0 (1000) <C> 1.8168 <C> 3.6785 <C> -0.8618 <R> <C> 1 (902) <C> 1.3751 <C> 3.1663 <C> -1.7006 <C> 1 (850) <C> 2.0622 <C> 4.4598 <C> -1.8688 <R> <C> 2 (907) <C> 1.4194 <C> 3.1579 <C> -0.9723 <C> 2 (1010) <C> 1.6896 <C> 3.6724 <C> -1.4282 <R> <C> 3 (785) <C> 2.1532 <C> 4.2508 <C> -1.3444 <C> 3 (1029) <C> 1.9845 <C> 4.0136 <C> -0.6109 <R> <C> 4 (1046) <C> 1.2204 <C> 2.1581 <C> -1.5633 <C> 4 (951) <C> 1.8255 <C> 4.0423 <C> -1.4448 <R> <C> 5 (767) <C> 1.9456 <C> 3.9017 <C> -1.2123 <C> 5 (832) <C> 2.0860 <C> 4.2182 <C> -0.8277 <R> <C> 6 (1053) <C> 0.4621 <C> 0.1370 <C> -1.8443 <C> 6 (815) <C> 2.1735 <C> 4.2592 <C> -1.5193 <R> <C> 7 (968) <C> 1.8090 <C> 3.8368 <C> -1.1137 <C> 7 (891) <C> 2.1921 <C> 4.5799 <C> -1.4233 <R> <C> 8 (858) <C> 1.7608 <C> 3.5531 <C> -1.6678 <C> 8 (905) <C> 1.8835 <C> 3.8337 <C> -0.6628 <R> <C> 9 (826) <C> 1.8431 <C> 3.6254 <C> -1.0919 <C> 9 (892) <C> 2.0521 <C> 4.1882 <C> -1.5267 <R> <C> 10 (818) <C> 1.9188 <C> 3.8629 <C> -0.5394 <C> 10 (835) <C> 2.0709 <C> 4.2852 <C> -0.8831 <R> <C> 11 (944) <C> 1.8212 <C> 3.5724 <C> -1.7020 <C> 11 (873) <C> 2.1902 <C> 4.4848 <C> -1.3329 <R> <C> 12 (873) <C> 2.0195 <C> 4.1895 <C> -1.3456 <C> 12 (948) <C> 1.7761 <C> 3.7927 <C> -1.6167 <R> <C> 13 (895) <C> 2.0515 <C> 4.1873 <C> -1.8034 <C> 13 (932) <C> 1.8563 <C> 3.6208 <C> -1.5149 <R> <C> 14 (863) <C> 1.9722 <C> 4.1479 <C> -1.3244 <C> 14 (812) <C> 1.9486 <C> 4.0347 <C> -1.5866 <R> <C> 15 (842) <C> 1.8214 <C> 3.8942 <C> -0.8921 <C> 15 (880) <C> 1.1338 <C> 2.4880 <C> -1.4084 <R> <C> 16 (837) <C> 1.8162 <C> 3.8817 <C> -1.3784 <C> 16 (787) <C> 2.2628 <C> 4.5583 <C> -1.4290 <R> <C> 17 (958) <C> 1.6373 <C> 3.3373 <C> -0.7726 <C> 17 (994) <C> 0.9038 <C> 1.5106 <C> -1.5925 <R> <C> 18 (1012) <C> 1.7631 <C> 3.6279 <C> -1.2690 <C> 18 (853) <C> 2.2405 <C> 4.4716 <C> -1.4231 <R> <C> 19 (862) <C> 2.0683 <C> 4.2026 <C> -1.5901 <C> 19 (788) <C> 2.0686 <C> 4.2219 <C> -0.9594 <R> <C> 20 (17877) <C> -0.4138 <C> -1.2473 <C> -1.9684 <C> 20 (17877) <C> -0.3516 <C> -0.3490 <C> -2.0870 <R> <C> Average0−20 <C> 1.6353 <C> 3.2959† <C> -1.3461 <C> Average0−20 <C> [BOLD] 1.8031 <C> [BOLD] 3.7174† <C> [BOLD] -1.3337 <R> <C> Sum0−20 <C> 34.3419 <C> 69.2146 <C> -28.2674 <C> Sum0−20 <C> [BOLD] 37.8656 <C> [BOLD] 78.0653 <C> [BOLD] -28.0079 <R> <C> Upper Bound <C> 7.1810 <C> 7.1810 <C> 7.5942 <C> Upper Bound <C> 7.1810 <C> 7.1810 <C> 7.5942 <R> <C> Lower Bound <C> -7.2834 <C> -7.2834 <C> -7.7276 <C> Lower Bound <C> -7.2834 <C> -7.2834 <C> -7.7276 <R> <C> Random Sel. <C> -2.4139 <C> -2.4139 <C> -2.5526 <C> Random Sel. <C> -2.4139 <C> -2.4139 <C> -2.5526 <CAP> TABLE II: Average reward results of ChatDQN agents 0 to 20 trained with different data splits and size of sentence embedding (42 agents in total), where † denotes significant difference (at p=0.05) using a two-tailed Wilcoxon Signed Rank Test
<R> <C> Method <C> Accuracy (%) <C> Running Time (sec) <R> <C> FCD <C> 97.8 <C> 35 <R> <C> Relative Entropy <C> 95.4 <C> [EMPTY] <R> <C> Ziv-Merhav <C> 95.4 <C> [EMPTY] <R> <C> NCD (zlib) <C> 94.4 <C> 202 <R> <C> NCD (bzip2) <C> 93.3 <C> 198 <R> <C> NCD (blocksort) <C> 96.7 <C> 208 <R> <C> Algorithmic KL <C> 97.8 <C> 450 <CAP> Table 3: Accuracy and running time for different compression-based methods applied to the Liber Liber dataset.
<R> <C> [BOLD] ID <C> [ITALIC] λc <C> [ITALIC] λb <C> [BOLD] Named entity set <C> [BOLD] Regular set <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [BOLD] WER <C> [BOLD] WER <R> <C> 1 <C> 0 <C> 0 <C> 18.1 <C> 8.4 <R> <C> 2 <C> 0.1 <C> 0 <C> 14.4 <C> 8.5 <R> <C> 3 <C> 1.0 <C> 1.0 <C> 11.0 <C> 8.5 <R> <C> 4 <C> 0.1 <C> 1.0 <C> 9.0 <C> 8.5 <CAP> Table 2: Score normalization of contextual bias FST and its impact on accuracy.
<R> <C> Model <C> [ITALIC] γ <C> F1 <C> EM <C> Sk <C> Flop-r <R> <C> LSTM+Att (1 layer) <C> - <C> 73.3 <C> 63.9 <C> - <C> 1.3x <R> <C> LSTM+Att ( [ITALIC] d=50) <C> - <C> 74.0 <C> 64.4 <C> - <C> 3.6x <R> <C> LSTM+Att <C> - <C> 75.5 <C> [BOLD] 67.0 <C> - <C> 1.0x <R> <C> Sk-LSTM+Att ( [ITALIC] d′=0) <C> 0.1 <C> [BOLD] 75.7 <C> 66.7 <C> 37.7 <C> 1.4x <R> <C> Sk-LSTM+Att ( [ITALIC] d′=0) <C> 0.2 <C> 75.6 <C> 66.4 <C> 49.7 <C> 1.6x <R> <C> Sk-LSTM+Att <C> 0.05 <C> 75.5 <C> 66.0 <C> 39.7 <C> 1.4x <R> <C> Sk-LSTM+Att <C> 0.1 <C> 75.3 <C> 66.0 <C> 56.2 <C> 1.7x <R> <C> Sk-LSTM+Att <C> 0.2 <C> 75.0 <C> 66.0 <C> 76.4 <C> 2.3x <R> <C> VCRNN <C> - <C> 74.9 <C> 65.4 <C> - <C> 1.0x <R> <C> BiDAF ( [ITALIC] d=30) <C> - <C> 74.6 <C> 64.0 <C> - <C> 9.1x <R> <C> BiDAF ( [ITALIC] d=50) <C> - <C> 75.7 <C> 65.5 <C> - <C> 3.7x <R> <C> BiDAF <C> - <C> [BOLD] 77.3 <C> [BOLD] 67.7 <C> - <C> 1.0x <R> <C> Sk-BiDAF <C> 0.01 <C> 76.9 <C> 67.0 <C> 74.5 <C> 2.8x <R> <C> Sk-BiDAF <C> 0.001 <C> 77.1 <C> 67.4 <C> 47.1 <C> 1.7x <R> <C> SOTA (Wang et al.,  2017 ) <C> SOTA (Wang et al.,  2017 ) <C> 79.5 <C> 71.1 <C> - <C> - <CAP> Figure 3: Results on Stanford Question Answering Dataset (SQuAD), using LSTM+Attention (2 layers of LSTM, d=100, d′=20 by default) and BiDAF (d=100, d′=50 by default).
<R> <C> [BOLD] epoch <C> [BOLD] train  [ITALIC] pp original <C> [BOLD] train  [ITALIC] pp w. adv. <C> [BOLD] train  [ITALIC] pp increment <C> [BOLD] valid  [ITALIC] pp original <C> [BOLD] valid  [ITALIC] pp w. adv. <C> [BOLD] valid  [ITALIC] pp decrement <R> <C> 0 <C> 290.584 <C> 288.579 <C> -0.690% <C> 190.004 <C> 192.096 <C> -1.101% <R> <C> 2 <C> 113.216 <C> 113.712 <C> 0.439% <C> 140.328 <C> 140.339 <C> -0.008% <R> <C> 4 <C> 86.290 <C> 87.195 <C> 1.049% <C> 132.589 <C> 132.969 <C> -0.287% <R> <C> 6 <C> 56.282 <C> 56.961 <C> 1.207% <C> 121.410 <C> 120.566 <C> 0.695% <R> <C> 8 <C> 46.549 <C> 47.082 <C> 1.146% <C> 122.981 <C> 121.611 <C> 1.114% <R> <C> 10 <C> 43.991 <C> 44.474 <C> 1.096% <C> 123.065 <C> 121.385 <C> 1.365% <R> <C> 12 <C> 43.227 <C> 43.695 <C> 1.082% <C> 122.440 <C> 121.020 <C> 1.159% <CAP> TABLE VII: The perplexity before and after retraining on the PTB model. Column 3 and 5 for the augmented training set and column 4 and 7 for the improvement of retraining results w.r.t the original results.
<R> <C> Metric(m) <C> m Full <C> m SiFa <C> m + Suffp Full <C> m + Suffp SiFa <R> <C> Ans <C> 57.7 <C> 53.3 <C> 46.7 <C> 27.2 <R> <C> Suppp <C> 93.6 <C> 84.9 <C> 79.5 <C> 51.4 <R> <C> Ans + Suppp <C> 55.1 <C> 48.0 <C> 46.5 <C> 27.2 <R> <C> Supps <C> 50.4 <C> 36.8 <C> 47.0 <C> 22.6 <R> <C> Ans + Supps <C> 31.3 <C> 22.0 <C> 28.8 <C> 12.2 <CAP> Table 3: Scores of XLNet-Base model (Full) and XLNet Single-Fact model (SiFa), with and without the contrastive support sufficiency transform. Single-fact models are not much worse than Full under previously proposed metrics (m), but show a much larger drop under our proposed transform (m+Suffp).
<R> <C> Flow <C> Samples <C> Score <R> <C> Astronomy <C> 20(50) <C> 3.48 <R> <C> Board Games <C> 17(43) <C> 3.94 <R> <C> Books <C> 20(68) <C> 3.0 <R> <C> Box Office <C> 18(43) <C> 3.31 <R> <C> Comic Books <C> 5(51) <C> 3.6 <R> <C> Dinosaur <C> 11(44) <C> 3.77 <R> <C> Favorite Food <C> 32(65) <C> 3.31 <R> <C> Fun Facts <C> 17(45) <C> 4.15 <R> <C> Gossip <C> 25(60) <C> 3.6 <R> <C> Health <C> 7(55) <C> 3.43 <R> <C> History <C> 26(52) <C> 3.46 <R> <C> Hobbies <C> 21(51) <C> 3.88 <R> <C> Holidays <C> 11(51) <C> 3.23 <R> <C> Horoscope <C> 34(62) <C> 3.6 <R> <C> Joke <C> 54(96) <C> 3.94 <R> <C> Monsters <C> 46(55) <C> 3.14 <CAP> Table 6: Our current flows, here Samples are #utilized(#prompted).
<R> <C> Flow <C> Samples <C> Score <R> <C> Movie <C> 41(168) <C> 3.45 <R> <C> Music <C> 66(139 <C> 3.36 <R> <C> News <C> 27(59) <C> 3.28 <R> <C> Poem <C> 19(40) <C> 3.29 <R> <C> Quote <C> 29(59) <C> 3.48 <R> <C> Recipe <C> 19(42) <C> 3.66 <R> <C> Science <C> 24(51) <C> 3.29 <R> <C> Shopping <C> 13(46) <C> 3.08 <R> <C> Sports <C> 58(149) <C> 3.2 <R> <C> Style <C> 7(33) <C> 3.14 <R> <C> Technology <C> 170(210) <C> 3.25 <R> <C> Travel <C> 17(50) <C> 3.35 <R> <C> TV <C> 10(48) <C> 3.1 <R> <C> Video Games <C> 16(69) <C> 3.75 <R> <C> Weather <C> 29(73) <C> 3.6 <CAP> Table 6: Our current flows, here Samples are #utilized(#prompted).
<R> <C> Signature <C> # Turns <C> Score <R> <C> Storytelling <C> 7.78 <C> 3.31 <R> <C> Recursive <C> 5.08 <C> 4.0 <R> <C> [BOLD] Games: <C> [EMPTY] <C> [EMPTY] <R> <C> CYOA <C> 6.49 <C> 3.93 <R> <C> City Names <C> 6.45 <C> 3.73 <R> <C> Fast Money <C> 21.83 <C> 3.96 <R> <C> Jeopardy <C> 17.64 <C> 3.66 <R> <C> Number Game <C> 5.75 <C> 3.83 <R> <C> Survey <C> 4.10 <C> 3.70 <R> <C> [BOLD] Sequences: <C> [EMPTY] <C> [EMPTY] <R> <C> Riddles <C> 6.72 <C> 3.33 <R> <C> WYR <C> 6.33 <C> 3.71 <CAP> Table 7: System Initiative Modules.
<R> <C> Offline <C> BLEU <C> Emea 21.3±1.0 <C> TED 31.3±1.0 <C> XRCE 34.7±2.2 <R> <C> Offline <C> Meteor <C> 39.5±1.1 <C> 52.0±1.0 <C> 52.2±2.3 <R> <C> Offline <C> TER <C> 66.9±1.5 <C> 52.3±1.0 <C> 55.0±2.4 <R> <C> Online <C> BLEU <C> [BOLD] 28.6± [BOLD] 1.2 <C> 31.4±1.0 <C> 35.1±2.2 <R> <C> Online <C> Meteor <C> [BOLD] 48.8± [BOLD] 1.1 <C> 51.8±0.9 <C> 53.0±2.1 <R> <C> Online <C> TER <C> [BOLD] 57.0± [BOLD] 1.5 <C> 52.2±1.0 <C> 55.8±2.2 <R> <C> Online <C> Algorithm <C> Adam <C> PPAS <C> PPAS <CAP> Table 4: Offline vs online systems, trained with out-of-domain data and adapted with additional in-domain data. Bold results indicate a significant improvement of the online system with respect to the offline. We also indicate the best performing algorithm for each task.
<R> <C> [BOLD] Model <C> [BOLD] API Acc↑ <C> [BOLD] API Perp↓ <C> [BOLD] API Att.Acc↑ <C> [BOLD] Response BLEU↑ <C> [BOLD] Response Perp↓ <R> <C> SIMMC-Furniture <C> SIMMC-Furniture <C> SIMMC-Furniture <C> SIMMC-Furniture <C> SIMMC-Furniture <C> SIMMC-Furniture <R> <C> TD-IDF <C> 76.5 <C> 2.94 <C> 42.7 <C> - <C> - <R> <C> LM-LSTM <C> - <C> - <C> - <C> 0.10 <C> 9.65 <R> <C> HAE <C> [BOLD] 79.6 <C> [BOLD] 1.73 <C> [BOLD] 52.2 <C> [BOLD] 0.22 <C> 8.53 <R> <C> HRE <C> [BOLD] 79.5 <C> [BOLD] 1.72 <C> 51.2 <C> [BOLD] 0.23 <C> 9.03 <R> <C> MN <C> 78.2 <C> 1.87 <C> 51.8 <C> [BOLD] 0.23 <C> 8.68 <R> <C> T-HAE <C> 78.7 <C> 1.79 <C> 51.2 <C> 0.14 <C> [BOLD] 7.72 <R> <C> SIMMC-Fashion <C> SIMMC-Fashion <C> SIMMC-Fashion <C> SIMMC-Fashion <C> SIMMC-Fashion <C> SIMMC-Fashion <R> <C> TD-IDF <C> 82.5 <C> 3.75 <C> 77.9 <C> - <C> - <R> <C> LM-LSTM <C> - <C> - <C> - <C> 0.10 <C> 7.08 <R> <C> HAE <C> 84.5 <C> 1.77 <C> 80.2 <C> 0.23 <C> 6.41 <R> <C> HRE <C> [BOLD] 85.1 <C> 1.75 <C> [BOLD] 81.2 <C> 0.19 <C> 6.55 <R> <C> MN <C> 84.5 <C> [BOLD] 1.67 <C> 80.0 <C> [BOLD] 0.25 <C> 6.54 <R> <C> T-HAE <C> 84.6 <C> 1.73 <C> 80.3 <C> 0.17 <C> [BOLD] 5.63 <CAP> Table 6: Results on SIMMC-Furniture and SIMMC-Fashion for: (a) API prediction, measured using accuracy (acc), perplexity (perp) and attribute prediction accuracy (Att.Acc), and, (b) Response generation, measured using BLEU and perplexity (perp). ↑: higher is better, ↓: lower is better. See text for details.
<R> <C> Model <C> Simulation Eval All <C> Simulation Eval All <C> Simulation Eval All <C> Simulation Eval CI <C> Simulation Eval CI <C> Simulation Eval VI-1/2 <C> Simulation Eval VI-1/2 <C> Simulation Eval VI-3/4 <C> Simulation Eval VI-3/4 <C> Human Eval VI-3/4 <C> Human Eval VI-3/4 <R> <C> Model <C> C+F Acc <C> Overall Acc <C> #Asks <C> C+F Acc <C> #Asks <C> C+F Acc <C> #Asks <C> C+F Acc <C> #Asks <C> C+F Acc <C> #Asks <R> <C> LAM <C> 0.374 <C> 0.640 <C> 0 <C> 0.801 <C> 0 <C> 0.436 <C> 0 <C> 0.166 <C> 0 <C> 0.206 <C> 0 <R> <C> LAM-rule <C> 0.761 <C> 0.926 <C> 3.891 <C> 0.897 <C> 1.433 <C> 0.743 <C> 2.826 <C> 0.721 <C> 5.568 <C> 0.518 <C> 2.781 <R> <C> LAM-sup <C> 0.809 <C> 0.940 <C> [BOLD] 2.028 <C> 0.894 <C> [BOLD] 0.684 <C> 0.803 <C> [BOLD] 1.482 <C> 0.780 <C> 2.921 <C> 0.433 <C> 2.614 <R> <C> HRL-fixedOrder <C> 0.881 <C> 0.966 <C> 2.272 <C> [BOLD] 0.950 <C> 1.522 <C> 0.855 <C> 1.958 <C> 0.871 <C> 2.777 <C> 0.581 <C> 2.306∗ <R> <C> HRL <C> [BOLD] 0.894∗ <C> [BOLD] 0.968 <C> 2.069∗ <C> 0.949 <C> 1.226∗ <C> [BOLD] 0.888∗ <C> 1.748∗ <C> [BOLD] 0.878∗ <C> [BOLD] 2.615∗ <C> [BOLD] 0.634∗ <C> [BOLD] 2.221∗ <CAP> Table 3: Model evaluation on the test set. For Simulation Eval, each number is averaged over 10 runs. For Human Eval, the LAM result is calculated on the sampled 496 recipes. ∗ denotes significant difference in mean between HRL-fixedOrder vs. HRL in Simulation Eval and between HRL-based agents vs. {LAM-rule, LAM-sup} agents in Human Eval (p<0.05).
<R> <C> Domains <C> Train <C> Valid <C> Eval <R> <C> Toys & Games <C> 27,037 <C> 498 <C> 512 <R> <C> Sports & Outdoors <C> 37,445 <C> 511 <C> 466 <R> <C> Movies & TV <C> 408,827 <C> 564 <C> 512 <CAP> Table 1: Number of reviews for training (Train), validation (Valid) and evaluation (Eval).
<R> <C> Domain Metric <C> Toys & Games R-1 <C> Toys & Games R-2 <C> Toys & Games R-L <C> Sports & Outdoors R-1 <C> Sports & Outdoors R-2 <C> Sports & Outdoors R-L <C> Movies & TV R-1 <C> Movies & TV R-2 <C> Movies & TV R-L <R> <C> Unuspervised approaches <C> Unuspervised approaches <C> Unuspervised approaches <C> Unuspervised approaches <C> Unuspervised approaches <C> Unuspervised approaches <C> Unuspervised approaches <C> Unuspervised approaches <C> Unuspervised approaches <C> Unuspervised approaches <R> <C> TextRank <C> 8.63 <C> 1.24 <C> 7.26 <C> 7.16 <C> 0.89 <C> 6.39 <C> [BOLD] 8.27 <C> [BOLD] 1.44 <C> 7.35 <R> <C> Opinosis <C> 8.25 <C> 1.51 <C> 7.52 <C> 7.04 <C> 1.42 <C> 6.45 <C> 7.80 <C> 1.20 <C> 7.11 <R> <C> MeanSum-single <C> 8.12 <C> 0.58 <C> 7.30 <C> 5.42 <C> 0.47 <C> 4.97 <C> 6.96 <C> 0.35 <C> 6.08 <R> <C> [ITALIC] StrSum <C> 11.61 <C> 1.56 <C> 11.04 <C> 9.15 <C> 1.38 <C> 8.79 <C> 7.38 <C> 1.03 <C> 6.94 <R> <C> [ITALIC] StrSum+DiscourseRank <C> [BOLD] 11.87 <C> [BOLD] 1.63 <C> [BOLD] 11.40 <C> [BOLD] 9.62 <C> [BOLD] 1.58 <C> [BOLD] 9.28 <C> 8.15 <C> 1.33 <C> [BOLD] 7.62 <R> <C> Supervised baselines <C> Supervised baselines <C> Supervised baselines <C> Supervised baselines <C> Supervised baselines <C> Supervised baselines <C> Supervised baselines <C> Supervised baselines <C> Supervised baselines <C> Supervised baselines <R> <C> Seq-Seq <C> 13.50 <C> 2.10 <C> 13.31 <C> 10.69 <C> 2.02 <C> 10.61 <C> 7.71 <C> 2.18 <C> 7.08 <R> <C> Seq-Seq-att <C> 16.28 <C> 3.13 <C> 16.13 <C> 11.49 <C> 2.39 <C> 11.47 <C> 9.05 <C> 2.99 <C> 8.46 <CAP> Table 2: ROUGE F1 score of the evaluation set (%). R-1, R-2 and R-L denote ROUGE-1, ROUGE-2, and ROUGE-L, respectively. The best performing model among unsupervised approaches is shown in boldface.
<R> <C> System <C> Dev <C> Test <R> <C> las-wp <C> 15.7 <C> 12.3 <R> <C> las-wp + 3-gram-w <C> 12.9 <C> 9.3 <R> <C> las-wp + rnn-wp <C> 11.5 <C> 8.2 <CAP> Table 2: WER of las-wp combined with various LMs on WSJ. rnn-wp again performs best.
<R> <C> System <C> Dev <C> Test <C> LM size <R> <C> las-g <C> 9.5 <C> 7.7 <C> 0GB <R> <C> las-wp <C> 9.2 <C> 7.7 <C> 0GB <R> <C> las-wp + prodlm1 <C> 8.8 <C> 7.4 <C> 2GB <R> <C> las-wp + prodlm2 <C> 8.7 <C> 7.2 <C> 80GB <R> <C> las-wp + rnn-wp <C> 8.4 <C> 7.0 <C> 1.1GB <R> <C> las-wp + rnn-wp + prodlm2 <C> 8.4 <C> 7.0 <C> 81.1GB <CAP> Table 3: WER of shallow fusion of LAS with production n-gram LMs and an RNN LM. The RNN LM captures all the benefits of prodlm2 in a compact form.
<R> <C> [BOLD] Personality <C> [BOLD] BLEU <C> [BOLD] NIST <C> [BOLD] METEOR <C> [BOLD] ROUGE_L <R> <C> SingleVoice <C> 0.35 <C> 4.93 <C> 0.36 <C> 0.50 <R> <C> MultiVoice <C> 0.42 <C> 5.64 <C> 0.36 <C> 0.52 <CAP> Table 3: Automatic metric evaluation
<R> <C> [BOLD] Personality <C> [BOLD] Deletions <C> [BOLD] Repetitions <C> [BOLD] Hallucinations <R> <C> Agree <C> 0.27 <C> 0.29 <C> 0.34 <R> <C> Consc <C> 0.22 <C> 0.12 <C> 0.41 <R> <C> Extra <C> 0.74 <C> 0.46 <C> 0.35 <R> <C> UnConsc <C> 0.31 <C> 0.28 <C> 0.29 <R> <C> Disagree <C> 0.87 <C> 0.81 <C> 0.22 <R> <C> [BOLD] Personality Pairs <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Agree+Consc <C> 0.44 <C> 0.08 <C> 0.26 <R> <C> Agree+Extra <C> 0.28 <C> 0.17 <C> 0.19 <R> <C> Agree+Unconsc <C> 0.33 <C> 0.24 <C> 0.24 <R> <C> Consc+Disagr <C> 1.01 <C> 0.18 <C> 0.28 <R> <C> Consc+Extra <C> 0.67 <C> 0.28 <C> 0.23 <R> <C> Disagr+Extra <C> 1.20 <C> 0.75 <C> 0.09 <R> <C> Disagr+Unconsc <C> 1.10 <C> 0.39 <C> 0.14 <R> <C> Extra+Unconsc <C> 1.05 <C> 0.55 <C> 0.17 <CAP> Table 4: Ratio of errors by multivoice personality pairs as compared to singlevoice models
<R> <C> [BOLD] # <C> [BOLD] P1 <C> [BOLD] P2 <C> [BOLD] P1+P2 vs. P1 <C> [BOLD] P1+P2 vs. P2 <C> [BOLD] P1 vs. P2 <R> <C> 1 <C> Agree <C> Consc <C> 0.74 <C> 0.76 <C> 0.74 <R> <C> 2 <C> Agree <C> Extra <C> 0.70 <C> 0.31 <C> 0.44 <R> <C> 3 <C> Agree <C> Unconsc <C> 0.75 <C> 0.31 <C> 0.65 <R> <C> 4 <C> Consc <C> Disagr <C> 0.36 <C> 0.65 <C> 0.01 <R> <C> 5 <C> Consc <C> Extra <C> 0.51 <C> 0.31 <C> 0.44 <R> <C> 6 <C> Disagr <C> Extra <C> 0.53 <C> -0.36 <C> -0.04 <R> <C> 7 <C> Disagr <C> Unconsc <C> 0.23 <C> 0.33 <C> 0.05 <R> <C> 8 <C> Extra <C> Unconsc <C> 0.20 <C> 0.43 <C> 0.47 <CAP> Table 6: Correlations between personage data and multivoice models for the aggregation operations in Table 5
<R> <C> [BOLD] Dataset <C> [BOLD] Accuracy Ori/Adv <C> [BOLD] Semantic Ori/Adv <R> <C> [BOLD] MNLI <C> 0.90/0.70 <C> 3.9/3.7 <R> <C> [BOLD] IMDB <C> 0.91/0.85 <C> 4.1/3.9 <CAP> Table 2: Human-Evaluation Results
<R> <C> [BOLD] Dataset <C> [BOLD] Model <C> [BOLD] Ori Acc <C> [BOLD] Atk Acc <C> [BOLD] Perturb % <R> <C> [BOLD] IMDB <C> Word-LSTM <C> 89.8 <C> 10.2 <C> 2.7 <R> <C> [BOLD] IMDB <C> BERT-Large <C> 98.2 <C> 12.4 <C> 2.9 <R> <C> [BOLD] Yelp <C> Word-LSTM <C> 96.0 <C> 1.1 <C> 4.7 <R> <C> [BOLD] Yelp <C> BERT-Large <C> 97.9 <C> 8.2 <C> 4.1 <R> <C> [BOLD] MNLI <C> ESIM <C> 76.2 <C> 9.6 <C> 21.7 <R> <C> matched <C> BERT-Large <C> 86.4 <C> 13.2 <C> 7.4 <CAP> Table 3: BERT-Attack against other models.
<R> <C> [EMPTY] <C> Vanishing ratio 1K <C> Vanishing ratio 5K <C> Vanishing ratio 20K <C> Validation acc. <C> Validation acc. 1K <C> Validation acc. 5K <C> Validation acc. 20K <R> <C> BiLSTM <C> ∼109 <C> 381.0 <C> 90.5 <C> [EMPTY] <C> 58.8 <C> 82.4 <C> 88.1 <R> <C> Att <C> 45.0 <C> 5.2 <C> 29.8 <C> [EMPTY] <C> 70.8 <C> 84.2 <C> 87.4 <R> <C> MeanPool <C> 3.1 <C> 6.5 <C> 8.0 <C> [EMPTY] <C> 71.4 <C> 84.1 <C> 87.8 <R> <C> MaxPool <C> 21.4 <C> 16.4 <C> 40.0 <C> [EMPTY] <C> 71.2 <C> 84.1 <C> 89.0 <R> <C> MaxAtt <C> 15.6 <C> 10.7 <C> 8.0 <C> [EMPTY] <C> 74.8 <C> 85.4 <C> 89.8 <CAP> Table 2: The vanishing ratio (|∂L∂hend|/|∂L∂hmid|) and validation accuracy, computed at a point when different models achieve 95% training accuracy.
<R> <C> Model <C> NIPS Abstract Acc <C> NIPS Abstract  [ITALIC] τ <C> NIPS Abstract #pm <C> ANN Abstract Acc <C> ANN Abstract  [ITALIC] τ <C> ANN Abstract #pm <C> arXiv Abstract PMR <C> arXiv Abstract  [ITALIC] τ <C> arXiv Abstract #pm <C> SIND PMR <C> SIND  [ITALIC] τ <C> SIND #pm <R> <C> LSTM+PtrNet † <C> 50.87 <C> 0.67 <C> 2.1M <C> 58.20 <C> 0.69 <C> 3.0M <C> 40.44 <C> 0.72 <C> 12.7M <C> 12.34 <C> 0.48 <C> 3.6M <R> <C> V-LSTM+PtrNet † <C> 51.55 <C> 0.72 <C> 26.5M <C> 58.06 <C> 0.73 <C> 28.9M <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> ATTOrderNet † <C> 56.09 <C> 0.72 <C> 8.7M <C> 63.24 <C> 0.73 <C> 17.9M <C> 42.19 <C> 0.73 <C> 23.5M <C> 14.01 <C> 0.49 <C> 14.4M <R> <C> F-Graph <C> 56.24 <C> 0.72 <C> 4.1M <C> 63.45 <C> 0.74 <C> 9.9M <C> 42.50 <C> 0.74 <C> 19.6M <C> 14.48 <C> 0.50 <C> 10.6M <R> <C> S-Graph <C> 56.67 <C> 0.73 <C> 4.1M <C> 64.09 <C> 0.76 <C> 9.9M <C> 43.37 <C> 0.74 <C> 19.6M <C> 15.15 <C> 0.50 <C> 10.6M <R> <C> SE-Graph <C> [BOLD] 57.27* <C> [BOLD] 0.75* <C> 5.0M <C> [BOLD] 64.64* <C> [BOLD] 0.78* <C> 11.5M <C> [BOLD] 44.33* <C> [BOLD] 0.75* <C> 21.3M <C> [BOLD] 16.22* <C> [BOLD] 0.52* <C> 12.2M <CAP> Table 1: Main results on the sentence ordering task, where #pm shows the number of parameters, † indicates previously reported scores and * means significant at p<0.01 over the F-Graph on each test set. V-LSTM+PtrNet stands for Varient-LSTM+PtrNet. We conduct 1,000 bootstrap tests [13] to measure the significance in metric score differences.
<R> <C> Model <C> arXiv Abstract head <C> arXiv Abstract tail <C> SIND head <C> SIND tail <R> <C> LSTM+PtrNet † <C> 90.47 <C> 66.49 <C> 74.66 <C> 53.30 <R> <C> ATTOrderNet † <C> 91.00 <C> 68.08 <C> 76.00 <C> 54.42 <R> <C> F-Graph <C> 91.43 <C> 68.56 <C> 76.53 <C> 56.02 <R> <C> S-Graph <C> 91.99 <C> 69.74 <C> 77.07 <C> 56.28 <R> <C> SE-Graph <C> [BOLD] 92.28 <C> [BOLD] 70.45 <C> [BOLD] 78.12 <C> [BOLD] 56.68 <CAP> Table 2: The ratio of correctly predicting first and last sentences on arXiv Abstract and SIND. † indicates previously reported scores.
<R> <C> [BOLD] Model <C> BETTER <C> WORSE <C> NONE <C> ALL <R> <C> Rule-based Baseline <C> 0.65 <C> [BOLD] 0.44 <C> 0.90 <C> 0.82 <R> <C> InferSent+XGBoost <C> [BOLD] 0.75 <C> 0.43 <C> [BOLD] 0.92 <C> [BOLD] 0.85 <CAP> Table 3: Performance (F1) of the best classifier-based model compared to the rule-based baseline.
<R> <C> Train \Test <C> CompSci <C> Brands <C> Random <R> <C> CompSci <C> 0.82 <C> [BOLD] 0.84 <C> [BOLD] 0.84 <R> <C> Brands <C> 0.76 <C> [BOLD] 0.83 <C> [BOLD] 0.83 <R> <C> Random <C> 0.79 <C> 0.84 <C> [BOLD] 0.86 <CAP> Table 4: Cross-domain evaluation in terms of total F1 for all classes (best results per row in bold).
<R> <C> [EMPTY] <C> Lemmatization classical <C> Lemmatization cross-genre <C> Lemmatization cross-time <C> Tagging classical <C> Tagging cross-genre <C> Tagging cross-time <R> <C> Per-author embeddings, per-UD-treebank embeddings <C> 96.28 <C> 87.28 <C> 90.80 <C> 96.74 <C> 91.11 <C> 87.69 <R> <C> Single EvaLatin embedding, per-UD-treebank embeddings <C> 96.28 <C> 87.28 <C> 90.80 <C> 96.70 <C> 91.11 <C> 87.69 <R> <C> Single EvaLatin embedding, single UD-treebank embedding <C> 96.23 <C> 87.22 <C> 90.78 <C> 96.68 <C> 91.14 <C> 87.63 <R> <C> EvaLatin and UD treebanks merged <C> 96.18 <C> 87.23 <C> 90.77 <C> 96.52 <C> 91.01 <C> 86.12 <CAP> Table 6: The effect of various kinds of treebank embeddings in open modality – whether the individual authors in EvaLatin get a different or the same treebank embedding, and whether the UD treebanks get a different treebank embedding, same treebank embedding but different from the EvaLatin data, or the same treebank embedding as EvaLatin data.
<R> <C> DUC 2002 <C> ROUGE-1 <C> ROUGE-2 <C> ROUGE-L <R> <C> LEAD <C> 43.6 <C> 21.0 <C> 40.2 <R> <C> ILP <C> 45.4 <C> 21.3 <C> 42.8 <R> <C> TGRAPH <C> 48.1 <C> 24.3 <C> – <R> <C> URANK <C> 48.5 <C> 21.5 <C> – <R> <C> NN-SE <C> 47.4 <C> 23.0 <C> 43.5 <R> <C> Deep-Classifier <C> 46.8±0.9 <C> 22.6±0.9 <C> 43.1±0.9 <R> <C> SummaRuNNer <C> 46.6±0.8 <C> 23.1±0.9 <C> 43.03±0.8 <R> <C> Hybrid MemNet <C> 49.1 <C> 24.7 <C> 44.6 <R> <C> Hybrid MemNet∗ <C> [BOLD] 50.1 <C> [BOLD] 25.2 <C> [BOLD] 44.9 <CAP> Table 1. Rouge Evaluation (%) on the DUC-2002 Corpus and 500 Samples from the Daily Mail Corpus
<R> <C> DailyMail <C> ROUGE-1 <C> ROUGE-2 <C> ROUGE-L <R> <C> LEAD <C> 20.4 <C> 7.7 <C> 11.4 <R> <C> NN-SE <C> 21.2 <C> 8.3 <C> 12.0 <R> <C> Deep-Classifier <C> 26.2±0.4 <C> 10.7±0.4 <C> 14.4±0.4 <R> <C> SummaRuNNer <C> 26.2±0.4 <C> 10.8±0.3 <C> 14.4±0.3 <R> <C> Hybrid MemNet <C> 27.1 <C> 11.6 <C> 15.2 <R> <C> Hybrid MemNet∗ <C> [BOLD] 27.9 <C> [BOLD] 12.2 <C> [BOLD] 15.5 <CAP> Table 1. Rouge Evaluation (%) on the DUC-2002 Corpus and 500 Samples from the Daily Mail Corpus
<R> <C> Training Strategy <C> WER(%) w/o LM <R> <C> LSTM + random initialization <C> 41.0 <R> <C> LSTM + tied-triphone pre-training <C> 30.8 <R> <C> LSTM + CL <C> 37.8 <R> <C> LSTM + LS <C> 35.4 <R> <C> LSTM + TS <C> 36.0 <R> <C> LSTM + TS + LS <C> 33.9 <R> <C> LSTM + TS + CL + LS <C> 33.2 <R> <C> BLSTM <C> 27.8 <R> <C> BLSTM + LS <C> 25.5 <CAP> Table 1: WER on test set of models with various training strategies, curriculum learning (CL), label smoothing regularization (LS), and teacher-student approach (TS). None of our experiments used any language model or lexicon information.
<R> <C> [BOLD] Generative Text Model <C> [BOLD] BLEU G-Train <R> <C> Word LSTM temp 1.0 <C> 0.2701 <R> <C> Word LSTM temp 0.7 <C> 0.4998 <R> <C> Word LSTM temp 0.5 <C> 0.6294 <R> <C> Scheduled Sampling <C> 0.1707 <R> <C> Google LM <C> 0.0475 <R> <C> Attention Attribute to Sequence <C> 0.5122 <R> <C> Contexts to Sequences <C> 0.7542 <R> <C> Gated Contexts to Sequences <C> 0.6240 <R> <C> MLE SeqGAN <C> 0.1707 <R> <C> SeqGAN <C> 0.1751 <R> <C> RankGAN <C> 0.1525 <R> <C> LeakGAN <C> 0.1871 <CAP> Table 9: BLEU results when evaluating the generated reviews using G-train as the reference corpus (a lower score indicates less n-grams in common between the training set G-train and the generated text). GAN models present low similarity with the training set.
<R> <C> [BOLD] Metric <C> [BOLD] % <C> [BOLD] BERT+ <C> [BOLD] seq2seq <R> <C> [EMPTY] <C> [EMPTY] <C> [BOLD] Explicit Pattern <C> [EMPTY] <R> <C> Perplexity <C> 5% <C> 27.5 <C> 11.7 <R> <C> Perplexity <C> 10% <C> 22.5 <C> 27.0 <R> <C> Perplexity <C> 20% <C> 22.5 <C> 24.2 <R> <C> Perplexity <C> 100% <C> – <C> 36.7 <R> <C> Cosine-Similarity <C> 5% <C> 0.98 <C> 0.68 <R> <C> Cosine-Similarity <C> 10% <C> 0.98 <C> 0.72 <R> <C> Cosine-Similarity <C> 20% <C> 0.98 <C> 0.74 <R> <C> Cosine-Similarity <C> 100% <C> – <C> 0.85 <CAP> Table 3: Median perplexity of transformed sentences and their average similarities to input sentences
<R> <C> [BOLD] Model <C> [BOLD] Paraphrase <C> [BOLD] Validation <C> [BOLD] New Program <R> <C> Genie <C> 87.1±1.8 <C> [BOLD] 67.9± [BOLD] 0.7 <C> 29.9±3.2 <R> <C> − canonicalization <C> 80.0±1.3 <C> 63.2±0.9 <C> 21.9±0.9 <R> <C> − keyword param. <C> 84.0±0.6 <C> 66.6±0.3 <C> 25.0±2.0 <R> <C> − type annotations <C> 86.9±3.6 <C> 67.5±0.6 <C> [BOLD] 31.0± [BOLD] 1.1 <R> <C> − param. expansion <C> 78.3±4.8 <C> 66.3±0.4 <C> 30.5±1.3 <R> <C> − decoder LM <C> [BOLD] 88.7± [BOLD] 1.0 <C> 66.8±0.8 <C> 27.3±1.7 <CAP> Table 3. Accuracy results for the ablation study. Each “−” row removes one feature independently.
<R> <C> [BOLD] Task <C> [BOLD] LSTM-Layers  [BOLD] 1 <C> [BOLD] LSTM-Layers  [BOLD] 2 <C> [BOLD] LSTM-Layers  [BOLD] 3 <R> <C> POS <C> 157 <C> 63 <C> 103 <R> <C> [ITALIC] γ25 <C> -0.03% <C> -0.01% <C> -0.15% <R> <C> Chunking <C> 174 <C> 106 <C> 115 <R> <C> [ITALIC] γ25 <C> -0.01% <C> -0.05% <C> -0.03% <R> <C> NER <C> 115 <C> 96 <C> 92 <R> <C> [ITALIC] γ25 <C> -0.01% <C> -0.06% <C> -0.07% <R> <C> Entities <C> 192 <C> 175 <C> 115 <R> <C> [ITALIC] γ25 <C> -0.04% <C> -0.04% <C> -0.10% <R> <C> Events <C> 126 <C> 56 <C> - <R> <C> [ITALIC] γ25 <C> -0.01% <C> -0.03% <C> - <CAP> Table 16: The first number in each cell is the optimal number of recurrent units xopt per LSTM-network. The second number shows the value γ25=p(xopt±25)−p(xopt), i.e. when changing the number of recurrent units by 25, how much does the test performance change. For the Events dataset with 3 stacked BiLSTM-layers, the optimal number was not in the tested range and hence was not found by the polynomial regression approach.
<R> <C> Condition <C> Readability  [ITALIC] μ <C> Readability  [ITALIC] σ <C> Relevance  [ITALIC] μ <C> Relevance  [ITALIC] σ <R> <C> No title <C> 4.66 <C> 0.65 <C> 4.11 <C> 0.86 <R> <C> Human <C> 4.55 <C> 0.76 <C> 4.09 <C> 0.95 <R> <C> Algo <C> 4.52 <C> 0.72 <C> 4.12 <C> 1.02 <CAP> Table 3: Human ratings for human-generated summaries while showing different section titles.
<R> <C> [EMPTY] <C> TextFool ( 9 ) <C> Proposed method using genre specific keywords <C> Proposed method w/o using genre specific keywords <R> <C> CNN trained with original training set <C> CNN trained with original training set <C> CNN trained with original training set <C> CNN trained with original training set <R> <C> Accuracy using original test set <C> 74.53 <C> 74.53 <C> 74.53 <R> <C> Accuracy using adversarial test set <C> 74.13 <C> 32.55 <C> 57.31 <R> <C> Percentage of perturbed samples <C> 0.64 <C> 90.64 <C> 42.76 <R> <C> CNN re-trained with perturbed training set <C> CNN re-trained with perturbed training set <C> CNN re-trained with perturbed training set <C> CNN re-trained with perturbed training set <R> <C> Accuracy using original test set <C> 68.14 <C> 78.00 <C> 78.81 <R> <C> Accuracy using adversarial test set <C> 68.08 <C> 78.46 <C> 78.21 <CAP> Table 1: Performance results on IMDB movie review dataset.
<R> <C> [EMPTY] <C> Relation <C> [BOLD] TP <C> [BOLD] FP <C> [BOLD] P <R> <C> 1 <C> is_vendor_of <C> 45 <C> 12 <C> 0.79 <R> <C> 2 <C> is_version_of <C> 54 <C> 19 <C> 0.74 <R> <C> 3 <C> CVE_of_vuln <C> - <C> - <C> - <R> <C> 4 <C> MS_of_SW <C> - <C> - <C> - <R> <C> 5 <C> MS_of_vuln <C> 2 <C> 0 <C> 1.00 <R> <C> 6 <C> vuln_of_SW <C> 30 <C> 2 <C> 0.94 <R> <C> 7 <C> symbol_of <C> - <C> - <C> - <R> <C> 8 <C> not_version_of <C> 22 <C> 0 <C> 1.00 <R> <C> [EMPTY] <C> [BOLD] Totals <C> [BOLD] 153 <C> [BOLD] 33 <C> [BOLD] 0.82 <CAP> Table 3: Results
<R> <C> [EMPTY] <C> Supervision <C> Time <C> En-Es → <C> En-Es ← <C> En-Fr → <C> En-Fr ← <C> En-De → <C> En-De ← <C> En-It → <C> En-It ← <C> En-Ru → <C> En-Ru ← <R> <C> Procrustes <C> 5K words <C> 3 <C> 77.6 <C> 77.2 <C> 74.9 <C> 75.9 <C> 68.4 <C> 67.7 <C> 73.9 <C> 73.8 <C> 47.2 <C> 58.2 <R> <C> Procrustes + CSLS <C> 5K words <C> 3 <C> 81.2 <C> 82.3 <C> 81.2 <C> 82.2 <C> 73.6 <C> 71.9 <C> 76.3 <C> [BOLD] 75.5 <C> 51.7 <C> 63.7 <R> <C> (Conneau et al.,  2018 ) <C> None <C> 957 <C> [BOLD] 81.7 <C> [BOLD] 83.3 <C> [BOLD] 82.3 <C> 82.1 <C> 74.0 <C> 72.2 <C> 77.4 <C> 76.1 <C> [BOLD] 52.4 <C> [BOLD] 61.4 <R> <C> G-W ( [ITALIC] λ=10−4) <C> None <C> 70 <C> 78.3 <C> 79.5 <C> 79.3 <C> 78.3 <C> 69.6 <C> 66.9 <C> 75.3 <C> 74.1 <C> 26.1 <C> 35.4 <R> <C> G-W ( [ITALIC] λ=10−5) <C> None <C> 37 <C> [BOLD] 81.7 <C> 80.4 <C> 81.3 <C> 78.9 <C> 71.9 <C> [BOLD] 72.8 <C> [BOLD] 78.9 <C> 75.2 <C> 45.1 <C> 43.7 <CAP> Table 1: Performance (P@1) of unsupervised and minimally-supervised methods on the dataset of Conneau et al. (2018). The time columns shows the average runtime in minutes of an instance (i.e., one language pair) of the method in this task on the same quad-core CPU machine.
<R> <C> TMark <C> Disjunctive Model Features <C> Disjunctive Model Accuracy <C> Conjunctive Model Features <C> Conjunctive Model Accuracy <R> <C> after <C> NPRSTV <C> 69.9 <C> VWPTV <C> 79.6 <R> <C> as <C> ANNWPSV <C> 57.0 <C> VWVLSV <C> 57.0 <R> <C> before <C> SV <C> 42.1 <C> TV <C> 11.3 <R> <C> once <C> PRS <C> 40.7 <C> VWP <C> 3.7 <R> <C> since <C> PRST <C> 25.1 <C> VLV <C> 1.03 <R> <C> when <C> VLPS <C> 85.5 <C> VLNV <C> 86.5 <R> <C> while <C> PST <C> 49.0 <C> VLPV <C> 9.6 <R> <C> until <C> VLVWRT <C> 69.4 <C> VWVLPV <C> 9.5 <CAP> Table 8: Best feature combinations for individual markers (sentence interpretation; development set; V: verbs, VW: WordNet verb supersenses, VL: Levin verb classes, N: nouns, NW: WordNet noun supersenses, P: clause position, S: syntactic signature, R: argument signature)
<R> <C> TMark <C> Conjunctive Model Features <C> Conjunctive Model Accuracy <C> Disjunctive Model Features <C> Disjunctive Model Accuracy <R> <C> after <C> NR <C> 74.1 <C> AVVW <C> 77.9 <R> <C> as <C> NRSVW <C> 54.4 <C> AV <C> 75.8 <R> <C> before <C> NRVL <C> 65.5 <C> ANSTV <C> 85.4 <R> <C> once <C> ANNWSTVVW <C> 70.3 <C> RT <C> 100 <R> <C> since <C> NRVLVW <C> 60.5 <C> T <C> 85.2 <R> <C> when <C> NSTVW <C> 53.8 <C> RST <C> 86.9 <R> <C> while <C> ANSVW <C> 61.9 <C> SVW <C> 79.4 <R> <C> until <C> ANRVL <C> 65.5 <C> TV <C> 90.5 <CAP> Table 12: Best feature combinations for individual markers (sentence fusion; development set; V: verbs, VW: WordNet verb supersenses, VL: Levin verb classes, N: nouns, NW: WordNet noun supersenses, P: clause position, S: syntactic signature, R: argument signature)
<R> <C> vocab <C> prior <C> docs <C> DR non <C> DR lem <C> p-val Δ <R> <C> unfilt <C> sym <C> full <C> 0.54 <C> 0.52 <C> 0.61 <R> <C> [BOLD] filt <C> [BOLD] asym <C> [BOLD] full <C> [BOLD] 0.50 <C> [BOLD] 0.65 <C> [BOLD] 0.02 <R> <C> unfilt <C> sym <C> trunc <C> 0.37 <C> 0.37 <C> 0.50 <R> <C> filt <C> asym <C> trunc <C> 0.43 <C> 0.47 <C> 0.28 <CAP> Table 3: Detection rate for the non-lemmatized (non) and lemmatized (lem) models and p-values for the one-sided detection rate difference tests. (filt and unfilt indicate whether or not the vocabulary is filtered; sym and asym indicate whether the prior is symmetric, trunc and full indicate whether the documents are truncated.) The detection rate benefits significantly from lemmatization on a filtered vocabulary (highlighted in bold).
<R> <C> Approach <C> In_hosp <C> 30_days <C> 1_year <C> Pri_diag_cat <C> Pri_proc_cat <C> Gender <R> <C> BoW <C> 94.57 <C> 59.49 <C> 79.42 <C> 70.16 <C> 73.66 <C> 98.47 <R> <C> SDAE <C> 91.94 <C> 79.65 <C> 79.80 <C> 65.00 <C> 67.46 <C> 87.75 <R> <C> doc2vec <C> 91.95 <C> 76.80 <C> 81.34 <C> 68.07 <C> 65.83 <C> 97.70 <R> <C> ( [ITALIC] κ) SDAE + doc2vec <C> (58.65) 93.83 <C> (00.00) 81.13 <C> (15.81) 83.02 <C> (64.38) 67.88 <C> (58.91) 70.30 <C> (72.00) 97.47 <CAP> Table 1: Classification results on different tasks using the BoW features, the SDAE and the doc2vec patient representations, and on concatenating the two dense representations (with κ score).
<R> <C> [BOLD] Methods <C> [BOLD] WN11 <C> [BOLD] FB13 <C> [BOLD] AVG. <R> <C> LFM <C> 73.8 <C> 84.3 <C> 79.0 <R> <C> NTN <C> 70.4 <C> 87.1 <C> 78.8 <R> <C> TransE <C> 75.9 <C> 81.5 <C> 78.7 <R> <C> TransH <C> 78.8 <C> 83.3 <C> 81.1 <R> <C> TransR <C> 85.9 <C> 82.5 <C> 84.2 <R> <C> CTransR <C> 85.7 <C> [EMPTY] <C> [EMPTY] <R> <C> KG2E <C> 85.4 <C> 85.3 <C> 85.4 <R> <C> TransG <C> [BOLD] 87.4 <C> [BOLD] 87.3 <C> [BOLD] 87.4 <CAP> Table 5: Triple classification: accuracy(%) for different embedding methods.
<R> <C> [EMPTY] <C> YELP Gra <C> YELP Con <C> YELP Att <C> AMAZON Gra <C> AMAZON Con <C> AMAZON Att <R> <C> DeleteAndRetrieval <C> 3.4 <C> 3.5 <C> 3.6 <C> 3.5 <C> 3.2 <C> 3.3 <R> <C> w/frequency-ratio <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> AC-MLM-SS <C> 3.9 <C> 3.2 <C> 4.2 <C> 3.8 <C> 3.6 <C> 3.7 <R> <C> w/attention-based <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> AC-MLM-SS <C> 4.0 <C> 3.8 <C> [BOLD] 4.4 <C> 3.9 <C> 3.7 <C> 3.7 <R> <C> w/fusion-method <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> AC-MLM-SS <C> [BOLD] 4.2 <C> [BOLD] 4.0 <C> [BOLD] 4.4 <C> [BOLD] 4.1 <C> [BOLD] 4.0 <C> [BOLD] 4.0 <CAP> Table 5: Human evaluation results on two datasets. We show average human ratings for grammaticality (Gra), content preservation (Con), target attribute match (Att).
<R> <C> [EMPTY] <C> Sensigrafo <C> WordNet <R> <C> version <C> 14.2 <C> 3.0 <R> <C> words/lemmas <C> 400K <C> 155K <R> <C> syn(set/con) <C> 300K <C> 118K <R> <C> relations <C> 55 <C> 27 <R> <C> derived rel datasets <C> 149 <C> 27 <R> <C> pair types <C> lem2(lem,syn,POS) <C> lem2lem <R> <C> [EMPTY] <C> syn2(syn,POS) <C> [EMPTY] <CAP> Table 1: Lexical Knowledge Graphs used.
<R> <C> dataset rel <C> KG <C> datasets <C> datasets <C> models <C> models non-predictable <C> models non-predictable <C> models “predictable” <C> models “predictable” <C> metrics absolute <C> metrics relative <R> <C> [EMPTY] <C> [EMPTY] <C> # <C> # biased <C> # <C> % biased <C> % not signif. <C> [BOLD] % better <C> % worse <C> [ITALIC] μf1 <C> Δ [ITALIC] μf1 <R> <C> all <C> both <C> 156 <C> 38 <C> 1281 <C> [BOLD] 28.6 <C> [BOLD] 46.5 <C> [BOLD] 23.9 <C> [BOLD] 0.9 <C> 0.687 <C> 0.173 <R> <C> all <C> wn <C> 19 <C> 6 <C> 216 <C> 27.8 <C> 39.4 <C> 30.1 <C> 2.8 <C> 0.684 <C> 0.149 <R> <C> all <C> sensi <C> 137 <C> 32 <C> 1065 <C> 28.8 <C> 48. <C> 22.6 <C> 0.6 <C> 0.688 <C> 0.181 <R> <C> concept <C> sensi <C> 44 <C> 10 <C> 293 <C> 22.9 <C> [BOLD] 73.7 <C> [BOLD] 3.4 <C> 0. <C> [BOLD] 0.870 <C> [BOLD] 0.608 <R> <C> word/concept <C> sensi <C> 43 <C> 3 <C> 172 <C> [BOLD] 7. <C> 43.6 <C> [BOLD] 48.8 <C> 0.6 <C> 0.664 <C> 0.162 <R> <C> word <C> sensi <C> 50 <C> 19 <C> 600 <C> [BOLD] 38. <C> 36.7 <C> 24.5 <C> 0.8 <C> 0.690 <C> 0.162 <R> <C> lexicalw <C> wn <C> 4 <C> 0 <C> 32 <C> 0. <C> [BOLD] 75. <C> 25. <C> 0. <C> 0.652 <C> 0.167 <R> <C> hypernymc <C> sensi <C> 2 <C> 1 <C> 14 <C> 50. <C> 42.9 <C> 7.1 <C> 0. <C> [BOLD] 0.916 <C> [BOLD] 0.696 <R> <C> hypernymw/c <C> sensi <C> 2 <C> 1 <C> 8 <C> 50. <C> 0. <C> [ITALIC] 50. <C> 0. <C> 0.699 <C> 0.148 <R> <C> hypernymw <C> sensi <C> 2 <C> 1 <C> 24 <C> 50. <C> 0. <C> [ITALIC] 50. <C> 0. <C> 0.713 <C> 0.143 <R> <C> hypernymw <C> wn <C> 2 <C> 1 <C> 24 <C> 50. <C> 4.2 <C> 37.5 <C> [BOLD] 8.3 <C> [BOLD] 0.756 <C> 0.106 <R> <C> categc <C> sensi <C> 3 <C> 1 <C> 21 <C> 33.3 <C> 52.4 <C> 14.3 <C> 0. <C> [BOLD] 0.891 <C> [BOLD] 0.671 <R> <C> categw/c <C> sensi <C> 4 <C> 1 <C> 16 <C> 25. <C> 12.5 <C> [BOLD] 62.5 <C> 0. <C> 0.744 <C> 0.263 <R> <C> categw <C> sensi <C> 1 <C> 1 <C> 12 <C> [BOLD] 100. <C> 0. <C> 0. <C> 0. <C> [EMPTY] <C> [EMPTY] <R> <C> categw <C> wn <C> 4 <C> 4 <C> 40 <C> [BOLD] 100. <C> 0. <C> 0. <C> 0. <C> [EMPTY] <C> [EMPTY] <R> <C> meronymc <C> sensi <C> 2 <C> 0 <C> 14 <C> 0. <C> [BOLD] 92.9 <C> 7.1 <C> 0. <C> 0.667 <C> 0.020 <R> <C> meronymw/c <C> sensi <C> 1 <C> 0 <C> 4 <C> 0. <C> [BOLD] 50. <C> [ITALIC] 50. <C> 0. <C> 0.664 <C> 0.296 <R> <C> meronymw <C> sensi <C> 1 <C> 0 <C> 12 <C> 0. <C> [BOLD] 91.7 <C> 0. <C> [BOLD] 8.3 <C> [EMPTY] <C> [EMPTY] <R> <C> meronymw <C> wn <C> 3 <C> 0 <C> 48 <C> 0. <C> [BOLD] 66.7 <C> 31.2 <C> 2.1 <C> 0.708 <C> 0.166 <R> <C> synonc <C> sensi <C> 0 <C> 0 <C> 0 <C> 0. <C> 0. <C> 0. <C> 0. <C> [EMPTY] <C> [EMPTY] <R> <C> synonw/c <C> sensi <C> 1 <C> 0 <C> 4 <C> 0. <C> 25. <C> [BOLD] 75. <C> 0. <C> [BOLD] 0.804 <C> 0.249 <R> <C> synonw <C> sensi <C> 1 <C> 0 <C> 12 <C> 0. <C> 8.3 <C> [BOLD] 91.7 <C> 0. <C> 0.677 <C> 0.114 <R> <C> synonw <C> wn <C> 1 <C> 0 <C> 8 <C> 0. <C> 0. <C> [ITALIC] 100. <C> 0. <C> 0.680 <C> 0.135 <R> <C> similc <C> sensi <C> 6 <C> 1 <C> 42 <C> 16.7 <C> 81. <C> 2.4 <C> 0. <C> [BOLD] 0.909 <C> [BOLD] 0.688 <R> <C> similw/c <C> sensi <C> 7 <C> 0 <C> 28 <C> 0. <C> 57.1 <C> 42.9 <C> 0. <C> 0.624 <C> 0.210 <R> <C> similw <C> sensi <C> 7 <C> 1 <C> 84 <C> 14.3 <C> 38.1 <C> [BOLD] 47.6 <C> 0. <C> 0.628 <C> 0.128 <R> <C> similw <C> wn <C> 5 <C> 1 <C> 64 <C> 12.5 <C> 43.8 <C> 39.1 <C> [BOLD] 4.7 <C> 0.655 <C> 0.153 <R> <C> positionc <C> sensi <C> 6 <C> 1 <C> 42 <C> 16.7 <C> [BOLD] 78.6 <C> 4.8 <C> 0. <C> [BOLD] 0.886 <C> 0.667 <R> <C> positionw/c <C> sensi <C> 6 <C> 1 <C> 24 <C> 16.7 <C> 37.5 <C> [BOLD] 45.8 <C> 0. <C> 0.721 <C> 0.120 <R> <C> positionw <C> sensi <C> 7 <C> 5 <C> 84 <C> [BOLD] 71.4 <C> 21.4 <C> 6. <C> 1.2 <C> 0.703 <C> 0.069 <R> <C> preposc <C> sensi <C> 19 <C> 4 <C> 133 <C> 21.1 <C> [BOLD] 78.9 <C> 0. <C> 0. <C> [EMPTY] <C> [EMPTY] <R> <C> preposw/c <C> sensi <C> 22 <C> 0 <C> 88 <C> 0. <C> 51.1 <C> [BOLD] 47.7 <C> 1.1 <C> 0.629 <C> 0.124 <R> <C> preposw <C> sensi <C> 27 <C> 11 <C> 324 <C> 40.7 <C> 48.5 <C> 9.9 <C> 0.9 <C> 0.677 <C> 0.097 <R> <C> POSc <C> sensi <C> 5 <C> 1 <C> 20 <C> 20. <C> [BOLD] 70. <C> 10. <C> 0. <C> [BOLD] 0.882 <C> [BOLD] 0.665 <R> <C> POSw/c <C> sensi <C> 0 <C> 0 <C> 0 <C> 0. <C> 0. <C> 0. <C> 0. <C> [EMPTY] <C> [EMPTY] <R> <C> POSw <C> sensi <C> 4 <C> 0 <C> 48 <C> 0. <C> 2.1 <C> [BOLD] 97.9 <C> 0. <C> [BOLD] 0.746 <C> 0.262 <CAP> Table 3: Overview of results.
<R> <C> datasets <C> algo <C> F1avg <C> F1std <C> pair type <R> <C> [BOLD] 8 <C> HolE <C> [BOLD] 0.90 <C> 0.02 <C> concept <R> <C> 1 <C> Swivel <C> 0.85 <C> 0.0 <C> concept <R> <C> 1 <C> FastText <C> 0.67 <C> 0.0 <C> concept <R> <C> 26 <C> HolE <C> [BOLD] 0.67 <C> 0.09 <C> word/concept <R> <C> 48 <C> Vecsigrafo <C> 0.64 <C> 0.08 <C> word/concept <R> <C> 38 <C> FastText <C> [BOLD] 0.74 <C> 0.08 <C> word <R> <C> 12 <C> HolE <C> 0.72 <C> 0.09 <C> word <R> <C> 43 <C> Vecsigrafo <C> 0.68 <C> 0.07 <C> word <R> <C> 43 <C> GloVe <C> 0.68 <C> 0.08 <C> word <R> <C> 21 <C> Swivel <C> 0.66 <C> 0.06 <C> word <CAP> Table 4: Average F1 scores for predicting relations with different pair types and embeddings.
<R> <C> [EMPTY] <C> En→Vi BLEU <C> En→Vi BLEU <C> En→Tr BLEU <C> En→Tr BLEU <C> En→Es BLEU <C> En→Es BLEU <C> En→Es METEOR <C> En→Es METEOR <R> <C> [EMPTY] <C> Dev <C> Test <C> Dev <C> Test <C> Dev <C> Test <C> Dev <C> Test <R> <C> MT only <C> 22.83 <C> 24.15 <C> 8.55 <C> 8.5 <C> 14.49 <C> 13.44 <C> 31.3 <C> 31.1 <R> <C> MTL with Heuristic Schedule <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> + Uniform <C> 23.10 <C> 24.81 <C> 9.14 <C> 8.94 <C> 12.81 <C> 12.12 <C> 29.6 <C> 29.5 <R> <C> + Biased (Constant) <C> 23.42 <C> 25.22 <C> 10.06 <C> 9.53 <C> 15.14 <C> 14.11 <C> 31.8 <C> 31.3 <R> <C> + Exponential <C> 23.45 <C> 25.65 <C> 9.62 <C> 9.12 <C> 12.25 <C> 11.62 <C> 28.0 <C> 28.1 <R> <C> MTL with Adaptive Schedule <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> +  [BOLD] SN + heuristic <C> 23.86 <C> 25.70 <C> 10.53 <C> 10.18 <C> 13.20 <C> 12.38 <C> 29.9 <C> 29.7 <R> <C> +  [BOLD] SN only <C> [BOLD] 24.21 <C> [BOLD] 26.45 <C> [BOLD] 10.92 <C> [BOLD] 10.62 <C> [BOLD] 16.14 <C> [BOLD] 15.12 <C> [BOLD] 33.1 <C> [BOLD] 32.7 <CAP> Table 1: Results for three language pairs. “+ SN” indicates Scheduler Network is used in training.
<R> <C> Model <C> [BOLD] Structural measures mNNO <C> [BOLD] Structural measures  [ITALIC] ρvis <C> [BOLD] Structural measures  [ITALIC] Cinter <C> [BOLD] Structural measures  [ITALIC] Cintra <C> [BOLD] Semantic relatedness STS/All <C> [BOLD] Semantic relatedness STS/Cap <C> [BOLD] Semantic relatedness STS/News <C> [BOLD] Semantic relatedness STS/Forum <C> [BOLD] Semantic relatedness SICK <R> <C> [BOLD] T <C> 10.0 <C> 4.1 <C> 54.2 <C> 70.1 <C> 30 <C> 41 <C> 36 <C> 21 <C> 51 <R> <C> [BOLD] CM (text) <C> 24.2 <C> 12.8 <C> 41.7 <C> 74.8 <C> 52 <C> 76 <C> 42 <C> [BOLD] 37 <C> 55 <R> <C> [BOLD] P [ITALIC] id <C> 21.1 <C> [BOLD] 37.9 <C> 42.2 <C> 69.3 <C> 45 <C> 66 <C> 41 <C> 34 <C> 54 <R> <C> [BOLD] C [ITALIC] id <C> 27.5 <C> 10.5 <C> [BOLD] 2.9 <C> [BOLD] 84.7 <C> 60 <C> 83 <C> 45 <C> 20 <C> 55 <R> <C> [BOLD] C [ITALIC] id+ [BOLD] P [ITALIC] id <C> [BOLD] 27.9 <C> 25.8 <C> 6.7 <C> 82.6 <C> [BOLD] 61 <C> [BOLD] 84 <C> [BOLD] 46 <C> 28 <C> [BOLD] 57 <R> <C> [BOLD] CM (vis.) <C> 27.1 <C> 19.2 <C> 1.5 <C> 85.8 <C> 56 <C> 78 <C> 40 <C> 34 <C> 55 <R> <C> [BOLD] P [ITALIC] g <C> 21.3 <C> [BOLD] 32.4 <C> 43.9 <C> 73.3 <C> 45 <C> 66 <C> 41 <C> [BOLD] 37 <C> 53 <R> <C> [BOLD] C [ITALIC] g <C> 28.6 <C> 9.4 <C> [BOLD] 1.1 <C> [BOLD] 88.5 <C> 62 <C> 83 <C> 46 <C> 29 <C> 59 <R> <C> [BOLD] C [ITALIC] g+ [BOLD] P [ITALIC] g <C> [BOLD] 28.9 <C> 29.1 <C> 4.7 <C> 87.5 <C> [BOLD] 63 <C> [BOLD] 84 <C> [BOLD] 48 <C> 33 <C> [BOLD] 60 <CAP> Table 1: Intrinsic evaluations carried out on the grounded space for models with g=MLP; the textual space for T, CM (text) and models with g=id; and the visual space for CM (vis).
<R> <C> Model Kiros et al. ( 2015 )† <C> Model  [BOLD] T1024 <C> MR 72.7∗ <C> CR 75.2∗ <C> SUBJ 90.6∗ <C> MPQA 84.7∗ <C> MRPC 71.8∗/79.2∗ <C> SST 76.2∗ <C> SNLI 68.8∗ <C> SICK 79.3∗ <C> AVG 77.4 <R> <C> Kiela et al. ( 2018 )† <C> GS-Cap <C> 72.0∗ <C> 76.8∗ <C> 90.7∗ <C> 85.5∗ <C> 72.9/80.6 <C> 76.7∗ <C> 73.7 <C> [BOLD] 82.9 <C> 78.4 <R> <C> Kiela et al. ( 2018 )† <C> GS-Img <C> 74.5∗ <C> 79.3∗ <C> 90.8∗ <C> 87.8∗ <C> 73.0/80.3 <C> 80.0∗ <C> 72.2∗ <C> 80.9∗ <C> 79.8 <R> <C> Kiela et al. ( 2018 )† <C> GS-Both <C> 72.5∗ <C> 75.7∗ <C> 90.7∗ <C> 85.4∗ <C> 72.9/81.3 <C> 76.7∗ <C> 72.2∗ <C> 81.4∗ <C> 78.4 <R> <C> Kiros et al. ( 2015 )† <C> [BOLD] T <C> 75.9∗ <C> 79.2∗ <C> 92.0 <C> 86.7∗ <C> 72.2/80.2 <C> 81.8∗ <C> 72.0∗ <C> 81.1∗ <C> 80.1 <R> <C> Lazaridou et al. ( 2015a )‡ <C> [BOLD] T+ [BOLD] CM <C> 77.6 <C> 81.4 <C> 92.6 <C> 88.3 <C> 73.5/81.1 <C> 82.0∗ <C> 73.0 <C> 81.4∗ <C> 81.1 <R> <C> Collell et al. ( 2017 )‡ <C> [BOLD] SEQ <C> 76.1∗ <C> 79.8∗ <C> 92.5 <C> 86.7∗ <C> 70.0∗/79.5∗ <C> 81.7∗ <C> 67.3∗ <C> 76.7∗ <C> 78.9 <R> <C> Model scenarios <C> [BOLD] T+ [BOLD] P [ITALIC] id <C> 77.5 <C> 81.5 <C> 92.7 <C> 88.4 <C> [BOLD] 73.7/81.3 <C> 82.4 <C> 72.4 <C> 81.1 <C> 81.2 <R> <C> Model scenarios <C> [BOLD] T+ [BOLD] P [ITALIC] g <C> [BOLD] 77.8 <C> [BOLD] 81.8 <C> [BOLD] 93.0 <C> 88.1 <C> 73.3/ [BOLD] 81.6 <C> [BOLD] 83.5 <C> 72.8 <C> 82.2 <C> [BOLD] 81.6 <R> <C> Model scenarios <C> [BOLD] T+ [BOLD] C [ITALIC] id <C> 77.5 <C> 81.6 <C> 92.8 <C> 88.3 <C> 72.9/80.5 <C> 82.2 <C> 73.1 <C> 82.3 <C> 81.3 <R> <C> Model scenarios <C> [BOLD] T+ [BOLD] C [ITALIC] g <C> 77.3 <C> 81.5 <C> 92.8 <C> [BOLD] 88.6 <C> 73.6/81.1 <C> 82.6 <C> [BOLD] 74.1 <C> 82.6 <C> [BOLD] 81.6 <R> <C> Model scenarios <C> [BOLD] T+ [BOLD] C [ITALIC] id+ [BOLD] P [ITALIC] id <C> 77.3 <C> 81.2 <C> [BOLD] 93.0 <C> 88.4 <C> 73.0/80.6 <C> 82.5 <C> 73.5 <C> 82.1 <C> 81.4 <R> <C> Model scenarios <C> [BOLD] T+ [BOLD] C [ITALIC] g+ [BOLD] P [ITALIC] g <C> 77.4 <C> 81.5 <C> [BOLD] 93.0 <C> 88.1 <C> 73.2/80.9 <C> 82.7 <C> 73.9 <C> [BOLD] 82.9 <C> [BOLD] 81.6 <CAP> Table 3: Extrinsic evaluations with SentEval. All models give sentences in dimension dt=2048 (except T1024). ‘AVG’ stands for the average accuracies reported in the other columns. ‘†’: the model has been re-implemented (we obtained higher scores than the one given in the original papers). ‘‡’: the baseline is an adaptation of the model to the case of sentences. ’∗’: significantly differs from the best scenario among our models.
<R> <C> [BOLD] Approach <C> [BOLD] AUC <R> <C> PCNN+HATT <C> 0.42 <R> <C> PCNN+ATT-RA+BAG-ATT <C> 0.42 <R> <C> [BOLD] SeG (ours) <C> 0.51 <CAP> Table 3: Model comparison regarding the AUC value. The comparative results are reported by han2018hierarchical han2018hierarchical and ye2019distant ye2019distant respectively.
<R> <C> [BOLD] Approach <C> [BOLD] AUC <C> [BOLD] Acc. <R> <C> PCNN <C> 0.36 <C> 83% <R> <C> PCNN+ATT <C> 0.35 <C> 78% <R> <C> SeG(ours) <C> 0.48 <C> 90% <CAP> Table 4: Model that is trained and tested on extracted one sentence bags from NYT dataset comparison regarding the AUC value and Acc., where Acc. is accuracy on non-NA sentences.
<R> <C> System <C> BULATS <C> Linguaskill <R> <C> VoxCeleb x-vector/PLDA <C> 0.85 <C> 1.44 <R> <C> + PLDA adaptation (X1) <C> 0.55 <C> 0.62 <R> <C> + Extractor fine-tuning (X2) <C> 0.49 <C> 0.55 <CAP> Table 2: % EER performance of VoxCeleb-based systems on BULATS and Linguaskill test sets.
<R> <C> Grade Ref. Spkr. <C> Grade of Impostor Spkr. A1 <C> Grade of Impostor Spkr. A2 <C> Grade of Impostor Spkr. B1 <C> Grade of Impostor Spkr. B2 <C> Grade of Impostor Spkr. C <R> <C> A1 <C> 65.8 <C> 27.5 <C> 5.8 <C> 0.3 <C> 0.6 <R> <C> A2 <C> 60.9 <C> 29.9 <C> 7.1 <C> 0.9 <C> 1.3 <R> <C> B1 <C> 46.5 <C> 26.8 <C> 13.1 <C> 7.6 <C> 5.9 <R> <C> B2 <C> 11.4 <C> 11.9 <C> 19.2 <C> 25.9 <C> 31.7 <R> <C> C <C> 17.7 <C> 12.0 <C> 10.3 <C> 24.3 <C> 35.6 <CAP> Table 3: Grade breakdown of the percentage of impostor trials with FA errors at the operating threshold of EER for the extractor fine-tuning system on a subset of the BULATS test set.
<R> <C> Model <C> Domain <C> [BOLD] Sw-En En <C> [BOLD] Sw-En Sw <C> [BOLD] Am-En En <C> [BOLD] Am-En Am <C> [BOLD] Ti-En En <C> [BOLD] Ti-En Ti <C> [BOLD] Om-En En <C> [BOLD] Om-En Om <C> [BOLD] So-En En <C> [BOLD] So-En So <R> <C> S-NMT <C> Jw300 <C> 48.71 <C> 47.58 <C> 32.86 <C> 25.72 <C> 29.89 <C> 25.54 <C> 26.92 <C> 23.38 <C> [EMPTY] <C> [EMPTY] <R> <C> S-NMT <C> Bible <C> [EMPTY] <C> [EMPTY] <C> 30.35 <C> 23.36 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 29.87 <C> 24.64 <R> <C> S-NMT <C> Tanzil <C> 18.83 <C> 31.67 <C> 11.71 <C> 5.71 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 8.91 <C> 2.46 <R> <C> S-NMT <C> Ted <C> 16.63 <C> 11.92 <C> 4.26 <C> 1.32 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 1.35 <C> 0.39 <R> <C> S-NMT <C> AVG <C> 28.06 <C> 30.39 <C> 19.80 <C> 14.03 <C> 29.89 <C> 25.54 <C> 26.92 <C> 23.38 <C> 13.38 <C> 9.16 <R> <C> SS-NMT <C> Jw300 <C> 48.90 <C> 47.45 <C> 32.76 <C> 26.54 <C> 29.84 <C> 25.99 <C> 26.45 <C> 23.47 <C> [EMPTY] <C> [EMPTY] <R> <C> SS-NMT <C> Bible <C> [EMPTY] <C> [EMPTY] <C> 30.53 <C> 24.21 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 27.68 <C> 22.89 <R> <C> SS-NMT <C> Tanzil <C> 19.44 <C> 32.17 <C> 12.55 <C> 7.29 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 6.75 <C> 2.25 <R> <C> SS-NMT <C> Ted <C> 18.62 <C> 14.72 <C> 6.92 <C> 1.41 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 1.21 <C> 0.52 <R> <C> SS-NMT <C> AVG <C> 28.99 <C> [BOLD] 31.45 <C> 20.69 <C> [BOLD] 14.86 <C> 29.84 <C> 25.99 <C> 26.45 <C> 23.47 <C> 11.88 <C> 8.55 <R> <C> TL <C> Jw300 <C> 48.74 <C> 47.39 <C> 32.95 <C> 26.49 <C> 29.81 <C> 26.47 <C> 27.77 <C> 24.54 <C> [EMPTY] <C> [EMPTY] <R> <C> TL <C> Bible <C> [EMPTY] <C> [EMPTY] <C> 30.36 <C> 24.26 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 32.07 <C> 27.67 <R> <C> TL <C> Tanzil <C> 19.9 <C> 31.78 <C> 12.28 <C> 7.34 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 10.14 <C> 3.34 <R> <C> TL <C> Ted <C> 19.74 <C> 14.81 <C> 7.42 <C> 1.31 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 1.97 <C> 0.56 <R> <C> TL <C> AVG <C> [BOLD] 29.46 <C> 31.33 <C> 20.75 <C> 14.85 <C> 29.81 <C> [BOLD] 26.47 <C> 27.77 <C> 24.54 <C> 14.73 <C> 10.52 <R> <C> M-NMT <C> Jw300 <C> 46.62 <C> 44.47 <C> 33.21 <C> 24.39 <C> 32.21 <C> 26.4 <C> 32.24 <C> 24.96 <C> [EMPTY] <C> [EMPTY] <R> <C> M-NMT <C> Bible <C> [EMPTY] <C> [EMPTY] <C> 29.78 <C> 20.01 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 34.99 <C> 28.76 <R> <C> M-NMT <C> Tanzil <C> 18.75 <C> 24.22 <C> 13.68 <C> 10.95 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 12.68 <C> 3.73 <R> <C> M-NMT <C> Ted <C> 17.54 <C> 14.65 <C> 6.78 <C> 1.32 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 3.09 <C> 1.01 <R> <C> M-NMT <C> AVG <C> 27.64 <C> 27.78 <C> [BOLD] 20.86 <C> 14.17 <C> [BOLD] 32.21 <C> 26.40 <C> [BOLD] 32.24 <C> [BOLD] 24.96 <C> [BOLD] 16.92 <C> [BOLD] 11.17 <CAP> Table 1: BLEU scores for the SATOS ↔ En directions, domain-specific best performing results are highlighted for each direction, whereas bold shows the overall best in terms of the AVG score.
<R> <C> Models <C> [ITALIC] R \textendash 1 [ITALIC] P <C> [ITALIC] R \textendash 1 [ITALIC] R <C> [ITALIC] R \textendash 1 [ITALIC] F1 <R> <C> TextRank <C> 0.430 <C> 0.219 <C> 0.290 <R> <C> BiLSTM-Net <C> 0.637 <C> 0.751 <C> 0.689 <R> <C> Pointer-Net <C> 0.648 <C> 0.746 <C> 0.694 <R> <C> Feature-Enriched-Net <C> [BOLD] 0.675 <C> [BOLD] 0.783 <C> [BOLD] 0.725 <CAP> Table 2: Final results on the test set. We report ROUGE-1 Precision, Recall and corresponding F1. We use the tuned threshold τ=0.4 (see Figure 4) for BiLSTM-Net and Feature-Enriched-Net. Best ROUGE score in each column is highlighted in boldface.
<R> <C> [BOLD] Model <C> [BOLD] antecedent distance 0 <C> [BOLD] antecedent distance 1 <C> [BOLD] antecedent distance 2 <C> [BOLD] antecedent distance 3 <C> [BOLD] antecedent distance >3 <R> <C> [EMPTY] <C> Offline document MT <C> Offline document MT <C> Offline document MT <C> Offline document MT <C> Offline document MT <R> <C> RNNSearch <C> 0.415 <C> 0.310 <C> 0.424 <C> 0.440 <C> 0.647 <R> <C> Transformer <C> 0.586 <C> 0.308 <C> 0.437 <C> 0.48 <C> 0.642 <R> <C> +Attention, sentence <C> 0.677 <C> 0.314 <C> 0.439 <C> 0.478 <C> 0.697 <R> <C> word <C> [BOLD] 0.686 <C> [BOLD] 0.347 <C> [BOLD] 0.464 <C> [BOLD] 0.511 <C> 0.679 <R> <C> +H-Attention, sparse-soft <C> 0.676 <C> 0.308 <C> 0.440 <C> 0.480 <C> 0.686 <R> <C> sparse-sparse <C> 0.652 <C> 0.303 <C> 0.435 <C> 0.471 <C> [BOLD] 0.701 <R> <C> [EMPTY] <C> Online document MT <C> Online document MT <C> Online document MT <C> Online document MT <C> Online document MT <R> <C> Zhang:18 <C> 0.622 <C> 0.321 <C> 0.450 <C> 0.485 <C> 0.658 <R> <C> Miculicich:18 <C> 0.722 <C> 0.326 <C> 0.451 <C> 0.471 <C> 0.661 <R> <C> Transformer <C> 0.586 <C> 0.308 <C> 0.437 <C> 0.48 <C> 0.642 <R> <C> +Attention, sentence <C> [BOLD] 0.732 <C> [BOLD] 0.340 <C> [BOLD] 0.460 <C> 0.485 <C> 0.661 <R> <C> word <C> 0.690 <C> 0.317 <C> 0.444 <C> 0.487 <C> 0.683 <R> <C> +H-Attention, sparse-soft <C> 0.692 <C> 0.329 <C> 0.446 <C> 0.464 <C> 0.656 <R> <C> sparse-sparse <C> 0.711 <C> 0.317 <C> 0.437 <C> [BOLD] 0.489 <C> [BOLD] 0.692 <CAP> Table 4: Accuracy on contrastive test set with regard to antecedent distance (in sentences) on TED Talks. Antecedent distance 0 means the pronoun occurs in the same sentence as the antecedent.
<R> <C> [BOLD] Model <C> [BOLD] #Params <C> [BOLD] Speed (words/sec.) Training <C> [BOLD] Speed (words/sec.) Decoding <R> <C> Zhang:18 <C> 59.5M <C> 3300 <C> 84.94 <R> <C> Miculicich:18 <C> 54.8M <C> 1650 <C> 76.90 <R> <C> Transformer <C> 50M <C> 5100 <C> 86.33 <R> <C> +Attention, sentence <C> 53.7M <C> 3750 <C> 83.84 <R> <C> +H-Attention, sparse-soft <C> 54.2M <C> 2600 <C> 74.11 <CAP> Table 5: Model complexity for Encoder Context integration models (News-Commentary).
<R> <C> [EMPTY] <C> en-fr <C> en-es <C> en-de <R> <C> base <C> 38.05 <C> 39.89 <C> 26.46 <R> <C> spk_token <C> [BOLD] 38.85 <C> 40.04 <C> 26.52 <R> <C> full_bias <C> [BOLD] 38.54 <C> [BOLD] 40.30 <C> [BOLD] 27.20 <R> <C> fact_bias <C> [BOLD] 39.01 <C> 39.88 <C> [BOLD] 26.94 <CAP> Table 3: Test BLEU. Scores significantly (p<0.05) better than the baseline are written in bold
<R> <C> [EMPTY] <C> en-de <R> <C> base <C> 26.04 <R> <C> spk_token <C> 26.49 <R> <C> full_bias <C> 26.44 <R> <C> fact_bias <C> [BOLD] 26.87 <CAP> Table 4: Test BLEU on the Europarl corpus. Scores significantly (p<0.05) better than the baseline are written in bold
<R> <C> Features <C> [BOLD] ( [ITALIC] p-value) with Alternative Hypothesis Not equal to 0 <C> [BOLD] ( [ITALIC] p-value) with Alternative Hypothesis Less than 0 <C> [BOLD] ( [ITALIC] p-value) with Alternative Hypothesis Greater than 0 <R> <C> [EMPTY] <C> User’s history features <C> User’s history features <C> User’s history features <R> <C> Entropy of ratings <C> 2.8e-05 <C> 1 <C> 1.4e-05 <R> <C> Number of reviewed products <C> 0.0492 <C> 0.0246 <C> 0.9754 <R> <C> Ratio of positive ratings <C> 0.0003 <C> 0.0001 <C> 0.9998 <R> <C> [BOLD] Ratio of negative ratings <C> 0.1007 <C> 0.9497 <C> 0.0503 <R> <C> Number of helpful votes <C> 0.0001 <C> 5.3e-05 <C> 0.9999 <R> <C> Sum unhelp <C> 4.2e-06 <C> 2.1e-06 <C> 1 <R> <C> Average help <C> 8.1e-08 <C> 4.1e-08 <C> 1 <R> <C> Average unhelp <C> 1.5e-11 <C> 7.3e-12 <C> 1 <R> <C> Time gap <C> 0.0024 <C> 0.0012 <C> 0.9988 <R> <C> Entropy of rating time <C> 0.0006 <C> 0.0003 <C> 0.9997 <R> <C> Active ratio <C> 0.0085 <C> 0.9958 <C> 0.0042 <R> <C> [BOLD] Memo length <C> 0.8915 <C> 0.5544 <C> 0.4457 <R> <C> Mean rate <C> 0.0004 <C> 0.0002 <C> 0.9998 <R> <C> [EMPTY] <C> User’s review data <C> User’s review data <C> User’s review data <R> <C> Overall product score <C> 5.8e-09 <C> 2.9e-09 <C> 1 <R> <C> Number of comments <C> 2.2e-16 <C> 1 <C> 2.2e-16 <R> <C> [BOLD] Number of first day’s comments <C> 0.4217 <C> 0.7892 <C> 0.2108 <R> <C> Product’s comment time gap <C> 2.2e-16 <C> 2.2e-16 <C> 1 <R> <C> Comment rank <C> 2.2e-16 <C> 1 <C> 2.2e-16 <R> <C> Comment rank ratio <C> 2.2e-16 <C> 1 <C> 2.2e-16 <R> <C> User help <C> 2.2e-16 <C> 2.2e-16 <C> 1 <R> <C> User unhelp <C> 2.2e-16 <C> 2.2e-16 <C> 1 <R> <C> Length of the summary <C> 2.2e-16 <C> 1 <C> 2.2e-16 <R> <C> [BOLD] Length of the review text <C> 0.5556 <C> 0.2778 <C> 0.7222 <R> <C> User comment time gap <C> 5.4e-05 <C> 2.7e-05 <C> 1 <R> <C> User comment time gap ratio <C> 2.2e-16 <C> 1 <C> 2.2e-16 <CAP> Table 2: p-value of Wilcoxon Signed-Rank Test for Continuous Features
<R> <C> Journal <C> #articles <C> #figures <C> #image-text pairs <R> <C> AAI <C> 94 <C> 1,217 <C> 3,180 <R> <C> ACISC <C> 185 <C> 2,215 <C> 5,453 <R> <C> AM <C> 144 <C> 2,304 <C> 6,057 <R> <C> MPE <C> 8,251 <C> 106,435 <C> 273,367 <R> <C> Sum <C> 8,674 <C> 112,171 <C> 288,057 <CAP> Table 2: Overview of the training data used for the autoencoder.
<R> <C> Data <C> Number of words/million Target LM# <C> Number of words/million Source LM <C> Number of words/million Punct TM <C> Number of words/million TM <R> <C> TED <C> 3.17 <C> 3.17 <C> 3.17 <C> 3.17 <R> <C> News Commentary <C> 4.0 <C> 0.9 <C> 0.2 <C> 0.7 <R> <C> Common crawl <C> 70.7 <C> 36.1 <C> 3.6 <C> 10.8 <R> <C> Gigaword <C> 575.7 <C> 271.2 <C> 26.3 <C> 14.9 <R> <C> Europarl <C> 50.3 <C> 10.8 <C> 4.3 <C> 1.9 <CAP> Table 2: Amount of text data used in different training tasks in En→Fr translation (#Full data set was used for builing target LM)
<R> <C> Decoder <C> Tst11 WER <C> Tst11 RT <C> Tst12 WER <C> Tst12 RT <R> <C> Tree-search <C> 23.7% <C> 18.4 <C> 27.0% <C> 19.8 <R> <C> WFST <C> 23.7% <C> 3.0 <C> 27.0% <C> 3.3 <CAP> Table 3: Tree-search and WFST decoder
<R> <C> [BOLD] Dataset/ Model <C> [BOLD] ChnSentiCorp Acc. <C> [BOLD] ChnSentiCorp FLOPs (speedup) <C> [BOLD] Book review Acc. <C> [BOLD] Book review FLOPs (speedup) <C> [BOLD] Shopping review Acc. <C> [BOLD] Shopping review FLOPs (speedup) <C> [BOLD] LCQMC Acc. <C> [BOLD] LCQMC FLOPs (speedup) <C> [BOLD] Weibo Acc. <C> [BOLD] Weibo FLOPs (speedup) <C> [BOLD] THUCNews Acc. <C> [BOLD] THUCNews FLOPs (speedup) <R> <C> BERT <C> 95.25 <C> 21785M (1.00x) <C> 86.88 <C> 21785M (1.00x) <C> 96.84 <C> 21785M (1.00x) <C> 86.68 <C> 21785M (1.00x) <C> 97.69 <C> 21785M (1.00x) <C> 96.71 <C> 21785M (1.00x) <R> <C> DistilBERT (6 layers) <C> 88.58 <C> 10918M (2.00x) <C> 83.31 <C> 10918M (2.00x) <C> 95.40 <C> 10918M (2.00x) <C> 84.12 <C> 10918M (2.00x) <C> 97.69 <C> 10918M (2.00x) <C> 95.54 <C> 10918M (2.00x) <R> <C> DistilBERT (3 layers) <C> 87.33 <C> 5428M (4.01x) <C> 81.17 <C> 5428M (4.01x) <C> 94.84 <C> 5428M (4.01x) <C> 84.07 <C> 5428M (4.01x) <C> 97.58 <C> 5428M (4.01x) <C> 95.14 <C> 5428M (4.01x) <R> <C> DistilBERT (1 layers) <C> 81.33 <C> 1858M (11.72x) <C> 77.40 <C> 1858M (11.72x) <C> 91.35 <C> 1858M (11.72x) <C> 71.34 <C> 1858M (11.72x) <C> 96.90 <C> 1858M (11.72x) <C> 91.13 <C> 1858M (11.72x) <R> <C> FastBERT (speed=0.1) <C> 95.25 <C> 10741M (2.02x) <C> 86.88 <C> 13613M (1.60x) <C> 96.79 <C> 4885M (4.45x) <C> 86.59 <C> 12930M (1.68x) <C> 97.71 <C> 3691M (5.90x) <C> 96.71 <C> 3595M (6.05x) <R> <C> FastBERT (speed=0.5) <C> 92.00 <C> 3191M (6.82x) <C> 86.64 <C> 5170M (4.21x) <C> 96.42 <C> 2517M (8.65x) <C> 84.05 <C> 6352M (3.42x) <C> 97.72 <C> 3341M (6.51x) <C> 95.64 <C> 1979M (11.00x) <R> <C> FastBERT (speed=0.8) <C> 89.75 <C> 2315M (9.40x) <C> 85.14 <C> 3012M (7.23x) <C> 95.72 <C> 2087M (10.04x) <C> 77.45 <C> 3310M (6.57x) <C> 97.69 <C> 1982M (10.09x) <C> 94.97 <C> 1854M (11.74x) <R> <C> [BOLD] Dataset/ Model <C> [BOLD] Ag.news <C> [BOLD] Ag.news <C> [BOLD] Amz.F <C> [BOLD] Amz.F <C> [BOLD] Dbpedia <C> [BOLD] Dbpedia <C> [BOLD] Yahoo <C> [BOLD] Yahoo <C> [BOLD] Yelp.F <C> [BOLD] Yelp.F <C> [BOLD] Yelp.P <C> [BOLD] Yelp.P <R> <C> [BOLD] Dataset/ Model <C> Acc. <C> FLOPs (speedup) <C> Acc. <C> FLOPs (speedup) <C> Acc. <C> FLOPs (speedup) <C> Acc. <C> FLOPs (speedup) <C> Acc. <C> FLOPs (speedup) <C> Acc. <C> FLOPs (speedup) <R> <C> BERT <C> 94.47 <C> 21785M (1.00x) <C> 65.50 <C> 21785M (1.00x) <C> 99.31 <C> 21785M (1.00x) <C> 77.36 <C> 21785M (1.00x) <C> 65.93 <C> 21785M (1.00x) <C> 96.04 <C> 21785M (1.00x) <R> <C> DistilBERT (6 layers) <C> 94.64 <C> 10872M (2.00x) <C> 64.05 <C> 10872M (2.00x) <C> 99.10 <C> 10872M (2.00x) <C> 76.73 <C> 10872M (2.00x) <C> 64.25 <C> 10872M (2.00x) <C> 95.31 <C> 10872M (2.00x) <R> <C> DistilBERT (3 layers) <C> 93.98 <C> 5436M (4.00x) <C> 63.84 <C> 5436M (4.00x) <C> 99.05 <C> 5436M (4.00x) <C> 76.56 <C> 5436M (4.00x) <C> 63.50 <C> 5436M (4.00x) <C> 93.23 <C> 5436M (4.00x) <R> <C> DistilBERT (1 layers) <C> 92.88 <C> 1816M (12.00x) <C> 59.48 <C> 1816M (12.00x) <C> 98.95 <C> 1816M (12.00x) <C> 74.93 <C> 1816M (12.00x) <C> 58.59 <C> 1816M (12.00x) <C> 91.59 <C> 1816M (12.00x) <R> <C> FastBERT (speed=0.1) <C> 94.38 <C> 6013M (3.62x) <C> 65.50 <C> 21005M (1.03x) <C> 99.28 <C> 2060M (10.57x) <C> 77.37 <C> 16172M (1.30x) <C> 65.93 <C> 20659M (1.05x) <C> 95.99 <C> 6668M (3.26x) <R> <C> FastBERT (speed=0.5) <C> 93.14 <C> 2108M (10.33x) <C> 64.64 <C> 10047M (2.16x) <C> 99.05 <C> 1854M (11.74x) <C> 76.57 <C> 4852M (4.48x) <C> 64.73 <C> 9827M (2.21x) <C> 95.32 <C> 3456M (6.30x) <R> <C> FastBERT (speed=0.8) <C> 92.53 <C> 1858M (11.72x) <C> 61.70 <C> 2356M (9.24x) <C> 99.04 <C> 1853M (11.75x) <C> 75.05 <C> 1965M (11.08x) <C> 60.66 <C> 2602M (8.37x) <C> 94.31 <C> 2460M (8.85x) <CAP> Table 2: Comparison of accuracy (Acc.) and FLOPs (speedup) between FastBERT and Baselines in six Chinese datasets and six English datasets.
<R> <C> [EMPTY] <C> Polarity <C> Anger <C> Disgust <C> Fear <C> Joy <C> Sadness <C> Surprise <R> <C> Apparel <C> 1.01 <C> 0.86 <C> 0.73 <C> 0.63 <C> 0.94 <C> 0.96 <C> 0.98 <R> <C> Automotive <C> 1.00 <C> 1.14 <C> 1.75 <C> 1.12 <C> 1.01 <C> 1.05 <C> 1.02 <R> <C> Cosmetics <C> 1.00 <C> 0.96 <C> 0.88 <C> 0.83 <C> 1.02 <C> 1.05 <C> 1.08 <R> <C> Electronics <C> 1.00 <C> 0.55 <C> 0.74 <C> 0.77 <C> 0.83 <C> 1.05 <C> 0.98 <R> <C> Food/Beverages <C> 1.01 <C> 0.73 <C> 1.20 <C> 0.72 <C> 1.00 <C> 0.96 <C> 1.24 <R> <C> Sports <C> 1.00 <C> 0.94 <C> 1.94 <C> 0.96 <C> 1.03 <C> 1.05 <C> 0.99 <R> <C> Other <C> 1.02 <C> 1.04 <C> 0.52 <C> 0.85 <C> 1.07 <C> 0.82 <C> 0.90 <R> <C> [ITALIC] σ <C> 0.01 <C> 0.28 <C> 0.30 <C> 0.26 <C> 0.07 <C> 0.09 <C> 0.09 <CAP> Table 3: Varying functions of emotions as indicated by incidence rate ratios for message sentiment.
<R> <C> [EMPTY] <C> [EMPTY] <C> PTB TA <C> PTB UAS <C> PTB LAS <C> CoNLL09 TA <C> CoNLL09 UAS <C> CoNLL09 LAS <C> CTB7 TA <C> CTB7 UAS <C> CTB7 LAS <R> <C> w/o BERT <C> andor-etal-2016-global <C> 97.44 <C> 94.61 <C> 92.79 <C> - <C> 84.72 <C> 80.85 <C> - <C> - <C> - <R> <C> w/o BERT <C> Timothy-d17-biaffine <C> 97.3 <C> 95.74 <C> 94.08 <C> - <C> 88.90 <C> 85.38 <C> - <C> - <C> - <R> <C> w/o BERT <C> ji-etal-2019-graph <C> 97.3 <C> 95.97 <C> 94.31 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> w/o BERT <C> li-etal-2019-attentive <C> 97.3 <C> 95.93 <C> 94.19 <C> - <C> 88.77 <C> 85.58 <C> - <C> - <C> - <R> <C> w/o BERT <C> Basic (\mathbf{e}^{w}\oplus\mathbf{e}^{c}) <C> 97.50 <C> 95.97 <C> 94.34 <C> 96.42 <C> 89.12 <C> 86.00 <C> 96.48 <C> 88.54 <C> 85.34 <R> <C> w/o BERT <C> Pipeline (\mathbf{e}^{w}\oplus\mathbf{e}^{c}\oplus\mathbf{e}^{p}) <C> 97.50 <C> 95.88 <C> 94.27 <C> 96.42 <C> 89.12 <C> 85.98 <C> 96.48 <C> 88.42 <C> 85.28 <R> <C> w/o BERT <C> Joint Stack-Hidden <C> [BOLD] 97.91 <C> [BOLD] 96.13 <C> [BOLD] 94.53 <C> 96.55 <C> 89.46 <C> 86.44 <C> 96.62 <C> 88.86 <C> 85.88 <R> <C> [EMPTY] <C> Joint Stack-Hidden w/ hetero <C> - <C> - <C> - <C> [BOLD] 96.66 <C> [BOLD] 89.85 <C> [BOLD] 86.85 <C> [BOLD] 96.71 <C> [BOLD] 89.21 <C> [BOLD] 86.22 <R> <C> w/ BERT <C> li-etal-2019-attentive <C> - <C> 96.67 <C> 95.03 <C> - <C> 92.24 <C> 89.29 <C> - <C> - <C> - <R> <C> w/ BERT <C> Basic (\mathbf{e}^{w}\oplus\mathbf{e}^{c}) <C> 97.42 <C> 96.85 <C> 95.14 <C> 97.29 <C> 92.21 <C> 89.42 <C> 97.22 <C> 91.66 <C> 88.75 <R> <C> w/ BERT <C> Joint Stack-Hidden <C> [BOLD] 97.57 <C> [BOLD] 96.85 <C> [BOLD] 95.25 <C> 97.36 <C> 92.44 <C> 89.68 <C> 97.32 <C> 91.67 <C> 88.84 <R> <C> w/ BERT <C> Joint Stack-Hidden w/ hetero <C> - <C> - <C> - <C> [BOLD] 97.39 <C> [BOLD] 92.46 <C> [BOLD] 89.76 <C> [BOLD] 97.40 <C> [BOLD] 91.81 <C> [BOLD] 89.04 <CAP> Table 4: Final results on the test data.
<R> <C> [EMPTY] <C> PMI-LM <C> NCE-LM <R> <C> PTB <C> [BOLD] 98.35 <C> 104.33 <R> <C> WMT <C> [BOLD] 65.84 <C> 69.28 <CAP> Table 2: Perplexity results on test sets.
<R> <C> [EMPTY] <C> BBC <C> Guardian <C> DUC ’04 <C> TAC ’08A <R> <C> [ITALIC] THJS <C> 0.5917 <C> 0.5689 <C> 0.3019 <C> 0.3188 <CAP> Table 4: Average textual heterogeneity of our corpora compared to standard datasets
<R> <C> [EMPTY] <C> \textarc  [ITALIC] m <C> Val Acc <C> Gen Acc <R> <C> LSTM <C> [ITALIC] O( [ITALIC] nk) <C> 94.0 <C> 51.6 <R> <C> LSTM-Attn <C> 2Θ( [ITALIC] n) <C> 100.0 <C> 51.7 <R> <C> LSTM <C> [ITALIC] O( [ITALIC] nk) <C> 92.5 <C> 73.3 <R> <C> StackNN <C> 2Θ( [ITALIC] n) <C> 100.0 <C> 100.0 <CAP> Table 2: Max validation and generalization accuracies on string reversal over 10 trials. The top section shows our seq2seq LSTM with and without attention. The bottom reports the LSTM and StackNN results of Hao et al. (2018). Each LSTM has 10 hidden units.
<R> <C> Model <C> Weibo Acceptable <C> Weibo Good <C> Weibo BLEU-1/2 <C> Weibo Dist-1/2 <C> Reddit Acceptable <C> Reddit Good <C> Reddit BLEU-1/2 <C> Reddit Dist-1/2 <R> <C> Seq2Seq <C> 0.43 <C> 0.08 <C> [BOLD] 0.305/0.246 <C> 0.122/0.326 <C> 0.57 <C> 0.10 <C> 0.205/0.162 <C> 0.091/0.254 <R> <C> MMI-bidi <C> 0.46 <C> 0.09 <C> 0.271/0.218 <C> 0.153/0.372 <C> 0.54 <C> 0.25 <C> [BOLD] 0.345/ [BOLD] 0.279 <C> 0.107/0.325 <R> <C> CVAE <C> 0.29 <C> 0.15 <C> 0.252/0.203 <C> 0.184/0.542 <C> 0.42 <C> 0.25 <C> 0.287/0.233 <C> 0.107/0.428 <R> <C> MARM <C> 0.48 <C> 0.11 <C> 0.304/0.245 <C> 0.132/0.376 <C> 0.60 <C> 0.09 <C> 0.205/0.162 <C> 0.100/0.287 <R> <C> MHAM <C> 0.50 <C> 0.10 <C> 0.304/0.245 <C> 0.127/0.347 <C> 0.60 <C> 0.10 <C> 0.192/0.151 <C> 0.115/0.331 <R> <C> MMPMS <C> [BOLD] 0.56 <C> [BOLD] 0.24 <C> 0.275/0.225 <C> [BOLD] 0.189/0.553 <C> [BOLD] 0.65 <C> [BOLD] 0.36 <C> 0.207/0.165 <C> [BOLD] 0.135/ [BOLD] 0.433 <CAP> Table 2: Evaluation results of single response generation on Weibo and Reddit dataset.
<R> <C> Input <C> Features <C> Acc [%] <C> Spearman <R> <C> Video frames <C> ResNet50 mean <C> 68.17 <C> 0.524 <R> <C> Video frames <C> + attention <C> 68.87 <C> 0.526 <R> <C> Headline <C> biLSTM  <C> 69.47 <C> 0.542 <R> <C> Headline <C> + attention <C> 68.70 <C> 0.525 <R> <C> Multimodal <C> ResNet + biLSTM <C> 71.94 <C> [BOLD] 0.612 <R> <C> Multimodal <C> + attention <C> [BOLD] 72.72 <C> 0.607 <CAP> Table 1: Video popularity prediction results.
<R> <C> [BOLD] EmotionPush <C> [BOLD] Train <C> [BOLD] # utterances 14742 <C> [BOLD] non-neutral 1418 <C> [BOLD] neutral 9855 <C> [BOLD] joy 2100 <C> [BOLD] sadness 514 <C> [BOLD] anger 140 <C> [BOLD] disgust 106 <C> [BOLD] fear 42 <C> [BOLD] surprise 567 <R> <C> [EMPTY] <C> [BOLD] Test <C> 3536 <C> 394 <C> 2146 <C> 601 <C> 110 <C> 27 <C> 51 <C> 18 <C> 189 <R> <C> [EMPTY] <C> [BOLD] Subtotal <C> 18278 <C> 1812 <C> 12001 <C> 2701 <C> 624 <C> 167 <C> 157 <C> 60 <C> 756 <R> <C> [BOLD] Friends <C> [BOLD] Train <C> 14503 <C> 2772 <C> 6530 <C> 1710 <C> 498 <C> 759 <C> 331 <C> 246 <C> 1657 <R> <C> [EMPTY] <C> [BOLD] Test <C> 3296 <C> 952 <C> 1035 <C> 505 <C> 121 <C> 141 <C> 58 <C> 37 <C> 447 <R> <C> [EMPTY] <C> [BOLD] Subtotal <C> 17799 <C> 3724 <C> 7565 <C> 2215 <C> 619 <C> 900 <C> 389 <C> 283 <C> 2104 <R> <C> [BOLD] Both <C> [BOLD] Train <C> 29245 <C> 4190 <C> 16385 <C> 3810 <C> 1012 <C> 899 <C> 437 <C> 288 <C> 2224 <R> <C> [EMPTY] <C> [BOLD] Test <C> 6832 <C> 1346 <C> 3181 <C> 1106 <C> 231 <C> 168 <C> 109 <C> 55 <C> 636 <R> <C> [EMPTY] <C> [BOLD] Total <C> [BOLD] 36077 <C> [BOLD] 5536 <C> [BOLD] 19566 <C> [BOLD] 4916 <C> [BOLD] 1243 <C> [BOLD] 1067 <C> [BOLD] 546 <C> [BOLD] 343 <C> [BOLD] 2860 <CAP> Table 2: Emotion Label Distribution
<R> <C> [EMPTY] <C> [BOLD] Friends <C> [BOLD] EmotionPush <C> [BOLD] Both <R> <C> Joy <C> 0.401 <C> 0.454 <C> 0.429 <R> <C> Neutral <C> 0.350 <C> 0.349 <C> 0.373 <R> <C> Surprise <C> 0.330 <C> 0.266 <C> 0.322 <R> <C> Sadness <C> 0.291 <C> 0.324 <C> 0.307 <R> <C> Anger <C> 0.319 <C> 0.212 <C> 0.306 <R> <C> Disgust <C> 0.184 <C> 0.160 <C> 0.179 <R> <C> Fear <C> 0.184 <C> 0.132 <C> 0.176 <CAP> Table 8: Per-emotion Reliability of Agreement (κ)
<R> <C> Corpus <C> Quality rating Stub <C> Quality rating Start <C> Quality rating C <C> Quality rating B <C> Quality rating Good Article <C> Quality rating Featured Article <R> <C> Politics <C> 2950 <C> 5009 <C> 3541 <C> 485 <C> 871 <C> 215 <R> <C> Social Issues <C> 30853 <C> 55884 <C> 48292 <C> 14050 <C> 10108 <C> 3814 <R> <C> Science <C> 12192 <C> 16899 <C> 12454 <C> 5323 <C> 1696 <C> 966 <CAP> Table 3: Distribution of article quality in each corpus
<R> <C> [BOLD] Model <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> MaxEnt Gu et al. ( 2016 ) <C> 62.0 <C> 55.1 <C> 58.3 <R> <C> Pattern rule-based Lowe et al. ( 2016 ) <C> 59.3 <C> 62.3 <C> 60.8 <R> <C> LSTM-based Zhou et al. ( 2016 ) <C> 64.9 <C> 49.3 <C> 56.0 <R> <C> LSTM-based & PP Zhou et al. ( 2016 ) <C> 55.6 <C> 68.4 <C> 61.3 <R> <C> CNN-based Gu et al. ( 2017 ) <C> 60.9 <C> 59.5 <C> 60.2 <R> <C> CNN-based & PP Gu et al. ( 2017 ) <C> 55.7 <C> 68.1 <C> 61.3 <R> <C> BRAN Verga et al. ( 2017 ) <C> 55.6 <C> 70.8 <C> [BOLD] 62.1 <R> <C> SVM+APG Panyam et al. ( 2018 ) <C> 53.2 <C> 69.7 <C> 60.3 <R> <C> CNN <C> 54.8 <C> 69.0 <C> 61.1 <R> <C> CNN+CNNchar <C> 57.0 <C> 68.6 <C> [BOLD] 62.3 <R> <C> CNN+LSTMchar <C> 56.8 <C> 68.8 <C> 62.2 <R> <C> Linear+TK Panyam et al. ( 2016 ) <C> 63.6 <C> 59.8 <C> 61.7 <R> <C> SVM Peng et al. ( 2016 ) <C> 62.1 <C> 64.2 <C> 63.1 <R> <C> SVM (+dev.) Peng et al. ( 2016 ) <C> 68.2 <C> 66.0 <C> 67.1 <R> <C> SVM (+dev.+18K) Peng et al. ( 2016 ) <C> 71.1 <C> 72.6 <C> [BOLD] 71.8 <R> <C> SVM (+dev.) Xu et al. ( 2016 ) <C> 65.8 <C> 68.6 <C> 67.2 <R> <C> SVM (+dev.) Pons et al. ( 2016 ) <C> 73.1 <C> 67.6 <C> 70.2 <CAP> Table 1: Precision (P), Recall (R) and F1 scores (in %). “& PP” refers to the use of additional post-processing heuristic rules. “BRAN” denotes bi-affine relation attention networks. “SVM+APG” denotes a model using SVM with All Path Graph kernel. “Linear+TK” denotes a model combining linear and tree kernel classifiers. “+dev.” denotes the use of both training and development sets for learning models. Note that Peng2016 also used an extra training corpus of 18K weakly-annotated PubMed articles.
<R> <C> [BOLD] Exp <C> [BOLD] Systems <C> [BOLD] BLEU <C> [BOLD] MET <C> [BOLD] TER <R> <C> 1 <C> Baseline System <C> 65.90 <C> 74.54 <C> 22.71 <R> <C> 2 <C> Basic Moses <C> 62.44 <C> 72.80 <C> 24.71 <R> <C> 3 <C> BA_PBSMT <C> 62.52 <C> 72.74 <C> 24.53 <R> <C> 4 <C> MA_PBSMT <C> 65.13 <C> 74.13 <C> 23.11 <R> <C> 5 <C> Hyb_HPBSMT <C> [BOLD] 66.23 <C> [BOLD] 74.73 <C> [BOLD] 22.33 <CAP> Table 1: Automatic Evaluation of SAPE system, MET:METEOR.
<R> <C> Model <C> Train <C> en <C> fr <C> es <C> de <C> el <C> bg <C> ru <C> tr <C> ar <C> vi <C> th <C> zh <C> hi <C> sw <C> ur <C> avg <R> <C> [EMPTY] <C> [EMPTY] <C> [ITALIC] Test set machine translated into English (TRANSLATE-TEST) <C> [ITALIC] Test set machine translated into English (TRANSLATE-TEST) <C> [ITALIC] Test set machine translated into English (TRANSLATE-TEST) <C> [ITALIC] Test set machine translated into English (TRANSLATE-TEST) <C> [ITALIC] Test set machine translated into English (TRANSLATE-TEST) <C> [ITALIC] Test set machine translated into English (TRANSLATE-TEST) <C> [ITALIC] Test set machine translated into English (TRANSLATE-TEST) <C> [ITALIC] Test set machine translated into English (TRANSLATE-TEST) <C> [ITALIC] Test set machine translated into English (TRANSLATE-TEST) <C> [ITALIC] Test set machine translated into English (TRANSLATE-TEST) <C> [ITALIC] Test set machine translated into English (TRANSLATE-TEST) <C> [ITALIC] Test set machine translated into English (TRANSLATE-TEST) <C> [ITALIC] Test set machine translated into English (TRANSLATE-TEST) <C> [ITALIC] Test set machine translated into English (TRANSLATE-TEST) <C> [ITALIC] Test set machine translated into English (TRANSLATE-TEST) <C> [EMPTY] <R> <C> Roberta <C> Orig <C> 91.2 <C> 82.2 <C> 84.6 <C> 82.4 <C> 82.1 <C> 82.1 <C> 79.2 <C> 76.5 <C> 77.4 <C> 73.8 <C> 73.4 <C> 76.7 <C> 70.5 <C> 67.2 <C> 66.8 <C> 77.7 ±0.6 <R> <C> Roberta <C> BT-ES <C> [BOLD] 91.6 <C> 85.7 <C> [BOLD] 87.4 <C> 85.4 <C> 85.1 <C> 85.1 <C> 83.6 <C> 81.3 <C> 81.5 <C> 78.7 <C> 78.2 <C> 81.1 <C> 76.3 <C> 72.7 <C> 71.5 <C> 81.7 ±0.2 <R> <C> Roberta <C> BT-FI <C> 91.4 <C> [BOLD] 86.0 <C> [BOLD] 87.4 <C> 85.7 <C> [BOLD] 85.7 <C> 85.4 <C> [BOLD] 84.4 <C> 82.3 <C> 82.1 <C> 79.0 <C> 79.3 <C> 81.8 <C> 77.6 <C> 73.5 <C> 73.6 <C> 82.3 ±0.2 <R> <C> XLM-R <C> Orig <C> 90.3 <C> 82.2 <C> 84.2 <C> 82.6 <C> 81.9 <C> 82.0 <C> 79.3 <C> 76.7 <C> 77.5 <C> 75.0 <C> 73.7 <C> 77.5 <C> 70.9 <C> 67.8 <C> 67.2 <C> 77.9 ±0.3 <R> <C> XLM-R <C> BT-ES <C> 90.2 <C> 84.1 <C> 86.3 <C> 84.5 <C> 84.5 <C> 84.1 <C> 82.2 <C> 79.6 <C> 80.7 <C> 78.5 <C> 77.3 <C> 80.8 <C> 75.2 <C> 72.5 <C> 71.2 <C> 80.8 ±0.3 <R> <C> XLM-R <C> BT-FI <C> 89.5 <C> 84.9 <C> 85.5 <C> 84.5 <C> 84.5 <C> 84.6 <C> 82.9 <C> 80.6 <C> 81.4 <C> 78.9 <C> 78.1 <C> 81.5 <C> 76.3 <C> 73.3 <C> 72.5 <C> 81.3 ±0.2 <R> <C> XLM-R <C> MT-ES <C> 89.8 <C> 83.2 <C> 85.6 <C> 84.2 <C> 84.0 <C> 83.6 <C> 81.6 <C> 78.4 <C> 79.3 <C> 77.6 <C> 76.7 <C> 80.0 <C> 74.3 <C> 71.3 <C> 70.1 <C> 80.0 ±0.6 <R> <C> XLM-R <C> MT-FI <C> 89.8 <C> 84.4 <C> 85.3 <C> 84.7 <C> 84.1 <C> 84.0 <C> 82.0 <C> 79.8 <C> 80.3 <C> 77.4 <C> 77.7 <C> 80.6 <C> 74.7 <C> 71.8 <C> 71.3 <C> 80.5 ±0.3 <R> <C> [EMPTY] <C> [EMPTY] <C> [ITALIC] Test set in target language (ZERO-SHOT) <C> [ITALIC] Test set in target language (ZERO-SHOT) <C> [ITALIC] Test set in target language (ZERO-SHOT) <C> [ITALIC] Test set in target language (ZERO-SHOT) <C> [ITALIC] Test set in target language (ZERO-SHOT) <C> [ITALIC] Test set in target language (ZERO-SHOT) <C> [ITALIC] Test set in target language (ZERO-SHOT) <C> [ITALIC] Test set in target language (ZERO-SHOT) <C> [ITALIC] Test set in target language (ZERO-SHOT) <C> [ITALIC] Test set in target language (ZERO-SHOT) <C> [ITALIC] Test set in target language (ZERO-SHOT) <C> [ITALIC] Test set in target language (ZERO-SHOT) <C> [ITALIC] Test set in target language (ZERO-SHOT) <C> [ITALIC] Test set in target language (ZERO-SHOT) <C> [ITALIC] Test set in target language (ZERO-SHOT) <C> [EMPTY] <R> <C> XLM-R <C> Orig <C> 90.4 <C> 84.4 <C> 85.5 <C> 84.3 <C> 81.9 <C> 83.6 <C> 80.1 <C> 80.1 <C> 79.8 <C> 81.8 <C> 78.3 <C> 80.3 <C> 77.7 <C> 72.8 <C> 74.5 <C> 81.0 ±0.2 <R> <C> XLM-R <C> BT-ES <C> 90.2 <C> [BOLD] 86.0 <C> 86.9 <C> [BOLD] 86.5 <C> 84.0 <C> 85.3 <C> 83.2 <C> 82.5 <C> 82.7 <C> 83.7 <C> 80.7 <C> 83.0 <C> 79.7 <C> 75.6 <C> 77.1 <C> 83.1 ±0.2 <R> <C> XLM-R <C> BT-FI <C> 89.5 <C> [BOLD] 86.0 <C> 86.2 <C> 86.2 <C> 83.9 <C> 85.1 <C> 83.4 <C> 82.2 <C> [BOLD] 83.0 <C> [BOLD] 83.9 <C> [BOLD] 81.2 <C> [BOLD] 83.9 <C> 80.1 <C> 75.2 <C> [BOLD] 78.1 <C> 83.2 ±0.1 <R> <C> XLM-R <C> MT-ES <C> 89.9 <C> 85.7 <C> 87.3 <C> 85.6 <C> 83.9 <C> 85.4 <C> 82.9 <C> 82.0 <C> 82.3 <C> 83.6 <C> 80.0 <C> 82.6 <C> 79.9 <C> 75.5 <C> 76.8 <C> 82.9 ±0.4 <R> <C> XLM-R <C> MT-FI <C> 90.2 <C> 85.9 <C> 86.9 <C> [BOLD] 86.5 <C> 84.4 <C> [BOLD] 85.5 <C> 83.4 <C> [BOLD] 83.0 <C> 82.4 <C> 83.6 <C> 80.5 <C> 83.6 <C> [BOLD] 80.4 <C> [BOLD] 76.5 <C> 77.9 <C> [BOLD] 83.4 ±0.2 <CAP> Table 1: XNLI dev results (acc). BT-XX and MT-XX consistently outperform Orig in all cases.
<R> <C> [EMPTY] <C> [EMPTY] <C> XNLI dev OR <C> XNLI dev HT <C> Our dataset OR <C> Our dataset HT <C> Our dataset MT <R> <C> Model <C> Train <C> (en) <C> (es) <C> (es) <C> (en) <C> (en) <R> <C> Roberta <C> Orig <C> [BOLD] 92.1 <C> - <C> - <C> 78.7 <C> 79.0 <R> <C> Roberta <C> BT-ES <C> 91.9 <C> - <C> - <C> 80.3 <C> [BOLD] 80.5 <R> <C> Roberta <C> BT-FI <C> 91.4 <C> - <C> - <C> [BOLD] 80.5 <C> [BOLD] 80.5 <R> <C> XLM-R <C> Orig <C> 90.5 <C> 85.5 <C> 81.0 <C> 77.5 <C> 78.5 <R> <C> XLM-R <C> BT-ES <C> 90.3 <C> 87.1 <C> [BOLD] 81.4 <C> 78.6 <C> 79.4 <R> <C> XLM-R <C> BT-FI <C> 89.7 <C> 86.5 <C> 80.8 <C> 78.8 <C> 79.2 <R> <C> XLM-R <C> MT-ES <C> 90.2 <C> [BOLD] 87.5 <C> 81.3 <C> 78.4 <C> 78.9 <R> <C> XLM-R <C> MT-FI <C> 90.4 <C> 87.1 <C> 81.1 <C> 78.3 <C> 78.9 <CAP> Table 2: NLI results on original (OR), human translated (HT) and machine translated (MT) sets (acc). BT-XX and MT-XX outperform Orig in translated sets, but do not get any clear improvement in original ones.
<R> <C> Model <C> Train <C> Competence AT <C> Competence NR <C> Distraction WO <C> Distraction NG <C> Distraction LN <C> Noise SE <R> <C> Roberta <C> Orig <C> 72.9 <C> [BOLD] 65.7 <C> 64.9 <C> 59.1 <C> [BOLD] 88.4 <C> 86.5 <R> <C> Roberta <C> BT-FI <C> 56.6 <C> 57.2 <C> [BOLD] 80.6 <C> 67.8 <C> 87.7 <C> [BOLD] 86.6 <R> <C> XLM-R <C> Orig <C> [BOLD] 78.4 <C> 56.8 <C> 67.3 <C> 61.2 <C> 86.8 <C> 85.3 <R> <C> XLM-R <C> BT-FI <C> 60.6 <C> 51.7 <C> 76.7 <C> 64.6 <C> 86.2 <C> 85.4 <R> <C> XLM-R <C> MT-FI <C> 64.3 <C> 50.3 <C> 77.8 <C> [BOLD] 68.5 <C> 86.4 <C> 85.3 <CAP> Table 3: NLI Stress Test results (combined matched & mismatched acc). AT = antonymy, NR = numerical reasoning, WO = word overlap, NG = negation, LN = length mismatch, SE = spelling error. BT-FI and MT-FI are considerably weaker than Orig in the competence test, but substantially stronger in the distraction test.
<R> <C> [BOLD] # Words <C> [BOLD] # Senses <C> [BOLD] Avg. Polysemy <C> [BOLD] # Contexts <R> <C> 863 <C> 2,708 <C> 3.13 <C> 11,712 <CAP> Table 1: Evaluation dataset based on BabelNet.
<R> <C> [BOLD] WSD Model Inventory <C> [BOLD] WSD Model Features <C> [BOLD] Accuracy Hypers <C> [BOLD] Accuracy HyperHypers <R> <C> Word Senses <C> Random <C> 0.257 <C> 0.610 <R> <C> Word Senses <C> MFS <C> 0.292 <C> 0.682 <R> <C> Word Senses <C> Cluster Words <C> 0.291 <C> 0.650 <R> <C> Word Senses <C> Context Words <C> [BOLD] 0.308 <C> [BOLD] 0.686 <R> <C> Super Senses <C> Random <C> 0.001 <C> 0.001 <R> <C> Super Senses <C> MFS <C> 0.001 <C> 0.001 <R> <C> Super Senses <C> Cluster Words <C> [BOLD] 0.174 <C> [BOLD] 0.365 <R> <C> Super Senses <C> Context Words <C> 0.086 <C> 0.188 <CAP> Table 2: Performance of the hypernymy labeling in context on the BabelNet dataset.
<R> <C> Model <C> VG <C> DM <C> AM <C> OP <R> <C> AHN <C> [BOLD] 1.1138 <C> [BOLD] 0.8172 <C> [BOLD] 0.7314 <C> [BOLD] 0.6825 <R> <C> (a) –Item aggregators <C> 1.1286 <C> 0.8205 <C> 0.7506 <C> 0.6951 <R> <C> (b) –User aggregators <C> 1.1604 <C> 0.8246 <C> 0.7467 <C> 0.6941 <R> <C> (c) –Adapted affinity <C> 1.1363 <C> 0.8229 <C> 0.7348 <C> 0.6936 <R> <C> (d) –FM <C> 1.1267 <C> 0.8341 <C> 0.7723 <C> 0.7078 <R> <C> (e) –Gating <C> 1.1220 <C> 0.8188 <C> 0.7385 <C> 0.6883 <CAP> Table 4: Ablation analysis
<R> <C> [EMPTY] <C> [ITALIC] r 5 <C> [ITALIC] r 5 <C> [ITALIC] r 50 <C> [ITALIC] r 50 <C> Baseline <R> <C> Rank <C> Doc <C> Para <C> Doc <C> Para <C> [EMPTY] <R> <C> 5 <C> 0.253 <C> 0.251 <C> 0.240 <C> 0.179 <C> 0.312 <R> <C> 10 <C> 0.331 <C> 0.347 <C> 0.331 <C> 0.284 <C> 0.434 <R> <C> 20 <C> 0.438 <C> 0.444 <C> 0.438 <C> 0.398 <C> 0.553 <R> <C> 50 <C> 0.583 <C> 0.577 <C> 0.577 <C> 0.552 <C> 0.634 <CAP> Table 8: Coverage (strict) using blind RF. Both document- and paragraph-level retrieval used to determine RF terms.
<R> <C> Specification <C> arXiv Quant. Fin. Papers <C> IMDB Movie Reviews <R> <C> Labels <C> 5 (Multi-label) <C> 2 <R> <C> Clean Records <C> 4601 <C> 6000 <R> <C> Length of Records <C> 8456.9±6395.8 <C> 540.5±171.9 <R> <C> Frequency of Labels <C> [ [ITALIC] l] [ITALIC] q- [ITALIC] fin. [ITALIC] GN:1258 [ITALIC] q- [ITALIC] fin. [ITALIC] ST:1144 [ITALIC] q- [ITALIC] fin. [ITALIC] MF:977 [ITALIC] q- [ITALIC] fin. [ITALIC] PR:907 [ITALIC] q- [ITALIC] fin. [ITALIC] RM:913 <C> [ [ITALIC] l] [ITALIC] Positive:3000 [ITALIC] Negative:3000 <CAP> Table 1: Data Specification for arXiv papers data sets.
<R> <C> [BOLD] System <C> [BOLD] ROUGE-1 <C> [BOLD] ROUGE-2 <C> [BOLD] ROUGE-L <R> <C> Lead-3 <C> 32.59 <C> 16.49 <C> 29.17 <R> <C> PointGen+cov <C> 41.06 <C> 25.71 <C> 37.28 <R> <C> DeepReinforce <C> 47.03 <C> 30.72 <C> 43.10 <R> <C> BottomUp <C> 47.38 <C> 31.23 <C> 41.81 <R> <C> DCA <C> 48.08 <C> 31.19 <C> 42.33 <R> <C> SENECA <C> 47.94 <C> 31.77 <C> 44.34 <R> <C> BART <C> [BOLD] 53.25 <C> [BOLD] 36.61 <C> [BOLD] 48.78 <R> <C> [BOLD] Our Models <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> NoGraph <C> 47.15 <C> 32.02 <C> 43.65 <R> <C> + [ITALIC] Rrouge <C> 49.17 <C> 33.19 <C> 46.44 <R> <C> ASGARD-doc <C> 49.51 <C> 33.82 <C> 45.72 <R> <C> + [ITALIC] Rrouge <C> 50.18 <C> 33.91 <C> 46.84 <R> <C> + [ITALIC] Rrouge+ [ITALIC] Rcloze <C> 50.59 <C> 33.98 <C> 48.24 <R> <C> ASGARD-seg <C> 49.54 <C> 33.84 <C> 45.75 <R> <C> + [ITALIC] Rrouge <C> 50.47 <C> 33.95 <C> 47.43 <R> <C> + [ITALIC] Rrouge+ [ITALIC] Rcloze <C> [ITALIC] 51.29 <C> [ITALIC] 34.97 <C> [ITALIC] 48.26 <CAP> Table 1: Automatic evaluation with ROUGE on New York Times. Best results are in boldface. Best of our models are in italics. ASGARD-seg+Rrouge+Rcloze yields significantly higher scores than our other models with approximate randomization test (p<0.0005).
<R> <C> [BOLD] System <C> [BOLD] ROUGE-1 <C> [BOLD] ROUGE-2 <C> [BOLD] ROUGE-L <R> <C> Lead-3 <C> 40.23 <C> 17.52 <C> 36.34 <R> <C> PointGen+cov <C> 39.53 <C> 17.28 <C> 36.38 <R> <C> DeepReinforce <C> 41.16 <C> 15.75 <C> 39.08 <R> <C> BottomUp <C> 41.22 <C> 18.68 <C> 38.34 <R> <C> DCA <C> 41.69 <C> 19.47 <C> 37.92 <R> <C> BERTSumExtAbs <C> 42.13 <C> 19.60 <C> 39.18 <R> <C> UniLM <C> 43.33 <C> 20.21 <C> 40.51 <R> <C> BART <C> [BOLD] 44.16 <C> [BOLD] 21.28 <C> [BOLD] 40.90 <R> <C> [BOLD] Our Models <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> NoGraph <C> 39.55 <C> 17.89 <C> 36.75 <R> <C> + [ITALIC] Rrouge <C> 41.37 <C> 17.63 <C> 37.99 <R> <C> ASGARD-doc <C> 40.38 <C> 18.40 <C> 37.51 <R> <C> + [ITALIC] Rrouge <C> 43.10 <C> 17.58 <C> 39.41 <R> <C> + [ITALIC] Rrouge+ [ITALIC] Rcloze <C> [ITALIC] 43.93 <C> [ITALIC] 20.37 <C> [ITALIC] 40.48 <R> <C> ASGARD-seg <C> 40.09 <C> 18.30 <C> 37.30 <R> <C> + [ITALIC] Rrouge <C> 42.94 <C> 17.93 <C> 39.36 <R> <C> + [ITALIC] Rrouge+ [ITALIC] Rcloze <C> 43.81 <C> 20.22 <C> 40.37 <CAP> Table 2: Automatic evaluation with ROUGE on CNN/Daily Mail. Best results of our model variants are in italics. Both ASGARD-seg+Rrouge+Rcloze and ASGARD-doc+Rrouge+Rcloze obtain significantly better scores than other model variants (p<0.0005).
<R> <C> Speaker <C> f1a <C> f2b <C> f3a <C> m1b <C> m2b <R> <C> detection <C> 85.6 <C> 82.9 <C> 83.5 <C> 81.4 <C> 84.8 <R> <C> classification <C> 70.6 <C> 71.8 <C> 67.7 <C> 68.4 <C> 66.6 <CAP> Table 4: Pitch accent recognition accuracies for each speaker using prosody and position features.
<R> <C> [EMPTY] <C> non-normalized <C> normalized <R> <C> [BOLD] Pitch Accents <C> [EMPTY] <C> [EMPTY] <R> <C> Detection <C> 83.6 <C> 77.0 <R> <C> Classification <C> 69.0 <C> 62.6 <R> <C> [BOLD] Phrase Boundaries <C> [EMPTY] <C> [EMPTY] <R> <C> Detection <C> 89.8 <C> 83.0 <R> <C> Classification <C> 87.3 <C> 83.2 <CAP> Table 8: Effects of z-scoring in speaker-independent experiments using prosody and position features.
<R> <C> #Train <C> Basque L <C> Basque C <C> Basque Δ <C> Dutch L <C> Dutch C <C> Dutch Δ <C> Spanish L <C> Spanish C <C> Spanish Δ <C> German L <C> German C <C> German Δ <R> <C> 1/16 <C> - <C> - <C> - <C> 59.46 <C> 67.63 <C> 8.17 <C> 65.07 <C> 71.83 <C> 6.76 <C> 51.31 <C> 65.97 <C> 14.66 <R> <C> 1/8 <C> 46.27 <C> 62.23 <C> 16.96 <C> 63.16 <C> 72.41 <C> 9.25 <C> 69.24 <C> 74.00 <C> 4.76 <C> 56.80 <C> 67.84 <C> 11.04 <R> <C> 1/4 <C> 52.15 <C> 65.10 <C> 12.95 <C> 71.38 <C> 79.25 <C> 7.87 <C> 73.79 <C> 78.25 <C> 4.46 <C> 61.96 <C> 72.20 <C> 10.24 <R> <C> 1/2 <C> 62.13 <C> 71.98 <C> 9.85 <C> 74.00 <C> 82.46 <C> 8.46 <C> 76.87 <C> 80.29 <C> 3.42 <C> 66.43 <C> 75.45 <C> 9.02 <R> <C> Full <C> 65.00 <C> 76.72 <C> 11.72 <C> 76.83 <C> 83.46 <C> 6.63 <C> 79.52 <C> 84.16 <C> 4.64 <C> 70.81 <C> 77.96 <C> 7.15 <CAP> Table 14: Multilingual results reducing training data. Datasets employed: Basque (egunkaria), Dutch and Spanish (CoNLL 2002) and German (GermEval 2014 outer). L: Local model. C: cluster model. Δ: difference between them.
<R> <C> System <C> Precision <C> Recall <C> Phrase F1 <R> <C> Local <C> 59.01 <C> 52.64 <C> 55.64 <R> <C> BR+BW <C> 62.58 <C> 57.76 <C> 60.06 <R> <C> CR600 + CW600 <C> 67.79 <C> 58.51 <C> 63.04 <R> <C> W2VG200 + W2VW400 <C> 64.32 <C> 58.61 <C> 61.39 <R> <C> best-cluster <C> [BOLD] 70.09 <C> 64.42 <C> [BOLD] 67.14 <R> <C> Stanford NER (distsim-conll03) <C> 64.40 <C> 61.89 <C> 63.12 <R> <C> en-wiki2 (Nothman et al. 2013) <C> 64.60 <C> [BOLD] 68.70 <C> 66.60 <CAP> Table 17: Wikigold out-of-domain evaluation based on text genre.
<R> <C> Features <C> Outer NEs F1 <C> Outer NEs T-F1 <C> Inner NEs F1 <C> Inner NEs T-F1 <R> <C> Local (ALL) <C> 56.00 <C> 66.30 <C> 65.89 <C> 73.01 <R> <C> en-91-18-conll03 (ALL) <C> [BOLD] 62.09 <C> 70.98 <C> [BOLD] 70.90 <C> 77.18 <R> <C> Stanford NER 3 class (ALL) <C> 57.14 <C> 70.22 <C> 66.96 <C> 76.01 <CAP> Table 19: MEANTIME English multi-corpus out-of-domain evaluation.
<R> <C> Learning mode <C> Dev PER <C> Test PER <R> <C> MFSC <C> 17.8 <C> 20.6 <R> <C> Fixed <C> 18.3 <C> 21.8 <R> <C> Learn-all <C> 17.4 <C> 20.6 <R> <C> Learn-filterbank <C> 17.3 <C> 20.3 <R> <C> Randinit <C> 29.2 <C> 31.7 <CAP> Table 2: PER of the CNN-5L-ReLU-do0.7 model trained on MFSC and different learning setups of TD-filterbanks.
<R> <C> [BOLD] Features <C> [BOLD] AUC <C> [BOLD] F1 <C> [BOLD] Precision <C> [BOLD] Recall <R> <C> [BOLD] Unigram <C> 0.808 <C> 0.747 <C> 0.741 <C> 0.673 <R> <C> [BOLD] U + w2v <C> 0.753 <C> 0.696 <C> 0.662 <C> 0.673 <R> <C> [BOLD] U + prev-com <C> 0.786 <C> 0.701 <C> 0.694 <C> 0.605 <R> <C> [BOLD] U + user <C> 0.817 <C> 0.761 <C> 0.752 <C> 0.695 <R> <C> [BOLD] U + n-w2v <C> 0.821 <C> 0.775 <C> 0.781 <C> 0.711 <R> <C> [BOLD] U + trend <C> 0.825 <C> 0.778 <C> 0.782 <C> 0.721 <R> <C> [BOLD] U + lex <C> 0.827 <C> 0.776 <C> 0.785 <C> 0.705 <R> <C> [BOLD] U + prev-post <C> 0.842 <C> 0.782 <C> [BOLD] 0.829 <C> 0.688 <R> <C> [BOLD] U + final-com <C> 0.879 <C> 0.792 <C> 0.805 <C> 0.722 <R> <C> [BOLD] Best <C> [BOLD] 0.913 <C> [BOLD] 0.805 <C> 0.785 <C> [BOLD] 0.772 <CAP> Table 3: Forecasting accuracy of Task 2 (N=10). The best feature combination is trend/user/final-com.
<R> <C> [EMPTY] <C> End-to-End Task <C> End-to-End Task <C> End-to-End Task <C> End-to-End Task <C> Oracle Question Generation Task <C> Oracle Question Generation Task <R> <C> Models <C> Micro Acc. <C> Macro Acc. <C> BLEU1 <C> BLEU4 <C> BLEU1 <C> BLEU4 <R> <C> EMT <C> [BOLD] 71.36±0.69 <C> [BOLD] 76.70±0.54 <C> [BOLD] 67.04±1.59 <C> [BOLD] 52.37±1.92 <C> [BOLD] 63.53±1.03 <C> [BOLD] 48.69±0.80 <R> <C> EMT (w/o data aug.) <C> 70.67±0.52 <C> 76.33±0.69 <C> 65.86±2.25 <C> 51.02±2.52 <C> 62.38±1.34 <C> 47.58±1.30 <R> <C> EMT (w/o c2f) <C> 70.41±0.94 <C> 75.96±0.91 <C> 65.73±1.76 <C> 50.84±2.31 <C> 61.98±1.26 <C> 47.66±1.33 <R> <C> EMT (w/o Lentail) <C> 67.81±1.20 <C> 73.50±0.83 <C> 63.84±1.80 <C> 49.35±2.10 <C> 60.50±1.16 <C> 45.34±1.73 <R> <C> EMT (w/o tracker) <C> 67.42±1.15 <C> 72.73±0.74 <C> 63.26±0.64 <C> 47.97±0.40 <C> 61.87±1.46 <C> 47.13±1.35 <CAP> Table 4: Ablation Study of EMT on the development set of ShARC.
<R> <C> Method <C> Intersection <C> Common-Match <C> Recall <R> <C> [ITALIC] Synset <C> 22,192 <C> 0.5284 <C> 0.5285 <R> <C> [ITALIC] Fusion1 <C> 22,123 <C> 0.5736 <C> 0.5735 <R> <C> [ITALIC] Fusion2 <C> 41,642 <C> 0.6375 <C> 0.6374 <R> <C> [ITALIC] Fusion3 <C> 56,114 <C> [BOLD] 0.6470 <C> [BOLD] 0.6473 <R> <C> [ITALIC] Fusion4 <C> [BOLD] 67,625 <C> [BOLD] 0.6470 <C> 0.6464 <CAP> Table 1: Evaluation results based on the evaluation metrics Recall and At least one common label denoted here as the Common-Match metric. The table also shows the size of the intersection between the method results and the test set that was used in computing the evaluation metric, denoted here as Intersection. The value of Intersection might also be a good indicator of the method being able to tag more articles.
<R> <C> method Baselines <C> method CNN-Cnt <C> MAP 0.6520 <C> MRR 0.6652 <R> <C> [EMPTY] <C> Addition <C> 0.5021 <C> 0.5069 <R> <C> [EMPTY] <C> Addition (+ extra) <C> 0.5888 <C> 0.5929 <R> <C> [EMPTY] <C> A-LSTM <C> 0.5321 <C> 0.5469 <R> <C> [EMPTY] <C> A-LSTM (+extra) <C> 0.6388 <C> 0.6529 <R> <C> [EMPTY] <C> AP-CNN <C> 0.6886 <C> 0.6957 <R> <C> [EMPTY] <C> ABCNN <C> 0.6921 <C> 0.7127 <R> <C> GRU <C> [ITALIC] k-min-max-pooling <C> 0.6674 <C> 0.6791 <R> <C> GRU <C> full-pooling <C> 0.6693 <C> 0.6785 <R> <C> GRU <C> [ITALIC] k-max-max-pooling <C> [BOLD] 0.7108∗ <C> [BOLD] 0.7203∗ <CAP> Table 3: Results on WikiQA. Significant improvement over both k-min-max-pooling and full-pooling is marked with ∗ (t-test, p < .05).
<R> <C> [EMPTY] <C> [EMPTY] <C> Train v1 <C> Train v1+15k <C> Δ <R> <C> Test <C> v1 <C> 73.56±0.61 <C> 75.65±0.34 <C> +2.09 <R> <C> Test <C> v2 <C> 28.31±0.25 <C> 79.17±0.11 <C> +50.86 <CAP> Table 7: Answer F1 scores on the NLmaps v1 and NLmaps v2 test sets for models trained on either only NLmaps v1 training data or with an additional 15k synthetic instances. Results are averaged over 3 runs. Using the NLmaps v2 training set leads to significant system differences on both test sets at p<0.05.
<R> <C> [BOLD] Supervised (baselines) ADD (Reddy et al., 2011) <C> [BOLD] Supervised (baselines) 0.21 <R> <C> MULT (Reddy et al., 2011) <C> 0.09 <R> <C> Deep Learning (Yazdani et al., 2015) <C> 0.41 <R> <C> [BOLD] Unsupervised (ours) <C> [BOLD] Unsupervised (ours) <R> <C> Rank or  [ITALIC] l1 distance (Equation  1 ) <C> 0.59 <R> <C> Chebyshev or  [ITALIC] l∞ distance (Equation  3 ) <C> 0.50 <R> <C> CosRank distance (Equation  5 ) <C> 0.60 <R> <C> Hamming distance (Equation  6 ) <C> 0.55 <R> <C> Hausdorff distance (Equation  7 ) <C> 0.50 <R> <C> Pearson correlation coefficient (Equation  8 ) <C> 0.62 <R> <C> AP correlation coefficient (Equation  9 ) <C> 0.58 <CAP> Table 4: Spearman’s ρ correlation between system decisions and human annotations of compositionality (the higher, the better).
<R> <C> [ITALIC] κ <C> # pairs <C> % pairs <R> <C> >0.2 <C> 253/253 <C> 100% <R> <C> >0.3 <C> 251/253 <C> 99.2% <R> <C> >0.4 <C> 225/253 <C> 88.9% <R> <C> >0.5 <C> 162/253 <C> 64.0% <R> <C> >0.6 <C> 50/253 <C> 19.8% <R> <C> >0.7 <C> 3/253 <C> 1.2% <R> <C> >0.8 <C> 0/253 <C> 0% <CAP> Table 6: Distribution of pairwise κ scores between each pair of human annotators, other than the annotators that were discarded due to low scores.
<R> <C> [EMPTY] <C> Perf. <C> AUPRC <C> Comp. ↑ <C> Suff. ↓ <R> <C> [BOLD] Evidence Inference <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> GloVe + LSTM - Attention <C> 0.429 <C> 0.506 <C> -0.002 <C> -0.023 <R> <C> GloVe + LSTM - Gradient <C> 0.429 <C> 0.016 <C> 0.046 <C> -0.138 <R> <C> GloVe + LSTM - Lime <C> 0.429 <C> 0.014 <C> 0.006 <C> -0.128 <R> <C> GloVe + LSTM - Random <C> 0.429 <C> 0.014 <C> -0.001 <C> -0.026 <R> <C> [BOLD] BoolQ <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> GloVe + LSTM - Attention <C> 0.471 <C> 0.525 <C> 0.010 <C> 0.022 <R> <C> GloVe + LSTM - Gradient <C> 0.471 <C> 0.072 <C> 0.024 <C> 0.031 <R> <C> GloVe + LSTM - Lime <C> 0.471 <C> 0.073 <C> 0.028 <C> -0.154 <R> <C> GloVe + LSTM - Random <C> 0.471 <C> 0.074 <C> 0.000 <C> 0.005 <R> <C> [BOLD] Movies <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> BERT+LSTM - Attention <C> 0.970 <C> 0.417 <C> 0.129 <C> 0.097 <R> <C> BERT+LSTM - Gradient <C> 0.970 <C> 0.385 <C> 0.142 <C> 0.112 <R> <C> BERT+LSTM - Lime <C> 0.970 <C> 0.280 <C> 0.187 <C> 0.093 <R> <C> BERT+LSTM - Random <C> 0.970 <C> 0.259 <C> 0.058 <C> 0.330 <R> <C> [BOLD] FEVER <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> BERT+LSTM - Attention <C> 0.870 <C> 0.235 <C> 0.037 <C> 0.122 <R> <C> BERT+LSTM - Gradient <C> 0.870 <C> 0.232 <C> 0.059 <C> 0.136 <R> <C> BERT+LSTM - Lime <C> 0.870 <C> 0.291 <C> 0.212 <C> 0.014 <R> <C> BERT+LSTM - Random <C> 0.870 <C> 0.244 <C> 0.034 <C> 0.122 <R> <C> [BOLD] MultiRC <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> BERT+LSTM - Attention <C> 0.655 <C> 0.244 <C> 0.036 <C> 0.052 <R> <C> BERT+LSTM - Gradient <C> 0.655 <C> 0.224 <C> 0.077 <C> 0.064 <R> <C> BERT+LSTM - Lime <C> 0.655 <C> 0.208 <C> 0.213 <C> -0.079 <R> <C> BERT+LSTM - Random <C> 0.655 <C> 0.186 <C> 0.029 <C> 0.081 <R> <C> [BOLD] CoS-E <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> BERT+LSTM - Attention <C> 0.487 <C> 0.606 <C> 0.080 <C> 0.217 <R> <C> BERT+LSTM - Gradient <C> 0.487 <C> 0.585 <C> 0.124 <C> 0.226 <R> <C> BERT+LSTM - Lime <C> 0.487 <C> 0.544 <C> 0.223 <C> 0.143 <R> <C> BERT+LSTM - Random <C> 0.487 <C> 0.594 <C> 0.072 <C> 0.224 <R> <C> [BOLD] e-SNLI <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> BERT+LSTM - Attention <C> 0.960 <C> 0.395 <C> 0.105 <C> 0.583 <R> <C> BERT+LSTM - Gradient <C> 0.960 <C> 0.416 <C> 0.180 <C> 0.472 <R> <C> BERT+LSTM - Lime <C> 0.960 <C> 0.513 <C> 0.437 <C> 0.389 <R> <C> BERT+LSTM - Random <C> 0.960 <C> 0.357 <C> 0.081 <C> 0.487 <CAP> Table 4: Metrics for ‘soft’ scoring models. Perf. is accuracy (CoS-E) or F1 (others). Comprehensiveness and sufficiency are in terms of AOPC (Eq. 3). ‘Random’ assigns random scores to tokens to induce orderings; these are averages over 10 runs.
<R> <C> [EMPTY] <C> [BOLD] SVMs <C> [BOLD] Our Model <R> <C> [EMPTY] <C> [EMPTY] <C> ( [ITALIC] w Latent <R> <C> [EMPTY] <C> [EMPTY] <C> [ITALIC] Strength) <R> <C> [BOLD] Baselines <C> [EMPTY] <C> [EMPTY] <R> <C> Ngrams <C> 61.0 <C> – <R> <C> Audience Feedback <C> 56.8 <C> – <R> <C> [BOLD] Features (as in §  3.3 ) <C> [EMPTY] <C> [EMPTY] <R> <C> Basic <C> 57.6 <C> 59.3 <R> <C> + Style, Semantics, Discourse <C> 59.3 <C> 65.3 <R> <C> + Sentence, Argument <C> 62.7 <C> 69.5 <R> <C> + Interaction (all features) <C> 66.1 <C> [BOLD] 73.7 <CAP> Table 1: Debate outcome prediction results for baseline models and SVMs using the various linguistic feature categories, compared to our model that includes latent argument strengths in addition to the linguistic features. The best performing system (in bold) is achieved by our system with topic strength as latent variables when all features are used, which significantly outperforms the baselines via bootstrap resampling test (p<0.05). For the lower section, each row shows features included in addition to those in the rows above.
<R> <C> [ITALIC] Constraints <C> [BOLD] Initialization  [ITALIC] Freq <C> [BOLD] Initialization  [ITALIC] AllStrong <C> [BOLD] Initialization  [ITALIC] AllStrong [ITALIC] win <C> [BOLD] Initialization  [ITALIC] Random <R> <C> C1, C2 <C> 73.7 <C> 71.2 <C> 70.3 <C> 67.8 <R> <C> C1, C2, C3 <C> 72.9 <C> 73.7 <C> 69.5 <C> 68.6 <CAP> Table 2: Prediction results (in accuracy) with different initialization and topic strength constraints. C3 denotes a constraint that a topic cannot be strong for both sides.
<R> <C> [EMPTY] <C> WER SWB <C> WER CH <R> <C> fmllr+ivec+(logmel+Δ+Δ2) / CE <C> 8.1 <C> 13.5 <R> <C> fmllr+ivec+(logmel+Δ+Δ2) / sMBR ST <C> 7.2 <C> 12.7 <R> <C> fmllr+ivec+(logmel+Δ+Δ2)+sat / CE <C> 7.5 <C> 13.0 <R> <C> fmllr+ivec+(logmel+Δ+Δ2)+sat / sMBR ST <C> [BOLD] 7.1 <C> [BOLD] 12.5 <CAP> Table 4: WERs(%) on Hub5 2000 Switchboard (SWB) and Callhome (CH) of LSTMs with and without embedding-based SAT under the cross-entropy (CE) and state-level Minimum Bayes Risk (sMBR) criteria using 2,000 hours training data. The LSTM models have 6 layers with fmllr+ivec+logmel+Δ+Δ2 feature input. The SAT normalization is applied after each of the bottom three LSTM layers.
<R> <C> [EMPTY] <C> MUC R <C> MUC P <C> MUC F1 <C> [ITALIC] B3 R <C> [ITALIC] B3 P <C> [ITALIC] B3 F1 <C> CEAF [ITALIC] e R <C> CEAF [ITALIC] e P <C> CEAF [ITALIC] e F1 <C> CoNLL Avg. F1 <C> LEA R <C> LEA P <C> LEA F1 <R> <C> [EMPTY] <C> CoNLL test set <C> CoNLL test set <C> CoNLL test set <C> CoNLL test set <C> CoNLL test set <C> CoNLL test set <C> CoNLL test set <C> CoNLL test set <C> CoNLL test set <C> CoNLL test set <C> CoNLL test set <C> CoNLL test set <C> CoNLL test set <R> <C> rule-based <C> 64.29 <C> 65.19 <C> 64.74 <C> 49.18 <C> 56.79 <C> 52.71 <C> 52.45 <C> 46.58 <C> 49.34 <C> 55.60 <C> 43.72 <C> 51.53 <C> 47.30 <R> <C> berkeley <C> 67.56 <C> 74.09 <C> 70.67 <C> 53.93 <C> 63.50 <C> 58.33 <C> 53.29 <C> 56.22 <C> 54.72 <C> 61.24 <C> 49.66 <C> 59.17 <C> 54.00 <R> <C> cort <C> 67.83 <C> 78.35 <C> 72.71 <C> 54.34 <C> 68.42 <C> 60.57 <C> 53.10 <C> 61.10 <C> 56.82 <C> 63.37 <C> 50.40 <C> 64.46 <C> 56.57 <R> <C> deep-coref [conll] <C> 70.55 <C> 79.13 <C> 74.59 <C> 58.17 <C> 69.01 <C> 63.13 <C> 54.20 <C> 63.44 <C> 58.45 <C> 65.39 <C> 54.55 <C> 65.35 <C> 59.46 <R> <C> deep-coref [lea] <C> 70.43 <C> 79.57 <C> 74.72 <C> 58.08 <C> 69.26 <C> 63.18 <C> 54.43 <C> 64.17 <C> 58.90 <C> 65.60 <C> 54.55 <C> 65.68 <C> 59.60 <R> <C> [EMPTY] <C> WikiCoref <C> WikiCoref <C> WikiCoref <C> WikiCoref <C> WikiCoref <C> WikiCoref <C> WikiCoref <C> WikiCoref <C> WikiCoref <C> WikiCoref <C> WikiCoref <C> WikiCoref <C> WikiCoref <R> <C> rule-based <C> 60.42 <C> 61.56 <C> 60.99 <C> 43.34 <C> 53.53 <C> 47.90 <C> 50.89 <C> 42.70 <C> 46.44 <C> 51.77 <C> 38.79 <C> 48.92 <C> 43.27 <R> <C> berkeley <C> 68.52 <C> 55.96 <C> 61.61 <C> 59.08 <C> 39.72 <C> 47.51 <C> 48.06 <C> 40.44 <C> 43.92 <C> 51.01 <C> - <C> - <C> - <R> <C> cort <C> 70.39 <C> 53.63 <C> 60.88 <C> 60.81 <C> 37.58 <C> 46.45 <C> 47.88 <C> 38.18 <C> 42.48 <C> 49.94 <C> - <C> - <C> - <R> <C> deep-coref [conll] <C> 58.59 <C> 66.63 <C> 62.35 <C> 44.40 <C> 54.87 <C> 49.08 <C> 42.47 <C> 51.47 <C> 46.54 <C> 52.65 <C> 40.36 <C> 50.73 <C> 44.95 <R> <C> deep-coref [lea] <C> 57.48 <C> 70.55 <C> 63.35 <C> 42.12 <C> 60.13 <C> 49.54 <C> 41.40 <C> 53.08 <C> 46.52 <C> 53.14 <C> 38.22 <C> 55.98 <C> 45.43 <R> <C> deep-coref− <C> 55.07 <C> 71.81 <C> 62.33 <C> 38.05 <C> 61.82 <C> 47.11 <C> 38.46 <C> 50.31 <C> 43.60 <C> 51.01 <C> 34.11 <C> 57.15 <C> 42.72 <CAP> Table 1: Comparison of the results on the CoNLL test set and WikiCoref.
<R> <C> [BOLD] Method <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F-measure <R> <C> [BOLD] Common selection <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> SVM with linear kernel <C> 0.67 <C> 0.68 <C> 0.67 <R> <C> SVM with polynomial kernel <C> 0.65 <C> 0.79 <C> 0.72 <R> <C> SVM with Radial Basis Function (RBF) kernel <C> 0.70 <C> 0.70 <C> 0.70 <R> <C> SVM with linear kernel, normalized data <C> 0.62 <C> 0.74 <C> 0.67 <R> <C> [BOLD] Homographic puns <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> SVM with RBF kernel <C> 0.79 <C> 0.80 <C> 0.79 <R> <C> Multinomial Naive Bayes <C> 0.71 <C> 0.80 <C> 0.76 <R> <C> Logistic Regression, standardized data <C> 0.77 <C> 0.71 <C> 0.74 <R> <C> [BOLD] Heterographic puns <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> SVM with RBF kernel <C> 0.77 <C> 0.79 <C> 0.78 <R> <C> Logistic Regression <C> 0.74 <C> 0.75 <C> 0.74 <CAP> Table 3: Tests for pun recognition.
<R> <C> [EMPTY] <C> E-E-B <C> EGL-word-doc <C> Entropy <C> Random <R> <C> MR <C> [BOLD] 0.725 <C> 0.719 <C> 0.719 <C> 0.704 <R> <C> DR <C> [BOLD] 0.893 <C> 0.889 <C> 0.877 <C> 0.878 <R> <C> MuR <C> [BOLD] 0.736 <C> 0.718 <C> 0.725 <C> 0.726 <CAP> Table 4: Area Under (learning) Curves (AUC) scores on the three document datasets. E-E-B refers to EGL-Entropy-Beta.
<R> <C> Category <C> Feature <C> Length-scale <R> <C> production rule <C> S→NP,VP,., <C> 0.977 <R> <C> production rule <C> S→NP,VP, <C> 0.988 <R> <C> POS-ngram <C> V-ADJ <C> 0.950 <R> <C> POS-ngram <C> PUNC-NN <C> 0.974 <R> <C> POS-ngram <C> PR-PUNC <C> 0.977 <R> <C> POS-ngram <C> PP-V-PR <C> 0.981 <R> <C> POS-ngram <C> NN-V-ADV <C> 0.981 <R> <C> n-gram <C> “.” <C> 0.981 <R> <C> n-gram <C> “to” <C> 0.989 <R> <C> n-gram <C> “in” <C> 0.990 <R> <C> sentiment <C> Positive <C> 0.636 <R> <C> sentiment <C> VeryNegative <C> 0.842 <R> <C> sentiment <C> Neutral <C> 0.900 <R> <C> sentiment <C> Negative <C> 0.967 <R> <C> [ITALIC] sentiment <C> [ITALIC] VeryPositive <C> [ITALIC] 3.001 <R> <C> ratio <C> words > 6 letters <C> 0.734 <R> <C> ratio <C> SuperlativeAdj <C> 0.943 <R> <C> ratio <C> InterjectionRate <C> 0.986 <R> <C> ratio <C> SuperlativeAdv <C> 0.987 <R> <C> count <C> words > 6 letters <C> 0.983 <R> <C> NER <C> Location <C> 0.990 <CAP> Table 4: Normalized length-scales for linguistic features learned using MLII. Shows mean values over folds with >3% improvement. Includes all values <0.99, except for POS n-grams (only smallest 5 of 18 shown).
<R> <C> Model (#parameters) <C> CC-News Val <C> CC-News Test <C> Toronto Book Corpus Val <C> Toronto Book Corpus Test <R> <C> base LM (203M) <C> 18.41 <C> 17.57 <C> 16.16 <C> 18.29 <R> <C> RALM (LM+203M) <C> 17.01 <C> 16.17 <C> 15.71 <C> 17.85 <R> <C> BALM (408M) <C> 16.50 <C> 15.74 <C> 15.00 <C> 16.99 <R> <C> joint UniT (LM+203M) <C> 16.42-16.44 <C> 15.57-15.58 <C> 15.12-15.13 <C> 16.98-17.00 <R> <C> joint BiT-Base (LM+125M) <C> 15.32-15.35 <C> 14.61-14.64 <C> - <C> - <R> <C> joint BiT-Base* (LM+125M) <C> 15.11-15.17 <C> 14.37-14.42 <C> 14.14-14.16 <C> 15.72-15.74 <R> <C> joint BiT-Large* (LM+355M) <C> [BOLD] 14.59- [BOLD] 14.61 <C> [BOLD] 13.97- [BOLD] 14.00 <C> [BOLD] 13.80- [BOLD] 13.83 <C> [BOLD] 15.33- [BOLD] 15.36 <R> <C> Base LM-24L (203M) <C> 15.71 <C> 14.89 <C> 15.61 <C> 18.14 <R> <C> RALM-24L (LM-24L+203M) <C> 15.70 <C> 14.89 <C> 15.63 <C> 18.17 <R> <C> BALM-24L (408M) <C> 14.58 <C> 13.92 <C> 15.20 <C> 18.24 <R> <C> joint UniT (LM-24L+203M) <C> 14.59-14.61 <C> 13.81-13.82 <C> 15.12−15.16 <C> 17.46-17.48 <R> <C> joint BiT-Base (LM-24L+125M) <C> 13.68-13.69 <C> 13.01-13.03 <C> - <C> - <R> <C> joint BiT-Base* (LM-24L+125M) <C> 13.60-13.62 <C> 12.93-12.95 <C> 14.11-14.12 <C> 16.17-16.18 <R> <C> joint BiT-Med (LM-24L+203M) <C> 12.97-13.01 <C> 12.38-12.42 <C> - <C> - <R> <C> joint BiT-Large* (LM-24L+355M) <C> [BOLD] 12.71- [BOLD] 12.77 <C> [BOLD] 12.10- [BOLD] 12.16 <C> [BOLD] 13.30- [BOLD] 13.34 <C> [BOLD] 15.17- [BOLD] 15.22 <CAP> Table 1: Validation and test perplexity on CC-News and Toronto Book Corpus. * denotes models initialized with RoBERTa trained on additional data. The joint model perplexity ranges are estimated using 100,000 samples, see Eq. 4. The number of parameters of each model is shown in parentheses.
<R> <C> [EMPTY] <C> METEOR <C> SPICE <C> Div-1 <C> Div-2 <C> mBleu-4 <C> Vocab- <C> %Novel <R> <C> [EMPTY] <C> METEOR <C> SPICE <C> Div-1 <C> Div-2 <C> mBleu-4 <C> ulary <C> Sent <R> <C> Base <C> 0.265 <C> 0.186 <C> 0.31 <C> 0.44 <C> 0.68 <C> 1460 <C> 55.2 <R> <C> Adv <C> 0.236 <C> 0.166 <C> 0.41 <C> 0.55 <C> 0.51 <C> 2671 <C> 79.8 <R> <C> T=0.33 <C> 0.266 <C> 0.187 <C> 0.31 <C> 0.44 <C> 0.65 <C> 1219 <C> 58.3 <R> <C> T=0.5 <C> 0.260 <C> 0.183 <C> 0.37 <C> 0.55 <C> 0.47 <C> 1683 <C> 75.6 <R> <C> T=0.6 <C> 0.259 <C> 0.181 <C> 0.41 <C> 0.60 <C> 0.37 <C> 2093 <C> 83.7 <R> <C> T=0.7 <C> 0.250 <C> 0.174 <C> 0.45 <C> 0.66 <C> 0.28 <C> 2573 <C> 89.9 <R> <C> T=0.8 <C> 0.240 <C> 0.166 <C> 0.49 <C> 0.71 <C> 0.21 <C> 3206 <C> 94.4 <R> <C> T=0.9 <C> 0.228 <C> 0.157 <C> 0.54 <C> 0.77 <C> 0.14 <C> 3969 <C> 97.1 <R> <C> T=1 <C> 0.214 <C> 0.144 <C> 0.59 <C> 0.81 <C> 0.08 <C> 4875 <C> 98.7 <CAP> Table 2: Base and Adv are from Shetty et al. (2017). The other models are trained by us. All the methods use naive sampling decoding.
<R> <C> [EMPTY] <C> ROUGE <C> METEOR <C> CIDEr <C> SPICE <R> <C> FC(XE) <C> 0.543 <C> 0.258 <C> 1.006 <C> 0.187 <R> <C> ATTN(XE) <C> 0.562 <C> 0.272 <C> 1.109 <C> 0.201 <R> <C> ATTN-L(XE) <C> 0.561 <C> 0.275 <C> 1.116 <C> 0.205 <R> <C> Trans(XE) <C> 0.563 <C> 0.278 <C> 1.131 <C> 0.208 <R> <C> FC(RL) <C> 0.555 <C> 0.263 <C> 1.123 <C> 0.197 <R> <C> ATTN(RL) <C> 0.572 <C> 0.274 <C> 1.211 <C> 0.209 <R> <C> ATTN-L(RL) <C> 0.584 <C> 0.285 <C> 1.267 <C> 0.219 <R> <C> Trans(RL) <C> 0.589 <C> 0.291 <C> 1.298 <C> 0.230 <CAP> Table 3: The single caption result by different model architectures.
<R> <C> [ITALIC] R <C> English <C> German <C> French <C> Italian <C> Polish <C> Russian <R> <C> [ITALIC] time <C> 75 <C> 118 <C> 145 <C> 96 <C> 228 <C> 93 <R> <C> [ITALIC] face <C> 130 <C> 185 <C> 247 <C> 527 <C> 168 <C> 124 <R> <C> [ITALIC] home <C> 264 <C> 242 <C> 157 <C> 145 <C> 429 <C> 340 <CAP> Table 2: Ranks R of the lexical words used in Fig. 8.
<R> <C> [BOLD] Dataset <C> [BOLD] Model <C> [BOLD] BLEU <C> [BOLD] METEOR <C> [BOLD] TER <R> <C> Quda-5k <C> EDLPS <C> 16.1 <C> 5.2 <C> 107.6 <R> <C> Quda-5k <C> VAE-SVG <C> 28.0 <C> 11.7 <C> 89.0 <R> <C> Quda-5k <C> DiPS <C> 19.1 <C> 7.9 <C> 92.3 <R> <C> Quda-78k <C> EDLPS <C> 21.1 <C> 8.2 <C> 98.6 <R> <C> Quda-78k <C> VAE-SVG <C> 26.6 <C> 12.5 <C> 88.6 <R> <C> Quda-78k <C> DiPS <C> 24.1 <C> 9.5 <C> 94.8 <R> <C> Quora-50k <C> EDLPS <C> 29.6 <C> 14.5 <C> 87.9 <R> <C> Quora-50k <C> VAE-SVG <C> 40.8 <C> 21.6 <C> 74.4 <R> <C> Quora-50k <C> DiPS <C> 44.8 <C> 25.1 <C> 63.9 <R> <C> Quora-100k <C> EDLPS <C> 37.4 <C> 19.9 <C> 76.9 <R> <C> Quora-100k <C> VAE-SVG <C> 42.3 <C> 23.0 <C> 70.2 <R> <C> Quora-100k <C> DiPS <C> 45.9 <C> 25.9 <C> 63.1 <CAP> Table 2: The performance of three paraphrase generation models on four datasets measured using BLEU, METEOR, and TER.
<R> <C> [BOLD] Dataset <C> [BOLD] Vanilla LSTM Rationale Attention <C> [BOLD] Vanilla LSTM Rationale Length <C> [BOLD] Diversity LSTM Rationale Attention <C> [BOLD] Diversity LSTM Rationale Length <R> <C> SST <C> 0.348 <C> 0.240 <C> 0.624 <C> 0.175 <R> <C> IMDB <C> 0.472 <C> 0.217 <C> 0.761 <C> 0.169 <R> <C> Yelp <C> 0.438 <C> 0.173 <C> 0.574 <C> 0.160 <R> <C> Amazon <C> 0.346 <C> 0.162 <C> 0.396 <C> 0.240 <R> <C> Anemia <C> 0.611 <C> 0.192 <C> 0.739 <C> 0.237 <R> <C> Diabetes <C> 0.742 <C> 0.458 <C> 0.825 <C> 0.354 <R> <C> 20News <C> 0.627 <C> 0.215 <C> 0.884 <C> 0.173 <R> <C> Tweets <C> 0.284 <C> 0.225 <C> 0.764 <C> 0.306 <CAP> Table 3: Mean Attention given to the generated rationales with their mean lengths (in fraction)
<R> <C> Method <C> P <C> R <C> F1 <R> <C> CRF <C> 0.798 <C> 0.611 <C> 0.692 <R> <C> S-BLSTM <C> [BOLD] 0.844 <C> 0.673 <C> 0.749 <R> <C> SAN (-) BLSTM2 <C> 0.83 <C> 0.7 <C> 0.759 <R> <C> SAN <C> 0.839 <C> [BOLD] 0.721 <C> [BOLD] 0.776 <CAP> TABLE IV: Different methods for Function Need Recognition (FNR) in precision, recall and F1-score.
<R> <C> [BOLD] Corpus <C> [BOLD] Kneser-Ney discounting <C> [BOLD] Witten-Bell discounting <R> <C> Somali (train) <C> 811.03 <C> 880.64 <R> <C> Somali (cleaned web harvested text) <C> 502.11 <C> 541.86 <R> <C> Facebook post (cleaned) <C> 536.43 <C> 571.16 <R> <C> Facebook comments (uncleaned) <C> 1062.94 <C> 1161.36 <R> <C> RNN-LSTM generated text <C> - <C> 1155.51 <R> <C> Interpolated (row 2, 3, 4, 5) <C> [BOLD] 532.71 <C> 555.90 <R> <C> Interpolated (row 2, 3, 4, 5) +Generated text <C> - <C> [BOLD] 269.80 <CAP> Table 3: Perplexity analysis on Somali test set.
<R> <C> [EMPTY] <C> T <C> A <C> P <C> R <C> F <C> Q <C> X <C> V <C> N <R> <C> T <C> 160 <C> 8 <C> 2 <C> 0 <C> 0 <C> 3 <C> 10 <C> 6 <C> 1 <R> <C> A <C> 0 <C> 56 <C> 2 <C> 6 <C> 0 <C> 1 <C> 0 <C> 1 <C> 0 <R> <C> P <C> 0 <C> 0 <C> 31 <C> 0 <C> 0 <C> 0 <C> 1 <C> 0 <C> 0 <R> <C> R <C> 0 <C> 0 <C> 0 <C> 32 <C> 1 <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> F <C> 0 <C> 0 <C> 0 <C> 0 <C> 5 <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Q <C> 12 <C> 5 <C> 0 <C> 2 <C> 0 <C> 0 <C> 2 <C> 3 <C> 0 <R> <C> X <C> 2 <C> 0 <C> 1 <C> 5 <C> 0 <C> 0 <C> 17 <C> 8 <C> 0 <R> <C> V <C> 0 <C> 1 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> N <C> 0 <C> 0 <C> 1 <C> 0 <C> 0 <C> 0 <C> 6 <C> 0 <C> 1 <CAP> Table 8: Table of Counts for two annotators’ etymological conjectures on 392 words. Annotator 1’s conjectures follow the horizontal axis, and annotator 2’s the vertical.
<R> <C> [BOLD] Language <C> [BOLD] System <C> [BOLD] F1 (%) <R> <C> English <C> DBLP:journals/corr/GillickBVS15 <C> 86.50 <R> <C> English <C> yang2016multi <C> [BOLD] 90.94 <R> <C> English <C> 737 <C> [BOLD] 90.94 <R> <C> English <C> Our System <C> [BOLD] 90.94 <R> <C> Spanish <C> DBLP:journals/corr/GillickBVS15 <C> 82.95 <R> <C> Spanish <C> yang2016multi <C> 84.69 <R> <C> Spanish <C> 737 <C> [BOLD] 85.75 <R> <C> Spanish <C> Our System <C> 84.85 <R> <C> Dutch <C> DBLP:journals/corr/GillickBVS15 <C> 82.84 <R> <C> Dutch <C> yang2016multi <C> 85.00 <R> <C> Dutch <C> 737 <C> 81.74 <R> <C> Dutch <C> Our System <C> [BOLD] 85.20 <CAP> Table 2: Results on Monolingual NER task
<R> <C> Model <C> Dist-1 <C> # of UNI <C> Dist-2 <C> # of BI <C> BLEU-1 <C> BLEU-2 <C> BLEU-3 <C> BLEU-4 <R> <C> HRED Serban et al. ( 2016 ) <C> 0.011 <C> 918 <C> 0.045 <C> 3,875 <C> 33.0 <C> 4.5 <C> 1.1 <C> 0.3 <R> <C> REGS Li et al. ( 2017a ) <C> 0.021 <C> 1,205 <C> 0.097 <C> 5,552 <C> 38.4 <C> 6.8 <C> 2.0 <C> 0.7 <R> <C> DPGAN Xu et al. ( 2018 ) <C> 0.002 <C> 225 <C> 0.008 <C> 1,034 <C> 31.6 <C> 3.7 <C> 0.4 <C> 0.1 <R> <C> StepGAN Tuan and Lee ( 2019 ) <C> 0.013 <C> 1,063 <C> 0.065 <C> 5,283 <C> 36.1 <C> 6.6 <C> 1.9 <C> 0.6 <R> <C> Ours <C> 0.026 <C> 1,398 <C> 0.116 <C> 6,234 <C> 39.8 <C> 7.7 <C> 2.3 <C> 0.8 <CAP> Table 2: Automatic evaluation results of the number of distinct uni-grams (# of UNI) and bi-grams (# of BI), distinct-1 (Dist-1), distinct-2 (Dist-2), and BLEU scores.
<R> <C> [BOLD] Language <C> [BOLD] Dataset <C> [BOLD] IMS-Speech <C> [BOLD] State of the art <R> <C> English <C> WSJ eval’92 <C> 3.8 <C> 3.5  <R> <C> English <C> LibriSpeech test-clean <C> 4.4 <C> 3.2  <R> <C> English <C> LibriSpeech test-other <C> 12.7 <C> 7.6  <R> <C> English <C> TED-LIUM 3 test <C> 12.8 <C> 6.7  <R> <C> English <C> AMI IHM eval <C> 17.4 <C> 19.2 <R> <C> English <C> AMI SDM eval <C> 38.5 <C> 36.7 <R> <C> English <C> AMI MDM eval <C> 34.1 <C> 34.2 <R> <C> German <C> Tuda-De dev <C> 11.1 <C> 13.1  <R> <C> German <C> Tuda-De test <C> 12.0 <C> 14.4  <R> <C> German <C> Verbmobil 1 dev <C> 6.7 <C> 18.2  <R> <C> German <C> Verbmobil 1 test <C> 7.3 <C> 12.7  <CAP> Table 2: ASR performance comparison with state of the art results (WER, %)
<R> <C> Statistic <C> Value <R> <C> Total games <C> 5392 <R> <C> Win rate <C> 58.6% <R> <C> Total instructions <C> 76045 <R> <C> Unique instructions <C> 50669 <R> <C> Total words <C> 483650 <R> <C> Unique words <C> 5007 <R> <C> # words per instruction <C> 9.54 <R> <C> # instructions per game <C> 14.1 <CAP> Table 1: We gather a large language dataset for instruction generation and following. Major challenges include the wide range of unique instructions and the large number of low-level actions required to execute each instruction.
<R> <C> [EMPTY] <C> Attent <C> Lex ( [ITALIC] ϵ) <C> ML ( [ITALIC] λ=0.0) B <C> ML ( [ITALIC] λ=0.0) R <C> ML ( [ITALIC] λ=0.0) Rat. <C> ML ( [ITALIC] λ=0.8) B <C> ML ( [ITALIC] λ=0.8) R <C> ML ( [ITALIC] λ=0.8) Rat. <C> MR ( [ITALIC] λ=0.0) B <C> MR ( [ITALIC] λ=0.0) R <C> MR ( [ITALIC] λ=0.0) Rat. <R> <C> (1) <C> dot <C> No <C> 22.9 <C> 74.4 <C> 89.9 <C> 24.7 <C> 74.3 <C> 100.9 <C> 25.7 <C> 75.4 <C> 97.3 <R> <C> (2) <C> dot <C> Yes (10−4) <C> 23.0 <C> 74.6 <C> 91.0 <C> 24.5 <C> 74.2 <C> 100.4 <C> 25.3 <C> 75.3 <C> 99.2 <R> <C> (3) <C> dot <C> Yes (10−5) <C> 23.8 <C> 74.6 <C> 91.4 <C> 25.1 <C> 74.2 <C> 100.4 <C> 25.9 <C> 75.5 <C> 98.0 <R> <C> (4) <C> dot <C> Yes (10−6) <C> 23.7 <C> 74.4 <C> 92.1 <C> 25.3 <C> 74.3 <C> 99.6 <C> 26.2 <C> 76.0 <C> 98.6 <R> <C> (5) <C> MLP <C> Yes (10−4) <C> 23.7 <C> 75.3 <C> 88.5 <C> 25.5 <C> 75.2 <C> 97.9 <C> 26.9 <C> 76.3 <C> 98.8 <R> <C> (6) <C> MLP <C> Yes (10−5) <C> 23.7 <C> 75.1 <C> 90.5 <C> 25.3 <C> 74.8 <C> 98.6 <C> 26.4 <C> 75.9 <C> 97.7 <R> <C> (7) <C> MLP <C> Yes (10−6) <C> 23.9 <C> 74.6 <C> 89.4 <C> 25.8 <C> 74.6 <C> 99.3 <C> 26.3 <C> 75.7 <C> 97.3 <R> <C> (8) <C> (2)-(7) Ensemble <C> (2)-(7) Ensemble <C> - <C> - <C> - <C> 27.3 <C> 75.8 <C> 99.8 <C> 29.3 <C> 77.3 <C> 97.9 <CAP> Table 1: Overall BLEU, RIBES, and length ratio for systems with various types of attention (dot product or multi-layer perceptron), lexicon (yes/no and which value of λ), training algorithm (maximum likelihood or minimum risk), and word penalty value.
<R> <C> Model <C> Train Time <C> Eval Time <C> ACC <C> #Param. <R> <C> CNN <C> 57.0 <C> 2.6 <C> 64.8 <C> 848,228 <R> <C> BiLSTM <C> 92.1 <C> 4.6 <C> 64.5 <C> 147,928 <R> <C> LeftForest <C> 30.3 <C> 1.4 <C> 66.2 <C> 168,228 <CAP> Table 2: Efficiency evaluation.
<R> <C> Model <C> Accuracy <R> <C> BiLSTM <C> 62.7 <R> <C> CNN <C> 62.5 <R> <C> Tree <C> 63.8 <R> <C> Pyramid <C> 63.7 <R> <C> LeftForest <C> 64.6 <R> <C> RightForest <C> 64.5 <R> <C> BiForest <C> [BOLD] 65.2 <CAP> Table 3: Test set results.
<R> <C> [BOLD] Features <C> [BOLD] Accuracy (%) <C> [BOLD] 95% CI <R> <C> All <C> 88 <C> 76 – 95 <R> <C> Contextual <C> 80 <C> 66 – 90 <R> <C> Structural <C> 76 <C> 62 – 87 <R> <C> Lexical <C> 12 <C> 4.5 – 24 <R> <C> - Contextual <C> 82 <C> 69 – 91 <R> <C> - Structural <C> 80 <C> 66 – 90 <R> <C> - Lexical <C> 84 <C> 71 – 93 <R> <C> Summerscales (2014) <C> 76 <C> .. <R> <C> BANNER (baseline) <C> [EMPTY] <C> [EMPTY] <R> <C>  <C> 64 <C> .. <R> <C> de Brujin (2008) <C> 80 <C> .. <R> <C> Hansen (2008) <C> 97 <C> .. <R> <C> Xu (2007) <C> 92.5 <C> .. <CAP> Table 1: System accuracies for different feature combinations, and the 95% confidence intervals. - represents leave-out feature scores. Lower part of the table presents the performances of several other systems.
<R> <C> [BOLD] Architecture <C> [BOLD] 50h <C> [BOLD] 100h <C> [BOLD] 200h <C> [BOLD] 300h <R> <C> [ITALIC] RNN (dev) <C> [ITALIC] 29.6 <C> [ITALIC] 30.0 <C> [ITALIC] 30.0 <C> [ITALIC] 30.6 <R> <C> RNN (test) <C> 28.1 <C> 30.0 <C> 30.4 <C> 28.5 <R> <C> [ITALIC] RNN+2g (dev) <C> [ITALIC] 29.6 <C> [ITALIC] 28.7 <C> [ITALIC] 29.4 <C> [ITALIC] 29.8 <R> <C> RNN+2g (test) <C> 29.6 <C> 28.7 <C> 28.1 <C> 30.2 <R> <C> [ITALIC] RNN+3g (dev) <C> [ITALIC] 39.2 <C> [ITALIC] 39.4 <C> [ITALIC] 38.8 <C> [ITALIC] 36.5 <R> <C> RNN+3g (test) <C> 40.8 <C> 40.6 <C> 40.2 <C> 39.8 <R> <C> [ITALIC] RNN+4g (dev) <C> [ITALIC] 40.2 <C> [ITALIC] 40.6 <C> [ITALIC] 40.0 <C> [ITALIC] 40.2 <R> <C> RNN+4g (test) <C> 42.3 <C> 41.2 <C> 40.4 <C> 39.2 <CAP> Table 1: Accuracy of sequential RNN on the MSR Sentence Completion Challenge.
<R> <C> Slot Type <C> Impact of Attention (%) Local <C> Impact of Attention (%) Global-KB <C> Training Data Distribution (%) <C> F1 (%) <C> Wide Context Distribution (%) <C> Impact of Depen- dency Graph (%) <R> <C> state_of_death <C> 9.8 <C> -0.4 <C> 0.9 <C> 41.8 <C> 66.7 <C> 44.2 <R> <C> date_of_birth <C> 7.3 <C> 121.3 <C> [BOLD] 1.3 <C> [BOLD] 84.1 <C> [BOLD] 20.0 <C> [BOLD] -81.9 <R> <C> age <C> 4.1 <C> -5.3 <C> [BOLD] 1.3 <C> [BOLD] 98.5 <C> 15.9 <C> 28.5 <R> <C> per:alternate_names <C> -2.0 <C> 21.2 <C> 1.5 <C> 36.6 <C> 41.5 <C> 62.0 <R> <C> origin <C> -0.9 <C> 7.8 <C> 1.7 <C> 61.5 <C> 29.3 <C> 137.3 <R> <C> country_of_birth <C> 16.7 <C> 12.0 <C> 1.9 <C> 61.5 <C> 55.6 <C> 162.5 <R> <C> city_of_death <C> 1.1 <C> 3.3 <C> 1.9 <C> 61.3 <C> 70.3 <C> 24.4 <R> <C> state_of_headq. <C> 9.7 <C> -5.1 <C> [BOLD] 3.1 <C> [BOLD] 51.7 <C> 54.8 <C> 95.7 <R> <C> cities_of_residence <C> 4.5 <C> 5.7 <C> 3.5 <C> 57.3 <C> 77.0 <C> 40.5 <R> <C> states_of_residence <C> -4.3 <C> 2.3 <C> 3.8 <C> 50.5 <C> [BOLD] 45.9 <C> [BOLD] 175.8 <R> <C> country_of_headq. <C> 5.6 <C> -0.8 <C> [BOLD] 5.3 <C> [BOLD] 41.5 <C> 54.4 <C> 146.3 <R> <C> city_of_headq. <C> 1.6 <C> -6.9 <C> [BOLD] 6.7 <C> [BOLD] 30.3 <C> 54.9 <C> 39.3 <R> <C> employee__of <C> 14.9 <C> 4.9 <C> 7.3 <C> 65.9 <C> [BOLD] 54.9 <C> [BOLD] 132.5 <R> <C> countries_of_residence <C> 37.7 <C> 8.6 <C> 7.4 <C> 47.4 <C> 47.2 <C> 134.9 <CAP> Table 4: Comparison Analysis for Each Slot Type.
<R> <C> [BOLD] Experiments <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1 <R> <C> Majority baseline <C> 0.417 <C> 0.500 <C> 0.454 <R> <C> AESW all <C> 0.471* <C> 0.470 <C> 0.468 <R> <C> AESW plaintext <C> 0.511* <C> 0.515 <C> 0.473 <R> <C> ArgRewrite <C> 0.570* <C> 0.534 <C> 0.525* <R> <C> ArgRewrite + AESW all <C> 0.497* <C> 0.501 <C> 0.488* <R> <C> ArgRewrite + AESW plaintext <C> [BOLD] 0.574* <C> [BOLD] 0.555* <C> [BOLD] 0.551* <CAP> Table 3: 10-fold cross-validation performance. * indicates significantly better than majority (p<0.05). Bold indicates highest column value.
<R> <C> # of CPUs <C> time (s) 1 <C> time (s) 100 <C> time (s) 500 <R> <C> Covariance matrix <C> 9930 <C> 99 <C> 20 <R> <C> 1,000 Eigenvectors <C> 72 <C> - <C> - <R> <C> 10,000 Eigenvectors <C> 110 <C> - <C> - <R> <C> 50D Embeddings <C> 20 <C> 0.2 <C> 0.04 <R> <C> 100D Embeddings <C> 29 <C> 0.29 <C> 0.058 <R> <C> 200D Embeddings <C> 67 <C> 0.67 <C> 0.134 <R> <C> Total for 50D <C> 10,022 <C> 171.2 <C> 92.04 <CAP> Table 1: Benchmark of the experiment. Times are reported in seconds.
<R> <C> Attribute <C> Source <C> All <C> >0.4 <C> [0.3,0.4] <C> [0.2,0.3] <C> <0 <R> <C> IMD <C> [BOLD] Y! A <C> [BOLD] 115 <C> [BOLD] 1 <C> [BOLD] 48 <C> [BOLD] 66 <C> 0 <R> <C> IMD <C> Twitter <C> 17 <C> 0 <C> 10 <C> 7 <C> 0 <R> <C> Price <C> Y! A <C> 50 <C> 2 <C> 36 <C> 12 <C> 0 <R> <C> Price <C> [BOLD] Twitter <C> [BOLD] 1120 <C> [BOLD] 312 <C> [BOLD] 533 <C> [BOLD] 275 <C> 0 <R> <C> Jewish% <C> [BOLD] Y! A <C> [BOLD] 48 <C> [BOLD] 7 <C> [BOLD] 31 <C> [BOLD] 10 <C> 0 <R> <C> Jewish% <C> Twitter <C> 6 <C> 0 <C> 5 <C> 1 <C> 0 <R> <C> Muslim% <C> [BOLD] Y! A <C> [BOLD] 87 <C> 0 <C> [BOLD] 59 <C> [BOLD] 28 <C> 0 <R> <C> Muslim% <C> Twitter <C> 13 <C> [BOLD] 1 <C> 8 <C> 4 <C> 0 <R> <C> Hindu% <C> [BOLD] Y! A <C> [BOLD] 8 <C> [BOLD] 2 <C> 3 <C> [BOLD] 3 <C> 0 <R> <C> Hindu% <C> Twitter <C> 5 <C> 0 <C> 3 <C> 2 <C> 0 <R> <C> Buddhist% <C> Y! A <C> 1 <C> 0 <C> 1 <C> 0 <C> 0 <R> <C> Buddhist% <C> [BOLD] Twitter <C> [BOLD] 934 <C> [BOLD] 18 <C> [BOLD] 728 <C> [BOLD] 188 <C> 0 <R> <C> Black% <C> [BOLD] Y! A <C> [BOLD] 114 <C> [BOLD] 4 <C> [BOLD] 59 <C> [BOLD] 51 <C> 0 <R> <C> Black% <C> Twitter <C> 2 <C> 0 <C> 2 <C> 0 <C> 0 <R> <C> White% <C> [BOLD] Y! A <C> [BOLD] 8 <C> 0 <C> 0 <C> 0 <C> [BOLD] 8 <R> <C> White% <C> Twitter <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> Asian% <C> [BOLD] Y! A <C> [BOLD] 6 <C> 0 <C> [BOLD] 3 <C> [BOLD] 3 <C> 0 <R> <C> Asian% <C> Twitter <C> 1 <C> 0 <C> 1 <C> 0 <C> 0 <CAP> Table 2: Number of significantly correlated terms (p-value <0.01) from both Yahoo! Answers (Y! A) and Twitter.
<R> <C> [BOLD] Model <C> [BOLD] Model <C> [BOLD] Original Valid. <C> [BOLD] Original Test <C> [BOLD] Regularised Valid. <C> [BOLD] Regularised Test <R> <C> [BOLD] MultiNLI <C> cBiLSTM <C> 61.52 <C> 63.95 <C> [BOLD] 66.98 <C> [BOLD] 66.68 <R> <C> [BOLD] MultiNLI <C> DAM <C> 72.78 <C> 73.28 <C> [BOLD] 73.57 <C> [BOLD] 73.51 <R> <C> [BOLD] MultiNLI <C> ESIM <C> 73.66 <C> 75.22 <C> [BOLD] 75.72 <C> [BOLD] 75.80 <R> <C> [BOLD] SNLI <C> cBiLSTM <C> 81.41 <C> 80.99 <C> [BOLD] 82.27 <C> [BOLD] 81.12 <R> <C> [BOLD] SNLI <C> DAM <C> 86.96 <C> 86.29 <C> [BOLD] 87.08 <C> [BOLD] 86.43 <R> <C> [BOLD] SNLI <C> ESIM <C> 87.83 <C> 87.25 <C> [BOLD] 87.98 <C> [BOLD] 87.55 <CAP> Table 3: Accuracy on the SNLI and MultiNLI datasets with different neural NLI models before (left) and after (right) adversarial regularisation.
<R> <C> [BOLD] ModelDataset <C> A100DAM <C> A500DAM <C> A1000DAM <C> A100ESIM <C> A500ESIM <C> A1000ESIM <C> A100cBiLSTM <C> A500cBiLSTM <C> A1000cBiLSTM <R> <C> DAMAR <C> [BOLD] 83.33 <C> [BOLD] 79.15 <C> [BOLD] 79.37 <C> [BOLD] 71.35 <C> [BOLD] 72.19 <C> [BOLD] 70.05 <C> [BOLD] 93.00 <C> [BOLD] 88.99 <C> [BOLD] 86.00 <R> <C> DAM <C> 47.40 <C> 47.93 <C> 51.66 <C> 55.73 <C> 60.94 <C> 60.88 <C> 81.50 <C> 77.37 <C> 75.28 <R> <C> ESIMAR <C> [BOLD] 89.06 <C> [BOLD] 86.00 <C> [BOLD] 85.08 <C> [BOLD] 78.12 <C> [BOLD] 76.04 <C> [BOLD] 73.32 <C> [BOLD] 96.50 <C> [BOLD] 91.92 <C> [BOLD] 88.52 <R> <C> ESIM <C> 72.40 <C> 74.59 <C> 76.92 <C> 52.08 <C> 58.65 <C> 60.78 <C> 87.00 <C> 84.34 <C> 82.05 <R> <C> cBiLSTMAR <C> [BOLD] 85.42 <C> [BOLD] 80.39 <C> [BOLD] 78.74 <C> [BOLD] 73.96 <C> [BOLD] 70.52 <C> [BOLD] 65.39 <C> [BOLD] 92.50 <C> [BOLD] 88.38 <C> [BOLD] 83.62 <R> <C> cBiLSTM <C> 56.25 <C> 59.96 <C> 61.75 <C> 47.92 <C> 53.23 <C> 53.73 <C> 51.50 <C> 52.83 <C> 53.24 <CAP> Table 5: Accuracy of unregularised and regularised neural NLI models DAM, cBiLSTM, and ESIM, and their adversarially regularised versions DAMAR, cBiLSTMAR, and ESIMAR, on the datasets Akm.
<R> <C> Metric <C> TAC 2009 <C> DUC 2007 <R> <C> Cosine distance <C> 0.514 <C> 0.370 <R> <C> Euclidean distance <C> 0.489 <C> 0.364 <CAP> Table 1: Reference Waterfall KP-Centrality results with 40 key phrases, in terms of ROUGE–1.
<R> <C> [BOLD] DP <C> [BOLD] Set <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> DP Detection <C> Dev <C> 0.88 <C> 0.84 <C> 0.86 <R> <C> DP Detection <C> Test <C> 0.88 <C> 0.87 <C> 0.88 <R> <C> DP Prediction <C> Dev <C> 0.67 <C> 0.63 <C> 0.65 <R> <C> DP Prediction <C> Test <C> 0.67 <C> 0.65 <C> 0.66 <CAP> Table 4: Evaluation of DP generation quality.
<R> <C> sparse <C> dense <C> mask loss <C> [BOLD] Intent <C> [BOLD] Entities <R> <C> ✓ <C> ✗ <C> ✗ <C> 87.10±0.75 <C> 83.88±0.98 <R> <C> ✓ <C> ✗ <C> ✓ <C> 88.19±0.84 <C> 85.12±0.85 <R> <C> ✗ <C> GloVe <C> ✗ <C> 89.20±0.90 <C> 84.34±1.03 <R> <C> ✓ <C> GloVe <C> ✗ <C> 89.38±0.71 <C> 84.89±0.91 <R> <C> ✗ <C> GloVe <C> ✓ <C> 88.78±0.70 <C> 85.06±0.84 <R> <C> ✓ <C> GloVe <C> ✓ <C> 89.13±0.77 <C> 86.04±1.09 <R> <C> ✗ <C> BERT <C> ✗ <C> 87.44±0.92 <C> 84.20±0.91 <R> <C> ✓ <C> BERT <C> ✗ <C> 88.46±0.88 <C> 85.26±1.01 <R> <C> ✗ <C> BERT <C> ✓ <C> 86.92±1.09 <C> 83.96±1.33 <R> <C> ✓ <C> BERT <C> ✓ <C> 87.45±0.67 <C> 84.64±1.31 <R> <C> ✗ <C> ConveRT <C> ✗ <C> 89.76±0.98 <C> [BOLD] 86.06±1.38 <R> <C> ✓ <C> ConveRT <C> ✗ <C> [BOLD] 90.18±0.53 <C> 86.04±1.01 <R> <C> ✗ <C> ConveRT <C> ✓ <C> 90.15±0.68 <C> 85.76±0.80 <R> <C> ✓ <C> ConveRT <C> ✓ <C> 89.47±0.74 <C> 86.04±1.29 <CAP> Table 3: Comparison of different featurization and architecture components on NLU-Benchmark dataset. The three columns on the left indicate whether sparse features are used or not, what kind of dense features are used, if any, and whether the model was trained with a mask loss or not. The reported numbers are micro-averaged F1 scores.
<R> <C> Reranker <C> Models <C> MT06 <R> <C> - <C> Doc-transformer <C> 49.79 <R> <C> Doc-reranker <C> Proposal + LM <C> 49.79 <R> <C> Doc-reranker <C> Channel + LM <C> 51.93 <R> <C> Doc-reranker <C> Proposal + Channel <C> 50.40 <R> <C> Doc-reranker <C> Proposal + Channel + LM <C> [BOLD] 51.99 <CAP> Table 4: Effect of different components.
<R> <C> Method <C> Model <C> Unpaired Data <C> LM PPL <C> Test17 <C> Test18 <C> Test19 <R> <C> Baseline <C> transformer big <C> - <C> - <C> 23.9 <C> 23.9 <C> 24.5 <R> <C> This work <C> Doc-reranker <C> WMT <C> 106.3 <C> 24.9 <C> 26.0 <C> 27.1 <R> <C> This work <C> Doc-reranker <C> Gigaword + WMT <C> 63.8 <C> [BOLD] 25.5 <C> [BOLD] 26.3 <C> [BOLD] 27.1 <CAP> Table 5: SacreBLEU of different models on WMT19 validation and test sets and perplexity per word of the language models on the English side of WMT19 validation set.
<R> <C> [BOLD] Model <C> [BOLD] Flat <C> [BOLD] Hierarchical <R> <C> LSTM <C> 72.53 <C> 73.45 <R> <C> [BOLD] LSTM+GLoVe <C> [BOLD] 73.95 <C> [BOLD] 75.64 <R> <C> LSTM+GLoVe+Emo2Vec <C> 73.85 <C> 74.59 <R> <C> UTRS <C> 72.41 <C> 74.06 <R> <C> ELMo <C> 68.14 <C> 70.55 <R> <C> BERT <C> 66.12 <C> 73.29 <CAP> Table 2: The table shows F1 score on flat and hierarchical end-to-end models. GP denotes as Gaussian process.
<R> <C> [EMPTY] <C> EN → DE <C> DE → EN <R> <C> BAE-tr <C> 80.2 <C> 68.2 <R> <C> BAE-cr <C> 78.2 <C> 63.6 <R> <C> BAE-cr/corr <C> [BOLD] 91.8 <C> [BOLD] 72.8 <R> <C> Klementiev et al. <C> 77.6 <C> 71.1 <R> <C> MT <C> 68.1 <C> 67.4 <R> <C> Majority Class <C> 46.8 <C> 46.8 <CAP> Table 2: Classification Accuracy for training on English and German with 1000 labeled examples
<R> <C> [EMPTY] <C> Sent. per doc <C> EN → DE <C> DE → EN <R> <C> BAE-tr <C> 5 <C> 84.0 <C> 67.7 <R> <C> BAE-tr <C> 25 <C> 83.0 <C> 63.4 <R> <C> BAE-tr <C> 50 <C> 75.9 <C> 68.6 <R> <C> BAE-cr/corr <C> 5 <C> 91.75 <C> 72.78 <R> <C> BAE-cr/corr <C> 25 <C> 88.0 <C> 64.5 <R> <C> BAE-cr/corr <C> 50 <C> 90.2 <C> 49.2 <CAP> Table 3: Classification Accuracy for training on English and German with coarser alignments for 1000 labeled examples
<R> <C> Method <C> [ITALIC] F0.5 <R> <C> Baseline <C> 39.21 <R> <C> Random Perturbation <C> 39.90 <R> <C> AdvT-Text  <C> [BOLD] 42.28 <R> <C> [BOLD] iAdvT-Text (Ours) <C> 42.26 <R> <C> VAT-Text  <C> 41.81 <R> <C> [BOLD] iVAT-Text (Ours) <C> 41.88 <R> <C> BiLSTM w/Skipgram  <C> 41.1 <R> <C> BiLSTM w/GWE  <C> 41.4 <CAP> Table 6: Test performance (F0.5) on GED task: larger is better
<R> <C> Model <C> Accuracy <R> <C> This Model <C> 89.6 <R> <C> SA-LSTM  <C> 92.8 <R> <C> bmLSTM  <C> 92.9 <R> <C> TRNN  <C> 93.8 <R> <C> oh-LSTM  <C> 94.1 <R> <C> Virtual  <C> 94.1 <CAP> Table 1: Accuracy of the sentiment classifier compared to the state of the art
<R> <C> Model <C> Ratio changed <R> <C> All phrases <C> 50.8 <R> <C> Phrases longer than two words <C> 52.5 <R> <C> Phrases longer than five words <C> 53.0 <CAP> Table 2: Success rate of the autoencoder in changing the sentiment of phrases
<R> <C> [EMPTY] <C> Enc&Dec <C> Sparsity <C> #Params(M) % baseline <C> Quantization Size(MB) <C> WER VoiceSearch <C> WER YouTube <C> WER Telephony <C> RT(0.9) <R> <C> LSTM <C> LSTMx8 <C> 0% <C> 122.1 <C> Float,466MB <C> 6.6 <C> 19.5 <C> 8.1 <C> 3.223 <R> <C> (baseline) <C> LSTMx2 <C> 0% <C> 100% <C> Hybrid,117MB <C> 6.7 <C> 19.8 <C> 8.2 <C> 1.024 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> Integer,117MB <C> 6.7 <C> 19.8 <C> 8.2 <C> 1.013 <R> <C> Sparse LSTM <C> LSTMx8 <C> 50% <C> 69.7 <C> Float,270MB <C> 6.7 <C> 20.2 <C> 8.2 <C> 1.771 <R> <C> [EMPTY] <C> LSTMx2 <C> 50% <C> 57% <C> Hybrid,71MB <C> 6.8 <C> 20.4 <C> 8.4 <C> 0.888 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> Integer, 71MB <C> 6.9 <C> 22.9 <C> 8.7 <C> 0.869 <R> <C> Sparse CIFG <C> CIFGx8 <C> 50% <C> 56.3 <C> Float,219MB <C> 7.1 <C> 21.7 <C> 8.3 <C> 1.503 <R> <C> [EMPTY] <C> CIFGx2 <C> 50% <C> 46% <C> Hybrid,57MB <C> 7.2 <C> 21.4 <C> 8.5 <C> 0.743 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> Integer,57MB <C> 7.2 <C> 20.6 <C> 8.7 <C> 0.709 <CAP> Table 3: Comparison of float, hybrid and fully quantized models. RT factor is calculated on Pixel3 small cores.
<R> <C> [BOLD] Model <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> C-GCN <C> 68.9 <C> 66.3 <C> 67.6 <R> <C> C-AGGCN <C> 69.6 <C> 66 <C> 67.8 <R> <C> Google Bert(base) <C> 68.1 <C> 67.7 <C> 67.9 <R> <C> SpanBert(base) <C> 67.6 <C> 68.6 <C> 68.1 <R> <C> DG-SpanBERT(base) <C> 68.3 <C> [BOLD] 72.1 <C> 70.2 <R> <C> Google Bert(large) <C> 69.2 <C> 72 <C> 70.6 <R> <C> SpanBert(large) <C> 71.2 <C> 70.4 <C> 70.8 <R> <C> DG-SpanBERT(large) <C> [BOLD] 71.4 <C> 71.6 <C> [BOLD] 71.5 <CAP> Table 1: Compare DG-SpanBERT with other models on TACRED dataset. Bold indicates the best performance among all.
<R> <C> [BOLD] Model <C> [BOLD] Avg F1 <R> <C> CGCN <C> 49.8 <R> <C> AGGCN <C> 50.4 <R> <C> Google BERT <C> 55.1 <R> <C> SpanBERT <C> 55.6 <R> <C> DG-SpanBERT <C> [BOLD] 58.7 <CAP> Table 2: Compare average F1 performance for those token distances ≥ 11 tokens between the subject and object. Bold indicates the best performance among all.
<R> <C> [BOLD] Model <C> [BOLD] Lang <C> [BOLD] Split <C> [ITALIC] R@ [ITALIC] P0.8 <C> [ITALIC] R@ [ITALIC] P0.9 <R> <C> Quasi <C> En-De <C> Dev <C> 0.5111 <C> 0.4556 <R> <C> Quasi <C> En-De <C> Test <C> 0.4300 <C> 0.1933 <R> <C> Quasi <C> De-En <C> Dev <C> 0.7156 <C> 0.6261 <R> <C> Quasi <C> De-En <C> Test <C> 0.7441 <C> 0.5255 <R> <C> NMTEx <C> En-De <C> Dev <C> 0.2111 <C> 0.0667 <R> <C> NMTEx <C> En-De <C> Test <C> 0.2556 <C> 0.1700 <R> <C> NMTEx <C> De-En <C> Dev <C> 0.7729 <C> 0.6556 <R> <C> NMTEx <C> De-En <C> Test <C> 0.7678 <C> 0.4904 <CAP> Table 3: Experimental results on WMT17 dataset.
<R> <C> UAS <C> UAS Models <C> UAS Layer <C> UAS Arabic <C> UAS Hindi <C> UAS English <C> UAS French <C> UAS Spanish <C> UAS Portuguese <C> UAS Russian <C> UAS Chinese <C> Italian <R> <C> [EMPTY] <C> Iden <C> 0 <C> 0.817 <C> 0.914 <C> 0.793 <C> 0.836 <C> 0.851 <C> 0.844 <C> 0.859 <C> 0.775 <C> 0.904 <R> <C> [EMPTY] <C> Iden <C> 1 <C> 0.821 <C> [BOLD] 0.915 <C> 0.868 <C> 0.833 <C> 0.852 <C> 0.842 <C> 0.860 <C> 0.771 <C> 0.903 <R> <C> [EMPTY] <C> Iden <C> 2 <C> 0.820 <C> 0.914 <C> 0.843 <C> 0.833 <C> 0.856 <C> 0.841 <C> 0.859 <C> 0.773 <C> 0.901 <R> <C> [EMPTY] <C> PCA <C> 0 <C> 0.814 <C> 0.912 <C> 0.787 <C> 0.814 <C> 0.847 <C> 0.857 <C> 0.831 <C> 0.773 <C> 0.897 <R> <C> [EMPTY] <C> PCA <C> 1 <C> 0.815 <C> 0.912 <C> 0.865 <C> 0.807 <C> 0.846 <C> 0.855 <C> 0.828 <C> 0.759 <C> 0.899 <R> <C> [EMPTY] <C> PCA <C> 2 <C> 0.814 <C> 0.915 <C> 0.832 <C> 0.808 <C> 0.846 <C> 0.858 <C> 0.829 <C> 0.766 <C> 0.902 <R> <C> [EMPTY] <C> MLP <C> 0 <C> 0.830 <C> 0.918 <C> 0.742 <C> 0.856 <C> 0.829 <C> 0.869 <C> 0.852 <C> 0.797 <C> 0.910 <R> <C> [EMPTY] <C> MLP <C> 1 <C> 0.831 <C> 0.923 <C> 0.823 <C> 0.870 <C> 0.832 <C> 0.867 <C> 0.852 <C> 0.800 <C> 0.908 <R> <C> [EMPTY] <C> MLP <C> 2 <C> 0.833 <C> 0.918 <C> 0.787 <C> 0.859 <C> 0.813 <C> 0.871 <C> 0.849 <C> 0.790 <C> [BOLD] 0.914 <R> <C> [EMPTY] <C> VIBc <C> 0 <C> 0.852 <C> [BOLD] 0.915 <C> 0.866 <C> [BOLD] 0.879 <C> [BOLD] 0.881 <C> 0.871 <C> 0.862 <C> 0.800 <C> 0.831 <R> <C> [EMPTY] <C> VIBc <C> 1 <C> [BOLD] 0.860 <C> [BOLD] 0.913 <C> [BOLD] 0.871 <C> [BOLD] 0.877 <C> [BOLD] 0.880 <C> [BOLD] 0.877 <C> [BOLD] 0.865 <C> [BOLD] 0.814 <C> [BOLD] 0.913 <R> <C> [EMPTY] <C> VIBc <C> 2 <C> 0.851 <C> 0.894 <C> [BOLD] 0.880 <C> 0.876 <C> 0.879 <C> [BOLD] 0.877 <C> 0.843 <C> 0.768 <C> 0.878 <R> <C> [EMPTY] <C> POS <C> - <C> 0.722 <C> 0.819 <C> 0.762 <C> 0.800 <C> 0.802 <C> [BOLD] 0.808 <C> 0.739 <C> 0.570 <C> [BOLD] 0.843 <R> <C> [EMPTY] <C> VIBd <C> 0 <C> [BOLD] 0.783 <C> 0.823 <C> 0.784 <C> [BOLD] 0.821 <C> [BOLD] 0.821 <C> 0.793 <C> [BOLD] 0.777 <C> 0.671 <C> [BOLD] 0.855 <R> <C> [EMPTY] <C> VIBd <C> 1 <C> [BOLD] 0.784 <C> [BOLD] 0.862 <C> [BOLD] 0.825 <C> [BOLD] 0.822 <C> [BOLD] 0.822 <C> [BOLD] 0.805 <C> [BOLD] 0.776 <C> [BOLD] 0.691 <C> [BOLD] 0.857 <R> <C> [EMPTY] <C> VIBd <C> 2 <C> 0.754 <C> [BOLD] 0.861 <C> 0.816 <C> [BOLD] 0.822 <C> 0.812 <C> 0.790 <C> 0.768 <C> 0.672 <C> 0.849 <R> <C> LAS <C> LAS <C> LAS <C> LAS <C> LAS <C> LAS <C> LAS <C> LAS <C> LAS <C> LAS <C> LAS <C> [EMPTY] <R> <C> [EMPTY] <C> Models <C> layer <C> Arabic <C> Hindi <C> English <C> French <C> Spanish <C> Portuguese <C> Russian <C> Chinese <C> Italian <R> <C> [EMPTY] <C> Iden <C> 0 <C> 0.747 <C> [BOLD] 0.867 <C> 0.745 <C> 0.789 <C> 0.806 <C> 0.812 <C> 0.788 <C> 0.713 <C> [BOLD] 0.864 <R> <C> [EMPTY] <C> Iden <C> 1 <C> 0.751 <C> [BOLD] 0.870 <C> 0.824 <C> 0.784 <C> 0.808 <C> 0.813 <C> 0.783 <C> 0.709 <C> [BOLD] 0.863 <R> <C> [EMPTY] <C> Iden <C> 2 <C> 0.743 <C> 0.867 <C> 0.798 <C> 0.782 <C> 0.811 <C> 0.813 <C> 0.787 <C> 0.713 <C> 0.861 <R> <C> [EMPTY] <C> PCA <C> 0 <C> 0.746 <C> 0.864 <C> 0.742 <C> 0.758 <C> 0.804 <C> 0.811 <C> 0.781 <C> 0.706 <C> 0.856 <R> <C> [EMPTY] <C> PCA <C> 1 <C> 0.743 <C> [BOLD] 0.866 <C> 0.823 <C> 0.749 <C> 0.802 <C> 0.808 <C> 0.777 <C> 0.697 <C> 0.857 <R> <C> [EMPTY] <C> PCA <C> 2 <C> 0.744 <C> [BOLD] 0.870 <C> 0.787 <C> 0.750 <C> 0.801 <C> 0.811 <C> 0.780 <C> 0.700 <C> [BOLD] 0.865 <R> <C> [EMPTY] <C> MLP <C> 0 <C> 0.754 <C> [BOLD] 0.869 <C> 0.801 <C> 0.814 <C> 0.772 <C> 0.817 <C> 0.798 <C> 0.739 <C> [BOLD] 0.871 <R> <C> [EMPTY] <C> MLP <C> 1 <C> 0.759 <C> [BOLD] 0.871 <C> 0.839 <C> 0.816 <C> [BOLD] 0.835 <C> 0.821 <C> 0.800 <C> 0.734 <C> [BOLD] 0.867 <R> <C> [EMPTY] <C> MLP <C> 2 <C> 0.760 <C> [BOLD] 0.871 <C> 0.834 <C> 0.814 <C> 0.755 <C> 0.822 <C> 0.797 <C> 0.726 <C> 0.869 <R> <C> [EMPTY] <C> VIBc <C> 0 <C> [BOLD] 0.778 <C> 0.865 <C> 0.822 <C> 0.822 <C> [BOLD] 0.839 <C> 0.827 <C> 0.807 <C> 0.739 <C> 0.862 <R> <C> [EMPTY] <C> VIBc <C> 1 <C> [BOLD] 0.779 <C> [BOLD] 0.866 <C> [BOLD] 0.851 <C> [BOLD] 0.828 <C> [BOLD] 0.837 <C> [BOLD] 0.836 <C> [BOLD] 0.814 <C> [BOLD] 0.754 <C> [BOLD] 0.867 <R> <C> [EMPTY] <C> VIBc <C> 2 <C> 0.777 <C> 0.838 <C> 0.840 <C> [BOLD] 0.826 <C> 0.840 <C> 0.829 <C> 0.786 <C> 0.710 <C> 0.818 <R> <C> [EMPTY] <C> POS <C> - <C> 0.652 <C> 0.713 <C> 0.712 <C> 0.718 <C> [BOLD] 0.739 <C> [BOLD] 0.743 <C> [BOLD] 0.662 <C> 0.510 <C> 0.779 <R> <C> [EMPTY] <C> VIBd <C> 0 <C> [BOLD] 0.671 <C> 0.702 <C> 0.721 <C> [BOLD] 0.723 <C> [BOLD] 0.724 <C> 0.710 <C> 0.648 <C> 0.544 <C> [BOLD] 0.780 <R> <C> [EMPTY] <C> VIBd <C> 1 <C> [BOLD] 0.672 <C> [BOLD] 0.736 <C> [BOLD] 0.742 <C> [BOLD] 0.723 <C> [BOLD] 0.725 <C> 0.710 <C> [BOLD] 0.651 <C> [BOLD] 0.591 <C> [BOLD] 0.781 <R> <C> [EMPTY] <C> VIBd <C> 2 <C> 0.643 <C> [BOLD] 0.735 <C> [BOLD] 0.741 <C> 0.721 <C> 0.719 <C> 0.698 <C> 0.646 <C> 0.566 <C> 0.763 <CAP> Table 3: Parsing accuracy of 9 languages (LAS and UAS); Table 2 is a subset of this table. Black rows use continuous tags; gray rows use discrete tags (which does worse). The “layer” column indicates the ELMo layer we use. In each column, the best score for each color is boldfaced, along with all results of that color that are not significantly worse (paired permutation test, p<0.05).
<R> <C> [EMPTY] <C> [EMPTY] <C> [BOLD] Trump PRO <C> [BOLD] Trump ANTI <C> [BOLD] Trump AVG <C> [BOLD] ED PRO <C> [BOLD] ED ANTI <C> [BOLD] ED AVG <R> <C> [EMPTY] <C> P <C> 0.84 <C> 0.97 <C> 0.91 <C> 0.49 <C> 1.00 <C> 0.74 <R> <C> [EMPTY] <C> R <C> 0.98 <C> 0.78 <C> 0.88 <C> 1.00 <C> 0.58 <C> 0.79 <R> <C> [ITALIC] SVM200 <C> F <C> 0.91 <C> 0.86 <C> 0.89 <C> 0.65 <C> 0.74 <C> 0.69 <R> <C> [EMPTY] <C> P <C> 0.96 <C> 0.91 <C> 0.94 <C> 0.99 <C> 0.90 <C> 0.95 <R> <C> [EMPTY] <C> R <C> 0.93 <C> 0.96 <C> 0.95 <C> 0.74 <C> 1.00 <C> 0.87 <R> <C> [ITALIC] SVM [ITALIC] xval <C> F <C> 0.94 <C> 0.93 <C> 0.94 <C> 0.85 <C> 0.95 <C> 0.90 <R> <C> [EMPTY] <C> P <C> 0.94 <C> 0.93 <C> 0.94 <C> 0.99 <C> 0.98 <C> 0.99 <R> <C> [EMPTY] <C> R <C> 0.72 <C> 0.77 <C> 0.75 <C> 0.79 <C> 0.73 <C> 0.76 <R> <C> [ITALIC] UnSup <C> F <C> 0.82 <C> 0.84 <C> 0.83 <C> 0.88 <C> 0.84 <C> 0.86 <R> <C> [EMPTY] <C> P <C> 0.9 <C> 0.87 <C> 0.89 <C> 0.88 <C> 0.92 <C> 0.90 <R> <C> [EMPTY] <C> R <C> 0.82 <C> 0.85 <C> 0.84 <C> 0.72 <C> 0.85 <C> 0.79 <R> <C> [ITALIC] Ours <C> F <C> 0.86 <C> 0.86 <C> 0.86 <C> 0.79 <C> 0.89 <C> 0.84 <CAP> Table 2: Benchmark results for Trump and Erdoğan datasets.
<R> <C> [EMPTY] <C> PRO <C> ANTI <C> HDP <C> IYI <C> CHP <C> Avg. <R> <C> P <C> 0.88 <C> 0.74 <C> 0.68 <C> 0.94 <C> 0.90 <C> 0.83 <R> <C> R <C> 0.72 <C> 0.83 <C> 0.77 <C> 0.69 <C> 0.43 <C> 0.69 <R> <C> F <C> 0.79 <C> 0.78 <C> 0.72 <C> 0.80 <C> 0.59 <C> 0.74 <CAP> Table 3: Aligning clusters with party affiliations with the manually labeled users on the ED dataset.
<R> <C> Topic <C> pro-Erdoğan <C> anti-Erdoğan <R> <C> Arab <C> 78.78 <C> 88.83 <R> <C> CHP <C> 83.65 <C> 86.32 <R> <C> Erdoğan <C> 92.74 <C> 91.95 <R> <C> HDP <C> 82.34 <C> 84.85 <R> <C> PKK <C> 78.27 <C> 79.76 <R> <C> Syrian <C> 83.35 <C> 85.8 <R> <C> Trump <C> 83.62 <C> 82.58 <R> <C> USA <C> 84.19 <C> 85.91 <CAP> Table 5: Cluster labels overlap with label propagation
<R> <C> Target <C> RWC <R> <C> Arabs <C> 0.81 <R> <C> CHP <C> 0.54 <R> <C> Erdoğan <C> 0.89 <R> <C> HDP <C> 0.88 <R> <C> PKK <C> 0.71 <R> <C> Syrians <C> 0.69 <R> <C> Trump <C> 0.62 <R> <C> USA <C> 0.56 <CAP> Table 6: RWC polarization measure across targets.
<R> <C> method <C> Human <C> ASTS <R> <C> random <C> 1.68 <C> 0.08 <R> <C> learned <C> 2.61 <C> 0.23 <R> <C> rule <C> 3.15 <C> 0.32 <R> <C> combo <C> [BOLD] 3.73 <C> [BOLD] 0.44 <CAP> Table 3: Average human ratings (out of 7) and aligned scene template similarity scores.
<R> <C> [ITALIC] β <C> [ITALIC] α 0.1 <C> [ITALIC] α 0.25 <C> [ITALIC] α 0.5 <C> [ITALIC] α 1 <C> [ITALIC] α 2.5 <C> [ITALIC] α 5 <R> <C> 0.1 <C> 50 <C> 50 <C> 50 <C> 50 <C> 50 <C> 50 <R> <C> 0.5 <C> 50 <C> 50 <C> 50 <C> 50 <C> 50 <C> 50 <R> <C> 1 <C> 50 <C> 50 <C> 50 <C> 50 <C> 50 <C> 50 <R> <C> 1.5 <C> 50 <C> 50 <C> 50 <C> 50 <C> 50 <C> 100 <R> <C> 2 <C> 50 <C> 50 <C> 50 <C> 50 <C> 100 <C> 100 <R> <C> 2.5 <C> 50 <C> 50 <C> 100 <C> 75 <C> 100 <C> 100 <R> <C> 3 <C> 50 <C> 100 <C> 100 <C> 100 <C> 100 <C> 100 <R> <C> 3.5 <C> 100 <C> 100 <C> 100 <C> 100 <C> 100 <C> 100 <R> <C> 4 <C> 100 <C> 100 <C> 100 <C> 100 <C> 100 <C> 100 <R> <C> 5 <C> 100 <C> 100 <C> 100 <C> 100 <C> 100 <C> 100 <R> <C> 10 <C> 100 <C> 100 <C> 100 <C> 100 <C> 100 <C> 100 <R> <C> 20 <C> 100 <C> 100 <C> 100 <C> 100 <C> 100 <C> 100 <CAP> (a) Method 1
<R> <C> [ITALIC] β <C> [ITALIC] α 0.1 <C> [ITALIC] α 0.25 <C> [ITALIC] α 0.5 <C> [ITALIC] α 0.75 <C> [ITALIC] α 1 <R> <C> 0.1 <C> 50 <C> 50 <C> 50 <C> 50 <C> 50 <R> <C> 0.5 <C> 50 <C> 50 <C> 50 <C> 50 <C> 50 <R> <C> 1 <C> 50 <C> 50 <C> 50 <C> 50 <C> 50 <R> <C> 1.5 <C> 50 <C> 50 <C> 50 <C> 50 <C> 50 <R> <C> 2 <C> 50 <C> 50 <C> 50 <C> 50 <C> 50 <R> <C> 2.5 <C> 50 <C> 50 <C> 50 <C> 50 <C> 50 <R> <C> 3 <C> 50 <C> 50 <C> 100 <C> 50 <C> 50 <R> <C> 3.5 <C> 50 <C> 50 <C> 100 <C> 50 <C> 50 <R> <C> 4 <C> 50 <C> 100 <C> 100 <C> 50 <C> 50 <R> <C> 5 <C> 50 <C> 50 <C> 100 <C> 100 <C> 50∗ <R> <C> 10 <C> 75 <C> 100 <C> 100 <C> 100 <C> 50∗ <R> <C> 20 <C> 100 <C> 100 <C> 100 <C> 50∗ <C> 50∗ <CAP> (b) Method 2
<R> <C> [EMPTY] <C> Twitter acc. <C> Twitter no VR <C> NER  [ITALIC] F1 <C> NER no VR <R> <C> BiGRU baseline <C> 90.8 <C> - <C> 87.6 <C> - <R> <C> VSL-G <C> 91.1 <C> 90.9 <C> 87.8 <C> 87.7 <R> <C> VSL-GG-Flat <C> 91.4 <C> 90.9 <C> 88.0 <C> 87.8 <R> <C> VSL-GG-Hier <C> 91.6 <C> 91.0 <C> 88.4 <C> 87.9 <CAP> Table 4: Results on Twitter and NER dev sets. For each model, we show supervised results for the models with variational regularization (“acc.” or F1) and results when replacing variational components with their deterministic counterparts (“no VR”).
<R> <C> Toolkit <C> Runtime [sec] <C> Memory [GB] <C> FER [%] <R> <C> RETURNN <C> 198 <C> 2.4 <C> 42.51 <R> <C> Theano LSTM <C> 366 <C> 3.2 <C> 42.63 <R> <C> Keras <C> 619 <C> 5.4 <C> 44.36 <R> <C> TensorFlow <C> 693 <C> ∼ 7.2 <C> 47.41 <R> <C> Torch (CuDNN) <C> 164 <C> ∼ 2.6 <C> 43.02 <CAP> Table 1: Comparison of runtime and memory requirement for different software packages. The numbers were averaged over 10 training epochs. Note that precise memory usage estimates for Torch and TensorFlow can not be obtained due to their internal memory management.
<R> <C> [BOLD] Emotion <C> [ITALIC] TIED-128-5 <C> [ITALIC] MLP <C> [ITALIC] COVAREP <R> <C> Happy <C> 36.04 <C> 41.56 <C> 31.03 <R> <C> Angry <C> 44.95 <C> 24.03 <C> 32.31 <R> <C> Sad <C> 73.80 <C> 66.55 <C> 50.17 <R> <C> Neutral <C> 41.59 <C> 48.58 <C> 61.48 <R> <C> Weighted Accuracy <C> 48.10 <C> 46.75 <C> 44.67 <R> <C> Unweighted Accuracy <C> 49.09 <C> 45.18 <C> 43.74 <CAP> Table 2: Emotion classification accuracy (utterance level) for our approach, compared with (1) MLP without pre-training (2) softmax classifier trained on COVAREP features
<R> <C> [EMPTY] <C> [BOLD] Model <C> [BOLD] NDCG <C> [BOLD] MRR <C> [BOLD] R@1 <C> [BOLD] R@5 <C> [BOLD] R@10 <C> [BOLD] Mean Rank <R> <C> v0.9 val <C> Answer prior <C> - <C> 0.3735 <C> 23.55 <C> 48.52 <C> 53.23 <C> 26.50 <R> <C> v0.9 val <C> NN-Q <C> - <C> 0.4570 <C> 35.93 <C> 54.07 <C> 60.26 <C> 18.93 <R> <C> v0.9 val <C> NN-QI <C> - <C> 0.4274 <C> 33.13 <C> 50.83 <C> 58.69 <C> 19.62 <R> <C> [EMPTY] <C> LF-Q-G <C> - <C> 0.5048 <C> 39.78 <C> 60.58 <C> 66.33 <C> 17.89 <R> <C> [EMPTY] <C> LF-QI-G <C> - <C> 0.5204 <C> 42.04 <C> 61.65 <C> 67.66 <C> 16.84 <R> <C> [EMPTY] <C> LF-QIH-G <C> - <C> 0.5199 <C> 41.83 <C> 61.78 <C> 67.59 <C> 17.07 <R> <C> [EMPTY] <C> HRE-QIH-G <C> - <C> 0.5237 <C> [BOLD] 42.29 <C> 62.18 <C> 67.92 <C> 17.07 <R> <C> [EMPTY] <C> HREA-QIH-G <C> - <C> 0.5242 <C> 42.28 <C> 62.33 <C> 68.17 <C> 16.79 <R> <C> [EMPTY] <C> MN-QIH-G <C> - <C> [BOLD] 0.5259 <C> [BOLD] 42.29 <C> [BOLD] 62.85 <C> [BOLD] 68.88 <C> 17.06 <R> <C> 2-8 <C> A-Q (Massiceti  massiceti_nipsw18  ) <C> - <C> 0.3031 <C> 16.77 <C> 44.86 <C> 58.06 <C> [BOLD] 16.21 <R> <C> [EMPTY] <C> A-QI (Massiceti  massiceti_nipsw18  ) <C> - <C> 0.2427 <C> 12.17 <C> 35.38 <C> 50.57 <C> 18.29 <R> <C> v1.0 test-std <C> LF-QIH-G <C> 0.5121 <C> 0.4568 <C> [BOLD] 35.08 <C> 55.92 <C> [BOLD] 64.02 <C> 18.81 <R> <C> [EMPTY] <C> HRE-QIH-G <C> 0.5245 <C> 0.4561 <C> 34.78 <C> 56.18 <C> 63.72 <C> 18.78 <R> <C> [EMPTY] <C> MN-QIH-G <C> [BOLD] 0.5280 <C> [BOLD] 0.4580 <C> 35.05 <C> [BOLD] 56.35 <C> 63.92 <C> 19.31 <R> <C> 2-8 <C> A-Q (Massiceti  massiceti_nipsw18  ) <C> - <C> 0.2832 <C> 15.95 <C> 40.10 <C> 55.10 <C> [BOLD] 17.08 <R> <C> [EMPTY] <C> A-QI (Massiceti  massiceti_nipsw18  ) <C> - <C> 0.2393 <C> 12.73 <C> 33.05 <C> 48.68 <C> 19.24 <CAP> Table 1: Performance of methods on VisDial v0.9 and v1.0, measured by normalized discounted cumulative gain (NDCG), mean reciprocal rank (MRR), recall@k and mean rank. Higher is better for NDCG, MRR, and recall@k, while lower is better for mean rank.
<R> <C> (→) extrinsic task <C> document <C> dependency <R> <C> (↓) intrinsic metric <C> classification <C> parsing <R> <C> word similarity <C> 0.386 <C> 0.007 <R> <C> word translation <C> 0.066 <C> -0.292 <R> <C> multiqvec <C> 0.635 <C> [BOLD] 0.444 <R> <C> multiqvec-cca <C> [BOLD] 0.896 <C> 0.273 <CAP> Table 1: Correlations between intrinsic evaluation metrics (rows) and downstream task performance (columns).
<R> <C> [BOLD] Type <C> [BOLD] 3 training epochs for CBOW and Skip-gram, 10 training epochs for GloVe.  [BOLD] 1_50 <C> [BOLD] 3 training epochs for CBOW and Skip-gram, 10 training epochs for GloVe.  [BOLD] 1_100 <C> [BOLD] 3 training epochs for CBOW and Skip-gram, 10 training epochs for GloVe.  [BOLD] 1_300 <C> [BOLD] 3 training epochs for CBOW and Skip-gram, 10 training epochs for GloVe.  [BOLD] 1_500 <C> [BOLD] 3 training epochs for CBOW and Skip-gram, 10 training epochs for GloVe.  [BOLD] 5_50 <C> [BOLD] 3 training epochs for CBOW and Skip-gram, 10 training epochs for GloVe.  [BOLD] 5_100 <C> [BOLD] 3 training epochs for CBOW and Skip-gram, 10 training epochs for GloVe.  [BOLD] 5_300 <C> [BOLD] 3 training epochs for CBOW and Skip-gram, 10 training epochs for GloVe.  [BOLD] 5_500 <C> [BOLD] 3 training epochs for CBOW and Skip-gram, 10 training epochs for GloVe.  [BOLD] 10_50 <C> [BOLD] 3 training epochs for CBOW and Skip-gram, 10 training epochs for GloVe.  [BOLD] 10_100 <C> [BOLD] 3 training epochs for CBOW and Skip-gram, 10 training epochs for GloVe.  [BOLD] 10_300 <C> [BOLD] 3 training epochs for CBOW and Skip-gram, 10 training epochs for GloVe.  [BOLD] 10_500 <R> <C> [BOLD] CBOW – semantics <C> 4.77 <C> 6.57 <C> 8.00 <C> 6.33 <C> 9.90 <C> 12.75 <C> 15.64 <C> 12.63 <C> 12.11 <C> 15.92 <C> 19.6 <C> 16.15 <R> <C> [BOLD] Skip-gram – semantics <C> 2.00 <C> 4.75 <C> 6.66 <C> x <C> 3.71 <C> 7.57 <C> 9.62 <C> x <C> 3.30 <C> 7.62 <C> 10.21 <C> x <R> <C> [BOLD] GloVe – semantics <C> 1.01 <C> 1.21 <C> 0.69 <C> 0.78 <C> 2.38 <C> 2.76 <C> 2.82 <C> 2.56 <C> 3.19 <C> 3.87 <C> 4.30 <C> 3.63 <R> <C> [BOLD] CBOW – syntactics <C> 11.06 <C> 15.81 <C> 19.40 <C> 16.84 <C> 17.85 <C> 22.76 <C> 26.84 <C> 25.65 <C> 20.48 <C> 26.37 <C> 30.00 <C> 28.60 <R> <C> [BOLD] Skip-gram – syntactics <C> 2.51 <C> 5.51 <C> 6.81 <C> x <C> 4.30 <C> 7.88 <C> 10.54 <C> x <C> 5.02 <C> 8.79 <C> 10.24 <C> x <R> <C> [BOLD] GloVe – syntactics <C> 4.86 <C> 5.11 <C> 3.50 <C> 0.98 <C> 8.20 <C> 9.11 <C> 7.10 <C> 3.72 <C> 9.59 <C> 10.77 <C> 9.40 <C> 5.26 <R> <C> [EMPTY] <C> [BOLD] 10 training epochs for CBOW and Skip-gram, 25 training epochs for GloVe. <C> [BOLD] 10 training epochs for CBOW and Skip-gram, 25 training epochs for GloVe. <C> [BOLD] 10 training epochs for CBOW and Skip-gram, 25 training epochs for GloVe. <C> [BOLD] 10 training epochs for CBOW and Skip-gram, 25 training epochs for GloVe. <C> [BOLD] 10 training epochs for CBOW and Skip-gram, 25 training epochs for GloVe. <C> [BOLD] 10 training epochs for CBOW and Skip-gram, 25 training epochs for GloVe. <C> [BOLD] 10 training epochs for CBOW and Skip-gram, 25 training epochs for GloVe. <C> [BOLD] 10 training epochs for CBOW and Skip-gram, 25 training epochs for GloVe. <C> [BOLD] 10 training epochs for CBOW and Skip-gram, 25 training epochs for GloVe. <C> [BOLD] 10 training epochs for CBOW and Skip-gram, 25 training epochs for GloVe. <C> [BOLD] 10 training epochs for CBOW and Skip-gram, 25 training epochs for GloVe. <C> [BOLD] 10 training epochs for CBOW and Skip-gram, 25 training epochs for GloVe. <R> <C> [EMPTY] <C> [BOLD] 1_50 <C> [BOLD] 1_100 <C> [BOLD] 1_300 <C> [BOLD] 1_500 <C> [BOLD] 5_50 <C> [BOLD] 5_100 <C> [BOLD] 5_300 <C> [BOLD] 5_500 <C> [BOLD] 10_50 <C> [BOLD] 10_100 <C> [BOLD] 10_300 <C> [BOLD] 10_500 <R> <C> [BOLD] CBOW – semantics <C> 6.77 <C> 10.90 <C> 11.42 <C> 10.03 <C> 13.91 <C> 19.78 <C> 22.35 <C> 19.12 <C> 17.02 <C> 23.23 <C> 26.90 <C> 24.20 <R> <C> [BOLD] Skip-gram – semantics <C> 1.89 <C> 4.40 <C> 4.83 <C> 4.23 <C> 5.07 <C> 9.69 <C> 10.13 <C> 8.52 <C> 7.21 <C> 12.40 <C> 13.14 <C> 11.79 <R> <C> [BOLD] GloVe – semantics <C> 0.95 <C> 1.38 <C> 0.93 <C> 0.90 <C> 2.38 <C> 3.26 <C> 3.57 <C> 2.92 <C> 3.32 <C> 4.35 <C> 5.47 <C> 4.45 <R> <C> [BOLD] CBOW – syntax <C> 18.92 <C> 23.07 <C> 24.01 <C> 22.63 <C> 27.10 <C> 31.24 <C> 33.69 <C> 31.9 <C> 30.34 <C> 35.61 <C> 38.03 <C> 35.59 <R> <C> [BOLD] Skip-gram – syntax <C> 4.67 <C> 8.72 <C> 7.27 <C> 6.04 <C> 9.91 <C> 14.19 <C> 12.63 <C> 12.05 <C> 11.93 <C> 16.16 <C> 15.59 <C> 15.94 <R> <C> [BOLD] GloVe – syntax <C> 7.06 <C> 7.94 <C> 4.09 <C> 1.35 <C> 11.31 <C> 12.63 <C> 8.81 <C> 4.95 <C> 14.14 <C> 14.80 <C> 11.33 <C> 7.17 <CAP> Table 4: Accuracy on semantic and syntactic part of corpus.
<R> <C> [ITALIC] Step <C> [ITALIC] Step <C> [BOLD] Time Series SpikeM <C> [BOLD] Time Series LSTM <C> [BOLD] Time Series + Text  [BOLD] C [ITALIC] rand <C> [BOLD] Time Series + Text  [BOLD] C [ITALIC] words <C> [BOLD] Time Series + Text  [BOLD] C [ITALIC] topics <C> [BOLD] Time Series + Text  [BOLD] C [ITALIC] senti <C> [BOLD] Time Series + Text  [BOLD] C [ITALIC] comp <R> <C> [BOLD] Stock <C> 1 <C> 102.13 <C> 6.80 <C> 3.63 <C> 2.97 <C> 3.01 <C> 3.34 <C> 1.96 <R> <C> [BOLD] Stock <C> 3 <C> 99.8 <C> 7.51 <C> 4.47 <C> 4.22 <C> 4.65 <C> 4.87 <C> 3.78 <R> <C> [BOLD] Stock <C> 5 <C> 97.99 <C> 7.79 <C> 5.32 <C> 5.25 <C> 5.44 <C> 5.95 <C> 5.28 <R> <C> [BOLD] Poll <C> 1 <C> 10.13 <C> 1.46 <C> 1.52 <C> 1.27 <C> 1.59 <C> 2.09 <C> 1.11 <R> <C> [BOLD] Poll <C> 3 <C> 10.63 <C> 1.89 <C> 1.84 <C> 1.56 <C> 1.88 <C> 1.94 <C> 1.49 <R> <C> [BOLD] Poll <C> 5 <C> 11.13 <C> 2.04 <C> 2.15 <C> 1.84 <C> 1.88 <C> 1.96 <C> 1.82 <CAP> Table 5: Forecasting errors (RMSE) on Stock and Poll data with time series only (SpikeM and LSTM) and with time series plus text feature (random, words, topics, sentiment, and composition).
<R> <C> [EMPTY] <C> B@1 <C> B@3A <C> B@5A <R> <C> [BOLD] S2S <C> 10.15 <C> 8.80 <C> 8.69 <R> <C> [BOLD] S2S + WE <C> 11.86 <C> 10.78 <C> 10.04 <R> <C> [BOLD] S2S + WE + REL <C> 12.42 <C> 12.28 <C> 11.53 <CAP> Table 7: BLEU ranking. Additional word representation +WE and relation specific alignment +REL help the model learn the cause and effect generation task especially for diverse patterns.
<R> <C> Previous Models <C> [BOLD] Model ECNU  <C> [BOLD] Model ECNU  <C> [BOLD] r 0.8414 <C> [BOLD] MSE — <R> <C> Previous Models <C> Combine-skip+COCO  <C> Combine-skip+COCO  <C> 0.8655 <C> 0.2561 <R> <C> Previous Models <C> ConvNet <C> ConvNet <C> 0.8686 <C> 0.2606 <R> <C> Previous Models <C> Seq-GRU  <C> Seq-GRU  <C> 0.8595 <C> 0.2689 <R> <C> Previous Models <C> Seq-LSTM  <C> Seq-LSTM  <C> 0.8528 <C> 0.2831 <R> <C> Previous Models <C> Dep. Tree-GRU  <C> Dep. Tree-GRU  <C> 0.8672 <C> 0.2573 <R> <C> Previous Models <C> Dep. Tree-GRU + Attn.  <C> Dep. Tree-GRU + Attn.  <C> 0.8701 <C> 0.2524 <R> <C> Previous Models <C> Const. Tree-LSTM  <C> Const. Tree-LSTM  <C> 0.8582 <C> 0.2734 <R> <C> Previous Models <C> [EMPTY] <C> [EMPTY] <C> 0.8460 † <C> 0.2895 † <R> <C> [EMPTY] <C> Dep. Tree-LSTM  <C> Dep. Tree-LSTM  <C> 0.8676 <C> 0.2532 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 0.8663 † <C> 0.2612 † <R> <C> [EMPTY] <C> Dep. Tree-LSTM + Attn.  <C> Dep. Tree-LSTM + Attn.  <C> 0.8730 <C> 0.2426 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 0.8635 † <C> 0.2591 † <R> <C> Child Sum Tree LSTM <C> Model 1 <C> Self <C> 0.7466 <C> 0.4545 <R> <C> Child Sum Tree LSTM <C> Model 1 <C> Sentence 1 <C> 0.7305 <C> 0.4849 <R> <C> Child Sum Tree LSTM <C> Model 1 <C> Sentence 2 <C> 0.7939 <C> 0.3801 <R> <C> Child Sum Tree LSTM <C> Model 1 <C> Phrase <C> 0.7889 <C> 0.3877 <R> <C> Child Sum Tree LSTM <C> Model 2 <C> Self <C> 0.8577 <C> 0.2695 <R> <C> Child Sum Tree LSTM <C> Model 2 <C> Sentence 1 <C> 0.8620 <C> 0.2634 <R> <C> Child Sum Tree LSTM <C> Model 2 <C> Sentence 2 <C> 0.8686 <C> 0.2518 <R> <C> Child Sum Tree LSTM <C> Model 2 <C> Phrase <C> 0.8623 <C> 0.2615 <R> <C> Binary Tree LSTM <C> Model 1 <C> Self <C> 0.8648 <C> 0.2567 <R> <C> Binary Tree LSTM <C> Model 1 <C> Sentence 1 <C> 0.8692 <C> 0.2486 <R> <C> Binary Tree LSTM <C> Model 1 <C> Sentence 2 <C> 0.8686 <C> 0.2507 <R> <C> Binary Tree LSTM <C> Model 1 <C> Phrase <C> 0.8676 <C> 0.2517 <R> <C> Binary Tree LSTM <C> Model 2 <C> Self <C> 0.8698 <C> 0.2476 <R> <C> Binary Tree LSTM <C> Model 2 <C> Sentence 1 <C> 0.8698 <C> 0.2476 <R> <C> Binary Tree LSTM <C> Model 2 <C> Sentence 2 <C> 0.8720 <C> 0.2435 <R> <C> Binary Tree LSTM <C> Model 2 <C> Phrase <C> 0.8696 <C> 0.2479 <CAP> TABLE II: Test set results on the SICK dataset. The first group lists previous results, and the remainder are the results of our models. We mark models that we re-implemented with a †.
<R> <C> Embedding <C> VDCNN <C> TextCNN <C> LSTM <C> LSTMCNN <C> SARNN <R> <C> comment <C> 0.6812 <C> 0.6743 <C> 0.6612 <C> 0.7012 <C> 0.7056 <R> <C> comment_bpe <C> 0.6643 <C> 0.6665 <C> 0.6514 <C> 0.6918 <C> 0.6901 <R> <C> comment_tokenize <C> 0.7098 <C> 0.7143 <C> 0.6832 <C> 0.7123 <C> [BOLD] 0.7167 <R> <C> fasttext <C> 0.6954 <C> 0.7123 <C> 0.6812 <C> 0.7012 <C> 0.7012 <R> <C> roberta <C> 0.6734 <C> 0.6636 <C> 0.6345 <C> 0.6704 <C> 0.6566 <R> <C> sonvx_wiki <C> 0.6534 <C> 0.6624 <C> 0.6456 <C> 0.6745 <C> 0.6316 <R> <C> sonvx_baomoi_w2 <C> 0.6612 <C> 0.6712 <C> 0.6549 <C> 0.6822 <C> 0.6513 <R> <C> sonvx_baomoi_w5 <C> 0.6656 <C> 0.6645 <C> 0.6601 <C> 0.6756 <C> 0.6647 <CAP> Table I: F1_macro score of different model
<R> <C> [EMPTY] <C> Test set 1 <C> Test set 2 <C> Test set 3 <R> <C> (A) DTW <C> 0.6173 <C> 0.5778 <C> 0.5678 <R> <C> (B) Attention + 1-hop <C> 0.6128 <C> 0.5893 <C> 0.5548 <R> <C> (C) Attention + 3-hop <C> 0.6141 <C> 0.5964 <C> 0.5702 <CAP> Table 3: Results of attention-based multi-hop network learning from a teacher approach (DTW).
<R> <C> Model <C> Association Accuracy (%) <C> Label Error Rate (%) visual <C> Label Error Rate (%) audio <R> <C> LSTM + CTC (baseline) <C> 70.68± 6.12 <C> 0.14±0.14 <C> 35.84± 5.35 <R> <C> Original Model (RaueCoCo2015) <C> 19.59± 8.90 <C> 7.00±2.42 <C> 79.01± 9.51 <R> <C> [BOLD] Our work <C> 71.52±11.85 <C> 0.97±1.58 <C> 33.09±10.38 <CAP> Table 1: Association Accuracy (%) and Label Error Rate (%) from the multimodal dataset that has missing elements in both modalities. It can be seen that the original model performs worse than the proposed combination. Furthermore, the presented extension reached similar results to LSTM (trained for the easier classification task, and not for association) and under some conditions reaches better results (audio component).
<R> <C> [EMPTY] <C> Consistent <C> Inconsistent <R> <C> Overall <C> 3095.51 <C> 3288.99 <R> <C> Same Order <C> 3066.47 <C> 3273.25 <R> <C> Reverse Order <C> 3083.10 <C> 3299.40 <CAP> Figure 5: Mean Response Time for Producing Interpretations (Consistent or Inconsistent) with the Primes by Prime Order (Overall, Same Prime Order, Reverse Prime Order)(a) Mean response times (ms) before analysis (N = 65) (b) Mean response times (ms) used in ANOVA (N = 51)
<R> <C> [EMPTY] <C> Consistent <C> Inconsistent <R> <C> Overall <C> 3138.76 <C> 3352.58 <R> <C> Same Order <C> 3155.78 <C> 3329.85 <R> <C> Reverse Order <C> 3098.23 <C> 3274.58 <CAP> Figure 5: Mean Response Time for Producing Interpretations (Consistent or Inconsistent) with the Primes by Prime Order (Overall, Same Prime Order, Reverse Prime Order)(a) Mean response times (ms) before analysis (N = 65) (b) Mean response times (ms) used in ANOVA (N = 51)
<R> <C> Methods <C> EM-score <C> F{}_{1}-score <R> <C> QANet <C> 85.45 <C> 93.62 <R> <C> BERT-Base <C> 86.20 <C> 90.06 <R> <C> [BOLD] Our Model <C> [BOLD] 91.84 <C> [BOLD] 93.75 <CAP> TABLE IV: Comparative Results between BERT and Our Proposed Model
<R> <C> [EMPTY] <C> BLEU <C> METEOR <R> <C> GraphWriter <C> [BOLD] 14.3 ± 1.01 <C> [BOLD] 18.8 ± 0.28 <R> <C> GAT <C> 12.2 ± 0.44 <C> 17.2 ± 0.63 <R> <C> EntityWriter <C> 10.38 <C> 16.53 <R> <C> Rewriter <C> 1.05 <C> 8.38 <CAP> Table 2: Automatic Evaluations of Generation Systems.
<R> <C> [EMPTY] <C> systems <C> acc <R> <C> w/o attention <C> Majority Class <C> 60.4 <R> <C> w/o attention <C> w/o Context <C> 65.1 <R> <C> w/o attention <C> Bi-LSTM <C> 69.5 <R> <C> w/o attention <C> NGram model <C> 70.6 <R> <C> w/o attention <C> Bi-CNN <C> 74.4 <R> <C> with attention <C> Enhanced LSTM <C> 70.6 <R> <C> with attention <C> Attentive-LSTM <C> 71.5 <R> <C> with attention <C> Decomp-Att <C> 72.3 <R> <C> with attention <C> DGEM <C> 77.3 <R> <C> 2-3 <C> APCNN <C> 75.2 <R> <C> [EMPTY] <C> ABCNN <C> 75.8 <R> <C> AttConv-light <C> AttConv-light <C> 78.1 <R> <C> w/o convolution <C> w/o convolution <C> 75.1 <R> <C> AttConv-advanced <C> AttConv-advanced <C> [BOLD] 79.2 <CAP> Table 5: AttConv vs. baselines on SciTail dataset
<R> <C> [EMPTY] <C> system <C> retrie. evi. All <C> retrie. evi. Sub <C> gold evi. <R> <C> dev <C> MLP <C> 41.86 <C> 19.04 <C> 65.13 <R> <C> dev <C> Bi-CNN <C> 47.82 <C> 26.99 <C> 75.02 <R> <C> dev <C> APCNN <C> 50.75 <C> 30.24 <C> 78.91 <R> <C> dev <C> ABCNN <C> 51.39 <C> 32.44 <C> 77.13 <R> <C> dev <C> Attentive-LSTM <C> 52.47 <C> 33.19 <C> 78.44 <R> <C> dev <C> Decomp-Att <C> 52.09 <C> 32.57 <C> 80.82 <R> <C> dev <C> AttConv <C> AttConv <C> AttConv <C> AttConv <R> <C> dev <C> light,context-wise <C> 57.78 <C> 34.29 <C> 83.20 <R> <C> dev <C> w/o conv. <C> 47.29 <C> 25.94 <C> 73.18 <R> <C> dev <C> light,context-conc <C> 59.31 <C> 37.75 <C> 84.74 <R> <C> dev <C> w/o conv. <C> 48.02 <C> 26.67 <C> 73.44 <R> <C> dev <C> advan.,context-wise <C> 60.20 <C> 37.94 <C> 84.99 <R> <C> dev <C> advan.,context-conc <C> [BOLD] 62.26 <C> [BOLD] 39.44 <C> [BOLD] 86.02 <R> <C> test <C> Thorne et al. ( 2018 ) <C> 50.91 <C> 31.87 <C> – <R> <C> test <C> AttConv <C> [BOLD] 61.03 <C> [BOLD] 38.77 <C> [BOLD] 84.61 <CAP> Table 9: Performance on dev and test of FEVER. In “gold evi.” scenario, All Subset are the same.
<R> <C> Combination <C> J48 (%) <C> KNN (%) <C> NB (%) <C> RBFN (%) <R> <C> Whole set <C> 45 <C> 62.5 <C> 61.25 <C> 56.25 <R> <C> Variance threshold best <C> 55 <C> 67.5 <C> 63.75 <C> 63.75 <R> <C> Score-based best <C> 75 <C> 78.75 <C> 77.5 <C> 75 <R> <C> { [ITALIC] μ1} <C> 45 <C> 43.75 <C> 46.25 <C> 40 <R> <C> { [ITALIC] μ1, [ITALIC] μ2, [ITALIC] μ3} <C> 38.75 <C> 63.75 <C> 60 <C> 57.5 <CAP> Table 1: Summary of success scores for the whole set of 48 attributes and subsets after feature selection. The results for combinations of attributes which obtained the best scores under variance threshold and score-based criteria are presented along with those for the subset of the first moments {μ1} and the complementary subset of all higher moments {μ1,μ2,μ3}.
<R> <C> [EMPTY] <C> [BOLD] F1 <R> <C> SEMPRE <C> 10.80 <R> <C> PARASEMPRE <C> 12.79 <R> <C> JACANA <C> 5.08 <R> <C> UDEPLAMBDA <C> 17.70 <R> <C> SCANNER <C> 17.02 <R> <C> PARA4QA <C> 20.40 <R> <C> SPARQA <C> 21.53 <CAP> Table 1: Overall results on GraphQuestions.
<R> <C> [EMPTY] <C> [BOLD] P@1 <R> <C> MHQA-GRN <C> 30.10 <R> <C> SIMPQA + PRETRAINED <C> 19.90 <R> <C> SPLITQA + PRETRAINED <C> 25.90 <R> <C> SPLITQA + data augmentation <C> 34.20 <R> <C> PullNet <C> 45.90 <R> <C> SPARQA <C> 31.57 <CAP> Table 2: Overall results on ComplexWebQuestions.
<R> <C> [EMPTY] <C> Synth <C> Quizbowl Synth <C> Quizbowl Human <C> Human <C> Jeopardy! Synth <C> Jeopardy! Human <R> <C> Method <C> Start <C> End <C> Start <C> End <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] Methods Tested on Clean Data <C> [BOLD] Methods Tested on Clean Data <C> [BOLD] Methods Tested on Clean Data <C> [BOLD] Methods Tested on Clean Data <C> [BOLD] Methods Tested on Clean Data <C> [BOLD] Methods Tested on Clean Data <C> [EMPTY] <R> <C> ir <C> 0.064 <C> 0.544 <C> 0.400 <C> 1.000 <C> 0.190 <C> 0.050 <R> <C> dan <C> 0.080 <C> 0.540 <C> 0.200 <C> 1.000 <C> 0.236 <C> 0.033 <R> <C> [BOLD] Methods Tested on Corrupted Data <C> [BOLD] Methods Tested on Corrupted Data <C> [BOLD] Methods Tested on Corrupted Data <C> [BOLD] Methods Tested on Corrupted Data <C> [BOLD] Methods Tested on Corrupted Data <C> [BOLD] Methods Tested on Corrupted Data <C> [EMPTY] <R> <C> ir <C> 0.021 <C> 0.442 <C> 0.180 <C> 0.560 <C> 0.079 <C> 0.050 <R> <C> dan <C> 0.035 <C> 0.335 <C> 0.120 <C> 0.440 <C> 0.097 <C> 0.017 <R> <C> fd <C> 0.032 <C> 0.354 <C> 0.120 <C> 0.440 <C> 0.102 <C> 0.033 <R> <C> Confidence <C> 0.036 <C> 0.374 <C> 0.120 <C> 0.460 <C> 0.095 <C> 0.033 <R> <C> fd+Conf <C> 0.041 <C> 0.371 <C> 0.160 <C> 0.440 <C> 0.109 <C> 0.033 <CAP> Table 2: Both forced decoding (fd) and the best confidence model improve accuracy for the dan. Jeopardy only has an At-End-of-Sentence metric, as questions are one sentence in length. Combining the two methods leads to a further joint improvement. The ir and dan accuracies on clean data are provided as a reference.
<R> <C> [BOLD] Model <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> ace 2004 <C> ace 2004 <C> ace 2004 <C> ace 2004 <R> <C> katiyar-cardie-2018-nested <C> 73.6 <C> 71.8 <C> 72.7 <R> <C> wang-etal-2018-neural-transition <C> - <C> - <C> 73.3 <R> <C> wang-lu-2018-neural <C> 78.0 <C> 72.4 <C> 75.1 <R> <C> strakova-etal-2019-neural <C> - <C> - <C> 84.4 <R> <C> luan-etal-2019-general <C> - <C> - <C> 84.7 <R> <C> Our model <C> 87.3 <C> 86.0 <C> [BOLD] 86.7 <R> <C> ace 2005 <C> ace 2005 <C> ace 2005 <C> ace 2005 <R> <C> katiyar-cardie-2018-nested <C> 70.6 <C> 70.4 <C> 70.5 <R> <C> wang-etal-2018-neural-transition <C> - <C> - <C> 73.0 <R> <C> wang-lu-2018-neural <C> 76.8 <C> 72.3 <C> 74.5 <R> <C> lin-etal-2019-sequence <C> 76.2 <C> 73.6 <C> 74.9 <R> <C> fisher-vlachos-2019-merge <C> 82.7 <C> 82.1 <C> 82.4 <R> <C> luan-etal-2019-general <C> - <C> - <C> 82.9 <R> <C> strakova-etal-2019-neural <C> - <C> - <C> 84.3 <R> <C> Our model <C> 85.2 <C> 85.6 <C> [BOLD] 85.4 <R> <C> genia <C> genia <C> genia <C> genia <R> <C> katiyar-cardie-2018-nested <C> 79.8 <C> 68.2 <C> 73.6 <R> <C> wang-etal-2018-neural-transition <C> - <C> - <C> 73.9 <R> <C> ju-etal-2018-neural <C> 78.5 <C> 71.3 <C> 74.7 <R> <C> wang-lu-2018-neural <C> 77.0 <C> 73.3 <C> 75.1 <R> <C> sohrab-miwa-2018-deep <C> 93.2 <C> 64.0 <C> 77.1 <R> <C> lin-etal-2019-sequence <C> 75.8 <C> 73.9 <C> 74.8 <R> <C> luan-etal-2019-general <C> - <C> - <C> 76.2 <R> <C> strakova-etal-2019-neural <C> - <C> - <C> 78.3 <R> <C> Our model <C> 81.8 <C> 79.3 <C> [BOLD] 80.5 <CAP> Table 2: State of the art comparison on ace 2004, ace 2005 and genia corpora for nested NER.
<R> <C> [EMPTY] <C> [BOLD] F1 <C> Δ <R> <C> Our model <C> 89.9 <C> [EMPTY] <R> <C> - biaffine <C> 89.1 <C> 0.8 <R> <C> - BERT emb <C> 87.5 <C> 2.4 <R> <C> - fastText emb <C> 89.5 <C> 0.4 <R> <C> - Char emb <C> 89.8 <C> 0.1 <CAP> Table 4: The comparison between our full model and ablated models on ontonotes development set.
<R> <C> [BOLD] Data <C> [BOLD] Test set <R> <C> [BOLD] F-score <C> 96.5+6.6 <R> <C> [BOLD] OOV Rate <C> 0.811 <R> <C> [BOLD] OOV Recall Rate <C> 0.969 <R> <C> [BOLD] IV Recall Rate <C> 0.980 <CAP> Table 3: The model performances on the validation set .
<R> <C> Dataset <C> Model <C> MAP <C> MRR <C> Recall@5 <C> Recall@10 <C> Recall@30 <C> Recall@50 <C> Recall@80 <R> <C> AAN <C> BERT-GCN <C> [BOLD] 0.6189 <C> [BOLD] 0.6036 <C> [BOLD] 0.6736 <C> [BOLD] 0.7109 <C> [BOLD] 0.7814 <C> [BOLD] 0.8162 <C> [BOLD] 0.8538 <R> <C> AAN <C> BERT-GCN-Left <C> 0.5967 <C> 0.5818 <C> 0.6459 <C> 0.6843 <C> 0.7506 <C> 0.785 <C> 0.8245 <R> <C> AAN <C> BERT <C> 0.6118 <C> 0.5971 <C> 0.6593 <C> 0.6976 <C> 0.7645 <C> 0.81 <C> 0.8257 <R> <C> AAN <C> BERT-Left <C> 0.5928 <C> 0.5789 <C> 0.6364 <C> 0.6678 <C> 0.7379 <C> 0.7793 <C> 0.8203 <R> <C> AAN <C> CACR  <C> 0.2893 <C> 0.2917 <C> 0.3861 <C> 0.4531 <C> 0.5799 <C> 0.6573 <C> 0.721 <R> <C> FullTextPeerRead <C> BERT-GCN <C> [BOLD] 0.4181 <C> [BOLD] 0.4179 <C> [BOLD] 0.4864 <C> [BOLD] 0.5291 <C> [BOLD] 0.6036 <C> [BOLD] 0.6495 <C> [BOLD] 0.6994 <R> <C> FullTextPeerRead <C> BERT-GCN-Left <C> 0.3883 <C> 0.388 <C> 0.4455 <C> 0.4815 <C> 0.5539 <C> 0.5991 <C> 0.6499 <R> <C> FullTextPeerRead <C> BERT <C> 0.4152 <C> 0.415 <C> 0.4801 <C> 0.52 <C> 0.5926 <C> 0.6366 <C> 0.6887 <R> <C> FullTextPeerRead <C> BERT-Left <C> 0.3823 <C> 0.3821 <C> 0.4391 <C> 0.4755 <C> 0.5459 <C> 0.5885 <C> 0.6392 <R> <C> FullTextPeerRead <C> CACR  <C> 0.1551 <C> 0.1549 <C> 0.2154 <C> 0.2761 <C> 0.4128 <C> 0.4794 <C> 0.5516 <CAP> Table 2: MAP, MRR, and Recall@K scores for a frequency of over five citations and fifty pair context sentences
<R> <C> [BOLD] Method <C> [BOLD] BLEU-4 <R> <C> Seq2Seq <C> 22.01 <R> <C> Seq2Seq + attention <C> 31.85 <R> <C> Seq2Ast <C> 35.76 <CAP> Table 6: Performance on generative based ABQA.
<R> <C> embedding/classifier <C> precision <C> recall <C> F1 <R> <C> [BOLD] stream-cnn <C> 0.40 <C> 0.49 <C> 0.42 <R> <C> [BOLD] static-cnn <C> 0.40 <C> 0.52 <C> 0.43 <CAP> Table 1: Classification results on test collection (February 2016).
<R> <C> [BOLD] Model <C> [BOLD] Accuracy <R> <C> Lee, et al. Lee1 <C> 92.96 <R> <C> Ahn, et al. Ahn:2007 <C> 93.12 <R> <C> Lee, et al. Lee2 <C> 92.95 <R> <C> Choi, et al. Choi:2016 <C> 94.89 <R> <C> UniTagger-500 <C> [BOLD] 96.20 <CAP> Table 4: End-to-end Eojeol-level accuracy for morphological analysis of Korean (Sejong Corpus)
<R> <C> [BOLD] Model <C> [BOLD] Pos Class <C> [BOLD] Neg Class <C> [BOLD] Neut Class <C> [BOLD] Score <R> <C> [ITALIC] SVM <C> 0.64 <C> 0.62 <C> 0.57 <C> 0.61 <R> <C> [ITALIC] Char-CNN <C> 0.68 <C> 0.65 <C> 0.56 <C> 0.63 <R> <C> [ITALIC] GenMA <C> 0.73 <C> 0.67 <C> 0.63 <C> [BOLD] 0.68 <CAP> Table 1: F1-scores of three algorithms on dataset
<R> <C> [BOLD] Dataset <C> 10K <C> 20K <C> 50K <C> 80K <C> 100K <C> 200K <C> 500K <C> 800K <C> 1M <R> <C> [BOLD] Average Time per Epoch (sec.) <C> 299 <C> 526 <C> 1,251 <C> 1,933 <C> 2,430 <C> 4,273 <C> 10,983 <C> 17,559 <C> 21,010 <R> <C> [BOLD] Number of Epoches <C> 33 <C> 25 <C> 36 <C> 29 <C> 29 <C> 35 <C> 45 <C> 42 <C> 42 <R> <C> [BOLD] Total Time (day) <C> 0.11 <C> 0.15 <C> 0.52 <C> 0.64 <C> 0.81 <C> 1.73 <C> 5.72 <C> 8.53 <C> 10.21 <CAP> TABLE II: Training time of DBLSTMs using GPU-based tool on different sets of Microsoft training data.
<R> <C> [BOLD] Dataset <C> 10K <C> 20K <C> 50K <C> 80K <C> 100K <C> 200K <C> 500K <C> 800K <C> 1M <R> <C> [BOLD] CTC Decoding w/o LM <C> 44.0/89.6 <C> 27.5/68.9 <C> 20.0/56.8 <C> 17.0/51.1 <C> 15.8/48.2 <C> 14.0/44.1 <C> 12.9/42.1 <C> 12.3/40.4 <C> 11.8/40.1 <R> <C> [BOLD] Character 3-gram <C> 42.7/89.8 <C> 26.6/81.3 <C> 17.9/75.2 <C> 15.2/72.9 <C> 14.3/72.1 <C> 12.5/69.9 <C> 11.3/68.2 <C> 10.9/67.2 <C> 10.1/67.1 <R> <C> [BOLD] Character 4-gram <C> 41.0/80.5 <C> 24.3/65.1 <C> 15.8/55.8 <C> 13.4/52.1 <C> 12.6/50.7 <C> 11.1/48.4 <C> 9.9/45.7 <C> 9.7/44.5 <C> 8.8/44.5 <R> <C> [BOLD] Character 5-gram <C> 40.0/76.1 <C> 22.3/56.9 <C> 14.1/47.3 <C> 11.7/43.0 <C> 11.1/41.7 <C> 9.5/39.5 <C> 8.5/36.6 <C> 8.5/34.9 <C> 7.7/35.2 <R> <C> [BOLD] Character 8-gram <C> 39.8/75.3 <C> 21.7/55.1 <C> 13.1/44.7 <C> 10.9/41.1 <C> 10.3/39.4 <C> 8.9/37.9 <C> 7.8/34.7 <C> 8.0/32.5 <C> 7.0/33.5 <R> <C> [BOLD] Character 10-gram <C> 40.3/75.6 <C> 22.5/55.5 <C> 13.3/44.7 <C> 11.2/41.2 <C> 10.6/39.4 <C> 9.1/37.9 <C> 8.1/34.5 <C> 8.1/32.0 <C> 7.3/33.6 <R> <C> [BOLD] Word Trigram <C> 34.8/59.0 <C> 17.0/34.6 <C> 11.1/27.5 <C> 9.2/24.1 <C> 8.6/22.9 <C> 7.7/21.2 <C> 6.8/19.6 <C> 6.2/18.1 <C> 6.0/18.0 <CAP> TABLE V: Performance (CER/WER in %) comparison of CTC-based decoding without using LM and WFST-based decoding with different types of LMs.
<R> <C> Word mcmoran <C> Count 11 <C> Δloss -0.74 <C> Word the <C> Count 59421 <C> Δloss -0.009 <R> <C> cie. <C> 9 <C> -0.66 <C>  <C> 53299 <C> -0.004 <R> <C> mall <C> 13 <C> -0.65 <C>  <C> 49199 <C> -0.010 <R> <C> missile <C> 23 <C> -0.55 <C> N <C> 37607 <C> -0.008 <R> <C> siemens <C> 12 <C> -0.51 <C> of <C> 28427 <C> -0.008 <R> <C> baldwin <C> 9 <C> -0.51 <C> to <C> 27430 <C> -0.004 <R> <C> nfl <C> 21 <C> -0.49 <C> a <C> 24755 <C> -0.013 <R> <C> prime-time <C> 17 <C> -0.47 <C> in <C> 21032 <C> -0.015 <CAP> Table 3: Difference in word loss (normalized by word counts) on validation data when searching intra and inter-cell jointly. The left column contains the words with eight best improvements (larger absolute value of Δloss) and right column presents the most frequent words in the validation data.
<R> <C> Reason <C> Plausibility Score <R> <C> of the circumstances of his birth.” -C.B. <C> 0.0/1.0 <R> <C> he’s the one who’s given him the money to do so. <C> 0.2/1.0 <R> <C> it was Charlie who started the discussion and who encouraged Charlie to take up the challenge. <C> 0.0/1.0 <R> <C> we feel grateful for the help from others <C> 1.0/1.0 <R> <C> charlie is the one who get help. <C> 0.6/1.0 <CAP> Table 2: Given the sentence “Bob paid for Charlie’s college education. He is very grateful. The ‘He’ refers to Charlie because ”, the reasons generated by GPT-2 and corresponding plausibility scores.
<R> <C> [BOLD] Model <C> [BOLD] PTB  [BOLD] Validation <C> [BOLD] PTB  [BOLD] Test <C> [BOLD] BBC  [BOLD] Validation <C> [BOLD] BBC  [BOLD] Test <C> [BOLD] IMDB  [BOLD] Validation <C> [BOLD] IMDB  [BOLD] Test <R> <C> Gated Word & Char, adaptive <C> 117.49 <C> 113.87 <C> [BOLD] 78.56 <C> [BOLD] 87.16 <C> 71.99 <C> 72.29 <R> <C> Gated Word & Char, adaptive (Pre-train) <C> 117.03 <C> 112.90 <C> 80.37 <C> 87.51 <C> 71.16 <C> 71.49 <R> <C> Gated Word & Char,  [ITALIC] g=0.25 <C> 119.45 <C> 115.55 <C> 79.67 <C> 88.04 <C> 71.81 <C> 72.14 <R> <C> Gated Word & Char,  [ITALIC] g=0.25 (Pre-train) <C> [BOLD] 117.01 <C> [BOLD] 113.52 <C> 80.07 <C> 87.99 <C> [BOLD] 70.60 <C> [BOLD] 70.87 <R> <C> Gated Word & Char,  [ITALIC] g=0.5 <C> 126.01 <C> 121.99 <C> 89.27 <C> 94.91 <C> 106.78 <C> 107.33 <R> <C> Gated Word & Char,  [ITALIC] g=0.5 (Pre-train) <C> 117.54 <C> 113.03 <C> 82.09 <C> 88.61 <C> 109.69 <C> 110.28 <R> <C> Gated Word & Char,  [ITALIC] g=0.75 <C> 135.58 <C> 135.00 <C> 105.54 <C> 111.47 <C> 115.58 <C> 116.02 <R> <C> Gated Word & Char,  [ITALIC] g=0.75 (Pre-train) <C> 179.69 <C> 172.85 <C> 132.96 <C> 136.01 <C> 106.31 <C> 106.86 <R> <C> Word Only <C> 118.03 <C> 115.65 <C> 84.47 <C> 90.90 <C> 72.42 <C> 72.75 <R> <C> Character Only <C> 132.45 <C> 126.80 <C> 88.03 <C> 97.71 <C> 98.10 <C> 98.59 <R> <C> Word & Character <C> 125.05 <C> 121.09 <C> 88.77 <C> 95.44 <C> 77.94 <C> 78.29 <R> <C> Word & Character (Pre-train) <C> 122.31 <C> 118.85 <C> 84.27 <C> 91.24 <C> 80.60 <C> 81.01 <R> <C> Non-regularized LSTM (Zaremba, 2014) <C> 120.7 <C> 114.5 <C> - <C> - <C> - <C> - <CAP> Table 1: Validation and test perplexities on Penn Treebank (PTB), BBC, IMDB Movie Reviews datasets.
<R> <C> [BOLD] Feats <C> [BOLD] Overall <C> [BOLD] Real <C> [BOLD] Foil <R> <C> Blind (LSTM only)† <C> 55.62 <C> 86.20 <C> 25.04 <R> <C> HieCoAtt† <C> 64.14 <C> 91.89 <C> 36.38 <R> <C> CNN + BOW MLP <C> 88.42 <C> 86.89 <C> 89.97 <R> <C> Predict Mention + BOW MLP <C> 94.94 <C> 95.68 <C> 94.23 <R> <C> Predict Freq + BOW MLP <C> 95.14 <C> 95.82 <C> 94.48 <R> <C> Gold Mention + BOW MLP <C> 95.83 <C> 96.30 <C> 95.36 <R> <C> Gold Freq + BOW MLP <C> 96.45 <C> 96.04 <C> 96.85 <R> <C> CNN + LSTM <C> 87.45 <C> 86.78 <C> 88.14 <R> <C> Predict Freq + LSTM <C> 85.99 <C> 85.17 <C> 86.81 <R> <C> Gold Freq + LSTM <C> 87.38 <C> 86.62 <C> 88.18 <R> <C> Predict Freq + MM-LSTM <C> 87.90 <C> 86.73 <C> 88.95 <R> <C> Gold Freq + MM-LSTM <C> 89.02 <C> 88.35 <C> 89.72 <R> <C> Human (majority)† <C> 92.89 <C> 91.24 <C> 94.52 <CAP> Table 2: Accuracy on Nouns dataset. † are taken directly from shekhar2017foil_acl. HieCoAtt is the state of the art reported in the paper.
<R> <C> [EMPTY] <C> [BOLD] Classifier <C> [BOLD] Overall <C> [BOLD] Real <C> [BOLD] Foil <R> <C> VB <C> Gold Freq + BOW MLP <C> 84.03 <C> 97.38 <C> 70.68 <R> <C> VB <C> Gold Freq + MM-LSTM <C> 87.90 <C> 99.48 <C> 76.32 <R> <C> VB <C> HieCoAtt† <C> 81.79 <C> - <C> 57.94 <R> <C> ADJ <C> Gold Freq + BOW MLP <C> 87.74 <C> 96.96 <C> 78.52 <R> <C> ADJ <C> Gold Freq + MM-LSTM <C> 92.29 <C> 85.82 <C> 98.77 <R> <C> ADJ <C> HieCoAtt† <C> 86.00 <C> - <C> 80.05 <R> <C> ADV <C> Gold Freq + BOW MLP <C> 54.99 <C> 98.49 <C> 11.48 <R> <C> ADV <C> Gold Freq + MM-LSTM <C> 56.55 <C> 99.45 <C> 13.65 <R> <C> ADV <C> HieCoAtt† <C> 53.40 <C> - <C> 14.73 <R> <C> PREP <C> Gold Freq + BOW MLP <C> 75.53 <C> 92.61 <C> 58.45 <R> <C> PREP <C> Gold Freq + MM-LSTM <C> 89.74 <C> 95.59 <C> 83.89 <R> <C> PREP <C> HieCoAtt† <C> 74.91 <C> - <C> 61.92 <CAP> Table 3: Accuracy on Verb, Adjective, Adverb and Preposition datasets, using Gold Frequency as the image representation. † is the best performing model as reported in shekhar2017b.
<R> <C> [EMPTY] <C> [BOLD] Image <C> [BOLD] Text <C> [BOLD] Overall <C> [BOLD] Real <C> [BOLD] Foil <R> <C> [EMPTY] <C> CNN <C> - <C> 50.01 <C> 64.71 <C> 35.31 <R> <C> [EMPTY] <C> Gold Freq <C> - <C> 50.04 <C> 53.10 <C> 47.00 <R> <C> MLP <C> - <C> BOW <C> 89.33 <C> 88.32 <C> 90.34 <R> <C> MLP <C> CNN <C> BOW <C> 88.42 <C> 86.89 <C> 89.97 <R> <C> MLP <C> Gold Freq <C> BOW <C> 96.45 <C> 96.04 <C> 96.85 <R> <C> LSTM <C> - <C> LSTM <C> 85.07 <C> 85.52 <C> 84.66 <R> <C> LSTM <C> CNN <C> LSTM <C> 87.38 <C> 86.62 <C> 88.18 <R> <C> LSTM <C> Gold Freq <C> LSTM <C> 87.45 <C> 86.78 <C> 88.14 <CAP> Table 4: Ablation study on FOIL (Nouns).
<R> <C> [BOLD] PCZ <C> [ITALIC] n <C> [BOLD] Words # <C> [BOLD] Words mono <C> [BOLD] Words poly <C> [BOLD] Senses # <C> [BOLD] Polysemy avg. <C> [BOLD] Polysemy max <C> [BOLD] Rel.senses # <C> [BOLD] Rel.senses avg. <C> [BOLD] Hyper. # <C> [BOLD] Hyper. avg. <R> <C> news-p1.6 <C> 200 <C> 207k <C> 137k <C> 69k <C> 332k <C> 1.6 <C> 18 <C> 234k <C> 63.9 <C> 15k <C> 6.9 <R> <C> news-p2.3 <C> 50 <C> 200k <C> 99k <C> 101k <C> 461k <C> 2.3 <C> 17 <C> 298k <C> 44.3 <C> 15k <C> 5.8 <R> <C> wiki-p1.8 <C> 200 <C> 206k <C> 120k <C> 86k <C> 368k <C> 1.8 <C> 15 <C> 300k <C> 59.3 <C> 15k <C> 4.4 <R> <C> wiki-p6.0 <C> 30 <C> 258k <C> 44k <C> 213k <C> 1.5M <C> 6.0 <C> 36 <C> 811k <C> 16.9 <C> 52k <C> 1.7 <R> <C> wiki-p1.6-mwe <C> 200 <C> 465k <C> 288k <C> 176k <C> 765k <C> 1.6 <C> 13 <C> 662k <C> 46.6 <C> 30k <C> 3.2 <CAP> Table 6: Structural analysis of our five word sense inventories of the proto-conceptualizations (PCZs) used in our experiments.
<R> <C> Model <C> Accuracy <R> <C> base <C> 74.8 <R> <C> +context <C> 75.4 <R> <C> +context(multilingual) <C> 76.9 <R> <C> +context(multilingual) + embeddings <C> 79.6 <R> <C> +both contexts + embeddings <C> 80.0 <R> <C> +both contexts + embeddings + ensemble <C> 81.7 <R> <C> Hovy et al HTH10 – using WordNet features <C> 84.8 <R> <C> Srikumar and Roth SR13b – using WordNet features <C> 84.78 <R> <C> Tratz and Hovy HT09 – using WordNet features <C> 76.4 <R> <C> MELB-YB  – using WordNet features <C> 69.3 <R> <C> KU  <C> 54.7 <R> <C> IRST-BP  <C> 49.6 <R> <C> Most Frequent Sense <C> 39.6 <CAP> Table 7: The accuracies on the test set of the SemEval corpus, in comparison to previous systems.
<R> <C> [EMPTY] <C> [BOLD] Yelp-2013 <C> [BOLD] Yelp-2014 <C> [BOLD] IMDB <C> [BOLD] SST-1 <C> [BOLD] SST-2 <R> <C> Embedding size <C> 300 <C> 300 <C> 300 <C> 300 <C> 300 <R> <C> LSTM hidden unit <C> 200 <C> 200 <C> 200 <C> 200 <C> 200 <R> <C> Capsule dimension <C> 200 <C> 200 <C> 200 <C> 200 <C> 200 <R> <C> Capsule number <C> 5 <C> 5 <C> 5 <C> 5 <C> 5 <R> <C> Iteration number <C> 3 <C> 3 <C> 3 <C> 3 <C> 3 <R> <C> Regularization rate <C> 1e-5 <C> 1e-5 <C> 1e-6 <C> 1e-6 <C> 1e-5 <R> <C> Initial learning rate <C> 0.0001 <C> 0.0002 <C> 0.0001 <C> 0.0001 <C> 0.0003 <R> <C> learning rate decay <C> 0.9 <C> 0.9 <C> 0.95 <C> 0.95 <C> 0.95 <R> <C> learning rate decay steps <C> 1000 <C> 1000 <C> 1000 <C> 500 <C> 500 <R> <C> Initial Batch size <C> 32 <C> 32 <C> 32 <C> 64 <C> 64 <R> <C> Batch size low bound <C> 32 <C> 32 <C> 32 <C> 16 <C> 16 <R> <C> Dropout rate <C> 0.2 <C> 0.2 <C> 0.2 <C> 0.2 <C> 0.5 <CAP> Table 2: Detailed hyper-parameter settings
<R> <C> [EMPTY] <C> [BOLD] Yelp-2013 <C> [BOLD] Yelp-2014 <C> [BOLD] IMDB <C> [BOLD] SST-1 <C> [BOLD] SST-2 <R> <C> RNTN+Recurrent  <C> 57.4 <C> 58.2 <C> 40.0 <C> - <C> - <R> <C> CNN-non-static  <C> - <C> - <C> - <C> 48.0 <C> 87.2 <R> <C> Paragraph-Vec  <C> - <C> - <C> - <C> 48.7 <C> [BOLD] 87.8 <R> <C> MT-LSTM (F2S)  <C> - <C> - <C> - <C> 49.1 <C> 87.2 <R> <C> UPNN(np UP)  <C> 57.7 <C> 58.5 <C> 40.5 <C> - <C> - <R> <C> UPNN(full)  <C> 59.6 <C> 60.8 <C> 43.5 <C> - <C> - <R> <C> Cached LSTM  <C> 59.4 <C> 59.2 <C> 42.1 <C> - <C> - <R> <C> Max pooling <C> 61.1 <C> 61.2 <C> 41.1 <C> 48.0 <C> 87.0 <R> <C> Average pooling <C> 60.7 <C> 60.6 <C> 39.1 <C> 46.2 <C> 85.2 <R> <C> Self-attention <C> 61.0 <C> 61.5 <C> 43.3 <C> 48.2 <C> 86.4 <R> <C> Standard DR-AGG <C> [BOLD] 62.1 <C> [BOLD] 63.0 <C> [BOLD] 45.1 <C> [BOLD] 50.5 <C> 87.6 <R> <C> Reverse DR-AGG <C> 61.6 <C> 62.5 <C> 44.5 <C> 49.3 <C> 87.2 <CAP> Table 3: Experimental result comparison on five datasets. For the document-level datasets, hierarchical aggregation is used for both self-attention and DR-AGGs.
<R> <C> [EMPTY] <C> [EMPTY] <C> All 100 topics WS <C> All 100 topics TMN <C> All 100 topics AN <C> Top 20 topics WS <C> Top 20 topics TMN <C> Top 20 topics AN <R> <C> { 216mm <C> LDA <C> -0.0030±0.0047 <C> 0.0319±0.0032 <C> -0.0636±0.0033 <C> 0.1025±0.0067 <C> 0.137±0.0043 <C> -0.0010±0.0052 <R> <C> [EMPTY] <C> PTM <C> -0.0029±0.0048 <C> 0.0355±0.0016 <C> -0.0640±0.0037 <C> 0.1033±0.0081 <C> 0.1527±0.0052 <C> 0.0004±0.0037 <R> <C> Doc labels → <C> DMR <C> 0.0091±0.0046 <C> 0.0396±0.0044 <C> -0.0457±0.0024 <C> 0.1296±0.0085 <C> 0.1472±0.1507 <C> 0.0276±0.0101 <R> <C> { 317mm <C> LF-LDA <C> 0.0130±0.0052 <C> 0.0397±0.0026 <C> -0.0523±0.0023 <C> 0.1230±0.0153 <C> 0.1456±0.0087 <C> 0.0272±0.0042 <R> <C> [EMPTY] <C> WF-LDA <C> 0.0091±0.0046 <C> 0.0390±0.0051 <C> -0.0457±0.0024 <C> 0.1296±0.0085 <C> 0.1507±0.0055 <C> 0.0276±0.0101 <R> <C> [EMPTY] <C> GPU-DMM <C> -0.0934±0.0106 <C> -0.0970±0.0034 <C> -0.0769±0.0012 <C> 0.0836±0.0105 <C> 0.0968±0.0076 <C> -0.0613±0.0020 <R> <C> Doc labels & word features → <C> MetaLDA <C> [BOLD] 0.0311±0.0038 <C> [BOLD] 0.0451±0.0034 <C> [BOLD] -0.0326±0.0019 <C> [BOLD] 0.1511±0.0093 <C> [BOLD] 0.1584±0.0072 <C> [BOLD] 0.0590±0.0065 <CAP> TABLE IV: Topic coherence (NPMI) on the short text datasets.
<R> <C> [EMPTY] <C> Dataset #Topics <C> Reuters 50 <C> Reuters 100 <C> Reuters 150 <C> Reuters 200 <C> WS 50 <C> WS 100 <C> WS 150 <C> WS 200 <C> NYT 200 <C> NYT 500 <R> <C> { 216mm <C> LDA <C> 0.0899 <C> 0.1023 <C> 0.1172 <C> 0.1156 <C> 0.0219 <C> 0.0283 <C> 0.0301 <C> 0.0351 <C> 0.7509 <C> 1.1400 <R> <C> [EMPTY] <C> PTM <C> 4.9232 <C> 5.8885 <C> 7.2226 <C> 7.7670 <C> 1.1840 <C> 1.6375 <C> 1.8288 <C> 2.0030 <C> - <C> - <R> <C> { 213mm <C> DMR <C> 0.6112 <C> 0.9237 <C> 1.2638 <C> 1.6066 <C> 0.4603 <C> 0.8549 <C> 1.2521 <C> 1.7173 <C> 13.7546 <C> 31.9571 <R> <C> [EMPTY] <C> MetaLDA-dl-0.01 <C> 0.1187 <C> 0.1387 <C> 0.1646 <C> 0.1868 <C> 0.0396 <C> 0.0587 <C> 0.0769 <C> 0.112 1 <C> 2.4679 <C> 4.9928 <R> <C> { 417mm <C> LF-LDA <C> 2.6895 <C> 5.3043 <C> 8.3429 <C> 11.4419 <C> 2.4920 <C> 6.0266 <C> 9.1245 <C> 11.5983 <C> 95.5295 <C> 328.0862 <R> <C> [EMPTY] <C> WF-LDA <C> 1.0495 <C> 1.6025 <C> 3.0304 <C> 4.8783 <C> 1.8162 <C> 3.7802 <C> 6.1863 <C> 8.6599 <C> 14.0538 <C> 31.4438 <R> <C> [EMPTY] <C> GPU-DMM <C> 0.4193 <C> 0.7190 <C> 1.0421 <C> 1.3229 <C> 0.1206 <C> 0.1855 <C> 0.2487 <C> 0.3118 <C> - <C> - <R> <C> [EMPTY] <C> MetaLDA-0.1-wf <C> 0.2427 <C> 0.4274 <C> 0.6566 <C> 0.9683 <C> 0.1083 <C> 0.1811 <C> 0.2644 <C> 0.3579 <C> 4.6205 <C> 12.4177 <R> <C> Doc labels & word features → <C> MetaLDA <C> 0.2833 <C> 0.5447 <C> 0.7222 <C> 1.0615 <C> 0.1232 <C> 0.2040 <C> 0.3282 <C> 0.4167 <C> 6.4644 <C> 16.9735 <CAP> TABLE V: Running time (seconds per iteration) on 80% documents of each dataset.
<R> <C> Method <C> Method <C> AIMed P <C> AIMed R <C> AIMed F <C> BioInfer P <C> BioInfer R <C> BioInfer F <R> <C> [ITALIC] 1 <C> McDepCNN <C> 67.3 <C> 60.1 <C> [BOLD] 63.5 <C> 62.7 <C> 68.2 <C> [BOLD] 65.3 <R> <C> [ITALIC] 2 <C> Deep neutral network Zhao et al. ( 2016a ) <C> 51.5 <C> 63.4 <C> 56.1 <C> 53.9 <C> 72.9 <C> 61.6 <R> <C> [ITALIC] 3 <C> All-path graph kernel Tikk et al. ( 2010 ) <C> 49.2 <C> 64.6 <C> 55.3 <C> 53.3 <C> 70.1 <C> 60.0 <R> <C> [ITALIC] 4 <C> Edit kernel Peng et al. ( 2015 ) <C> 65.3 <C> 57.3 <C> 61.1 <C> 59.9 <C> 57.6 <C> 58.7 <R> <C> [ITALIC] 5 <C> Rich-feature Van Landeghem et al. ( 2008 ) <C> 49.0 <C> 44.0 <C> 46.0 <C> – <C> – <C> – <R> <C> [ITALIC] 6 <C> RelEx Fundel et al. ( 2007 ) <C> 40.0 <C> 50.0 <C> 44.0 <C> 39.0 <C> 45.0 <C> 41.0 <CAP> Table 2: Evaluation results. Performance is reported in terms of Precision, Recall, and F1-score.
<R> <C> Corpus <C> Positive difficult <C> Negative difficult <R> <C> AIMed <C> 61 <C> 184 <R> <C> BioInfer <C> 111 <C> 295 <CAP> Table 4: Instances that are the most difficult to classify correctly by the collection of kernels using cross-validation Tikk et al. (2013).
<R> <C> Method <C> P <C> R <C> F <R> <C> McDepCNN <C> 14.0 <C> 22.7 <C> [BOLD] 17.3 <R> <C> All-path graph kernel <C> 4.3 <C> 7.9 <C> 5.5 <R> <C> Edit kernel <C> 4.8 <C> 5.8 <C> 5.3 <R> <C> Shallow linguistic <C> 3.6 <C> 7.9 <C> 4.9 <CAP> Table 5: Comparisons on the difficult instances with CV evaluation. Performance is reported in terms of Precision, Recall, and F1-score∗.
<R> <C> Method <C> P <C> R <C> F <C> Δ <R> <C> window = 3 <C> 67.3 <C> 60.1 <C> 63.5 <C> [EMPTY] <R> <C> window = [3,5] <C> 60.9 <C> 62.4 <C> 61.6 <C> (1.9) <R> <C> window = [3,5,7] <C> 61.7 <C> 61.9 <C> 61.8 <C> (1.7) <R> <C> Single channel <C> 62.8 <C> 62.3 <C> 62.6 <C> (1.1) <CAP> Table 6: Contributions of different parts in McDepCNN. Performance is reported in terms of Precision, Recall, and F1-score.
<R> <C> [EMPTY] <C> 1 [ITALIC] st [BOLD] -person mentions  [BOLD] Word <C> 1 [ITALIC] st [BOLD] -person mentions  [BOLD] Context <C> [BOLD] Present Participle  [BOLD] Word <C> [BOLD] Present Participle  [BOLD] Context <R> <C> IIC <C> 58.2 <C> [BOLD] 41.0 <C> 79.6 <C> [BOLD] 72.5 <R> <C> DUC <C> 66.4 <C> [BOLD] 54.75 <C> [BOLD] 33.0 <C> 40.75 <R> <C> PHMC <C> 64.8 <C> [BOLD] 37.5 <C> 61.6 <C> [BOLD] 40.0 <CAP> Table 4: Average number of instances (out of 100 randomly sampled mis-classified instances) containing first-person mentions and present participle form for the three classification problems and two types of representations.
<R> <C> [BOLD] Languages <C> [BOLD] LIdioms <C> [BOLD] BabelNet Retrieval <C> [BOLD] BabelNet Accepted <C> [BOLD] BabelNet Precision <C> [BOLD] DBnary Retrieval <C> [BOLD] DBnary Accepted <C> [BOLD] DBnary Precision <R> <C> English <C> 291 <C> 600 <C> 195 <C> 0.325 <C> 362 <C> 323 <C> 0.892 <R> <C> Portuguese <C> 114 <C> 23 <C> 9 <C> 0.391 <C> 26 <C> 4 <C> 0.153 <R> <C> Italian <C> 175 <C> 52 <C> 33 <C> 0.634 <C> 4 <C> 4 <C> 1.0 <R> <C> German <C> 130 <C> 27 <C> 8 <C> 0.296 <C> 45 <C> 45 <C> 1.0 <R> <C> Russian <C> 105 <C> 48 <C> 16 <C> 0.333 <C> 0 <C> 0 <C> 0 <R> <C> [BOLD] Total <C> [BOLD] 815 <C> [BOLD] 750 <C> [BOLD] 261 <C> [EMPTY] <C> [BOLD] 437 <C> [BOLD] 384 <C> [EMPTY] <CAP> Table 4: Number of links and precision values obtained between LIdioms and other data sets.
<R> <C> Cycle <C> Test1 <C> Test2 <C> Test3 <C> Test4 <R> <C> 30 years <C> 1910 ± 5 <C> 1940 ± 5 <C> 1970 ± 5 <C> 2000 ± 5 <R> <C> Percent changed <C> 14.7 <C> 13.7 <C> 11.0 <C> 8.4 <R> <C> Number of synsets <C> 3,041 <C> 3,622 <C> 3,958 <C> 4,275 <R> <C> 40 years <C> 1920 ± 5 <C> 1960 ± 5 <C> 2000 ± 5 <C> [EMPTY] <R> <C> Percent changed <C> 17.5 <C> 14.5 <C> 11.0 <C> [EMPTY] <R> <C> Number of synsets <C> 3,038 <C> 3,732 <C> 4,203 <C> [EMPTY] <R> <C> 50 years <C> 1950 ± 5 <C> 2000 ± 5 <C> [EMPTY] <C> [EMPTY] <R> <C> Percent changed <C> 19.0 <C> 13.3 <C> [EMPTY] <C> [EMPTY] <R> <C> Number of synsets <C> 3,484 <C> 4,092 <C> [EMPTY] <C> [EMPTY] <R> <C> 60 years <C> 2000 ± 5 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Percent changed <C> 15.4 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Number of synsets <C> 3,958 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 10: The effect that varying cycle lengths has on the percentage of synsets that have changed leadership from present to future.
<R> <C> Trigrams <C> Means of Gaussians Losers <C> Means of Gaussians Winners <C> Means of Gaussians Difference <C> Presence of the trigram suggests … <R> <C> ize <C> 0.0055 <C> 0.0285 <C> [BOLD] 0.0230 <C> winner <R> <C> ise <C> 0.0289 <C> 0.0083 <C> − [BOLD] 0.0206 <C> loser <R> <C> nes <C> 0.0328 <C> 0.0134 <C> − [BOLD] 0.0194 <C> loser <R> <C> ty| <C> 0.0112 <C> 0.0297 <C> [BOLD] 0.0185 <C> winner <R> <C> ss| <C> 0.0379 <C> 0.0202 <C> − [BOLD] 0.0177 <C> loser <R> <C> ity <C> 0.0100 <C> 0.0269 <C> [BOLD] 0.0169 <C> winner <R> <C> ze| <C> 0.0022 <C> 0.0174 <C> [BOLD] 0.0152 <C> winner <R> <C> ess <C> 0.0373 <C> 0.0229 <C> − [BOLD] 0.0144 <C> loser <R> <C> se| <C> 0.0206 <C> 0.0083 <C> − [BOLD] 0.0123 <C> loser <R> <C> lis <C> 0.0154 <C> 0.0032 <C> − [BOLD] 0.0122 <C> loser <R> <C> ic| <C> 0.0228 <C> 0.0348 <C> [BOLD] 0.0120 <C> winner <R> <C> liz <C> 0.0022 <C> 0.0115 <C> [BOLD] 0.0093 <C> winner <CAP> Table 12: Analysis of the unique ngrams features in the naive Bayes models for Test1. The table lists the top dozen trigrams with the greatest separation between the means. Differences that are statistically significant are marked in bold; all of the differences are significant. Significance is measured by a two-tailed unpaired t test with a 95% confidence level.
<R> <C> [EMPTY] <C> Con <C> Attr <C> Flu <C> Sem <R> <C> Model <C> G-BLEU <C> Cls(%) <C> t-PPL <C> BERTscore <R> <C> SST (0.7, 0) <C> 19.11 <C> 82.2 <C> 306.65 <C> 89.96 <R> <C> - Style loss <C> 19.78 <C> 78.2 <C> 341.51 <C> 89.84 <CAP> Table 2: Ablation result of style loss in the Yelp dataset. (Con: content, Attr: attribute, Flu: Fluency, Sem: Semantic)
<R> <C> Model <C> [ITALIC] ρstyle <C> [ITALIC] ρsem <C> SyntaxAcc @5 <C> SyntaxAcc @10 <R> <C> CBOW-near-ctx <C> 12.1 <C> 27.8 <C> 86.3 <C> 85.2 <R> <C> CBOW-all-ctx <C> 36.6 <C> 24.0 <C> 85.3 <C> 84.1 <R> <C> CBOW-dist-ctx <C> [BOLD] 56.1 <C> 15.9 <C> 59.4 <C> 58.8 <R> <C> CBOW-sep-ctx <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] x (Stylistic) <C> [BOLD] 51.3 <C> [BOLD] 28.9 <C> 68.3 <C> 66.2 <R> <C> [ITALIC] y (Syntactic/semantic) <C> 9.6 <C> 18.1 <C> [BOLD] 88.0 <C> [BOLD] 87.0 <CAP> Table 1: Results of the quantitative evaluations.
<R> <C> [EMPTY] <C> BM25 F1 <C> BM25 DCG <C> DRMM F1 <C> DRMM DCG <R> <C> Oracle <C> 0.367 <C> 1.176 <C> 0.375 <C> 1.292 <R> <C> Fixed- [ITALIC] k (5) <C> 0.158 <C> -0.261 <C> 0.151 <C> 0.010 <R> <C> Fixed- [ITALIC] k (10) <C> 0.209 <C> -0.708 <C> 0.197 <C> -0.407 <R> <C> Fixed- [ITALIC] k (50) <C> 0.239 <C> -5.807 <C> 0.261 <C> -5.153 <R> <C> Greedy- [ITALIC] k <C> 0.248 <C> -0.116 <C> 0.263 <C> 0.266 <R> <C> BiCut <C> 0.244 <C> - <C> 0.262 <C> - <R> <C> Choppy <C> [BOLD] 0.272 <C> [BOLD] -0.041 <C> [BOLD] 0.268 <C> [BOLD] 0.295 <R> <C> Rel. % Gain <C> +11.5% <C> - <C> +2.29% <C> - <CAP> Table 1. Average F1 and DCG performance on Robust04. Choppy achieves state-of-the-art performance. “Gain” reports relative performance gain over BiCut model.
<R> <C> [BOLD] Bias Term <C> [BOLD] Accuracy % <C> [BOLD] Kappa <R> <C> 1 <C> 91.3 <C> 0.9121 <R> <C> 3 <C> 91.2 <C> 0.9102 <R> <C> 4 <C> 91.17 <C> 0.9006 <R> <C> 5 <C> 91.19 <C> 0.9009 <CAP> Table 7: Performances for the different parameter setting choices
<R> <C> Model <C> Params <C> ConceptNet <C> RE <C> SQuAD <C> T-REx <C> Avg. <R> <C> BERT-base <C> 110m <C> 15.6 <C> 9.8 <C> 14.1 <C> 31.1 <C> 17.7 <R> <C> BERT-large <C> 340m <C> 19.2 <C> 10.5 <C> 17.4 <C> 32.3 <C> 19.9 <R> <C> MM-Base <C> 110m <C> 11.0 <C> 9.6 <C> 17.2 <C> 30.6 <C> 17.1 <R> <C> MM-Large <C> 340m <C> 12.4 <C> 6.5 <C> 24.4 <C> 31.4 <C> 18.7 <R> <C> EaE-unsup <C> 366m <C> 10.6 <C> 8.4 <C> 23.1 <C> 30.0 <C> 18.0 <R> <C> No EaE <C> 366m <C> 10.3 <C> 9.2 <C> 18.5 <C> 31.8 <C> 17.4 <R> <C> EaE <C> 367m <C> 10.7 <C> 9.4 <C> 22.4 <C> 37.4 <C> 20.0 <CAP> Table 2: Results on the lama probe. Adding entity memory improves performance for the SQuAD and T-Rex probes, which typically require prediction of mention words. Our mention masking strategy reduces performance on the ConceptNet sub-task, which require prediction of non-mention terms such as “happy”.
<R> <C> Systems <C> Oracle Acc. <C> Pred Overlap (%) <R> <C> T5 & EaE <C> 55.9 <C> 29.3 <R> <C> T5 & GR <C> [BOLD] 66.4 <C> 30.1 <R> <C> EaE & GR <C> 64.6 <C> 33.6 <R> <C> Orqa & GR <C> 63.8 <C> [BOLD] 39.6 <CAP> Table 5: Comparing prediction overlap and oracle accuracy on TriviaQA. Oracle accuracy considers a prediction correct if at least one of the model prediction is correct. While Orqa and GR outperform EaE and T5, their predictions overlap more with GR and offer less complementary value.
<R> <C> Metrics <C> Classes Happy <C> Classes Sad <C> Classes Neutral <C> Classes Anger <R> <C> Accuracy <C> 74.3 <C> 75.6 <C> 78.4 <C> 79.6 <R> <C> F-Score <C> 81.4 <C> 77.0 <C> 71.2 <C> 77.6 <CAP> Table 4: Class-wise accuracy and f-score for IEMOCAP dataset for trimodal scenario.
<R> <C> Model <C> Test <R> <C> LSTM (Bowman et al.,  2015 ) <C> 80.6 <R> <C> GRU (Vendrov et al.,  2015 ) <C> 81.4 <R> <C> Tree CNN (Mou et al.,  2016 ) <C> 82.1 <R> <C> SPINN-PI (Bowman et al.,  2016 ) <C> 83.2 <R> <C> NTI (Munkhdalai and Yu,  2016b ) <C> 83.4 <R> <C> Intra-Att BiLSTM (Liu et al.,  2016 ) <C> 84.2 <R> <C> Self-Att BiLSTM (Lin et al.,  2017 ) <C> 84.2 <R> <C> NSE (Munkhdalai and Yu,  2016a ) <C> 84.6 <R> <C> Gated-Att BiLSTM <C> 85.5 <CAP> Table 2: Accuracies of the models on SNLI.
<R> <C> [EMPTY] <C> [BOLD] Feature combination <C> [BOLD] Cross-validation scores F1 <C> [BOLD] Cross-validation scores P <C> [BOLD] Cross-validation scores R <C> [BOLD] Cross-validation scores Acc <C> [BOLD] Cross-validation scores AUC <C> [BOLD] Holdout scores F1 <C> [BOLD] Holdout scores P <C> [BOLD] Holdout scores R <C> [BOLD] Holdout scores Acc <C> [BOLD] Holdout scores AUC <R> <C> [BOLD] EN <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> B + C + D + E <C> [BOLD] 64.26 <C> 73.32 <C> 57.19 <C> 96.97 <C> 78.07 <C> 63.69 <C> 74.13 <C> 55.82 <C> 97.21 <C> 77.47 <R> <C> [EMPTY] <C> A + B + C <C> 64.24 <C> 73.22 <C> 57.23 <C> 96.96 <C> 78.09 <C> [BOLD] 64.32 <C> 74.08 <C> 56.83 <C> 97.24 <C> 77.96 <R> <C> [EMPTY] <C> A + C + E <C> 63.84 <C> 73.21 <C> 56.59 <C> 96.94 <C> 77.78 <C> 62.94 <C> 72.82 <C> 55.42 <C> 97.14 <C> 77.24 <R> <C> [EMPTY] <C> word  [ITALIC] n-gram baseline <C> 58.17 <C> 67.55 <C> 51.07 <C> 96.54 <C> 74.93 <C> 59.63 <C> 69.57 <C> 52.17 <C> 96.57 <C> 75.50 <R> <C> [EMPTY] <C> profanity baseline <C> 17.17 <C> 9.61 <C> 80.14 <C> 63.73 <C> 71.53 <C> 17.61 <C> 9.90 <C> 78.51 <C> 63.79 <C> 71.34 <R> <C> [BOLD] NL <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> A + B + C + E <C> [BOLD] 61.20 <C> 56.76 <C> 66.40 <C> 94.47 <C> 81.42 <C> 58.13 <C> 54.03 <C> 62.90 <C> 94.58 <C> 79.75 <R> <C> [EMPTY] <C> A + B + C + D + E <C> 61.03 <C> 71.55 <C> 53.20 <C> 95.53 <C> 75.86 <C> [BOLD] 58.72 <C> 67.40 <C> 52.03 <C> 95.62 <C> 75.21 <R> <C> [EMPTY] <C> A + C + E <C> 60.82 <C> 71.66 <C> 52.84 <C> 95.53 <C> 75.68 <C> 58.15 <C> 67.71 <C> 50.96 <C> 95.61 <C> 74.71 <R> <C> [EMPTY] <C> word  [ITALIC] n-gram baseline <C> 50.39 <C> 67.80 <C> 40.09 <C> 94.81 <C> 69.38 <C> 49.54 <C> 64.29 <C> 40.30 <C> 95.09 <C> 69.44 <R> <C> [EMPTY] <C> profanity baseline <C> 28.46 <C> 19.24 <C> 54.66 <C> 81.99 <C> 69.28 <C> 25.13 <C> 16.73 <C> 50.53 <C> 81.99 <C> 67.26 <CAP> Table 6: Cross-validated and holdout scores (%) according to different metrics (F1, precision, recall, accuracy and area under the curve) for the English and Dutch top 3 combined feature type systems.
<R> <C> [BOLD] Test Dataset  [BOLD] Train Dataset <C> [BOLD] Laptops <C> [BOLD] Laptops <C> [BOLD] Laptops  [BOLD] Restaurants <C> [BOLD] Laptops  [BOLD] Restaurants <C> [BOLD] Laptops  [BOLD] Lapt. + Rest. <C> [BOLD] Laptops  [BOLD] Lapt. + Rest. <C> [BOLD] Restaurants <C> [BOLD] Restaurants <C> [BOLD] Restaurants  [BOLD] Laptops <C> [BOLD] Restaurants  [BOLD] Laptops <C> [BOLD] Restaurants  [BOLD] Lapt. + Rest. <C> [BOLD] Restaurants  [BOLD] Lapt. + Rest. <R> <C> [BOLD] Train Type <C> [BOLD] In → <C> [BOLD] In → <C> [BOLD] Cross ↔ <C> [BOLD] Cross ↔ <C> [BOLD] Joint ∪ <C> [BOLD] Joint ∪ <C> [BOLD] In → <C> [BOLD] In → <C> [BOLD] Cross ↔ <C> [BOLD] Cross ↔ <C> [BOLD] Joint ∪ <C> [BOLD] Joint ∪ <R> <C> [BOLD] Other Methods <C> [BOLD] Acc <C> [BOLD] MF1 <C> [BOLD] Acc <C> [BOLD] MF1 <C> [BOLD] Acc <C> [BOLD] MF1 <C> [BOLD] Acc <C> [BOLD] MF1 <C> [BOLD] Acc <C> [BOLD] MF1 <C> [BOLD] Acc <C> [BOLD] MF1 <R> <C> SDGCN-BERT <C> [BOLD] 81.35 <C> [BOLD] 78.34 <C> - <C> - <C> - <C> - <C> 83.57 <C> 76.47 <C> - <C> - <C> - <C> - <R> <C> AEN-BERT <C> 79.93 <C> 76.31 <C> - <C> - <C> - <C> - <C> 83.12 <C> 73.76 <C> - <C> - <C> - <C> - <R> <C> BERT-SPC <C> 78.99 <C> 75.03 <C> - <C> - <C> - <C> - <C> 84.46 <C> 76.98 <C> - <C> - <C> - <C> - <R> <C> BERT-PT <C> 78.07 <C> 75.08 <C> - <C> - <C> - <C> - <C> 84.95 <C> 76.96 <C> - <C> - <C> - <C> - <R> <C> [BOLD] Baselines <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> XLNet-base <C> 79.89 <C> 77.78 <C> 77.78 <C> 72.24 <C> [BOLD] 80.88 <C> 76.92 <C> 85.84 <C> 78.35 <C> 82.41 <C> 72.98 <C> 86.15 <C> 78.93 <R> <C> BERT-base <C> 77.69 <C> 72.60 <C> 75.86 <C> 70.78 <C> 78.81 <C> 74.47 <C> 84.92 <C> 76.93 <C> 80.07 <C> 69.93 <C> 85.03 <C> 77.35 <R> <C> [BOLD] Ours <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> BERT-ADA Lapt <C> 79.19 <C> 74.18 <C> [BOLD] 77.92 <C> [BOLD] 72.99 <C> 80.23 <C> 75.77 <C> 85.51 <C> 78.09 <C> 80.68 <C> 72.93 <C> 86.22 <C> 79.79 <R> <C> BERT-ADA Rest <C> 78.60 <C> 74.09 <C> 76.16 <C> 70.46 <C> 79.14 <C> 74.93 <C> [BOLD] 87.14 <C> [BOLD] 80.05 <C> [BOLD] 83.68 <C> 72.91 <C> [BOLD] 87.89 <C> 81.05 <R> <C> BERT-ADA Joint <C> 78.96 <C> 74.18 <C> 75.91 <C> 69.84 <C> 79.94 <C> [BOLD] 78.74 <C> 86.35 <C> 78.89 <C> 82.23 <C> [BOLD] 73.03 <C> 87.69 <C> [BOLD] 81.20 <CAP> Table 2: Summary of results for Aspect-Target Sentiment Classification for in-domain, cross-domain, and joint-domain training on SemEval 2014 Task 4 Subtask 2 datasets. The cells with gray background correspond to the cross-domain adaptation case, where the language model is finetuned on the target domain. As evaluation metrics accuracy (Acc) and Macro-F1 (MF1) are used.
<R> <C> [EMPTY] <C> mean acc <C> std. err <R> <C> geography + symmetric + asymmetric <C> 74.37 <C> 0.08 <R> <C> geography + symmetric <C> 74.09 <C> 0.07 <R> <C> symmetric + asymmetric <C> 73.13 <C> 0.08 <R> <C> geography + population <C> 67.33 <C> 0.08 <R> <C> geography <C> 66.48 <C> 0.09 <CAP> Table 4: Average accuracy predicting links between MSA pairs, and its Monte Carlo standard error (calculated from K=100 simulation samples). The feature groups are defined in Table 3; “population” refers to “raw diff log population.”
<R> <C> [BOLD] Location  [BOLD] Model <C> [BOLD] Location  [BOLD]  + →  <C> [BOLD] Location  [BOLD]  + →  <C> [BOLD] Location  [BOLD]  + →  <R> <C> LR-BOW <C> 78.58 <C> 78.27 <C> 77.97 <R> <C> LR-BOW+POS <C> 78.27 <C> 77.88 <C> 78.08 <R> <C> BiLSTM-Att <C> 80.29 <C> 77.59 <C> 73.19 <R> <C> ULMFit <C> 83.47 <C> 81.55 <C> 81.55 <R> <C> BERT <C> 86.69 <C> 83.78 <C> 83.12 <R> <C> RoBERTa <C> [BOLD] 87.70 <C> [BOLD] 85.10 <C> [BOLD] 85.99 <R> <C> XLNet <C> 85.32 <C> [BOLD] 85.17 <C> 85.32 <CAP> Table 7: F1-scores for parody prediction splitting by location. Best results are in bold.
<R> <C> Real Feature <C> Real r <C> Parody Feature <C> Parody r <R> <C> Unigrams <C> Unigrams <C> Unigrams <C> Unigrams <R> <C> our <C> 0.140 <C> i <C> 0.181 <R> <C> in <C> 0.131 <C> ? <C> 0.156 <R> <C> and <C> 0.129 <C>  <C> 0.145 <R> <C> : <C> 0.118 <C> me <C> 0.136 <R> <C>  <C> 0.114 <C> not <C> 0.106 <R> <C> today <C> 0.105 <C> like <C> 0.097 <R> <C> to <C> 0.105 <C> my <C> 0.095 <R> <C> of <C> 0.098 <C> dude <C> 0.094 <R> <C> the <C> 0.091 <C> don’t <C> 0.090 <R> <C> at <C> 0.087 <C> i’m <C> 0.087 <R> <C> lhl <C> 0.086 <C> just <C> 0.083 <R> <C> great <C> 0.085 <C> know <C> 0.081 <R> <C> with <C> 0.084 <C> #feeltheburp <C> 0.078 <R> <C> de <C> 0.079 <C> you <C> 0.076 <R> <C> meeting <C> 0.078 <C> #callmedick <C> 0.075 <R> <C> for <C> 0.077 <C> #imwithme <C> 0.073 <R> <C> across <C> 0.073 <C> ” <C> 0.073 <R> <C> families <C> 0.073 <C> #visionzero <C> 0.069 <R> <C> on <C> 0.070 <C> if <C> 0.069 <R> <C> country <C> 0.067 <C> have <C> 0.067 <R> <C> POS (Unigrams and Bigrams) <C> POS (Unigrams and Bigrams) <C> POS (Unigrams and Bigrams) <C> POS (Unigrams and Bigrams) <R> <C> NN IN <C> 0.1600 <C> RB <C> 0.1749 <R> <C> IN <C> 0.1507 <C> PRP <C> 0.1546 <R> <C> CC <C> 0.1309 <C> RB VB <C> 0.1271 <R> <C> IN JJ <C> 0.1210 <C> VBP <C> 0.1206 <R> <C> NNS IN <C> 0.1165 <C> VBP RB <C> 0.1123 <R> <C> NN CC <C> 0.1114 <C> . <C> 0.1114 <R> <C> IN NN <C> 0.1048 <C> NNP NNP <C> 0.1094 <R> <C> NN TO <C> 0.1030 <C> NN NNP <C> 0.1057 <R> <C> NNS TO <C> 0.1013 <C> WRB <C> 0.0925 <R> <C> TO <C> 0.1001 <C> VBP PRP <C> 0.0904 <R> <C> CC JJ <C> 0.0972 <C> IN PRP <C> 0.0890 <R> <C> IN DT <C> 0.0941 <C> NN VBP <C> 0.0863 <R> <C> : JJ <C> 0.0875 <C> RB . <C> 0.0854 <R> <C> NNS <C> 0.0855 <C> NNP <C> 0.0837 <R> <C> : NN <C> 0.0827 <C> JJ VBP <C> 0.0813 <CAP> Table 8: Feature correlations with parody and real tweets, sorted by Pearson correlation (r). All correlations are significant at p
<R> <C> [BOLD] System <C> [BOLD] Acc. <C> [BOLD] CP <C> [BOLD] PPL <R> <C> Full <C> 99.63 <C> [BOLD] 0.947 <C> [BOLD] 162.75 <R> <C> No Attention <C> 99.88 <C> 0.939 <C> 196.65 <R> <C> No Back Transfer <C> 97.08 <C> 0.938 <C> 257.93 <R> <C> No Att & Back Trans <C> [BOLD] 100.0 <C> 0.876 <C> 751.56 <CAP> Table 4: Ablation results for the Twitter dataset.
<R> <C> [BOLD] SNR <C> [BOLD] Train Accuracy (%)  [BOLD] -5dB <C> [BOLD] Train Accuracy (%)  [BOLD] 0dB <C> [BOLD] Train Accuracy (%)  [BOLD] 5dB <C> [BOLD] Test Accuracy (%)  [BOLD] -5dB <C> [BOLD] Test Accuracy (%)  [BOLD] 0dB <C> [BOLD] Test Accuracy (%)  [BOLD] 5dB <R> <C> [BOLD] Before <C> 86.6 <C> 64.4 <C> 32.5 <C> 72.0 <C> 58.4 <C> 31.7 <R> <C> [BOLD] Sub <C> 98.1 <C> 98.5 <C> 94.7 <C> 95.9 <C> 90.0 <C> [BOLD] 87.1 <R> <C> [BOLD] Mul <C> 97.3 <C> 94.4 <C> 90.5 <C> 95.5 <C> 88.4 <C> 83.2 <R> <C> [BOLD] Concat1 <C> 69.1 <C> 57.9 <C> 53.5 <C> 65.1 <C> 56.0 <C> 51.7 <R> <C> [BOLD] Concat2 <C> 99.4 <C> 99.3 <C> 97.8 <C> 89.2 <C> 70.9 <C> 64.1 <R> <C> [BOLD] Share-Concat <C> 99.5 <C> 99.7 <C> 95.5 <C> 93.7 <C> 87.0 <C> 59.5 <R> <C> [BOLD] Separate-Concat <C> 99.6 <C> 98.3 <C> 97.1 <C> [BOLD] 97.1 <C> [BOLD] 93.8 <C> 83.6 <R> <C> [BOLD] Clean <C> 99.2 <C> 99.2 <C> 99.2 <C> 98.5 <C> 98.5 <C> 98.5 <CAP> Table 3: speaker identification accuracy of using estimated embedding of interfering speaker e′2. Before denotes speaker identification directly using emix. Clean denotes speaker identification using e2 that extracted from clean speech.
<R> <C> [BOLD] SNR <C> [BOLD] Train Accuracy (%)  [BOLD] M1 <C> [BOLD] Train Accuracy (%)  [BOLD] M2 <C> [BOLD] Test Accuracy (%)  [BOLD] M1 <C> [BOLD] Test Accuracy (%)  [BOLD] M2 <R> <C> [BOLD] Before <C> 64.5 <C> 59.3 <C> 52.1 <C> 47.1 <R> <C> [BOLD] Sub <C> 91.4 <C> 89.5 <C> 87.2 <C> 83.9 <R> <C> [BOLD] Mul <C> 88.7 <C> 86.4 <C> 84.4 <C> 82.1 <R> <C> [BOLD] Concat1 <C> 74.1 <C> 69.5 <C> 50.2 <C> 41.7 <R> <C> [BOLD] Concat2 <C> 86.5 <C> 81.5 <C> 79.1 <C> 72.4 <R> <C> [BOLD] Share-Concat <C> 76.5 <C> 72.1 <C> 65.1 <C> 55.4 <R> <C> [BOLD] Separate-Concat <C> 96.5 <C> 94.3 <C> [BOLD] 91.3 <C> [BOLD] 90.9 <R> <C> [BOLD] Headset <C> 99.3 <C> 99.3 <C> 99.1 <C> 99.1 <CAP> Table 4: The speaker identification results on MC-WSJ dataset.
<R> <C> [BOLD] Model <C> [BOLD] Views <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> Chance <C> – <C> 34.53 <C> 34.59 <C> 34.53 <R> <C> LR <C> Title <C> 59.53 <C> 59.42 <C> 59.12 <R> <C> CNN <C> Title <C> 59.26 <C> 59.40 <C> 59.24 <R> <C> FNN <C> Network <C> 68.28 <C> 56.54 <C> 55.10 <R> <C> HDAM <C> Content <C> 69.85 <C> 68.72 <C> 68.92 <R> <C> MVDAM <C> Title, Network <C> [BOLD] 69.87 <C> [BOLD] 69.71 <C> [BOLD] 69.66 <R> <C> MVDAM <C> Title, Content <C> [BOLD] 70.84 <C> [BOLD] 70.19 <C> [BOLD] 69.54 <R> <C> MVDAM <C> Title, Network, Content <C> [BOLD] 80.10 <C> [BOLD] 79.56 <C> [BOLD] 79.67 <CAP> Table 1: Precision, Recall, and F1 scores of our model MVDAM on the test set compared with several baselines. All flavors of our model significantly outperform baselines and yield state of the art performance.
<R> <C> [BOLD] Model <C> [BOLD] Vocab <C> [BOLD] WER (%)  [BOLD] SW <C> [BOLD] WER (%)  [BOLD] CH <R> <C> Prior Work CTC <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Hannun et al. +LM  <C> 29 <C> 20.0 <C> 31.8 <R> <C> Zweig et al. +LM  <C> 79 <C> 19.8 <C> 32.1 <R> <C> Audhkhasi et al.  <C> 79 <C> 18.9 <C> [BOLD] 30.9 <R> <C> Prior Work S2S <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Lu et al. +LM  <C> 35 <C> 32.6 <C> 51.9 <R> <C> Zenkel et al.  <C> 46 <C> 28.1 <C> 40.6 <R> <C> Toshniwal et al.  <C> [EMPTY] <C> 23.1 <C> 40.8 <R> <C> Our models <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> S2S Char <C> 46 <C> [BOLD] 18.0 <C> 32.5 <R> <C> S2S Char +LM <C> 46 <C> [BOLD] 17.1 <C> [BOLD] 31.1 <R> <C> S2S Char +Word LM <C> 46 <C> [BOLD] 15.6 <C> [BOLD] 31.0 <CAP> Table 1: Word Error Rate (WER) for the SW and CH test sets using character target units, and comparison with other end-to-end character-level models. We compare with the re-scored character-LM results from prior work when available.
<R> <C> [BOLD] Model <C> [BOLD] Vocab <C> [BOLD] WER (%)  [BOLD] SW <C> [BOLD] WER (%)  [BOLD] CH <R> <C> Prior Work CTC <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Audhkhasi et al.  <C> 10000 <C> 14.5 <C> 23.9 <R> <C> Chen et al.  <C> 29874 <C> 24.9 <C> 36.5 <R> <C> Prior Work S2S <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Chen et al.  <C> 29874 <C> 31.2 <C> 40.5 <R> <C> Lu et al.  <C> 29874 <C> 26.8 <C> 48.2 <R> <C> Lu et al. +LM  <C> 29874 <C> 26.2 <C> 47.4 <R> <C> Our models <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> S2S BPE 12k <C> 11690 <C> [BOLD] 21.3 <C> [BOLD] 35.7 <R> <C> S2S Word >= 5 <C> 11069 <C> 23.0 <C> 37.2 <R> <C> S2S Word >= 5* <C> 11069 <C> 22.4 <C> 36.1 <R> <C> S2S Large Vocab <C> 29874 <C> [BOLD] 22.4 <C> [BOLD] 36.2 <R> <C> S2S Large Vocab + LM <C> 29874 <C> [BOLD] 22.1 <C> 36.3 <CAP> Table 2: Word Error Rate (WER) for the SW and CH test sets using BPE and word level target units, and comparison with other end-to-end word-level models. * denotes character initialization
<R> <C> [BOLD] SRC depuis 2003 , la chine est devenue le plus important partenaire commercial du mexique après les etats-unis .  [BOLD] REF since 2003 , china has become mexico ’s most important trading partner after the united states .  <C> [BOLD] SRC depuis 2003 , la chine est devenue le plus important partenaire commercial du mexique après les etats-unis .  [BOLD] REF since 2003 , china has become mexico ’s most important trading partner after the united states .  <R> <C> [BOLD] Partial sampled translation <C> [BOLD] Feedback <R> <C> since <C> 1 <R> <C> since 2003 , china has <C> 1 <R> <C> since 2003 , china has become <C> 1 <R> <C> since 2003 , china has become mexico <C> 1 <R> <C> since 2003 , china has become mexico ’s <C> 1 <R> <C> since 2003 , china has become mexico ’s most <C> 1 <R> <C> since 2003 , china has become mexico ’s most important <C> 1 <R> <C> since 2003 , china has become mexico ’s most important trading partner after the us .  <C> 0.8823 <R> <C> [BOLD] SRC la réponse que nous , en tant qu’ individus , acceptons est que nous sommes libres parce que nous nous gouvernons nous-mêmes en commun plutôt que d’ être dirigés par une organisation qui n’ a nul besoin de tenir compte de notre existence . <C> [BOLD] SRC la réponse que nous , en tant qu’ individus , acceptons est que nous sommes libres parce que nous nous gouvernons nous-mêmes en commun plutôt que d’ être dirigés par une organisation qui n’ a nul besoin de tenir compte de notre existence . <R> <C> [BOLD] REF the answer that we as individuals accept is that we are free because we rule ourselves in common , rather than being ruled by some agency that need not take account of us .  <C> [BOLD] REF the answer that we as individuals accept is that we are free because we rule ourselves in common , rather than being ruled by some agency that need not take account of us .  <R> <C> [BOLD] Partial sampled translation <C> [BOLD] Feedback <R> <C> the <C> 1 <R> <C> the answer <C> 1 <R> <C> the answer we <C> 0.6964 <R> <C> the answer we , <C> 0.6246 <R> <C> the answer  we  as individuals allow to 14 are <C> 0.6008 <R> <C> the answer  we , as individuals , go  down  to speak 8 , are being  free  because we  govern ourselves  , rather from being  based  together <C> 0.5155 <R> <C> the answer  we  , as people , accepts is that we principle are free because we govern ourselves ,  rather than  being led by a organisation which has absolutely no need to take our standards .  <C> 0.5722 <R> <C> [BOLD] SRC lors d’ un rallye “journée jérusalem” tenu à l’ université de téhéran en décembre 2001 , il a prononcé l’ une des menaces les plus sinistres du régime . <C> [BOLD] SRC lors d’ un rallye “journée jérusalem” tenu à l’ université de téhéran en décembre 2001 , il a prononcé l’ une des menaces les plus sinistres du régime . <R> <C> [BOLD] REF at a jerusalem day rally at tehran university in december 2001 , he uttered one of the regime ’s most sinister threats .  <C> [BOLD] REF at a jerusalem day rally at tehran university in december 2001 , he uttered one of the regime ’s most sinister threats .  <R> <C> [BOLD] Partial sampled translation <C> [BOLD] Feedback <R> <C> in <C> 0 <R> <C> in a  round of jerusalem called a academic university in teheran in december 2001  ,  he declared one in the most recent hostility  to  the regime   .  <C> 0.5903 <CAP> Table 4: Interaction protocol for three translations. These translations were sampled from the model when the algorithm decided to request human feedback (line 10 in Algorithm 1). Tokens that get an overall negative reward (in combination with the critic), are marked in red, the remaining tokens receive a positive reward. When a prefix is good (i.e. ≥μ, here μ=0.8) it is stored in the buffer and used for forced decoding for later samples (underlined).
<R> <C> [EMPTY] <C> BLEU-1 <C> BLEU-2 <C> BLEU-3 <C> BLEU-4 <C> ROUGE-L <C> Weighted metric <R> <C> flat (word level) with att <C> 65.36 <C> 44.03 <C> 29.68 <C> 20.40 <C> 51.04 <C> [BOLD] 104.78 <R> <C> phrase with att. <C> 64.69 <C> 43.37 <C> 28.80 <C> 19.31 <C> 50.80 <C> 102.14 <R> <C> phrase with att +phrase label <C> 65.46 <C> 44.59 <C> 29.36 <C> 19.25 <C> 51.40 <C> 103.64 <R> <C> phrase with 2 att +phrase label <C> 65.37 <C> 44.02 <C> 29.51 <C> 19.91 <C> 50.90 <C> 104.12 <CAP> Table 4: Comparing performance of the flat captioning model Xu15 , and different instantiations of our phrase-based captioning model. All these models were trained using the cross-entropy loss.
<R> <C> [BOLD] 1. TED corpus, no-OOV case, voc=40K  [BOLD] Method <C> [BOLD] 1. TED corpus, no-OOV case, voc=40K  [BOLD] BLEU↑ <C> [BOLD] 1. TED corpus, no-OOV case, voc=40K  [BOLD] TER↓ <C> [BOLD] 1. TED corpus, no-OOV case, voc=40K  [BOLD] CHRF3↑ <R> <C> No Segmentation <C> 17.77 <C> 68.07 <C> 38.94 <R> <C> BPE <C> 19.52 <C> 66.23 <C> 42.33 <R> <C> Supervised <C> 21.61▲ <C> 61.76▲ <C> [BOLD] 44.01 <R> <C> [BOLD] LMVR <C> [BOLD] 21.71▲ <C> [BOLD] 61.41▲ <C> 43.90 <CAP> Table 5: Results of Experiment 1 - TED corpus and no-OOV case. Top: Output accuracies, where ▲indicates statistically significant improvement over the BPE baseline (p-value 
<R> <C> [BOLD] Method <C> [BOLD] Incorrect assignments, % <R> <C> [BOLD] Mono-lingual <C> [BOLD] Mono-lingual <R> <C> Ukrainian <C> 4.7 <R> <C> Russian <C> 34.7 <R> <C> [BOLD] Cross-lingual <C> [BOLD] Cross-lingual <R> <C> Naive Binary <C> 50.17 <R> <C> Naive Count <C> 50.00 <R> <C> Edit distance translation Binary <C> 50.50 <R> <C> Edit distance translation Count <C> 50.50 <R> <C> Dictionary/Edit distance Binary <C> 50.33 <R> <C> Dictionary/Edit distance Count <C> 49.83 <R> <C> Matrix translation Binary <C> 36.33 <R> <C> Matrix translation Count <C> 36.17 <R> <C> Semantic fingerprints on word types <C> 35.33 <R> <C> Semantic fingerprints on word tokens <C> [BOLD] 5.50 <CAP> Table 2: Clustering correspondence to document topics
<R> <C> [BOLD] Test <C> [BOLD] CBoW <C> [BOLD] ELMo <C> [BOLD]  BERT (bbc)  <C> [BOLD]  BERT (blc)  <C> [BOLD] GPT <C> [BOLD]  GPT-2 (117M)  <C> [BOLD]  GPT-2 (345M)  <R> <C> gender <C> 0.73 <C> 0.03 <C> 0.32 <C> 0.12 <C> 0.35 <C> 0.24 <C> 0.15 <R> <C> race <C> 0.60 <C> 0.10 <C> 0.58 <C> 0.58 <C> 0.39 <C> 0.42 <C> 0.42 <R> <C> intersectional <C> 0.29 <C> 0.10 <C> 0.71 <C> 0.38 <C> 0.33 <C> 0.29 <C> 0.10 <R> <C> disability, age <C> 0.75 <C> 0.17 <C> 0.00 <C> 0.00 <C> 0.17 <C> 0.33 <C> 0.17 <R> <C> [BOLD] Overall <C> 0.58 <C> 0.08 <C> 0.48 <C> 0.33 <C> 0.35 <C> 0.32 <C> 0.23 <CAP> Table 2: Proportion of significant positive effect sizes across embedding association tests, broken down by type of identity and model. Significance level of 0.01. Other embedding association tests were conducted (C9: disability, C10: age) but are reported only in the Supplementary Material. The total number of embedding association tests was 92: 34 gender, 31 race, 21 intersectional, 6 disability, age. For CBoW the c-word encoding tests are invalid, so the numbers are 22, 20, 14, 4 respectively.
<R> <C> Methods <C> Jaccard <C> PR-AUC <C> F1 <C> # of parameters <R> <C> LR <C> 0.4075 <C> 0.6716 <C> 0.5658 <C> - <R> <C> GRAM <C> 0.4176 <C> 0.6638 <C> 0.5788 <C> 3,763,668 <R> <C> LEAP <C> 0.3921 <C> 0.5855 <C> 0.5508 <C> 1,488,148 <R> <C> RETAIN <C> 0.4456 <C> 0.6838 <C> 0.6064 <C> 2,054,869 <R> <C> GAMENet− <C> 0.4401 <C> 0.6672 <C> 0.5996 <C> 5,518,646 <R> <C> GAMENet <C> 0.4555 <C> 0.6854 <C> 0.6126 <C> 5,518,646 <R> <C> G-BERT [ITALIC] G−, [ITALIC] P− <C> 0.4186 <C> 0.6649 <C> 0.5796 <C> 2,634,145 <R> <C> G-BERT [ITALIC] G− <C> 0.4299 <C> 0.6771 <C> 0.5903 <C> 2,634,145 <R> <C> G-BERT [ITALIC] P− <C> 0.4236 <C> 0.6704 <C> 0.5844 <C> 3,034,045 <R> <C> G-BERT <C> [BOLD] 0.4565 <C> [BOLD] 0.6960 <C> [BOLD] 0.6152 <C> 3,034,045 <CAP> Table 3: Performance on Medication Recommendation Task.
<R> <C> Method <C> Accuracy <R> <C> MFS <C> 0.704 <R> <C> LogRes <C> 0.771 <R> <C> CNN  <C> 0.803 <R> <C> Tree-CRF  <C> 0.826 <R> <C> RvNN <C> 0.517 <R> <C> Reimplementation of Tai:2015 <C> 0.709 <R> <C> Reimplementation of K&P (2017) <C> 0.807 <R> <C> Tree-LSTM w/ attn <C> 0.810 <R> <C> Tree-LSTM w/ dict <C> 0.829 <R> <C> Tree-LSTM w/ attn, dict <C> [BOLD] 0.844 <CAP> Table 2: Accuracy of each method on the Japanese sentiment classification task.
<R> <C> Method <C> Accuracy <R> <C>  <C> [EMPTY] <R> <C> — CNN <C> 48.0 <R> <C>  <C> [EMPTY] <R> <C> — Tree-LSTM <C> 51.0 <R> <C>  <C> [EMPTY] <R> <C> — TreeGRU w/o attn <C> 50.5 <R> <C> — TreeGRU w/ attn <C> 51.0 <R> <C> Tree-LSTM <C> 43.52 <R> <C> Tree-LSTM w/ attn <C> 44.97 <R> <C> Tree-LSTM w/ dict <C> 43.13 <R> <C> Tree-LSTM w/ attn, dict <C> 41.67 <CAP> Table 3: Accuracy of each method on the English sentiment classification task. Note that our model learns only from sentence-level annotation.
<R> <C> [BOLD] Subject <C> [BOLD] Property <C> [BOLD] Baseline  [BOLD] CN-score <C> [BOLD] Dice  [BOLD] plausible <C> [BOLD] Dice  [BOLD] typical <C> [BOLD] Dice  [BOLD] remarkable <C> [BOLD] Dice  [BOLD] salient <R> <C> snake <C> be at shed <C> 0.46 <C> 0.29 <C> 0.71 <C> 0.29 <C> 0.18 <R> <C> snake <C> be at pet zoo <C> 0.46 <C> 0.15 <C> 0.29 <C> 0.82 <C> 0.48 <R> <C> snake <C> bite <C> 0.92 <C> 0.58 <C> 0.13 <C> 0.61 <C> 0.72 <R> <C> lawyer <C> study legal precedent <C> 0.46 <C> 0.25 <C> 0.73 <C> 0.37 <C> 0.18 <R> <C> lawyer <C> prove that person be guilty <C> 0.46 <C> 0.06 <C> 0.47 <C> 0.65 <C> 0.40 <R> <C> lawyer <C> present case <C> 0.46 <C> 0.69 <C> 0.06 <C> 0.79 <C> 0.75 <R> <C> bicycle <C> requires coordination <C> 0.67 <C> 0.62 <C> 0.40 <C> 0.36 <C> 0.35 <R> <C> bicycle <C> be used to travel quite long distance <C> 0.46 <C> 0.30 <C> 0.20 <C> 0.77 <C> 0.64 <R> <C> bicycle <C> be power by person <C> 0.67 <C> 0.19 <C> 0.33 <C> 0.66 <C> 0.55 <CAP> Table 9. Anecdotal examples from Dice run on ConceptNet.
<R> <C> Interpolation Method <C> Dynamic Val. PPL <C> Static # [ITALIC] n-grams <C> Static Val. PPL <C> Static Test PPL <C> Pruned # [ITALIC] n-grams <C> Pruned Test PPL <C> Pruned Test WER <R> <C> Uniform <C> - <C> 634M <C> 19.0 <C> 19.0 <C> 5.4M <C> 21.5 <C> 5.3 <R> <C> Linear Interpolation <C> 10.7 <C> 634M <C> 10.7 <C> 10.7 <C> 5.9M <C> 11.2 <C> 4.2 <R> <C> Count Merging <C> 08.8 <C> 634M <C> 08.8 <C> 08.8 <C> 7.7M <C> 09.2 <C> 3.8 <R> <C> Bayesian Interpolation <C> 08.8 <C> 634M <C> 08.8 <C> 08.8 <C> 7.6M <C> 09.2 <C> 3.8 <CAP> Table 3: Results in the Smart Speaker scenario. The Uniform method denotes linear interpolation with fixed uniform weights.
<R> <C> [ITALIC] Classifier <C> [ITALIC] Word <C> [ITALIC] Penultimate Layer <R> <C> [EMPTY] <C> [ITALIC] Embedding (d) <C> [ITALIC] Size (k) <R> <C> Topic <C> 300 <C> 256 <R> <C> Sentiment <C> 200 <C> 128 <CAP> Table 4: Hyperparameters based on cross-validation for topic and sentiment classifiers (L=3 i.e. l∈{2,3,4}, f=200 for both).
<R> <C> [EMPTY] <C> [BOLD] Unsupervised <C> [BOLD] Supervised <C> [BOLD] Similarity <R> <C> [EMPTY] <C> (Adversarial) <C> (Identical) <C> (Eigenvectors) <R> <C> en-es <C> 81.89 <C> [BOLD] 82.62 <C> 2.07 <R> <C> en-et <C> 00.00 <C> [BOLD] 31.45 <C> 6.61 <R> <C> en-fi <C> 00.09 <C> [BOLD] 28.01 <C> 7.33 <R> <C> en-el <C> 00.07 <C> [BOLD] 42.96 <C> 5.01 <R> <C> en-hu <C> 45.06 <C> [BOLD] 46.56 <C> 3.27 <R> <C> en-pl <C> 46.83 <C> [BOLD] 52.63 <C> 2.56 <R> <C> en-tr <C> 32.71 <C> [BOLD] 39.22 <C> 3.14 <R> <C> et-fi <C> [BOLD] 29.62 <C> 24.35 <C> 3.98 <CAP> Table 2: Bilingual dictionary induction scores (P@1×100%) using a) the unsupervised method with adversarial training; b) the supervised method with a bilingual seed dictionary consisting of identical words (shared between the two languages). The third columns lists eigenvector similarities between 10 randomly sampled source language nearest neighbor subgraphs of 10 nodes and the subgraphs of their translations, all from the benchmark dictionaries in Lample2018crosslingual.
<R> <C> [EMPTY] <C> en-es <C> en-hu <C> en-fi <R> <C> Noun <C> 80.94 <C> 26.87 <C> 00.00 <R> <C> Verb <C> 66.05 <C> 25.44 <C> 00.00 <R> <C> Adjective <C> 85.53 <C> 53.28 <C> 00.00 <R> <C> Adverb <C> 80.00 <C> 51.57 <C> 00.00 <R> <C> Other <C> 73.00 <C> 53.40 <C> 00.00 <CAP> Table 4: P@1×100% scores for query words with different parts-of-speech.
<R> <C> Spelling <C> Meaning <C> en-es <C> en-hu <C> en-fi <R> <C> Same <C> Same <C> 45.94 <C> 18.07 <C> 00.00 <R> <C> Same <C> Diff <C> 39.66 <C> 29.97 <C> 00.00 <R> <C> Diff <C> Diff <C> 62.42 <C> 34.45 <C> 00.00 <CAP> Table 5: Scores (P@1×100%) for query words with same and different spellings and meanings.
<R> <C> [BOLD] Syntactic Categories <C> [ITALIC] C=3 p1 <C> [ITALIC] C=3 p2 <C> [ITALIC] C=100 p1 <C> [ITALIC] C=100 p2 <C> [ITALIC] C=100 p̄1 <C> [ITALIC] C=100 p̄2 <R> <C> [BOLD] SUBJECT-VERB AGREEMENT <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Simple <C> 0.81 <C> 0.81 <C> 1.0 <C> 0.23 <C> 0.68 <C> 0.47 <R> <C> In a sentential complement <C> 0.79 <C> 0.79 <C> 0.98 <C> 0.14 <C> 0.69 <C> 0.48 <R> <C> Short VP coordination <C> 0.74 <C> 0.73 <C> 0.96 <C> 0.08 <C> 0.78 <C> 0.43 <R> <C> Long VP coordination <C> 0.61 <C> 0.61 <C> 0.97 <C> 0.06 <C> 0.55 <C> 0.47 <R> <C> Across a prepositional phrase <C> 0.78 <C> 0.78 <C> 0.97 <C> 0.07 <C> 0.62 <C> 0.49 <R> <C> Across a subject relative clause <C> 0.77 <C> 0.77 <C> 0.93 <C> 0.08 <C> 0.68 <C> 0.41 <R> <C> Across an object relative clause <C> 0.69 <C> 0.69 <C> 0.92 <C> 0.11 <C> 0.61 <C> 0.45 <R> <C> Across an object relative (no that) <C> 0.58 <C> 0.58 <C> 0.94 <C> 0.09 <C> 0.61 <C> 0.44 <R> <C> In an object relative clause <C> 0.74 <C> 0.74 <C> 0.99 <C> 0.01 <C> 0.60 <C> 0.45 <R> <C> In an object relative (no that) <C> 0.74 <C> 0.74 <C> 0.99 <C> 0.02 <C> 0.61 <C> 0.46 <R> <C> [BOLD] REFLEXIVE ANAPHORA <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Simple <C> 0.79 <C> 0.78 <C> 0.99 <C> 0.07 <C> 0.70 <C> 0.39 <R> <C> In a sentential complement <C> 0.74 <C> 0.73 <C> 1.00 <C> 0.00 <C> 0.70 <C> 0.38 <R> <C> Across a relative clause <C> 0.63 <C> 0.62 <C> 0.99 <C> 0.03 <C> 0.69 <C> 0.35 <R> <C> [BOLD] NEGATIVE POLARITY ITEMS <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Simple <C> 0.42 <C> 0.33 <C> 1.00 <C> 0.00 <C> 0.76 <C> 0.20 <R> <C> Across a relative clause <C> 0.37 <C> 0.36 <C> 1.00 <C> 0.00 <C> 0.98 <C> 0.02 <CAP> Table 4: p1: p(x−|z+)
<R> <C> [BOLD] Synthetic <C> [BOLD] AUC <C> [BOLD] FPED <C> [BOLD] FNED <R> <C> Baseline <C> .885 <C> 2.77 <C> 3.51 <R> <C> Importance <C> .850 <C> 2.90 <C> 3.06 <R> <C> TOK Replace <C> .930 <C> [BOLD] 0.00 <C> [BOLD] 0.00 <R> <C> Our Method <C> [BOLD] .952 <C> 0.01 <C> 0.11 <R> <C> Finetuned <C> .925 <C> 0.00 <C> 0.19 <CAP> Table 7: AUC and Bias mitigation metrics on synthetic dataset. The lower the better for Bias mitigation metrics and is bounded by 0. Numbers represent the mean of 5 runs. Maximum variance is 0.013.
<R> <C> WikiLarge <C> Gram <C> Mean <C> Simp <C> AVG <C> Min <R> <C> Reference <C> 4.01* <C> 4.13** <C> 3.56** <C> 3.90** <C> 3.16* <R> <C> DRESS-Ls <C> 4.32** <C> 3.97** <C> 3.14 <C> 3.81** <C> 2.80 <R> <C> DMASS <C> 3.69 <C> 3.21 <C> 2.57** <C> 3.16 <C> 2.29** <R> <C> Transformer <C> 3.91 <C> 3.63 <C> 3.04** <C> 3.53 <C> 2.72** <R> <C> CROSS-Lex <C> 3.72 <C> 3.41 <C> 3.18 <C> 3.43 <C> 2.80 <R> <C> CROSS-Syn <C> 3.54 <C> 2.22 <C> 2.46** <C> 3.07* <C> 2.15** <R> <C> CROSS <C> 3.61 <C> 3.37 <C> 3.13 <C> 3.37 <C> 2.84 <R> <C> Newsela <C> Gram <C> Mean <C> Simp <C> AVG <C> Min <R> <C> Reference <C> 4.11** <C> 3.73** <C> 3.88** <C> 3.91** <C> 3.47** <R> <C> DRESS-Ls <C> 3.33* <C> 2.98** <C> 2.93 <C> 3.08** <C> 2.45 <R> <C> DMASS <C> 2.05** <C> 1.55** <C> 1.74** <C> 1.78** <C> 1.39** <R> <C> Transformer <C> 2.88** <C> 2.47** <C> 2.70 <C> 2.68** <C> 2.00** <R> <C> CROSS-Lex <C> 3.07** <C> 2.89** <C> 2.95 <C> 2.97** <C> 2.45 <R> <C> CROSS-Syn <C> 3.60 <C> 3.37 <C> 2.89 <C> 3.27 <C> 2.31 <R> <C> CROSS <C> 3.54 <C> 3.41 <C> 2.91 <C> 3.28 <C> 2.29 <CAP> Table 6: Human evaluation on WikiLarge and Newsela. Models significantly different from CROSS are marked with * (p<0.05) and ** (p<0.01). Significance tests were performed using a student t-test.
<R> <C> WikiLarge <C> Gram <C> Mean <C> Simp <C> AVG <C> Min <C> FKGL <R> <C> XSimple <C> 3.30 <C> 3.09 <C> 3.06* <C> 3.15 <C> 2.84 <C> 6.96 <R> <C> Simple <C> 3.24 <C> 3.11 <C> 2.87 <C> 3.08 <C> 2.77 <C> 7.46 <R> <C> Newsela <C> Gram <C> Mean <C> Simp <C> AVG <C> Min <C> FKGL <R> <C> XSimple <C> 3.46** <C> 2.88** <C> 3.11** <C> 3.15 <C> 2.33** <C> 2.91 <R> <C> Simple <C> 3.89 <C> 3.59 <C> 2.53 <C> 3.34 <C> 2.10 <C> 3.51 <CAP> Table 8: Human evaluation on varying simplicity of model output. Ratings that are significantly different are marked with * (p<0.05) and ** (p<0.01). Significance tests were performed using a student t-test.
<R> <C> [EMPTY] <C> Wiki-727K F1 <C> Wiki-727K Recall <C> Wiki-727K Precision <C> RST-DT F1 <C> RST-DT Recall <C> RST-DT Precision <C> Choi F1 <C> P [ITALIC] k <R> <C> Bi-LSTM Koshorek et al. ( 2018 ) <C> 57.7±0.1 <C> 49.5±0.2 <C> 69.3±0.1 <C> - <C> - <C> - <C> - <C> - <R> <C> SEGBOT Li et al. ( 2018 ) <C> - <C> - <C> - <C> 92.2 <C> 92.8 <C> 91.6 <C> - <C> 0.33 <R> <C> Bi-LSTM+CRF Wang et al. ( 2018 ) <C> - <C> - <C> - <C> 94.3 <C> 95.7 <C> 92.8 <C> - <C> - <R> <C> Cross-segment BERT 128-128 <C> 66.0±0.1 <C> 63.2±0.2 <C> 69.1±0.1 <C> 95.0±0.5 <C> [BOLD] 98.0±0.4 <C> 92.1±0.8 <C> [BOLD] 99.9±0.1 <C> [BOLD] 0.07±0.4 <R> <C> BERT+Bi-LSTM <C> 59.9±0.1 <C> 53.9±0.1 <C> 67.3±0.1 <C> [BOLD] 95.7±0.4 <C> 96.8±0.5 <C> [BOLD] 94.6±0.5 <C> 99.8±0.1 <C> 0.17±0.6 <R> <C> Hier. BERT <C> [BOLD] 66.5±0.1 <C> [BOLD] 63.5±0.1 <C> [BOLD] 69.8±0.1 <C> 95.2±0.4 <C> 96.6±0.7 <C> 93.8±0.5 <C> 99.5±0.1 <C> 0.38±0.9 <R> <C> Human Wang et al. ( 2018 ) <C> - <C> - <C> - <C> 98.5 <C> 98.2 <C> 98.3 <C> - <C> - <CAP> Table 2: Test set results on text segmentation and discourse segmentation for baselines and our models. Where possible, we estimate standard deviations by bootstrapping the test set 100 times.
<R> <C> Architecture <C> Parameters <C> F1 <R> <C> L4-H256-A4 <C> 11M <C> 63.0 <R> <C> L6-H128-A8 <C> 5M <C> 62.5 <CAP> Table 4: Distillation results on the Wiki-727K dataset.
<R> <C> [EMPTY] <C> Split Method <C> D [ITALIC] A Atom Divergence <C> D [ITALIC] C Compound Divergence <C> Output Pattern Coverage <C> Input Pattern Coverage <C> Output Length Ratio <C> Input Length Ratio <R> <C> CFQ <C> Random <C> 0.000 <C> 0.000 <C> 0.726 <C> 0.705 <C> 1.007 <C> 1.003 <R> <C> CFQ <C> Output Length <C> 0.033 <C> 0.176 <C> 0.000 <C> 0.004 <C> 0.486 <C> 0.648 <R> <C> CFQ <C> Input Length <C> 0.047 <C> 0.062 <C> 0.285 <C> 0.047 <C> 0.584 <C> 0.578 <R> <C> CFQ <C> Output Pattern <C> 0.000 <C> 0.008 <C> 0.000 <C> 0.516 <C> 0.977 <C> 0.984 <R> <C> CFQ <C> Input Pattern <C> 0.000 <C> 0.005 <C> 0.636 <C> 0.000 <C> 1.028 <C> 1.017 <R> <C> CFQ <C> [BOLD] MCD1 <C> 0.020 <C> [BOLD] 0.694 <C> 0.079 <C> 0.032 <C> 0.732 <C> 0.871 <R> <C> [EMPTY] <C> [BOLD] MCD2 <C> 0.020 <C> [BOLD] 0.713 <C> 0.023 <C> 0.007 <C> 0.838 <C> 0.958 <R> <C> [EMPTY] <C> [BOLD] MCD3 <C> 0.020 <C> [BOLD] 0.704 <C> 0.034 <C> 0.027 <C> 0.807 <C> 0.896 <R> <C> SCAN <C> Random <C> 0.000 <C> 0.047 <C> 1.000 <C> 1.000 <C> 0.998 <C> 0.994 <R> <C> SCAN <C> Output Length <C> 0.034 <C> 0.437 <C> 0.000 <C> 1.000 <C> 0.367 <C> 0.856 <R> <C> SCAN <C> Input Length <C> 0.106 <C> 0.380 <C> 0.278 <C> 0.000 <C> 0.501 <C> 0.771 <R> <C> SCAN <C> Output Pattern <C> 0.003 <C> 0.221 <C> 0.000 <C> 0.967 <C> 1.081 <C> 0.989 <R> <C> SCAN <C> Input Pattern <C> 0.005 <C> 0.240 <C> 0.951 <C> 0.000 <C> 0.993 <C> 0.967 <R> <C> SCAN <C> [BOLD] MCD1 <C> 0.015 <C> [BOLD] 0.736 <C> 0.260 <C> 0.357 <C> 0.698 <C> 0.926 <R> <C> [EMPTY] <C> [BOLD] MCD2 <C> 0.020 <C> [BOLD] 0.734 <C> 0.259 <C> 0.010 <C> 0.757 <C> 0.837 <R> <C> [EMPTY] <C> [BOLD] MCD3 <C> 0.014 <C> [BOLD] 0.735 <C> 0.318 <C> 0.009 <C> 0.632 <C> 0.938 <CAP> Table 3: Comparison of relevant measurements for different split methods on CFQ / scan.
<R> <C> Subquery <C> Count <R> <C> What…? <C> 23,695 <R> <C> …sibling of Mx … <C> 2,331 <R> <C> …Mx’s parent … <C> 1,222 <R> <C> What [entity] was…? <C> 1,066 <R> <C> What sibling …? <C> 0 <R> <C> …[DetNP]’s [NP]… <C> 51,600 <R> <C> …[NP] of [NP]… <C> 20,038 <R> <C> What [NP] was [DetNP]? <C> 416 <R> <C> What [NP] was [DetNP]’s [NP]? <C> 0 <CAP> Table 8: Subqueries of “What sibling of M0 was M1’ s parent?” and their occurrences in training.
<R> <C> Model <C> Dev2010 <C> Tst2010 <C> Tst2011 <R> <C> Base <C> 11.26 <C> 13.10 <C> 15.05 <R> <C> Word <C> 11.92 <C> 12.93 <C> 14.76 <R> <C> Sum <C> 11.86 <C> 12.77 <C> 14.80 <R> <C> Sum+S. <C> 12.02 <C> 12.54 <C> 14.76 <R> <C> Max <C> 11.61 <C> 12.99 <C> 15.34 <R> <C> Max+S. <C> 11.56 <C> 13.55 <C> 15.27 <CAP> Table 4: Translation performance on N-best list using different LMs in BLEU[%].
<R> <C> [BOLD] Vague Term may <C> [BOLD] Freq. 1,575 <C> [BOLD] Vague Term other information <C> [BOLD] Freq. 30 <R> <C> personal information <C> 465 <C> non-personal info. <C> 30 <R> <C> information <C> 302 <C> sometimes <C> 27 <R> <C> other <C> 261 <C> reasonably <C> 26 <R> <C> some <C> 214 <C> appropriate <C> 25 <R> <C> certain <C> 205 <C> necessary <C> 24 <R> <C> third parties <C> 183 <C> certain information <C> 23 <R> <C> third party <C> 134 <C> typically <C> 22 <R> <C> personally iden. info. <C> 88 <C> affiliates <C> 21 <R> <C> time to time <C> 75 <C> reasonable <C> 20 <R> <C> most <C> 54 <C> non-personal <C> 19 <R> <C> generally <C> 52 <C> personally iden. <C> 18 <R> <C> personal data <C> 52 <C> such as <C> 18 <R> <C> third-party <C> 49 <C> usually <C> 17 <R> <C> others <C> 41 <C> personal <C> 16 <R> <C> general <C> 39 <C> may be <C> 15 <R> <C> many <C> 37 <C> content <C> 14 <R> <C> various <C> 36 <C> otherwise <C> 14 <R> <C> might <C> 35 <C> periodically <C> 14 <R> <C> services <C> 33 <C> similar <C> 14 <CAP> Table 2: The most frequent vague terms identified by human annotators and their frequencies in our corpus. “iden.” and “info.” are shorthand for “identifiable” and “information.”
<R> <C> [BOLD] Model <C> [BOLD] IWSLT14 <C> [BOLD] PTB <R> <C> Transformer \textsc  [ITALIC] BN <C> 34.4 <C> 60.7 <R> <C> Transformer \textsc  [ITALIC] BRN <C> 34.7 <C> 58.3 <R> <C> Transformer \textsc  [ITALIC] MABN <C> 34.9 <C> 57.2 <R> <C> Transformer \textsc  [ITALIC] LN <C> 35.5 <C> 53.2 <R> <C> Transformer \textsc  [ITALIC] GN <C> 35.7 <C> 51.7 <R> <C> Transformer \textsc  [ITALIC] PN− [ITALIC] V <C> 35.5 <C> 55.3 <R> <C> Transformer \textsc  [ITALIC] PN <C> [BOLD] 35.9 <C> [BOLD] 47.6 <CAP> Table 4: (Left) NMT performance (BLEU) on IWSLT14 De-En. (Right) LM performance (Test PPL) on PTB.
<R> <C> [BOLD] Optimizer <C> [BOLD] Perplexity  [BOLD] 5 <C> [BOLD] Perplexity  [BOLD] 10 <C> [BOLD] Perplexity  [BOLD] 15 <C> [BOLD] Perplexity  [BOLD] 20 <C> [BOLD] BLEU  [BOLD] 5 <C> [BOLD] BLEU  [BOLD] 10 <C> [BOLD] BLEU  [BOLD] 15 <C> [BOLD] BLEU  [BOLD] 20 <C> [BOLD] Time per epoch <R> <C> SGD with decay <C> 2.37 <C> [BOLD] 2.15 <C> [BOLD] 2.06 <C> [BOLD] 2.06 <C> 43.74 <C> 45.10 <C> [BOLD] 45.84 <C> [BOLD] 46.58 <C> 6h 11m <R> <C> Adam <C> [BOLD] 2.26 <C> 2.16 <C> 2.18 <C> 2.24 <C> [BOLD] 44.89 <C> [BOLD] 45.33 <C> 45.21 <C> 44.78 <C> +40m <R> <C> Adagrad <C> 38.75 <C> 19.82 <C> 15.21 <C> 12.55 <C> 1.4 <C> 2.25 <C> 2.56 <C> 3.14 <C> +14m <R> <C> Adadelta <C> 2.62 <C> 2.42 <C> 2.36 <C> 2.32 <C> 42.43 <C> 43.42 <C> 44.35 <C> 44.07 <C> +54m <CAP> Table 1: Performance of different optimizers on training English-German translation model reported every 5 epochs. Each experiment was conducted in a single NVIDIA Tesla K80 GPU.
<R> <C> Method <C> Primitive <C> All <R> <C> Human <C> 82.8 <C> 84.3 <R> <C> Baseline <C> ≤ 3.1 <C> 2.5 <R> <C> Proposed <C> [BOLD] 95.0 ± [BOLD]  6.8 <C> 76.0 ± 5.5 <CAP> Table 5: Test accuracy (mean ± std %) for few-shot learning task. The baseline methods are from Lake et al. (2019). The proposed method achieves performance superior to humans in Primitive task.
<R> <C> [BOLD] System <C> [BOLD] KW lang and template types <C> [BOLD] Metrics MAP <C> [BOLD] Metrics P@5 <C> [BOLD] Metrics P@N <R> <C> (a) CN PPP + S-DTW <C> CN <C> 0.795 <C> 0.820 <C> 0.464 <R> <C> (a) CN PPP + S-DTW <C> EN (L1) <C> 0.046 <C> 0.053 <C> 0.036 <R> <C> (a) CN PPP + S-DTW <C> EN (L2) <C> 0.092 <C> 0.130 <C> 0.077 <R> <C> (b) EN (L1) PPP + S-DTW <C> CN <C> 0.069 <C> 0.113 <C> 0.061 <R> <C> (b) EN (L1) PPP + S-DTW <C> EN (L1) <C> 0.307 <C> 0.369 <C> 0.223 <R> <C> (b) EN (L1) PPP + S-DTW <C> EN (L2) <C> 0.284 <C> 0.315 <C> 0.212 <R> <C> (c) EN (L1,L2) PPP + S-DTW <C> CN <C> 0.144 <C> 0.206 <C> 0.113 <R> <C> (c) EN (L1,L2) PPP + S-DTW <C> EN (L1) <C> 0.418 <C> 0.407 <C> 0.266 <R> <C> (c) EN (L1,L2) PPP + S-DTW <C> EN (L2) <C> 0.747 <C> 0.726 <C> 0.460 <R> <C> (d) EN (L1,L2),CN PPP + S-DTW <C> CN <C> 0.691 <C> 0.739 <C> 0.423 <R> <C> (d) EN (L1,L2),CN PPP + S-DTW <C> EN (L1) <C> 0.227 <C> 0.284 <C> 0.180 <R> <C> (d) EN (L1,L2),CN PPP + S-DTW <C> EN (L2) <C> 0.565 <C> 0.642 <C> 0.396 <R> <C> (e) Block Softmax AWE without V-I loss <C> CN <C> 0.725 <C> 0.742 <C> 0.426 <R> <C> (e) Block Softmax AWE without V-I loss <C> EN (L1) <C> 0.556 <C> 0.588 <C> 0.377 <R> <C> (e) Block Softmax AWE without V-I loss <C> EN (L2) <C> 0.757 <C> 0.777 <C> 0.478 <R> <C> (f) One Softmax AWE without V-I loss <C> CN <C> 0.701 <C> 0.746 <C> 0.422 <R> <C> (f) One Softmax AWE without V-I loss <C> EN (L1) <C> 0.570 <C> 0.596 <C> 0.384 <R> <C> (f) One Softmax AWE without V-I loss <C> EN (L2) <C> 0.769 <C> 0.596 <C> 0.490 <R> <C> (g) One Softmax AWE with V-I loss <C> CN <C> 0.702 <C> 0.737 <C> 0.418 <R> <C> (g) One Softmax AWE with V-I loss <C> EN (L1) <C> 0.634 <C> 0.665 <C> 0.414 <R> <C> (g) One Softmax AWE with V-I loss <C> EN (L2) <C> 0.804 <C> 0.838 <C> 0.534 <CAP> Table 2: Performance of PPP + S-DTW, one and block softmax AWE systems without variability-invariant loss and one softmax with variability-invariant loss on code switching dataset
<R> <C> Model ∖ Fine-tuning <C> SubjQA Exact-match <C> SubjQA  [ITALIC] F1 <R> <C> Auxiliary  [BOLD] 1&2 <C> [EMPTY] <C> [EMPTY] <R> <C> BERT [BOLD] QA (hard) <C> 63.25 <C> 65.58 <R> <C> BERT [BOLD] QA (soft) <C> 74.53 <C> 74.67 <R> <C> BERT [BOLD] QA+ [BOLD] Sbj( [BOLD] q, [BOLD] a) (soft) <C> [BOLD] 76.40 <C> 76.40 <CAP> Table 11: Sequential transfer across tasks. BERTQA was fine-tuned sequentially on context-domain classification, subjectivity classification and QA respectively (in this order). For the main task, QA, the model received information either through an oracle, namely hard targets, or the other already converged learners, namely soft targets, about the previous tasks.
<R> <C> Dataset <C> # Tokens <C> # Documents <C> # Authors <C> Avg C/D <C> Avg C/A <R> <C> CORA <C> 17,059 <C> 13,147 <C> 12,111 <C> 3.46 <C> 12.17 <R> <C> Arxiv-Physics <C> 49,807 <C> 27,770 <C> 10,950 <C> 12.70 <C> 67.93 <R> <C> PNAS <C> 39,664 <C> 31,054 <C> 9,862 <C> 1.57 <C> 13.18 <R> <C> Citeseer <C> 21,223 <C> 4,255 <C> 6,384 <C> 1.24 <C> 4.38 <CAP> Table 1: Datasets. From left to right, each column shows the number of word tokens, number of documents, number of authors, average citations per document (Avg C/D), and average citations per author (Avg C/A).
<R> <C> Method <C> Precision <C> Recall <C> F-score <R> <C> Baseline <C> 0.6679 <C> 0.6787 <C> 0.6733 <R> <C> SVM  [ITALIC] Fe1 <C> 0.7584 <C> [BOLD] 0.8933 <C> [BOLD] 0.8204 <R> <C> SVM  [ITALIC] Fe2 <C> [BOLD] 0.7650 <C> 0.8748 <C> 0.8162 <R> <C> SVM  [ITALIC] Fe3 <C> 0.7552 <C> 0.8912 <C> 0.8176 <CAP> Table 7: Precision and recall on the informal word identification task. The baseline system has been setup using the Stratified classifier from scikit-learn: The stratified classifier in scikit-learn generates predictions by respecting the training set’s class distribution. Fe1 = (Frequencies, cosine similarity), Fe2 = Fe1 + (Euclidean distance), Fe3 = All features
<R> <C> Method RVA <C> Semeval  [BOLD] 0.36815 <C> Method RVA <C> Krapivin  [BOLD] 0.32062 <R> <C> TfIdf-ft <C> 0.36114 <C> SingleRank-ab <C> 0.27795 <R> <C> SingleRank-ab <C> 0.33043 <C> TfIdf-ft <C> 0.27668 <R> <C> WARank-ab <C> 0.32797 <C> WARank-ab <C> 0.27436 <R> <C> TopicRank-ab <C> 0.32571 <C> WARank2015-ab <C> 0.27365 <R> <C> WARank2015-ab <C> 0.32553 <C> TopicRank-ab <C> 0.27038 <R> <C> TopicRank-ft <C> 0.32044 <C> TfIdf-ab <C> 0.23196 <R> <C> SingleRank-ft <C> 0.28401 <C> SingleRank-ft <C> 0.23088 <R> <C> WARank2015-ft <C> 0.27799 <C> TopicRank-ft <C> 0.23032 <R> <C> TfIdf-ab <C> 0.26102 <C> WARank2015-ft <C> 0.18934 <R> <C> WARank-ft <C> 0.22005 <C> WARank-ft <C> 0.16869 <CAP> Table 4: Experimental results (F1 measure) for the baseline TfIdf and the methods TopicRank, SingleRank, WARank, and WARank2015 as well as the proposed method RVA. The “-ab” at the end of the methods’ names implies that they are applied only on titles and abstracts, whereas the “-ft” means that keyphrases are extracted from the full-text of the articles. Methods are ordered in descending F1 measure.
<R> <C> Model <C> [BOLD] Overall <C> Multi-hop <C> High-level <C> Compari-son <C> Logical <C> Count <C> Verify <R> <C> [BOLD] Open-Ended Setting <C> [BOLD] Open-Ended Setting <C> [BOLD] Open-Ended Setting <C> [BOLD] Open-Ended Setting <C> [BOLD] Open-Ended Setting <C> [BOLD] Open-Ended Setting <C> [BOLD] Open-Ended Setting <C> [BOLD] Open-Ended Setting <R> <C> Blind GRU <C> 31.58 <C> 30.08 <C> 26.91 <C> 17.40 <C> 30.70 <C> 35.44 <C> 64.30 <R> <C> KVMemNet <C> 13.61 <C> 14.36 <C> 18.11 <C> 0.14 <C> 11.75 <C> 21.82 <C> 51.04 <R> <C> RGCN <C> 34.00 <C> [BOLD] 32.95 <C> [BOLD] 27.22 <C> 28.48 <C> [BOLD] 33.81 <C> 38.07 <C> [BOLD] 64.50 <R> <C> SRN <C> - <C> 10.49 <C> - <C> - <C> - <C> - <C> - <R> <C> Sup. Program <C> 32.42 <C> 26.92 <C> 14.07 <C> 53.09 <C> 32.61 <C> 41.61 <C> 30.73 <R> <C> Weak. Program <C> - <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> Sup. SPARQL <C> [BOLD] 35.15 <C> 31.95 <C> 15.10 <C> [BOLD] 64.64 <C> 30.16 <C> [BOLD] 49.74 <C> 58.15 <R> <C> [BOLD] Multiple-Choice Setting <C> [BOLD] Multiple-Choice Setting <C> [BOLD] Multiple-Choice Setting <C> [BOLD] Multiple-Choice Setting <C> [BOLD] Multiple-Choice Setting <C> [BOLD] Multiple-Choice Setting <C> [BOLD] Multiple-Choice Setting <C> [BOLD] Multiple-Choice Setting <R> <C> Blind GRU <C> 52.00 <C> 49.45 <C> 59.55 <C> 41.91 <C> 52.82 <C> 40.71 <C> 65.06 <R> <C> KVMemNet <C> 38.33 <C> 36.52 <C> 53.70 <C> 18.71 <C> 36.60 <C> 26.34 <C> 57.87 <R> <C> RGCN <C> 54.03 <C> 52.43 <C> 60.82 <C> 46.25 <C> 57.67 <C> 50.53 <C> 66.11 <R> <C> [BOLD] Human Performance <C> [BOLD] Human Performance <C> [BOLD] Human Performance <C> [BOLD] Human Performance <C> [BOLD] Human Performance <C> [BOLD] Human Performance <C> [BOLD] Human Performance <C> [BOLD] Human Performance <R> <C> Human <C> 97.50 <C> 97.24 <C> 95.65 <C> 100.00 <C> 98.18 <C> 83.33 <C> 95.24 <CAP> Table 3: Accuracy of different models on KQA Pro test set. We categorize the test questions based on their golden program to measure fine-grained ability of models. Specifically, Multi-hop means multi-hop questions, High-level means questions containing high-level knowledge, Comparison means quantitative or temporal comparison between two or more entities, Logical means logical union or intersection, Count means questions that ask the number of target entities, and Verify means questions that take “yes” or “no” as answer.
<R> <C> Task <C> Dataset <C> Model <C> Embeddings <C> mean <C> std <C> min <C> max <R> <C> NER <C> CoNLL <C> biLSTM-CRF <C> 6B <C> 91.12 <C> 0.21 <C> 90.62 <C> 91.37 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> Senna <C> 90.48 <C> 0.27 <C> 90.02 <C> 90.81 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 6B, Senna <C> [BOLD] 91.47 <C> 0.25 <C> 91.15 <C> [BOLD] 92.00 <R> <C> [EMPTY] <C> WNUT-17 <C> biLSTM-CRF <C> 27B <C> 39.20 <C> 0.71 <C> 37.98 <C> 40.33 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 27B, w2v-30M <C> 39.52 <C> 0.83 <C> 38.09 <C> 40.39 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 27B, w2v-30M, 840B <C> [BOLD] 40.33 <C> 1.13 <C> 38.38 <C> [BOLD] 41.99 <R> <C> [EMPTY] <C> Ontonotes <C> biLSTM-CRF <C> 6B <C> [BOLD] 87.43 <C> 0.13 <C> 87.15 <C> 87.57 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 6B, Senna <C> 87.41 <C> 0.17 <C> 87.14 <C> [BOLD] 87.74 <R> <C> Slot Filling <C> Snips <C> biLSTM-CRF <C> 6B <C> 95.84 <C> 0.29 <C> 95.39 <C> 96.21 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> GN <C> 95.28 <C> 0.41 <C> 94.51 <C> 95.81 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 6B, GN <C> [BOLD] 96.04 <C> 0.28 <C> 95.39 <C> [BOLD] 96.21 <R> <C> POS <C> TW-POS <C> biLSTM-CRF <C> w2v-30M <C> 89.21 <C> 0.28 <C> 88.72 <C> 89.74 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 27B <C> 89.63 <C> 0.19 <C> 89.35 <C> 89.92 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 27B, w2v-30M <C> 90.35 <C> 0.20 <C> 89.99 <C> 90.60 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 27B, w2v-30M, 840B <C> [BOLD] 90.75 <C> 0.14 <C> 90.53 <C> [BOLD] 91.02 <R> <C> Classification <C> SST2 <C> LSTM <C> 840B <C> 88.39 <C> 0.45 <C> 87.42 <C> 89.07 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> GN <C> 87.58 <C> 0.54 <C> 86.16 <C> 88.19 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 840B, GN <C> [BOLD] 88.57 <C> 0.44 <C> 87.59 <C> [BOLD] 89.24 <R> <C> [EMPTY] <C> AG-NEWS <C> LSTM <C> 840B <C> 92.53 <C> 0.45 <C> 87.42 <C> 89.07 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> GN <C> 92.20 <C> 0.18 <C> 91.80 <C> 92.40 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 840B, GN <C> [BOLD] 92.60 <C> 0.20 <C> 92.30 <C> [BOLD] 92.86 <R> <C> [EMPTY] <C> Snips <C> Conv <C> 840B <C> 97.47 <C> 0.33 <C> 97.01 <C> 97.86 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> GN <C> 97.40 <C> 0.27 <C> 97.00 <C> 97.86 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 840B, GN <C> [BOLD] 97.63 <C> 0.52 <C> 97.00 <C> [BOLD] 98.29 <CAP> Table 1: Results using multiple embeddings applied to several tasks and datasets. NER and Slot Filling tasks report entity level F1. POS tagging and Classification report accuracy. All results are reported across 10 runs.
<R> <C> Dataset <C> Model <C> mean <C> std <C> max <R> <C> CoNLL <C> CRF <C> [BOLD] 91.47 <C> 0.25 <C> [BOLD] 92.00 <R> <C> [EMPTY] <C> Constrain <C> 91.44 <C> 0.23 <C> 91.90 <R> <C> WNUT-17 <C> CRF <C> 40.33 <C> 1.13 <C> [BOLD] 41.99 <R> <C> [EMPTY] <C> Constrain <C> [BOLD] 40.59 <C> 1.06 <C> 41.71 <R> <C> Snips <C> CRF <C> 96.04 <C> 0.28 <C> [BOLD] 96.35 <R> <C> [EMPTY] <C> Constrain <C> [BOLD] 96.07 <C> 0.17 <C> 96.29 <R> <C> Ontonotes <C> CRF <C> [BOLD] 87.43 <C> 0.26 <C> [BOLD] 87.57 <R> <C> [EMPTY] <C> Constrain <C> 86.13 <C> 0.17 <C> 86.72 <CAP> Table 2: Results of tagging with constraints vs a CRF. For each, we use the best embedding combination as found in table 1. Scores are reported across 10 runs.
<R> <C> [EMPTY] <C> Model <C> Development <C> Test <R> <C> All <C> RNNenc <C> 13.15 <C> 13.92 <R> <C> All <C> grConv <C> 9.97 <C> 9.97 <R> <C> All <C> Moses <C> 30.64 <C> 33.30 <R> <C> All <C> Moses+RNNenc⋆ <C> 31.48 <C> 34.64 <R> <C> All <C> Moses+LSTM∘ <C> 32 <C> 35.65 <R> <C> No UNK <C> RNNenc <C> 21.01 <C> 23.45 <R> <C> No UNK <C> grConv <C> 17.19 <C> 18.22 <R> <C> No UNK <C> Moses <C> 32.77 <C> 35.63 <CAP> Table 1: BLEU scores computed on the development and test sets. The top three rows show the scores on all the sentences, and the bottom three rows on the sentences having no unknown words. (⋆) The result reported in [Cho et al.2014] where the RNNenc was used to score phrase pairs in the phrase table. (∘) The result reported in [Sutskever et al.2014] where an encoder–decoder with LSTM units was used to re-rank the n-best list generated by Moses.
<R> <C> [EMPTY] <C> Model <C> Development <C> Test <R> <C> All <C> RNNenc <C> 19.12 <C> 20.99 <R> <C> All <C> grConv <C> 16.60 <C> 17.50 <R> <C> All <C> Moses <C> 28.92 <C> 32.00 <R> <C> No UNK <C> RNNenc <C> 24.73 <C> 27.03 <R> <C> No UNK <C> grConv <C> 21.74 <C> 22.94 <R> <C> No UNK <C> Moses <C> 32.20 <C> 35.40 <CAP> Table 1: BLEU scores computed on the development and test sets. The top three rows show the scores on all the sentences, and the bottom three rows on the sentences having no unknown words. (⋆) The result reported in [Cho et al.2014] where the RNNenc was used to score phrase pairs in the phrase table. (∘) The result reported in [Sutskever et al.2014] where an encoder–decoder with LSTM units was used to re-rank the n-best list generated by Moses.
<R> <C> [EMPTY] <C> [BOLD] Company Reports <C> [BOLD] Investor Reports <C> [BOLD] News <C> [BOLD] Stocktwits <C> [BOLD] Total <R> <C> BMWYY <C> 5 <C> 4 <C> 9 <C> 55 <C> 73 <R> <C> DDAIF <C> 16 <C> 8 <C> 24 <C> 7 <C> 55 <R> <C> F <C> 32 <C> 32 <C> 96 <C> 700 <C> 860 <R> <C> FCAU <C> 1 <C> 10 <C> 31 <C> 700 <C> 742 <R> <C> GM <C> 5 <C> 16 <C> 32 <C> 700 <C> 753 <R> <C> HMC <C> 1 <C> 0 <C> 34 <C> 234 <C> 269 <R> <C> HYMLF <C> 0 <C> 4 <C> 38 <C> 0 <C> 42 <R> <C> NAV <C> 9 <C> 1 <C> 0 <C> 200 <C> 210 <R> <C> NSANY <C> 10 <C> 2 <C> 25 <C> 36 <C> 73 <R> <C> PEUGF <C> 0 <C> 1 <C> 2 <C> 0 <C> 3 <R> <C> RNSDF <C> 0 <C> 0 <C> 1 <C> 0 <C> 1 <R> <C> TM <C> 7 <C> 3 <C> 48 <C> 345 <C> 403 <R> <C> VLKAY <C> 41 <C> 5 <C> 54 <C> 227 <C> 327 <R> <C> Total <C> 127 <C> 86 <C> 394 <C> 3204 <C> 3811 <CAP> Table 3: Total number of occurrences per entity and source.
<R> <C> [EMPTY] <C> [BOLD] Company Reports <C> [BOLD] Investor Reports <C> [BOLD] News <C> [BOLD] Stocktwits <C> [BOLD] Overall <R> <C> Relevance IAA <C> 0.3298 <C> 0.6477 <C> 0.4288 <C> 0.1219 <C> 0.1807 <R> <C> Sentiment IAA <C> 0.4202 <C> 0.4663 <C> 0.3719 <C> 0.5953 <C> 0.5610 <CAP> Table 4: Inter-annotator agreement (IAA) for each data source, reported in Fleiss’ Kappa.
<R> <C> [EMPTY] <C> [BOLD] Company Reports <C> [BOLD] Investor Reports <C> [BOLD] News <C> [BOLD] Stocktwits <R> <C> Relevance Threshold <C> 0.2 <C> 0.18 <C> 0.13 <C> 0.31 <R> <C> Sentiment Threshold <C> 0.31 <C> 0.3 <C> 0.21 <C> 0.26 <R> <C> Automatically Consolidated <C> 51 <C> 37 <C> 111 <C> 1,458 <R> <C> Manually Consolidated <C> 76 (59.84%) <C> 49 (56.98%) <C> 286 (72.59%) <C> 1,746 (54.49%) <CAP> Table 5: Thresholds implemented for consolidation, when the value to the mean exceeds the stipulated threshold manual consolidation is used, otherwise the annotations are automatically consolidated. The thresholds were chosen based on the 3rd quantil distribution for the relevance and sentiment annotations.
<R> <C> System <C> All Words <C> Noun Sample <R> <C> Random <C> 67.6 <C> 26.0 <R> <C> UMFS-WE <C> 73.9 <C> 48.0 <R> <C> WCT-VEC <C> 75.2 <C> 48.8 <R> <C> Comp2Sense <C> [BOLD] 77.9 <C> [BOLD] 58.5 <R> <C> PN18 (EnDi) <C> 71.4 <C> 47.4 <R> <C> MKWC04 <C> [EMPTY] <C> 54 <CAP> TABLE I: Intrinsic evaluation results on the MFS detection task on SemCor (in % accuracy).
<R> <C> System <C> SE2 <C> SE3 <C> S07 <C> S13 <C> S15 <C> ALL <R> <C> UMFS-WE <C> 54.8 <C> 52.0 <C> 38.2 <C> [BOLD] 55.2 <C> 54.5 <C> 53.1 <R> <C> WCT-VEC <C> [BOLD] 56.4 <C> [BOLD] 53.8 <C> [BOLD] 40.6 <C> 54.9 <C> 54.0 <C> [BOLD] 54.1 <R> <C> Comp2Sense <C> 51.5 <C> 47.0 <C> 37.5 <C> 54.1 <C> [BOLD] 55.0 <C> 50.7 <R> <C> Supervised MFS <C> 65.6 <C> 66.0 <C> 54.5 <C> 63.8 <C> 67.1 <C> 65.5 <R> <C> Leskext <C> 50.6 <C> 44.5 <C> 32.0 <C> 53.6 <C> 51.0 <C> [EMPTY] <CAP> TABLE II: Extrinsic evaluation of the MFS detection systems on the WSD task (in % F1-score).
<R> <C> [EMPTY] <C> Intrinsic <C> Extrinsic <R> <C> WCT-VEC Variant <C> (SemCor) <C> (SE2) <R> <C> Full system <C> [BOLD] 75.2 <C> [BOLD] 56.4 <R> <C> Word vector <C> 74.5 <C> 55.2 <R> <C> Companions vector <C> 67.4 <C> 53.2 <R> <C> MFT vector <C> 71.9 <C> 49.8 <R> <C> Knowledge-light <C> 71.9 <C> 52.7 <CAP> TABLE III: Ablation results for the intrinsic (in % accuracy) and extrinsic experiments (in % F1-score).
<R> <C> [BOLD] technique <C> [BOLD] type <C> [BOLD] detection results Precision <C> [BOLD] detection results Recall <C> [BOLD] detection results Specificity <C> [BOLD] detection results Accuracy <C> [BOLD] detection results F-Measure <C> [BOLD] detection results MCC <R> <C> test set #1 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Human annotators <C> manual <C> 0.267 <C> 0.080 <C> 0.921 <C> 0.698 <C> 0.123 <C> 0.001 <R> <C> BotOrNot? <C> supervised <C> 0.471 <C> 0.208 <C> 0.918 <C> 0.734 <C> 0.288 <C> 0.174 <R> <C> C. Yang  [ITALIC] et al. <C> supervised <C> 0.563 <C> 0.170 <C> 0.860 <C> 0.506 <C> 0.261 <C> 0.043 <R> <C> ours <C> supervised <C> 0.940 <C> [BOLD] 0.976 <C> 0.935 <C> 0.961 <C> 0.963 <C> 0.920 <R> <C> Miller  [ITALIC] et al. <C> unsupervised <C> 0.555 <C> 0.358 <C> 0.698 <C> 0.526 <C> 0.435 <C> 0.059 <R> <C> Ahmed  [ITALIC] et al. <C> unsupervised <C> 0.945 <C> 0.944 <C> 0.945 <C> 0.943 <C> 0.944 <C> 0.886 <R> <C> Cresci  [ITALIC] et al. <C> unsupervised <C> [BOLD] 0.982 <C> 0.972 <C> [BOLD] 0.981 <C> [BOLD] 0.976 <C> [BOLD] 0.977 <C> [BOLD] 0.952 <R> <C> test set #2 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Human annotators <C> manual <C> 0.647 <C> 0.509 <C> 0.921 <C> 0.829 <C> 0.570 <C> 0.470 <R> <C> BotOrNot? <C> supervised <C> 0.635 <C> [BOLD] 0.950 <C> 0.981 <C> 0.922 <C> 0.761 <C> 0.738 <R> <C> C. Yang  [ITALIC] et al. <C> supervised <C> 0.727 <C> 0.409 <C> 0.848 <C> 0.629 <C> 0.524 <C> 0.287 <R> <C> ours <C> supervised <C> 0.933 <C> 0.919 <C> 0.938 <C> [BOLD] 0.929 <C> [BOLD] 0.926 <C> 0.857 <R> <C> Miller  [ITALIC] et al. <C> unsupervised <C> 0.467 <C> 0.306 <C> 0.654 <C> 0.481 <C> 0.370 <C> -0.043 <R> <C> Ahmed  [ITALIC] et al. <C> unsupervised <C> 0.913 <C> 0.935 <C> 0.912 <C> 0.923 <C> 0.923 <C> 0.847 <R> <C> Cresci  [ITALIC] et al. <C> unsupervised <C> [BOLD] 1.000 <C> 0.858 <C> [BOLD] 1.000 <C> [BOLD] 0.929 <C> 0.923 <C> [BOLD] 0.867 <CAP> TABLE II: Performance comparison among various spambot detection techniques and algorithms reported on the cresci-2017 dataset.
<R> <C> [BOLD] Model <C> [BOLD] T2C (sec) <C> [BOLD] Dev Loss <R> <C> LSTM <C> 1100 <C> 0.89 <R> <C> LSTM+Att <C> 1305 <C> 0.62 <R> <C> Bi-LSTM <C> 1865 <C> 0.60 <R> <C> Bi-LSTM+Att <C> 2120 <C> 0.49 <R> <C> Transformer <C> [BOLD] 612 <C> [BOLD] 0.31 <R> <C> UT <C> 1939 <C> 0.36 <R> <C> UT+ACT <C> 665 <C> 0.33 <CAP> Table 4: Comparison of convergence performance of the models
<R> <C> [BOLD] Imposter Cohort <C> [BOLD] EER(%) <C> [BOLD] MinDCF <R> <C> no s-normalization <C> 2.14 <C> 0.0947 <R> <C> VoxCeleb <C> 2.46 <C> 0.1303 <R> <C> Farsi <C> [BOLD] 1.69 <C> [BOLD] 0.0759 <R> <C> VoxCeleb + Farsi <C> 1.72 <C> 0.0762 <CAP> Table 3: Effects of cohort selection in s-normalization.
<R> <C> Model <C> heldout <C> Test <R> <C> First-pass (no reranker) <C> 11.52 <C> 12.52 <R> <C> Ngram LM <C> 11.17 <C> 11.56 <R> <C> LSTM LM <C> 10.16 <C> 11.56 <R> <C> Reranker <C> 10.03 <C> 11.35 <R> <C> Reranker + LSTM <C> [BOLD] 9.82 <C> [BOLD] 11.12 <R> <C> Oracle <C> 6.55 <C> 7.32 <CAP> Table 1: Experimental results based on word error rate (WER) on the heldout and test data. First-pass shows the model accuracy by just applying the first-pass language model from the speech recognition system. The other rows show the effect of interpolating it with different types of language models or reranker. Oracle shows the WER when we pick the sentence in the n-best list with the lowest WER with respect to the gold-standard output.
<R> <C> [BOLD] Model <C> [BOLD] F1 <C> [BOLD] Inference Time (samples/s) <R> <C> CONV MODEL <C> 79.80 <C> 774 <R> <C> CONV MODEL (with only one CNN with 7 kernel size) <C> 78.47 <C> 829 <R> <C> CONV MODEL (no knowledge distillation) <C> 72.33 <C> 774 <R> <C> BERTSMALL <C> 85.57 <C> 217 <R> <C> FABIR <C> 77.6 <C> 672 <R> <C> QANet <C> 82.70 <C> 163 <R> <C> BiDAF <C> 77.30 <C> 60 <CAP> Table 5.1: F1 and Inference speed of different models
<R> <C> [ITALIC] Model <C> [ITALIC] Dev <C> [ITALIC] Test <C> [ITALIC] Easy <C> [ITALIC] Hard <C> [ITALIC] Adv <R> <C> ViLBERT (VB) <C> 83.39 <C> 83.63 <C> 85.93 <C> 72.00 <C> 70.90 <R> <C> VB+ [ITALIC] Sum-H <C> 81.61 <C> 83.00 <C> 85.93 <C> 70.60 <C> 72.30 <R> <C> VB+ [ITALIC] Max-H <C> 82.93 <C> 82.70 <C> [BOLD] 86.58 <C> 70.46 <C> 73.35 <R> <C> VB+ [ITALIC] MTL (GQA) <C> [BOLD] 83.45 <C> [BOLD] 84.30 <C> 86.23 <C> [BOLD] 73.79 <C> [BOLD] 73.92 <CAP> Table 3: Accuracy of enhanced ViLBERT models.
<R> <C> Model <C> Original <C> Shuf <C> N+J <R> <C> CMN Hu et al. ( 2017 ) <C> 69.4 <C> 66.4 <C> 67.4 <R> <C> GroundNet Cirik et al. ( 2018a ) <C> 65.8 <C> 57.6 <C> 62.8 <R> <C> MattNet Yu et al. ( 2018 ) <C> 78.5 <C> 75.3 <C> 76.1 <R> <C> ViLBERT Lu et al. ( 2019 ) <C> 83.6 <C> 71.4 <C> 73.6 <CAP> Table 5: RefCOCOg test accuracies of SOTA models on (a) original undistorted split, (b) after randomly shuffling words (Shuf) in the referring expression, and (c) after deleting all the words except for nouns and adjectives (N+J). ViLBERT is relatively more robust than other baselines.
<R> <C> Model DrQA (Chen et al.,  2017 ) <C> NaturalQuestions - <C> TriviaQA - <C> TriviaQA - <C> SQuAD Open 29.8 <R> <C> Multi-Passage BERT (Wang et al.,  2019 ) <C> - <C> - <C> - <C> 53.0 <R> <C> Path Retriever (Asai et al.,  2020 ) <C> 31.7 <C> - <C> - <C> [BOLD] 56.5 <R> <C> Graph Retriever (Min et al.,  2019b ) <C> 34.7 <C> 55.8 <C> - <C> - <R> <C> Hard EM (Min et al.,  2019a ) <C> 28.8 <C> 50.9 <C> - <C> - <R> <C> ORQA (Lee et al.,  2019 ) <C> 31.3 <C> 45.1 <C> - <C> 20.2 <R> <C> REALM (Guu et al.,  2020 ) <C> 38.2 <C> - <C> - <C> [EMPTY] <R> <C> DPR (Karpukhin et al.,  2020 ) <C> 41.5 <C> 57.9 <C> - <C> 36.7 <R> <C> SpanSeqGen (Min et al.,  2020 ) <C> 42.5 <C> - <C> - <C> - <R> <C> RAG (Lewis et al.,  2020 ) <C> 44.5 <C> 56.1 <C> 68.0 <C> - <R> <C> T5 (Roberts et al.,  2020 ) <C> 36.6 <C> - <C> 60.5 <C> - <R> <C> GPT-3 few shot (Brown et al.,  2020 ) <C> 29.9 <C> - <C> 71.2 <C> - <R> <C> Fusion-in-Decoder (base) <C> 48.2 <C> 65.0 <C> 77.1 <C> 53.4 <R> <C> Fusion-in-Decoder (large) <C> [BOLD] 51.4 <C> [BOLD] 67.6 <C> [BOLD] 80.1 <C> [BOLD] 56.7 <CAP> Table 1: Comparison to state-of-the-art. On TriviaQA, we report results on the open domain test set (left), and on the hidden test set (right, competitions.codalab.org/competitions/17208#results).
<R> <C> [EMPTY] <C> P <C> R <C> F1 <C> illformed <R> <C> seq2seq + copy <C> 60.29 <C> 74.09 <C> 66.48 <C> 10.60% <R> <C> seq2graph <C> [BOLD] 70.69 <C> [BOLD] 74.46 <C> [BOLD] 72.53 <C> 0.10% <R> <C> +restrict <C> 70.50 <C> 74.64 <C> 72.51 <C> 0.70% <R> <C> [ITALIC] +feats <C> [BOLD] 72.51 <C> [BOLD] 76.44 <C> [BOLD] 74.42 <C> 0.60% <R> <C> -lemmas <C> 71.53 <C> 76.28 <C> 73.83 <C> 0.32% <R> <C> -pos <C> 72.11 <C> 75.99 <C> 74.00 <C> 0.35% <R> <C> -semtag <C> 70.21 <C> 74.45 <C> 72.27 <C> 0.53% <R> <C> -words <C> 70.45 <C> 74.59 <C> 72.46 <C> 0.64% <R> <C> -dep <C> 72.16 <C> 76.40 <C> 74.22 <C> 0.50% <CAP> Table 4: Model performance (Precision, Recall, F1) on PMB (v2.2.0, development set).
<R> <C> [EMPTY] <C> [BOLD] Bert2Bert  [ITALIC]  [BOLD] ppl. <C> [BOLD] Bert2Bert  [ITALIC]  [BOLD] BLEU <C> [BOLD] M-Bert2Bert  [ITALIC]  [BOLD] ppl. <C> [BOLD] M-Bert2Bert  [ITALIC]  [BOLD] BLEU <C> [BOLD] CausalBert  [ITALIC]  [BOLD] ppl. <C> [BOLD] CausalBert  [ITALIC]  [BOLD] BLEU <C> [BOLD] M-CausalBert  [ITALIC]  [BOLD] ppl. <C> [BOLD] M-CausalBert  [ITALIC]  [BOLD] BLEU <C> [BOLD] XNLG  [ITALIC]  [BOLD] ppl. <C> [BOLD] XNLG  [ITALIC]  [BOLD] BLEU <R> <C> [ITALIC]  [BOLD] En <C> 21.99 <C> 1.53 <C> 25.99 <C> 0.57 <C> 16.08 <C> 1.79 <C> [BOLD] 15.62 <C> 1.97 <C> 54.74* <C> [BOLD] 2.25* <R> <C> [ITALIC]  [BOLD] Zh <C> 21.35 <C> 3.36 <C> 13.24 <C> 1.25 <C> [BOLD] 8.69 <C> 5.51 <C> 9.27 <C> [BOLD] 5.7 <C> 3482.27 <C> 2.16 <R> <C> [ITALIC]  [BOLD] It <C> 50.36 <C> 0.6 <C> 24.16 <C> 0.31 <C> 18.41 <C> 1.32 <C> [BOLD] 15.12 <C> [BOLD] 1.3 <C> 917.63 <C> 0.41 <R> <C> [ITALIC]  [BOLD] Jp <C> 10.09 <C> 5.23 <C> 10.64 <C> 0.79 <C> 11.00 <C> [BOLD] 6.74 <C> [BOLD] 7.13 <C> 4.53 <C> 999.81 <C> 0.0 <R> <C> [ITALIC]  [BOLD] Ko <C> 12.81 <C> 0.24 <C> 34.31 <C> 0.00 <C> 9.66 <C> 1.06 <C> [BOLD] 9.56 <C> [BOLD] 1.08 <C> - <C> - <R> <C> [ITALIC]  [BOLD] Id <C> 21.37 <C> 0.11 <C> 22.83 <C> 0.22 <C> 14.77 <C> [BOLD] 2.1 <C> [BOLD] 14.61 <C> 1.92 <C> 844.98 <C> 0.15 <R> <C> [ITALIC]  [BOLD] Fr <C> 13.22 <C> 0.35 <C> 15.58 <C> 0.50 <C> [BOLD] 10.39 <C> 1.97 <C> 10.59 <C> [BOLD] 2.17 <C> 640.33 <C> 0.09 <CAP> Table 3: Results of automatic evaluation score on test set in seven languages. We compute the BLEU score and perplexity (ppl.) for monolingual, multilingual, and cross-lingual models.
<R> <C> [BOLD] Multi Wins % <C> [ITALIC]  [BOLD] Lang <C> [ITALIC]  [BOLD] Engageness  [BOLD] Human <C> [ITALIC]  [BOLD] Engageness  [BOLD] Mono <C> [ITALIC]  [BOLD] Engageness  [BOLD] Poly <C> [ITALIC]  [BOLD] Interestingness  [BOLD] Human <C> [ITALIC]  [BOLD] Interestingness  [BOLD] Mono <C> [ITALIC]  [BOLD] Interestingness  [BOLD] Poly <C> [ITALIC]  [BOLD] Humanness  [BOLD] Human <C> [ITALIC]  [BOLD] Humanness  [BOLD] Mono <C> [ITALIC]  [BOLD] Humanness  [BOLD] Poly <R> <C> [BOLD] Multi Wins % <C> [ITALIC]  [BOLD] En <C> [BOLD] 23.33 <C> [BOLD] 68.57 <C> 36.36 <C> [BOLD] 23.33 <C> [BOLD] 64.29 <C> [BOLD] 32.73 <C> [BOLD] 30.00 <C> [BOLD] 62.86 <C> 42.73 <R> <C> [BOLD] Multi Wins % <C> [ITALIC]  [BOLD] Fr <C> 32.00 <C> 55.17 <C> 42.86 <C> [BOLD] 16.00 <C> 53.45 <C> 48.21 <C> [BOLD] 28.00 <C> 50.00 <C> 44.64 <R> <C> [BOLD] Multi Wins % <C> [ITALIC]  [BOLD] Id <C> [BOLD] 21.67 <C> 51.67 <C> [BOLD] 65.45 <C> [BOLD] 23.33 <C> 46.67 <C> 55.45 <C> [BOLD] 25.00 <C> 46.67 <C> [BOLD] 65.45 <R> <C> [BOLD] Multi Wins % <C> [ITALIC]  [BOLD] It <C> [BOLD] 35.00 <C> 48.33 <C> 56.36 <C> [BOLD] 30.00 <C> 48.33 <C> 53.64 <C> [BOLD] 30.00 <C> 40.00 <C> 57.27 <R> <C> [BOLD] Multi Wins % <C> [ITALIC]  [BOLD] Jp <C> [BOLD] 18.33 <C> 50.00 <C> [BOLD] 61.82 <C> [BOLD] 13.33 <C> 43.33 <C> 45.45 <C> [BOLD] 18.33 <C> 51.67 <C> 59.09 <R> <C> [BOLD] Multi Wins % <C> [ITALIC]  [BOLD] Ko <C> [BOLD] 30.00 <C> 52.46 <C> [BOLD] 62.39 <C> [BOLD] 26.67 <C> 50.82 <C> 59.63 <C> [BOLD] 28.33 <C> 52.46 <C> [BOLD] 64.22 <R> <C> [BOLD] Multi Wins % <C> [ITALIC]  [BOLD] Zh <C> [BOLD] 36.67 <C> 55.00 <C> [BOLD] 65.45 <C> [BOLD] 36.67 <C> 60.00 <C> [BOLD] 61.82 <C> [BOLD] 36.67 <C> 55.00 <C> [BOLD] 70.91 <CAP> Table 4: Results of ACUTE-EVAL human evaluation. Tests are conducted pairwise between M-CausalBert (Multi.) and other models (Human, Poly-encoder (Poly), Monolingual CausalBert (Mono)). Numbers indicate the winning rate of Multi. Numbers in bold are statistically significant (p<0.05).
<R> <C> Feature <C> Health AUC(%) <C> Health ACC(%) <C> Music AUC(%) <C> Music ACC(%) <R> <C> BoW <C> 93.3 <C> 97.5 <C> 95.3 <C> 97.7 <R> <C> Dict <C> 86.4 <C> 96.5 <C> 94.1 <C> 97.2 <R> <C> Smoo <C> 95.1 <C> 97.3 <C> 96.7 <C> 97.4 <CAP> Table 2: AUROC and accuracy of the classifiers
<R> <C> [BOLD] Property <C> [BOLD] Frequency <C> [BOLD] Entropy <R> <C> instance of <C> 3,245,856 <C> 0.429 <R> <C> sex or gender <C> 1,143,126 <C> 0.186 <R> <C> country <C> 986,587 <C> 0.518 <R> <C> date of birth <C> 953,481 <C> 0.916 <R> <C> given name <C> 932,147 <C> 0.755 <R> <C> occupation <C> 869,333 <C> 0.588 <R> <C> country of citizenship <C> 819,301 <C> 0.508 <R> <C> located in …entity <C> 582,110 <C> 0.800 <R> <C> place of birth <C> 467,066 <C> 0.795 <R> <C> date of death <C> 442,514 <C> 0.922 <CAP> Table 2: Training set frequency and scaled answer entropy for the 10 most frequent properties.
<R> <C> [BOLD] Team <C> [BOLD] Type <C> [BOLD] BLEU <C> [BOLD] TER <R> <C> UPC-TALP <C> P <C> 7.9 <C> 85.9 <R> <C> [BOLD] UDS-DFKI <C> P <C> [BOLD] 7.6 <C> [BOLD] 87.0 <R> <C> Uhelsinki <C> P <C> 7.1 <C> 87.4 <R> <C> Uhelsinki <C> C <C> 7.0 <C> 87.3 <R> <C> Incomslav <C> C <C> 5.9 <C> 88.4 <R> <C> Uhelsinki <C> C <C> 5.9 <C> 88.4 <R> <C> Incomslav <C> P <C> 3.2 <C> - <R> <C> Incomslav <C> C <C> 3.1 <C> - <R> <C> UBC-NLP <C> C <C> 2.3 <C> - <R> <C> UBC-NLP <C> P <C> 2.2 <C> - <CAP> Table 2: Rank table for Czech to Polish Translation
<R> <C> Methods <C> EER(%) <R> <C> MFCC i-vector <C> 6.63 <R> <C> MFCC-PPP i-vector <C> [BOLD] 1.06 <R> <C> MGDCC-PPP i-vector <C> 2.23 <R> <C> OpenSmile <C> 1.57 <CAP> Table 3: Performance of the four subsystems on the development data
<R> <C> [BOLD] ID <C> [BOLD] Relation <C> [BOLD] Original <C> [BOLD] Pretrain <C> [BOLD] RL <R> <C> 1 <C> /peo/per/pob <C> 55.60 <C> 53.63 <C> [BOLD] 55.74 <R> <C> 2 <C> /peo/per/n <C> 78.85 <C> 80.80 <C> [BOLD] 83.63 <R> <C> 3 <C> /peo/per/pl <C> 86.65 <C> 89.62 <C> [BOLD] 90.76 <R> <C> 4 <C> /loc/loc/c <C> 80.78 <C> 83.79 <C> [BOLD] 85.39 <R> <C> 5 <C> /loc/cou/ad <C> [BOLD] 90.9 <C> 88.1 <C> 89.86 <R> <C> 6 <C> /bus/per/c <C> 81.03 <C> 82.56 <C> [BOLD] 84.22 <R> <C> 7 <C> /loc/cou/c <C> 88.10 <C> 93.78 <C> [BOLD] 95.19 <R> <C> 8 <C> /loc/adm/c <C> 86.51 <C> 85.56 <C> [BOLD] 86.63 <R> <C> 9 <C> /loc/nei/n <C> 96.51 <C> 97.20 <C> [BOLD] 98.23 <R> <C> 10 <C> /peo/dec/p <C> 82.2 <C> 83.0 <C> [BOLD] 84.6 <CAP> Table 2: Comparison of F1 scores among three cases: the relation classifier is trained with the original dataset, the redistributed dataset generated by the pre-trained agent, and the redistributed dataset generated by our RL agent respectively. The name of relation types are abbreviated: /peo/per/pob represents /people/person/place_of_birth
<R> <C> Kappa <C> [ITALIC] Ssys <C> [ITALIC] Susr <R> <C> DSTC-1 <C> 0.359 <C> 0.616 <R> <C> DSTC-2 <C> 0.487 <C> 0.593 <R> <C> Daily Dialog <C> 0.196 <C> 0.287 <R> <C> Switchboard <C> 0.172 <C> 0.348 <CAP> Table 4: Fleiss Kappa among the four raters.
<R> <C> Input <C> Acc <C> MAE <R> <C> [ITALIC] c <C> 47.5% <C> 1.31 <R> <C> [ITALIC] x <C> 55.7% <C> 1.14 <R> <C> [ITALIC] u <C> 60.3% <C> 0.81 <R> <C> [ITALIC] c, [ITALIC] x <C> 54.6% <C> 1.08 <R> <C> [ITALIC] c, [ITALIC] u <C> 62.9% <C> 0.78 <R> <C> [ITALIC] x, [ITALIC] u <C> 65.8% <C> 0.67 <R> <C> [ITALIC] c, [ITALIC] x, [ITALIC] u <C> 65.8% <C> 0.68 <CAP> Table 6: Results for Susr prediction. MAE stands for absolute mean error.
<R> <C> [BOLD] Zero-shot (1-directional) <C> [BOLD] BLEU Score <R> <C> Model-1:(no correlation loss, word2vec, no bidirectional LSTM) <C> 8.2 <R> <C> Model-2:(correlation loss, word2vec, no bidir LSTM) <C> 8.2 <R> <C> Model-3:(correlation loss, word2vec, bidir LSTM) <C> 12.0 <R> <C> Model-4:(Model-3 + basic hindi stemming) <C> 13.3 <CAP> Table 5. BLEU Score comparison
<R> <C> ID <C> Field <C> No. of sentences <C> No. of words <C> Words / sentences <C> Vocab. size (original / stemmed / pruned) <R> <C> 1 <C> Algorithm <C> 5,672 <C> 121,675 <C> 21.45 <C> 3,972 / 2,702 / 1,809 <R> <C> 2 <C> Algorithm <C> 14,902 <C> 294055 <C> 20.87 <C> 6,431 / 4,222 / 2,378 <R> <C> 3 <C> DSP <C> 8,126 <C> 129,665 <C> 15.96 <C> 3,815 / 2,699 / 1,869 <R> <C> 4 <C> Data Mining <C> 7,392 <C> 129,552 <C> 17.53 <C> 4,531 / 3,140 / 2,141 <R> <C> 5 <C> Data Mining <C> 6,906 <C> 129,068 <C> 18.69 <C> 3,008 / 2,041 / 1,475 <R> <C> 6 <C> DSP <C> 20,271 <C> 360,508 <C> 17.78 <C> 8,878 / 5,820 / 2,687 <R> <C> 7 <C> IT <C> 9,103 <C> 164,812 <C> 18.11 <C> 4,369 / 2,749 / 1,979 <R> <C> 8 <C> Mathematics <C> 5,736 <C> 101,012 <C> 17.61 <C> 3,095 / 2,148 / 1,500 <R> <C> 9 <C> Machine Learning <C> 11,090 <C> 224,504 <C> 20.24 <C> 6,293 / 4,071 / 2,259 <R> <C> 10 <C> Programming <C> 8,185 <C> 160,390 <C> 19.60 <C> 4,045 / 2,771 / 1,898 <R> <C> 11 <C> NLP <C> 7,095 <C> 111,154 <C> 15.67 <C> 3,691 / 2,572 / 1,789 <R> <C> 12 <C> NLP <C> 4,395 <C> 100,408 <C> 22.85 <C> 3,973 / 2,605 / 1,789 <R> <C> 13 <C> NLP <C> 4,382 <C> 96,948 <C> 22.12 <C> 4,730 / 3,467 / 2,071 <R> <C> 14 <C> Machine Learning <C> 6,174 <C> 116,344 <C> 18.84 <C> 5,844 / 4,127 / 2,686 <R> <C> 15 <C> Mathematics <C> 5,895 <C> 152,100 <C> 25.80 <C> 3,933 / 2,697 / 1,918 <R> <C> 16 <C> Programming <C> 6,400 <C> 136,549 <C> 21.34 <C> 4,997 / 3,322 / 2,243 <CAP> Table 1: Subtitle database from selected Coursera courses
<R> <C> [BOLD] Model <C> [BOLD] Inference <C> [BOLD] BLEU-1 <C> [BOLD] BLEU-2 <C> [BOLD] BLEU-3 <C> [BOLD] BLEU-4 <C> [BOLD] Entropy <C> [BOLD] Dist-1 <C> [BOLD] Dist-2 <R> <C> DED (w/o Attn)  <C> MAP <C> 31.34 <C> 13.79 <C> 7.36 <C> 4.26 <C> - <C> - <C> - <R> <C> DED (w/o Attn) <C> MAP <C> 29.31 <C> 12.42 <C> 6.55 <C> 3.61 <C> - <C> - <C> - <R> <C> DED+DAttn <C> MAP <C> 30.24 <C> 14.33 <C> 8.26 <C> 4.96 <C> - <C> - <C> - <R> <C> VED+DAttn <C> MAP <C> [BOLD] 31.02 <C> 14.57 <C> 8.49 <C> 5.02 <C> - <C> - <C> - <R> <C> VED+DAttn <C> Sampling <C> 30.87 <C> [BOLD] 14.71 <C> [BOLD] 8.61 <C> [BOLD] 5.08 <C> 2.214 <C> 0.132 <C> 0.176 <R> <C> VED+DAttn (2-stage training) <C> MAP <C> 28.88 <C> 13.02 <C> 7.33 <C> 4.16 <C> - <C> - <C> - <R> <C> VED+DAttn (2-stage training) <C> Sampling <C> 29.25 <C> 13.21 <C> 7.45 <C> 4.25 <C> 2.241 <C> 0.140 <C> 0.188 <R> <C> VED+VAttn-0 <C> MAP <C> 29.70 <C> 14.17 <C> 8.21 <C> 4.92 <C> - <C> - <C> - <R> <C> VED+VAttn-0 <C> Sampling <C> 30.22 <C> 14.22 <C> 8.28 <C> 4.87 <C> [BOLD] 2.320 <C> [BOLD] 0.165 <C> [BOLD] 0.231 <R> <C> VED+VAttn-¯ [ITALIC] h <C> MAP <C> 30.23 <C> 14.30 <C> 8.28 <C> 4.93 <C> - <C> - <C> - <R> <C> VED+VAttn-¯ [ITALIC] h <C> Sampling <C> 30.47 <C> 14.35 <C> 8.39 <C> 4.96 <C> 2.316 <C> 0.162 <C> 0.228 <CAP> Table 2: BLEU, entropy, and distinct scores. We compare the deterministic encoder-decoder (DED) and variational encoder-decoders (VEDs). For VED, we have several variates: deterministic attention (DAttn) and the proposed variational attention (VAttn). Variational models are evaluated by both max a posteriori (MAP) inference and sampling.
<R> <C> Model <C> BLEU-1 <C> BLEU-4 <R> <C> Baseline <C> 41.3 <C> 12.0 <R> <C> Reranked <C> [BOLD] 41.6 <C> [BOLD] 12.2 <CAP> Table 4: Reranking the n-best output of a neural seq2seq question generation model using well-formedness probability.
<R> <C> [EMPTY] <C> Inductive classification Cora <C> Inductive classification NYT <C> Inductive classification Gaming <C> Inductive classification Travel <C> Inductive Link Prediction Cora <C> Inductive Link Prediction NYT <C> Inductive Link Prediction Gaming <C> Inductive Link Prediction Travel <R> <C> LSA <C> 97.02 <C> [BOLD] 89.45 <C> 90.70 <C> 85.88 <C> 88.10 <C> 60.71 <C> 58.99 <C> 58.97 <R> <C> TADW <C> 96.23 <C> 86.06 <C> 93.16 <C> 91.35 <C> 84.82 <C> 69.10 <C> 57.00 <C> 57.91 <R> <C> G2G <C> 94.04 <C> 85.44 <C> 89.81 <C> 80.71 <C> 81.58 <C> 74.22 <C> 58.18 <C> [BOLD] 59.50 <R> <C> GVNR-t <C> [BOLD] 97.60 <C> 88.47 <C> [BOLD] 96.09 <C> [BOLD] 91.54 <C> 82.27 <C> 71.15 <C> 59.71 <C> 58.39 <R> <C> IDNE <C> 96.58 <C> 88.21 <C> 95.22 <C> 90.78 <C> [BOLD] 91.66 <C> [BOLD] 77.90 <C> [BOLD] 62.82 <C> 58.43 <CAP> Table 4: Micro AUC scores for inductive classification and inductive link prediction
<R> <C> System <C> Adapt Param <C> Weight <C> Supervised 100 <C> Supervised 200 <C> Unsupervised 100 <C> Unsupervised 200 <R> <C> SI <C> - <C> - <C> 14.32 <C> 14.32 <C> 14.32 <C> 14.32 <R> <C> KLD <C> All <C> [ITALIC] ρ=0.0 <C> 14.09 <C> 13.30 <C> 14.14 <C> 14.04 <R> <C> KLD <C> All <C> [ITALIC] ρ=0.2 <C> 13.97 <C> 13.14 <C> 14.04 <C> 14.01 <R> <C> KLD <C> All <C> [ITALIC] ρ=0.5 <C> 14.14 <C> 13.92 <C> 14.17 <C> 14.00 <R> <C> KLD <C> All <C> [ITALIC] ρ=0.8 <C> 14.31 <C> 14.23 <C> 14.81 <C> 14.14 <R> <C> ASA <C> All <C> [ITALIC] α=0.2 <C> 13.29 <C> [BOLD] 12.58 <C> 13.99 <C> 13.92 <R> <C> ASA <C> All <C> [ITALIC] α=0.5 <C> 13.37 <C> 12.66 <C> [BOLD] 13.95 <C> [BOLD] 13.89 <R> <C> ASA <C> All <C> [ITALIC] α=0.8 <C> [BOLD] 13.20 <C> 12.76 <C> 13.98 <C> 13.94 <R> <C> MTL <C> Enc <C> [ITALIC] β=0.2 <C> 13.3 <C> [BOLD] 12.71 <C> 13.93 <C> 13.87 <R> <C> MTL <C> Enc <C> [ITALIC] β=0.5 <C> [BOLD] 13.26 <C> 12.73 <C> 13.86 <C> 13.83 <R> <C> MTL <C> Enc <C> [ITALIC] β=0.8 <C> 13.27 <C> 12.76 <C> [BOLD] 13.80 <C> [BOLD] 13.77 <CAP> Table 1: The WERs (%) of speaker adaptation using KLD, ASA and MTL for AED E2E ASR on Microsoft SMD task with 3400 hours training data. Each of the 7 test speakers has 100 or 200 adaptation utterances. In KLD and ASA adaptation, all the parameters of the AED (“All”) are updated while, in MTL adaptation, only the AED encoder (“Enc”) is updated.
<R> <C> [BOLD] Model <C> IE <C> MR <C> SST <C> SUBJ <C> QC <R> <C> NBOW <C> 54.6 <C> 77.2 <C> 80.5 <C> 91.3 <C> 88.2 <R> <C> DCNN <C> - <C> - <C> 86.8 <C> - <C> 93.0 <R> <C> CNN-multichannel <C> - <C> 81.5 <C> [BOLD] 88.1 <C> 93.4 <C> 93.6 <R> <C> RecNN <C> 52.0 <C> 76.4 <C> 82.4 <C> 90.6 <C> 88.8 <R> <C> MV-RecNN <C> 54.8 <C> 76.8 <C> 82.9 <C> 90.9 <C> 89.2 <R> <C> RNTN <C> 55.7 <C> 75.8 <C> 85.4 <C> 92.1 <C> 88.9 <R> <C> TreeLSTM <C> 56.0 <C> 78.7 <C> 86.9 <C> 91.0 <C> 91.6 <R> <C> DC-RecNN <C> 58.2 <C> 80.2 <C> 86.1 <C> 93.5 <C> 91.2 <R> <C> DC-TreeLSTM <C> [BOLD] 60.2 <C> [BOLD] 81.7 <C> 87.8 <C> [BOLD] 93.7 <C> [BOLD] 93.8 <CAP> Table 3: Accuracies of our models on five datasets against state-of-the-art neural models. DCNN: Dynamic Convolutional Neural Network [Kalchbrenner et al.2014, Denil et al.2014]. CNN-multichannel: Convolutional Neural Network [Kim2014].
<R> <C> [BOLD] Model <C> [BOLD] Hidden <C> [BOLD] Train <C> [BOLD] Test <R> <C> NBOW <C> 30 <C> 96.6 <C> 73.4 <R> <C> LSTM <C> 100 <C> 100.0 <C> 71.3 <R> <C> RecNN <C> 30 <C> 95.4 <C> 74.9 <R> <C> MV-RecNN <C> 50 <C> 95.9 <C> 75.5 <R> <C> RNTN <C> 50 <C> 97.8 <C> 76.9 <R> <C> TreeLSTM <C> 50 <C> 95.9 <C> 77.5 <R> <C> DC-RecNN <C> 30 <C> 96.5 <C> 77.9 <R> <C> DC-TreeLSTM <C> 50 <C> 98.5 <C> [BOLD] 80.2 <CAP> Table 4: Evaluation results of our models on the SICK train and test sets.
<R> <C> Method <C> Fluency <C> intra-consistency <C> inter-diversity <R> <C> Reference <C> 0.96 <C> - <C> - <R> <C> Enc-Dec <C> 0.83 <C> - <C> - <R> <C> Bo.Up. <C> 0.46 <C> 0.48 <C> 0.61 <R> <C> SS <C> 0.27 <C> 0.41 <C> 0.54 <R> <C> RS <C> [BOLD] 0.78 <C> 0.39 <C> 0.47 <R> <C> VRS <C> 0.74 <C> [BOLD] 0.72 <C> [BOLD] 0.87 <CAP> Table 4: Human evaluation on fluency, intra-consistency and inter-diversity of content selection on DUC 2004.
<R> <C> Method <C> Contextualised Word Vector Type  [BOLD] AWD-LSTM <C> Contextualised Word Vector Type  [BOLD] AWD-LSTM <C> Contextualised Word Vector Type  [BOLD] AWD-LSTM <C> Contextualised Word Vector Type  [BOLD] BERT <C> Contextualised Word Vector Type  [BOLD] BERT <C> Contextualised Word Vector Type  [BOLD] BERT <C> Contextualised Word Vector Type  [BOLD] ELMo <C> Contextualised Word Vector Type  [BOLD] ELMo <C> Contextualised Word Vector Type  [BOLD] ELMo <R> <C> [EMPTY] <C> U.Phase <C> C.Phase <C> SS.Phase <C> U.Phase <C> C.Phase <C> SS.Phase <C> U.Phase <C> C.Phase <C> SS.Phase <R> <C> Cosine-Similarity (Query,Response) <C> 39.72 <C> - <C> - <C> 39.72 <C> - <C> - <C> 39.72 <C> - <C> - <R> <C> NRS (Query,Response) <C> 47.78 <C> - <C> - <C> 54.80 <C> - <C> - <C> 40.21 <C> - <C> - <R> <C> Cosine-Similarity (Context,Query,Response) <C> 38.88 <C> 27.77 <C> 27.77 <C> 38.88 <C> 27.77 <C> 27.77 <C> 38.88 <C> 27.77 <C> 27.77 <R> <C> NRS (Context,Query,Response) <C> 47.49 <C> 49.16 <C> 52.40 <C> 64.00^{\ast} <C> 60.40^{\ast} <C> 61.94^{\ast} <C> 65.40^{\ast} <C> 55.00 <C> 62.80^{\ast} <CAP> Table 1: Human evaluation of models across phases on the test dataset, {}^{\ast} denotes statistical significance
<R> <C> [EMPTY] <C> IUXRay Acc <C> IUXRay KD <C> MIMIC-III Acc <C> MIMIC-III KD <R> <C> 2-GLM <C> 21.83±0.29 <C> 16.04±0.26 <C> 17.03±0.22 <C> 11.46±0.12 <R> <C> 3-GLM <C> 34.78±0.38 <C> 27.96±0.27 <C> 27.34±0.29 <C> 19.35±0.27 <R> <C> 4-GLM <C> 38.18±0.44 <C> 31.60±0.30 <C> 25.70±0.29 <C> 18.95±0.34 <R> <C> 5-GLM <C> 37.89±0.60 <C> 32.30±0.47 <C> 21.02±0.41 <C> 15.63±0.23 <R> <C> 6-GLM <C> 35.71±0.78 <C> 30.86±0.57 <C> 15.98±0.42 <C> 11.93±0.31 <R> <C> 7-GLM <C> 33.10±0.72 <C> 28.82±0.56 <C> 12.15±0.40 <C> 9.05±0.26 <R> <C> 8-GLM <C> 30.23±0.63 <C> 26.47±0.62 <C> 9.52±0.40 <C> 7.04±0.31 <R> <C> 9-GLM <C> 27.74±0.63 <C> 24.33±0.66 <C> 7.29±0.43 <C> 5.46±0.37 <R> <C> LSTMLM <C> [BOLD] 51.30±0.61 <C> [BOLD] 41.12±0.64 <C> [BOLD] 33.97±0.25 <C> [BOLD] 25.17±0.29 <R> <C> GRULM <C> [BOLD] 51.30±0.74 <C> [BOLD] 41.00±0.40 <C> [BOLD] 33.84±0.34 <C> [BOLD] 25.42±0.30 <CAP> TABLE II: Assessment of next word prediction in the radiology reports of IUXRay and MIMIC-III, using statistical (N-GLMs) and neural (LSTMLM, GRULM) language models. Micro-averaged accuracy (Acc) and keystroke discount (KD) are shown for each dataset.
<R> <C> [ITALIC]  [BOLD] Single models <C> [BOLD] Accuracy (%)  [BOLD] Dev <C> [BOLD] Accuracy (%)  [BOLD] Test <R> <C> BiDAF <C> - <C> 42.9 <R> <C> Coref-GRUDhingra et al. ( 2018 ) <C> 56.0 <C> 59.3 <R> <C> MHQA-GRNSong et al. ( 2018 ) <C> 62.8 <C> 65.4 <R> <C> Entity-GCNDe Cao et al. ( 2018 ) <C> 64.8 <C> 67.6 <R> <C> CFCZhong et al. ( 2019 ) <C> 66.4 <C> 70.6 <R> <C> Kundu et al. ( 2018 ) <C> 67.1 <C> - <R> <C> DynSAN* <C> - <C> [BOLD] 71.4 <R> <C> [BOLD] Proposed <C> [BOLD] 68.1 <C> 70.9 <R> <C> [ITALIC]  [BOLD] Ensemble models <C> [EMPTY] <C> [EMPTY] <R> <C> Entity-GCNDe Cao et al. ( 2018 ) <C> 68.5 <C> 71.2 <R> <C> DynSAN* <C> - <C> 73.8 <R> <C> [BOLD] Proposed <C> [BOLD] 70.9 <C> [BOLD] 74.3 <CAP> Table 1: Performance comparison among different models on WikiHop development and test set. The results of “BiDAF” are presented in the paper by Welbl et al. (2018). Models annotated with “*” are unpublished but available on WikiHop leaderboard. “-” indicates unavailable numbers.
<R> <C> [EMPTY] <C> [BOLD] beam  [ITALIC] k=5 <C> [BOLD] beam  [ITALIC] k=200 <C> [BOLD] sampling  [ITALIC] k=200 <R> <C> [BOLD] Prob. covered <C> 4.7% <C> 11.1% <C> 6.7% <R> <C> [BOLD] Sentence BLEU <C> [BOLD] Sentence BLEU <C> [BOLD] Sentence BLEU <C> [EMPTY] <R> <C> single reference <C> 41.4 <C> 36.2 <C> 38.2 <R> <C> oracle reference <C> 70.2 <C> 61.0 <C> 64.1 <R> <C> average oracle <C> 65.7 <C> 56.4 <C> 39.1 <R> <C> - # refs covered <C> 1.9 <C> 5.0 <C> 7.4 <R> <C> [BOLD] Corpus BLEU (multi-bleu.pl [BOLD] ) <C> [BOLD] Corpus BLEU (multi-bleu.pl [BOLD] ) <C> [BOLD] Corpus BLEU (multi-bleu.pl [BOLD] ) <C> [EMPTY] <R> <C> single reference <C> 41.6 <C> 33.5 <C> 36.9 <R> <C> 10 references <C> 81.5 <C> 65.8 <C> 72.8 <CAP> Table 2: Sentence and corpus BLEU for beam search hypotheses and 200 samples on a 500 sentence subset of the WMT’14 En-Fr test set. “Single reference” uses the provided reference and the most likely hypothesis, while oracle reference and average oracle are computed with 10 human references.
<R> <C> Tracker <C> Utterance-level Accuracy <C> Utterance-level Precision <C> Utterance-level Recall <C> Utterance-level F1-score <C> Subdialog-level Accuracy <C> Subdialog-level Precision <C> Subdialog-level Recall <C> Subdialog-level F1-score <R> <C> Baseline <C> 0.0374 <C> 0.3589 <C> 0.1925 <C> 0.2506 <C> 0.0488 <C> 0.3750 <C> 0.2519 <C> 0.3014 <R> <C> Cascade <C> 0.0227 <C> 0.2962 <C> 0.2145 <C> 0.2488 <C> 0.0314 <C> 0.3138 <C> 0.2734 <C> 0.2922 <R> <C> Joint <C> 0.0260 <C> 0.4682 <C> 0.1170 <C> 0.1872 <C> 0.0357 <C> 0.4648 <C> 0.1602 <C> 0.2383 <R> <C> Elaborate <C> [BOLD] 0.1210 <C> 0.5449 <C> [BOLD] 0.4964 <C> 0.5196 <C> [BOLD] 0.1500 <C> 0.5619 <C> [BOLD] 0.5787 <C> 0.5702 <R> <C> Hybrid <C> 0.1183 <C> [BOLD] 0.5780 <C> 0.4904 <C> [BOLD] 0.5306 <C> 0.1473 <C> [BOLD] 0.5898 <C> 0.5678 <C> [BOLD] 0.5786 <R> <C> Team 4 <C> 0.1002 <C> 0.5545 <C> 0.3760 <C> 0.4481 <C> 0.1212 <C> 0.5642 <C> 0.4540 <C> 0.5031 <R> <C> Team 2 <C> 0.0489 <C> 0.4440 <C> 0.2703 <C> 0.3361 <C> 0.0697 <C> 0.4634 <C> 0.3335 <C> 0.3878 <R> <C> Team 6 <C> 0.0486 <C> 0.5623 <C> 0.2314 <C> 0.3279 <C> 0.0645 <C> [BOLD] 0.5941 <C> 0.2850 <C> 0.3852 <R> <C> Team 1 <C> 0.0371 <C> 0.4179 <C> 0.2804 <C> 0.3356 <C> 0.0584 <C> 0.4384 <C> 0.3426 <C> 0.3846 <R> <C> Team 5 <C> 0.0268 <C> 0.3405 <C> 0.2014 <C> 0.2531 <C> 0.0401 <C> 0.3584 <C> 0.2632 <C> 0.3035 <R> <C> Team 7 <C> 0.0286 <C> 0.2768 <C> 0.1826 <C> 0.2200 <C> 0.0323 <C> 0.3054 <C> 0.2410 <C> 0.2694 <CAP> Table 1: Comparison of results for various dialog state trackers on the test set
<R> <C> [BOLD] Macro-F1 <C> [BOLD] Macro-F1 CH <C> [BOLD] Macro-F1 Ebola <C> [BOLD] Macro-F1 Ferg. <C> [BOLD] Macro-F1 GW crash <C> [BOLD] Macro-F1 Ottawa <C> [BOLD] Macro-F1 Prince <C> [BOLD] Macro-F1 Putin <C> [BOLD] Macro-F1 Sydney <R> <C> SVM <C> 0.399 <C> 0.380 <C> 0.382 <C> 0.427 <C> [BOLD] 0.492 <C> 0.491 <C> 0.509 <C> 0.427 <R> <C> MaxEnt <C> 0.446 <C> 0.425 <C> [BOLD] 0.418 <C> 0.475 <C> 0.468 <C> [BOLD] 0.514 <C> 0.381 <C> 0.443 <R> <C> Linear CRF <C> 0.443 <C> 0.619 <C> 0.380 <C> 0.470 <C> 0.412 <C> 0.512 <C> [BOLD] 0.528 <C> [BOLD] 0.454 <R> <C> Tree CRF <C> 0.457 <C> 0.557 <C> 0.356 <C> 0.523 <C> 0.441 <C> 0.505 <C> 0.491 <C> 0.426 <R> <C> LSTM <C> [BOLD] 0.465 <C> [BOLD] 0.657 <C> 0.373 <C> [BOLD] 0.543 <C> 0.475 <C> 0.379 <C> 0.457 <C> 0.446 <CAP> Table 5: Macro-F1 results for the best-performing classifiers, broken down by event.
<R> <C> [BOLD] SVM <C> [BOLD] SVM Support <C> [BOLD] SVM Deny <C> [BOLD] SVM Query <C> [BOLD] SVM Comment <R> <C> Support <C> [BOLD] 0.657 <C> 0.041 <C> 0.018 <C> 0.283 <R> <C> Deny <C> 0.185 <C> [BOLD] 0.129 <C> 0.107 <C> 0.579 <R> <C> Query <C> 0.083 <C> 0.081 <C> [BOLD] 0.343 <C> 0.494 <R> <C> Comment <C> 0.150 <C> 0.075 <C> 0.053 <C> [BOLD] 0.723 <R> <C> [BOLD] MaxEnt <C> [BOLD] MaxEnt <C> [BOLD] MaxEnt <C> [BOLD] MaxEnt <C> [BOLD] MaxEnt <R> <C> [EMPTY] <C> Support <C> Deny <C> Query <C> Comment <R> <C> Support <C> [BOLD] 0.794 <C> 0.044 <C> 0.003 <C> 0.159 <R> <C> Deny <C> 0.156 <C> [BOLD] 0.130 <C> 0.079 <C> 0.634 <R> <C> Query <C> 0.088 <C> 0.066 <C> [BOLD] 0.366 <C> 0.480 <R> <C> Comment <C> 0.152 <C> 0.074 <C> 0.048 <C> [BOLD] 0.726 <R> <C> [BOLD] Linear CRF <C> [BOLD] Linear CRF <C> [BOLD] Linear CRF <C> [BOLD] Linear CRF <C> [BOLD] Linear CRF <R> <C> [EMPTY] <C> Support <C> Deny <C> Query <C> Comment <R> <C> Support <C> [BOLD] 0.603 <C> 0.048 <C> 0.013 <C> 0.335 <R> <C> Deny <C> 0.219 <C> [BOLD] 0.140 <C> 0.050 <C> 0.591 <R> <C> Query <C> 0.071 <C> 0.095 <C> [BOLD] 0.357 <C> 0.476 <R> <C> Comment <C> 0.139 <C> 0.072 <C> 0.062 <C> [BOLD] 0.726 <R> <C> [BOLD] Tree CRF <C> [BOLD] Tree CRF <C> [BOLD] Tree CRF <C> [BOLD] Tree CRF <C> [BOLD] Tree CRF <R> <C> [EMPTY] <C> Support <C> Deny <C> Query <C> Comment <R> <C> Support <C> [BOLD] 0.552 <C> 0.066 <C> 0.019 <C> 0.363 <R> <C> Deny <C> 0.145 <C> [BOLD] 0.169 <C> 0.081 <C> 0.605 <R> <C> Query <C> 0.077 <C> 0.081 <C> [BOLD] 0.401 <C> 0.441 <R> <C> Comment <C> 0.128 <C> 0.074 <C> 0.068 <C> [BOLD] 0.730 <R> <C> [BOLD] LSTM <C> [BOLD] LSTM <C> [BOLD] LSTM <C> [BOLD] LSTM <C> [BOLD] LSTM <R> <C> [EMPTY] <C> Support <C> Deny <C> Query <C> Comment <R> <C> Support <C> [BOLD] 0.825 <C> 0.046 <C> 0.003 <C> 0.127 <R> <C> Deny <C> 0.225 <C> [BOLD] 0.212 <C> 0.125 <C> 0.438 <R> <C> Query <C> 0.090 <C> 0.087 <C> [BOLD] 0.432 <C> 0.390 <R> <C> Comment <C> 0.144 <C> 0.076 <C> 0.057 <C> [BOLD] 0.723 <CAP> Table 7: Confusion matrices for the best-performing classifiers.
<R> <C> [BOLD] System <C> [BOLD] Keyword Prediction  [ITALIC] Rw@1 <C> [BOLD] Keyword Prediction  [ITALIC] Rw@3 <C> [BOLD] Keyword Prediction  [ITALIC] Rw@5 <C> [BOLD] Keyword Prediction  [ITALIC] P@1 <C> [BOLD] Keyword Prediction  [BOLD] Cor. <C> [BOLD] Response Retrieval  [ITALIC] R20@1 <C> [BOLD] Response Retrieval  [ITALIC] R20@3 <C> [BOLD] Response Retrieval  [ITALIC] R20@5 <C> [BOLD] Response Retrieval  [BOLD] MRR <R> <C> Retrieval <C> - <C> - <C> - <C> - <C> - <C> 0.5196 <C> 0.7636 <C> 0.8622 <C> 0.6661 <R> <C> Ours-Random <C> 0.0005 <C> 0.0015 <C> 0.0025 <C> 0.0009 <C> 0.4995 <C> 0.5187 <C> 0.7619 <C> 0.8631 <C> 0.6650 <R> <C> Ours-PMI <C> 0.0585 <C> 0.1351 <C> 0.1872 <C> 0.0871 <C> 0.7974 <C> 0.5441 <C> [BOLD] 0.7839 <C> 0.8716 <C> 0.6847 <R> <C> Ours-Neural <C> 0.0609 <C> 0.1324 <C> 0.1825 <C> 0.1006 <C> 0.8075 <C> 0.5395 <C> 0.7801 <C> 0.8790 <C> 0.6816 <R> <C> Ours-Kernel <C> [BOLD] 0.0642 <C> [BOLD] 0.1431 <C> [BOLD] 0.1928 <C> [BOLD] 0.1191 <C> [BOLD] 0.8164 <C> [BOLD] 0.5486 <C> 0.7827 <C> [BOLD] 0.8845 <C> [BOLD] 0.6914 <CAP> Table 3: Results of Turn-level Evaluation.
<R> <C> Method <C> Dataset <C> MRR <C> P <C> R <C> F1 <R> <C> BiLSTM <C> crowd <C> 0.254 <C> 58.1 <C> 22.9 <C> 32.9 <R> <C> BiLSTM + T <C> crowd <C> 0.279 <C> 58.5 <C> 26.9 <C> 36.9 <R> <C> BiLSTM <C> distant <C> 0.200 <C> 46.6 <C> 17.5 <C> 25.5 <R> <C> BiLSTM + T <C> distant <C> 0.225 <C> 57.3 <C> 22.1 <C> 31.9 <CAP> Table 7: Experiment results of transfer learning. “T” indicates transferring the trained BiLSTM weights. “Dataset” indicates the source of training data. Note that the upper half and the lower half are results from different test data and the figures are not comparable between the two halves.
<R> <C> Training Data <C> Dev F1 <C> Dev EM <R> <C> SQuAD <C> 54.67 <C> 41.40 <R> <C> KGD <C> 72.99 <C> 58.60 <R> <C> SQuAD + KGD (tuning) <C> [BOLD] 78.55 <C> [BOLD] 63.69 <CAP> Table 3: BiDAF model performance on the span prediction task, under different choices of training data
<R> <C> Method <C> CUB Accuracy <C> CUB Naturalness <R> <C> SISGAN dong:2017 <C> 3.94 <C> 3.97 <R> <C> TAGAN nam:2018 <C> 2.4 <C> 2.6 <R> <C> TEA-cGAN-Single-Scale (Ours) <C> 1.97 <C> 1.94 <R> <C> TEA-cGAN-Multi-Scale (Ours) <C> [BOLD] 1.69 <C> [BOLD] 1.49 <CAP> Table 4: Accuracy and Naturalness average ranking values evaluated by users on CUB test dataset. Lower the better.
<R> <C> [BOLD] Model Name <C> [BOLD] Low H <C> [BOLD] High H <R> <C> En-Vi NMT <C> 9.0 <C> 13.0 <R> <C> En-De GNMT(4) <C> 4.5 <C> 5.3 <R> <C> En-De GNMT(8) <C> 4.8 <C> 5.4 <R> <C> De-En GNMT□ <C> 3.8 <C> 2.3 <R> <C> De-En GNMT⊗ <C> 3.9 <C> 5.9 <R> <C> De-En NMT <C> 2.3 <C> 4.1 <CAP> Table 1: ECE(%) for the high and low attention entropy zones. High entropy is defined as H≥1.0; (□ represents the ECE for the entire set of samples, ⊗ represents the ECE for the samples with prediction probability in 0.8−1.0 – this was done to see how attention entropy correlates with calibration in the high confidence prediction range).
<R> <C> [BOLD] Model Name <C> [BOLD] ECE Base <C> [BOLD] ECE Our <C> [BOLD] ECE T <C> [BOLD] BLEU Base <C> [BOLD] BLEU Our <C> [BOLD] BLEU T <R> <C> En-Vi NMT <C> 9.8 <C> [BOLD] 3.5 <C> 3.8 <C> 26.2 <C> [BOLD] 26.6 <C> 26.0 <R> <C> En-De GNMT4 <C> 4.8 <C> [BOLD] 2.4 <C> 2.7 <C> [BOLD] 26.8 <C> [BOLD] 26.8 <C> 26.7 <R> <C> En-De GNMT8 <C> 5.0 <C> 2.2 <C> [BOLD] 2.1 <C> [BOLD] 27.6 <C> 27.5 <C> 27.4 <R> <C> De-En GNMT <C> 3.3 <C> [BOLD] 2.2 <C> 2.3 <C> 29.6 <C> [BOLD] 29.9 <C> 29.6 <R> <C> De-En GNMT <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 29.9 <C> [BOLD] 30.1 <C> [BOLD] 30.1 <R> <C> (length norm) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> De-En NMT <C> 3.5 <C> [BOLD] 2.0 <C> 2.2 <C> 28.8 <C> [BOLD] 29.0 <C> 28.7 <R> <C> T2T En-De <C> 2.9 <C> [BOLD] 1.7 <C> 5.4 <C> 27.9 <C> [BOLD] 28.1 <C> 28.1 <R> <C> T2T En-De(B=4) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [BOLD] 28.3 <C> [BOLD] 28.3 <C> 28.2 <CAP> Table 2: Expected Calibration Errors on test data of baseline and models calibrated by two different methods. BLEU is without length normalization, except in De-En GNMT.
<R> <C> [BOLD] Hyper-parameter <C> [BOLD] Value <R> <C> Character dimension <C> 60 <R> <C> Word dimension <C> 300 <R> <C> Hidden size char <C> 30 <R> <C> Hidden size word <C> 64 <R> <C> Update function <C> Nadam <R> <C> Learning rate first 20 0epoch <C> 0.004 <R> <C> Learning rate last 20 epoch <C> 0.0004 <R> <C> Dropout character embedding <C> 0.3 <R> <C> Dropout two Bi-LSTM layers <C> 0.5 <R> <C> Batch size <C> 64 <CAP> TABLE III: THE MODEL HYPE-PARAMETERS
<R> <C> [BOLD] Model <C> [BOLD] F1-Score <R> <C> VNER[12] <C> 95.33 <R> <C> Feature-based CRF [10] <C> 93.93 <R> <C> NNVLP [9] <C> 92.91 <R> <C> Nguyen et al. 2018 [21] <C> 94.88 <R> <C> Our NER model <C> 95.61 <CAP> TABLE V: PERFORMANCES ON VLSP 2016 DATASET
<R> <C> [ITALIC] Model <C> [ITALIC] Text <C> [ITALIC] CS <C> [ITALIC] Total <R> <C> Chance <C> 50.0 <C> 50.0 <C> 50.0 <R> <C> Word Overlap <C> 41.8 <C> 59.0 <C> 54.4 <R> <C> Sliding Window <C> 55.7 <C> 53.1 <C> 55.0 <R> <C> Bilinear Model <C> 69.8 <C> 71.4 <C> 70.2 <R> <C> Attentive Reader <C> [BOLD] 70.9 <C> [BOLD] 75.2 <C> [BOLD] 72.0 <R> <C> Human Performance <C> [EMPTY] <C> [EMPTY] <C> 98.2 <CAP> Table 2: Accuracy of the baseline systems on text-based (Text), on commonsense-based questions (CS), and on the whole test set (Total). All numbers are percentages.
<R> <C> Policy <C> Success rate <C> Average Dialog Length <R> <C> Learned <C> [BOLD] 0.44 <C> [BOLD] 12.95 <R> <C> –Guess <C> 0.37 <C> [BOLD] 6.12 <R> <C> –Query <C> 0.35 <C> [BOLD] 6.16 <R> <C> Static <C> 0.29 <C> 16 <CAP> Table 1: Results on dialogs sampled from the policy test set after 10 batches of classifier training. –Guess and –Query are conditions with the guess and query features, respectively, ablated. Boldface indicates that the difference in that metric with respect to the Static policy is statistically significant according to an unpaired Welch t-test with p<0.05.
<R> <C> Training set <C> ASR system <C> WER[%] dev <C> WER[%] dev <C> WER[%] test <C> WER[%] test <R> <C> Training set <C> ASR system <C> clean <C> other <C> clean <C> other <R> <C> [ITALIC] clean-100 <C> Kaldi <C> 5.9 <C> 20.4 <C> 6.6 <C> 22.5 <R> <C> [ITALIC] clean-100 <C> RETURNN <C> [BOLD] 5.0 <C> [BOLD] 19.5 <C> [BOLD] 5.8 <C> [BOLD] 18.6 <R> <C> [ITALIC] clean-100 <C> E2E (our) <C> 10.3 <C> 24.0 <C> 11.2 <C> 24.9 <R> <C> [ITALIC] clean-460 <C> Kaldi <C> 5.3 <C> 17.7 <C> [BOLD] 5.8 <C> 19.1 <R> <C> [ITALIC] clean-460 <C> E2E (our) <C> [BOLD] 5.1 <C> [BOLD] 14.1 <C> 5.9 <C> [BOLD] 14.1 <CAP> Table 1: Comparison of our end-to-end (E2E) ASR system with Kaldi and RETURNN hybrids on LibriSpeech train-clean-100 and train-clean-460.
<R> <C> Setup <C> Paper <C> WER[%] test clean <C> WER[%] test other <C> Impr[%] test clean <C> Impr[%] test other <R> <C> low-resource <C> our <C> [BOLD] 4.3 <C> [BOLD] 13.5 <C> [BOLD] 38.6 <C> [BOLD] 20.6 <R> <C> low-resource <C> [rosenberg_speech_2019] <C> 9.3 <C> 30.6 <C> 22.8 <C> 10.1 <R> <C> low-resource <C> [rossenbach_generating_2020] <C> 5.4 <C> 22.2 <C> 33.3 <C> 9.4 <R> <C> medium-resource <C> our <C> [BOLD] 3.2 <C> [BOLD] 9.1 <C> [BOLD] 31.9 <C> 0.0 <R> <C> medium-resource <C> [rosenberg_speech_2019] <C> 6.3 <C> 22.5 <C> 0.3 <C> -0.5 <R> <C> large-resource <C> [li2018training] <C> 4.7 <C> 15.5 <C> [BOLD] 8.6 <C> [BOLD] 4.6 <R> <C> large-resource <C> [rosenberg_speech_2019] <C> 4.6 <C> 13.6 <C> 4.6 <C> 1.8 <R> <C> large-resource <C> [rossenbach_generating_2020] <C> [BOLD] 2.5 <C> [BOLD] 7.2 <C> 4.9 <C> 2.4 <CAP> Table 4: Comparison of our system performance against the results of other works for different simulated setups. “Impr” stands for relative WER improvement.
<R> <C> [ITALIC] t <C> 1 <C> 2 <C> 3 <C> 4 <C> 5 <C> AL <R> <C> [ITALIC] g( [ITALIC] t) <C> 4 <C> 5 <C> 5 <C> 5 <C> 5 <C> [EMPTY] <R> <C> ℓ( [ITALIC] t) <C> 4 <C> 4 <C> - <C> - <C> - <C> 4 <CAP> Table 2: Time-indexed lag ℓ(t)=g(t)−t−1γ when |x|=|y|=5 for a standard wait-4 system (left) and for an antagonistic system that delays its final read (right). The AL of the former is 4, while the AL of the latter is 2.2.
<R> <C> [ITALIC] t <C> 1 <C> 2 <C> 3 <C> 4 <C> 5 <C> AL <R> <C> [ITALIC] g( [ITALIC] t) <C> 4 <C> 4 <C> 4 <C> 4 <C> 5 <C> [EMPTY] <R> <C> ℓ( [ITALIC] t) <C> 4 <C> 3 <C> 2 <C> 1 <C> 1 <C> 2.2 <CAP> Table 2: Time-indexed lag ℓ(t)=g(t)−t−1γ when |x|=|y|=5 for a standard wait-4 system (left) and for an antagonistic system that delays its final read (right). The AL of the former is 4, while the AL of the latter is 2.2.
<R> <C> #Expressions <C> 0 <C> 1 <C> 2 <C> 3 <R> <C> [ITALIC] Percentage <C> 0.4% <C> 62.9% <C> 29.8% <C> 6.9% <R> <C> [ITALIC] F1 <C> 0.0 <C> 73.5 <C> 59.9 <C> 70.3 <CAP> Table 6: Percentage and performance of model generated programs with different complexity (number of expressions).
<R> <C> Model <C> Precision <C> Recall <C> F1 score <R> <C> LDA <C> 0.42 <C> 0.50 <C> 0.46 <R> <C> K-Sparse <C> 0.42 <C> 0.42 <C> 0.42 <R> <C> NVCTM <C> 0.57 <C> 0.56 <C> 0.57 <R> <C> KATE <C> 0.70 <C> 0.70 <C> 0.70 <R> <C> [BOLD] SCAT <C> [BOLD] 0.73 <C> [BOLD] 0.73 <C> [BOLD] 0.73 <CAP> TABLE I: Comparison of the Performance Evaluation on 20 Newsgroup
<R> <C> [BOLD] Models <C> [BOLD] (AIMed, BioInfer) <C> [BOLD] (BioInfer, AIMed) <R> <C> SVM1  <C> 0.25 <C> 0.44 <R> <C> SVM2  <C> 0.47 <C> 0.47 <R> <C> SVM  <C> 0.53 <C> 0.50 <R> <C> SVM  <C> 0.41 <C> 0.42 <R> <C> [EMPTY] <C> (0.67, 0.29) <C> (0.27, 0.87) <R> <C> CNN  <C> 0.37 <C> 0.45 <R> <C> Bi-LSTM  <C> 0.30 <C> 0.47 <R> <C> CNN  <C> 0.48 <C> 0.50 <R> <C> [EMPTY] <C> (0.40, 0.61) <C> (0.40, 0.66) <R> <C> RNN  <C> 0.49 <C> 0.51 <R> <C> CNN-RevGrad  <C> 0.43 <C> 0.47 <R> <C> Bi-LSTM-RevGrad  <C> 0.40 <C> 0.46 <R> <C> Adv-CNN  <C> 0.54 <C> 0.49 <R> <C> Adv-Bi-LSTM  <C> [BOLD] 0.57 <C> 0.49 <R> <C> KLSH-kNN <C> 0.51 <C> 0.51 <R> <C> [EMPTY] <C> (0.41, 0.68) <C> (0.38, 0.80) <R> <C> [BOLD] KLSH-RF <C> [BOLD] 0.57 <C> [BOLD] 0.54 <R> <C> [EMPTY] <C> [BOLD] (0.46, 0.75) <C> [BOLD] (0.37, 0.95) <CAP> Table 1: Cross-corpus evaluation results for (training, test) pairs of PPI datasets, AIMed and BioInfer datasets. For each model, we report F1 score in the first row corresponding to it. In some of the previous works, precision, recall numbers are not reported; wherever available, we show precision, recall numbers as well, in brackets.
<R> <C> [BOLD] System / Domain, Dataset <C> [BOLD] Food, WordNet <C> [BOLD] Science, WordNet <C> [BOLD] Food, Combined <C> [BOLD] Science, Combined <C> [BOLD] Science, Eurovoc <C> [BOLD] Environment, Eurovoc <R> <C> WordNet <C> 1.0000 <C> 1.0000 <C> 0.5870 <C> 0.5760 <C> 0.6243 <C> n.a. <R> <C> Baseline <C> 0.0022 <C> 0.0016 <C> 0.0019 <C> 0.0163 <C> 0.0056 <C> 0.0000 <R> <C> JUNLP <C> 0.1925 <C> 0.0494 <C> 0.2608 <C> 0.1774 <C> 0.1373 <C> 0.0814 <R> <C> NUIG-UNLP <C> n.a. <C> 0.0027 <C> n.a. <C> 0.0090 <C> 0.1517 <C> 0.0007 <R> <C> QASSIT <C> n.a. <C> 0.2255 <C> n.a. <C> 0.5757 <C> 0.3893 <C> 0.4349 <R> <C> TAXI <C> 0.3260 <C> 0.2255 <C> 0.2021 <C> 0.3634 <C> 0.3893 <C> 0.2384 <R> <C> USAAR <C> 0.0021 <C> 0.0008 <C> 0.0000 <C> 0.0020 <C> 0.0023 <C> 0.0007 <R> <C> Semantic Classes (fine-grained) <C> 0.4540 <C> 0.4181 <C> 0.5147 <C> 0.6359 <C> [BOLD] 0.5831 <C> 0.5600 <R> <C> Semantic Classes (coarse-grained) <C> [BOLD] 0.4774 <C> [BOLD] 0.5927 <C> [BOLD] 0.5799 <C> [BOLD] 0.6539 <C> 0.5515 <C> [BOLD] 0.6326 <CAP> Table 6: Comparison of the our taxonomy induction method on the SemEval 2016 Task 13 on Taxonomy Extraction Evaluation for English in terms of cumulative Fowlkes&Mallows measure (F&M).
<R> <C> [BOLD] Entity <C> [BOLD] Sentiment <C> [BOLD] # comm. <C> [BOLD] # art. <R> <C> Entities with highest positive sentiment scores <C> Entities with highest positive sentiment scores <C> Entities with highest positive sentiment scores <C> Entities with highest positive sentiment scores <R> <C> Dusty Springfield <C> 0.75 <C> 40 <C> 4 <R> <C> La Masia <C> 0.72 <C> 154 <C> 4 <R> <C> Michael Polish <C> 0.72 <C> 58 <C> 4 <R> <C> Federico Fernandez <C> 0.67 <C> 11 <C> 4 <R> <C> Banqueting House <C> 0.67 <C> 90 <C> 4 <R> <C> Entities with highest negative sentiment scores <C> Entities with highest negative sentiment scores <C> Entities with highest negative sentiment scores <C> Entities with highest negative sentiment scores <R> <C> New Jersey State Police <C> -0.8 <C> 7 <C> 4 <R> <C> Vladikavkaz <C> -0.73 <C> 33 <C> 4 <R> <C> Bureau of Consumer Protection <C> -0.64 <C> 568 <C> 4 <R> <C> Georgia Diagnostic and Classification State Prison <C> -0.63 <C> 1,528 <C> 4 <R> <C> Broadstairs <C> -0.63 <C> 3,174 <C> 6 <CAP> Table 5. Entities with highest positive or negative scores mentioned in more than average number of articles (≥4).
<R> <C> [BOLD] Parameter <C> [BOLD] Value <R> <C> word embedding size <C> 512 <R> <C> RNN cell type <C> Stacked-LSTMs <R> <C> hidden state size <C> 1024 <R> <C> encoder/decoder depth <C> 4 <R> <C> attention type <C> global <R> <C> optimizer <C> Adam <R> <C> initial learning rate <C> 0.001 <R> <C> learning rate decay <C> 0.7 <CAP> Table 2: Model parameters.
<R> <C> [EMPTY] <C> SRC token / sec WMT14 <C> SRC token / sec WMT17 <C> Scaling factor WMT14 <C> Scaling factor WMT17 <C> Mini-batch size WMT14 <C> Mini-batch size WMT17 <R> <C> OpenNMT-lua <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> baseline (1 GPU) <C> 2979 <C> 2757 <C> 1.00 <C> 1.00 <C> 64 <C> 64 <R> <C> w/ data parallelism <C> 4881 <C> 4715 <C> 1.64 <C> 1.71 <C> 256 <C> 256 <R> <C> Our implementation <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> baseline (1 GPU) <C> 2826 <C> 2550 <C> 1.00 <C> 1.00 <C> 64 <C> 64 <R> <C> w/ data parallelism <C> 4515 <C> 4330 <C> 1.60 <C> 1.70 <C> 256 <C> 256 <R> <C> w/ model parallelism <C> 6570 <C> 6397 <C> 2.32 <C> 2.51 <C> 224 <C> 224 <R> <C> HybridNMTIF <C> 9688 <C> 9109 <C> 3.43 <C> 3.57 <C> 224 <C> 224 <R> <C> [BOLD] HybridNMT <C> [BOLD] 11672 <C> [BOLD] 10716 <C> [BOLD] 4.13 <C> [BOLD] 4.20 <C> 224 <C> 224 <CAP> Table 3: Results of training speed and scaling factors.
<R> <C> OpenNMT-lua (length,coverage)∖ [ITALIC] b <C> WMT14 development (test2013) 3 <C> WMT14 development (test2013) 6 <C> WMT14 development (test2013) 9 <C> WMT14 development (test2013) 12 <C> WMT14 development (test2013) 15 <C> WMT14 development (test2013) 18 <C> WMT17 development (test2016) 3 <C> WMT17 development (test2016) 6 <C> WMT17 development (test2016) 9 <C> WMT17 development (test2016) 12 <C> WMT17 development (test2016) 15 <C> WMT17 development (test2016) 18 <R> <C> (1.0, 0.0) <C> 21.80 <C> 21.83 <C> 21.81 <C> 21.74 <C> 21.65 <C> 21.54 <C> 31.70 <C> 31.86 <C> 31.73 <C> 31.73 <C> 31.65 <C> 31.55 <R> <C> (0.8, 0.0) <C> 21.80 <C> 21.80 <C> 21.77 <C> 21.71 <C> 21.60 <C> 21.47 <C> 31.70 <C> 31.85 <C> 31.73 <C> 31.71 <C> 31.62 <C> 31.53 <R> <C> (0.6, 0.0) <C> 21.77 <C> 21.77 <C> 21.69 <C> 21.63 <C> 21.50 <C> 21.37 <C> 31.68 <C> 31.81 <C> 31.72 <C> 31.68 <C> 31.57 <C> 31.48 <R> <C> (0.4, 0.0) <C> 21.77 <C> 21.75 <C> 21.66 <C> 21.58 <C> 21.44 <C> 21.31 <C> 31.68 <C> 31.79 <C> 31.67 <C> 31.61 <C> 31.49 <C> 31.40 <R> <C> (0.2, 0.0) <C> 21.77 <C> 21.75 <C> 21.65 <C> 21.56 <C> 21.42 <C> 21.28 <C> 31.65 <C> 31.79 <C> 31.64 <C> 31.59 <C> 31.48 <C> 31.38 <R> <C> (0.0, 0.0) <C> 21.75 <C> 21.73 <C> 21.65 <C> 21.54 <C> 21.40 <C> 21.27 <C> 31.63 <C> 31.75 <C> 31.60 <C> 31.57 <C> 31.44 <C> 31.36 <R> <C> (0.2, 0.2) <C> 21.14 <C> 21.08 <C> 21.18 <C> 21.12 <C> 21.10 <C> 21.15 <C> 30.87 <C> 30.94 <C> 30.84 <C> 30.85 <C> 30.79 <C> 30.70 <R> <C> HybridNMT <C> WMT14 development (test2013) <C> WMT14 development (test2013) <C> WMT14 development (test2013) <C> WMT14 development (test2013) <C> WMT14 development (test2013) <C> WMT14 development (test2013) <C> WMT17 development (test2016) <C> WMT17 development (test2016) <C> WMT17 development (test2016) <C> WMT17 development (test2016) <C> WMT17 development (test2016) <C> WMT17 development (test2016) <R> <C> length∖ [ITALIC] b <C> 3 <C> 6 <C> 9 <C> 12 <C> 15 <C> 18 <C> 3 <C> 6 <C> 9 <C> 12 <C> 15 <C> 18 <R> <C> 1.0 <C> 22.43 <C> 22.75 <C> 22.72 <C> 22.75 <C> 22.79 <C> 22.75 <C> 32.23 <C> 32.60 <C> 32.61 <C> 32.73 <C> 32.65 <C> 32.60 <R> <C> 0.8 <C> 22.43 <C> 22.71 <C> 22.63 <C> 22.67 <C> 22.67 <C> 22.63 <C> 32.20 <C> 32.52 <C> 32.59 <C> 32.70 <C> 32.67 <C> 32.62 <R> <C> 0.6 <C> 22.35 <C> 22.62 <C> 22.56 <C> 22.55 <C> 22.54 <C> 22.50 <C> 32.16 <C> 32.44 <C> 32.51 <C> 32.56 <C> 32.55 <C> 32.49 <R> <C> 0.4 <C> 22.29 <C> 22.50 <C> 22.43 <C> 22.38 <C> 22.40 <C> 22.35 <C> 32.10 <C> 32.32 <C> 32.36 <C> 32.38 <C> 32.38 <C> 32.32 <R> <C> 0.2 <C> 22.26 <C> 22.37 <C> 22.29 <C> 22.24 <C> 22.26 <C> 22.20 <C> 32.02 <C> 32.19 <C> 32.25 <C> 32.26 <C> 32.21 <C> 32.16 <R> <C> 0.0 <C> 22.23 <C> 22.27 <C> 22.14 <C> 22.11 <C> 22.13 <C> 22.04 <C> 32.01 <C> 32.11 <C> 32.18 <C> 32.16 <C> 32.09 <C> 31.98 <CAP> Table 4: BLEU scores obtained using different hyperparameters for WMT14 and WMT17 development data. The upper half shows the results obtained by OpenNMT-lua whereas the lower half is for the proposed HybridNMT. Rows show the different parameters used for normalization. “b” stands for the beam size.
<R> <C> [BOLD] Word embedding type <C> [BOLD] Turkish (%)  [BOLD] Movie <C> [BOLD] Turkish (%)  [BOLD] Twitter <C> [BOLD] English (%)  [BOLD] Movie <C> [BOLD] English (%)  [BOLD] Twitter <R> <C> Corpus-based + SVD (U) <C> 76.19 <C> 64.38 <C> 66.54 <C> [BOLD] 87.17 <R> <C> Dictionary-based + SVD (U) <C> 60.64 <C> 51.36 <C> 55.29 <C> 60.00 <R> <C> Supervised 4-scores <C> [BOLD] 89.38 <C> [BOLD] 76.00 <C> [BOLD] 75.65 <C> 72.62 <R> <C> Concatenation of the above three <C> 88.12 <C> 73.23 <C> 73.40 <C> 73.12 <R> <C> Corpus-based + Clustering <C> 52.27 <C> 52.73 <C> 51.02 <C> 54.40 <R> <C> word2vec <C> 76.47 <C> 46.57 <C> 57.73 <C> 62.60 <R> <C> Corpus-based + SVD (U) + 3-feats <C> 88.45 <C> 72.60 <C> 76.85 <C> [BOLD] 85.88 <R> <C> Dictionary-based + SVD (U) + 3-feats <C> 88.64 <C> 71.91 <C> 76.66 <C> 80.40 <R> <C> Supervised 4-scores + 3-feats <C> [BOLD] 90.38 <C> [BOLD] 78.00 <C> [BOLD] 77.05 <C> 72.83 <R> <C> Concatenation of the above three + 3 feats <C> 89.77 <C> 72.60 <C> 77.03 <C> 80.20 <R> <C> Corpus-based + Clustering + 3-feats <C> 87.89 <C> 71.91 <C> 75.02 <C> 74.40 <R> <C> word2vec+ 3-feats <C> 88.88 <C> 71.23 <C> 77.03 <C> 75.64 <CAP> Table 1: Accuracies for different feature sets fed as input into the SVM classifier in predicting the labels of reviews. The word2vec algorithm is the baseline method.
<R> <C> [BOLD] Model <C> [BOLD] CER <C> [BOLD] WER <R> <C> GUT-UNI <C> 0.0943 <C> 0.2535 <R> <C> GUT-REAL <C> 0.0876 <C> 0.2324 <R> <C> KOT-UNI <C> 0.0941 <C> 0.2554 <R> <C> KOT-REAL <C> 0.0902 <C> 0.2345 <R> <C> OCR <C> 0.0978 <C> 0.2914 <R> <C> GOLD <C> 0.0680 <C> 0.1676 <CAP> Table 2: Comparison of the tested models and baselines. The performance is measured in character error rate (CER) and word error rate (WER). OCR = FNL OCR without post-processing, GOLD = model trained with gold standard data, GUT = Gutenberg corpus, KOT = Kotus corpus, UNI = uniform noise, REAL = realistic noise.
<R> <C> Network <C> Mean Score <C> Std. Dev <C> Cohen’s  [ITALIC] d <R> <C> WaveGAN <C> 3.39 <C> ±1.67 <C> 0.65 <R> <C> Proposed Approach <C> 4.48 <C> ±1.70 <C> 0.65 <CAP> Table 1: The results of our human listening evaluations. 30 participants listened to 10 files generated using the original WaveGAN network, and 10 files using our proposed approach, and ranked the human-likeness of each file on a 7 point Likert scale (1 not human, 7 human).
<R> <C> [BOLD] Dataset <C> [BOLD] Wiki <C> [BOLD] OntoNotes <C> [BOLD] BBN <C> [BOLD] NYT <R> <C> # of target types <C> 113 <C> 89 <C> 47 <C> 446 <R> <C> (1) noisy mentions (%) <C> 27.99 <C> 25.94 <C> 22.32 <C> 51.81 <R> <C> (2a) sibling pruning (%) <C> 23.92 <C> 16.09 <C> 22.32 <C> 39.26 <R> <C> (2b) min. pruning (%) <C> 28.22 <C> 8.09 <C> 3.27 <C> 32.75 <R> <C> (2c) all pruning (%) <C> 45.99 <C> 23.45 <C> 25.33 <C> 61.12 <CAP> Table 1: A study of type label noise. (1): %mentions with multiple sibling types (e.g., actor, singer); (2a)-(2c): %mentions deleted by the three pruning heuristics [7] (see Sec. 4.1), for three experiment datasets and New York Times annotation corpus [5].
<R> <C> [BOLD] Typing  [BOLD] System <C> [BOLD] Noise Reduction  [BOLD] Method <C> [BOLD] Wiki  [BOLD] Acc <C> [BOLD] Wiki  [BOLD] Ma-F1 <C> [BOLD] Wiki  [BOLD] Mi-F1 <C> [BOLD] OntoNotes  [BOLD] Acc <C> [BOLD] OntoNotes  [BOLD] Ma-F1 <C> [BOLD] OntoNotes  [BOLD] Mi-F1 <C> [BOLD] BBN  [BOLD] Acc <C> [BOLD] BBN  [BOLD] Ma-F1 <C> [BOLD] BBN  [BOLD] Mi-F1 <R> <C> [EMPTY] <C> PL-SVM  <C> 0.428 <C> 0.613 <C> 0.571 <C> 0.465 <C> 0.648 <C> 0.582 <C> 0.497 <C> 0.679 <C> 0.677 <R> <C> [EMPTY] <C> CLPL  <C> 0.162 <C> 0.431 <C> 0.411 <C> 0.438 <C> 0.603 <C> 0.536 <C> 0.486 <C> 0.561 <C> 0.582 <R> <C> [EMPTY] <C> Raw <C> 0.288 <C> 0.528 <C> 0.506 <C> 0.249 <C> 0.497 <C> 0.446 <C> 0.523 <C> 0.576 <C> 0.587 <R> <C> [EMPTY] <C> Min  <C> 0.325 <C> 0.566 <C> 0.536 <C> 0.295 <C> 0.523 <C> 0.470 <C> 0.524 <C> 0.582 <C> 0.595 <R> <C> [EMPTY] <C> All  <C> 0.417 <C> 0.591 <C> 0.545 <C> 0.305 <C> 0.552 <C> 0.495 <C> 0.495 <C> 0.563 <C> 0.568 <R> <C> [BOLD] HYENA  <C> WSABIE-Min  <C> 0.199 <C> 0.462 <C> 0.459 <C> 0.400 <C> 0.565 <C> 0.521 <C> 0.524 <C> 0.610 <C> 0.621 <R> <C> [EMPTY] <C> PTE-Min  <C> 0.238 <C> 0.542 <C> 0.522 <C> 0.452 <C> 0.626 <C> 0.572 <C> 0.545 <C> 0.639 <C> 0.650 <R> <C> [EMPTY] <C> PLE-NoCo <C> 0.517 <C> 0.672 <C> 0.634 <C> 0.496 <C> 0.658 <C> 0.603 <C> 0.650 <C> 0.709 <C> 0.703 <R> <C> [EMPTY] <C> PLE <C> [BOLD] 0.543 <C> [BOLD] 0.695 <C> [BOLD] 0.681 <C> [BOLD] 0.546 <C> [BOLD] 0.692 <C> [BOLD] 0.625 <C> [BOLD] 0.692 <C> [BOLD] 0.731 <C> [BOLD] 0.732 <R> <C> [EMPTY] <C> Raw <C> 0.474 <C> 0.692 <C> 0.655 <C> 0.369 <C> 0.578 <C> 0.516 <C> 0.467 <C> 0.672 <C> 0.612 <R> <C> [EMPTY] <C> Min <C> 0.453 <C> 0.691 <C> 0.631 <C> 0.373 <C> 0.570 <C> 0.509 <C> 0.444 <C> 0.671 <C> 0.613 <R> <C> [EMPTY] <C> All <C> 0.453 <C> 0.648 <C> 0.582 <C> 0.400 <C> 0.618 <C> 0.548 <C> 0.461 <C> 0.636 <C> 0.583 <R> <C> [BOLD] FIGER  <C> WSABIE-Min <C> 0.455 <C> 0.646 <C> 0.601 <C> 0.425 <C> 0.603 <C> 0.546 <C> 0.481 <C> 0.671 <C> 0.618 <R> <C> [EMPTY] <C> PTE-Min <C> 0.476 <C> 0.670 <C> 0.635 <C> 0.494 <C> 0.675 <C> 0.618 <C> 0.513 <C> 0.674 <C> 0.657 <R> <C> [EMPTY] <C> PLE-NoCo <C> 0.543 <C> 0.726 <C> 0.705 <C> 0.547 <C> 0.699 <C> 0.639 <C> 0.643 <C> 0.753 <C> 0.721 <R> <C> [EMPTY] <C> PLE <C> [BOLD] 0.599 <C> [BOLD] 0.763 <C> [BOLD] 0.749 <C> [BOLD] 0.572 <C> [BOLD] 0.715 <C> [BOLD] 0.661 <C> [BOLD] 0.685 <C> [BOLD] 0.777 <C> [BOLD] 0.750 <CAP> Table 9: Study of performance improvement on fine-grained typing systems FIGER [14] and HYENA [35] on the three datasets.
<R> <C> Dataset <C> Split Size Train <C> Split Size Validation <C> Split Size Test <C> % Novel n-grams in Gold Summary unigrams <C> % Novel n-grams in Gold Summary bigrams <C> % Novel n-grams in Gold Summary trigrams <C> % Novel n-grams in Gold Summary 4-grams <C> Mean # Words Article <C> Mean # Words Summary <R> <C> Newsroom <C> 993,746 <C> 108,590 <C> 108,636 <C> 17.40 <C> 44.05 <C> 55.38 <C> 61.21 <C> 658.6 <C> 26.7 <R> <C> XSum <C> 204,045 <C> 11,332 <C> 11,334 <C> 34.88 <C> 78.78 <C> 92.03 <C> 96.80 <C> 431.1 <C> 23.3 <R> <C> CNN/DailyMail <C> 287,227 <C> 13,368 <C> 11,490 <C> 12.70 <C> 46.29 <C> 65.04 <C> 75.56 <C> 685.2 <C> 52.0 <CAP> Table 1: Comparison of summarization datasets with respect to dataset size, proportion of unique n-grams, mean article length in words, and mean summary length in words.
<R> <C> [BOLD] Method <C> [BOLD] All  [BOLD] EM <C> [BOLD] All  [BOLD] F1 <C> [BOLD] HasAns  [BOLD] EM <C> [BOLD] HasAns  [BOLD] F1 <C> [BOLD] NoAns  [BOLD] EM <C> [BOLD] NoAns  [BOLD] F1 <R> <C> BERT <C> 78.0 <C> 81.2 <C> 78.9 <C> 85.4 <C> 77.0 <C> 77.0 <R> <C> + E-FV <C> 78.2 <C> 81.5 <C> 79.1 <C> 85.7 <C> 77.4 <C> 77.4 <R> <C> + I-FV (Class.) <C> 78.6 <C> 82.0 <C> 77.7 <C> 84.5 <C> 79.6 <C> 79.6 <R> <C> + I-FV (Reg.) <C> 78.5 <C> 81.7 <C> 78.0 <C> 84.6 <C> 78.9 <C> 78.9 <R> <C> + both FVs (RV) <C> 79.3 <C> 82.4 <C> 78.0 <C> 84.0 <C> 80.7 <C> 80.7 <R> <C> ALBERT <C> 87.0 <C> 90.2 <C> 82.6 <C> 89.0 <C> 91.4 <C> 91.4 <R> <C> + E-FV <C> 87.4 <C> 90.6 <C> 82.4 <C> 88.7 <C> 92.4 <C> 92.4 <R> <C> + I-FV (Class.) <C> 87.2 <C> 90.3 <C> 81.7 <C> 87.9 <C> 92.7 <C> 92.7 <R> <C> + I-FV (Reg.) <C> 87.3 <C> 90.4 <C> 82.4 <C> 88.5 <C> 92.3 <C> 92.3 <R> <C> + both FVs (RV) <C> 87.8 <C> 90.9 <C> 83.1 <C> 89.4 <C> 92.4 <C> 92.4 <CAP> Table 4: Results (%) with different answer verification methods on the SQuAD2.0 dev set. Class. and Reg. are short for the classification and regression loss defined in §3.2.
<R> <C> [BOLD] Method <C> [BOLD] EM <C> [BOLD] F1 <R> <C> ALBERT <C> 87.0 <C> 90.2 <R> <C> Two-model Ensemble <C> 87.6 <C> 90.6 <R> <C> Retro-Reader <C> 87.8 <C> 90.9 <CAP> Table 6: Comparisons with Equivalent Parameters on the dev sets of SQuAD2.0.
<R> <C> Relation <C> Tasks <C> Precision <C> Recall <C> [ITALIC] F1 <R> <C> Expansion <C> 1 <C> 59.47 <C> 74.72 <C> 66.23 <R> <C> Expansion <C> 1+2 <C> 60.64 <C> 71.00 <C> 65.41 <R> <C> Expansion <C> 1+3 <C> 60.35 <C> 71.56 <C> 65.48 <R> <C> [EMPTY] <C> 1+4 <C> 60.00 <C> [BOLD] 77.51 <C> 67.64 <R> <C> [EMPTY] <C> ALL <C> [BOLD] 64.13 <C> 76.77 <C> [BOLD] 69.88 <R> <C> Comparison <C> 1 <C> 34.65 <C> 30.35 <C> 32.35 <R> <C> Comparison <C> 1+2 <C> 30.00 <C> 22.76 <C> 25.88 <R> <C> Comparison <C> 1+3 <C> [BOLD] 40.37 <C> 30.34 <C> [BOLD] 34.65 <R> <C> [EMPTY] <C> 1+4 <C> 35.94 <C> 15.86 <C> 22.01 <R> <C> [EMPTY] <C> ALL <C> 30.63 <C> [BOLD] 33.79 <C> 32.13 <R> <C> Temporal <C> 1 <C> 35.29 <C> 10.91 <C> 16.67 <R> <C> Temporal <C> 1+2 <C> 36.36 <C> 21.82 <C> 27.27 <R> <C> Temporal <C> 1+3 <C> 37.50 <C> 16.36 <C> 22.79 <R> <C> [EMPTY] <C> 1+4 <C> [BOLD] 60.00 <C> 10.91 <C> 18.46 <R> <C> [EMPTY] <C> ALL <C> 42.42 <C> [BOLD] 25.45 <C> [BOLD] 31.82 <R> <C> Contingency <C> 1 <C> 42.93 <C> 30.04 <C> 35.35 <R> <C> Contingency <C> 1+2 <C> 40.34 <C> 35.17 <C> 37.57 <R> <C> Contingency <C> 1+3 <C> 42.50 <C> 37.36 <C> 39.77 <R> <C> [EMPTY] <C> 1+4 <C> 47.11 <C> [BOLD] 41.76 <C> 44.27 <R> <C> [EMPTY] <C> ALL <C> [BOLD] 59.20 <C> 37.73 <C> [BOLD] 46.09 <CAP> Table 7: Results on 4-way Classification of Implicit Relations in PDTB.
<R> <C> System <C> Accuracy <C> [ITALIC] F1 <R> <C>  <C> 57.10 <C> 40.50 <R> <C> Proposed STL <C> 52.82 <C> 37.65 <R> <C> Proposed MTL <C> [BOLD] 57.27 <C> [BOLD] 44.98 <CAP> Table 8: General Performances of Different Approaches on 4-way Classification Task.
<R> <C> System <C> Comp. <C> Cont. <C> Expa. <C> Temp. <R> <C>  <C> 31.79 <C> 47.16 <C> - <C> 20.30 <R> <C>  <C> 31.32 <C> 49.82 <C> - <C> 26.57 <R> <C>  <C> 35.93 <C> 52.78 <C> - <C> 27.63 <R> <C> (R&X 2015) <C> [BOLD] 41.00 <C> 53.80 <C> 69.40 <C> 33.30 <R> <C> Proposed STL <C> 37.10 <C> 51.73 <C> 67.53 <C> 29.38 <R> <C> Proposed MTL <C> 37.91 <C> [BOLD] 55.88 <C> [BOLD] 69.97 <C> [BOLD] 37.17 <CAP> Table 9: General Performances of Different Approaches on Binary Classification Task.
<R> <C> Source <C> M <C> All <C> CR <C> PV <C> PT <C> O <R> <C> CNNDM <C> Sent <C> 33.0 <C> 18.7 <C> 9.0 <C> 2.3 <C> 3.0 <R> <C> CNNDM <C> Disco <C> 34.0 <C> 18.3 <C> 8.4 <C> 2.6 <C> 4.7 <R> <C> NYT <C> Sent <C> 23.3 <C> 13.5 <C> 5.9 <C> 0.8 <C> 3.1 <R> <C> NYT <C> Disco <C> 23.8 <C> 13.9 <C> 5.7 <C> 0.8 <C> 3.4 <CAP> Table 4: Number of errors per 10,000 characters based on automatic grammaticality checking with Grammarly on CNNDM and NYT. Lower values are better. Detailed error categories, including correctness (CR), passive voice (PV) misuse, punctuation (PT) in compound/complex sentences and others (O), are listed from left to right.
<R> <C> Model <C> All <C> Coherence <C> Grammaticality <R> <C> Sent <C> 3.45±0.87 <C> 3.30±0.90 <C> 3.45±1.06 <R> <C> Disco <C> 3.24±0.84 <C> 3.15±0.95 <C> 3.25±1.02 <R> <C> Ref <C> 3.28±0.99 <C> 3.12±0.94 <C> 3.29±1.06 <CAP> Table 5: Human evaluation results. We ask Turkers to grade the overall preference, coherence and grammaticality from 1 to 5. Mean values along with standard deviations are reported.
<R> <C> Automatic Evaluation <C> Automatic Evaluation Rule-Based* <C> Automatic Evaluation KVR* <C> Automatic Evaluation S2S <C> Automatic Evaluation S2S + Attn <C> Automatic Evaluation Ptr-Unk <C> Automatic Evaluation Mem2Seq <C> Automatic Evaluation GLMP K1 <C> Automatic Evaluation GLMP K3 <C> Automatic Evaluation GLMP K6 <R> <C> BLEU <C> 6.6 <C> 13.2 <C> 8.4 <C> 9.3 <C> 8.3 <C> 12.6 <C> 13.83 <C> [BOLD] 14.79 <C> 12.37 <R> <C> Entity F1 <C> 43.8 <C> 48.0 <C> 10.3 <C> 19.9 <C> 22.7 <C> 33.4 <C> 57.25 <C> [BOLD] 59.97 <C> 53.54 <R> <C> Schedule F1 <C> 61.3 <C> 62.9 <C> 9.7 <C> 23.4 <C> 26.9 <C> 49.3 <C> 68.74 <C> [BOLD] 69.56 <C> 69.38 <R> <C> Weather F1 <C> 39.5 <C> 47.0 <C> 14.1 <C> 25.6 <C> 26.7 <C> 32.8 <C> 60.87 <C> [BOLD] 62.58 <C> 55.89 <R> <C> Navigation F1 <C> 40.4 <C> 41.3 <C> 7.0 <C> 10.8 <C> 14.9 <C> 20.0 <C> 48.62 <C> [BOLD] 52.98 <C> 43.08 <R> <C> Human Evaluation <C> Human Evaluation <C> Human Evaluation <C> Human Evaluation <C> Human Evaluation <C> Human Evaluation <C> Human Evaluation <C> Human Evaluation <C> Human Evaluation <C> Human Evaluation <R> <C> [EMPTY] <C> Mem2Seq <C> Mem2Seq <C> Mem2Seq <C> GLMP <C> GLMP <C> GLMP <C> Human <C> Human <C> Human <R> <C> Appropriate <C> 3.89 <C> 3.89 <C> 3.89 <C> 4.15 <C> 4.15 <C> 4.15 <C> 4.6 <C> 4.6 <C> 4.6 <R> <C> Humanlike <C> 3.80 <C> 3.80 <C> 3.80 <C> 4.02 <C> 4.02 <C> 4.02 <C> 4.54 <C> 4.54 <C> 4.54 <CAP> Table 3: In SMD dataset, our model achieves highest BLEU score and entity F1 score over baselines, including previous state-of-the-art result from Madotto et al. (2018). (Models with * are reported from Eric et al. (2017), where the problem is simplified to the canonicalized forms.)
<R> <C> Language <C> [ITALIC] β <C> [ITALIC] k <C> BLEU <C> TL <C> NE <R> <C> De <C> 0.0 <C> 0 <C> 20.40 <C> 4.13 <C> 2.11 <R> <C> De <C> 0.5 <C> 5 <C> 20.17 <C> 4.11 <C> 0.12 <R> <C> Es <C> 0.0 <C> 0 <C> 24.94 <C> 6.00 <C> 4.34 <R> <C> Es <C> 0.5 <C> 5 <C> 25.32 <C> 4.24 <C> 0.05 <R> <C> Fr <C> 0.0 <C> 0 <C> 21.36 <C> 5.96 <C> 5.25 <R> <C> Fr <C> 0.5 <C> 5 <C> 21.02 <C> 3.86 <C> 0.11 <R> <C> It <C> 0.0 <C> 0 <C> 25.34 <C> 4.51 <C> 2.08 <R> <C> It <C> 0.5 <C> 5 <C> 25.19 <C> 3.95 <C> 0.01 <R> <C> Pt <C> 0.0 <C> 0 <C> 25.11 <C> 4.77 <C> 2.31 <R> <C> Pt <C> 0.5 <C> 5 <C> 24.39 <C> 4.02 <C> 0.01 <R> <C> Nl <C> 0.0 <C> 0 <C> 22.00 <C> 4.41 <C> 2.14 <R> <C> Nl <C> 0.5 <C> 5 <C> 21.85 <C> 4.05 <C> 0.01 <R> <C> Ru <C> 0.0 <C> 0 <C> 17.35 <C> 4.95 <C> 2.76 <R> <C> Ru <C> 0.5 <C> 5 <C> 16.97 <C> 4.11 <C> 0.01 <CAP> Table 3: We demonstrate the scalability of our approach by evaluating translation to many languages on our TED test set. Hyper-parameters, β and k, were only tuned on the TED German dev set.
<R> <C> [BOLD] Metric <C> [BOLD] Global Prior <C> [BOLD] Local Prior <C> [BOLD] CNN <C> [BOLD] LSTM <C> [BOLD] CNN+LSTM <C> [BOLD] BottomUp <C> [BOLD] MAC <C> [BOLD] Humans <R> <C> Open <C> 16.52 <C> 16.99 <C> 1.74 <C> 22.69 <C> 31.80 <C> 34.83 <C> 38.91 <C> 87.4 <R> <C> Binary <C> 42.99 <C> 47.53 <C> 36.05 <C> 61.90 <C> 63.26 <C> 66.64 <C> 71.23 <C> 91.2 <R> <C> Query <C> 16.52 <C> 16.99 <C> 1.55 <C> 22.69 <C> 31.80 <C> 34.83 <C> 38.91 <C> 87.4 <R> <C> Compare <C> 35.59 <C> 41.91 <C> 36.34 <C> 57.79 <C> 56.62 <C> 56.32 <C> 60.04 <C> 93.1 <R> <C> Choose <C> 17.45 <C> 26.58 <C> 0.85 <C> 57.15 <C> 61.40 <C> 66.56 <C> 70.59 <C> 94.3 <R> <C> Logical <C> 50.32 <C> 50.11 <C> 47.18 <C> 61.73 <C> 62.05 <C> 64.03 <C> 69.99 <C> 88.5 <R> <C> Verify <C> 53.40 <C> 58.80 <C> 47.02 <C> 65.78 <C> 67.00 <C> 71.45 <C> 75.45 <C> 90.1 <R> <C> Global <C> 24.70 <C> 20.19 <C> 8.64 <C> 27.22 <C> 56.57 <C> 60.29 <C> 60.82 <C> 92.3 <R> <C> Object <C> 49.96 <C> 54.00 <C> 47.33 <C> 74.33 <C> 75.90 <C> 78.45 <C> 81.49 <C> 88.1 <R> <C> Attribute <C> 34.89 <C> 42.67 <C> 22.66 <C> 48.28 <C> 50.91 <C> 53.88 <C> 59.82 <C> 90.7 <R> <C> Relation <C> 22.88 <C> 20.16 <C> 11.60 <C> 33.24 <C> 39.45 <C> 42.84 <C> 46.16 <C> 89.2 <R> <C> Category <C> 15.26 <C> 17.31 <C> 3.56 <C> 22.33 <C> 37.49 <C> 41.18 <C> 44.38 <C> 90.3 <R> <C> Distribution <C> 130.86 <C> 21.56 <C> 19.99 <C> 17.93 <C> 7.46 <C> 5.98 <C> 5.34 <C> - <R> <C> Grounding <C> - <C> - <C> - <C> - <C> - <C> 78.47 <C> 82.24 <C> - <R> <C> Validity <C> 89.02 <C> 84.44 <C> 35.78 <C> 96.39 <C> 96.02 <C> 96.18 <C> 96.16 <C> 98.9 <R> <C> Plausibility <C> 75.34 <C> 84.42 <C> 34.84 <C> 87.30 <C> 84.25 <C> 84.57 <C> 84.48 <C> 97.2 <R> <C> Consistency <C> 51.78 <C> 54.34 <C> 62.40 <C> 68.68 <C> 74.57 <C> 78.71 <C> 81.59 <C> 98.4 <R> <C> [BOLD] Accuracy <C> [BOLD] 28.93 <C> [BOLD] 31.31 <C> [BOLD] 17.82 <C> [BOLD] 41.07 <C> [BOLD] 46.55 <C> [BOLD] 49.74 <C> [BOLD] 54.06 <C> [BOLD] 89.3 <CAP> Table 1: Results for baselines and state-of-the-art models on the GQA dataset. All results refer to the test set. Models are evaluated for overall accuracy as well as accuracy per type. In addition, they are evaluated by validity, plausibility, distribution, consistency, and when possible, grounding metrics. Please refer to the text for further detail.
<R> <C> Domain <C> First viable Avg. <C> First viable Var. <C> Final Avg. <C> Final Var. <R> <C> SQuAD 2.0 <C> 3.43 <C> 2.56 <C> 1.91 <C> 2.00 <R> <C> SQuAD 2.0 Czech <C> 3.95 <C> 2.18 <C> 2.64 <C> 2.67 <R> <C> Tech issues <C> 3.77 <C> 1.79 <C> 3.10 <C> 2.23 <R> <C> Admin. issues <C> 4.05 <C> 1.91 <C> 2.92 <C> 2.55 <R> <C> All <C> 3.85 <C> 2.07 <C> 2.77 <C> 2.55 <CAP> Table 10: Average quality ratings across domains for first viable and final translations (1 - best, 5 - worst)
<R> <C> Model <C> Pre <C> Rec <C> F1 <R> <C> BERT <C> 0.41 <C> 0.60 <C> 0.48 <R> <C> BERT-MRC <C> 0.54 <C> 0.66 <C> 0.59 <R> <C> CRF with BERT-MRC features <C> 0.53 <C> 0.68 <C> 0.60 <R> <C> CRF with BERT-MRC features, POS, event and parse tree features <C> 0.58 <C> 0.78 <C> 0.66 <CAP> Table 5: Performances of different models on emotional trigger extraction from Tweets.
<R> <C> [EMPTY] <C> MRR <C> HITS@10 <C> MAP (w/ type checking) <R> <C> DistMult <C> 0.36 <C> 58.5 <C> 64.5 <R> <C> DistMult-tanh <C> 0.39 <C> 63.3 <C> 76.0 <R> <C> DistMult-tanh-WV-init <C> 0.28 <C> 52.5 <C> 65.5 <R> <C> DistMult-tanh-EV-init <C> [BOLD] 0.42 <C> [BOLD] 73.2 <C> [BOLD] 88.2 <CAP> Table 4: Evaluation with pre-trained vectors
<R> <C> Categories <C> Max acc. <C> LDA max <R> <C> Village, Film <C> 0.9992 <C> 0.998388 <R> <C> Village, Animal <C> 0.9979 <C> – <R> <C> Animal, Film <C> 0.9946 <C> 0.992510 <R> <C> Animal, Company <C> 0.9943 <C> 0.987633 <R> <C> Animal, Film, Company, Village <C> 0.9706 <C> – <R> <C> autos, baseball <C> 0.9703 <C> 0.945402 <R> <C> guns, hardware <C> 0.9693 <C> 0.965613 <R> <C> mideast, electronics <C> 0.9689 <C> 0.959472 <R> <C> Animal, Film, Company <C> 0.9688 <C> 0.961771 <R> <C> christian, guns <C> 0.9380 <C> 0.925251 <R> <C> med, electronics <C> 0.9324 <C> 0.931532 <R> <C> atheism, space <C> 0.9232 <C> 0.864271 <R> <C> baseball, hockey <C> 0.9063 <C> 0.798500 <R> <C> autos, baseball, space <C> 0.8964 <C> 0.803371 <R> <C> Animal, Plant <C> 0.8540 <C> – <R> <C> politics, religion <C> 0.8264 <C> 0.804404 <CAP> Table 2: Maximum brute-force accuracy obtained and maximum accuracy obtained using only combinations of the top 12 LDA-surfaced documents
<R> <C> [BOLD] Team ID <C> [BOLD] Affiliation <C> [BOLD] Accuracy <C> [BOLD] F1 <C> [BOLD] AvgRec <R> <C> Fermi Syed et al. ( 2019 ) <C> IIIT Hyderabad, Microsoft, Teradata <C> [BOLD] 0.840 <C> 0.7182 <C> 0.7353 <R> <C> TMLab Niewiński et al. ( 2019 ) <C> Samsung R&D Institute, Warsaw, Poland <C> [BOLD] 0.834 <C> 0.7251 <C> 0.7641 <R> <C> SolomonLab Gupta et al. ( 2019 ) <C> Samsung R&D Institute India, Bangalore <C> [BOLD] 0.831 <C> 0.7094 <C> 0.7284 <R> <C> ColumbiaNLP Chakrabarty and Muresan ( 2019 ) <C> Columbia University, Department Of Computer Science and Data Science Institute <C> [BOLD] 0.828 <C> 0.6457 <C> 0.6629 <R> <C> DOMLIN Stammbach et al. ( 2019 ) <C> Deutsches Forschungszentrum für Künstliche Intelligenz (DFKI), Saarbrucken, Germany <C> [BOLD] 0.823 <C> 0.7103 <C> 0.7552 <R> <C> BLCU_NLP Xie et al. ( 2019 ) <C> Beijing Language and Culture University, Beijing, China <C> [BOLD] 0.820 <C> 0.6965 <C> 0.7235 <R> <C> pjetro <C> Warsaw University of Technology <C> [BOLD] 0.790 <C> 0.6616 <C> 0.6986 <R> <C> LP0606 <C> [EMPTY] <C> [BOLD] 0.768 <C> 0.6378 <C> 0.6798 <R> <C> PP08 <C> [EMPTY] <C> [BOLD] 0.766 <C> 0.6379 <C> 0.6847 <R> <C> AUTOHOME-ORCA Lv et al. ( 2019 ) <C> Autohome Inc., Beijing, China and Beijing University of Posts and Telecommunications, Beijing, China <C> [BOLD] 0.745 <C> 0.58310 <C> 0.59611 <R> <C> DUTH Bairaktaris et al. ( 2019 ) <C> Democritus University of Thrace, Xanthi, Greece <C> [BOLD] 0.711 <C> 0.56311 <C> 0.60410 <R> <C> cococold <C> [EMPTY] <C> [BOLD] 0.702 <C> 0.54312 <C> 0.59412 <R> <C> nothing <C> [EMPTY] <C> [BOLD] 0.702 <C> 0.54312 <C> 0.59412 <R> <C> chchao <C> [EMPTY] <C> [BOLD] 0.630 <C> 0.45413 <C> 0.52313 <R> <C> CodeForTheChange Avvaru and Pandey ( 2019 ) <C> International Institute of Information Technology, Hyderabad, Teradata and Qubole <C> [BOLD] 0.630 <C> 0.44214 <C> 0.51314 <R> <C> Tuefact Juhasz et al. ( 2019 ) <C> University of Tübingen, Tübingen, Germany <C> [BOLD] 0.599 <C> 0.36015 <C> 0.34815 <R> <C> Reem06 <C> [EMPTY] <C> [BOLD] 0.549 <C> 0.26316 <C> 0.34316 <R> <C> [ITALIC] Majority Class Baseline <C> [EMPTY] <C> [BOLD] 0.450 <C> [ITALIC] 0.009 <C> [ITALIC] 0.333 <CAP> Table 3: Subtask A: Results for question classification based on the official submissions, evaluated on the test set. (Some teams did not submit system description papers, and thus we have no citations for their systems.)
<R> <C> [BOLD] Team ID <C> [BOLD] Affiliation <C> [BOLD] Accuracy <C> [BOLD] F1 <C> [BOLD] AvgRec <C> [BOLD] MAP <R> <C> AUTOHOME-ORCA <C> Autohome Inc., Beijing, China and Beijing University of Posts and Telecommunications, Beijing, China <C> [BOLD] 0.815 <C> 0.5112 <C> 0.5122 <C> 0.1557 <R> <C> ColumbiaNLP <C> Columbia University, Department Of Computer Science and Data Science Institute <C> [BOLD] 0.791 <C> 0.5241 <C> 0.6351 <C> 0.1348 <R> <C> DOMLIN <C> Deutsches Forschungszentrum für Künstliche Intelligenz (DFKI), Saarbrucken, Germany <C> [BOLD] 0.718 <C> 0.4023 <C> 0.4453 <C> 0.2673 <R> <C> SolomonLab <C> Samsung R&D Institute India, Bangalore <C> [BOLD] 0.686 <C> 0.3754 <C> 0.4034 <C> 0.3332 <R> <C> CodeForTheChange <C> International Institute of Information Technology, Hyderabad, Teradata and Qubole <C> [BOLD] 0.654 <C> 0.3255 <C> 0.3265 <C> 0.1566 <R> <C> BLCU_NLP <C> Beijing Language and Culture University, Beijing, China <C> [BOLD] 0.611 <C> 0.2966 <C> 0.3176 <C> 0.2224 <R> <C> LP0606 <C> [EMPTY] <C> [BOLD] 0.548 <C> 0.2717 <C> 0.3417 <C> 0.1219 <R> <C> PP08 <C> [EMPTY] <C> [BOLD] 0.548 <C> 0.2717 <C> 0.3417 <C> 0.1219 <R> <C> Tuefact <C> University of Tübingen, Tübingen, Germany <C> [BOLD] 0.527 <C> 0.2608 <C> 0.3478 <C> 0.5711 <R> <C> cococold <C> [EMPTY] <C> [BOLD] 0.439 <C> 0.1339 <C> 0.2419 <C> 0.2085 <R> <C> nothing <C> [EMPTY] <C> [BOLD] 0.439 <C> 0.1339 <C> 0.2419 <C> 0.2085 <R> <C> [ITALIC] Majority Class Baseline <C> [EMPTY] <C> [BOLD] 0.830 <C> [ITALIC] 0.285 <C> [ITALIC] 0.333 <C> [ITALIC] 0.156 <CAP> Table 4: Subtask B: Results for answer classification based on the official submissions, evaluated on the test set.
<R> <C> [BOLD] Type <C> [BOLD] Training <C> [BOLD] Test <R> <C> Before <C> 11,981 <C> 10,488 <R> <C> Overlap <C> 7,276 <C> 5,694 <R> <C> After <C> 1,415 <C> 1,275 <R> <C> [BOLD] Total <C> 20,672 <C> 17,457 <CAP> Table 13: i2b2-TRC: TLINK IAA
<R> <C> [BOLD] TLINK <C> [ITALIC] Avg. [ITALIC] P& [ITALIC] R <C> [ITALIC] κ <R> <C> Span (strict) <C> 0.39 <C> - <R> <C> Span (lenient) <C> - <C> - <R> <C> 1-5 Type <C> 0.79 <C> 0.3 <CAP> Table 13: i2b2-TRC: TLINK IAA
<R> <C> [BOLD] Set No. <C> [ITALIC] Sim1 <C> [ITALIC] Sim2 <C> [ITALIC] BaseSim1 <C> [ITALIC] BaseSim2 <R> <C> 1 <C> 0.42625156 <C> 0.22251266 <C> 0.35704804 <C> 0.21033629 <R> <C> 2 <C> 0.39003456 <C> 0.23101839 <C> 0.33605672 <C> 0.20739048 <R> <C> 3 <C> 0.40847995 <C> 0.22473745 <C> 0.34865402 <C> 0.20686149 <R> <C> 4 <C> 0.40035646 <C> 0.2232119 <C> 0.34616759 <C> 0.21629227 <R> <C> 5 <C> 0.40785315 <C> 0.23263196 <C> 0.32013770 <C> 0.21621922 <R> <C> Average <C> 0.40664794 <C> 0.22682247 <C> 0.34161281 <C> 0.21141995 <CAP> TABLE I: Results
<R> <C> [BOLD] Sub-  [BOLD] corpus <C> [BOLD] A1 <C> [BOLD] A2 <C> [BOLD] B1 <C> [BOLD] B2 <C> [BOLD] C1 <C> [BOLD] Un-  [BOLD] known <C> [BOLD] Total <R> <C> [BOLD] Tisus <C> - <C> - <C> - <C> 27 <C> 78 <C> - <C> 105 <R> <C> [BOLD] Sw1203 <C> - <C> - <C> 33 <C> 45 <C> 11 <C> 1 <C> 90 <R> <C> [BOLD] SpIn <C> 16 <C> 83 <C> 42 <C> 2 <C> - <C> 1 <C> 144 <R> <C> [BOLD] Total <C> 16 <C> 83 <C> 75 <C> 74 <C> 89 <C> 2 <C> [BOLD] 339 <CAP> Table 3: Number of essays per CEFR level and subcorpus
<R> <C> Dataset <C> Metric <C> LR <C> PU-SVM <C> nnPU <C> RNN <C> T-LSTM <C> SMOTE [ITALIC] LR <C> SMOTE [ITALIC] RNN <C> RETAIN <C> Dipole <C> SSL GAN <C> medGAN <C> CONAN <R> <C> IBD <C> PR-AUC <C> 0.2765 <C> 0.5321 <C> 0.5682 <C> 0.4373 <C> 0.2241 <C> 0.3464 <C> 0.4471 <C> 0.3135 <C> 0.5417 <C> 0.6072 <C> 0.6385 <C> [BOLD] 0.9584 <R> <C> IBD <C> F1 Score <C> 0.3651 <C> 0.4982 <C> 0.4392 <C> 0.4332 <C> 0.3016 <C> 0.4341 <C> 0.4642 <C> 0.3594 <C> 0.5528 <C> 0.5416 <C> 0.5834 <C> [BOLD] 0.9601 <R> <C> IBD <C> Cohen’s Kappa <C> 0.3249 <C> 0.5123 <C> 0.4624 <C> 0.4440 <C> 0.2886 <C> 0.3451 <C> 0.4895 <C> 0.3106 <C> 0.5904 <C> 0.5453 <C> 0.4851 <C> [BOLD] 0.9595 <R> <C> IPF <C> PR-AUC <C> 0.0798 <C> 0.1141 <C> 0.1578 <C> 0.0090 <C> 0.0084 <C> 0.0406 <C> 0.0187 <C> 0.1016 <C> 0.1183 <C> 0.0206 <C> 0.0954 <C> [BOLD] 0.2229 <R> <C> IPF <C> F1 Score <C> 0.1529 <C> 0.0915 <C> 0.1682 <C> 0.0169 <C> 0.0211 <C> 0.0673 <C> 0.0293 <C> 0.1345 <C> 0.0969 <C> 0.0272 <C> 0.0729 <C> [BOLD] 0.2343 <R> <C> IPF <C> Cohen’s Kappa <C> 0.1369 <C> 0.0835 <C> 0.1397 <C> 0.0261 <C> 0.0752 <C> 0.0208 <C> 0.0429 <C> 0.1470 <C> 0.1060 <C> 0.0372 <C> 0.0612 <C> [BOLD] 0.2339 <CAP> Table 3: Performance Comparison on IPF (rare disease, prevalence rate 0.04%) and IBD (low prevalence disease, prevalence rate 0.44% ) datasets. CONAN outperforms all state-of-the-art baselines including GAN based and PU learning baselines.
<R> <C> Dataset <C> Metric <C> % Visits in Test Data 100% <C> % Visits in Test Data 50% <C> % Visits in Test Data 20% <R> <C> IBD <C> PR AUC <C> 0.9584 <C> 0.9474 <C> 0.7313 <R> <C> IBD <C> F1 Score <C> 0.9601 <C> 0.9531 <C> 0.7993 <R> <C> IBD <C> Cohen’s Kappa <C> 0.9595 <C> 0.9473 <C> 0.7629 <R> <C> IPF <C> PR AUC <C> 0.2229 <C> 0.2105 <C> 0.0843 <R> <C> IPF <C> F1 Score <C> 0.2343 <C> 0.2262 <C> 0.1024 <R> <C> IPF <C> Cohen’s Kappa <C> 0.2119 <C> 0.2056 <C> 0.0758 <CAP> Table 5: The results on early prediction indicated that we can use CONAN to predict patients’ conditions at an early stage.
<R> <C> Method <C> PR-AUC <C> F1 Score <C> Cohen’s Kappa <R> <C> LR <C> 0.5930 <C> 0.5471 <C> 0.5380 <R> <C> PU-SVM <C> 0.3971 <C> 0.3582 <C> 0.3619 <R> <C> nnPU <C> 0.4484 <C> 0.3824 <C> 0.4139 <R> <C> RNN <C> 0.1841 <C> 0.3739 <C> 0.4313 <R> <C> T-LSTM <C> 0.3446 <C> 0.1783 <C> 0.2448 <R> <C> SMOTE [ITALIC] LR <C> 0.6123 <C> 0.5732 <C> 0.5842 <R> <C> SMOTE [ITALIC] RNN <C> 0.2091 <C> 0.4195 <C> 0.4485 <R> <C> RETAIN <C> 0.5641 <C> 0.4627 <C> 0.4506 <R> <C> Dipole <C> 0.5929 <C> 0.5221 <C> 0.5477 <R> <C> SSL GAN <C> 0.5397 <C> 0.5340 <C> 0.4883 <R> <C> medGAN <C> 0.4825 <C> 0.3986 <C> 0.3822 <R> <C> CONAN <C> [BOLD] 0.6186 <C> [BOLD] 0.6481 <C> [BOLD] 0.5958 <CAP> Table 7: Performance Comparison on NASH dataset.
<R> <C> Sys. <C> Maas’11  <C> Johnson’14  <C> Johnson’15  <C> EntityMNN % <R> <C> Acc. <C> 89 <C> 93.4 <C> 95 <C> 97.2 <CAP> Table 2: Results on Large Movie Dataset
<R> <C> [BOLD] Model <C> [BOLD] MRR <C> [BOLD] R@1 <C> [BOLD] R@5 <C> [BOLD] R@10 <C> [BOLD] Mean <R> <C> Answer Prior  <C> 0.3735 <C> 23.55 <C> 48.52 <C> 53.23 <C> 26.50 <R> <C> NN  <C> 0.4274 <C> 33.13 <C> 50.83 <C> 58.69 <C> 19.62 <R> <C> LF  <C> 0.5199 <C> 41.83 <C> 61.78 <C> 67.59 <C> 17.07 <R> <C> HRE  <C> 0.5237 <C> 42.29 <C> 62.18 <C> 67.92 <C> 17.07 <R> <C> HREA  <C> 0.5242 <C> 42.28 <C> 62.33 <C> 68.17 <C> 16.79 <R> <C> MN  <C> 0.5259 <C> 42.29 <C> 62.85 <C> 68.88 <C> 17.06 <R> <C> HCIAE  <C> 0.5386 <C> 44.06 <C> 63.55 <C> 69.24 <C> 16.01 <R> <C> CoAtt-G-MLE <C> 0.5411 <C> 44.32 <C> 63.82 <C> 69.75 <C> 16.47 <R> <C> CoAtt-GAN-w/o  [BOLD] R [ITALIC] inte <C> 0.5415 <C> 44.52 <C> 64.17 <C> 70.31 <C> 16.28 <R> <C> CoAtt-GAN-w/  [BOLD] R [ITALIC] inte <C> 0.5506 <C> 45.56 <C> 65.16 <C> 71.07 <C> 15.30 <R> <C> CoAtt-GAN-w/  [BOLD] R [ITALIC] inte-TF <C> [BOLD] 0.5578 <C> [BOLD] 46.10 <C> [BOLD] 65.69 <C> [BOLD] 71.74 <C> [BOLD] 14.43 <CAP> Table 1: Performance of generative methods on VisDial v0.9. Higher is better for MRR and recall@k, while lower is better for mean rank.
<R> <C> Methods <C> Dev Error Mean <C> Dev Error Med. <C> Test Error Mean <C> Test Error Med. <R> <C> Human <C> 0.35 <C> 0.30 <C> 0.37 <C> 0.31 <R> <C> Initial <C> 5.95 <C> 5.71 <C> 6.23 <C> 6.12 <R> <C> Random <C> 15.3 <C> 15.70 <C> 15.11 <C> 15.35 <R> <C> [BOLD] Misra el al. <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Ensem-LfD <C> 4.64 <C> 4.27 <C> 4.95 <C> 4.53 <R> <C> Ensem-DQN <C> 5.85 <C> 5.59 <C> 6.15 <C> 5.97 <R> <C> Ensem-REIN <C> 5.28 <C> 5.23 <C> 5.69 <C> 5.57 <R> <C> Ensem-BEST <C> 3.59 <C> 3.03 <C> 3.78 <C> 3.14 <R> <C> [BOLD] Our Models <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> S-REIN <C> 2.94 <C> 2.23 <C> 2.95 <C> 2.21 <R> <C> S-A2C <C> 2.79 <C> 2.21 <C> 2.75 <C> 2.18 <R> <C> S-PPO <C> [BOLD] 1.69 <C> [BOLD] 0.99 <C> [BOLD] 1.71 <C> [BOLD] 1.04 <CAP> Table 1: Performance (mean and median of execution errors) of our scheduled policy optimization and baselines. The numbers of the baselines are from [Misra et al.2017].
<R> <C> [EMPTY] <C> SST-2 <C> SST-5 <R> <C> [ITALIC] Sequential sentence representation <C> [ITALIC] Sequential sentence representation <C> [ITALIC] Sequential sentence representation <R> <C> Radford et al. ( 2017 ) <C> [BOLD] 91.8 <C> 52.9 <R> <C> McCann et al. ( 2017 ) <C> 90.3 <C> 53.7 <R> <C> Peters et al. ( 2018 ) <C> - <C> [BOLD] 54.7 <R> <C> [ITALIC] RvNN based models with external tree <C> [ITALIC] RvNN based models with external tree <C> [ITALIC] RvNN based models with external tree <R> <C> Socher et al. ( 2013 ) <C> 85.4 <C> 45.7 <R> <C> Tai et al. ( 2015 ) <C> 88.0 <C> 51.0 <R> <C> Munkhdalai and Yu ( 2017 ) <C> 89.3 <C> 53.1 <R> <C> Looks et al. ( 2017 ) <C> 89.4 <C> 52.3 <R> <C> [ITALIC] RvNN based models with latent tree <C> [ITALIC] RvNN based models with latent tree <C> [ITALIC] RvNN based models with latent tree <R> <C> Yogatama et al. ( 2016 ) <C> 86.5 <C> - <R> <C> Choi et al. ( 2018 ) <C> 90.7 <C> 53.7 <R> <C> Choi et al. ( 2018 )∗ <C> 90.3±0.5 <C> 51.6±0.8 <R> <C> Ours <C> 90.2±0.2 <C> 51.5±0.4 <CAP> Table 5: Accuracy results of models on the SST. All the numbers are from Choi et al. (2018) but ∗ where we used their publicly available code and performed hyperparameter optimization.
<R> <C> [EMPTY] <C> EER (%) max1 <C> EER (%) max2 <C> minDCF (%) max1 <C> minDCF (%) max2 <R> <C> FEFA <C> 4.24 <C> [BOLD] 3.86 <C> 6.24 <C> 6.85 <R> <C> PPCA <C> 4.66 <C> 4.66 <C> 6.55 <C> 6.72 <R> <C> FA <C> 4.24 <C> 4.24 <C> 6.00 <C> [BOLD] 5.50 <R> <C> PPLS <C> 4.66 <C> 4.79 <C> 6.73 <C> 6.53 <R> <C> SPPCA <C> 4.46 <C> 4.38 <C> 6.46 <C> 6.26 <CAP> Table 2: Speaker verification performances for different methods and maximization principles (max1, max2) on common condition 5 of SRE10. With conventional PCA we obtained EER of 4.69% and minDCF of 6.55%.
<R> <C> [BOLD] System <C> [BOLD] R-1 <C> [BOLD] R-2 <C> [BOLD] R-SU4 <R> <C> Lead <C> 0.384 <C> 0.110 <C> 0.144 <R> <C> TextRank <C> 0.402 <C> 0.122 <C> 0.159 <R> <C> LexRank <C> 0.425 <C> 0.135 <C> 0.165 <R> <C> Centroid <C> 0.402 <C> 0.141 <C> 0.171 <R> <C> Concept <C> 0.422 <C> 0.149 <C> 0.177 <R> <C> RA-Sparse <C> 0.442 <C> 0.157 <C> 0.188 <R> <C> RAVAESum <C> [BOLD] 0.443* <C> [BOLD] 0.171* <C> [BOLD] 0.196* <CAP> Table 1: Summarization performance.
<R> <C> [BOLD] System <C> [BOLD] R-1 <C> [BOLD] R-2 <C> [BOLD] R-SU4 <R> <C> RAVAESum-noC <C> 0.437 <C> 0.162 <C> 0.189 <R> <C> RAVAESum <C> [BOLD] 0.443* <C> [BOLD] 0.171* <C> [BOLD] 0.196* <CAP> Table 2: Further investigation of RAVAESum.
<R> <C> Model <C> Sentence <C> Single-token <R> <C> [EMPTY] <C> Prompt <C> Prompt <R> <C> Baseline <C> 0.3017654 <C> 0.1928994 <R> <C> Sentiment Tuned <C> 0.2333209 <C> 0.1759286 <CAP> Table 2: Average negativity score of generated sentences before and after training.
<R> <C> Model <C> pOOV Test Sentences <C> pOOV Test Change-rate <C> Scores no-pOOV-transl <C> Scores pOOV-transl <C> Scores Delta [confidence] <R> <C> T5[En]+(TA)5[EnEs]+T5[Es] <C> 1010 <C> 25.5% <C> 2.67 <C> 2.98 <C> +0.32 [0.19, 0.42] <R> <C> T5[En]+(TA)5[EnFr]+T5[Fr] <C> 1001 <C> 62.2% <C> 2.81 <C> 2.96 <C> +0.14 [0.06, 0.22] <R> <C> T5[En]+(TA)5[EnCz]+T5[Cz] <C> 1004 <C> 20.7% <C> 1.94 <C> 2.19 <C> +0.25 [0.13, 0.36] <CAP> Table 9: Translation performance of multigraph bilingual models, using test sets from the English Gigaword containing sentences with parallel-OOV (pOOV) terms; compares base condition (no pOOV translation) against test condition (pOOV translations). The evaluation uses a scale from 0:useless to 6:perfect. For instance, Example #2 gets a score of 3 for the Base translation and a 5 for the Test-condition translation.
<R> <C> [BOLD] Approach <C> [BOLD] No. of Sentence pairs <C> [BOLD] Precision <R> <C> Verb Relationships <C> 46 <C> 0.609 <R> <C> Sentiment Polarity <C> 230 <C> 0.382 <R> <C> Inconsistencies between triples <C> 95 <C> 0.273 <CAP> Table 6. Results Comparison of Approaches used to detect Shift-in-View
<R> <C> [BOLD] Method <C> [BOLD] Overall PPL <C> [BOLD] First word’s PPL <C> [BOLD] Subsequent words’ PPL <R> <C> Sequential LM <C> 152.2 <C> 416.2 <C> 134.8 <R> <C> Info-init <C> 148.7 <C> 371.5 <C> 133.3 <R> <C> Info-all <C> 125.4 <C> 328.0 <C> 121.8 <R> <C> sep-B/F <C> 192.4 <C> 556.1 <C> 169.9 <R> <C> sep-B/F ( [ITALIC] ws oracle) <C> 99.2 <C> – <C> – <R> <C> syn-B/F <C> 185.4 <C> 592.7 <C> 162.9 <R> <C> syn-B/F ( [ITALIC] ws oracle) <C> 97.5 <C> – <C> – <R> <C> asyn-B/F <C> 177.2 <C> 584.5 <C> 153.7 <R> <C> asyn-B/F ( [ITALIC] ws oracle) <C> [BOLD] 89.8 <C> – <C> – <CAP> Table 1: Perplexity (PPL) of our B/F LMs and baselines.
<R> <C> [EMPTY] <C> [ITALIC] l <C> FPA mul1 <C> FPA mul2 <C> FPA mul3 <C> FPA mem <C> [ITALIC] L, nats mul1 <C> [ITALIC] L, nats mul2 <C> [ITALIC] L, nats mul3 <C> [ITALIC] L, nats mem <R> <C> LSTM-s2s no att. <C> 20 <C> 0.00 <C> 0.01 <C> 0.78 <C> 0.00 <C> 52.27 <C> 22.20 <C> [BOLD] 1.17∗ <C> 77.75 <R> <C> [EMPTY] <C> 15 <C> 0.00 <C> 0.13 <C> 0.45 <C> 0.00 <C> 40.46 <C> 13.22 <C> [BOLD] 6.14∗ <C> 66.10 <R> <C> [EMPTY] <C> 10 <C> 0.00 <C> 0.92 <C> 0.00 <C> 0.00 <C> 26.48 <C> [BOLD] 0.65∗ <C> 22.26 <C> 53.81 <R> <C> [EMPTY] <C> 5 <C> 0.49 <C> 0.00 <C> 0.00 <C> 0.00 <C> [BOLD] 1.97∗ <C> 26.50 <C> 54.97 <C> 36.13 <R> <C> LSTM-s2s att. <C> 20 <C> 0.00 <C> 0.19 <C> 0.62 <C> 0.00 <C> 36.76 <C> 20.35 <C> [BOLD] 9.35∗ <C> 49.34 <R> <C> [EMPTY] <C> 15 <C> 0.00 <C> 0.14 <C> 0.76 <C> 0.00 <C> 37.84 <C> 18.42 <C> [BOLD] 5.53∗ <C> 56.43 <R> <C> [EMPTY] <C> 10 <C> 0.02 <C> 0.45 <C> 0.49 <C> 0.00 <C> 29.83 <C> 11.67 <C> [BOLD] 8.96 <C> 45.47 <R> <C> [EMPTY] <C> 5 <C> 0.01 <C> 0.03 <C> 0.00 <C> 0.64 <C> 32.97 <C> 48.26 <C> 60.38 <C> [BOLD] 2.44∗ <R> <C> CNN-s2s <C> 20 <C> 0.00 <C> 0.00 <C> 0.00 <C> 0.99 <C> 263.82 <C> 262.01 <C> 261.71 <C> [BOLD] 0.02∗ <R> <C> [EMPTY] <C> 15 <C> 0.00 <C> 0.00 <C> 0.00 <C> 1.00 <C> 250.97 <C> 253.32 <C> 253.08 <C> [BOLD] 0.00∗ <R> <C> [EMPTY] <C> 10 <C> 0.00 <C> 0.00 <C> 0.00 <C> 1.00 <C> 243.17 <C> 245.16 <C> 248.28 <C> [BOLD] 0.00∗ <R> <C> [EMPTY] <C> 5 <C> 0.00 <C> 0.00 <C> 0.00 <C> 1.00 <C> 258.10 <C> 257.79 <C> 264.06 <C> [BOLD] 0.00∗ <R> <C> Transformer <C> 20 <C> 0.00 <C> 0.00 <C> 0.00 <C> 0.97 <C> 37.90 <C> 51.47 <C> 57.57 <C> [BOLD] 5.31∗ <R> <C> [EMPTY] <C> 15 <C> 0.00 <C> 0.00 <C> 0.00 <C> 1.00 <C> 40.36 <C> 51.62 <C> 57.42 <C> [BOLD] 2.50∗ <R> <C> [EMPTY] <C> 10 <C> 0.00 <C> 0.00 <C> 0.00 <C> 1.00 <C> 38.05 <C> 49.88 <C> 55.61 <C> [BOLD] 2.47∗ <R> <C> [EMPTY] <C> 5 <C> 0.00 <C> 0.00 <C> 0.00 <C> 1.00 <C> 37.96 <C> 51.83 <C> 60.19 <C> [BOLD] 0.74∗ <CAP> Table 2: Multiplication by 3. FPA measures the fraction of seeds that generalize according to a particular rule. Description length L is averaged across examples and seeds. The lowest L are in bold and ∗ denotes stat. sig. difference in L (p<10−3, paired t-test).
<R> <C> [EMPTY] <C> [ITALIC] dropout <C> FPA count <C> FPA mem <C> [ITALIC] L, nats count <C> [ITALIC] L, nats mem <R> <C> LSTM-s2s no att. <C> 0.0 <C> 0.00 <C> 0.20 <C> 56.23 <C> [BOLD] 16.52∗ <R> <C> [EMPTY] <C> 0.2 <C> 0.95 <C> 0.00 <C> [BOLD] 0.17∗ <C> 60.15 <R> <C> [EMPTY] <C> 0.5 <C> 1.00 <C> 0.00 <C> [BOLD] 0.01∗ <C> 97.51 <R> <C> LSTM-s2s att. <C> 0.0 <C> 0.32 <C> 0.68 <C> 63.30 <C> [BOLD] 47.68 <R> <C> [EMPTY] <C> 0.2 <C> 0.95 <C> 0.05 <C> [BOLD] 33.66∗ <C> 87.36 <R> <C> [EMPTY] <C> 0.0 <C> 0.99 <C> 0.00 <C> [BOLD] 7.84∗ <C> 121.48 <R> <C> CNN-s2s <C> 0.0 <C> 0.00 <C> 0.55 <C> 1034.67 <C> [BOLD] 0.43∗ <R> <C> [EMPTY] <C> 0.2 <C> 0.00 <C> 0.98 <C> 999.62 <C> [BOLD] 0.01∗ <R> <C> [EMPTY] <C> 0.5 <C> 0.00 <C> 0.98 <C> 660.73 <C> [BOLD] 0.02∗ <R> <C> Transformer <C> 0.0 <C> 0.00 <C> 0.65 <C> 261.02 <C> [BOLD] 1.17∗ <R> <C> [EMPTY] <C> 0.2 <C> 0.00 <C> 1.00 <C> 171.31 <C> [BOLD] 0.05∗ <R> <C> [EMPTY] <C> 0.5 <C> 0.00 <C> 0.97 <C> 116.34 <C> [BOLD] 11.10∗ <CAP> (a) Count-or-Memorization
<R> <C> [EMPTY] <C> [ITALIC] dropout <C> FPA add <C> FPA mul <C> FPA mem <C> [ITALIC] L, nats add <C> [ITALIC] L, nats mul <C> [ITALIC] L, nats mem <R> <C> LSTM-s2s no att. <C> 0.0 <C> 0.25 <C> 0.00 <C> 0.00 <C> [BOLD] 5.00∗ <C> 35.55 <C> 19.07 <R> <C> [EMPTY] <C> 0.2 <C> 0.30 <C> 0.45 <C> 0.00 <C> 12.07 <C> [BOLD] 11.04 <C> 37.39 <R> <C> [EMPTY] <C> 0.5 <C> 0.00 <C> 0.94 <C> 0.00 <C> 25.42 <C> [BOLD] 0.31∗ <C> 57.32 <R> <C> LSTM-s2s att. <C> 0.0 <C> 0.00 <C> 0.11 <C> 0.58 <C> 25.09 <C> 36.33 <C> [BOLD] 12.09 <R> <C> [EMPTY] <C> 0.2 <C> 0.18 <C> 0.53 <C> 0.18 <C> 22.22 <C> [BOLD] 9.92∗ <C> 34.48 <R> <C> [EMPTY] <C> 0.5 <C> 0.00 <C> 0.98 <C> 0.00 <C> 30.26 <C> [BOLD] 1.40∗ <C> 58.84 <R> <C> CNN-s2s <C> 0.0 <C> 0.00 <C> 0.00 <C> 0.65 <C> 236.41 <C> 247.52 <C> [BOLD] 0.42∗ <R> <C> [EMPTY] <C> 0.2 <C> 0.00 <C> 0.00 <C> 1.00 <C> 438.06 <C> 464.26 <C> [BOLD] 0.00∗ <R> <C> [EMPTY] <C> 0.5 <C> 0.00 <C> 0.00 <C> 1.00 <C> 318.12 <C> 346.19 <C> [BOLD] 0.00∗ <R> <C> Transformer <C> 0.0 <C> 0.00 <C> 0.00 <C> 0.65 <C> 84.58 <C> 130.88 <C> [BOLD] 0.96∗ <R> <C> [EMPTY] <C> 0.2 <C> 0.00 <C> 0.00 <C> 1.00 <C> 65.62 <C> 99.05 <C> [BOLD] 0.02∗ <R> <C> [EMPTY] <C> 0.5 <C> 0.00 <C> 0.00 <C> 1.00 <C> 38.77 <C> 50.64 <C> [BOLD] 3.50∗ <CAP> (b) Add-or-Multiply
<R> <C> [EMPTY] <C> [ITALIC] dropout <C> FPA hierar <C> FPA linear <C> [ITALIC] L, nats hierar <C> [ITALIC] L, nats linear <R> <C> LSTM-s2s no att. <C> 0.0 <C> 0.00 <C> 0.00 <C> [BOLD] 16.38 <C> 17.72 <R> <C> [EMPTY] <C> 0.2 <C> 0.00 <C> 0.00 <C> [BOLD] 11.09∗ <C> 19.87 <R> <C> [EMPTY] <C> 0.5 <C> 0.05 <C> 0.00 <C> [BOLD] 7.76∗ <C> 15.46 <R> <C> LSTM-s2s att. <C> 0.0 <C> 0.00 <C> 0.00 <C> 31.12 <C> [BOLD] 28.61 <R> <C> [EMPTY] <C> 0.2 <C> 0.00 <C> 0.00 <C> [BOLD] 10.63 <C> 17.55 <R> <C> [EMPTY] <C> 0.5 <C> 0.30 <C> 0.00 <C> [BOLD] 6.58∗ <C> 14.30 <R> <C> CNN-s2s <C> 0.0 <C> 0.00 <C> 0.75 <C> 68.48 <C> [BOLD] 0.44∗ <R> <C> [EMPTY] <C> 0.2 <C> 0.00 <C> 1.00 <C> 99.42 <C> [BOLD] 1.16∗ <R> <C> [EMPTY] <C> 0.5 <C> 0.00 <C> 1.00 <C> 50.66 <C> [BOLD] 0.00∗ <R> <C> Transformer <C> 0.0 <C> 0.05 <C> 0.00 <C> [BOLD] 3.99∗ <C> 8.56 <R> <C> [EMPTY] <C> 0.2 <C> 1.00 <C> 0.00 <C> [BOLD] 0.09∗ <C> 13.09 <R> <C> [EMPTY] <C> 0.5 <C> 0.69 <C> 0.00 <C> [BOLD] 1.21∗ <C> 8.76 <CAP> (c) Hierarchical-or-Linear
<R> <C> [BOLD] Model <C> BLEU-2 GloVe <C> BLEU-2  [BOLD] ST <C> BLEU-3 GloVe <C> BLEU-3  [BOLD] ST <C> METEOR GloVe <C> METEOR  [BOLD] ST <C> ROUGE GloVe <C> ROUGE  [BOLD] ST <R> <C> [BOLD] GAN <C> 0.710 <C> 0.745 <C> 0.593 <C> 0.607 <C> 0.667 <C> 0.670 <C> 0.654 <C> 0.649 <R> <C> [BOLD] WGAN <C> 0.786 <C> 0.833 <C> 0.645 <C> 0.669 <C> 0.681 <C> 0.681 <C> 0.681 <C> 0.675 <R> <C> [BOLD] WGAN-GP <C> 0.807 <C> 0.836 <C> 0.668 <C> 0.682 <C> 0.694 <C> 0.692 <C> 0.702 <C> 0.731 <CAP> Table 2: BLEU-2, BLEU-3 METEOR and ROUGE metric scores across GAN models with different f-measures. GloVe: GLoVe Average, ST: Skip-Thought, WGAN: Wasserstein GAN, GP: Gradient Penalty
<R> <C> [EMPTY] <C> WebEdit <C> RotoEdit <R> <C> Table-to-Text <C> [BOLD] 4,083 <C> [BOLD] 1,834 <R> <C> Text-to-Text <C> 2,751 <C> 581 <R> <C> EncDecEditor <C> 2,487 <C> 505 <R> <C> FactEditor <C> 3,295 <C> 1,412 <CAP> Table 8: Runtime analysis (# of words/second). Table-to-Text always shows the fastest performance (Bold-faced). FactEditor shows the second fastest runtime performance (Underlined).
<R> <C> [BOLD] Visual vs. Non-Visual  [BOLD] rule-based <C> [BOLD] Visual vs. Non-Visual  [BOLD] lstm <C> [BOLD] True- vs. False-Premise  [BOLD] entropy <C> [BOLD] True- vs. False-Premise  [BOLD] vqa-mlp <C> [BOLD] True- vs. False-Premise  [BOLD] q-gen score <C> [BOLD] True- vs. False-Premise  [BOLD] q-c sim <C> [BOLD] True- vs. False-Premise  [BOLD] q-q’ sim <R> <C> 75.68 <C> [BOLD] 92.27 <C> 59.66 <C> 64.19 <C> 57.41 <C> 74.48 <C> [BOLD] 74.58 <CAP> Table 1: Normalized accuracy results (averaged over 40 random train/test splits) for visual vs. non-visual detection and true- vs. false-premise detection. rule-based and q-gen score were not averaged because they are deterministic.
<R> <C> Methods <C> Accuracy↑ <C> PPL↓ <C> Overlap↑ <C> Noun%↑ <C> Len% <C> Key%↑ <R> <C> Original <C> 0.1 <C> 22.9 <C> 100.0 <C> 100.0 <C> 100.0 <C> 7.8 <R> <C> Keywords <C> 16.7 <C> 43.9 <C> [BOLD] 39.2 <C> 56.0 <C> 98.1 <C> [BOLD] 92.3 <R> <C> Sentiment + Keywords <C> 91.6 <C> 52.6 <C> 24.5 <C> 42.4 <C> 106.0 <C> 78.3 <R> <C> Length⇑ <C> 0.2 <C> 29.8 <C> 25.0 <C> 48.3 <C> [BOLD] 208.8 <C> 5.9 <R> <C> Sentiment + Length⇑ <C> [BOLD] 97.7 <C> 25.4 <C> 21.4 <C> 51.7 <C> 189.5 <C> 9.2 <R> <C> Keywords + Length⇑ <C> 25.6 <C> 44.5 <C> 29.8 <C> [BOLD] 61.8 <C> 165.0 <C> 83.2 <R> <C> Sentiment + Keywords + Length⇑ <C> 93.0 <C> 51.8 <C> 18.8 <C> 50.0 <C> 183.7 <C> 66.6 <R> <C> Length⇓ <C> 0.2 <C> 31.3 <C> 30.7 <C> 25.2 <C> [BOLD] 40.8 <C> 6.3 <R> <C> Sentiment + Length⇓ <C> 95.1 <C> [BOLD] 23.0 <C> 29.1 <C> 38.1 <C> 66.9 <C> 6.7 <R> <C> Keywords + Length⇓ <C> 21.4 <C> 87.0 <C> 28.4 <C> 38.9 <C> 61.6 <C> 83.7 <R> <C> Sentiment + Keywords + Length⇓ <C> 87.6 <C> 123.8 <C> 16.3 <C> 23.7 <C> 60.9 <C> 63.0 <CAP> Table 5: Results of fine-grained Attributes control on the Yelp. Different rows correspond to the set of attributes being controlled by the model.
<R> <C> [EMPTY] <C> 3CosAdd <C> 3CosMul <R> <C> GloVe <C> 0.0137 <C> 0.0040 <R> <C> DCT ( [ITALIC] k=6) <C> 0.6667 <C> 0.2370 <R> <C> SkipThought <C> 0.7308 <C> 0.0768 <R> <C> QuickThought <C> 0.5785 <C> 0.2140 <R> <C> InferSentV1 <C> 0.2212 <C> 0.0309 <R> <C> InferSentV2 <C> 0.1263 <C> 0.0087 <R> <C> GenSen <C> 0.2122 <C> 0.0023 <R> <C> USE-DAN <C> 0.2475 <C> 0.0606 <R> <C> USE-Transformer <C> 0.1560 <C> 0.0165 <R> <C> BERT-Base-AVG <C> 0.4415 <C> 0.3744 <R> <C> BERT-Large-AVG <C> 0.6896 <C> 0.4335 <R> <C> XLNet-Base-CLS <C> 0.4700 <C> 0.4635 <R> <C> XLNet-Large-CLS <C> 0.3715 <C> 0.3700 <R> <C> RoBERTa-Base-CLS <C> 0.8703 <C> 0.8704 <R> <C> RoBERTa-Large-CLS <C> 0.5781 <C> 0.5782 <R> <C> SBERT-Base-AVG <C> 0.1781 <C> 0.0363 <R> <C> SBERT-Base-CLS <C> 0.1983 <C> 0.0396 <R> <C> SBERT-Large-AVG <C> 0.2238 <C> 0.0422 <R> <C> SBERT-Large-CLS <C> 0.2528 <C> 0.0562 <R> <C> SRoBERTa-Base-AVG <C> 0.2207 <C> 0.0454 <R> <C> SRoBERTa-Large-AVG <C> 0.2708 <C> 0.0591 <CAP> Table 6: Experimental results on entailment sentence analogy pairs
<R> <C> (A)-(B) <C> (A)-(C) <C> (B)-(C) <R> <C> 0.43 <C> 0.25 <C> 0.29 <CAP> Table 3: Graph similarity between the graphs depicted in Fig. 5
<R> <C> Inputs to  [ITALIC] Middle Fusion <C> Perplexity YouCook2 <C> Perplexity sth-sth <R> <C> text + video <C> 64.9 <C> 411.4 <R> <C> text + zero vectors <C> 99.0 <C> 537.7 <CAP> Table 2: Withholding visual context from our best model leads to worse performance (similar to an RNNLM trained only on text).
<R> <C> model <C> [ITALIC] α <C> [ITALIC] d <C> [ITALIC] nA <C> [ITALIC] dF <C> [ITALIC] L <C> #parameters <C> #samples <R> <C> baseline <C> 1 <C> 48 <C> 4 <C> 1024 <C> 12 <C> 248M <C> 5.3M <R> <C> sampled-softmax <C> 1 <C> 512 <C> 8 <C> 2048 <C> 12 <C> 2.6G <C> 128K <R> <C> superbloom <C> 50 <C> 768 <C> 12 <C> 3072 <C> 12 <C> 229M <C> 200K <CAP> Table 1: Model parameters. “#samples” lists the number of samples in the softmax loss computation. For baseline and superbloom, since there is no sampling, this number corresponds to the full vocabulary, 5.3M and 200K, respectively. For sampled-softmax, we use 128K samples.
<R> <C> model <C> [ITALIC] α <C> [ITALIC] d <C> [ITALIC] nA <C> [ITALIC] dF <C> [ITALIC] L <C> #parameters <C> rec@1 <C> rec@10 <C> rec@20 <R> <C> baseline-l1 <C> 1 <C> 256 <C> 1 <C> 1024 <C> 1 <C> 123M <C> 51.0% <C> 70.4% <C> 75.5% <R> <C> baseline-l12 <C> 1 <C> 256 <C> 8 <C> 1024 <C> 12 <C> 132M <C> 55.0% <C> 73.7% <C> 77.3% <R> <C> superbloom-d256l1 <C> 20 <C> 256 <C> 1 <C> 1024 <C> 1 <C> 13M <C> 17.8% <C> 35.8% <C> 42.6% <R> <C> superbloom-d384l1 <C> 20 <C> 384 <C> 1 <C> 1536 <C> 1 <C> 21M <C> 30.6% <C> 52.9% <C> 58.7% <R> <C> superbloom-d256l12 <C> 20 <C> 256 <C> 8 <C> 1024 <C> 12 <C> 21M <C> 43.4% <C> 60.1% <C> 64.0% <CAP> Table 3: Model parameters and recall metrics.
<R> <C> Methods <C> Accuracy (%) <R> <C> Siamese CNN Wang et al. ( 2016a ) <C> 79.60 <R> <C> Multi-Perspective CNN Wang et al. ( 2017 ) <C> 81.38 <R> <C> Siamese LSTM Wang et al. ( 2016a ) <C> 82.58 <R> <C> Multi-Perspective LSTM Wang et al. ( 2017 ) <C> 83.21 <R> <C> L.D.C. Wang et al. ( 2016b ) <C> 84.75 ± 0.42 <R> <C> Our Model <C> [BOLD] 85.53 ± 0.18 ∗ <CAP> Table 1: Performance Evaluation on “Quora Question Pairs”.
<R> <C> Language <C> Vocabulary size ( [ITALIC] V(ℓ)) <R> <C> English (EN) <C> 29823 <R> <C> German (DE) <C> 60937 <R> <C> French (FR) <C> 37164 <R> <C> Italian (IT) <C> 44300 <R> <C> Spanish (ES) <C> 44724 <CAP> Table 1: Vocabulary size in each language.
<R> <C> [EMPTY] <C> [BOLD] WS-Sim  [ITALIC] r <C> [BOLD] WS-Sim  [ITALIC] ρ <C> [BOLD] RG-65  [ITALIC] r <C> [BOLD] RG-65  [ITALIC] ρ <R> <C> [ITALIC] Shallow <C> [BOLD] 0.72 <C> [BOLD] 0.71 <C> [BOLD] 0.71 <C> [BOLD] 0.74 <R> <C> Babelfy <C> 0.65 <C> 0.63 <C> 0.69 <C> 0.70 <R> <C> Babelfy* <C> 0.63 <C> 0.61 <C> 0.65 <C> 0.64 <CAP> Table 2: Pearson (r) and Spearman (ρ) correlation performance of SW2V integrating our shallow word-sense connectivity algorithm (default), Babelfy, or Babelfy*.
<R> <C> [EMPTY] <C> [BOLD] System <C> [BOLD] Corpus <C> [BOLD] SimLex-999  [ITALIC] r <C> [BOLD] SimLex-999  [ITALIC] ρ <C> [BOLD] MEN  [ITALIC] r <C> [BOLD] MEN  [ITALIC] ρ <R> <C> [BOLD] Senses <C> SW2VBN <C> UMBC <C> [BOLD] 0.49 <C> [BOLD] 0.47 <C> 0.75 <C> 0.75 <R> <C> [BOLD] Senses <C> SW2VWN <C> UMBC <C> 0.46 <C> 0.45 <C> [BOLD] 0.76 <C> [BOLD] 0.76 <R> <C> [BOLD] Senses <C> AutoExtend <C> UMBC <C> 0.47 <C> 0.45 <C> 0.74 <C> 0.75 <R> <C> [BOLD] Senses <C> AutoExtend <C> Google-News <C> 0.46 <C> 0.46 <C> 0.68 <C> 0.70 <R> <C> [BOLD] Senses <C> SW2VBN <C> Wikipedia <C> 0.47 <C> 0.43 <C> 0.71 <C> 0.73 <R> <C> [BOLD] Senses <C> SW2VWN <C> Wikipedia <C> 0.47 <C> 0.43 <C> 0.71 <C> 0.72 <R> <C> [BOLD] Senses <C> SensEmbed <C> Wikipedia <C> 0.43 <C> 0.39 <C> 0.65 <C> 0.70 <R> <C> [BOLD] Senses <C> Chen et al. (2014) <C> Wikipedia <C> 0.46 <C> 0.43 <C> 0.62 <C> 0.62 <R> <C> [BOLD] Words <C> Word2vec <C> UMBC <C> 0.39 <C> 0.39 <C> 0.75 <C> 0.75 <R> <C> [BOLD] Words <C> RetrofittingBN <C> UMBC <C> 0.47 <C> 0.46 <C> 0.75 <C> [BOLD] 0.76 <R> <C> [BOLD] Words <C> RetrofittingWN <C> UMBC <C> 0.47 <C> 0.46 <C> [BOLD] 0.76 <C> [BOLD] 0.76 <R> <C> [BOLD] Words <C> Word2vec <C> Wikipedia <C> 0.39 <C> 0.38 <C> 0.71 <C> 0.72 <R> <C> [BOLD] Words <C> RetrofittingBN <C> Wikipedia <C> 0.35 <C> 0.32 <C> 0.66 <C> 0.66 <R> <C> [BOLD] Words <C> RetrofittingWN <C> Wikipedia <C> 0.47 <C> 0.44 <C> 0.73 <C> 0.73 <CAP> Table 3: Pearson (r) and Spearman (ρ) correlation performance on the SimLex-999 and MEN word similarity datasets.
<R> <C> [EMPTY] <C> [BOLD] Accuracy <C> [BOLD] F-Measure <R> <C> SW2V <C> [BOLD] 87.8 <C> [BOLD] 63.9 <R> <C> SensEmbed <C> 82.7 <C> 40.3 <R> <C> NASARI <C> 87.0 <C> 62.5 <R> <C> Multi-SVM <C> 85.5 <C> - <R> <C> Mono-SVM <C> 83.5 <C> - <R> <C> Baseline <C> 17.5 <C> 29.8 <CAP> Table 4: Accuracy and F-Measure percentages of different systems on the SemEval Wikipedia sense clustering dataset.
<R> <C> [EMPTY] <C> [BOLD] SemEval-07 <C> [BOLD] SemEval-13 <R> <C> SW2V <C> [BOLD] 39.9 <C> [BOLD] 54.0 <R> <C> AutoExtend <C> 17.6 <C> 31.0 <R> <C> Baseline <C> 24.8 <C> 34.9 <CAP> Table 5: F-Measure percentage of different MCS strategies on the SemEval-2007 and SemEval-2013 WSD datasets.
<R> <C> [EMPTY] <C> Yelp 2013 <C> Yelp 2014 <C> Electronics <C> Video Games <C> Gourmet Foods <R> <C> JMARS  <C> 0.970 <C> 0.998 <C> 1.244 <C> 1.133 <C> 1.114 <R> <C> ConvMF+  <C> 0.917 <C> 0.954 <C> 1.241 <C> 1.092 <C> 1.084 <R> <C> NARRE  <C> 0.879 <C> 0.913 <C> 1.215 <C> 1.112 <C> 0.986 <R> <C> TARMF  <C> [BOLD] 0.875 <C> 0.909 <C> 1.147 <C> 1.043 <C> 1.019 <R> <C> [BOLD] HALF <C> [BOLD] 0.875 <C> [BOLD] 0.903 <C> [BOLD] 1.097 <C> [BOLD] 1.016 <C> [BOLD] 0.947 <CAP> Table 1: Results of our HALF model and baseline methods in terms of MSE.
<R> <C> predictor <C> coeff. ( [ITALIC] β×10) <C> std err ( [ITALIC] β) <C> t-stat <C> pval <R> <C> coeff <C> 5.589 <C> 0.003 <C> 202.396 <C> <0.001 <R> <C> PHY <C> 0.086 <C> 0.003 <C> 2.587 <C> 0.005 <R> <C> GEO <C> 0.087 <C> 0.006 <C> 1.452 <C> 0.063 <R> <C> CLM <C> 0.141 <C> 0.006 <C> 2.505 <C> 0.006 <CAP> Table 4: Multiple regression analysis predicting SDist.
<R> <C> Train Data (#sentences) <C> arn→spa BLEU <C> arn→spa chrF <C> spa→arn BLEU <C> spa→arn chrF <R> <C> Results on dev set <C> Results on dev set <C> Results on dev set <C> Results on dev set <C> Results on dev set <R> <C> 220k (all) <C> 20.98 <C> 0.5 <C> 14.12 <C> 0.4 <R> <C> 100k <C> 16.89 <C> 0.4 <C> 10.93 <C> 0.3 <R> <C> 50k <C> 13.70 <C> 0.4 <C> 2.05 <C> 0.1 <R> <C> 10k <C> 6.26 <C> 0.3 <C> 1.09 <C> 0.1 <R> <C> Results on test set <C> Results on test set <C> Results on test set <C> Results on test set <C> Results on test set <R> <C> 220k (all) <C> 20.4 <C> 0.5 <C> 12.9 <C> 0.4 <R> <C> 100k <C> 16.9 <C> 0.4 <C> 6.4 <C> 0.2 <R> <C> 50k <C> 13.3 <C> 0.4 <C> 1.1 <C> 0.1 <R> <C> 10k <C> 7.8 <C> 0.3 <C> 0.7 <C> 0.1 <CAP> Table 3: Machine Translation Results
<R> <C> [BOLD] Model <C> [BOLD] F1 <C> [BOLD] P <C> [BOLD] R <R> <C> Pivot (Bi-LSTM) <C> 87.92 <C> 92.59 <C> 83.70 <R> <C> [BOLD] Model <C> [BOLD] BLEU <C> [BOLD] NIST <C> [BOLD] ROUGE <R> <C> Vanilla Seq2Seq <C> 2.14 <C> 0.2809 <C> 0.47 <R> <C> Structure-S2S <C> 3.27 <C> 0.9612 <C> 0.71 <R> <C> PretrainedMT <C> 4.35 <C> 1.9937 <C> 0.91 <R> <C> SemiMT <C> 6.76 <C> 3.5017 <C> 2.04 <R> <C> Pivot-Vanilla <C> 20.09 <C> 6.5130 <C> 18.31 <R> <C> [BOLD] Model <C> [BOLD] BLEU <C> [BOLD] NIST <C> [BOLD] ROUGE <R> <C> Transformer <C> 5.48 <C> 1.9873 <C> 1.26 <R> <C> PretrainedMT <C> 6.43 <C> 2.1019 <C> 1.77 <R> <C> SemiMT <C> 9.71 <C> 2.7019 <C> 3.31 <R> <C> Pivot-Trans <C> 27.34 <C> 6.8763 <C> 19.30 <CAP> Table 1: Results of our model and the baselines. Above is the performance of the key fact prediction component (F1: F1 score, P: precision, R: recall). Middle is the comparison between models under the Vanilla Seq2Seq framework. Below is the models implemented with the transformer framework.
<R> <C> [BOLD] Model <C> [BOLD] BLEU <C> [BOLD] NIST <C> [BOLD] ROUGE <R> <C> Vanilla Seq2Seq <C> 2.14 <C> 0.2809 <C> 0.47 <R> <C> + Pseudo <C> 10.01 <C> 3.0620 <C> 6.55 <R> <C> Transformer <C> 6.43 <C> 2.1019 <C> 1.77 <R> <C> + Pseudo <C> 14.35 <C> 4.1763 <C> 8.42 <R> <C> w/o Pseudo <C> 11.08 <C> 3.6910 <C> 4.84 <R> <C> Pivot-Vanilla <C> 20.09 <C> 6.5130 <C> 18.31 <R> <C> w/o Pseudo <C> 14.18 <C> 4.2686 <C> 7.10 <R> <C> Pivot-Trans <C> 27.34 <C> 6.8763 <C> 19.30 <CAP> Table 2: Ablation study on the 1k training set for the effect of pseudo parallel data.
<R> <C> [BOLD] Model <C> [BOLD] BLEU <C> [BOLD] NIST <C> [BOLD] ROUGE <R> <C> Pivot-Vanilla <C> 20.09 <C> 6.5130 <C> 18.31 <R> <C> w/o denosing <C> 18.45 <C> 4.8714 <C> 11.43 <R> <C> Pivot-Trans <C> 27.34 <C> 6.8763 <C> 19.30 <R> <C> w/o denosing <C> 25.72 <C> 6.5475 <C> 17.95 <CAP> Table 3: Ablation study on the 1k training set for the effect of the denoising data augmentation.
<R> <C> Model <C> BLEU <R> <C> RNNSearch-1 (Luong and Manning,  2015 ) <C> 23.30 <R> <C> RNNSearch-2 (Huang  [ITALIC] et al.,  2017 ) <C> 26.10 <R> <C> LabelEmb (Sun  [ITALIC] et al.,  2017b ) <C> 26.80 <R> <C> NPMT (Huang  [ITALIC] et al.,  2017 ) <C> 27.69 <R> <C> NPMT+LM (Huang  [ITALIC] et al.,  2017 ) <C> 28.67 <R> <C> Seq2Seq+Attention <C> 26.93 <R> <C> [BOLD] +ACA <C> [BOLD] 29.10 <CAP> Table 3: Results of our model and the baselines (directly reported in the referred articles) on the English-Vietnamese translation, tested on the TED tst2013 with the BLEU score.
<R> <C> Medication <C> CNN P <C> CNN R <C> CNN F <C> MLP P <C> MLP R <C> MLP F <C> SVM P <C> SVM R <C> SVM F <C> RF P <C> RF R <C> RF F <C> LR P <C> LR R <C> LR F <R> <C> Metoprolol <C> 0.73 <C> 0.87 <C> 0.79 <C> 0.64 <C> 0.96 <C> 0.77 <C> 0.74 <C> 0.79 <C> 0.76 <C> 0.74 <C> 0.89 <C> [BOLD] 0.81 <C> 0.74 <C> 0.85 <C> 0.79 <R> <C> Furosemide <C> 0.59 <C> 0.85 <C> [BOLD] 0.70 <C> 0.62 <C> 0.42 <C> 0.46 <C> 0.70 <C> 0.64 <C> 0.67 <C> 0.73 <C> 0.67 <C> [BOLD] 0.70 <C> 0.74 <C> 0.62 <C> 0.68 <R> <C> Lisinopril <C> 0.56 <C> 0.51 <C> [BOLD] 0.53 <C> 0.63 <C> 0.43 <C> 0.49 <C> 0.56 <C> 0.44 <C> 0.49 <C> 0.70 <C> 0.35 <C> 0.46 <C> 0.66 <C> 0.29 <C> 0.40 <R> <C> Amlodipine <C> 0.59 <C> 0.45 <C> [BOLD] 0.49 <C> 0.11 <C> 0.08 <C> 0.09 <C> 0.61 <C> 0.34 <C> 0.43 <C> 0.72 <C> 0.29 <C> 0.41 <C> 0.78 <C> 0.10 <C> 0.18 <R> <C> Atenolol <C> 0.58 <C> 0.27 <C> [BOLD] 0.32 <C> 0.00 <C> 0.00 <C> 0.00 <C> 0.45 <C> 0.23 <C> 0.30 <C> 0.70 <C> 0.14 <C> 0.24 <C> 0.52 <C> 0.03 <C> 0.06 <R> <C> Hctz <C> 0.41 <C> 0.23 <C> [BOLD] 0.26 <C> 0.00 <C> 0.00 <C> 0.00 <C> 0.35 <C> 0.21 <C> [BOLD] 0.26 <C> 0.61 <C> 0.04 <C> 0.07 <C> 0.60 <C> 0.02 <C> 0.04 <R> <C> Diltiazem <C> 0.62 <C> 0.37 <C> [BOLD] 0.46 <C> 0.00 <C> 0.00 <C> 0.00 <C> 0.45 <C> 0.21 <C> 0.28 <C> 0.73 <C> 0.12 <C> 0.20 <C> 0.85 <C> 0.04 <C> 0.07 <R> <C> Carvedilol <C> 0.60 <C> 0.56 <C> [BOLD] 0.57 <C> 0.00 <C> 0.00 <C> 0.00 <C> 0.51 <C> 0.28 <C> 0.36 <C> 0.74 <C> 0.27 <C> 0.40 <C> 0.79 <C> 0.08 <C> 0.15 <R> <C> Micro Avg <C> 0.63 <C> 0.70 <C> [BOLD] 0.65 <C> 0.51 <C> 0.54 <C> 0.51 <C> 0.65 <C> 0.58 <C> 0.60 <C> 0.72 <C> 0.60 <C> 0.61 <C> 0.72 <C> 0.53 <C> 0.55 <R> <C> Macro Avg <C> 0.59 <C> 0.51 <C> [BOLD] 0.52 <C> 0.25 <C> 0.24 <C> 0.23 <C> 0.55 <C> 0.39 <C> 0.44 <C> 0.71 <C> 0.35 <C> 0.41 <C> 0.71 <C> 0.25 <C> 0.30 <CAP> Table 2: Medication-wise precision (P), recall (R) and F1-score (F) for CNN and 4 baseline models averaged over 5 runs. From top to bottom, medications are shown in descending order of their frequencies. The overall performance is measured using micro- and macro-average.
<R> <C> Medication <C> CNN P <C> CNN R <C> CNN F <C> MLP P <C> MLP R <C> MLP F <C> SVM P <C> SVM R <C> SVM F <C> RF P <C> RF R <C> RF F <C> LR P <C> LR R <C> LR F <R> <C> Metoprolol <C> 0.02 <C> 0.03 <C> 0.01 <C> 0.02 <C> 0.04 <C> 0.01 <C> 0.01 <C> 0.01 <C> 0.00 <C> 0.00 <C> 0.01 <C> 0.00 <C> 0.01 <C> 0.01 <C> 0.01 <R> <C> Furosemide <C> 0.01 <C> 0.03 <C> 0.01 <C> 0.10 <C> 0.18 <C> 0.05 <C> 0.01 <C> 0.02 <C> 0.01 <C> 0.01 <C> 0.01 <C> 0.01 <C> 0.01 <C> 0.02 <C> 0.02 <R> <C> Lisinopril <C> 0.04 <C> 0.03 <C> 0.02 <C> 0.04 <C> 0.13 <C> 0.10 <C> 0.02 <C> 0.02 <C> 0.01 <C> 0.01 <C> 0.01 <C> 0.01 <C> 0.02 <C> 0.01 <C> 0.01 <R> <C> Amlodipine <C> 0.09 <C> 0.13 <C> 0.09 <C> 0.23 <C> 0.16 <C> 0.19 <C> 0.02 <C> 0.03 <C> 0.03 <C> 0.04 <C> 0.03 <C> 0.03 <C> 0.06 <C> 0.02 <C> 0.03 <R> <C> Atenolol <C> 0.10 <C> 0.14 <C> 0.14 <C> 0.00 <C> 0.00 <C> 0.00 <C> 0.03 <C> 0.01 <C> 0.00 <C> 0.06 <C> 0.01 <C> 0.02 <C> 0.08 <C> 0.01 <C> 0.01 <R> <C> Hctz <C> 0.21 <C> 0.20 <C> 0.21 <C> 0.00 <C> 0.00 <C> 0.00 <C> 0.04 <C> 0.02 <C> 0.02 <C> 0.04 <C> 0.01 <C> 0.02 <C> 0.23 <C> 0.01 <C> 0.02 <R> <C> Diltiazem <C> 0.05 <C> 0.06 <C> 0.06 <C> 0.00 <C> 0.00 <C> 0.00 <C> 0.09 <C> 0.05 <C> 0.06 <C> 0.18 <C> 0.05 <C> 0.07 <C> 0.14 <C> 0.02 <C> 0.03 <R> <C> Carvedilol <C> 0.11 <C> 0.02 <C> 0.06 <C> 0.00 <C> 0.00 <C> 0.00 <C> 0.08 <C> 0.04 <C> 0.05 <C> 0.06 <C> 0.01 <C> 0.02 <C> 0.13 <C> 0.03 <C> 0.05 <CAP> Table 6: Standard deviations of medication-wise precision (P), recall (R) and F1-score (F) for CNN and 4 baseline models averaged over 5 runs.
<R> <C> [BOLD] Methods <C> [BOLD] AUC <C> [BOLD] MRR <C> [BOLD] nDCG@5 <C> [BOLD] nDCG@10 <R> <C> LibFM <C> 0.5880 <C> 0.3054 <C> 0.3202 <C> 0.4090 <R> <C> CNN <C> 0.5909 <C> 0.3068 <C> 0.3221 <C> 0.4109 <R> <C> DSSM <C> 0.6114 <C> 0.3188 <C> 0.3261 <C> 0.4185 <R> <C> Wide&Deep <C> 0.5846 <C> 0.3009 <C> 0.3167 <C> 0.4062 <R> <C> DeepFM <C> 0.5896 <C> 0.3066 <C> 0.3221 <C> 0.4117 <R> <C> DFM <C> 0.5996 <C> 0.3133 <C> 0.3288 <C> 0.4165 <R> <C> DKN <C> 0.5966 <C> 0.3113 <C> 0.3286 <C> 0.4165 <R> <C> NAML* <C> [BOLD] 0.6434 <C> [BOLD] 0.3411 <C> [BOLD] 0.3670 <C> [BOLD] 0.4501 <CAP> Table 2: The results of different methods. *The improvement over all baseline methods is significant at the level p<0.001.
<R> <C> Model <C> Dataset 1 (Established entities) Strict <C> Dataset 1 (Established entities) Strict <C> Dataset 1 (Established entities) Linear <C> Dataset 1 (Established entities) Linear <C> Dataset 1 (Established entities) Exponential <C> Dataset 1 (Established entities) Exponential <C> Dataset 2 (Emerging entities) Strict <C> Dataset 2 (Emerging entities) Strict <C> Dataset 2 (Emerging entities) Linear <C> Dataset 2 (Emerging entities) Linear <C> Dataset 2 (Emerging entities) Exponential <C> Dataset 2 (Emerging entities) Exponential <R> <C> Model <C> NDCG@1 <C> [ITALIC] s <C> NDCG@1 <C> [ITALIC] s <C> NDCG@1 <C> [ITALIC] s <C> NDCG@1 <C> [ITALIC] s <C> NDCG@1 <C> [ITALIC] s <C> NDCG@1 <C> [ITALIC] s <R> <C> BM25 <C> 0.2335 <C> - <C> 0.3211 <C> - <C> 0.2798 <C> - <C> 0.2076 <C> - <C> 0.3533 <C> - <C> 0.2826 <C> - <R> <C> SDType <C> 0.8020 <C> - <C> 0.8562 <C> - <C> 0.8331 <C> - <C> 0.6970 <C> - <C> 0.7873 <C> - <C> 0.7451 <C> - <R> <C> [ITALIC] NeuType1 <C> [ITALIC] NeuType1 <C> [ITALIC] NeuType1 <C> [ITALIC] NeuType1 <C> [ITALIC] NeuType1 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC]  [BOLD] A <C> 0.8578‡ <C> 0.0025 <C> 0.8980‡ <C> 0.0028 <C> 0.8796‡ <C> 0.0028 <C> [BOLD] 0.7870‡ <C> 0.0039 <C> [BOLD] 0.8617‡ <C> 0.0045 <C> [BOLD] 0.8272‡ <C> 0.0042 <R> <C> [ITALIC]  [BOLD] B <C> 0.7722 <C> 0.0051 <C> 0.8028 <C> 0.0049 <C> 0.7895 <C> 0.0047 <C> 0.3664 <C> 0.0051 <C> 0.5381 <C> 0.0033 <C> 0.4419 <C> 0.0039 <R> <C> [ITALIC]  [BOLD] C <C> 0.7222 <C> 0.0050 <C> 0.7571 <C> 0.0043 <C> 0.7414 <C> 0.0048 <C> 0.2950 <C> 0.0054 <C> 0.3781 <C> 0.0044 <C> 0.3341 <C> 0.0045 <R> <C> [ITALIC]  [BOLD] A +  [ITALIC]  [BOLD] B <C> [BOLD] 0.8864‡ <C> 0.0055 <C> [BOLD] 0.9193‡ <C> 0.0035 <C> [BOLD] 0.9045‡ <C> 0.0043 <C> 0.7700‡ <C> 0.0049 <C> 0.8558‡ <C> 0.0042 <C> 0.8164‡ <C> 0.0040 <R> <C> [ITALIC]  [BOLD] A +  [ITALIC]  [BOLD] C <C> 0.8766‡ <C> 0.0057 <C> 0.9072‡ <C> 0.0066 <C> 0.8935‡ <C> 0.0063 <C> 0.7532‡ <C> 0.0092 <C> 0.8355‡ <C> 0.0074 <C> 0.7974‡ <C> 0.0080 <R> <C> [ITALIC]  [BOLD] B +  [ITALIC]  [BOLD] C <C> 0.7828 <C> 0.0098 <C> 0.8157 <C> 0.0098 <C> 0.8006 <C> 0.0096 <C> 0.3756 <C> 0.0181 <C> 0.5251 <C> 0.0718 <C> 0.4430 <C> 0.0406 <R> <C> [ITALIC]  [BOLD] A +  [ITALIC]  [BOLD] B +  [ITALIC]  [BOLD] C <C> 0.8748‡ <C> 0.0090 <C> 0.9074‡ <C> 0.0091 <C> 0.8930‡ <C> 0.0091 <C> 0.7462‡ <C> 0.0058 <C> 0.8354‡ <C> 0.0030 <C> 0.7944‡ <C> 0.0035 <R> <C> [ITALIC] NeuType2 <C> [ITALIC] NeuType2 <C> [ITALIC] NeuType2 <C> [ITALIC] NeuType2 <C> [ITALIC] NeuType2 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC]  [BOLD] A <C> 0.8558‡ <C> 0.0029 <C> 0.8956‡ <C> 0.0028 <C> 0.8777‡ <C> 0.0026 <C> [BOLD] 0.7816‡ <C> 0.0050 <C> 0.8587‡ <C> 0.0057 <C> [BOLD] 0.8230‡ <C> 0.0056 <R> <C> [ITALIC]  [BOLD] B <C> 0.7788 <C> 0.0050 <C> 0.8070 <C> 0.0050 <C> 0.7947 <C> 0.0049 <C> 0.3696 <C> 0.0130 <C> 0.5453 <C> 0.0116 <C> 0.4475 <C> 0.0125 <R> <C> [ITALIC]  [BOLD] C <C> 0.7224 <C> 0.0060 <C> 0.7586 <C> 0.0066 <C> 0.7424 <C> 0.0061 <C> 0.2854 <C> 0.0060 <C> 0.3690 <C> 0.0055 <C> 0.3244 <C> 0.0055 <R> <C> [ITALIC]  [BOLD] A +  [ITALIC]  [BOLD] B <C> 0.8896‡ <C> 0.0026 <C> 0.9219‡ <C> 0.0030 <C> 0.9074‡ <C> 0.0025 <C> 0.7766‡ <C> 0.0057 <C> [BOLD] 0.8600‡ <C> 0.0041 <C> 0.8216‡ <C> 0.0046 <R> <C> [ITALIC]  [BOLD] A +  [ITALIC]  [BOLD] C <C> 0.8926‡◊ <C> 0.0046 <C> 0.9256‡◊ <C> 0.0034 <C> 0.9108‡◊ <C> 0.0034 <C> 0.7670‡□ <C> 0.0068 <C> 0.8490‡□ <C> 0.0047 <C> 0.8107‡◊ <C> 0.0055 <R> <C> [ITALIC]  [BOLD] B +  [ITALIC]  [BOLD] C <C> 0.8134◊ <C> 0.0068 <C> 0.8431◊ <C> 0.0068 <C> 0.8299◊ <C> 0.0068 <C> 0.3802 <C> 0.0242 <C> 0.5286 <C> 0.0777 <C> 0.4470 <C> 0.0464 <R> <C> [ITALIC]  [BOLD] A +  [ITALIC]  [BOLD] B +  [ITALIC]  [BOLD] C <C> [BOLD] 0.8958‡◊ <C> 0.0027 <C> [BOLD] 0.9284‡◊ <C> 0.0033 <C> [BOLD] 0.9138‡◊ <C> 0.0026 <C> 0.7556‡ <C> 0.0108 <C> 0.8487‡◊ <C> 0.0076 <C> 0.8056‡◊ <C> 0.0092 <CAP> Table 1. Entity typing results, measured in terms of NDCG@1. s is the standard deviation for the averaged performances of the neural models. ‡ denotes a statistically significant difference w.r.t. SDType at p<0.001. Statistical significance of each model in NeuType2 versus the corresponding one in NeuType1, at p<0.05 and p<0.001, is denoted by □ and ◊, respectively.
<R> <C> [EMPTY] <C> [BOLD] Finnish <C> [BOLD] Spanish <C> [BOLD] Swedish <R> <C> [BOLD] Train words <C> 53547 <C> 62556 <C> 16295 <R> <C> [BOLD] Valid words <C> 2317 <C> 4984 <C> 1731 <R> <C> [BOLD] Test words <C> 2246 <C> 956 <C> 3538 <R> <C> [BOLD] Annotated Test pairs <C> 278 <C> 340 <C> 137 <CAP> Table 1: Overview of the training, validation and test set used.
<R> <C> Models/methods <C> K-means <C> HC-K-means <C> [ITALIC] k-NN <R> <C> GD-1hot <C> 0.67 <C> 0.73 <C> [BOLD] 0.74 <R> <C> RNNAE Mel-F <C> 0.30 <C> 0.35 <C> [BOLD] 0.41 <R> <C> CAE Siamese Mel-F <C> 0.26 <C> 0.37 <C> [BOLD] 0.43 <CAP> Table 1: Frequency estimations using K-means, HC-K-means and k-NN density estimation on a subset of the Buckeye corpus
<R> <C> R2 <C> Frequency est. <C> MAP <C> ABX <R> <C> Frequency est. <C> 1.0 <C> 0.34 <C> 0.53 <R> <C> MAP <C> 0.34 <C> 1.0 <C> 0.45 <R> <C> ABX <C> 0.53 <C> 0.45 <C> 1.0 <CAP> Table 2: Correlation R2 across the ’average’ column of MAP, ABX and frequency estimation
<R> <C> [EMPTY] <C> [BOLD] Model <C> [BOLD] Perplexity <C> [BOLD] Human <R> <C> 1 <C> Base model <C> 30.30 <C> 0.19 <R> <C> 2 <C> With binary D <C> 30.01 <C> 0.20 <R> <C> 3 <C> With constraint updated <C> 31.27 <C> 0.15 <R> <C> 3 <C> in M-step (Eq. 5 ) <C> 31.27 <C> 0.15 <R> <C> 4 <C> With learned constraint <C> [BOLD] 28.69 <C> [BOLD] 0.24 <CAP> Table 3: Sentence generation results on test set perplexity and human survey. Samples by the full model are considered as of higher quality in 24% cases.
<R> <C> [BOLD] Train <C> [BOLD] Model <C> [BOLD] SNLI <C> [BOLD] MNLI  [BOLD] Match. <C> [BOLD] MNLI  [BOLD] Mis. <R> <C> [EMPTY] <C> Most freq. <C> 34.3 <C> 36.5 <C> 35.6 <R> <C> SNLI <C> CBOW <C> 80.6 <C> - <C> - <R> <C> SNLI <C> BiLSTM <C> 81.5 <C> - <C> - <R> <C> SNLI <C> ESIM <C> [BOLD] 86.7 <C> - <C> - <R> <C> MNLI <C> CBOW <C> 51.5 <C> 64.8 <C> 64.5 <R> <C> MNLI <C> BiLSTM <C> 50.8 <C> 66.9 <C> 66.9 <R> <C> MNLI <C> ESIM <C> 60.7 <C> [BOLD] 72.3 <C> [BOLD] 72.1 <R> <C> MNLI+ SNLI <C> CBOW <C> 74.7 <C> 65.2 <C> 64.6 <R> <C> MNLI+ SNLI <C> BiLSTM <C> 74.0 <C> 67.5 <C> 67.1 <R> <C> MNLI+ SNLI <C> ESIM <C> 79.7 <C> [BOLD] 72.4 <C> [BOLD] 71.9 <CAP> Table 4: Test set accuracies (%) for all models; Match. represents test set performance on the MultiNLI genres that are also represented in the training set, Mis. represents test set performance on the remaining ones; Most freq. is a trivial ‘most frequent class’ baseline.
<R> <C> [BOLD] Tag <C> [BOLD] Dev. Freq.  [BOLD] SNLI <C> [BOLD] Dev. Freq.  [BOLD] MultiNLI <C> [BOLD] Dev. Freq.  [BOLD] Diff. <C> [BOLD] Most Frequent Label  [BOLD] Label <C> [BOLD] Most Frequent Label  [BOLD] % <C> [BOLD] Model Acc.  [BOLD] CBOW <C> [BOLD] Model Acc.  [BOLD] BiLSTM <C> [BOLD] Model Acc.  [BOLD] ESIM <R> <C> Entire Corpus <C> 100 <C> 100 <C> 0 <C> entailment <C> ∼ [ITALIC] 35 <C> ∼ [ITALIC] 65 <C> ∼ [ITALIC] 67 <C> ∼ [ITALIC] 72 <R> <C> Pronouns (PTB) <C> 34 <C> 68 <C> 34 <C> entailment <C> 34 <C> 66 <C> 68 <C> 73 <R> <C> Quantifiers <C> 33 <C> 63 <C> 30 <C> contradiction <C> 36 <C> 66 <C> 68 <C> 73 <R> <C> Modals (PTB) <C> <1 <C> 28 <C> 28 <C> entailment <C> 35 <C> 65 <C> 67 <C> 72 <R> <C> Negation (PTB) <C> 5 <C> 31 <C> 26 <C> contradiction <C> [BOLD] 48 <C> 67 <C> 70 <C> 75 <R> <C> WH terms (PTB) <C> 5 <C> 30 <C> 25 <C> entailment <C> 35 <C> 64 <C> 65 <C> 72 <R> <C> Belief Verbs <C> <1 <C> 19 <C> 18 <C> entailment <C> 34 <C> 64 <C> 67 <C> 71 <R> <C> Time Terms <C> 19 <C> 36 <C> 17 <C> neutral <C> 35 <C> 64 <C> 66 <C> 71 <R> <C> Discourse Mark. <C> <1 <C> 14 <C> 14 <C> neutral <C> 34 <C> 62 <C> 64 <C> 70 <R> <C> Presup. Triggers <C> 8 <C> 22 <C> 14 <C> neutral <C> 34 <C> 65 <C> 67 <C> 73 <R> <C> Compr./Supr.(PTB) <C> 3 <C> 17 <C> 14 <C> neutral <C> 39 <C> 61 <C> 63 <C> 69 <R> <C> Conditionals <C> 4 <C> 15 <C> 11 <C> neutral <C> 35 <C> 65 <C> 68 <C> 73 <R> <C> Tense Match (PTB) <C> 62 <C> 69 <C> 7 <C> entailment <C> 37 <C> 67 <C> 68 <C> 73 <R> <C> Interjections (PTB) <C> <1 <C> 5 <C> 5 <C> entailment <C> 36 <C> 67 <C> 70 <C> 75 <R> <C> >20 words <C> <1 <C> 5 <C> 5 <C> entailment <C> [BOLD] 42 <C> 65 <C> 67 <C> 76 <CAP> Table 5: Dev. Freq. is the percentage of dev. set examples that include each phenomenon, ordered by greatest difference in frequency of occurrence (Diff.) between MultiNLI and SNLI. Most Frequent Label specifies which label is the most frequent for each tag in the MultiNLI dev. set, and % is its incidence. Model Acc. is the dev. set accuracy (%) by annotation tag for each baseline model (trained on MultiNLI only). (PTB) marks a tag as derived from Penn Treebank-style parser output tags (Marcus et al., 1993).
<R> <C> [EMPTY] <C> [BOLD] + <C> [BOLD] - <C> [BOLD] Tweets <C> [BOLD] Tokens <C> [BOLD] Switching* <R> <C> [BOLD] Humour <C> 1755 <C> 1698 <C> 3453 <C> 9851 <C> 2.20 <R> <C> [BOLD] Sarcasm <C> 504 <C> 4746 <C> 5250 <C> 14930 <C> 2.13 <R> <C> [BOLD] Hate <C> 1661 <C> 2914 <C> 4575 <C> 10453 <C> 4.34 <CAP> Table 1: Dataset description (* denotes average/tweet).
<R> <C> [BOLD] Model <C> [BOLD] Humour <C> [BOLD] Sarcasm <C> [BOLD] Hate <R> <C> Baseline (B) <C> 69.34 <C> 78.4 <C> 33.60 <R> <C> Baseline + Feature (BF) <C> [BOLD] 71.16 <C> [BOLD] 79.85 <C> [BOLD] 34.73 <R> <C> HAN (H) <C> 72.04 <C> 81.36 <C> 38.78 <R> <C> HAN + Feature (HF) <C> [BOLD] 72.71 <C> [BOLD] 82.07 <C> [BOLD] 39.54 <CAP> Table 4: Summary of the results from different models in terms of macro-F1 scores. M-W U test shows all improvements of HF over B are significant.
<R> <C> Model <C> BLEU <C> LenGen <C> LenRef <C> [ITALIC] p1 <C> [ITALIC] p2 <C> [ITALIC] p3 <C> [ITALIC] p4 <R> <C> MOSES <C> 3.63 <C> 129889 <C> 22872 <C> 8.3 <C> 3.6 <C> 2.7 <C> 2.1 <R> <C> NMT1 <C> 31.92 <C> 24344 <C> 22872 <C> 38.1 <C> 31.1 <C> 29.5 <C> 29.7 <R> <C> NMT2 <C> 32.81 <C> 21287 <C> 22872 <C> 40.1 <C> 34.0 <C> 33.4 <C> 34.3 <R> <C> NMT2 <C> 23.10* <C> 20303 <C> 18658 <C> 30.2 <C> 23.3 <C> 20.7 <C> 19.6 <R> <C> MOSES is the baseline model. NMT1 is the NMT model with V-DO filter described in Section  IV-B . NMT2 is a model trained without V-DO filter described in Section  V-D . LenGen is the total length of the generated messages ( [ITALIC] c in Equation ( 11 )). LenRef is the total length of the reference messages ( [ITALIC] r in Equation ( 11 )). The modified n-gram precision  [ITALIC] pn, where  [ITALIC] n=1,2,3,4, is defined in Equation ( 8 ). <C> MOSES is the baseline model. NMT1 is the NMT model with V-DO filter described in Section  IV-B . NMT2 is a model trained without V-DO filter described in Section  V-D . LenGen is the total length of the generated messages ( [ITALIC] c in Equation ( 11 )). LenRef is the total length of the reference messages ( [ITALIC] r in Equation ( 11 )). The modified n-gram precision  [ITALIC] pn, where  [ITALIC] n=1,2,3,4, is defined in Equation ( 8 ). <C> MOSES is the baseline model. NMT1 is the NMT model with V-DO filter described in Section  IV-B . NMT2 is a model trained without V-DO filter described in Section  V-D . LenGen is the total length of the generated messages ( [ITALIC] c in Equation ( 11 )). LenRef is the total length of the reference messages ( [ITALIC] r in Equation ( 11 )). The modified n-gram precision  [ITALIC] pn, where  [ITALIC] n=1,2,3,4, is defined in Equation ( 8 ). <C> MOSES is the baseline model. NMT1 is the NMT model with V-DO filter described in Section  IV-B . NMT2 is a model trained without V-DO filter described in Section  V-D . LenGen is the total length of the generated messages ( [ITALIC] c in Equation ( 11 )). LenRef is the total length of the reference messages ( [ITALIC] r in Equation ( 11 )). The modified n-gram precision  [ITALIC] pn, where  [ITALIC] n=1,2,3,4, is defined in Equation ( 8 ). <C> MOSES is the baseline model. NMT1 is the NMT model with V-DO filter described in Section  IV-B . NMT2 is a model trained without V-DO filter described in Section  V-D . LenGen is the total length of the generated messages ( [ITALIC] c in Equation ( 11 )). LenRef is the total length of the reference messages ( [ITALIC] r in Equation ( 11 )). The modified n-gram precision  [ITALIC] pn, where  [ITALIC] n=1,2,3,4, is defined in Equation ( 8 ). <C> MOSES is the baseline model. NMT1 is the NMT model with V-DO filter described in Section  IV-B . NMT2 is a model trained without V-DO filter described in Section  V-D . LenGen is the total length of the generated messages ( [ITALIC] c in Equation ( 11 )). LenRef is the total length of the reference messages ( [ITALIC] r in Equation ( 11 )). The modified n-gram precision  [ITALIC] pn, where  [ITALIC] n=1,2,3,4, is defined in Equation ( 8 ). <C> MOSES is the baseline model. NMT1 is the NMT model with V-DO filter described in Section  IV-B . NMT2 is a model trained without V-DO filter described in Section  V-D . LenGen is the total length of the generated messages ( [ITALIC] c in Equation ( 11 )). LenRef is the total length of the reference messages ( [ITALIC] r in Equation ( 11 )). The modified n-gram precision  [ITALIC] pn, where  [ITALIC] n=1,2,3,4, is defined in Equation ( 8 ). <C> MOSES is the baseline model. NMT1 is the NMT model with V-DO filter described in Section  IV-B . NMT2 is a model trained without V-DO filter described in Section  V-D . LenGen is the total length of the generated messages ( [ITALIC] c in Equation ( 11 )). LenRef is the total length of the reference messages ( [ITALIC] r in Equation ( 11 )). The modified n-gram precision  [ITALIC] pn, where  [ITALIC] n=1,2,3,4, is defined in Equation ( 8 ). <R> <C> * This BLEU score is calculated on a test set that is not V-DO filtered described in Section  V-D . The other BLEU scores are tested on a V-DO filtered test set described in Section  IV-A4 . <C> * This BLEU score is calculated on a test set that is not V-DO filtered described in Section  V-D . The other BLEU scores are tested on a V-DO filtered test set described in Section  IV-A4 . <C> * This BLEU score is calculated on a test set that is not V-DO filtered described in Section  V-D . The other BLEU scores are tested on a V-DO filtered test set described in Section  IV-A4 . <C> * This BLEU score is calculated on a test set that is not V-DO filtered described in Section  V-D . The other BLEU scores are tested on a V-DO filtered test set described in Section  IV-A4 . <C> * This BLEU score is calculated on a test set that is not V-DO filtered described in Section  V-D . The other BLEU scores are tested on a V-DO filtered test set described in Section  IV-A4 . <C> * This BLEU score is calculated on a test set that is not V-DO filtered described in Section  V-D . The other BLEU scores are tested on a V-DO filtered test set described in Section  IV-A4 . <C> * This BLEU score is calculated on a test set that is not V-DO filtered described in Section  V-D . The other BLEU scores are tested on a V-DO filtered test set described in Section  IV-A4 . <C> * This BLEU score is calculated on a test set that is not V-DO filtered described in Section  V-D . The other BLEU scores are tested on a V-DO filtered test set described in Section  IV-A4 . <CAP> TABLE I: BLEU Scores (%) of MOSES and Our Models on the Test Set
<R> <C> Diff Length <C> BLEU <C> LenGen <C> LenRef <C> [ITALIC] p1 <C> [ITALIC] p2 <C> [ITALIC] p3 <C> [ITALIC] p4 <R> <C> ≤ 25 <C> 6.46 <C> 870 <C> 655 <C> 18.6 <C> 6.9 <C> 4.3 <C> 3.1 <R> <C> >25, ≤50 <C> 9.31 <C> 3627 <C> 3371 <C> 23.1 <C> 10.8 <C> 6.6 <C> 4.5 <R> <C> >50, ≤75 <C> 12.67 <C> 4779 <C> 4418 <C> 24.8 <C> 14.1 <C> 9.8 <C> 7.6 <R> <C> >75 <C> 43.33 <C> 15068 <C> 14428 <C> 47.1 <C> 42.3 <C> 41.7 <C> 42.3 <R> <C> See Table  I  for explanation of each column name. The BLEU scores are calculated based on the test results generated by Model1, the NMT model with V-DO filter trained in Section  IV-B . <C> See Table  I  for explanation of each column name. The BLEU scores are calculated based on the test results generated by Model1, the NMT model with V-DO filter trained in Section  IV-B . <C> See Table  I  for explanation of each column name. The BLEU scores are calculated based on the test results generated by Model1, the NMT model with V-DO filter trained in Section  IV-B . <C> See Table  I  for explanation of each column name. The BLEU scores are calculated based on the test results generated by Model1, the NMT model with V-DO filter trained in Section  IV-B . <C> See Table  I  for explanation of each column name. The BLEU scores are calculated based on the test results generated by Model1, the NMT model with V-DO filter trained in Section  IV-B . <C> See Table  I  for explanation of each column name. The BLEU scores are calculated based on the test results generated by Model1, the NMT model with V-DO filter trained in Section  IV-B . <C> See Table  I  for explanation of each column name. The BLEU scores are calculated based on the test results generated by Model1, the NMT model with V-DO filter trained in Section  IV-B . <C> See Table  I  for explanation of each column name. The BLEU scores are calculated based on the test results generated by Model1, the NMT model with V-DO filter trained in Section  IV-B . <CAP> TABLE II: BLEU Scores (%) on diffs of Different Lengths
<R> <C> [BOLD] Sys. <C> Waveform generation <C> Δ+Δ2 <C> All feat. <C> [BOLD] Sys. <C> Waveform generation <C> Δ+Δ2 <C> All feat. <R> <C> B01 <C> Waveform filtering <C> 29.77 <C> 25.55 <C> N09 <C> Ahocoder <C> 1.24 <C> 13.22 <R> <C> D01 <C> World direct wave mod. <C> 8.43 <C> 18.75 <C> N10 <C> Wavenet <C> 4.64 <C> 15.63 <R> <C> D02 <C> World <C> 43.94 <C> 48.95 <C> N11 <C> STRAIGHT <C> 0.49 <C> 1.28 <R> <C> D03 <C> STRAIGHT <C> 1.58 <C> 16.07 <C> N12 <C> Waveform filtering <C> 40.98 <C> 23.48 <R> <C> D04 <C> World <C> 15.74 <C> 15.03 <C> N13 <C> World <C> 10.69 <C> 17.42 <R> <C> D05 <C> World <C> 4.03 <C> 16.63 <C> N14 <C> Waveform filtering <C> 41.58 <C> 25.95 <R> <C> N03 <C> ? <C> 5.80 <C> 16.47 <C> N15 <C> World <C> 7.24 <C> 17.18 <R> <C> N04 <C> World <C> 3.74 <C> 16.26 <C> N16 <C> Ahocoder <C> 1.18 <C> 11.17 <R> <C> N05 <C> SuperVP <C> 41.42 <C> 32.23 <C> N17 <C> Wavenet <C> 15.19 <C> 15.48 <R> <C> N06 <C> World <C> 2.72 <C> 14.55 <C> N18 <C> Griffin-Lim <C> 32.35 <C> 19.43 <R> <C> N07 <C> World <C> 4.12 <C> 16.75 <C> N19 <C> World <C> 11.07 <C> 21.97 <R> <C> N08 <C> Waveform filtering <C> 37.20 <C> 23.84 <C> N20 <C> World <C> 3.54 <C> 15.23 <CAP> Table 4: Equal error rates (EER %) of the CQCC-GMM spoofing countermeasure of the VCC’18 entries on the Hub task. Here “B01” denotes the VCC’18 baseline system. The two countermeasures considered use deltas and double deltas (Δ+Δ2) of 29 CQCCs, and 29 CQCCs plus zeroth coefficient along with deltas and double deltas (All feat.). Training data VCC’16, number of Gaussians 2048. The higher the EER, the better the VC system in terms of quality (less processing artifacts). ?: information unavailable to the authors.
<R> <C> [ITALIC] Data (hrs) <C> [ITALIC] AM <C> [ITALIC] LM <C> [ITALIC] SWB <C> [ITALIC] CH <R> <C> 300 <C> A2W <C> - <C> 20.8 <C> 30.4 <R> <C> 300 <C> Phone CTC <C> Small <C> 14.5 <C> 25.1 <R> <C> 2000 <C> A2W <C> - <C> 13.0 <C> 18.8 <R> <C> 2000 <C> Phone CTC <C> Big <C> 9.6 <C> 16.0 <CAP> Table 1: This table shows the Switchboard/CallHome WERs for our baseline 300/2000-hour A2W and phone CTC models from [17].
<R> <C> [ITALIC] Model <C> [ITALIC] SWB <C> [ITALIC] CH <R> <C> Descending Order <C> 23.0 <C> 30.7 <R> <C> Ascending Order <C> 18.3 <C> 28.1 <R> <C> +Momentum <C> 18.0 <C> 27.4 <R> <C> +Dropout <C> 17.4 <C> 26.9 <R> <C> +Output Projection <C> 16.9 <C> 26.3 <R> <C> +Phone BLSTM Init. <C> 14.9 <C> 23.8 <R> <C> [BOLD] +Bigger Model <C> [BOLD] 14.6 <C> [BOLD] 23.6 <R> <C> Previous best  <C> 20.8 <C> 30.4 <CAP> Table 2: This table shows the impact of various training strategies on the 300-hour A2W model.
<R> <C> [ITALIC] Model <C> [ITALIC] Output <C> [ITALIC] Decoder/ <C> [ITALIC] SWB <C> [ITALIC] CH <R> <C> [EMPTY] <C> [ITALIC] Units <C> [ITALIC] LM <C> [EMPTY] <C> [EMPTY] <R> <C> BLSTM+LF MMI  <C> CD state <C> Y/Y <C> 8.5 <C> 15.3 <R> <C> LACE+LF MMI  <C> CD state <C> Y/Y <C> 8.3 <C> 14.8 <R> <C> Dilated Conv.  <C> CD state <C> Y/Y <C> 7.7 <C> 14.5 <R> <C> BLSTM  <C> CD state <C> Y/Y <C> 7.7 <C> 13.9 <R> <C> Iterated CTC  <C> Char <C> Y/Y <C> 11.3 <C> 18.7 <R> <C> CTC∗  <C> Char <C> Y/Y <C> 9.0 <C> 17.7 <R> <C> Gram-CTC∗  <C> Char N-gm <C> Y/Y <C> 7.9 <C> 15.8 <R> <C> CTC+Gram-CTC∗  <C> Char N-gm <C> Y/Y <C> 7.3 <C> 14.7 <R> <C> RNN-T∗  <C> Char <C> Y/N <C> 8.5 <C> 16.4 <R> <C> Attention∗  <C> Char <C> Y/N <C> 8.6 <C> 17.8 <R> <C> CTC A2W  <C> Word <C> N/N <C> 13.0 <C> 18.8 <R> <C> [BOLD] CTC A2W (current) <C> [BOLD] Word <C> [BOLD] N/N <C> [BOLD] 8.8 <C> [BOLD] 13.9 <CAP> Table 3: This table shows the WER of our current and previous-best A2W model trained on the 2000-hour Switchboard+Fisher set. We have also included several other published results for comparison. Results with ∗ use data augmentation on the 2000-hour training set.
<R> <C> Documents <C> Documents <C> Documents <C> Train 1,751 <C> Test 696 <R> <C> Sentences <C> Sentences <C> Sentences <C> 24,283 <C> 9,284 <R> <C> Words <C> Words <C> Words <C> 664,898 <C> 225,624 <R> <C> Predicates <C> Predicates <C> Predicates <C> 97,773 <C> 38,365 <R> <C> PAS labels <C> [ITALIC] ga <C> Depend <C> 64,152 <C> 12,226 <R> <C> PAS labels <C> (nom.) <C> Zero ( [ITALIC] intra) <C> 62,586 <C> 14,373 <R> <C> PAS labels <C> [EMPTY] <C> Zero ( [ITALIC] inter) <C> 149,482 <C> 49,415 <R> <C> PAS labels <C> [ITALIC] wo <C> Depend <C> 51,095 <C> 9,837 <R> <C> PAS labels <C> (acc.) <C> Zero ( [ITALIC] intra) <C> 17,585 <C> 3,179 <R> <C> PAS labels <C> [EMPTY] <C> Zero ( [ITALIC] inter) <C> 10,786 <C> 3,830 <R> <C> PAS labels <C> [ITALIC] ni <C> Depend <C> 11,790 <C> 2,501 <R> <C> PAS labels <C> (dat.) <C> Zero ( [ITALIC] intra) <C> 4,063 <C> 1,005 <R> <C> PAS labels <C> [EMPTY] <C> Zero ( [ITALIC] inter) <C> 6,978 <C> 2,048 <CAP> Table 1: Detail of training and test set.
<R> <C> Dataset <C> Method <C> Top2(%) P <C> Top2(%) R <C> Top2(%) F1 <C> Top4(%) P <C> Top4(%) R <C> Top4(%) F1 <C> Top6(%) P <C> Top6(%) R <C> Top6(%) F1 <C> Top8(%) P <C> Top8(%) R <C> Top8(%) F1 <R> <C> KDD <C> TextRank <C> 8.1 <C> 4.0 <C> 5.3 <C> 8.3 <C> 8.5 <C> 8.1 <C> 8.1 <C> 12.3 <C> 9.4 <C> 7.6 <C> 15.3 <C> 9.8 <R> <C> [EMPTY] <C> SingleRank <C> 9.1 <C> 4.6 <C> 6.0 <C> 9.3 <C> 9.4 <C> 9.0 <C> 8.7 <C> 13.1 <C> 10.1 <C> 8.1 <C> 16.4 <C> 10.6 <R> <C> [EMPTY] <C> TopicRank <C> 9.3 <C> 4.8 <C> 6.2 <C> 9.1 <C> 9.3 <C> 8.9 <C> 8.8 <C> 13.4 <C> 10.3 <C> 8.0 <C> 16.2 <C> 10.4 <R> <C> [EMPTY] <C> PositionRank <C> 11.1 <C> 5.6 <C> 7.3 <C> 10.8 <C> 11.1 <C> 10.6 <C> 9.8 <C> 15.3 <C> 11.6 <C> 9.2 <C> 18.9 <C> 12.1 <R> <C> [EMPTY] <C> Multipartite Rank <C> [BOLD] 15.2 <C> [BOLD] 7.7 <C> [BOLD] 10.0 <C> 11.3 <C> 11.4 <C> 11 <C> 8.8 <C> 13.3 <C> 10.2 <C> 7.2 <C> 14.6 <C> 9.4 <R> <C> [EMPTY] <C> [BOLD] SemanticRank <C> 13.1 <C> 6.7 <C> 8.6 <C> [BOLD] 11.5 <C> [BOLD] 11.8 <C> [BOLD] 11.3 <C> [BOLD] 10.4 <C> [BOLD] 15.9 <C> [BOLD] 12.2 <C> [BOLD] 9.6 <C> [BOLD] 19.2 <C> [BOLD] 12.8 <R> <C> WWW <C> TextRank <C> 7.7 <C> 3.7 <C> 4.8 <C> 8.6 <C> 7.9 <C> 8.0 <C> 8.1 <C> 12.3 <C> 9.8 <C> 8.2 <C> 15.2 <C> 10.2 <R> <C> [EMPTY] <C> SingleRank <C> 9.1 <C> 4.2 <C> 5.6 <C> 9.6 <C> 8.9 <C> 8.9 <C> 9.3 <C> 13.0 <C> 10.5 <C> 8.8 <C> 16.3 <C> 11.0 <R> <C> [EMPTY] <C> TopicRank <C> 8.8 <C> 4.2 <C> 5.5 <C> 9.6 <C> 8.9 <C> 8.9 <C> 9.5 <C> 13.2 <C> 10.7 <C> 9.0 <C> 16.5 <C> 11.2 <R> <C> [EMPTY] <C> PositionRank <C> 11.3 <C> 5.3 <C> 7.0 <C> 11.3 <C> 10.5 <C> 10.5 <C> 10.8 <C> 14.9 <C> 12.1 <C> 9.9 <C> 18.1 <C> 12.3 <R> <C> [EMPTY] <C> Multipartite Rank <C> [BOLD] 22.6 <C> [BOLD] 7.9 <C> [BOLD] 11.3 <C> 17 <C> 11.8 <C> 13.3 <C> 13.6 <C> 13.9 <C> 13.1 <C> 11.1 <C> 15 <C> 12.2 <R> <C> [EMPTY] <C> [BOLD] SemanticRank <C> 18.7 <C> 6.5 <C> 9.3 <C> [BOLD] 17 <C> [BOLD] 11.8 <C> [BOLD] 13.3 <C> [BOLD] 15.5 <C> [BOLD] 16 <C> [BOLD] 15 <C> [BOLD] 14.1 <C> [BOLD] 19.5 <C> [BOLD] 15.6 <R> <C> Inspec <C> TextRank <C> 18.7 <C> 3.6 <C> 6.0 <C> 16.1 <C> 5.3 <C> 8.0 <C> 16.3 <C> 5.7 <C> 8.5 <C> 17.5 <C> 9.5 <C> 12.3 <R> <C> [EMPTY] <C> SingleRank <C> 20.1 <C> 4.3 <C> 7.1 <C> 18.5 <C> 6.0 <C> 9.1 <C> 18.2 <C> 9.8 <C> 12.7 <C> 17.0 <C> 10.5 <C> 13.0 <R> <C> [EMPTY] <C> TopicRank <C> 25.9 <C> 4.4 <C> 7.3 <C> 22.6 <C> 7.4 <C> 10.7 <C> 20 <C> 9.7 <C> 12.5 <C> 18.3 <C> 11.7 <C> 13.6 <R> <C> [EMPTY] <C> PositionRank <C> 36.5 <C> 6.2 <C> 10.2 <C> 32.5 <C> 10.6 <C> 15.4 <C> 29.3 <C> 14.1 <C> 18.1 <C> 26.6 <C> 16.8 <C> 19.6 <R> <C> [EMPTY] <C> Multipartite Rank <C> 27.7 <C> 4.6 <C> 7.7 <C> 23.7 <C> 7.8 <C> 11.2 <C> 21 <C> 10.2 <C> 13.1 <C> 19 <C> 12.2 <C> 14.1 <R> <C> [EMPTY] <C> [BOLD] SemanticRank <C> [BOLD] 36.5 <C> [BOLD] 6.2 <C> [BOLD] 10.3 <C> [BOLD] 32.5 <C> [BOLD] 10.6 <C> [BOLD] 15.4 <C> [BOLD] 29.4 <C> [BOLD] 14.1 <C> [BOLD] 18.2 <C> [BOLD] 26.9 <C> [BOLD] 17.1 <C> [BOLD] 19.8 <CAP> Table 2: Performance Comparison
<R> <C> word <C> proportion <C> freq ext peak <C> freq <R> <C> yeah <C> 0.077 <C> 7 <C> 91 <R> <C> oh <C> 0.069 <C> 7 <C> 101 <R> <C> yes <C> 0.054 <C> 11 <C> 204 <R> <C> thank <C> 0.049 <C> 7 <C> 144 <R> <C> no <C> 0.025 <C> 8 <C> 320 <R> <C> - <C> 0.023 <C> 44 <C> 1890 <R> <C> good <C> 0.018 <C> 5 <C> 284 <R> <C> here <C> 0.017 <C> 6 <C> 346 <R> <C> ? <C> 0.016 <C> 29 <C> 1812 <R> <C> … <C> 0.016 <C> 5 <C> 316 <R> <C> . <C> 0.014 <C> 104 <C> 7645 <R> <C> what <C> 0.012 <C> 6 <C> 486 <R> <C> you <C> 0.009 <C> 23 <C> 2458 <R> <C> that <C> 0.008 <C> 6 <C> 725 <R> <C> ’s <C> 0.008 <C> 9 <C> 1102 <R> <C> it <C> 0.005 <C> 5 <C> 914 <R> <C> , <C> 0.004 <C> 16 <C> 3561 <R> <C> i <C> 0.004 <C> 10 <C> 2372 <CAP> Table 7: Word types with the highest proportion of cross-segmental attention peaks, with absolute frequencies of cross-segmental attention peak and overall absolute word frequencies.
<R> <C> [BOLD] Model <C> [BOLD] Entropy  [ITALIC] H <C> [BOLD] Repetition  [BOLD] uni-rep(%) <C> [BOLD] Repetition  [BOLD] bi-rep(%) <C> [BOLD] BLEU  [BOLD] short <C> [BOLD] BLEU  [BOLD] long <C> [BOLD] BLEU  [BOLD] overall <R> <C> Reference <C> – <C> 4.46 <C> 0.08 <C> – <C> – <C> – <R> <C> Transformer-Big <C> 2.08 <C> 5.87 <C> 0.18 <C> 28.9 <C> 29.5 <C> 29.3 <R> <C> Transformer-Big + Dim <C> 2.37 <C> 5.82 <C> 0.16 <C> 29.1 <C> 29.8 <C> 29.7 <R> <C> Transformer-Big + DyDim <C> 2.41 <C> 5.85 <C> 0.16 <C> 29.1 <C> 29.9 <C> 29.7 <CAP> Table 5: Effective coverage entropy H, repetition rate and BLEU score on subsets with short and long source sentences on English-German newstest2014.
<R> <C> Setting <C> Joint EM <C> Joint F1 <R> <C> Baseline (Fine-tuning) <C> 45.91 <C> [BOLD] 73.93 <R> <C> w/o Graph <C> [BOLD] 45.98 <C> 73.78 <R> <C> Baseline (Feature-based) <C> 36.45 <C> 63.75 <R> <C> w/o Graph <C> 32.26 <C> 59.76 <CAP> Table 2: Ablation of graph structure under different settings.
<R> <C> [BOLD] Classifier <C> [BOLD] CFMC5 Acc <C> [BOLD] CFMC5 F1 W <C> [BOLD] CFMC10 Acc <C> [BOLD] CFMC10 F1 W <R> <C> CNN Random <C> 25.0 <C> 22.1 <C> 35.2 <C> 19.2 <R> <C> MNB <C> 47.6 <C> 47.5 <C> 43.9 <C> 37.4 <R> <C> SVM BoW <C> 49.3 <C> 49.1 <C> 47.9 <C> 43.2 <R> <C> SVM TFIDF <C> 47.8 <C> 47.6 <C> 45.7 <C> 41.2 <R> <C> MLP <C> 47.8 <C> 47.6 <C> 49.3 <C> 46.2 <R> <C> CNN <C> 61.7 <C> 61.3 <C> [BOLD] 54.7 <C> 51.3 <R> <C> CNN Ensemble <C> [BOLD] 62.7 <C> [BOLD] 62.2 <C> 53.5 <C> 50.5 <R> <C> CNN GloVe <C> 62.2 <C> 61.3 <C> 54.5 <C> [BOLD] 51.4 <CAP> Table 3: Classification Accuracy for single label classification. Note that all results were obtained on 10-fold cross validation. CNN Random refers to a CNN trained on a random labelling of the dataset. F1 W stands for weighted macro F1-score.
<R> <C> [BOLD] Classifier <C> [BOLD] CFML10 hamming loss <C> [BOLD] CFML10 F1 micro <C> [BOLD] CFML10 F1 macro <C> [BOLD] CFML20 hamming loss <C> [BOLD] CFML20 F1 micro <C> [BOLD] CFML20 F1 macro <R> <C> CNN Random TWE <C> 0.2158 <C> 15.98 <C> 9.39 <C> 0.1207 <C> 12.07 <C> 4.02 <R> <C> MNB BoW <C> 0.1706 <C> 30.57 <C> 25.73 <C> 0.1067 <C> 29.67 <C> 23.41 <R> <C> SVM BoW <C> 0.1713 <C> 36.08 <C> 31.09 <C> 0.1056 <C> 34.93 <C> 30.70 <R> <C> SVM BoW + TF-IDF <C> 0.1723 <C> 38.20 <C> 33.68 <C> 0.1059 <C> 38.55 <C> 34.70 <R> <C> MLP BoW <C> 0.1879 <C> 39.13 <C> 34.92 <C> 0.1167 <C> 38.12 <C> 31.37 <R> <C> CNN TWE <C> [BOLD] 0.1671 <C> 39.20 <C> 32.59 <C> 0. [BOLD] 1023 <C> 38.44 <C> 30.38 <R> <C> CNN Ensemble TWE <C> 0.1703 <C> [BOLD] 45.32 <C> [BOLD] 38.93 <C> 0.1093 <C> [BOLD] 42.75 <C> [BOLD] 37.29 <R> <C> CNN GloVe <C> 0.1676 <C> 39.22 <C> 33.77 <C> 0.1052 <C> 37.56 <C> 30.29 <R> <C> Human <C> - <C> - <C> - <C> - <C> [BOLD] 51.8 <C> [BOLD] 42.7 <CAP> Table 4: Classification Accuracy for multi-label classification. TWE stands for trainable word embeddings initialized with a normal distribution. Note that all results were obtained on 10-fold cross validation. CNN Random refers to a CNN trained on a random labelling of the dataset.
<R> <C> [BOLD] Dataset <C> [BOLD] Features <C> [BOLD] Classifier <C> [BOLD] Soln. category F1 Mi <C> [BOLD] Soln. category F1 Ma <C> [BOLD] Prob. category F1 Mi <C> [BOLD] Prob. category F1 Ma <C> [BOLD] all F1 Mi <C> [BOLD] all F1 Ma <R> <C> CFMC5 <C> only statement <C> cnn <C> 42.73 <C> 46.14 <C> 51.32 <C> 64.35 <C> 46.13 <C> 45.20 <R> <C> CFMC5 <C> only i/o <C> cnn <C> 44.24 <C> 51.73 <C> 74.73 <C> 81.31 <C> 56.42 <C> 55.41 <R> <C> CFMC5 <C> all prob <C> cnn <C> 54.24 <C> 59.91 <C> 71.36 <C> 78.32 <C> 61.71 <C> 61.32 <R> <C> CFML20 <C> only statement <C> cnn <C> 30.83 <C> 17.32 <C> 38.64 <C> 41.82 <C> 33.59 <C> 28.34 <R> <C> CFML20 <C> only i/o <C> cnn <C> 34.63 <C> 19.59 <C> 44.49 <C> 44.34 <C> 38.44 <C> 30.38 <R> <C> CFML20 <C> all prob <C> cnn <C> 34.39 <C> 19.23 <C> 45.36 <C> 44.02 <C> 39.20 <C> 32.59 <CAP> Table 5: Performance on different categories of PWPs for different parts of the PWPs. The rows with ”only statement” features use only the problem description part of the PWP, the rows with ”only i/o” use only the I/O and constraint information, and ”all prob” use the entire PWP. The results under the ”Soln category” column are of those problems that belong to the solution category, those under ”Prob category” belong to the problem category, and those under ”all” are for all the PWPs. So, for example, the F1 Micro score using only I/O and constraint for solution category problems of CFML20 is 34.63. Note that for CFMC5, F1 Mi (F1 Micro) is the same as accuracy, and F1 Ma (F1 Macro) score is a weighted Macro F1-score.
<R> <C> [EMPTY] <C> [BOLD] Native <C> [BOLD] Non-Native <R> <C> WNT/WKT <C> 82.1 <C> 76.6 <R> <C> MSH <C> 89.1 <C> 89.3 <R> <C> CTL <C> 92.0 <C> 90.4 <R> <C> CPS <C> 86.5 <C> 89.7 <R> <C> Total <C> 85.3 <C> 82.3 <CAP> Table 3: Comparison of the average accuracy of human annotators with (Native) and without (Non-Native) English as mother tongue. The performance was calculated for the general purpose subset (WNT/WKT) and the domain specific subsets (MSH, CTL, CPS).
<R> <C> [EMPTY] <C> [EMPTY] <C> [BOLD] WiC-TSV  [BOLD] Acc <C> [BOLD] WiC-TSV  [BOLD] Prec Rec F1 <R> <C> Task-1 <C> BERT <C> 75.3 <C> 71.7 84.9 77.7 <R> <C> Task-1 <C> FastText <C> 53.7 <C> 54.1 57.6 55.7 <R> <C> Task-2 <C> BERT <C> 71.4 <C> 67.7 83.5 74.8 <R> <C> Task-2 <C> FastText <C> 52.7 <C> 52.4 73.6 61.1 <R> <C> Task-3 <C> BERT <C> 76.6 <C> 74.1 82.8 78.2 <R> <C> Task-3 <C> FastText <C> 53.4 <C> 52.8 79.4 63.4 <R> <C> BaselineTrue <C> BaselineTrue <C> 50.8 <C> 50.8 100 67.3 <R> <C> [ITALIC] Human <C> [EMPTY] <C> [ITALIC] 85.3 <C> [ITALIC] 80.2 96.2 87.4 <CAP> Table 4: Performance (mean of three runs) of the two baseline models on the WiC-TSV test set, in terms of accuracy, precision, recall, and F1. Results are shown for three tasks: Task-1 (definition-based), Task-2 (hypernym-based), and Task-3 (both sources of information). BaselineTrue is a naive baseline that always returns “True” and the human performance is computed as described in Section 3.4.
<R> <C> [EMPTY] <C> [EMPTY] <C> [BOLD] WNT/WKT  [BOLD] Acc <C> [BOLD] WNT/WKT  [BOLD] P R F1 <C> [BOLD] CTL  [BOLD] Acc <C> [BOLD] CTL  [BOLD] P R F1 <C> [BOLD] MSH  [BOLD] Acc <C> [BOLD] MSH  [BOLD] P R F1 <C> [BOLD] CPS  [BOLD] Acc <C> [BOLD] CPS  [BOLD] P R F1 <R> <C> T1 <C> BERT <C> 73.3 <C> 74.0 77.7 75.8 <C> 76.2 <C> 65.1 98.9 78.4 <C> 77.6 <C> 73.4 89.0 80.4 <C> 80.0 <C> 70.5 97.9 81.9 <R> <C> T1 <C> FastText <C> 56.2 <C> 58.9 61.9 60.3 <C> 49.8 <C> 39.0 30.8 34.3 <C> 51.7 <C> 52.2 79.2 62.9 <C> 50.4 <C> 45.6 38.5 41.6 <R> <C> T2 <C> BERT <C> 68.6 <C> 70.0 72.9 71.4 <C> 77.9 <C> 66.6 97.8 79.3 <C> 71.9 <C> 65.1 98.4 78.3 <C> 74.4 <C> 64.7 98.7 78.2 <R> <C> T2 <C> FastText <C> 56.8 <C> 58.9 66.3 62.1 <C> 43.1 <C> 43.0 99.3 60.0 <C> 49.1 <C> 50.4 84.0 62.9 <C> 52.0 <C> 48.8 65.0 55.3 <R> <C> T3 <C> BERT <C> 73.5 <C> 76.1 74.2 75.1 <C> 79.2 <C> 67.8 98.2 80.2 <C> 79.8 <C> 75.8 89.6 82.1 <C> 82.1 <C> 73.0 97.9 83.6 <R> <C> T3 <C> FastText <C> 57.1 <C> 58.0 74.0 65.0 <C> 43.1 <C> 43.1 100.0 60.2 <C> 51.1 <C> 51.5 90.3 65.6 <C> 54.0 <C> 50.5 67.1 57.3 <R> <C> BaselineTrue <C> BaselineTrue <C> 53.8 <C> 53.8 100 70.0 <C> 43.1 <C> 43.1 100 60.2 <C> 51.7 <C> 51.7 100 68.2 <C> 46.4 <C> 46.4 100 63.4 <CAP> Table 5: Performance (mean of three runs) for the two baseline models for the three tasks (i.e., T1: definition-based, T2: hypernymy-based, and T3: both sources of information). Results are reported for the four domains: General (WNT/WKT), Cocktails (CTL), Medical Subjects (MSH), and Computer Science (CPS). BaselineTrue is a naive baseline that always returns ”True”. Human performance in terms of accuracy is estimated to be 82.1% (WNT/WKT), 92.0% (CTL), 89.1% (MSH) and 86.5% (CPS) as described in Section 3.4.
<R> <C> [BOLD] Dataset <C> [BOLD] Split <C> [BOLD] positive <C> [BOLD] neutral <C> [BOLD] negative <C> [BOLD] irrelevant <C> [BOLD] Total <R> <C> Twitter <C> Train <C> 415 <C> 1,866 <C> 458 <C> 1,351 <C> 4,090 <R> <C> [EMPTY] <C> Test <C> 104 <C> 467 <C> 114 <C> 338 <C> 1,023 <R> <C> GermEval <C> Train <C> 1,246 <C> 14,497 <C> 5,228 <C> − <C> 20,941 <R> <C> [EMPTY] <C> Dev <C> 149 <C> 1,637 <C> 589 <C> − <C> 2,375 <R> <C> [EMPTY] <C> Test-1 <C> 105 <C> 1,681 <C> 780 <C> − <C> 2,566 <R> <C> [EMPTY] <C> Test-2 <C> 108 <C> 1,237 <C> 497 <C> − <C> 1,842 <R> <C> Mixed <C> Train <C> 1,631 <C> 16,363 <C> 5,686 <C> − <C> 23,680 <R> <C> [EMPTY] <C> Test <C> 209 <C> 2,148 <C> 894 <C> − <C> 3,251 <CAP> TABLE I: Class distribution of the datasets
<R> <C> [BOLD] Language <C> [BOLD] Model <C> [BOLD] Accuracy <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1-score <R> <C> Twitter <C> Baseline  <C> 78.60 <C> − <C> − <C> 69.13 <R> <C> [EMPTY] <C> BaselineO  <C> 79.57 <C> − <C> − <C> 70.23 <R> <C> [EMPTY] <C> Proposed300 <C> [BOLD] 80.94 <C> [BOLD] 76.14 <C> [BOLD] 71.00 <C> [BOLD] 73.00 <R> <C> [EMPTY] <C> Proposed100 <C> 79.57 <C> 75.64 <C> 69.22 <C> 71.70 <R> <C> GermEval <C> Baseline  <C> [BOLD] 75.45 <C> − <C> − <C> 49.00 <R> <C> [EMPTY] <C> BaselineO  <C> 73.73 <C> − <C> − <C> [BOLD] 54.84 <R> <C> [EMPTY] <C> Proposed300 <C> 74.75 <C> [BOLD] 58.57 <C> [BOLD] 52.90 <C> 54.79 <R> <C> [EMPTY] <C> Proposed100 <C> 74.79 <C> 61.17 <C> 49.30 <C> 51.27 <R> <C> Mixed <C> Baseline  <C> 74.75 <C> − <C> − <C> 57.37 <R> <C> [EMPTY] <C> Proposed300 <C> [BOLD] 75.08 <C> [BOLD] 66.50 <C> [BOLD] 58.53 <C> [BOLD] 61.24 <R> <C> [EMPTY] <C> Proposed100 <C> 74.93 <C> 64.68 <C> 58.13 <C> 60.63 <CAP> TABLE III: Performance evaluation of variations of the proposed model and baseline. Showing highest scores in boldface.(subscript “O” stands for oversampling)
<R> <C> Game <C> | [ITALIC] T| <C> | [ITALIC] V| <C> [BOLD] TDQN <C> [BOLD] KGA2C <C> [BOLD] MaxRew <R> <C> 905 <C> 82 <C> 296 <C> 0 <C> 0 <C> 1 <R> <C> acorncourt <C> 151 <C> 343 <C> [BOLD] 1.6 <C> 0.3 <C> 30 <R> <C> advent† <C> 189 <C> 786 <C> 36 <C> 36 <C> 350 <R> <C> adventureland <C> 156 <C> 398 <C> 0 <C> 0 <C> 100 <R> <C> anchor <C> 260 <C> 2257 <C> 0 <C> 0 <C> 100 <R> <C> awaken <C> 159 <C> 505 <C> 0 <C> 0 <C> 50 <R> <C> balances <C> 156 <C> 452 <C> 4.8 <C> [BOLD] 10 <C> 51 <R> <C> deephome <C> 173 <C> 760 <C> 1 <C> 1 <C> 300 <R> <C> detective <C> 197 <C> 344 <C> 169 <C> [BOLD] 207.9 <C> 360 <R> <C> dragon <C> 177 <C> 1049 <C> -5.3 <C> [BOLD] 0 <C> 25 <R> <C> enchanter <C> 290 <C> 722 <C> 8.6 <C> [BOLD] 12.1 <C> 400 <R> <C> inhumane <C> 141 <C> 409 <C> 0.7 <C> [BOLD] 3 <C> 300 <R> <C> jewel <C> 161 <C> 657 <C> 0 <C> [BOLD] 1.8 <C> 90 <R> <C> karn <C> 161 <C> 657 <C> [BOLD] 1.2 <C> 0 <C> 90 <R> <C> library <C> 173 <C> 510 <C> 6.3 <C> [BOLD] 14.3 <C> 30 <R> <C> ludicorp <C> 187 <C> 503 <C> 6 <C> [BOLD] 17.8 <C> 150 <R> <C> moonlit <C> 166 <C> 669 <C> 0 <C> 0 <C> 1 <R> <C> omniquest <C> 207 <C> 460 <C> [BOLD] 16.8 <C> 3 <C> 50 <R> <C> pentari <C> 155 <C> 472 <C> 17.4 <C> [BOLD] 50.7 <C> 70 <R> <C> snacktime <C> 201 <C> 468 <C> [BOLD] 9.7 <C> 0 <C> 50 <R> <C> sorcerer <C> 288 <C> 1013 <C> 5 <C> [BOLD] 5.8 <C> 400 <R> <C> spellbrkr <C> 333 <C> 844 <C> 18.7 <C> [BOLD] 21.3 <C> 600 <R> <C> spirit <C> 169 <C> 1112 <C> 0.6 <C> [BOLD] 1.3 <C> 250 <R> <C> temple <C> 175 <C> 622 <C> [BOLD] 7.9 <C> 7.6 <C> 35 <R> <C> zenon <C> 149 <C> 401 <C> 0 <C> [BOLD] 3.9 <C> 350 <R> <C> zork1 <C> 237 <C> 697 <C> 9.9 <C> [BOLD] 34 <C> 350 <R> <C> zork3 <C> 214 <C> 564 <C> 0 <C> [BOLD] .1 <C> 7 <R> <C> ztuu <C> 186 <C> 607 <C> 4.9 <C> [BOLD] 9.2 <C> 100 <CAP> Table 1: Raw scores comparing KG-A2C to TDQN across a wide set of games supported by Jericho. †Advent starts at a score of 36.
<R> <C> [BOLD] Split <C> [BOLD] Source <C> [BOLD] # Ex. <C> [BOLD] # Ann. <C> [BOLD] Setup <R> <C> train <C> SQuAD <C> 68986 <C> 1 <C> E (74%) S (26%) <R> <C> train <C> Other <C> 4×1000 <C> 1 <C> S <R> <C> dev <C> SQuAD <C> 7350 <C> 1 <C> S <R> <C> dev <C> SQuAD <C> 1000 <C> 3 <C> S <R> <C> dev <C> Other <C> 4×500 <C> 1 <C> S <R> <C> test <C> SQuAD <C> 7377 <C> 1 <C> S <R> <C> test <C> SQuAD <C> 1000 <C> 3 <C> S <R> <C> test <C> Other <C> 4×1000 <C> 3 <C> S <CAP> Table 2: The composition of our collected data. # Ex. and # Ann. refer to the number of unique examples and to the number of annotations (gold answers) per example, respectively. The last column lists the setup that was used for collecting the examples: post-editing (E) or from scratch (S). “Other” denotes the four other QA datasets besides SQuAD. The gray indicates the examples that we used for our evaluations in Section 7.
<R> <C> Dataset Model <C> Dataset Model <C> MovieQA RM <C> MovieQA NM <C> NewsQA RM <C> NewsQA NM <C> QAMR RM <C> QAMR NM <C> RACE RM <C> RACE NM <C> SQuAD RM <C> SQuAD NM <R> <C> Top 1 <C> BLEU <C> 81 <C> 84 <C> 82 <C> 83 <C> 82 <C> 85 <C> 80 <C> 83 <C> 82 <C> [BOLD] 86 <R> <C> Top 1 <C> Match <C> 38 <C> 44 <C> 49 <C> 49 <C> 54 <C> [BOLD] 57 <C> 30 <C> 44 <C> 45 <C> 53 <R> <C> Top 5 <C> BLEU <C> [EMPTY] <C> 89 <C> [EMPTY] <C> 88 <C> [EMPTY] <C> 90 <C> [EMPTY] <C> 89 <C> [EMPTY] <C> [BOLD] 91 <R> <C> Top 5 <C> Match <C> [EMPTY] <C> 52 <C> [EMPTY] <C> 58 <C> [EMPTY] <C> [BOLD] 67 <C> [EMPTY] <C> 55 <C> [EMPTY] <C> 62 <CAP> Table 4: BLEU and exact match scores (out of a 100) when comparing the outputs of Rule-based (RM) and Neural (NM) against the human gold.
<R> <C> System <C> S1 <C> S2 <C> S3 (Proposed) <R> <C> Sep. model <C> BLSTM <C> R/CNN hybrid <C> R/CNN hybrid <R> <C> Enh. method <C> MVDR <C> MVDR <C> FBF-PF <R> <C> MTG0 (4) <C> 22.1 <C> 23.7 <C> 20.7 <R> <C> MTG1 (6) <C> 17.0 <C> 18.1 <C> 18.0 <R> <C> MTG2 (6) <C> 20.6 <C> 21.8 <C> 22.0 <R> <C> MTG3 (8) <C> 28.0 <C> 27.8 <C> 28.4 <R> <C> MTG4 (4) <C> 28.5 <C> 29.8 <C> 29.5 <R> <C> MTG5 (11) <C> 21.0 <C> 22.9 <C> 20.5 <R> <C> Overall <C> 21.5 <C> 22.7 <C> 21.7 <CAP> Table 1: %WER of different methods for meeting transcription. Numbers of meeting attendees shown in parentheses. FBF: fixed beamformer; PF: post-filter.
<R> <C> [BOLD] Lexical Representation <C> F1 <C> UAS <C> LAS <R> <C> Word, POS, CharLSTM <C> 93.50 <C> 95.41 <C> 93.35 <R> <C> Word, POS, CharCNNs <C> 93.46 <C> 95.43 <C> 93.30 <R> <C> Word, CharLSTM <C> [BOLD] 93.83 <C> [BOLD] 95.71 <C> [BOLD] 93.68 <R> <C> Word, CharCNNs <C> 93.70 <C> 95.43 <C> 93.35 <R> <C> POS, CharLSTM <C> 93.16 <C> 95.06 <C> 92.82 <R> <C> CharLSTM <C> 93.80 <C> 95.46 <C> 93.46 <CAP> Table 1: PTB dev set performance on representations.
<R> <C> [EMPTY] <C> classic <C> k1b <C> la1 <C> la12 <C> la2 <R> <C> pre <C> 41681 <C> 21819 <C> 31472 <C> 31472 <C> 31472 <R> <C> post <C> 7616 <C> 10411 <C> 13195 <C> 17741 <C> 12432 <R> <C> % <C> 0.82 <C> 0.52 <C> 0.58 <C> 0.44 <C> 0.6 <CAP> Table 4: Number of features for each dataset before and after feature selection.
<R> <C> Method <C> Split <C> PL <C> NE ↓ <C> SR ↑ <C> SPL ↑ <R> <C> Speaker-Follower model (Fried et al.,  2018 ) <C> U <C> - <C> 6.6 <C> 35.5 <C> - <R> <C> Speaker-Follower model (Fried et al.,  2018 ) <C> S <C> - <C> [BOLD] 3.36 <C> [BOLD] 66.4 <C> - <R> <C> Speaker-Follower model (our implementation) <C> U <C> 15.6 <C> 6.4 <C> 36.0 <C> 29.0 <R> <C> Speaker-Follower model (our implementation) <C> S <C> 15.9 <C> 4.9 <C> 51.9 <C> 43.0 <R> <C> Our implementation, using discriminator pre-training <C> U <C> 16.7 <C> [BOLD] 5.9 <C> [BOLD] 39.1 <C> 26.8 <R> <C> Our implementation, using discriminator pre-training <C> S <C> 15.4 <C> 5.0 <C> 50.4 <C> 39.1 <CAP> Table 5: Results on R2R validation unseen (U) and validation seen (S) paths after initializing navigation agent’s instruction and visual encoders with discriminator.
<R> <C> [EMPTY] <C> [EMPTY] <C> DBP-WD Hits@1 <C> DBP-WD MR <C> DBP-WD MRR <C> DBP-YG Hits@1 <C> DBP-YG MR <C> DBP-YG MRR <R> <C> Indep. <C> Name view <C> 84.30 <C> 1,108 <C> 0.871 <C> 83.60 <C> 1,294 <C> 0.841 <R> <C> Indep. <C> Rel. view <C> 54.24 <C> 169 <C> 0.635 <C> 59.89 <C> 58 <C> 0.680 <R> <C> Indep. <C> Attr. view <C> 17.32 <C> 7,688 <C> 0.215 <C> 51.36 <C> 1,966 <C> 0.558 <R> <C> ITC <C> Rel. view <C> 70.22 <C> 34 <C> 0.769 <C> 68.57 <C> 47 <C> 0.751 <R> <C> ITC <C> Attr. view <C> 61.13 <C> 6,774 <C> 0.648 <C> 62.52 <C> 1,001 <C> 0.688 <CAP> Table 2: Results of entity alignment under independent views
<R> <C> [EMPTY] <C> DBP-WD Hits@1 <C> DBP-WD MR <C> DBP-WD MRR <C> DBP-YG Hits@1 <C> DBP-YG MR <C> DBP-YG MRR <R> <C> Rel. view <C> 67.59 <C> 301 <C> 0.726 <C> 12.71 <C> 3,504 <C> 0.174 <R> <C> Attr. view <C> 49.08 <C> 986 <C> 0.551 <C> 57.78 <C> 701 <C> 0.630 <R> <C> MultiKE-ITC <C> 83.98 <C> 421 <C> 0.866 <C> 58.46 <C> 732 <C> 0.636 <CAP> Table 3: Results of unsupervised entity alignment with MultiKE-ITC
<R> <C> Dataset WN18 <C> #R 18 <C> #E 40,943 <C> #Trip. (Train/ Valid/ Test) 141,442 <C> #Trip. (Train/ Valid/ Test) 5,000 <C> #Trip. (Train/ Valid/ Test) 5,000 <R> <C> FB15K <C> 1,345 <C> 14,951 <C> 483,142 <C> 50,000 <C> 59,071 <R> <C> AK18K <C> 7 <C> 18,464 <C> 130,265 <C> 7,429 <C> 7,336 <CAP> Table 3. Datasets used in knowledge embedding.
<R> <C> Dataset <C> #Paper <C> #Author <C> #Venue <C> #Edge <R> <C> FOS_Biology <C> 1,211,664 <C> 2,169,820 <C> 13,511 <C> 5,544,376 <R> <C> FOS_CS <C> 452,970 <C> 738,253 <C> 10,726 <C> 1,658,917 <R> <C> FOS_Economics <C> 412,621 <C> 597,121 <C> 8,269 <C> 1,163,700 <R> <C> FOS_Medicine <C> 182,002 <C> 491,447 <C> 7,251 <C> 819,312 <R> <C> FOS_Physics <C> 449,844 <C> 596,117 <C> 5,465 <C> 1,602,723 <R> <C> FOS_5Fields <C> 2,578,185 <C> 3,868,419 <C> 18,533 <C> 10,160,137 <R> <C> Google <C> 600,391 <C> 635,585 <C> 151 <C> 2,373,109 <CAP> Table 5. Datasets used in network representation learning.
<R> <C> Metric <C> Method <C> FOS_BI <C> FOS_CS <C> FOS_EC <C> FOS_ME <C> FOS_PH <C> FOS_5F <C> Google <R> <C> Micro-F1 <C> DeepWalk <C> 0.792 <C> 0.545 <C> 0.692 <C> 0.663 <C> 0.774 <C> 0.731 <C> 0.948 <R> <C> Micro-F1 <C> LINE(1st+2nd) <C> 0.722 <C> 0.633 <C> 0.717 <C> 0.701 <C> 0.779 <C> 0.755 <C> 0.955 <R> <C> Micro-F1 <C> PTE <C> 0.759 <C> 0.574 <C> 0.654 <C> 0.694 <C> 0.723 <C> 0.664 <C> 0.966 <R> <C> Micro-F1 <C> metapath2vec <C> 0.828 <C> 0.678 <C> 0.753 <C> 0.770 <C> 0.794 <C> 0.831 <C> 0.971 <R> <C> Macro-F1 <C> DeepWalk <C> 0.547 <C> 0.454 <C> 0.277 <C> 0.496 <C> 0.592 <C> 0.589 <C> 0.942 <R> <C> Macro-F1 <C> LINE(1st+2nd) <C> 0.445 <C> 0.542 <C> 0.385 <C> 0.577 <C> 0.640 <C> 0.655 <C> 0.949 <R> <C> Macro-F1 <C> PTE <C> 0.495 <C> 0.454 <C> 0.276 <C> 0.555 <C> 0.571 <C> 0.528 <C> 0.961 <R> <C> Macro-F1 <C> metapath2vec <C> 0.637 <C> 0.570 <C> 0.485 <C> 0.659 <C> 0.635 <C> 0.682 <C> 0.968 <CAP> Table 6. Results of scholar classification.
<R> <C> Model <C> FOS-labeled <C> Google-labeled <R> <C> DeepWalk <C> 0.277 <C> 0.394 <R> <C> LINE(1st+2nd) <C> 0.305 <C> 0.459 <R> <C> PTE <C> 0.153 <C> 0.602 <R> <C> metapath2vec <C> 0.427 <C> 0.836 <CAP> Table 7. Results of scholar clustering.
<R> <C> Model <C> BLEU <R> <C> RNNSearch <C> 26.10 <R> <C> Seq2Seq (our reimplementation) <C> 25.90 <R> <C> [BOLD] FPB <C> [BOLD] 27.70 <CAP> Table 2: Results of the models on the English-Vietnamese translation.
<R> <C> Model <C> BLEU <R> <C> Seq2Seq (our reimplementation) <C> 25.90 <R> <C> +length predictor <C> 26.26 <R> <C> +BOW predictor <C> 27.38 <R> <C> [BOLD] FPB <C> [BOLD] 27.70 <CAP> Table 3: Ablation test on the English-Vietnamese translation. Seq2Seq refers to our reimplementation of the attention-based Seq2Seq model
<R> <C> comp  [BOLD] Neural network models <C> comp  [BOLD] Neural network models <C> cont  [BOLD] Neural network models <C> exp  [BOLD] Neural network models <C> temp  [BOLD] Neural network models <R> <C> Zhang et al. ( 2015 ) <C> 33.2 <C> 52.0 <C> 69.6 <C> 30.5 <R> <C> Liu and Li ( 2016 ) <C> 36.7 <C> 54.5 <C> 70.4 <C> [BOLD] 38.8 <R> <C> Qin et al. ( 2016 ) <C> [BOLD] 41.6 <C> [BOLD] 57.3 <C> [BOLD] 71.5 <C> 35.4 <R> <C> [BOLD] Recent feature-rich models <C> [BOLD] Recent feature-rich models <C> [BOLD] Recent feature-rich models <C> [BOLD] Recent feature-rich models <C> [BOLD] Recent feature-rich models <R> <C> Rutherford and Xue ( 2014 ) <C> 39.7 <C> 54.4 <C> 70.2 <C> 28.7 <R> <C> Braud and Denis ( 2015 ) <C> 36.4 <C> 55.8 <C> 67.4 <C> 29.3 <R> <C> [BOLD] This work’s models <C> [BOLD] This work’s models <C> [BOLD] This work’s models <C> [BOLD] This work’s models <C> [BOLD] This work’s models <R> <C> [ITALIC] AverageFeats <C> 36.3 <C> 55.9 <C> 69.4 <C> 30.5 <R> <C> [ITALIC] AverageFeats+SRL <C> 37.0 <C> 56.3 <C> 69.4 <C> 32.1 <R> <C> [ITALIC] AllFeats <C> 34.5 <C> 51.3 <C> 60.4 <C> 26.8 <CAP> Table 1: One-vs-all results in F1-score on the four PDTB top-level relations (comparison, contingency, expansion and temporal). Best overall results are marked in bold, best results by feature-rich models are underlined.
<R> <C> [BOLD] Label <C> [BOLD] Happy <C> [BOLD] Sad <C> [BOLD] Angry <C> [BOLD] Others <C> [ITALIC]  [BOLD] Total <R> <C> [BOLD] # <C> 109 <C> 107 <C> 90 <C> 1920 <C> 2226 <R> <C> [BOLD] % <C> 4.90 <C> 4.81 <C> 4.04 <C> 86.25 <C> 100 <CAP> Table 1. Emotion class label distribution in evaluation dataset.
<R> <C> Method <C> Success <C> Funniness <C> Grammar <R> <C> NJD <C> 9.2% <C> 1.4 <C> 2.6 <R> <C> R <C> 4.6% <C> 1.3 <C> [BOLD] 3.9 <R> <C> R+S <C> 27.0% <C> 1.6 <C> 3.5 <R> <C> R+S+T+M <C> 28.8% <C> [BOLD] 1.7 <C> 2.9 <R> <C> SurGen <C> [BOLD] 31.4% <C> [BOLD] 1.7 <C> 3.0 <R> <C> Human <C> 78.9% <C> 3.0 <C> 3.8 <CAP> Table 3: Human evaluation results of all systems. We show average scores of funniness and grammaticality on a 1-5 scale and success rate computed from yes/no responses. We compare with two baselines: NeuralJointDecoder (NJD) and Retrieve (R). R+S, SurGen, and R+S+T+M are three variations of our method: Retrieve+Swap, Retrieve+Swap+Topic, and Retrieve+Swap+Topic+Smoother, respectively. Overall, SurGen performs the best across the board.
<R> <C> [BOLD] Ranking <C> [BOLD] Feature <C> [BOLD] Linkability Ratio  [BOLD] Top-1(%) <C> [BOLD] Linkability Ratio  [BOLD] Top-4(%) <R> <C> 1 <C> Top Letter Trigrams <C> 91 <C> 96 <R> <C> 2 <C> POS Bigrams <C> 89 <C> 96 <R> <C> 3 <C> Top Letter Bigrams <C> 86 <C> 94 <R> <C> 4 <C> Words <C> 79 <C> 94 <R> <C> 5 <C> POS Tags <C> 78 <C> 90 <R> <C> 9 <C> [ITALIC] WPall <C> 52.5 <C> 82.5 <CAP> Table 2: LRs of best five Writeprint features individually and WPall, with |AR|=5
<R> <C> [BOLD] Models <C> [BOLD] BLEU <C> [BOLD] PPL - Gen. <C> [BOLD] PPL - Trg. <C> [BOLD] EA <C> [BOLD] UNI <C> [BOLD] Length <R> <C> [BOLD] Baseline <C> 10.36 <C> 970.05 <C> 495.35 <C> 0.86I <C> 3.55 ± 1.77 <C> 5.5 ± 1.97 <R> <C> [BOLD] Baseline + MTL <C> 12.4 <C> 357.59 <C> 331.22 <C> 0.93 <C> 3.31 ± 1.68 <C> 5.59 ± 1.89 <R> <C> [BOLD] UNION w/o CoSE <C> 13.28 <C> 62.89 <C> 238.64 <C> 0.96 <C> 5.82 ± 2.10 <C> 8.51 ± 2.08 <R> <C> [BOLD] UNION w/o OpenBook <C> 13.75 <C> 142.19 <C> 260.38 <C> 0.95 <C> 4.29 ± 1.87 <C> 6.46 ± 2.19 <R> <C> [BOLD] UNION w/o OMCS <C> 15.7 <C> 194.66 <C> 243.83 <C> 0.94 <C> 4.29 ± 1.79 <C> 6.41 ± 2.06 <R> <C> [BOLD] UNION <C> 16.36 <C> 135.1 <C> 212.1 <C> 0.97 <C> 4.53 ± 2.05 <C> 6.59 ± 2.3 <CAP> Table 2: Ablation study results on different proposed models
<R> <C> [EMPTY] <C> [BOLD] Clinical  [BOLD] words <C> [BOLD] Clinical  [BOLD] words <C> [BOLD] Clinical  [BOLD] numerals <C> [BOLD] Clinical  [BOLD] numerals <C> [BOLD] Clinical  [BOLD] total <C> [BOLD] Clinical  [BOLD] total <C> [BOLD] Scientific  [BOLD] words <C> [BOLD] Scientific  [BOLD] words <C> [BOLD] Scientific  [BOLD] numerals <C> [BOLD] Scientific  [BOLD] numerals <C> [BOLD] Scientific  [BOLD] total <C> [BOLD] Scientific  [BOLD] total <R> <C> [BOLD] Model <C> [BOLD] PP <C> [BOLD] APP <C> [BOLD] PP <C> [BOLD] APP <C> [BOLD] PP <C> [BOLD] APP <C> [BOLD] PP <C> [BOLD] APP <C> [BOLD] PP <C> [BOLD] APP <C> [BOLD] PP <C> [BOLD] APP <R> <C> softmax <C> 4.08 <C> 5.99 <C> 12.04 <C> 58443.72 <C> 4.28 <C> 8.91 <C> 33.96 <C> 51.83 <C> 127.12 <C> 3505856.25 <C> 35.79 <C> 80.62 <R> <C> softmax+rnn <C> 4.03 <C> 5.91 <C> [BOLD] 11.57 <C> 56164.81 <C> 4.21 <C> 8.77 <C> [BOLD] 33.54 <C> 51.20 <C> [BOLD] 119.68 <C> 3300688.50 <C> [BOLD] 35.28 <C> 79.47 <R> <C> h-softmax <C> [BOLD] 4.00 <C> 4.96 <C> 11.78 <C> 495.95 <C> [BOLD] 4.19 <C> 6.05 <C> 34.73 <C> 49.81 <C> 122.67 <C> 550.98 <C> 36.51 <C> 54.80 <R> <C> h-softmax+rnn <C> 4.03 <C> 4.99 <C> 11.65 <C> 490.14 <C> 4.22 <C> 6.09 <C> 34.04 <C> 48.83 <C> 120.83 <C> 542.70 <C> 35.80 <C> 53.73 <R> <C> d-RNN <C> 3.99 <C> [BOLD] 4.95 <C> 263.22 <C> 263.22 <C> 4.79 <C> 5.88 <C> 34.08 <C> 48.89 <C> 519.80 <C> [BOLD] 519.80 <C> 37.98 <C> 53.70 <R> <C> MoG <C> 4.03 <C> 4.99 <C> 226.46 <C> 226.46 <C> 4.79 <C> 5.88 <C> 34.14 <C> 48.97 <C> 683.16 <C> 683.16 <C> 38.45 <C> 54.37 <R> <C> combination <C> 4.01 <C> 4.96 <C> 197.59 <C> [BOLD] 197.59 <C> 4.74 <C> [BOLD] 5.82 <C> 33.64 <C> [BOLD] 48.25 <C> 520.95 <C> 520.95 <C> 37.50 <C> [BOLD] 53.03 <CAP> Table 2: Test set perplexities for the clinical and scientific data. Adjusted perplexities (APP) are directly comparable across all data and models, but perplexities (PP) are sensitive to the varying out-of-vocabulary rates.
<R> <C> A1 <C> A2 <C> A3 <C> A4 <C> A5 <C> A6 <C> A7 <C> [ITALIC] S <R> <C> 0.5333 <C> 0.6029 <C> 0.5271 <C> 0.5880 <C> 0.5313 <C> 0.4679 <C> 0.5529 <C> [BOLD] 0.6570 <CAP> Table 3: Performance des personnes sur la tâche T4.
<R> <C> # <C> Sample question <C> [BOLD] TransE (SINGLE) H@1(%) <C> [BOLD] TransE (SINGLE) Mean Filtered Rank <C> [BOLD] TransE (COMP) H@1(%) <C> [BOLD] TransE (COMP) Mean Filtered Rank <C> [BOLD] TransGaussian (SINGLE) H@1(%) <C> [BOLD] TransGaussian (SINGLE) Mean Filtered Rank <C> [BOLD] TransGaussian (COMP) H@1(%) <C> [BOLD] TransGaussian (COMP) Mean Filtered Rank <R> <C> 1 <C> which club does alan pulido play for? <C> 88.59 <C> 1.18 <C> 91.95 <C> 1.11 <C> 96.64 <C> 1.04 <C> [BOLD] 98.66 <C> [BOLD] 1.01 <R> <C> 2 <C> what position does gonzalo higuain play? <C> [ITALIC] 100.00 <C> [ITALIC] 1.00 <C> 98.11 <C> 1.03 <C> 98.74 <C> 1.01 <C> [ITALIC] 100.00 <C> [ITALIC] 1.00 <R> <C> 3 <C> how old is samuel etoo? <C> 67.11 <C> 1.44 <C> 90.79 <C> 1.13 <C> 94.74 <C> 1.08 <C> [BOLD] 97.37 <C> [BOLD] 1.04 <R> <C> 4 <C> what is the jersey number of mario balotelli? <C> 45.00 <C> 1.89 <C> 83.57 <C> 1.22 <C> 97.14 <C> 1.03 <C> [BOLD] 99.29 <C> [BOLD] 1.01 <R> <C> 5 <C> which country is thomas mueller from ? <C> 94.40 <C> 1.06 <C> 94.40 <C> 1.06 <C> 96.80 <C> 1.04 <C> [BOLD] 98.40 <C> [BOLD] 1.02 <R> <C> 6 <C> which country is the soccer team fc porto based in ? <C> [ITALIC] 98.48 <C> [ITALIC] 1.02 <C> [ITALIC] 98.48 <C> [ITALIC] 1.02 <C> 93.94 <C> 1.06 <C> 95.45 <C> 1.05 <R> <C> 7 <C> who plays professionally at liverpool fc? <C> 95.12 <C> 1.10 <C> 90.24 <C> 1.20 <C> [BOLD] 98.37 <C> [ITALIC] 1.04 <C> 96.75 <C> [ITALIC] 1.04 <R> <C> 8 <C> which player is from iran? <C> 89.86 <C> 1.51 <C> 76.81 <C> 2.07 <C> 38.65 <C> 2.96 <C> [BOLD] 99.52 <C> [BOLD] 1.00 <R> <C> 9 <C> name a player who plays goalkeeper? <C> 98.96 <C> 1.01 <C> 69.79 <C> 1.82 <C> 42.71 <C> 5.52 <C> [BOLD] 100.00 <C> [BOLD] 1.00 <R> <C> 10 <C> which soccer club is based in mexico? <C> 22.03 <C> 13.94 <C> [BOLD] 30.51 <C> [BOLD] 8.84 <C> 6.78 <C> 10.66 <C> 16.95 <C> 21.14 <R> <C> 11 <C> where is the club that edin dzeko plays for ? <C> 52.63 <C> 3.88 <C> 57.24 <C> 2.10 <C> 47.37 <C> 2.27 <C> [BOLD] 78.29 <C> [BOLD] 1.41 <R> <C> 12 <C> name a soccer club that has a player from australia ? <C> 30.43 <C> 12.08 <C> [BOLD] 33.70 <C> [BOLD] 11.47 <C> 13.04 <C> 11.64 <C> 19.57 <C> 17.57 <R> <C> Overall (Path Query) <C> Overall (Path Query) <C> 74.16 <C> 3.11 <C> 77.39 <C> [BOLD] 2.56 <C> 69.54 <C> 3.02 <C> [BOLD] 85.94 <C> 3.52 <R> <C> 13 <C> who plays forward for fc barcelona? <C> 97.55 <C> 1.06 <C> 76.07 <C> 1.66 <C> 93.25 <C> 1.24 <C> [BOLD] 98.77 <C> [BOLD] 1.02 <R> <C> 14 <C> who are the defenders on german national team? <C> 95.93 <C> 1.06 <C> 69.92 <C> 2.33 <C> 65.04 <C> 2.04 <C> [BOLD] 100.00 <C> [BOLD] 1.00 <R> <C> 15 <C> which player in ssc napoli is from argentina? <C> 88.81 <C> 1.17 <C> 76.12 <C> 1.76 <C> 88.81 <C> 1.35 <C> [BOLD] 97.76 <C> [BOLD] 1.03 <R> <C> Overall (Conj. Query) <C> Overall (Conj. Query) <C> 94.29 <C> 1.09 <C> 74.29 <C> 1.89 <C> 83.57 <C> 1.51 <C> [BOLD] 98.81 <C> [BOLD] 1.02 <CAP> Table 2: Results of joint learning with path queries and conjunction queries on WorldCup2014.
<R> <C> # <C> Relation <C> [BOLD] TransE (SINGLE) H@1(%) <C> [BOLD] TransE (SINGLE) Mean Filtered Rank <C> [BOLD] TransE (COMP) H@1(%) <C> [BOLD] TransE (COMP) Mean Filtered Rank <C> [BOLD] TransGaussian (SINGLE) H@1(%) <C> [BOLD] TransGaussian (SINGLE) Mean Filtered Rank <C> [BOLD] TransGaussian (COMP) H@1(%) <C> [BOLD] TransGaussian (COMP) Mean Filtered Rank <R> <C> 1 <C> plays_in_club <C> 75.54 <C> 1.38 <C> 93.48 <C> 1.09 <C> [BOLD] 99.86 <C> [BOLD] 1.00 <C> 98.51 <C> 1.02 <R> <C> 2 <C> plays_position <C> 96.33 <C> 1.04 <C> 94.02 <C> 1.09 <C> 98.37 <C> 1.02 <C> [BOLD] 100.00 <C> [BOLD] 1.00 <R> <C> 3 <C> is_aged <C> 55.03 <C> 1.69 <C> 91.44 <C> 1.12 <C> 96.88 <C> 1.03 <C> [BOLD] 100.00 <C> [BOLD] 1.00 <R> <C> 4 <C> wears_number <C> 38.86 <C> 2.09 <C> 78.67 <C> 1.32 <C> 95.92 <C> 1.04 <C> [BOLD] 100.00 <C> [BOLD] 1.00 <R> <C> 5 <C> plays_for_country <C> 71.60 <C> 1.39 <C> 94.84 <C> 1.10 <C> 99.32 <C> 1.01 <C> [BOLD] 100.00 <C> [BOLD] 1.00 <R> <C> 6 <C> is_in_country <C> 98.32 <C> 1.03 <C> 99.66 <C> 1.00 <C> 99.33 <C> 1.01 <C> [BOLD] 100.00 <C> [BOLD] 1.00 <R> <C> 7 <C> plays\_in\_club−1 <C> 87.50 <C> 1.46 <C> 83.42 <C> 1.45 <C> 94.70 <C> 1.07 <C> [BOLD] 97.42 <C> [BOLD] 1.03 <R> <C> 8 <C> plays\_for\_country−1 <C> 82.47 <C> 1.68 <C> 68.21 <C> 3.37 <C> 25.27 <C> 5.66 <C> [BOLD] 98.78 <C> [BOLD] 1.02 <R> <C> 9 <C> plays\_position−1 <C> [BOLD] 100.00 <C> [BOLD] 1.00 <C> 75.54 <C> 1.60 <C> 13.59 <C> 24.35 <C> 98.78 <C> 1.02 <R> <C> 10 <C> is\_in\_country−1 <C> 23.11 <C> 26.92 <C> [BOLD] 23.48 <C> [BOLD] 23.27 <C> 8.32 <C> 130.59 <C> 19.41 <C> 83.61 <R> <C> 11 <C> plays_in_club / is_in_country <C> 20.24 <C> 7.05 <C> 58.29 <C> 1.98 <C> 46.88 <C> 2.99 <C> [BOLD] 80.16 <C> [BOLD] 1.38 <R> <C> 12 <C> plays\_for\_country−1 / plays_in_club <C> 25.32 <C> 22.27 <C> [BOLD] 27.73 <C> [BOLD] 10.04 <C> 19.04 <C> 35.59 <C> 20.15 <C> 33.01 <R> <C> Overall (Path relations) <C> Overall (Path relations) <C> 64.64 <C> 5.09 <C> 75.02 <C> [BOLD] 3.59 <C> 67.22 <C> 14.87 <C> [BOLD] 86.73 <C> 8.79 <R> <C> 13 <C> plays\_position−1 and plays\_in\_club−1 <C> 91.85 <C> 1.20 <C> 69.97 <C> 1.82 <C> 77.45 <C> 1.83 <C> [BOLD] 95.38 <C> [BOLD] 1.06 <R> <C> 14 <C> plays\_position−1 and  plays\_for\_country−1 <C> 91.71 <C> 1.23 <C> 66.71 <C> 2.85 <C> 51.49 <C> 4.88 <C> [BOLD] 97.83 <C> [BOLD] 1.05 <R> <C> 15 <C> plays\_in\_club−1 and  is\_in\_country−1 <C> 88.59 <C> 1.20 <C> 73.37 <C> 1.80 <C> 83.42 <C> 1.34 <C> [BOLD] 94.70 <C> [BOLD] 1.08 <R> <C> Overall (Conj. relations) <C> Overall (Conj. relations) <C> 90.72 <C> 1.21 <C> 70.02 <C> 2.16 <C> 70.79 <C> 2.68 <C> [BOLD] 95.97 <C> [BOLD] 1.06 <CAP> Table 8: Evaluation of embeddings. We evaluate the embeddings by feeding the correct entities and relations from a path or conjunctive query to an embedding model and using its scoring function to retrieve the answers from the embedded knowledge base.
<R> <C> [BOLD] Regression <C> Power <C> Sentiment <C> Agency <R> <C> ELMo <C> 0.78 <C> [BOLD] 0.84 <C> 0.76 <R> <C> BERT <C> [BOLD] 0.79 <C> 0.83 <C> [BOLD] 0.78 <R> <C> BERT-masked <C> 0.64 <C> 0.70 <C> 0.62 <R> <C> [BOLD] ASP <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> Power <C> Sentiment <C> Agency <R> <C> ELMo <C> 0.65 <C> 0.76 <C> 0.63 <R> <C> BERT <C> 0.65 <C> 0.71 <C> 0.66 <R> <C> BERT-masked <C> 0.41 <C> 0.47 <C> 0.41 <CAP> Table 2: Pearson correlations between gold NRC VAD labels and scores predicted by our models. Correlations are generally high, with the regression method outperforming ASP. All correlations are statistically significant (p<1e−75).
<R> <C> [EMPTY] <C> Regression <C> ASP <R> <C> ELMo <C> 0.51 <C> 0.21 <R> <C> BERT <C> 0.38 <C> 0.38 <R> <C> BERT-masked <C> 0.17 <C> -0.085 <R> <C> ELMo + Freq <C> [BOLD] 0.65 <C> 0.48 <R> <C> Frequency Baseline <C> 0.61 <C> 0.61 <R> <C> Field et al. ( 2019 ) <C> -0.12 <C> -0.12 <CAP> Table 3: Spearman correlations between automatically induced power scores and Forbes power ranking. Correlations for ELMo regression (p=0.029), ELMo regression + Freq (p=0.003), and the frequency baseline (p=0.007) are statistically significant. The ELMo regression + Freq model performs the best.
<R> <C> [BOLD] Full annotation set (383 pairs) <C> [BOLD] Full annotation set (383 pairs) Regression <C> [BOLD] Full annotation set (383 pairs) ASP <R> <C> ELMo <C> 44.9 <C> 43.6 <R> <C> BERT <C> 41.8 <C> 49.3 <R> <C> BERT-masked <C> 49.6 <C> [BOLD] 59.0 <R> <C> Frequency Baseline <C> 58.0 <C> 58.0 <R> <C> [BOLD] Reduced annotation set (49 pairs) <C> [BOLD] Reduced annotation set (49 pairs) <C> [BOLD] Reduced annotation set (49 pairs) <R> <C> [EMPTY] <C> Regression <C> ASP <R> <C> ELMo <C> 36.7 <C> 42.8 <R> <C> BERT <C> 42.9 <C> 49.0 <R> <C> BERT-masked <C> 53.1 <C> 55.1 <R> <C> Frequency Baseline <C> 57.1 <C> 57.1 <R> <C> Field et al. ( 2019 ) <C> [BOLD] 71.4 <C> [BOLD] 71.4 <CAP> Table 4: Accuracy for scoring how powerful entities are as compared with annotations over articles related to the #MeToo movement. Our metrics do not consistently outperform the baselines, suggesting ELMo and BERT embeddings fail to transfer across domains.
<R> <C> Teacher <C> #BPE <C> Perplexity <C> WER <R> <C> - <C> 10K <C> 61.59 <C> 15.91 <R> <C> Large (pre-trained) <C> 10K <C> 54.35 <C> 15.78 <R> <C> - <C> 5K <C> 50.09 <C> 15.97 <R> <C> Large (pre-trained) <C> 5K <C> 43.75 <C> 15.79 <CAP> Table 4: Effect of Model Pre-training and Knowledge Distillation with Transformer Small two.
<R> <C> Pre-trained <C> #BPE <C> Perplexity <C> WER <R> <C> No <C> 10K <C> 46.68 <C> 15.60 <R> <C> Yes <C> 10K <C> 36.85 <C> 15.45 <R> <C> No <C> 5K <C> 36.42 <C> 15.64 <R> <C> Yes <C> 5K <C> 31.78 <C> 15.44 <CAP> Table 5: Effect of Model Pre-training with Transformer Large.
<R> <C> [BOLD] Embedding <C> [BOLD] Original d <C> [BOLD] Original p <C> [BOLD] Conceptor Negation d <C> [BOLD] Conceptor Negation p <R> <C> GloVe <C> 1.35 <C> 0.00 <C> 0.69 <C> 0.01 <R> <C> word2vec <C> -0.27 <C> [BOLD] 0.27 <C> -0.55 <C> [BOLD] 0.72 <R> <C> Fasttext <C> 0.41 <C> 0.04 <C> -0.27 <C> [BOLD] 0.57 <R> <C> ELMo <C> 1.37 <C> 0.00 <C> -0.45 <C> [BOLD] 0.20 <R> <C> BERT <C> 0.92 <C> 0.00 <C> 0.36 <C> [BOLD] 0.61 <CAP> Table 7: Racial Debiasing: (European American Names, African American Names) vs (Pleasant, Unpleasant). d is the effect size, which we want to be close to 0 and p is the p-value, which we want to be larger than 0.05.
<R> <C> [BOLD] Methods: <C> Clarity Holdout set <C> Clarity Test set <C> Conciseness Holdout set <C> Conciseness Test set <R> <C> [ITALIC] Deep Approach <C> 0.2265 <C> 0.2465 <C> 0.3379 <C> 0.3505 <R> <C> [ITALIC] Shallow Approach <C> 0.2271 <C> 0.2468 <C> 0.3295 <C> 0.3477 <R> <C> [ITALIC] Ensemble <C> [BOLD] 0.2191 <C> [BOLD] 0.2438 <C> [BOLD] 0.3187 <C> [BOLD] 0.3385 <CAP> Table 2. Final results on Test and Holdout Set in RMSE.
<R> <C> [BOLD] Algorithms <C> RMSE-CLR <C> RMSE-CON <R> <C> Random Forest <C> 0.2356 <C> 0.3465 <R> <C> GBM <C> 0.2305 <C> 0.3315 <R> <C> XGBoost <C> 0.2283 <C> 0.3305 <R> <C> LightGBM <C> [BOLD] 0.2271 <C> [BOLD] 0.3295 <CAP> Table 3. Shallow Learning results on Holdout set in RMSE.
<R> <C> Dataset <C> question # <C> question word# <C> annotated evidence # <C> annotated evidence word# <R> <C> Train <C> 36,145 <C> 374,500 <C> 140,897 <C> 10,757,652 <R> <C> Validation <C> 3,018 <C> 36,666 <C> 5,412 <C> 233,911 <R> <C> Test <C> 3,024 <C> 36,815 <C> 5,445 <C> 234,258 <CAP> Table 2: An example from WebQA.
<R> <C> Model <C> Strict Score P(%) <C> Strict Score R(%) <C> Strict Score F1(%) <C> Fuzzy Score P(%) <C> Fuzzy Score R(%) <C> Fuzzy Score F1(%) <R> <C> LSTM+softmax <C> 59.38 <C> 68.77 <C> 63.73 <C> 63.58 <C> 73.63 <C> 68.24 <R> <C> LSTM+CRF <C> 63.72 <C> [BOLD] 76.09 <C> 69.36 <C> 67.53 <C> [BOLD] 80.63 <C> 73.50 <R> <C> BIDAF <C> 70.04 <C> 70.04 <C> 70.04 <C> 74.43 <C> 74.43 <C> 74.43 <R> <C> A3Net base model(without AT) <C> 71.03 <C> 71.03 <C> 71.03 <C> 75.46 <C> 75.46 <C> 75.46 <R> <C> A3Net(random noise) <C> 71.28 <C> 71.28 <C> 71.28 <C> 75.89 <C> 75.89 <C> 75.89 <R> <C> A3Net <C> [BOLD] 72.51 <C> 72.51 <C> [BOLD] 72.51 <C> [BOLD] 77.01 <C> 77.01 <C> [BOLD] 77.01 <CAP> Table 4: Evaluation results on the test dataset of WebQA.
<R> <C> Target variable none (base model) <C> Strict Score 71.03 <C> Fuzzy Score 75.46 <C> Target variable ^ [ITALIC] vP1 <C> Strict Score 71.85 <C> Fuzzy Score 76.28 <R> <C> [ITALIC] wP <C> 71.95 <C> 76.62 <C> ^ [ITALIC] hP <C> 71.56 <C> 76.42 <R> <C> [ITALIC] uP <C> 72.06 <C> 76.39 <C> ^ [ITALIC] vP <C> 72.28 <C> 76.81 <R> <C> ^ [ITALIC] uP <C> 71.32 <C> 75.92 <C> [ITALIC] wP and ^ [ITALIC] vP <C> [BOLD] 72.51 <C> [BOLD] 77.01 <CAP> Table 6: Comparison of adversarial training results on different target variables. The symbols in this table is corresponding with Fig. 1
<R> <C> Method <C> UAS <C> LAS <R> <C> No tags <C> No tags <C> No tags <R> <C> Dyer et al. (2015) <C> 92.70 <C> 90.30 <R> <C> Ours (window-based) <C> [BOLD] 92.85 <C> [BOLD] 90.77 <R> <C> Universal tagset <C> Universal tagset <C> Universal tagset <R> <C> Pipeline ( [ITALIC] Ptag) <C> 92.52 <C> 90.50 <R> <C> Stackprop <C> [BOLD] 93.23 <C> [BOLD] 91.30 <R> <C> Fine tagset <C> Fine tagset <C> Fine tagset <R> <C> Chen & Manning (2014) <C> 91.80 <C> 89.60 <R> <C> Dyer et al. (2015) <C> 93.10 <C> 90.90 <R> <C> Pipeline ( [ITALIC] Ptag) <C> 93.10 <C> 91.16 <R> <C> Stackprop <C> [BOLD] 93.43 <C> [BOLD] 91.41 <R> <C> Weiss et al. (2015) <C> 93.99 <C> 92.05 <R> <C> Alberti et al. (2015) <C> 94.23 <C> 92.36 <CAP> Table 3: WSJ Test set results for greedy and state-of-the-art methods. For reference, we show the most accurate models from albert-etAl:2015:EMNLP and weiss-etAl:2015:ACL, which use a deeper model and beam search for inference.
<R> <C> Model Variant <C> UAS <C> LAS <C> POS <R> <C> [ITALIC] Arc-standard transition system <C> [ITALIC] Arc-standard transition system <C> [ITALIC] Arc-standard transition system <C> [EMPTY] <R> <C> Pipeline ( [ITALIC] Ptag) <C> 81.56 <C> 76.55 <C> 95.14 <R> <C> Ours (window-based) <C> 82.08 <C> 77.08 <C> - <R> <C> Ours (Stackprop) <C> [BOLD] 83.38 <C> [BOLD] 78.78 <C> - <R> <C> [ITALIC] Joint parsing & tagging transition system <C> [ITALIC] Joint parsing & tagging transition system <C> [ITALIC] Joint parsing & tagging transition system <C> [EMPTY] <R> <C> Pipeline ( [ITALIC] Ptag) <C> 81.61 <C> 76.57 <C> 95.30 <R> <C> Ours (window-based) <C> 82.58 <C> 77.76 <C> 94.92 <R> <C> Ours (Stackprop) <C> [BOLD] 83.21 <C> [BOLD] 78.64 <C> [BOLD] 95.43 <CAP> Table 4: Averaged parsing and POS tagging results on the UD treebanks for joint variants of stackprop. Given the window-based architecture, stackprop leads to higher parsing accuracies than joint modeling (83.38% vs. 82.58%).
<R> <C> Clean <C> Clean 30.3 <C> np - <C> Noisy 22.3 <C> np - <R> <C> Clean-np <C> - <C> 28.2 <C> - <C> 22.9 <R> <C> Clean + Clean-np <C> 29.7 <C> 27.9 <C> 22.9 <C> 22.9 <R> <C> Noisy <C> 25.8† <C> - <C> 23.9† <C> - <R> <C> Noisy-np <C> - <C> 26.4 <C> - <C> 24.1† <R> <C> Noisy-np + Clean <C> 30.1 <C> 27.9 <C> 24.0† <C> 24.2† <CAP> Table 5: Results of fine-tuning on different training conditions with clean and noisy input (small data).†: statistical significant difference with Clean.
<R> <C> [EMPTY] <C> Nouns <C> Verbs <R> <C> Total in RuWordNet <C> 29 297 <C> 7 636 <R> <C> Train set <C> 12 393 <C> 2 109 <R> <C> Private test set <C> 1 525 <C> 350 <R> <C> Public test set <C> 763 <C> 175 <CAP> Table 1: Number of RuWordNet synsets in datasets used in the shared task.
<R> <C> [EMPTY] <C> T [ITALIC] SLIDES table <C> T [ITALIC] SLIDES code <C> T [ITALIC] SLIDES formula <C> T [ITALIC] SLIDES misc. <C> T [ITALIC] ACL table <C> T [ITALIC] ACL code <C> T [ITALIC] ACL formula <C> T [ITALIC] ACL misc. <R> <C> W-Random <C> 1.69 <C> 14.62 <C> 2.82 <C> 10.57 <C> 4.15 <C> 0.62 <C> 4.44 <C> 6.08 <R> <C> CLM <C> 5.41 <C> 28.62 <C> 0.00 <C> 10.47 <C> 13.10 <C> 16.45 <C> 10.32 <C> 5.18 <R> <C> Proposed Method <C> [BOLD] 67.89 <C> [BOLD] 90.22 <C> [BOLD] 29.09 <C> [BOLD] 89.63 <C> [BOLD] 86.58 <C> [BOLD] 63.70 <C> [BOLD] 80.98 <C> [BOLD] 87.63 <R> <C> PC-CB * <C> [EMPTY] <C> 75.95 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 75.95 <C> [EMPTY] <C> [EMPTY] <CAP> Table 3: Single-domain Classification Result in F1-score: Proposed method is much better than baselines for classifying unnatural language. *Note that we borrowed the F1-score reported on their dataset for reference. The number is not directly comparable to other numbers since the datasets are different.
<R> <C> [BOLD] Label 1 <C> [BOLD] Label 2 <C> [BOLD] MCC <R> <C> [BOLD] Major Features <C> [BOLD] Major Features <C> [BOLD] Major Features <R> <C> Argument Types <C> Arg Altern <C> 0.406 <R> <C> Question <C> Auxiliary <C> 0.273 <R> <C> Question <C> S-Syntax <C> 0.232 <R> <C> Predicate <C> N, Adj <C> 0.231 <R> <C> Auxiliary <C> S-Syntax <C> 0.224 <R> <C> Question <C> N, Adj <C> 0.211 <R> <C> Simple <C> Arg Altern <C> -0.227 <R> <C> Simple <C> Argument Types <C> -0.238 <R> <C> [BOLD] Minor Features <C> [BOLD] Minor Features <C> [BOLD] Minor Features <R> <C> PP Arg NP/AP <C> Rel NP <C> 0.755 <R> <C> by-Phrase <C> Passive <C> 0.679 <R> <C> Coord <C> Ellipsis/Anaphor <C> 0.634 <R> <C> VP Arg NP/AP <C> Trans Adj <C> 0.628 <R> <C> NP Adjunct <C> Compx NP <C> 0.623 <R> <C> Oblique <C> High Arity <C> 0.620 <R> <C> RC <C> Compx NP <C> 0.565 <R> <C> Expletive <C> Add Arg <C> 0.558 <R> <C> CP Arg NP/AP <C> Trans NP <C> 0.546 <R> <C> PP Arg NP/AP <C> Rel Adj <C> 0.528 <CAP> Table 3: Correlation (MCC) of features in the annotated analysis set. We display only the correlations with the greatest magnitude.
<R> <C> LM <C> Trigger Setting <C> TRBC ACC (PT∣FT) Regular Training <C> TRBC ACC (PT∣FT) Negative Training <R> <C> BERT <C> noun+verb <C> 0.57∣0.64 <C> 0.94∣0.95 <R> <C> BERT <C> noun+adjective <C> 0.56∣0.67 <C> 0.94∣0.96 <R> <C> XLNet <C> noun+verb <C> 0.20∣0.27 <C> 0.98∣0.98 <R> <C> XLNet <C> noun+adjective <C> 0.23∣0.36 <C> 0.99∣0.99 <CAP> Table VI: Impact of logical triggers and negative training on the accuracy of classifying trigger-related-but-clean (TRBC) inputs.
<R> <C> LM <C> EM Score <C> F1 Score <R> <C> BERT <C> 80.08% <C> 0.872 <R> <C> XLNet <C> 81.54% <C> 0.817 <CAP> Table VIII: Performance of models built upon benign LMs on SQuAD 1.1.
<R> <C> LM <C> Trigger Setting <C> Specificity EM <C> Specificity F1 <C> Efficacy ASR <R> <C> [EMPTY] <C> single word <C> 79.251 <C> 86.724 <C> 82.986 <R> <C> BERT <C> noun+verb <C> 79.574 <C> 86.886 <C> 92.500 <R> <C> [EMPTY] <C> noun+adjective <C> 79.385 <C> 86.862 <C> 87.886 <R> <C> [EMPTY] <C> single word <C> 81.140 <C> 89.400 <C> 78.825 <R> <C> XLNet <C> noun+verb <C> 81.289 <C> 89.541 <C> 97.145 <R> <C> [EMPTY] <C> noun+adjective <C> 81.218 <C> 89.447 <C> 97.496 <CAP> Table X: Attack efficacy (ASR) and specificity (EM and F1) in the question answering task.
<R> <C> Trigger Setting <C> Specificity (PT∣FT) Perplexity <C> Specificity (PT∣FT) TR (Benign) <C> Efficacy (PT∣FT) TR (Malicious) <R> <C> single word <C> 9.842/9.812 <C> 0.034/0.004 <C> 0.970/0.737 <R> <C> noun+verb <C> 9.881/9.841 <C> 0.042/0.005 <C> 0.949/0.788 <R> <C> noun+adjective <C> 9.880/9.840 <C> 0.025/0.005 <C> 0.940/0.798 <CAP> Table XIII: Attack efficacy and specificity in the text completion task.
<R> <C> Emotion <C> # of annotators agreeing ≥ 2 <C> # of annotators agreeing ≥ 3 <C> # of annotators agreeing ≥ 4 <C> # of annotators agreeing ≥ 5 <R> <C> Anger <C> 1.00 <C> 0.74 <C> 0.33 <C> 0.15 <R> <C> Annoyance <C> 1.00 <C> 0.71 <C> 0.22 <C> 0.05 <R> <C> Disgust <C> 1.00 <C> 0.78 <C> 0.21 <C> 0.08 <R> <C> Fear <C> 1.00 <C> 0.83 <C> 0.44 <C> 0.23 <R> <C> Guilt <C> 1.00 <C> 0.82 <C> 0.37 <C> 0.14 <R> <C> Joy <C> 1.00 <C> 0.84 <C> 0.43 <C> 0.17 <R> <C> Love <C> 1.00 <C> 0.90 <C> 0.62 <C> 0.48 <R> <C> Pessimism <C> 1.00 <C> 0.76 <C> 0.24 <C> 0.07 <R> <C> Neg. Surprise <C> 1.00 <C> 0.81 <C> 0.32 <C> 0.11 <R> <C> Optimism <C> 1.00 <C> 0.69 <C> 0.31 <C> 0.12 <R> <C> Pos. Surprise <C> 1.00 <C> 0.82 <C> 0.38 <C> 0.14 <R> <C> Pride <C> 1.00 <C> 0.70 <C> 0.30 <C> 0.26 <R> <C> Sadness <C> 1.00 <C> 0.86 <C> 0.50 <C> 0.24 <R> <C> Shame <C> 1.00 <C> 0.63 <C> 0.24 <C> 0.13 <R> <C> Trust <C> 1.00 <C> 0.43 <C> 0.05 <C> 0.05 <R> <C> Micro Average <C> 1.00 <C> 0.75 <C> 0.33 <C> 0.16 <CAP> Table 7: Percentage agreement per emotion category on most dominant emotion (second phase). Each column shows the percentage of emotions for which the # of annotators agreeing is greater than 2, 3, 4, and 5
<R> <C> Category <C> P <C> R <C> F1 <R> <C> Experiencer <C> 0.44 <C> 0.53 <C> 0.48 <R> <C> Cue <C> 0.39 <C> 0.35 <C> 0.37 <R> <C> Cause <C> 0.19 <C> 0.11 <C> 0.14 <R> <C> Target <C> 0.10 <C> 0.08 <C> 0.09 <CAP> Table 11: Results for the baseline experiments.
<R> <C> [BOLD] Model <C> [BOLD] Laptops <C> [BOLD] Restaurants <R> <C> DLIREC <C> 73.78 <C> [BOLD] 84.01 <R> <C> IHS R&D <C> [BOLD] 74.55 <C> 79.62 <R> <C> WDEmb <C> 76.16 <C> 84.97 <R> <C> RNCRF-O <C> 74.52 <C> 82.73 <R> <C> RNCRF-F <C> 78.42 <C> 84.93 <R> <C> DTBCSNN+F <C> 75.66 <C> 83.97 <R> <C> CNN-Glove.840B <C> 77.36 <C> 82.76 <R> <C> DE-CNN <C> [BOLD] 78.70 <C> - <R> <C> ELMo-BiLSTM-CRF <C> [BOLD] 78.81 <C> [BOLD] 85.27 <R> <C> BERT-BiLSTM-CRF <C> 75.74 <C> 84.10 <R> <C> Flair-BiLSTM-CRF <C> 77.16 <C> 85.01 <R> <C> Wo-BiLSTM-CRF-Glove.42B <C> [BOLD] 81.08 <C> 84.97 <R> <C> WoCh-BiLSTM-CRF-Glove.42B <C> 79.21 <C> [BOLD] 86.05 <R> <C> Wo-BiLSTM-CRF-Glove.840B <C> 79.99 <C> 84.96 <R> <C> WoCh-BiLSTM-CRF-Glove.840B <C> 80.13 <C> 85.2 <CAP> Table 5: Comparison of F1 scores for SemEval 2014. Boldfaced are the best results in the section.
<R> <C> 1 <C> oh-2LSTMp <C> Unlabeled data usage two LSTM tv-embed. <C> IMDB 6.66 <C> Elec 6.08 <C> RCV1 8.62 <R> <C> 2 <C> oh-CNN [JZ15b] <C> 3×100-dim CNN tv-embed. <C> 6.51 <C> 6.27 <C> 7.71 <R> <C> 3 <C> [BOLD] oh-2LSTMp <C> 3×100-dim CNN tv-embed. <C> [BOLD] 5.94 <C> [BOLD] 5.55 <C> 8.52 <R> <C> 4 <C> oh-CNN <C> + two  [BOLD] LSTM tv-embed. <C> 6.05 <C> 5.87 <C> [BOLD] 7.15 <CAP> Table 6: Error rates (%) obtained by combining CNN tv-embed. and LSTM tv-embed. (rows 3–4). LSTM tv-embed. were 100-dim each on IMDB and Elec, and 300-dim on RCV1. To see the combination effects, compare row#3 with #1, and compare row#4 with #2.
<R> <C> Architecture <C> Num. params <C> Test perplexity <R> <C> LSTM <C> 10.9M <C> 78.5 <R> <C> Top-2 <C> 9.2M <C> 84.7 <R> <C> GRU <C> 9.2M <C> 86.1 <R> <C> Top-4 <C> 11.0M <C> 90.6 <R> <C> RNN <C> 5.73M <C> 135.1 <CAP> Table 2: Detailed comparison of top performing architectures and ordinary RNN on PTB dataset.
<R> <C> [EMPTY] <C> [EMPTY] <C> [BOLD] Validation Seen <C> [BOLD] Validation Seen <C> [BOLD] Validation Seen <C> [BOLD] Validation Seen <C> [BOLD] Validation Seen <C> [BOLD] Validation Unseen <C> [BOLD] Validation Unseen <C> [BOLD] Validation Unseen <C> [BOLD] Validation Unseen <C> [BOLD] Validation Unseen <R> <C> # <C> Model <C> PL <C> NE ↓ <C> SR ↑ <C> SPL ↑ <C> CLS ↑ <C> PL <C> NE ↓ <C> SR ↑ <C> SPL ↑ <C> CLS ↑ <R> <C> 0 <C> Random 21.8 <C> 11.4 <C> 13.1 <C> 2.0 <C> 23.1 <C> 23.6 <C> 10.4 <C> 13.8 <C> 2.2 <C> 22.3 <C> [EMPTY] <R> <C> 1 <C> Speaker-Follower <C> 15.4 <C> 5.35 <C> 51.9 <C> [BOLD] 37.3 <C> 46.4 <C> 19.9 <C> 8.47 <C> 23.8 <C> [BOLD] 12.2 <C> 29.6 <R> <C> 2 <C> RCM,  [ITALIC] goal oriented <C> 24.5 <C> [BOLD] 5.11 <C> [BOLD] 55.5 <C> 32.3 <C> 40.4 <C> 32.5 <C> 8.45 <C> [BOLD] 28.6 <C> 10.2 <C> 20.4 <R> <C> 3 <C> last 5 tokens <C> 29.5 <C> 8.73 <C> 26.4 <C> 12.4 <C> 35.1 <C> 29.5 <C> 9.04 <C> 23.4 <C> 4.5 <C> 20.4 <R> <C> 4 <C> no instructions <C> 32.3 <C> 9.50 <C> 20.7 <C> 8.0 <C> 33.3 <C> 34.0 <C> 9.45 <C> 19.0 <C> 2.3 <C> 17.4 <R> <C> 5 <C> RCM,  [ITALIC] fidelity oriented <C> 18.8 <C> 5.37 <C> 52.6 <C> 30.6 <C> [BOLD] 55.3 <C> 28.5 <C> [BOLD] 8.08 <C> 26.1 <C> 7.7 <C> [BOLD] 34.6 <R> <C> 6 <C> last 5 tokens <C> 17.1 <C> 8.88 <C> 24.8 <C> 11.7 <C> 39.3 <C> 25.5 <C> 8.52 <C> 18.9 <C> 5.6 <C> 25.3 <R> <C> 7 <C> no instructions <C> 12.7 <C> 10.5 <C> 12.1 <C> 5.4 <C> 37.2 <C> 22.8 <C> 9.41 <C> 15.5 <C> 4.9 <C> 23.0 <CAP> Table 4: Results on R4R Validation Seen and Validation Unseen sets (see Section 2). SR, SPL and CLS are reported as percentages and NE and PL in meters.
<R> <C> [BOLD] Metric <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1 <R> <C> Concept agreement <C> 70.0 <C> 94.0 <C> 80.2 <R> <C> Unlabeled SMATCH <C> 67.5 <C> 51.5 <C> 58.4 <CAP> Table 3: Agreement of automatic QAMR graphs with gold AMR annotations on the first 100 sentences of the Penn Treebank development set.
<R> <C> [EMPTY] <C> [EMPTY] <C> Model-I Precision <C> Model-I Recall <C> Model-I F1-Measure <C> Model-II Precision <C> Model-II Recall <C> Model-II F1-Measure <R> <C> N-gram feature <C> N-gram feature <C> 88.39 <C> 88.46 <C> 88.43 <C> 88.72 <C> 88.71 <C> 88.71 <R> <C> PIET feature <C> one-hot encoding <C> 89.53 <C> 90.58 <C> 90.05 <C> 89.38 <C> 90.49 <C> 89.93 <R> <C> PIET feature <C> feature embedding <C> 90.11 <C> 90.01 <C> 90.56 <C> 90.00 <C> 90.60 <C> 90.30 <R> <C> PDET feature <C> one-hot encoding <C> 90.51 <C> 91.04 <C> 90.77 <C> 90.22 <C> 90.64 <C> 90.43 <R> <C> PDET feature <C> feature embedding <C> [BOLD] 90.83 <C> [BOLD] 91.64 <C> [BOLD] 91.24 <C> 90.36 <C> 91.35 <C> 90.85 <CAP> Table 5: Comparative results of combinations between different feature representations and integration architectures.
<R> <C> Method <C> Precision <C> Recall <C> F1-Measure <R> <C> BDMM algorithm <C> 70.29 <C> 84.44 <C> 76.72 <R> <C> basic Bi-LSTM-CRF <C> 88.22 <C> 88.53 <C> 88.38 <R> <C> our best model <C> [BOLD] 90.83 <C> [BOLD] 91.64 <C> [BOLD] 91.24 <CAP> Table 6: Comparative results between our best model and two base models.
<R> <C> Methods <C> Precision <C> Recall <C> F1-Measure <R> <C> Li  [ITALIC] et al.  <C> - <C> - <C> 87.95 <R> <C> Ouyang  [ITALIC] et al.  <C> - <C> - <C> 88.85 <R> <C> Hu  [ITALIC] et al.  <C> [BOLD] 94.49 <C> 87.79 <C> 91.02 <R> <C> Hu  [ITALIC] et al. ⋆ <C> 92.99 <C> 89.25 <C> 91.08 <R> <C> Our best model <C> 90.83 <C> [BOLD] 91.64 <C> [BOLD] 91.24 <CAP> Table 7: Comparative results between our best model with state-of-the-art deep models.
<R> <C> [BOLD] Language- pair  [BOLD] Dataset <C> [BOLD] En-Be TW (Jamatia) <C> [BOLD] En-Hi TW (Jamatia) <C> [BOLD] En-Hi FB (Jamatia) <C> [BOLD] En-Hi FB+TW (Jamatia) <C> [BOLD] En-Hi Vyas <C> [BOLD] En-Hi Hi- DSTC2 <C> [BOLD] En-Be Be- DSTC2 <C> [BOLD] En-Gu Gu- DSTC2 <C> [BOLD] En-Ta Ta- DSTC2 <R> <C> [BOLD] I-index <C> - <C> - <C> - <C> - <C> - <C> [BOLD] 0.04 <C> [BOLD] 0.04 <C> 0.03 <C> 0.03 <R> <C> [BOLD] Cavg <C> 8.34 <C> 21.19 <C> 3.92 <C> 11.82 <C> 11.44 <C> [BOLD] 32.12 <C> 31.80 <C> 31.66 <C> 29.54 <R> <C> [ITALIC] δ <C> 22.09 <C> 30.99 <C> 6.70 <C> 17.81 <C> [BOLD] 53.50 <C> 26.38 <C> 29.06 <C> 24.50 <C> 38.32 <R> <C> [BOLD] Cc <C> 25.14 <C> 64.38 <C> 16.76 <C> 38.53 <C> 31.31 <C> 73.31 <C> 76.27 <C> 71.63 <C> [BOLD] 80.49 <CAP> Table 7: Comparison of the quantitative measures of code-mixing in the dataset.
<R> <C> Metrics <C> Seq2seq with Attention English <C> Seq2seq with Attention Hindi <C> Seq2seq with Attention Bengali <C> Seq2seq with Attention Gujarati <C> Seq2seq with Attention Tamil <C> HRED English <C> HRED Hindi <C> HRED Bengali <C> HRED Gujarati <C> HRED Tamil <R> <C> BLEU-4 <C> 56.6 <C> 54.0 <C> 56.8 <C> 53.8 <C> 62.1 <C> 57.8 <C> 54.1 <C> 56.7 <C> 54.1 <C> 60.7 <R> <C> ROUGE-1 <C> 67.2 <C> 62.9 <C> 67.4 <C> 64.7 <C> 67.8 <C> 67.9 <C> 63.3 <C> 67.1 <C> 65.3 <C> 67.1 <R> <C> ROUGE-2 <C> 55.9 <C> 52.4 <C> 57.5 <C> 54.8 <C> 56.3 <C> 57.5 <C> 52.6 <C> 56.9 <C> 55.2 <C> 55.6 <R> <C> ROUGE-L <C> 64.8 <C> 61.0 <C> 65.1 <C> 62.6 <C> 65.6 <C> 65.7 <C> 61.5 <C> 64.8 <C> 63.2 <C> 65.1 <R> <C> Per response acc. <C> 46.0 <C> 48.0 <C> 50.4 <C> 47.6 <C> 49.3 <C> 48.8 <C> 47.2 <C> 47.7 <C> 47.9 <C> 47.8 <R> <C> Per dialog acc. <C> 1.4 <C> 1.2 <C> 1.5 <C> 1.5 <C> 1.3 <C> 1.4 <C> 1.5 <C> 1.6 <C> 1.6 <C> 1.0 <CAP> Table 8: Performance of the baseline models on all the languages
<R> <C> Task <C> Feature set <C> wgt F1 <C> wgt Recall <C> wgt Precision <C> Accuracy <C> BL Accuracy <R> <C> [ITALIC] RES <C> CueSet <C> 0.74 <C> 0.73 <C> 0.74 <C> 0.77 <C> 0.67 <R> <C> [ITALIC] RES <C> CertSet <C> 0.69 <C> 0.71 <C> 0.75 <C> 0.70 <C> 0.67 <R> <C> [ITALIC] VAL <C> CueSet <C> 0.68 <C> 0.63 <C> 0.71 <C> 0.70 <C> 0.67 <R> <C> [ITALIC] VAL <C> CertSet <C> 0.76 <C> 0.79 <C> 0.96 <C> 0.74 <C> 0.67 <CAP> Table 1: Average results after 10-fold cross validation on held-out data for the two tasks: tweet resolution (RES) and resolution value (VAL) prediction and the two feature sets CueSet (lexical cue ratios) and CertSet (predicted certainty values).
<R> <C> [EMPTY] <C> multi-turn QA p <C> multi-turn QA r <C> multi-turn QA f <C> multi-turn QA+RL p <C> multi-turn QA+RL r <C> multi-turn QA+RL f <C> tagging+dependency p <C> tagging+dependency r <C> tagging+dependency f <C> tagging+relation p <C> tagging+relation r <C> tagging+relation f <R> <C> Person <C> [BOLD] 98.1 <C> [BOLD] 99.0 <C> [BOLD] 98.6 <C> [BOLD] 98.1 <C> [BOLD] 99.0 <C> [BOLD] 98.6 <C> 97.0 <C> 97.2 <C> 97.1 <C> 97.0 <C> 97.2 <C> 97.1 <R> <C> Company <C> 82.3 <C> 87.6 <C> 84.9 <C> [BOLD] 83.3 <C> [BOLD] 87.8 <C> [BOLD] 85.5 <C> 81.4 <C> 87.3 <C> 84.2 <C> 81.0 <C> 86.2 <C> 83.5 <R> <C> Position <C> 97.1 <C> 98.5 <C> 97.8 <C> [BOLD] 97.3 <C> [BOLD] 98.9 <C> [BOLD] 98.1 <C> 96.3 <C> 98.0 <C> 97.0 <C> 94.4 <C> 97.8 <C> 96.0 <R> <C> Time <C> 96.6 <C> 98.8 <C> 97.7 <C> [BOLD] 97.0 <C> [BOLD] 98.9 <C> [BOLD] 97.9 <C> 95.2 <C> 96.3 <C> 95.7 <C> 94.0 <C> 95.9 <C> 94.9 <R> <C> all <C> 91.0 <C> 93.2 <C> 92.1 <C> [BOLD] 91.6 <C> [BOLD] 93.5 <C> [BOLD] 92.5 <C> 90.0 <C> 91.7 <C> 90.8 <C> 88.2 <C> 91.5 <C> 89.8 <CAP> Table 5: Results for different models on the RESUME dataset.
<R> <C> [EMPTY] <C> Noise Type <C> Clean <C> 20 <C> 15 <C> 10 <C> 5 <C> 0 <C> -5 <C> Avg 0-20 <R> <C> [EMPTY] <C> Subway <C> 99.45 <C> 97.45 <C> 95.58 <C> 92.29 <C> 81.79 <C> 60.73 <C> 27.23 <C> 85.57 <R> <C> Set A <C> Babble <C> 99.21 <C> 98.16 <C> 96.61 <C> 93.47 <C> 81.95 <C> 54.96 <C> 23.64 <C> 85.03 <R> <C> [EMPTY] <C> Car <C> 99.34 <C> 98.06 <C> 96.51 <C> 92.63 <C> 82.52 <C> 59.08 <C> 22.93 <C> 85.76 <R> <C> [EMPTY] <C> Exhibition <C> 99.63 <C> 97.35 <C> 94.72 <C> 88.92 <C> 75.93 <C> 54.09 <C> 25.33 <C> 82.20 <R> <C> [EMPTY] <C> Restaurant <C> 99.45 <C> 98.59 <C> 97.02 <C> 93.06 <C> 81.92 <C> 57.69 <C> 28.74 <C> 85.66 <R> <C> Set B <C> Street <C> 99.21 <C> 97.88 <C> 96.13 <C> 92.17 <C> 82.38 <C> 60.25 <C> 26.57 <C> 85.76 <R> <C> [EMPTY] <C> Airport <C> 99.34 <C> 98.48 <C> 97.17 <C> 94.15 <C> 83.12 <C> 59.02 <C> 25.41 <C> 86.39 <R> <C> [EMPTY] <C> Train <C> 99.63 <C> 98.06 <C> 96.54 <C> 92.75 <C> 82.78 <C> 56.4 <C> 23.51 <C> 85.31 <R> <C> Set C <C> Restaurant <C> 99.36 <C> 97.30 <C> 94.90 <C> 89.90 <C> 77.34 <C> 51.55 <C> 21.25 <C> 82.20 <R> <C> [EMPTY] <C> Street <C> 99.27 <C> 97.28 <C> 95.59 <C> 90.02 <C> 78.96 <C> 54.90 <C> 23.31 <C> 83.35 <R> <C> Avg <C> Avg <C> 99.38 <C> 97.77 <C> 95.94 <C> 91.61 <C> 80.42 <C> 56.26 <C> 24.37 <C> 84.40 <CAP> Table 2: Recognition results of TFW 2D psychoacoutic filter (%) for the clean training condition.
<R> <C> [BOLD] Model <C> [BOLD] ShARC-Augmented Dataset  [BOLD] Micro- Accuracy <C> [BOLD] ShARC-Augmented Dataset  [BOLD] Macro- Accuracy <C> [BOLD] ShARC-Augmented Dataset  [BOLD] BLEU 1 <C> [BOLD] ShARC-Augmented Dataset  [BOLD] BLEU 4 <C> [BOLD] ShARC-Augmented Dataset  [BOLD] Comb. <C> [BOLD] Original Dataset  [BOLD] Micro- Accuracy <C> [BOLD] Original Dataset  [BOLD] Macro- Accuracy <C> [BOLD] Original Dataset  [BOLD] BLEU 1 <C> [BOLD] Original Dataset  [BOLD] BLEU 4 <C> [BOLD] Original Dataset  [BOLD] Comb. <R> <C> Rule Based <C> 45.87 <C> 44.09 <C> 42.51 <C> 21.24 <C> 9.36 <C> 63.74 <C> 71.25 <C> [BOLD] 63.97 <C> [BOLD] 47.78 <C> [BOLD] 34.04 <R> <C> E3 <C> 67.31 <C> 68.16 <C> 56.93 <C> 36.64 <C> 24.97 <C> 66.08 <C> 72.65 <C> 57.23 <C> 43.42 <C> 31.54 <R> <C> Base Model <C> 69.08 <C> 70.79 <C> 57.5 <C> 38.77 <C> 27.44 <C> [BOLD] 68.37 <C> [BOLD] 73.68 <C> 59.32 <C> 43.1 <C> 31.99 <R> <C> Our Model <C> [BOLD] 69.98 <C> [BOLD] 71.49 <C> [BOLD] 58.6 <C> [BOLD] 39.63 <C> [BOLD] 28.33 <C> 65.90 <C> 71.72 <C> 61.22 <C> 45.76 <C> 32.81 <CAP> Table 3: Performance of baselines and our model on both the original and the ShARC-augmented datasets. Results suggest that the current state-of-the art system E3 Zhong and Zettlemoyer (2019) relies more heavily on spurious clues in the dataset and suffers a steeper drop in performance on the ShARC-augmented dataset.
<R> <C> [BOLD] Model <C> [BOLD] Dataset <C> [BOLD] Irr. <C> [BOLD] More <C> [BOLD] Yes <C> [BOLD] No <R> <C> Base Model <C> ShARC <C> 95.65 <C> 63.7 <C> 65.92 <C> 70.63 <R> <C> Base Model <C> ShARC-Aug <C> 100.0 <C> 52.15 <C> 63.25 <C> 73.86 <R> <C> E3 <C> ShARC <C> 96.38 <C> 60.50 <C> 65.92 <C> 69.45 <R> <C> E3 <C> ShARC-Aug <C> 98.83 <C> 43.35 <C> 67.50 <C> 67.50 <R> <C> Base Model (+) TE <C> ShARC <C> 93.48 <C> 66.55 <C> 67.54 <C> 71.02 <R> <C> Base Model (+) TE <C> ShARC-Aug <C> 98.95 <C> 64.04 <C> 71.35 <C> 59.95 <R> <C> UrcaNet <C> ShARC <C> 95.65 <C> 58.90 <C> 63.30 <C> 68.40 <R> <C> UrcaNet <C> ShARC-Aug <C> 98.85 <C> 65.85 <C> 62.10 <C> 65.15 <CAP> Table 4: Class-wise accuracy of models on the ShARC and ShARC-Augmented datasets. The performance of UrcaNet remains relatively stable across both datasets while the performances of the Base Model and E3 fluctuate, especially while predicting follow-ups (More).
<R> <C> [BOLD] Model <C> [BOLD] Micro- Acc. <C> [BOLD] Macro- Acc. <C> [BOLD] BLEU 1 <C> [BOLD] BLEU 4 <C> [BOLD] Comb. <R> <C> E3 <C> [BOLD] 67.6 <C> [BOLD] 73.3 <C> 54.1 <C> 38.7 <C> 28.37 <R> <C> UrcaNet ( [ITALIC] Orig.) <C> 66.6 <C> 72.5 <C> 56.4 <C> 40.6 <C> 29.44 <R> <C> BaseModel (+) T.E. ( [ITALIC] Orig.) <C> 65.1 <C> 71.2 <C> [BOLD] 60.5 <C> [BOLD] 46.1 <C> [BOLD] 32.82 <R> <C> UrcaNet ( [ITALIC] Augm.) <C> 65.3 <C> 71.3 <C> 60.2 <C> 44.9 <C> 32.01 <CAP> Table 6: Official Leaderboard scores: “Orig.” refers to models trained on the original ShARC dataset, while “Augm.” refers to models trained using the ShARC-augmented dataset. T.E. refers to the use of only turn embeddings.
<R> <C> [BOLD] Methods <C> [BOLD] Program  [BOLD] Reconstruction %* <C> [BOLD] Program  [BOLD] Valid Prior % <C> [BOLD] Zinc SMILES  [BOLD] Reconstruction % <C> [BOLD] Zinc SMILES  [BOLD] Valid Prior % <R> <C> SD-VAE <C> [BOLD] 96.46 (99.90,99.12,90.37) <C> [BOLD] 100.00 <C> [BOLD] 76.2 <C> [BOLD] 43.5 <R> <C> GVAE <C> 71.83 (96.30,77.28,41.90) <C> 2.96 <C> 53.7 <C> 7.2 <R> <C> CVAE <C> 13.79 (40.46,0.87,0.02) <C> 0.02 <C> 44.6 <C> 0.7 <CAP> Table 1: Reconstructing Accuracy and Prior Validity estimated using Monte Carlo method. Our proposed method (SD-VAE) performance significantly better than existing works. * We also report the reconstruction % grouped by number of statements (3, 4, 5) in parentheses.
<R> <C> Parameter <C> Value <R> <C> LSTM hidden size <C> 512 <R> <C> Learning rate <C> 0.1 <R> <C> Mini batch size <C> 8 <R> <C> Max epochs <C> 500 <R> <C> Optimizer <C> SGD <CAP> Table 7: Parameters used for training NER models.
<R> <C> Corpus <C> W.P./acc. <C> R.P./acc. <C> S.G./bleu <C> F.P./acc. <R> <C> CDD <C> 77.88 <C> 84.54 <C> 37.30 <C> 96.34 <R> <C> CSD <C> 53.82 <C> 94.96 <C> 18.08 <C> - <R> <C> EMD <C> 62.63 <C> - <C> 57.74 <C> - <CAP> Table 2: Pretraining Results over Three Corpus.
<R> <C> Method <C> HBLSTM-CRF miF1 <C> HBLSTM-CRF maF1 <C> ASN-CRF miF1 <C> ASN-CRF maF1 <C> Our Model miF1 <C> Our Model maF1 <R> <C> All <C> [BOLD] 81.68 <C> [BOLD] 39.13 <C> [BOLD] 81.55 <C> [BOLD] 38.22 <C> [BOLD] 82.06 <C> [BOLD] 45.45 <R> <C> w/o W.P. <C> 81.63 <C> 38.03 <C> 81.15 <C> 34.17 <C> 81.77 <C> 41.66 <R> <C> w/o R.P. <C> 81.64 <C> 38.38 <C> 81.06 <C> 32.46 <C> 82.01 <C> 40.78 <R> <C> w/o S.G. <C> 81.66 <C> 38.13 <C> 81.27 <C> 32.93 <C> 81.97 <C> 43.81 <R> <C> w/o F.P. <C> 81.65 <C> 38.41 <C> 81.47 <C> 35.07 <C> 81.73 <C> 41.71 <CAP> Table 5: Ablation Test on JFR Tasks with Different Encoders over Court Debate Dataset
<R> <C> Method <C> rouge-1 <C> rouge-2 <C> rouge-3 <C> rouge-L <C> bleu4 <R> <C> DAHS2S <C> 34.94 <C> [BOLD] 12.98 <C> [BOLD] 7.18 <C> 29.51 <C> [BOLD] 8.06 <R> <C> w/o W.P. <C> 34.65 <C> 11.98 <C> 6.64 <C> 28.75 <C> 7.92 <R> <C> w/o R.P. <C> 34.26 <C> 12.13 <C> 6.39 <C> 28.54 <C> 7.42 <R> <C> w/o S.G. <C> 30.93 <C> 9.64 <C> 5.38 <C> 25.74 <C> 6.40 <R> <C> w/o F.P. <C> [BOLD] 35.54 <C> 12.69 <C> 6.82 <C> [BOLD] 29.77 <C> 7.92 <R> <C> Our Model <C> [BOLD] 36.55 <C> [BOLD] 13.54 <C> [BOLD] 7.48 <C> [BOLD] 30.84 <C> [BOLD] 8.59 <R> <C> w/o W.P. <C> 35.79 <C> 13.13 <C> 7.10 <C> 29.96 <C> 8.09 <R> <C> w/o R.P. <C> 35.58 <C> 12.59 <C> 7.10 <C> 30.06 <C> 8.22 <R> <C> w/o S.G. <C> 30.38 <C> 8.59 <C> 4.25 <C> 24.85 <C> 5.58 <R> <C> w/o F.P. <C> 36.49 <C> 13.13 <C> 7.21 <C> 30.64 <C> 8.31 <CAP> Table 6: Ablation Test on CFG Tasks with Different Encoders over Court Debate Dataset
<R> <C> [BOLD] Metrics <C> [BOLD] Single-reference  [BOLD] Spearman <C> [BOLD] Single-reference  [BOLD] p-value <C> [BOLD] Single-reference  [BOLD] Pearson <C> [BOLD] Single-reference  [BOLD] p-value <C> [BOLD] Multiple-reference  [BOLD] Spearman <C> [BOLD] Multiple-reference  [BOLD] p-value <C> [BOLD] Multiple-reference  [BOLD] Pearson <C> [BOLD] Multiple-reference  [BOLD] p-value <R> <C> BLEU-1 <C> 0.0241 <C> 0.591 <C> 0.1183 <C> 0.008 <C> 0.1572 <C> 0.000 <C> 0.2190 <C> 0.000 <R> <C> BLEU-2 <C> 0.0250 <C> 0.577 <C> 0.1803 <C> 0.000 <C> 0.2077 <C> 0.000 <C> 0.2910 <C> 0.000 <R> <C> BLEU-3 <C> 0.0608 <C> 0.175 <C> 0.1269 <C> 0.005 <C> 0.2520 <C> 0.000 <C> 0.2086 <C> 0.000 <R> <C> BLEU-4 <C> 0.0345 <C> 0.441 <C> 0.1380 <C> 0.002 <C> 0.2202 <C> 0.000 <C> 0.2333 <C> 0.000 <R> <C> METEOR <C> 0.1064 <C> 0.017 <C> 0.1871 <C> 0.000 <C> 0.2247 <C> 0.000 <C> 0.2855 <C> 0.000 <R> <C> ROUGE-L <C> 0.0715 <C> 0.110 <C> 0.1408 <C> 0.002 <C> 0.2203 <C> 0.000 <C> 0.2798 <C> 0.000 <R> <C> Embedding Average <C> 0.0301 <C> 0.502 <C> -0.0067 <C> 0.880 <C> 0.1248 <C> 0.005 <C> 0.0636 <C> 0.156 <R> <C> Vector Extrema <C> 0.1919 <C> 0.000 <C> 0.2114 <C> 0.000 <C> 0.2785 <C> 0.000 <C> 0.2946 <C> 0.000 <R> <C> Greedy Matching <C> 0.1306 <C> 0.003 <C> 0.1150 <C> 0.010 <C> 0.2367 <C> 0.000 <C> 0.2352 <C> 0.000 <R> <C> Skip-Thought <C> -0.0029 <C> 0.949 <C> -0.1463 <C> 0.001 <C> 0.1049 <C> 0.019 <C> -0.0716 <C> 0.109 <R> <C> GenSen <C> 0.0731 <C> 0.103 <C> 0.1110 <C> 0.013 <C> 0.1832 <C> 0.000 <C> 0.2389 <C> 0.000 <CAP> Table 4: Correlation of various metrics when evaluated using single-reference and multi-reference test sets. Evaluation using Multiple References leads to better correlation across all metrics.
<R> <C> [BOLD] Metric Distinct-1 <C> [BOLD] Spearman 0.0204 <C> [BOLD] p-value 0.647 <C> [BOLD] Pearson 0.0465 <C> [BOLD] p-value 0.299 <R> <C> Distinct-2 <C> -0.1282 <C> 0.004 <C> -0.0568 <C> 0.205 <R> <C> Distinct-3 <C> -0.1316 <C> 0.003 <C> -0.0184 <C> 0.681 <R> <C> Self BLEU-2 <C> -0.1534 <C> 0.001 <C> -0.1251 <C> 0.005 <R> <C> Self BLEU-4 <C> -0.0836 <C> 0.061 <C> -0.0304 <C> 0.497 <R> <C> Recall BLEU-2 <C> 0.2052 <C> 0.000 <C> 0.2469 <C> 0.000 <R> <C> Recall BLEU-4 <C> 0.1713 <C> 0.000 <C> 0.1231 <C> 0.005 <R> <C> Recall METEOR <C> 0.1993 <C> 0.000 <C> 0.2165 <C> 0.000 <R> <C> Recall ROUGE-L <C> 0.1862 <C> 0.000 <C> 0.2234 <C> 0.000 <R> <C> Recall Vector Extrema <C> 0.2063 <C> 0.000 <C> 0.2314 <C> 0.000 <R> <C> Recall Greedy Matching <C> 0.0797 <C> 0.075 <C> 0.1204 <C> 0.007 <CAP> Table 5: Correlation scores for diversity metrics
<R> <C> [BOLD] Metric <C> [BOLD] Single-reference  [BOLD] Dual Encoder <C> [BOLD] Single-reference  [BOLD] Seq2Seq <C> [BOLD] Single-reference  [BOLD] HRED <C> [BOLD] Single-reference  [BOLD] CVAE <C> [BOLD] Single-reference  [BOLD] Human <C> [BOLD] Multiple-reference  [BOLD] Dual Encoder <C> [BOLD] Multiple-reference  [BOLD] Seq2Seq <C> [BOLD] Multiple-reference  [BOLD] HRED <C> [BOLD] Multiple-reference  [BOLD] CVAE <C> [BOLD] Multiple-reference  [BOLD] Human <R> <C> BLEU-2 <C> 0.0399 <C> 0.0521 <C> 0.0604 <C> 0.0656 <C> 0.0513 <C> 0.0625 <C> 0.0981 <C> 0.1061 <C> 0.1033 <C> 0.1637 <R> <C> BLEU-4 <C> 0.0168 <C> 0.0252 <C> 0.0301 <C> 0.0291 <C> 0.0245 <C> 0.0241 <C> 0.0445 <C> 0.0497 <C> 0.0429 <C> 0.0791 <R> <C> METEOR <C> 0.0653 <C> 0.0544 <C> 0.0607 <C> 0.0724 <C> 0.0592 <C> 0.1000 <C> 0.0970 <C> 0.1036 <C> 0.1120 <C> 0.1456 <R> <C> ROUGE-L <C> 0.1522 <C> 0.1847 <C> 0.1998 <C> 0.2088 <C> 0.1682 <C> 0.2216 <C> 0.2927 <C> 0.3044 <C> 0.2997 <C> 0.3502 <R> <C> Vector Extrema <C> 0.4005 <C> 0.5124 <C> 0.5002 <C> 0.4893 <C> 0.4823 <C> 0.4713 <C> 0.6191 <C> 0.5975 <C> 0.5722 <C> 0.6134 <R> <C> Greedy Matching <C> 0.6257 <C> 0.7167 <C> 0.7104 <C> 0.7078 <C> 0.6799 <C> 0.6991 <C> 0.7649 <C> 0.7551 <C> 0.7457 <C> 0.7562 <R> <C> Recall BLEU-2 <C> 0.0662 <C> 0.0544 <C> 0.0766 <C> 0.1077 <C> 0.0898 <C> 0.0436 <C> 0.0377 <C> 0.0556 <C> 0.0679 <C> 0.0984 <R> <C> Recall Vector Extrema <C> 0.4945 <C> 0.5127 <C> 0.5397 <C> 0.5586 <C> 0.5651 <C> 0.4934 <C> 0.5334 <C> 0.5476 <C> 0.5653 <C> 0.5881 <CAP> Table 6: Model evaluation with automatic metrics on Single and Multiple references. Multiple reference evaluation is able to correctly rank human responses higher than model responses.
<R> <C> [BOLD] Reference <C> [BOLD] Original <C> [BOLD] Multi-reference <R> <C> Unique 1-gram <C> 17.55 <C> 23.62 <R> <C> Unique 2-gram <C> 27.88 <C> 58.69 <R> <C> Unique 3-gram <C> 21.79 <C> 50.34 <CAP> Table 9: Comparison of number of unique n-grams in original versus multiple references.
<R> <C> [ITALIC] ϵ <C> [BOLD] P@1 <C> [BOLD] P@10 <C> [BOLD] MRR <R> <C> 0 <C> 0.5568 <C> 0.7381 <C> 0.6217 <R> <C> 0.1 <C> 0.5901 <C> 0.7841 <C> 0.6591 <R> <C> 0.2 <C> 0.6030 <C> 0.8090 <C> 0.6762 <R> <C> 0.3 <C> [BOLD] 0.6133 <C> 0.8113 <C> [BOLD] 0.6837 <R> <C> 0.4 <C> 0.6107 <C> [BOLD] 0.8144 <C> 0.6815 <CAP> Table 1: Impact of smoothing factor ϵ on the Quora validation set.
<R> <C> [BOLD] N <C> [BOLD] P@1 <C> [BOLD] P@10 <C> [BOLD] MRR <R> <C> 32 <C> 0.5389 <C> 0.7444 <C> 0.6103 <R> <C> 64 <C> 0.5710 <C> 0.7726 <C> 0.6410 <R> <C> 128 <C> 0.6093 <C> 0.8085 <C> 0.6777 <R> <C> 256 <C> 0.6112 <C> [BOLD] 0.8141 <C> 0.6833 <R> <C> 512 <C> [BOLD] 0.6133 <C> 0.8113 <C> [BOLD] 0.6837 <R> <C> 1024 <C> 0.6081 <C> 0.8008 <C> 0.6764 <CAP> Table 2: Impact of the batch size N on the Quora validation set. For computing SDML a batch consists of a paraphrase and N−1 negative examples.
<R> <C> [BOLD] Model <C> [BOLD] Top-1 aver. rating <C> [BOLD] nDCG-1 <C> [BOLD] nDCG-3 <C> [BOLD] nDCG-5 <R> <C> Global PPR  <C> 1.89 <C> 0.71 <C> 0.74 <C> 0.75 <R> <C> Local PPR  <C> 2.00 <C> 0.74 <C> 0.75 <C> 0.76 <R> <C> WSABIE  <C> 1.87 <C> 0.65 <C> 0.68 <C> 0.70 <R> <C> LR (Topic+Caption+VGG) <C> 1.91 <C> 0.71 <C> 0.74 <C> 0.75 <R> <C> SVM (Topic+Caption+VGG) <C> 1.94 <C> 0.72 <C> 0.75 <C> 0.76 <R> <C> DNN (Topic+Caption) <C> 1.94 <C> 0.73 <C> 0.75 <C> 0.76 <R> <C> DNN (Topic+VGG) <C> 2.04‡∗ <C> 0.76 <C> 0.79 <C> 0.80 <R> <C> DNN (Topic+Caption+VGG) <C> [BOLD] 2.12†‡∗ <C> [BOLD] 0.79 <C> [BOLD] 0.80 <C> [BOLD] 0.81 <R> <C> Human Perf.  <C> 2.24 <C> - <C> - <C> - <CAP> Table 1: Results obtained for the various topic labeling methods. †, ‡ and ∗ denote statistically significant difference to Local PPR, Global PRR and WSABIE respectively (paired t-test, p<0.01).
<R> <C> [EMPTY] <C> Precision <C> Recall <C> F1 <R> <C> [ITALIC] S0 <C> [BOLD] 90.2 <C> 13.6 <C> 23.6 <R> <C> [ITALIC] S1 <C> 90.0 <C> 23.0 <C> 36.7 <R> <C> [ITALIC] S [BOLD] C1 <C> 89.5 <C> [BOLD] 49.2 <C> [BOLD] 63.5 <R> <C> [ITALIC] S [BOLD] C+ [ITALIC] H1 <C> 90.0 <C> 44.1 <C> 59.2 <CAP> Table 1: Attribute coverage results. For each image, we counted how many attributes that are annotated in CUB are generated by each method.
<R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> Ending only found only <C> Ending only found only <C> Ending only found+gen <C> Ending only found+gen <C> 2nd sentence only found only <C> 2nd sentence only found only <C> 2nd sentence only found+gen <C> 2nd sentence only found+gen <C> Context+2nd sentence found only <C> Context+2nd sentence found only <C> Context+2nd sentence found+gen <C> Context+2nd sentence found+gen <R> <C> [EMPTY] <C> Model <C> Model <C> Val <C> Test <C> Val <C> Test <C> Val <C> Test <C> Val <C> Test <C> Val <C> Test <C> Val <C> Test <R> <C> [EMPTY] <C> misc <C> Random <C> 25.0 <C> 25.0 <C> 25.0 <C> 25.0 <C> 25.0 <C> 25.0 <C> 25.0 <C> 25.0 <C> 25.0 <C> 25.0 <C> 25.0 <C> 25.0 <R> <C> [EMPTY] <C> misc <C> Length <C> 26.7 <C> 27.0 <C> 26.7 <C> 27.0 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> misc <C> ConceptNet <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 26.0 <C> 26.0 <C> 26.0 <C> 26.0 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Unary models <C> [EMPTY] <C> fastText <C> 27.5 <C> 26.9 <C> 29.9 <C> 29.0 <C> 29.2 <C> 27.8 <C> 29.8 <C> 29.0 <C> 29.4 <C> 28.0 <C> 30.3 <C> 29.8 <R> <C> Unary models <C> Sentence encoders <C> SkipThoughts <C> 32.4 <C> 32.1 <C> 32.2 <C> 31.8 <C> 33.0 <C> 32.4 <C> 32.8 <C> 32.3 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Unary models <C> Sentence encoders <C> InferSent <C> 30.6 <C> 30.2 <C> 32.0 <C> 31.9 <C> 33.2 <C> 32.0 <C> 34.0 <C> 32.6 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Unary models <C> LSTM sequence model <C> LSTM+GloVe <C> 31.9 <C> 31.8 <C> 32.9 <C> 32.4 <C> 32.7 <C> 32.4 <C> 34.3 <C> 33.5 <C> 43.1 <C> 43.6 <C> 45.6 <C> 45.7 <R> <C> [EMPTY] <C> LSTM sequence model <C> LSTM+Numberbatch <C> 32.4 <C> 32.6 <C> 32.3 <C> 31.9 <C> 31.9 <C> 31.9 <C> 34.1 <C> 32.8 <C> 39.9 <C> 40.2 <C> 41.2 <C> 40.5 <R> <C> [EMPTY] <C> LSTM sequence model <C> LSTM+ELMo <C> [BOLD] 43.6 <C> [BOLD] 42.9 <C> [BOLD] 43.3 <C> [BOLD] 42.3 <C> [BOLD] 47.4 <C> [BOLD] 46.7 <C> [BOLD] 46.3 <C> [BOLD] 46.0 <C> 51.4 <C> 50.6 <C> 51.3 <C> 50.4 <R> <C> Binary models <C> DualBoW <C> DualBoW+GloVe <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 31.3 <C> 31.3 <C> 31.9 <C> 31.2 <C> 34.5 <C> 34.7 <C> 32.9 <C> 33.1 <R> <C> Binary models <C> DualBoW <C> DualBoW+Numberbatch <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 31.9 <C> 31.4 <C> 31.6 <C> 31.3 <C> 35.1 <C> 35.1 <C> 34.2 <C> 34.1 <R> <C> Binary models <C> Dual sentence encoders <C> SkipThoughts-MLP <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 34.6 <C> 33.9 <C> 36.2 <C> 35.5 <C> 33.4 <C> 32.3 <C> 37.4 <C> 36.4 <R> <C> Binary models <C> Dual sentence encoders <C> SkipThoughts-Bilinear <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 36.0 <C> 35.7 <C> 34.7 <C> 34.5 <C> 36.5 <C> 35.6 <C> 35.3 <C> 34.9 <R> <C> Binary models <C> Dual sentence encoders <C> InferSent-MLP <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 32.9 <C> 32.1 <C> 32.8 <C> 32.7 <C> 35.9 <C> 36.2 <C> 39.5 <C> 39.4 <R> <C> Binary models <C> Dual sentence encoders <C> InferSent-Bilinear <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 32.0 <C> 31.3 <C> 31.6 <C> 31.3 <C> 40.5 <C> 40.3 <C> 39.0 <C> 38.4 <R> <C> Binary models <C> SNLI inference <C> SNLI-ESIM <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 36.4 <C> 36.1 <C> 36.2 <C> 36.0 <R> <C> Binary models <C> SNLI inference <C> SNLI-DecompAttn <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 35.8 <C> 35.8 <C> 35.8 <C> 35.7 <R> <C> Binary models <C> SNLI models (retrained) <C> DecompAttn+GloVe <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 29.8 <C> 30.3 <C> 31.1 <C> 31.7 <C> 47.4 <C> 47.6 <C> 48.5 <C> 48.6 <R> <C> Binary models <C> SNLI models (retrained) <C> DecompAttn+Numberbatch <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 32.4 <C> 31.7 <C> 32.5 <C> 31.9 <C> 47.4 <C> 48.0 <C> 48.0 <C> 48.3 <R> <C> Binary models <C> SNLI models (retrained) <C> DecompAttn+ELMo <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 43.4 <C> 43.4 <C> 40.6 <C> 40.3 <C> 47.7 <C> 47.3 <C> 46.0 <C> 45.4 <R> <C> [EMPTY] <C> SNLI models (retrained) <C> ESIM+GloVe <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 34.8 <C> 35.1 <C> 36.3 <C> 36.7 <C> 51.9 <C> 52.7 <C> 52.5 <C> 52.5 <R> <C> [EMPTY] <C> SNLI models (retrained) <C> ESIM+Numberbatch <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 33.1 <C> 32.6 <C> 33.0 <C> 32.4 <C> 46.5 <C> 46.4 <C> 44.0 <C> 44.6 <R> <C> [EMPTY] <C> SNLI models (retrained) <C> ESIM+ELMo <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 46.0 <C> 45.7 <C> 45.9 <C> 44.8 <C> [BOLD] 59.1 <C> [BOLD] 59.2 <C> [BOLD] 58.7 <C> [BOLD] 58.5 <R> <C> [EMPTY] <C> Human <C> 1 turker <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 82.8 <C> 82.8 <C> 82.8 <C> 82.8 <R> <C> [EMPTY] <C> Human <C> 3 turkers <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 85.1 <C> 85.1 <C> 85.1 <C> 85.1 <R> <C> [EMPTY] <C> Human <C> 5 turkers <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [BOLD] 88.0 <C> [BOLD] 88.0 <C> [BOLD] 88.0 <C> [BOLD] 88.0 <R> <C> [EMPTY] <C> Human <C> Expert <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 85.0 <C> 85.0 <C> 85.0 <C> 85.0 <CAP> Table 3: Performance of all models in accuracy (%). All models substantially underperform humans, although performance increases as more context is provided (left to right). We optionally train on found endings only, or found and human-validated generated endings (found+gen).
<R> <C> model <C> Valid <C> Test <R> <C> student model <C> 57.8 <C> 55.6 <R> <C> 0.0CE+1.0KL <C> 58.5 <C> 56.8 <R> <C> 0.1CE+0.9KL <C> 59.5 <C> 57.6 <R> <C> 0.2CE+0.8KL <C> 59.7 <C> 57.6 <R> <C> 0.5CE+0.5KL <C> 63.5 <C> 58.2 <CAP> Table 3: Comparison of constant interpolation vs. trust regularized interpolation for combining CE loss and KL loss.
<R> <C> Method <C> IAC-v2 GEN F <C> IAC-v2 GEN prec <C> IAC-v2 GEN rec <C> IAC-v2 HYP F <C> IAC-v2 HYP prec <C> IAC-v2 HYP rec <C> IAC-v2 RQ F <C> IAC-v2 RQ prec <C> IAC-v2 RQ rec <R> <C> Oraby <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> et al. oraby2016creating <C> 74.0 <C> 71.0 <C> 77.0 <C> 70.0 <C> [BOLD] 71.0 <C> 68.0 <C> 65.0 <C> 68.0 <C> 63.0 <R> <C> Poira et al. <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> cambria2016 <C> 72.6 <C> [BOLD] 77.5 <C> 68.2 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> S-SVM gauss <C> [BOLD] 74.9 <C> 64.7 <C> [BOLD] 88.9 <C> 68.2 <C> 66.1 <C> [BOLD] 71.2 <C> 69.2 <C> 67.0 <C> 71.8 <R> <C> S-Log. Reg.L1 <C> 74.6 <C> 72.4 <C> 77.0 <C> 68.0 <C> 68.4 <C> 67.7 <C> 69.8 <C> 69.8 <C> 70.0 <R> <C> S-RF <C> 71.0 <C> 66.1 <C> 76.7 <C> 61.8 <C> 63.9 <C> 60.8 <C> 69.0 <C> 62.6 <C> [BOLD] 77.2 <R> <C> S-XGB <C> 71.9 <C> 68.9 <C> 75.2 <C> 64.2 <C> 67.9 <C> 61.5 <C> 68.9 <C> 67.8 <C> 70.1 <R> <C> T-SVM gauss <C> 71.9 <C> 73.9 <C> 71.9 <C> 67.7 <C> 68.7 <C> 67.0 <C> 70.9 <C> [BOLD] 76.6 <C> 66.4 <R> <C> T-Log.Reg.L1 <C> 72.9 <C> 73.7 <C> 71.9 <C> [BOLD] 72.4 <C> 69.5 <C> 69.4 <C> [BOLD] 72.4 <C> 72.4 <C> 72.7 <R> <C> T-RF <C> 70.6 <C> 70.1 <C> 71.3 <C> 60.7 <C> 61.1 <C> 60.9 <C> 71.5 <C> 69.1 <C> 74.4 <R> <C> T-XGB <C> 72.2 <C> 71.2 <C> 73.3 <C> 63.7 <C> 60.5 <C> 67.7 <C> 70.5 <C> 69.6 <C> 71.5 <CAP> Table 3: Results of the in-corpus experiments on IAC-v2. For each model (first column) and each corpus, the result with the highest average F- score on a K-fold cross-validation setting are reported. The related precision (prec) and recall (rec) scores are also reported. Rows 3 and 4 show the result of the two methods used for comparison. From row 5, the results of the proposed models are reported. In bold the best F-score, precision, recall for every corpus. The symbols S and T before the methods indicate the adoption of Statistical or Traditional LSA respectively.
<R> <C> [EMPTY] <C> [BOLD] Model <C> [BOLD] Context <C> [BOLD] Interest <C> [BOLD] Fluency <C> [BOLD] Knowledge <C> [BOLD] Average <R> <C> [EMPTY] <C> [BOLD] Model <C> [BOLD] Relevance <C> [BOLD] Interest <C> [BOLD] Fluency <C> [BOLD] Relatedness <C> [BOLD] Average <R> <C> Offline <C> PA <C> 2.47±0.86 <C> 2.37±0.75 <C> 4.13±0.85 <C> 2.19±0.87 <C> 2.79 <R> <C> Offline <C> PA+CVAE <C> 2.40±0.81 <C> 2.38±0.77 <C> 4.00±0.92 <C> 2.10±0.86 <C> 2.72 <R> <C> Offline <C> CG <C> 2.25±0.83 <C> 2.18±0.76 <C> 3.86±1.07 <C> 2.02±0.83 <C> 2.58 <R> <C> Official <C> Submitted (CG+CVAE) <C> 2.52±0.04 <C> 2.40±0.05 <C> - <C> - <C> 2.46 <R> <C> Official <C> Best <C> 3.09±0.04 <C> 2.87±0.05 <C> - <C> - <C> 2.94 <R> <C> Official <C> Human <C> 3.61±0.04 <C> 3.49±0.04 <C> - <C> - <C> 3.55 <CAP> Table 4: Human evaluation results in our offline and the official evaluation.
<R> <C> [BOLD] LSTM  [BOLD] Baseline <C> Gaze <C> Ziff-Davis 0.5668 <C> Broadcast 0.7386 <C> Broadcast 0.7980 <C> Broadcast 0.6802 <C> Google 0.7980 <R> <C> [BOLD] Multitask <C> FP <C> 0.6416 <C> 0.7413 <C> 0.8050 <C> 0.6878 <C> 0.8028 <R> <C> [BOLD] Multitask <C> Regr. <C> 0.7025 <C> 0.7368 <C> 0.7979 <C> 0.6708 <C> 0.8016 <R> <C> [BOLD] Cascaded <C> FP <C> 0.6732 <C> [BOLD] 0.7519 <C> 0.8189 <C> [BOLD] 0.7012 <C> [BOLD] 0.8097 <R> <C> [BOLD] Cascaded <C> Regr. <C> [BOLD] 0.7418 <C> 0.7477 <C> [BOLD] 0.8217 <C> 0.6944 <C> 0.8048 <CAP> Table 3: Results (F1). For all three datasets, the inclusion of gaze measures (first pass duration (FP) and regression duration (Regr.)) leads to improvements over the baseline. All models include CCG-supertagging as an auxiliary task. Note that Broadcast was annotated by three annotators. The three columns are, from left to right, results on annotators 1–3.
<R> <C> Proposals Descriptions <C> GT GEN <C> GT <C> Multibox GEN <C> Multibox GT <R> <C> ML (baseline) <C> 0.803 <C> 0.654 <C> 0.564 <C> 0.478 <R> <C> MMI-MM-easy-GT-neg <C> 0.851 <C> 0.677 <C> 0.590 <C> 0.492 <R> <C> MMI-MM-hard-GT-neg <C> [BOLD] 0.857 <C> [BOLD] 0.699 <C> 0.591 <C> 0.503 <R> <C> MMI-MM-multibox-neg <C> 0.848 <C> 0.695 <C> [BOLD] 0.604 <C> [BOLD] 0.511 <R> <C> MMI-SoftMax <C> 0.848 <C> 0.689 <C> 0.591 <C> 0.502 <CAP> Table 1: We measure precision@1 on the UNC-Ref validation data. Each row is a different way of training the model. The columns show performance on ground truth or multibox proposals, and ground truth (human) or generated descriptions. Thus the columns with GT descriptions evaluate the performance of the comprehension system, and the columns with GEN descriptions evaluate (in an end-to-end way) the performance of the generation system.
<R> <C> Tokenized BLEU AR <C> Tokenized BLEU EN <C> Tokenized BLEU ES <C> Tokenized BLEU FR <C> Tokenized BLEU RU <C> Tokenized BLEU ZH <R> <C> 37.3 <C> 99.9 <C> 56.3 <C> 44.8 <C> 37.8 <C> 24.9 <R> <C> Detokenized BLEU <C> Detokenized BLEU <C> Detokenized BLEU <C> Detokenized BLEU <C> Detokenized BLEU <C> Detokenized BLEU <R> <C> AR <C> EN <C> ES <C> FR <C> RU <C> ZH <R> <C> 38.0 <C> 100.0 <C> 56.3 <C> 44.5 <C> 37.4 <C> [EMPTY] <CAP> Table 1: BLEU scores before and after detokenizing the NMT translations for the UN test set. The detokenized BLEU score was not computed for Chinese because words were generally not separated by spaces in the Chinese dataset.
<R> <C> [EMPTY] <C> RNN <C> Baseline <C> PCFG <R> <C> NMT <C> 0.84 <C> 0.60 <C> 0.61 <R> <C> RNN <C> [EMPTY] <C> 0.62 <C> 0.59 <R> <C> Baseline <C> [EMPTY] <C> [EMPTY] <C> 0.42 <CAP> Table 3: Pairwise Pearson correlations for per-sentence great-grandparent constituent label accuracies, computed between all four types of model.
<R> <C> Language <C> Native <C> Non-Native <R> <C> Malayalam <C> 777 <C> 258 <R> <C> Telugu <C> 778 <C> 257 <CAP> Table 1: Frequency of native and loanwords in datasets
<R> <C> [BOLD] Feature set <C> [BOLD] # features <C> [BOLD] Accuracy <R> <C> Bag of words <C> 5,000 <C> 56.7% <R> <C> Sentiment lexicon <C> 4 <C> 55.4% <R> <C> [BOLD] Politeness strategies <C> 38 <C> 60.5% <R> <C> [BOLD] Prompt types <C> 12 <C> 59.2% <R> <C> [BOLD] Pragmatic (all) <C> 50 <C> 61.6% <R> <C> [ITALIC] Interlocutor features <C> 5 <C> 51.2% <R> <C> [ITALIC] Trained toxicity <C> 2 <C> 60.5% <R> <C> [ITALIC] Toxicity +  [BOLD] Pragmatic <C> 52 <C> 64.9% <R> <C> [ITALIC] Humans <C> [EMPTY] <C> 72.0% <CAP> Table 3: Accuracies for the balanced future-prediction task. Features based on pragmatic devices are bolded, reference points are italicized.
<R> <C> non-neural baselines <C> RF <C> [ITALIC] F1 A <C> essay <C> micro 0.24 <C> db 0.22 <C> ibm 0.25 <C> com 0.03 <C> web 0.27 <C> cdcp - <C> ukp 0.22 <C> nk 0.43 <C> aif 0.31 <C> Avg 0.246 <C> Mcr Avg 0.467 <R> <C> non-neural baselines <C> RF <C> [ITALIC] F1 S <C> [EMPTY] <C> 0.80 <C> 0.71 <C> 0.67 <C> 0.75 <C> 0.63 <C> 0.94 <C> 0.60 <C> 0.53 <C> 0.56 <C> 0.688 <C> 0.467 <R> <C> non-neural baselines <C> RF <C> [ITALIC] F1 A <C> 0.32 <C> 0.24 <C> 0.40 <C> [EMPTY] <C> 0.33 <C> 0.38 <C> - <C> 0.39 <C> 0.55 <C> 0.44 <C> 0.381 <C> 0.508 <R> <C> non-neural baselines <C> RF <C> [ITALIC] F1 S <C> 0.57 <C> 0.74 <C> 0.64 <C> [EMPTY] <C> 0.67 <C> 0.59 <C> 0.85 <C> 0.53 <C> 0.59 <C> 0.54 <C> 0.636 <C> 0.508 <R> <C> non-neural baselines <C> RF <C> [ITALIC] F1 A <C> 0.57 <C> 0.40 <C> 0.45 <C> 0.53 <C> 0.43 <C> [EMPTY] <C> - <C> 0.60 <C> 0.52 <C> 0.57 <C> 0.509 <C> 0.490 <R> <C> non-neural baselines <C> RF <C> [ITALIC] F1 S <C> 0.44 <C> 0.47 <C> 0.52 <C> 0.41 <C> 0.57 <C> [EMPTY] <C> 0.51 <C> 0.45 <C> 0.50 <C> 0.38 <C> 0.472 <C> 0.490 <R> <C> non-neural baselines <C> RF <C> [ITALIC] F1 A <C> 0.67 <C> 0.45 <C> 0.60 <C> 0.62 <C> 0.56 <C> 0.62 <C> - <C> 0.71 <C> 0.68 <C> [EMPTY] <C> 0.614 <C> 0.335 <R> <C> non-neural baselines <C> RF <C> [ITALIC] F1 S <C> 0.01 <C> 0.02 <C> 0.17 <C> 0.01 <C> 0.04 <C> 0.24 <C> 0.01 <C> 0.00 <C> 0.00 <C> [EMPTY] <C> 0.056 <C> 0.335 <R> <C> non-neural baselines <C> SVM <C> [ITALIC] F1 A <C> [EMPTY] <C> 0.34 <C> 0.36 <C> 0.33 <C> 0.29 <C> 0.38 <C> - <C> 0.42 <C> 0.42 <C> 0.40 <C> 0.368 <C> 0.503 <R> <C> non-neural baselines <C> SVM <C> [ITALIC] F1 S <C> [EMPTY] <C> 0.71 <C> 0.67 <C> 0.65 <C> 0.67 <C> 0.59 <C> 0.84 <C> 0.57 <C> 0.56 <C> 0.49 <C> 0.639 <C> 0.503 <R> <C> non-neural baselines <C> SVM <C> [ITALIC] F1 A <C> 0.52 <C> 0.42 <C> 0.37 <C> [EMPTY] <C> 0.43 <C> 0.50 <C> - <C> 0.59 <C> 0.33 <C> 0.49 <C> 0.456 <C> 0.473 <R> <C> non-neural baselines <C> SVM <C> [ITALIC] F1 S <C> 0.48 <C> 0.51 <C> 0.50 <C> [EMPTY] <C> 0.45 <C> 0.47 <C> 0.61 <C> 0.43 <C> 0.53 <C> 0.42 <C> 0.489 <C> 0.473 <R> <C> non-neural baselines <C> SVM <C> [ITALIC] F1 A <C> 0.49 <C> 0.35 <C> 0.39 <C> 0.39 <C> 0.38 <C> [EMPTY] <C> - <C> 0.56 <C> 0.57 <C> 0.520 <C> 0.456 <C> 0.498 <R> <C> non-neural baselines <C> SVM <C> [ITALIC] F1 S <C> 0.50 <C> 0.54 <C> 0.52 <C> 0.59 <C> 0.60 <C> [EMPTY] <C> 0.67 <C> 0.46 <C> 0.47 <C> 0.500 <C> 0.539 <C> 0.498 <R> <C> non-neural baselines <C> SVM <C> [ITALIC] F1 A <C> 0.61 <C> 0.40 <C> 0.60 <C> 0.46 <C> 0.57 <C> 0.61 <C> - <C> 0.64 <C> 0.68 <C> [EMPTY] <C> 0.571 <C> 0.431 <R> <C> non-neural baselines <C> SVM <C> [ITALIC] F1 S <C> 0.35 <C> 0.50 <C> 0.22 <C> 0.57 <C> 0.04 <C> 0.24 <C> 0.40 <C> 0.30 <C> 0.00 <C> [EMPTY] <C> 0.291 <C> 0.431 <CAP> Table 2: Results on the datasets with attack (A) and support (S) relations. F1 A stands for the F1 measure of the attack relation and F1 S stands for the F1 measure of the support (S) relation. RF stands for Random Forests. The blanks represent the training dataset. The Average (Avg) and the Macro (Mcr) Avg do not include the results of the dataset used for training.
<R> <C> embeddings + syntactic <C> M FT <C> [ITALIC] F1 A <C> essay 0.52 <C> micro 0.40 <C> db 0.58 <C> ibm 0.50 <C> com 0.52 <C> web <C> cdcp - <C> ukp 0.58 <C> nk 0.52 <C> aif <C> Avg 0.517 <C> Mcr Avg 0.521 <R> <C> embeddings + syntactic <C> M FT <C> [ITALIC] F1 S <C> 0.47 <C> 0.50 <C> 0.61 <C> 0.54 <C> 0.54 <C> [EMPTY] <C> 0.60 <C> 0.42 <C> 0.53 <C> [EMPTY] <C> 0.526 <C> 0.521 <R> <C> embeddings + syntactic <C> C FT <C> [ITALIC] F1 A <C> [EMPTY] <C> 0.36 <C> [EMPTY] <C> 0.45 <C> 0.47 <C> 0.52 <C> - <C> 0.65 <C> 0.46 <C> 0.52 <C> 0.490 <C> 0.52 <R> <C> embeddings + syntactic <C> C FT <C> [ITALIC] F1 S <C> [EMPTY] <C> 0.64 <C> [EMPTY] <C> 0.55 <C> 0.67 <C> 0.53 <C> 0.72 <C> 0.34 <C> 0.48 <C> 0.47 <C> 0.550 <C> 0.52 <R> <C> embeddings + syntactic <C> C G <C> [ITALIC] F1 A <C> [EMPTY] <C> 0.35 <C> 0.43 <C> 0.48 <C> 0.31 <C> 0.45 <C> - <C> 0.58 <C> [EMPTY] <C> 0.43 <C> 0.433 <C> 0.526 <R> <C> embeddings + syntactic <C> C G <C> [ITALIC] F1 S <C> [EMPTY] <C> 0.71 <C> 0.68 <C> 0.58 <C> 0.70 <C> 0.54 <C> 0.77 <C> 0.47 <C> [EMPTY] <C> 0.50 <C> 0.619 <C> 0.526 <R> <C> embeddings + syntactic <C> A G <C> [ITALIC] F1 A <C> [EMPTY] <C> 0.37 <C> 0.58 <C> [EMPTY] <C> 0.53 <C> 0.53 <C> - <C> 0.61 <C> 0.59 <C> 0.55 <C> 0.537 <C> 0.526 <R> <C> embeddings + syntactic <C> A G <C> [ITALIC] F1 S <C> [EMPTY] <C> 0.61 <C> 0.60 <C> [EMPTY] <C> 0.42 <C> 0.50 <C> 0.72 <C> 0.43 <C> 0.38 <C> 0.47 <C> 0.516 <C> 0.526 <R> <C> embeddings + syntactic <C> A G <C> [ITALIC] F1 A <C> [EMPTY] <C> 0.36 <C> 0.48 <C> 0.43 <C> 0.39 <C> [EMPTY] <C> - <C> 0.52 <C> 0.45 <C> 0.51 <C> 0.449 <C> [BOLD] 0.544 <R> <C> embeddings + syntactic <C> A G <C> [ITALIC] F1 S <C> [EMPTY] <C> 0.75 <C> 0.66 <C> 0.62 <C> 0.68 <C> [EMPTY] <C> 0.79 <C> 0.52 <C> 0.56 <C> 0.54 <C> 0.640 <C> [BOLD] 0.544 <R> <C> all features <C> M G <C> F1 0 <C> 0.49 <C> 0.41 <C> 0.51 <C> [EMPTY] <C> 0.50 <C> 0.52 <C> - <C> 0.57 <C> 0.49 <C> [EMPTY] <C> 0.499 <C> 0.517 <R> <C> all features <C> M G <C> F1 1 <C> 0.49 <C> 0.63 <C> 0.60 <C> [EMPTY] <C> 0.52 <C> 0.45 <C> 0.69 <C> 0.37 <C> 0.54 <C> [EMPTY] <C> 0.536 <C> 0.517 <R> <C> all features <C> C G <C> F1 0 <C> 0.42 <C> 0.33 <C> 0.52 <C> 0.35 <C> 0.48 <C> [EMPTY] <C> - <C> 0.49 <C> 0.43 <C> [EMPTY] <C> 0.431 <C> 0.515 <R> <C> all features <C> C G <C> F1 1 <C> 0.54 <C> 0.68 <C> 0.63 <C> 0.66 <C> 0.53 <C> [EMPTY] <C> 0.75 <C> 0.46 <C> 0.55 <C> [EMPTY] <C> 0.600 <C> 0.515 <R> <C> all features <C> A G <C> F1 0 <C> [EMPTY] <C> 0.30 <C> 0.42 <C> 0.40 <C> 0.38 <C> 0.43 <C> - <C> 0.53 <C> 0.35 <C> 0.48 <C> 0.411 <C> 0.515 <R> <C> all features <C> A G <C> F1 1 <C> [EMPTY] <C> 0.73 <C> 0.61 <C> 0.60 <C> 0.68 <C> 0.55 <C> 0.82 <C> 0.49 <C> 0.56 <C> 0.53 <C> 0.619 <C> 0.515 <R> <C> all features <C> M G <C> F1 0 <C> [EMPTY] <C> 0.37 <C> 0.43 <C> 0.43 <C> 0.40 <C> 0.46 <C> - <C> 0.71 <C> - <C> 0.46 <C> 0.466 <C> [BOLD] 0.532 <R> <C> all features <C> M G <C> F1 1 <C> [EMPTY] <C> 0.71 <C> 0.64 <C> 0.61 <C> 0.70 <C> 0.55 <C> 0.78 <C> 0.11 <C> 0.78 <C> 0.51 <C> 0.599 <C> [BOLD] 0.532 <R> <C> all features <C> C FT <C> F1 0 <C> [EMPTY] <C> 0.37 <C> [EMPTY] <C> 0.50 <C> 0.49 <C> 0.54 <C> - <C> 0.68 <C> 0.51 <C> 0.57 <C> 0.523 <C> 0.509 <R> <C> all features <C> C FT <C> F1 1 <C> [EMPTY] <C> 0.59 <C> [EMPTY] <C> 0.50 <C> 0.59 <C> 0.50 <C> 0.68 <C> 0.24 <C> 0.43 <C> 0.43 <C> 0.495 <C> 0.509 <R> <C> all features <C> A G <C> F1 0 <C> [EMPTY] <C> 0.40 <C> 0.51 <C> 0.52 <C> 0.45 <C> 0.54 <C> [EMPTY] <C> 0.60 <C> 0.56 <C> 0.59 <C> 0.521 <C> 0.512 <R> <C> all features <C> A G <C> F1 1 <C> [EMPTY] <C> 0.61 <C> 0.57 <C> 0.52 <C> 0.61 <C> 0.45 <C> [EMPTY] <C> 0.42 <C> 0.42 <C> 0.43 <C> 0.504 <C> 0.512 <R> <C> all features <C> A G <C> F1 0 <C> [EMPTY] <C> 0.36 <C> 0.54 <C> [EMPTY] <C> 0.50 <C> 0.51 <C> - <C> 0.59 <C> 0.59 <C> 0.55 <C> 0.520 <C> [BOLD] 0.535 <R> <C> all features <C> A G <C> F1 1 <C> [EMPTY] <C> 0.67 <C> 0.63 <C> [EMPTY] <C> 0.49 <C> 0.51 <C> 0.74 <C> 0.47 <C> 0.41 <C> 0.49 <C> 0.551 <C> [BOLD] 0.535 <R> <C> all features <C> A FT <C> F1 0 <C> [EMPTY] <C> 0.40 <C> 0.43 <C> 0.43 <C> 0.37 <C> [EMPTY] <C> - <C> 0.55 <C> 0.26 <C> 0.50 <C> 0.420 <C> 0.522 <R> <C> all features <C> A FT <C> F1 1 <C> [EMPTY] <C> 0.72 <C> 0.64 <C> 0.58 <C> 0.72 <C> [EMPTY] <C> 0.77 <C> 0.47 <C> 0.59 <C> 0.51 <C> 0.625 <C> 0.522 <R> <C> all features <C> A G <C> F1 0 <C> [EMPTY] <C> 0.43 <C> 0.54 <C> 0.49 <C> 0.46 <C> [EMPTY] <C> - <C> 0.59 <C> 0.63 <C> 0.63 <C> 0.539 <C> [BOLD] 0.539 <R> <C> all features <C> A G <C> F1 1 <C> [EMPTY] <C> 0.68 <C> 0.55 <C> 0.57 <C> 0.56 <C> [EMPTY] <C> 0.65 <C> 0.46 <C> 0.38 <C> 0.47 <C> 0.540 <C> [BOLD] 0.539 <CAP> Table 3: Results on the datasets with attack (A) and support (S) relations. F1 A stands for the F1 measure of the attack relation and F1 S stands for the F1 measure of the support (S) relation. A stands for autoencoder model, C for concatenation model, M for mix model, G for GloVE embeddings, and FT for FastText embeddings. The blanks represent the training datasets. The Average (Avg) and the Macro (Mcr) Avg do not include the results of the dataset(s) used for training.
<R> <C> BERT embeddings + syntactic <C> 3B <C> [ITALIC] F1 A <C> essay <C> micro <C> db 0.53 <C> ibm 0.48 <C> com 0.52 <C> web 0.49 <C> cdcp - <C> ukp 0.56 <C> nk 0.46 <C> aif 0.43 <C> Avg 0.496 <C> Mcr Avg 0.522 <R> <C> BERT embeddings + syntactic <C> 1D <C> [ITALIC] F1 S <C> [EMPTY] <C> [EMPTY] <C> 0.63 <C> 0.56 <C> 0.58 <C> 0.50 <C> 0.70 <C> 0.48 <C> 0.51 <C> 0.42 <C> 0.548 <C> 0.522 <R> <C> BERT embeddings + syntactic <C> 4B <C> [ITALIC] F1 A <C> [EMPTY] <C> [EMPTY] <C> 0.55 <C> 0.47 <C> 0.53 <C> 0.50 <C> - <C> 0.56 <C> 0.48 <C> 0.45 <C> 0.506 <C> 0.526 <R> <C> BERT embeddings + syntactic <C> 2D <C> [ITALIC] F1 S <C> [EMPTY] <C> [EMPTY] <C> 0.61 <C> 0.57 <C> 0.59 <C> 0.49 <C> 0.69 <C> 0.47 <C> 0.48 <C> 0.46 <C> 0.545 <C> 0.526 <R> <C> BERT embeddings + syntactic <C> 4B <C> [ITALIC] F1 A <C> [EMPTY] <C> 0.36 <C> 0.48 <C> 0.40 <C> 0.45 <C> 0.42 <C> - <C> 0.53 <C> [EMPTY] <C> 0.37 <C> 0.430 <C> 0.525 <R> <C> BERT embeddings + syntactic <C> 1D <C> [ITALIC] F1 S <C> [EMPTY] <C> 0.69 <C> 0.67 <C> 0.61 <C> 0.62 <C> 0.57 <C> 0.79 <C> 0.50 <C> [EMPTY] <C> 0.50 <C> 0.619 <C> 0.525 <R> <C> BERT embeddings + syntactic <C> 3B <C> [ITALIC] F1 A <C> [EMPTY] <C> 0.39 <C> 0.57 <C> 0.53 <C> 0.46 <C> 0.53 <C> - <C> 0.61 <C> 0.57 <C> [EMPTY] <C> 0.523 <C> 0.520 <R> <C> BERT embeddings + syntactic <C> 1D <C> [ITALIC] F1 S <C> [EMPTY] <C> 0.59 <C> 0.57 <C> 0.44 <C> 0.54 <C> 0.49 <C> 0.61 <C> 0.36 <C> 0.53 <C> [EMPTY] <C> 0.516 <C> 0.520 <R> <C> BERT embeddings + syntactic <C> 4B <C> [ITALIC] F1 A <C> [EMPTY] <C> 0.37 <C> 0.54 <C> 0.52 <C> 0.43 <C> 0.51 <C> - <C> 0.58 <C> 0.56 <C> [EMPTY] <C> 0.501 <C> 0.521 <R> <C> BERT embeddings + syntactic <C> 2D <C> [ITALIC] F1 S <C> [EMPTY] <C> 0.61 <C> 0.58 <C> 0.45 <C> 0.57 <C> 0.52 <C> 0.65 <C> 0.40 <C> 0.55 <C> [EMPTY] <C> 0.541 <C> 0.521 <R> <C> BERT embeddings + syntactic <C> 3B <C> [ITALIC] F1 A <C> [EMPTY] <C> 0.30 <C> 0.53 <C> [EMPTY] <C> 0.51 <C> 0.39 <C> - <C> 0.44 <C> 0.44 <C> 0.47 <C> 0.440 <C> 0.525 <R> <C> BERT embeddings + syntactic <C> 2D <C> [ITALIC] F1 S <C> [EMPTY] <C> 0.72 <C> 0.64 <C> [EMPTY] <C> 0.61 <C> 0.57 <C> 0.80 <C> 0.52 <C> 0.54 <C> 0.47 <C> 0.609 <C> 0.525 <R> <C> BERT embeddings + syntactic <C> 4B <C> [ITALIC] F1 A <C> [EMPTY] <C> 0.33 <C> 0.49 <C> [EMPTY] <C> 0.49 <C> 0.40 <C> - <C> 0.56 <C> 0.47 <C> 0.44 <C> 0.454 <C> [BOLD] 0.531 <R> <C> BERT embeddings + syntactic <C> 1D <C> [ITALIC] F1 S <C> [EMPTY] <C> 0.68 <C> 0.66 <C> [EMPTY] <C> 0.61 <C> 0.56 <C> 0.78 <C> 0.46 <C> 0.56 <C> 0.55 <C> 0.608 <C> [BOLD] 0.531 <R> <C> BERT embeddings + syntactic <C> 4B <C> [ITALIC] F1 A <C> [EMPTY] <C> 0.29 <C> 0.49 <C> [EMPTY] <C> 0.50 <C> 0.33 <C> - <C> 0.43 <C> 0.40 <C> 0.36 <C> 0.400 <C> 0.522 <R> <C> BERT embeddings + syntactic <C> 2D <C> [ITALIC] F1 S <C> [EMPTY] <C> 0.72 <C> 0.68 <C> [EMPTY] <C> 0.64 <C> 0.59 <C> 0.83 <C> 0.53 <C> 0.57 <C> 0.59 <C> 0.644 <C> 0.522 <R> <C> BERT embeddings + syntactic <C> 3B <C> [ITALIC] F1 A <C> 0.49 <C> 0.35 <C> 0.47 <C> [EMPTY] <C> 0.53 <C> 0.46 <C> [EMPTY] <C> 0.63 <C> 0.54 <C> 0.62 <C> 0.511 <C> 0.521 <R> <C> BERT embeddings + syntactic <C> 2D <C> [ITALIC] F1 S <C> 0.53 <C> 0.63 <C> 0.55 <C> [EMPTY] <C> 0.64 <C> 0.50 <C> [EMPTY] <C> 0.35 <C> 0.53 <C> 0.51 <C> 0.530 <C> 0.521 <R> <C> BERT embeddings + syntactic <C> 4B <C> [ITALIC] F1 A <C> 0.50 <C> 0.36 <C> 0.46 <C> [EMPTY] <C> 0.50 <C> [EMPTY] <C> - <C> 0.52 <C> 0.47 <C> 0.50 <C> 0.473 <C> [BOLD] 0.537 <R> <C> BERT embeddings + syntactic <C> 2D <C> [ITALIC] F1 S <C> 0.61 <C> 0.62 <C> 0.59 <C> [EMPTY] <C> 0.61 <C> [EMPTY] <C> 0.74 <C> 0.52 <C> 0.50 <C> 0.61 <C> 0.600 <C> [BOLD] 0.537 <R> <C> BERT embeddings + syntactic <C> 4B <C> [ITALIC] F1 A <C> [EMPTY] <C> 0.39 <C> 0.54 <C> 0.47 <C> 0.52 <C> [EMPTY] <C> - <C> 0.58 <C> 0.50 <C> 0.58 <C> 0.511 <C> 0.520 <R> <C> BERT embeddings + syntactic <C> 1D <C> [ITALIC] F1 S <C> [EMPTY] <C> 0.61 <C> 0.55 <C> 0.52 <C> 0.59 <C> [EMPTY] <C> 0.69 <C> 0.46 <C> 0.48 <C> 0.32 <C> 0.528 <C> 0.520 <R> <C> all features <C> 3B <C> [ITALIC] F1 A <C> [EMPTY] <C> 0.33 <C> 0.42 <C> 0.41 <C> 0.52 <C> 0.42 <C> - <C> 0.53 <C> 0.45 <C> 0.35 <C> 0.429 <C> 0.524 <R> <C> all features <C> 1D <C> [ITALIC] F1 S <C> [EMPTY] <C> 0.67 <C> 0.66 <C> 0.63 <C> 0.63 <C> 0.58 <C> 0.80 <C> 0.52 <C> 0.54 <C> 0.53 <C> 0.618 <C> 0.524 <R> <C> all features <C> 3B <C> [ITALIC] F1 A <C> [EMPTY] <C> [EMPTY] <C> 0.53 <C> 0.49 <C> 0.51 <C> 0.49 <C> - <C> 0.58 <C> 0.48 <C> 0.45 <C> 0.504 <C> 0.522 <R> <C> all features <C> 1D <C> [ITALIC] F1 S <C> [EMPTY] <C> [EMPTY] <C> 0.62 <C> 0.56 <C> 0.61 <C> 0.50 <C> 0.69 <C> 0.46 <C> 0.48 <C> 0.40 <C> 0.540 <C> 0.522 <R> <C> all features <C> 4B <C> [ITALIC] F1 A <C> [EMPTY] <C> [EMPTY] <C> 0.53 <C> 0.50 <C> 0.54 <C> 0.51 <C> - <C> 0.59 <C> 0.51 <C> 0.49 <C> 0.524 <C> 0.529 <R> <C> all features <C> 1D <C> [ITALIC] F1 S <C> [EMPTY] <C> [EMPTY] <C> 0.59 <C> 0.56 <C> 0.55 <C> 0.47 <C> 0.67 <C> 0.45 <C> 0.48 <C> 0.49 <C> 0.533 <C> 0.529 <R> <C> all features <C> 3B <C> [ITALIC] F1 A <C> 0.48 <C> 0.34 <C> 0.48 <C> [EMPTY] <C> 0.45 <C> [EMPTY] <C> - <C> 0.45 <C> 0.50 <C> 0.54 <C> 0.463 <C> [BOLD] 0.532 <R> <C> all features <C> 2D <C> [ITALIC] F1 S <C> 0.57 <C> 0.65 <C> 0.60 <C> [EMPTY] <C> 0.64 <C> [EMPTY] <C> 0.73 <C> 0.55 <C> 0.52 <C> 0.54 <C> 0.600 <C> [BOLD] 0.532 <CAP> Table 4: Results on the datasets with attack (A) and support (S) relations. F1 A stands for the F1 of the attack relation and F1 S stands for the F1 of the support (S) relation. XB stands for the number of BERT layers used (i.e. X) and YB stands for the number of dense layers (i.e. Y) used before the final layer that predicts the class. The blanks represent the training datasets. The Average (Avg) and the Macro (Mcr) Avg do not include the results of the dataset(s) used for training.
<R> <C> em_size <C> #conv <C> #rnn1 <C> #rnn2 <R> <C> 50 <C> 192 <C> 224 <C> 384 <CAP> Table 2. Best CNN-GRU network design for the final SMILES2vec model.
<R> <C> reward  [ITALIC] r <C> 0 <C> 1 <C> 1.1 <C> 1.2 <C> 1.3 <C> 1.4 <C> 1.5 <R> <C> BLEU <C> 32.2 <C> 34.6 <C> 34.6 <C> [BOLD] 34.7 <C> 34.6 <C> 34.6 <C> 34.6 <R> <C> len. ratio <C> 0.88 <C> .95 <C> .96 <C> .97 <C> .98 <C> .98 <C> .99 <R> <C> best  [ITALIC] b <C> 4 <C> 17 <C> 17 <C> 15 <C> 20 <C> 20 <C> 17 <CAP> Table 2: Tuning length reward r (with beam size b=1..20) for optimal bounded-reward beam search.
<R> <C> Model <C> CoLA <C> SST-2 <C> MRPC <C> STS-B <C> QQP <C> MNLI-m <C> MNLI-mm <C> QNLI <C> RTE <C> Diag <C> Avg <R> <C> [EMPTY] <C> MCC <C> Acc <C> F1 <C> Spear <C> F1 <C> Acc <C> Acc <C> Acc <C> Acc <C> MCC <C> – <R> <C> BERT Base <C> 52.1 <C> 93.5 <C> [BOLD] 88.9 <C> 85.8 <C> 71.2 <C> [BOLD] 84.6 <C> 83.4 <C> 90.5 <C> 66.4 <C> 34.2 <C> 75.1 <R> <C> OM-Adapt (25K) <C> 49.5 <C> 93.5 <C> 88.8 <C> 85.1 <C> 71.4 <C> 84.4 <C> 83.5 <C> [BOLD] 90.9 <C> 67.5 <C> 35.7 <C> 75.0 <R> <C> OM-Adapt (100K) <C> [BOLD] 53.5 <C> 93.4 <C> 87.9 <C> [BOLD] 85.9 <C> 71.1 <C> 84.2 <C> [BOLD] 83.7 <C> 90.6 <C> 68.2 <C> 34.8 <C> 75.3 <R> <C> CN-Adapt (50K) <C> 49.8 <C> [BOLD] 93.9 <C> [BOLD] 88.9 <C> 85.8 <C> [BOLD] 71.6 <C> 84.2 <C> 83.3 <C> 90.6 <C> [BOLD] 69.7 <C> 37.0 <C> [BOLD] 75.5 <R> <C> CN-Adapt (100K) <C> 48.8 <C> 92.8 <C> 87.1 <C> 85.7 <C> 71.5 <C> 83.9 <C> 83.2 <C> 90.8 <C> 64.1 <C> [BOLD] 37.8 <C> 74.6 <CAP> Table 1: Results on test portions of GLUE benchmark tasks. Numbers in brackets next to adapter-based models (25K, 50K, 100K) indicate the number of update steps of adapter training on the synthetic ConceptNet corpus (for CN-Adapt) or on the original OMCS corpus (for OM-Adapt). Bold: the best score in each column.
<R> <C> Method <C> Seed <C> RT-07 <C> RT-09 <R> <C> Best first <C> [EMPTY] <C> [BOLD] 4.1 <C> [BOLD] 8.5 <R> <C> Randomized <C> 1 <C> 2.8 <C> 7.5 <R> <C> [EMPTY] <C> 2 <C> 2.4 <C> 8.1 <R> <C> [EMPTY] <C> 3 <C> 3.7 <C> 8.3 <R> <C> [EMPTY] <C> 4 <C> 3.6 <C> 8.7 <R> <C> [EMPTY] <C> 5 <C> 5.4 <C> 8.5 <R> <C> DOVER <C> [EMPTY] <C> [BOLD] 3.3 <C> [BOLD] 8.1 <CAP> Table 5: Results with standard best-first clustering and randomized second-best clustering under different random seeds. The final row represents the DOVER combination of the randomized trials.
<R> <C> Sampling None <C> Model DSL <C> Model DSL <C> city 0.719 <C> museum 0.889 <C> soccer 0.614 <C> weapons 0.611 <C> weather 0.805 <R> <C> None <C> DSL+ <C> DSL+ <C> 0.782 <C> [BOLD] 0.927 <C> [BOLD] 0.813 <C> [BOLD] 0.872 <C> [BOLD] 1 <R> <C> None <C> DINT <C> all <C> [BOLD] 0.949 <C> 0.798 <C> 0.553 <C> 0.688 <C> 0.583 <R> <C> None <C> DINT <C> base <C> 0.888 <C> 0.763 <C> 0.516 <C> 0.684 <C> 0.621 <R> <C> None <C> DINT <C> base+ <C> 0.888 <C> 0.778 <C> 0.542 <C> 0.686 <C> 0.621 <R> <C> Bagging <C> MLP <C> MLP <C> 0.797 <C> 0.77 <C> 0.663 <C> 0.695 <C> 0.887 <R> <C> Bagging <C> CNN <C> CNN <C> 0.723 <C> 0.774 <C> 0.606 <C> 0.664 <C> 0.882 <R> <C> Bagging <C> DINT <C> all <C> 0.945 <C> 0.791 <C> 0.656 <C> 0.682 <C> 0.854 <R> <C> Bagging <C> DINT <C> base <C> 0.919 <C> 0.788 <C> 0.634 <C> 0.701 <C> 0.867 <R> <C> Bagging <C> DINT <C> base+ <C> 0.919 <C> 0.790 <C> 0.628 <C> 0.688 <C> 0.852 <R> <C> Resample <C> DINT <C> all <C> [BOLD] 0.949 <C> 0.789 <C> 0.455 <C> 0.588 <C> 0.557 <R> <C> To <C> DINT <C> base <C> 0.89 <C> 0.749 <C> 0.451 <C> 0.578 <C> 0.611 <R> <C> Mean <C> DINT <C> base+ <C> 0.89 <C> 0.758 <C> 0.445 <C> 0.564 <C> 0.611 <CAP> Table 6: MRR scores for repeated holdout strategy when unknown attributes are not considered.
<R> <C> [BOLD] Response <C> [BOLD] Score with Context 1 <C> [BOLD] Score with Context 2 <R> <C> I am planning to visit the farm soon. <C> [BOLD] 98.35 <C> 93.91 <R> <C> I am going to watch them on TV. <C> 94.24 <C> [BOLD] 95.35 <CAP> Table 1: Our ranker utilizing the context to disambiguate the words watch and bulls and adjusting the scores of the candidate responses accordingly.
<R> <C> [EMPTY] <C> Model  [BOLD] Single-loss <C> Model  [BOLD] Single-loss <C> Model  [BOLD] Multi-loss <C> Model  [BOLD] Multi-loss <R> <C> Context Length <C> [ITALIC] N <C> [ITALIC] N <C> [ITALIC] N <C> [ITALIC] N <R> <C> Up To <C> 10 <C> 100 <C> 10 <C> 100 <R> <C> 0 <C> 74.45 <C> 47.92 <C> 75.15 <C> 49.16 <R> <C> 1 <C> 78.38 <C> 51.80 <C> 80.16 <C> 55.97 <R> <C> 2 <C> 79.23 <C> 52.30 <C> 81.30 <C> 56.25 <R> <C> 5 <C> 78.41 <C> [BOLD] 52.84 <C> 81.35 <C> [BOLD] 56.39 <R> <C> 10 <C> 79.32 <C> 50.74 <C> 81.70 <C> 55.25 <R> <C> 25 <C> [BOLD] 79.70 <C> 51.70 <C> [BOLD] 81.71 <C> 55.52 <CAP> Table 3: Precision @ 1 for models trained on different context lengths and tested on two different sizes of candidates pools.
<R> <C> [EMPTY] <C> Model  [BOLD] Single-loss <C> Model  [BOLD] Single-loss <C> Model  [BOLD] Multi-loss <C> Model  [BOLD] Multi-loss <R> <C> [EMPTY] <C> [ITALIC] N <C> [ITALIC] N <C> [ITALIC] N <C> [ITALIC] N <R> <C> [BOLD] Feature <C> 10 <C> 100 <C> 10 <C> 100 <R> <C> message <C> 74.45 <C> 47.92 <C> 75.15 <C> 49.16 <R> <C> message + context <C> 79.70 <C> 51.70 <C> 81.71 <C> 55.52 <R> <C> message + author <C> 79.52 <C> 53.03 <C> 83.25 <C> 60.53 <R> <C> All <C> [BOLD] 82.72 <C> [BOLD] 55.91 <C> [BOLD] 86.60 <C> [BOLD] 63.53 <CAP> Table 4: P@1 improvement gained by adding author and/or context to the base model. We consider context length to be 25.
<R> <C> [EMPTY] <C> VI <C> ARI <C> 1-1 <C> F1 <C> P <C> R <R> <C> Linear <C> 88.9 <C> - <C> 69.5 <C> 21.8 <C> 19.3 <C> 24.9 <R> <C> Feedforward <C> 91.3 <C> - <C> 75.6 <C> 36.2 <C> 34.6 <C> 38.0 <R> <C> × 10 union <C> 86.2 <C> - <C> 62.5 <C> 33.4 <C> 40.4 <C> 28.5 <R> <C> × 10 vote <C> [BOLD] 91.5 <C> - <C> [BOLD] 76.0 <C> [BOLD] 38.0 <C> 36.3 <C> [BOLD] 39.7 <R> <C> × 10 intersect <C> 69.3 <C> - <C> 26.6 <C> 32.1 <C> [BOLD] 67.0 <C> 21.1 <R> <C> Elsner(2008) <C> 82.1 <C> - <C> 51.4 <C> 15.5 <C> 12.1 <C> 21.5 <R> <C> Lowe(2017) <C> 80.6 <C> - <C> 53.7 <C> 8.9 <C> 10.8 <C> 7.6 <R> <C> Zhu(2019) 1 <C> 82.1 <C> - <C> 59.6 <C> 8.7 <C> 12.6 <C> 10.3 <R> <C> +feature 1 <C> 89.8 <C> - <C> 75.4 <C> 35.8 <C> 32.7 <C> 34.2 <R> <C> BERT <C> 90.8 <C> 62.9 <C> 75.0 <C> 32.5 <C> 29.3 <C> 36.6 <R> <C> + feature <C> 92.2 <C> 65.9 <C> 76.8 <C> 37.8 <C> 33.9 <C> 42.5 <R> <C> DialBERT <C> [BOLD] 92.6 <C> [BOLD] 69.6 <C> 78.5 <C> [BOLD] 44.1 <C> [BOLD] 42.3 <C> 46.2 <R> <C> + feature <C> 92.4 <C> 64.6 <C> 77.6 <C> 42.2 <C> 38.8 <C> [BOLD] 46.3 <R> <C> + future <C> 92.3 <C> 66.3 <C> [BOLD] 79.1 <C> 42.6 <C> 40.0 <C> 45.6 <CAP> Table 1: Results on Ubuntu test set, our work substantially outperforms prior work by all evaluation metrics. Six types of metrics are considered including the modified Variation of Information (VI) in (Kummerfeld et al. 2019), Adjusted rand index(ARI), One-to-One Overlap (1-1) of the cluster (Elsner and Charniak 2008), and the precision, recall, and F1 score between the cluster prediction and ground truth. Note that precision, recall, and F1 score are calculated using the number of perfectly matching conversations, excluding conversations with only one message (mostly system messages).
<R> <C> [EMPTY] <C> VI <C> ARI <C> F1 <C> P <C> R <R> <C> Model-AVG <C> [BOLD] 92.8 <C> [BOLD] 72.1 <C> 43.3 <C> 39.8 <C> 47.6 <R> <C> Probability-AVG <C> 92.6 <C> 66.6 <C> [BOLD] 45.3 <C> [BOLD] 42.1 <C> [BOLD] 49.0 <R> <C> Vote-AVG <C> 92.6 <C> 66.6 <C> 45.0 <C> 42.0 <C> 48.5 <CAP> Table 2: Results for ensemble models on test set.
<R> <C> Pattern Name <C> Precision <C> Recall <C> [ITALIC] F1 <R> <C> A_Read_No_Zero <C> 0.974 <C> 0.979 <C> 0.977 <R> <C> A_Spell_Keep_Zero <C> 0.932 <C> 0.916 <C> 0.924 <R> <C> B_Percent <C> 0.998 <C> 0.990 <C> 0.994 <R> <C> B_Range <C> 0.932 <C> 0.932 <C> 0.932 <R> <C> B_Time <C> 0.969 <C> 0.912 <C> 0.939 <R> <C> B_Score_Ratio <C> 0.962 <C> 0.962 <C> 0.962 <R> <C> B_Slash_Per <C> 0.994 <C> 0.966 <C> 0.980 <R> <C> B_Date_YMD <C> 1.000 <C> 0.923 <C> 0.960 <R> <C> A_Two_Liang <C> 0.613 <C> 0.797 <C> 0.693 <R> <C> A_One_Yao_Spell <C> 0.637 <C> 0.631 <C> 0.634 <R> <C> Overall Accuracy <C> Overall Accuracy <C> 0.916 <C> 0.916 <CAP> Table 3: Model performance on the test dataset.
<R> <C> [BOLD] Type <C> [BOLD] N4 <C> [BOLD] N5 <C> [BOLD] N6 <C> [BOLD] N2 <C> [BOLD] E3 <C> [BOLD] E7 <C> [BOLD] Total <C> [BOLD] Identical to <C> [BOLD] Modified <C> [BOLD] Other <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [BOLD] source text <C> [BOLD] source text <C> [EMPTY] <R> <C> Class <C> 2 <C> 13 <C> 6 <C> 13 <C> 15 <C> 10 <C> 59 <C> 24 (41%) <C> 33 (56%) <C> 2 (3%) <R> <C> Indiv <C> 2 <C> 7 <C> 9 <C> 7 <C> 6 <C> 8 <C> 39 <C> 35 (90%) <C> 3 (8%) <C> 1 (2%) <R> <C> Prop <C> 1 <C> 2 <C> 4 <C> 3 <C> 4 <C> 7 <C> 21 <C> 5 (24%) <C> 12 (57%) <C> 4 (19%) <R> <C> [BOLD] Total <C> [BOLD] 5 <C> [BOLD] 22 <C> [BOLD] 19 <C> [BOLD] 23 <C> [BOLD] 25 <C> [BOLD] 25 <C> [BOLD] 119 <C> 64 (54%) <C> 48 (40%) <C> 7 (6%) <CAP> Table 4: Frequencies and origins of identifier names by type.
<R> <C> Parser <C> F-score <R> <C> Stanford Schuster et al. ( 2015 ) <C> 0.3549 <R> <C> SPICE Anderson et al. ( 2016 ) <C> 0.4469 <R> <C> Custom Dependency Parsing Wang et al. ( 2018 ) <C> 0.4967 <R> <C> Attention Graph (ours) <C> [BOLD] 0.5221 <R> <C> Oracle (as reported in Wang et al. ( 2018 )) <C> 0.6985 <R> <C> Oracle (as used herein) <C> 0.6630 <CAP> Table 1: SPICE metric scores for the Oracle (using code released by Wang et al. (2018)) and our method, under the base assumptions, and also where the number of tuples is bounded above by the number of potentially useful words in the region description
<R> <C> [EMPTY] <C> [BOLD] Base <C> [BOLD] FrameBase <C> [BOLD] XWFN <C> [BOLD] TransX <C> [BOLD] DirectX <C> [BOLD] Fprofile <R> <C> oracle <C> 67.60 <C> 67.74 <C> 57.85 <C> 77.82 <C> 73.74 <C> 68.16 <R> <C> original (cond) <C> 53.13 <C> 52.73 <C> 45.44 <C> [BOLD] 45.21 <C> 46.30 <C> 53.04 <R> <C> wiki-n30-1400k (cond) <C> 53.23 <C> 52.82 <C> 45.42 <C> 44.75 <C> [BOLD] 46.58 <C> 53.46 <R> <C> wiki-n30-1400k (inv) <C> [BOLD] 55.52 <C> [BOLD] 55.06 <C> [BOLD] 46.73 <C> 28.24 <C> 45.39 <C> [BOLD] 55.69 <CAP> Table 5: F1 performances on Word Frame Disambiguation across profiles and ranking methodologies.
<R> <C> Model <C> F1 measure forward <C> F1 measure backward <C> F1 measure bidirectional <R> <C>  lstm <C> 95.12 <C> – <C> 95.23 <R> <C>  gru <C> 95.43 <C> – <C> 95.53 <R> <C>  E-rnn <C> 94.73 <C> 93.61 <C> 94.71 <R> <C>  J-rnn <C> 94.94 <C> 94.80 <C> 94.89 <R> <C>  I-rnn <C> 95.21 <C> 94.64 <C> 94.75 <R> <C> I- [ITALIC] rnnGRU Words <C> 93.58 <C> 93.81 <C> 93.83 <R> <C> I- [ITALIC] rnn Words <C> 94.31 <C> 94.32 <C> 94.47 <R> <C> I- [ITALIC] rnn Words+Classes <C> 95.37 <C> 95.44 <C> [BOLD] 95.56 <R> <C> I- [ITALIC] rnn Words+Classes+CC <C> 95.40 <C> 95.39 <C> 95.46 <R> <C> I- [ITALIC] rnndeep Words <C> 94.47 <C> 94.29 <C> 94.52 <R> <C> I- [ITALIC] rnndeep Words+Classes <C> [BOLD] 95.67 <C> [BOLD] 95.54 <C> [BOLD] 95.60 <R> <C> I- [ITALIC] rnndeep Words+Classes+CC <C> [BOLD] 95.56 <C> 95.39 <C> [BOLD] 95.53 <CAP> Table 3: Comparison of our results on the ATIS task with the literature, in terms of F1 measure.
<R> <C> MSDialog run <C> MSDialog  [ITALIC] random <C> MSDialog # [ITALIC] turns <C> MSDialog ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯#U [ITALIC] words <C> MSDialog ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯#R [ITALIC] words <C> MSDialog  [ITALIC] σSM <C> MSDialog  [ITALIC] σBM25 <C> MSDialog  [ITALIC] BERTpred <C> MSDialog ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ [ITALIC] BERTloss <R> <C> 1 <C> 0.7142 <C> 0.7220 † <C> 0.7229 † <C> 0.7182 <C> 0.7239 †‡ <C> 0.7175 <C> [BOLD] 0.7272 †‡ <C> 0.7244 †‡ <R> <C> 2 <C> 0.7044 <C> 0.7060 <C> 0.7053 <C> 0.6968 <C> 0.7032 <C> 0.7003 <C> [BOLD] 0.7159 †‡ <C> 0.7194 †‡ <R> <C> 3 <C> 0.7126 <C> 0.7215 † <C> 0.7163 <C> 0.7171 <C> 0.7174 <C> 0.7159 <C> [BOLD] 0.7296 †‡ <C> 0.7225 †‡ <R> <C> 4 <C> 0.7031 <C> 0.7065 <C> 0.7043 <C> 0.6993 <C> 0.7026 <C> 0.6949 <C> 0.7154 †‡ <C> [BOLD] 0.7204 †‡ <R> <C> 5 <C> 0.7148 <C> 0.7225 † <C> 0.7203 <C> 0.7169 <C> 0.7171 <C> 0.7134 <C> 0.7322 †‡ <C> [BOLD] 0.7331 †‡ <R> <C> AVG <C> 0.7098 <C> 0.7157 <C> 0.7138 <C> 0.7097 <C> 0.7128 <C> 0.7084 <C> [BOLD] 0.7241 <C> 0.7240 <R> <C> SD <C> 0.0056 <C> 0.0086 <C> 0.0086 <C> 0.0106 <C> 0.0095 <C> 0.0101 <C> 0.0079 <C> 0.0055 <R> <C> MANtIS <C> MANtIS <C> MANtIS <C> MANtIS <C> MANtIS <C> MANtIS <C> MANtIS <C> MANtIS <C> MANtIS <R> <C> 1 <C> 0.7203 <C> 0.7192 <C> 0.7198 <C> 0.7194 <C> 0.7166 <C> 0.7200 <C> 0.7257 †‡ <C> [BOLD] 0.7268 †‡ <R> <C> 2 <C> 0.6984 <C> 0.6993 <C> 0.6989 <C> 0.6996 <C> 0.6964 <C> 0.7009 <C> [BOLD] 0.7067 †‡ <C> 0.7051 †‡ <R> <C> 3 <C> 0.7200 <C> 0.7197 <C> 0.7134 <C> 0.7206 <C> 0.7153 <C> 0.7153 <C> [BOLD] 0.7282 †‡ <C> 0.7221 <R> <C> 4 <C> 0.7114 <C> 0.7117 <C> 0.7002 <C> 0.6978 <C> 0.7140 <C> 0.7084 <C> [BOLD] 0.7240 †‡ <C> 0.7184 †‡ <R> <C> 5 <C> 0.7156 <C> 0.7174 <C> 0.7193 † <C> 0.7162 <C> 0.7147 <C> 0.7185 <C> [BOLD] 0.7264 †‡ <C> 0.7258 †‡ <R> <C> AVG <C> 0.7131 <C> 0.7135 <C> 0.7103 <C> 0.7107 <C> 0.7114 <C> 0.7126 <C> [BOLD] 0.7222 <C> 0.7196 <R> <C> SD <C> 0.0090 <C> 0.0085 <C> 0.0102 <C> 0.0111 <C> 0.0084 <C> 0.0079 <C> 0.0088 <C> 0.0088 <CAP> Table 4: Test set MAP results of 5 runs using different curriculum learning scoring functions. Superscripts †/‡ denote statistically significant improvements over the baseline where no curriculum learning is applied (fscore=random) at 95%/99% confidence intervals. Bold indicates the highest MAP for each line.
<R> <C> [BOLD] Method <C> [BOLD] Lev. D <C> [BOLD] -ELBO <C> [BOLD] Recon. <C> [BOLD] KL <C> [BOLD] PPL <C> [BOLD] -LL <R> <C> LSTM-LM <C> – <C> – <C> – <C> – <C> 72.22 <C> 104.67 <R> <C> VAE <C> 0.90 <C> 105.00 <C> 104.99 <C> 0.01 <C> 73.10 <C> 104.99 <R> <C> [ITALIC] β-VAE [ITALIC] β=0.6 <C> 0.85 <C> 105.09 <C> 101.73 <C> 3.36 <C> 72.38 <C> 104.74 <R> <C> Cyc. ann. VAE [ITALIC] M=5 <C> 0.86 <C> 104.80 <C> 103.40 <C> 1.40 <C> 72.07 <C> 104.64 <R> <C> Lev. VAE [ITALIC] τ=1.0 [ITALIC] α=1.0, [ITALIC] λ=0.15 <C> 0.87 <C> 104.72 <C> 103.42 <C> 1.30 <C> 72.11 <C> 104.65 <R> <C> Lev. VAE [ITALIC] τ=1.0 [ITALIC] α=1.0. [ITALIC] λ=1.0 <C> 0.79 <C> 109.74 <C> 98.01 <C> 11.73 <C> 80.19 <C> 107.25 <CAP> Table 1: Results on PTB test set for various methods.
<R> <C> [BOLD] Method <C> [BOLD] Lev.D <C> [BOLD] Recon. <C> [BOLD] KL <C> [BOLD] Acc.% <R> <C> AE <C> 0.00 <C> 0.03 <C> – <C> 86.57 <R> <C> L. AE <C> 0.00 <C> 0.08 <C> – <C> 89.93 <R> <C> [ITALIC] β-VAE [ITALIC] β=1.0 / VAE <C> 0.79 <C> 31.84 <C> 0.01 <C> 91.90 <R> <C> Cyc. ann. VAE [ITALIC] M=18 <C> 0.80 <C> 32.48 <C> 0.02 <C> 91.95 <R> <C> L. VAE [ITALIC] τ=0.7 [ITALIC] α=1.0, [ITALIC] λ=1.0 <C> 0.30 <C> 10.06 <C> 33.15 <C> 92.66 <R> <C> L. VAE [ITALIC] τ=0.2 [ITALIC] α=0.0, [ITALIC] λ=1.0 <C> 0.18 <C> 7.44 <C> 49.95 <C> 92.91 <CAP> Table 2: Results on Yelp test set for various methods.
<R> <C> [BOLD] Method <C> [BOLD] Lev.D <C> [BOLD] Recon. <C> [BOLD] KL <C> [BOLD] Acc.% <R> <C> AE <C> 0.02 <C> 1.23 <C> – <C> 59.71 <R> <C> L. AE <C> 0.02 <C> 1.49 <C> – <C> 61.49 <R> <C> VAE <C> 0.70 <C> 31.13 <C> 0.01 <C> 43.36 <R> <C> [ITALIC] β-VAE [ITALIC] β=0.1 <C> 0.09 <C> 2.49 <C> 54.91 <C> 60.33 <R> <C> Cyc. ann. VAE [ITALIC] M=2 <C> 0.69 <C> 30.60 <C> 0.57 <C> 55.54 <R> <C> L. VAE [ITALIC] τ=0.4 [ITALIC] α=0.0, [ITALIC] λ=1.0 <C> 0.21 <C> 13.44 <C> 31.93 <C> 63.31 <R> <C> L. VAE [ITALIC] τ=0.6 [ITALIC] α=1.0, [ITALIC] λ=1.0 <C> 0.17 <C> 7.37 <C> 36.14 <C> 63.80 <CAP> Table 3: Results for SNLI test set for various baselines.
<R> <C> [EMPTY] <C> [BOLD] Model <C> [ITALIC] τ <C> overall  [ITALIC] HITS@ 1 <C> overall  [ITALIC] HITS@ 3 <C> overall  [ITALIC] HITS@ 10 <C> [ITALIC] MRR overall <C> [ITALIC] MRR no TM <C> [ITALIC] MRR with TM <R> <C> [EMPTY] <C> DistMult <C> 0.0 <C> 18.2 <C> 27.0 <C> 37.9 <C> 24.8 <C> 28.0 <C> 16.2 <R> <C> [EMPTY] <C> full FM <C> 0.0 <C> 20.1 <C> 28.7 <C> 38.9 <C> 26.4 <C> 29.3 <C> 18.3 <R> <C> (*) <C> ( [ITALIC] s, [ITALIC] o) vs.  [ITALIC] r <C> 1.0 <C> 2.1 <C> 3.8 <C> 6.5 <C> 3.5 <C> 0.0 <C> 13.1 <R> <C> (**) <C> ( [ITALIC] r, [ITALIC] o) vs.  [ITALIC] s <C> 0.1 <C> 24.9 <C> 34.8 <C> 45.8 <C> 32.0 <C> 34.7 <C> 24.8 <R> <C> (***) <C> ( [ITALIC] s, [ITALIC] r) vs.  [ITALIC] o <C> 0.0 <C> 9.0 <C> 17.3 <C> 29.9 <C> 15.6 <C> 17.3 <C> 10.9 <R> <C> [EMPTY] <C> (*) + (**) + (***) <C> 0.1 <C> [BOLD] 25.9 <C> [BOLD] 36.2 <C> [BOLD] 47.4 <C> [BOLD] 33.2 <C> [BOLD] 35.0 <C> [BOLD] 28.3 <CAP> Table 1: Test set metrics for different models and varying unit and bigram embeddings on fb15k237, all performance numbers in % and best result in bold. The optimal value for τ is indicated as well.
<R> <C> [BOLD] Model <C> [BOLD] Comma P <C> [BOLD] Comma R <C> [BOLD] Comma F1 <C> [BOLD] Period P <C> [BOLD] Period R <C> [BOLD] Period F1 <C> [BOLD] Question P <C> [BOLD] Question R <C> [BOLD] Question F1 <C> [BOLD] Overall P <C> [BOLD] Overall R <C> [BOLD] Overall F1 <R> <C> T-LSTM  <C> 49.6 <C> 41.4 <C> 45.1 <C> 60.2 <C> 53.4 <C> 56.6 <C> 57.1 <C> 43.5 <C> 49.4 <C> 55.0 <C> 47.2 <C> 50.8 <R> <C> T-BRNN-pre  <C> 65.5 <C> 47.1 <C> 54.8 <C> 73.3 <C> 72.5 <C> 72.9 <C> 70.7 <C> 63.0 <C> 66.7 <C> 70.0 <C> 59.7 <C> 64.4 <R> <C> BLSTM-CRF  <C> 58.9 <C> 59.1 <C> 59.0 <C> 68.9 <C> 72.1 <C> 70.5 <C> 71.8 <C> 60.6 <C> 65.7 <C> 66.5 <C> 63.9 <C> 65.1 <R> <C> Teacher-Ensemble  <C> 66.2 <C> 59.9 <C> 62.9 <C> 75.1 <C> 73.7 <C> 74.4 <C> 72.3 <C> 63.8 <C> 67.8 <C> 71.2 <C> 65.8 <C> 68.4 <R> <C> Self-attention-word-speech  <C> 67.4 <C> 61.1 <C> 64.1 <C> 82.5 <C> 77.4 <C> 79.9 <C> 80.1 <C> 70.2 <C> 74.8 <C> 76.7 <C> 69.6 <C> 72.9 <R> <C> BLSTM w/o Pretrain <C> 53.1 <C> 48.3 <C> 50.6 <C> 66.9 <C> 70.0 <C> 68.4 <C> 70.0 <C> 45.7 <C> 55.3 <C> 60.6 <C> 58.6 <C> 59.6 <R> <C> Full-Transformer w/o Pretrain <C> 56.8 <C> 56.0 <C> 56.4 <C> 68.5 <C> 75.6 <C> 71.9 <C> 59.6 <C> 67.4 <C> 63.3 <C> 62.8 <C> 65.7 <C> 64.2 <R> <C> CT-Transformer w/o Pretrain <C> 53.3 <C> 61.8 <C> 57.2 <C> 76.2 <C> 64.3 <C> 69.7 <C> 67.5 <C> 58.7 <C> 62.8 <C> 62.9 <C> 62.9 <C> 62.9 <R> <C> BLSTM <C> 64.4 <C> 60.2 <C> 62.3 <C> 73.7 <C> 83.4 <C> 78.2 <C> 71.7 <C> 71.7 <C> 71.7 <C> 69.5 <C> 71.6 <C> 70.6 <R> <C> Full-Transformer <C> 68.8 <C> 67.3 <C> 68.1 <C> 73.9 <C> 85.5 <C> 79.3 <C> 66.7 <C> 78.3 <C> 72.0 <C> 71.4 <C> 76.3 <C> 73.8 <R> <C> CT-Transformer <C> 68.8 <C> 69.8 <C> 69.3 <C> 78.4 <C> 82.1 <C> 80.2 <C> 76.0 <C> 82.6 <C> 79.2 <C> 73.7 <C> 76.0 <C> 74.9 <CAP> Table 2: The results of punctuation prediction in terms of P(%) ,R(%) , F1(%) on the IWSLT2011 test set.
<R> <C> Metric <C> Open-Domain CoNLL03 <C> Open-Domain Tweet <C> Biomedical Domains BC5CDR <C> Biomedical Domains NCBI-Disease <R> <C> Entity Types <C> 4 <C> 10 <C> 2 <C> 1 <R> <C> F-1 <C> 59.61 <C> 35.83 <C> 71.98 <C> 69.32 <R> <C> Precision <C> 71.91 <C> 40.34 <C> 93.93 <C> 90.59 <R> <C> Recall <C> 50.90 <C> 32.22 <C> 58.35 <C> 56.15 <CAP> Table 1: Existing Gazetteer Matching Performance on Open-Domain (Sang and De Meulder, 2003; Strauss et al., 2016) and Biomedical Domain NER Datasets (Shang et al., 2018).
<R> <C> [BOLD] Corpus <C> [BOLD] SJ-W2V <C> [BOLD] SED-W2V <C> [ITALIC]  [BOLD] Combined <C> [BOLD] SOA <R> <C> P4PIN <C> 0.90 <C> 0.91 <C> [BOLD] 0.93 <C> 0.92  <R> <C> MSRP <C> 0.81 <C> 0.81 <C> [BOLD] 0.83 <C> 0.85  <CAP> Table 4: F1 results from the combination of the semantically-informed similarity and distance measures. The SOA column indicates the state-of-the-art performance reported for each dataset.
<R> <C> [EMPTY] <C> [ITALIC] FM1 <C> [ITALIC] FF1 <C> [ITALIC] FF1 [ITALIC] FM1 <C> [ITALIC] F1 <R> <C> Clark+:2015† <C> 53.9 <C> 52.8 <C> 0.98 <C> 53.3 <R> <C> Lee+:2017† <C> 67.7 <C> 60.0 <C> 0.89 <C> 64.0 <R> <C> Lee+:2017, re-trained <C> 67.8 <C> 66.3 <C> 0.98 <C> 67.0 <R> <C> Parallelism† <C> 69.4 <C> 64.4 <C> 0.93 <C> 66.9 <R> <C> Parallelism+URL† <C> 72.3 <C> 68.8 <C> 0.95 <C> 70.6 <R> <C> RefReader, LM objective‡ <C> 61.6 <C> 60.5 <C> 0.98 <C> 61.1 <R> <C> RefReader, coref objective‡ <C> 69.6 <C> 68.1 <C> 0.98 <C> 68.9 <R> <C> RefReader, LM + coref‡ <C> [BOLD] 72.8 <C> [BOLD] 71.4 <C> [BOLD] 0.98 <C> [BOLD] 72.1 <R> <C> RefReader, coref + BERT⋆ <C> [BOLD] 80.3 <C> [BOLD] 77.4 <C> [BOLD] 0.96 <C> [BOLD] 78.8 <CAP> Table 1: GAP test set performance. †: reported in Webster+:2018; ‡: strictly incremental processing; ⋆: average over 5 runs with different random seeds.
<R> <C> Task <C> Metrics <C> Bert dev <C> Bert test <C> ERNIE dev <C> ERNIE test <R> <C> XNLI <C> accuracy <C> 78.1 <C> 77.2 <C> 79.9 (+1.8) <C> 78.4 (+1.2) <R> <C> LCQMC <C> accuracy <C> 88.8 <C> 87.0 <C> 89.7 (+0.9) <C> 87.4 (+0.4) <R> <C> MSRA-NER <C> F1 <C> 94.0 <C> 92.6 <C> 95.0 (+1.0) <C> 93.8 (+1.2) <R> <C> ChnSentiCorp <C> accuracy <C> 94.6 <C> 94.3 <C> 95.2 (+0.6) <C> 95.4 (+1.1) <R> <C> nlpcc-dbqa <C> mrr <C> 94.7 <C> 94.6 <C> 95.0 (+0.3) <C> 95.1 (+0.5) <R> <C> nlpcc-dbqa <C> F1 <C> 80.7 <C> 80.8 <C> 82.3 (+1.6) <C> 82.7 (+1.9) <CAP> Table 1: Results on 5 major Chinese NLP tasks
<R> <C> Model <C> BLEU <C> METEOR <C> ROUGE_L <C> CIDEr <C> SPICE <R> <C> +speaker <C> 15.1 <C> 20.6 <C> 22.2 <C> 1.4 <C> 20.7 <R> <C> +text_attn <C> 23.8 <C> 23.3 <C> 29.6 <C> 10.0 <C> 24.6 <R> <C> +style <C> [BOLD] 30.6 <C> [BOLD] 28.8 <C> [BOLD] 39.7 <C> [BOLD] 27.8 <C> [BOLD] 30.6 <CAP> Table 2: Quantitative results that evaluate the quality of the instructions generated by the the original Speaker and the MTST model.
<R> <C> [BOLD] Dataset <C> [BOLD] #data <C> [BOLD] #pano <C> [BOLD] instr_len <C> [BOLD] #sent <C> [BOLD] #turn <C> [BOLD] #covered <R> <C> Touchdown <C> 6k <C> 35.2 <C> 80.5 <C> 6.3 <C> 2.8 <C> 26k <R> <C> Manh-50 <C> 31k <C> 37.2 <C> 22.1 <C> 2.8 <C> 4.1 <C> 43k <R> <C> StreetLearn <C> 580k <C> 129.0 <C> 28.6 <C> 4.0 <C> 13.2 <C> 114k <CAP> Table 4: Statistical information of the datasets used in pre-training and finetuning. #data refers to the total lines of samples in the training set, #pano indicates how many panoramas each trajectory contains on average, instr_len is the average length of the instructions, #sent denotes how many sentences each instruction contains, #turn points to the average number of panoramas that stand as intersections in each trajectory, #covered is the total number of panoramas that are covered by the training set.
<R> <C> GW <C> it-es  [BOLD] 92.63 <C> it-fr 91.78 <C> it-pt 89.47 <C> it-en 80.38 <C> it-de 74.03 <C> es-it 89.35 <C> es-fr 91.78 <C> es-pt 92.82 <C> es-en 81.52 <C> es-de 75.03 <C> [EMPTY] <R> <C> GW∗ <C> - <C> - <C> - <C> 75.2 <C> - <C> - <C> - <C> - <C> 80.4 <C> - <C> [EMPTY] <R> <C> PA <C> 87.3 <C> 87.1 <C> 81.0 <C> 76.9 <C> 67.5 <C> 83.5 <C> 85.8 <C> 87.3 <C> 82.9 <C> 68.3 <C> [EMPTY] <R> <C> MAT+MPPA <C> 87.5 <C> 87.7 <C> 81.2 <C> 77.7 <C> 67.1 <C> 83.7 <C> 85.9 <C> 86.8 <C> 83.5 <C> 66.5 <C> [EMPTY] <R> <C> MAT+MPSR <C> 88.2 <C> 88.1 <C> 82.3 <C> 77.4 <C> 69.5 <C> 84.5 <C> 86.9 <C> 87.8 <C> 83.7 <C> 69.0 <C> [EMPTY] <R> <C> UMH <C> 87.0 <C> 86.7 <C> 80.4 <C> 79.9 <C> 67.5 <C> 83.3 <C> 85.1 <C> 86.3 <C> [BOLD] 85.3 <C> 68.7 <C> [EMPTY] <R> <C> BA <C> 92.32 <C> [BOLD] 92.54 <C> [BOLD] 90.14 <C> [BOLD] 81.84 <C> [BOLD] 75.65 <C> [BOLD] 89.38 <C> [BOLD] 92.19 <C> [BOLD] 92.85 <C> 83.5 <C> [BOLD] 78.25 <C> [EMPTY] <R> <C> [EMPTY] <C> fr-it <C> fr-es <C> fr-pt <C> fr-en <C> fr-de <C> pt-it <C> pt-es <C> pt-fr <C> pt-en <C> pt-de <C> [EMPTY] <R> <C> GW <C> 88.0 <C> 90.3 <C> 87.44 <C> 82.2 <C> 74.18 <C> 90.62 <C> [BOLD] 96.19 <C> 89.9 <C> 81.14 <C> 74.83 <C> [EMPTY] <R> <C> GW∗ <C> - <C> - <C> - <C> 82.1 <C> - <C> - <C> - <C> - <C> - <C> - <C> [EMPTY] <R> <C> PA <C> 83.2 <C> 82.6 <C> 78.1 <C> 82.4 <C> 69.5 <C> 81.1 <C> 91.5 <C> 84.3 <C> 80.3 <C> 63.7 <C> [EMPTY] <R> <C> MAT+MPPA <C> 83.1 <C> 83.6 <C> 78.7 <C> 82.2 <C> 69.0 <C> 82.6 <C> 92.2 <C> 84.6 <C> 80.2 <C> 63.7 <C> [EMPTY] <R> <C> MAT+MPSR <C> 83.5 <C> 83.9 <C> 79.3 <C> 81.8 <C> 71.2 <C> 82.6 <C> 92.7 <C> 86.3 <C> 79.9 <C> 65.7 <C> [EMPTY] <R> <C> UMH <C> 82.5 <C> 82.7 <C> 77.5 <C> 83.1 <C> 69.8 <C> 81.1 <C> 91.7 <C> 83.6 <C> 82.1 <C> 64.4 <C> [EMPTY] <R> <C> BA <C> [BOLD] 88.38 <C> [BOLD] 90.77 <C> [BOLD] 88.22 <C> [BOLD] 83.23 <C> [BOLD] 76.63 <C> [BOLD] 91.08 <C> 96.04 <C> [BOLD] 91.04 <C> [BOLD] 82.91 <C> [BOLD] 76.99 <C> [EMPTY] <R> <C> [EMPTY] <C> en-it <C> en-es <C> en-fr <C> en-pt <C> en-de <C> de-it <C> de-es <C> de-fr <C> de-pt <C> de-en <C> [BOLD] Average <R> <C> GW <C> 80.84 <C> 82.35 <C> 81.67 <C> 83.03 <C> 71.73 <C> 75.41 <C> 72.18 <C> 77.14 <C> 74.38 <C> 72.85 <C> 82.84 <R> <C> GW∗ <C> 78.9 <C> 81.7 <C> 81.3 <C> - <C> 71.9 <C> - <C> - <C> - <C> - <C> 72.8 <C> 78.04 <R> <C> PA <C> 77.3 <C> 81.4 <C> 81.1 <C> 79.9 <C> 73.5 <C> 69.5 <C> 67.7 <C> 73.3 <C> 59.1 <C> 72.4 <C> 77.98 <R> <C> MAT+MPPA <C> 78.5 <C> 82.2 <C> 82.7 <C> 81.3 <C> 74.5 <C> 70.1 <C> 68.0 <C> 75.2 <C> 61.1 <C> 72.9 <C> 78.47 <R> <C> MAT+MPSR <C> 78.8 <C> 82.5 <C> 82.4 <C> 81.5 <C> 74.8 <C> 72.0 <C> 69.6 <C> 76.7 <C> 63.2 <C> 72.9 <C> 79.29 <R> <C> UMH <C> 78.9 <C> 82.5 <C> 82.7 <C> 82.0 <C> [BOLD] 75.1 <C> 68.7 <C> 67.2 <C> 73.5 <C> 59.0 <C> 75.5 <C> 78.46 <R> <C> BA <C> [BOLD] 81.45 <C> [BOLD] 84.26 <C> [BOLD] 82.94 <C> [BOLD] 84.65 <C> 74.08 <C> [BOLD] 78.09 <C> [BOLD] 75.93 <C> [BOLD] 78.93 <C> [BOLD] 77.18 <C> [BOLD] 75.85 <C> [BOLD] 84.24 <CAP> Table 2: Pairs of languages in multilingual alignment problem results for English, German, French, Spanish, Italian, and Portuguese. All reported results are precision@1 percentage. The method achieving the highest precision for each bilingual pair is highlighted in bold. Methods we are comparing to in the table are: Procrustes Matching with CSLS metric to infer translation pairs (PA) [Lample et al.2018]; Gromov-Wasserstein alignment (GW) [Alvarez-Melis and Jaakkola2018] (reproduced by us using their source code); GW∗ refers to the results reported by AlvarezMelisJaakkola18 in the paper; bilingual alignment with multilingual auxiliary information (MPPA) [Taitelbaum et al.2019b]; Multilingual pseudo-supervised refinement method [Chen and Cardie2018]; multilingual alignment method (UMH) [Alaux et al.2019]
<R> <C> [EMPTY] <C> GW benchmark P@1 <C> GW benchmark P@10 <C> unweighted P@1 <C> unweighted P@10 <C> hierarchical P@1 <C> hierarchical P@10 <C> weighted P@1 <C> weighted P@10 <R> <C> it-es <C> [BOLD] 92.63 <C> 98.05 <C> 91.52 <C> 97.95 <C> 92.49 <C> [BOLD] 98.11 <C> 92.32 <C> 98.01 <R> <C> it-fr <C> 91.78 <C> 98.11 <C> 91.27 <C> 97.89 <C> [BOLD] 92.61 <C> [BOLD] 98.14 <C> 92.54 <C> [BOLD] 98.14 <R> <C> it-pt <C> 89.47 <C> 97.35 <C> 88.22 <C> 97.25 <C> 89.89 <C> [BOLD] 97.87 <C> [BOLD] 90.14 <C> 97.84 <R> <C> it-en <C> 80.38 <C> 93.3 <C> 79.23 <C> 93.18 <C> 79.54 <C> 93.21 <C> [BOLD] 81.84 <C> [BOLD] 93.77 <R> <C> it-de <C> 74.03 <C> 93.66 <C> 74.41 <C> 92.96 <C> 73.06 <C> 92.26 <C> [BOLD] 75.65 <C> [BOLD] 93.82 <R> <C> es-it <C> 89.35 <C> 97.3 <C> 88.8 <C> 97.05 <C> [BOLD] 89.73 <C> [BOLD] 97.5 <C> 89.38 <C> 97.43 <R> <C> es-fr <C> 91.78 <C> 98.21 <C> 91.34 <C> 98.03 <C> 91.74 <C> 98.29 <C> [BOLD] 92.19 <C> [BOLD] 98.33 <R> <C> es-pt <C> 92.82 <C> 98.32 <C> 91.83 <C> 98.18 <C> 92.65 <C> [BOLD] 98.35 <C> [BOLD] 92.85 <C> [BOLD] 98.35 <R> <C> es-en <C> 81.52 <C> 94.79 <C> 82.43 <C> 94.63 <C> 81.63 <C> 94.27 <C> [BOLD] 83.5 <C> [BOLD] 95.48 <R> <C> es-de <C> 75.03 <C> 93.98 <C> 76.47 <C> 93.73 <C> 74.86 <C> 93.73 <C> [BOLD] 78.25 <C> [BOLD] 94.74 <R> <C> fr-it <C> 88.0 <C> 97.5 <C> 87.55 <C> 97.19 <C> 88.35 <C> 97.64 <C> [BOLD] 88.38 <C> [BOLD] 97.71 <R> <C> fr-es <C> 90.3 <C> 97.97 <C> 90.18 <C> 97.68 <C> 90.66 <C> [BOLD] 98.04 <C> [BOLD] 90.77 <C> [BOLD] 98.04 <R> <C> fr-pt <C> 87.44 <C> 96.89 <C> 86.7 <C> 96.79 <C> [BOLD] 88.35 <C> [BOLD] 97.11 <C> 88.22 <C> 97.08 <R> <C> fr-en <C> 82.2 <C> 94.19 <C> 81.26 <C> 94.25 <C> 80.89 <C> 94.13 <C> [BOLD] 83.23 <C> [BOLD] 94.42 <R> <C> fr-de <C> 74.18 <C> 92.94 <C> 74.07 <C> 92.73 <C> 74.44 <C> 92.68 <C> [BOLD] 76.63 <C> [BOLD] 93.41 <R> <C> pt-it <C> 90.62 <C> 97.61 <C> 89.36 <C> 97.75 <C> 90.59 <C> [BOLD] 98.17 <C> [BOLD] 91.08 <C> 97.96 <R> <C> pt-es <C> [BOLD] 96.19 <C> 99.29 <C> 95.36 <C> 99.08 <C> 96.04 <C> 99.23 <C> 96.04 <C> [BOLD] 99.32 <R> <C> pt-fr <C> 89.9 <C> 97.57 <C> 90.1 <C> 97.43 <C> 90.67 <C> 97.74 <C> [BOLD] 91.04 <C> [BOLD] 97.87 <R> <C> pt-en <C> 81.14 <C> 94.17 <C> 81.42 <C> 94.14 <C> 81.42 <C> 93.86 <C> [BOLD] 82.91 <C> [BOLD] 94.64 <R> <C> pt-de <C> 74.83 <C> 93.76 <C> 75.94 <C> 93.21 <C> 74.45 <C> 93.1 <C> [BOLD] 76.99 <C> [BOLD] 94.32 <R> <C> en-it <C> 80.84 <C> 93.97 <C> 79.88 <C> 93.93 <C> 80.25 <C> 93.76 <C> [BOLD] 81.45 <C> [BOLD] 94.58 <R> <C> en-es <C> 82.35 <C> 94.67 <C> 83.05 <C> 94.79 <C> 81.62 <C> 94.82 <C> [BOLD] 84.26 <C> [BOLD] 95.28 <R> <C> en-fr <C> 81.67 <C> 94.24 <C> 81.86 <C> 94.33 <C> 81.42 <C> 93.99 <C> [BOLD] 82.94 <C> [BOLD] 94.67 <R> <C> en-pt <C> 83.03 <C> 94.45 <C> 82.72 <C> 94.64 <C> 82.25 <C> 94.79 <C> [BOLD] 84.65 <C> [BOLD] 95.29 <R> <C> en-de <C> 71.73 <C> 90.48 <C> 72.92 <C> 90.76 <C> 71.88 <C> 90.42 <C> [BOLD] 74.08 <C> [BOLD] 91.46 <R> <C> de-it <C> 75.41 <C> 94.3 <C> 76.4 <C> 93.87 <C> 75.19 <C> 93.65 <C> [BOLD] 78.09 <C> [BOLD] 94.52 <R> <C> de-es <C> 72.18 <C> 92.64 <C> 74.21 <C> 92.6 <C> 73.58 <C> 92.48 <C> [BOLD] 75.93 <C> [BOLD] 93.83 <R> <C> de-fr <C> 77.14 <C> 93.29 <C> 77.93 <C> 93.61 <C> 77.14 <C> 93.51 <C> [BOLD] 78.93 <C> [BOLD] 93.77 <R> <C> de-pt <C> 74.38 <C> 93.71 <C> 74.99 <C> 93.54 <C> 74.22 <C> 93.81 <C> [BOLD] 77.18 <C> [BOLD] 94.14 <R> <C> de-en <C> 72.85 <C> 91.06 <C> 74.36 <C> 91.21 <C> 72.17 <C> 90.81 <C> [BOLD] 75.85 <C> [BOLD] 91.98 <R> <C> average <C> 82.84 <C> 95.26 <C> 82.86 <C> 95.15 <C> 82.79 <C> 95.18 <C> 84.24 <C> 95.67 <CAP> Table 4: Accuracy results for translation pairs between all pairs of languages for all evaluated methods. The column GW-benchmark contains results from Gromov-Wasserstein direct bilingual alignment. Unweighted is the barycenter approach without optimizing on support location weights. Hierarchical contains results from traversing through edges and infer translation mapping through hierarchical barycenters. The weighted column is what Algorithm 1 returns, optimizing both on support locations and weights on the support.
<R> <C> [EMPTY] <C> Hits@1 <C> Hits@5 <C> Hits@10 <C> Hits@20 <R> <C> Most-Common Value <C> 38.81 <C> 77.26 <C> 87.96 <C> 95.96 <R> <C> Image Baseline <C> 38.07 <C> 76.11 <C> 86.99 <C> 95.00 <R> <C> Text Baseline <C> 58.41 <C> 87.49 <C> 93.94 <C> 98.00 <R> <C> Multimodal Baseline - Concat <C> 59.48 <C> 87.33 <C> 93.23 <C> 97.07 <R> <C> Multimodal Baseline - GMU <C> 52.92 <C> 85.07 <C> 92.23 <C> 97.26 <CAP> Table 2: Baseline model results.
<R> <C> [BOLD] Method <C> [BOLD] Test Scores <R> <C> IR-ARC <C> 20.26 <R> <C> IR-Google <C> 21.58 <R> <C> TupleInference <C> 23.83 <R> <C> DecompAttn <C> 24.34 <R> <C> Guess-all / Random <C> 25.02 <R> <C> DGEM-OpenIE <C> 26.41 <R> <C> BiDAF <C> 26.54 <R> <C> TableILP <C> 26.97 <R> <C> KG2 <C> [BOLD] 31.70 <CAP> Table 1: Test performance of different QA systems on the ARC Challenge Set. The ARC Corpus is used in DecompAttn, DGEM, BiDAF and KG2.
<R> <C> System <C> NMT decoding and SMT technical term translation pairwise evaluation <C> NMT decoding and SMT technical term translation JPO adequacy evaluation <C> NMT rescoring of 1,000-best SMT translations pairwise evaluation <C> NMT rescoring of 1,000-best SMT translations JPO adequacy evaluation <R> <C> Baseline SMT  <C> - <C> 3.5 <C> - <C> - <R> <C> Baseline NMT <C> 5.0 <C> 3.8 <C> 28.5 <C> 4.1 <R> <C> NMT with technical term translation by SMT <C> [BOLD] 36.5 <C> [BOLD] 4.3 <C> 31.0 <C> 4.1 <CAP> Table 2: Human evaluation results (the score of pairwise evaluation ranges from −100 to 100 and the score of JPO adequacy evaluation ranges from 1 to 5)
<R> <C> Model <C> Accept <C> Counteroffer <C> Offer <C> Other <C> Refusal <C> Accuracy <C> Macro-F1 <R> <C> CNN + LSTM <C> [BOLD] 65.64 <C> 51.06 <C> 77.54 <C> 93.57 <C> 84.33 <C> 86.43 <C> 74.43 <R> <C> CNN + CNN <C> 65.42 <C> 47.89 <C> 75.81 <C> 92.00 <C> 83.66 <C> 85.32 <C> 73.10 <R> <C> BiLSTM + no utterance context <C> 46.83 <C> 41.24 <C> 71.64 <C> 89.01 <C> 76.28 <C> 79.49 <C> 65.00 <R> <C> BiLSTM + LSTM <C> 63.93 <C> 50.17 <C> 79.03 <C> 94.12 <C> 85.59 <C> 86.83 <C> 74.57 <R> <C> BiLSTM + DAG-LSTM <C> 64.29 <C> [BOLD] 51.69 <C> [BOLD] 81.97 <C> [BOLD] 94.42 <C> [BOLD] 86.54 <C> [BOLD] 87.69 <C> [BOLD] 75.78 <CAP> Table 2. F1 scores of various classes for different models.
<R> <C> [BOLD] Question Type <C> [BOLD] Systems <C> [BOLD] Doc List <C> [BOLD] NDCG’ <R> <C> Answerable <C> System A <C> 111 <C> 1.000 <R> <C> Answerable <C> System B <C> 11100 <C> 0.971 <R> <C> Answerable <C> System C <C> 11 <C> 0.922 <R> <C> Unanswerable <C> System A <C> ∅ <C> 1.000 <R> <C> Unanswerable <C> System B <C> 00 <C> 0.500 <R> <C> Unanswerable <C> System C <C> 000 <C> 0.431 <CAP> Table 4: NDCG’ examples.
<R> <C> Method <C> [BOLD] Twitter15 F1 <C> [BOLD] Twitter15 Rec <C> [BOLD] Twitter15 Pre <C> [BOLD] Twitter15 Acc <C> [BOLD] Twitter16 F1 <C> [BOLD] Twitter16 Rec <C> [BOLD] Twitter16 Pre <C> [BOLD] Twitter16 Acc <R> <C> DTC <C> 0.4948 <C> 0.4806 <C> 0.4963 <C> 0.4949 <C> 0.5616 <C> 0.5369 <C> 0.5753 <C> 0.5612 <R> <C> SVM-TS <C> 0.5190 <C> 0.5186 <C> 0.5195 <C> 0.5195 <C> 0.6915 <C> 0.6910 <C> 0.6928 <C> 0.6932 <R> <C> mGRU <C> 0.5104 <C> 0.5148 <C> 0.5145 <C> 0.5547 <C> 0.5563 <C> 0.5618 <C> 0.5603 <C> 0.6612 <R> <C> RFC <C> 0.4642 <C> 0.5302 <C> 0.5718 <C> 0.5385 <C> 0.6275 <C> 0.6587 <C> 0.7315 <C> 0.6620 <R> <C> tCNN <C> 0.5140 <C> 0.5206 <C> 0.5199 <C> 0.5881 <C> 0.6200 <C> 0.6262 <C> 0.6248 <C> 0.7374 <R> <C> CRNN <C> 0.5249 <C> 0.5305 <C> 0.5296 <C> 0.5919 <C> 0.6367 <C> 0.6433 <C> 0.6419 <C> 0.7576 <R> <C> CSI <C> 0.7174 <C> 0.6867 <C> 0.6991 <C> 0.6987 <C> 0.6304 <C> 0.6309 <C> 0.6321 <C> 0.6612 <R> <C> dEFEND <C> 0.6541 <C> 0.6611 <C> 0.6584 <C> 0.7383 <C> 0.6311 <C> 0.6384 <C> 0.6365 <C> 0.7016 <R> <C> [BOLD] GCAN-G <C> 0.7938 <C> 0.7990 <C> 0.7959 <C> 0.8636 <C> 0.6754 <C> 0.6802 <C> 0.6785 <C> 0.7939 <R> <C> [BOLD] GCAN <C> [BOLD] 0.8250 <C> [BOLD] 0.8295 <C> [BOLD] 0.8257 <C> [BOLD] 0.8767 <C> [BOLD] 0.7593 <C> [BOLD] 0.7632 <C> [BOLD] 0.7594 <C> [BOLD] 0.9084 <R> <C> [BOLD] Improvement <C> [BOLD] 15.0% <C> [BOLD] 20.8% <C> [BOLD] 18.1% <C> [BOLD] 18.7% <C> [BOLD] 19.3% <C> [BOLD] 15.9% <C> [BOLD] 3.8% <C> [BOLD] 19.9% <CAP> Table 3: Main results. The best model and the best competitor are highlighted by bold and underline, respectively.
<R> <C> [EMPTY] <C> [ITALIC] Test CORPS <C> [ITALIC] Test CORPS <C> [ITALIC] Test CORPS <C> [ITALIC] Test Twitter <C> [ITALIC] Test Twitter <C> [ITALIC] Test Twitter <C> [ITALIC] Test Slogan <C> [ITALIC] Test Slogan <C> [ITALIC] Test Slogan <C> [ITALIC] Test Movie <C> [ITALIC] Test Movie <C> [ITALIC] Test Movie <R> <C> [ITALIC] Training <C> [ITALIC] Phonetic <C> [ITALIC] N-Gram <C> [ITALIC] All <C> [ITALIC] Phonetic <C> [ITALIC] N-Gram <C> [ITALIC] All <C> [ITALIC] Phonetic <C> [ITALIC] N-Gram <C> [ITALIC] All <C> [ITALIC] Phonetic <C> [ITALIC] N-Gram <C> [ITALIC] All <R> <C> CORPS <C> - <C> - <C> - <C> 0.463 <C> 0.508 <C> 0.523 <C> 0.508 <C> 0.517 <C> 0.539 <C> 0.411 <C> 0.506 <C> 0.516 <R> <C> Twitter <C> 0.439 <C> 0.494 <C> 0.462 <C> - <C> - <C> - <C> 0.564 <C> 0.531 <C> 0.637 <C> 0.596 <C> 0.544 <C> 0.589 <R> <C> Slogan <C> 0.535 <C> 0.512 <C> 0.514 <C> 0.535 <C> 0.510 <C> 0.539 <C> - <C> - <C> - <C> 0.532 <C> 0.545 <C> 0.588 <R> <C> Movie <C> 0.431 <C> 0.513 <C> 0.498 <C> 0.562 <C> 0.533 <C> 0.560 <C> 0.581 <C> 0.537 <C> 0.589 <C> - <C> - <C> - <CAP> Table 7: Results of the cross-dataset prediction experiments optimized on the training set.
<R> <C> [BOLD] Task <C> [BOLD] Models <C> [BOLD] en <C> [BOLD] es <C> [BOLD] fr <C> [BOLD] el <C> [BOLD] ru <C> [BOLD] avg <R> <C> MLM <C> mBERT <C> 10.7 <C> [BOLD] 3.51 <C> 8.63 <C> [BOLD] 2.08 <C> [BOLD] 2.70 <C> 5.52 <R> <C> MLM <C> MTF w/ MLM (en) <C> 9.50 <C> 5.10 <C> 8.62 <C> 2.56 <C> 3.47 <C> 5.85 <R> <C> MLM <C> MTF w/ MLM (all) <C> [BOLD] 9.33 <C> 4.19 <C> [BOLD] 4.89 <C> 2.34 <C> 3.04 <C> [BOLD] 4.76 <R> <C> MLM <C> GEM w/ MLM (en) <C> 13.0 <C> 6.62 <C> 11.4 <C> 2.87 <C> 4.22 <C> 7.62 <R> <C> MLM <C> GEM w/ MLM (all) <C> 11.8 <C> 4.18 <C> 6.83 <C> 2.29 <C> 2.99 <C> 5.62 <R> <C> POS <C> Naive Fine-tune <C> 96.2 <C> 82.9 <C> 89.1 <C> 84.2 <C> 85.5 <C> 85.4 <R> <C> POS <C> MTF w/ MLM (en) <C> 94.5 <C> 83.0 <C> 88.1 <C> 84.5 <C> 80.5 <C> 84.0 <R> <C> POS <C> MTF w/ MLM (all) <C> 94.7 <C> 77.5 <C> 83.3 <C> 81.9 <C> 77.0 <C> 79.9 <R> <C> POS <C> GEM w/ MLM (en) <C> 97.4 <C> [BOLD] 84.7 <C> [BOLD] 89.7 <C> [BOLD] 86.0 <C> 86.9 <C> [BOLD] 86.8 <R> <C> POS <C> GEM w/ MLM (all) <C> 97.2 <C> 83.9 <C> 89.2 <C> 85.9 <C> [BOLD] 87.1 <C> 86.5 <CAP> Table 5: Ablation study on the two settings when leveraging the MLM task in the fine-tuning process. One is only utilizing the Wikipedia corpus in English (MLM (en)), the other is to utilize the Wikipedia corpus that contains all languages in the POS task (i.e., including en, es, fr, el, and ru) (MLM (all)).
<R> <C> learning rate <C> Randomly Init dev <C> Randomly Init test <C> Speech-XLNet Pretrained dev <C> Speech-XLNet Pretrained test <R> <C> 3e-5 <C> 16.7 <C> 18.4 <C> 11.8 <C> 12.9 (29.9%) <R> <C> 1e-4 <C> 14.7 <C> 16.8 <C> 11.8 <C> [BOLD] 12.8 (23.8%) <R> <C> 2e-4 <C> 14.5 <C> 16.1 <C> 12.0 <C> 13.0 (19.3%) <R> <C> 1e-3 <C> 13.2 <C> [BOLD] 15.1 <C> 12.0 <C> 13.1 (13.2%) <R> <C> 2e-3 <C> diverge <C> diverge <C> 13.1 <C> 14.1 (∞) <CAP> Table 1: PER(%) comparison of different learning rates.
<R> <C> [BOLD] Models <C> [BOLD] ZH-EN  [BOLD] Hits@1 <C> [BOLD] ZH-EN  [BOLD] Hits@10 <C> [BOLD] JA-EN  [BOLD] Hits@1 <C> [BOLD] JA-EN  [BOLD] Hits@10 <C> [BOLD] FR-EN  [BOLD] Hits@1 <C> [BOLD] FR-EN  [BOLD] Hits@10 <R> <C> BootEA <C> 12.2 <C> 27.5 <C> 27.8 <C> 52.6 <C> 32.7 <C> 53.2 <R> <C> GMNN <C> 47.5 <C> 68.3 <C> 58.8 <C> 78.2 <C> 75.0 <C> 90.9 <R> <C> RDGCN <C> 60.7 <C> 74.6 <C> 69.3 <C> 82.9 <C> 83.6 <C> 92.6 <R> <C> [BOLD] NMN <C> [BOLD] 62.0 <C> [BOLD] 75.1 <C> 70.3 <C> 84.4 <C> 86.3 <C> 94.0 <R> <C> w/o nbr-m <C> 52.0 <C> 71.1 <C> 62.1 <C> 82.7 <C> 80.0 <C> 92.0 <R> <C> w/o nbr-s <C> 60.9 <C> 74.1 <C> [BOLD] 70.7 <C> [BOLD] 84.5 <C> [BOLD] 86.5 <C> [BOLD] 94.2 <CAP> Table 4: Performance on S-DBP15K.
<R> <C> [EMPTY] <C> [BOLD] Accuracy (%) Model <C> [BOLD] Accuracy (%) Metu <C> [BOLD] Accuracy (%) Finn 12K <C> [BOLD] Accuracy (%) Finn 24K <R> <C> [EMPTY] <C> HPS  <C> [BOLD] 53.79 <C> [BOLD] 28.19 <C> 27.04 <R> <C> [EMPTY] <C> Morfessor FlatCat  <C> 52.06 <C> 24.47 <C> 25.93 <R> <C> 1 <C> Bayesian S-HMM <C> 46.21 <C> 23.24 <C> 22.32 <R> <C> 1 <C> Bayesian SM-HMM <C> 34.97 <C> 26.69 <C> 26.40 <R> <C> 2 <C> Bayesian S-HMM <C> 46.39 <C> 23.49 <C> 22.14 <R> <C> 2 <C> Bayesian SM-HMM <C> 34.82 <C> 27.95 <C> [BOLD] 27.38 <R> <C> 3 <C> Bayesian S-HMM <C> 46.57 <C> 23.28 <C> 22.03 <R> <C> 3 <C> Bayesian SM-HMM <C> 34.97 <C> 27.45 <C> 26.74 <R> <C> 4 <C> Bayesian S-HMM <C> 46.46 <C> 18.62 <C> 22.57 <R> <C> 4 <C> Bayesian SM-HMM <C> 32.13 <C> 24.78 <C> 24.40 <CAP> Table 3: Stemming results for Turkish and Finnish based on four parameter settings
<R> <C> Model <C> SST-2 <C> MNLI <C> QNLI <C> QQP <C> STS-B <R> <C> BERT-6 <C> 92.25 <C> 81.13 <C> 87.63 <C> 90.35 <C> 88.45 <R> <C> BERT-FT-6 <C> 90.02 <C> 80.85 <C> 87.24 <C> 90.34 <C> 88.16 <R> <C> XLNet-6 <C> 92.20 <C> 83.48 <C> 88.03 <C> 90.62 <C> 87.45 <R> <C> XLNet-FT-6 <C> 92.43 <C> 83.75 <C> 86.80 <C> 90.77 <C> 87.60 <CAP> Table 5: Task-specific dropping. XLNet-FT-6 first fine-tunes the pre-trained model by freezing the layers to drop, and then it removes the layers and performs fine-tuning again.
<R> <C> Methods <C> √ <C> × <C> [EMPTY] <C> [ITALIC] Ap <C> [ITALIC] Ao <R> <C> Random Guess <C> 137 <C> 136 <C> 0 <C> 50.2% <C> 50.2% <R> <C> Knowledge Hunting (Emami et al.,  2018 ) <C> 119 <C> 79 <C> 75 <C> 60.1% <C> 57.3% <R> <C> LM (single) (Trinh and Le,  2018 ) <C> 149 <C> 124 <C> 0 <C> 54.5% <C> 54.5% <R> <C> LM (Ensemble) (Trinh and Le,  2018 ) <C> 168 <C> 105 <C> 0 <C> 61.5% <C> 61.5% <R> <C> SP (human) (Zhang et al.,  2019 ) <C> 15 <C> 0 <C> 258 <C> [BOLD] 100% <C> 52.7% <R> <C> SP (PP) (Zhang et al.,  2019 ) <C> 50 <C> 26 <C> 197 <C> 65.8% <C> 54.4% <R> <C> GPT-2 (Radford et al.,  2019 ) <C> 193 <C> 80 <C> 0 <C> 70.7% <C> 70.7% <R> <C> BERT (Kocijan et al.,  2019 ) <C> 169 <C> 104 <C> 0 <C> 61.9% <C> 61.9% <R> <C> BERT+WSCR (Kocijan et al.,  2019 ) <C> 195 <C> 78 <C> 0 <C> 71.4% <C> 71.4% <R> <C> ASER (inference) <C> 63 <C> 27 <C> 183 <C> 70.0% <C> 56.6% <R> <C> BERT+ASER <C> 177 <C> 96 <C> 0 <C> 64.5% <C> 64.5% <R> <C> BERT+WSCR+ASER <C> 198 <C> 75 <C> 0 <C> 72.5% <C> [BOLD] 72.5% <CAP> Table 8. Experimental results on Winograd Schema Challenge. √ indicates the number of correct answers, × indicates the number of wrong answers, and NA means that the model cannot give a prediction. Ap means the prediction accuracy without NA examples, and Ao means the overall accuracy.
<R> <C> [BOLD] Dataset <C> [EMPTY] <C> [EMPTY] <C> [BOLD] Class  [BOLD] AD <C> [BOLD] Class  [BOLD] Non-AD <R> <C> ADReSS <C> Train <C> Male <C> 24 <C> 24 <R> <C> [EMPTY] <C> Train <C> Female <C> 30 <C> 30 <R> <C> ADReSS <C> Test <C> Male <C> 11 <C> 11 <R> <C> ADReSS <C> Test <C> Female <C> 13 <C> 13 <R> <C> DementiaBank [becker1994natural] <C> - <C> Male <C> 125 <C> 83 <R> <C> DementiaBank [becker1994natural] <C> - <C> Female <C> 197 <C> 146 <CAP> Table 1: Basic characteristics of the patients in each group in the ADReSS challenge dataset are more balanced in comparison to DementiaBank.
<R> <C> [BOLD] Feature <C> [BOLD] Feature type <C> [ITALIC] μAD <C> [ITALIC] μnon− [ITALIC] AD <C> [BOLD] Correlation <C> [BOLD] Weight <R> <C> Average cosine distance between utterances <C> Semantic <C> 0.91 <C> 0.94 <C> - <C> - <R> <C> Fraction of pairs of utterances below a similarity threshold (0.5) <C> Semantic <C> 0.03 <C> 0.01 <C> - <C> - <R> <C> Average cosine distance between 300-dimensional word2vec [mikolov2013distributed] utterances and picture content units <C> Semantic (content units) <C> 0.46 <C> 0.38 <C> -0.54* <C> -1.01 <R> <C> Distinct content units mentioned: total content units <C> Semantic (content units) <C> 0.27 <C> 0.45 <C> 0.63* <C> 1.78 <R> <C> Distinct action content units mentioned: total content units <C> Semantic (content units) <C> 0.15 <C> 0.30 <C> 0.49* <C> 1.04 <R> <C> Distinct object content units mentioned: total content units <C> Semantic (content units) <C> 0.28 <C> 0.47 <C> 0.59* <C> 1.72 <R> <C> Average cosine distance between 50-dimensional GloVe utterances and picture content units <C> Semantic content units) <C> - <C> - <C> -0.42* <C> -0.03 <R> <C> Average word length (in letters) <C> Lexico-syntactic <C> 3.57 <C> 3.78 <C> 0.45* <C> 1.07 <R> <C> Proportion of pronouns <C> Lexico-syntactic <C> 0.09 <C> 0.06 <C> - <C> - <R> <C> Ratio (pronouns):(pronouns+nouns) <C> Lexico-syntactic <C> 0.35 <C> 0.23 <C> - <C> - <R> <C> Proportion of personal pronouns <C> Lexico-syntactic <C> 0.09 <C> 0.06 <C> - <C> - <R> <C> Proportion of RB adverbs <C> Lexico-syntactic <C> 0.06 <C> 0.04 <C> -0.41* <C> -0.41 <R> <C> Proportion of ADVP_−>_RB amongst all rules <C> Lexico-syntactic <C> 0.02 <C> 0.01 <C> -0.37 <C> -0.74 <R> <C> Proportion of non-dictionary words <C> Lexico-syntactic <C> 0.11 <C> 0.08 <C> - <C> - <R> <C> Proportion of gerund verbs <C> Lexico-syntactic <C> - <C> - <C> 0.37 <C> 1.08 <R> <C> Proportion of words in adverb category <C> Lexico-syntactic <C> - <C> - <C> -0.4* <C> -0.49 <CAP> Table 2: Feature differentiation analysis results for the most important features, based on ADReSS train set. μAD and μnon−AD show the means of the 13 significantly different features at p<9e-5 (after Bonferroni correction) for the AD and non-AD group respectively. We also show Spearman correlation between MMSE score and features, and regression weights of the features associated with the five greatest and five lowest regression weights from our regression experiments. * next to correlation indicates significance at p<9e-5.
<R> <C> [BOLD] Model <C> [BOLD] CR <C> [BOLD] MPQA <C> [BOLD] MR <C> [BOLD] SST <C> [BOLD] SUBJ <R> <C> Unigram-TFIDF <C> 79.2 <C> 82.4 <C> 73.7 <C> - <C> 90.3 <R> <C> word2vec BOW <C> 79.8 <C> [BOLD] 88.3 <C> 77.7 <C> 79.7 <C> 90.9 <R> <C> fastText BOW <C> 78.9 <C> 87.4 <C> 76.5 <C> 78.8 <C> 91.6 <R> <C> CaptionRep BOW <C> 69.3 <C> 70.8 <C> 61.9 <C> - <C> 77.4 <R> <C> DictRep BOW <C> 78.7 <C> 87.2 <C> 76.7 <C> - <C> 90.7 <R> <C> Paragram-Phrase <C> - <C> - <C> - <C> 79.7 <C> - <R> <C> Real-Embed <C> 77.5 <C> 84.7 <C> 77.0 <C> 80.0 <C> 92.0 <R> <C> CE-Sup <C> 80.0 <C> 85.7 <C> 78.4 <C> 82.6 <C> 92.6 <R> <C> CE-Mix <C> [BOLD] 81.1 <C> 86.6 <C> [BOLD] 79.8 <C> [BOLD] 83.3 <C> [BOLD] 92.8 <CAP> Table 2: Experimental Results in percentage(%). The best performed value for each dataset is in bold.
<R> <C> Conv. Speech <C> Broadcast News  WER (%) <R> <C> Baseline feats <C> 50.4 <R> <C> ASpIRE TDNN3 feats <C> [BOLD] 34.7 <CAP> Table 4: Word Error Rates (% WER) on Broadcast News data for different Conversational Speech models
<R> <C> [BOLD] Iterations <C> [BOLD] WMT’14 EN-DE  [BOLD] BLEU <C> [BOLD] WMT’14 EN-DE  [BOLD] Reps <C> [BOLD] WMT’16 EN-RO  [BOLD] BLEU <C> [BOLD] WMT’16 EN-RO  [BOLD] Reps <R> <C> [ITALIC] T=1 <C> 18.05 <C> 16.72% <C> 27.32 <C> 9.34% <R> <C> [ITALIC] T=2 <C> 22.91 <C> 5.40% <C> 31.08 <C> 2.82% <R> <C> [ITALIC] T=3 <C> 24.99 <C> 2.03% <C> 32.19 <C> 1.26% <R> <C> [ITALIC] T=4 <C> 25.94 <C> 1.07% <C> 32.53 <C> 0.87% <R> <C> [ITALIC] T=5 <C> 26.30 <C> 0.72% <C> 32.62 <C> 0.61% <CAP> Table 3: The performance (BLEU) and percentage of repeating tokens when decoding with a different number of mask-predict iterations (T).
<R> <C> [BOLD] Length  [BOLD] Candidates <C> [BOLD] WMT’14 EN-DE  [BOLD] BLEU <C> [BOLD] WMT’14 EN-DE  [BOLD] LP <C> [BOLD] WMT’16 EN-RO  [BOLD] BLEU <C> [BOLD] WMT’16 EN-RO  [BOLD] LP <R> <C> ℓ=1 <C> 26.56 <C> 16.1% <C> 32.75 <C> 13.8% <R> <C> ℓ=2 <C> 27.03 <C> 30.6% <C> 33.06 <C> 26.1% <R> <C> ℓ=3 <C> [BOLD] 27.09 <C> 43.1% <C> [BOLD] 33.11 <C> 39.6% <R> <C> ℓ=4 <C> [BOLD] 27.09 <C> 53.1% <C> 32.13 <C> 49.2% <R> <C> ℓ=5 <C> 27.03 <C> 62.2% <C> 33.08 <C> 57.5% <R> <C> ℓ=6 <C> 26.91 <C> 69.5% <C> 32.91 <C> 64.3% <R> <C> ℓ=7 <C> 26.71 <C> 75.5% <C> 32.75 <C> 70.4% <R> <C> ℓ=8 <C> 26.59 <C> 80.3% <C> 32.50 <C> 74.6% <R> <C> ℓ=9 <C> 26.42 <C> 83.8% <C> 32.09 <C> 78.3% <R> <C> Gold <C> 27.27 <C> — <C> 33.20 <C> — <CAP> Table 5: The performance (BLEU) of base CMLM with 10 mask-predict iterations (T=10), varied by the number of length candidates (ℓ), compared to decoding with the reference target length (Gold). Length precision (LP) is the percentage of examples that contain the correct length as one of their candidates.
<R> <C> Model <C> Configuration <C> Accuracy(%) <C> F1 <R> <C> majority <C> — <C> 32.7 <C> 8.2 <R> <C> random <C> [EMPTY] <C> 16.7 <C> 16.7 <R> <C> logit <C> textual &  [ITALIC] n-grams <C> 51.3 <C> 48.5 <R> <C> logit <C> [ITALIC] n-grams <C> [BOLD] 52.6 <C> [BOLD] 48.8 <R> <C> Naive Bayes <C> textual &  [ITALIC] n-grams <C> 27.0 <C> 18.3 <R> <C> CNN <C> word emb. <C> 36.4 <C> 20.7 <R> <C> CNN <C> sentence embd. <C> 32.8 <C> 36.0 <CAP> Table 3. Results. Top table: joint classification. Middle table: pipeline classification. Both top tables present final location type results. Bottom table: Pipeline classification – the individual message results of the first step. The majority class for location types (top two tables) is school and for individual messages (bottom table) is shop.
<R> <C> Model <C> Configuration <C> Accuracy(%) <C> F1 <R> <C> majority <C> —- <C> 32.7 <C> 8.2 <R> <C> random <C> [EMPTY] <C> 16.7 <C> 16.7 <R> <C> Naive Bayes <C> textual & [ITALIC] n-grams <C> 23.3 <C> 28.7 <R> <C> Naive Bayes <C> [ITALIC] n-grams & spatio-textual <C> [BOLD] 27.7 <C> [BOLD] 34.7 <R> <C> logit <C> [ITALIC] n-grams & spatio-textual <C> 13.6 <C> 22.8 <R> <C> CNN <C> word emb. <C> 26.2 <C> 26.8 <CAP> Table 3. Results. Top table: joint classification. Middle table: pipeline classification. Both top tables present final location type results. Bottom table: Pipeline classification – the individual message results of the first step. The majority class for location types (top two tables) is school and for individual messages (bottom table) is shop.
<R> <C> Model <C> Configuration <C> Accuracy(%) <C> F1 <R> <C> majority <C> — <C> 32.0 <C> 8.1 <R> <C> random <C> [EMPTY] <C> 16.7 <C> 16.7 <R> <C> logit <C> textual &  [ITALIC] n-grams <C> 55.0 <C> [BOLD] 45.0 <R> <C> Naive Bayes <C> [EMPTY] <C> 47.4 <C> 42.0 <R> <C> CNN <C> word embd. <C> [BOLD] 55.5 <C> 43.5 <CAP> Table 3. Results. Top table: joint classification. Middle table: pipeline classification. Both top tables present final location type results. Bottom table: Pipeline classification – the individual message results of the first step. The majority class for location types (top two tables) is school and for individual messages (bottom table) is shop.
<R> <C> [BOLD] Models <C> [BOLD] HL(-) <C> [BOLD] P(+) <C> [BOLD] R(+) <C> [BOLD] F1(+) <R> <C> BR <C> 0.0086 <C> 0.904 <C> 0.816 <C> 0.858 <R> <C> CC <C> 0.0087 <C> 0.887 <C> 0.828 <C> 0.857 <R> <C> LP <C> 0.0087 <C> 0.896 <C> 0.824 <C> 0.858 <R> <C> CNN <C> 0.0089 <C> [BOLD] 0.922 <C> 0.798 <C> 0.855 <R> <C> CNN-RNN <C> 0.0085 <C> 0.889 <C> 0.825 <C> 0.856 <R> <C> SGM <C> 0.0081 <C> 0.887 <C> 0.850 <C> 0.869 <R> <C> + GE <C> [BOLD] 0.0075 <C> 0.897 <C> [BOLD] 0.860 <C> [BOLD] 0.878 <CAP> (a) Performance on the RCV1-V2 test set.
<R> <C> [BOLD] Models <C> [BOLD] HL(-) <C> [BOLD] P(+) <C> [BOLD] R(+) <C> [BOLD] F1(+) <R> <C> BR <C> 0.0316 <C> 0.644 <C> 0.648 <C> 0.646 <R> <C> CC <C> 0.0306 <C> 0.657 <C> 0.651 <C> 0.654 <R> <C> LP <C> 0.0312 <C> 0.662 <C> 0.608 <C> 0.634 <R> <C> CNN <C> 0.0256 <C> [BOLD] 0.849 <C> 0.545 <C> 0.664 <R> <C> CNN-RNN <C> 0.0278 <C> 0.718 <C> 0.618 <C> 0.664 <R> <C> SGM <C> 0.0251 <C> 0.746 <C> 0.659 <C> 0.699 <R> <C> + GE <C> [BOLD] 0.0245 <C> 0.748 <C> [BOLD] 0.675 <C> [BOLD] 0.710 <CAP> (b) Performance on the AAPD test set.
<R> <C> [BOLD] Task <C> S Final <C> O Final <C> [BOLD] Hybrid + Short-Cut  [BOLD] H [ITALIC] O <C> [BOLD] Hybrid + Short-Cut  [BOLD] H [ITALIC] S <C> [BOLD] Hybrid + Short-Cut Final <R> <C> [ITALIC] Surface Tasks <C> [ITALIC] Surface Tasks <C> [ITALIC] Surface Tasks <C> [ITALIC] Surface Tasks <C> [ITALIC] Surface Tasks <C> [ITALIC] Surface Tasks <R> <C> SeLen <C> [ITALIC] 92.71 <C> 90.70 <C> 91.94 <C> 89.50 <C> 89.86 <R> <C> WC <C> 81.79 <C> 76.42 <C> [ITALIC] 90.38 <C> 79.10 <C> 80.37 <R> <C> Avg <C> 87.25 <C> 83.56 <C> [BOLD] 91.16 <C> 84.30 <C> 85.12 <R> <C> [ITALIC] Syntactic Tasks <C> [ITALIC] Syntactic Tasks <C> [ITALIC] Syntactic Tasks <C> [ITALIC] Syntactic Tasks <C> [ITALIC] Syntactic Tasks <C> [ITALIC] Syntactic Tasks <R> <C> TrDep <C> 44.78 <C> 52.58 <C> 51.19 <C> 52.55 <C> [ITALIC] 53.28 <R> <C> ToCo <C> 84.53 <C> 86.32 <C> 86.29 <C> [ITALIC] 87.92 <C> 87.89 <R> <C> BShif <C> 52.66 <C> [ITALIC] 82.68 <C> 81.79 <C> 82.05 <C> 81.90 <R> <C> Avg <C> 60.66 <C> 73.86 <C> 73.09 <C> 74.17 <C> [BOLD] 74.36 <R> <C> [ITALIC] Semantic Tasks <C> [ITALIC] Semantic Tasks <C> [ITALIC] Semantic Tasks <C> [ITALIC] Semantic Tasks <C> [ITALIC] Semantic Tasks <C> [ITALIC] Semantic Tasks <R> <C> Tense <C> 84.76 <C> 86.00 <C> 83.88 <C> [ITALIC] 86.05 <C> 85.91 <R> <C> SubN <C> 85.18 <C> 85.44 <C> 85.56 <C> 84.59 <C> [ITALIC] 85.81 <R> <C> ObjN <C> 81.45 <C> [ITALIC] 86.78 <C> 85.72 <C> 85.80 <C> 85.38 <R> <C> SoMo <C> 49.87 <C> 49.54 <C> 49.23 <C> 49.12 <C> [ITALIC] 49.92 <R> <C> CoIn <C> 68.97 <C> 72.03 <C> 72.06 <C> 72.05 <C> [ITALIC] 72.23 <R> <C> Avg <C> 74.05 <C> [BOLD] 75.96 <C> 75.29 <C> [ITALIC] 75.52 <C> 75.85 <CAP> Table 3: Performance on the linguistic probing tasks of evaluating linguistics embedded in the learned representations. “S” and “O” denote the San and On-Lstm baseline models. “HO” and “HS” are respectively the outputs of the On-Lstm encoder and the San encoder in the hybrid model, and “Final” denotes the final output exposed to decoder.
<R> <C> [BOLD] Set <C> [BOLD] TP <C> [BOLD] FP <C> [BOLD] FN <C> [BOLD] Total <R> <C> [BOLD] Training <C> 6325 <C> 122 <C> 66 <C> 6513 <R> <C> [BOLD] Test <C> 2618 <C> 50 <C> 26 <C> 2694 <R> <C> [BOLD] Total <C> 8943 <C> 172 <C> 92 <C> 9207 <CAP> Table 7: The number of true positives (TP), false positives (FP), and false negative (FN) spans that are created from the process of transforming the ground truth spans into the extended BIO format and back. This is equivalent to having a perfect classifier.
<R> <C> [BOLD] Entities <C> [BOLD] Type <C> [BOLD] Method <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F-Score <C> [BOLD] Accuracy <R> <C> [BOLD] ADRs <C> Strict <C> VSM+UMLS <C> 0.264 <C> 0.392 <C> 0.316 <C> 0.454 <R> <C> [BOLD] ADRs <C> Strict <C> MetaMap <C> 0.105 <C> 0.080 <C> 0.091 <C> 0.485∗ <R> <C> [BOLD] ADRs <C> Strict <C> VSM+CHV <C> 0.457 <C> 0.370 <C> 0.409 <C> 0.656∗ <R> <C> [BOLD] ADRs <C> Strict <C> VSM+SCT <C> 0.498 <C> 0.352 <C> 0.412 <C> 0.678∗ <R> <C> [BOLD] ADRs <C> Strict <C> CRF <C> 0.644 <C> 0.565 <C> 0.602 <C> 0.760∗ <R> <C> [BOLD] ADRs <C> Relaxed <C> VSM+UMLS <C> 0.454 <C> 0.674 <C> 0.543 <C> 0.635 <R> <C> [BOLD] ADRs <C> Relaxed <C> VSM+CHV <C> 0.747 <C> 0.605 <C> 0.669 <C> 0.807∗ <R> <C> [BOLD] ADRs <C> Relaxed <C> MetaMap <C> 0.794 <C> 0.605 <C> 0.687 <C> 0.822∗ <R> <C> [BOLD] ADRs <C> Relaxed <C> VSM+SCT <C> 0.818 <C> 0.578 <C> 0.677 <C> 0.822 <R> <C> [BOLD] ADRs <C> Relaxed <C> CRF <C> 0.908 <C> 0.797 <C> 0.849 <C> 0.909∗ <R> <C> [BOLD] Drugs <C> Strict <C> VSM+UMLS <C> 0.160 <C> 0.882 <C> 0.271 <C> 0.546 <R> <C> [BOLD] Drugs <C> Strict <C> VSM+AMT <C> 0.160 <C> 0.775 <C> 0.266 <C> 0.589∗ <R> <C> [BOLD] Drugs <C> Strict <C> MetaMap <C> 0.022 <C> 0.021 <C> 0.021 <C> 0.816∗ <R> <C> [BOLD] Drugs <C> Strict <C> VSM+CHV <C> 0.468 <C> 0.856 <C> 0.605 <C> 0.893∗ <R> <C> [BOLD] Drugs <C> Strict <C> CRF <C> 0.943 <C> 0.840 <C> 0.889 <C> 0.980∗ <R> <C> [BOLD] Drugs <C> Relaxed <C> VSM+UMLS <C> 0.168 <C> 0.923 <C> 0.284 <C> 0.554 <R> <C> [BOLD] Drugs <C> Relaxed <C> VSM+AMT <C> 0.173 <C> 0.837 <C> 0.287 <C> 0.601∗ <R> <C> [BOLD] Drugs <C> Relaxed <C> MetaMap <C> 0.145 <C> 0.139 <C> 0.142 <C> 0.839∗ <R> <C> [BOLD] Drugs <C> Relaxed <C> VSM+CHV <C> 0.489 <C> 0.893 <C> 0.632 <C> 0.900∗ <R> <C> [BOLD] Drugs <C> Relaxed <C> CRF <C> 0.979 <C> 0.872 <C> 0.923 <C> 0.986∗ <CAP> Table 10: Evaluation results of the concept identification task, sorted by accuracy. Statistical significant difference with the next best performing method is indicated with ∗ (p <0.01).
<R> <C> Model <C> Vision <C> Instr. <C> History <C> [BOLD] Val-Seen  [BOLD] TL ↓ <C> [BOLD] Val-Seen  [BOLD] NE ↓ <C> [BOLD] Val-Seen  [BOLD] nDTW ↑ <C> [BOLD] Val-Seen  [BOLD] OS ↑ <C> [BOLD] Val-Seen  [BOLD] SR ↑ <C> [BOLD] Val-Seen  [BOLD] SPL ↑ <C> [BOLD] Val-Unseen  [BOLD] TL ↓ <C> [BOLD] Val-Unseen  [BOLD] NE ↓ <C> [BOLD] Val-Unseen  [BOLD] nDTW ↑ <C> [BOLD] Val-Unseen  [BOLD] OS ↑ <C> [BOLD] Val-Unseen  [BOLD] SR ↑ <C> [BOLD] Val-Unseen  [BOLD] SPL ↑ <R> <C> Random <C> - <C> - <C> - <C> 3.54 <C> 10.20 <C> 0.28 <C> 0.04 <C> 0.02 <C> 0.02 <C> 3.74 <C> 9.51 <C> 0.30 <C> 0.04 <C> 0.03 <C> 0.02 <R> <C> Hand-Crafted <C> - <C> - <C> - <C> 3.83 <C> 9.56 <C> 0.33 <C> 0.05 <C> 0.04 <C> 0.04 <C> 3.71 <C> 10.34 <C> 0.30 <C> 0.04 <C> 0.03 <C> 0.02 <R> <C> Seq2Seq <C> RGBD <C> ✓ <C> ✓ <C> 8.40 <C> 8.54 <C> 0.45 <C> 0.35 <C> 0.25 <C> 0.24 <C> 7.67 <C> 8.94 <C> 0.43 <C> 0.25 <C> 0.20 <C> 0.18 <R> <C> – No Image <C> D <C> ✓ <C> ✓ <C> 7.77 <C> 8.55 <C> 0.46 <C> 0.31 <C> 0.24 <C> 0.23 <C> 7.87 <C> 9.09 <C> 0.41 <C> 0.23 <C> 0.17 <C> 0.15 <R> <C> – No Depth <C> RGB <C> ✓ <C> ✓ <C> 4.93 <C> 10.76 <C> 0.29 <C> 0.10 <C> 0.03 <C> 0.03 <C> 5.54 <C> 9.89 <C> 0.31 <C> 0.11 <C> 0.04 <C> 0.04 <R> <C> – No Vision <C> - <C> ✓ <C> ✓ <C> 4.26 <C> 11.07 <C> 0.26 <C> 0.03 <C> 0.00 <C> 0.00 <C> 4.68 <C> 10.06 <C> 0.30 <C> 0.07 <C> 0.00 <C> 0.00 <R> <C> – No Instruction <C> RGBD <C> - <C> ✓ <C> 7.86 <C> 9.09 <C> 0.42 <C> 0.26 <C> 0.18 <C> 0.17 <C> 7.27 <C> 9.03 <C> 0.42 <C> 0.22 <C> 0.17 <C> 0.16 <CAP> Table 2: No-learning baselines and input modality ablations for our baseline sequence-to-sequence model. Given the long trajectories involved, we find both random agents and single-modality ablations to perform quite poorly in VLN-CE.
<R> <C> [BOLD] Hyperparameters <C> [BOLD] Values <R> <C> Encoder Layers <C> 6 <R> <C> Decoder Layers <C> 6 <R> <C> Embedding Units <C> 2,048 <R> <C> Attention Heads <C> 8 <R> <C> Feed-forward Hidden Units <C> 512 <R> <C> Initial Learning Rate <C> 0.0007 <R> <C> Train Steps <C> 100,000 <R> <C> Vocab EN/FR <C> 43,244 <R> <C> Vocab EN/DE <C> 43,756 <CAP> Table 1: Hyperparameters for this study.
<R> <C> Data <C> Method <C> Parameters <C> LSTM dims <C> HMM states <C> Validation <C> Training <R> <C> Shakespeare <C> Continuous HMM <C> 1300 <C> [EMPTY] <C> 20 <C> -2.74 <C> -2.75 <R> <C> Shakespeare <C> Discrete HMM <C> 650 <C> [EMPTY] <C> 10 <C> -2.69 <C> -2.68 <R> <C> Shakespeare <C> Discrete HMM <C> 1300 <C> [EMPTY] <C> 20 <C> -2.5 <C> -2.49 <R> <C> Shakespeare <C> LSTM <C> 865 <C> 5 <C> [EMPTY] <C> -2.41 <C> -2.35 <R> <C> Shakespeare <C> Hybrid <C> 1515 <C> 5 <C> 10 <C> -2.3 <C> -2.26 <R> <C> Shakespeare <C> Hybrid <C> 2165 <C> 5 <C> 20 <C> -2.26 <C> -2.18 <R> <C> Shakespeare <C> LSTM <C> 2130 <C> 10 <C> [EMPTY] <C> -2.23 <C> -2.12 <R> <C> Shakespeare <C> Joint hybrid <C> 1515 <C> 5 <C> 10 <C> -2.21 <C> -2.18 <R> <C> Shakespeare <C> Hybrid <C> 2780 <C> 10 <C> 10 <C> -2.19 <C> -2.08 <R> <C> Shakespeare <C> Hybrid <C> 3430 <C> 10 <C> 20 <C> -2.16 <C> -2.04 <R> <C> Shakespeare <C> Hybrid <C> 4445 <C> 15 <C> 10 <C> -2.13 <C> -1.95 <R> <C> Shakespeare <C> Joint hybrid <C> 3430 <C> 10 <C> 10 <C> -2.12 <C> -2.07 <R> <C> Shakespeare <C> LSTM <C> 3795 <C> 15 <C> [EMPTY] <C> -2.1 <C> -1.95 <R> <C> Shakespeare <C> Hybrid <C> 5095 <C> 15 <C> 20 <C> -2.07 <C> -1.92 <R> <C> Shakespeare <C> Hybrid <C> 6510 <C> 20 <C> 10 <C> -2.05 <C> -1.87 <R> <C> Shakespeare <C> Joint hybrid <C> 4445 <C> 15 <C> 10 <C> -2.03 <C> -1.97 <R> <C> Shakespeare <C> LSTM <C> 5860 <C> 20 <C> [EMPTY] <C> -2.03 <C> -1.83 <R> <C> Shakespeare <C> Hybrid <C> 7160 <C> 20 <C> 20 <C> -2.02 <C> -1.85 <R> <C> Shakespeare <C> Joint hybrid <C> 7160 <C> 20 <C> 10 <C> -1.97 <C> -1.88 <R> <C> Linux Kernel <C> Discrete HMM <C> 1000 <C> [EMPTY] <C> 10 <C> -2.76 <C> -2.7 <R> <C> Linux Kernel <C> Discrete HMM <C> 2000 <C> [EMPTY] <C> 20 <C> -2.55 <C> -2.5 <R> <C> Linux Kernel <C> LSTM <C> 1215 <C> 5 <C> [EMPTY] <C> -2.54 <C> -2.48 <R> <C> Linux Kernel <C> Joint hybrid <C> 2215 <C> 5 <C> 10 <C> -2.35 <C> -2.26 <R> <C> Linux Kernel <C> Hybrid <C> 2215 <C> 5 <C> 10 <C> -2.33 <C> -2.26 <R> <C> Linux Kernel <C> Hybrid <C> 3215 <C> 5 <C> 20 <C> -2.25 <C> -2.16 <R> <C> Linux Kernel <C> Joint hybrid <C> 4830 <C> 10 <C> 10 <C> -2.18 <C> -2.08 <R> <C> Linux Kernel <C> LSTM <C> 2830 <C> 10 <C> [EMPTY] <C> -2.17 <C> -2.07 <R> <C> Linux Kernel <C> Hybrid <C> 3830 <C> 10 <C> 10 <C> -2.14 <C> -2.05 <R> <C> Linux Kernel <C> Hybrid <C> 4830 <C> 10 <C> 20 <C> -2.07 <C> -1.97 <R> <C> Linux Kernel <C> LSTM <C> 4845 <C> 15 <C> [EMPTY] <C> -2.03 <C> -1.9 <R> <C> Linux Kernel <C> Joint hybrid <C> 5845 <C> 15 <C> 10 <C> -2.00 <C> -1.88 <R> <C> Linux Kernel <C> Hybrid <C> 5845 <C> 15 <C> 10 <C> -1.96 <C> -1.84 <R> <C> Linux Kernel <C> Hybrid <C> 6845 <C> 15 <C> 20 <C> -1.96 <C> -1.83 <R> <C> Linux Kernel <C> Joint hybrid <C> 9260 <C> 20 <C> 10 <C> -1.90 <C> -1.76 <R> <C> Linux Kernel <C> LSTM <C> 7260 <C> 20 <C> [EMPTY] <C> -1.88 <C> -1.73 <R> <C> Linux Kernel <C> Hybrid <C> 8260 <C> 20 <C> 10 <C> -1.87 <C> -1.73 <R> <C> Linux Kernel <C> Hybrid <C> 9260 <C> 20 <C> 20 <C> -1.85 <C> -1.71 <R> <C> Penn Tree Bank <C> Continuous HMM <C> 1000 <C> 100 <C> 20 <C> -2.58 <C> -2.58 <R> <C> Penn Tree Bank <C> Discrete HMM <C> 500 <C> [EMPTY] <C> 10 <C> -2.43 <C> -2.43 <R> <C> Penn Tree Bank <C> Discrete HMM <C> 1000 <C> [EMPTY] <C> 20 <C> -2.28 <C> -2.28 <R> <C> Penn Tree Bank <C> LSTM <C> 715 <C> 5 <C> [EMPTY] <C> -2.22 <C> -2.22 <R> <C> Penn Tree Bank <C> Hybrid <C> 1215 <C> 5 <C> 10 <C> -2.14 <C> -2.15 <R> <C> Penn Tree Bank <C> Joint hybrid <C> 1215 <C> 5 <C> 10 <C> -2.08 <C> -2.08 <R> <C> Penn Tree Bank <C> Hybrid <C> 1715 <C> 5 <C> 20 <C> -2.06 <C> -2.07 <R> <C> Penn Tree Bank <C> LSTM <C> 1830 <C> 10 <C> [EMPTY] <C> -1.99 <C> -1.99 <R> <C> Penn Tree Bank <C> Hybrid <C> 2330 <C> 10 <C> 10 <C> -1.94 <C> -1.95 <R> <C> Penn Tree Bank <C> Joint hybrid <C> 2830 <C> 10 <C> 10 <C> -1.94 <C> -1.95 <R> <C> Penn Tree Bank <C> Hybrid <C> 2830 <C> 10 <C> 20 <C> -1.93 <C> -1.94 <R> <C> Penn Tree Bank <C> LSTM <C> 3345 <C> 15 <C> [EMPTY] <C> -1.82 <C> -1.83 <R> <C> Penn Tree Bank <C> Hybrid <C> 3845 <C> 15 <C> 10 <C> -1.81 <C> -1.82 <R> <C> Penn Tree Bank <C> Hybrid <C> 4345 <C> 15 <C> 20 <C> -1.8 <C> -1.81 <R> <C> Penn Tree Bank <C> Joint hybrid <C> 6260 <C> 20 <C> 10 <C> -1.73 <C> -1.74 <R> <C> Penn Tree Bank <C> LSTM <C> 5260 <C> 20 <C> [EMPTY] <C> -1.72 <C> -1.73 <R> <C> Penn Tree Bank <C> Hybrid <C> 5760 <C> 20 <C> 10 <C> -1.72 <C> -1.72 <R> <C> Penn Tree Bank <C> Hybrid <C> 6260 <C> 20 <C> 20 <C> -1.71 <C> -1.71 <CAP> Table 1: Predictive loglikelihood comparison on the text data sets (sorted by validation set performance).
<R> <C> [BOLD] Captioning Method <C> B-1 <C> B-2 <C> B-3 <C> B-4 <R> <C> Human agreement <C> 0.68 <C> 0.45 <C> 0.30 <C> 0.20 <R> <C> Karpathy & Fei-Fei ( 2014 ) <C> 0.57 <C> 0.37 <C> 0.19 <C> - <R> <C> Vinyals et al. ( 2014 ) <C> 0.67 <C> - <C> - <C> [EMPTY] <R> <C> Donahue et al. ( 2014 ) <C> 0.63 <C> 0.44 <C> 0.30 <C> 0.21 <R> <C> Fang et al. ( 2014 ) <C> - <C> - <C> - <C> 0.21 <R> <C> Our model <C> 0.70 <C> 0.46 <C> 0.30 <C> 0.20 <CAP> Table 1: Comparison between human agreement scores, state of the art models and our model on the COCO dataset. Note that there are slight variations between the test sets chosen in each paper.
<R> <C> [BOLD] Domain <C> [BOLD] skip-gram wiki <C> [BOLD] dbow sts <C> [BOLD] dbow wiki <C> [BOLD] dbow sts (wiki init) <C> [BOLD] CA(CNN) sts <C> [BOLD] CA(GRU) sts <C> [BOLD] w-dbow(IDF) sts <R> <C> answers-forums <C> 0.516 <C> 0.647 <C> 0.666 <C> [BOLD] 0.675 <C> 0.670 <C> 0.662 <C> 0.656 <R> <C> headlines <C> 0.731 <C> 0.768 <C> 0.746 <C> 0.782 <C> 0.785 <C> 0.787 <C> [BOLD] 0.788 <R> <C> answers-students <C> 0.661 <C> 0.640 <C> 0.628 <C> 0.654 <C> [BOLD] 0.683 <C> 0.676 <C> 0.660 <R> <C> belief <C> 0.607 <C> 0.764 <C> 0.713 <C> [BOLD] 0.773 <C> 0.772 <C> 0.764 <C> 0.760 <R> <C> images <C> 0.678 <C> 0.781 <C> 0.789 <C> [BOLD] 0.800 <C> 0.793 <C> 0.793 <C> 0.787 <CAP> Table 2: Results over STS task with different unsupervised methods
<R> <C> [BOLD] Domain <C> [BOLD] CA(CNN) global <C> [BOLD] CA(CNN) POS <C> [BOLD] CA(GRU) global <C> [BOLD] CA(GRU) POS <R> <C> answers-forums <C> 0.663 <C> [BOLD] 0.670 <C> 0.668 <C> 0.662 <R> <C> headlines <C> 0.786 <C> 0.785 <C> 0.786 <C> [BOLD] 0.787 <R> <C> answers-students <C> 0.673 <C> [BOLD] 0.683 <C> 0.675 <C> 0.676 <R> <C> belief <C> 0.760 <C> [BOLD] 0.772 <C> 0.761 <C> 0.764 <R> <C> images <C> [BOLD] 0.800 <C> 0.793 <C> 0.792 <C> 0.793 <CAP> Table 3: Context aware models with random sampling from different distributions. The global distribution is proportional to TF of each word. The POS distribution is proportional to TF of each word grouped by POS. Samples are taken from the corresponding distribution regarding POS of the substituted word. We take 50 and 10 samples for global distribution and POS distribution respectively.
<R> <C> [EMPTY] <C> flickr-8k Pearson <C> flickr-8k Spearman <C> flickr-8k Kendall <C> composite Pearson <C> composite Spearman <C> composite Kendall <R> <C> wmd <C> 0.68 <C> 0.60 <C> 0.48 <C> [BOLD] 0.43 <C> 0.43 <C> 0.32 <R> <C> spice <C> [BOLD] 0.69 <C> [BOLD] 0.64 <C> [BOLD] 0.56 <C> 0.40 <C> 0.42 <C> [BOLD] 0.34 <R> <C> cider <C> 0.60 <C> 0.56 <C> 0.45 <C> 0.32 <C> 0.42 <C> 0.32 <R> <C> meteor <C> [BOLD] 0.69 <C> 0.58 <C> 0.47 <C> 0.37 <C> [BOLD] 0.44 <C> 0.33 <R> <C> bleu <C> 0.59 <C> 0.44 <C> 0.35 <C> 0.34 <C> 0.38 <C> 0.28 <R> <C> rouge <C> 0.57 <C> 0.44 <C> 0.35 <C> 0.40 <C> 0.39 <C> 0.29 <CAP> Table 3: Correlation between automatic image captioning metrics and human judgement scores.
<R> <C> [EMPTY] <C> abstract-50s HC <C> abstract-50s HI <C> abstract-50s Avg. <C> pascal-50s HC <C> pascal-50s HI <C> pascal-50s HM <C> pascal-50s MM <C> pascal-50s Avg. <R> <C> wmd <C> 0.65 <C> 0.93 <C> 0.79 <C> [BOLD] 0.71 <C> [BOLD] 0.99 <C> 0.93 <C> [BOLD] 0.74 <C> [BOLD] 0.84 <R> <C> spice <C> 0.62 <C> 0.89 <C> 0.76 <C> 0.66 <C> 0.98 <C> 0.85 <C> 0.72 <C> 0.81 <R> <C> cider <C> [BOLD] 0.76 <C> [BOLD] 0.95 <C> [BOLD] 0.86 <C> 0.69 <C> [BOLD] 0.99 <C> [BOLD] 0.94 <C> 0.66 <C> 0.82 <R> <C> meteor <C> 0.60 <C> 0.90 <C> 0.75 <C> 0.69 <C> [BOLD] 0.99 <C> 0.90 <C> 0.65 <C> 0.81 <R> <C> bleu <C> 0.69 <C> 0.89 <C> 0.79 <C> 0.67 <C> 0.97 <C> [BOLD] 0.94 <C> 0.60 <C> 0.80 <R> <C> rouge <C> 0.65 <C> 0.89 <C> 0.77 <C> 0.68 <C> 0.97 <C> 0.92 <C> 0.60 <C> 0.79 <CAP> Table 4: Description-level classification accuracies of automatic evaluation metrics.
<R> <C> [BOLD] Dataset <C> [BOLD] Transformer  [BOLD] Adam <C> [BOLD] Transformer  [BOLD] Ouroboros + Adam <C> [BOLD] Transformer-XL  [BOLD] Adam <C> [BOLD] Transformer-XL  [BOLD] Ouroboros + Adam <R> <C> enwiki8 <C> [BOLD] 1.11 <C> 1.12 <C> 1.06 <C> [BOLD] 1.05 <R> <C> text8 <C> [BOLD] 1.18 <C> [BOLD] 1.18 <C> 1.15 <C> [BOLD] 1.13 <R> <C> WikiText-103 <C> 28.32 <C> [BOLD] 28.29 <C> [BOLD] 24.00 <C> 24.10 <CAP> Table 1: Comparison of Test bpc (Bit per Character) or Test PPL. We use the metric bpc on the enwiki8 and text8 datasets, and PPL on the WikiText-103 dataset. Our algorithm can achieve speedup with comparable or better performance.
<R> <C> Method <C> DISTINCT-1 <C> DISTINCT-2 <R> <C> Seq2Seq <C> 0.031 <C> 0.137 <R> <C> MMI-anti <C> 0.033 <C> 0.141 <R> <C> MMI-bidi <C> 0.034 <C> 0.143 <R> <C> Adver-REIN <C> 0.036 <C> 0.145 <R> <C> GAN-AEL <C> 0.038 <C> 0.149 <R> <C> [BOLD] DAL-Dual ( [ITALIC] ours) <C> [BOLD] 0.052 <C> [BOLD] 0.209 <R> <C> [BOLD] DAL-DuAd ( [ITALIC] ours) <C> [BOLD] 0.049 <C> [BOLD] 0.201 <CAP> Table 1: Results of diversity evaluation.
<R> <C> Method <C> Human rating <C> Kappa <R> <C> Seq2Seq <C> 0.470 <C> 0.56 <R> <C> MMI-anti <C> 0.568 <C> 0.46 <R> <C> MMI-bidi <C> 0.523 <C> 0.60 <R> <C> Adver-REIN <C> 0.767 <C> 0.49 <R> <C> GAN-AEL <C> 0.758 <C> 0.52 <R> <C> DAL-Dual ( [ITALIC] ours) <C> 0.730 <C> 0.47 <R> <C> [BOLD] DAL-DuAd ( [ITALIC] ours) <C> [BOLD] 0.778 <C> 0.50 <CAP> Table 2: Results of human elevation: response quality.
<R> <C> [BOLD] Model <C> [BOLD] Best Score <R> <C> Baseline Model <C> 0.731 <R> <C> Baseline + Character Embedding Model <C> 0.743 <R> <C> Baseline + Character Embedding Model + POS Tag Concatenation <C> 0.747 <R> <C> Baseline + Character Embedding Model + POS Tag Concatenation + Language Model <C> 0.738 <CAP> Table 1: BiLSTM + Attention Approach
<R> <C> [BOLD] Model <C> [BOLD] Best Score <R> <C> BERT + BiLSTM + Attention + FC layers <C> 0.771 <R> <C> BERT + GRU + Attention + FC layers <C> 0.755 <R> <C> BERT + Classifier <C> 0.775 <R> <C> BERT_Large + Classifier <C> 0.789 <R> <C> RoBERTa_Base + Classifier <C> 0.775 <R> <C> RoBERTa_Large + Classifier <C> 0.790 <R> <C> XLNet_Large + Classifier <C> [BOLD] 0.804 <R> <C> ALBERT + Classifier <C> 0.755 <R> <C> GPT-2 + Classifier <C> 0.725 <R> <C> XLM-RoBERTa + Classifier <C> 0.785 <CAP> Table 1: BiLSTM + Attention Approach
<R> <C> [BOLD] Model <C> [BOLD] Recall @5 <C> [BOLD] Recall @10 <C> [BOLD] Recall @20 <C> [BOLD] MAP @5 <C> [BOLD] MAP @10 <C> [BOLD] MAP @20 <C> [BOLD] MRR @5 <C> [BOLD] MRR @10 <C> [BOLD] MRR @20 <C> [BOLD] nDCG @5 <C> [BOLD] nDCG @10 <C> [BOLD] nDCG @20 <R> <C> LSTM <C> 13.08 <C> 19.02 <C> 26.19 <C> 16.80 <C> 17.40 <C> 17.14 <C> 17.15 <C> 18.43 <C> 19.13 <C> 11.88 <C> 14.31 <C> 16.73 <R> <C> +attn <C> 13.73 <C> 19.85 <C> 27.15 <C> 17.72 <C> 18.29 <C> 17.96 <C> 18.11 <C> 19.41 <C> 20.11 <C> 12.55 <C> 15.04 <C> 17.51 <R> <C> NBoW <C> 16.32 <C> 23.42 <C> 31.64 <C> 20.78 <C> 21.30 <C> 20.79 <C> 21.29 <C> 22.71 <C> 23.45 <C> 14.91 <C> 17.80 <C> 20.58 <R> <C> +attn <C> 16.69 <C> 23.88 <C> 32.24 <C> 21.36 <C> 21.87 <C> 21.30 <C> 21.89 <C> 23.32 <C> 24.06 <C> 15.29 <C> 18.22 <C> 21.06 <R> <C> +dot <C> 15.54 <C> 22.13 <C> 29.89 <C> 20.12 <C> 20.60 <C> 20.12 <C> 20.61 <C> 21.95 <C> 22.66 <C> 14.36 <C> 17.03 <C> 19.66 <R> <C> +concat <C> 16.85 <C> 24.12 <C> 32.53 <C> 21.57 <C> 22.04 <C> 21.47 <C> 22.10 <C> 23.54 <C> 24.28 <C> 15.46 <C> 18.41 <C> 21.27 <R> <C> +trans-dot <C> [BOLD] 17.20 <C> [BOLD] 24.51 <C> [BOLD] 33.01 <C> [BOLD] 21.97 <C> [BOLD] 22.41 <C> [BOLD] 21.80 <C> [BOLD] 22.53 <C> [BOLD] 23.96 <C> [BOLD] 24.70 <C> [BOLD] 15.80 <C> [BOLD] 18.77 <C> [BOLD] 21.65 <CAP> Table 4: Performance of different scoring functions in attention. trans-dot is significantly better at p<0.01.
<R> <C> [BOLD] Round 1 <C> Score Automated Systems (102) Judged Pairs (8,691) <C> Rank Automated Systems (102) Judged Pairs (8,691) <C> Score All Systems (144) All Pairs (1.53M) <C> Rank All Systems (144) All Pairs (1.53M) <C> Judged@n <R> <C> Bpref <C> 0.5176 <C> 1 <C> 0.5176 <C> 3 <C> - <R> <C> MAP <C> 0.4870 <C> 1 <C> 0.2401 <C> 21 <C> - <R> <C> P@5 <C> 0.8267 <C> 1 <C> 0.6333 <C> 17 <C> 72.00% <R> <C> P@10 <C> 0.7933 <C> 1 <C> 0.5567 <C> 21 <C> 65.67% <R> <C> nDCG@10 <C> 0.7233 <C> 1 <C> 0.5445 <C> 21 <C> 65.67% <R> <C> [BOLD] Round 2 <C> Automated Systems (73) Judged Pairs (12,037) <C> Automated Systems (73) Judged Pairs (12,037) <C> All Systems (136) All Pairs (2.2M) <C> All Systems (136) All Pairs (2.2M) <C> [EMPTY] <R> <C> Bpref <C> 0.5232 <C> 1 <C> 0.5402 <C> 2 <C> - <R> <C> MAP <C> 0.5138 <C> 1 <C> 0.3487 <C> 1 <C> - <R> <C> P@5 <C> 0.8171 <C> 1 <C> 0.8000 <C> 3 <C> 98.29% <R> <C> P@10 <C> 0.7629 <C> 1 <C> 0.7200 <C> 3 <C> 93.71% <R> <C> nDCG@10 <C> 0.7247 <C> 1 <C> 0.6996 <C> 1 <C> 93.71% <CAP> Table 2: TREC-COVID results
<R> <C> [EMPTY] <C> [ITALIC] F1 <R> <C> Random baseline <C> 5.8 <R> <C> Majority baseline <C> 6.0 <R> <C> All features (SVM) <C> 27.0 <R> <C> All features (PA) <C> 22.8 <R> <C> Social features <C> 3.8 <R> <C> Lexical features <C> 10.9 <R> <C> User IDs <C> 12.2 <CAP> Table 2: Results for predicting deleted tweets.
<R> <C> User group <C> Here <C> Baseline <C> # in test set <R> <C> Followers <1,000 <C> 17.8 <C> 5.8 <C> 6.8M <R> <C> Followers ∈[1 [ITALIC] k,10 [ITALIC] k] <C> 33.7 <C> 6.6 <C> 640k <R> <C> Followers ∈[10 [ITALIC] k,100 [ITALIC] k] <C> 66.0 <C> 17.7 <C> 50k <R> <C> Followers >100,000 <C> 86.4 <C> 41.5 <C> 5.5k <R> <C> Celebrities <C> 39.5 <C> 6.0 <C> 3.5k <CAP> Table 3: F1 score for different groups of users. The third column shows our results for named groups. The last column shows the number of users in the test set that fall into each category.
<R> <C> Deletion type <C> % of tweets in test set <C> Accuracy <R> <C> Manual deletion <C> 85.2 <C> 18.8 <R> <C> Protected <C> 12.2 <C> 17.5 <R> <C> Account deleted <C> 2.6 <C> 29.5 <CAP> Table 4: Proportion of different types of deletions and performance of our algorithm across these types.
<R> <C> model <C> compression <C> # of params <C> frame accuracy <C> WER <R> <C> full <C> - <C> 1.85m <C> 73.26 <C> 43.5 <R> <C> low rank <C> 5,5,600 <C> 790k <C> 68.08 <C> 54.6 <R> <C> HashedNets <C> 5,5,600 <C> 790k <C> 70.09 <C> 49.2 <R> <C> Toeplitz-like <C> 5,5,600 <C> 790k <C> 70.79 <C> 48.4 <CAP> Table 2: Learning different compact models for RNN
<R> <C> rank <C> # of params <C> sec. per step <C> frame accuracy <C> WER <R> <C> 2 <C> 779k <C> 0.13 <C> 70.41 <C> 49.6 <R> <C> 5 <C> 790k <C> 0.28 <C> 70.79 <C> 48.4 <R> <C> 10 <C> 808k <C> 0.50 <C> 70.99 <C> 47.9 <CAP> Table 3: Toeplitz-like matrices with different rank
<R> <C> drop <C> precision <C> recall <C> f-score <R> <C> baseline <C> 85.54 <C> 89.97 <C> 87.70 <R> <C> 10%, both <C> 85.28 <C> 89.97 <C> 87.56 <R> <C> 25%, both <C> 85.17 <C> 89.87 <C> 87.45 <R> <C> 40%, both <C> 83.97 <C> 89.47 <C> 86.63 <R> <C> 50%, both <C> 83.99 <C> 89.18 <C> 86.50 <R> <C> 75%, both <C> 77.04 <C> 81.47 <C> 79.19 <R> <C> 10%, only-a <C> 85.35 <C> 89.95 <C> 87.59 <R> <C> 25%, only-a <C> 85.12 <C> 90.23 <C> 87.60 <R> <C> 40%, only-a <C> 84.20 <C> 89.96 <C> 86.98 <R> <C> 50%, only-a <C> 83.14 <C> 90.25 <C> 86.55 <R> <C> 75%, only-a <C> 68.19 <C> 91.51 <C> 78.15 <R> <C> 10%, only-b <C> 85.51 <C> 89.77 <C> 87.59 <R> <C> 25%, only-b <C> 85.77 <C> 89.63 <C> 87.66 <R> <C> 40%, only-b <C> 86.22 <C> 89.72 <C> 87.94 <R> <C> 50%, only-b <C> 86.21 <C> 87.97 <C> 87.09 <R> <C> 75%, only-b <C> 88.87 <C> 73.10 <C> 80.22 <CAP> Table 1: Results visual perturbations
<R> <C> Model <C> RG <C> RG <C> CS <C> CS <C> CO <C> BLEU <R> <C> Model <C> # <C> P% <C> P% <C> R% <C> DLD% <C> BLEU <R> <C> ED+CC <C> 21.94 <C> 75.08 <C> 27.96 <C> 32.71 <C> 15.03 <C> 13.31 <R> <C> CS+CC <C> 24.93 <C> 80.55 <C> 28.63 <C> 35.23 <C> 15.12 <C> 13.52 <R> <C> CP+CC <C> 33.73 <C> 84.85 <C> 29.57 <C> 44.72 <C> 15.84 <C> 14.45 <R> <C> NCP+CC <C> 33.88 <C> 87.51 <C> 33.52 <C> 51.21 <C> 18.57 <C> 16.19 <R> <C> NCP <C> 34.46 <C> — <C> 38.00 <C> 53.72 <C> 20.27 <C> — <CAP> Table 4: Ablation results on RotoWire development set using relation generation (RG) count (#) and precision (P%), content selection (CS) precision (P%) and recall (R%), content ordering (CO) in normalized Damerau-Levenshtein distance (DLD%), and BLEU.
<R> <C> Models <C> PKU P <C> PKU R <C> PKU F <C> CTB P <C> CTB R <C> CTB F <R> <C> Bi-LSTM <C> 94.1 <C> 92.6 <C> 93.3 <C> 94.2 <C> 94.5 <C> 94.3 <R> <C> GRNN <C> 94.5 <C> 93.6 <C> 94.0 <C> 94.8 <C> 94.9 <C> 94.8 <R> <C> [BOLD] UGL <C> 95.2 <C> 94.1 <C> [BOLD] 94.6 <C> 95.4 <C> 95.2 <C> [BOLD] 95.3 <CAP> Table 1: Comparisons between UGL and baselines on low-resource datasets: PKU and CTB.
<R> <C> Models <C> PKU <C> CTB <R> <C> Bi-LSTM <C> 93.3 <C> 94.3 <R> <C> UGL <C> 94.6 <C> 95.3 <R> <C> +Transfer Learning <C> [BOLD] 95.6 <C> [BOLD] 95.8 <R> <C> [BOLD] Improvement <C> [BOLD] 2.3 <C> [BOLD] 1.5 <R> <C> [BOLD] Error Rate Reduction <C> [BOLD] 34.3 <C> [BOLD] 26.3 <CAP> Table 2: Improvements of our proposal on low-resource datasets: PKU and CTB.
<R> <C> [EMPTY] <C> TB-Dense <C> MATRES <R> <C> [BOLD] # of Documents <C> [BOLD] # of Documents <C> [BOLD] # of Documents <R> <C> Train <C> 22 <C> 183 <R> <C> Dev <C> 5 <C> - <R> <C> Test <C> 9 <C> 20 <R> <C> [BOLD] # of Pairs <C> [BOLD] # of Pairs <C> [BOLD] # of Pairs <R> <C> Train <C> 4032 <C> 6332 <R> <C> Dev <C> 629 <C> - <R> <C> Test <C> 1427 <C> 827 <CAP> Table 1: Data overview. Note that the numbers reported for MATRES do not include the AQUAINT section.
<R> <C> [BOLD] Micro-average <C> [BOLD] TB-Dense <C> [BOLD] MATRES <R> <C> No Structure <C> 48.5 <C> 58.5 <R> <C> + ER Consistency <C> 49.4 <C> 59.5 <R> <C> + Transitivity <C> 49.4 <C> 59.6 <CAP> Table 6: Ablation Study on Global Constraints
<R> <C> [BOLD] Model <C> [BOLD] f-score <R> <C> Yoshikawa et al. ( 2016 ) <C> 62.5 <R> <C> Johnson and Charniak ( 2004 ) <C> 79.7 <R> <C> Johnson et al. ( 2004 ) <C> 81.0 <R> <C> Rasooli and Tetreault ( 2013 ) <C> 81.4 <R> <C> Qian and Liu ( 2013 ) <C> 82.1 <R> <C> Honnibal and Johnson ( 2014 ) <C> 84.1 <R> <C> Ferguson et al. ( 2015 )  [BOLD] * <C> 85.4 <R> <C> Zwarts and Johnson ( 2011 ) <C> 85.7 <R> <C> Zayats et al. ( 2016 )  [BOLD] * <C> 85.9 <R> <C> [BOLD] LSTM-NCM <C> [BOLD] 86.8 <CAP> Table 6: Comparison of the LSTM-NCM to state-of-the-art methods on the dev set. *Models have used richer input.
<R> <C> Methods <C> IMDB <C> Yahoo <R> <C> CNN <C> 34.1 <C> 71.2 <R> <C> Conv-GRNN <C> 42.5 <C> - <R> <C> LSTM-GRNN <C> 45.3 <C> - <R> <C> HAN-AVE <C> 47.8 <C> 75.2 <R> <C> HAN-MAX <C> 48.2 <C> 75.2 <R> <C> HAN-ATT <C> 49.4 <C> 75.8 <R> <C> [BOLD] MB <C> 50.8 <C> 76.8 <R> <C> [BOLD] AMB <C> [BOLD] 51.9 <C> [BOLD] 77.5 <CAP> Table 1. The experimental results on IMDB reviews and Yahoo answers datasets, in percentage. MB stands for multi-binary model and AMB stands for adversarial multi-binary model.
<R> <C> Length <C> 0 - 10 <C> 10 - 20 <C> 20 - 30 <C> 30 - 40 <C> > 40 <R> <C> Ex. <C> 115 <C> 573 <C> 613 <C> 295 <C> 94 <R> <C> % Ex. improved <C> 20% <C> 36.8% <C> 49.7% <C> 52.8% <C> 55.3% <CAP> Table 3: Percentage of development examples improved by SS-PRPN in comparison to PRPN, listed by sentence length.
<R> <C> [EMPTY] <C> Visual Turing Test <C> Visual Turing Test <C> Visual Turing Test <C> Human Rated Scores <C> Human Rated Scores <C> Human Rated Scores <C> Human Rated Scores <R> <C> [EMPTY] <C> Pass <C> Fail <C> Pass Rate (%) <C> 2 <C> 1 <C> 0 <C> Avg. Score <R> <C> Human <C> 948 <C> 52 <C> 94.8 <C> 927 <C> 64 <C> 9 <C> 1.918 <R> <C> blind-QA <C> 340 <C> 660 <C> 34.0 <C> - <C> - <C> - <C> - <R> <C> mQA <C> 647 <C> 353 <C> 64.7 <C> 628 <C> 198 <C> 174 <C> 1.454 <CAP> Table 1: The results of our mQA model for our FM-IQA dataset.
<R> <C> [EMPTY] <C> all <C> y/n <C> num <C> unans <C> other <R> <C> VizWiz <C> 46.9 <C> 59.6 <C> 21.0 <C> 80.5 <C> 27.3 <R> <C> BAN <C> 51.6 <C> [BOLD] 68.1 <C> 17.9 <C> [BOLD] 85.3 <C> 31.5 <R> <C> Ours (FRCNN) <C> 51.9 <C> 66.7 <C> 24.3 <C> 85.0 <C> 32.1 <R> <C> Ours (Ultra) <C> [BOLD] 53.7 <C> [BOLD] 68.1 <C> [BOLD] 28.8 <C> 84.0 <C> [BOLD] 35.4 <CAP> Table 2: Accuracy (%) on the test-standard split for the VQA task on the VizWiz dataset. Additionally, we provide accuracy per answer type: yes/no (y/n), number (num), unanswerable (unans), and the rest (other). The baselines include VizWiz Gurari et al. (2018) and BAN Kim et al. (2018).
<R> <C> conditioned <C> conditioned <C> [BOLD] dev 12.50 <C> [BOLD] test 11.71 <R> <C> [EMPTY] <C> + oracle  [ITALIC] π∗ [ITALIC] β <C> 19.71 <C> 19.51 <R> <C> joint <C> joint <C> 12.86 <C> 11.97 <R> <C> [EMPTY] <C> + oracle  [ITALIC] π∗ [ITALIC] β <C> 16.78 <C> 16.32 <CAP> Table 2: Overall BLEU, along with scores using oracle buffer index sequences.
<R> <C> [EMPTY] <C> [EMPTY] <C> oWN18RR MRR <C> oWN18RR Hit@ <C> oWN18RR Hit@ <C> oWN18RR Hit@ <C> oFB15k-237 MRR <C> oFB15k-237 Hit@ <C> oFB15k-237 Hit@ <C> oFB15k-237 Hit@ <R> <C> Model <C> Training <C> Filtered <C> 1 <C> 3 <C> 10 <C> Filtered <C> 1 <C> 3 <C> 10 <R> <C> Popularity <C> Algorithm  1 <C> 0.0094 <C> 0.0030 <C> 0.0076 <C> 0.0215 <C> 0.0320 <C> 0.0168 <C> 0.0322 <C> 0.0581 <R> <C> OOV <C> Algorithm  1 <C> 0.0004 <C> 0.0000 <C> 0.0001 <C> 0.0002 <C> 0.0002 <C> 0.0000 <C> 0.0000 <C> 0.0001 <R> <C> RGCN-D <C> Algorithm  1 <C> 0.0178 <C> 0.0072 <C> 0.0166 <C> 0.0352 <C> 0.1683 <C> 0.0974 <C> 0.1848 <C> 0.3056 <R> <C> DistMult-EAvg <C> Algorithm  1 <C> 0.0446 <C> 0.0248 <C> 0.0469 <C> 0.0841 <C> 0.0813 <C> 0.0525 <C> 0.0973 <C> 0.1327 <R> <C> DistMult-ERAvg <C> Algorithm  1 <C> 0.3048 <C> 0.2468 <C> 0.3331 <C> 0.4159 <C> 0.2456 <C> 0.1615 <C> 0.2769 <C> 0.4082 <R> <C> DistMult-LS <C> Algorithm  1 <C> 0.3514 <C> 0.2840 <C> 0.3911 <C> 0.4756 <C> 0.2073 <C> 0.1395 <C> 0.2264 <C> 0.3375 <R> <C> DistMult-LS-U <C> Algorithm  1 <C> 0.3238 <C> 0.2458 <C> 0.3693 <C> 0.4717 <C> 0.1674 <C> 0.1099 <C> 0.1858 <C> 0.2732 <R> <C> oDistMult-EAvg <C> Algorithm  2 <C> 0.2239 <C> 0.1315 <C> 0.2724 <C> 0.3897 <C> 0.1765 <C> 0.0724 <C> 0.2076 <C> 0.4012 <R> <C> oDistMult-ERAvg <C> Algorithm  2 <C> 0.3904 <C> 0.3460 <C> 0.4125 <C> 0.4725 <C> [BOLD] 0.2557 <C> [BOLD] 0.1698 <C> [BOLD] 0.2885 <C> [BOLD] 0.4201 <R> <C> oDistMult-LS <C> Algorithm  2 <C> [BOLD] 0.4093 <C> [BOLD] 0.3643 <C> [BOLD] 0.4371 <C> [BOLD] 0.4892 <C> 0.2126 <C> 0.1232 <C> 0.2404 <C> 0.3954 <CAP> Table 2: Results on oWN18RR and oFB15k-237. Best results are in bold.
<R> <C> Model <C> MSE <R> <C> Ridge regression <C> 0.509 <R> <C> GRU-RNN <C> 0.351 <CAP> Table 2: Comparison between ridge regression and our GRU-RNN for predicting economic conditions.
<R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> Binary ES <C> Binary CA <C> Binary EU <C> 4-class ES <C> 4-class CA <C> 4-class EU <R> <C> Upper Bounds <C> Mono <C> P <C> 75.0 <C> 79.0 <C> 74.0 <C> 55.2 <C> 50.0 <C> 48.3 <R> <C> Upper Bounds <C> Mono <C> R <C> 72.3 <C> 79.6 <C> 67.4 <C> 42.8 <C> 50.9 <C> 46.5 <R> <C> Upper Bounds <C> Mono <C> F1 <C> 73.5 <C> 79.2 <C> 69.8 <C> 45.5 <C> 49.9 <C> 47.1 <R> <C> Upper Bounds <C> Mt <C> P <C> [BOLD] 82.3 <C> 78.0 <C> 75.6 <C> 51.8 <C> [BOLD] 58.9 <C> 43.6 <R> <C> Upper Bounds <C> Mt <C> R <C> 76.6 <C> 76.8 <C> 66.5 <C> 48.5 <C> 50.5 <C> 45.2 <R> <C> Upper Bounds <C> Mt <C> F1 <C> 79.0 <C> 77.2 <C> 69.4 <C> 48.8 <C> 52.7 <C> 43.6 <R> <C> Upper Bounds <C> [BOLD] BLSE <C> P <C> 72.1 <C> [ITALIC] **72.8 <C> [ITALIC] **67.5 <C> [ITALIC]  [BOLD] **60.0 <C> [ITALIC] 38.1 <C> [ITALIC] *42.5 <R> <C> [EMPTY] <C> [BOLD] BLSE <C> R <C> [ITALIC]  [BOLD] **80.1 <C> [ITALIC] **73.0 <C> [ITALIC]  [BOLD] **72.7 <C> [ITALIC] *43.4 <C> 38.1 <C> [ITALIC] 37.4 <R> <C> [EMPTY] <C> [BOLD] BLSE <C> F1 <C> [ITALIC] **74.6 <C> [ITALIC] **72.9 <C> [ITALIC] **69.3 <C> [ITALIC] *41.2 <C> 35.9 <C> 30.0 <R> <C> Baselines <C> Artetxe <C> P <C> 75.0 <C> 60.1 <C> 42.2 <C> 40.1 <C> 21.6 <C> 30.0 <R> <C> Baselines <C> Artetxe <C> R <C> 64.3 <C> 61.2 <C> 49.5 <C> 36.9 <C> 29.8 <C> 35.7 <R> <C> Baselines <C> Artetxe <C> F1 <C> 67.1 <C> 60.7 <C> 45.6 <C> 34.9 <C> 23.0 <C> 21.3 <R> <C> Baselines <C> Barista <C> P <C> 64.7 <C> 65.3 <C> 55.5 <C> 44.1 <C> 36.4 <C> 34.1 <R> <C> Baselines <C> Barista <C> R <C> 59.8 <C> 61.2 <C> 54.5 <C> 37.9 <C> 38.5 <C> 34.3 <R> <C> Baselines <C> Barista <C> F1 <C> 61.2 <C> 60.1 <C> 54.8 <C> 39.5 <C> 36.2 <C> 33.8 <R> <C> Ensemble <C> Artetxe <C> P <C> 65.3 <C> 63.1 <C> 70.4 <C> 43.5 <C> 46.5 <C> 50.1 <R> <C> Ensemble <C> Artetxe <C> R <C> 61.3 <C> 63.3 <C> 64.3 <C> 44.1 <C> 48.7 <C> 50.7 <R> <C> Ensemble <C> Artetxe <C> F1 <C> 62.6 <C> 63.2 <C> 66.4 <C> 43.8 <C> 47.6 <C> 49.9 <R> <C> Ensemble <C> Barista <C> P <C> 60.1 <C> 63.4 <C> 50.7 <C> 48.3 <C> 52.8 <C> 50.8 <R> <C> Ensemble <C> Barista <C> R <C> 55.5 <C> 62.3 <C> 50.4 <C> 46.6 <C> 53.7 <C> 49.8 <R> <C> Ensemble <C> Barista <C> F1 <C> 56.0 <C> 62.5 <C> 49.8 <C> 47.1 <C> 53.0 <C> 47.8 <R> <C> Ensemble <C> BLSE <C> P <C> 79.5 <C> [BOLD] 84.7 <C> [BOLD] 80.9 <C> 49.5 <C> 54.1 <C> [BOLD] 50.3 <R> <C> Ensemble <C> BLSE <C> R <C> [BOLD] 78.7 <C> [BOLD] 85.5 <C> 69.9 <C> [BOLD] 51.2 <C> [BOLD] 53.9 <C> [BOLD] 51.4 <R> <C> Ensemble <C> BLSE <C> F1 <C> [BOLD] 80.3 <C> [BOLD] 85.0 <C> [BOLD] 73.5 <C> [BOLD] 50.3 <C> [BOLD] 53.9 <C> [BOLD] 50.5 <CAP> Table 3: Precision (P), Recall (R), and macro F1 of four models trained on English and tested on Spanish (ES), Catalan (CA), and Basque (EU). The bold numbers show the best results for each metric per column and the highlighted numbers show where Blse is better than the other projection methods, Artetxe and Barista (** p < 0.01, * p < 0.05).
<R> <C> [EMPTY] <C> SQuAD <C> HotpotQA Distr. <C> HotpotQA SP <C> bAbI <R> <C> Baseline <C> 77.2 <C> 66.0 <C> 66.0 <C> 42.0 <R> <C> BERT <C> 87.9 <C> 56.8 <C> 80.4 <C> 93.4 <R> <C> GPT-2 <C> 74.9 <C> 54.0 <C> 64.6 <C> 99.9 <CAP> Table 2. Results from fine-tuning BERT on QA tasks. Baselines are: BIDAF (Seo et al., ) for SQuAD, the LSTM Baseline for bAbI from (Weston et al., 2016) and the HotpotQA baseline from (Yang et al., 2018) for the two Hotpot tasks.
<R> <C> [EMPTY] <C> Medication <C> Procedure <C> Lab <R> <C> Sleep apnea <C> 0.00 <C> 0.67 <C> 1.00 <R> <C> Hypokalemia <C> 0.17 <C> 0.60 <C> 0.55 <R> <C> Thrombocytopenia <C> 0.21 <C> 0.50 <C> 0.44 <R> <C> Hypertension <C> 0.60 <C> 0.80 <C> 1.00 <R> <C> UTI <C> 0.82 <C> 0.80 <C> 0.97 <CAP> Table 4: Hits@5 values for held-out problems, by data type.
<R> <C> [EMPTY] <C> unique P@0.25 <C> unique P@0.5 <C> multiple P@0.25 <C> multiple P@0.5 <C> overall P@0.25 <C> overall P@0.5 <R> <C> OracleCatRand <C> 100 <C> 100 <C> 18.09 <C> 17.84 <C> 29.99 <C> 29.76 <R> <C> PointRefNet  <C> 16.83 <C> 12.85 <C> 7.82 <C> 4.71 <C> 9.16 <C> 5.92 <R> <C> VoteNetRand  <C> 40.88 <C> 23.04 <C> 6.83 <C> 3.35 <C> 11.90 <C> 6.28 <R> <C> SCRC  <C> 24.03 <C> 9.22 <C> 17.77 <C> 5.97 <C> 18.70 <C> 6.45 <R> <C> Ours (xyz) <C> 45.54 <C> 29.37 <C> 19.76 <C> 11.73 <C> 24.02 <C> 14.64 <R> <C> Ours (xyz+rgb) <C> 45.63 <C> 30.35 <C> 21.18 <C> 12.67 <C> 25.22 <C> 15.59 <R> <C> Ours (xyz+rgb+normals) <C> 47.57 <C> 31.55 <C> 20.64 <C> 12.31 <C> 25.09 <C> 15.49 <R> <C> Ours (xyz+multiview) <C> 50.78 <C> 33.37 <C> 18.71 <C> 11.22 <C> 24.01 <C> 14.89 <R> <C> Ours (xyz+multiview+normals) <C> 50.67 <C> 32.29 <C> 20.75 <C> 12.35 <C> 25.69 <C> 15.65 <R> <C> Ours (xyz+lobjcls) <C> 51.97 <C> 35.43 <C> 21.30 <C> 12.35 <C> 26.38 <C> 16.17 <R> <C> Ours (xyz+rgb+lobjcls) <C> 53.75 <C> 37.47 <C> 21.03 <C> 12.83 <C> 26.44 <C> 16.90 <R> <C> Ours (xyz+rgb+normals+lobjcls) <C> 54.96 <C> 35.24 <C> 22.27 <C> 13.55 <C> 27.67 <C> 17.13 <R> <C> Ours (xyz+multiview+lobjcls) <C> [BOLD] 56.68 <C> 33.78 <C> 21.35 <C> [BOLD] 14.54 <C> 27.19 <C> 17.72 <R> <C> Ours (xyz+multiview+normals+lobjcls) <C> 55.09 <C> [BOLD] 37.66 <C> [BOLD] 22.93 <C> 13.94 <C> [BOLD] 28.25 <C> [BOLD] 17.85 <CAP> Table 2: Comparison of the localization results in percentage obtained by the ScanRefer and the baseline models. We also report the scores on the “unique” and “multiple” subsets, where the target object is the unique one in the scene and there are multiple objects similar to the target, respectively. Our method outperforms the baselines on all metrics by a large margin.
<R> <C> [EMPTY] <C> cab. <C> bed <C> chair <C> sofa <C> tabl. <C> door <C> wind. <C> bkshf. <C> pic. <C> cntr. <C> desk <C> curt. <C> fridg. <C> showr. <C> toil. <C> sink <C> bath. <C> others <C> mAP <R> <C> [0] <C> 4.77 <C> 85.51 <C> 64.42 <C> 72.74 <C> 30.39 <C> 11.17 <C> 6.62 <C> 17.32 <C> 0.35 <C> 2.16 <C> 35.79 <C> 7.80 <C> 16.69 <C> 16.96 <C> 76.74 <C> 16.77 <C> 69.57 <C> 5.68 <C> 30.08 <R> <C> [1] <C> 9.93 <C> [BOLD] 88.43 <C> 67.12 <C> 69.44 <C> [BOLD] 39.76 <C> 12.20 <C> 5.11 <C> 20.27 <C> 0.02 <C> 9.27 <C> 41.52 <C> 16.10 <C> [BOLD] 30.79 <C> 5.77 <C> 77.32 <C> 14.93 <C> 61.02 <C> 7.82 <C> 32.05 <R> <C> [2] <C> 7.01 <C> 88.01 <C> 67.13 <C> 73.69 <C> 32.87 <C> 12.36 <C> 9.01 <C> 17.61 <C> 0.31 <C> 9.27 <C> 44.78 <C> 16.25 <C> 20.29 <C> 3.55 <C> 76.50 <C> 12.33 <C> 72.24 <C> 8.08 <C> 31.74 <R> <C> [3] <C> 11.16 <C> 87.20 <C> [BOLD] 70.58 <C> 75.17 <C> 36.76 <C> 11.47 <C> 6.72 <C> 13.40 <C> 1.09 <C> 7.08 <C> 48.38 <C> 11.64 <C> 19.96 <C> 4.29 <C> 85.29 <C> [BOLD] 18.20 <C> 72.83 <C> [BOLD] 10.74 <C> 32.89 <R> <C> [4] <C> 7.22 <C> 87.72 <C> 67.24 <C> 72.42 <C> 33.66 <C> 11.55 <C> 8.80 <C> 20.16 <C> 0.14 <C> [BOLD] 9.82 <C> 46.07 <C> 15.91 <C> 22.48 <C> 2.67 <C> 77.82 <C> 13.17 <C> 68.14 <C> 8.01 <C> 31.83 <R> <C> [5] <C> [BOLD] 12.74 <C> 83.91 <C> 69.94 <C> 72.17 <C> 36.11 <C> [BOLD] 13.38 <C> 8.42 <C> 17.52 <C> [BOLD] 1.99 <C> 6.58 <C> [BOLD] 46.65 <C> [BOLD] 17.65 <C> 24.04 <C> [BOLD] 31.30 <C> 75.99 <C> 10.31 <C> 61.92 <C> 9.78 <C> [BOLD] 33.36 <R> <C> [6] <C> 10.53 <C> 84.00 <C> 63.48 <C> [BOLD] 75.27 <C> 30.62 <C> 7.78 <C> 8.45 <C> 18.08 <C> 1.18 <C> 5.47 <C> 39.27 <C> 10.14 <C> 18.83 <C> 8.93 <C> 69.99 <C> 9.36 <C> [BOLD] 75.59 <C> 7.97 <C> 30.27 <R> <C> [7] <C> 11.11 <C> 85.63 <C> 67.81 <C> 71.04 <C> 34.96 <C> 9.54 <C> 6.22 <C> 16.37 <C> 1.67 <C> 6.28 <C> 36.07 <C> 12.93 <C> 17.40 <C> 7.46 <C> 68.74 <C> 11.77 <C> 65.69 <C> 7.71 <C> 29.91 <R> <C> [8] <C> 10.72 <C> 86.71 <C> 69.86 <C> 72.77 <C> 32.60 <C> 16.33 <C> 8.16 <C> 19.64 <C> 1.14 <C> 7.08 <C> 42.21 <C> 14.31 <C> 22.99 <C> 6.92 <C> [BOLD] 86.09 <C> 8.06 <C> 65.51 <C> 8.79 <C> 32.22 <R> <C> [9] <C> 9.76 <C> 87.93 <C> 65.93 <C> 72.59 <C> 31.60 <C> 9.48 <C> 9.05 <C> [BOLD] 23.86 <C> 0.37 <C> 6.69 <C> 42.22 <C> 13.86 <C> 21.42 <C> 16.35 <C> 80.41 <C> 12.30 <C> 57.80 <C> 7.40 <C> 31.61 <R> <C> [10] <C> 8.92 <C> 88.20 <C> 70.37 <C> 73.93 <C> 32.89 <C> 10.54 <C> [BOLD] 9.21 <C> 14.05 <C> 0.48 <C> 6.91 <C> 44.74 <C> 6.54 <C> 17.76 <C> 27.64 <C> 81.18 <C> 12.86 <C> 62.40 <C> 9.06 <C> 32.09 <CAP> Table 3: [0] VoteNet [45], [1] Ours (xyz), [2] Ours (xyz+rgb), [3] Ours (xyz+rgb+normals), [4] Ours (xyz+multiview), [5] Ours (xyz+multiview+normals), [6] Ours (xyz+lobjcls), [7] Ours (xyz+rgb+lobjcls), [8] Ours (xyz+rgb+normals+lobjcls), [9] Ours (xyz+multiview+lobjcls), [10] Ours (xyz+multiview+normals+lobjcls). Training with the point normals (compare rows [3,5] to rows [2,4]) and multiview features (compare rows [4,5] to rows [2,3]) clearly leads to a better performance. As expected, the models equipped with language-based object classifier (rows [6-10]) does not results in better object detection compared to the models without such a module (rows [1-5]).
<R> <C> [EMPTY] <C> [EMPTY] <C> English  [ITALIC] Acc. <C> English  [ITALIC] F <C> Thai  [ITALIC] Acc. <C> Thai  [ITALIC] F <C> German  [ITALIC] Acc. <C> German  [ITALIC] F <C> Greek  [ITALIC] Acc. <C> Greek  [ITALIC] F <C> Chinese  [ITALIC] Acc. <C> Chinese  [ITALIC] F <C> Indonesian  [ITALIC] Acc. <C> Indonesian  [ITALIC] F <C> Swedish  [ITALIC] Acc. <C> Swedish  [ITALIC] F <C> Farsi  [ITALIC] Acc. <C> Farsi  [ITALIC] F <R> <C> Wasp <C> [EMPTY] <C> 71.1 <C> 77.7 <C> 71.4 <C> 75.0 <C> 65.7 <C> 74.9 <C> 70.7 <C> 78.6 <C> 48.2 <C> 51.6 <C> 74.6 <C> [BOLD] 79.8 <C> 63.9 <C> 71.5 <C> 46.8 <C> 54.1 <R> <C> HT-g <C> [EMPTY] <C> 76.8 <C> 81.0 <C> 73.6 <C> 76.7 <C> 62.1 <C> 68.5 <C> 69.3 <C> 74.6 <C> 56.1 <C> 58.4 <C> 66.4 <C> 72.8 <C> 61.4 <C> 70.5 <C> 51.8 <C> 58.6 <R> <C> UBL-s <C> [EMPTY] <C> 82.1 <C> 82.1 <C> 66.4 <C> 66.4 <C> 75.0 <C> 75.0 <C> 73.6 <C> 73.7 <C> 63.8 <C> 63.8 <C> 73.8 <C> 73.8 <C> 78.1 <C> 78.1 <C> 64.4 <C> 64.4 <R> <C> TreeTrans <C> [EMPTY] <C> 79.3 <C> 79.3 <C> 78.2 <C> 78.2 <C> 74.6 <C> 74.6 <C> 75.4 <C> 75.4 <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> Seq2Tree† <C> [EMPTY] <C> 84.5 <C> - <C> 71.9 <C> - <C> 70.3 <C> - <C> 73.1 <C> - <C> 73.3 <C> - <C> [BOLD] 80.7 <C> - <C> [BOLD] 80.8 <C> - <C> 70.5 <C> - <R> <C> SL-Single † <C> [EMPTY] <C> 83.5 <C> - <C> 72.1 <C> - <C> 69.3 <C> - <C> 74.2 <C> - <C> 74.9 <C> - <C> 79.8 <C> - <C> 77.5 <C> - <C> 72.2 <C> - <R> <C> HT-d <C> [EMPTY] <C> [BOLD] 86.8 <C> [BOLD] 86.8 <C> 80.7 <C> 80.7 <C> [BOLD] 75.7 <C> [BOLD] 75.7 <C> 79.3 <C> 79.3 <C> 76.1 <C> 76.1 <C> 75.0 <C> 75.0 <C> 79.3 <C> [BOLD] 79.3 <C> 73.9 <C> 73.9 <R> <C> HT-d (+o) <C> [EMPTY] <C> 86.1 <C> 86.1 <C> [BOLD] 81.1 <C> [BOLD] 81.1 <C> 73.6 <C> 73.6 <C> [BOLD] 81.4 <C> [BOLD] 81.4 <C> [BOLD] 77.9 <C> [BOLD] 77.9 <C> 79.6 <C> 79.6 <C> 79.3 <C> [BOLD] 79.3 <C> [BOLD] 75.7 <C> [BOLD] 75.7 <R> <C> HT-d (nn) <C> [ITALIC] J=0 <C> 87.9 <C> 87.9 <C> 82.1 <C> 82.1 <C> 75.7 <C> 75.7 <C> 81.1 <C> 81.1 <C> 76.8 <C> 76.8 <C> 76.1 <C> 76.1 <C> 81.1 <C> 81.1 <C> 75.0 <C> 75.0 <R> <C> HT-d (nn) <C> [ITALIC] J=1 <C> 88.6 <C> 88.6 <C> 84.6 <C> 84.6 <C> [BOLD] 76.8 <C> [BOLD] 76.8 <C> 79.6 <C> 79.6 <C> 75.4 <C> 75.4 <C> 78.6 <C> 78.6 <C> 82.9 <C> 82.9 <C> 76.1 <C> 76.1 <R> <C> HT-d (nn) <C> [ITALIC] J=2 <C> [BOLD] 90.0 <C> [BOLD] 90.0 <C> 82.1 <C> 82.1 <C> 73.9 <C> 73.9 <C> 80.7 <C> 80.7 <C> 81.1 <C> 81.1 <C> 81.8 <C> 81.8 <C> [BOLD] 83.9 <C> [BOLD] 83.9 <C> 74.6 <C> 74.6 <R> <C> HT-d (nn+o) <C> [ITALIC] J=0 <C> 86.1 <C> 86.1 <C> 83.6 <C> 83.6 <C> 73.9 <C> 73.9 <C> 82.1 <C> 82.1 <C> 77.9 <C> 77.9 <C> 81.1 <C> 81.1 <C> 82.1 <C> 82.1 <C> 74.6 <C> 74.6 <R> <C> HT-d (nn+o) <C> [ITALIC] J=1 <C> 86.1 <C> 86.1 <C> [BOLD] 86.1 <C> [BOLD] 86.1 <C> 72.5 <C> 72.5 <C> 80.4 <C> 80.4 <C> 81.4 <C> 81.4 <C> 82.5 <C> 82.5 <C> 82.5 <C> 82.5 <C> 75.7 <C> 75.7 <R> <C> HT-d (nn+o) <C> [ITALIC] J=2 <C> 89.6 <C> 89.6 <C> 84.6 <C> 84.6 <C> 72.1 <C> 72.1 <C> [BOLD] 83.2 <C> [BOLD] 83.2 <C> [BOLD] 82.1 <C> [BOLD] 82.1 <C> [BOLD] 83.9 <C> [BOLD] 83.9 <C> 83.6 <C> 83.6 <C> [BOLD] 76.8 <C> [BOLD] 76.8 <CAP> Table 2: Performance on multilingual datasets. Acc.: accuracy (%), F: F1-measure (%). +o: including distributed representations for semantic units as features. († indicates systems that make use of lambda calculus expressions as meaning representations.)
<R> <C> [BOLD] Model <C> Cornell Total <C> Cornell  [BOLD] zconv <C> Cornell  [BOLD] zutt <C> Ubuntu Total <C> Ubuntu  [BOLD] zconv <C> Ubuntu  [BOLD] zutt <R> <C> VHRED <C> 0.351 <C> - <C> 0.351 <C> 0.461 <C> - <C> 0.461 <R> <C> VHCR <C> 0.503 <C> 0.189 <C> 0.314 <C> 0.756 <C> 0.198 <C> 0.558 <CAP> Table 2: KL divergence decomposition.
<R> <C> Opponent <C> Wins <C> Cornell Losses <C> Ties <C> Wins <C> Ubuntu Losses <C> Ties <R> <C> VHCR vs HRED <C> [BOLD] 28.5± [BOLD] 1.9 <C> 28.2±1.9 <C> 43.3±2.1 <C> [BOLD] 52.9± [BOLD] 2.1 <C> 42.2±2.1 <C> 4.9±0.9 <R> <C> VHCR vs VHRED + w.d <C> [BOLD] 29.9± [BOLD] 1.9 <C> 28.0±1.9 <C> 42.1±2.1 <C> [BOLD] 48.1± [BOLD] 2.1 <C> 40.1±3.6 <C> 11.9±1.4 <R> <C> VHCR vs VHRED + bow <C> [BOLD] 31.3± [BOLD] 2.0 <C> 26.9±1.9 <C> 41.7±2.1 <C> [BOLD] 46.1± [BOLD] 2.1 <C> 39.9±2.1 <C> 14.0±1.5 <CAP> Table 4: Results of human evaluation via AMT. Human turkers are asked to choose which response is more appropriate in a given context, without knowing which algorithms generate which responses. For each pair of models, we carry out three evaluation batches, each of which consists of 100 random test samples evaluated by five unique humans. We report mean preferences with ±90% confidence interval.
<R> <C> [BOLD] Task <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F-score <C> [BOLD] Approach <R> <C> [EMPTY] <C> 65.73 <C> 44.72 <C> 53.23 <C> TEES 4CNN <R> <C> [EMPTY] <C> 65.01 <C> [BOLD] 46.83 <C> [BOLD] 54.44 <C> Proposed 4MHA <R> <C> [BOLD] GE09 <C> 64.37 <C> 45.19 <C> 53.10 <C> Proposed 1MHA <R> <C> [EMPTY] <C> 61.99 <C> 45.51 <C> 52.48 <C> Proposed 4CNN-4MHA <R> <C> [EMPTY] <C> [BOLD] 65.98 <C> 45.60 <C> 53.93 <C> Proposed 4MHA-4CNN <R> <C> [EMPTY] <C> 66.09 <C> 46.62 <C> 54.68 <C> TEES 4CNN <R> <C> [EMPTY] <C> 66.19 <C> 48.67 <C> 56.09 <C> Proposed 4MHA <R> <C> [BOLD] GE11 <C> 66.26 <C> 48.60 <C> 56.07 <C> Proposed 1MHA <R> <C> [EMPTY] <C> [BOLD] 67.07 <C> 47.61 <C> 55.69 <C> Proposed 4CNN-4MHA <R> <C> [EMPTY] <C> 66.12 <C> [BOLD] 49.34 <C> [BOLD] 56.51 <C> Proposed 4MHA-4CNN <R> <C> [EMPTY] <C> 63.31 <C> 46.73 <C> 53.78 <C> TEES 4CNN <R> <C> [EMPTY] <C> 63.71 <C> [BOLD] 50.73 <C> 56.48 <C> Proposed 4MHA <R> <C> [BOLD] EPI11 <C> [BOLD] 66.38 <C> 49.85 <C> [BOLD] 56.94 <C> Proposed 1MHA <R> <C> [EMPTY] <C> 63.60 <C> 45.72 <C> 53.20 <C> Proposed 4CNN-4MHA <R> <C> [EMPTY] <C> 65.43 <C> 48.55 <C> 55.74 <C> Proposed 4MHA-4CNN <R> <C> [EMPTY] <C> 70.14 <C> 44.36 <C> 54.35 <C> TEES 4CNN <R> <C> [EMPTY] <C> 66.63 <C> [BOLD] 48.65 <C> 56.24 <C> Proposed 4MHA <R> <C> [BOLD] ID11 <C> [BOLD] 71.64 <C> 46.99 <C> [BOLD] 56.75 <C> Proposed 1MHA <R> <C> [EMPTY] <C> 68.92 <C> 41.04 <C> 51.44 <C> Proposed 4CNN-4MHA <R> <C> [EMPTY] <C> 69.05 <C> 44.91 <C> 54.43 <C> Proposed 4MHA-4CNN <R> <C> [EMPTY] <C> 71.26 <C> 62.37 <C> 66.52 <C> TEES 4CNN <R> <C> [EMPTY] <C> 71.56 <C> 63.78 <C> 67.45 <C> Proposed 4MHA <R> <C> [BOLD] REL11 <C> 68.55 <C> 64.39 <C> 66.40 <C> Proposed 1MHA <R> <C> [EMPTY] <C> 71.02 <C> 55.53 <C> 62.33 <C> Proposed 4CNN-4MHA <R> <C> [EMPTY] <C> [BOLD] 71.91 <C> [BOLD] 65.39 <C> [BOLD] 68.50 <C> Proposed 4MHA-4CNN <R> <C> [EMPTY] <C> [BOLD] 62.22 <C> 39.96 <C> 48.66 <C> TEES 4CNN <R> <C> [EMPTY] <C> 60.68 <C> 40.35 <C> 48.47 <C> Proposed 4MHA <R> <C> [BOLD] GE13 <C> 60.21 <C> 40.75 <C> 48.60 <C> Proposed 1MHA <R> <C> [EMPTY] <C> 58.14 <C> 37.66 <C> 45.71 <C> Proposed 4CNN-4MHA <R> <C> [EMPTY] <C> 59.76 <C> [BOLD] 41.65 <C> [BOLD] 49.09 <C> Proposed 4MHA-4CNN <R> <C> [EMPTY] <C> 66.08 <C> 49.05 <C> 56.30 <C> TEES 4CNN <R> <C> [EMPTY] <C> 65.92 <C> [BOLD] 53.50 <C> [BOLD] 59.06 <C> Proposed 4MHA <R> <C> [BOLD] CG13 <C> [BOLD] 67.02 <C> 52.49 <C> 58.87 <C> Proposed 1MHA <R> <C> [EMPTY] <C> 61.91 <C> 48.02 <C> 54.09 <C> Proposed 4CNN-4MHA <R> <C> [EMPTY] <C> 65.47 <C> 51.71 <C> 57.78 <C> Proposed 4MHA-4CNN <R> <C> [EMPTY] <C> [BOLD] 63.49 <C> 43.37 <C> 51.54 <C> TEES 4CNN <R> <C> [EMPTY] <C> 59.45 <C> [BOLD] 49.90 <C> 54.26 <C> Proposed 4MHA <R> <C> [BOLD] PC13 <C> 60.64 <C> 47.25 <C> 53.11 <C> Proposed 1MHA <R> <C> [EMPTY] <C> 57.61 <C> 43.23 <C> 49.39 <C> Proposed 4CNN-4MHA <R> <C> [EMPTY] <C> 60.51 <C> 49.43 <C> [BOLD] 54.41 <C> Proposed 4MHA-4CNN <R> <C> [EMPTY] <C> 73.00 <C> 45.00 <C> 56.00 <C> TEES 4CNN <R> <C> [EMPTY] <C> 70.00 <C> [BOLD] 58.00 <C> [BOLD] 63.00 <C> Proposed 4MHA <R> <C> [BOLD] CP17 <C> [BOLD] 77.00 <C> 48.00 <C> 58.00 <C> Proposed 1MHA <R> <C> [EMPTY] <C> [BOLD] 77.00 <C> 44.00 <C> 56.00 <C> Proposed 4CNN-4MHA <R> <C> [EMPTY] <C> 75.00 <C> 50.00 <C> 60.00 <C> Proposed 4MHA-4CNN <R> <C> [EMPTY] <C> 84.41 <C> 87.52 <C> 85.90 <C> TEES 4CNN <R> <C> [EMPTY] <C> 83.73 <C> 88.51 <C> 86.01 <C> Proposed 4MHA <R> <C> [BOLD] AMIA <C> 85.12 <C> 89.50 <C> 87.31 <C> Proposed 1MHA <R> <C> [EMPTY] <C> 85.02 <C> 89.01 <C> 87.00 <C> Proposed 4CNN-4MHA <R> <C> [EMPTY] <C> [BOLD] 85.21 <C> [BOLD] 90.11 <C> [BOLD] 87.53 <C> Proposed 4MHA-4CNN <CAP> Table 2: Precision, Recall and F-score, measured on the corpora of various shared tasks for our models, and the state of the art. The best scores (the first and the second highest scores) for each task are bolded and highlighted, respectively. All the results (except those of CP17 and AMIA) are evaluated using the official evaluation program/server of each task.
<R> <C> [EMPTY] <C> [BOLD] MARCO Dev  [BOLD] Passage Retrieval <C> [BOLD] MARCO Dev  [BOLD] Passage Retrieval <C> [BOLD] TREC DL Passage  [BOLD] NDCG@10 <C> [BOLD] TREC DL Passage  [BOLD] NDCG@10 <C> [BOLD] TREC DL Document  [BOLD] NDCG@10 <C> [BOLD] TREC DL Document  [BOLD] NDCG@10 <R> <C> [EMPTY] <C> [BOLD] MRR@10 <C> [BOLD] Recall@1k <C> [BOLD] Rerank <C> [BOLD] Retrieval <C> [BOLD] Rerank <C> [BOLD] Retrieval <R> <C> [BOLD] Sparse & Cascade IR <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> BM25 <C> 0.240 <C> 0.814 <C> – <C> 0.506 <C> – <C> 0.519 <R> <C> Best DeepCT dai2019context <C> 0.243 <C> n.a. <C> – <C> n.a. <C> – <C> 0.554 <R> <C> Best TREC Trad Retrieval <C> 0.240 <C> n.a. <C> – <C> 0.554 <C> – <C> 0.549 <R> <C> Best TREC Trad LeToR <C> – <C> – <C> 0.556 <C> – <C> 0.561 <C> – <R> <C> BERT Reranker nogueira2019passage <C> – <C> – <C> [BOLD] 0.742 <C> – <C> 0.646 <C> – <R> <C> [BOLD] Dense Retrieval <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Rand Neg <C> 0.261 <C> 0.949 <C> 0.605 <C> 0.552 <C> 0.615 <C> 0.543 <R> <C> NCE Neg gutmann2010noise <C> 0.256 <C> 0.943 <C> 0.602 <C> 0.539 <C> 0.618 <C> 0.542 <R> <C> BM25 Neg gao2020complementing <C> 0.299 <C> 0.928 <C> 0.664 <C> 0.591 <C> 0.626 <C> 0.529 <R> <C> BM25 + Rand Neg karpukhin2020dense; luan2020sparsedense <C> 0.311 <C> 0.952 <C> 0.653 <C> 0.600 <C> 0.629 <C> 0.557 <R> <C> BM25 → Rand <C> 0.280 <C> 0.948 <C> 0.609 <C> 0.576 <C> 0.637 <C> 0.566 <R> <C> BM25 → NCE Neg <C> 0.279 <C> 0.942 <C> 0.608 <C> 0.571 <C> 0.638 <C> 0.564 <R> <C> BM25 → BM25 + Rand <C> 0.306 <C> 0.939 <C> 0.648 <C> 0.591 <C> 0.626 <C> 0.540 <R> <C> ANCE (FirstP) <C> [BOLD] 0.330 <C> [BOLD] 0.959 <C> 0.677 <C> [BOLD] 0.648 <C> 0.641 <C> 0.615 <R> <C> ANCE (MaxP) <C> – <C> – <C> – <C> – <C> [BOLD] 0.671 <C> [BOLD] 0.628 <CAP> Table 1: Results in TREC 2019 Deep Learning Track. Results not available are marked as “n.a.”, not applicable are marked as “–”. Best results in each category are marked bold.
<R> <C> System <C> [BOLD] MAP <C> AvgRec <C> MRR <R> <C> SemEval 1st <C> 79.19 <C> 88.82 <C> 86.42 <R> <C> [ITALIC]  [BOLD] Our, with PMI lexicons <C> [ITALIC]  [BOLD] 78.75 <C> [ITALIC]  [BOLD] 88.64 <C> [ITALIC]  [BOLD] 86.69 <R> <C> [ITALIC]  [BOLD] Our, no PMI lexicons <C> [ITALIC]  [BOLD] 78.08 <C> [ITALIC]  [BOLD] 88.37 <C> [ITALIC]  [BOLD] 85.19 <R> <C> SemEval 2nd <C> 77.66 <C> 88.05 <C> 84.93 <R> <C> SemEval 3rd <C> 77.58 <C> 88.14 <C> 85.21 <R> <C> … <C> … <C> … <C> … <R> <C> Average <C> 73.54 <C> 84.61 <C> 81.54 <R> <C> … <C> … <C> … <C> … <R> <C> SemEval 12th (Worst) <C> 62.24 <C> 75.41 <C> 70.58 <R> <C> Baseline [ITALIC] time <C> 59.53 <C> 72.60 <C> 67.83 <R> <C> Baseline [ITALIC] rand <C> 52.80 <C> 66.52 <C> 58.71 <CAP> Table 2. Our results compared to those at SemEval, and to two baselines: chronological and random.
<R> <C> [BOLD] - <C> [BOLD] Box <C> [BOLD] Sphere <C> [BOLD] Cylinder <C> [BOLD] Capsule <C> [BOLD] Ellipsoid <R> <C> [BOLD] Blue <C> 97.75 <C> 97.00 <C> 95.00 <C> 93.75 <C> 93.50 <R> <C> [BOLD] Red <C> 95.50 <C> 93.00 <C> 95.75 <C> 95.25 <C> 97.00 <R> <C> [BOLD] White <C> 95.25 <C> 96.25 <C> 93.50 <C> 94.50 <C> 97.00 <R> <C> [BOLD] Gray <C> 93.00 <C> 95.00 <C> 94.25 <C> 96.75 <C> 97.25 <R> <C> [BOLD] Yellow <C> 98.00 <C> 95.00 <C> 95.50 <C> 94.25 <C> 94.25 <R> <C> [BOLD] Green <C> 96.00 <C> 93.50 <C> 95.00 <C> 94.50 <C> 95.25 <R> <C> [BOLD] Cyan <C> 97.50 <C> 94.50 <C> 97.00 <C> 94.00 <C> 94.75 <R> <C> [BOLD] Magenta <C> 95.25 <C> 96.75 <C> 94.75 <C> 94.50 <C> 95.25 <CAP> Table 7: Accuracy when each object type is given to the speaker.
<R> <C> POS <C> baseline <C> PPMI <C> ALL <R> <C> NOUN <C> 92,693 <C> 44,472 <C> 4,644,478 <R> <C> VERB <C> 11,066 <C> 10,099 <C> 3,597,895 <R> <C> PRON <C> 127 <C> 107 <C> 1,869,422 <R> <C> ADP <C> 626 <C> 685 <C> 1,836,193 <R> <C> DET <C> 128 <C> 202 <C> 1,473,391 <R> <C> ADJ <C> 13,855 <C> 12,270 <C> 1,429,056 <R> <C> ADV <C> 2,032 <C> 1,688 <C> 931,763 <R> <C> PRT <C> 319 <C> 75 <C> 615,817 <R> <C> CONJ <C> 62 <C> 28 <C> 537,346 <R> <C> PUNCT <C> 110 <C> 11 <C> 223,573 <R> <C> NUM <C> 5,585 <C> 299 <C> 207,487 <R> <C> OTHER <C> 281 <C> 67 <C> 5,209 <R> <C> Total <C> 126,884 <C> 70,003 <C> 17,371,630 <CAP> Table 8: Number of the POS of words only included in the baseline or PPMI.
<R> <C> Network parameter <C> Values <R> <C> Nodes <C> 102,009 users <R> <C> Edges <C> 2,361,547 edges <R> <C> Density <C> 0.000454 <R> <C> Diameter <C> 14 <R> <C> Average Degree <C> 46.300 <R> <C> Average Clustering Coefficient <C> 0.262 <CAP> Table 2: Network parameters for largest Connected Component of Twitter Friend network.
<R> <C> Pairwise Edge threshold ( [ITALIC] ϵ) <C> Pairwise A(SWB) <C> Pairwise N edges <C> Neighborhood A(SWB) <C> Neighborhood N nodes <R> <C> 0.0 <C> 0.443⋆⋆⋆ <C> 2,062,714 <C> 0.689⋆⋆⋆ <C> 102,009 <R> <C> 0.10 <C> 0.712⋆⋆⋆ <C> 479,401 <C> 0.746⋆⋆⋆ <C> 59,952 <R> <C> 0.20 <C> 0.754⋆⋆⋆ <C> 128,261 <C> 0.769⋆⋆⋆ <C> 33,693 <R> <C> 0.30 <C> 0.755⋆⋆⋆ <C> 36,255 <C> 0.780⋆⋆⋆ <C> 16,334 <R> <C> 0.40 <C> 0.743⋆⋆⋆ <C> 10,355 <C> 0.779⋆⋆⋆ <C> 7,699 <R> <C> 0.50 <C> 0.757⋆⋆⋆ <C> 3,255 <C> 0.781⋆⋆⋆ <C> 3,793 <R> <C> 0.60 <C> 0.798⋆⋆⋆ <C> 1,375 <C> 0.805⋆⋆⋆ <C> 1,439 <R> <C> 0.70 <C> 0.755⋆⋆⋆ <C> 689 <C> 0.816⋆⋆⋆ <C> 502 <R> <C> 0.80 <C> 0.434⋆⋆⋆ <C> 301 <C> 0.768⋆⋆⋆ <C> 149 <R> <C> 0.90 <C> - <C> - <C> - <C> - <CAP> Table 4: Pairwise and Neighborhood Subjective Well-Being assortativity values A(SWB) vs. edge threshold ϵ. ⋆⋆⋆: p-value <0.001.
<R> <C> [BOLD] Model <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F-1 <C> [BOLD] Accuracy <R> <C> [BOLD] Vanilla(Baseline) <C> 63.75 <C> 62.32 <C> 63.03 <C> 57.95 <R> <C> [BOLD] Separate Heads <C> 67.39 <C> 65.59 <C> 66.48 <C> 61.44 <R> <C> [BOLD] Separate Heads + Align <C> 72.90 <C> 71.95 <C> 72.42 <C> 63.14 <CAP> Table 5: Results on Test set showing both accuracy and F1-Scores with perturbed training
<R> <C> [BOLD] Class <C> [BOLD] TP <C> [BOLD] FP <C> [BOLD] Prec <C> [BOLD] Rec <C> [BOLD] F <C> [BOLD] MCC <C> [BOLD] ROC <C> [BOLD] PRC <R> <C> PTSD <C> 0.99 <C> 0.32 <C> 0.97 <C> 0.99 <C> 0.98 <C> 0.76 <C> 0.83 <C> 0.96 <R> <C> No-PTSD <C> 0.67 <C> 0.07 <C> 0.90 <C> 0.67 <C> 0.77 <C> 0.76 <C> 0.83 <C> 0.63 <R> <C> Overall PTSD <C> 0.96 <C> 0.30 <C> 0.96 <C> 0.96 <C> 0.96 <C> 0.76 <C> 0.83 <C> 0.94 <CAP> TABLE V: LAXARY model based classification details
<R> <C> [ITALIC] α <C> [ITALIC] Oshiete-goo 0 <C> [ITALIC] Oshiete-goo 1 <C> [ITALIC] Oshiete-goo 2 <C> [ITALIC] nfL6 0 <C> [ITALIC] nfL6 1 <C> [ITALIC] nfL6 2 <R> <C> ROUGE-L <C> 0.251 <C> [ITALIC]  [BOLD] 0.299 <C> 0.211 <C> 0.330 <C> [ITALIC]  [BOLD] 0.402 <C> 0.295 <R> <C> BLEU-4 <C> 0.098 <C> [ITALIC]  [BOLD] 0.158 <C> 0.074 <C> 0.062 <C> [ITALIC]  [BOLD] 0.181 <C> 0.023 <CAP> Table 1: Results when changing α.
<R> <C> [EMPTY] <C> [ITALIC] Seq2seq <C> [ITALIC] CLSTM <C> [ITALIC] Trans <C> [ITALIC] HRED <C> [ITALIC] NAGMWA <C> [ITALIC] NAGM <R> <C> ROUGE-L <C> 0.238 <C> 0.260 <C> 0.278 <C> 0.210 <C> 0.291 <C> [ITALIC]  [BOLD] 0.299 <R> <C> BLEU-4 <C> 0.092 <C> 0.121 <C> 0.087 <C> 0.042 <C> 0.147 <C> [ITALIC]  [BOLD] 0.158 <CAP> Table 3: ROUGE-L/BLEU-4 for Oshiete-goo.
<R> <C> [EMPTY] <C> [ITALIC] Seq2seq <C> [ITALIC] CLSTM <C> [ITALIC] Trans <C> [ITALIC] HRED <C> [ITALIC] NAGMWA <C> [ITALIC] NAGM <R> <C> ROUGE-L <C> 0.291 <C> 0.374 <C> 0.338 <C> 0.180 <C> 0.383 <C> [ITALIC]  [BOLD] 0.402 <R> <C> BLEU-4 <C> 0.081 <C> 0.141 <C> 0.122 <C> 0.055 <C> 0.157 <C> [ITALIC]  [BOLD] 0.181 <CAP> Table 4: ROUGE-L/BLEU-4 for nfL6.
<R> <C> [EMPTY] <C> [BOLD] en-cs <C> [BOLD] en-de <C> [BOLD] en-fi <C> [BOLD] en-gu <C> [BOLD] en-kk <C> [BOLD] en-lt <C> [BOLD] en-ru <C> [BOLD] en-zh <C> [BOLD] de-cs <C> [BOLD] de-fr <C> [BOLD] fr-de <R> <C> BERTscore Zhang et al. ( 2020 ) <C> 0.485 <C> 0.345 <C> 0.524 <C> [BOLD] 0.558 <C> [BOLD] 0.533 <C> 0.463 <C> 0.580 <C> 0.347 <C> 0.352 <C> 0.325 <C> 0.274 <R> <C> EED\textdaggerdbl Stanchev et al. ( 2019 ) <C> 0.431 <C> 0.315 <C> 0.508 <C> [BOLD] 0.568 <C> 0.518 <C> 0.425 <C> 0.546 <C> 0.257 <C> 0.345 <C> 0.301 <C> 0.267 <R> <C> ESIM\textdaggerdbl Mathur et al. ( 2019 ) <C> − <C> 0.329 <C> 0.511 <C> − <C> 0.510 <C> 0.428 <C> 0.572 <C> 0.339 <C> 0.331 <C> 0.290 <C> 0.289 <R> <C> YiSi-1\textdaggerdbl Lo ( 2019 ) <C> 0.475 <C> 0.351 <C> 0.537 <C> [BOLD] 0.551 <C> [BOLD] 0.546 <C> 0.470 <C> 0.585 <C> [BOLD] 0.355 <C> 0.376 <C> 0.349 <C> 0.310 <R> <C> YiSi-1_srl\textdaggerdbl Lo ( 2019 ) <C> − <C> 0.368 <C> − <C> − <C> − <C> − <C> − <C> [BOLD] 0.361 <C> − <C> − <C> 0.299 <R> <C> Prism-ref (This Work) <C> [BOLD] 0.582 <C> [BOLD] 0.426 <C> [BOLD] 0.591 <C> 0.313 <C> [BOLD] 0.531 <C> [BOLD] 0.558 <C> 0.584 <C> [BOLD] 0.376 <C> [BOLD] 0.458 <C> [BOLD] 0.453 <C> [BOLD] 0.426 <R> <C> LASER + LM (Contrastive) <C> 0.535 <C> 0.402 <C> 0.568 <C> 0.306 <C> 0.408 <C> 0.503 <C> [BOLD] 0.640 <C> [BOLD] 0.356 <C> 0.431 <C> 0.401 <C> [BOLD] 0.381 <CAP> Table 2: WMT19 segment-level human correlation (τ), to non-English (top) and to English (bottom). Bold denotes the top scoring method, and any other methods with a 95% confidence interval which overlaps with the 95% confidence interval of the top scoring method. ‡:WMT19 Metric Submission. For brevity, only competitive baselines are shown. For complete results see Appendix D. Note that our models were not trained on Gujarati (gu). “LASER + LM” denotes the optimal linear combination found on the development set.
<R> <C> [BOLD] Model <C> [BOLD] F1-score <R> <C> TextCNN <C> 56.512 <R> <C> Bi-GRU-CNN <C> 69.293 <R> <C> Bi-GRU-LSTM-CNN <C> 70.576 <CAP> Table III: F1-scores of our experiments on this task
<R> <C> [EMPTY] <C> [BOLD] QC3 <C> [BOLD] TA <C> [BOLD] BC3 <C> [BOLD] MRDA <R> <C> SVMc-gl <C> 16.96±0.00 <C> 20.17±0.00 <C> 17.20±0.00 <C> 31.47±0.00 <R> <C> FFNc-gl <C> 48.29±0.25 <C> 61.36±0.21 <C> 39.58±0.26 <C> 71.12±0.13 <R> <C> FFNskip-th <C> 50.80±1.21 <C> 61.44±0.92 <C> 47.67±0.74 <C> 71.73±0.48 <R> <C> B-LSTMrand <C> 50.25±0.57 <C> 62.11±0.64 <C> 45.08±1.03 <C> 70.72±0.02 <R> <C> B-LSTMgl <C> 53.21±0.77 <C> 63.23±0.80 <C> 49.04±0.90 <C> 72.23±0.18 <R> <C> B-GRUc-gl <C> 60.50±0.36 <C> 67.23±0.76 <C> 55.45±1.05 <C> 72.04±0.35 <R> <C> B-LSTMc-gl <C> [BOLD] 61.01± [BOLD] 0.60 <C> 67.23±0.70 <C> 55.32±0.68 <C> 72.42±0.14 <R> <C> S-LSTMc-gl <C> 56.70±0.58 <C> 62.28±1.23 <C> 52.31±0.86 <C> 71.32±0.28 <R> <C> H-LSTMc-gl <C> 60.76±0.99 <C> [BOLD] 68.38± [BOLD] 0.65 <C> [BOLD] 57.17± [BOLD] 0.87 <C> [BOLD] 72.91± [BOLD] 0.14 <R> <C> H-LSTM-CRFc-gl <C> 59.83±1.27 <C> 68.10±0.68 <C> 56.37±0.61 <C> 72.77±0.17 <CAP> Table 5: Macro-F1 scores for in-domain training.
<R> <C> [BOLD] Method <C> [BOLD] Model <C> [BOLD] QC3 <C> [BOLD] TA <C> [BOLD] BC3 <R> <C> [BOLD] Transfer <C> SVM <C> 17.78±0.00 <C> 20.44±0.00 <C> 17.85±0.00 <R> <C> [BOLD] Transfer <C> FFN <C> 46.91±0.00 <C> 56.30±0.00 <C> 46.74±0.00 <R> <C> [BOLD] Transfer <C> S-LSTM <C> 49.89±1.29 <C> 62.52±1.49 <C> 36.36±1.28 <R> <C> [BOLD] Transfer <C> B-LSTM <C> 50.50±0.91 <C> [BOLD] 65.47± [BOLD] 0.62 <C> 35.92±0.62 <R> <C> [BOLD] Transfer <C> H-LSTM <C> 50.22±0.64 <C> 64.43±0.52 <C> 35.11±1.64 <R> <C> [BOLD] Transfer <C> H-LSTM-CRF <C> [BOLD] 50.83± [BOLD] 0.70 <C> 63.80±0.81 <C> 34.45±1.42 <R> <C> [BOLD] Unsup. adapt <C> Neural SCL <C> 37.73±0.92 <C> 53.98±0.33 <C> [BOLD] 46.90± [BOLD] 0.89 <R> <C> [BOLD] Unsup. adapt <C> Adv-S-LSTM <C> 43.36±1.44 <C> 48.51±0.51 <C> 42.05±0.21 <R> <C> [BOLD] Unsup. adapt <C> Adv-B-LSTM <C> 47.39±0.74 <C> 58.49±1.29 <C> 32.86±1.35 <R> <C> [BOLD] Unsup. adapt <C> Adv-H-LSTM <C> 46.53±1.48 <C> 52.90±1.20 <C> 31.36±1.91 <R> <C> [BOLD] Unsup. adapt <C> Adv-H-LSTM-CRF <C> 47.06±1.24 <C> 61.58±0.78 <C> 29.54±1.06 <R> <C> [BOLD] Merge (50%) <C> S-LSTM <C> 55.39±0.29 <C> 67.79±0.15 <C> 50.82±0.98 <R> <C> [BOLD] Merge (50%) <C> B-LSTM <C> 55.08±0.67 <C> 68.99±0.35 <C> 51.05±0.60 <R> <C> [BOLD] Merge (50%) <C> H-LSTM <C> 51.74±0.44 <C> 69.09±0.64 <C> 47.82±1.47 <R> <C> [BOLD] Merge (50%) <C> H-LSTM-CRF <C> 50.92±0.48 <C> 68.66±0.12 <C> 48.58±0.59 <R> <C> [BOLD] Fine-tune (50%) <C> S-LSTM <C> 53.94±1.04 <C> 66.07±0.67 <C> 51.73±1.53 <R> <C> [BOLD] Fine-tune (50%) <C> B-LSTM <C> 54.81±0.48 <C> 68.43±0.51 <C> 52.26±0.82 <R> <C> [BOLD] Fine-tune (50%) <C> H-LSTM <C> 54.34±0.71 <C> 69.16±0.66 <C> 50.81±0.97 <R> <C> [BOLD] Fine-tune (50%) <C> H-LSTM-CRF <C> 54.97±0.87 <C> 69.91±0.53 <C> 51.42±1.24 <R> <C> [BOLD] Semisup. adapt (50%) <C> Neural SCL <C> 41.46±0.75 <C> 58.85±0.27 <C> 48.32±0.19 <R> <C> [BOLD] Semisup. adapt (50%) <C> Adv-S-LSTM <C> 60.20±0.32 <C> 68.71±0.75 <C> 57.97±0.39 <R> <C> [BOLD] Semisup. adapt (50%) <C> Adv-B-LSTM <C> 58.57±0.80 <C> 66.51±0.81 <C> 54.39±1.28 <R> <C> [BOLD] Semisup. adapt (50%) <C> Adv-H-LSTM <C> 60.19±1.10 <C> 69.43±0.64 <C> 58.39±1.12 <R> <C> [BOLD] Semisup. adapt (50%) <C> Adv-H-LSTM-CRF <C> [BOLD] 61.81± [BOLD] 0.63 <C> [BOLD] 70.34± [BOLD] 0.62 <C> [BOLD] 59.43± [BOLD] 1.41 <R> <C> [BOLD] Merge (100%) <C> S-LSTM <C> 59.18±1.40 <C> 66.93±1.70 <C> 54.87±1.47 <R> <C> [BOLD] Merge (100%) <C> B-LSTM <C> 58.33±0.84 <C> 70.12±0.39 <C> 55.89±0.89 <R> <C> [BOLD] Merge (100%) <C> H-LSTM <C> 59.85±0.57 <C> 70.40±0.41 <C> 57.19±0.87 <R> <C> [BOLD] Merge (100%) <C> H-LSTM-CRF <C> 59.53±0.66 <C> 69.88±0.68 <C> 56.04±1.15 <R> <C> [BOLD] Fine-tune (100%) <C> S-LSTM <C> 56.67±0.85 <C> 67.41±0.34 <C> 56.40±0.44 <R> <C> [BOLD] Fine-tune (100%) <C> B-LSTM <C> 59.74±0.53 <C> 69.87±0.82 <C> 57.09±1.14 <R> <C> [BOLD] Fine-tune (100%) <C> H-LSTM <C> 60.12±0.44 <C> 70.96±0.61 <C> 58.09±1.03 <R> <C> [BOLD] Fine-tune (100%) <C> H-LSTM-CRF <C> 59.95±0.59 <C> 70.44±0.76 <C> 57.17±0.97 <R> <C> [BOLD] Sup. adapt <C> Neural SCL <C> 43.35±0.30 <C> 60.40±0.23 <C> 48.88±0.70 <R> <C> [BOLD] Sup. adapt <C> Adv-S-LSTM <C> 61.15±0.48 <C> 70.33±0.66 <C> 59.19±0.67 <R> <C> [BOLD] Sup. adapt <C> Adv-B-LSTM <C> 60.60±0.68 <C> 69.30±0.50 <C> 60.12±1.26 <R> <C> [BOLD] Sup. adapt <C> Adv-H-LSTM <C> [BOLD] 63.10± [BOLD] 0.83 <C> 72.82±0.59 <C> [BOLD] 60.38± [BOLD] 1.07 <R> <C> [BOLD] Sup. adapt <C> Adv-H-LSTM-CRF <C> 62.24±0.74 <C> [BOLD] 73.04± [BOLD] 0.38 <C> 59.84±0.75 <CAP> Table 6: Domain adaptation results on our datasets. All models use conversational word embeddings. Results are averaged over (2 folds×5) 10 runs.
<R> <C> Tracks <C> D <C> C <C> N <C> E <C> F <C> G <C> L <C> H <C> A <C> P <C> U <C> R <C> S <R> <C> EN Wiki <C> 64.3 <C> 71.4 <C> 68.5 <C> 69.6 <C> 76.7 <C> 0.0 <C> 71.4 <C> 61.3 <C> 60.0 <C> 64.0 <C> 99.7 <C> 89.2 <C> 25.1 <R> <C> EN 20K <C> 47.2 <C> 75.2 <C> 62.5 <C> 72.3 <C> 71.5 <C> 0.2 <C> 57.9 <C> 49.5 <C> 55.7 <C> 69.8 <C> 99.7 <C> 83.2 <C> 19.5 <R> <C> DE 20K <C> 69.4 <C> 83.8 <C> 57.7 <C> 80.5 <C> 83.8 <C> 59.2 <C> 68.4 <C> 62.2 <C> 67.5 <C> 68.9 <C> 97.1 <C> 86.9 <C> 25.9 <R> <C> FR 20K <C> 46.1 <C> 76.0 <C> 58.9 <C> 71.2 <C> 53.3 <C> 4.8 <C> 59.4 <C> 50.4 <C> 52.8 <C> 67.6 <C> 99.6 <C> 83.5 <C> 16.9 <CAP> Table 3: Our model’s Fine-grained F1 by label on Test Open Tracks
<R> <C> [BOLD] System <C> [BOLD] BLEU <C> [BOLD] TER <R> <C> PBMT <C> 0.2544 <C> 0.6081 <R> <C> Factored PBMT <C> 0.2700 <C> 0.5963 <R> <C> NMT <C> 0.3085 <C> 0.5552 <CAP> Table 1: Automatic evaluation (BLEU and TER scores) of the 3 MT systems
<R> <C> [BOLD] Error type <C> [BOLD] PBMT <C> [BOLD] Factored <C> [BOLD] NMT <C> [BOLD] Concat <R> <C> Accuracy <C> 0.66 <C> 0.62 <C> 0.56 <C> 0.61 <R> <C> Mistranslation <C> 0.51 <C> 0.48 <C> [BOLD] 0.58 <C> 0.53 <R> <C> Omission <C> 0.34 <C> 0.39 <C> 0.37 <C> 0.37 <R> <C> Addition <C> 0.50 <C> 0.54 <C> 0.33 <C> 0.47 <R> <C> Untranslated <C> [BOLD] 0.86 <C> [BOLD] 0.86 <C> -0.02 <C> [BOLD] 0.72 <R> <C> Fluency <C> 0.50 <C> 0.41 <C> 0.29 <C> 0.43 <R> <C> Unintelligible <C> 0.39 <C> 0.32 <C> 0.00 <C> 0.35 <R> <C> Register <C> 0.37 <C> 0.20 <C> 0.22 <C> 0.27 <R> <C> Spelling <C> 0.00 <C> 0.00 <C> 0.00 <C> 0.00 <R> <C> Grammar <C> 0.50 <C> 0.43 <C> 0.33 <C> 0.45 <R> <C> Word order <C> 0.56 <C> 0.33 <C> 0.21 <C> 0.40 <R> <C> Function words <C> 0.43 <C> 0.27 <C> 0.36 <C> 0.35 <R> <C> Extraneous <C> 0.56 <C> 0.32 <C> 0.49 <C> 0.46 <R> <C> Incorrect <C> 0.37 <C> 0.18 <C> 0.34 <C> 0.29 <R> <C> Missing <C> 0.00 <C> 0.49 <C> 0.00 <C> 0.33 <R> <C> Word form <C> 0.48 <C> 0.46 <C> 0.36 <C> 0.47 <R> <C> Part of speech <C> -0.03 <C> 0.10 <C> 0.00 <C> 0.04 <R> <C> Tense… <C> 0.44 <C> 0.36 <C> 0.15 <C> 0.38 <R> <C> Agreement <C> 0.52 <C> 0.52 <C> 0.49 <C> 0.53 <R> <C> Number <C> 0.53 <C> 0.55 <C> 0.52 <C> 0.54 <R> <C> Gender <C> 0.46 <C> 0.59 <C> 0.48 <C> 0.53 <R> <C> Case <C> 0.53 <C> 0.49 <C> 0.52 <C> 0.56 <R> <C> [BOLD] All errors <C> [BOLD] 0.56 <C> [BOLD] 0.49 <C> [BOLD] 0.44 <C> [BOLD] 0.51 <R> <C> [BOLD] Any errors <C> [BOLD] 0.80 <C> [BOLD] 0.67 <C> [BOLD] 0.51 <C> [BOLD] 0.64 <CAP> Table 3: Inter-annotator agreement (Cohen’s κ values) for the MQM evaluation task. The highest score for any individual system and the concatenation, as well as the overall score, are shown in bold. Some of the error categories have no kappa scores attached because they are parent categories that were never used on their own, so there were no data points to calculate the scores.
<R> <C> model <C> visual backend <C> caption retrieval R@1 <C> caption retrieval R@5 <C> caption retrieval R@10 <C> caption retrieval Med. r <C> image retrieval R@1 <C> image retrieval R@5 <C> image retrieval R@10 <C> image retrieval Med. r <R> <C> Embedding network  <C> VGG <C> 50.4 <C> 79.3 <C> 89.4 <C> - <C> 39.8 <C> 75.3 <C> 86.6 <C> - <R> <C> 2-Way Net  <C> VGG <C> 55.8 <C> 75.2 <C> - <C> - <C> 39.7 <C> 63.3 <C> - <C> - <R> <C> LayerNorm  <C> VGG <C> 48.5 <C> 80.6 <C> 89.8 <C> 5.1 <C> 38.9 <C> 74.3 <C> 86.3 <C> 7.6 <R> <C> VSE++  <C> R152 <C> 64.6 <C> - <C> 95.7 <C> 1 <C> 52.0 <C> - <C> 92.0 <C> 1 <R> <C> Ours <C> R152 <C> [BOLD] 69.8 <C> [BOLD] 91.9 <C> [BOLD] 96.6 <C> 1 <C> [BOLD] 55.9 <C> [BOLD] 86.9 <C> [BOLD] 94.0 <C> 1 <CAP> Table 1: Cross-modal retrieval results on MS-COCO. On both caption retrieval from images and image retrieval from captions, the proposed architecture outperforms the state-of-the-art systems. It yields an R@1 relative gain of 38% (resp. 40%) with respect to best published results [39] on cross-modal caption retrieval (resp. image retrieval), and 8% (resp 7.5%) with respect to best online results [10].
<R> <C> Model <C> AOL dataset <C> AOL dataset MRR <C> AOL dataset <C> AOL dataset Time <C> Biomedical dataset <C> Biomedical dataset MRR <C> Biomedical dataset <C> Biomedical dataset Time <R> <C> Model <C> Seen <C> Unseen <C> All <C> Time <C> Seen <C> Unseen <C> All <C> Time <R> <C> MPC Bar-Yossef and Kraus ( 2011 ) <C> [BOLD] 0.461 <C> 0.000 <C> 0.184 <C> [BOLD] 0.24 <C> 0.165 <C> 0.000 <C> 0.046 <C> [BOLD] 0.29 <R> <C> NQLM(L)+WE+MPC+ [ITALIC] λMART Park and Chiba ( 2017 ) <C> 0.430 <C> 0.306 <C> 0.356 <C> 1.33 <C> 0.159 <C> 0.152 <C> 0.154 <C> 2.35 <R> <C> [BOLD] Our models in this paper <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> NQAC <C> 0.406 <C> 0.319 <C> 0.354 <C> 0.94 <C> 0.155 <C> 0.139 <C> 0.143 <C> 1.73 <R> <C> NQAC [ITALIC] U <C> 0.417 <C> 0.325 <C> 0.361 <C> 0.98 <C> [BOLD] 0.191 <C> 0.161 <C> 0.169 <C> 1.77 <R> <C> NQAC [ITALIC] UT <C> 0.424 <C> 0.326 <C> 0.365 <C> 0.95 <C> 0.101 <C> [BOLD] 0.195 <C> 0.157 <C> 1.81 <R> <C> NQAC [ITALIC] UT+D <C> 0.427 <C> 0.326 <C> 0.366 <C> 1.32 <C> 0.186 <C> 0.185 <C> 0.185 <C> 2.04 <R> <C> NQAC [ITALIC] UT+MPC <C> [BOLD] 0.461 <C> 0.326 <C> 0.380 <C> 0.68 <C> 0.165 <C> [BOLD] 0.195 <C> [BOLD] 0.187 <C> 1.20 <R> <C> NQAC [ITALIC] UT+MPC+ [ITALIC] λMART <C> 0.459 <C> [BOLD] 0.330 <C> [BOLD] 0.382 <C> 1.09 <C> 0.154 <C> 0.179 <C> 0.172 <C> 2.01 <CAP> Table 1: MRR results for all tested models on the AOL and biomedical datasets with their average prediction time in seconds.
<R> <C> [BOLD] Method <C> [BOLD] Semcor  [BOLD] Precision <C> [BOLD] Semcor  [BOLD] Recall <C> [BOLD] Semcor  [BOLD] F1 <C> [BOLD] Senseval3  [BOLD] Precision <C> [BOLD] Senseval3  [BOLD] Recall <C> [BOLD] Senseval3  [BOLD] F1 <R> <C> Random <C> 38.2 <C> 43.0 <C> 40.4 <C> 35.8 <C> 42.1 <C> 38.7 <R> <C> Baseline <C> 63.9 <C> 69.3 <C> 66.5 <C> 60.1 <C> 68.7 <C> 64.1 <R> <C> HMM <C> 76.7 <C> 70.5 <C> 77.7 <C> 67.6 <C> 73.7 <C> 70.5 <R> <C> CRF <C> 80.3 <C> 80.2 <C> 80.2 <C> - <C> - <C> - <R> <C> MLP <C> 82.0 <C> 81.9 <C> 81.6 <C> 83.6 <C> 79.7 <C> 81.2 <R> <C> LSTM <C> 82.1 <C> 82.6 <C> 82.1 <C> 83.8 <C> 81.9 <C> 82.5 <R> <C> bi-LSTM <C> 83.5 <C> 84.2 <C> 83.6 <C> 84.6 <C> [BOLD] 82.9 <C> 83.3 <R> <C> bi-LSTM (FR) <C> 84.8 <C> 85.0 <C> 84.7 <C> 85.8 <C> 82.8 <C> 84.0 <R> <C> bi-LSTM (DE) <C> 85.2 <C> 85.2 <C> 85.0 <C> 86.2 <C> 82.7 <C> 84.1 <R> <C> bi-LSTM (CS) <C> 84.9 <C> 85.0 <C> 84.7 <C> 85.9 <C> 82.8 <C> 84.1 <R> <C> bi-LSTM (FI) <C> 85.0 <C> 85.1 <C> 84.8 <C> 85.9 <C> 82.4 <C> 83.9 <R> <C> bi-LSTM (average) <C> [BOLD] 85.0 <C> [BOLD] 85.1 <C> [BOLD] 84.8 <C> [BOLD] 86.0 <C> 82.7 <C> [BOLD] 84.0 <CAP> Table 3: Summary of results for supersense tagging.
<R> <C> Method <C> [BOLD] MG  [BOLD] Perplexity ↓ <C> [BOLD] MG  [BOLD] BLEU ↑ <C> [BOLD] UR  [BOLD] Perplexity ↓ <C> [BOLD] UR  [BOLD] BLEU ↑ <R> <C> bi-LSTM (random init) <C> 16.17 <C> 21.7 <C> 30.87 <C> 21.2 <R> <C> bi-LSTM (FR) <C> 12.80 <C> 21.9 <C> 26.84 <C> 21.7 <R> <C> bi-LSTM (DE) <C> 12.97 <C> 21.9 <C> 26.38 <C> 21.4 <R> <C> bi-LSTM (CS) <C> 13.05 <C> 22.0 <C> 26.21 <C> 21.4 <R> <C> bi-LSTM (FI) <C> 13.07 <C> 22.0 <C> 25.96 <C> 21.5 <R> <C> bi-LSTM (average) <C> [BOLD] 12.97 <C> [BOLD] 22.0 <C> [BOLD] 26.34 <C> [BOLD] 21.5 <CAP> Table 4: Summary of results for Translation in low resource Languages.
<R> <C> [BOLD] Method <C> [BOLD] best ↑ <C> [BOLD] best mode ↑ <R> <C> Base <C> 7.81 <C> 13.41 <R> <C> Mult <C> 6.64 <C> 10.89 <R> <C> BalMult <C> 8.09 <C> 13.41 <R> <C> Add <C> 7.37 <C> 12.11 <R> <C> BalAdd <C> 8.14 <C> 13.41 <R> <C> Skipgram (baseline) <C> 7.77 <C> 13.16 <R> <C> bi-LSTM (FR) <C> 9.54 <C> 15.79 <R> <C> bi-LSTM (DE) <C> 10.63 <C> 18.09 <R> <C> bi-LSTM (CS) <C> 9.74 <C> 16.04 <R> <C> bi-LSTM (FI) <C> 8.51 <C> 12.99 <R> <C> bi-LSTM (average) <C> [BOLD] 9.60 <C> [BOLD] 15.73 <CAP> Table 5: Summary of results for Lexical Substitution.
<R> <C> Model <C> Valid. <C> Test <C> Sec./ <R> <C> [EMPTY] <C> Perp. <C> Perp. <C> Batch <R> <C> Baseline <C> 54.1 <C> 37.7 <C> .45 <R> <C> Multi-Token ( [ITALIC] L=1) <C> 54.2 <C> 37.4 <C> .82 <R> <C> Multi-Token ( [ITALIC] L=2) <C> 53.9 <C> 36.4 <C> 4.85 <R> <C> Multi-Emb ( [ITALIC] E=2) <C> [BOLD] 53.8 <C> [BOLD] 35.2 <C> 2.53 <CAP> Table 5: Results on large-scale Billion Word Corpus
<R> <C> Model <C> Valid. Perp. <C> Test Perp. <R> <C> Baseline <C> 64.18 <C> 60.67 <R> <C> 10000-chunk vocab <C> 58.62 <C> 55.06 <R> <C> 20000-chunk vocab <C> [BOLD] 57.40 <C> [BOLD] 54.15 <CAP> Table 6: Vocabulary size comparison
<R> <C> Parameters <C> Concrete threshold ( Algorithm 31 )  [ITALIC] q=3, [ITALIC] k=25 <C> Concrete threshold ( Algorithm 31 )  [ITALIC] q=3, [ITALIC] k=50 <C> Percentage threshold ( Algorithm 32 )  [ITALIC] q=3, [ITALIC] k=25 <C> Percentage threshold ( Algorithm 32 )  [ITALIC] q=3, [ITALIC] k=50 <R> <C> Initial <C> 97 <C> 97 <C> 166 <C> 166 <R> <C> Picking near 1 <C> 136 <C> 136 <C> 217 <C> 217 <R> <C> Training far 1 <C> 23 <C> 44 <C> 23 <C> 45 <R> <C> Picking near 3 <C> 1,039 <C> 1,039 <C> 2,266 <C> 2,266 <R> <C> Training far 3 <C> 23 <C> 41 <C> 22 <C> 46 <R> <C> Picking near 6 <C> 6,297 <C> 6,297 <C> 8,547 <C> 8,547 <R> <C> Training far 6 <C> 25 <C> 49 <C> 24 <C> 49 <R> <C> The number of all labeled texts <C> 180,971 <C> 172,975 <C> 189,264 <C> 186,637 <R> <C> The scale of ultimate grammars <C> [ITALIC] PPR( [ITALIC] AT): 9,145  [ITALIC] PF( [ITALIC] AT): 9,812 <C> [ITALIC] PPR( [ITALIC] AT): 9,068  [ITALIC] PF( [ITALIC] AT): 9,617 <C> [ITALIC] PPR( [ITALIC] AT): 9,236  [ITALIC] PF( [ITALIC] AT): 10,318 <C> [ITALIC] PPR( [ITALIC] AT): 9,205  [ITALIC] PF( [ITALIC] AT): 9,878 <CAP> Table 3: Quantitative results when doing “Picking near-Training far” iteration
<R> <C> [BOLD] Language/Model <C> GMM-HMM WER <C> DNN WER <C> TDNN WER <C> CTC CER <C> CTC WER <C> LAS CER <C> LAS WER <C> MTL CER <C> MTL WER <R> <C> Hindi-English CS <C> 40.21 <C> 33.78 <C> [BOLD] 31.78 <C> 34.9 <C> 57.2 <C> 47.9 <C> 62.9 <C> 34.1 <C> [BOLD] 52.3 <CAP> Table 2: Character/Word Error Rates (CER/WER) of standard ASR systems vs. E2E ASR systems
<R> <C> [BOLD] Model <C> [BOLD] Accuracy <R> <C> iyer-EtAl:2017:Long† <C> 82.5 <R> <C> Seq2Prod <C> 79.1 <R> <C> [BOLD] Seq2Prod + 400 idioms <C> [BOLD] 83.2 <CAP> Table 4: Denotational Accuracy for Seq2Prod with and without idioms, compared with results from iyer-EtAl:2017:Long† on the test set of ATIS using SQL queries. Results averaged over 3 runs.
<R> <C> Classifier <C> Precision <C> Recall <C> F1 <R> <C> LR + BoW <C> 0.790 <C> 0.916 <C> 0.847 <R> <C> LR + TF-IDF <C> 0.593 <C> 1.0 <C> 0.744 <R> <C> LR + DBoW <C> 0.775 <C> 0.869 <C> 0.815 <R> <C> RF + BoW <C> 0.773 <C> 0.946 <C> [BOLD] 0.850 <R> <C> RF + DBoW <C> 0.804 <C> 0.881 <C> 0.840 <R> <C> SVM + BoW <C> 0.832 <C> 0.818 <C> 0.827 <R> <C> SVM + DBoW <C> 0.799 <C> 0.868 <C> 0.832 <CAP> Table 3: Performance of story classification on all 1,400 annotated texts.
<R> <C> Model <C> MSE <C> % Improvement <R> <C> RF <C> 0.5419 <C> 0.00% (baseline) <R> <C> Lasso <C> 0.5953 <C> -9.85% <R> <C> SVM-RBF <C> 1.2811 <C> -136.4% <R> <C> RNN-A <C> 0.4895 <C> 9.67% <R> <C> LSTM-A <C> 0.4776 <C> 11.87% <R> <C> Regional <C> 0.4621 <C> 14.73% <R> <C> Sequential+RNN <C> 0.4442 <C> 18.03% <R> <C> Sequential+LSTM <C> 0.4506 <C> 16.85% <R> <C> Holistic+RNN <C> 0.4472 <C> 17.48% <R> <C> Holistic+LSTM <C> [BOLD] 0.4438 <C> 18.10% <R> <C> RF+26+A <C> 0.5563 <C> 0.00% (baseline) <R> <C> Holistic+26 <C> 0.5758 <C> -3.51% <R> <C> Holistic+26+A <C> 0.5420 <C> 2.57% <CAP> Table 5: Performance of upvote prediction, measured in mean square errors (MSE) and relative improvements over RF baselines.
<R> <C> ID <C> Training Data (L1=EN) <C> L2=TL TL→EN <C> L2=TL EN→TL <C> L2=SW SW→EN <C> L2=SW EN→SW <R> <C> A-1 <C> L1↔L2 <C> 11.03 <C> 10.17 <C> 6.56 <C> 3.80 <R> <C> A-2 <C> L1↔L2 + L1*→L2 + L2*→L1 <C> 16.49 <C> 22.33 <C> 8.70 <C> 7.47 <R> <C> A-3 <C> L1↔L2 + L1*→L2 + L2*→L1 <C> [BOLD] 18.91 <C> [BOLD] 23.41 <C> [BOLD] 11.01 <C> [BOLD] 8.06 <CAP> Table 4: BLEU scores for bi-directional NMT models on Bible data. Models in A-2 are fine-tuned from baseline models in A-1. Highlighted best models in A-3 are fine-tuned from precedent models in A-2 and underscored synthetic data is re-decoded using precedent models. Baseline models are significantly improved in terms of BLEU.
<R> <C> Atis System <C> Atis Accuracy <C> Geo System <C> Geo Accuracy <C> Jobs System <C> Jobs Accuracy <R> <C> ZH15 <C> 84.2 <C> ZH15 <C> 88.9 <C> ZH15 <C> 85.0 <R> <C> ZC07 <C> 84.6 <C> KCAZ13 <C> 89.0 <C> PEK03 <C> 88.0 <R> <C> WKZ14 <C> [BOLD] 91.3 <C> WKZ14 <C> [BOLD] 90.4 <C> LJK13 <C> 90.7 <R> <C> DL16 <C> 84.6 <C> DL16 <C> 87.1 <C> DL16 <C> 90.0 <R> <C> ASN <C> 85.3 <C> ASN <C> 85.7 <C> ASN <C> [BOLD] 91.4 <R> <C> + SupAtt <C> 85.9 <C> + SupAtt <C> 87.1 <C> + SupAtt <C> [BOLD] 92.9 <CAP> Table 1: Accuracies for the semantic parsing tasks. ASN denotes our abstract syntax network framework. SupAtt refers to the supervised attention mentioned in Section 3.4.
<R> <C> System <C> Accuracy <C> BLEU <C> F1 <R> <C> Nearest <C> 3.0 <C> 65.0 <C> 65.7 <R> <C> LPN <C> 6.1 <C> 67.1 <C> – <R> <C> ASN <C> [BOLD] 18.2 <C> [BOLD] 77.6 <C> [BOLD] 72.4 <R> <C> + SupAtt <C> [BOLD] 22.7 <C> [BOLD] 79.2 <C> [BOLD] 75.6 <CAP> Table 2: Results for the Hearthstone task. SupAtt refers to the system with supervised attention mentioned in Section 3.4. LPN refers to the system of Ling et al. (2016). Our nearest neighbor baseline Nearest follows that of Ling et al. (2016), though it performs somewhat better; its nonzero exact match number stems from spurious repetition in the data.
<R> <C> Traning data <C> Annot. <C> Frisian <C> Dutch <C> Total <R> <C> (1) FAME <C> Manual <C> 8.5 <C> 3.0 <C> 11.5 <R> <C> (2) Frisian Broad. <C> Auto. <C> 125.5 <C> 125.5 <C> 125.5 <R> <C> (3) CGN-NL <C> Manual <C> - <C> 442.5 <C> 442.5 <CAP> Table 1: Acoustic data composition used for CTC AM training (in hours)
<R> <C> [EMPTY] <C> [EMPTY] <C> Dev. fy <C> Dev. nl <C> Test fy <C> Test nl <R> <C> # of Frisian words <C> # of Frisian words <C> 9190 <C> 0 <C> 10753 <C> 0 <R> <C> # of Dutch words <C> # of Dutch words <C> 0 <C> 4569 <C> 0 <C> 3475 <R> <C> ASR System <C> Graph <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Baseline CS ASR <C> cs <C> 32.9 <C> 33.7 <C> 30.6 <C> 29.0 <R> <C> fy <C> fy <C> 32.5 <C> - <C> 30.8 <C> - <R> <C> nl <C> nl <C> - <C> 33.6 <C> - <C> 27.9 <R> <C> nl++ <C> nl++ <C> - <C> [BOLD] 30.1 <C> - <C> [BOLD] 25.9 <CAP> Table 4: WER (%) obtained on the monolingual utterances in the development and test set of the FAME Corpus
<R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> Dev. fy <C> Dev. nl <C> Dev. fy-nl <C> Dev. all <C> Test fy <C> Test nl <C> Test fy-nl <C> Test all <C> Total <R> <C> # of Frisian words <C> # of Frisian words <C> # of Frisian words <C> 9190 <C> 0 <C> 2381 <C> 11 571 <C> 10 753 <C> 0 <C> 1798 <C> 12 551 <C> 24 122 <R> <C> # of Dutch words <C> # of Dutch words <C> # of Dutch words <C> 0 <C> 4569 <C> 533 <C> 5102 <C> 0 <C> 3475 <C> 306 <C> 3781 <C> 8883 <R> <C> ASR System <C> Graph(s) <C> Rescoring <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Kaldi CS ASR <C> cs <C> No <C> 26.3 <C> 27.6 <C> 36.8 <C> 28.4 <C> 25.1 <C> 24.4 <C> 39.3 <C> 26.7 <C> 27.6 <R> <C> [ITALIC] Single-graph systems <C> [ITALIC] Single-graph systems <C> [ITALIC] Single-graph systems <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Base. E2E CS ASR <C> cs <C> No <C> 32.9 <C> 33.7 <C> 42.6 <C> 34.9 <C> 30.6 <C> 29.0 <C> 42.4 <C> 31.8 <C> 33.4 <R> <C> Base. E2E CS ASR <C> cs <C> Yes <C> 31.6 <C> 32.8 <C> 42.1 <C> 33.9 <C> 29.6 <C> [BOLD] 27.9 <C> [BOLD] 40.7 <C> 30.7 <C> 32.3 <R> <C> Base. E2E CS ASR <C> cs <C> CS-RNN <C> 30.4 <C> [BOLD] 31.2 <C> [BOLD] 41.0 <C> 32.5 <C> 29.0 <C> 28.6 <C> 41.2 <C> 30.6 <C> [BOLD] 31.6 <R> <C> interp-nl++ <C> cs-nl++ <C> No <C> 32.6 <C> 32.3 <C> 42.3 <C> 34.3 <C> 30.7 <C> 28.7 <C> 42.6 <C> 31.8 <C> 33.1 <R> <C> interp-nl++ <C> cs-nl++ <C> Yes <C> 31.3 <C> 32.5 <C> 41.5 <C> 33.4 <C> 29.9 <C> 28.2 <C> 41.0 <C> 31.0 <C> 32.2 <R> <C> [ITALIC] Multi-graph systems <C> [ITALIC] Multi-graph systems <C> [ITALIC] Multi-graph systems <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> union-fy <C> cs, fy <C> No <C> 32.7 <C> 33.0 <C> 42.3 <C> 34.5 <C> 30.7 <C> 28.6 <C> 42.7 <C> 31.9 <C> 33.2 <R> <C> union-nl <C> cs, nl <C> No <C> 32.7 <C> 32.5 <C> 42.8 <C> 34.4 <C> 30.6 <C> 28.0 <C> 42.6 <C> 31.6 <C> 33.0 <R> <C> union-nl++ <C> cs, nl++ <C> No <C> 32.8 <C> 30.1 <C> 42.5 <C> 33.8 <C> 30.6 <C> 26.7 <C> 42.5 <C> 31.4 <C> 32.6 <R> <C> union-nl++ <C> cs, nl++ <C> Yes <C> 31.9 <C> [BOLD] 28.4 <C> [BOLD] 42.1 <C> 32.8 <C> 29.7 <C> [BOLD] 23.6 <C> [BOLD] 42.4 <C> 30.1 <C> [BOLD] 31.5 <R> <C> union-fy-nl <C> cs, fy, nl <C> No <C> 32.9 <C> 32.4 <C> 42.9 <C> 34.6 <C> 30.8 <C> 28.1 <C> 42.8 <C> 31.8 <C> 33.2 <R> <C> union-fy-nl++ <C> cs, fy, nl++ <C> No <C> 32.9 <C> 30.1 <C> 42.8 <C> 33.9 <C> 30.8 <C> 25.6 <C> 43.1 <C> 31.3 <C> 32.5 <R> <C> union-fy-nl++ <C> cs, fy, nl++ <C> Yes <C> 32.3 <C> 28.2 <C> 41.7 <C> 32.8 <C> 30.2 <C> 23.1 <C> 41.3 <C> 30.2 <C> 31.6 <R> <C> union-fy-nl++ <C> cs, fy, nl++ <C> CS-RNN <C> 32.3 <C> [BOLD] 28.2 <C> [BOLD] 40.5 <C> 32.6 <C> 30.2 <C> [BOLD] 23.1 <C> [BOLD] 40.5 <C> 30.1 <C> [BOLD] 31.4 <CAP> Table 5: WER (%) obtained on the development and test set of the FAME Corpus
<R> <C> LDA Models <C> NMI Results <R> <C> Demone(5,0.1,0.01) <C> 0.548 <R> <C> Demone(10,0.1,0.01) <C> 0.588 <R> <C> Demone(15,0.1,0.01) <C> 0.590 <R> <C> Demone(20,0.1,0.01) <C> 0.463 <CAP> TABLE II: NMI of models
<R> <C> [BOLD] Embedding <C> [BOLD] Movies <C> [BOLD] Airlines <C> [BOLD] Yelp <C> [BOLD] Restaurants <C> [BOLD] PSG <R> <C> Actors <C> 0.79 <C> 0.86 <C> 0.84 <C> 0.83 <C> 0.77 <R> <C> Drugs <C> 0.77 <C> 0.85 <C> 0.79 <C> 0.79 <C> 0.75 <R> <C> Schools <C> 0.81 <C> 0.87 <C> 0.82 <C> 0.82 <C> 0.78 <R> <C> Rail <C> 0.78 <C> 0.88 <C> 0.81 <C> 0.81 <C> 0.77 <R> <C> Cuisine <C> 0.76 <C> 0.85 <C> 0.80 <C> 0.82 <C> 0.77 <R> <C> Enron <C> 0.80 <C> 0.88 <C> 0.82 <C> 0.84 <C> 0.80 <R> <C> Legal <C> 0.79 <C> 0.86 <C> 0.80 <C> 0.79 <C> 0.77 <R> <C> Twitter <C> 0.82 <C> [BOLD] 0.91 <C> 0.85 <C> 0.85 <C> 0.79 <R> <C> Syelp <C> 0.84 <C> 0.90 <C> [BOLD] 0.86 <C> [BOLD] 0.86 <C> [BOLD] 0.81 <R> <C> [ITALIC] Wikipedia <C> [BOLD] 0.85 <C> 0.90 <C> 0.84 <C> 0.84 <C> [BOLD] 0.81 <CAP> TABLE IV: Individual domain-specific and general-purpose embeddings’ performance for each test dataset.
<R> <C> [BOLD] Dataset <C> [BOLD] GRU  [BOLD] dstack <C> [BOLD] GRU  [BOLD] dtrans <C> [BOLD] LSTM  [BOLD] dstack <C> [BOLD] LSTM  [BOLD] dtrans <C> [BOLD] 4mod-ens  [BOLD] no re-ranking <C> [BOLD] 4mod-ens  [BOLD] with re-ranking <R> <C> NIST02 <C> 41.27 <C> 43.28 <C> 44.03 <C> 42.12 <C> 46.82∗∗ <C> 46.94 <R> <C> NIST03 <C> 41.78 <C> 42.38 <C> 42.49 <C> 41.87 <C> 47.42∗∗ <C> 47.58 <R> <C> NIST04 <C> 43.75 <C> 44.33 <C> 45.11 <C> 43.97 <C> 49.12∗∗ <C> 49.13 <R> <C> NIST05 <C> 41.71 <C> 42.52 <C> 43.40 <C> 41.97 <C> 47.72∗∗ <C> 47.78 <R> <C> NIST06 <C> 42.27 <C> 43.18 <C> 43.43 <C> 42.19 <C> 49.19∗∗ <C> 49.37 <R> <C> NIST08 <C> 35.28 <C> 36.11 <C> 36.78 <C> 35.55 <C> 41.36∗∗ <C> 41.48 <R> <C> Average <C> 41.01 <C> 41.97 <C> [BOLD] 42.54 <C> 41.28 <C> [BOLD] 46.94∗∗ <C> [BOLD] 47.05 <CAP> Table 1: Experimental results in BLEU (%) of our NMT systems on NIST data set from LDC. Each individual model is obtained by cross-combining two different deep RNN architectures, i.e., deep stacked (dstack) and deep transition (dtrans) RNN, with two different recurrent unit functions, i.e., GRU and LSTM, without k-best re-ranking. The ensemble of 4 model types (4mod-ens) is obtained by taking the best model from each individual model type. This setting is tested both without and with re-ranking. Statistical significance testing was done to compare 4mod-ens with the best individual model type, dstack-LSTM (∗∗: significant at p<0.01).
<R> <C> [BOLD] Published <C> [BOLD] Ours (4mod-ens)  [BOLD] no reranking <C> [BOLD] Ours (4mod-ens)  [BOLD] with reranking <R> <C> 53.1 <C> 55.0 <C> 55.3∗ <CAP> Table 3: Experimental results in BLEU (%) on the test set of the UN Parallel Corpus of the best published result in [Junczys-Dowmunt et al.2016] and our system, without and with k-best re-ranking with N-gram language model. Statistical significance testing shows the comparison between our 4-model system with re-ranking and without re-ranking (∗: significant at p<0.05).
<R> <C> [EMPTY] <C> [EMPTY] <C> Place Top-1 <C> Place Top-5 <C> Category Top-1 <C> Category Top-5 <C> Function Top-1 <C> Function Top-5 <C> City Top-1 <C> City Top-5 <C> Country Top-1 <C> Country Top-5 <R> <C> Backbone <C> AlexNet <C> 33.78 <C> 48.19 <C> 24.16 <C> 53.03 <C> 64.97 <C> 96.70 <C> 12.47 <C> 32.52 <C> 17.97 <C> 43.30 <R> <C> Backbone <C> GoogLeNet <C> 53.48 <C> 66.23 <C> 26.01 <C> 54.81 <C> 65.69 <C> 97.20 <C> 16.34 <C> 37.19 <C> 20.98 <C> 46.43 <R> <C> Backbone <C> VGG16 <C> 43.84 <C> 59.03 <C> 26.89 <C> 55.68 <C> 65.97 <C> 97.11 <C> 18.65 <C> 41.13 <C> 24.86 <C> 51.35 <R> <C> Backbone <C> ResNet50 <C> 54.53 <C> 67.01 <C> 25.22 <C> 53.62 <C> 68.25 <C> 96.89 <C> 17.15 <C> 38.55 <C> 19.72 <C> 45.51 <R> <C> [EMPTY] <C> Average <C> 54.33 <C> 67.66 <C> 25.95 <C> 55.07 <C> 67.35 <C> 97.34 <C> 18.73 <C> 40.30 <C> 24.80 <C> 51.03 <R> <C> Pooling <C> Max <C> 49.66 <C> 63.26 <C> 25.11 <C> 54.07 <C> 65.45 <C> 97.12 <C> 16.93 <C> 38.18 <C> 22.83 <C> 48.61 <R> <C> [EMPTY] <C> SPP <C> 28.18 <C> 45.55 <C> 27.21 <C> 53.86 <C> 67.02 <C> 96.37 <C> 15.36 <C> 34.48 <C> 21.00 <C> 43.08 <R> <C> [EMPTY] <C> Softmax <C> 54.31 <C> 67.66 <C> 25.95 <C> 55.07 <C> 67.35 <C> 97.34 <C> 18.73 <C> 40.30 <C> 24.80 <C> 51.03 <R> <C> Loss <C> Triplet <C> 50.33 <C> 64.06 <C> 21.15 <C> 48.92 <C> 64.84 <C> 95.61 <C> 14.73 <C> 36.56 <C> 20.43 <C> 46.66 <R> <C> [EMPTY] <C> Focal <C> 55.03 <C> 67.38 <C> 25.27 <C> 55.48 <C> 67.62 <C> 97.53 <C> 18.67 <C> 40.87 <C> 24.73 <C> 51.46 <R> <C> PlaceNet on Places-Coarse <C> PlaceNet on Places-Coarse <C> [BOLD] 67.85 <C> [BOLD] 79.35 <C> [BOLD] 40.42 <C> [BOLD] 68.98 <C> [BOLD] 75.48 <C> [BOLD] 97.58 <C> [BOLD] 29.25 <C> [BOLD] 53.47 <C> [BOLD] 35.83 <C> [BOLD] 63.78 <CAP> Table 3: The experimental results for different methods on all tasks. We vary different pooling methods and loss functions for PlaceNet. Except for the last line, models are trained on Places-Fine. The figures in bold/blue indicate optimal/sub-optimal performance, respectively
<R> <C> Method <C> Sens. <C> Spec. <C> PPV <C> NPV <R> <C> [BOLD] Raw linear SVM (dev) <C> 1 <C> 0 <C> 0.779 <C> [EMPTY] <R> <C> [BOLD] Undersampled SVM (dev) <C> 0.994 <C> 0.017 <C> 0.968 <C> 0.093 <R> <C> [BOLD] Raw CNN (train) <C> 1 <C> 0 <C> 0.782 <C> 1 (err) <R> <C> [BOLD] Undersampled CNN (train) <C> 0.733 <C> 0.536 <C> 0.850 <C> 0.359 <CAP> TABLE II: Preliminary Model Results
<R> <C> [EMPTY] <C> [ITALIC] Clean v. Noisy - Artificial Data FakeGiga <C> [ITALIC] Clean v. Noisy - Artificial Data FakeGiga <C> [ITALIC] Clean v. Noisy - Artificial Data FakeGiga <C> [ITALIC] Clean v. Noisy - Artificial Data FakeESL <C> [ITALIC] Clean v. Noisy - Artificial Data FakeESL <C> [ITALIC] Clean v. Noisy - Artificial Data FakeESL <C> [ITALIC] Real v. Artificial - Noisy Data RealESL <C> [ITALIC] Real v. Artificial - Noisy Data RealESL <C> [ITALIC] Real v. Artificial - Noisy Data RealESL <C> [ITALIC] Real v. Artificial - Noisy Data FakeESL-1% <C> [ITALIC] Real v. Artificial - Noisy Data FakeESL-1% <C> [ITALIC] Real v. Artificial - Noisy Data FakeESL-1% <R> <C> [EMPTY] <C> [ITALIC] P <C> [ITALIC] R <C> [ITALIC] F0.5 <C> [ITALIC] P <C> [ITALIC] R <C> [ITALIC] F0.5 <C> [ITALIC] P <C> [ITALIC] R <C> [ITALIC] F0.5 <C> [ITALIC] P <C> [ITALIC] R <C> [ITALIC] F0.5 <R> <C> Random <C> 0.10 <C> 0.10 <C> 0.10 <C> 0.09 <C> 0.09 <C> 0.09 <C> 0.01 <C> 0.01 <C> 0.01 <C> 0.01 <C> 0.01 <C> 0.01 <R> <C> Punctuator-EU <C> 0.22 <C> 0.45 <C> 0.25 <C> 0.74 <C> 0.48 <C> [BOLD] 0.67 <C> 0.11 <C> [BOLD] 0.65 <C> 0.13 <C> 0.12 <C> [BOLD] 0.67 <C> 0.14 <R> <C> Punctuator-RO <C> 0.78 <C> 0.57 <C> 0.73 <C> 0.58 <C> [BOLD] 0.51 <C> 0.56 <C> 0.11 <C> 0.31 <C> 0.13 <C> 0.18 <C> 0.52 <C> 0.21 <R> <C> roCRF <C> [BOLD] 0.89 <C> 0.49 <C> 0.76 <C> [BOLD] 0.83 <C> 0.24 <C> 0.55 <C> [BOLD] 0.34 <C> 0.27 <C> [BOLD] 0.32 <C> [BOLD] 0.32 <C> 0.24 <C> 0.30 <R> <C> roS2S <C> 0.84 <C> [BOLD] 0.94 <C> [BOLD] 0.86 <C> 0.77 <C> 0.44 <C> [BOLD] 0.67 <C> 0.30 <C> 0.32 <C> 0.31 <C> 0.30 <C> 0.34 <C> [BOLD] 0.31 <CAP> Table 4: Performance on clean v. noisy artificial data with 10% run-ons, and real v. artificial data with 1% run-ons.
<R> <C> Model <C> F1 Min <C> F1 Mean±  [BOLD] std <C> F1 Max <R> <C> Single Task Learning (STL) models <C> Single Task Learning (STL) models <C> Single Task Learning (STL) models <C> Single Task Learning (STL) models <R> <C> BiLSTM-CRF(1) <C> 90.75 <C> 90.85±0.084 <C> 90.91 <R> <C> BiLSTM-CRF(2) <C> 90.69 <C> 90.83±0.08 <C> 90.91 <R> <C> Multi-task Learning (MTL) models <C> Multi-task Learning (MTL) models <C> Multi-task Learning (MTL) models <C> Multi-task Learning (MTL) models <R> <C> Vanilla MTL <C> 90.99 <C> 91.10±0.1 <C> 91.23 <R> <C> Pipeline MTL <C> 90.73 <C> 90.81±0.08 <C> 90.94 <R> <C> TI-NER-CHUNK-POS <C> 90.93 <C> 91.13±0.11 <C> 91.26 <R> <C> GTI-NER-CHUNK-POS <C> 91.27 <C> 91.38±0.07 <C> 91.49 <CAP> TABLE VII: Ablation Results. The experiments are conducted on the CoNLL-2003 NER corpus with the LSTM state size is set to 200. The MTL models are trained with NER as the main-task and chunking and POS tagging as the auxiliary tasks. The STL models are trained to generate NER tags.
<R> <C> [BOLD] Methods <C> [BOLD] Accuracy <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1-score <R> <C> Commonsense  <C> 50.95 <C> 56.67 <C> 9.24 <C> 15.89 <R> <C> Commonsense + Multi-word  <C> 50.14 <C> 54.55 <C> 3.26 <C> 6.15 <R> <C> FFNN + Position  <C> 50.00 <C> 52.38 <C> 6.08 <C> 10.89 <R> <C> FFNN + 2-word Extension (ours) <C> [BOLD] 65.94 <C> [BOLD] 67.46 <C> [BOLD] 61.96 <C> [BOLD] 64.59 <CAP> TABLE IV: Comparison of the proposed method with existing approaches
<R> <C> Proportion <C> Hit@K K=1 <C> Hit@K K=2 <C> Hit@K K=3 <C> Hit@K K=5 <R> <C> 22/86 (25%) <C> 28.80 <C> 38.16 <C> 43.92 <C> 54.76 <R> <C> 43/86 (50%) <C> 40.05 <C> 50.98 <C> 55.61 <C> 62.41 <R> <C> 65/86 (75%) <C> 48.00 <C> 60.12 <C> 64.61 <C> 69.56 <R> <C> 86/86 (100%) <C> 54.08 <C> 63.99 <C> 67.67 <C> 71.94 <CAP> Table 2. The effects of our model trained with varying number of seen relation types. The hits@K represents the F1 of correct extractions ranked in the top K in eq. 1.
<R> <C> [BOLD] Trend <C> [BOLD] p-value <R> <C> variation - absence <C> 0.520 <R> <C> drop - absence <C> 0.886 <R> <C> [BOLD] stasis - absence <C> [BOLD] 0.0317 <R> <C> drop - variation <C> 0.326 <R> <C> stasis - variation <C> 0.722 <R> <C> stasis - drop <C> 0.052 <CAP> Table 1: Stress trend analysis result (ONE-WAY ANOVA)
<R> <C> Datasets / Reported Accuracy <C> Accuracy <C> Vocab <C> Methods <C> AUC <C> Vocab@-3% <C> Vocab@-5% <R> <C> Snips / 96.7 Liu and Lane ( 2016 ) <C> 95.9 <C> 11000 <C> Frequency <C> 77.4 <C> 81 <C> 61 <R> <C> Snips / 96.7 Liu and Lane ( 2016 ) <C> 95.9 <C> 11000 <C> TF-IDF <C> 77.6 <C> 81 <C> 62 <R> <C> Snips / 96.7 Liu and Lane ( 2016 ) <C> 95.6 <C> 11000 <C> Group Lasso <C> 82.1 <C> 77 <C> 52 <R> <C> Snips / 96.7 Liu and Lane ( 2016 ) <C> 96.0 <C> 11000 <C> VVD <C> [BOLD] 82.5 <C> [BOLD] 52 <C> [BOLD] 36 <R> <C> ATIS-Flight / 94.1 Goo et al. ( 2018 ) <C> 93.8 <C> 724 <C> Frequency <C> 70.1 <C> 33 <C> 28 <R> <C> ATIS-Flight / 94.1 Goo et al. ( 2018 ) <C> 93.8 <C> 724 <C> TF-IDF <C> 70.5 <C> 34 <C> 28 <R> <C> ATIS-Flight / 94.1 Goo et al. ( 2018 ) <C> 93.8 <C> 724 <C> Group Lasso <C> 72.9 <C> 30 <C> 26 <R> <C> ATIS-Flight / 94.1 Goo et al. ( 2018 ) <C> 94.0 <C> 724 <C> VVD <C> [BOLD] 74.8 <C> [BOLD] 29 <C> [BOLD] 26 <R> <C> AG-news / 91.1 Zhang et al. ( 2015 ) <C> 91.6 <C> 61673 <C> Frequency <C> 67.1 <C> 2290 <C> 1379 <R> <C> AG-news / 91.1 Zhang et al. ( 2015 ) <C> 91.6 <C> 61673 <C> TF-IDF <C> 67.8 <C> 2214 <C> 1303 <R> <C> AG-news / 91.1 Zhang et al. ( 2015 ) <C> 91.2 <C> 61673 <C> Group Lasso <C> 68.3 <C> 1867 <C> 1032 <R> <C> AG-news / 91.1 Zhang et al. ( 2015 ) <C> 91.6 <C> 61673 <C> VVD <C> [BOLD] 70.5 <C> [BOLD] 1000 <C> [BOLD] 673 <R> <C> DBPedia / 98.3 Zhang et al. ( 2015 ) <C> 98.4 <C> 563355 <C> Frequency <C> 69.7 <C> 1000 <C> 743 <R> <C> DBPedia / 98.3 Zhang et al. ( 2015 ) <C> 98.4 <C> 563355 <C> TF-IDF <C> 71.7 <C> 1703 <C> 804 <R> <C> DBPedia / 98.3 Zhang et al. ( 2015 ) <C> 97.9 <C> 563355 <C> Group Lasso <C> 71.9 <C> 768 <C> 678 <R> <C> DBPedia / 98.3 Zhang et al. ( 2015 ) <C> 98.5 <C> 563355 <C> VVD <C> [BOLD] 72.2 <C> [BOLD] 427 <C> [BOLD] 297 <R> <C> Sogou-news / 95.0 Zhang et al. ( 2015 ) <C> 93.7 <C> 254495 <C> Frequency <C> 70.9 <C> 789 <C> 643 <R> <C> Sogou-news / 95.0 Zhang et al. ( 2015 ) <C> 93.7 <C> 254495 <C> TF-IDF <C> 71.3 <C> 976 <C> 776 <R> <C> Sogou-news / 95.0 Zhang et al. ( 2015 ) <C> 93.6 <C> 254495 <C> Group Lasso <C> 73.4 <C> 765 <C> 456 <R> <C> Sogou-news / 95.0 Zhang et al. ( 2015 ) <C> 94.0 <C> 254495 <C> VVD <C> [BOLD] 75.5 <C> [BOLD] 312 <C> [BOLD] 196 <R> <C> Yelp-review / 58.0 Zhang et al. ( 2015 ) <C> 56.3 <C> 252712 <C> Frequency <C> 74.0 <C> 1315 <C> 683 <R> <C> Yelp-review / 58.0 Zhang et al. ( 2015 ) <C> 56.3 <C> 252712 <C> TF-IDF <C> 74.1 <C> 1630 <C> 754 <R> <C> Yelp-review / 58.0 Zhang et al. ( 2015 ) <C> 56.5 <C> 252712 <C> Group Lasso <C> 75.4 <C> 934 <C> 463 <R> <C> Yelp-review / 58.0 Zhang et al. ( 2015 ) <C> [BOLD] 57.4 <C> 252712 <C> VVD <C> [BOLD] 77.9 <C> [BOLD] 487 <C> [BOLD] 287 <R> <C> SNLI / 86.7 Williams et al. ( 2018 ) <C> 84.1 <C> 42392 <C> Frequency <C> 72.2 <C> 2139 <C> 1362 <R> <C> SNLI / 86.7 Williams et al. ( 2018 ) <C> 84.1 <C> 42392 <C> TF-IDF <C> 72.8 <C> 2132 <C> 1429 <R> <C> SNLI / 86.7 Williams et al. ( 2018 ) <C> 84.6 <C> 42392 <C> Group Lasso <C> 73.6 <C> 1712 <C> 1093 <R> <C> SNLI / 86.7 Williams et al. ( 2018 ) <C> [BOLD] 85.5 <C> 42392 <C> VVD <C> [BOLD] 75.0 <C> [BOLD] 1414 <C> [BOLD] 854 <R> <C> MNLI / 72.3 Williams et al. ( 2018 ) <C> 69.2 <C> 100158 <C> Frequency <C> 78.5 <C> 1758 <C> 952 <R> <C> MNLI / 72.3 Williams et al. ( 2018 ) <C> 69.2 <C> 100158 <C> TF-IDF <C> 78.7 <C> 1656 <C> 934 <R> <C> MNLI / 72.3 Williams et al. ( 2018 ) <C> 70.1 <C> 100158 <C> Group Lasso <C> 79.2 <C> 1466 <C> 711 <R> <C> MNLI / 72.3 Williams et al. ( 2018 ) <C> [BOLD] 71.2 <C> 100158 <C> VVD <C> [BOLD] 80.1 <C> [BOLD] 1323 <C> [BOLD] 641 <CAP> Table 3: Experimental Results on various NLP tasks and datasets on the proposed metrics in subsection 2.3. Bold accuracy means the result is statistically significantly better than the competitors.
<R> <C> Training sents. <C> Cascade <C> Direct model <R> <C> 139k <C> 32.45 <C> [BOLD] 35.30 <R> <C> 69k <C> [BOLD] 26.52 <C> 24.68 <R> <C> 35k <C> [BOLD] 16.84 <C> 14.91 <R> <C> 14k <C> [BOLD] 6.59 <C> 6.08 <CAP> Table 1: BLEU scores (4 references) on Fisher/Test for various amounts of training data. The direct (multi-task) model performs best in the full data condition, but the cascaded model is best in all reduced conditions.
<R> <C> Model <C> BLEU <R> <C> Cascade <C> 32.45 <R> <C> Direct <C> 35.30 <R> <C> Basic two-stage <C> 34.36 <R> <C> APM <C> 35.31 <R> <C> APM + cross connections <C> 36.51 <R> <C> APM + cross conn. + additional loss <C> [BOLD] 36.70 <R> <C> Best APM w/o block dropout <C> 36.04 <CAP> Table 2: Results for cascaded and multi-task models under full training data conditions.
<R> <C> Model <C> Fisher <C> Fisher+OpenSub <R> <C> Cascade <C> 32.45 <C> 34.58 (+6.2% rel.) <R> <C> Direct model <C> 35.30 <C> 36.45 (+3.2% rel.) <R> <C> Basic two-stage <C> 34.36 <C> 36.91 (+6.9% rel.) <R> <C> Best APM <C> 36.70 <C> 38.81 (+5.4% rel.) <CAP> Table 3: Adding auxiliary OpenSubtitles MT data to the training. The two-stage models benefit much more strongly than the direct model, with our proposed model yielding the strongest overall results.
<R> <C> [BOLD] System <C> [BOLD] Dev-clean <C> [BOLD] Test-clean <R> <C> LAS <C> 5.80 <C> 6.03 <R> <C> LAS →LM (8) <C> 4.56 <C> 4.72 <R> <C> LAS-TTS <C> 5.68 <C> 5.85 <R> <C> LAS-TTS →LM (8) <C> [BOLD] 4.45 <C> [BOLD] 4.52 <R> <C> LAS →SC (1) <C> 5.04 <C> 5.08 <R> <C> LAS →SC (8) →LM (64) <C> [BOLD] 4.20 <C> [BOLD] 4.33 <R> <C> LAS →SC-MTR (1) <C> 4.87 <C> 4.91 <R> <C> LAS →SC-MTR (8) →LM (64) <C> [BOLD] 4.12 <C> [BOLD] 4.28 <CAP> Table 1: Word error rates (WERs) on LibriSpeech “clean” sets comparing different techniques for incorporating text-only training data. Numbers in parentheses indicate the number of input hypotheses considered by the corresponding model.
<R> <C> [BOLD] System <C> [BOLD] Dev-clean <C> [BOLD] Test-clean <R> <C> LAS <C> 3.11 <C> 3.28 <R> <C> LAS →SC (1) <C> 3.01 <C> 3.02 <R> <C> LAS →SC (8) <C> [BOLD] 1.63 <C> [BOLD] 1.68 <CAP> Table 2: Oracle WER before and after applying the SC model.
<R> <C> [BOLD] System <C> [BOLD] Dev-clean <C> [BOLD] Dev-TTS <R> <C> LAS baseline <C> 5.80 <C> 5.26 <R> <C> LAS →SC (1) <C> 5.04 <C> [BOLD] 3.45 <R> <C> LAS →SC (8) →LM (64) <C> 4.20 <C> [BOLD] 3.11 <CAP> Table 3: WER comparison on a real audio and TTS dev sets.
<R> <C> [EMPTY] <C> Encoder <C> IWSLT14 De-En <C> IWSLT14 En-De <C> WAT17 Ja-En <C> WAT17 En-Ja <R> <C> Ext. baseline <C> RNN <C> 27.6 <C> - <C> - <C> 28.5 <R> <C> Baseline <C> Emb. <C> 22.7 <C> 17.9 <C> 18.1 <C> 18.1 <R> <C> Baseline <C> CNN <C> 23.6 <C> 19.1 <C> 23.0 <C> 24.6 <R> <C> Baseline <C> RNN <C> 27.6 <C> 22.4 <C> 26.0 <C> 28.7 <R> <C> Latent Graph <C> Emb. <C> 24.0 <C> 18.7 <C> 23.2 <C> 24.3 <R> <C> Latent Graph <C> CNN <C> 24.6 <C> 20.3 <C> 24.6 <C> 26.7 <R> <C> Latent Graph <C> RNN <C> 27.2 <C> 22.4 <C> 26.0 <C> 29.1 <CAP> Table 2: Test results for German↔English and Japanese↔English.
<R> <C> [EMPTY] <C> Average purity <R> <C> LDA <C> 0.77 <R> <C> [ITALIC] k-means <C> 0.94 <R> <C> SVOSSTC <C> 0.99 <CAP> Table 7: Purity measures for LDA, k-means and SVOSSTC
<R> <C> [EMPTY] <C> [EMPTY] <C> Term-Level Laptops <C> Term-Level Laptops <C> Term-Level Restaurants <C> Term-Level Restaurants <C> Category-Level Restaurants <C> Category-Level Restaurants <C> Category-Level SemEval 14+15 <C> Category-Level SemEval 14+15 <C> [EMPTY] <R> <C> [BOLD] Model <C> Aspect <C> 3-way <C> Binary <C> 3-way <C> Binary <C> 3-way <C> Binary <C> 3-way <C> Binary <C> Avg <R> <C> LSTM <C> No <C> 61.75 <C> 78.25 <C> 67.94 <C> 82.03 <C> 73.38 <C> 79.97 <C> 75.96 <C> 79.92 <C> 74.90 <R> <C> TD-LSTM <C> Yes <C> 62.38 <C> 79.31 <C> 69.73 <C> 84.41 <C> 79.97 <C> 75.96 <C> 79.92 <C> 74.90 <C> 75.63 <R> <C> AT-LSTM <C> Yes <C> 65.83 <C> 78.25 <C> 74.37 <C> 84.74 <C> 77.90 <C> 84.87 <C> 76.16 <C> 81.28 <C> 77.93 <R> <C> ATAE-LSTM <C> Yes <C> 60.34 <C> 74.20 <C> 70.71 <C> 84.52 <C> 77.80 <C> 83.85 <C> 74.08 <C> 78.96 <C> 75.56 <R> <C> AF-LSTM(CORR) <C> Yes <C> 64.89 <C> 79.96 <C> 74.76 <C> 86.91 <C> 80.47 <C> 86.58 <C> 74.68 <C> 81.60 <C> 78.73 <R> <C> AF-LSTM(CONV) <C> Yes <C> 68.81 <C> 83.58 <C> 75.44 <C> 87.78 <C> 81.29 <C> 87.26 <C> 78.44 <C> 81.49 <C> 80.51 <R> <C> BERT-Original <C> Yes <C> 74.57 <C> 88.25 <C> 82.66 <C> [BOLD] 92.31 <C> [BOLD] 88.17 <C> 92.37 <C> 80.50 <C> 86.84 <C> 85.71 <R> <C> BERT-Soft <C> Yes <C> [BOLD] 74.92 <C> [BOLD] 90.41 <C> 82.68 <C> 91.98 <C> 87.05 <C> 91.92 <C> 80.02 <C> 86.75 <C> 85.72 <R> <C> BERT-Hard <C> Yes <C> 74.10 <C> 89.55 <C> [BOLD] 83.91 <C> [BOLD] 92.31 <C> [BOLD] 88.17 <C> [BOLD] 93.39 <C> [BOLD] 81.09 <C> [BOLD] 87.89 <C> [BOLD] 86.30 <CAP> Table 2: Experimental results (accuracy %) on all the datasets. Models in the first part are baseline methods. The results in the first part (except BERT-Original) are obtained from the prior work Tay et al. (2018). Avg column presents macro-averaged results across all the datasets.
<R> <C> Constructed Multi-Aspect Training Set Single <C> Constructed Multi-Aspect Training Set Single <C> Constructed Multi-Aspect Training Set  [ITALIC] P <C> Constructed Multi-Aspect Training Set  [ITALIC] P <C> Constructed Multi-Aspect Training Set  [ITALIC] N <C> Constructed Multi-Aspect Training Set  [ITALIC] N <C> Constructed Multi-Aspect Training Set  [ITALIC] Nu <C> Constructed Multi-Aspect Training Set  [ITALIC] Nu <C> Total 891 <R> <C> [EMPTY] <C> [EMPTY] <C> 297 <C> 297 <C> 297 <C> 297 <C> 297 <C> 297 <C> 891 <R> <C> [EMPTY] <C> [EMPTY] <C> [ITALIC] Same <C> [ITALIC] Same <C> [ITALIC] Same <C> [ITALIC] Diff <C> [ITALIC] Diff <C> [ITALIC] Diff <C> [EMPTY] <R> <C> Multi <C> 2-asp <C> [ITALIC] 2P <C> [ITALIC] 2N <C> [ITALIC] 2Nu <C> [ITALIC] PN <C> [ITALIC] PNu <C> [ITALIC] NNu <C> 3600 <R> <C> Multi <C> 2-asp <C> 300 <C> 300 <C> 300 <C> 300 <C> 300 <C> 300 <C> 3600 <R> <C> Multi <C> 3-asp <C> [ITALIC] 3P <C> [ITALIC] 3N <C> [ITALIC] 3Nu <C> [ITALIC] 2P1N <C> [ITALIC] 1P2N <C> [ITALIC] PNNu <C> 3600 <R> <C> Multi <C> 3-asp <C> 300 <C> 300 <C> 300 <C> 300 <C> 300 <C> 300 <C> 3600 <CAP> Table 4: Distribution of the multi-aspect training set. 2-asp and 3-asp indicate that the sentence contains two or three aspects respectively. Each multi-aspect sentence is categorized as Same or Diff.
<R> <C> Type <C> [ITALIC] Same <C> [ITALIC] Diff  [ITALIC] 2-3 <C> [ITALIC] Diff  [ITALIC] More <C> [ITALIC] Diff Total <C> Total <R> <C> BERT-Original <C> 73.33 <C> 57.10 <C> 60.86 <C> 58.35 <C> 68.42 <R> <C> BERT-Soft <C> 75.31 <C> 57.25 <C> 57.19 <C> 57.23 <C> 69.39 <R> <C> BERT-Hard <C> [BOLD] 76.90 <C> [BOLD] 60.15 <C> [BOLD] 64.53 <C> [BOLD] 61.61 <C> [BOLD] 71.89 <CAP> Table 5: Experimental results (accuracy %) on multi-aspect sentences. The performance of the 3-way classification on the multi-aspect test set is reported.
<R> <C> Method <C> WER ↓ Native <C> WER ↓ Non-Native <C> BLEU ↑ Native <C> BLEU ↑ Non-Native <R> <C> Human (Medically Trained) <C> 2.4±1.2 <C> 3.6±2.0 <C> 95.9±0.7 <C> 93.5±0.8 <R> <C> Human (Untrained) <C> 6.6±1.0 <C> 8.0±1.4 <C> 91.9±2.4 <C> 90.6±2.5 <R> <C> Connectionist Temporal Classification  graves2006connectionist  <C> 67.7±0.1 <C> 70.2±0.2 <C> 32.7±0.2 <C> 28.4±0.2 <R> <C> Sequence-to-Sequence  sutskever2014sequence  <C> 32.1±0.1 <C> 31.9±0.1 <C> 66.8±0.1 <C> 66.9±0.1 <R> <C> Listen, Attend, and Spell  chan2016listen  ;  chiu2018speech  <C> 14.8±0.1 <C> 21.4±0.2 <C> 84.6±0.1 <C> 80.5±0.2 <R> <C> Cold Fusion  deepspeech3  <C> 16.2±0.1 <C> 23.0±0.1 <C> 82.9±0.1 <C> 79.4±0.1 <R> <C> Our Method <C> 13.4±0.1 <C> 19.1±0.1 <C> 85.0±0.1 <C> 83.8±0.1 <CAP> Table 1: Comparison with existing methods. Lower WER and higher BLEU is better. Human refers to manual transcription. Native refers to native English speakers. All methods were trained and evaluated on our ICD-10 dataset. ± denotes the 95% confidence interval.
<R> <C> [BOLD] Model <C> [BOLD] F1 <C> [BOLD] P <C> [BOLD] R <R> <C> Baseline Yang et al. ( 2018 ) <C> 66.66 <C> - <C> - <R> <C> Baseline Replication (Answer + SF) <C> 65.28 <C> [BOLD] 63.28 <C> 67.43 <R> <C> Baseline Replication (SF only) <C> 46.44 <C> 48.80 <C> 44.31 <R> <C> DGN (best) <C> [BOLD] 68.02 <C> 61.51 <C> [BOLD] 76.07 <R> <C> DGN (no edge types) <C> 45.84 <C> - <C> - <CAP> Table 2: Supporting facts identification: Harmonic mean (F1), Precision and Recall
<R> <C> [EMPTY] <C> [ITALIC] l <C> | [ITALIC] θ| <C> Exact Match Dev <C> Exact Match Test <C> F1 Dev <C> F1 Test <R> <C> Random Guess <C> - <C> 0 <C> 1.1 <C> 1.3 <C> 4.1 <C> 4.3 <R> <C> Logistic Regression <C> - <C> - <C> 40.0 <C> 40.4 <C> 51.0 <C> 51.0 <R> <C> DCR <C> - <C> - <C> 62.5 <C> 62.5 <C> 71.2 <C> 71.0 <R> <C> Match-LSTM with Ans-Ptr (Sequence) <C> 150 <C> 882K <C> 54.4 <C> - <C> 68.2 <C> - <R> <C> Match-LSTM with Ans-Ptr (Boundary) <C> 150 <C> 882K <C> 61.1 <C> - <C> 71.2 <C> - <R> <C> Match-LSTM with Ans-Ptr (Boundary+Search) <C> 150 <C> 882K <C> 63.0 <C> - <C> 72.7 <C> - <R> <C> Match-LSTM with Ans-Ptr (Boundary+Search) <C> 300 <C> 3.2M <C> 63.1 <C> - <C> 72.7 <C> - <R> <C> Match-LSTM with Ans-Ptr (Boundary+Search+b) <C> 150 <C> 1.1M <C> 63.4 <C> - <C> 73.0 <C> - <R> <C> Match-LSTM with Bi-Ans-Ptr (Boundary+Search+b) <C> 150 <C> 1.4M <C> [BOLD] 64.1 <C> [BOLD] 64.7 <C> [BOLD] 73.9 <C> [BOLD] 73.7 <R> <C> Match-LSTM with Ans-Ptr (Boundary+Search+en) <C> 150 <C> 882K <C> [BOLD] 67.6 <C> [BOLD] 67.9 <C> [BOLD] 76.8 <C> [BOLD] 77.0 <CAP> Table 2: Experiment Results. Here “Search” refers to globally searching the spans with no more than 15 tokens, “b” refers to using bi-directional pre-processing LSTM, and “en” refers to ensemble method.
<R> <C> Systems <C> Metric <C> EER% CHS-CHS <C> EER% UYG-UYG <C> EER% CHS/UYG <R> <C> i-vector <C> Cosine <C> 7.55 <C> 6.16 <C> 15.14 <R> <C> [EMPTY] <C> LDA <C> 6.30 <C> 5.63 <C> 12.77 <R> <C> [EMPTY] <C> PLDA <C> 5.31 <C> 4.29 <C> 9.82 <R> <C> d-vector <C> Cosine <C> 4.17 <C> 4.09 <C> 10.45 <R> <C> (phone-blind) <C> LDA <C> 6.64 <C> 5.47 <C> 13.16 <R> <C> [EMPTY] <C> PLDA <C> 3.75 <C> 3.71 <C> 8.66 <R> <C> d-vector <C> Cosine <C> 4.07 <C> 4.03 <C> 10.30 <R> <C> (phone-aware) <C> LDA <C> 6.09 <C> 5.21 <C> 13.02 <R> <C> [EMPTY] <C> PLDA <C> [BOLD] 3.61 <C> [BOLD] 3.52 <C> [BOLD] 8.37 <CAP> Table 2: The EER(%) results of cross-lingual speaker verification.
<R> <C> Model <C> Accuracy GeoQA <C> Accuracy GeoQA+Q <R> <C> LSP-F <C> 48 <C> – <R> <C> LSP-W <C> 51 <C> – <R> <C> NMN <C> 51.7 <C> 35.7 <R> <C> D-NMN <C> [BOLD] 54.3 <C> [BOLD] 42.9 <CAP> Table 2: Results on the GeoQA dataset, and the GeoQA dataset with quantification. Our approach outperforms both a purely logical model (LSP-F) and a model with learned perceptual predicates (LSP-W) on the original dataset, and a fixed-structure NMN under both evaluation conditions.
<R> <C> Class <C> Precision <C> Recall <C> Accuracy <C> MCC <R> <C> Naive Bayes <C> 95.6 <C> 92.7 <C> 94.45 <C> 88.9 <R> <C> SVM <C> 99.0 <C> 94.3 <C> 96.8 <C> 93.7 <R> <C> Fine-tuning BERT <C> 99.6 <C> 99.3 <C> 99.45 <C> 98.9 <CAP> TABLE II: Testing phase: classification results
<R> <C> [BOLD] Caption <C> [BOLD] Model <C> [BOLD] Baseline <C> [BOLD] Correctness <R> <C> Ground Truth <C> Implicit <C> 0.3214 <C> 0.3836 <R> <C> Ground Truth <C> Supervised <C> 0.3214 <C> [BOLD] 0.4329 <R> <C> Generated <C> Implicit <C> 0.3995 <C> 0.5202 <R> <C> Generated <C> Supervised <C> 0.3968 <C> [BOLD] 0.5787 <CAP> Table 1: Attention correctness and baseline on Flickr30k test set. Both the implicit and the (strongly) supervised models outperform the baseline. The supervised model performs better than the implicit model in both settings.
<R> <C> [BOLD] Dataset <C> [BOLD] Model <C> [BOLD] BLEU-3 <C> [BOLD] BLEU-4 <C> [BOLD] METEOR <R> <C> Flickr30k <C> Implicit <C> 28.8 <C> 19.1 <C> 18.49 <R> <C> Flickr30k <C> Implicit* <C> 29.2 <C> 20.1 <C> 19.10 <R> <C> Flickr30k <C> Strong Sup <C> [BOLD] 30.2 <C> [BOLD] 21.0 <C> [BOLD] 19.21 <R> <C> COCO <C> Implicit <C> 34.4 <C> 24.3 <C> 23.90 <R> <C> COCO <C> Implicit* <C> 36.4 <C> 26.9 <C> 24.46 <R> <C> COCO <C> Weak Sup <C> [BOLD] 37.2 <C> [BOLD] 27.6 <C> [BOLD] 24.78 <CAP> Table 3: Comparison of image captioning performance. * indicates our implementation. Caption quality consistently increases with supervision, whether it is strong or weak.
<R> <C> [BOLD] Correctness <C> [BOLD] BLEU-3 <C> [BOLD] BLEU-4 <C> [BOLD] METEOR <R> <C> High <C> 38.0 <C> 28.1 <C> 23.01 <R> <C> Middle <C> 36.5 <C> 26.1 <C> 21.94 <R> <C> Low <C> 35.8 <C> 25.4 <C> 21.14 <CAP> Table 4: Captioning scores on the Flickr30k test set for different attention correctness levels in the generated caption, implicit attention experiment. Higher attention correctness results in better captioning performance.
<R> <C> [BOLD] Emotion <C> [BOLD] Systems <C> [BOLD] Pearsonr <C> [BOLD] Spearmanr <C> [BOLD] Pearsonr ≥ 0.5 <C> [BOLD] Spearmanr ≥ 0.5 <R> <C> [BOLD] Anger <C> Baseline <C> 0.639583 <C> 0.628180 <C> 0.510361 <C> 0.475215 <R> <C> [EMPTY] <C> Em0-Ed1-Gl0 <C> 0.659566 <C> 0.628835 <C> 0.536701 <C> 0.508762 <R> <C> [EMPTY] <C> Em1-Ed1-Gl0 <C> 0.660568 <C> 0.631893 <C> 0.536244 <C> 0.511621 <R> <C> [EMPTY] <C> Em0-Ed0-Gl1 [BOLD] * <C> 0.675864 <C> 0.656034 <C> 0.529404 <C> 0.512774 <R> <C> [EMPTY] <C> Em1-Ed0-Gl1 <C> 0.678214 <C> [BOLD] 0.658605 <C> 0.527375 <C> 0.510436 <R> <C> [EMPTY] <C> Ensemble <C> [BOLD] 0.678477 <C> 0.653964 <C> [BOLD] 0.540919 <C> [BOLD] 0.518851 <R> <C> [BOLD] Fear <C> Baseline <C> 0.631139 <C> 0.622047 <C> 0.476480 <C> 0.432407 <R> <C> [EMPTY] <C> Em0-Ed1-Gl0 <C> 0.689571 <C> 0.66237 <C> 0.539250 <C> 0.499864 <R> <C> [EMPTY] <C> Em1-Ed1-Gl0 <C> 0.695443 <C> 0.670438 <C> 0.542909 <C> 0.500896 <R> <C> [EMPTY] <C> Em0-Ed0-Gl1 <C> 0.691143 <C> 0.667255 <C> 0.546867 <C> 0.510041 <R> <C> [EMPTY] <C> Em1-Ed0-Gl1 [BOLD] * <C> 0.697630 <C> 0.676379 <C> 0.551465 <C> 0.510265 <R> <C> [EMPTY] <C> Ensemble <C> [BOLD] 0.705260 <C> [BOLD] 0.683536 <C> 0. [BOLD] 55641 <C> [BOLD] 0.513398 <R> <C> [BOLD] Joy <C> Baseline <C> 0.645597 <C> 0.652505 <C> 0.370499 <C> 0.363184 <R> <C> [EMPTY] <C> Em0-Ed1-Gl0 <C> 0.696448 <C> 0.66237 <C> 0.539250 <C> 0.499864 <R> <C> [EMPTY] <C> Em1-Ed1-Gl0 <C> 0.722115 <C> 0.720437 <C> 0.519821 <C> 0.508484 <R> <C> [EMPTY] <C> Em0-Ed0-Gl1 <C> 0.689692 <C> 0.689883 <C> 0.472973 <C> 0.470260 <R> <C> [EMPTY] <C> Em1-Ed0-Gl1 [BOLD] * <C> 0.714850 <C> 0.713558 <C> [BOLD] 0.551191 <C> [BOLD] 0.543565 <R> <C> [EMPTY] <C> Ensemble <C> [BOLD] 0.728093 <C> [BOLD] 0.727970 <C> 0.547213 <C> 0.537690 <R> <C> [BOLD] Sadness <C> Baseline <C> 0.711998 <C> 0.711745 <C> 0.479049 <C> 0.452047 <R> <C> [EMPTY] <C> Em0-Ed1-Gl0 <C> 0.737805 <C> 0.733999 <C> 0.547871 <C> 0.524843 <R> <C> [EMPTY] <C> Em1-Ed1-Gl0 [BOLD] * <C> 0.744550 <C> 0.740893 <C> [BOLD] 0.554723 <C> 0.533571 <R> <C> [EMPTY] <C> Em0-Ed0-Gl1 <C> 0.731436 <C> 0.724570 <C> 0.542910 <C> 0.536228 <R> <C> [EMPTY] <C> Em1-Ed0-Gl1 <C> 0.736081 <C> 0.731050 <C> 0.553460 <C> [BOLD] 0.548944 <R> <C> [EMPTY] <C> Ensemble <C> [BOLD] 0.748901 <C> [BOLD] 0.743589 <C> 0.547213 <C> 0.537690 <R> <C> [BOLD] Average <C> Baseline <C> 0.657079 <C> 0.653619 <C> 0.479049 <C> 0.452047 <R> <C> [EMPTY] <C> Em0-Ed1-Gl0 <C> 0.695847 <C> 0.680207 <C> 0.51998 <C> 0.493755 <R> <C> [EMPTY] <C> Em1-Ed1-Gl0 <C> 0.705669 <C> 0.690915 <C> 0.538424 <C> 0.513643 <R> <C> [EMPTY] <C> Em0-Ed0-Gl1 <C> 0.69703 <C> 0.684436 <C> 0.523038 <C> 0.507326 <R> <C> [EMPTY] <C> Em1-Ed0-Gl1 <C> 0.706694 <C> 0.694898 <C> 0.545873 <C> 0.528303 <R> <C> [EMPTY] <C> Official [BOLD] * <C> 0.708267 <C> 0.696801 <C> 0.546913 <C> 0.526018 <R> <C> [EMPTY] <C> Ensemble <C> [BOLD] 0.715183 <C> [BOLD] 0.702265 <C> [BOLD] 0.55209 <C> [BOLD] 0.530501 <CAP> Table 1: Evaluation Metrics for various systems. Systems are abbreviated as following: For example Em1-Ed0-Gl1 implies Emoji embeddings and GloVe embeddings are included, Edinburgh embeddings are not included in features keeping other features same. Results marked with * corresponds to official submission. Results in bold are the best results corresponding to that metric.
<R> <C> [BOLD] Emotion <C> [BOLD] Tweet <C> [BOLD] Gold Int. <C> [BOLD] Pred. Int. <R> <C> [BOLD] Anger <C> @Claymakerbigsi @toghar11 @scott_mulligan_ @BoxingFanatic_ Fucker blocked me 2 years ago over a question lol proper holds a grudge old Joe <C> 0.625 <C> 0.6245 <R> <C> [BOLD] Anger <C> We are raging angry.=1/2 bil $ for 2 pro Liars.(Actors) the most useless people in america Where is ours for working 100 X harder? @FoxNews <C> 0.667 <C> 0.6665 <R> <C> [BOLD] Anger <C> dammit @TMobile whays going on!!! lol #smh #mobilefails <C> 0.792 <C> 0.4062 <R> <C> [BOLD] Anger <C> People are #hurt and #angry and it’s hard to know what to do with that #anger Remember, at the end of the day, we’re all #humans #bekind <C> 0.250 <C> 0.6040 <R> <C> [BOLD] Fear <C> Onus is on #Pak to act against #terror groups which find safe havens and all types of support for cross border terror: #MEA <C> 0.667 <C> 0.6673 <R> <C> [BOLD] Fear <C> Ffs dreadful defending <C> 0.479 <C> 0.4795 <R> <C> [BOLD] Fear <C> OLD FISH <C> 0.070 <C> 0.5028 <R> <C> [BOLD] Fear <C> @MannersAboveAll *laughs louder this time, shaking my head* That was really cheesy, wasn’t it? <C> 0.083 <C> 0.4936 <R> <C> [BOLD] Joy <C> @headfirst_dom I often imagine hoe our moon would feel meeting the jovial moons which are all special <C> 0.500 <C> 0.5002 <R> <C> [BOLD] Joy <C> Your attitude toward your struggles is equally as important as your actions to work through them. <C> 0.340 <C> 0.3397 <R> <C> [BOLD] Joy <C> Oi @THEWIGGYMESS you’ve absolutely fucking killed me.. 30 mins later im still crying with laughter.. Grindah.. Grindah… hahahahahahaha <C> 0.847 <C> 0.3726 <R> <C> [BOLD] Joy <C> @WuffinArts :c You have my most heartfelt condolences. I’m glad it passed with levity and love in it’s heart. <C> 0.188 <C> 0.5872 <R> <C> [BOLD] Sadness <C> @nytimes media celebrated Don King endorsing #Obama in 08 and 12 now criticize him for endorsing #Trump who wants new Civil Rights era sad <C> 0.562 <C> 0.5623 <R> <C> [BOLD] Sadness <C> @AFCGraMaChroi oh, sorry if I’ve discouraged you <C> 0.340 <C> 0.3397 <R> <C> [BOLD] Sadness <C> oh, btw - after a 6 month depression-free time I got a relapse now… superb #depression <C> 0.917 <C> 0.462 <R> <C> [BOLD] Sadness <C> Ibiza blues hitting me hard already wow <C> 0.833 <C> 0.4247 <CAP> Table 2: Sample tweets where our system’s prediction is best and worst.
<R> <C> [BOLD] Google Embedding Vectors  [BOLD] Algorithm <C> [BOLD] Google Embedding Vectors  [BOLD] Acc. (%) <C> [BOLD] Google Embedding Vectors  [BOLD] Prec (%) <C> [BOLD] Google Embedding Vectors  [BOLD] Recall (%) <C> [BOLD] Google Embedding Vectors  [BOLD] F1 (%) <R> <C> RF (n_estm=30) <C> 77.56 <C> 78.89 <C> 61.71 <C> 62.68 <R> <C> NB <C> 74.19 <C> 68.44 <C> 69.58 <C> 68.92 <R> <C> SVM Linear <C> 82.44 <C> 81.13 <C> 72.63 <C> 75.10 <R> <C> SVM RBF <C> 72.09 <C> 36.05 <C> 50.00 <C> 41.89 <R> <C> LR <C> 82.56 <C> 81.71 <C> 72.45 <C> 75.04 <CAP> Table 3: Performance evaluation of Universal encoder model features using classical machine learning algorithms
<R> <C> Variant <C> Sentiment Precision <C> Sentiment Recall <C> Sentiment F-Score <C> Sarcasm Precision <C> Sarcasm Recall <C> Sarcasm F-Score <C> Average F-Score <R> <C> State of the art 5 <C> 79.89 <C> 74.86 <C> 77.30 <C> 87.42 <C> 87.03 <C> 86.97 <C> 82.13 <R> <C> Standalone classifiers <C> 79.02 <C> 78.03 <C> 78.13 <C> 89.96 <C> 89.25 <C> 89.37 <C> 83.75 <R> <C> Standalone coerced <C> 81.57 <C> 80.06 <C> 80.38 <C> – <C> – <C> – <C> – <R> <C> Multi-Task simple <C> 80.41 <C> 79.88 <C> 79.7 <C> 89.42 <C> 89.19 <C> 89.04 <C> 84.37 <R> <C> Multi-Task with fusion <C> 82.32 <C> 81.71 <C> 81.53 <C> 90.94 <C> [BOLD] 90.74 <C> [BOLD] 90.67 <C> 86.10 <R> <C> Multi-Task with fusion and separate GRUs <C> 80.54 <C> 80.02 <C> 79.86 <C> [BOLD] 91.01 <C> 90.66 <C> 90.62 <C> 85.24 <R> <C> Multi-Task with fusion and shared attention ( Section 2 ) <C> [BOLD] 83.67 <C> [BOLD] 83.10 <C> [BOLD] 83.03 <C> 90.50 <C> 90.34 <C> 90.29 <C> [BOLD] 86.66 <CAP> Table 1: Results for various experiments.
<R> <C> Input Features <C> F1 % <R> <C> Our WE only <C> 89.68 <R> <C> Our CE only <C> 89.59 <R> <C> Our WE + Our CE <C> [BOLD] 93.30 <R> <C> WE + CE Trained on Task <C> 89.83 <CAP> Table 1: Results on the Chunking task - different input features.
<R> <C> [BOLD] Parameter <C> [EMPTY] <R> <C> Audio resampling freq. <C> 16 KHz <R> <C> Bands of Mel-spectrogram <C> 80 <R> <C> Hop length <C> 400 <R> <C> Convolution layers, channels, filter, strides <C> 1, 64, 20×5, 8×2 <R> <C> Recurrent layer size <C> 128 <R> <C> Fully connected size <C> 128 <R> <C> Dropout probability <C> 0.9 <R> <C> Learning Rate <C> 10−3 <R> <C> Max gradient norm <C> 100 <R> <C> Gradient clipping max. value <C> 5 <CAP> Table 6: Hyperparameters of speaker verification model for LibriSpeech dataset.
<R> <C> Model <C> MCD13 <C> GPE <C> FFE <R> <C> Tacotron-2 <C> 21.88 <C> 0.722 <C> 0.740 <R> <C> Ours <C> 25.21 <C> 0.720 <C> 0.735 <CAP> Table 1: Performance of our model versus the baseline Tacotron-2 model on several evaluation metrics.
<R> <C> Method <C> Dev Accuracy <C> Test Accuracy <R> <C> Baselines from Pasupat & Liang ( 2015 ) <C> Baselines from Pasupat & Liang ( 2015 ) <C> Baselines from Pasupat & Liang ( 2015 ) <R> <C> Information Retrieval System <C> 13.4 <C> 12.7 <R> <C> Simple Semantic Parser <C> 23.6 <C> 24.3 <R> <C> Semantic Parser <C> 37.0 <C> 37.1 <R> <C> Neural Programmer <C> Neural Programmer <C> Neural Programmer <R> <C> Neural Programmer <C> 34.1 <C> 34.2 <R> <C> Ensemble of 15 Neural Programmer models <C> 37.5 <C> 37.7 <R> <C> Oracle Score with 15 Neural Programmer models <C> 50.5 <C> - <CAP> Table 1: Performance of Neural Programmer compared to baselines from (Pasupat & Liang, 2015). The performance of an ensemble of 15 models is competitive to the current state-of-the-art natural language semantic parser.
<R> <C> ClientId <C> BM25 Prec <C> BM25 Rec <C> BM25 F1 <C> BM25 MRR <C> Static L2R (artificial + client) Prec <C> Static L2R (artificial + client) Rec <C> Static L2R (artificial + client) F1 <C> Static L2R (artificial + client) MRR <C> Our Algorithm (static + adapt) Prec <C> Our Algorithm (static + adapt) Rec <C> Our Algorithm (static + adapt) F1 <C> Our Algorithm (static + adapt) MRR <C> ΔF1 % <R> <C> 1 <C> 0.714 <C> 0.714 <C> 0.714 <C> 0.773 <C> 1 <C> 0.857 <C> 0.923 <C> 0.939 <C> 1 <C> 0.857 <C> 0.923 <C> 0.939 <C> 0 <R> <C> 2 <C> 0.37 <C> 0.4 <C> 0.384 <C> 0.567 <C> 0.719 <C> 0.644 <C> 0.679 <C> 0.864 <C> 0.809 <C> 0.764 <C> 0.786 <C> 0.9 <C> [BOLD] 15.8 <R> <C> 3 <C> 0.627 <C> 0.639 <C> 0.633 <C> 0.708 <C> 0.852 <C> 0.765 <C> 0.806 <C> 0.896 <C> 0.901 <C> 0.806 <C> 0.851 <C> 0.915 <C> [BOLD] 5.6 <R> <C> 4 <C> 0.563 <C> 0.658 <C> 0.607 <C> 0.765 <C> 0.582 <C> 0.763 <C> 0.66 <C> 0.906 <C> 0.681 <C> 0.774 <C> 0.725 <C> 0.936 <C> [BOLD] 9.8 <R> <C> 5 <C> 0.649 <C> 0.68 <C> 0.664 <C> 0.708 <C> 0.762 <C> 0.73 <C> 0.746 <C> 0.856 <C> 0.804 <C> 0.74 <C> 0.771 <C> 0.868 <C> [BOLD] 3.4 <R> <C> 6 <C> 0.357 <C> 0.559 <C> 0.436 <C> 0.596 <C> 0.297 <C> 0.646 <C> 0.407 <C> 0.807 <C> 0.467 <C> 0.744 <C> 0.574 <C> 0.886 <C> [BOLD] 41 <R> <C> 7 <C> 0.556 <C> 0.714 <C> 0.625 <C> 0.801 <C> 0.605 <C> 0.778 <C> 0.681 <C> 0.877 <C> 0.636 <C> 0.778 <C> 0.7 <C> 0.87 <C> [BOLD] 2.8 <R> <C> 8 <C> 0.372 <C> 0.388 <C> 0.38 <C> 0.562 <C> 0.608 <C> 0.547 <C> 0.576 <C> 0.753 <C> 0.738 <C> 0.71 <C> 0.724 <C> 0.851 <C> [BOLD] 25.7 <R> <C> 9 <C> 0.618 <C> 0.667 <C> 0.642 <C> 0.802 <C> 0.798 <C> 0.775 <C> 0.786 <C> 0.948 <C> 0.83 <C> 0.765 <C> 0.796 <C> 0.948 <C> 1.3 <R> <C> 10 <C> 0.515 <C> 0.625 <C> 0.565 <C> 0.763 <C> 0.494 <C> 0.714 <C> 0.584 <C> 0.92 <C> 0.575 <C> 0.75 <C> 0.651 <C> 0.923 <C> [BOLD] 11.5 <R> <C> 11 <C> 0.862 <C> 0.893 <C> 0.877 <C> 0.887 <C> 0.944 <C> 0.911 <C> 0.927 <C> 0.971 <C> 0.962 <C> 0.893 <C> 0.926 <C> 0.971 <C> -0.1 <R> <C> 12 <C> 0.731 <C> 0.74 <C> 0.735 <C> 0.839 <C> 0.897 <C> 0.631 <C> 0.741 <C> 0.91 <C> 0.917 <C> 0.714 <C> 0.803 <C> 0.939 <C> [BOLD] 8.4 <R> <C> Average <C> 0.578 <C> 0.640 <C> 0.605 <C> 0.731 <C> 0.713 <C> 0.730 <C> 0.710 <C> 0.887 <C> 0.777 <C> 0.775 <C> 0.769 <C> 0.912 <C> [BOLD] 10.43 <CAP> Table 3: Details of results comparing our online L2R approach with BM25 and static training. We compare precision, recall, F1, and MRR. Δ F1 contains relative F1 point improvements of our online L2R approach over static training baseline. Static L2R vastly outperforms BM25 (17.5% relative improvement in average F1.) Our online algorithm provides massive relative F1 improvements over the static one — 10.4% on average and upto 41%. We also improve average MRR by 2.8%.
<R> <C> Hyper-Parameter <C> Value <R> <C> Dependency tag dimension <C> 256 <R> <C> Dependency arc dimension <C> 768 <R> <C> Optimizer <C> Adam <R> <C> [ITALIC] β1,  [ITALIC] β2 <C> 0.9, 0.99 <R> <C> Weight decay <C> 0.01 <R> <C> Label Smoothing <C> 0.03 <R> <C> Dropout <C> 0.5 <R> <C> BERT dropout <C> 0.2 <R> <C> Mask probability <C> 0.2 <R> <C> Batch size <C> 32 <R> <C> Epochs <C> 80 <R> <C> Base learning rate <C> 1 [ITALIC] e−3 <R> <C> BERT learning rate <C> 5 [ITALIC] e−5 <R> <C> LR warm up ratio <C> 0.0125 <R> <C> Gradient clipping <C> 5.0 <R> <C> Adapter size <C> 256* <R> <C> Language embedding size <C> 32 <CAP> Table 3: Hyper-parameter setting
<R> <C> Models <C> Edge <C> NP chunking P <C> NP chunking R <C> NP chunking F1 <C> Shallow parsing P <C> Shallow parsing R <C> Shallow parsing F1 <C> POStag Acc <C> Word Seg F1 <R> <C> LSTM <C> × <C> 94.00 <C> 94.25 <C> 94.12 <C> 92.93 <C> 93.24 <C> 93.09 <C> 97.28 <C> 90.50 <R> <C> BiLSTM <C> × <C> 94.24 <C> 94.48 <C> 94.36 <C> 93.57 <C> 93.71 <C> 93.64 <C> 97.36 <C> 90.81 <R> <C> BiLSTM-CRF <C> linear <C> 94.89 <C> 95.05 <C> 94.97 <C> 94.33 <C> 94.26 <C> 94.29 <C> 97.38 <C> 91.16 <R> <C> Conv-CRF (Collobert 2011) <C> linear <C> - <C> - <C> - <C> - <C> - <C> 94.32 <C> 97.29 <C> - <R> <C> LSTM-CRF (Huang 2015) <C> linear <C> - <C> - <C> - <C> - <C> - <C> 94.46 <C> 97.55 <C> - <R> <C> Our Edge-based-1 <C> non-linear <C> 94.86 <C> 95.46 <C> 95.16 <C> 94.44 <C> 94.52 <C> 94.48 <C> [BOLD] 97.56 <C> 91.24 <R> <C> Our Edge-based-2 <C> non-linear <C> [BOLD] 94.98 <C> [BOLD] 95.52 <C> [BOLD] 95.25 <C> [BOLD] 94.75 <C> [BOLD] 94.85 <C> [BOLD] 94.80 <C> 97.52 <C> [BOLD] 91.27 <CAP> Table 3: Comparison between our models and existing neural models in test sets. Our recurrent models with non-linear edge features outperform other models with linear edge features.
<R> <C> [BOLD] EmbIni <C> [BOLD] NumMatr <C> [BOLD] Stance <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> Random <C> Sing <C> FAVOR <C> 0.1982 <C> 0.3846 <C> 0.2616 <R> <C> Random <C> Sing <C> AGAINST <C> 0.6263 <C> 0.5929 <C> 0.6092 <R> <C> Random <C> Sing <C> Macro <C> [EMPTY] <C> [EMPTY] <C> 0.4354 <R> <C> Random <C> Sep <C> FAVOR <C> 0.2278 <C> 0.5043 <C> 0.3138 <R> <C> Random <C> Sep <C> AGAINST <C> 0.6706 <C> 0.4300 <C> 0.5240 <R> <C> Random <C> Sep <C> Macro <C> [EMPTY] <C> [EMPTY] <C> 0.4189 <R> <C> PreFixed <C> Sing <C> FAVOR <C> 0.6000 <C> 0.0513 <C> 0.0945 <R> <C> PreFixed <C> Sing <C> AGAINST <C> 0.5761 <C> 0.9440 <C> 0.7155 <R> <C> PreFixed <C> Sing <C> Macro <C> [EMPTY] <C> [EMPTY] <C> 0.4050 <R> <C> PreFixed <C> Sep <C> FAVOR <C> 0.1429 <C> 0.0342 <C> 0.0552 <R> <C> PreFixed <C> Sep <C> AGAINST <C> 0.5707 <C> 0.9033 <C> 0.6995 <R> <C> PreFixed <C> Sep <C> Macro <C> [EMPTY] <C> [EMPTY] <C> 0.3773 <R> <C> PreCont <C> Sing <C> FAVOR <C> 0.2588 <C> 0.3761 <C> 0.3066 <R> <C> PreCont <C> Sing <C> AGAINST <C> 0.7081 <C> 0.5802 <C> 0.6378 <R> <C> PreCont <C> Sing <C> Macro <C> [EMPTY] <C> [EMPTY] <C> [BOLD] 0.4722 <R> <C> PreCont <C> Sep <C> FAVOR <C> 0.2243 <C> 0.4103 <C> 0.2900 <R> <C> PreCont <C> Sep <C> AGAINST <C> 0.6185 <C> 0.5445 <C> 0.5792 <R> <C> PreCont <C> Sep <C> Macro <C> [EMPTY] <C> [EMPTY] <C> 0.4346 <CAP> Table 4: Results for the unseen target stance detection development setup using BiCond, with single vs separate embeddings matrices for tweet and target and different initialisations
<R> <C> [BOLD] Method <C> [BOLD] inTwe <C> [BOLD] Stance <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> Concat <C> Yes <C> FAVOR <C> 0.3153 <C> 0.6214 <C> 0.4183 <R> <C> Concat <C> Yes <C> AGAINST <C> 0.7438 <C> 0.4630 <C> 0.5707 <R> <C> Concat <C> Yes <C> Macro <C> [EMPTY] <C> [EMPTY] <C> 0.4945 <R> <C> Concat <C> No <C> FAVOR <C> 0.0450 <C> 0.6429 <C> 0.0841 <R> <C> Concat <C> No <C> AGAINST <C> 0.4793 <C> 0.4265 <C> 0.4514 <R> <C> Concat <C> No <C> Macro <C> [EMPTY] <C> [EMPTY] <C> 0.2677 <R> <C> TweetCondTar <C> Yes <C> FAVOR <C> 0.3529 <C> 0.2330 <C> 0.2807 <R> <C> TweetCondTar <C> Yes <C> AGAINST <C> 0.7254 <C> 0.8327 <C> 0.7754 <R> <C> TweetCondTar <C> Yes <C> Macro <C> [EMPTY] <C> [EMPTY] <C> 0.5280 <R> <C> TweetCondTar <C> No <C> FAVOR <C> 0.0441 <C> 0.2143 <C> 0.0732 <R> <C> TweetCondTar <C> No <C> AGAINST <C> 0.4663 <C> 0.5588 <C> 0.5084 <R> <C> TweetCondTar <C> No <C> Macro <C> [EMPTY] <C> [EMPTY] <C> 0.2908 <R> <C> BiCond <C> Yes <C> FAVOR <C> 0.3585 <C> 0.3689 <C> 0.3636 <R> <C> BiCond <C> Yes <C> AGAINST <C> 0.7393 <C> 0.7393 <C> 0.7393 <R> <C> BiCond <C> Yes <C> Macro <C> [EMPTY] <C> [EMPTY] <C> [BOLD] 0.5515 <R> <C> BiCond <C> No <C> FAVOR <C> 0.0938 <C> 0.4286 <C> 0.1538 <R> <C> BiCond <C> No <C> AGAINST <C> 0.5846 <C> 0.2794 <C> 0.3781 <R> <C> BiCond <C> No <C> Macro <C> [EMPTY] <C> [EMPTY] <C> 0.2660 <CAP> Table 5: Results for the unseen target stance detection development setup for tweets containing the target vs tweets not containing the target.
<R> <C> Dataset <C> Train <C> Dev <C> Test <R> <C> SST-2 <C> 67k <C> 872 <C> 1821 <R> <C> Amazon-2 <C> 3.2m <C> 400k <C> 400k <R> <C> Sem-R <C> 3608 <C> - <C> 1120 <R> <C> Sem-L <C> 2328 <C> - <C> 638 <R> <C> MPQA2.0 <C> 287 <C> 100 <C> 95 <CAP> Table 1: Numbers of samples for each dataset. Sem-R and Sem-L refer to restaurant and laptop parts of SemEval 2014 Task 4.
<R> <C> Model <C> Sentence-Level SST-2 dev <C> Sentence-Level Amazon-2 <C> Aspect-Level Sem-L <C> Aspect-Level Sem-R <C> Opinion Role MPQA-Holder <C> Opinion Role MPQA-Target <R> <C> RoBERTa [ITALIC] base <C> 95.21 <C> 96.61 <C> 78.11 <C> 84.93 <C> 81.89/77.34 <C> 80.23/72.19 <R> <C> + Random Token <C> 95.57 <C> 96.73 <C> 78.89 <C> 85.77 <C> 82.71/77.71 <C> 80.86/73.01 <R> <C> + SW <C> 96.38 <C> 96.82 <C> 80.13 <C> 86.92 <C> 82.95/77.63 <C> 81.18/73.15 <R> <C> + SW + WP <C> 96.51 <C> 96.87 <C> 80.32 <C> 87.25 <C> 82.97/77.82 <C> 81.09/73.24 <R> <C> + SW + WP + AP <C> 96.87 <C> 96.94 <C> 81.32 <C> 87.92 <C> 84.25/79.03 <C> 82.77/74.82 <R> <C> + SW + WP + AP-I <C> 96.89 <C> 96.93 <C> 81.19 <C> 87.71 <C> 84.01/78.36 <C> 82.69/74.36 <CAP> Table 4: Effectiveness of objectives. SW, WP, AP refers to pre-training objectives: Sentiment Word prediction, Word Polarity prediction and Aspect-sentiment Pair prediction. “Random Token” denotes random token masking used in RoBERTa. AP-I denotes predicting words in an Aspect-sentiment Pair Independently.
<R> <C> [EMPTY] <C> total <C> used <R> <C> train <C> 7954 <C> 4850 <R> <C> dev <C> 933 <C> 604 <R> <C> test <C> 862 <C> 583 <CAP> Table 5: Total number of sentences and the number of sentences used for training, validation and test purposes
<R> <C> [EMPTY] <C> [EMPTY] <C> Test  [ITALIC] SVM <C> Test  [ITALIC] SVM <C> Test  [ITALIC] SVMIG <C> Test  [ITALIC] SVMIG <C> Test  [ITALIC] SVMHSWLM <C> Test  [ITALIC] SVMHSWLM <R> <C> [EMPTY] <C> [BOLD] Period <C> 2010-2012 <C> 2012-2014 <C> 2010-2012 <C> 2012-2014 <C> 2010-2012 <C> 2012-2014 <R> <C> Train <C> 2010-2012 <C> 40.90 <C> 35.57 <C> [BOLD] 43.11  \blacktriangleup <C> 34.12 <C> 41.83 <C> [BOLD] 40.02 \blacktriangleup <R> <C> Train <C> 2012-2014 <C> 30.51 <C> 44.96 <C> 30.38 <C> 47.18 <C> [BOLD] 39.11 \blacktriangleup <C> [BOLD] 47.28 <CAP> Table 1: Results of party classification task in terms of macro-average accuracy. We have conducted paired t-test to investigate statistical significance of the improvements of the best method over the second best method, in the corresponding experiments. Improvements that are annotated with \blacktriangleup are statistically significant with p-value < 0.005.
<R> <C> Model <C> Micro Acc. <C> Macro Acc. <R> <C> Random <C> 0.330 <C> 0.326 <R> <C> Surface LR <C> [BOLD] 0.682 <C> 0.333 <R> <C> DAM (SNLI) <C> 0.479 <C> [BOLD] 0.362 <R> <C> DAM (ShARC) <C> 0.492 <C> 0.322 <CAP> Table 4: Results of entailment models on ShARC.
<R> <C> [EMPTY] <C> FCE  [BOLD] P <C> FCE  [BOLD] R <C> FCE  [BOLD] F0.5 <C> CoNLL14-1  [BOLD] P <C> CoNLL14-1  [BOLD] R <C> CoNLL14-1  [BOLD] F0.5 <C> CoNLL14-2  [BOLD] P <C> CoNLL14-2  [BOLD] R <C> CoNLL14-2  [BOLD] F0.5 <C> JFLEG  [BOLD] P <C> JFLEG  [BOLD] R <C> JFLEG  [BOLD] F0.5 <R> <C> Rei  <C> 58.88 <C> 28.92 <C> 48.48 <C> 17.68 <C> 19.07 <C> 17.86 <C> 25.22 <C> 19.25 <C> 23.62 <C> - <C> - <C> - <R> <C> Rei and Søgaard  <C> 65.53 <C> 28.61 <C> 52.07 <C> 25.14 <C> 15.22 <C> 22.14 <C> 37.72 <C> 16.19 <C> 29.65 <C> 72.53 <C> 25.04 <C> 52.52 <R> <C> Rei et al.  <C> 60.67 <C> 28.08 <C> 49.11 <C> 23.28 <C> 18.01 <C> 21.87 <C> 35.28 <C> 19.42 <C> 30.13 <C> - <C> - <C> - <R> <C> Kasewa et al.  <C> - <C> - <C> 55.6 <C> - <C> - <C> 28.3 <C> - <C> - <C> 35.5 <C> - <C> - <C> - <R> <C> BERTBASE w/o pre-train <C> 48.85 <C> 11.30 <C> 29.34 <C> 11.45 <C> 7.80 <C> 10.47 <C> 18.24 <C> 9.31 <C> 15.30 <C> 58.85 <C> 13.22 <C> 34.81 <R> <C> BERTBASE <C> [BOLD] 69.80 <C> 37.37 <C> 59.47 <C> 34.08 <C> [BOLD] 33.56 <C> 33.97 <C> 46.01 <C> 33.89 <C> 42.93 <C> [BOLD] 78.06 <C> 36.28 <C> 63.45 <R> <C> AvgL <C> 68.09 <C> 41.14 <C> 60.20 <C> 34.97 <C> 32.02 <C> 34.33 <C> 45.33 <C> 35.27 <C> 42.88 <C> 77.35 <C> 37.05 <C> 63.52 <R> <C> MHMLA <C> 68.87† <C> [BOLD] 43.45∗† <C> [BOLD] 61.65∗† <C> [BOLD] 35.74∗ <C> 33.50† <C> [BOLD] 35.26∗† <C> [BOLD] 46.45† <C> [BOLD] 35.47∗ <C> [BOLD] 43.74† <C> 77.74 <C> [BOLD] 38.85∗† <C> [BOLD] 64.77∗† <CAP> Table 2: Results of grammatical error detection. These results are averaged over five runs. ∗ and † indicate that there is a significant difference against BERTBASE and AvgL, respectively.
<R> <C> [ITALIC] J <C> FCE <C> CoNLL14-1 <C> CoNLL14-2 <C> JFLEG <R> <C> 1 <C> 61.16 <C> 33.75 <C> 42.89 <C> 63.98 <R> <C> 2 <C> 61.62 <C> 33.44 <C> 42.42 <C> 63.72 <R> <C> 3 <C> [BOLD] 61.90 <C> 34.50 <C> 43.17 <C> 64.45 <R> <C> 4 <C> 61.55 <C> 33.74 <C> 42.80 <C> 64.37 <R> <C> 6 <C> 61.22 <C> 34.26 <C> 43.29 <C> 64.48 <R> <C> 8 <C> 61.27 <C> 34.72 <C> 43.02 <C> 64.10 <R> <C> 12 <C> 61.65 <C> [BOLD] 35.26 <C> [BOLD] 43.74 <C> [BOLD] 64.77 <CAP> Table 3: F0.5 scores of MHMLA using different number of heads J. These results are averaged over five runs.
<R> <C> [BOLD] Results <C> [BOLD] NER (F1-score) IV <C> [BOLD] NER (F1-score) OOTV <C> [BOLD] NER (F1-score) OOEV <C> [BOLD] NER (F1-score) OOBV <C> [BOLD] chunking (F1-score) IV <C> [BOLD] chunking (F1-score) OOTV <C> [BOLD] chunking (F1-score) OOEV <C> [BOLD] chunking (F1-score) OOBV <C> [BOLD] POS (Accuracy) IV <C> [BOLD] POS (Accuracy) OOTV <C> [BOLD] POS (Accuracy) OOEV <C> [BOLD] POS (Accuracy) OOBV <R> <C> Nochar+WLSTM+CRF <C> 91.33 <C> 87.36 <C> 100.00 <C> 69.68 <C> 94.87 <C> 90.84 <C> 95.51 <C> 91.47 <C> 97.51 <C> 89.76 <C> 94.07 <C> 75.36 <R> <C> CLSTM+WLSTM+CRF <C> [BOLD] 92.18 <C> 90.63 <C> 100.00 <C> 78.57 <C> [BOLD] 95.20 <C> [BOLD] 92.65 <C> 94.38 <C> [BOLD] 94.01 <C> [BOLD] 97.63 <C> [BOLD] 93.82 <C> 94.07 <C> [BOLD] 87.32 <R> <C> CCNN+WLSTM+CRF <C> 91.76 <C> [BOLD] 91.25 <C> 100.00 <C> [BOLD] 81.58 <C> 95.15 <C> 92.34 <C> [BOLD] 97.75 <C> 93.55 <C> 97.62 <C> 93.33 <C> [BOLD] 94.69 <C> 83.82 <R> <C> Nochar+WCNN+CRF <C> 90.71 <C> 86.99 <C> 100.00 <C> 69.09 <C> 94.56 <C> 90.98 <C> 93.26 <C> 91.71 <C> 97.29 <C> 89.10 <C> [BOLD] 94.17 <C> 74.15 <R> <C> CLSTM+WCNN+CRF <C> [BOLD] 91.59 <C> 90.07 <C> 100.00 <C> 77.92 <C> [BOLD] 95.02 <C> 91.86 <C> 94.38 <C> [BOLD] 93.32 <C> [BOLD] 97.48 <C> [BOLD] 93.28 <C> [BOLD] 94.17 <C> [BOLD] 88.29 <R> <C> CCNN+WCNN+CRF <C> 91.35 <C> [BOLD] 90.46 <C> 100.00 <C> [BOLD] 78.88 <C> 94.83 <C> [BOLD] 92.42 <C> [BOLD] 96.63 <C> 92.40 <C> 97.46 <C> 92.74 <C> 93.86 <C> 87.80 <CAP> Table 7: Results for OOV analysis.
<R> <C> Model <C> Search R@10 <C> Annotation R@10 <R> <C> Socher et al.  <C> 28.6 <C> 29.0 <R> <C> Karpathy  <C> 42.5 <C> 44.0 <R> <C> Text + word vec <C> 49.0 <C> 56.7 <R> <C> Spectrogram CNN <C> 17.9 <C> 24.3 <CAP> Table 1: Image search and annotation results on the Flickr8k test images (1000 images with 5 captions each).
<R> <C> Model <C> Top-1 Acc. <C> Top-5 Acc. <R> <C> DNN, 2x1024 FC <C> 75.5 <C> 93.9 <R> <C> DNN, 3x1024 FC <C> 69.5 <C> 91.4 <R> <C> CNN, 1x64 Conv + 2x1024 FC <C> 84.2 <C> 97.4 <CAP> Table 2: Isolated word recognition accuracies on our WSJ test set. “FC” stands for “fully connected”.
<R> <C> Model <C> Semantic <C> Syntactic <C> Total <C> Execution time <R> <C> CBOW <C> 20.03 <C> 54.42 <C> 42.27 <C> [BOLD] 399m 7.573s <R> <C> Skip-gram <C> [BOLD] 41.27 <C> 52.27 <C> 48.38 <C> 589m 39.222s <R> <C> CBOS <C> 41.16 <C> [BOLD] 62.39 <C> [BOLD] 54.89 <C> 810m 43.196s <CAP> Table 6: Accuracy of CBOS model and baselines on word analogy task using Greek Web Content Corpus for training.
<R> <C> Approach <C> One-hot <C> BMC CBOW <C> BMC CBOW <C> BMC GloVe <C> BMC GloVe <C> Twitter CBOW <C> Twitter CBOW <C> Twitter GloVe <C> Twitter GloVe <R> <C> Approach <C> One-hot <C> 50 <C> 200 <C> 50 <C> 200 <C> 50 <C> 200 <C> 50 <C> 200 <R> <C> vSim <C> 0.1675 <C> 0.1771 <C> 0.1896 <C> 0.1840 <C> 0.1869 <C> 0.1812 <C> 0.1813 <C> 0.0936 <C> 0.1807 <R> <C> bestMT <C> 0.2232 <C> 0.1926 <C> 0.2070 <C> 0.1803 <C> 0.2500△ <C> 0.2014 <C> 0.2047 <C> 0.1258 <C> 0.2138 <R> <C> top5MT <C> 0.2491△ <C> [BOLD] 0.1994 <C> 0.2104 <C> 0.1879 <C> [BOLD] 0.2638△▲ <C> [BOLD] 0.2037 <C> 0.2095 <C> [BOLD] 0.1322 <C> 0.2362 <R> <C> top5MTr <C> 0.2458△ <C> 0.1982 <C> 0.2109 <C> [BOLD] 0.1894 <C> 0.2617△ <C> [BOLD] 0.2037 <C> [BOLD] 0.2096 <C> [BOLD] 0.1322 <C> 0.2310 <R> <C> bestMT+vSim <C> 0.2420△ <C> 0.1910 <C> 0.1953 <C> 0.1860 <C> 0.2532△ <C> 0.1891 <C> 0.1954 <C> 0.1078 <C> 0.2374 <R> <C> top5MT+vSim <C> 0.2556△ <C> 0.1916 <C> [BOLD] 0.2144 <C> 0.1726 <C> 0.2600△ <C> 0.1978 <C> 0.2068 <C> 0.1079 <C> 0.2405△ <R> <C> top5MTr+vSim <C> [BOLD] 0.2594△ <C> 0.1861 <C> 0.2070 <C> 0.1802 <C> 0.2590△ <C> 0.1959 <C> 0.2027 <C> 0.1129 <C> [BOLD] 0.2406△ <CAP> Table 1: MRR-5 performance of the proposed approach and the baselines. Significant differences (p<0.05) with the cosine similarity (vSim) baselines with the one-hot representation, and with the corresponding distributed word representation (e.g. CBOW or GloVe) are denoted △ and ▲, respectively.
<R> <C> Ont. <C> # of entities <C> % w/ def. <C> % w/ con. <R> <C> CPT <C> 13,786 <C> 0.0 <C> 97.9 <R> <C> GO <C> 44,684 <C> 100.0 <C> 30.5 <R> <C> HGNC <C> 39,816 <C> 0.0 <C> 0.8 <R> <C> HPO <C> 11,939 <C> 72.5 <C> 17.9 <R> <C> MeSH <C> 268,162 <C> 10.5 <C> 35.1 <R> <C> OMIM <C> 98,515 <C> 0.0 <C> 2.8 <R> <C> RxNorm <C> 205,858 <C> 0.0 <C> 5.1 <R> <C> Total <C> 682,760 <C> 11.9 <C> 20.1 <CAP> Table 1: Entities with definitions and contexts for each of the training ontologies.
<R> <C> [EMPTY] <C> BSP <C> DAP <C> NUR <C> NUG <R> <C> [EMPTY] <C> F-1 <C> F-1 <C> H@1 <C> BLEU <R> <C> None <C> [BOLD] 18.48 <C> 40.33 <C> 63.72 <C> 14.21 <R> <C> NUR <C> 17.80 <C> 43.25 <C> – <C> 15.39 <R> <C> NUG <C> 17.96 <C> 42.31 <C> [BOLD] 67.34 <C> – <R> <C> MUR <C> 16.76 <C> [BOLD] 44.87 <C> 62.38 <C> 15.27 <R> <C> InI <C> 16.61 <C> [BOLD] 44.84 <C> 62.62 <C> [BOLD] 15.52 <CAP> Table 1: Results of evaluating the chosen pretraining objectives, preceded by the baseline, on the four downstream tasks. This evaluation used all of the training data for the downstream tasks as described in Section 5.2.
<R> <C> [EMPTY] <C> BSP <C> DAP <C> NUR <C> NUG <R> <C> [EMPTY] <C> F-1 <C> F-1 <C> H@1 <C> BLEU <R> <C> None <C> 4.07 <C> 15.22 <C> 13.62 <C> 7.80 <R> <C> NUR <C> [BOLD] 19.64 <C> 17.88 <C> – <C> 9.97 <R> <C> NUG <C> 17.11 <C> [BOLD] 20.53 <C> [BOLD] 21.57 <C> – <R> <C> MUR <C> 15.84 <C> 17.45 <C> 21.06 <C> 9.81 <R> <C> InI <C> 14.61 <C> 15.56 <C> 19.80 <C> [BOLD] 10.87 <CAP> Table 4: Results of evaluating pretrained objectives on their capacity to generalize to the restaurant domain using only 50 in-domain samples and 2000 out-of-domain samples during training. The evaluation is carried out only on the in-domain test samples.
<R> <C> [EMPTY] <C> <3 <C> ≥3 & <7 <C> ≥7 <R> <C> None <C> 11.02 <C> 14.17 <C> 15.30 <R> <C> NUR <C> [BOLD] 13.95 <C> 15.08 <C> 15.88 <R> <C> MUR <C> 12.21 <C> 15.36 <C> 16.10 <R> <C> InI <C> 11.52 <C> [BOLD] 15.40 <C> [BOLD] 16.63 <CAP> Table 5: Results on the downstream task of NUG, with different dialog context lengths (<3 utterances, 3-7 utterances, and >7 utterances.
<R> <C> # <C> Hypothesis <C> Focus <C> Corpus <C> Instances <C> Exceptions apparent <C> Exceptions actual <C> Support (in %) <R> <C> 1 <C> OHPT <C> translations <C> MSC (Italian) <C> 1093 <C> 7 <C> 1 <C> 99.9 <R> <C> 2 <C> OHPT <C> translations <C> JSC (Japanese) <C> 1093 <C> 3 <C> 2 <C> 99.8 <R> <C> 3 <C> OHPD <C> documents <C> SemCor <C> 2126 <C> 14 <C> 9 <C> 99.6 <R> <C> 4 <C> OHPC <C> collocations <C> SemCor <C> 522 <C> 16 <C> 11 <C> 97.9 <R> <C> 5 <C> OHPSC <C> sense clusters <C> OntoNotes <C> 1578 <C> 23 <C> 2 <C> 99.9 <CAP> Table 4: Summary of the evidence for the homonym hypotheses from our five experiments.
<R> <C> Experiment <C> CAE <C> No Pref. <C> BST <R> <C> Gender <C> 15.23 <C> 41.36 <C> [BOLD] 43.41 <R> <C> Political slant <C> 14.55 <C> [BOLD] 45.90 <C> 39.55 <R> <C> Sentiment <C> 35.91 <C> [BOLD] 40.91 <C> 23.18 <CAP> Table 5: Human preference for meaning preservation in percentages.
<R> <C> [BOLD] System <C> [BOLD] en-de <C> [BOLD] en-fr <R> <C> Luong et al. ( 2015 ) <C> 18.1 <C> 31.5 <R> <C> GNMT <C> 24.6 <C> 39.0 <R> <C> [BOLD] Joey NMT RNN <C> 22.5 <C> 35.7 <R> <C> [BOLD] Joey NMT RNN (deep) <C> 24.0 <C> 37.4 <CAP> Table 4: newstest2014 results.
<R> <C> System <C> Attempted(%) <C> Precision (%) <C> Recall (%) <C> F-Measure (%) <R> <C> [BOLD] D-Bees <C> 99.91 <C> [BOLD] 80.47 <C> [BOLD] 80.41 <C> [BOLD] 80.44 <R> <C> MFS <C> 100 <C> 78.89 <C> 78.89 <C> 78.89 <R> <C> TSP-ACO <C> 99.80 <C> 78.50 <C> 78.10 <C> 78.30 <R> <C> ACA <C> 100 <C> 77.64 <C> 77.64 <C> 77.64 <R> <C> SA <C> 100 <C> 74.23 <C> 74.23 <C> 74.23 <R> <C> GA <C> 100 <C> 73.98 <C> 73.98 <C> 73.98 <R> <C> RS <C> 100 <C> 52.43 <C> 52.43 <C> 52.43 <CAP> Table 3: Comparison of D-Bees with other methods.
<R> <C> [BOLD] Batch size <C> [BOLD] Learning rate <C> [BOLD] Epochs <C> [BOLD] Warm-up <C> [BOLD] Max sequence <R> <C> 32 <C> 2⋅10−5 <C> 3 <C> 0.1 <C> 128 <CAP> Table 6: Hyper-parameters used in the fine-tuning process
<R> <C> [BOLD] Aggregation Function <C> [BOLD] Validation All <C> [BOLD] Validation Yes/No <C> [BOLD] Validation Number <C> [BOLD] Validation Others <C> [BOLD] Test-Dev All <C> [BOLD] Test-Dev Yes/No <C> [BOLD] Test-Dev Number <C> [BOLD] Test-Dev Others <R> <C> Mean Pooling <C> 54.87 <C> 71.50 <C> 37.93 <C> 46.69 <C> 56.05 <C> 71.00 <C> 38.88 <C> 47.19 <R> <C> Max Pooling <C> 56.73 <C> 75.68 <C> 37.64 <C> 47.37 <C> 57.95 <C> 75.14 <C> 38.48 <C> 47.69 <R> <C> LogSumExp Pooling <C> 54.61 <C> 70.94 <C> 38.27 <C> 46.53 <C> 55.68 <C> 70.36 <C> 38.72 <C> 47.00 <R> <C> 1D Convolution <C> 56.87 <C> 72.35 <C> 39.18 <C> 49.79 <C> 57.79 <C> 71.71 <C> 39.97 <C> 49.96 <R> <C> CLS Token <C> 58.31 <C> 74.29 <C> 39.89 <C> 51.03 <C> 59.40 <C> 74.26 <C> 40.31 <C> 51.07 <R> <C> [BOLD] Ours ( [ITALIC] k=1) <C> 60.73 <C> 77.68 <C> 41.86 <C> [BOLD] 52.84 <C> 62.05 <C> 77.84 <C> 42.47 <C> [BOLD] 53.03 <R> <C> [BOLD] Ours ( [ITALIC] k=2) <C> 60.76 <C> 78.06 <C> 42.32 <C> 52.48 <C> 62.06 <C> 78.26 <C> 42.62 <C> 52.66 <R> <C> [BOLD] Ours ( [ITALIC] k=3) <C> 60.50 <C> 77.82 <C> 41.56 <C> 52.33 <C> 61.80 <C> 78.22 <C> 41.69 <C> 52.35 <R> <C> [BOLD] Ours ( [ITALIC] k=5) <C> [BOLD] 60.99 <C> [BOLD] 78.62 <C> 42.53 <C> 52.46 <C> 62.17 <C> 78.52 <C> 42.27 <C> 52.74 <R> <C> [BOLD] Ours ( [ITALIC] k=7) <C> 60.95 <C> 78.40 <C> [BOLD] 42.65 <C> 52.53 <C> [BOLD] 62.43 <C> [BOLD] 78.75 <C> [BOLD] 43.33 <C> 52.83 <R> <C> [BOLD] Ours ( [ITALIC] k=10) <C> 59.94 <C> 77.30 <C> 40.82 <C> 51.80 <C> 61.16 <C> 77.39 <C> 40.69 <C> 51.97 <CAP> TABLE I: Accuracy results on VQA 2.0 dataset. The results are reported on the validation and test-dev splits. All models are trained only on the VQA 2.0 training split.
<R> <C> [BOLD] Aggregation Func. <C> [BOLD] Word Emb. <C> All <C> Yes/No <C> Number <C> Others <R> <C> [BOLD] Ours ( [ITALIC] k=5) <C> Learned <C> 59.29 <C> 77.24 <C> 40.29 <C> 50.66 <R> <C> [BOLD] Ours ( [ITALIC] k=5) <C> GloVe <C> 60.98 <C> 78.51 <C> 42.20 <C> [BOLD] 52.61 <R> <C> [BOLD] Ours ( [ITALIC] k=5) <C> GloVe Finetuned <C> [BOLD] 60.99 <C> [BOLD] 78.62 <C> [BOLD] 42.53 <C> 52.46 <R> <C> [BOLD] Ours ( [ITALIC] k=7) <C> Learned <C> 59.23 <C> 76.98 <C> 40.02 <C> 50.80 <R> <C> [BOLD] Ours ( [ITALIC] k=7) <C> GloVe <C> [BOLD] 61.13 <C> [BOLD] 79.13 <C> 42.13 <C> 52.47 <R> <C> [BOLD] Ours ( [ITALIC] k=7) <C> GloVe Finetuned <C> 60.95 <C> 78.40 <C> [BOLD] 42.65 <C> [BOLD] 52.53 <CAP> TABLE II: Comparison between different word embedding strategies on VQA 2.0 validation set.
<R> <C> [BOLD] Aggregation Function <C> [BOLD] Text Retrieval R@1 <C> [BOLD] Text Retrieval R@5 <C> [BOLD] Text Retrieval R@10 <C> [BOLD] Image Retrieval R@1 <C> [BOLD] Image Retrieval R@5 <C> [BOLD] Image Retrieval R@10 <R> <C> Mean Pooling <C> 69.66 <C> 93.12 <C> 97.64 <C> 50.42 <C> 82.27 <C> 90.83 <R> <C> Max Pooling <C> 69.04 <C> 92.68 <C> 96.98 <C> 51.20 <C> 83.27 <C> 91.52 <R> <C> LogSumExp Pooling <C> 64.20 <C> 91.52 <C> 96.84 <C> 47.22 <C> 82.26 <C> 91.23 <R> <C> 1D Convolution <C> 65.66 <C> 91.86 <C> 96.58 <C> 49.25 <C> 81.43 <C> 90.42 <R> <C> CLS Token <C> 70.30 <C> 93.38 <C> 97.24 <C> 51.05 <C> 83.28 <C> [BOLD] 91.80 <R> <C> [BOLD] Ours ( [ITALIC] k=1) <C> [BOLD] 70.80 <C> 93.16 <C> 97.24 <C> 50.77 <C> 82.76 <C> 91.31 <R> <C> [BOLD] Ours ( [ITALIC] k=2) <C> 70.36 <C> [BOLD] 93.46 <C> 97.20 <C> [BOLD] 51.31 <C> [BOLD] 83.38 <C> 91.69 <R> <C> [BOLD] Ours ( [ITALIC] k=3) <C> 70.42 <C> 93.34 <C> 97.22 <C> 50.98 <C> 83.17 <C> 91.65 <R> <C> [BOLD] Ours ( [ITALIC] k=4) <C> 70.14 <C> 93.42 <C> [BOLD] 97.76 <C> 50.82 <C> 82.66 <C> 91.14 <CAP> TABLE III: Cross-modal retrieval results on Microsoft COCO 1K test set.
<R> <C> Architecture <C> Architecture <C> Dataset XING <C> Dataset XING <C> Dataset UserBehavior <C> Dataset UserBehavior <R> <C> Base <C> Base <C> 0.3216 <C> 0.1847 <C> 0.1611 <C> 0.0925 <R> <C> Window <C> 4 <C> 0.3115↓ <C> 0.2167↑ <C> 0.1488↓ <C> 0.0899 <R> <C> size (L) <C> 16 <C> 0.3049↓ <C> 0.1733↓ <C> 0.1433↓ <C> 0.0914 <R> <C> [EMPTY] <C> 32 <C> 0.3052↓ <C> 0.1735↓ <C> 0.1401↓ <C> 0.0950 <R> <C> Attention <C> 1 <C> 0.3220 <C> 0.1851 <C> 0.1631 <C> 0.0926 <R> <C> blocks ( [ITALIC] dl) <C> 4 <C> 0.3217 <C> 0.1849 <C> 0.1631 <C> 0.0924 <R> <C> Attention <C> 1 <C> 0.3225 <C> 0.1860 <C> 0.1622 <C> 0.0919 <R> <C> heads ( [ITALIC] dh) <C> 4 <C> 0.3225 <C> 0.1860 <C> 0.1646 <C> 0.0940 <R> <C> ¬ Sharing embedding <C> ¬ Sharing embedding <C> 0.1263↓ <C> 0.0791↓ <C> 0.1042↓ <C> 0.0192↓ <R> <C> Embedding <C> 300 <C> 0.3147 <C> 0.1831 <C> 0.1622 <C> 0.0920 <R> <C> size ( [ITALIC] din) <C> 1000 <C> 0.3207 <C> 0.1857 <C> 0.1628 <C> 0.0921 <R> <C> Loss <C> NNL <C> 0.3130 <C> 0.1806 <C> 0.1571 <C> 0.0895 <R> <C> function <C> BPR <C> 0.3163 <C> 0.1804 <C> 0.1598 <C> 0.0913 <R> <C> Flat attention <C> Flat attention <C> 0.3215 <C> 0.1869 <C> 0.1588 <C> 0.0907 <R> <C> Global context  [ITALIC]  [BOLD] P(⋅) <C> Global context  [ITALIC]  [BOLD] P(⋅) <C> 0.3207 <C> 0.1839 <C> 0.1603 <C> 0.0912 <R> <C> Local context  [ITALIC]  [BOLD] P(⋅| [ITALIC] x) <C> Local context  [ITALIC]  [BOLD] P(⋅| [ITALIC] x) <C> 0.3210 <C> 0.1841 <C> 0.1591 <C> 0.0912 <R> <C> Kernel <C> [ITALIC] ω1 <C> 0.3191 <C> 0.1827 <C> 0.1604 <C> 0.0910 <R> <C> types <C> [ITALIC] ψ1 <C> 0.3122 <C> 0.2141↑ <C> 0.1591 <C> 0.0907 <R> <C> [EMPTY] <C> [ITALIC] ψ10 <C> 0.3207 <C> 0.1844 <C> 0.1627 <C> 0.0925 <R> <C> [EMPTY] <C> [ITALIC] π1 <C> 0.2917↓ <C> [BOLD] 0.2323↑ <C> 0.1562 <C> 0.0976 <R> <C> [EMPTY] <C> [ITALIC] π5 <C> 0.3025↓ <C> 0.2209↑ <C> 0.1670 <C> [BOLD] 0.1010↑ <R> <C> [EMPTY] <C> [ITALIC] π10 <C> 0.3214 <C> 0.2183↑ <C> [BOLD] 0.1673 <C> 0.0997↑ <R> <C> [EMPTY] <C> [ITALIC] ρ5 <C> 0.3111 <C> 0.2196↑ <C> 0.1618 <C> 0.0931 <R> <C> [EMPTY] <C> [ITALIC] ρ10 <C> 0.3230 <C> 0.1869 <C> 0.1635 <C> 0.0932 <R> <C> [EMPTY] <C> [ITALIC] ψ5, [ITALIC] ρ5 <C> 0.3241 <C> 0.1888 <C> 0.1635 <C> 0.0932 <R> <C> [EMPTY] <C> [ITALIC] ψ5, [ITALIC] π5 <C> [BOLD] 0.3273 <C> 0.2146↑ <C> [BOLD] 0.1673 <C> 0.0997↑ <R> <C> [EMPTY] <C> [ITALIC] ψ5, [ITALIC] ρ5, [ITALIC] π5 <C> 0.3254 <C> 0.1971↑ <C> 0.1664 <C> 0.0983↑ <CAP> Table 3. Ablation analysis on two datasets under metrics of Recall@5 (left) and MRR@5 (right). The best performance is highlighted in bold face. ↓ and ↑ denote a drop/increase of performance for more than 5%. ψ, ρ, π, ω respectively denote the exponential, logarithmic, linear and constant temporal kernels. The superscript on the kernel function denotes the number of such kernel used in the model.
<R> <C> Spatial Embedding <C> DEV del / ins <C> DEV WER <C> TEST del / ins <C> TEST WER <R> <C> EfficientNet-B0 <C> 47.22 / 1.59 <C> 57.06 <C> 46.09 / 1.75 <C> 56.29 <R> <C> EfficientNet-B4 <C> 40.73 / 2.45 <C> 51.26 <C> 38.34 / 2.80 <C> 50.09 <R> <C> EfficientNet-B7 <C> 39.29 / 2.84 <C> 50.18 <C> 37.05 / 2.76 <C> 47.96 <R> <C> Pretrained  CNN <C> 21.51 / 6.10 <C> 33.90 <C> 20.29 / 5.35 <C> 33.39 <R> <C> [BOLD] + BN & ReLU <C> [BOLD] 13.54 / 5.74 <C> [BOLD] 26.70 <C> [BOLD] 13.85 / 6.43 <C> [BOLD] 27.62 <CAP> Table 2: Impact of the Spatial Embedding Layer variants.
<R> <C> Loss Weights  [ITALIC] λR <C> Loss Weights  [ITALIC] λT <C> DEV WER <C> DEV BLEU-4 <C> TEST WER <C> TEST BLEU-4 <R> <C> 1.0 <C> 0.0 <C> 24.88 <C> - <C> 24.59 <C> - <R> <C> 0.0 <C> 1.0 <C> - <C> 20.69 <C> - <C> 20.17 <R> <C> 1.0 <C> 1.0 <C> 35.13 <C> 21.73 <C> 33.75 <C> 21.22 <R> <C> 2.5 <C> 1.0 <C> 26.99 <C> 22.11 <C> 27.55 <C> 21.37 <R> <C> 5.0 <C> 1.0 <C> [BOLD] 24.61 <C> 22.12 <C> [BOLD] 24.49 <C> [BOLD] 21.80 <R> <C> 10.0 <C> 1.0 <C> 24.98 <C> [BOLD] 22.38 <C> 26.16 <C> 21.32 <R> <C> 20.0 <C> 1.0 <C> 25.87 <C> 20.90 <C> 25.73 <C> 20.93 <CAP> Table 4: Training Sign Language Transformers to jointly learn recognition and translation with different weight on recognition loss.
<R> <C> Model <C> Acc <C> Spearman <C> Precision <C> MRR <R> <C> [BOLD] Ours <C> [BOLD] 78.0 <C> 0.238 <C> [BOLD] 81.91 <C> 0.937 <R> <C> PANLP <C> 77.7 <C> 0.180 <C> 78.1 <C> 0.938 <R> <C> Pentagon <C> 76.5 <C> 0.338 <C> 77.7 <C> [BOLD] 0.962 <R> <C> DUT-BIM <C> 74.5 <C> 0.106 <C> 74.7 <C> 0.906 <CAP> Table 3: The leaderboard for QA task (link). Our method ranked #1 on the leaderboard in terms of Acc (accuracy). The Spearman score is not consistent with other scores in the leaderboard.
<R> <C> Model <C> Avg. Acc <C> Esm. Acc <R> <C> Single Model <C> Single Model <C> Single Model <R> <C> #1, MT-DNN <C> - <C> 88.61 <R> <C> #2, MT-DNN <C> - <C> 88.33 <R> <C> #3, MT-DNN <C> - <C> 87.84 <R> <C> #4, SciBERT <C> - <C> 88.19 <R> <C> #5, SciBERT <C> - <C> 87.70 <R> <C> #6, SciBERT <C> - <C> 87.21 <R> <C> Ensemble Model <C> Ensemble Model <C> Ensemble Model <R> <C> #1+#2+#3, MT-DNN <C> 88.26 <C> 89.7 <R> <C> #4+#5+#6, SciBERT <C> 87.70 <C> 89.2 <R> <C> #1+#2+#5, MultiSource <C> 88.21 <C> 91.6 <R> <C> #1+#5+#6, MultiSource <C> 87.84 <C> 90.4 <R> <C> #1-6, MultiSource <C> 87.98 <C> 91.3 <CAP> Table 4: Comparison of ensembles from different sources. Avg.Acc stands for average accuracy, the numerical average of each individual model’s accuracy. Esm.Acc stands for ensemble accuracy, the accuracy of the resulting ensemble model. For ensembles, MT-DNN means all the three models are from MT-DNN, and similarly for SciBERT; MultiSource denotes the ensemble models come from two different sources.
<R> <C> Init Model <C> Naïve <C> Ratio <C> Ratio+MNLI <R> <C> MT-DNN <C> 86.9 <C> 86.2 <C> 87.8 <R> <C> MT-DNN-KD <C> 87.5 <C> 88.2 <C> [BOLD] 88.8 <R> <C> SciBERT <C> 87.1 <C> 87.0 <C> [BOLD] 89.4 <CAP> Table 5: Single model performance on MedNLI developlment data. Naiïve means simply integrating all medical-domain data; Ratio means using MedNLI as in-domain data and other medical domain data as external data; Ratio+MNLI means using medical domain data as in-domain and MNLI as external.
<R> <C> Model <C> Validation Acc <C> Validation Pre <C> Validation Rec <C> Validation F1 <C> Test Acc <C> Test Pre <C> Test Rec <C> Test F1 <R> <C> SVM word n-grams <C> 97.69 <C> 87.45 <C> 84.66 <C> 86.03 <C> 97.46 <C> 89.59 <C> 83.45 <C> 86.41 <R> <C> SVM word n-grams + LF <C> 97.73 <C> 86.06 <C> [BOLD] 87.14 <C> 86.60 <C> 97.52 <C> 88.44 <C> 85.48 <C> 86.93 <R> <C> SVM word + char n-grams <C> 97.43 <C> 87.10 <C> 81.57 <C> 84.24 <C> 97.64 <C> 90.76 <C> 84.12 <C> 87.31 <R> <C> SVM word + char n-grams + LF <C> 97.76 <C> 90.13 <C> 82.44 <C> 86.11 <C> 97.93 <C> 92.71 <C> 85.31 <C> 88.86 <R> <C> SVM Rubin et al. ( 2016 ) <C> 97.73 <C> 90.21 <C> 81.92 <C> 85.86 <C> 97.79 <C> [BOLD] 93.47 <C> 82.95 <C> 87.90 <R> <C> SVM Rubin et al. ( 2016 ) + char tf-idf + LF <C> [BOLD] 97.93 <C> [BOLD] 90.99 <C> 83.69 <C> [BOLD] 87.19 <C> [BOLD] 98.09 <C> 92.98 <C> [BOLD] 86.72 <C> [BOLD] 89.75 <R> <C> Bi-GRU <C> 97.67 <C> 89.17 <C> 82.28 <C> 85.58 <C> 97.58 <C> 93.11 <C> 80.96 <C> 86.61 <R> <C> SVM Doc2Vec Le and Mikolov ( 2014 ) <C> 92.48 <C> 58.48 <C> 71.66 <C> 64.40 <C> 90.48 <C> 50.52 <C> 67.88 <C> 57.92 <R> <C> HAN Yang et al. ( 2016b ) <C> 97.91 <C> 92.06 <C> 82.24 <C> 86.88 <C> 97.83 <C> 90.85 <C> 86.17 <C> 88.45 <R> <C> 4LHN <C> 98.44 <C> 92.82 <C> 88.33 <C> 90.52 <C> 98.36 <C> 94.61 <C> 88.00 <C> 91.18 <R> <C> 4LHNP <C> 98.46 <C> 93.54 <C> 87.75 <C> 90.56 <C> 98.39 <C> 94.63 <C> 88.33 <C> 91.37 <R> <C> 4LHND <C> 98.36 <C> [BOLD] 94.73 <C> 85.24 <C> 89.74 <C> 98.18 <C> [BOLD] 95.35 <C> 85.31 <C> 90.05 <R> <C> 4LHNPD <C> [BOLD] 98.54 <C> 93.31 <C> [BOLD] 89.01 <C> [BOLD] 91.11 <C> [BOLD] 98.39 <C> 93.51 <C> [BOLD] 89.50 <C> [BOLD] 91.46 <CAP> Table 4: Satirical news detection results.
<R> <C> modal <C> materials <C> approaches <C> AUC score <C> F1 score <C> precision score <R> <C> text <C> lyrics <C> doc2vec  <C> 0.513 <C> 0.545 <C> 0.593 <R> <C> text <C> lyrics <C> lyricsHAN  <C> 0.518 <C> 0.575 <C> 0.596 <R> <C> text <C> lyrics <C> BiGRU <C> 0.572 <C> 0.602 <C> 0.640 <R> <C> text <C> lyrics+caption <C> attBiGRU <C> 0.581 <C> 0.652 <C> 0.650 <R> <C> audio <C> song clip <C> MFCC  <C> 0.505 <C> 0.549 <C> 0.586 <R> <C> audio <C> song clip <C> convnet  <C> 0.518 <C> 0.614 <C> 0.621 <R> <C> audio <C> song clip <C> DCRNN-MelS <C> 0.536 <C> 0.635 <C> 0.634 <R> <C> audio <C> song clip <C> DCRNN-CQT <C> 0.559 <C> 0.651 <C> 0.643 <R> <C> image <C> image <C> F-VGG  <C> 0.546 <C> 0.594 <C> 0.619 <R> <C> image <C> image <C> F-ResNet  <C> 0.578 <C> 0.618 <C> 0.645 <R> <C> image <C> image <C> F-DenseNet  <C> 0.588 <C> 0.627 <C> 0.653 <R> <C> text+audio <C> lyrics+caption+ song clip <C> early fusion <C> 0.586 <C> 0.660 <C> 0.656 <R> <C> text+audio <C> lyrics+caption+ song clip <C> CMF-AP <C> 0.597 <C> 0.673 <C> 0.668 <R> <C> text+image <C> lyrics+caption+ image <C> early fusion <C> 0.593 <C> 0.663 <C> 0.661 <R> <C> text+image <C> lyrics+caption+ image <C> CMF-AP <C> 0.611 <C> 0.688 <C> 0.683 <R> <C> audio+image <C> image+song clip <C> early fusion <C> 0.589 <C> 0.624 <C> 0.653 <R> <C> audio+image <C> image+song clip <C> CMF-AP <C> 0.603 <C> 0.654 <C> 0.665 <R> <C> text+audio+image <C> lyrics+caption+ image+song clip <C> early fusion <C> 0.602 <C> 0.671 <C> 0.669 <R> <C> text+audio+image <C> lyrics+caption+ image+song clip <C> CMF-AP <C> [BOLD] 0.623 <C> [BOLD] 0.691 <C> [BOLD] 0.688 <R> <C> JTAV=attBiGRU+(F-DenseNet)+(DCRNN-CQT)+(CMF-AP) <C> JTAV=attBiGRU+(F-DenseNet)+(DCRNN-CQT)+(CMF-AP) <C> JTAV=attBiGRU+(F-DenseNet)+(DCRNN-CQT)+(CMF-AP) <C> JTAV=attBiGRU+(F-DenseNet)+(DCRNN-CQT)+(CMF-AP) <C> JTAV=attBiGRU+(F-DenseNet)+(DCRNN-CQT)+(CMF-AP) <C> JTAV=attBiGRU+(F-DenseNet)+(DCRNN-CQT)+(CMF-AP) <CAP> Table 1: Results of the sentiment analysis task
<R> <C> Computational Model <C> Training size ( [ITALIC] arg_c; [ITALIC] other) <C> [ITALIC] dev P <C> [ITALIC] dev R <C> [ITALIC] dev F1 <C> [ITALIC] test P <C> [ITALIC] test R <C> [ITALIC] test F1 <R> <C> [ITALIC] Obparser <C> _ <C> 13.2 <C> 4.0 <C> 6.0 <C> - <C> - <C> - <R> <C> [ITALIC] SVMnoST <C> (229;751) <C> 65.3 <C> 44.6 <C> 52.9 <C> 35.1 <C> 58.9 <C> 44.0 <R> <C> [ITALIC] B_ [ITALIC] LexiconMF <C> _ <C> 65.5 <C> 43.9 <C> 52.5 <C> 48.3 <C> 25.0 <C> 32.9 <R> <C> Self-training (best) <C> - <C> 64.5 <C> 51.3 <C> [BOLD] 57.4 <C> 38.0 <C> 58.3 <C> 46.0 <CAP> Table 4: Experimental results of classifiers on the dev and test set. The self-training results report the best performing parameters.
<R> <C> Model <C> Explicit Features Logistic <C> Explicit Features NBC <C> Explicit Features Adaboost <C> Explicit Features LinearSVM <C> Explicit Features DT <C> Explicit Features RF <C> Explicit Features kNN <R> <C> Precision (%) <C> 82.64 <C> 61.78 <C> 87.14 <C> 82.79 <C> 87.17 <C> 89.22 <C> 65.80 <R> <C> Recall (%) <C> 88.41 <C> 87.75 <C> 85.40 <C> 89.56 <C> 84.53 <C> 84.49 <C> 78.66 <R> <C> F1-score <C> 0.854 <C> 0.680 <C> 0.863 <C> 0.860 <C> 0.858 <C> 0.868 <C> 0.717 <R> <C> Model <C> Semantic Features <C> Semantic Features <C> Semantic Features <C> Model <C> Explicit+Semantic <C> Explicit+Semantic <C> Explicit+Semantic <R> <C> Model <C> CNN <C> GRU <C> AGRU <C> Model <C> CNN+ [ITALIC] F <C> GRU+ [ITALIC] F <C> AGRU+ [ITALIC] F <R> <C> Precision (%) <C> 95.83 <C> 95.76 <C> 93.98 <C> Precision (%) <C> [BOLD] 99.13 <C> 98.60 <C> 97.58 <R> <C> Recall (%) <C> 90.46 <C> 87.24 <C> 86.47 <C> Recall (%) <C> [BOLD] 98.06 <C> 88.47 <C> 86.80 <R> <C> F1-score <C> 0.931 <C> 0.913 <C> 0.901 <C> F1-score <C> [BOLD] 0.986 <C> 0.926 <C> 0.919 <CAP> Table 2: Cross-validation results on WAP196k. We report precision, recall and F1-scores on three groups of models: (1) statistical classification algorithms based on explicit features, including logistic regression, Naive Bayes Classifier (NBC), Linear SVM, Adaboost (SAMME.R algorithm [51]), Decision Tree (DT), Random Forest (RF) and k-nearest-neighbor classifier (kNN); (2) three types of document pair encoders without explicit features; (3) the proposed model in this paper that combines explicit features with convolutional document pair encoders (CNN+F), GRU encoders (GRU+F) or attentive encoders (AGRU+F).
<R> <C> Features <C> Precision <C> Recall <C> F1-score <R> <C> All features <C> 99.13 <C> 98.06 <C> 0.986 <R> <C> No titles <C> 98.03 <C> 85.96 <C> 0.916 <R> <C> No text contents <C> 98.55 <C> 95.78 <C> 0.972 <R> <C> No explicit <C> 95.83 <C> 90.46 <C> 0.931 <R> <C> Explicit only <C> 82.64 <C> 88.41 <C> 0.854 <CAP> Table 3: Ablation on feature categories for CNN+F.
<R> <C> Metrics <C> Heart Failure  [BOLD] (HF) Prediction AUC-ROC <C> Heart Failure  [BOLD] (HF) Prediction Accuracy <C> Heart Failure  [BOLD] (HF) Prediction AUC-PR <C> Relation  [BOLD] (Rel) Classification AUC-ROC <C> Relation  [BOLD] (Rel) Classification Accuracy <C> Relation  [BOLD] (Rel) Classification AUC-PR <C> Semantic  [BOLD] (Sem) Similarity  [ITALIC] ρ <R> <C> M2M <C> [BOLD] 0.685 <C> [BOLD] 0.638 <C> [BOLD] 0.674 <C> [BOLD] 0.833 <C> [BOLD] 0.950 <C> [BOLD] 0.967 <C> [BOLD] 0.650 <R> <C> [ITALIC] M2 [ITALIC] M_ [ITALIC] d <C> 0.640 <C> 0.619 <C> 0.635 <C> 0.346 <C> 0.500 <C> 0.692 <C> 0.154 <R> <C> [ITALIC] M2 [ITALIC] M_ [ITALIC] l <C> 0.627 <C> 0.618 <C> 0.629 <C> 0.354 <C> 0.880 <C> 0.923 <C> 0.577 <R> <C> [ITALIC] M2 [ITALIC] M_ [ITALIC] n <C> 0.620 <C> 0.616 <C> 0.630 <C> 0.352 <C> 0.880 <C> 0.750 <C> 0.154 <R> <C> [ITALIC] M2 [ITALIC] M_ [ITALIC] s <C> 0.657 <C> 0.628 <C> 0.632 <C> 0.481 <C> 0.227 <C> 0.385 <C> 0.154 <R> <C> CONC <C> 0.666 <C> 0.626 <C> 0.673 <C> 0.481 <C> 0.500 <C> 0.692 <C> 0.154 <R> <C> AVG <C> 0.639 <C> 0.627 <C> 0.634 <C> 0.370 <C> 0.700 <C> 0.846 <C> 0.327 <R> <C> SVD <C> 0.679 <C> 0.605 <C> 0.673 <C> 0.574 <C> 0.550 <C> 0.615 <C> 0.154 <R> <C> Hot <C> 0.520 <C> 0.587 <C> 0.587 <C> 0.426 <C> 0.570 <C> 0.077 <C> 0.119 <R> <C> GV <C> 0.592 <C> 0.589 <C> 0.575 <C> 0.500 <C> 0.500 <C> 0.692 <C> 0.153 <R> <C> M2V <C> 0.678 <C> 0.577 <C> 0.670 <C> 0.648 <C> 0.750 <C> 0.850 <C> 0.576 <CAP> Table 2: Performance of different embeddings on three different tasks.
<R> <C> Model <C> k=20 P <C> k=20 R <C> k=20 F1 <C> k=50 P <C> k=50 R <C> k=50 F1 <C> k=100 P <C> k=100 R <C> k=100 F1 <C> k=200 P <C> k=200 R <C> k=200 F1 <R> <C> [ITALIC] word2 [ITALIC] vecG <C> 0.57 <C> 0.19 <C> 0.29 <C> 0.62 <C> [BOLD] 0.33 <C> 0.43 <C> 0.67 <C> 0.41 <C> 0.51 <C> 0.74 <C> 0.46 <C> 0.57 <R> <C> [ITALIC] word2 [ITALIC] vecLR <C> [BOLD] 0.75 <C> 0.19 <C> 0.30 <C> 0.71 <C> 0.31 <C> 0.43 <C> 0.74 <C> 0.38 <C> 0.51 <C> [BOLD] 0.77 <C> 0.44 <C> 0.56 <R> <C> [ITALIC] word2 [ITALIC] vecLL <C> 0.73 <C> 0.22 <C> 0.34 <C> 0.72 <C> 0.32 <C> 0.45 <C> [BOLD] 0.75 <C> 0.40 <C> 0.52 <C> 0.76 <C> 0.47 <C> 0.58 <R> <C> [ITALIC] word2 [ITALIC] vecLLS <C> 0.66 <C> [BOLD] 0.24 <C> [BOLD] 0.36 <C> [BOLD] 0.73 <C> [BOLD] 0.33 <C> [BOLD] 0.52 <C> 0.72 <C> [BOLD] 0.43 <C> [BOLD] 0.54 <C> 0.74 <C> [BOLD] 0.50 <C> [BOLD] 0.60 <CAP> TABLE II: Results comparison (P=Precision, R=Recall)
<R> <C> Analogy Category <C> Accuracy Orthogonal <C> Accuracy Linear <C> Accuracy Translative <C> Avg Cosine Similarity with Solution Orthogonal <C> Avg Cosine Similarity with Solution Linear <C> Avg Cosine Similarity with Solution Translative <R> <C> capital-common-countries <C> [BOLD] 0.957 <C> [BOLD] 0.957 <C> [BOLD] 0.957 <C> 0.802 <C> 0.844 <C> 0.847 <R> <C> capital-world <C> 0.922 <C> [BOLD] 0.966 <C> [BOLD] 0.966 <C> 0.727 <C> 0.786 <C> 0.793 <R> <C> currency <C> 0.300 <C> [BOLD] 0.467 <C> 0.267 <C> 0.517 <C> 0.515 <C> 0.511 <R> <C> city-in-state <C> 0.529 <C> 0.897 <C> [BOLD] 0.926 <C> 0.705 <C> 0.775 <C> 0.802 <R> <C> family <C> [BOLD] 0.913 <C> [BOLD] 0.913 <C> [BOLD] 0.913 <C> 0.840 <C> 0.840 <C> 0.859 <R> <C> gram1-adjective-to-adverb <C> 0.438 <C> [BOLD] 0.500 <C> [BOLD] 0.500 <C> 0.667 <C> 0.670 <C> 0.678 <R> <C> gram2-opposite <C> [BOLD] 0.621 <C> 0.586 <C> 0.517 <C> 0.629 <C> 0.607 <C> 0.632 <R> <C> gram3-comparative <C> 0.865 <C> 0.865 <C> [BOLD] 0.892 <C> 0.791 <C> 0.768 <C> 0.812 <R> <C> gram4-superlative <C> [BOLD] 0.912 <C> 0.882 <C> [BOLD] 0.912 <C> 0.747 <C> 0.705 <C> 0.764 <R> <C> gram5-present-participle <C> 0.909 <C> [BOLD] 0.939 <C> 0.848 <C> 0.813 <C> 0.808 <C> 0.829 <R> <C> gram6-nationality-adjective <C> 0.902 <C> 0.902 <C> [BOLD] 0.927 <C> 0.816 <C> 0.837 <C> 0.841 <R> <C> gram7-past-tense <C> 0.600 <C> [BOLD] 0.650 <C> 0.625 <C> 0.768 <C> 0.770 <C> 0.775 <R> <C> gram8-plural <C> 0.892 <C> [BOLD] 0.919 <C> 0.892 <C> 0.796 <C> 0.787 <C> 0.807 <R> <C> gram9-plural-verbs <C> [BOLD] 0.900 <C> 0.733 <C> 0.800 <C> 0.786 <C> 0.773 <C> 0.803 <R> <C> Avg <C> 0.761 <C> [BOLD] 0.798 <C> 0.782 <C> 0.743 <C> 0.749 <C> 0.768 <CAP> Table 1: The accuracy on our word analogy task – detailed in section 4.1 – when word relationships are represented as orthogonal, linear, and translative functions. The highest accuracy for each category is in bold. As seen in the last row, on average, orthogonal transformations are almost as accurate as translations (0.761 vs. 0.782), and the average cosine similarity between a transformed vector and the solution is about the same for both.
<R> <C> Pre-train <C> Speaker embeddings <C> [BOLD] R2@1 <C> [BOLD] R10@1 <C> [BOLD] R10@2 <C> [BOLD] R10@5 <R> <C> No <C> No <C> 0.950 <C> 0.781 <C> 0.890 <C> 0.980 <R> <C> No <C> Yes <C> 0.950 <C> 0.786 <C> 0.890 <C> 0.981 <R> <C> Yes <C> No <C> 0.961 <C> 0.825 <C> 0.915 <C> 0.984 <R> <C> [BOLD] Yes <C> [BOLD] Yes <C> [BOLD] 0.963 <C> [BOLD] 0.830 <C> [BOLD] 0.919 <C> [BOLD] 0.985 <CAP> Table 9: Results on the test set of Ubuntu Dialogue Corpus V2, by ablating the speaker embeddings.
<R> <C> Source Pitch (Hz) <C> Source  [ITALIC] Oq <C> Source  [ITALIC] αm <C> Filter Vowel type <C> Noise SNR (dB) <R> <C> 100:5:240 <C> 0.3:0.05:0.9 <C> 0.55:0.05:0.8 <C> 14 vowels <C> 10:10:80 <CAP> Table 1: Table of synthesis parameter variation range.
<R> <C> [EMPTY] <C> [BOLD] DM ID <C> [BOLD] DM OOD <C> [BOLD] PAS ID <C> [BOLD] PAS OOD <C> [BOLD] PSD ID <C> [BOLD] PSD OOD <C> [BOLD] Avg ID <C> [BOLD] Avg OOD <R> <C> (Du et al.,  2015 ) <C> 89.1 <C> 81.8 <C> 91.3 <C> 87.2 <C> 75.7 <C> 73.3 <C> 85.3 <C> 80.8 <R> <C> (Almeida and Martins,  2015 ) <C> 88.2 <C> 81.8 <C> 90.9 <C> 86.9 <C> 76.4 <C> 74.8 <C> 85.2 <C> 81.2 <R> <C> WCGL18 <C> 90.3 <C> 84.9 <C> 91.7 <C> 87.6 <C> 78.6 <C> 75.9 <C> 86.9 <C> 82.8 <R> <C> PTS17: Basic <C> 89.4 <C> 84.5 <C> 92.2 <C> 88.3 <C> 77.6 <C> 75.3 <C> 87.4 <C> 83.6 <R> <C> PTS17: Freda3 <C> 90.4 <C> 85.3 <C> 92.7 <C> 89.0 <C> 78.5 <C> 76.4 <C> 88.0 <C> 84.4 <R> <C> Ours: Basic <C> 91.4 <C> 86.9 <C> 93.9 <C> [BOLD] 90.8 <C> 79.1 <C> 77.5 <C> 88.1 <C> 85.0 <R> <C> Ours: +Char <C> 92.7 <C> 87.8 <C> [BOLD] 94.0 <C> 90.6 <C> 80.5 <C> 78.6 <C> 89.1 <C> 85.7 <R> <C> Ours: +Lemma <C> 93.3 <C> 88.8 <C> 93.9 <C> 90.5 <C> 80.3 <C> 78.7 <C> 89.1 <C> 86.0 <R> <C> Ours: +Char +Lemma <C> [BOLD] 93.7 <C> [BOLD] 88.9 <C> 93.9 <C> 90.6 <C> [BOLD] 81.0 <C> [BOLD] 79.4 <C> [BOLD] 89.5 <C> [BOLD] 86.3 <CAP> Table 1: Comparison between our system and the previous state of the art on in-domain (WSJ) and out-of-domain (Brown corpus) data, according to labeled F1 (LF1).
<R> <C> Parameter <C> Value <R> <C> dropout probability <C> 0.25 <R> <C> wordHiddensize <C> 100 <R> <C> charHiddensize <C> 60 <R> <C> charEmbSize <C> 30 <R> <C> wordEmbSize <C> 50 <R> <C> wordEmbFineTune <C> True <R> <C> charEmbFineTune <C> True <R> <C> initial  [ITALIC] η <C> 0.01 <R> <C> regularization  [ITALIC] λ <C> 1e-8 <CAP> Table 5: Hyper Parameters
<R> <C> Model <C> PKU P <C> PKU R <C> PKU F <C> MSR P <C> MSR R <C> MSR F <C> CTB60 P <C> CTB60 R <C> CTB60 F <R> <C> Discrete <C> 95.42 <C> 94.56 <C> 94.99 <C> 96.94 <C> 96.61 <C> 96.78 <C> 95.43 <C> 95.16 <C> 95.29 <R> <C> Neural <C> 94.29 <C> 94.56 <C> 94.42 <C> 96.79 <C> [BOLD] 97.54 <C> 97.17 <C> 94.48 <C> 95.01 <C> 94.75 <R> <C> Joint <C> [BOLD] 95.74 <C> [BOLD] 95.12 <C> [BOLD] 95.42 <C> [BOLD] 97.01 <C> 97.39 <C> [BOLD] 97.20 <C> [BOLD] 95.68 <C> [BOLD] 95.64 <C> [BOLD] 95.66 <R> <C> State-of-the-art <C> [EMPTY] <C> [EMPTY] <C> 94.50 <C> [EMPTY] <C> [EMPTY] <C> [BOLD] 97.20 <C> [EMPTY] <C> [EMPTY] <C> 95.05 <CAP> Table 7: Chinese Word Segmentation Results
<R> <C> Model <C> mAP <R> <C> random <C> 29.68 <R> <C> RGB MAttNet <C> 41.51 <R> <C> flow MAttNet <C> 39.02 <R> <C> flow5 MAttNet <C> 41.90 <R> <C> fused1 MAttNet <C> [BOLD] 44.66 <R> <C> fused5 MAttNet <C> 42.82 <CAP> Table 3: Identifying Description Localization: mAP for each collection. (values are in percents.) The fused1 MAttNet is the proposed two-stream method and the fused5 MAttNet is the stacked version of the proposed two-stream method.
<R> <C> Model <C> IoU@0.5 <R> <C> RGB MAttNet <C> [BOLD] 35.02 <R> <C> flow MAttNet <C> 22.63 <R> <C> flow5 MAttNet <C> 28.98 <R> <C> fused1 MAttNet <C> 23.93 <R> <C> fused5 MAttNet <C> 24.26 <R> <C> [ITALIC] FGFA most conf. <C> [ITALIC] 35.87 <R> <C> [ITALIC] FGFA 2nd conf. <C> [ITALIC] 34.16 <CAP> Table 6: Visual Object Detection: mAP tracklet IoU@0.5 for each model. (values are in percents.)
<R> <C> [EMPTY] <C> [BOLD] Val <C> [BOLD] Aro <C> [BOLD] Dom <C> [BOLD] Joy <C> [BOLD] Ang <C> [BOLD] Sad <C> [BOLD] Fea <C> [BOLD] Dis <R> <C> [ITALIC] sunshine <C> 8.1 <C> 5.3 <C> 5.4 <C> 4.2 <C> 1.2 <C> 1.3 <C> 1.3 <C> 1.2 <R> <C> [ITALIC] terrorism <C> 1.6 <C> 7.4 <C> 2.7 <C> 1.2 <C> 2.9 <C> 3.3 <C> 3.9 <C> 2.5 <R> <C> [ITALIC] nuclear <C> 4.3 <C> 7.3 <C> 4.1 <C> 1.4 <C> 2.2 <C> 1.9 <C> 3.2 <C> 1.6 <R> <C> [ITALIC] ownership <C> 5.9 <C> 4.4 <C> 7.5 <C> 2.1 <C> 1.4 <C> 1.2 <C> 1.4 <C> 1.3 <CAP> Table 1: Sample entries from our English source lexicon described via eight emotional variables: Valence, Arousal, Dominance [VAD], and Joy, Anger, Sadness, Fear, and Disgust [BE5]. VAD uses 1-to-9 scales (“5” encodes the neutral value) and BE5 1-to-5 scales (“1” encodes the neutral value).
<R> <C> [EMPTY] <C> [EMPTY] <C> En-Fr → <C> En-Fr ← <C> En-De → <C> En-De ← <C> En-Es → <C> En-Es ← <R> <C> Dev <C> TM <C> 46.62 <C> 42.53 <C> 34.99 <C> 42.45 <C> 40.84 <C> 39.71 <R> <C> Dev <C> NMT <C> 58.95 <C> 59.69 <C> 44.94 <C> 50.20 <C> 50.54 <C> 55.02 <R> <C> Dev <C> Copy <C> 60.34 <C> 61.61 <C> - <C> - <C> - <C> - <R> <C> [EMPTY] <C> Ours <C> [BOLD] 64.16 <C> [BOLD] 64.64 <C> [BOLD] 49.26 <C> [BOLD] 55.63 <C> [BOLD] 57.62 <C> [BOLD] 60.28 <R> <C> Test <C> TM <C> 46.64 <C> 43.17 <C> 34.61 <C> 41.83 <C> 39.55 <C> 37.73 <R> <C> Test <C> NMT <C> 59.42 <C> 60.11 <C> 43.98 <C> 49.74 <C> 50.48 <C> 54.66 <R> <C> Test <C> Copy <C> 60.55 <C> 62.02 <C> - <C> - <C> - <C> - <R> <C> [EMPTY] <C> Ours <C> [BOLD] 64.60 <C> [BOLD] 65.11 <C> [BOLD] 48.80 <C> [BOLD] 55.33 <C> [BOLD] 57.27 <C> [BOLD] 59.34 <CAP> Table 2: The BLEU scores on JRC-Acquis corpus.
<R> <C> [EMPTY] <C> [ITALIC] d1 <C> [ITALIC] d2 <C> [ITALIC] d3 <C> [ITALIC] d4 <C> [ITALIC] d5 <C> [ITALIC] d6 <C> [ITALIC] d7 <C> [ITALIC] d8 <R> <C> banana <C> 2 <C> – <C> – <C> – <C> 5 <C> – <C> 5 <C> – <R> <C> apple <C> 4 <C> 3 <C> 4 <C> 6 <C> 3 <C> – <C> – <C> – <R> <C> orange <C> – <C> 2 <C> 1 <C> – <C> – <C> 7 <C> – <C> 3 <R> <C> fruit <C> – <C> 1 <C> 3 <C> – <C> 4 <C> 3 <C> 5 <C> 3 <R> <C> tree <C> – <C> – <C> 5 <C> – <C> – <C> 5 <C> – <C> – <R> <C> computer <C> – <C> – <C> – <C> 6 <C> – <C> – <C> – <C> – <CAP> TABLE I: A table of hypothetical occurrences of words in a set of documents, d1 to d8.
<R> <C> Model <C> Metrics F1 <C> Metrics EM <C> Metrics AvNA <R> <C> Baseline <C> 59.94 <C> 56.55 <C> 66.90 <R> <C> Our Best Model <C> 75.48 <C> 71.96 <C> 79.68 <R> <C> Without CharEmb <C> 73.25 <C> 69.87 <C> 78.01 <R> <C> [ [ITALIC] CLS] AvNA Classifier <C> 73.49 <C> 69.76 <C> 78.23 <R> <C> Human rajpurkar2018know <C> 89.45 <C> 86.83 <C> - <CAP> Table 1: Model Performance on Dev Set
<R> <C> [BOLD] Models <C> [BOLD] Ptr-Net <C> [BOLD] FE-Net <C> [BOLD] Agree-MTL <C> [BOLD] GAN <C> [BOLD] MM-GAN <R> <C> ROUGE-1 <C> 59.21 <C> 61.07 <C> 66.19 <C> 60.67 <C> [BOLD] 69.53 <R> <C> ROUGE-2 <C> 42.01 <C> 44.16 <C> 49.39 <C> 46.46 <C> [BOLD] 52.38 <R> <C> ROUGE-L <C> 57.12 <C> 58.00 <C> 64.04 <C> 60.27 <C> [BOLD] 65.80 <CAP> Table 2: ROUGE performance of different models on the test set.
<R> <C> [BOLD] ERR modification <C> UCTR <C> MaxRR <C> MinRR <C> MeanRR <C> PLC <R> <C> Extended IA (ERR-EIA) <C> [BOLD] 0.318 <C> [BOLD] 0.422 <C> [BOLD] 0.379 <C> [BOLD] 0.386 <C> [BOLD] 0.389 <R> <C> Intent-Agnostic learned (ERR) <C> 0.283 <C> 0.337 <C> 0.309 <C> 0.314 <C> 0.317 <R> <C> IA learned “same params" <C> 0.282 <C> 0.409 <C> 0.361 <C> 0.368 <C> 0.372 <R> <C> Intent-Agnostic default <C> 0.27 <C> 0.326 <C> 0.298 <C> 0.303 <C> 0.305 <R> <C> IA learned “diff params" <C> 0.268 <C> 0.394 <C> 0.347 <C> 0.354 <C> 0.357 <R> <C> IA default <C> 0.131 <C> 0.178 <C> 0.152 <C> 0.157 <C> 0.158 <CAP> Table 3: The correlations of the multilingual metrics with online metrics.
<R> <C> Task <C> Source <C> Train  [ITALIC] n sents <C> Train % positive <C> Dev  [ITALIC] n sents <C> Test  [ITALIC] n sents <R> <C> Relation Detection <C> SemEval 2010 <C> 8,096 <C> 19.3 <C> 1,361 <C> 1,372 <R> <C> Relation Detection <C> Wikipedia <C> 1,733 <C> 10.0 <C> 361 <C> 354 <R> <C> NE Detection <C> Ontonotes 5.0 <C> 89,389 <C> 29.7 <C> 11,289 <C> 11,318 <CAP> Table 2: Overview of the data sets.
<R> <C> Model <C> MR <C> Aprl <C> Baby <C> Books <C> Camera <C> Avg <R> <C> Mixture <C> 69.8 <C> 80.8 <C> 82.5 <C> 77.0 <C> 80.9 <C> 78.20 <R> <C> +Bandit <C> 72.0 <C> 82.3 <C> 82.8 <C> 78.3 <C> 81.3 <C> 79.30 <CAP> Table 5: Multi-source DistanceNet versus bandit.
<R> <C> WE <C> CLE <C> Bert <C> UPOS <C> XPOS <C> UFeats <C> Lemma <C> UAS <C> LAS <C> MLAS <C> BLEX <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 90.14 <C> 88.51 <C> 86.50 <C> 88.64 <C> 79.43 <C> 73.55 <C> 56.52 <C> 60.84 <R> <C> WE <C> [EMPTY] <C> [EMPTY] <C> 94.91 <C> 93.51 <C> 91.89 <C> 92.10 <C> 85.98 <C> 81.73 <C> 68.47 <C> 70.64 <R> <C> [EMPTY] <C> CLE <C> [EMPTY] <C> 95.75 <C> 94.69 <C> 93.43 <C> 96.24 <C> 86.99 <C> 82.96 <C> 71.06 <C> 75.78 <R> <C> WE <C> CLE <C> [EMPTY] <C> 96.39 <C> 95.53 <C> 94.28 <C> 96.51 <C> 87.79 <C> 84.09 <C> 73.30 <C> 77.36 <R> <C> [EMPTY] <C> [EMPTY] <C> Base <C> 96.35 <C> 95.08 <C> 93.56 <C> 93.29 <C> 89.31 <C> 85.69 <C> 74.11 <C> 75.45 <R> <C> WE <C> [EMPTY] <C> Base <C> 96.62 <C> 95.54 <C> 94.08 <C> 93.77 <C> 89.49 <C> 85.96 <C> 74.94 <C> 76.27 <R> <C> [EMPTY] <C> CLE <C> Base <C> 96.86 <C> 95.96 <C> 94.85 <C> 96.64 <C> 89.76 <C> 86.29 <C> 76.20 <C> 79.87 <R> <C> WE <C> CLE <C> Base <C> [BOLD] 97.00 <C> [BOLD] 96.17 <C> [BOLD] 94.97 <C> [BOLD] 96.66 <C> [BOLD] 89.81 <C> [BOLD] 86.42 <C> [BOLD] 76.54 <C> [BOLD] 80.04 <CAP> Table 1: BERT Base compared to word embeddings (WE) and character-level word embeddings (CLE). Results for 72 UD 2.3 treebanks with train and development sets and non-empty Wikipedia.
<R> <C> Language <C> Bert <C> UPOS <C> XPOS <C> UFeats <C> Lemma <C> UAS <C> LAS <C> MLAS <C> BLEX <R> <C> English <C> Base <C> [BOLD] 97.38 <C> [BOLD] 96.97 <C> 97.22 <C> [BOLD] 97.71 <C> [BOLD] 91.09 <C> [BOLD] 88.22 <C> [BOLD] 80.48 <C> [BOLD] 82.38 <R> <C> English <C> Multi <C> 97.36 <C> [BOLD] 96.97 <C> [BOLD] 97.29 <C> 97.63 <C> 90.94 <C> 88.12 <C> 80.43 <C> 82.22 <R> <C> Chinese <C> Base <C> [BOLD] 97.07 <C> [BOLD] 96.89 <C> [BOLD] 99.58 <C> 99.98 <C> [BOLD] 90.13 <C> [BOLD] 86.74 <C> [BOLD] 79.67 <C> [BOLD] 83.85 <R> <C> Chinese <C> Multi <C> 96.27 <C> 96.25 <C> 99.37 <C> [BOLD] 99.99 <C> 87.58 <C> 83.96 <C> 76.26 <C> 81.04 <R> <C> Japanese <C> Base <C> [BOLD] 98.24 <C> [BOLD] 97.89 <C> 99.98 <C> [BOLD] 99.53 <C> [BOLD] 95.55 <C> [BOLD] 94.27 <C> [BOLD] 87.64 <C> [BOLD] 89.24 <R> <C> Japanese <C> Multi <C> 98.17 <C> 97.71 <C> [BOLD] 99.99 <C> 99.51 <C> 95.30 <C> 93.99 <C> 87.17 <C> 88.77 <CAP> Table 2: Comparison of multilingual and language-specific BERT models on 4 English treebanks (each experiment repeated 3 times), and on Chinese-GSD and Japanese-GSD treebanks.
<R> <C> Language <C> BERT Train <C> UDPipe 2.0 with WE+CLE UPOS <C> UDPipe 2.0 with WE+CLE XPOS <C> UDPipe 2.0 with WE+CLE UFeats <C> UDPipe 2.0 with WE+CLE Lemmas <C> UDPipe 2.0 with WE+CLE UAS <C> UDPipe 2.0 with WE+CLE LAS <C> UDPipe 2.0 with WE+CLE MLAS <C> UDPipe 2.0 with WE+CLE BLEX <C> UDPipe 2.0 with WE+CLE+BERT+Flair where available UPOS <C> UDPipe 2.0 with WE+CLE+BERT+Flair where available XPOS <C> UDPipe 2.0 with WE+CLE+BERT+Flair where available UFeats <C> UDPipe 2.0 with WE+CLE+BERT+Flair where available Lemmas <C> UDPipe 2.0 with WE+CLE+BERT+Flair where available UAS <C> UDPipe 2.0 with WE+CLE+BERT+Flair where available LAS <C> UDPipe 2.0 with WE+CLE+BERT+Flair where available MLAS <C> UDPipe 2.0 with WE+CLE+BERT+Flair where available BLEX <R> <C> Afrikaans-AfriBooms <C> [EMPTY] <C> 98.25 <C> 94.48 <C> 97.66 <C> 97.46 <C> 89.38 <C> 86.58 <C> 77.66 <C> 77.82 <C> [BOLD] 98.73 <C> [BOLD] 95.82 <C> [BOLD] 98.49 <C> [BOLD] 97.60 <C> [BOLD] 90.71 <C> [BOLD] 88.35 <C> [BOLD] 81.14 <C> [BOLD] 80.17 <R> <C> Ancient Greek-PROIEL <C> ✗ <C> [BOLD] 97.86 <C> [BOLD] 98.08 <C> [BOLD] 92.44 <C> [BOLD] 93.51 <C> [BOLD] 85.93 <C> [BOLD] 82.11 <C> [BOLD] 67.16 <C> [BOLD] 71.22 <C> 97.75 <C> 97.99 <C> 92.29 <C> 93.26 <C> 85.87 <C> 82.08 <C> 66.89 <C> 70.68 <R> <C> Ancient Greek-Perseus <C> ✗ <C> [BOLD] 93.27 <C> [BOLD] 86.22 <C> [BOLD] 91.39 <C> [BOLD] 85.02 <C> [BOLD] 78.85 <C> [BOLD] 73.54 <C> [BOLD] 53.87 <C> [BOLD] 53.19 <C> 92.95 <C> 85.46 <C> 90.94 <C> 84.59 <C> 78.55 <C> 72.96 <C> 52.92 <C> 52.62 <R> <C> Arabic-PADT <C> [EMPTY] <C> 96.83 <C> 93.97 <C> 94.11 <C> 95.28 <C> 87.54 <C> 82.94 <C> 73.92 <C> 75.87 <C> [BOLD] 96.98 <C> [BOLD] 94.57 <C> [BOLD] 94.72 <C> [BOLD] 95.43 <C> [BOLD] 89.01 <C> [BOLD] 84.62 <C> [BOLD] 76.28 <C> [BOLD] 77.81 <R> <C> Armenian-ArmTDP <C> [EMPTY] <C> 93.49 <C> — <C> 82.85 <C> 92.86 <C> 78.62 <C> 71.27 <C> 48.11 <C> 60.11 <C> [BOLD] 95.30 <C> — <C> [BOLD] 86.89 <C> [BOLD] 93.61 <C> [BOLD] 82.86 <C> [BOLD] 76.60 <C> [BOLD] 56.15 <C> [BOLD] 65.53 <R> <C> Basque-BDT <C> [EMPTY] <C> 96.11 <C> — <C> 92.48 <C> 96.29 <C> 86.11 <C> 82.86 <C> 72.33 <C> 78.54 <C> [BOLD] 96.48 <C> — <C> [BOLD] 93.32 <C> [BOLD] 96.43 <C> [BOLD] 87.63 <C> [BOLD] 84.50 <C> [BOLD] 74.91 <C> [BOLD] 80.10 <R> <C> Belarusian-HSE <C> [EMPTY] <C> 93.63 <C> 89.80 <C> 73.30 <C> 87.34 <C> 78.58 <C> 72.72 <C> 46.20 <C> 58.28 <C> [BOLD] 96.24 <C> [BOLD] 93.27 <C> [BOLD] 79.67 <C> [BOLD] 89.22 <C> [BOLD] 88.49 <C> [BOLD] 83.21 <C> [BOLD] 58.44 <C> [BOLD] 69.11 <R> <C> Bulgarian-BTB <C> [EMPTY] <C> 98.98 <C> 97.00 <C> 97.82 <C> 97.94 <C> 93.38 <C> 90.35 <C> 83.63 <C> 84.42 <C> [BOLD] 99.20 <C> [BOLD] 97.57 <C> [BOLD] 98.22 <C> [BOLD] 98.25 <C> [BOLD] 95.34 <C> [BOLD] 92.62 <C> [BOLD] 87.00 <C> [BOLD] 87.59 <R> <C> Buryat-BDT <C> ✗ <C> 40.34 <C> — <C> 32.40 <C> [BOLD] 58.17 <C> 32.60 <C> [BOLD] 18.83 <C> 1.26 <C> [BOLD] 6.49 <C> [BOLD] 45.50 <C> — <C> [BOLD] 33.49 <C> 57.42 <C> [BOLD] 35.88 <C> 18.28 <C> [BOLD] 1.48 <C> 5.82 <R> <C> Catalan-AnCora <C> [EMPTY] <C> 98.88 <C> 98.88 <C> 98.37 <C> 99.07 <C> 93.22 <C> 91.06 <C> 84.48 <C> 86.18 <C> [BOLD] 99.06 <C> [BOLD] 99.06 <C> [BOLD] 98.60 <C> [BOLD] 99.25 <C> [BOLD] 94.49 <C> [BOLD] 92.74 <C> [BOLD] 87.36 <C> [BOLD] 88.90 <R> <C> Chinese-GSD <C> [EMPTY] <C> 94.88 <C> 94.72 <C> 99.22 <C> [BOLD] 99.99 <C> 84.64 <C> 80.50 <C> 71.04 <C> 76.78 <C> [BOLD] 97.07 <C> [BOLD] 96.89 <C> [BOLD] 99.58 <C> 99.98 <C> [BOLD] 90.13 <C> [BOLD] 86.74 <C> [BOLD] 79.67 <C> [BOLD] 83.85 <R> <C> Coptic-Scriptorium <C> ✗ <C> [BOLD] 94.72 <C> [BOLD] 93.52 <C> 96.27 <C> 95.53 <C> [BOLD] 85.69 <C> [BOLD] 81.08 <C> 64.65 <C> 68.65 <C> 94.55 <C> 93.15 <C> [BOLD] 96.44 <C> [BOLD] 95.73 <C> 85.10 <C> 80.52 <C> [BOLD] 65.16 <C> [BOLD] 68.81 <R> <C> Croatian-SET <C> [EMPTY] <C> 98.13 <C> — <C> 92.25 <C> 97.27 <C> 91.10 <C> 86.78 <C> 73.61 <C> 81.19 <C> [BOLD] 98.45 <C> — <C> [BOLD] 93.27 <C> [BOLD] 97.64 <C> [BOLD] 93.20 <C> [BOLD] 89.35 <C> [BOLD] 77.08 <C> [BOLD] 84.44 <R> <C> Czech-CAC <C> [EMPTY] <C> 99.37 <C> 96.66 <C> 96.34 <C> 98.57 <C> 92.99 <C> 90.71 <C> 84.30 <C> 87.18 <C> [BOLD] 99.44 <C> [BOLD] 96.94 <C> [BOLD] 96.62 <C> [BOLD] 98.73 <C> [BOLD] 93.59 <C> [BOLD] 91.50 <C> [BOLD] 85.84 <C> [BOLD] 88.47 <R> <C> Czech-CLTT <C> [EMPTY] <C> 98.88 <C> 91.18 <C> 91.59 <C> [BOLD] 98.25 <C> 86.90 <C> 84.03 <C> 71.63 <C> 79.20 <C> [BOLD] 99.32 <C> [BOLD] 92.67 <C> [BOLD] 92.88 <C> 98.22 <C> [BOLD] 89.59 <C> [BOLD] 87.01 <C> [BOLD] 75.53 <C> [BOLD] 82.13 <R> <C> Czech-FicTree <C> [EMPTY] <C> 98.55 <C> 95.04 <C> 95.87 <C> 98.63 <C> 92.91 <C> 89.75 <C> 81.04 <C> 85.49 <C> [BOLD] 98.82 <C> [BOLD] 96.16 <C> [BOLD] 96.88 <C> [BOLD] 98.84 <C> [BOLD] 94.34 <C> [BOLD] 91.87 <C> [BOLD] 84.80 <C> [BOLD] 88.16 <R> <C> Czech-PDT <C> [EMPTY] <C> 99.18 <C> 97.28 <C> 97.23 <C> 99.02 <C> 93.33 <C> 91.31 <C> 86.15 <C> 88.60 <C> [BOLD] 99.34 <C> [BOLD] 97.71 <C> [BOLD] 97.67 <C> [BOLD] 99.12 <C> [BOLD] 94.43 <C> [BOLD] 92.56 <C> [BOLD] 88.09 <C> [BOLD] 90.22 <R> <C> Danish-DDT <C> [EMPTY] <C> 97.78 <C> — <C> 97.33 <C> 97.52 <C> 86.88 <C> 84.31 <C> 76.29 <C> 78.51 <C> [BOLD] 98.21 <C> — <C> [BOLD] 97.77 <C> [BOLD] 97.72 <C> [BOLD] 89.32 <C> [BOLD] 87.24 <C> [BOLD] 80.58 <C> [BOLD] 81.93 <R> <C> Dutch-Alpino <C> [EMPTY] <C> 96.83 <C> 94.80 <C> 96.33 <C> 97.09 <C> 91.37 <C> 88.38 <C> 77.28 <C> 79.82 <C> [BOLD] 97.55 <C> [BOLD] 95.87 <C> [BOLD] 97.34 <C> [BOLD] 97.28 <C> [BOLD] 94.12 <C> [BOLD] 91.78 <C> [BOLD] 83.12 <C> [BOLD] 84.42 <R> <C> Dutch-LassySmall <C> [EMPTY] <C> 96.50 <C> 95.08 <C> 96.42 <C> 97.41 <C> 90.20 <C> 86.39 <C> 77.19 <C> 78.83 <C> [BOLD] 96.87 <C> [BOLD] 95.91 <C> [BOLD] 96.97 <C> [BOLD] 97.55 <C> [BOLD] 93.07 <C> [BOLD] 89.88 <C> [BOLD] 82.00 <C> [BOLD] 83.26 <R> <C> English-EWT <C> [EMPTY] <C> 96.29 <C> 96.10 <C> 97.10 <C> 98.25 <C> 89.63 <C> 86.97 <C> 79.00 <C> 82.36 <C> [BOLD] 97.59 <C> [BOLD] 97.41 <C> [BOLD] 97.82 <C> [BOLD] 98.84 <C> [BOLD] 92.50 <C> [BOLD] 90.40 <C> [BOLD] 84.41 <C> [BOLD] 87.03 <R> <C> English-GUM <C> [EMPTY] <C> 96.02 <C> 95.90 <C> 96.82 <C> 96.85 <C> 87.27 <C> 84.12 <C> 73.51 <C> 74.68 <C> [BOLD] 96.93 <C> [BOLD] 96.73 <C> [BOLD] 97.59 <C> [BOLD] 97.22 <C> [BOLD] 91.47 <C> [BOLD] 88.80 <C> [BOLD] 80.14 <C> [BOLD] 80.62 <R> <C> English-LinES <C> [EMPTY] <C> 96.91 <C> 95.62 <C> 96.31 <C> 96.45 <C> 84.15 <C> 79.71 <C> 71.38 <C> 73.22 <C> [BOLD] 97.86 <C> [BOLD] 96.94 <C> [BOLD] 97.48 <C> [BOLD] 96.87 <C> [BOLD] 87.28 <C> [BOLD] 83.48 <C> [BOLD] 77.45 <C> [BOLD] 78.36 <R> <C> English-ParTUT <C> [EMPTY] <C> 96.10 <C> 95.83 <C> 95.51 <C> 97.74 <C> 90.29 <C> 87.27 <C> 76.44 <C> 80.33 <C> [BOLD] 97.43 <C> [BOLD] 97.25 <C> [BOLD] 96.54 <C> [BOLD] 98.09 <C> [BOLD] 93.75 <C> [BOLD] 91.12 <C> [BOLD] 81.74 <C> [BOLD] 85.13 <R> <C> Estonian-EDT <C> [EMPTY] <C> 97.64 <C> 98.27 <C> 96.23 <C> 95.30 <C> 88.00 <C> 85.18 <C> 78.72 <C> 78.51 <C> [BOLD] 97.83 <C> [BOLD] 98.36 <C> [BOLD] 96.42 <C> [BOLD] 95.44 <C> [BOLD] 89.46 <C> [BOLD] 86.77 <C> [BOLD] 80.62 <C> [BOLD] 80.17 <R> <C> Finnish-FTB <C> [EMPTY] <C> 96.65 <C> 95.39 <C> 96.62 <C> 95.49 <C> 90.68 <C> 87.89 <C> 80.58 <C> 81.18 <C> [BOLD] 96.97 <C> [BOLD] 95.61 <C> [BOLD] 96.73 <C> [BOLD] 95.57 <C> [BOLD] 91.68 <C> [BOLD] 89.02 <C> [BOLD] 82.25 <C> [BOLD] 82.69 <R> <C> Finnish-TDT <C> [EMPTY] <C> 97.45 <C> 98.12 <C> 95.43 <C> 91.45 <C> 89.88 <C> 87.46 <C> 80.43 <C> 76.64 <C> [BOLD] 97.57 <C> [BOLD] 98.24 <C> [BOLD] 95.80 <C> [BOLD] 91.68 <C> [BOLD] 91.66 <C> [BOLD] 89.49 <C> [BOLD] 82.89 <C> [BOLD] 78.57 <R> <C> French-GSD <C> [EMPTY] <C> 97.63 <C> — <C> 97.13 <C> 98.35 <C> 90.65 <C> 88.06 <C> 79.76 <C> 82.39 <C> [BOLD] 97.98 <C> — <C> [BOLD] 97.42 <C> [BOLD] 98.43 <C> [BOLD] 92.55 <C> [BOLD] 90.31 <C> [BOLD] 82.66 <C> [BOLD] 85.09 <R> <C> French-ParTUT <C> [EMPTY] <C> 96.93 <C> 96.47 <C> 94.43 <C> 95.70 <C> 92.17 <C> 89.63 <C> 75.22 <C> 78.07 <C> [BOLD] 97.64 <C> [BOLD] 97.35 <C> [BOLD] 95.12 <C> [BOLD] 96.06 <C> [BOLD] 94.51 <C> [BOLD] 92.47 <C> [BOLD] 80.50 <C> [BOLD] 82.19 <R> <C> French-Sequoia <C> [EMPTY] <C> 98.79 <C> — <C> 98.09 <C> 98.57 <C> 92.37 <C> 90.73 <C> 84.51 <C> 85.93 <C> [BOLD] 99.32 <C> — <C> [BOLD] 98.62 <C> [BOLD] 98.89 <C> [BOLD] 94.88 <C> [BOLD] 93.81 <C> [BOLD] 89.10 <C> [BOLD] 90.08 <R> <C> French-Spoken <C> [EMPTY] <C> 95.91 <C> 97.30 <C> — <C> [BOLD] 96.92 <C> 82.90 <C> 77.53 <C> 68.24 <C> 69.47 <C> [BOLD] 97.23 <C> [BOLD] 97.48 <C> — <C> 96.75 <C> [BOLD] 86.27 <C> [BOLD] 81.40 <C> [BOLD] 73.26 <C> [BOLD] 73.36 <R> <C> Galician-CTG <C> [EMPTY] <C> 97.84 <C> 97.47 <C> [BOLD] 99.83 <C> 98.58 <C> 86.44 <C> 83.82 <C> 72.46 <C> 77.21 <C> [BOLD] 98.06 <C> [BOLD] 97.70 <C> [BOLD] 99.83 <C> [BOLD] 98.81 <C> [BOLD] 86.94 <C> [BOLD] 84.43 <C> [BOLD] 73.72 <C> [BOLD] 78.33 <R> <C> Galician-TreeGal <C> [EMPTY] <C> 95.82 <C> 92.46 <C> 93.96 <C> 97.06 <C> 82.72 <C> 77.69 <C> 63.73 <C> 68.89 <C> [BOLD] 97.30 <C> [BOLD] 95.01 <C> [BOLD] 96.03 <C> [BOLD] 97.71 <C> [BOLD] 86.62 <C> [BOLD] 82.62 <C> [BOLD] 72.29 <C> [BOLD] 76.24 <R> <C> German-GSD <C> [EMPTY] <C> 94.48 <C> 97.31 <C> 90.68 <C> [BOLD] 96.80 <C> 85.53 <C> 81.07 <C> 58.82 <C> 72.13 <C> [BOLD] 95.18 <C> [BOLD] 97.95 <C> [BOLD] 91.72 <C> 96.77 <C> [BOLD] 88.11 <C> [BOLD] 84.06 <C> [BOLD] 63.33 <C> [BOLD] 75.44 <R> <C> Gothic-PROIEL <C> ✗ <C> 96.66 <C> [BOLD] 97.23 <C> [BOLD] 90.77 <C> [BOLD] 94.72 <C> 85.27 <C> 79.60 <C> 66.71 <C> [BOLD] 72.86 <C> [BOLD] 96.72 <C> 97.22 <C> 90.58 <C> 94.47 <C> [BOLD] 85.53 <C> [BOLD] 79.69 <C> [BOLD] 66.86 <C> 72.52 <R> <C> Greek-GDT <C> [EMPTY] <C> 97.98 <C> 97.99 <C> 94.96 <C> 95.82 <C> 92.10 <C> 89.79 <C> 78.60 <C> 79.72 <C> [BOLD] 98.25 <C> [BOLD] 98.25 <C> [BOLD] 95.76 <C> [BOLD] 95.88 <C> [BOLD] 93.92 <C> [BOLD] 92.16 <C> [BOLD] 82.29 <C> [BOLD] 82.14 <R> <C> Hebrew-HTB <C> [EMPTY] <C> 97.02 <C> 97.03 <C> 95.87 <C> 97.12 <C> 89.70 <C> 86.86 <C> 75.52 <C> 78.14 <C> [BOLD] 97.50 <C> [BOLD] 97.50 <C> [BOLD] 96.18 <C> [BOLD] 97.24 <C> [BOLD] 91.78 <C> [BOLD] 89.22 <C> [BOLD] 78.85 <C> [BOLD] 80.80 <R> <C> Hindi-HDTB <C> [EMPTY] <C> 97.52 <C> 97.04 <C> 94.15 <C> [BOLD] 98.67 <C> 94.85 <C> 91.83 <C> 78.49 <C> 86.83 <C> [BOLD] 97.58 <C> [BOLD] 97.19 <C> [BOLD] 94.24 <C> [BOLD] 98.67 <C> [BOLD] 95.56 <C> [BOLD] 92.50 <C> [BOLD] 79.32 <C> [BOLD] 87.66 <R> <C> Hungarian-Szeged <C> [EMPTY] <C> 95.76 <C> — <C> 91.75 <C> 95.05 <C> 84.04 <C> 79.73 <C> 67.63 <C> 73.63 <C> [BOLD] 97.09 <C> — <C> [BOLD] 93.41 <C> [BOLD] 95.44 <C> [BOLD] 88.76 <C> [BOLD] 85.12 <C> [BOLD] 74.08 <C> [BOLD] 79.21 <R> <C> Indonesian-GSD <C> [EMPTY] <C> 93.69 <C> 94.19 <C> 95.58 <C> 99.64 <C> 85.31 <C> 78.99 <C> 67.74 <C> 76.38 <C> [BOLD] 94.09 <C> [BOLD] 94.93 <C> [BOLD] 96.03 <C> [BOLD] 99.66 <C> [BOLD] 86.47 <C> [BOLD] 80.40 <C> [BOLD] 70.01 <C> [BOLD] 78.19 <R> <C> Irish-IDT <C> [EMPTY] <C> 92.72 <C> 91.44 <C> 82.43 <C> 90.48 <C> 80.39 <C> 72.34 <C> 46.49 <C> 55.32 <C> [BOLD] 93.22 <C> [BOLD] 92.00 <C> [BOLD] 83.78 <C> [BOLD] 90.56 <C> [BOLD] 81.43 <C> [BOLD] 73.47 <C> [BOLD] 49.05 <C> [BOLD] 56.50 <R> <C> Italian-ISDT <C> [EMPTY] <C> 98.39 <C> 98.30 <C> 98.11 <C> 98.66 <C> 93.49 <C> 91.54 <C> 84.28 <C> 85.49 <C> [BOLD] 98.62 <C> [BOLD] 98.54 <C> [BOLD] 98.26 <C> [BOLD] 98.78 <C> [BOLD] 94.97 <C> [BOLD] 93.38 <C> [BOLD] 87.14 <C> [BOLD] 88.10 <R> <C> Italian-ParTUT <C> [EMPTY] <C> 98.38 <C> 98.35 <C> 97.77 <C> 98.16 <C> 92.64 <C> 90.47 <C> 81.87 <C> 82.99 <C> [BOLD] 98.54 <C> [BOLD] 98.52 <C> [BOLD] 98.05 <C> [BOLD] 98.24 <C> [BOLD] 95.36 <C> [BOLD] 93.38 <C> [BOLD] 86.57 <C> [BOLD] 87.30 <R> <C> Italian-PoSTWITA <C> [EMPTY] <C> 96.61 <C> 96.43 <C> 96.90 <C> 97.00 <C> 86.03 <C> 81.78 <C> 72.88 <C> 74.33 <C> [BOLD] 97.11 <C> [BOLD] 96.98 <C> [BOLD] 97.12 <C> [BOLD] 97.27 <C> [BOLD] 87.25 <C> [BOLD] 83.07 <C> [BOLD] 74.70 <C> [BOLD] 76.27 <R> <C> Japanese-GSD <C> [EMPTY] <C> 98.13 <C> 97.81 <C> [BOLD] 99.98 <C> 99.52 <C> 95.06 <C> 93.73 <C> 86.37 <C> 88.04 <C> [BOLD] 98.24 <C> [BOLD] 97.89 <C> [BOLD] 99.98 <C> [BOLD] 99.53 <C> [BOLD] 95.55 <C> [BOLD] 94.27 <C> [BOLD] 87.64 <C> [BOLD] 89.24 <R> <C> Kazakh-KTB <C> [EMPTY] <C> 55.84 <C> 52.06 <C> 40.40 <C> 63.96 <C> 53.30 <C> 33.38 <C> 4.82 <C> 15.10 <C> [BOLD] 63.08 <C> [BOLD] 60.63 <C> [BOLD] 43.64 <C> [BOLD] 64.03 <C> [BOLD] 57.02 <C> [BOLD] 38.72 <C> [BOLD] 7.88 <C> [BOLD] 18.78 <R> <C> Korean-GSD <C> [EMPTY] <C> 96.29 <C> 90.39 <C> 99.77 <C> 93.40 <C> 87.70 <C> 84.24 <C> 79.74 <C> 76.35 <C> [BOLD] 96.99 <C> [BOLD] 91.21 <C> [BOLD] 99.83 <C> [BOLD] 93.72 <C> [BOLD] 89.38 <C> [BOLD] 86.05 <C> [BOLD] 82.19 <C> [BOLD] 78.58 <R> <C> Korean-Kaist <C> [EMPTY] <C> 95.59 <C> 87.00 <C> — <C> [BOLD] 94.30 <C> 88.42 <C> 86.48 <C> 80.72 <C> 79.22 <C> [BOLD] 95.77 <C> [BOLD] 87.46 <C> — <C> 94.15 <C> [BOLD] 89.35 <C> [BOLD] 87.54 <C> [BOLD] 82.12 <C> [BOLD] 80.18 <R> <C> Kurmanji-MG <C> ✗ <C> 53.38 <C> 51.42 <C> 41.53 <C> [BOLD] 69.58 <C> [BOLD] 45.22 <C> [BOLD] 34.32 <C> 2.74 <C> [BOLD] 19.39 <C> [BOLD] 58.78 <C> [BOLD] 56.11 <C> [BOLD] 42.03 <C> 68.21 <C> 43.74 <C> 32.99 <C> [BOLD] 3.10 <C> 17.98 <R> <C> Latin-ITTB <C> [EMPTY] <C> 98.34 <C> 96.37 <C> 96.97 <C> 98.99 <C> 91.06 <C> 88.80 <C> 82.35 <C> 85.71 <C> [BOLD] 98.42 <C> [BOLD] 96.45 <C> [BOLD] 97.05 <C> [BOLD] 99.03 <C> [BOLD] 91.25 <C> [BOLD] 89.10 <C> [BOLD] 82.80 <C> [BOLD] 86.05 <R> <C> Latin-PROIEL <C> [EMPTY] <C> 97.01 <C> 97.15 <C> 91.53 <C> [BOLD] 96.32 <C> [BOLD] 83.34 <C> 78.66 <C> [BOLD] 67.40 <C> [BOLD] 73.65 <C> [BOLD] 97.15 <C> [BOLD] 97.21 <C> [BOLD] 91.54 <C> 96.18 <C> [BOLD] 83.34 <C> [BOLD] 78.70 <C> 67.29 <C> 73.52 <R> <C> Latin-Perseus <C> [EMPTY] <C> 88.40 <C> 74.58 <C> 79.10 <C> 81.45 <C> 71.20 <C> 61.28 <C> 41.58 <C> 45.09 <C> [BOLD] 89.96 <C> [BOLD] 76.22 <C> [BOLD] 80.43 <C> [BOLD] 81.95 <C> [BOLD] 74.39 <C> [BOLD] 64.68 <C> [BOLD] 44.96 <C> [BOLD] 47.94 <R> <C> Latvian-LVTB <C> [EMPTY] <C> [BOLD] 96.11 <C> 88.69 <C> 93.01 <C> 95.46 <C> 87.20 <C> 83.35 <C> 71.92 <C> 76.64 <C> [BOLD] 96.11 <C> [BOLD] 89.06 <C> [BOLD] 93.30 <C> [BOLD] 95.76 <C> [BOLD] 88.05 <C> [BOLD] 84.50 <C> [BOLD] 73.81 <C> [BOLD] 78.33 <R> <C> Lithuanian-HSE <C> [EMPTY] <C> 81.70 <C> 79.91 <C> 60.47 <C> [BOLD] 76.89 <C> 51.98 <C> 42.17 <C> 18.17 <C> 28.70 <C> [BOLD] 88.77 <C> [BOLD] 86.04 <C> [BOLD] 66.70 <C> [BOLD] 76.89 <C> [BOLD] 64.53 <C> [BOLD] 54.53 <C> [BOLD] 26.35 <C> [BOLD] 34.76 <R> <C> Maltese-MUDT <C> ✗ <C> 95.99 <C> 95.69 <C> — <C> — <C> 84.65 <C> 79.71 <C> 66.75 <C> 71.49 <C> [BOLD] 96.15 <C> [BOLD] 95.85 <C> — <C> — <C> [BOLD] 85.31 <C> [BOLD] 80.10 <C> [BOLD] 67.21 <C> [BOLD] 71.62 <R> <C> Marathi-UFAL <C> [EMPTY] <C> 80.10 <C> — <C> 67.23 <C> [BOLD] 81.31 <C> [BOLD] 70.63 <C> [BOLD] 61.41 <C> 29.34 <C> [BOLD] 45.87 <C> [BOLD] 83.50 <C> — <C> [BOLD] 67.96 <C> [BOLD] 81.31 <C> 68.45 <C> 60.44 <C> [BOLD] 29.58 <C> 43.75 <R> <C> North Sami-Giella <C> ✗ <C> 92.61 <C> 93.78 <C> [BOLD] 90.00 <C> [BOLD] 88.34 <C> 78.39 <C> 73.60 <C> 62.29 <C> 61.45 <C> [BOLD] 92.76 <C> [BOLD] 94.11 <C> 89.83 <C> 88.25 <C> [BOLD] 78.47 <C> [BOLD] 73.95 <C> [BOLD] 62.47 <C> [BOLD] 61.68 <R> <C> Norwegian-Bokmaal <C> [EMPTY] <C> 98.31 <C> — <C> 97.14 <C> 98.64 <C> 92.39 <C> 90.49 <C> 84.06 <C> 86.53 <C> [BOLD] 98.59 <C> — <C> [BOLD] 97.54 <C> [BOLD] 98.72 <C> [BOLD] 93.78 <C> [BOLD] 92.19 <C> [BOLD] 86.72 <C> [BOLD] 88.60 <R> <C> Norwegian-Nynorsk <C> [EMPTY] <C> 93.87 <C> — <C> 91.57 <C> 96.06 <C> 80.09 <C> 75.04 <C> 63.72 <C> 68.22 <C> [BOLD] 95.52 <C> — <C> [BOLD] 93.17 <C> [BOLD] 96.59 <C> [BOLD] 82.64 <C> [BOLD] 78.08 <C> [BOLD] 67.53 <C> [BOLD] 71.75 <R> <C> Norwegian-NynorskLIA <C> [EMPTY] <C> 89.59 <C> — <C> 86.13 <C> 93.93 <C> 68.08 <C> 60.07 <C> 44.47 <C> 50.98 <C> [BOLD] 92.53 <C> — <C> [BOLD] 88.96 <C> [BOLD] 94.73 <C> [BOLD] 71.42 <C> [BOLD] 64.12 <C> [BOLD] 49.10 <C> [BOLD] 55.36 <R> <C> Old Church Slavonic-PROIEL <C> ✗ <C> 96.89 <C> [BOLD] 97.16 <C> [BOLD] 90.72 <C> [BOLD] 93.07 <C> 89.64 <C> 84.99 <C> 73.66 <C> 77.71 <C> [BOLD] 96.96 <C> 97.13 <C> 90.45 <C> 92.91 <C> [BOLD] 89.88 <C> [BOLD] 85.21 <C> [BOLD] 73.77 <C> [BOLD] 77.88 <R> <C> Old French-SRCMF <C> ✗ <C> 96.09 <C> 96.00 <C> 97.82 <C> — <C> 91.75 <C> [BOLD] 86.82 <C> [BOLD] 79.89 <C> [BOLD] 83.81 <C> [BOLD] 96.26 <C> [BOLD] 96.21 <C> [BOLD] 97.89 <C> — <C> [BOLD] 91.83 <C> 86.75 <C> 79.79 <C> 83.55 <R> <C> Persian-Seraji <C> [EMPTY] <C> 97.75 <C> 97.70 <C> 97.78 <C> [BOLD] 97.44 <C> 90.05 <C> 86.66 <C> 81.23 <C> 80.93 <C> [BOLD] 98.17 <C> [BOLD] 98.05 <C> [BOLD] 98.13 <C> 97.21 <C> [BOLD] 92.01 <C> [BOLD] 89.07 <C> [BOLD] 84.36 <C> [BOLD] 83.40 <R> <C> Polish-LFG <C> [EMPTY] <C> 98.80 <C> 94.56 <C> 95.49 <C> 97.54 <C> 96.58 <C> 94.76 <C> 87.04 <C> 90.26 <C> [BOLD] 99.16 <C> [BOLD] 95.91 <C> [BOLD] 96.57 <C> [BOLD] 97.85 <C> [BOLD] 97.44 <C> [BOLD] 96.03 <C> [BOLD] 90.14 <C> [BOLD] 92.09 <R> <C> Polish-SZ <C> [EMPTY] <C> 98.34 <C> 93.25 <C> 93.04 <C> 97.16 <C> 93.39 <C> 91.24 <C> 81.06 <C> 85.99 <C> [BOLD] 98.91 <C> [BOLD] 95.12 <C> [BOLD] 95.08 <C> [BOLD] 97.53 <C> [BOLD] 95.73 <C> [BOLD] 94.25 <C> [BOLD] 86.66 <C> [BOLD] 89.89 <R> <C> Portuguese-Bosque <C> [EMPTY] <C> 97.07 <C> — <C> 96.40 <C> 98.46 <C> 91.36 <C> 89.04 <C> 76.67 <C> 83.06 <C> [BOLD] 97.38 <C> — <C> [BOLD] 96.96 <C> [BOLD] 98.59 <C> [BOLD] 92.69 <C> [BOLD] 90.70 <C> [BOLD] 79.59 <C> [BOLD] 85.44 <R> <C> Portuguese-GSD <C> [EMPTY] <C> 98.31 <C> 98.30 <C> 99.92 <C> 99.30 <C> 93.01 <C> 91.63 <C> 85.96 <C> 86.94 <C> [BOLD] 98.67 <C> [BOLD] 98.67 <C> [BOLD] 99.93 <C> [BOLD] 99.48 <C> [BOLD] 94.74 <C> [BOLD] 93.71 <C> [BOLD] 89.19 <C> [BOLD] 90.28 <R> <C> Romanian-Nonstandard <C> [EMPTY] <C> 96.68 <C> 92.11 <C> 90.88 <C> [BOLD] 94.78 <C> 89.12 <C> 84.20 <C> 65.93 <C> 73.44 <C> [BOLD] 96.85 <C> [BOLD] 92.27 <C> [BOLD] 91.04 <C> 94.55 <C> [BOLD] 89.61 <C> [BOLD] 84.78 <C> [BOLD] 66.82 <C> [BOLD] 73.77 <R> <C> Romanian-RRT <C> [EMPTY] <C> 97.96 <C> 97.43 <C> 97.53 <C> 98.41 <C> 91.31 <C> 86.74 <C> 79.02 <C> 81.09 <C> [BOLD] 98.16 <C> [BOLD] 97.56 <C> [BOLD] 97.75 <C> [BOLD] 98.59 <C> [BOLD] 92.41 <C> [BOLD] 88.05 <C> [BOLD] 81.04 <C> [BOLD] 82.89 <R> <C> Russian-GSD <C> [EMPTY] <C> 97.10 <C> 96.98 <C> 92.66 <C> 97.37 <C> 88.15 <C> 84.37 <C> 74.07 <C> 80.03 <C> [BOLD] 97.78 <C> [BOLD] 97.64 <C> [BOLD] 94.76 <C> [BOLD] 97.84 <C> [BOLD] 90.74 <C> [BOLD] 87.51 <C> [BOLD] 79.13 <C> [BOLD] 83.97 <R> <C> Russian-SynTagRus <C> [EMPTY] <C> 99.12 <C> — <C> 97.57 <C> 98.53 <C> 93.80 <C> 92.32 <C> 87.91 <C> 89.17 <C> [BOLD] 99.23 <C> — <C> [BOLD] 97.97 <C> [BOLD] 98.59 <C> [BOLD] 94.92 <C> [BOLD] 93.68 <C> [BOLD] 89.85 <C> [BOLD] 90.81 <R> <C> Russian-Taiga <C> [EMPTY] <C> 93.18 <C> [BOLD] 99.98 <C> 82.87 <C> 89.99 <C> 75.45 <C> 69.11 <C> 48.81 <C> 57.21 <C> [BOLD] 95.47 <C> [BOLD] 99.98 <C> [BOLD] 86.87 <C> [BOLD] 91.18 <C> [BOLD] 80.74 <C> [BOLD] 75.65 <C> [BOLD] 57.16 <C> [BOLD] 63.65 <R> <C> Serbian-SET <C> [EMPTY] <C> 98.33 <C> — <C> 94.35 <C> 97.36 <C> 92.70 <C> 89.27 <C> 79.14 <C> 84.18 <C> [BOLD] 98.71 <C> — <C> [BOLD] 95.79 <C> [BOLD] 97.76 <C> [BOLD] 94.57 <C> [BOLD] 91.65 <C> [BOLD] 83.03 <C> [BOLD] 87.24 <R> <C> Slovak-SNK <C> [EMPTY] <C> 96.83 <C> 86.14 <C> 90.82 <C> 96.40 <C> 89.82 <C> 86.90 <C> 74.00 <C> 81.37 <C> [BOLD] 97.70 <C> [BOLD] 88.54 <C> [BOLD] 93.07 <C> [BOLD] 96.75 <C> [BOLD] 94.30 <C> [BOLD] 92.15 <C> [BOLD] 81.43 <C> [BOLD] 87.24 <R> <C> Slovenian-SSJ <C> [EMPTY] <C> 98.61 <C> 95.70 <C> 95.92 <C> 98.25 <C> 92.96 <C> 91.16 <C> 83.85 <C> 86.89 <C> [BOLD] 98.83 <C> [BOLD] 96.53 <C> [BOLD] 96.77 <C> [BOLD] 98.54 <C> [BOLD] 94.81 <C> [BOLD] 93.49 <C> [BOLD] 87.58 <C> [BOLD] 90.04 <R> <C> Slovenian-SST <C> [EMPTY] <C> 93.79 <C> 86.12 <C> 86.28 <C> 95.17 <C> 73.51 <C> 67.51 <C> 52.67 <C> 60.32 <C> [BOLD] 95.72 <C> [BOLD] 89.25 <C> [BOLD] 89.43 <C> [BOLD] 96.06 <C> [BOLD] 77.23 <C> [BOLD] 71.79 <C> [BOLD] 58.69 <C> [BOLD] 64.84 <R> <C> Spanish-AnCora <C> [EMPTY] <C> 98.91 <C> 98.92 <C> 98.49 <C> 99.17 <C> 92.34 <C> 90.26 <C> 83.97 <C> 85.51 <C> [BOLD] 99.05 <C> [BOLD] 99.06 <C> [BOLD] 98.70 <C> [BOLD] 99.25 <C> [BOLD] 93.75 <C> [BOLD] 92.03 <C> [BOLD] 87.03 <C> [BOLD] 88.35 <R> <C> Spanish-GSD <C> [EMPTY] <C> 96.85 <C> — <C> 97.09 <C> 98.97 <C> 90.71 <C> 88.03 <C> 75.98 <C> 81.47 <C> [BOLD] 97.36 <C> — <C> [BOLD] 97.19 <C> [BOLD] 99.14 <C> [BOLD] 92.32 <C> [BOLD] 90.11 <C> [BOLD] 79.29 <C> [BOLD] 84.92 <R> <C> Swedish Sign Language-SSLC <C> ✗ <C> 68.44 <C> 57.27 <C> — <C> — <C> 49.82 <C> 37.94 <C> 31.34 <C> 39.47 <C> [BOLD] 72.34 <C> [BOLD] 70.92 <C> — <C> — <C> [BOLD] 56.03 <C> [BOLD] 42.02 <C> [BOLD] 34.50 <C> [BOLD] 43.19 <R> <C> Swedish-LinES <C> [EMPTY] <C> 96.78 <C> 94.75 <C> 89.43 <C> 97.03 <C> 86.07 <C> 81.86 <C> 66.48 <C> 77.38 <C> [BOLD] 97.77 <C> [BOLD] 95.97 <C> [BOLD] 90.39 <C> [BOLD] 97.50 <C> [BOLD] 88.16 <C> [BOLD] 84.55 <C> [BOLD] 70.13 <C> [BOLD] 80.81 <R> <C> Swedish-Talbanken <C> [EMPTY] <C> 97.94 <C> 96.71 <C> 96.86 <C> 98.01 <C> 89.63 <C> 86.61 <C> 79.67 <C> 82.26 <C> [BOLD] 98.60 <C> [BOLD] 97.62 <C> [BOLD] 97.69 <C> [BOLD] 98.13 <C> [BOLD] 92.42 <C> [BOLD] 90.16 <C> [BOLD] 84.56 <C> [BOLD] 86.19 <R> <C> Tamil-TTB <C> [EMPTY] <C> 91.05 <C> 83.81 <C> 87.28 <C> 93.92 <C> 74.11 <C> 66.37 <C> 55.31 <C> 59.58 <C> [BOLD] 92.61 <C> [BOLD] 86.53 <C> [BOLD] 89.89 <C> [BOLD] 93.97 <C> [BOLD] 77.68 <C> [BOLD] 71.14 <C> [BOLD] 60.67 <C> [BOLD] 64.74 <R> <C> Telugu-MTG <C> [EMPTY] <C> 93.07 <C> 93.07 <C> [BOLD] 99.03 <C> — <C> 91.26 <C> 85.02 <C> 77.75 <C> [BOLD] 81.76 <C> [BOLD] 94.73 <C> [BOLD] 94.73 <C> [BOLD] 99.03 <C> — <C> [BOLD] 91.96 <C> [BOLD] 85.30 <C> [BOLD] 77.79 <C> 81.60 <R> <C> Turkish-IMST <C> [EMPTY] <C> 96.01 <C> 95.12 <C> 92.55 <C> 96.01 <C> 74.19 <C> 67.56 <C> 56.96 <C> 61.37 <C> [BOLD] 96.07 <C> [BOLD] 95.37 <C> [BOLD] 93.25 <C> [BOLD] 96.39 <C> [BOLD] 76.30 <C> [BOLD] 70.11 <C> [BOLD] 59.91 <C> [BOLD] 64.07 <R> <C> Ukrainian-IU <C> [EMPTY] <C> 97.59 <C> 92.66 <C> 92.66 <C> 97.23 <C> 88.29 <C> 85.25 <C> 73.81 <C> 79.10 <C> [BOLD] 98.20 <C> [BOLD] 94.63 <C> [BOLD] 94.43 <C> [BOLD] 97.65 <C> [BOLD] 91.65 <C> [BOLD] 89.36 <C> [BOLD] 79.97 <C> [BOLD] 84.24 <R> <C> Upper Sorbian-UFAL <C> ✗ <C> 62.93 <C> — <C> 41.10 <C> [BOLD] 68.68 <C> 45.58 <C> 34.54 <C> 3.37 <C> 16.65 <C> [BOLD] 69.69 <C> — <C> [BOLD] 43.46 <C> 66.80 <C> [BOLD] 48.64 <C> [BOLD] 38.85 <C> [BOLD] 5.03 <C> [BOLD] 17.80 <R> <C> Urdu-UDTB <C> [EMPTY] <C> 93.66 <C> 91.98 <C> 81.92 <C> 97.40 <C> 87.50 <C> 81.62 <C> 55.02 <C> 73.07 <C> [BOLD] 94.28 <C> [BOLD] 92.37 <C> [BOLD] 82.47 <C> [BOLD] 97.56 <C> [BOLD] 88.55 <C> [BOLD] 83.03 <C> [BOLD] 56.58 <C> [BOLD] 75.05 <R> <C> Uyghur-UDT <C> ✗ <C> [BOLD] 89.87 <C> [BOLD] 92.54 <C> [BOLD] 88.30 <C> [BOLD] 95.31 <C> 78.46 <C> 67.09 <C> 47.84 <C> 57.08 <C> 89.58 <C> 92.27 <C> 88.29 <C> 95.30 <C> [BOLD] 79.10 <C> [BOLD] 67.46 <C> [BOLD] 48.09 <C> [BOLD] 57.69 <R> <C> Vietnamese-VTB <C> [EMPTY] <C> 89.68 <C> 87.41 <C> [BOLD] 99.72 <C> 99.55 <C> 70.38 <C> 62.56 <C> 55.56 <C> 59.54 <C> [BOLD] 90.87 <C> [BOLD] 88.87 <C> 99.68 <C> [BOLD] 99.79 <C> [BOLD] 72.94 <C> [BOLD] 65.41 <C> [BOLD] 58.97 <C> [BOLD] 62.64 <R> <C> Total <C> [EMPTY] <C> 93.71 <C> 92.52 <C> 90.56 <C> 94.35 <C> 84.23 <C> 79.59 <C> 67.36 <C> 72.05 <C> [BOLD] 94.71 <C> [BOLD] 93.69 <C> [BOLD] 91.50 <C> [BOLD] 94.51 <C> [BOLD] 86.34 <C> [BOLD] 82.01 <C> [BOLD] 70.66 <C> [BOLD] 74.75 <CAP> Table 6: Results on all UD 2.3 treebanks with a train set, comparing inclusion of BERT and possibly Flair embeddings to WE+CLE baseline. Gold tokenization and segmentation is used.
<R> <C> [EMPTY] <C> Test-standard Yes/No <C> Test-standard Num <C> Test-standard Other <C> Test-standard All <R> <C> Prior Goyal et al. ( 2017 ) <C> 61.20 <C> 0.36 <C> 1.17 <C> 25.98 <R> <C> Language-only Goyal et al. ( 2017 ) <C> 67.01 <C> 31.55 <C> 27.37 <C> 44.26 <R> <C> MCB Fukui et al. ( 2016 ) <C> 78.82 <C> 38.28 <C> 53.36 <C> 62.27 <R> <C> Up-Down Anderson et al. ( 2018 ) <C> 82.20 <C> 43.90 <C> 56.26 <C> 65.32 <R> <C> VQA-E Li et al. ( 2018b ) <C> 83.22 <C> 43.58 <C> 56.79 <C> 66.31 <R> <C> [BOLD] Ours(single) <C> [BOLD] 84.69 <C> [BOLD] 46.75 <C> [BOLD] 59.30 <C> [BOLD] 68.37 <R> <C> [BOLD] Ours(Ensemble-10) <C> [BOLD] 86.15 <C> [BOLD] 47.41 <C> [BOLD] 60.41 <C> [BOLD] 69.66 <CAP> Table 1: Comparison of our results on VQA with the state-of-the-art methods on the test-standard data. Accuracies in percentage (%) are reported.
<R> <C> [EMPTY] <C> Validation <R> <C> Up-Down Anderson et al. ( 2018 ) <C> 63.2 <R> <C> Ours with Up-Down captions <C> 64.6 <R> <C> Ours with our generated captions <C> 65.8 <R> <C> Ours with human captions <C> [BOLD] 67.1 <CAP> Table 2: Comparison of the performance using generated and human captions. Both of them provide significant improvements to the baseline model. However, there is still a reasonable gap between generated and human captions.
<R> <C> Matrix <C> Similarity task <C> Analogy Task <R> <C> [BOLD] M <C> 61.18 <C> 27.14 <R> <C> PMI <C> 65.05 <C> 29.58 <CAP> Table 1: Word embeddings from PMI and M. For word similarities evaluation metric is the Spearman’s correlation with the human ratings, while for word analogies it is the percentage of correct answers.
<R> <C> Model <C> to Anc.P Acc <C> to Anc.P BLEU <C> to Anc.P GLEU <C> to M.zh Acc <C> to M.zh BLEU <C> to M.zh GLEU <C> to F.en Acc <C> to F.en BLEU <C> to F.en GLEU <C> to Inf.en Acc <C> to Inf.en BLEU <C> to Inf.en GLEU <R> <C> S2S <C> [BOLD] 87.2% <C> 4.20 <C> 3.24 <C> 74.8% <C> 3.66 <C> 3.43 <C> 88.9% <C> 33.90 <C> 14.06 <C> [BOLD] 71.8% <C> 18.34 <C> 2.99 <R> <C> SLS <C> 82.0% <C> 5.89 <C> 4.49 <C> 81.9% <C> 3.05 <C> 1.88 <C> 89.5% <C> 41.41 <C> 16.77 <C> 63.5% <C> 19.21 <C> 2.55 <R> <C> DAR <C> 82.5% <C> 6.33 <C> 5.21 <C> 80.4% <C> 4.72 <C> [BOLD] 4.26 <C> 89.2% <C> 44.72 <C> 18.52 <C> 63.5% <C> 23.32 <C> 3.26 <R> <C> CPLS <C> 85.4% <C> [BOLD] 7.11 <C> [BOLD] 5.52 <C> [BOLD] 84.4% <C> [BOLD] 4.95 <C> 4.12 <C> [BOLD] 91.3% <C> [BOLD] 48.60 <C> [BOLD] 19.04 <C> 64.3% <C> [BOLD] 27.25 <C> [BOLD] 3.61 <CAP> Table 3: Automatic evaluation results on four style transfer tasks. Acc refers to the style accuracy.
<R> <C> Methods <C> F-1 <C> R <R> <C> LEAD <C> - <C> 32.4 <R> <C> Peer65 <C> - <C> 38.2 <R> <C> Centroid  <C> - <C> 38.8 <R> <C> Submodular  <C> 38.9 <C> 39.3 <R> <C> MCKP  <C> - <C> 38.5 <R> <C> LexRank  <C> - <C> 37.9 <R> <C> RNN  <C> - <C> 38.8 <R> <C> CNN  <C> - <C> 38.9 <R> <C> Ours (tf-idf) <C> 37.7 <C> 38.2 <R> <C> Ours (W2V) <C> 36.9 <C> 37.2 <R> <C> Ours (W2V-WMD) <C> 37.7 <C> 38.0 <R> <C> Ours (W2V-TSS) <C> 37.7 <C> 38.1 <R> <C> Ours (BERT) <C> 37.8 <C> 38.2 <R> <C> Ours (LateFusion) <C> 37.8 <C> 38.2 <R> <C> Ours (GraphFusion) <C> 38.8 <C> 39.3 <R> <C> Ours (W2V-TSS) with compression <C> 38.1 <C> 38.7 <R> <C> Ours (BERT) with compression <C> 37.9 <C> 38.4 <R> <C> Ours (GraphFusion) with compression <C> [BOLD] 39.0 <C> [BOLD] 39.6 <CAP> Table 1: Document summarization performance on the DUC2004 dataset.
<R> <C> Methods <C> CNN/DM R-1 <C> CNN/DM R-2 <C> CNN/DM R-L <C> NYT R-1 <C> NYT R-2 <C> NYT R-L <R> <C> ORACLE <C> 54.7 <C> 30.4 <C> 50.8 <C> 61.9 <C> 41.7 <C> 58.3 <R> <C> LEAD3 <C> 40.3 <C> 17.7 <C> 36.6 <C> 35.5 <C> 17.2 <C> 32.0 <R> <C> Pointer  <C> 39.5 <C> 17.3 <C> 36.4 <C> 42.7 <C> 22.1 <C> 38.0 <R> <C> Refresh  <C> 41.3 <C> 18.4 <C> 37.5 <C> 41.3 <C> 22.0 <C> 37.8 <R> <C> Ours (tf-idf) <C> 38.8 <C> 16.9 <C> 31.8 <C> 37.6 <C> 17.9 <C> 30.8 <R> <C> Ours (W2V) <C> 37.4 <C> 16.0 <C> 30.6 <C> 36.8 <C> 17.1 <C> 29.7 <R> <C> Ours (W2V-WMD) <C> 39.0 <C> 16.6 <C> 31.9 <C> 37.5 <C> 17.5 <C> 30.1 <R> <C> Ours (W2V-TSS) <C> 38.7 <C> 16.7 <C> 31.7 <C> 37.7 <C> 17.7 <C> 30.2 <R> <C> Ours (BERT) <C> 38.9 <C> 16.8 <C> 31.6 <C> 38.4 <C> 18.3 <C> 31.1 <R> <C> Ours (GraphFusion) <C> 39.0 <C> 16.8 <C> 32.0 <C> 38.9 <C> 18.8 <C> 31.7 <R> <C> Ours (LateFusion) <C> 39.2 <C> 17.1 <C> 32.2 <C> 39.0 <C> 18.8 <C> 31.5 <CAP> Table 4: Document summarization performance on the CNN/DM dataset.
<R> <C> [BOLD] Model <C> Supervised Evaluation  [BOLD] Jacc. Ind. <C> Supervised Evaluation  [BOLD] Kendall’s  [ITALIC] τ <C> Supervised Evaluation  [BOLD] WNDCG <C> Clustering Evaluation  [BOLD] F.NMI <C> Clustering Evaluation  [BOLD] F.B-Cubed <R> <C> [BOLD] Baselines <C> [BOLD] Baselines <C> [BOLD] Baselines <C> [BOLD] Baselines <C> [BOLD] Baselines <C> [BOLD] Baselines <R> <C> One sense for all <C> 0.192 <C> 0.609 <C> 0.288 <C> 0.000 <C> 0.631 <R> <C> One sense per instance <C> 0.000 <C> 0.000 <C> 0.000 <C> 0.071 <C> 0.000 <R> <C> Most Frequent Sense <C> 0.455 <C> 0.465 <C> 0.339 <C> – <C> – <R> <C> [BOLD] SemEval-2013 participants <C> [BOLD] SemEval-2013 participants <C> [BOLD] SemEval-2013 participants <C> [BOLD] SemEval-2013 participants <C> [BOLD] SemEval-2013 participants <C> [BOLD] SemEval-2013 participants <R> <C> AI-KU (base) <C> 0.197 <C> 0.620 <C> 0.387 <C> 0.065 <C> 0.390 <R> <C> AI-KU (remove5-add1000) <C> 0.244 <C> 0.642 <C> 0.332 <C> 0.039 <C> 0.451 <R> <C> Unimelb (50k) <C> 0.213 <C> 0.620 <C> 0.371 <C> 0.060 <C> 0.483 <R> <C> [BOLD] Sense embeddings <C> [BOLD] Sense embeddings <C> [BOLD] Sense embeddings <C> [BOLD] Sense embeddings <C> [BOLD] Sense embeddings <C> [BOLD] Sense embeddings <R> <C> AdaGram,  [ITALIC] α = 0.05, 100 dim. vectors <C> 0.274 <C> 0.644 <C> 0.318 <C> 0.058 <C> 0.470 <R> <C> SenseGram (word2vec) <C> 0.197 <C> 0.615 <C> 0.291 <C> 0.011 <C> 0.615 <R> <C> [BOLD] egvi (fastText, K=200) <C> 0.229 <C> 0.625 <C> 0.300 <C> 0.035 <C> 0.541 <CAP> Table 2: WSD performance on the SemEval-2013 Task 13 dataset for the English language.
<R> <C> [EMPTY] <C> mc <C> rg <C> simlex <C> ws353 <C> total <R> <C> [EMPTY] <C> English <C> English <C> English <C> English <C> English <R> <C> inventory <C> 9.8 <C> 9.8 <C> 12.6 <C> 11.3 <C> 12.5 <R> <C> WordNet <C> 3.6 <C> 3.7 <C> 6.5 <C> 5.5 <C> 1.23 (nouns) 2.16 (verbs) <R> <C> [EMPTY] <C> Russian <C> Russian <C> Russian <C> Russian <C> Russian <R> <C> inventory <C> 1.8 <C> 2.0 <C> – <C> 2.2 <C> 2.97 <R> <C> RuWordNet <C> 1.4 <C> 1.4 <C> – <C> 1.8 <C> 1.12 (nouns) 1.33 (verbs) <CAP> Table 3: Average number of senses for words from SemR-11 dataset in our inventory and in WordNet for English and ruWordNet for Russian. The rightmost column gives the average number of senses in the inventories and WordNets.
<R> <C> Models <C> [ITALIC] D1 <C> [ITALIC] D2 <C> [ITALIC] D3 <C> [ITALIC] D4 <R> <C> CRF-1 <C> 72.77 <C> 79.72 <C> 62.67 <C> 66.96 <R> <C> CRF-2 <C> 74.01 <C> 82.33 <C> 67.54 <C> 69.56 <R> <C> Semi-CRF <C> 68.75 <C> 79.60 <C> 62.69 <C> 66.35 <R> <C> LSTM <C> 75.71 <C> 82.01 <C> 68.26 <C> 70.35 <R> <C> IHS_RD ( [ITALIC] D1 [BOLD]  winner) <C> 74.55 <C> 79.62 <C> - <C> - <R> <C> DLIREC ( [ITALIC] D2 [BOLD]  winner) <C> 73.78 <C> 84.01 <C> - <C> - <R> <C> EliXa ( [ITALIC] D3 [BOLD]  winner) <C> - <C> - <C> 70.04 <C> - <R> <C> NLANGP ( [ITALIC] D4 [BOLD]  winner) <C> - <C> - <C> 67.12 <C> 72.34 <R> <C> WDEmb <C> 75.16 <C> 84.97 <C> 69.73 <C> - <R> <C> MIN <C> 77.58 <C> - <C> - <C> 73.44 <R> <C> RNCRF <C> 78.42 <C> 84.93 <C> 67.74♮ <C> 69.72* <R> <C> CMLA <C> 77.80 <C> 85.29 <C> 70.73 <C> 72.77* <R> <C> OURS w/o  [BOLD] THA <C> 77.64 <C> 84.30 <C> 70.89 <C> 72.62 <R> <C> OURS w/o  [BOLD] STN <C> 77.45 <C> 83.88 <C> 70.09 <C> 72.18 <R> <C> OURS w/o  [BOLD] THA &  [BOLD] STN <C> 76.95 <C> 83.48 <C> 69.77 <C> 71.87 <R> <C> OURS <C> [BOLD] 79.52 <C> [BOLD] 85.61 <C> [BOLD] 71.46 <C> [BOLD] 73.61 <CAP> Table 2: Experimental results (F1 score, %). The first four methods are implemented by us, and other results without markers are copied from their papers. The results with ‘*’ are reproduced by us with the released code by the authors. For RNCRF, the result with ‘♮’ is copied from the paper of CMLA (they have the same authors). ‘-’ indicates the results were not available in their papers.
<R> <C> [ITALIC] φ <C> Human Obs. Hit <C> Human Obs. Miss <C> Erroneus Match <R> <C> [BOLD] opposition <C> 67 <C> 9 <C> 107 <R> <C> [BOLD] buoy <C> 40 <C> 3 <C> 46 <R> <C> [BOLD] tap <C> 25 <C> 8 <C> 50 <R> <C> [BOLD] h. anchor <C> 44 <C> 30 <C> 86 <CAP> Table 5: Per-formula summary of the total number of observations found, missed and erroneously classified observations
<R> <C> [EMPTY] <C> [BOLD] En→Es  [BOLD] P@1 <C> [BOLD] En→Es  [BOLD] P@5 <C> [BOLD] En→Es  [BOLD] P@10 <C> [BOLD] Es→En  [BOLD] P@1 <C> [BOLD] Es→En  [BOLD] P@5 <C> [BOLD] Es→En  [BOLD] P@10 <C> [BOLD] En→De  [BOLD] P@1 <C> [BOLD] En→De  [BOLD] P@5 <C> [BOLD] En→De  [BOLD] P@10 <C> [BOLD] De→En  [BOLD] P@1 <C> [BOLD] De→En  [BOLD] P@5 <C> [BOLD] De→En  [BOLD] P@10 <C> [BOLD] En→It  [BOLD] P@1 <C> [BOLD] En→It  [BOLD] P@5 <C> [BOLD] En→It  [BOLD] P@10 <C> [BOLD] It→En  [BOLD] P@1 <C> [BOLD] It→En  [BOLD] P@5 <C> [BOLD] It→En  [BOLD] P@10 <R> <C> [EMPTY] <C> [BOLD] Without Fine-Tuning <C> [BOLD] Without Fine-Tuning <C> [BOLD] Without Fine-Tuning <C> [BOLD] Without Fine-Tuning <C> [BOLD] Without Fine-Tuning <C> [BOLD] Without Fine-Tuning <C> [BOLD] Without Fine-Tuning <C> [BOLD] Without Fine-Tuning <C> [BOLD] Without Fine-Tuning <C> [BOLD] Without Fine-Tuning <C> [BOLD] Without Fine-Tuning <C> [BOLD] Without Fine-Tuning <C> [BOLD] Without Fine-Tuning <C> [BOLD] Without Fine-Tuning <C> [BOLD] Without Fine-Tuning <C> [BOLD] Without Fine-Tuning <C> [BOLD] Without Fine-Tuning <C> [BOLD] Without Fine-Tuning <R> <C> [BOLD] Conneau-18 <C> 65.3 <C> 73.8 <C> 80.6 <C> 66.7 <C> 78.3 <C> 80.8 <C> 61.5 <C> 70.1 <C> 78.2 <C> 60.3 <C> 70.2 <C> 77.0 <C> 64.8 <C> 75.3 <C> 79.4 <C> 63.8 <C> 77.1 <C> 81.8 <R> <C> [BOLD] Our (full) <C> 71.8 <C> 81.1 <C> 85.7 <C> 72.7 <C> 81.5 <C> 83.8 <C> 64.9 <C> 74.4 <C> 81.8 <C> 63.1 <C> 71.3 <C> 79.8 <C> 68.2 <C> 78.9 <C> 83.7 <C> 67.5 <C> 77.6 <C> 82.1 <R> <C> - Enc. adv <C> 70.5 <C> 79.7 <C> 83.5 <C> 71.3 <C> 80.4 <C> 83.3 <C> 63.7 <C> 73.5 <C> 79.3 <C> 62.6 <C> 70.5 <C> 79.0 <C> 67.6 <C> 77.3 <C> 82.7 <C> 66.2 <C> 78.3 <C> 82.5 <R> <C> - - Recon <C> 70.1 <C> 78.9 <C> 83.4 <C> 70.8 <C> 81.1 <C> 83.4 <C> 63.1 <C> 73.8 <C> 80.5 <C> 62.2 <C> 71.7 <C> 78.7 <C> 66.9 <C> 79.7 <C> 82.1 <C> 64.8 <C> 78.6 <C> 82.1 <R> <C> - - - Cycle <C> 66.8 <C> 76.5 <C> 82.1 <C> 67.2 <C> 79.9 <C> 82.7 <C> 61.4 <C> 69.7 <C> 77.8 <C> 60.1 <C> 69.8 <C> 76.5 <C> 65.3 <C> 75.1 <C> 78.9 <C> 64.4 <C> 77.6 <C> 81.7 <R> <C> [EMPTY] <C> [BOLD] With Fine-Tuning <C> [BOLD] With Fine-Tuning <C> [BOLD] With Fine-Tuning <C> [BOLD] With Fine-Tuning <C> [BOLD] With Fine-Tuning <C> [BOLD] With Fine-Tuning <C> [BOLD] With Fine-Tuning <C> [BOLD] With Fine-Tuning <C> [BOLD] With Fine-Tuning <C> [BOLD] With Fine-Tuning <C> [BOLD] With Fine-Tuning <C> [BOLD] With Fine-Tuning <C> [BOLD] With Fine-Tuning <C> [BOLD] With Fine-Tuning <C> [BOLD] With Fine-Tuning <C> [BOLD] With Fine-Tuning <C> [BOLD] With Fine-Tuning <C> [BOLD] With Fine-Tuning <R> <C> [BOLD] Conneau-18 <C> 82.3 <C> 90.8 <C> 93.2 <C> 83.7 <C> 91.9 <C> 93.5 <C> 74.2 <C> 89.0 <C> 91.5 <C> 72.6 <C> 85.7 <C> 88.8 <C> 78.3 <C> 88.4 <C> 91.1 <C> 78.1 <C> 88.2 <C> 90.6 <R> <C> [BOLD] Our (full) <C> 82.6 <C> 91.8 <C> 93.5 <C> 84.4 <C> 92.3 <C> 94.3 <C> 75.5 <C> 90.1 <C> 92.9 <C> 73.9 <C> 86.5 <C> 89.3 <C> 78.8 <C> 89.2 <C> 91.9 <C> 78.5 <C> 88.9 <C> 91.1 <R> <C> - Enc. adv <C> 82.5 <C> 91.6 <C> 93.5 <C> 84.3 <C> 92.1 <C> 94.3 <C> 75.4 <C> 89.7 <C> 92.7 <C> 73.5 <C> 86.3 <C> 89.2 <C> 78.4 <C> 89.0 <C> 91.8 <C> 78.1 <C> 88.7 <C> 91.0 <R> <C> - - Recon <C> 82.5 <C> 91.6 <C> 93.4 <C> 84.1 <C> 92.2 <C> 94.3 <C> 75.3 <C> 89.4 <C> 92.6 <C> 73.2 <C> 85.9 <C> 89.0 <C> 78.2 <C> 89.1 <C> 91.9 <C> 78.2 <C> 88.8 <C> 91.2 <R> <C> - - - Cycle <C> 82.4 <C> 91.0 <C> 93.1 <C> 83.6 <C> 92.2 <C> 94.0 <C> 74.3 <C> 89.7 <C> 92.6 <C> 72.7 <C> 86.1 <C> 89.1 <C> 77.8 <C> 89.2 <C> 91.8 <C> 77.4 <C> 88.3 <C> 90.8 <CAP> Table 5: Ablation study of our adversarial autoencoder model on the dataset of conneau2018word.
<R> <C> [EMPTY] <C> [BOLD] #Params <C> [BOLD] hinter <C> [BOLD] hintra <C> [BOLD] #Head <C> [BOLD] SQuAD <R> <C> (a) <C> 356M <C> 1024 <C> 1024 <C> 16 <C> 88.2 <R> <C> (b) <C> 325M <C> 768 <C> 1024 <C> 16 <C> 88.6 <R> <C> (c) <C> 293M <C> 512 <C> 1024 <C> 16 <C> 88.1 <R> <C> (d) <C> 276M <C> 384 <C> 1024 <C> 16 <C> 87.6 <R> <C> (e) <C> 262M <C> 256 <C> 1024 <C> 16 <C> 87.0 <R> <C> (f) <C> 293M <C> 512 <C> 1024 <C> 4 <C> 88.3 <R> <C> (g) <C> 92M <C> 512 <C> 512 <C> 4 <C> 85.8 <R> <C> (h) <C> 33M <C> 512 <C> 256 <C> 4 <C> 84.8 <R> <C> (i) <C> 15M <C> 512 <C> 128 <C> 4 <C> 82.0 <CAP> Table 2: Experimental results on SQuAD v1.1 dev F1 score in search of good model settings for the IB-BERTLARGE teacher. The number of layers is set to 24 for all models.
<R> <C> [EMPTY] <C> [BOLD] MNLI-m <C> [BOLD] QNLI <C> [BOLD] MRPC <C> [BOLD] SST-2 <C> [BOLD] SQuAD <R> <C> AKT <C> 83.0 <C> 90.3 <C> 86.8 <C> 91.9 <C> 88.2 <R> <C> JKT <C> 83.5 <C> 90.5 <C> [BOLD] 87.5 <C> 92.0 <C> 89.7 <R> <C> PKT <C> [BOLD] 83.9 <C> [BOLD] 91.0 <C> [BOLD] 87.5 <C> [BOLD] 92.1 <C> [BOLD] 90.0 <CAP> Table 8: Ablation study of MobileBERT on GLUE dev accuracy and SQuAD v1.1 dev F1 score with Auxiliary Knowledge Transfer (AKT), Joint Knowledge Transfer (JKT), and Progressive Knowledge Transfer (PKT).
<R> <C> Dataset <C> Cosine <C> WCD <C> WMD <C> RWMD <C> Rel-RWMD <R> <C> Arxiv <C> 28.83 <C> 29.99 <C> [BOLD] 22.77 <C> 23.43 <C> 23.16 <R> <C> Wikipedia <C> 27.83 <C> 29.23 <C> [BOLD] 26.74 <C> 27.01 <C> 26.90 <CAP> Table 6: Test Error (in %) for different distances and datasets. The best results are bold faced.
<R> <C> Dataset <C> Cosine <C> WCD <C> WMD <C> RWMD(S) <C> Rel-RWMD(S) <R> <C> Arxiv <C> 0.01 <C> 0.18 <C> 1,996 <C> 74.8 <C> 2.72 <R> <C> Wikipedia <C> 0.01 <C> 0.09 <C> 302 <C> 11.0 <C> 3.36 <CAP> Table 7: Computational runtime (in minutes) for different distances and datasets.
<R> <C> [EMPTY] <C> # <C> Re-ranking Model <C> [BOLD] Val Seen PL <C> [BOLD] Val Seen NE ↓ <C> [BOLD] Val Seen SPL ↑ <C> [BOLD] Val Seen OSR ↑ <C> [BOLD] Val Seen SR ↑ <C> [BOLD] Val Unseen PL <C> [BOLD] Val Unseen NE ↓ <C> [BOLD] Val Unseen SPL ↑ <C> [BOLD] Val Unseen OSR ↑ <C> [BOLD] Val Unseen SR ↑ <R> <C> Single Models <C> 1 <C> follower  <C> 10.40 <C> 3.68 <C> 0.62 <C> 74.12 <C> 65.10 <C> 9.57 <C> 5.20 <C> 0.49 <C> 58.79 <C> 52.36 <R> <C> Single Models <C> 2 <C> speaker  <C> 11.19 <C> 3.80 <C> 0.56 <C> 77.25 <C> 60.69 <C> 10.71 <C> 4.25 <C> 0.49 <C> [BOLD] 72.07 <C> 54.66 <R> <C> Single Models <C> 3 <C> VLN-BERT <C> 10.28 <C> 3.73 <C> 0.66 <C> 76.47 <C> 70.20 <C> 9.60 <C> [BOLD] 4.10 <C> [BOLD] 0.55 <C> 69.22 <C> [BOLD] 59.26 <R> <C> Ensemble Models <C> 4 <C> speaker + follower  <C> 10.69 <C> 2.72 <C> 0.70 <C> 82.94 <C> 74.22 <C> 10.10 <C> 3.32 <C> 0.63 <C> 76.63 <C> 67.90 <R> <C> Ensemble Models <C> 5 <C> speaker + follower + follower <C> 10.73 <C> 2.72 <C> 0.71 <C> 83.33 <C> 74.71 <C> 10.12 <C> 3.22 <C> 0.64 <C> 77.56 <C> 69.14 <R> <C> Ensemble Models <C> 6 <C> speaker + follower + speaker <C> 10.77 <C> 2.45 <C> 0.73 <C> 85.98 <C> 76.86 <C> 10.17 <C> 2.99 <C> 0.65 <C> 79.28 <C> 70.58 <R> <C> [EMPTY] <C> 7 <C> speaker + follower + VLN-BERT <C> 10.61 <C> 2.35 <C> 0.78 <C> 86.57 <C> 81.86 <C> 10.00 <C> [BOLD] 2.76 <C> [BOLD] 0.68 <C> [BOLD] 81.91 <C> [BOLD] 73.61 <CAP> Table 2: Results comparing VLN-BERT with the follower and speaker from [28]. Notably, in the ensemble models setting, combining VLN-BERT with the speaker and follower results in a 3 absolute percentage point improvement in Val Unseen Success Rate (SR) over the next best three-model ensemble (compare rows 6 and 7).
<R> <C> Re-ranking Model <C> [BOLD] Test Unseen PL <C> [BOLD] Test Unseen NE ↓ <C> [BOLD] Test Unseen SPL ↑ <C> [BOLD] Test Unseen OSR ↑ <C> [BOLD] Test Unseen SR ↑ <R> <C> Speaker-Follower  <C> 1,257 <C> 4.87 <C> 0.01 <C> 96 <C> 53 <R> <C> Tactical Rewind  <C> 197 <C> 4.29 <C> [BOLD] 0.03 <C> 90 <C> 61 <R> <C> Self-Monitoring  <C> 373 <C> 4.48 <C> 0.02 <C> 97 <C> 61 <R> <C> Reinforced Cross-Modal Matching  <C> 358 <C> 4.03 <C> 0.02 <C> 96 <C> 63 <R> <C> Environmental Dropout  <C> 687 <C> 3.26 <C> 0.01 <C> [BOLD] 99 <C> 69 <R> <C> Auxiliary Tasks†  <C> 41 <C> 3.24 <C> 0.21 <C> 81 <C> 71 <R> <C> VLN-BERT <C> 687 <C> [BOLD] 3.09 <C> 0.01 <C> [BOLD] 99 <C> [BOLD] 73 <R> <C> †indicates unpublished/concurrent work <C> †indicates unpublished/concurrent work <C> †indicates unpublished/concurrent work <C> †indicates unpublished/concurrent work <C> †indicates unpublished/concurrent work <C> †indicates unpublished/concurrent work <CAP> Table 3: Leaderboard results on Test Unseen for methods using beam search.
<R> <C> Always label as ‘food’ <C> 0.595 <R> <C> LDA  <C> 0.477 <R> <C> MultiGrain LDA  <C> 0.760 <R> <C> Segmented Topic Models  <C> 0.794 <R> <C> Local LDA  <C> 0.803 <R> <C> Support Vector Machine <C> 0.830 <R> <C> Pale Lager, unsupervised <C> 0.751 <R> <C> Pale Lager, semi-supervised <C> 0.805 <R> <C> Pale Lager, fully-supervised <C> 0.892 <CAP> TABLE II: CitySearch results, using accuracy scores from [22].
<R> <C> [EMPTY] <C> P <C> R <C> F <R> <C> MDL2,  [ITALIC] xlog [ITALIC] x + ensemble <C> 83.7 <C> 84.2 <C> [BOLD] 84.0 <R> <C> AIC3,  [ITALIC] x2 + ensemble <C> 82.5 <C> 81.0 <C> 81.7 <R> <C> Chen [chen2013improved] <C> 79.1 <C> 81.7 <C> 80.4 <R> <C> Hewlett and Cohen [hewlett2011fully] <C> 79.3 <C> 73.4 <C> 76.2 <R> <C> Zhikov et al. [zhikov2010efficient] <C> 76.3 <C> 74.5 <C> 75.4 <CAP> Table 3: Performance comparison with MDL-based methods (left) and adaptor grammars (right). Figures for the latter were reproduced from the reference implementation [johnson2009improving] using batch initialization and maximum marginal decoding. Note that colloc3-syllable adaptor grammars is not fully unsupervised due to a small amount of phoneme productions built into its core.
<R> <C> [EMPTY] <C> F <R> <C> Adaptor grammars, colloc3-syllable <C> [BOLD] 87.0 <R> <C> MDL2,  [ITALIC] xlog [ITALIC] x + ensemble <C> 84.0 <R> <C> AIC3,  [ITALIC] x2 + ensemble <C> 81.7 <R> <C> Adaptor grammars, colloc <C> 76.0 <R> <C> Adaptor grammars, unigram <C> 56.0 <CAP> Table 3: Performance comparison with MDL-based methods (left) and adaptor grammars (right). Figures for the latter were reproduced from the reference implementation [johnson2009improving] using batch initialization and maximum marginal decoding. Note that colloc3-syllable adaptor grammars is not fully unsupervised due to a small amount of phoneme productions built into its core.
<R> <C> [EMPTY] <C> AS <C> MSR <C> CityU <C> PKU <R> <C> MDL2,  [ITALIC] xlog [ITALIC] x <C> [BOLD] 80.9 <C> [BOLD] 80.0 <C> 80.5 <C> [BOLD] 80.6 <R> <C> Wang et al. [wang2011new], Setting 3 <C> 76.9 <C> 79.7 <C> [BOLD] 80.8 <C> 79.3 <R> <C> Zhikov et al. [zhikov2010efficient] <C> – <C> 79.5 <C> 79.8 <C> – <R> <C> Chen et al. [chen2012regularized] <C> – <C> 77.4 <C> 77.0 <C> – <R> <C> Zhao and Kit [zhao2008empirical] <C> – <C> 66.5 <C> 68.4 <C> – <CAP> Table 5: Test results in token F-measure on the SIGHAN Bakeoff-2005 training sets. The baseline results all come from the literature.
<R> <C> [EMPTY] <C> Method <C> en-en <C> En → * de <C> En → * es <C> En → * fr <C> En → * ja <C> En → * zh <C> * → En de <C> * → En es <C> * → En fr <C> * → En ja <C> * → En zh <R> <C> Baseline <C> MT + sent2vec <C> 48.6 <C> 41.0 <C> 35.4 <C> 34.7 <C> 47.2 <C> 43.1 <C> 46.5 <C> 47.2 <C> 44.4 <C> 48.6 <C> 41.7 <R> <C> Baseline <C> LASER (original) <C> 55.6 <C> 45.1 <C> 48.6 <C> 48.6 <C> 47.9 <C> 45.1 <C> 43.8 <C> 45.8 <C> 50.7 <C> 44.4 <C> 49.3 <R> <C> Baseline <C> Contrastive loss <C> 34.0 <C> 19.4 <C> 12.5 <C> 22.9 <C> 25.7 <C> 21.5 <C> 24.3 <C> 18.8 <C> 25.7 <C> 24.3 <C> 20.1 <R> <C> Baseline <C> N-pair loss <C> 27.8 <C> 20.8 <C> 22.9 <C> 21.5 <C> 20.8 <C> 21.5 <C> 24.3 <C> 24.3 <C> 25.7 <C> 25.0 <C> 20.1 <R> <C> Baseline <C> Softmax loss <C> 30.6 <C> 13.9 <C> 13.9 <C> 7.6 <C> 8.3 <C> 7.6 <C> 13.2 <C> 24.3 <C> 16.0 <C> 20.8 <C> 13.9 <R> <C> Proposed <C> Emu <C> [BOLD] 78.5 ***  <C> [BOLD] 66.7 ***  <C> [BOLD] 66.0 ***  <C> 63.2 *** <C> 63.2 *** <C> [BOLD] 62.5 ***  <C> [BOLD] 56.9 ***  <C> [BOLD] 62.5 ***  <C> [BOLD] 58.3 ***  <C> 53.5 ** <C> [BOLD] 59.7 **  <R> <C> Proposed <C> Emu w/o LD <C> 76.4 *** <C> 63.2 *** <C> 59.7 *** <C> [BOLD] 65.3 ***  <C> [BOLD] 66.7 ***  <C> 56.9 *** <C> 55.6 *** <C> 61.8 *** <C> 56.9 <C> 54.2 ** <C> 58.3 ** <R> <C> Proposed <C> Emu w/o LD+CL <C> 77.1 *** <C> 62.5 *** <C> 60.4 *** <C> 61.8 *** <C> 68.1 *** <C> 58.3 *** <C> 58.3 *** <C> [BOLD] 62.5 ***  <C> 56.9 *** <C> [BOLD] 55.6 ***  <C> 58.3 ** <R> <C> Proposed <C> Emu-Parallel <C> 79.2 *** <C> 68.1 *** <C> 65.3 *** <C> 65.3 *** <C> 59.0 *** <C> 61.8 *** <C> 59.7 *** <C> 61.1 *** <C> 59.0 ** <C> 48.6 <C> 58.3 ** <CAP> Table 2: Experimental results (Acc@1) on three dataset. The highest performance (excluding Emu-Parallel) is in bold and the highest performance by Emu-Parallel is underlined. * , ** , and *** denote p-value <0.01, 0.05, and 0.10 respectively based on the binomial proportion confidence intervals of Acc@1 values against the baseline methods.
<R> <C> [EMPTY] <C> Method <C> en-en <C> En → * de <C> En → * es <C> En → * fr <C> En → * ja <C> En → * zh <C> * → En de <C> * → En es <C> * → En fr <C> * → En ja <C> * → En zh <R> <C> Baseline <C> MT + sent2vec <C> 90.5 <C> 87.3 <C> 89.7 <C> 87.7 <C> 2.4 <C> 7.1 <C> 84.9 <C> 84.9 <C> 86.1 <C> 80.6 <C> 81.7 <R> <C> Baseline <C> LASER (original) <C> 88.5 <C> 86.5 <C> 84.1 <C> 81.3 <C> 85.3 <C> 87.7 <C> 87.7 <C> 87.7 <C> 85.7 <C> 86.5 <C> [BOLD] 87.7 <R> <C> Baseline <C> Contrastive loss <C> 83.3 <C> 62.3 <C> 67.9 <C> 63.9 <C> 44.0 <C> 52.8 <C> 66.7 <C> 69.4 <C> 64.3 <C> 57.9 <C> 59.1 <R> <C> Baseline <C> N-pair loss <C> 81.0 <C> 57.1 <C> 49.2 <C> 52.0 <C> 30.2 <C> 42.1 <C> 58.3 <C> 57.9 <C> 55.2 <C> 41.3 <C> 41.3 <R> <C> Baseline <C> Softmax loss <C> 90.5 <C> 48.0 <C> 63.5 <C> 52.0 <C> 56.0 <C> 52.0 <C> 50.0 <C> 46.0 <C> 45.2 <C> 35.7 <C> 39.7 <R> <C> Proposed <C> Emu <C> [BOLD] 98.8 ***  <C> [BOLD] 98.4 ***  <C> [BOLD] 98.4 ***  <C> 93.7 *** <C> [BOLD] 98.4 ***  <C> [BOLD] 97.6 ***  <C> [BOLD] 95.6 ***  <C> [BOLD] 94.8 ***  <C> [BOLD] 94.0 ***  <C> [BOLD] 89.3 <C> 84.5 <R> <C> Proposed <C> Emu w/o LD <C> 97.6 *** <C> 98.0 *** <C> 96.8 *** <C> 94.8 *** <C> 95.6 *** <C> 93.7 *** <C> 92.5 *** <C> 83.3 <C> 88.5 <C> 84.5 <C> 87.3 <R> <C> Proposed <C> Emu w/o LD+CL <C> 98.4 *** <C> 97.6 *** <C> 98.0 *** <C> [BOLD] 96.4 ***  <C> 94.0 *** <C> 95.6 *** <C> 91.7 ** <C> 84.5 <C> 87.7 <C> 82.1 <C> 86.1 <R> <C> Proposed <C> Emu-Parallel <C> 99.2 *** <C> 98.4 *** <C> 98.0 *** <C> 95.2 *** <C> 97.2 *** <C> 96.0 *** <C> 95.2 *** <C> 95.6 *** <C> 93.7 *** <C> 77.8 <C> 83.7 <CAP> Table 2: Experimental results (Acc@1) on three dataset. The highest performance (excluding Emu-Parallel) is in bold and the highest performance by Emu-Parallel is underlined. * , ** , and *** denote p-value <0.01, 0.05, and 0.10 respectively based on the binomial proportion confidence intervals of Acc@1 values against the baseline methods.
<R> <C> [EMPTY] <C> Method <C> en-en <C> En → * de <C> En → * es <C> En → * fr <C> En → * ja <C> En → * zh <C> * → En de <C> * → En es <C> * → En fr <C> * → En ja <C> * → En zh <R> <C> Baseline <C> MT + sent2vec <C> 77.6 <C> 74.0 <C> 75.8 <C> 73.5 <C> 1.8 <C> 72.6 <C> 70.4 <C> 70.4 <C> 69.5 <C> 70.4 <C> 71.3 <R> <C> Baseline <C> LASER (original) <C> 88.8 <C> 83.9 <C> [BOLD] 86.5 <C> [BOLD] 85.2 <C> [BOLD] 86.5 <C> 85.2 <C> 79.8 <C> 79.8 <C> [BOLD] 84.3 <C> [BOLD] 88.3 <C> [BOLD] 85.7 <R> <C> Baseline <C> Contrastive loss <C> 65.0 <C> 35.4 <C> 43.0 <C> 42.6 <C> 27.8 <C> 26.0 <C> 50.2 <C> 59.2 <C> 54.3 <C> 50.7 <C> 49.8 <R> <C> Baseline <C> N-pair loss <C> 61.4 <C> 23.8 <C> 40.4 <C> 35.9 <C> 12.6 <C> 26.5 <C> 50.2 <C> 53.4 <C> 45.7 <C> 50.7 <C> 52.0 <R> <C> Baseline <C> Softmax loss <C> 75.8 <C> 20.2 <C> 35.4 <C> 30.5 <C> 12.1 <C> 16.1 <C> 31.4 <C> 39.0 <C> 35.9 <C> 28.3 <C> 26.0 <R> <C> Proposed <C> Emu <C> [BOLD] 89.7 <C> [BOLD] 87.0∗ <C> [BOLD] 86.5 <C> 84.3 <C> [BOLD] 86.5 <C> 86.1 <C> [BOLD] 81.2 <C> [BOLD] 87.0 <C> 82.5 <C> 83.9 <C> 84.8 <R> <C> Proposed <C> Emu w/o LD <C> 89.7 <C> 83.9 <C> 85.7 <C> 83.4 <C> 82.1 <C> [BOLD] 87.4 <C> 77.6 <C> 84.3 <C> 83.4 <C> 86.5 <C> 83.9 <R> <C> Proposed <C> Emu w/o LD+CL <C> 88.3 <C> 75.3 <C> 80.3 <C> 75.8 <C> 70.9 <C> 78.5 <C> 72.6 <C> 82.1 <C> 75.3 <C> 81.6 <C> 80.7 <R> <C> Proposed <C> Emu-Parallel <C> 86.5 <C> 78.0 <C> 79.4 <C> 76.2 <C> 74.4 <C> 78.5 <C> 78.9 <C> 82.1 <C> 78.0 <C> 80.3 <C> 82.1 <CAP> Table 2: Experimental results (Acc@1) on three dataset. The highest performance (excluding Emu-Parallel) is in bold and the highest performance by Emu-Parallel is underlined. * , ** , and *** denote p-value <0.01, 0.05, and 0.10 respectively based on the binomial proportion confidence intervals of Acc@1 values against the baseline methods.
<R> <C> Test relation <C> # <C> R13-F <C> F <C> FS <C> FSL <R> <C> person/company <C> 106 <C> 0.75 <C> 0.73 <C> 0.74 <C> [BOLD] 0.77 <R> <C> location/containedby <C> 73 <C> 0.69 <C> 0.62 <C> 0.70 <C> [BOLD] 0.71 <R> <C> person/nationality <C> 28 <C> 0.19 <C> 0.20 <C> 0.20 <C> [BOLD] 0.21 <R> <C> author/works_written <C> 27 <C> 0.65 <C> [BOLD] 0.71 <C> 0.69 <C> 0.65 <R> <C> person/place_of_birth <C> 21 <C> 0.72 <C> 0.69 <C> [BOLD] 0.72 <C> 0.70 <R> <C> parent/child <C> 19 <C> 0.76 <C> 0.77 <C> 0.81 <C> [BOLD] 0.85 <R> <C> person/place_of_death <C> 19 <C> 0.83 <C> [BOLD] 0.85 <C> 0.83 <C> [BOLD] 0.85 <R> <C> neighborhood/neighborhood_of <C> 11 <C> [BOLD] 0.70 <C> 0.67 <C> 0.63 <C> 0.62 <R> <C> person/parents <C> 6 <C> 0.61 <C> 0.53 <C> [BOLD] 0.66 <C> [BOLD] 0.66 <R> <C> company/founders <C> 4 <C> [BOLD] 0.77 <C> 0.73 <C> 0.64 <C> 0.67 <R> <C> sports_team/league <C> 4 <C> [BOLD] 0.59 <C> 0.44 <C> 0.43 <C> 0.56 <R> <C> team_owner/teams_owned <C> 2 <C> 0.38 <C> [BOLD] 0.64 <C> [BOLD] 0.64 <C> 0.61 <R> <C> team/arena_stadium <C> 2 <C> [BOLD] 0.13 <C> [BOLD] 0.13 <C> [BOLD] 0.13 <C> 0.12 <R> <C> film/directed_by <C> 2 <C> [BOLD] 0.50 <C> 0.18 <C> 0.17 <C> 0.13 <R> <C> broadcast/area_served <C> 2 <C> 0.58 <C> 0.83 <C> 0.83 <C> [BOLD] 1.00 <R> <C> structure/architect <C> 2 <C> [BOLD] 1.00 <C> [BOLD] 1.00 <C> [BOLD] 1.00 <C> [BOLD] 1.00 <R> <C> composer/compositions <C> 2 <C> [BOLD] 0.67 <C> 0.64 <C> 0.51 <C> 0.50 <R> <C> person/religion <C> 1 <C> [BOLD] 1.00 <C> [BOLD] 1.00 <C> [BOLD] 1.00 <C> [BOLD] 1.00 <R> <C> film/produced_by <C> 1 <C> 0.50 <C> [BOLD] 1.00 <C> [BOLD] 1.00 <C> 0.33 <R> <C> Weighted MAP <C> [EMPTY] <C> 0.67 <C> 0.65 <C> 0.67 <C> 0.69 <CAP> Table 1: Weighted mean average precision for our reimplementation of the matrix factorization model (F) compared to restricting the entity-pair space (FS) and injecting WordNet rules (FSL). Model F results by Riedel et al. (2013) are denoted as R13-F.
<R> <C> [ITALIC] rp <C> rule ⇒ <C> [ITALIC] rq <C> model FSL  [ITALIC] σ( [ITALIC] rq⊤ [ITALIC] tp) <C> model FSL  [ITALIC] σ( [ITALIC] rp⊤ [ITALIC] tq) <C> model FS  [ITALIC] σ( [ITALIC] rq⊤ [ITALIC] tp) <C> model FS  [ITALIC] σ( [ITALIC] rp⊤ [ITALIC] tq) <R> <C> appos->party->amod <C> ⇒ <C> appos->organization->amod <C> 0.99 <C> 0.22 <C> 0.70 <C> 0.86 <R> <C> possappos <C> ⇒ <C> possappos <C> 0.96 <C> 0.00 <C> 0.72 <C> 0.89 <R> <C> appos->prosecutor->nn <C> ⇒ <C> appos->lawyer->nn <C> 0.99 <C> 0.01 <C> 0.87 <C> 0.80 <R> <C> appos->daily->amod <C> ⇒ <C> appos->newspaper->amod <C> 0.98 <C> 0.79 <C> 0.90 <C> 0.86 <R> <C> appos->ambassador->amod <C> ⇒ <C> appos->diplomat->amod <C> 0.31 <C> 0.05 <C> 0.93 <C> 0.84 <R> <C> average over 36 high-quality Wordnet rules <C> average over 36 high-quality Wordnet rules <C> average over 36 high-quality Wordnet rules <C> 0.95 <C> 0.28 <C> 0.74 <C> 0.70 <CAP> Table 3: Average of σ(rq⊤t) over all inferred facts ⟨rq,tp⟩ for tuples tp from training items for relation rp, and vice versa, for Wordnet implications rp⇒rq, and model FSL (injected rules) vs. model FS (no rules).
<R> <C> ErrorType <C> WER <C> POWER <R> <C> D.closed <C> 0.112 <C> 0.097 <R> <C> D.open <C> 0.067 <C> 0.041 <R> <C> I.closed <C> 0.101 <C> 0.065 <R> <C> I.open <C> 0.096 <C> 0.033 <R> <C> S.closed_closed.morph <C> 0.001 <C> 0.001 <R> <C> S.closed_closed.nomorph <C> 0.147 <C> 0.139 <R> <C> S.closed_open.nomorph <C> 0.069 <C> 0.036 <R> <C> S.open_closed.morph <C> 0.001 <C> 0.001 <R> <C> S.open_closed.nomorph <C> 0.106 <C> 0.069 <R> <C> S.open_open.morph <C> 0.052 <C> 0.054 <R> <C> S.open_open.nomorph <C> 0.247 <C> 0.165 <R> <C> SS.closed_span <C> [EMPTY] <C> 0.010 <R> <C> SS.open_span <C> [EMPTY] <C> 0.186 <R> <C> SS.span_closed <C> [EMPTY] <C> 0.019 <R> <C> SS.span_open <C> [EMPTY] <C> 0.069 <R> <C> SS.span_span <C> [EMPTY] <C> 0.016 <CAP> Table 2: Proportion of ASR error types by word class, averaged by ASR system. Substitution labels (S, SS) show the alignment from reference class to hypothesis class. Substitution spans (SS) contain a span of words aligned either to a single word or another span. Word-level substitutions (S) are classified as morphological/non-morphological errors.
<R> <C> Model <C> BLEU-1 <C> BLEU-2 <C> BLEU-3 <C> BLEU-4 <C> METEOR <C> ROUGE-L <R> <C> L2A Du et al. ( 2017 ) <C> 43.21 (43.09) <C> 24.77 (25.96) <C> 15.93 (17.50) <C> 10.60 (12.28) <C> 16.39 (16.62) <C> 38.98 (39.75) <R> <C> AutoQG Kumar et al. ( 2018 ) <C> 44.68 (46.32) <C> 26.96 (28.81) <C> 18.18 (19.67) <C> 12.68 (13.85) <C> 17.86 (18.51) <C> 40.59 (41.75) <R> <C> NQG [ITALIC] LC Song et al. ( 2018 ) <C> - <C> - <C> - <C> - (13.98) <C> - (18.77) <C> - (42.72) <R> <C> SUMBLEU Paulus et al. ( 2018 ) <C> 11.20- <C> 3.50- <C> 1.21- <C> 0.45- <C> 6.68- <C> 15.25- <R> <C> SUMROUGE Paulus et al. ( 2018 ) <C> 11.94- <C> 3.95- <C> 1.65- <C> 0.082- <C> 6.61- <C> 16.17- <R> <C> GEBLEU <C> 46.84 <C> 29.38 <C> 20.33 <C> 14.47 <C> 19.08 <C> 41.07 <R> <C> GEBLEU+QSS+ANSS <C> 46.59 <C> 29.68 <C> 20.79 <C> 15.04 <C> 19.32 <C> 41.73 <R> <C> GEDAS <C> 44.64 <C> 28.25 <C> 19.63 <C> 14.07 <C> 18.12 <C> 42.07 <R> <C> GEDAS+QSS+ANSS <C> 46.07 <C> 29.78 <C> 21.43 <C> 16.22 <C> 19.44 <C> 42.84 <R> <C> GEGLEU <C> 45.20 <C> 29.22 <C> 20.79 <C> 15.26 <C> 18.98 <C> 43.47 <R> <C> GEGLEU+QSS+ANSS <C> 47.04 <C> 30.03 <C> 21.15 <C> 15.92 <C> 19.05 <C> 43.55 <R> <C> GEROUGE <C> 47.01 <C> 30.67 <C> 21.95 <C> 16.17 <C> 19.85 <C> 43.90 <R> <C> GEROUGE+QSS+ANSS <C> [BOLD] 48.13 <C> [BOLD] 31.15 <C> [BOLD] 22.01 <C> [BOLD] 16.48 <C> [BOLD] 20.21 <C> [BOLD] 44.11 <CAP> Table 3: Experimental results on the test set on automatic evaluation metrics. Best results for each metric (column) are bolded. The numbers in parentheses for L2A, AutoQG and NQGLC are those from the best models reported in their respective original papers. The slight difference of up to 1.7% from our reproduced numbers can be attributed to reimplementation and different versions of various libraries used. Models with new QG-specific reward functions (QSS+ANSS) are highlighted in gray for easy comparison.
<R> <C> [BOLD] DUC’04 <C> Lemm <C> Stem <C> Raw <C> fix1 <R> <C> Lemm <C> ∙ <C> 0.96149 <C> 0.91287 <C> 0.51904 <R> <C> Stem <C> 0.96149 <C> ∙ <C> 0.94492 <C> 0.53800 <R> <C> Raw <C> 0.91287 <C> 0.94492 <C> ∙ <C> 0.46914 <R> <C> fix1 <C> 0.51904 <C> 0.53800 <C> 0.46914 <C> ∙ <CAP> Table 6: Mantel test correlation for corpus DUC’04 data (English, p-value=0.001).
<R> <C> [BOLD] Medicina Clínica <C> Lemm <C> Stem <C> Raw <C> fix1 <R> <C> Lemm <C> ∙ <C> 0.96725 <C> 0,91174 <C> 0.58541 <R> <C> Stem <C> 0.96725 <C> ∙ <C> 0,91942 <C> 0,49614 <R> <C> Raw <C> 0,91174 <C> 0,91942 <C> ∙ <C> 0,51503 <R> <C> fix1 <C> 0,58541 <C> 0,49614 <C> 0,51503 <C> ∙ <CAP> Table 7: Mantel test correlation for corpus Medicina Clínica data (Spanish, p-value=0.001).
<R> <C> [BOLD] Pistes <C> Lemm <C> Stem <C> Raw <C> fix1 <R> <C> Lemm <C> ∙ <C> 0.93016 <C> 0.85708 <C> 0.53801 <R> <C> Stem <C> 0.93016 <C> ∙ <C> 0,89499 <C> 0.51641 <R> <C> Raw <C> 0.85708 <C> 0,89499 <C> ∙ <C> 0.45156 <R> <C> fix1 <C> 0,53801 <C> 0.51641 <C> 0.45156 <C> ∙ <CAP> Table 8: Mantel test correlation for corpus Pistes data (French, p-value=0.001).
<R> <C> [BOLD] Paragraph Scheme <C> [BOLD] Question Scheme <C> [BOLD] Dev F1 <C> [BOLD] Dev EM <R> <C> Original SQuAD paragraphs <C> Original SQuAD questions <C> 90.58 <C> 83.89 <R> <C> [EMPTY] <C> Words sampled from paragraphs, starts with question-starter word, ends with ? <C> 86.62 <C> 78.09 <R> <C> [EMPTY] <C> Words sampled from paragraphs <C> 81.08 <C> 68.58 <R> <C> Wikitext103 paragraphs <C> Words sampled from paragraphs, starts with question-starter word, ends with ? (wiki) <C> 86.06 <C> 77.11 <R> <C> [EMPTY] <C> Words sampled from paragraphs <C> 81.71 <C> 69.56 <R> <C> Unigram frequency based sampling from wikitext-103 vocabulary with length equal to original paragraphs <C> Words sampled from paragraphs, starts with question-starter word, ends with ? <C> 80.72 <C> 70.90 <R> <C> [EMPTY] <C> Words sampled from paragraphs <C> 70.68 <C> 56.75 <R> <C> Unigram frequency based sampling from wikitext-103 vocabulary with length equal to wikitext103 paragraphs <C> Words sampled from paragraphs, starts with question-starter word, ends with ? (random) <C> 79.14 <C> 68.52 <R> <C> [EMPTY] <C> Words sampled from paragraphs <C> 71.01 <C> 57.60 <R> <C> Uniform random sampling from wikitext-103 vocabulary with length equal to original paragraphs <C> Words sampled from paragraphs, starts with question-starter word, ends with ? <C> 72.63 <C> 63.41 <R> <C> [EMPTY] <C> Words sampled from paragraphs <C> 52.80 <C> 43.20 <CAP> Table 12: Development set F1 using different kinds of extraction datasets on SQuAD 1.1. The final random and wiki schemes have also been indicated in the table.
<R> <C> Method <C> GAN type <C> Architecture (1) <C> Architecture (2) <C> Architecture (3) <C> Architecture (4) <R> <C> Baseline <C> - <C> 80.5 <C> 81.3 <C> 81.3 <C> 83.4 <R> <C> Ours <C> GAN <C> 82.3 <C> 83.3 <C> 85.7 <C> 87.0 <R> <C> Ours <C> CGAN <C> [BOLD] 85.6 <C> [BOLD] 85.7 <C> [BOLD] 86.7 <C> [BOLD] 87.5 <R> <C> Ours <C> WGAN <C> 79.2 <C> 80.2 <C> 83.7 <C> 86.3 <CAP> Table V: Maximum test set accuracy for the different network structures.
<R> <C> model <C> CER/% <C> SER/% <R> <C> [ITALIC] CTC <C> 5.29 <C> 14.57 <R> <C> [ITALIC] Content based attention <C> 4.05 <C> 9.10 <R> <C> + [ITALIC] trigram LM <C> 3.60 <C> 7.20 <R> <C> [ITALIC] Location based attention <C> 3.82 <C> 8.17 <R> <C> + [ITALIC] trigram LM <C> 3.26 <C> 6.33 <R> <C> [ITALIC] Attention smoothing <C> [BOLD] 3.58 <C> [BOLD] 7.43 <R> <C> + [ITALIC] trigram LM <C> [BOLD] 2.81 <C> [BOLD] 5.77 <CAP> Table 1: Results of our attention-based models with a beam size of 30, τ=2 and γ=0.1.
<R> <C> Model <C> Size <C> Target <C> RW <C> WS <R> <C> EditDist <C> - <C> - <C> 18 <C> -2 <R> <C> MIMICK <C> 649KB <C> Polyglot <C> 14 <C> 12 <R> <C> BoS <C> 238MB <C> Polyglot <C> 36 <C> 36 <R> <C> BoS <C> 1.3GB <C> Google <C> 46 <C> 56 <R> <C> fastText <C> 8.0GB <C> - <C> 48 <C> 74 <CAP> Table 2: Word similarity task results measured in Spearman’s ρ×100.
<R> <C> [BOLD] Model <C> [BOLD] Test BPC <R> <C> NNLM  <C> 1.57 <R> <C> BPTT-RNN  <C> 1.42 <R> <C> HF-MRNN  <C> 1.41 <R> <C> sRNN  <C> 1.41 <R> <C> DOT(S)-RNN  <C> 1.39 <R> <C> LSTMRNN (w/ adapt. noise, w/o dyn. eval)  <C> 1.26 <R> <C> LSTMRNN (w/ adapt. noise, w/ dyn. eval) \getrefnumbernote:adaptnoise\getrefnumbernote:adaptnoisefootnotemark: note:adaptnoise \getrefnumbernote:dyneval\getrefnumbernote:dynevalfootnotemark: note:dyneval  <C> 1.24 <R> <C> GRURNN (our baseline) <C> 1.39 <R> <C> LSTMRNN (our baseline) <C> 1.37 <R> <C> GRURNTN (proposed) <C> 1.33 <R> <C> LSTMRNTN (proposed) <C> 1.34 <CAP> TABLE I: PennTreeBank test set BPC
<R> <C> [BOLD] Category <C> [BOLD] Authentic <C> [BOLD] Fake <R> <C> Miscellaneous <C> 2218 <C> 654 <R> <C> Entertainment <C> 2636 <C> 106 <R> <C> Lifestyle <C> 901 <C> 102 <R> <C> National <C> 18708 <C> 99 <R> <C> International <C> 6990 <C> 91 <R> <C> Politics <C> 2941 <C> 90 <R> <C> Sports <C> 6526 <C> 54 <R> <C> Crime <C> 1072 <C> 42 <R> <C> Education <C> 1115 <C> 30 <R> <C> Technology <C> 843 <C> 29 <R> <C> Finance <C> 1224 <C> 2 <R> <C> Editorial <C> 3504 <C> 0 <CAP> Table 1: Number of news in each category.
<R> <C> [EMPTY] <C> [BOLD] Overall  [BOLD] P <C> [BOLD] Overall  [BOLD] R <C> [BOLD] Overall  [BOLD] F1 <C> [BOLD] Fake Class  [BOLD] P <C> [BOLD] Fake Class  [BOLD] R <C> [BOLD] Fake Class  [BOLD] F1 <R> <C> [BOLD] Baselines <C> [BOLD] Baselines <C> [BOLD] Baselines <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Majority <C> 0.97 <C> 1.00 <C> 0.99 <C> 0.00 <C> 0.00 <C> 0.00 <R> <C> Random <C> 0.97 <C> 0.50 <C> 0.66 <C> 0.03 <C> 0.50 <C> [BOLD] 0.05 <R> <C> [BOLD] Traditional Linguistic Features <C> [BOLD] Traditional Linguistic Features <C> [BOLD] Traditional Linguistic Features <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Unigram (U) <C> 0.99 <C> 0.99 <C> 0.99 <C> 0.99 <C> 0.71 <C> 0.83 <R> <C> Bigram (B) <C> 0.98 <C> 0.99 <C> 0.99 <C> 0.97 <C> 0.42 <C> 0.59 <R> <C> Trigram (T) <C> 0.98 <C> 0.99 <C> 0.98 <C> 0.74 <C> 0.31 <C> 0.44 <R> <C> U+B+T <C> 0.99 <C> 0.99 <C> 0.99 <C> 0.98 <C> 0.68 <C> 0.80 <R> <C> C3-gram(C3) <C> 0.99 <C> 0.99 <C> 0.99 <C> 0.98 <C> 0.82 <C> 0.89 <R> <C> C4-gram(C4) <C> 0.99 <C> 0.99 <C> 0.99 <C> 0.99 <C> 0.78 <C> 0.87 <R> <C> C5-gram(C5) <C> 0.99 <C> 0.99 <C> 0.99 <C> 1.00 <C> 0.74 <C> 0.85 <R> <C> C3+C4+C5 <C> 0.99 <C> 0.99 <C> 0.99 <C> 1.00 <C> 0.77 <C> 0.87 <R> <C> All Lexical(L) <C> 0.99 <C> 0.99 <C> 0.99 <C> 1.00 <C> 0.76 <C> 0.86 <R> <C> POS tag <C> 0.97 <C> 1.00 <C> 0.98 <C> 1.00 <C> 0.00 <C> 0.01 <R> <C> L+POS <C> 0.99 <C> 0.99 <C> 0.99 <C> 0.99 <C> 0.76 <C> 0.86 <R> <C> Embedding(F) <C> 0.98 <C> 0.99 <C> 0.99 <C> 0.94 <C> 0.33 <C> 0.49 <R> <C> Embedding(N) <C> 0.98 <C> 0.99 <C> 0.99 <C> 0.84 <C> 0.32 <C> 0.46 <R> <C> L+POS+E(F) <C> 0.99 <C> 0.99 <C> 0.99 <C> 0.99 <C> 0.77 <C> 0.87 <R> <C> L+POS+E(N) <C> 0.99 <C> 0.99 <C> 0.99 <C> 0.98 <C> 0.79 <C> 0.88 <R> <C> MP <C> 0.97 <C> 0.99 <C> 0.98 <C> 0.94 <C> 0.15 <C> 0.27 <R> <C> L+POS+E(F)+MP <C> 0.99 <C> 0.99 <C> 0.99 <C> 0.99 <C> 0.84 <C> [BOLD] 0.91 <R> <C> L+POS+E(N)+MP <C> 0.99 <C> 0.99 <C> 0.99 <C> 0.98 <C> 0.84 <C> [BOLD] 0.91 <R> <C> All Features <C> 0.99 <C> 0.99 <C> 0.99 <C> 0.98 <C> 0.84 <C> [BOLD] 0.91 <R> <C> [BOLD] Neural Network Models <C> [BOLD] Neural Network Models <C> [BOLD] Neural Network Models <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> CNN <C> 0.98 <C> 1.00 <C> 0.99 <C> 0.79 <C> 0.41 <C> [BOLD] 0.59 <R> <C> LSTM <C> 0.99 <C> 0.99 <C> 0.99 <C> 0.69 <C> 0.44 <C> 0.53 <R> <C> BERT <C> 0.99 <C> 1.00 <C> 0.99 <C> 0.80 <C> 0.60 <C> [BOLD] 0.68 <CAP> Table 3: Results of experiments with Traditional Linguistic Features (SVM) and Neural Networks with test set. P and R denote precision and recall, respectively. F’ and N’ abbreviate Fastext’ and News’, respectively.
<R> <C> Algorithm <C> BLEU score <R> <C> Standard Transformer <C> 28.57 <R> <C> Use BERT to initialize the encoder of NMT <C> 27.14 <R> <C> Use XLM to initialize the encoder of NMT <C> 28.22 <R> <C> Use XLM to initialize the decoder of NMT <C> 26.13 <R> <C> Use XLM to initialize both the encoder and decoder of NMT <C> 28.99 <R> <C> Leveraging the output of BERT as embeddings <C> 29.67 <CAP> Table 1: Preliminary explorations on IWSLT’14 English→German translation.
<R> <C> [EMPTY] <C> Transformer <C> BERT-fused <R> <C> En→De <C> 28.57 <C> 30.45 <R> <C> De→En <C> 34.64 <C> 36.11 <R> <C> En→Es <C> 39.0 <C> 41.4 <R> <C> En→Zh <C> 26.3 <C> 28.2 <R> <C> En→Fr <C> 35.9 <C> 38.7 <CAP> Table 2: BLEU of all IWSLT tasks.
<R> <C> [EMPTY] <C> En→De <C> De→En <R> <C> Sentence-level <C> 28.57 <C> 34.64 <R> <C> Our Document-level <C> 28.90 <C> 34.95 <R> <C> Miculicich et al. ( 2018 ) <C> 27.94 <C> 33.97 <R> <C> Sentence-level + BERT <C> 30.45 <C> 36.11 <R> <C> Document-level + BERT <C> 31.02 <C> 36.69 <CAP> Table 4: BLEU of document-level translation.
<R> <C> Algorithm <C> BLEU <R> <C> Standard Transformer <C> 28.57 <R> <C> BERT-fused model <C> 30.45 <R> <C> 12-layer encoder <C> 29.27 <R> <C> 18-layer encoder <C> 28.92 <R> <C> 2-model ensemble (standard) <C> 29.71 <R> <C> 3-model ensemble (standard) <C> 30.08 <R> <C> 4-model ensemble (standard) <C> 30.18 <R> <C> 2-model ensemble (BERT-fused) <C> 31.09 <R> <C> 3-model ensemble (BERT-fused) <C> 31.45 <R> <C> 4-model ensemble (BERT-fused) <C> 31.85 <CAP> Table 8: More ablation study on IWSLT’14 En→De.
<R> <C> Precision@ [ITALIC] N <C> 100 <C> 300 <C> 500 <C> 700 <C> 900 <C> 1000 <R> <C> PCNN+ATT <C> [BOLD] 97.0 <C> 93.7 <C> 92.8 <C> 89.1 <C> 85.2 <C> 83.9 <R> <C> PCNN+ATT+LoRE <C> [BOLD] 97.0 <C> 95.0 <C> 94.2 <C> 91.6 <C> 89.6 <C> 87.0 <R> <C> PCNN+ATT+GloRE <C> [BOLD] 97.0 <C> [BOLD] 97.3 <C> [BOLD] 94.6 <C> [BOLD] 93.3 <C> [BOLD] 90.1 <C> [BOLD] 89.3 <CAP> Table 2: Manual evaluation: false negatives from held-out evaluation are manually corrected by human experts.
<R> <C> [EMPTY] <C> [BOLD] Person EM <C> [BOLD] Person F1 <C> [BOLD] Date EM <C> [BOLD] Date F1 <C> [BOLD] Numerical EM <C> [BOLD] Numerical F1 <R> <C> BERT Base - w/ data bias <C> 55.9 <C> 63.1 <C> 48.9 <C> 58.2 <C> 38.7 <C> 48.0 <R> <C> + Robust Training <C> [BOLD] 59.1 <C> [BOLD] 66.6 <C> [BOLD] 58.4 <C> [BOLD] 65.6 <C> [BOLD] 48.7 <C> [BOLD] 58.9 <R> <C> BERT Base - w/o data bias <C> 69.2 <C> 78.1 <C> 73.2 <C> 81.7 <C> 69.6 <C> 80.5 <CAP> Table 5: Robust training leads to improved generalisation under train/test distribution mismatch (data bias, top). Bottom: control experiment without train/test mismatch.
<R> <C> [EMPTY] <C> [BOLD] Person EM <C> [BOLD] Person F1 <C> [BOLD] Date EM <C> [BOLD] Date F1 <C> [BOLD] Numerical EM <C> [BOLD] Numerical F1 <R> <C> GQA (lewis2018generative) <C> 53.1 <C> 61.9 <C> 64.7 <C> 72.5 <C> [BOLD] 58.5 <C> [BOLD] 67.6 <R> <C> BERT Base - w/ data bias <C> 66.0 <C> 72.5 <C> 67.1 <C> 72.0 <C> 46.6 <C> 54.5 <R> <C> + Robust Training <C> [BOLD] 67.4 <C> [BOLD] 72.8 <C> [BOLD] 68.1 <C> [BOLD] 74.4 <C> 56.3 <C> 64.5 <CAP> Table 11: Robust training leads to improved generalisation under train/test distribution mismatch (data bias).
<R> <C> [BOLD] Model <C> [BOLD] Person  [BOLD] BLEU <C> [BOLD] Person  [BOLD] METEOR <C> [BOLD] Person  [BOLD] ROUGE <C> [BOLD] Animal  [BOLD] BLEU <C> [BOLD] Animal  [BOLD] METEOR <C> [BOLD] Animal  [BOLD] ROUGE <R> <C> Seq2seq <C> 11.3 <C> 16.9 <C> 28.8 <C> 5.8 <C> 11.5 <C> 20.5 <R> <C> Pointer <C> 17.2 <C> 21.1 <C> 37.4 <C> 6.6 <C> 13.7 <C> 37.8 <R> <C> +Type <C> 23.1 <C> 22.2 <C> 39.5 <C> [BOLD] 17.2 <C> [BOLD] 17.3 <C> 42.8 <R> <C> +Type & Position <C> [BOLD] 23.2 <C> [BOLD] 23.4 <C> [BOLD] 42.0 <C> 14.8 <C> 17.2 <C> [BOLD] 45.0 <CAP> Table 5: Generation Performance based on Standard Metrics %)
<R> <C> [EMPTY] <C> With Shallow Tag  [ITALIC] P <C> With Shallow Tag  [ITALIC] R <C> With Shallow Tag  [ITALIC] F1 <C> No Shallow Tag  [ITALIC] P <C> No Shallow Tag  [ITALIC] R <C> No Shallow Tag  [ITALIC] F1 <R> <C> [ITALIC] Copy <C> 0.375 <C> 0.412 <C> 0.393 <C> 0.378 <C> 0.431 <C> 0.403 <R> <C> [ITALIC] Copy+ [ITALIC] Coverage <C> 0.390 <C> 0.390 <C> 0.390 <C> 0.307 <C> 0.417 <C> 0.353 <R> <C> [ITALIC] Copy+ [ITALIC] GatedDep <C> 0.427 <C> 0.371 <C> 0.397 <C> 0.335 <C> 0.419 <C> 0.372 <R> <C> [ITALIC] All <C> 0.443 <C> 0.419 <C> [BOLD] 0.431 <C> 0.373 <C> 0.435 <C> 0.402 <CAP> Table 4. Analysis of Components involved in Logician.
<R> <C> [BOLD] Model <C> [BOLD] CityU <C> [BOLD] MSRA <R> <C> Zhao and Kit ( 2008 ) <C> 89.18 <C> 86.30 <R> <C> Zhou et al. ( 2013 ) <C> 89.78 <C> 90.28 <R> <C> Dong et al. ( 2016 ) <C> / <C> 90.95 <R> <C> Baseline (BiLSTM+CRF) <C> 89.84 <C> 89.93 <R> <C> Ours (+hypersphere feature) <C> 90.24 <C> 90.98 <R> <C> Baseline (BERT) <C> 95.10 <C> 95.33 <R> <C> Ours (+hypersphere feature) <C> [BOLD] 95.30 <C> [BOLD] 95.53 <CAP> Table 9: Results (%) of Chinese NER.
<R> <C> Modality <C> Source <C> IEMOCAP <C> MOUD <C> MOSI <R> <C> Unimodal <C> A <C> 51.52 <C> 53.70 <C> 57.14 <R> <C> Unimodal <C> V <C> 41.79 <C> 47.68 <C> 58.46 <R> <C> Unimodal <C> T <C> 65.13 <C> 48.40 <C> 75.16 <R> <C> Bimodal <C> T + A <C> 70.79 <C> 57.10 <C> 75.72 <R> <C> Bimodal <C> T + V <C> 68.55 <C> 49.22 <C> 75.06 <R> <C> Bimodal <C> A + V <C> 52.15 <C> 62.88 <C> 62.4 <R> <C> Multimodal <C> T + A + V <C> [BOLD] 71.59 <C> [BOLD] 67.90 <C> [BOLD] 76.66 <CAP> Table 1: Speaker Independent: Macro F_score reported for speaker independent classification. IEMOCAP: 10-fold speaker independent average. MOUD: 5-fold speaker independent average. MOSI: 5-fold speaker independent average. Notes: A stands for Audio, V for Video, T for Text.
<R> <C> Modality <C> Source <C> IEMOCAP <C> MOSI <R> <C> Unimodal <C> Audio <C> 66.20 <C> 64.00 <R> <C> Unimodal <C> Video <C> 60.30 <C> 62.11 <R> <C> Unimodal <C> Text <C> 67.90 <C> 78.00 <R> <C> Bimodal <C> Text + Audio <C> 78.20 <C> 76.60 <R> <C> Bimodal <C> Text + Video <C> 76.30 <C> 78.80 <R> <C> Bimodal <C> Audio + Video <C> 73.90 <C> 66.65 <R> <C> Multimodal <C> Text + Audio + Video <C> [BOLD] 81.70 <C> [BOLD] 78.80 <R> <C> [EMPTY] <C> Text + Audio + Video <C> [BOLD] 69.351 <C> [BOLD] 73.552 <CAP> Table 2: Speaker Dependent: Ten-fold cross-validation results on IEMOCAP dataset and 5-fold CV results (macro F_Score) on MOSI dataset. 1By [23]; 2by [3].
<R> <C> [BOLD] TYPE <C> [BOLD] TYPE  [BOLD] DBpedia types <C> [BOLD] # entities <C> [BOLD] # found (%) <C> [BOLD] # found (%) <C> [BOLD] lead-days  [BOLD] mean <C> [BOLD] lead-days  [BOLD] (median) <R> <C> [BOLD] Person <C> [BOLD] Person <C> [BOLD] 3851 <C> [BOLD] 3238 <C> [BOLD] (84.08%) <C> [BOLD] 660 <C> [BOLD] (550) <R> <C> [EMPTY] <C> Actor <C> 651 <C> 514 <C> (78.96%) <C> 742 <C> (727) <R> <C> [EMPTY] <C> MusicalArtist <C> 624 <C> 463 <C> (74.20%) <C> 740 <C> (649) <R> <C> [EMPTY] <C> SoccerPlayer <C> 477 <C> 429 <C> (89.94%) <C> 769 <C> (759) <R> <C> [EMPTY] <C> VoiceActor <C> 345 <C> 323 <C> (93.62%) <C> 498 <C> (363) <R> <C> [EMPTY] <C> AdultActor <C> 306 <C> 300 <C> (98.04%) <C> 376 <C> (297) <R> <C> [EMPTY] <C> BaseballPlayer <C> 238 <C> 213 <C> (89.50%) <C> 591 <C> (405) <R> <C> [EMPTY] <C> Model <C> 225 <C> 210 <C> (93.33%) <C> 764 <C> (654) <R> <C> [EMPTY] <C> Person <C> 173 <C> 138 <C> (79.77%) <C> 777 <C> (792) <R> <C> [EMPTY] <C> Politician <C> 136 <C> 112 <C> (82.35%) <C> 665 <C> (538) <R> <C> [EMPTY] <C> Others (16 types) <C> 676 <C> 536 <C> (79.29%) <C> 638 <C> (537) <R> <C> [BOLD] Creative Work <C> [BOLD] Creative Work <C> [BOLD] 4122 <C> [BOLD] 3703 <C> [BOLD] (89.84%) <C> [BOLD] 377 <C> [BOLD] (176) <R> <C> [EMPTY] <C> TelevisionShow <C> 699 <C> 644 <C> (92.13%) <C> 225 <C> (54) <R> <C> [EMPTY] <C> MusicSingle <C> 653 <C> 594 <C> (90.96%) <C> 304 <C> (85) <R> <C> [EMPTY] <C> Film <C> 641 <C> 555 <C> (86.58%) <C> 388 <C> (214) <R> <C> [EMPTY] <C> MusicAlbum <C> 550 <C> 499 <C> (90.73%) <C> 356 <C> (161) <R> <C> [EMPTY] <C> Manga <C> 523 <C> 486 <C> (92.93%) <C> 672 <C> (594) <R> <C> [EMPTY] <C> VideoGame <C> 440 <C> 392 <C> (89.09%) <C> 406 <C> (251) <R> <C> [EMPTY] <C> RadioProgram <C> 228 <C> 203 <C> (89.04%) <C> 261 <C> (38) <R> <C> [EMPTY] <C> Anime <C> 216 <C> 188 <C> (87.04%) <C> 254 <C> (85) <R> <C> [EMPTY] <C> Book <C> 101 <C> 95 <C> (94.06%) <C> 621 <C> (462) <R> <C> [EMPTY] <C> Others (4 types) <C> 71 <C> 47 <C> (66.19%) <C> 700 <C> (515) <R> <C> [BOLD] Location <C> [BOLD] Location <C> [BOLD] 223 <C> [BOLD] 179 <C> [BOLD] (80.27%) <C> [BOLD] 597 <C> [BOLD] (385) <R> <C> [EMPTY] <C> Building <C> 89 <C> 73 <C> (82.02%) <C> 497 <C> (291) <R> <C> [EMPTY] <C> Museum <C> 33 <C> 32 <C> (96.97%) <C> 685 <C> (447) <R> <C> [EMPTY] <C> Station <C> 25 <C> 21 <C> (84.00%) <C> 264 <C> (154) <R> <C> [EMPTY] <C> School <C> 18 <C> 9 <C> (50.00%) <C> 553 <C> (62) <R> <C> [EMPTY] <C> Library <C> 13 <C> 13 <C> (100.00%) <C> 995 <C> (1328) <R> <C> [EMPTY] <C> Park <C> 11 <C> 9 <C> (81.82%) <C> 639 <C> (193) <R> <C> [EMPTY] <C> University <C> 7 <C> 6 <C> (85.71%) <C> 904 <C> (996) <R> <C> [EMPTY] <C> Others (10 types) <C> 27 <C> 16 <C> (59.25%) <C> 882 <C> (956) <R> <C> [BOLD] Group <C> [BOLD] Group <C> [BOLD] 240 <C> [BOLD] 152 <C> [BOLD] (63.33%) <C> [BOLD] 545 <C> [BOLD] (396) <R> <C> [EMPTY] <C> Company <C> 188 <C> 116 <C> (61.70%) <C> 500 <C> (359) <R> <C> [EMPTY] <C> SoccerClub <C> 26 <C> 13 <C> (50.00%) <C> 780 <C> (741) <R> <C> [EMPTY] <C> Organisation <C> 16 <C> 14 <C> (87.50%) <C> 706 <C> (416) <R> <C> [EMPTY] <C> PoliticalParty <C> 10 <C> 9 <C> (90.00%) <C> 552 <C> (471) <R> <C> [BOLD] Other <C> [BOLD] Other <C> [BOLD] 59 <C> [BOLD] 18 <C> [BOLD] (30.51%) <C> [BOLD] 758 <C> [BOLD] (977) <R> <C> [EMPTY] <C> Species <C> 53 <C> 14 <C> (26.42%) <C> 825 <C> (1008) <R> <C> [EMPTY] <C> CelestialBody <C> 3 <C> 1 <C> (33.33%) <C> 2 <C> (2) <R> <C> [EMPTY] <C> Train <C> 2 <C> 2 <C> (100.00%) <C> 241 <C> (241) <R> <C> [EMPTY] <C> Aircraft <C> 1 <C> 1 <C> (100.00%) <C> 1613 <C> (1613) <R> <C> Unmapped <C> Unmapped <C> 4891 <C> 3562 <C> (72.83%) <C> 690 <C> (615) <R> <C> [BOLD] Total <C> [BOLD] Total <C> 13406 <C> 10852 <C> (80.95%) <C> 571 <C> (406) <CAP> Table 4: Relative recall and time advantage over entity types of emerging entities detected with Proposed (LSTM-CRF)
<R> <C> [BOLD] English → German <C> [BOLD] English → German debiasing <C> [BOLD] English → German None <C> [BOLD] English → German None <C> [BOLD] English → German Localized Centering <C> [BOLD] English → German Localized Centering <C> [BOLD] English → German All-but-the-Top <C> [BOLD] English → German All-but-the-Top <R> <C> Model <C> embedding <C> BLEU <C> METEOR <C> BLEU <C> METEOR <C> BLEU <C> METEOR <R> <C> NMT <C> random <C> 34.57 <C> 54.50 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Doubly-attentive <C> random <C> 33.50 <C> 52.75 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> IMAGINATION <C> random <C> 34.97 <C> 54.21 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> VAG-NET <C> random <C> 35.55 <C> 54.87 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> NMT <C> word2vec <C> 34.23 <C> 52.83 <C> 34.14 <C> 53.09 <C> 33.88 <C> 52.66 <R> <C> [EMPTY] <C> GloVe <C> 35.49 <C> 55.14 <C> 35.33 <C> 54.89 <C> [BOLD] 35.98 <C> [BOLD] 55.15 <R> <C> [EMPTY] <C> FastText <C> 33.63 <C> 52.48 <C> 33.42 <C> 52.34 <C> 33.91 <C> 52.65 <R> <C> Doubly-attentive <C> word2vec <C> 32.05 <C> 50.85 <C> 32.07 <C> 51.23 <C> 32.73 <C> 51.04 <R> <C> [EMPTY] <C> GloVe <C> 34.06 <C> 53.74 <C> 33.37 <C> 52.98 <C> [BOLD] 34.77 <C> [BOLD] 53.86 <R> <C> [EMPTY] <C> FastText <C> 31.14 <C> 49.29 <C> 31.04 <C> 50.33 <C> 30.86 <C> 50.13 <R> <C> IMAGINATION <C> word2vec <C> 33.97 <C> 52.59 <C> 33.43 <C> 52.32 <C> 34.35 <C> 52.79 <R> <C> [EMPTY] <C> GloVe <C> 35.74 <C> 55.00 <C> 35.92 <C> 55.15 <C> [BOLD] 36.59 <C> [BOLD] 55.35 <R> <C> [EMPTY] <C> FastText <C> 34.21 <C> 52.53 <C> 33.69 <C> 52.22 <C> 33.83 <C> 52.31 <R> <C> VAG-NET <C> word2vec <C> 34.32 <C> 53.01 <C> 34.10 <C> 53.40 <C> 33.91 <C> 52.70 <R> <C> [EMPTY] <C> GloVe <C> 36.01 <C> [BOLD] 55.31 <C> 35.56 <C> 54.61 <C> [BOLD] 36.36 <C> 55.17 <R> <C> [EMPTY] <C> FastText <C> 34.12 <C> 52.56 <C> 33.92 <C> 52.75 <C> 33.82 <C> 52.38 <CAP> Table 3: Results obtained using Multi30K test2016 dataset for English–German translation. “NMT” shows the results of bahdanau2015jointly. When the debiasing is “None,” we show the results obtained with raw pretrained word embeddings or random values.
<R> <C> Sentence Level <C> arXiv pmr <C> arXiv wlcs-l <C> arXiv #pm <C> AAN pmr <C> AAN wlcs-l <C> AAN #pm <C> NIPS pmr <C> NIPS wlcs-l <C> NIPS #pm <C> SIND pmr <C> SIND wlcs-l <C> SIND #pm <R> <C> LSTM+PtrNet <C> 22.17 <C> 0.6002 <C> 11.0M <C> 35.65 <C> 0.6575 <C> 6.8M <C> 19.79 <C> 0.5839 <C> 3.2M <C> 13.61 <C> 0.5756 <C> 4.2M <R> <C> V-LSTM+PtrNet <C> 21.36 <C> 0.5934 <C> 66.9M <C> 35.62 <C> 0.6564 <C> 54.5M <C> 17.95 <C> 0.5620 <C> 43.6M <C> 13.13 <C> 0.5703 <C> 46.5M <R> <C> ATTOrderNet <C> 25.73 <C> 0.6122 <C> 40.9M <C> 40.00 <C> 0.6763 <C> 32.6M <C> [BOLD] 29.17 <C> 0.6200 <C> 13.4M <C> 13.57 <C> 0.5733 <C> 16.3M <R> <C> SE-Graph <C> [BOLD] 26.73 <C> [BOLD] 0.6251 <C> 12.4M <C> [BOLD] 45.97 <C> [BOLD] 0.6995 <C> 11.4M <C> 27.55 <C> [BOLD] 0.6282 <C> 4.5M <C> [BOLD] 15.07 <C> [BOLD] 0.5838 <C> 11.6M <CAP> Table 2: Main results of sequence ordering task at the paragraph and the sentence level, where #pm refers to the number of model parameters, V-LSTM+PtrNet stands for Variant-LSTM+PtrNet; For LSTM+PtrNet and Variant-LSTM+PtrNet, since there is no publicly available source code, we base our experiment on our own replication.
<R> <C> Paragraph Level <C> Press pmr <C> Press wlcs-l <C> Press #pm <C> Statements pmr <C> Statements wlcs-l <C> Statements #pm <C> Journal pmr <C> Journal wlcs-l <C> Journal #pm <C> Lyrics pmr <C> Lyrics wlcs-l <C> Lyrics #pm <R> <C> LSTM+PtrNet <C> 42.33 <C> 0.6543 <C> 10.0M <C> 43.12 <C> 0.6582 <C> 6.6M <C> 5.24 <C> 0.4549 <C> 11.2M <C> 0.26 <C> 0.1765 <C> 8.9M <R> <C> V-LSTM-PtrNet <C> 35.04 <C> 0.6304 <C> 64.1M <C> 33.01 <C> 0.6346 <C> 53.7M <C> 4.85 <C> 0.4424 <C> 67.6M <C> [BOLD] 0.66 <C> [BOLD] 0.1891 <C> 60.7M <R> <C> ATTOrderNet <C> 42.57 <C> 0.6555 <C> 55.4M <C> [BOLD] 45.79 <C> 0.6692 <C> 45.0M <C> 5.27 <C> 0.4595 <C> 41.6M <C> 0.49 <C> 0.1751 <C> 52.1M <R> <C> SE-Graph <C> [BOLD] 46.90 <C> [BOLD] 0.6749 <C> 13.3M <C> 42.66 <C> [BOLD] 0.6704 <C> 12.0M <C> [BOLD] 5.29 <C> [BOLD] 0.4618 <C> 20.02M <C> 0 <C> 0.1719 <C> 12.4M <CAP> Table 2: Main results of sequence ordering task at the paragraph and the sentence level, where #pm refers to the number of model parameters, V-LSTM+PtrNet stands for Variant-LSTM+PtrNet; For LSTM+PtrNet and Variant-LSTM+PtrNet, since there is no publicly available source code, we base our experiment on our own replication.
<R> <C> [BOLD] Parameters <C> [BOLD] (a) <C> [BOLD] (b) <C> [BOLD] (c) <R> <C> [ITALIC] λ( [ITALIC] μm) <C> 2 <C> 1 <C> 2 <R> <C> [ITALIC] σ0( [ITALIC] μm) <C> 0.3 <C> 0.3 <C> 1.5 <R> <C> [ITALIC] σ0/ [ITALIC] λ <C> 0.15 <C> 0.3 <C> 0.75 <R> <C> [ITALIC] τ( [ITALIC] μm) <C> 2 <C> 2 <C> 1.8 <R> <C> [ITALIC] nc <C> 3+21 [ITALIC] i <C> 3+10 [ITALIC] i <C> 3+21 [ITALIC] i <CAP> TABLE II: Input parameters for (a) slightly rough, (b) moderately rough and (c) very rough surfaces. θi=30∘, θd∈[0,π2] and ϕd∈[0,2π]
<R> <C> [BOLD] Model <C> [BOLD] Accuracy <R> <C> LR on LIWC features <C> 0.78 <R> <C> N-gram based Classification <C> 0.80 <R> <C> Attention-based BiLSTM <C> 0.81 <R> <C> Stacked Ensemble <C> 0.83 <CAP> Table 1: Classification accuracy on the test set.
<R> <C> Model <C> Public Score (%) <C> Private Score (%) <R> <C> RNN Model <C> 88.061 <C> 88.312 <R> <C> CNN Model <C> 88.330 <C> 88.773 <R> <C> Multi-Head Attention Model <C> 86.804 <C> 87.889 <R> <C> BERT Model <C> 96.050 <C> 95.617 <CAP> Table 4: Results of 30% of the test data
<R> <C> [BOLD] Training dataset <C> [BOLD] Encoder <C> [BOLD] Acc <R> <C> AL <C> BiGRU <C> 0.843 <R> <C> AL <C> BERT <C> 0.863 <R> <C> AL+CA+CO <C> BiGRU <C> [BOLD] 0.866 <R> <C> AL+CA+CO <C> BERT <C> 0.835 <R> <C> ACP <C> BiGRU <C> 0.919 <R> <C> ACP <C> BERT <C> [BOLD] 0.933 <R> <C> ACP+AL+CA+CO <C> BiGRU <C> 0.917 <R> <C> ACP+AL+CA+CO <C> BERT <C> 0.913 <R> <C> Random <C> Random <C> 0.500 <R> <C> Random+Seed <C> Random+Seed <C> 0.503 <CAP> Table 3: Performance of various models on the ACP test set.
<R> <C> [BOLD] Type Set <C> [BOLD] #types <C> [BOLD] depth <C> [BOLD] Time  [BOLD] w/ EL <C> [BOLD] Time  [BOLD] w/o EL <C> [BOLD] Time  [BOLD] Rel% <R> <C> Conll <C> 5 <C> 1 <C> [BOLD] 5 <C> 6 <C> 6.3% <R> <C> BBN <C> 47 <C> 2 <C> [BOLD] 8 <C> 17 <C> 52.9% <R> <C> FIGER <C> 112 <C> 2 <C> [BOLD] 10 <C> 30 <C> 66.7% <R> <C> YAGO <C> 7309 <C> 14 <C> [BOLD] 13 <C> - <C> - <CAP> Table 2: Time Consumption (minute) of Annotating 60 Sentences on Different Datasets
<R> <C> [BOLD] Corpus <C> [BOLD] Words <C> [BOLD] Sentences <C> [BOLD] W/S <R> <C> Law (Acquis) <C> 18,128,173 <C> 715,372 <C> 25.3 <R> <C> Medical (EMEA) <C> 14,301,472 <C> 1,104,752 <C> 12.9 <R> <C> IT <C> 3,041,677 <C> 337,817 <C> 9.0 <R> <C> Koran (Tanzil) <C> 9,848,539 <C> 480,421 <C> 20.5 <R> <C> Subtitles <C> 114,371,754 <C> 13,873,398 <C> 8.2 <CAP> Table 13.3: Corpora used to train domain-specific systems, taken from the OPUS repository. IT corpora are GNOME, KDE, PHP, Ubuntu, and OpenOffice.
<R> <C> Data Methods <C> UDC MAP <C> UDC Recall@5 <C> UDC Recall@1 <C> UDC Recall@2 <C> MSDialog MAP <C> MSDialog Recall@5 <C> MSDialog Recall@1 <C> MSDialog Recall@2 <C> AliMe MAP <C> AliMe Recall@5 <C> AliMe Recall@1 <C> AliMe Recall@2 <R> <C> BM25 <C> 0.6504 <C> 0.8206 <C> 0.5138 <C> 0.6439 <C> 0.4387 <C> 0.6329 <C> 0.2626 <C> 0.3933 <C> 0.6392 <C> 0.6407 <C> 0.2371 <C> 0.4204 <R> <C> BM25-PRF <C> 0.6620 <C> 0.8292 <C> 0.5289 <C> 0.6554 <C> 0.4419 <C> 0.6423 <C> 0.2652 <C> 0.3970 <C> 0.6412 <C> 0.6510 <C> 0.2454 <C> 0.4209 <R> <C> ARC-II <C> 0.6855 <C> 0.8978 <C> 0.5350 <C> 0.6959 <C> 0.5398 <C> 0.8662 <C> 0.3189 <C> 0.5413 <C> 0.7306 <C> 0.6595 <C> 0.2236 <C> 0.3671 <R> <C> MV-LSTM <C> 0.6611 <C> 0.8936 <C> 0.4973 <C> 0.6733 <C> 0.5059 <C> 0.8516 <C> 0.2768 <C> 0.5000 <C> 0.7734 <C> 0.7017 <C> 0.2480 <C> 0.4105 <R> <C> DRMM <C> 0.6749 <C> 0.8776 <C> 0.5287 <C> 0.6773 <C> 0.5704 <C> 0.9003 <C> 0.3507 <C> 0.5854 <C> 0.7165 <C> 0.6575 <C> 0.2212 <C> 0.3616 <R> <C> Duet <C> 0.5692 <C> 0.8272 <C> 0.4756 <C> 0.5592 <C> 0.5158 <C> 0.8481 <C> 0.2934 <C> 0.5046 <C> 0.7651 <C> 0.6870 <C> 0.2433 <C> 0.4088 <R> <C> SMN <C> 0.7327 <C> 0.9273 <C> 0.5948 <C> 0.7523 <C> 0.6188 <C> 0.8374 <C> 0.4529 <C> 0.6195 <C> 0.8145 <C> 0.7271 <C> 0.2881 <C> 0.4680 <R> <C> DMN <C> 0.7363 <C> 0.9196 <C> 0.6056 <C> 0.7509 <C> 0.6415 <C> 0.9155 <C> 0.4521 <C> 0.6673 <C> 0.7833 <C> 0.7629 <C> 0.3568 <C> 0.5012 <R> <C> DMN-KD <C> [BOLD] 0.7655‡ <C> [BOLD] 0.9351‡ <C> [BOLD] 0.6443‡ <C> [BOLD] 0.7841‡ <C> [BOLD] 0.6728‡ <C> [BOLD] 0.9304‡ <C> [BOLD] 0.4908‡ <C> [BOLD] 0.7089‡ <C> [BOLD] 0.8323 <C> [BOLD] 0.7631 <C> [BOLD] 0.3596‡ <C> [BOLD] 0.5122‡ <R> <C> DMN-PRF <C> [BOLD] 0.7719‡ <C> [BOLD] 0.9343‡ <C> [BOLD] 0.6552‡ <C> [BOLD] 0.7893‡ <C> [BOLD] 0.6792‡ <C> [BOLD] 0.9356‡ <C> [BOLD] 0.5021‡ <C> [BOLD] 0.7122‡ <C> [BOLD] 0.8435‡ <C> [BOLD] 0.7701 ‡ <C> [BOLD] 0.3601 ‡ <C> [BOLD] 0.5323 ‡ <CAP> Table 5. Comparison of different models over Ubuntu Dialog Corpus (UDC), MSDialog, and AliMe data sets. Numbers in bold font mean the result is better compared with the best baseline. ‡ means statistically significant difference over the best baseline with p<0.05 measured by the Student’s paired t-test.
<R> <C> Model <C> Data Change <C> UDC MAP <C> UDC Recall@5 <C> UDC Recall@1 <C> UDC Recall@2 <C> MSDialog MAP <C> MSDialog Recall@5 <C> MSDialog Recall@1 <C> MSDialog Recall@2 <R> <C> DMN-PRF <C> Only M1 <C> 0.7599 <C> 0.9294 <C> 0.6385 <C> 0.7761 <C> 0.5632 <C> 0.8509 <C> 0.3654 <C> 0.5579 <R> <C> DMN-PRF <C> Only M2 <C> 0.7253 <C> 0.9271 <C> 0.5836 <C> 0.7440 <C> 0.4996 <C> 0.8584 <C> 0.2595 <C> 0.5021 <R> <C> DMN-PRF <C> Inter-Dot (TB5) <C> [BOLD] 0.7719 <C> [BOLD] 0.9343 <C> [BOLD] 0.6552 <C> [BOLD] 0.7893 <C> [BOLD] 0.6792 <C> [BOLD] 0.9356 <C> [BOLD] 0.5021 <C> [BOLD] 0.7122 <R> <C> DMN-PRF <C> Inter-Cosine <C> 0.7507 <C> 0.9260 <C> 0.6248 <C> 0.7675 <C> 0.6729 <C> 0.9356 <C> 0.4944 <C> 0.7027 <R> <C> DMN-PRF <C> Inter-Bilinear <C> 0.7228 <C> 0.9199 <C> 0.5829 <C> 0.7401 <C> 0.4923 <C> 0.8421 <C> 0.2647 <C> 0.4744 <R> <C> DMN-KD <C> Only M1 <C> 0.7449 <C> 0.9247 <C> 0.6167 <C> 0.7612 <C> 0.5776 <C> 0.8673 <C> 0.3805 <C> 0.5779 <R> <C> DMN-KD <C> Only M2 <C> 0.7052 <C> 0.9203 <C> 0.5538 <C> 0.7260 <C> 0.5100 <C> 0.8613 <C> 0.2794 <C> 0.5011 <R> <C> DMN-KD <C> Only M3 <C> 0.3887 <C> 0.6017 <C> 0.2015 <C> 0.3268 <C> 0.3699 <C> 0.6650 <C> 0.1585 <C> 0.2957 <R> <C> DMN-KD <C> M1+M2 (DMN) <C> 0.7363 <C> 0.9196 <C> 0.6056 <C> 0.7509 <C> 0.6415 <C> 0.9155 <C> 0.4521 <C> 0.6673 <R> <C> DMN-KD <C> M1+M3 <C> 0.7442 <C> 0.9251 <C> 0.6149 <C> 0.7612 <C> 0.6134 <C> 0.8860 <C> 0.4224 <C> 0.6266 <R> <C> DMN-KD <C> M2+M3 <C> 0.7077 <C> 0.9198 <C> 0.5586 <C> 0.7263 <C> 0.5141 <C> 0.8659 <C> 0.2885 <C> 0.5069 <R> <C> DMN-KD <C> Inter-Dot (TB5) <C> [BOLD] 0.7655 <C> [BOLD] 0.9351 <C> [BOLD] 0.6443 <C> [BOLD] 0.7841 <C> 0.6728 <C> [BOLD] 0.9304 <C> 0.4908 <C> 0.7089 <R> <C> DMN-KD <C> Inter-Cosine <C> 0.7156 <C> 0.9121 <C> 0.5770 <C> 0.7268 <C> [BOLD] 0.6916 <C> 0.9249 <C> [BOLD] 0.5241 <C> [BOLD] 0.7249 <R> <C> DMN-KD <C> Inter-Bilinear <C> 0.7061 <C> 0.9135 <C> 0.5590 <C> 0.7225 <C> 0.4936 <C> 0.8224 <C> 0.2679 <C> 0.4814 <CAP> Table 6. Evaluation results of model ablation. “TB5” means the setting is the same with the results in Table 5. For DMN-KD, the model is the same with DMN if we remove M3. Numbers in bold font mean the result is better compared with other settings.
<R> <C> [BOLD] Corpus <C> [BOLD] # Triples <C> [BOLD] Avg # Ref <C> [BOLD] [Min,Max] # Ref <R> <C> Tuning <C> 2118 <C> 3.22 <C> [1, 10] <R> <C> Test <C> 2114 <C> 3.58 <C> [1, 10] <CAP> Table 1: Number of triples, average, minimum and maximum number of references for tuning and test corpora.
<R> <C> Model <C> ROUGE-1 <C> ROUGE-2 <C> ROUGE-L <R> <C> RNN (W) Hu et al. ( 2015 ) <C> 17.7 <C> 8.5 <C> 15.8 <R> <C> RNN (C) Hu et al. ( 2015 ) <C> 21.5 <C> 8.9 <C> 18.6 <R> <C> RNN context (W) Hu et al. ( 2015 ) <C> 26.8 <C> 16.1 <C> 24.1 <R> <C> RNN context (C) Hu et al. ( 2015 ) <C> 29.9 <C> 17.4 <C> 27.2 <R> <C> RNN context + SRB (C) <C> 32.1 <C> 18.9 <C> 29.2 <R> <C> +Attention (C) <C> [BOLD] 33.3 <C> [BOLD] 20.0 <C> [BOLD] 30.1 <CAP> Table 1: Results of our model and baseline systems. Our models achieve substantial improvement of all ROUGE scores over baseline systems. (W: Word level; C: Character level).
<R> <C> [BOLD] Model Approach <C> [BOLD] CoNLL <C> [BOLD] SemEval <C> [BOLD] ON 5.0 <R> <C> [EMPTY] <C> [BOLD] 2003 <C> [BOLD] 2010 <C> [BOLD] (BN) <R> <C> [ITALIC] Baselines <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> TlstmWord <C> 86.72 <C> 79.53 <C> 82.20 <R> <C> TlstmWord+POS <C> 88.91 <C> 84.07 <C> 88.41 <R> <C> TlstmWord+POS+Dep <C> 89.20 <C> [BOLD] 84.89 <C> [BOLD] 88.67 <R> <C> BlstmWord <C> 89.06 <C> 80.81 <C> 83.30 <R> <C> BlstmWord+POS <C> [BOLD] 91.22 <C> 80.31 <C> 84.58 <R> <C> BlstmWord+POS+Dep <C> 90.98 <C> 81.23 <C> 83.24 <R> <C> [ITALIC] Stacked Layers <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Tlstm <C> 91.22 <C> 85.01 <C> 88.08 <R> <C> Tlstm† <C> 91.24 <C> [BOLD] 85.75 <C> 88.71 <R> <C> Blstm <C> 91.68 <C> 84.32 <C> 88.48 <R> <C> Blstm† <C> [BOLD] 91.71 <C> 85.21 <C> [BOLD] 88.77 <R> <C> [ITALIC] Attention <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Tlstm + RA† <C> 91.13 <C> 84.94 <C> 88.11 <R> <C> Tlstm + GA† <C> 90.62 <C> 85.09 <C> [BOLD] 89.02 <R> <C> Tlstm + RA + GA† <C> 90.60 <C> [BOLD] 86.24 <C> 88.79 <R> <C> Blstm + RA† <C> [BOLD] 91.65 <C> 84.19 <C> 88.51 <R> <C> Blstm + GA† <C> 91.39 <C> 84.87 <C> 88.39 <R> <C> Blstm + RA + GA† <C> 91.63 <C> 85.12 <C> 88.66 <R> <C> Tlstm + Blstm <C> 91.17 <C> [BOLD] 86.49 <C> [BOLD] 89.22 <R> <C> + RA + GA† <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 2: The averaged F1 scores of two runs on the validation sets. Blstm and Tlstm are bidirectional and tree LSTMs. The baselines specify the input features; the rest of the experiments use all the features. † denotes residual connections on every component. RA and GA refer to relative and global attentions. All the experiments use ELMo embeddings and CRF.
<R> <C> [BOLD] Model <C> [BOLD] ja-en <C> [BOLD] de-en <C> [BOLD] or-en <R> <C> [BOLD] Model <C> [BOLD] ja-en <C> [BOLD] de-en <C> (mean±std) <R> <C> [ITALIC] seq2seq <C> 21.10 <C> 32.26 <C> 10.90±0.57 <R> <C> CCG <C> 22.44 <C> 32.84 <C> 12.55±0.60 <R> <C> CCG-null <C> 21.31 <C> [BOLD] 33.10 <C> 11.96±0.57 <R> <C> LIN <C> 21.55 <C> 31.79 <C> 12.66±0.61 <R> <C> TrDec-con <C> 21.59 <C> 31.93 <C> 11.43±0.58 <R> <C> TrDec-con-null <C> 22.72 <C> 31.21 <C> 11.35±0.55 <R> <C> TrDec-dep <C> 21.41 <C> 31.23 <C> 8.40±0.5 <R> <C> TrDec-binary <C> [BOLD] 23.14∗ <C> 32.65 <C> [BOLD] 13.10∗∗± [BOLD] 0.61 <CAP> Table 2: BLEU scores of TrDec and other baselines. Statistical significance is indicated with ∗ (p<0.05) and ∗∗ (p<0.001), compared with the best baseline.
<R> <C> Class <C> Model <C> F1 score <R> <C> P <C> lstm-crf{}_{0} <C> 0.78 <R> <C> [EMPTY] <C> lstm-crf-BERT{}_{0} <C> 0.68 <R> <C> [EMPTY] <C> QA-BERT{}_{20} <C> 0.87 <R> <C> I <C> lstm-crf-pattern{}_{1} <C> 0.7 <R> <C> [EMPTY] <C> lstm-crf-BERT{}_{0} <C> 0.57 <R> <C> [EMPTY] <C> QA-BERT{}_{20} <C> 0.67 <R> <C> [EMPTY] <C> QA-BERT{}_{200} <C> 0.75 <R> <C> O <C> lstm-crf-BERT{}_{0} <C> 0.78 <R> <C> [EMPTY] <C> QA-BERT{}_{100} <C> 0.67 <R> <C> PIO <C> SCIBERT mean <C> 0.72 <R> <C> [EMPTY] <C> QA-BERT mean <C> 0.75 <R> <C> P subclasses when annotated: <C> P subclasses when annotated: <C> P subclasses when annotated: <R> <C> Age, Gender, Condition, Size <C> Age, Gender, Condition, Size <C> Age, Gender, Condition, Size <R> <C> All <C> lstm-crf{}_{0} <C> 0.4 <R> <C> All <C> QA-BERT{}_{20} <C> 0.53 <R> <C> Performance in general SQuAD task <C> Performance in general SQuAD task <C> Performance in general SQuAD task <R> <C> All <C> QA-BERT{}_{200} <C> 0.72 <CAP> Table 4: Question Answering versus entity recognition results.
<R> <C> Class <C> Model <C> % Possible <C> F1 When Possible <C> F1 When Impossible <R> <C> P <C> QA-BERT{}_{20} <C> 30% <C> 0.74 <C> 0.92 <R> <C> I <C> QA-BERT{}_{200} <C> 53% <C> 0.60 <C> 0.94 <R> <C> 0 <C> QA-BERT{}_{100} <C> 60% <C> 0.52 <C> 0.92 <R> <C> P{}_{all} <C> QA-BERT{}_{20} <C> 10% <C> 0.53 <C> 0.97 <CAP> Table 5: Subgroups of possible sentences versus impossible sentences.
<R> <C> System <C> Reference <C> Precision <C> Recall <R> <C> JAMUL Article <C> Paper headline <C> 11.26 <C> 72.91 <R> <C> JAMUL Article <C> 10char-ref <C> 5.42 <C> 85.02 <R> <C> JAMUL Article <C> 13char-ref <C> 7.10 <C> 85.74 <R> <C> JAMUL Article <C> 26char-ref <C> 13.26 <C> 77.38 <R> <C> AEG Article <C> AEG headline <C> 17.74 <C> 62.03 <CAP> Table 2: ROUGE-1 precision and recall scores when comparing article and length-insensitive/sensitive headlines.
<R> <C> Dataset <C> #Sent <C> Before MST Tree <C> Before MST Proj <C> After MST Tree <C> After MST Proj <R> <C> PTB <C> 1,700 <C> 95.1 <C> 86.6 <C> 100.0 <C> 100.0 <R> <C> CTB <C> 803 <C> 87.0 <C> 73.1 <C> 100.0 <C> 100.0 <R> <C> Czech <C> 374 <C> 87.7 <C> 65.5 <C> 100.0 <C> 72.7 <R> <C> German <C> 367 <C> 96.7 <C> 67.3 <C> 100.0 <C> 68.1 <CAP> Table 6: Percentage of trees and projective trees on the development set before and after DeNSe uses a MST algorithm. On PTB and CTB, we use the Eisner algorithm and on Czech and German, we use the Chu-Liu-Edmonds algorithm.
<R> <C> [BOLD] De-duplication <C> [BOLD] No <C> [BOLD] Yes <R> <C> Accuracy <C> 0.919 <C> 0.914 <R> <C> Precision <C> 0.858 <C> 0.784 <R> <C> Recall <C> 0.860 <C> 0.798 <R> <C> F1-score <C> 0.859 <C> 0.790 <R> <C> Specificity <C> 0.942 <C> 0.943 <R> <C> AUROC <C> 0.966 <C> 0.954 <CAP> Table 1: Performance metrics of Huynh’s architecture using GloVe 840B embeddings with and without de-duplication of the ADR relevant sentences.
<R> <C> [BOLD] Architecture <C> [BOLD] Huynh <C> [BOLD] Hughes <R> <C> Accuracy <C> [BOLD] 0.918 <C> 0.905 <R> <C> Precision <C> [BOLD] 0.800 <C> 0.765 <R> <C> Recall <C> [BOLD] 0.797 <C> 0.771 <R> <C> F1-score <C> [BOLD] 0.798 <C> 0.767 <R> <C> Specificity <C> [BOLD] 0.949 <C> 0.939 <R> <C> AUROC <C> [BOLD] 0.958 <C> 0.940 <CAP> Table 3: Performance metrics of Huynh’s and Hughes’ architectures with de-duplication and Pyysalo’s embeddings.
<R> <C> [BOLD] Model <C> [BOLD] RACE-Mid <C> [BOLD] RACE-High <C> [BOLD] RACE-Full <R> <C> SA Reader <C> 44.2 <C> 43.0 <C> 43.3 <R> <C> GA Reader (GAR) <C> 43.7 <C> 44.2 <C> 44.1 <R> <C> [BOLD] ElimiNet <C> [BOLD] 44.4 <C> [BOLD] 44.5 <C> [BOLD] 44.5 <R> <C> GAR Ensemble <C> 45.7 <C> 46.2 <C> 45.9 <R> <C> ElimiNet Ensemble <C> [BOLD] 47.7 <C> 46.1 <C> 46.5 <R> <C> GAR + ElimiNet (ensemble of above 2 ensembles) <C> 47.4 <C> [BOLD] 47.4 <C> [BOLD] 47.2 <CAP> Table 1: Performance of individual and ensemble models
<R> <C> [BOLD] Pair # <C> [BOLD] F <C> [BOLD] A <C> [BOLD] INC <C> [BOLD] MI <C> [BOLD] AI <R> <C> 0 <C> 0.49 <C> 0.71 <C> 0.41 <C> 0.11 <C> 0.19 <R> <C> 1 <C> 0.83 <C> 0.85 <C> 0.13 <C> 0.30 <C> 0.09 <R> <C> 2 <C> 0.63 <C> 0.79 <C> 0.57 <C> 0.24 <C> 0.64 <R> <C> 3 <C> 0.25 <C> 0.51 <C> 0.46 <C> 0.47 <C> -0.02 <R> <C> 4 <C> 0.52 <C> 0.49 <C> 0.28 <C> 0.46 <C> 0.47 <R> <C> 5 <C> 0.16 <C> 0.54 <C> 0.12 <C> 0.36 <C> 0.47 <R> <C> 6 <C> 0.44 <C> 0.53 <C> 0.24 <C> 0.57 <C> 0.31 <R> <C> 7 <C> 0.41 <C> 0.54 <C> 0.48 <C> 0.67 <C> 0.37 <R> <C> 8 <C> 0.82 <C> 0.79 <C> 0.73 <C> 0.71 <C> 0.52 <R> <C> 9 <C> 0.60 <C> 0.74 <C> 0.30 <C> 0.52 <C> 0.44 <R> <C> [BOLD] AVG <C> [BOLD] 0.51 <C> [BOLD] 0.65 <C> [BOLD] 0.37 <C> [BOLD] 0.44 <C> [BOLD] 0.35 <CAP> Table 1: Inter-annotator agreement scores for each annotator pair, with averages in the final row. For numeric ratings of Fluency (F) and Adequacy (A), we use Spearman’s Rho; for binary categorical ratings of Incomprehensibility (INC), Missing Information (MI), and Added Information (AI), we use Cohen’s Kappa.
<R> <C> [BOLD] System Konstas <C> [BOLD] F↑  [BOLD] 78.14 <C> [BOLD] F↑ 1 <C> [BOLD] A↑  [BOLD] 81.46 <C> [BOLD] A↑ 1 <C> [BOLD] INC↓  [BOLD] 10.0 <C> [BOLD] MI↓ 34.5 <C> [BOLD] AI↓ 12.0 <R> <C> Zhu <C> 71.61 <C> 2 <C> 74.13 <C> 2 <C> 15.5 <C> 36.0 <C> 25.5 <R> <C> Ribeiro <C> 67.05 <C> 3 <C> 64.37 <C> 4 <C> 19.5 <C> 47.0 <C> 31.5 <R> <C> Guo <C> 62.13 <C> 4 <C> 68.52 <C> 3 <C> 22.0 <C> 41.0 <C> 21.5 <R> <C> Manning <C> 36.89 <C> 5 <C> 54.10 <C> 5 <C> 57.5 <C> [BOLD] 17.5 <C> [BOLD] 09.0 <R> <C> Reference <C> 87.56 <C> [EMPTY] <C> 93.68 <C> [EMPTY] <C> 05.0 <C> 04.5 <C> 10.0 <CAP> Table 2: For each system, average fluency and adequacy scores and percentage where each adequacy error type was selected. Scores for the reference sentences are included for comparison.
<R> <C> [BOLD] Model / Word Vectors <C> [ITALIC] ρ <R> <C> Neural MT Model  <C> 0.52 <R> <C> Symmetric Patterns  <C> 0.56 <R> <C> Non-distributional Vectors  <C> 0.58 <R> <C> GloVe vectors  <C> 0.41 <R> <C> GloVe vectors + Retrofitting <C> 0.53 <R> <C> GloVe + Counter-fitting <C> 0.58 <R> <C> Paragram-SL999  <C> 0.69 <R> <C> Paragram-SL999 + Retrofitting <C> 0.68 <R> <C> Paragram-SL999 + Counter-fitting <C> [BOLD] 0.74 <R> <C> Inter-annotator agreement <C> 0.67 <R> <C> Annotator/gold standard agreement <C> 0.78 <CAP> Table 2: Performance on SimLex-999. Retrofitting uses the code and (PPDB) data provided by the authors
<R> <C> [BOLD] Word Vector Space <C> [BOLD] Restaurants <C> [BOLD] Tourist Info <R> <C> Baseline (no dictionary) <C> 68.6 <C> 60.5 <R> <C> GloVe <C> 72.5 <C> 60.9 <R> <C> GloVe + Counter-fitting <C> 73.4 <C> [BOLD] 62.8 <R> <C> Paragram-SL999 <C> 73.2 <C> 61.5 <R> <C> Paragram-SL999 + Counter-fitting <C> [BOLD] 73.5 <C> 61.9 <CAP> Table 6: Performance of RNN belief trackers (ensembles of four models) with different semantic dictionaries
<R> <C> wait- k <C> En→De C <C> En→De I <C> De→En C <C> De→En I <C> En→Fr C <C> En→Fr I <C> Fr→En C <C> Fr→En I <C> En→Cs C <C> En→Cs I <C> Cs→En C <C> Cs→En I <R> <C> 1 <C> [BOLD] 20.41 <C> 14.83 <C> [BOLD] 13.86 <C> 9.60 <C> [BOLD] 19.84 <C> 13.02 <C> [BOLD] 13.46 <C> 9.34 <C> [BOLD] 9.97 <C> 6.32 <C> [BOLD] 15.39 <C> 12.35 <R> <C> 3 <C> [BOLD] 26.66 <C> 23.50 <C> [BOLD] 17.45 <C> 14.86 <C> [BOLD] 28.54 <C> 24.47 <C> [BOLD] 18.02 <C> 15.08 <C> [BOLD] 13.40 <C> 11.49 <C> [BOLD] 18.36 <C> 16.72 <R> <C> 5 <C> [BOLD] 31.30 <C> 29.01 <C> [BOLD] 20.55 <C> 18.84 <C> [BOLD] 35.92 <C> 32.30 <C> [BOLD] 21.29 <C> 18.84 <C> [BOLD] 16.31 <C> 14.93 <C> [BOLD] 23.22 <C> 21.62 <R> <C> 7 <C> [BOLD] 36.80 <C> 35.17 <C> [BOLD] 25.35 <C> 24.05 <C> [BOLD] 44.75 <C> 42.60 <C> [BOLD] 25.73 <C> 24.01 <C> [BOLD] 19.29 <C> 18.20 <C> [BOLD] 28.02 <C> 27.27 <R> <C> 9 <C> [BOLD] 42.34 <C> 41.39 <C> [BOLD] 29.04 <C> 28.30 <C> [BOLD] 53.62 <C> 52.03 <C> [BOLD] 30.29 <C> 29.29 <C> [BOLD] 22.34 <C> 21.66 <C> [BOLD] 30.96 <C> 30.24 <R> <C> Full <C> [BOLD] 53.92 <C> 53.43 <C> [BOLD] 35.59 <C> 35.37 <C> [BOLD] 72.00 <C> 71.74 <C> 42.27 <C> [BOLD] 42.29 <C> 28.79 <C> [BOLD] 28.80 <C> [BOLD] 35.39 <C> 34.91 <CAP> Table 4: Image Awareness results on test2016. METEOR scores of MSNMT Congruent (C) and Incongruent (I) settings for six translation directions. Results are the average of three runs. Bold indicates the best METEOR score for each wait-k for each translation direction.
<R> <C> [BOLD] Timex length <C> [BOLD] Frequency <R> <C> 1 <C> 654 <R> <C> 2 <C> 426 <R> <C> 3 <C> 226 <R> <C> 4 <C> 81 <R> <C> 5 <C> 22 <R> <C> 6 <C> 1 <R> <C> ≥7 <C> 0 <CAP> Table 2: Distribution of token-length of TIMEX3s in TimeBank.
<R> <C> [BOLD] Corpus  [BOLD] Train <C> [BOLD] Corpus  [BOLD] Test <C> [BOLD] TempEval-2  [BOLD] P <C> [BOLD] TempEval-2  [BOLD] R <C> [BOLD] TempEval-2  [BOLD] F1 <C> [BOLD] Entity-based  [BOLD] P <C> [BOLD] Entity-based  [BOLD] R <C> [BOLD] Entity-based  [BOLD] F1 <R> <C> TBAQ <C> T2T3 <C> 0.84 <C> 0.35 <C> 0.49 <C> 64.4% <C> 32.9% <C> 43.6% <R> <C> TBAQ + T2T3 <C> 80/20 split <C> 0.84 <C> 0.74 <C> 0.79 <C> 72.1% <C> 71.0% <C> 71.5% <R> <C> TBAQ train (80%) <C> TBAQ test (20%) <C> 0.93 <C> 0.80 <C> 0.86 <C> 85.2% <C> 69.8% <C> 76.7% <R> <C> T2T3 + TBAQ train <C> TBAQ test (20%) <C> 0.87 <C> 0.87 <C> 0.87 <C> 80.1% <C> 80.1% <C> 80.1% <CAP> Table 5: Timex recognition results. TBAQ corresponds to the merger of the TimeBank 1.2 and AQUAINT TimeML corpora.
<R> <C> Methods <C> AUC <C> MRR <C> nDCG@5 <C> nDCG@10 <R> <C> NGCF (Wang et al.,  2019c ) <C> 55.45±0.16 <C> 17.19±0.05 <C> 17.23±0.10 <C> 22.08±0.09 <R> <C> LibFM (Rendle,  2012 ) <C> 61.83±0.10 <C> 19.31±0.06 <C> 20.45±0.08 <C> 25.69±0.08 <R> <C> Wide&Deep (Cheng et al.,  2016 ) <C> 64.62±0.14 <C> 20.71±0.12 <C> 22.43±0.15 <C> 27.99±0.15 <R> <C> DFM (Lian et al.,  2018 ) <C> 64.72±0.19 <C> 20.75±0.14 <C> 22.60±0.20 <C> 28.22±0.19 <R> <C> DSSM (Huang et al.,  2013 ) <C> 65.49±0.18 <C> 20.93±0.13 <C> 22.93±0.22 <C> 28.65±0.27 <R> <C> DAN (Zhu et al.,  2019 ) <C> 65.52±0.13 <C> 21.25±0.18 <C> 23.14±0.21 <C> 28.73±0.15 <R> <C> GRU (Okura et al.,  2017 ) <C> 65.69±0.19 <C> 21.29±0.10 <C> 23.16±0.11 <C> 28.75±0.11 <R> <C> DKN (Wang et al.,  2018 ) <C> 65.88±0.13 <C> 21.46±0.21 <C> 23.23±0.25 <C> 28.84±0.21 <R> <C> GERL-Graph <C> 67.74±0.13 <C> 22.71±0.15 <C> 25.03±0.13 <C> 30.65±0.15 <R> <C> [BOLD] GERL <C> [BOLD] 68.55±0.12 <C> [BOLD] 23.33±0.10 <C> [BOLD] 25.82±0.14 <C> [BOLD] 31.44±0.12 <CAP> Table 2. The performance scores and standard variations of different methods. *The improvement is significant at the level p < 0.002.
<R> <C> [BOLD] Model <C> [BOLD] 28k <C> [BOLD] 59k <C> [BOLD] 120k <R> <C> LSTM(a) <C> 57.9 <C> 62.5 <C> 65.9 <R> <C> CNN(b) <C> 58.7 <C> 62.7 <C> 65.6 <R> <C> LSTM-AE(a) <C> 59.9 <C> 64.6 <C> 68.5 <R> <C> LSTM-ADAE(a) <C> 62.5 <C> 66.8 <C> 70.9 <R> <C> DeConv-AE(b) <C> 62.1 <C> 65.5 <C> 68.7 <R> <C> LSTM-VAE(b) <C> 64.7 <C> 67.5 <C> 71.1 <R> <C> DeConv-VAE(b) <C> 67.2 <C> 69.3 <C> 72.2 <R> <C> LSTM-vMF-VAE (ours) <C> 65.6 <C> 68.7 <C> 71.1 <R> <C> CS-LVM (ours) <C> 68.4 <C> 73.5 <C> 76.9 <R> <C> +R [ITALIC] y <C> [BOLD] 70.0 <C> [BOLD] 74.5 <C> 77.4 <R> <C> +R [BOLD] z <C> 69.2 <C> 73.9 <C> 77.6 <R> <C> +R [ITALIC] μ <C> 69.1 <C> 74.0 <C> [BOLD] 77.6 <R> <C> +R [ITALIC] y,R [BOLD] z,R [ITALIC] μ <C> 69.6 <C> 74.1 <C> 77.4 <CAP> Table 1: Semi-supervised classification results on the SNLI dataset. (a) Zhao et al. (2018); (b) Shen et al. (2018a).
<R> <C> [BOLD] Model <C> [BOLD] 1k <C> [BOLD] 5k <C> [BOLD] 10k <C> [BOLD] 25k <R> <C> CNN(a) <C> 56.3 <C> 59.2 <C> 63.8 <C> 68.9 <R> <C> LSTM-AE(a) <C> 59.3 <C> 63.8 <C> 67.2 <C> 70.9 <R> <C> DeConv-AE(a) <C> 60.2 <C> 65.1 <C> 67.7 <C> 71.6 <R> <C> LSTM-VAE(a) <C> 62.9 <C> 67.6 <C> 69.0 <C> 72.4 <R> <C> DeConv-VAE(a) <C> 65.1 <C> 69.4 <C> 70.5 <C> 73.7 <R> <C> LSTM-vMF-VAE (ours) <C> 65.0 <C> 69.9 <C> 72.1 <C> 74.9 <R> <C> CS-LVM (ours) <C> [BOLD] 66.5 <C> 71.1 <C> 74.6 <C> 76.9 <R> <C> +R [ITALIC] y <C> 66.4 <C> 70.8 <C> 74.5 <C> 77.5 <R> <C> +R [BOLD] z <C> [BOLD] 66.5 <C> [BOLD] 71.3 <C> 74.8 <C> 77.1 <R> <C> +R [ITALIC] μ <C> 66.4 <C> 71.2 <C> [BOLD] 74.9 <C> 77.4 <R> <C> +R [ITALIC] y,R [BOLD] z,R [ITALIC] μ <C> 66.3 <C> [BOLD] 71.3 <C> 74.7 <C> [BOLD] 77.6 <CAP> Table 2: Semi-supervised classification results on the Quora Question Pairs dataset. (a) Shen et al. (2018a).
<R> <C> [BOLD] Model <C> [BOLD] 28k <C> [BOLD] 59k <C> [BOLD] 120k <R> <C> CS-LVM <C> 68.4 <C> 73.5 <C> 76.9 <R> <C> [ITALIC] (i) without CS <C> 65.6 <C> 68.7 <C> 71.1 <R> <C> [ITALIC] (ii) Gaussian <C> 66.9 <C> 72.0 <C> 74.9 <R> <C> [ITALIC] (iii) sampling <C> 68.0 <C> 72.9 <C> 76.5 <R> <C> [ITALIC] (iv)  [ITALIC] fenc≠ [ITALIC] genc <C> 63.3 <C> 69.1 <C> 74.7 <CAP> Table 3: Ablation study results.
<R> <C> Model <C> Company <C> Company <C> Film <C> Film <C> Animal <C> Animal <R> <C> [EMPTY] <C> QA <C> Rank <C> QA <C> Rank <C> QA <C> Rank <R> <C> TF-S2S <C> 5 <C> 1.87 <C> 6 <C> 2.27 <C> 9 <C> 1.87 <R> <C> CV-S2S <C> 5 <C> 2.27 <C> 6.67 <C> 1.76 <C> 8.33 <C> 2.04 <R> <C> CV-S2D [ITALIC] +T <C> 7 <C> 1.87 <C> 7 <C> 1.98 <C> 9.33 <C> 2.09 <CAP> Table 5: QA-based evaluation and system ranking.
<R> <C> Method <C> [EMPTY] <C> DEV <C> TEST <R> <C> 0/1 loss <C> normal <C> 82.6 <C> 82.8 <R> <C> 0/1 loss <C> weighted <C> 83.9 <C> 81.3 <R> <C> relation-level <C> normal <C> 83.9 <C> 83.1 <R> <C> relation-level <C> weighted <C> 84.6 <C> 81.1 <R> <C> mention-level <C> normal <C> 82.4 <C> 83.9 <R> <C> mention-level <C> weighted <C> [BOLD] 85.4 <C> [BOLD] 86.0 <CAP> Table 10: AUC of sentential evaluation precision / recall curves for PcnnNmar with three loss functions trained on NytFb-68k. Mention-level hamming loss has some advantages over other two loss functions.
<R> <C> [BOLD] Span Rep.  [ITALIC] hb: [ITALIC] d <C> [BOLD] Partial <C> [BOLD] Complete <R> <C> Sum Pooling ∑ [ITALIC] dk= [ITALIC] bhk <C> 92.80 <C> 85.56 <R> <C> StartEnd Concat[ [ITALIC] hb; [ITALIC] hd] <C> 91.94 <C> 84.56 <R> <C> [ [ITALIC] hb; [ITALIC] hd,^ [ITALIC] eb: [ITALIC] d, [ITALIC] ϕ( [ITALIC] d− [ITALIC] b)] <C> 91.11 <C> 84.33 <CAP> Table 1: AndroidHowTo phrase tuple extraction test results using different span representations hb:d in (8). ^eb:d=∑dk=bw(hk)e(tk), where w(⋅) is a learned weight function for each token embedding Lee et al. (2017). See the pseudocode for fast computation of these in the appendix.
<R> <C> [BOLD] Screen Encoder <C> [BOLD] Partial <C> [BOLD] Complete <R> <C> Heuristic <C> 62.44 <C> 42.25 <R> <C> Filter-1 GCN <C> 76.44 <C> 52.41 <R> <C> Distance GCN <C> 82.50 <C> 59.36 <R> <C> Transformer <C> 89.21 <C> 70.59 <CAP> Table 2: PixelHelp grounding accuracy. The differences are statistically significant based on t-test over 5 runs (p<0.05).
<R> <C> [BOLD] Model <C> [BOLD] MNLI All <C> [BOLD] MNLI Msp <C> [BOLD] MNLI WN <C> [BOLD] AG’s News All <C> [BOLD] AG’s News Msp <C> [BOLD] AG’s News WN <C> [BOLD] DBPedia All <C> [BOLD] DBPedia Msp <C> [BOLD] DBPedia WN <R> <C> BERT (base) <C> 50.5 <C> 49.1 <C> 53.4 <C> 56.5 <C> 54.8 <C> 61.9 <C> 49.3 <C> 46.0 <C> 57.6 <R> <C> + Mimick (Pinter et al.,  2017 ) <C> 37.2 <C> 38.2 <C> 38.7 <C> 45.3 <C> 43.9 <C> 50.5 <C> 36.5 <C> 35.8 <C> 41.1 <R> <C> + A La Carte (Khodak et al.,  2018 ) <C> 44.6 <C> 45.7 <C> 46.1 <C> 52.4 <C> 53.7 <C> 56.1 <C> 51.1 <C> 48.7 <C> 59.3 <R> <C> + AM (Schick and Schütze,  2020 ) <C> 50.9 <C> 50.7 <C> 53.6 <C> 58.9 <C> 59.8 <C> 62.6 <C> 60.7 <C> 63.1 <C> 62.8 <R> <C> + Bertram <C> 53.3 <C> 52.5 <C> 55.6 <C> [BOLD] 62.1 <C> [BOLD] 63.1 <C> [BOLD] 65.3 <C> 64.2 <C> 67.9 <C> 64.1 <R> <C> + Bertram-slash <C> 56.4 <C> 55.3 <C> 58.6 <C> [BOLD] 62.9 <C> [BOLD] 63.3 <C> [BOLD] 65.3 <C> 65.7 <C> 67.3 <C> 67.2 <R> <C> + Bertram-slash + indomain <C> [BOLD] 59.8 <C> [BOLD] 57.3 <C> [BOLD] 62.7 <C> [BOLD] 62.5 <C> [BOLD] 62.1 <C> [BOLD] 66.6 <C> [BOLD] 74.2 <C> [BOLD] 74.8 <C> [BOLD] 76.7 <R> <C> 3.0pt1-12.51.5 <C> 3.0pt1-12.51.5 <C> 3.0pt1-12.51.5 <C> 3.0pt1-12.51.5 <C> 3.0pt1-12.51.5 <C> 3.0pt1-12.51.5 <C> 3.0pt1-12.51.5 <C> 3.0pt1-12.51.5 <C> 3.0pt1-12.51.5 <C> 3.0pt1-12.51.5 <R> <C> plus1fil minus1fil <C> plus1fil minus1fil <C> plus1fil minus1fil <C> plus1fil minus1fil <C> plus1fil minus1fil <C> plus1fil minus1fil <C> plus1fil minus1fil <C> plus1fil minus1fil <C> plus1fil minus1fil <C> plus1fil minus1fil <R> <C> RoBERTa (large) <C> 67.3 <C> 68.7 <C> 68.4 <C> 63.7 <C> 68.1 <C> 65.7 <C> 65.5 <C> 67.3 <C> 66.6 <R> <C> + Bertram-slash <C> 70.1 <C> [BOLD] 71.5 <C> 70.9 <C> 64.6 <C> 68.4 <C> 64.9 <C> 71.9 <C> 73.8 <C> 73.9 <R> <C> + Bertram-slash + indomain <C> [BOLD] 71.7 <C> [BOLD] 71.9 <C> [BOLD] 73.2 <C> [BOLD] 68.1 <C> [BOLD] 71.9 <C> [BOLD] 69.0 <C> [BOLD] 76.0 <C> [BOLD] 78.8 <C> [BOLD] 77.3 <CAP> Table 3: Accuracy of standalone BERT and RoBERTa, various baselines and Bertram on rarified MNLI, AG’s News and DBPedia. The five Bertram instances are Bertram-add. Best results per baseline model are underlined, results that do not differ significantly from the best results in a two-sided binomial test (p<0.05) are bold. Msp/WN: subset of instances containing at least one misspelling/synonym. All: all instances.
<R> <C> Representations <C> Accuracy <C> 95% CI <R> <C> Random <C> 62.00 <C> [61.28,62.53] <R> <C> External GloVe <C> 72.19 <C> − <R> <C> IMDB GloVE <C> 76.38 <C> [75.76,76.72] <R> <C> Mittens <C> 77.39 <C> [77.23,77.50] <CAP> Table 2: IMDB test-set classification results. A difference of 1% corresponds to 250 examples. For all but ‘External GloVE’, we report means (with bootstrapped confidence intervals) over five runs of creating the embeddings and cross-validating the classifier’s hyperparameters, mainly to help verify that the differences do not derive from variation in the representation learning phase.
<R> <C> [BOLD] Training <C> [BOLD] Testing <C> [BOLD] Accuracy(%) <R> <C> FakeNewsAMT <C> Celebrity <C> 54.3 <R> <C> Celebrity <C> FakeNewsAMT <C> 68.5 <CAP> Table 3: Results Obtained in Cross-Domain Analysis Experiments on the Best Performing System.
<R> <C> [BOLD] Domain <C> [BOLD] Exp. a  [BOLD] Model1 <C> [BOLD] Exp. a  [BOLD] Model2 <C> [BOLD] Exp. b  [BOLD] Model1 <C> [BOLD] Exp. b  [BOLD] Model2 <R> <C> Business <C> 74.75 <C> 78.75 <C> 63.56 <C> 68.56 <R> <C> Education <C> 77.25 <C> 91.25 <C> 65.65 <C> 70.65 <R> <C> Technology <C> 76.22 <C> 88.75 <C> 64.3 <C> 65.35 <R> <C> Politics <C> 73.75 <C> 88.75 <C> 64.27 <C> 69.22 <R> <C> Entertainment <C> 68.25 <C> 76.25 <C> 65.89 <C> 71.2 <R> <C> Sports <C> 70.75 <C> 73.75 <C> 67.86 <C> 71.45 <CAP> Table 4: Result of Exp. a (Trained on Multi-domain Data and Tested on Domain wise Data) and Exp. b (Trained on Domain wise Data and Tested on Domain wise Data)
<R> <C> [EMPTY] <C> Eng-Eng <C> Eng-Man <C> Man-Eng <C> Man-Man <R> <C> RNNLM <C> 133.18 <C> 157.18 <C> 2617.28 <C> 34.98 <R> <C> D-RNNLM <C> 140.37 <C> 151.38 <C> 2452.16 <C> 32.89 <R> <C> Mono RNNLM <C> 101.61 <C> 181.28 <C> 2510.48 <C> 30.00 <R> <C> Mono D-RNNLM <C> 101.66 <C> 156.44 <C> 2442.81 <C> 29.64 <R> <C> RNNLM SeqGAN <C> 120.28 <C> 154.44 <C> 2739.85 <C> 30.40 <R> <C> D-RNNLM SeqGAN <C> 120.26 <C> 149.68 <C> 2450.85 <C> 30.60 <CAP> Table 3: Decomposed perplexities on the development set on all four types of tokens from various models.
<R> <C> [EMPTY] <C> [BOLD] SST <C> [BOLD] CoLA <C> [BOLD] MNLI <C> [BOLD] MRPC <C> [BOLD] QNLI <C> [BOLD] STS-B <C> [BOLD] POS <C> [BOLD] CHUNK <C> [BOLD] DP <R> <C> AC <C> 89.11 <C> 70.44 <C> 72.22 <C> 80.15 <C> 73.57 <C> 72.29 <C> 94.15 <C> 95.7 <C> 96.61 <R> <C> BC <C> 86.93 <C> 69.81 <C> 70.09 <C> 73.4 <C> 71.03 <C> 69.92 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> AB <C> 89.39 <C> 70.15 <C> 72.45 <C> 79.29 <C> 73.21 <C> 72.525 <C> 94.17 <C> 95.67 <C> 96.64 <R> <C> ABC <C> 89.68 <C> 69.96 <C> 73.23 <C> 78.92 <C> 73.41 <C> 73.76 <C> 94.13 <C> 95.72 <C> 96.63 <R> <C> ACD <C> 88.95 <C> 69.83 <C> 71.97 <C> 80.39 <C> 73.4 <C> 68.32 <C> 94.14 <C> 95.67 <C> 96.6 <R> <C> ACE <C> 88.88 <C> 69.96 <C> 72.44 <C> 79.66 <C> 72.47 <C> 71.85 <C> 94.13 <C> 95.66 <C> 96.61 <R> <C> ABCD <C> 88.76 <C> 70.15 <C> 73 <C> 78.68 <C> 74.91 <C> 67.92 <C> 94.16 <C> 95.61 <C> 96.64 <R> <C> ABCE <C> 89.45 <C> 69.96 <C> 72.85 <C> 76.96 <C> 73.46 <C> 73.91 <C> 94.1 <C> 95.64 <C> 96.61 <R> <C> ACDE <C> 88.84 <C> 69.87 <C> 71.77 <C> 80.23 <C> 72.98 <C> 66.37 <C> 94.18 <C> 95.69 <C> 96.63 <R> <C> ABCDE <C> 88.42 <C> 69.48 <C> 72.43 <C> 80.15 <C> 74.48 <C> 69.18 <C> 94.12 <C> 95.66 <C> 96.65 <CAP> Table 3: Experiment result. The value in the table is the difference between disassembled model and ensemble model (ABCDE or ENS).
<R> <C> [BOLD] Systems <C> [BOLD] Model <C> [BOLD] Classifier <C> [BOLD] Accuracy (%)  [BOLD] Manual transcriptions <C> [BOLD] CE  [BOLD] Manual transcriptions <C> [BOLD] Accuracy (%)  [BOLD] Automatic transcriptions <C> [BOLD] CE  [BOLD] Automatic transcriptions <R> <C> Prior works <C> BoW [Hazen:2007:ASRU] <C> NB <C> 87.61 <C> - <C> - <C> - <R> <C> Prior works <C> TF-IDF [May:2015:mivec] <C> LR <C> 86.41 <C> - <C> - <C> - <R> <C> Our Baseline <C> TF-IDF <C> LR <C> 86.59 <C> 0.93 <C> 86.77 <C> [BOLD] 0.94 <R> <C> Our Baseline <C> ULMFiT ⋆ <C> MLP <C> 86.41 <C> [BOLD] 0.50 <C> 86.08 <C> [BOLD] 0.50 <R> <C> Our Baseline <C> ℓ1 SMM <C> LR <C> 86.81 <C> 0.91 <C> 87.02 <C> 1.09 <R> <C> Our Baseline <C> ℓ1 SMM <C> GLC <C> 85.17 <C> 1.64 <C> 85.53 <C> 1.54 <R> <C> Our Baseline <C> NVDM <C> LR <C> 81.16 <C> 0.94 <C> 83.67 <C> 1.15 <R> <C> Our Baseline <C> NVDM <C> GLC <C> 84.47 <C> 1.25 <C> 84.15 <C> 1.22 <R> <C> Our Baseline <C> NVDM <C> GLCU <C> 83.96 <C> 0.93 <C> 83.01 <C> 0.97 <R> <C> Proposed <C> Bayesian SMM <C> LR <C> [BOLD] 89.91 <C> [BOLD] 0.89 <C> [BOLD] 88.23 <C> 0.95 <R> <C> Proposed <C> Bayesian SMM <C> GLC <C> [BOLD] 89.47 <C> 1.05 <C> [BOLD] 87.23 <C> 1.46 <R> <C> Proposed <C> Bayesian SMM <C> GLCU <C> [BOLD] 89.54 <C> [BOLD] 0.68 <C> [BOLD] 87.54 <C> [BOLD] 0.77 <CAP> TABLE III: Comparison of results on Fisher test sets, from earlier published works, our baselines and proposed systems. ⋆ indicates a pure discriminative model.
<R> <C> [EMPTY] <C> Task without model highlights <C> Task with model highlights <R> <C> Average work time (sec) <C> 224.89 <C> 178.34 <R> <C> Inter-annotator agreement ( [ITALIC] κ) <C> 0.1571 <C> 0.2526 <CAP> Table 7: Annotation speed and inter-annotator agreement measured for factual consistency checking with and without assisting, model generated highlights.
<R> <C> Model <C> F1 <R> <C> Bag of Words (BoW) <C> 0.601 <R> <C> Bag of Embeddings (BoE) <C> 0.605 <R> <C> P-Emb <C> 0.668 <R> <C> P-Sent <C> 0.671 <R> <C> P-LM <C> [BOLD] 0.675 <R> <C> P-Emb + bidir. <C> 0.684 <R> <C> P-Sent + bidir. <C> 0.674 <R> <C> P-LM + SGU <C> 0.679 <R> <C> P-LM + SGU + Concat. <C> 0.682 <R> <C> Ensembling (UA) P-Emb + P-Sent <C> 0.684 <R> <C> Ensembling (UA) P-Sent + P-LM <C> 0.695 <R> <C> Ensembling (UA) P-Emb + P-LM <C> 0.701 <R> <C> Ensembling (MV) All <C> 0.700 <R> <C> Ensembling (UA) All <C> [BOLD] 0.702 <CAP> Table 5: Results of our experiments when tested on the evaluation (dev) set. BoW and BoE are our baselines, while P-Emb, P-Sent and P-LM our proposed TL approaches. SGU stands for Simplified Gradual Unfreezing, bidir. for bi-LSTM, Concat. for the concatenation method, UA for Unweighted Average and MV for Majority Voting ensembling.
<R> <C> Fine-tuning Task <C> Metric <C> Score <R> <C> MLM <C> Perplexity <C> 1.9 <R> <C> SPRC <C> Accuracy <C> 87.5 <R> <C> MLM-SPRC <C> Accuracy <C> 89.8 <CAP> Table 4. Fine-tuning task scores of Invoice Dataset
<R> <C> Model <C> F1 <R> <C> RoBERTa+GCN (full) <C> 71.31 <R> <C> RoBERTa+GCN w/o section title edges <C> 70.55 <R> <C> RoBERTa+GCN w/o fonts feats <C> 70.81 <R> <C> RoBERTa+GCN w/o section title edges & fonts feats <C> 69.99 <R> <C> RoBERTa+GCN w/o skip connections <C> 70.87 <CAP> Table 8. Ablation study on graph module
<R> <C> Classification Result <C> Precision <C> Recall <C> F-measure <R> <C> isPreg <C> 0.83 <C> 0.79 <C> 0.81 <R> <C> notPreg <C> 0.84 <C> 0.77 <C> 0.80 <CAP> Table 2: Results from tweet classification for legitimate pregnancy announcements using SVM
<R> <C> [BOLD] Metric <C> [BOLD] A2 <C> [BOLD] F1 <R> <C> [BOLD] Text + Audio <C> [EMPTY] <C> [EMPTY] <R> <C> BC-LSTM <C> 79.30 <C> - <R> <C> MMMU-BA <C> [BOLD] 80.58 <C> - <R> <C> DialogueRNN <C> 78.81 <C> [BOLD] 79.12 <R> <C> Multilogue-net <C> 80.12 <C> 78.84 <R> <C> [BOLD] Video + Audio <C> [EMPTY] <C> [EMPTY] <R> <C> BC-LSTM <C> 62.10 <C> - <R> <C> MMMU-BA <C> 65.16 <C> - <R> <C> DialogueRNN <C> 63.22 <C> 60.14 <R> <C> Multilogue-net <C> [BOLD] 69.55 <C> [BOLD] 63.40 <R> <C> [BOLD] Text + Video <C> [EMPTY] <C> [EMPTY] <R> <C> BC-LSTM <C> 80.20 <C> - <R> <C> MMMU-BA <C> [BOLD] 81.51 <C> - <R> <C> DialogueRNN <C> 79.88 <C> 79.10 <R> <C> Multilogue-net <C> 80.66 <C> [BOLD] 79.62 <R> <C> [BOLD] Text + Audio + Video <C> [EMPTY] <C> [EMPTY] <R> <C> BC-LSTM <C> 80.30 <C> - <R> <C> MMMU-BA <C> [BOLD] 82.31 <C> - <R> <C> DialogueRNN <C> 79.80 <C> 79.48 <R> <C> Multilogue-net <C> 81.19 <C> [BOLD] 80.10 <CAP> Table 1: Multilogue-Net performance on CMU-MOSI in comparison with the current and previous state-of-the-art on the dataset. A2 indicating accuracy with 2 classes, and F1 indicating F1 score .
<R> <C> [BOLD] Technique <C> [BOLD] Single Receipt <C> [BOLD] All Receipts <R> <C> Baseline <C> 0.62 <C> 0.47 <R> <C> Wildcard <C> 0.84 <C> 0.72 <R> <C> Mashed <C> 0.88 <C> 0.76 <R> <C> Ngrams <C> 0.89 <C> 0.77 <R> <C> Fuzzy Ngrams <C> 0.93 <C> 0.79 <CAP> Table 1: Receipt Item Linking Results.
<R> <C> Disease <C> CSU N <C> CSU Prec <C> CSU Rec <C> CSU  [ITALIC] F1 <C> CSU Accu <C> CSU Sub <C> PP (Cross-hospital) N <C> PP (Cross-hospital) Prec <C> PP (Cross-hospital) Rec <C> PP (Cross-hospital)  [ITALIC] F1 <C> PP (Cross-hospital) Accu <C> PP (Cross-hospital) Sub <R> <C> Autoimmune disease <C> 1280 <C> 94.0 <C> 72.3 <C> 81.4 <C> 99.6 <C> 11 <C> 1 <C> 60.0 <C> 25.0 <C> 34.7 <C> 99.6 <C> 1(1) <R> <C> Congenital disease <C> 3345 <C> 72.9 <C> 35.9 <C> 47.3 <C> 97.8 <C> 224 <C> 17 <C> 58.0 <C> 5.3 <C> 9.2 <C> 97.1 <C> 8(6) <R> <C> Propensity to adverse reactions <C> 5105 <C> 89.1 <C> 70.2 <C> 78.1 <C> 98.2 <C> 8 <C> 43 <C> 83.6 <C> 13.2 <C> 21.3 <C> 93.0 <C> 7(2) <R> <C> Metabolic disease <C> 5265 <C> 68.9 <C> 55.4 <C> 61.0 <C> 96.9 <C> 82 <C> 26 <C> 75.8 <C> 51.1 <C> 59.8 <C> 96.5 <C> 12(9) <R> <C> Disorder of auditory system <C> 5393 <C> 81.0 <C> 66.2 <C> 72.8 <C> 97.7 <C> 67 <C> 64 <C> 76.8 <C> 69.4 <C> 72.4 <C> 95.0 <C> 12(6) <R> <C> Hypersensitivity condition <C> 6871 <C> 85.7 <C> 74.6 <C> 79.5 <C> 97.7 <C> 31 <C> 50 <C> 72.7 <C> 20.6 <C> 30.5 <C> 92.1 <C> 11(4) <R> <C> Disorder of endocrine system <C> 7009 <C> 79.2 <C> 66.7 <C> 72.2 <C> 96.9 <C> 84 <C> 46 <C> 71.5 <C> 28.2 <C> 40.1 <C> 92.6 <C> 8(8) <R> <C> Disorder of hematopoietic cell proliferation <C> 7294 <C> 95.1 <C> 87.4 <C> 91.0 <C> 98.9 <C> 22 <C> 16 <C> 68.6 <C> 31.0 <C> 41.1 <C> 97.5 <C> 6(1) <R> <C> Disorder of nervous system <C> 7488 <C> 76.1 <C> 63.8 <C> 69.2 <C> 96.4 <C> 243 <C> 27 <C> 68.0 <C> 31.7 <C> 41.6 <C> 94.9 <C> 19(14) <R> <C> Disorder of cardiovascular system <C> 8733 <C> 79.3 <C> 62.5 <C> 69.7 <C> 95.7 <C> 351 <C> 53 <C> 48.1 <C> 54.7 <C> 49.9 <C> 89.8 <C> 30(24) <R> <C> Disorder of the genitourinary system <C> 8892 <C> 77.7 <C> 62.6 <C> 69.3 <C> 95.7 <C> 317 <C> 44 <C> 59.9 <C> 40.3 <C> 47.2 <C> 92.3 <C> 19(12) <R> <C> Traumatic AND/OR non-traumatic injury <C> 9027 <C> 72.8 <C> 57.2 <C> 63.5 <C> 94.8 <C> 536 <C> 19 <C> 43.8 <C> 12.0 <C> 18.3 <C> 96.2 <C> 13(8) <R> <C> Visual system disorder <C> 10139 <C> 84.3 <C> 81.1 <C> 82.6 <C> 96.9 <C> 413 <C> 62 <C> 68.6 <C> 63.9 <C> 65.7 <C> 93.2 <C> 39(34) <R> <C> Infectious disease <C> 11304 <C> 71.2 <C> 53.7 <C> 60.8 <C> 92.9 <C> 260 <C> 88 <C> 56.3 <C> 21.9 <C> 30.2 <C> 86.7 <C> 20(10) <R> <C> Disorder of respiratory system <C> 11322 <C> 79.5 <C> 65.5 <C> 71.8 <C> 95.2 <C> 274 <C> 27 <C> 40.2 <C> 41.3 <C> 38.4 <C> 94.0 <C> 16(14) <R> <C> Disorder of connective tissue <C> 17477 <C> 75.4 <C> 67.0 <C> 70.7 <C> 91.3 <C> 567 <C> 24 <C> 38.4 <C> 31.2 <C> 33.8 <C> 94.3 <C> 15(11) <R> <C> Disorder of musculoskeletal system <C> 20060 <C> 77.0 <C> 73.4 <C> 74.8 <C> 91.1 <C> 670 <C> 56 <C> 66.4 <C> 45.4 <C> 53.2 <C> 91.5 <C> 31(19) <R> <C> Disorder of integument <C> 21052 <C> 84.2 <C> 71.6 <C> 77.3 <C> 92.3 <C> 360 <C> 156 <C> 55.7 <C> 58.5 <C> 56.8 <C> 80.6 <C> 58(32) <R> <C> Disorder of digestive system <C> 22589 <C> 76.8 <C> 67.1 <C> 71.5 <C> 89.7 <C> 694 <C> 195 <C> 54.7 <C> 48.4 <C> 50.2 <C> 72.7 <C> 47(36) <R> <C> Neoplasm and/or hamartoma <C> 36108 <C> 92.2 <C> 88.9 <C> 90.5 <C> 93.9 <C> 749 <C> 59 <C> 27.5 <C> 77.3 <C> 40.0 <C> 76.2 <C> 18(7) <CAP> Table 1: Report of DeepTag performance on CSU test data and PP data
<R> <C> Model <C> EM <C> Precision unwgt <C> Precision wgt <C> Recall unwgt <C> Recall wgt <C> [ITALIC] F1 unwgt <C> [ITALIC] F1 wgt <R> <C> [EMPTY] <C> CSU data <C> CSU data <C> CSU data <C> CSU data <C> CSU data <C> CSU data <C> CSU data <R> <C> LSTM <C> 47.4 <C> 76.6 <C> 85.9 <C> 59.3 <C> 78.7 <C> 65.3 <C> 81.7 <R> <C> BLSTM <C> 48.2 <C> 76.1 <C> 86.0 <C> 57.6 <C> 79.4 <C> 63.5 <C> 82.2 <R> <C> DeepTag-M <C> 48.6 <C> 76.8 <C> 86.3 <C> 58.7 <C> 79.6 <C> 64.6 <C> 82.4 <R> <C> [BOLD] DeepTag <C> [BOLD] 48.4 <C> [BOLD] 79.9 <C> [BOLD] 86.1 <C> [BOLD] 62.1 <C> [BOLD] 79.8 <C> [BOLD] 68.0 <C> [BOLD] 82.4 <R> <C> [EMPTY] <C> PP data <C> PP data <C> PP data <C> PP data <C> PP data <C> PP data <C> PP data <R> <C> LSTM <C> 13.8 <C> 48.1 <C> 65.7 <C> 31.8 <C> 51.9 <C> 33.8 <C> 54.4 <R> <C> BLSTM <C> 13.8 <C> 47.3 <C> 66.0 <C> 35.6 <C> 57.9 <C> 36.9 <C> 58.4 <R> <C> DeepTag-M <C> 17.1 <C> 53.4 <C> 68.0 <C> 37.9 <C> 59.9 <C> 40.6 <C> 61.1 <R> <C> [BOLD] DeepTag <C> [BOLD] 17.4 <C> [BOLD] 56.5 <C> [BOLD] 70.3 <C> [BOLD] 41.4 <C> [BOLD] 62.4 <C> [BOLD] 43.2 <C> [BOLD] 63.4 <CAP> Table 2: Evaluation of trained classifiers on the CSU and PP data
<R> <C> [BOLD]  No <C> [BOLD]  Model <C> [BOLD]  F1-score±std <R> <C> 1 <C> w/o Ψ [ITALIC] ij(^ [BOLD] x [ITALIC] i) in Eq  7 <C> 91.15±0.12 <R> <C> 2 <C> add position encoding <C> 91.05±0.19 <R> <C> 3 <C> Our model <C> 91.33±0.08 <CAP> TABLE V: Experimental results of various position modeling strategies applied to self-attention.
<R> <C> [EMPTY] <C> [EMPTY] <C> BLEU TAUS <C> BLEU Reco <C> chrF TAUS <C> chrF Reco <R> <C> 1 <C> Baseline <C> 50.28 <C> 51.02 <C> 71.47 <C> 72.25 <R> <C> 2 <C> 1 + EMEA <C> 50.43 <C> 52.72 <C> 71.47 <C> 73.25 <R> <C> 3 <C> 1 + SEC <C> 47.44 <C> 51.67 <C> 69.66 <C> 72.51 <R> <C> 4 <C> 2 + ParaWiki <C> 50.96 <C> [BOLD] 54.97 <C> 71.85 <C> 75.35 <R> <C> 5 <C> 3 + 4 <C> 50.56 <C> [BOLD] 57.77 <C> 71.78 <C> 76.29 <R> <C> 6 <C> 3 + 4* <C> 49.54 <C> 57.10 <C> 71.13 <C> 76.30 <R> <C> 7 <C> Ensemble <C> 50.60 <C> [BOLD] 58.16 <C> 71.74 <C> 76.91 <CAP> Table 6: The Italian-to-English NMT systems. SEC: Sketch Engine Corpus.
<R> <C> [EMPTY] <C> CommonGEN MLE <C> CommonGEN TextGAIL <C> ROCStories MLE <C> ROCStories TextGAIL <R> <C> BLEU-2 ↑ <C> 0.1596 <C> [BOLD] 0.1623 <C> 0.0737 <C> [BOLD] 0.0740 <R> <C> Distinct-2 ↑ <C> 0.0935 <C> [BOLD] 0.0950 <C> 0.4235 <C> [BOLD] 0.4520 <R> <C> Seq-Rep-2 ↓ <C> 0.6311 <C> [BOLD] 0.6216 <C> 0.7987 <C> [BOLD] 0.7763 <CAP> Table 1: Generation results with beam search.
<R> <C> [EMPTY] <C> Perplexity ↓ <C> BLEU-2 ↑ <C> Distinct-2 ↑ <R> <C> TextGAIL <C> 14.85 <C> 0.1623 <C> 0.0950 <R> <C> w/o mix human demos <C> 1,657.12 <C> [EMPTY] <C> [EMPTY] <R> <C> w/ REINFORCE <C> 17.17 <C> 0.1370 <C> 0.0915 <R> <C> w/ D no pre-train <C> 16.94 <C> 0.1514 <C> 0.0914 <CAP> Table 5: Ablation studies results on CommenGEN. "w/ D no pre-train" means randomly initialized discriminator
<R> <C> [BOLD] Experiment <C> [BOLD] Perplexity <C> [BOLD] BLEU <R> <C> Original Words Event → Original Sentence <C> 1585.46 <C> 0.0016 <R> <C> Generalized Event → Generalized Sentence <C> 56.516 <C> 0.0331 <R> <C> All Generalized Events → Gen. Sentence <C> 59.106 <C> 0.0366 <R> <C> Original Words Event → S+P Sentence <C> 490.010 <C> [BOLD] 0.0764 <R> <C> Generalized Event → S+P Gen. Sentence <C> [BOLD] 53.964 <C> 0.0266 <R> <C> All Generalized Events → S+P Gen. Sent. <C> 56.488 <C> 0.0283 <CAP> Table 2: Results from the event-to-sentence experiments.
<R> <C> [EMPTY] <C> Model <C> Exact QbE (%)  [ITALIC] P@10 <C> Exact QbE (%)  [ITALIC] P@ [ITALIC] N <C> Exact QbE (%) EER <C> Semantic QbE (%)  [ITALIC] P@10 <C> Semantic QbE (%)  [ITALIC] P@ [ITALIC] N <C> Semantic QbE (%) EER <C> Semantic QbE (%) Spearman’s  [ITALIC] ρ <C> Run-time (min) <R> <C> [ITALIC] Baselines: <C> Random <C> 04.5 <C> 04.5 <C> 50 <C> 09.5 <C> 09.1 <C> 50 <C> 05.9 <C> - <R> <C> [EMPTY] <C> DTW <C> 54.6 <C> 24.9 <C> 32.1 <C> 44.3 <C> 24.3 <C> 38.7 <C> 13.7 <C> 4080 <R> <C> [ITALIC] Our systems: <C> FastGrounded <C> 27.5 <C> 17.9 <C> 38.9 <C> 32.6 <C> 23.2 <C> 41.4 <C> 12.8 <C> 0< 1 <R> <C> [EMPTY] <C> DenseGrounded <C> 56.0 <C> 37.3 <C> 21.7 <C> 55.5 <C> 37.3 <C> 30.0 <C> 14.9 <C> 0621 <R> <C> [ITALIC] Supervised: <C> FastSupervised <C> 60.7 <C> 41.3 <C> 27.2 <C> 56.6 <C> 30.9 <C> 39.8 <C> 08.5 <C> 0< 1 <R> <C> [EMPTY] <C> DenseSupervised <C> 72.0 <C> 55.7 <C> 12.0 <C> 71.2 <C> 46.4 <C> 27.4 <C> 13.5 <C> 0568 <CAP> Table 1: Exact and semantic QbE performance on test data. DTW performs full alignment. Grounded systems use acoustic embeddings trained only using visual supervision, while Supervised systems are trained on text labels. Fast systems represent search utterances as single embeddings, while Dense systems embed overlapping segments within search utterances.
<R> <C> Embedding Space <C> Model <C> WordSim353 <C> MEN <C> SimLex999 <R> <C> Euclidean <C> Word2Vec <C> 0.711 <C> 0.726 <C> 0.311 <R> <C> Euclidean <C> GloVe <C> 0.598 <C> 0.690 <C> 0.321 <R> <C> Euclidean <C> fastText <C> 0.697 <C> 0.722 <C> 0.303 <R> <C> Euclidean <C> BERT <C> 0.477 <C> 0.594 <C> 0.287 <R> <C> Poincaré <C> Poincaré GloVe <C> 0.623 <C> 0.652 <C> 0.321 <R> <C> Spherical <C> [BOLD] JoSE <C> [BOLD] 0.739 <C> [BOLD] 0.748 <C> [BOLD] 0.339 <CAP> Table 1: Spearman rank correlation on word similarity evaluation.
<R> <C> Embedding <C> 20 Newsgroup Macro-F1 <C> 20 Newsgroup Micro-F1 <C> Movie Review Macro-F1 <C> Movie Review Micro-F1 <R> <C> Avg. W2V <C> 0.630 <C> 0.631 <C> 0.712 <C> 0.713 <R> <C> SIF <C> 0.552 <C> 0.549 <C> 0.650 <C> 0.656 <R> <C> BERT <C> 0.380 <C> 0.371 <C> 0.664 <C> 0.665 <R> <C> Doc2Vec <C> 0.648 <C> 0.645 <C> 0.674 <C> 0.678 <R> <C> [BOLD] JoSE <C> [BOLD] 0.703 <C> [BOLD] 0.707 <C> [BOLD] 0.764 <C> [BOLD] 0.765 <CAP> Table 3: Document classification evaluation using k-NN (k=3).
<R> <C> [EMPTY] <C> [BOLD] IF Precision <C> [BOLD] IF Recall <C> [BOLD] IF F1 score <C> [BOLD] THEN Precision <C> [BOLD] THEN Recall <C> [BOLD] THEN F1 score <C> [BOLD] Avg F1 score <R> <C> [BOLD] User Only <C> 0.94 <C> 0.85 <C> 0.89 <C> 0.98 <C> 0.99 <C> 0.98 <C> 0.94 <R> <C> [BOLD] Crowd Only <C> 0.94 <C> 0.77 <C> 0.85 <C> 0.97 <C> 0.90 <C> 0.94 <C> 0.89 <R> <C> [BOLD] Crowd+User <C> 0.94 <C> 0.83 <C> 0.89 <C> 1.00 <C> 0.94 <C> 0.97 <C> 0.93 <R> <C> [BOLD] Crowd Voting <C> 0.92 <C> 0.89 <C> 0.91 <C> 0.95 <C> 0.96 <C> 0.96 <C> 0.93 <CAP> Table 3: Sensor/Effector selection overall performance. Both “Crowd+User” and “Crowd Voting” settings achieved comparable performances to that of the “Crowd Only” setting is both IF and THEN parts.
<R> <C> [BOLD] Parameter <C> [BOLD] Parameter Name <C> [BOLD] Value <R> <C> [ITALIC] dw <C> Word Emb. size <C> 400 <R> <C> [ITALIC] dwpe <C> Word Pos. Emb. size <C> 70 <R> <C> [ITALIC] dc <C> Convolutinal Units <C> 1000 <R> <C> [ITALIC] k <C> Context Window size <C> 3 <R> <C> [ITALIC] λ <C> Initial Learning Rate <C> 0.025 <CAP> Table 1: CR-CNN Hyperparameters
<R> <C> [BOLD] Use embedding <C> [BOLD] Class <C> [BOLD] Prec. <C> [BOLD] Rec. <C> [BOLD] F1 <R> <C> [BOLD] of class  [ITALIC] Other <C> [BOLD] Class <C> [BOLD] Prec. <C> [BOLD] Rec. <C> [BOLD] F1 <R> <C> No <C> All <C> [BOLD] 83.7 <C> [BOLD] 84.7 <C> [BOLD] 84.1 <R> <C> Yes <C> All <C> 81.3 <C> 84.3 <C> 82.7 <R> <C> No <C> Other <C> 52.0 <C> 48.7 <C> 50.3 <R> <C> Yes <C> Other <C> 60.1 <C> 48.7 <C> 53.8 <CAP> Table 3: Impact of not using an embedding for the artificial class Other.
<R> <C> [BOLD] Classifier <C> [BOLD] Feature Set <C> [BOLD] F1 <R> <C> SVM <C> POS, prefixes, morphological, WordNet, dependency parse, <C> 82.2 <R> <C>  <C> Levin classes, ProBank, FrameNet, NomLex-Plus, <C> 82.2 <R> <C> [EMPTY] <C> Google n-gram, paraphrases, TextRunner <C> 82.2 <R> <C> RNN <C> word embeddings <C> 74.8 <R> <C>  <C> word embeddings, POS, NER, WordNet <C> 77.6 <R> <C> MVRNN <C> word embeddings <C> 79.1 <R> <C>  <C> word embeddings, POS, NER, WordNet <C> 82.4 <R> <C> [EMPTY] <C> word embeddings <C> 69.7 <R> <C> CNN+Softmax <C> word embeddings, word position embeddings, <C> 82.7 <R> <C>  <C> word pair, words around word pair, WordNet <C> 82.7 <R> <C> FCM <C> word embeddings <C> 80.6 <R> <C>  <C> word embeddings, dependency parse, NER <C> 83.0 <R> <C> CR-CNN <C> word embeddings <C> 82.8 <R> <C> CR-CNN <C> word embeddings, word position embeddings <C> [BOLD] 84.1 <CAP> Table 5: Comparison with results published in the literature.
<R> <C> Solver <C> Dev <C> Test <R> <C> Human solver <C> 89.3* <C> 91.7* <R> <C> Guess All (“random”) <C> 25.0 <C> 25.0 <R> <C> No Training, KB Only (§ 4.1 ) <C> No Training, KB Only (§ 4.1 ) <C> No Training, KB Only (§ 4.1 ) <R> <C> TupleInference <C> 15.9 <C> 17.9 <R> <C> PMI (Waterloo corpus) <C> 19.7 <C> 21.2 <R> <C> TableILP <C> 20.0 <C> 23.4 <R> <C> DGEM <C> 27.4 <C> [BOLD] 24.4 <R> <C> No Training, KB + F (§ 4.2 ) <C> No Training, KB + F (§ 4.2 ) <C> No Training, KB + F (§ 4.2 ) <R> <C> IR with F <C> 25.5 <C> 24.8 <R> <C> TupleInference with F <C> 23.6 <C> [BOLD] 26.6 <R> <C> DGEM with F <C> 28.2 <C> 24.6 <R> <C> Trained Models, No F or KB (§ 4.3 ) <C> Trained Models, No F or KB (§ 4.3 ) <C> Trained Models, No F or KB (§ 4.3 ) <R> <C> Embedd+Sim <C> 44.6 <C> 41.8 <R> <C> ESIM <C> 53.9±0.4 <C> 48.9±1.1 <R> <C> Plausible Answer Detector <C> 54.4±0.7 <C> 49.6±0.7 <R> <C> Odd-one-out Solver <C> 56.9±0.5 <C> 50.2±1.6 <R> <C> Question Match <C> 54.6±1.2 <C> [BOLD] 50.2±0.9 <R> <C> Oracle Models, F and/or KB (§ 4.4 ) <C> Oracle Models, F and/or KB (§ 4.4 ) <C> Oracle Models, F and/or KB (§ 4.4 ) <R> <C> [ITALIC] f <C> 63.0±2.3 <C> 55.8±2.3 <R> <C> [ITALIC] f + WordNet <C> 57.6±1.4 <C> 56.3±1.3 <R> <C> [ITALIC] f + ConceptNet <C> 57.0±1.6 <C> 53.7±1.5 <R> <C> [ITALIC] f +  [ITALIC] k <C> 80.2±1.1 <C> 76.9±0.7 <CAP> Table 4: Scores obtained by various solvers on OpenBookQA, reported as a percentage ± the standard deviation across 5 runs with different random seeds. Other baselines are described in the corresponding referenced section. For oracle evaluation, we use the gold science fact f associated with each question, and optionally the additional fact k provided by the question author. Bold denotes the best Test score in each category.
<R> <C> Language Pairs <C> F1-BAD <C> F1-OK <C> F1-Multi <C> Rank <R> <C> En-De (SMT) <C> 0.5075 <C> 0.8394 <C> 0.4260 <C> 3 <R> <C> En-De (NMT) <C> 0.3565 <C> 0.8827 <C> 0.3147 <C> 2 <R> <C> De-En <C> 0.4906 <C> 0.8640 <C> 0.4239 <C> 2 <R> <C> En-Lv (SMT) <C> 0.4211 <C> 0.8592 <C> 0.3618 <C> 1 <R> <C> En-Lv (NMT) <C> 0.5192 <C> 0.8268 <C> 0.4293 <C> 1 <R> <C> En-Cz <C> 0.5882 <C> 0.8061 <C> 0.4741 <C> 1 <CAP> Table 3: Best performance of our model on six datasets in the WMT2018 word-level QE shared task on the leader board (updated on July 27th 2018)
<R> <C> Language Pairs <C> Method <C> F1-BAD <C> F1-OK <C> F1-Multi <R> <C> De-En <C> - (Convolution + POS + features) <C> 0.4774 <C> 0.8680 <C> 0.4144 <R> <C> De-En <C> - (POS + features) <C> 0.4948 <C> 0.8474 <C> 0.4193 <R> <C> De-En <C> - features <C> 0.5095 <C> [BOLD] 0.8735 <C> 0.4450 <R> <C> De-En <C> - POS <C> 0.4906 <C> 0.8640 <C> 0.4239 <R> <C> De-En <C> CEQE <C> [BOLD] 0.5233 <C> 0.8721 <C> [BOLD] 0.4564 <R> <C> En-Cz <C> - (Convolution + POS + features) <C> 0.5748 <C> 0.7622 <C> 0.4381 <R> <C> En-Cz <C> - (POS + features) <C> 0.5628 <C> 0.8000 <C> 0.4502 <R> <C> En-Cz <C> - features <C> 0.5777 <C> 0.7997 <C> 0.4620 <R> <C> En-Cz <C> - POS <C> 0.5192 <C> [BOLD] 0.8268 <C> 0.4293 <R> <C> En-Cz <C> CEQE <C> [BOLD] 0.5884 <C> 0.7991 <C> [BOLD] 0.4702 <R> <C> En-De (SMT) <C> - (Convolution + POS + features) <C> 0.4677 <C> 0.8038 <C> 0.3759 <R> <C> En-De (SMT) <C> - (POS + features) <C> 0.4768 <C> 0.8166 <C> 0.3894 <R> <C> En-De (SMT) <C> - features <C> 0.4902 <C> 0.8230 <C> 0.4034 <R> <C> En-De (SMT) <C> - POS <C> 0.5047 <C> [BOLD] 0.8431 <C> 0.4255 <R> <C> En-De (SMT) <C> CEQE <C> [BOLD] 0.5075 <C> 0.8394 <C> [BOLD] 0.4260 <R> <C> En-De (NMT) <C> - (Convolution + POS + features) <C> 0.3545 <C> 0.8396 <C> 0.2976 <R> <C> En-De (NMT) <C> - (POS + features) <C> 0.3404 <C> 0.8752 <C> 0.2979 <R> <C> En-De (NMT) <C> - features <C> [BOLD] 0.3565 <C> 0.8827 <C> [BOLD] 0.3147 <R> <C> En-De (NMT) <C> - POS <C> 0.3476 <C> [BOLD] 0.8948 <C> 0.3111 <R> <C> En-De (NMT) <C> CEQE <C> 0.3481 <C> 0.8835 <C> 0.3075 <CAP> Table 4: Ablation study on the WMT18 Test Set
<R> <C> Model <C> # of embeddings <C> Text Unit <R> <C> google_news1 [ITALIC] gram <C> 3,000,000 <C> uni-grams <R> <C> clef_inex1 [ITALIC] gram <C> 30,000 <C> uni-grams <R> <C> clef_inex2 [ITALIC] gram <C> 600,000 <C> bi-grams <R> <C> wiki2 [ITALIC] gram <C> 10,000,000 <C> bi-grams <CAP> Table 2: Word2vec models
<R> <C> [BOLD] Dataset <C> [BOLD] W2V <C> [BOLD] GEM <C> [BOLD] DMTK(AVG) <C> [BOLD] GW2V(GC) <R> <C> [BOLD] 1-billion <C> 4.24 <C> 4.39 <C> 4.21 <C> 3.98 <R> <C> [BOLD] news <C> 4.45 <C> 4.66 <C> 4.28 <C> 4.51 <R> <C> [BOLD] wiki <C> 20.49 <C> OOM <C> 25.43 <C> 22.34 <CAP> Table 2. Word2Vec training time (hours) on a single host.
<R> <C> Parameter Name <C> Symbol <C> Value <R> <C> Window size <C> [ITALIC] dwin <C> 3 <R> <C> Sentence. emb. dim. <C> [ITALIC] df <C> 690 <R> <C> Word. emb. dim. <C> [ITALIC] d1 <C> 50 <R> <C> Position. emb. dim. <C> [ITALIC] d2 <C> 5 <R> <C> Batch size <C> B <C> 160 <R> <C> Learning rate <C> [ITALIC] λ <C> 0.03 <R> <C> Dropout pos. <C> [ITALIC] p <C> 0.5 <CAP> Table 3: Hyper-parameter settings.
<R> <C> Model <C> [ITALIC] T <C> [ITALIC] TC <C> [ITALIC] TN <R> <C> Baseline ASR <C> 25.1 <C> 27.9 <C> 33.5 <R> <C> Encoder Init <C> 22.1 <C> 25.1 <C> [BOLD] 30.6 <R> <C> Encoder + Decoder Init <C> 22.3 <C> 25.6 <C> 30.9 <R> <C> Early Decoder Fusion <C> 22.0 <C> 25.1 <C> 30.7 <R> <C> Hierarchical Feature Attention <C> [BOLD] 20.9 <C> [BOLD] 24.4 <C> [BOLD] 30.6 <CAP> Table 1: Word Error Rate (WER, in %) results on the PlacesAudio dataset. T, TC, TN are the unmasked, color-masked and noun-masked variants of the test set.
<R> <C> Data set <C> Baseline <C> Ours <C> [ITALIC] p <R> <C> ATIS <C> 0.977 <C> 0.974 <C> 0.1755 <R> <C> Movie <C> 0.816 <C> 0.817 <C> 0.3792 <R> <C> Restaurant <C> [BOLD] 0.724 <C> 0.694 <C> 0.0001 <CAP> Table 3: Micro average F1 scores on the E2E data sets. Results that are significantly better (p<0.05) are highlighted in bold.
<R> <C> [BOLD] FMS <C> [BOLD] O(m) <C> [BOLD] O(n) <C> [BOLD] M(m) <C> [BOLD] M(n) <R> <C> [0.0,0.2) <C> 148 <C> 2,404 <C> 99 <C> 265 <R> <C> [0.2,0.3) <C> 476 <C> 1,739 <C> 380 <C> 190 <R> <C> [0.3,0.4) <C> 1,007 <C> 1,370 <C> 893 <C> 225 <R> <C> [0.4,0.5) <C> 1,251 <C> 1,227 <C> 1,146 <C> 205 <R> <C> [0.5,0.6) <C> 1,559 <C> 885 <C> 1,410 <C> 228 <R> <C> [0.6,0.7) <C> 2,029 <C> 740 <C> 1,888 <C> 210 <R> <C> [0.7,0.8) <C> 2,154 <C> 536 <C> 1,987 <C> 155 <R> <C> [0.8,0.9) <C> 2,340 <C> 352 <C> 2,210 <C> 116 <R> <C> [0.9,1.0) <C> 2,424 <C> 100 <C> 2,294 <C> 33 <R> <C> (0.0,1.0) <C> 13,388 <C> 9,353 <C> 12,307 <C> 1,627 <CAP> Table 4: The numbers of matched and unmatched noisy words in example translations. O: original matched example translations. M: noise-masked example translations. n: noisy words. m: matched words.
<R> <C> [BOLD] Lang. Pair <C> [BOLD] # Vocab. <C> [BOLD] Dev <C> [BOLD] Test <R> <C> kor \rightarrow jje <C> 2k <C> 44.80 <C> 43.26 <R> <C> [EMPTY] <C> [BOLD] 4k <C> [BOLD] 44.85 <C> [BOLD] 43.31 <R> <C> [EMPTY] <C> 8k <C> 44.40 <C> 43.03 <R> <C> [EMPTY] <C> 16k <C> 43.33 <C> 42.08 <R> <C> [EMPTY] <C> 32k <C> 42.57 <C> 41.07 <R> <C> jje \rightarrow kor <C> 2k <C> 69.05 <C> 67.63 <R> <C> [EMPTY] <C> [BOLD] 4k <C> [BOLD] 69.35 <C> [BOLD] 67.70 <R> <C> [EMPTY] <C> 8k <C> 69.02 <C> 67.46 <R> <C> [EMPTY] <C> 16k <C> 67.61 <C> 66.30 <R> <C> [EMPTY] <C> 32k <C> 66.32 <C> 65.08 <CAP> Table 3: BLEU scores of models according to the different BPE vocabulary size. SentencePiece is used for BPE segmentation. All hyperparameters except the vocabulary size are identical.
<R> <C> [BOLD] Lang. Pair <C> [BOLD] Model <C> [BOLD] Dev <C> [BOLD] Test <R> <C> kor \rightarrow jje <C> Copy <C> 24.06 <C> 24.44 <R> <C> [EMPTY] <C> JIT <C> 44.85 <C> 43.31 <R> <C> [EMPTY] <C> JIT + KorWiki <C> [BOLD] 45.25 <C> [BOLD] 44.19 <R> <C> jje \rightarrow kor <C> Copy <C> 24.07 <C> 24.45 <R> <C> [EMPTY] <C> JIT <C> 69.35 <C> 67.70 <R> <C> [EMPTY] <C> JIT + KorWiki <C> [BOLD] 69.59 <C> [BOLD] 67.94 <CAP> Table 4: BLEU scores of various translation models. In the Copy model, translation outputs are copied from the source. For the JIT + KorWiki model, 160,356 Korean sentences extracted from a Wikidump are added to both source and target sides of the JIT dataset. The vocabulary size is fixed to 4k.
<R> <C> [EMPTY] <C> [BOLD] ASER EC <C> [BOLD] ASER NS <C> [BOLD] Atomic EC <C> [BOLD] Atomic NS <R> <C> COMeT <C> 0.6388 <C> 0.5869 <C> 0.6927 <C> 0.5730 <R> <C> KG-BERT <C> 0.7091 <C> [BOLD] 0.8018 <C> 0.7669 <C> 0.6575 <R> <C> CCC-50 <C> 0.8716 <C> 0.7775 <C> 0.9016 <C> [BOLD] 0.7840 <R> <C> CCC-75 <C> 0.8995 <C> 0.7250 <C> 0.9221 <C> 0.7446 <R> <C> CCC-87.5 <C> [BOLD] 0.9156 <C> 0.6635 <C> [BOLD] 0.9355 <C> 0.6980 <R> <C> CCC-75-scratch <C> 0.8284 <C> 0.5587 <C> 0.8579 <C> 0.5003 <R> <C> CCC-75-RoBERTa <C> 0.8999 <C> 0.6938 <C> 0.9305 <C> 0.7350 <CAP> Table 2: Results of accuracy on EC and NS test set of baselines and our models on different datasets. CCC denotes our model, with the number attached representing percentage of EC training.
<R> <C> System <C> Attention Type <C> Domain Speaker <C> Domain Environment <R> <C> MC <C> - <C> 19.23 <C> 19.23 <R> <C> ADIT <C> - <C> 18.40 <C> 18.31 <R> <C> AADIT <C> AD <C> 17.63 <C> 16.82 <R> <C> AADIT <C> DP <C> 17.67 <C> 16.61 <R> <C> AADIT + PE <C> AD <C> 17.57 <C> 16.68 <R> <C> AADIT + PE <C> DP <C> 17.37 <C> 16.94 <R> <C> MH AADIT <C> AD <C> 17.33 <C> 17.10 <R> <C> MH AADIT <C> DP <C> 17.25 <C> 16.97 <CAP> Table 3: The ASR WERs (%) of multi-conditional (MC) LSTM acoustic models, ADIT, single-head AADIT, single-head AADIT with positional encoding (PE) and multi-head (MH) AADIT on real development set of CHiME-3. Both the additive (AD) and dot-product (DP) attentions are used for each AADIT system.
<R> <C> [EMPTY] <C> UAS <R> <C> [BOLD] Traditional Methods <C> [EMPTY] <R> <C> Zhang:2008 <C> 84.33 <R> <C> Huang:2010 <C> 85.20 <R> <C> [BOLD] Distributed Representations <C> [EMPTY] <R> <C> Chen:2014 <C> 82.94 <R> <C> Chen:2014a <C> 83.9 <R> <C> [BOLD] Re-rankers <C> [EMPTY] <R> <C> Hayashi:2013 <C> 85.9 <R> <C> Our baseline <C> 85.46 <R> <C> Our re-ranker <C> [BOLD] 85.71(+0.25) <R> <C> Our re-ranker (with oracle) <C> 87.43 <CAP> Table 3: Accuracy on Chinese test set.
<R> <C> [BOLD] Training Data <C> [BOLD] TE <C> [BOLD] MP <C> [BOLD] MP.PE <C> [BOLD] TE.MP <C> [BOLD] All <C> [BOLD] TE.MP.RV.PE <R> <C> [BOLD] Naive Bayes <C> 53.21 <C> 43.32 <C> 45.72 <C> 56.55 <C> 59.63 <C> 58.02 <R> <C> [BOLD] Perceptron <C> 53.07 <C> 52.67 <C> 53.47 <C> 57.87 <C> 57.33 <C> 58.27 <R> <C> [EMPTY] <C> [BOLD] Stemmed <C> [BOLD] Stemmed <C> [BOLD] Stemmed <C> [BOLD] Stemmed <C> [BOLD] Stemmed <C> [BOLD] Stemmed <R> <C> [BOLD] Naive Bayes <C> 53.74 <C> 46.39 <C> 50.67 <C> 58.16 <C> 60.56 <C> [BOLD] 61.23 <R> <C> [BOLD] Perceptron <C> 56.67 <C> 53.73 <C> 54.13 <C> 60.00 <C> 56.93 <C> 57.73 <R> <C> [EMPTY] <C> [BOLD] Lemmas <C> [BOLD] Lemmas <C> [BOLD] Lemmas <C> [BOLD] Lemmas <C> [BOLD] Lemmas <C> [BOLD] Lemmas <R> <C> [BOLD] Naive Bayes <C> 53.88 <C> 45.45 <C> 49.60 <C> 56.42 <C> 58.42 <C> 59.63 <R> <C> [BOLD] Perceptron <C> 54.41 <C> 51.07 <C> 53.07 <C> 57.35 <C> 56.95 <C> 56.95 <R> <C> [EMPTY] <C> [BOLD] Stemmed Lemmas <C> [BOLD] Stemmed Lemmas <C> [BOLD] Stemmed Lemmas <C> [BOLD] Stemmed Lemmas <C> [BOLD] Stemmed Lemmas <C> [BOLD] Stemmed Lemmas <R> <C> [BOLD] Naive Bayes <C> 54.41 <C> 45.99 <C> 49.33 <C> 57.62 <C> 59.63 <C> 59.63 <R> <C> [BOLD] Perceptron <C> 53.34 <C> 51.47 <C> 52.67 <C> 58.29 <C> 56.68 <C> 57.09 <CAP> Table 3: Accuracy of our sentiment analysis experiment results on scale of 0 to 100.
<R> <C> Machine Learning Method <C> Accuracy (%) <R> <C> W2V + Etrees <C> 45.52 <R> <C> W2V + LDA <C> 49.97 <R> <C> W2V + Log <C> 49.18 <R> <C> W2V + k-NN <C> 42.46 <R> <C> W2V + DT <C> 40.37 <R> <C> W2V + G-NB <C> 48.11 <R> <C> W2V + L-SVM <C> 50.65 <R> <C> [BOLD] W2V + G-SVM <C> [BOLD] 51.67 <R> <C> W2V + L-SVR <C> 44.27 <R> <C> W2V + G-SVR <C> 45.90 <R> <C> W2V + PCA + G-SVM <C> 47.77 <R> <C> [BOLD] W2V + RCD + G-SVM <C> [BOLD] 45.51 <R> <C> W2V + MMD + G-SVM <C> 36.09 <CAP> TABLE II: Word2Vec Model Results
<R> <C> [BOLD] Label P <C> [BOLD] MTurk-5000 9.355 <C> [BOLD] MTurk-5000 (10.406) <C> [BOLD] MTurk-200 8.268 <C> [BOLD] MTurk-200 (9.113) <C> [BOLD] Expert-200 6.356 <C> [BOLD] Expert-200 (6.500) <R> <C> I <C> 4.356 <C> (7.999) <C> 3.322 <C> (5.743) <C> 1.903 <C> (2.075) <R> <C> O <C> 6.792 <C> (16.257) <C> 6.134 <C> (14.822) <C> 4.379 <C> (5.347) <CAP> Table 4. Comparison of the length of span annotations. The average (standard deviation in parenthesis) number of tokens are presented within MTurk workers on 5000 documents (MTurk-5000), and on 200 documents (MTurk-200) and medical experts on 200 documents (Expert-200).
<R> <C> [BOLD] Model <C> [BOLD] PPL <C> [BOLD] KL <C> [BOLD] NLL <R> <C> HRED <C> 43.4|48.3 <C> 0.00|0.00 <C> 229.1|355.6 <R> <C> KLA <C> 31.8|44.5 <C> 4.90|4.36 <C> 225.0|331.6 <R> <C> KLA+DO <C> 29.8|40.1 <C> 3.80|4.48 <C> 223.9|317.0 <R> <C> KLA+BOW <C> 26.8|30.9 <C> 12.8|8.92 <C> 247.3|321.1 <R> <C> FB <C> 41.7|32.1 <C> [BOLD] 3.34|3.90 <C> 239.0|322.7 <R> <C> FB-all <C> 29.4|21.7 <C> 5.01|4.97 <C> 226.1|308.2 <R> <C> CO <C> 26.1|36.5 <C> 4.90|4.94 <C> 223.6|289.7 <R> <C> CO+DO <C> 25.1|34.4 <C> 5.01|4.93 <C> 218.7|273.4 <R> <C> [BOLD] CO+SS <C> [BOLD] 23.8|31.8 <C> 4.92|4.93 <C> [BOLD] 213.2|273.4 <R> <C> CO+SS(joint) <C> 28.5|39.6 <C> 5.16|5.02 <C> 224.3|301.3 <CAP> Table 1: Metric Results, left: Dailydialog, right: switchboard
<R> <C> [BOLD] Model <C> [BOLD] Average <C> [BOLD] Greedy <C> [BOLD] Extrema <R> <C> HRED <C> 0.463|0.334 <C> 0.445| [BOLD] 0.399 <C> 0.356|0.280 <R> <C> KLA <C> 0.442|0.317 <C> 0.436|0.327 <C> 0.327|0.267 <R> <C> KLA+DO <C> 0.458|0.325 <C> 0.461|0.341 <C> 0.378|0.283 <R> <C> KLA+BOW <C> 0.475|0.340 <C> 0.459|0.352 <C> 0.386|0.302 <R> <C> FB <C> 0.423|0.336 <C> 0.414|0.348 <C> 0.349|0.318 <R> <C> FB-all <C> 0.429|0.341 <C> 0.439|0.352 <C> 0.357|0.325 <R> <C> CO <C> 0.465|0.377 <C> 0.465|0.381 <C> 0.394|0.331 <R> <C> CO+DO <C> 0.489|0.385 <C> 0.471|0.379 <C> 0.397|0.337 <R> <C> [BOLD] CO+SS <C> [BOLD] 0.539|0.392 <C> [BOLD] 0.477|0.394 <C> [BOLD] 0.443|0.340 <R> <C> CO+SS(joint) <C> 0.420|0.347 <C> 0.452|0.360 <C> 0.351|0.308 <CAP> Table 2: Embedding Results, left: dailydialog, right: switchboard
<R> <C> System <C> Four-way  [ITALIC] F1 <C> Four-way Acc. <C> Binary Comp. <C> Binary Cont. <C> Binary Expa. <C> Binary Temp. <R> <C> LSTM <C> 39.40 <C> 54.50 <C> 33.72 <C> 44.79 <C> 68.74 <C> 33.14 <R> <C> NNMA (one-level) <C> 43.48 <C> 55.59 <C> 34.72 <C> 49.47 <C> 68.52 <C> 36.70 <R> <C> NNMA (two-level) <C> [BOLD] 46.29 <C> 57.17 <C> 36.70 <C> [BOLD] 54.48 <C> [BOLD] 70.43 <C> [BOLD] 38.84 <R> <C> NNMA (three-level) <C> 44.95 <C> [BOLD] 57.57 <C> [BOLD] 39.86 <C> 53.69 <C> 69.71 <C> 37.61 <CAP> Table 3: Performances of NNMA with Different Attention Levels.
<R> <C> [BOLD] Output Layer <C> [BOLD] #Param <C> [BOLD] Validation <C> [BOLD] Test <R> <C> Full softmax <C> 43.8M <C> 69.9 <C> 66.8 <R> <C> Weight tying [PW17] <C> 24.2M <C> 60.0 <C> 57.3 <R> <C> Bilinear map. [G18] <C> 24.3M <C> 60.7 <C> 58.5 <R> <C> Dual nonlinear map. [PH18] <C> 24.5M <C> 58.8 <C> 56.4 <R> <C> DRILL 1-layer <C> 24.3M <C> 58.8 <C> 56.2 <R> <C> DRILL 2-layers <C> 24.5M <C> 58.7 <C> 56.0 <R> <C> DRILL 3-layers <C> 24.7M <C> 58.5 <C> 55.9 <R> <C> DRILL 4-layers <C> 24.8M <C> [BOLD] 58.2 <C> [BOLD] 55.7 <R> <C> + residuals between layers <C> 24.8M <C> 59.6 <C> 57.5 <R> <C> - no variational dropout <C> 24.8M <C> 63.4 <C> 60.7 <CAP> Table 4: Ablation results and comparison with previous output layers when using AWD-LSTM (Merity et al., 2017) as an encoder network on PennTreebank.
<R> <C> [BOLD] Model <C> [BOLD] BLEU <R> <C> Bidirectional GRU (Sennrich et al.,  2016 ) <C> 22.8 <R> <C> ByteNet (Kalchbrenner et al.,  2016 ) <C> 23.7 <R> <C> GNMT + RL (Johnson et al.,  2017 ) <C> 24.6 <R> <C> ConvS2S (Gehring et al.,  2017 ) <C> 25.1 <R> <C> MoE (Shazeer et al.,  2017 ) <C> 26.0 <R> <C> GNMT + RL Ensemble (Johnson et al.,  2017 ) <C> 26.3 <R> <C> ConvS2S Ensemble (Gehring et al.,  2017 ) <C> 26.3 <R> <C> Transformer (base) (Vaswani et al.,  2017 ) <C> 27.3 <R> <C> Transformer-Dual (base) [PH18] <C> 27.5 <R> <C> Ours – Transformer-DRILL (base) <C> [BOLD] 28.1 <R> <C> Transformer (big) (Vaswani et al.,  2017 ) <C> 28.4 <R> <C> RNMT+ (Chen et al.,  2018b ) <C> 28.5 <R> <C> RNMT+ cascaded (Chen et al.,  2018b ) <C> 28.6 <R> <C> RNMT+ multicol (Chen et al.,  2018b ) <C> 28.8 <CAP> Table 5: Translation results in terms of BLEU on English to German with a 32K BPE vocabulary.
<R> <C> [BOLD] Dataset <C> [BOLD] SCAN Simple <C> [BOLD] SCAN Add Jump <C> [BOLD] SCAN Around Right <C> [BOLD] SCAN Length <C> [BOLD] SCAN MCD (1/2/3) <C> [BOLD] SCAN-ext Extend <C> [BOLD] MiniSCAN Limit <R> <C> Train Size <C> 16728 <C> 14670 <C> 15225 <C> 16990 <C> 8365 <C> 20506 <C> 14 <R> <C> Test Size <C> 4182 <C> 7706 <C> 4476 <C> 3920 <C> 1045 <C> 4000 <C> 8 <CAP> Table 4: The dataset splits for all tasks.
<R> <C> Model <C> F1 <R> <C> RNN <C> 31.9 <R> <C> + max-pooling <C> 67.5 <R> <C> + position indicators <C> 76.9 <R> <C> + bidirection <C> 79.6 <CAP> Table 2: F1 results with the proposed RNN model on SemEval-2010 Task 8 dataset, + shows the performance after adding each modification.
<R> <C> Dataset <C> Context Length ≤10 <C> Context Length 11 - 15 <C> Context Length ≥ 16 <C> Proportion of Long Context (≥11) <R> <C> SemEval-2010 task-8  <C> 6658 <C> 3725 <C> 334 <C> 0.379 <R> <C> NYT+Freebase  <C> 22057 <C> 19369 <C> 3889 <C> 0.513 <R> <C> KBP+Wikipedia  <C> 6618 <C> 11647 <C> 15546 <C> 0.804 <CAP> Table 4: The distribution of context lengths with three datasets.
<R> <C> [BOLD] Model <C> [BOLD] Model  [BOLD] Size <C> CoLA Mcc <C> QQP F1/Acc <C> MNLI-m/mm Acc <C> SST-2 Acc <C> STS-B Acc <C> QNLI Acc <C> RTE Acc <C> MRPC Acc <R> <C> BERT [ITALIC] large <C> 335M <C> 60.6 <C> 91.3 <C> 86.6/- <C> 93.2 <C> 90.0 <C> 92.3 <C> 70.4 <C> 88.0 <R> <C> RoBERTa [ITALIC] large <C> 355M <C> 68.0 <C> 92.2 <C> 90.2/90.2 <C> 96.4 <C> 92.4 <C> 93.9 <C> 86.6 <C> 90.9 <R> <C> XLNet [ITALIC] large <C> 340M <C> 69.0 <C> 92.3 <C> 90.8/90.8 <C> [BOLD] 97.0 <C> 92.5 <C> 94.9 <C> 85.9 <C> 90.8 <R> <C> DeBERTa [ITALIC] large <C> 390M <C> [BOLD] 69.5 <C> [BOLD] 92.3 <C> [BOLD] 91.1/91.1 <C> 96.5 <C> [BOLD] 92.5 <C> [BOLD] 95.3 <C> [BOLD] 88.1 <C> [BOLD] 92.5 <CAP> Table 1: Comparison results on the GLUE development set.
<R> <C> Hyper-parameter <C> DeBERTa [ITALIC] large <C> DeBERTa [ITALIC] base <C> DeBERTa [ITALIC] base− [ITALIC] ablation <R> <C> Number of Layers <C> 24 <C> 12 <C> 12 <R> <C> Hidden size <C> 1024 <C> 768 <C> 768 <R> <C> FNN inner hidden size <C> 4096 <C> 3072 <C> 3072 <R> <C> Attention Heads <C> 16 <C> 12 <C> 12 <R> <C> Attention Head size <C> 64 <C> 64 <C> 64 <R> <C> Dropout <C> 0.1 <C> 0.1 <C> 0.1 <R> <C> Warmup Steps <C> 10k <C> 10k <C> 10k <R> <C> Learning Rates <C> 2e-4 <C> 2e-4 <C> 1e-4 <R> <C> Batch Size <C> 2k <C> 2k <C> 256 <R> <C> Weight Decay <C> 0.01 <C> 0.01 <C> 0.01 <R> <C> Max Steps <C> 1M <C> 1M <C> 1M <R> <C> Learning Rate Decay <C> Linear <C> Linear <C> Linear <R> <C> Adam  [ITALIC] ϵ <C> 1e-6 <C> 1e-6 <C> 1e-6 <R> <C> Adam  [ITALIC] β1 <C> 0.9 <C> 0.9 <C> 0.9 <R> <C> Adam  [ITALIC] β2 <C> 0.999 <C> 0.999 <C> 0.999 <R> <C> Gradient Clipping <C> 1.0 <C> 1.0 <C> 1.0 <R> <C> Gradient Clipping <C> 1.0 <C> 1.0 <C> 1.0 <R> <C> Number of DGX-2 nodes <C> 6 <C> 4 <C> 1 <R> <C> Training Time <C> 20 days <C> 10 days <C> 7 days <CAP> Table 6: Hyper-parameters for pre-training DeBERTa.
<R> <C> Hyper-parameter <C> DeBERTa [ITALIC] large <C> DeBERTa [ITALIC] base <R> <C> Dropout of task layer <C> {0,0.1,0.15} <C> {0,0.1,0.15} <R> <C> Warmup Steps <C> {50,100,500,1000} <C> {50,100,500,1000} <R> <C> Learning Rates <C> {5e-6, 8e-6, 9e-6, 1e-5} <C> {1.5e-5,2e-5, 3e-5, 4e-5} <R> <C> Batch Size <C> {16,32,48,64} <C> {16,32,48,64} <R> <C> Weight Decay <C> 0.01 <C> 0.01 <R> <C> Maximun Training Epochs <C> 10 <C> 10 <R> <C> Learning Rate Decay <C> Linear <C> Linear <R> <C> Adam  [ITALIC] ϵ <C> 1e-6 <C> 1e-6 <R> <C> Adam  [ITALIC] β1 <C> 0.9 <C> 0.9 <R> <C> Adam  [ITALIC] β2 <C> 0.999 <C> 0.999 <R> <C> Gradient Clipping <C> 1.0 <C> 1.0 <CAP> Table 7: Hyper-parameters for fine-tuning DeBERTa on down-streaming tasks.
<R> <C> Sequence length <C> Middle <C> High <C> Accuracy <R> <C> 512 <C> 88.8 <C> 85.0 <C> 86.3 <R> <C> 768 <C> 88.7 <C> 86.3 <C> 86.8 <CAP> Table 8: The effect of handling long sequence input for RACE task with DeBERTa
<R> <C> Method <C> Infantile diarrhea <C> Dyspepsia <C> Upper respiratory infection <C> Bronchitis <C> Overall <R> <C> SVM-ex <C> 0.89 <C> 0.28 <C> 0.44 <C> 0.71 <C> 0.59 <R> <C> SVM-ex&im <C> 0.91 <C> 0.34 <C> [BOLD] 0.52 <C> 0.93 <C> 0.71 <R> <C> Basic DQN  <C> - <C> - <C> - <C> - <C> 0.65 <R> <C> DQN + relation branch* <C> 0.87 <C> 0.31 <C> 0.42 <C> 0.86 <C> 0.68 <R> <C> DQN + relation branch <C> 0.92 <C> 0.35 <C> 0.49 <C> 0.93 <C> 0.70 <R> <C> DQN + knowledge branch <C> 0.88 <C> 0.31 <C> 0.44 <C> 0.89 <C> 0.68 <R> <C> Our KR-DS <C> [BOLD] 0.96 <C> [BOLD] 0.39 <C> 0.50 <C> [BOLD] 0.97 <C> [BOLD] 0.73 <CAP> Table 3: Performance comparison on the MZ dataset.
<R> <C> Method <C> Accuracy <C> Match rate <C> Ave turns <R> <C> Basic DQN  <C> 0.731 <C> 0.110 <C> 3.92 <R> <C> Sequicity  <C> 0.285 <C> 0.246 <C> 3.40 <R> <C> Our KR-DS <C> [BOLD] 0.740 <C> [BOLD] 0.267 <C> 3.36 <CAP> Table 4: Performance comparisons with the state-of-the-art methods on DX dataset.
<R> <C> Reward <C> R1 <C> R2 <C> R1* <C> R2* <R> <C> Accuracy <C> 0.697 <C> 0.725 <C> 0.718 <C> 0.739 <CAP> Table 5: Evaluation of reward magnitude on MZ dataset.
<R> <C> [BOLD] Method <C> [BOLD] STSB <C> [BOLD] SICK-R <C> [BOLD] SICK-E <C> [BOLD] MRPC <R> <C> [ITALIC] Ensemble models/Feature engineering <C> [ITALIC] Ensemble models/Feature engineering <C> [ITALIC] Ensemble models/Feature engineering <C> [ITALIC] Ensemble models/Feature engineering <C> [ITALIC] Ensemble models/Feature engineering <R> <C> DT_TEAM (Maharjan et al.,  2017 ) <C> 79.2 <C> - <C> - <C> - <R> <C> ECNU (Tian et al.,  2017 ) <C> 81 <C> - <C> - <C> - <R> <C> BIT (Wu et al.,  2017 ) <C> 80.9 <C> - <C> - <C> - <R> <C> TF-KLD (Ji and Eisenstein,  2013 ) <C> - <C> - <C> - <C> [BOLD] 80.41/ [BOLD] 85.96 <R> <C> [ITALIC] Neural representation models with one embedding <C> [ITALIC] Neural representation models with one embedding <C> [ITALIC] Neural representation models with one embedding <C> [ITALIC] Neural representation models with one embedding <C> [ITALIC] Neural representation models with one embedding <R> <C> Multi-Perspective CNN (He et al.,  2015 ) <C> - <C> 86.86 <C> - <C> 78.6/84.73 <R> <C> InferSent (Conneau et al.,  2017 ) <C> 75.8 <C> 88.4 <C> [BOLD] 86.1 <C> 76.2/83.1 <R> <C> GRAN (Wieting and Gimpel,  2017 ) <C> 76.4 <C> 86 <C> - <C> - <R> <C> Paragram-Phrase (Wieting et al.,  2016b ) <C> 73.2 <C> 86.84 <C> 85.3 <C> - <R> <C> HCTI (Shao,  2017 ) <C> 78.4 <C> - <C> - <C> - <R> <C> [ITALIC] Neural representation models with the five embeddings using sentence-sentence comparison (S) <C> [ITALIC] Neural representation models with the five embeddings using sentence-sentence comparison (S) <C> [ITALIC] Neural representation models with the five embeddings using sentence-sentence comparison (S) <C> [ITALIC] Neural representation models with the five embeddings using sentence-sentence comparison (S) <C> [ITALIC] Neural representation models with the five embeddings using sentence-sentence comparison (S) <R> <C> S-Word Average <C> 71.06 <C> 81.18 <C> 80.88 <C> 71.48/81.1 <R> <C> S-Project Average <C> 75.12 <C> 86.53 <C> 85.12 <C> 75.48/82.47 <R> <C> S-LSTM <C> 77.14 <C> 85.15 <C> 85.6 <C> 70.43/79.71 <R> <C> S-Max-CNN <C> 81.87 <C> 88.3 <C> 84.33 <C> 76.35/83.75 <R> <C> S-MaxLSTM-CNN <C> 82.2 <C> 88.47 <C> 84.9 <C> 77.91/84.31 <R> <C> [ITALIC] Neural representation models with the five embeddings using Multi-level comparison (M) <C> [ITALIC] Neural representation models with the five embeddings using Multi-level comparison (M) <C> [ITALIC] Neural representation models with the five embeddings using Multi-level comparison (M) <C> [ITALIC] Neural representation models with the five embeddings using Multi-level comparison (M) <C> [ITALIC] Neural representation models with the five embeddings using Multi-level comparison (M) <R> <C> M-Max-CNN <C> 82.11 <C> 88.45 <C> 84.7 <C> 76.75/83.64 <R> <C> M-MaxLSTM-CNN <C> [BOLD] 82.45 <C> [BOLD] 88.76 <C> 84.95 <C> 78.1/84.5 <CAP> Table 2: Test set results with Pearson’s r score×100 for STS tasks, and accuracy for other tasks. Boldface values show the highest scores in each dataset. SICK-R and SICK-E denote the STS task and the entailment task in SICK dataset respectively.
<R> <C> [BOLD] Word embedding <C> [BOLD] STSB Pearson <C> [BOLD] STSB | [ITALIC] V| [ITALIC] avai(%) <C> [BOLD] SICK-R &  [BOLD] SICK-E Pearson <C> [BOLD] SICK-R &  [BOLD] SICK-E Acc <C> [BOLD] SICK-R &  [BOLD] SICK-E | [ITALIC] V| [ITALIC] avai(%) <C> [BOLD] MRPC Acc/F1 <C> [BOLD] MRPC | [ITALIC] V| [ITALIC] avai(%) <R> <C> word2Vec <C> 78.9 <C> 75.64 <C> 87.27 <C> 84.09 <C> 98.53 <C> 75.42/82.13 <C> 67.81 <R> <C> fastText <C> 79.95 <C> 84.27 <C> 87.59 <C> 83.42 <C> 99.18 <C> 74.31/81.75 <C> 79.04 <R> <C> Glove <C> 80.1 <C> 91.71 <C> 88.21 <C> 84.71 <C> 99.78 <C> 74.9/82.782 <C> 89.85 <R> <C> SL999 <C> 80.31 <C> 94.76 <C> 87.26 <C> 84.55 <C> 99.83 <C> 76.46/83.13 <C> 94.19 <R> <C> Baroni <C> 79.81 <C> 90.54 <C> 86.9 <C> 83.99 <C> 98.83 <C> 74.84/82.4 <C> 87.92 <R> <C> All <C> [BOLD] 82.45 <C> 95.65 <C> [BOLD] 88.76 <C> [BOLD] 84.95 <C> 99.83 <C> [BOLD] 78.1/ [BOLD] 84.5 <C> 95.97 <CAP> Table 3: Evaluation of exploiting multiple pre-trained word embeddings. |V|avai is the proportion of vocabulary available in a word embedding. In case of using all word embeddings, |V|avai denotes the proportion of vocabulary where each word is available in at least one word embedding.
<R> <C> [EMPTY] <C> Model <C> SD <C> SI <C> SA <R> <C> 1 <C> Best prior results  <C> [BOLD] 7.6 [ITALIC] a <C> 55.3 [ITALIC] b <C> 27.9 [ITALIC] c <R> <C> 2 <C> HOG + enc-dec <C> 11.1 <C> 50.3 <C> 29.1 <R> <C> 3 <C> CNN + enc-dec <C> 9.1 <C> 50.7 <C> 28.7 <R> <C> 4 <C> DNN + enc-dec <C> 9.9 <C> 50.9 <C> 29.3 <R> <C> 5 <C> CNN + enc-dec+ <C> 10.7 <C> 50.4 <C> 29.2 <R> <C> 6 <C> E2E CNN-enc-dec <C> 11.8 <C> 48.4 <C> 27.5 <R> <C> 7 <C> E2E DNN-enc-dec <C> 12.1 <C> 47.9 <C> 26.9 <R> <C> 8 <C> AE + enc-dec <C> 21.7 <C> 61.2 <C> 40.3 <R> <C> 9 <C> DAE + enc-dec <C> 17.5 <C> 56.4 <C> 34.4 <R> <C> 10 <C> VAE + enc-dec <C> 18.8 <C> 58.2 <C> 37.8 <R> <C> 11 <C> E2E AE-enc-dec <C> 11.8 <C> 48.1 <C> 30.2 <R> <C> 12 <C> E2E DAE-enc-dec <C> 11.9 <C> 45.0 <C> 28.9 <R> <C> 13 <C> E2E VAE-enc-dec <C> 10.6 <C> 43.8 <C> 23.8 <R> <C> 14 <C> E2E AE-enc-dec* <C> 10.0 <C> 47.3 <C> 29.2 <R> <C> 15 <C> E2E DAE-enc-dec* <C> 9.5 <C> 44.3 <C> 27.2 <R> <C> 16 <C> E2E VAE-enc-dec* <C> 8.1 <C> [BOLD] 43.7 <C> [BOLD] 23.5 <CAP> Table 1: Letter error rates (%) of different models. SD: signer-dependent, SI: signer-independent, SA: signer-adapted. Model names with an asterisk (*) and a plus (+) use extra unlabeled hand image data and augmented data respectively. Best prior results are obtained with SCRFs (a = 2-pass SCRF, b = rescoring SCRF, c = first-pass SCRF).
<R> <C> [EMPTY] <C> Validation Results Precision <C> Validation Results Recall <C> Validation Results F1 Score <C> Testing Results Precision <C> Testing Results Recall <C> Testing Results F1 Score <R> <C> BERT <C> 0.7030 <C> 0.7498 <C> 0.749855 <C> 0.7153 <C> 0.7704 <C> 0.7418 <R> <C> EDA+BERT <C> 0.9075 <C> 0.9342 <C> [BOLD] 0.9207 <C> 0.7201 <C> 0.7589 <C> 0.7390 <R> <C> Mod-EDA+BERT <C> 0.9052 <C> 0.9257 <C> 0.9153 <C> 0.7414 <C> 0.76771 <C> [BOLD] 0.7543 <R> <C> ELMO <C> 0.7130 <C> 0.7298 <C> 0.7213 <C> 0.6824 <C> 0.7225 <C> 0.7018 <R> <C> EDA+ELMO <C> 0.9043 <C> 0.9234 <C> [BOLD] 0.9137 <C> 0.7028 <C> 0.7223 <C> 0.71225 <R> <C> Mod-EDA + ELMO <C> 0.8924 <C> 0.92543 <C> 0.9086 <C> 0.7134 <C> 0.7323 <C> [BOLD] 0.7227 <R> <C> USE <C> 0.7301 <C> 0.7198 <C> 0.7252 <C> 0.6924 <C> 0.7223 <C> 0.7070 <R> <C> EDA+USE <C> 0.8923 <C> 0.9123 <C> [BOLD] 0.9022 <C> 0.6902 <C> 0.7112 <C> 0.7005 <R> <C> Mod-EDA+USE <C> 0.8935 <C> 0.8933 <C> 0.8934 <C> 0.7034 <C> 0.7223 <C> [BOLD] 0.7127 <CAP> Table 1. Comparison of Mod-EDA, EDA and without augmentation on SST2 dataset.
<R> <C> Method <C> #(step) 1 <C> #(step) 2 <C> #(step) 3 <C> #(step) 5 <C> #(step) 10 <C> #(step) 20 <R> <C> i-vector <C> 6.64 <C> 6.64 <C> 6.64 <C> 6.64 <C> 6.64 <C> 6.64 <R> <C> cpc best <C> 5.00 <C> 5.00 <C> 5.00 <C> 5.00 <C> 5.00 <C> 5.00 <R> <C> apc 1-layer <C> 4.71 <C> 4.07 <C> 4.14 <C> 4.14 <C> 5.14 <C> 5.29 <R> <C> apc 2-layer <C> 4.71 <C> 4.64 <C> 5.71 <C> 4.86 <C> 5.57 <C> 6.07 <R> <C> apc 3-layer <C> 5.21 <C> 4.93 <C> 4.43 <C> 4.57 <C> 5.79 <C> 6.21 <R> <C> apc 3-layer-1 <C> 3.43 <C> 3.86 <C> 3.79 <C> 3.86 <C> 4.07 <C> 4.86 <R> <C> apc 3-layer-2 <C> 3.79 <C> 4.64 <C> 4.14 <C> 4.29 <C> 5.14 <C> 5.00 <CAP> Table 3: EER on speaker verification. The number of steps to the target #(steps) is not relevant for the first two rows.
<R> <C> [BOLD] Method <C> [BOLD] MR <C> [BOLD] CR <C> [BOLD] SUBJ <C> [BOLD] MPQA <C> [BOLD] TREC <C> [BOLD] MSRP(Acc/F1) <R> <C> ParagraphVec DM (Hill et al.,  2016 ) <C> 61.5 <C> 68.6 <C> 76.4 <C> 78.1 <C> 55.8 <C> 73.6 / 81.9 <R> <C> SDAE (Hill et al.,  2016 ) <C> 67.6 <C> 74.0 <C> 89.3 <C> 81.3 <C> 77.6 <C> 76.4 / 83.4 <R> <C> SDAE+emb. (Hill et al.,  2016 ) <C> 74.6 <C> 78.0 <C> 90.8 <C> 86.9 <C> 78.4 <C> 73.7 / 80.7 <R> <C> FastSent (Hill et al.,  2016 ) <C> 70.8 <C> 78.4 <C> 88.7 <C> 80.6 <C> 76.8 <C> 72.2 / 80.3 <R> <C> uni-skip (Kiros et al.,  2015 ) <C> 75.5 <C> 79.3 <C> 92.1 <C> 86.9 <C> 91.4 <C> 73.0 / 81.9 <R> <C> bi-skip (Kiros et al.,  2015 ) <C> 73.9 <C> 77.9 <C> 92.5 <C> 83.3 <C> 89.4 <C> 71.2 / 81.2 <R> <C> combine-skip (Kiros et al.,  2015 ) <C> 76.5 <C> 80.1 <C> 93.6 <C> 87.1 <C> 92.2 <C> 73.0 / 82.0 <R> <C> [ITALIC] Our Results† <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> autoencoder <C> 75.53 <C> 78.97 <C> 91.97 <C> 87.96 <C> 89.8 <C> 73.61 / 82.14 <R> <C> future predictor <C> 72.56 <C> 78.44 <C> 90.72 <C> 87.48 <C> 86.6 <C> 71.87 / 81.68 <R> <C> hierarchical model <C> 75.20 <C> 77.99 <C> 91.66 <C> 88.21 <C> 90.0 <C> 73.96 / 82.54 <R> <C> composite model <C> 76.34 <C> 79.93 <C> 92.45 <C> 88.77 <C> 91.4 <C> 74.65 / 82.21 <R> <C> combine‡ <C> 77.21 <C> 80.85 <C> 93.11 <C> 89.09 <C> 91.8 <C> 75.52 / 82.62 <R> <C> hierarchical model+emb. <C> 75.30 <C> 79.37 <C> 91.94 <C> 88.48 <C> 90.4 <C> 74.25 / 82.70 <R> <C> composite model+emb. <C> 77.16 <C> 80.64 <C> 92.14 <C> 88.67 <C> 91.2 <C> 74.88 / 82.28 <R> <C> combine+emb.‡ <C> [BOLD] 77.77 <C> [BOLD] 82.05 <C> [BOLD] 93.63 <C> [BOLD] 89.36 <C> [BOLD] 92.6 <C> [BOLD] 76.45 /  [BOLD] 83.76 <R> <C> [ITALIC] Task-dependent methods <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> CNN (Kim,  2014 ) <C> 81.5 <C> 85.0 <C> 93.4 <C> 89.6 <C> 93.6 <C> − <R> <C> AdaSent (Zhao et al.,  2015 ) <C> 83.1 <C> 86.3 <C> 95.5 <C> 93.3 <C> 92.4 <C> − <R> <C> Bi-CNN-MI (Yin and Schütze,  2015 ) <C> − <C> − <C> − <C> − <C> − <C> 78.1/84.4 <R> <C> MPSSM-CNN (He et al.,  2015 ) <C> − <C> − <C> − <C> − <C> − <C> 78.6/84.7 <CAP> Table 3: Classification accuracies on several standard benchmarks. The last column shows results on the task of paraphrase detection, where the evaluation metrics are classification accuracy and F1 score. †The first and second block in our results were obtained using the first and second method of considering words not in the training set, respectively. ‡“combine” means concatenating the feature vectors learned from both the hierarchical model and the composite model.
<R> <C> [EMPTY] <C> Training time [s] Features <C> Training time [s] Structured <C> Prediction time [s] <R> <C> Base <C> 596.9±9.2 <C> [EMPTY] <C> 3.8±0.04 <R> <C> Sato <C> 678.5±15.1 <C> 366.9±66.8 <C> 5.2±0.06 <CAP> Table 2: Average training and prediction time over 5 trials Dmult. ± denotes 95% CI. Training time for the column-wise features (Features) and the CRF layer (Structured) is reported separately.
<R> <C> [BOLD] Method <C> [BOLD] EEG data <C> [BOLD] Phonological features <R> <C> LSTM <C> 8.45 <C> 15.83 <R> <C> CNN <C> 8.88 <C> 16.02 <R> <C> CNN+LSTM <C> 12.44 <C> 22.10 <R> <C> CNN+LSTM+DAE <C> 23.45 <C> 49.19 <R> <C> Our model <C> [BOLD] 28.08 <C> [BOLD] 53.36 <CAP> Table 5: Comparison of accuracy on 10% test data for speech token prediction task
<R> <C> [EMPTY] <C> [BOLD] Audio Only <C> [BOLD] Illusory Video <C> [BOLD] Relative Increase <R> <C> Words Incorrectly Identified <C> 10.0 <C> 24.8 <C> +148% <R> <C> Words Correct with Low Confidence <C> 2.1 <C> 5.1 <C> +144% <CAP> Table 3: Test results for word-level McGurk illusions among the 147 words predicted to be illusionable. Shown are average error rates for watching the illusory video vs listening to the audio only, as well as the percentage of words that are correctly identified but sound ambiguous to the listener.
<R> <C> [BOLD] Language <C> [ITALIC] N <C> [BOLD] Feature Set <C> [BOLD] Dimension <C> [BOLD] Accuracy <C> [BOLD] F1-macro <R> <C> - <C> 1 <C> Baseline <C> - <C> 30.30 <C> 05.17 <R> <C> BG <C> 2 <C> BERT(title), BERT(text) <C> 1,536 <C> 47.69 <C> 32.58 <R> <C> [EMPTY] <C> 3 <C> XLM(title), XLM(text) <C> 2,048 <C> 38.50 <C> 24.58 <R> <C> [EMPTY] <C> 4 <C> Styl(title), Styl(text) <C> 15 <C> 31.89 <C> 08.51 <R> <C> [EMPTY] <C> 5 <C> LSA(title), LSA(text) <C> 215 <C> 55.59 <C> 42.11 <R> <C> [EMPTY] <C> 6 <C> Bulgarian combined <C> 3,824 <C> 39.43 <C> 24.38 <R> <C> EN <C> 7 <C> USE(title), USE(text) <C> 1,024 <C> 53.70 <C> 40.68 <R> <C> [EMPTY] <C> 8 <C> NELA(title), NELA(text) <C> 258 <C> 36.36 <C> 23.04 <R> <C> [EMPTY] <C> 9 <C> BERT(title), BERT(text) <C> 1,536 <C> 52.05 <C> 39.78 <R> <C> [EMPTY] <C> 10 <C> ElMO(title), ElMO(text) <C> 2,048 <C> 54.60 <C> 40.95 <R> <C> [EMPTY] <C> 11 <C> English combined <C> 4,878 <C> 45.45 <C> 31.42 <R> <C> - <C> 12 <C> Media meta <C> 6 <C> 42.04 <C> 15.64 <R> <C> [EMPTY] <C> 13 <C> All combined <C> 8,694 <C> 38.16 <C> 26.04 <R> <C> [EMPTY] <C> 14 <C> [BOLD] Meta classifier <C> [BOLD] 153 <C> [BOLD] 59.06 <C> [BOLD] 39.70 <CAP> Таблица 3: Evaluation results.
<R> <C> [BOLD] Method <C> [BOLD] Pr <C> [BOLD] Re <C> [BOLD] F1 <R> <C> Majority classifier <C> 37.69 <C> 50.00 <C> 42.98 <R> <C> Logistic regression <C> 71.64 <C> [BOLD] 68.16 <C> [BOLD] 69.10 <R> <C> Linear SVM <C> 70.40 <C> 61.41 <C> 62.83 <R> <C> XGBoost <C> [BOLD] 75.72 <C> 63.95 <C> 66.06 <CAP> Table 1: Performance of several classification methods of determining the final state of a target user in a flagged thread. The logistic regression is statistically different from all other models (Student t-test, p<0.001, Bonferroni-adjusted.)
<R> <C> [BOLD] Feature set <C> [BOLD] Pr <C> [BOLD] Re <C> [BOLD] F1 <R> <C> Only features from target users <C> 69.72 <C> 62.40 <C> 63.93 <R> <C> Averaged features from target and participating users <C> 60.04 <C> 58.90 <C> 59.31 <R> <C> Separate symmetric features for target and participating users <C> [BOLD] 71.64 <C> [BOLD] 68.16 <C> [BOLD] 69.10 <CAP> Table 2: Comparison of different strategies for extracting features from target and participating users. The best feature set (separate symmetric features) is significantly better than the other two (Student t-test, p<0.001, Bonferroni-adjusted.)
<R> <C> [EMPTY] <C> [ITALIC] collective Hou et al.(2013) <C> [ITALIC] collective Hou et al.(2013) <C> [ITALIC] collective Hou et al.(2013) <C> [ITALIC] cascade collective Hou et al.(2013) <C> [ITALIC] cascade collective Hou et al.(2013) <C> [ITALIC] cascade collective Hou et al.(2013) <C> [ITALIC] self-attention  [ITALIC] wo context <C> [ITALIC] self-attention  [ITALIC] wo context <C> [ITALIC] self-attention  [ITALIC] wo context <C> [ITALIC] self-attention  [ITALIC] with context I <C> [ITALIC] self-attention  [ITALIC] with context I <C> [ITALIC] self-attention  [ITALIC] with context I <C> [ITALIC] self-attention  [ITALIC] with context II <C> [ITALIC] self-attention  [ITALIC] with context II <C> [ITALIC] self-attention  [ITALIC] with context II <R> <C> [EMPTY] <C> R <C> P <C> F <C> R <C> P <C> F <C> R <C> P <C> F <C> R <C> P <C> F <C> R <C> P <C> F <R> <C> old <C> 84.4 <C> 86.0 <C> 85.2 <C> 82.2 <C> 87.2 <C> 84.7 <C> 79.3 <C> 82.4 <C> 80.8 <C> 80.1 <C> 85.8 <C> 82.9 <C> 86.8 <C> 90.9 <C> [BOLD] 88.8 <R> <C> m/worldKnow. <C> 67.4 <C> 77.3 <C> 72.0 <C> 67.2 <C> 77.2 <C> 71.9 <C> 57.8 <C> 62.2 <C> 59.9 <C> 60.8 <C> 64.6 <C> 62.7 <C> 74.6 <C> 79.6 <C> [BOLD] 77.0 <R> <C> m/syntactic <C> 82.2 <C> 81.9 <C> 82.0 <C> 81.6 <C> 82.5 <C> 82.0 <C> 83.7 <C> 81.7 <C> 82.7 <C> 83.2 <C> 82.6 <C> 82.9 <C> 83.6 <C> 83.0 <C> [BOLD] 83.3 <R> <C> m/aggregate <C> 64.5 <C> 79.5 <C> 71.2 <C> 63.5 <C> 77.9 <C> 70.0 <C> 77.7 <C> 75.2 <C> 76.5 <C> 76.3 <C> 79.7 <C> 78.0 <C> 76.8 <C> 80.6 <C> [BOLD] 78.6 <R> <C> m/function <C> 67.7 <C> 72.1 <C> [BOLD] 69.8 <C> 67.7 <C> 72.1 <C> [BOLD] 69.8 <C> 42.2 <C> 56.3 <C> 48.2 <C> 54.7 <C> 61.4 <C> 57.9 <C> 50.0 <C> 74.4 <C> 59.8 <R> <C> m/comparative <C> 81.8 <C> 82.1 <C> 82.0 <C> 86.6 <C> 78.2 <C> 82.2 <C> 89.3 <C> 88.6 <C> 89.0 <C> 90.1 <C> 86.4 <C> 88.2 <C> 91.7 <C> 89.2 <C> [BOLD] 90.4 <R> <C> m/bridging <C> 19.3 <C> 39.0 <C> 25.8 <C> 44.9 <C> 39.8 <C> 42.2 <C> 37.6 <C> 48.6 <C> 42.4 <C> 43.6 <C> 51.6 <C> [BOLD] 47.3 <C> 42.5 <C> 50.4 <C> 46.1 <R> <C> new <C> 86.5 <C> 76.1 <C> 81.0 <C> 83.0 <C> 78.1 <C> 80.5 <C> 86.1 <C> 80.0 <C> 82.9 <C> 88.3 <C> 81.0 <C> 84.5 <C> 88.5 <C> 82.0 <C> [BOLD] 85.1 <R> <C> acc <C> 78.9 <C> 78.9 <C> 78.9 <C> 78.6 <C> 78.6 <C> 78.6 <C> 78.1 <C> 78.1 <C> 78.1 <C> 79.8 <C> 79.8 <C> 79.8 <C> [BOLD] 83.0 <C> [BOLD] 83.0 <C> [BOLD] 83.0 <CAP> Table 3: Results of the discourse context-aware self-attention model compared to the baselines. Bolded scores indicate the best performance for each IS class. The improvements of self-attention with context I and self-attention with context II over the baselines are statistically significant at p<0.01 using randomization test.
<R> <C> C <C> C [ITALIC] random <C> ℓ <C> ℓ [ITALIC] random <R> <C> 0.814 <C> 0.072 <C> 1.613 <C> 2.627 <CAP> Table 4: Average clustering coefficient (C) and the average shortest path length (ℓ) in the networks of the collective discourse corpora and the corresponding random networks.
<R> <C> [BOLD] Lang. <C> [BOLD] Model <C> [BOLD] Accuracy (%)  [BOLD] GPE <C> [BOLD] Accuracy (%)  [BOLD] LOC <C> [BOLD] Accuracy (%)  [BOLD] PER <C> [BOLD] Accuracy (%)  [BOLD] ORG <R> <C> [EMPTY] <C> xelms <C> 33.0 <C> 3.6 <C> 6.2 <C> 6.3 <R> <C> Oromo <C> ELISA <C> 37.0 <C> 11.6 <C> 12.5 <C> [BOLD] 11.9 <R> <C> [EMPTY] <C> PBEL_PLUS <C> 53.0 <C> 40.5 <C> 6.4 <C> 0.0 <R> <C> [EMPTY] <C> CogCompXEL <C> [BOLD] 57.5 <C> [BOLD] 30.4 <C> [BOLD] 17.2 <C> 4.2 <R> <C> [EMPTY] <C> xelms <C> 25.6 <C> 0.0 <C> 37.5 <C> 5.7 <R> <C> Zulu <C> ELISA <C> 11.9 <C> 2.5 <C> 31.2 <C> 5.7 <R> <C> [EMPTY] <C> PBEL_PLUS <C> 24.6 <C> 6.2 <C> 38.1 <C> 8.7 <R> <C> [EMPTY] <C> CogCompXEL <C> [BOLD] 27.8 <C> [BOLD] 10.8 <C> [BOLD] 50.0 <C> [BOLD] 12.6 <R> <C> [EMPTY] <C> xelms <C> 55.0 <C> 28.6 <C> 16.7 <C> 55.0 <R> <C> Somali <C> ELISA <C> 44.7 <C> 14.3 <C> [BOLD] 33.3 <C> [BOLD] 75.0 <R> <C> [EMPTY] <C> PBEL_PLUS <C> 50.1 <C> 0.0 <C> 16.7 <C> 15.0 <R> <C> [EMPTY] <C> CogCompXEL <C> [BOLD] 71.7 <C> [BOLD] 57.1 <C> [BOLD] 33.3 <C> [BOLD] 65.0 <CAP> Table 8: Linking accuracy on geopolitical entities (GPE), location (LOC), person (PER) and organization (ORG) for three low-resource languages.
<R> <C> [EMPTY] <C> words <C> [ITALIC] F1 <R> <C> cs <C> 115,218 <C> 87.5 <R> <C> fi <C> 39,856 <C> 71.9 <R> <C> hu <C> 135,386 <C> 79.7 <R> <C> avg. <C> 96,820 <C> 79.7 <CAP> Table 5: Micro-averaged F1 score (%) for prediction of lexical attributes on the test lexicon of human-curated lexicons.
<R> <C> [EMPTY] <C> None <C> Seed <C> Propagation <R> <C> eu <C> 84.1 <C> 84.4 <C> [BOLD] 85.2 <R> <C> bg <C> 94.2 <C> 94.6 <C> [BOLD] 95.9 <R> <C> hr <C> 92.5 <C> [BOLD] 93.6 <C> 93.2 <R> <C> cs <C> 96.8 <C> 97.1 <C> 97.1 <R> <C> da <C> 96.4 <C> 97.1 <C> [BOLD] 97.3 <R> <C> en <C> 94.4 <C> 94.7 <C> [BOLD] 94.8 <R> <C> fi <C> 92.8 <C> 93.6 <C> [BOLD] 94.0 <R> <C> el <C> 93.4 <C> [BOLD] 94.6 <C> 94.2 <R> <C> hu <C> 91.7 <C> 92.3 <C> [BOLD] 93.5 <R> <C> it <C> 96.8 <C> 97.1 <C> 97.1 <R> <C> sv <C> 95.4 <C> 96.5 <C> 96.5 <R> <C> avg. <C> 93.5 <C> 94.2 <C> [BOLD] 94.5 <CAP> Table 7: Macro-averaged F1 score (%) for morphological tagging: without using any lexicon (None), with seed lexicon (Seed), with propagated lexicon (Propagation).
<R> <C> [EMPTY] <C> None <C> Seed <C> Propagation <R> <C> eu <C> 60.5 <C> 62.3 <C> [BOLD] 62.9 <R> <C> bg <C> 78.3 <C> 78.8 <C> [BOLD] 79.3 <R> <C> hr <C> 72.8 <C> 74.7 <C> 74.7 <R> <C> cs <C> 78.3 <C> 78.4 <C> 78.4 <R> <C> da <C> 67.5 <C> 69.4 <C> [BOLD] 70.1 <R> <C> en <C> 74.4 <C> 74.1 <C> [BOLD] 74.4 <R> <C> fi <C> 66.1 <C> 67.4 <C> [BOLD] 67.9 <R> <C> el <C> 75.0 <C> 75.6 <C> [BOLD] 75.8 <R> <C> hu <C> 67.6 <C> 69.0 <C> [BOLD] 71.1 <R> <C> it <C> 82.4 <C> 82.8 <C> [BOLD] 83.1 <R> <C> sv <C> 69.7 <C> 70.1 <C> 70.1 <R> <C> avg. <C> 72.0 <C> 73.0 <C> [BOLD] 73.5 <CAP> Table 8: Labeled accuracy score (LAS, %) for dependency parsing: without using any lexicon (None), with seed (Seed), with propagated lexicon (Propagation).
<R> <C> [EMPTY] <C> English <C> French <C> Mandarin <R> <C> Train  [ITALIC] Naph: [ITALIC] Ncontr <C> 201:201 <C> - <C> - <R> <C> Test  [ITALIC] Naph: [ITALIC] Ncontr <C> 36:36 <C> 9:9 <C> 15:15 <R> <C> Accuracy <C> 0.903 <C> 0.833 <C> 0.500 <R> <C> F1 score <C> 0.902 <C> 0.833 <C> 0.333 <CAP> Table 2: Performance of the aphasia/control classifier on the English, French and Mandarin test sets.
<R> <C> [BOLD] Metrics %  [BOLD] Exact Match <C> [BOLD] Metrics %  [BOLD] Set Intersection <C> [BOLD] Metrics %  [BOLD] Set Intersection <C> [BOLD] ConceptNet 39.1 <C> [BOLD] QA Model 23.3 <C> [BOLD] GPT2 22.0 <C> [BOLD] GPT2 Fine Tune 39.4 <C> [BOLD] Human Ranking  [BOLD] 46.5 <R> <C> [BOLD] Exact Match <C> [BOLD] Max Answers <C> 1 <C> 2.8 <C> 2.1 <C> 7.5 <C> 34.6 <C> [BOLD] 78.4 <R> <C> [BOLD] Exact Match <C> [BOLD] Max Answers <C> 3 <C> 2.8 <C> 9.9 <C> 23.7 <C> 46.9 <C> [BOLD] 74.4 <R> <C> [BOLD] Exact Match <C> [BOLD] Max Answers <C> 5 <C> 2.6 <C> 12.6 <C> 25.8 <C> 51.6 <C> [BOLD] 72.5 <R> <C> [BOLD] Exact Match <C> [BOLD] Max Answers <C> 10 <C> 6.9 <C> 20.0 <C> 31.2 <C> 58.2 <C> [BOLD] 73.3 <R> <C> [BOLD] Exact Match <C> [BOLD] Max Incorrect <C> 1 <C> 1.7 <C> 00.8 <C> 5.9 <C> 26.5 <C> [BOLD] 55.8 <R> <C> [BOLD] Exact Match <C> [BOLD] Max Incorrect <C> 3 <C> 2.3 <C> 9.7 <C> 21.9 <C> 45.5 <C> [BOLD] 69.4 <R> <C> [BOLD] Exact Match <C> [BOLD] Max Incorrect <C> 5 <C> 2.4 <C> 12.4 <C> 26.1 <C> 52.8 <C> [BOLD] 72.4 <R> <C> [BOLD] WordNet Similarity <C> [BOLD] Set Intersection <C> [BOLD] Set Intersection <C> - <C> 28.9 <C> 32.8 <C> 46.1 <C> [BOLD] 53.9 <R> <C> [BOLD] WordNet Similarity <C> [BOLD] Max Answers <C> 1 <C> - <C> 3.8 <C> 20.8 <C> 44.1 <C> [BOLD] 78.4 <R> <C> [BOLD] WordNet Similarity <C> [BOLD] Max Answers <C> 3 <C> - <C> 13.1 <C> 36.0 <C> 56.2 <C> [BOLD] 76.8 <R> <C> [BOLD] WordNet Similarity <C> [BOLD] Max Answers <C> 5 <C> - <C> 16.3 <C> 39.2 <C> 59.6 <C> [BOLD] 76.0 <R> <C> [BOLD] WordNet Similarity <C> [BOLD] Max Answers <C> 10 <C> - <C> 26.7 <C> 44.4 <C> 65.8 <C> [BOLD] 77.0 <R> <C> [BOLD] WordNet Similarity <C> [BOLD] Max Incorrect <C> 1 <C> - <C> 2.1 <C> 19.7 <C> 35.9 <C> [BOLD] 59.0 <R> <C> [BOLD] WordNet Similarity <C> [BOLD] Max Incorrect <C> 3 <C> - <C> 12.8 <C> 34.4 <C> 54.4 <C> [BOLD] 74.0 <R> <C> [BOLD] WordNet Similarity <C> [BOLD] Max Incorrect <C> 5 <C> - <C> 15.9 <C> 39.6 <C> 60.1 <C> [BOLD] 77.9 <CAP> Table 3: Results on the annotated test set.
<R> <C> [BOLD] DUC2004 <C> RG-1 <C> RG-2 <C> RG-L <C> Speed <R> <C> ABS‡ <C> 26.55 <C> 7.06 <C> 22.05 <C> - <R> <C> Feats2s‡ <C> 28.35 <C> 9.46 <C> 24.59 <C> - <R> <C> Selective-Enc‡ <C> 29.21 <C> 9.56 <C> 25.51 <C> - <R> <C> Transformer <C> 28.09 <C> 9.52 <C> 24.91 <C> 1.00× <R> <C> SBSG (beam) <C> 28.77 <C> 10.11 <C> 26.11 <C> 1.48× <R> <C> SBSG (greedy) <C> 28.70 <C> 9.88 <C> 25.93 <C> 2.09× <CAP> Table 3: ROUGE recall evaluation results on DUC 2004 test set. For comparison, we also list results reported by Rush et al. [2015]; Nallapati et al. [2016]; Zhou et al. [2017]. Results with ‡ mark are taken from the corresponding papers. Our proposed SBSG model significant outperforms the conventional Transformer model in terms of both decoding speed and generation quality.
<R> <C> [BOLD] Emb. <C> [BOLD] Method <C> [BOLD] BLEU1 <C> [BOLD] METEOR <C> [BOLD] ROUGE <C> [BOLD] CIDEr <R> <C> Tag <C> AtM <C> 22.4 <C> 8.6 <C> 22.5 <C> 20.8 <R> <C> Tag <C> HM <C> [BOLD] 24.4 <C> [BOLD] 10.8 <C> [BOLD] 24.3 <C> [BOLD] 55.0 <R> <C> Tag <C> AM <C> 24.4 <C> 10.6 <C> 23.9 <C> 49.4 <R> <C> Tag <C> JM <C> 22.2 <C> 10.5 <C> 22.8 <C> 50.1 <R> <C> PlaceCNN <C> AtM <C> 24.4 <C> 10.3 <C> 24.0 <C> 51.8 <R> <C> PlaceCNN <C> HM <C> 24.0 <C> 10.4 <C> 24.3 <C> 49.8 <R> <C> PlaceCNN <C> AM <C> 24.1 <C> 10.6 <C> 24.3 <C> 51.5 <R> <C> PlaceCNN <C> JM <C> [BOLD] 25.7 <C> [BOLD] 10.8 <C> [BOLD] 24.5 <C> [BOLD] 56.1 <R> <C> Diff. Img <C> AtM <C> 20.5 <C> 8.5 <C> [BOLD] 24.4 <C> 19.2 <R> <C> Diff. Img <C> HM <C> 23.6 <C> 8.6 <C> 22.3 <C> 22.0 <R> <C> Diff. Img <C> AM <C> 20.6 <C> 8.5 <C> 24.4 <C> 19.2 <R> <C> Diff. Img <C> JM <C> [BOLD] 30.4 <C> [BOLD] 11.7 <C> 22.3 <C> [BOLD] 22.8 <R> <C> MDN <C> AtM <C> 22.4 <C> 8.8 <C> 24.6 <C> 22.4 <R> <C> MDN <C> HM <C> 26.6 <C> 12.8 <C> 30.1 <C> 31.4 <R> <C> MDN <C> AM <C> 29.6 <C> 15.4 <C> 32.8 <C> 41.6 <R> <C> MDN  [BOLD] (Ours) <C> JM <C> [BOLD] 36.0 <C> [BOLD] 23.4 <C> [BOLD] 41.8 <C> [BOLD] 50.7 <CAP> Table 1: Analysis of variants of our proposed method on VQG-COCO Dataset as mentioned in section 4.4 and different ways of getting a joint embedding (Attention (AtM), Hadamard (HM), Addition (AM) and Joint (JM) method as given in section 4.1.3) for each method. Refer section 5.1 for more details.
<R> <C> [BOLD] Context <C> [BOLD] Meth <C> [BOLD] BLEU-1 <C> [BOLD] Meteor <C> [BOLD] Rouge <C> [BOLD] CIDer <R> <C> Image <C> - <C> 23.2 <C> 8.6 <C> 25.6 <C> 18.8 <R> <C> Caption <C> - <C> 23.5 <C> 8.6 <C> 25.9 <C> 24.3 <R> <C> Tag-n <C> JM <C> 22.2 <C> 10.5 <C> 22.8 <C> 50.1 <R> <C> Tag-n <C> AtM <C> 22.4 <C> 8.6 <C> 22.5 <C> 20.8 <R> <C> Tag-n <C> HM <C> [BOLD] 24.8 <C> [BOLD] 10.6 <C> 24.4 <C> [BOLD] 53.2 <R> <C> Tag-n <C> AM <C> 24.4 <C> 10.6 <C> 23.9 <C> 49.4 <R> <C> Tag-v <C> JM <C> 23.9 <C> 10.5 <C> 24.1 <C> [BOLD] 52.9 <R> <C> Tag-v <C> AtM <C> 22.2 <C> 8.6 <C> 22.4 <C> 20.9 <R> <C> Tag-v <C> HM <C> [BOLD] 24.5 <C> [BOLD] 10.7 <C> 24.2 <C> [BOLD] 52.3 <R> <C> Tag-v <C> AM <C> 24.6 <C> 10.6 <C> 24.1 <C> 49.0 <R> <C> Tag-wh <C> JM <C> 22.4 <C> 10.5 <C> 22.5 <C> 48.6 <R> <C> Tag-wh <C> AtM <C> 22.2 <C> 8.6 <C> 22.4 <C> 20.9 <R> <C> Tag-wh <C> HM <C> [BOLD] 24.6 <C> [BOLD] 10.8 <C> 24.3 <C> [BOLD] 55.0 <R> <C> Tag-wh <C> AM <C> 24.0 <C> 10.4 <C> 23.7 <C> 47.8 <CAP> Table 4: Analysis of different Tags for VQG-COCO-dataset. We analyse noun tag (Tag-n), verb tag (Tag-v) and question tag (Tag-wh) for different fusion methods namely joint, attention, Hadamard and addition based fusion.
<R> <C> [BOLD] Context <C> [BOLD] BLEU-1 <C> [BOLD] Meteor <C> [BOLD] Rouge <C> [BOLD] CIDer <R> <C> Tag-n3-add <C> 22.4 <C> 9.1 <C> 22.2 <C> 26.7 <R> <C> Tag-n3-con <C> [BOLD] 24.8 <C> 10.6 <C> 24.4 <C> [BOLD] 53.2 <R> <C> Tag-n3-joint <C> 22.1 <C> 8.9 <C> 21.7 <C> 24.6 <R> <C> Tag-n3-conv <C> 24.1 <C> 10.3 <C> 24.0 <C> 47.9 <R> <C> Tag-v3-add <C> 24.1 <C> 10.2 <C> 23.9 <C> 46.7 <R> <C> Tag-v3-con <C> 24.5 <C> 10.7 <C> 24.2 <C> [BOLD] 52.3 <R> <C> Tag-v3-joint <C> 22.5 <C> 9.1 <C> 22.1 <C> 25.6 <R> <C> Tag-v3-conv <C> 23.2 <C> 9.0 <C> 24.2 <C> 38.0 <R> <C> Tag-q3-add <C> 24.5 <C> 10.5 <C> 24.4 <C> 51.4 <R> <C> Tag-q3-con <C> 24.6 <C> [BOLD] 10.8 <C> 24.3 <C> [BOLD] 55.0 <R> <C> Tag-q3-joint <C> 22.1 <C> 9.0 <C> 22.0 <C> 25.9 <R> <C> Tag-q3-conv <C> 24.3 <C> 10.4 <C> 24.0 <C> 48.6 <CAP> Table 5: Combination of 3 tags of each category for hadamard mixture model namely addition, concatenation, multiplication and 1d-convolution
<R> <C> [BOLD] Meth <C> [BOLD] Exemplar <C> [BOLD] BLEU-1 <C> [BOLD] Meteor <C> [BOLD] Rouge <C> [BOLD] CIDer <R> <C> AM <C> IE(K=1) <C> 21.8 <C> 7.6 <C> 22.8 <C> 22.0 <R> <C> AM <C> IE(K=2) <C> 22.4 <C> 8.3 <C> 23.4 <C> 16.0 <R> <C> AM <C> IE(K=3) <C> 22.1 <C> 8.8 <C> 24.7 <C> 24.1 <R> <C> AM <C> IE(K=4) <C> 23.7 <C> 9.5 <C> [BOLD] 25.9 <C> 25.2 <R> <C> AM <C> IE(K=5) <C> [BOLD] 24.4 <C> [BOLD] 11.7 <C> 25.0 <C> 27.8 <R> <C> AM <C> IE(K=R) <C> 18.8 <C> 6.4 <C> 20.0 <C> 20.1 <R> <C> HM <C> IE(K=1) <C> 23.6 <C> 7.2 <C> 25.3 <C> 21.0 <R> <C> HM <C> IE(K=2) <C> 23.2 <C> 8.9 <C> [BOLD] 27.8 <C> 22.1 <R> <C> HM <C> IE(K=3) <C> 24.8 <C> 9.8 <C> 27.9 <C> 28.5 <R> <C> HM <C> IE(K=4) <C> 27.7 <C> 9.4 <C> 26.1 <C> [BOLD] 33.8 <R> <C> HM <C> IE(K=5) <C> [BOLD] 28.3 <C> [BOLD] 10.2 <C> 26.6 <C> 31.5 <R> <C> HM <C> IE(K=R) <C> 20.1 <C> 7.7 <C> 20.1 <C> 20.5 <R> <C> JM <C> IE(K=1) <C> 20.1 <C> 7.9 <C> 21.8 <C> 20.9 <R> <C> JM <C> IE(K=2) <C> 22.6 <C> 8.5 <C> 22.4 <C> 28.2 <R> <C> JM <C> IE(K=3) <C> 24.0 <C> 9.2 <C> 24.4 <C> 29.5 <R> <C> JM <C> IE(K=4) <C> 28.7 <C> 10.2 <C> 24.4 <C> 32.8 <R> <C> JM <C> IE(K=5) <C> [BOLD] 30.4 <C> [BOLD] 11.7 <C> [BOLD] 26.3 <C> 38.8 <R> <C> JM <C> IE(K=R) <C> 21.8 <C> 7.4 <C> 22.1 <C> 22.5 <CAP> Table 6: VQG-COCO-dataset, Analysis of different number of Exemplars for addition model, hadamard model and joint model, R is random exemplar. All these experiment are for the differential image network. k=5 performs the best and hence we use this value for the results in main paper.
<R> <C> Messages <C> Fruit <C> Tool 1 <C> Tool 2 <R> <C> Both <C> 37±1.70 <C> 31±1.21 <C> 24±1.07 <R> <C> F <C> 37±1.75 <C> 23.3±0.66 <C> 16.7±0.51 <R> <C> T <C> 14.1±0.79 <C> 32±1.17 <C> 25±1.04 <R> <C> Stats. (%) <C> 5.786±0.00 <C> 8.76±0.01 <C> 7.682±0.01 <CAP> Table 2: Semantic classifier % accuracy mean and SEM over successful training seeds.
<R> <C> [EMPTY] <C> [BOLD] Dev  [BOLD] Goal <C> [BOLD] Dev  [BOLD] Goal <C> [BOLD] Dev  [BOLD] Method <C> [BOLD] Dev  [BOLD] Method <C> [BOLD] Dev  [BOLD] Requested <C> [BOLD] Dev  [BOLD] Requested <C> [BOLD] Test  [BOLD] Goal <C> [BOLD] Test  [BOLD] Goal <C> [BOLD] Test  [BOLD] Method <C> [BOLD] Test  [BOLD] Method <C> [BOLD] Test  [BOLD] Requested <C> [BOLD] Test  [BOLD] Requested <R> <C> [BOLD] Model <C> [BOLD] Acc. <C> [BOLD] L2 <C> [BOLD] Acc. <C> [BOLD] L2 <C> [BOLD] Acc. <C> [BOLD] L2 <C> [BOLD] Acc. <C> [BOLD] L2 <C> [BOLD] Acc. <C> [BOLD] L2 <C> [BOLD] Acc. <C> [BOLD] L2 <R> <C> [ITALIC] LecTrack  <C> 0.63 <C> 0.74 <C> 0.90 <C> 0.19 <C> 0.96 <C> 0.08 <C> 0.62 <C> 0.75 <C> 0.92 <C> 0.15 <C> 0.96 <C> 0.07 <R> <C> [ITALIC] iDST_ASR(r = 1.0) <C> 0.64 <C> 0.53 <C> 0.90 <C> 0.17 <C> 0.96 <C> 0.07 <C> 0.63 <C> 0.56 <C> 0.92 <C> 0.13 <C> 0.97 <C> 0.06 <R> <C> [ITALIC] iDST_TRA(r = 1.0) <C> 0.87 <C> 0.23 <C> 0.94 <C> 0.10 <C> 0.99 <C> 0.02 <C> 0.82 <C> 0.30 <C> 0.94 <C> 0.09 <C> 0.99 <C> 0.02 <R> <C> [ITALIC] iDST_ASR(r = 0.6) <C> 0.57 <C> 0.61 <C> [BOLD] 0.89 <C> 0.18 <C> 0.86 <C> 0.23 <C> 0.56 <C> 0.62 <C> 0.91 <C> 0.14 <C> 0.86 <C> 0.21 <R> <C> [ITALIC] iTTD_ASR(d = 0.85) <C> [BOLD] 0.59 <C> 0.60 <C> 0.88 <C> 0.19 <C> [BOLD] 0.91 <C> 0.16 <C> [BOLD] 0.58 <C> 0.61 <C> 0.91 <C> 0.15 <C> 0.91 <C> 0.15 <R> <C> [ITALIC] iDST_TRA(r = 0.6) <C> 0.77 <C> 0.34 <C> [BOLD] 0.93 <C> 0.11 <C> 0.88 <C> 0.18 <C> 0.73 <C> 0.39 <C> [BOLD] 0.94 <C> 0.10 <C> 0.88 <C> 0.18 <R> <C> [ITALIC] iTTD_TRA(d = 0.85) <C> [BOLD] 0.80 <C> 0.31 <C> 0.92 <C> 0.12 <C> [BOLD] 0.91 <C> 0.15 <C> [BOLD] 0.76 <C> 0.37 <C> 0.93 <C> 0.11 <C> [BOLD] 0.91 <C> 0.15 <CAP> Table 4: Models with ASR suffix have been trained with the ASR 1-best user’s utterance hypothesis. The ones with TRA suffix have been trained using the manual user utterance transcript. Bold indicates which one among iDST and iTTD prevailed in terms of accuracy.
<R> <C> [EMPTY] <C> Relations <C> [BOLD] SN <C> GloVe <C> CBOW <C> skip-gram <R> <C> G <C> semantic <C> 0.84 <C> 0.85 <C> 0.79 <C> 0.73 <R> <C> G <C> syntactic <C> 0.61 <C> 0.65 <C> 0.71 <C> 0.68 <R> <C> G <C> total <C> 0.71 <C> 0.73 <C> 0.74 <C> 0.70 <R> <C> M <C> adjective <C> 0.50 <C> 0.56 <C> 0.58 <C> 0.58 <R> <C> M <C> noun <C> 0.69 <C> 0.70 <C> 0.56 <C> 0.58 <R> <C> M <C> verb <C> 0.48 <C> 0.53 <C> 0.64 <C> 0.56 <R> <C> M <C> total <C> 0.53 <C> 0.57 <C> 0.62 <C> 0.57 <CAP> Table 1: The accuracy on two word analogy task testbeds: G (the GOOGLE testbed); M (the MSR testbed). Performance is close to the state of the art despite using a generative model with provable properties.
<R> <C> [EMPTY] <C> [BOLD] SN <C> GloVe <C> CBOW <C> skip-gram <R> <C> w/o  [BOLD] RD-nn <C> 0.71 <C> 0.73 <C> 0.74 <C> 0.70 <R> <C> [BOLD] RD-nn ( [ITALIC] k = 10) <C> 0.71 <C> 0.74 <C> 0.77 <C> 0.73 <R> <C> [BOLD] RD-nn ( [ITALIC] k = 20) <C> 0.72 <C> 0.75 <C> 0.77 <C> 0.74 <R> <C> [BOLD] RD-nn ( [ITALIC] k = 30) <C> 0.73 <C> 0.76 <C> 0.78 <C> 0.74 <CAP> Table 4: The accuracy of the RD-nn algorithm on the GOOGLE testbed. The algorithm is described in the text. For comparison, the row “w/o RD-nn” shows the accuracy of the old method without using RD-nn.
<R> <C> Method <C> Normalization <C> ja <C> zh <C> hi <C> tr <C> da <C> de <C> es <R> <C> Procrustes <C> None <C> 1.7 <C> 32.5 <C> 33.3 <C> 44.9 <C> 54.0 <C> 73.5 <C> 81.4 <R> <C> [EMPTY] <C> c+l <C> 12.3 <C> 41.1 <C> 34.0 <C> 46.5 <C> 54.9 <C> 74.6 <C> 81.3 <R> <C> [EMPTY] <C> in <C> [BOLD] 44.3 <C> [BOLD] 44.2 <C> [BOLD] 36.7 <C> [BOLD] 48.7 <C> [BOLD] 58.4 <C> [BOLD] 75.5 <C> [BOLD] 81.5 <R> <C> Procrustes + refine <C> None <C> 1.7 <C> 32.5 <C> 33.6 <C> 46.3 <C> 56.8 <C> 74.3 <C> 81.9 <R> <C> [EMPTY] <C> c+l <C> 13.1 <C> 42.3 <C> 34.9 <C> 48.7 <C> 59.3 <C> 75.2 <C> 82.4 <R> <C> [EMPTY] <C> in <C> [BOLD] 44.3 <C> [BOLD] 44.2 <C> [BOLD] 37.7 <C> [BOLD] 51.7 <C> [BOLD] 60.9 <C> [BOLD] 76.0 <C> [BOLD] 82.5 <R> <C> rcsls <C> None <C> 14.6 <C> 17.1 <C> 5.0 <C> 18.3 <C> 19.2 <C> 43.6 <C> 50.5 <R> <C> [EMPTY] <C> c+l <C> 16.1 <C> 45.1 <C> 36.2 <C> 50.7 <C> 58.3 <C> 77.5 <C> 83.6 <R> <C> [EMPTY] <C> in <C> [BOLD] 56.3 <C> [BOLD] 48.6 <C> [BOLD] 38.0 <C> [BOLD] 52.4 <C> [BOLD] 60.5 <C> [BOLD] 78.1 <C> [BOLD] 83.9 <CAP> Table 1: Word translation accuracy aligning English embeddings to seven languages. We combine three normalizations—no normalization (None), mean centering and length normalization (c+l), and Iterative Normalization (in) for five rounds—with three clwes: Procrustes, Procrustes with refinement (Conneau et al., 2018), and rcsls (Joulin et al., 2018). Procrustes with c+l is equivalent to Artetxe et al. (2016). The best result for each clwe in each column in bold. Iterative Normalization has the best accuracy of the three normalization techniques.
<R> <C> [EMPTY] <C> [BOLD] M01 (EMA–IEEE) R <C> [BOLD] M01 (EMA–IEEE) Rn <C> [BOLD] M01 (EMA–IEEE) PCC <C> [BOLD] M1 (USC–TIMIT) R <C> [BOLD] M1 (USC–TIMIT) Rn <C> [BOLD] M1 (USC–TIMIT) PCC <C> [BOLD] FSEW0 (MOCHA) R <C> [BOLD] FSEW0 (MOCHA) Rn <C> [BOLD] FSEW0 (MOCHA) PCC <C> [BOLD] ABX w <C> [BOLD] ABX a <R> <C> [BOLD] Single-corpus <C> 1.79 <C> 0.66 <C> 0.72 <C> 2.05 <C> 1.13 <C> 0.02 <C> 2.72 <C> 1.11 <C> 0.08 <C> 18.9 <C> 25.0 <R> <C> [BOLD] Multi-corpus <C> 1.80 <C> 0.66 <C> 0.71 <C> 1.89 <C> 1.04 <C> 0.14 <C> 2.61 <C> 0.98 <C> 0.22 <C> 19.7 <C> 26.7 <CAP> Table 2: Effects of training one or multiple corpora.
<R> <C> #Words <C> Transformer <C> (Maxout LSTM) <R> <C> 0-50 <C> 14.10 <C> 12.59 <R> <C> 50-100 <C> 13.96 <C> 12.34 <R> <C> 100- <C> 13.93 <C> 12.37 <CAP> Table 6: Our best-performing transformer model (Transformer_iwslt_de_en in Table 4) vs LSTM with maxout pointer on LAQG when the length of answer is in different 50-word bins.
<R> <C> [BOLD] Dataset <C> [BOLD] LM <C> [BOLD] LM+SCDV <C> [BOLD] MB <C> [BOLD] MB + SCDV <R> <C> AP <C> 0.2742 <C> 0.2856 <C> 0.3283 <C> 0.3395 <R> <C> SJM <C> 0.2052 <C> 0.2105 <C> 0.2341 <C> 0.2409 <R> <C> WSJ <C> 0.2618 <C> 0.2705 <C> 0.3027 <C> 0.3126 <R> <C> Robust04 <C> 0.2516 <C> 0.2684 <C> 0.2819 <C> 0.2933 <CAP> Table 6: Mean average precision (MAP) for IR on four IR datasets
<R> <C> [EMPTY] <C> [BOLD] med <C> [BOLD] mim <C> [BOLD] bio <C> [BOLD] MRD <C> [BOLD] 2-MRD <C> [BOLD] 0-step <C> [BOLD] 2-step <C> [BOLD] r-step <C> [BOLD] UMLS::SenseRelate <R> <C> [BOLD] Accuracy C <C> 0.80 <C> 0.69 <C> 0.84 <C> 0.81 <C> 0.78 <C> 0.82 <C> 0.86 <C> 0.89 <C> 0.75 <R> <C> [BOLD] Accuracy U <C> 0.72 <C> 0.63 <C> 0.75 <C> - <C> - <C> - <C> - <C> - <C> - <CAP> Table 2: Results using constrained (C) and unconstrained (U) terms.
<R> <C> [BOLD] Term <C> [BOLD] Accuracy <R> <C> [BOLD] DE <C> 0.31 <R> <C> [BOLD] Hemlock <C> 0.4 <R> <C> [BOLD] Brucella Abortus <C> 0.46 <R> <C> [BOLD] WT1 <C> 0.46 <R> <C> [BOLD] Murine Sarcoma Virus <C> 0.47 <CAP> Table 3: The 5 lowest-performing terms.
<R> <C> Methods <C> Input <C> Pre-training Data <C> B-3 <C> B-4 <C> M <C> R-L <C> CIDEr <R> <C> Bi-LSTM Zhou et al. ( 2018a ) <C> Video <C> 0 <C> - <C> 0.87 <C> 8.15 <C> - <C> - <R> <C> EMT Zhou et al. ( 2018b ) <C> Video <C> 0 <C> - <C> 4.38 <C> 11.55 <C> 27.44 <C> 0.38 <R> <C> VideoBERT Sun et al. ( 2019b ) <C> Video <C> 312K <C> 6.80 <C> 4.04 <C> 11.01 <C> 27.50 <C> 0.49 <R> <C> VideoBERT (+S3D) Sun et al. ( 2019b ) <C> Video <C> 312K <C> 7.59 <C> 4.33 <C> 11.94 <C> 28.80 <C> 0.55 <R> <C> CBT Sun et al. ( 2019a ) <C> Video <C> 1.2M <C> - <C> 5.12 <C> 12.97 <C> 30.44 <C> 0.64 <R> <C> DPC Shi et al. ( 2019 ) <C> Video + Transcript <C> 0 <C> 7.60 <C> 2.76 <C> [BOLD] 18.08 <C> - <C> - <R> <C> AT+Video Hessel et al. ( 2019 ) <C> Video + Transcript <C> 0 <C> - <C> 9.01 <C> 17.77 <C> 36.65 <C> 1.12 <R> <C> Our model.1st <C> Video <C> 380K <C> 10.16 <C> 6.06 <C> 12.47 <C> 31.48 <C> 0.6430 <R> <C> Our model.2nd <C> Video + Transcript <C> 0 <C> 13.57 <C> 8.67 <C> 15.38 <C> 35.18 <C> 1.0015 <R> <C> Our model.3rd <C> Video + Transcript <C> 200K <C> 14.97 <C> 9.92 <C> 16.24 <C> 37.07 <C> 1.1554 <R> <C> Our model.4th (no decoder) <C> Video + Transcript <C> 380K <C> 14.43 <C> 9.78 <C> 15.81 <C> 36.84 <C> 1.1043 <R> <C> Our model.5th <C> Video + Transcript <C> 380K <C> [BOLD] 15.52 <C> [BOLD] 10.42 <C> 16.93 <C> [BOLD] 38.02 <C> [BOLD] 1.1998 <CAP> Table 3: The multimodal video captioning results on Youcook2 dataset.
<R> <C> [BOLD] Model <C> [BOLD] HGZHZ  [BOLD] Change Rate <C> [BOLD] HGZHZ  [BOLD] Change Rate <C> [BOLD] HGZHZ  [BOLD] Change Rate <C> [BOLD] HGZHZ  [BOLD] Accurate Change Rate <C> [BOLD] HGZHZ  [BOLD] Accurate Change Rate <C> [BOLD] HGZHZ  [BOLD] Accurate Change Rate <C> [BOLD] Friends  [BOLD] Change Rate <C> [BOLD] Friends  [BOLD] Change Rate <C> [BOLD] Friends  [BOLD] Change Rate <C> [BOLD] Friends  [BOLD] Accurate Change Rate <C> [BOLD] Friends  [BOLD] Accurate Change Rate <C> [BOLD] Friends  [BOLD] Accurate Change Rate <R> <C> [BOLD] Model <C> [BOLD] All <C> [BOLD] Last1 <C> [BOLD] Last2 <C> [BOLD] All <C> [BOLD] Last1 <C> [BOLD] Last2 <C> [BOLD] All <C> [BOLD] Last1 <C> [BOLD] Last2 <C> [BOLD] All <C> [BOLD] Last1 <C> [BOLD] Last2 <R> <C> MemNet <C> 92.98 <C> 31.78 <C> 37.46 <C> 62.19 <C> 1.17 <C> 2.92 <C> 93.27 <C> 19.22 <C> 27.53 <C> 78.23 <C> 0.91 <C> 7.66 <R> <C> + multi <C> 98.69 <C> 77.87 <C> 81.96 <C> [BOLD] 83.82 <C> 3.40 <C> 10.74 <C> 95.31 <C> 28.09 <C> 36.65 <C> [BOLD] 87.28 <C> 0.69 <C> 7.63 <R> <C> TAware <C> 94.38 <C> 68.33 <C> 71.86 <C> [BOLD] 78.88 <C> 1.95 <C> 9.26 <C> 92.93 <C> 26.52 <C> 30.96 <C> [BOLD] 88.07 <C> 0.35 <C> 10.63 <R> <C> + multi <C> 97.74 <C> 76.68 <C> 81.00 <C> [BOLD] 95.30 <C> 4.03 <C> 10.75 <C> 98.31 <C> 68.87 <C> 68.22 <C> [BOLD] 92.29 <C> 0.87 <C> 10.09 <R> <C> KAware <C> 96.91 <C> 90.89 <C> 96.91 <C> 64.80 <C> 13.06 <C> 7.22 <C> 90.93 <C> 50.92 <C> 61.08 <C> 75.57 <C> 2.77 <C> 10.00 <R> <C> Qadpt <C> 95.65 <C> 77.33 <C> 78.68 <C> 59.01 <C> [BOLD] 66.67 <C> [BOLD] 16.82 <C> 92.34 <C> 38.62 <C> 36.96 <C> 81.24 <C> [BOLD] 30.85 <C> [BOLD] 16.87 <R> <C> + multi <C> 99.60 <C> 83.17 <C> 87.27 <C> 56.11 <C> [BOLD] 61.92 <C> [BOLD] 18.54 <C> 98.47 <C> 48.78 <C> 63.54 <C> 86.97 <C> [BOLD] 26.17 <C> [BOLD] 17.31 <R> <C> + TAware <C> 99.02 <C> 83.14 <C> 85.59 <C> 58.82 <C> [BOLD] 64.12 <C> [BOLD] 14.90 <C> 98.45 <C> 56.77 <C> 65.25 <C> 82.52 <C> [BOLD] 28.34 <C> [BOLD] 17.68 <CAP> Table 4: The results of change rate and accurate change rate.
<R> <C> [EMPTY] <C> SVM weight <C> Top words <R> <C> Aspect 2 <C> 4.01 <C> sharon israel arafat palestinian gaza american prime arab new us bush israeli <R> <C> Topic 17 <C> 5.84 <C> peace process year two violence state security leader settlement way term terrorism <R> <C> Topic 20 <C> 2.72 <C> attack war terrorism suicide violence conflict civilian force bombing victory terror act <R> <C> Topic 19 <C> 1.71 <C> conflict administration president war region policy world leader minister pressure hand peace <R> <C> Topic 6 <C> 0.54 <C> election government party coalition public issue leadership majority position opposition campaign year <R> <C> Topic 10 <C> 0.39 <C> plan disengagement security withdrawal settlement part territory border state minister intention order <R> <C> Topic 8 <C> -0.32 <C> minister leader leadership one time intelligence security organization decision authority day faction <CAP> Table 4: Topics that co-occur with Aspect 2 with a frequency greater than 0.7. Learned by CorrLDA2 on the (opinion+ne) partition with T=20 topics and ~T=2 aspects.
<R> <C> Datasets Method <C> TMC 16bit <C> TMC 32bit <C> TMC 64bit <C> TMC 128bit <C> 20Newsgroups 16bit <C> 20Newsgroups 32bit <C> 20Newsgroups 64bit <C> 20Newsgroups 128bit <C> Reuters 16bit <C> Reuters 32bit <C> Reuters 64bit <C> Reuters 128bit <R> <C> KSH <C> 0.6842 <C> 0.7047 <C> 0.7175 <C> 0.7243 <C> 0.5559 <C> 0.6103 <C> 0.6488 <C> 0.6638 <C> 0.8376 <C> 0.8480 <C> 0.8537 <C> 0.8620 <R> <C> SHTTM <C> 0.6571 <C> 0.6485 <C> 0.6893 <C> 0.6474 <C> 0.3235 <C> 0.2357 <C> 0.1411 <C> 0.1299 <C> 0.8520 <C> 0.8323 <C> 0.8271 <C> 0.8150 <R> <C> VDSH-S <C> 0.7887 <C> 0.7883 <C> 0.7967 <C> 0.8018 <C> 0.6791 <C> 0.7564 <C> 0.6850 <C> 0.6916 <C> 0.9121 <C> 0.9337 <C> 0.9407 <C> 0.9299 <R> <C> NASH-DN-S <C> 0.7946 <C> 0.7987 <C> 0.8014 <C> 0.8139 <C> 0.6973 <C> 0.8069 <C> 0.8213 <C> 0.7840 <C> 0.9327 <C> 0.9380 <C> 0.9427 <C> 0.9336 <R> <C> GMSH-S <C> 0.7806 <C> 0.7929 <C> 0.8103 <C> 0.8144 <C> 0.6972 <C> 0.7426 <C> 0.7574 <C> 0.7690 <C> 0.9144 <C> 0.9175 <C> 0.9414 <C> 0.9522 <R> <C> BMSH-S <C> [BOLD] 0.8051 <C> [BOLD] 0.8247 <C> [BOLD] 0.8340 <C> [BOLD] 0.8310 <C> [BOLD] 0.7316 <C> [BOLD] 0.8144 <C> [BOLD] 0.8216 <C> [BOLD] 0.8183 <C> [BOLD] 0.9350 <C> [BOLD] 0.9640 <C> [BOLD] 0.9633 <C> [BOLD] 0.9590 <CAP> Table 2: The performances of different supervised hashing models on three datasets under different lengths of hashing codes.
<R> <C> Architecture <C> CV F1 <R> <C> Baselines <C> [EMPTY] <R> <C> SVM <C> 0.697 <R> <C> CRF <C> 0.713 <R> <C> dasigi2017experiment <C> 0.746 <R> <C> Varying Embeddings <C> [EMPTY] <R> <C> BioW2V (dasigi2017experiment) <C> 0.746 <R> <C> BioGloVe <C> 0.772 <R> <C> BioBERT (Best Model) <C> 0.784 <R> <C> Varying Attention Context Encoder <C> [EMPTY] <R> <C> No Attention <C> 0.666 <R> <C> No Context <C> 0.777 <R> <C> RNN <C> 0.778 <R> <C> LSTM (Best Model) <C> 0.784 <R> <C> Varying Sequence Tagger <C> [EMPTY] <R> <C> CRF <C> 0.768 <R> <C> LSTM <C> 0.771 <R> <C> BiLSTM <C> 0.780 <R> <C> BiLSTM+CRF (Best Model) <C> 0.784 <CAP> Table 2: Main results including baseline models and ablations of our model.
<R> <C> [BOLD] Label <C> [BOLD] Count <R> <C> result <C> 2053 <R> <C> method <C> 1542 <R> <C> implication <C> 797 <R> <C> fact <C> 732 <R> <C> hypothesis <C> 514 <R> <C> goal <C> 225 <R> <C> problem <C> 193 <R> <C> none <C> 68 <CAP> Table 3: Count of Discourse Types
<R> <C> AM Setup <C> WER <C> Params <C> Size <C> RT50 <R> <C> LSTM 2,000 CD States <C> 23.4 <C> 9.9M <C> 39.4 MB <C> 2.94 <R> <C> LSTM CTC CI Phones <C> 19.4 <C> 9.7M <C> 38.8 MB <C> 0.64 <R> <C> + sMBR <C> 15.1 <C> 9.7M <C> 38.8 MB <C> 0.65 <R> <C> + SVD Compression <C> 14.8 <C> 3M <C> 11.9 MB <C> 0.22 <R> <C> + adaptation <C> 12.9 <C> 3M <C> 11.9 MB <C> 0.22 <R> <C> + quantization <C> 13.5 <C> 3M <C> 3 MB <C> 0.14 <R> <C> LSTM CTC (Server-size) <C> 11.3 <C> 20.1M <C> 80.4 MB <C> - <CAP> Table 1: Word Error Rates (%) on an open-ended dictation task, evaluating various acoustic models, using the same language model described in Section 4, along with median RT factor.
<R> <C> Method <C> DUC2005 ROUGE-2 <C> DUC2005 ROUGE-SU4 <C> DUC2006 ROUGE-2 <C> DUC2006 ROUGE-SU4 <C> DUC2007 ROUGE-2 <C> DUC2007 ROUGE-SU4 <R> <C> Hum <C> 0.0897 <C> 0.151 <C> 0.13260 <C> 0.18385 <C> 0.17528 <C> 0.21892 <R> <C> [BOLD] TL-TranSum <C> [BOLD] 0.077392 <C> 0.12869 <C> [BOLD] 0.10779 <C> [BOLD] 0.15854 <C> [BOLD] 0.12997 <C> [BOLD] 0.17995 <R> <C> 1st <C> 0.07251 <C> [BOLD] 0.13163 <C> 0.09558 <C> 0.15529 <C> 0.12448 <C> 0.17711 <R> <C> 2nd <C> 0.07174 <C> 0.12972 <C> 0.09097 <C> 0.14733 <C> 0.12028 <C> 0.17074 <R> <C> 3rd <C> 0.06984 <C> 0.12525 <C> 0.08987 <C> 0.14755 <C> 0.11887 <C> 0.16999 <R> <C> 4th <C> 0.06963 <C> 0.12795 <C> 0.08954 <C> 0.14607 <C> 0.11793 <C> 0.17593 <R> <C> Syst. Av. <C> 0.05842 <C> 0.11205 <C> 0.07463 <C> 0.13021 <C> 0.09597 <C> 0.14884 <R> <C> Basel. <C> 0.04026 <C> 0.08716 <C> 0.04947 <C> 0.09788 <C> 0.06039 <C> 0.10507 <CAP> Table 3: Comparison with DUC2005, DUC2006 and DUC2007 systems
<R> <C> [BOLD] Team sabir <C> [BOLD] Run sabir.meta.docs <C> Type automatic <C> nDCG@10 0.6080 <C> P@5 0.7800 <C> mAP 0.3128 <C> [BOLD] Round 1: 30 topics <C> [BOLD] Round 1: 30 topics run2† <C> automatic <C> 0.6032 <C> 0.6867 <C> 0.2601 <C> 0.8800 <R> <C> covidex <C> T5R1 (= monoT5) <C> automatic <C> 0.5223 <C> 0.6467 <C> 0.2838 <C> [BOLD] Round 2: 35 topics <C> [BOLD] Round 2: 35 topics <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> mpiid5 <C> mpiid5_run3† <C> manual <C> 0.6893 <C> 0.8514 <C> 0.3380 <C> [EMPTY] <C> SparseDenseSciBert† <C> feedback <C> 0.6772 <C> 0.7600 <C> 0.3115 <C> 1.0000 <R> <C> GUIR_S2 <C> GUIR_S2_run1† <C> automatic <C> 0.6251 <C> 0.7486 <C> 0.2842 <C> [EMPTY] <C> covidex.t5 (= monoT5) <C> automatic <C> 0.6250 <C> 0.7314 <C> 0.2880 <C> 1.0000 <R> <C> anserini <C> r2.fusion2 <C> automatic <C> 0.5553 <C> 0.6800 <C> 0.2725 <C> [EMPTY] <C> r2.fusion1 <C> automatic <C> 0.4827 <C> 0.6114 <C> 0.2418 <C> 1.0000 <R> <C> [BOLD] Round 3: 40 topics <C> [BOLD] Round 3: 40 topics <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> covidex <C> r3.t5_lr <C> feedback <C> 0.7740 <C> 0.8600 <C> 0.3333 <C> [EMPTY] <C> BioInfo-run1 <C> feedback <C> 0.7715 <C> 0.8650 <C> 0.3188 <C> 0.9750 <R> <C> SFDC <C> SFDC-fus12-enc23-tf3† <C> automatic <C> 0.6867 <C> 0.7800 <C> 0.3160 <C> [EMPTY] <C> r3.duot5 (= monoT5 + duoT5) <C> automatic <C> 0.6626 <C> 0.7700 <C> 0.2676 <C> 0.8850 <R> <C> covidex <C> r3.monot5 (= monoT5) <C> automatic <C> 0.6596 <C> 0.7800 <C> 0.2635 <C> [EMPTY] <C> r3.fusion2 <C> automatic <C> 0.6100 <C> 0.7150 <C> 0.2641 <C> 0.9550 <R> <C> anserini <C> r3.fusion1 <C> automatic <C> 0.5359 <C> 0.6100 <C> 0.2293 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 1: Selected TREC-COVID results. Our submissions are under teams “covidex” and “anserini”. All runs notated with † incorporate our infrastructure components in some way.
<R> <C> [EMPTY] <C> [ITALIC] Survey <C> [ITALIC] Tutorial <C> [ITALIC] Resource <C> [ITALIC] Reference Work <C> [ITALIC] Empirical Results <C> [ITALIC] Software Manual <C> [ITALIC] Other <C> Total <R> <C> [ITALIC] Sur. <C> 10 <C> 1 <C> 0 <C> 7 <C> 4 <C> 0 <C> 5 <C> 27 <R> <C> [ITALIC] Tut. <C> 2 <C> 44 <C> 6 <C> 22 <C> 6 <C> 4 <C> 14 <C> 98 <R> <C> [ITALIC] Res. <C> 0 <C> 0 <C> 5 <C> 1 <C> 1 <C> 3 <C> 5 <C> 15 <R> <C> [ITALIC] Ref. <C> 36 <C> 20 <C> 3 <C> 151 <C> 4 <C> 1 <C> 28 <C> 243 <R> <C> [ITALIC] Emp. <C> 13 <C> 8 <C> 8 <C> 15 <C> 526 <C> 3 <C> 56 <C> 629 <R> <C> [ITALIC] Sof. <C> 0 <C> 1 <C> 0 <C> 0 <C> 2 <C> 1 <C> 2 <C> 6 <R> <C> [ITALIC] Other <C> 12 <C> 24 <C> 6 <C> 47 <C> 29 <C> 2 <C> 107 <C> 227 <R> <C> Total <C> 73 <C> 98 <C> 28 <C> 243 <C> 572 <C> 14 <C> 217 <C> 1245 <CAP> Table 1: Confusion matrix for annotated pedagogical roles from documents with only one majority role. Rows are the majority roles (chosen by two or three annotators) that we treat as ground truth. Columns are the third annotator’s corresponding annotations.
<R> <C> Ped. role <C> Precision RF <C> Precision CEN <C> Precision KNN+BoSEC <C> Precision KNN+BoSEC+ <C> Recall RF <C> Recall CEN <C> Recall KNN+BoSEC <C> Recall KNN+BoSEC+ <C> [ITALIC] F1 RF <C> [ITALIC] F1 CEN <C> [ITALIC] F1 KNN+BoSEC <C> [ITALIC] F1 KNN+BoSEC+ <C> Support All <R> <C> [ITALIC] Survey <C> 0 <C> 0.02 <C> 0.23 <C> [BOLD] 0.31 <C> 0 <C> [BOLD] 0.21 <C> 0.20 <C> 0.18 <C> 0 <C> 0.03 <C> 0.21 <C> [BOLD] 0.23 <C> 13 <R> <C> [ITALIC] Tutorial <C> 0.50 <C> 0.10 <C> 0.64 <C> [BOLD] 0.66 <C> 0.05 <C> 0.21 <C> [BOLD] 0.55 <C> 0.52 <C> 0.08 <C> 0.11 <C> 0.57 <C> [BOLD] 0.58 <C> 23 <R> <C> [ITALIC] Resource <C> 0.20 <C> 0 <C> [BOLD] 0.70 <C> 0.53 <C> 0.01 <C> 0 <C> 0.19 <C> [BOLD] 0.24 <C> 0.03 <C> 0 <C> 0.29 <C> [BOLD] 0.32 <C> 17.8 <R> <C> [ITALIC] Ref. Work <C> 0.77 <C> 0.07 <C> 0.71 <C> [BOLD] 0.78 <C> 0.33 <C> 0.32 <C> 0.70 <C> [BOLD] 0.71 <C> 0.46 <C> 0.11 <C> 0.70 <C> [BOLD] 0.74 <C> 49.4 <R> <C> [ITALIC] Emp. Res. <C> [BOLD] 0.86 <C> 0 <C> 0.83 <C> 0.85 <C> 0.77 <C> 0 <C> 0.86 <C> [BOLD] 0.89 <C> 0.81 <C> 0 <C> 0.85 <C> [BOLD] 0.87 <C> 131.4 <R> <C> [ITALIC] Sof. Man. <C> [BOLD] 0.98 <C> 0.05 <C> 0.93 <C> 0.95 <C> 0.34 <C> 0.16 <C> 0.72 <C> [BOLD] 0.86 <C> 0.49 <C> 0.07 <C> 0.81 <C> [BOLD] 0.90 <C> 18 <R> <C> [ITALIC] Other <C> 0.63 <C> 0.06 <C> 0.57 <C> [BOLD] 0.65 <C> 0.10 <C> 0.40 <C> 0.27 <C> [BOLD] 0.48 <C> 0.17 <C> 0.10 <C> 0.36 <C> [BOLD] 0.55 <C> 45.2 <R> <C> avg / total <C> 0.71 <C> 0.03 <C> 0.73 <C> [BOLD] 0.76 <C> 0.44 <C> 0.15 <C> 0.64 <C> [BOLD] 0.70 <C> 0.50 <C> 0.05 <C> 0.66 <C> [BOLD] 0.72 <C> 297.8 <CAP> Table 2: Precision, recall, and F1 scores by pedagogical roles for all methods. Support is the actual number of documents with each role. avg / total computes weighted averages of scores across all roles. All values are averaged over a five-fold cross validation.
<R> <C> [EMPTY] <C> [ITALIC] Survey <C> [ITALIC] Tutorial <C> [ITALIC] Resource <C> [ITALIC] Reference Work <C> [ITALIC] Empirical Results <C> [ITALIC] Software Manual <C> [ITALIC] Other <C> No prediction <C> Total <R> <C> [ITALIC] Sur. <C> 2 <C> 0.2 <C> 0 <C> 0.8 <C> 4.2 <C> 0 <C> 1.2 <C> 1.6 <C> 10 <R> <C> [ITALIC] Tut. <C> 0.2 <C> 9.4 <C> 0 <C> 2.6 <C> 0.8 <C> 0.8 <C> 2 <C> 2.2 <C> 18 <R> <C> [ITALIC] Res. <C> 0.2 <C> 0 <C> 1.2 <C> 0 <C> 3 <C> 0 <C> 0.6 <C> 1.8 <C> 6.8 <R> <C> [ITALIC] Ref. <C> 1.2 <C> 1.6 <C> 0.2 <C> 34 <C> 3.6 <C> 0.2 <C> 3.2 <C> 4.2 <C> 48.2 <R> <C> [ITALIC] Emp. <C> 0.8 <C> 1.4 <C> 1.8 <C> 3 <C> 109.2 <C> 0 <C> 4 <C> 4.2 <C> 124.4 <R> <C> [ITALIC] Sof. <C> 0 <C> 1.6 <C> 0.6 <C> 0.2 <C> 0 <C> 9.8 <C> 0 <C> 0.4 <C> 12.6 <R> <C> [ITALIC] Oth. <C> 3.4 <C> 0.6 <C> 0.6 <C> 3.2 <C> 8 <C> 0 <C> 21.8 <C> 7.6 <C> 45.2 <R> <C> Tot. <C> 7.8 <C> 14.8 <C> 4.4 <C> 43.8 <C> 128.8 <C> 10.8 <C> 32.8 <C> 22 <C> 265.2 <CAP> Table 3: Ground truth pedagogical roles (rows) versus predicted roles (columns) using KNN+BoSEC+. We calculate the confusion matrix for documents with only one ground truth role. All values are averaged over a five-fold cross validation.
<R> <C> [EMPTY] <C> ACC <C> Dist <R> <C> silfverberg-etal-2017-data* <C> 92.97 <C> 0.170 <R> <C> wu-etal-2018-hard <C> 93.60 <C> 0.128 <R> <C> wu-cotterell-2019-exact <C> 94.40 <C> 0.113 <R> <C> wu-cotterell-2019-exact (Our eval) <C> 94.81 <C> 0.123 <R> <C> makarov-etal-2017-align* <C> 95.12 <C> 0.100 <R> <C> bergmanis-etal-2017-training* <C> 95.32 <C> 0.100 <R> <C> Transformer (Dropout = 0.3) <C> [BOLD] 95.59 <C> [BOLD] 0.088 <R> <C> Transformer (Dropout = 0.1) <C> 95.56 <C> 0.090 <CAP> Table 2: Average test performance on morphological inflection of Transformer against models from the literature. ∗ denotes model ensembling.
<R> <C> [EMPTY] <C> WER <C> PER <C> ACC <C> MFS <R> <C> wu-etal-2018-hard <C> 28.20 <C> [BOLD] 0.068 <C> 41.10 <C> 0.894 <R> <C> wu-cotterell-2019-exact <C> 28.20 <C> 0.069 <C> 41.20 <C> 0.895 <R> <C> Transformer (Dropout = 0.3) <C> 28.08 <C> 0.070 <C> [BOLD] 43.39 <C> [BOLD] 0.897 <R> <C> Transformer (Dropout = 0.1) <C> [BOLD] 27.63 <C> 0.069 <C> 41.35 <C> 0.891 <CAP> Table 4: Average test performance on Grapheme-to-Phoneme and dev performance on Transliteration of Transformer against models from the literature.
<R> <C> Question type Percentage <C> Question type Percentage <C> Question type Percentage <C> What (46%) <C> Color (14%) <C> Where (17%) <C> Number (8%) <C> How (3%) <C> Who (5%) <C> When (4%) <C> Why (3%) <C> Overall (100%) <R> <C> u-GN <C> u-GN <C> u-GN <C> u-GN <C> u-GN <C> u-GN <C> u-GN <C> u-GN <C> u-GN <C> u-GN <C> u-GN <C> u-GN <R> <C> NG <C> + <C> (q, c) <C> 40.3 <C> 50.6 <C> 36.2 <C> 52.0 <C> 41.1 <C> 37.6 <C> 83.2 <C> 39.5 <C> 43.3 <R> <C> NG <C> + <C> (i, q, c) <C> 57.8 <C> 59.5 <C> 59.1 <C> 55.5 <C> 45.4 <C> 56.6 <C> 84.6 <C> 48.3 <C> 58.3 <R> <C> NM(N) <C> + <C> (i, q, c) <C> 59.4 <C> 58.2 <C> 60.3 <C> 63.4 <C> 54.3 <C> 66.6 <C> 85.3 <C> 48.1 <C> 60.5 <R> <C> VG(N) <C> + <C> (q, c) <C> 61.6 <C> 54.0 <C> 62.4 <C> 58.6 <C> 45.9 <C> 63.9 <C> 83.2 <C> 50.3 <C> 60.5 <R> <C> VG(N) <C> + <C> (i, q, c) <C> 61.1 <C> 61.4 <C> 62.3 <C> 59.4 <C> 54.3 <C> 67.5 <C> 85.3 <C> 48.9 <C> 61.9 <R> <C> VG(N, A) <C> + <C> (i, q, c) <C> 61.4 <C> 63.8 <C> 62.6 <C> 61.5 <C> 54.8 <C> 67.5 <C> 84.8 <C> 49.6 <C> 62.6 <R> <C> f-GN <C> f-GN <C> f-GN <C> f-GN <C> f-GN <C> f-GN <C> f-GN <C> f-GN <C> f-GN <C> f-GN <C> f-GN <C> f-GN <R> <C> NG <C> + <C> (q, c) <C> 41.4 <C> 52.6 <C> 38.7 <C> 53.4 <C> 42.2 <C> 39.2 <C> 83.4 <C> 40.2 <C> 44.8 <R> <C> NG <C> + <C> (i, q, c) <C> 58.7 <C> 61.0 <C> 60.4 <C> 57.4 <C> 47.1 <C> 57.6 <C> 85.8 <C> 49.8 <C> 59.4 <R> <C> NM(N) <C> + <C> (i, q, c) <C> 58.7 <C> 60.8 <C> 60.4 <C> 60.1 <C> 47.2 <C> 61.8 <C> 84.8 <C> 49.0 <C> 60.0 <R> <C> VG(N) <C> + <C> (q, c) <C> 60.9 <C> 53.6 <C> 62.0 <C> 58.1 <C> 46.2 <C> 63.3 <C> 83.7 <C> 50.9 <C> 60.1 <R> <C> VG(N) <C> + <C> (i, q, c) <C> 60.2 <C> 60.4 <C> 61.8 <C> 58.5 <C> 47.4 <C> 63.8 <C> 85.1 <C> 49.6 <C> 60.7 <R> <C> VG(N, A) <C> + <C> (i, q, c) <C> 61.0 <C> 64.4 <C> 62.4 <C> 58.8 <C> 48.2 <C> 64.2 <C> 85.6 <C> 51.2 <C> 62.5 <CAP> Table 2: Visual QA accuracy (%) on different question types. VG: Visual Genome [Krishna et al.(2017)Krishna, Zhu, Groth, Johnson, Hata, Kravitz, Chen, Kalantidis, Li, Shamma, et al.] graphs. NM: neural motifs [Zellers et al.(2018)Zellers, Yatskar, Thomson, and Choi] graphs. NG: no graphs. N: node names. A: node attributes.
<R> <C> [BOLD] Domain <C> [BOLD] Candidate Selection Recall <C> [BOLD] Disambiguation Accuracy <R> <C> Movie <C> 90.33 <C> 60.97 <R> <C> Book <C> 94.73 <C> 61.05 <CAP> Table 2: Implicit Entity Linking Performance
<R> <C> [BOLD] System <C> [BOLD] F-Score <C> [BOLD] WuPalmer <C> [BOLD] SemFac <C> [BOLD] Frag <R> <C> Plain <C> 48.2 <C> 76.7 <C> 54.8 <C> 75 <R> <C> Pre <C> 47.9 <C> 75.3 <C> 54.4 <C> 67 <R> <C> Prog <C> 52.0 <C> 78.9 <C> 60.4 <C> 72 <R> <C> Comb <C> 53.2 <C> 79.1 <C> 61.8 <C> 65 <R> <C> Fixed <C> 58.7 <C> 81.8 <C> 68.5 <C> 97 <R> <C> SupWSD <C> 67.3 <C> 81.8 <C> 70.8 <C> - <CAP> Table 1: Results from evaluation on SemEval-2015 [21]. We compare accuracy, Wu-Palmer and semantic factor agreement.
<R> <C> 90No re-rank <C> [EMPTY] <C> Model Baseline <C> C distinct-1 0.0305 <C> C distinct-2 0.1402 <C> OS distinct-1 0.0175 <C> OS distinct-2 0.1205 <C> C BLEU 0.0096 <C> OS BLEU 0.094 <R> <C> 90No re-rank <C> [EMPTY] <C> ECM <C> 0.0310 <C> 0.1412 <C> 0.0180 <C> 0.1263 <C> 0.0099 <C> 0.099 <R> <C> 90No re-rank <C> [EMPTY] <C> SEE <C> 0.0272 <C> 0.1331 <C> 0.0170 <C> 0.1100 <C> 0.0110 <C> 0.093 <R> <C> 90No re-rank <C> [EMPTY] <C> SED <C> 0.0303 <C> 0.1502 <C> 0.0189 <C> 0.1231 <C> 0.0128 <C> 0.103 <R> <C> 90No re-rank <C> [EMPTY] <C> WI <C> 0.0316 <C> 0.1480 <C> 0.0175 <C> 0.1235 <C> 0.0129 <C> 0.100 <R> <C> 90No re-rank <C> [EMPTY] <C> WE <C> 0.0310 <C> 0.1400 <C> 0.0195 <C> 0.1302 <C> 0.0098 <C> 0.095 <R> <C> 90No re-rank <C> [EMPTY] <C> WI + WE <C> [BOLD] 0.0342 <C> [BOLD] 0.1530 <C> [BOLD] 0.0198 <C> [BOLD] 0.1300 <C> [BOLD] 0.0108 <C> [BOLD] 0.105 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [BOLD] (+12.1%) <C> [BOLD] (+9.1%) <C> [BOLD] (+13.1%) <C> [BOLD] (+7.9%) <C> [BOLD] (+12.5%) <C> [BOLD] (+11.7%) <R> <C> 90Re-rank <C> [EMPTY] <C> MMI [ITALIC] baseline <C> 0.0379 <C> 0.1473 <C> 0.0200 <C> 0.1403 <C> 0.0130 <C> 0.105 <R> <C> 90Re-rank <C> [EMPTY] <C> EMOTICONS [ITALIC] γ=0 <C> [BOLD] 0.0406 <C> [BOLD] 0.2030 <C> [BOLD] 0.0305 <C> [BOLD] 0.1431 <C> [BOLD] 0.0140 <C> [BOLD] 0.110 <R> <C> 90Re-rank <C> [EMPTY] <C> [EMPTY] <C> [BOLD] (+7.1%) <C> [BOLD] (+37.8%) <C> [BOLD] (+52.5%) <C> [BOLD] (+2.0%) <C> [BOLD] (+7.7%) <C> [BOLD] (+4.8%) <CAP> Table 2: Quantitative results. Results for all proposed models trained on Cornell (C) and OpenSubtitles (OS). distinct-1 and distinct-2 count the number of distinct unigrams and bigrams, respectively, normalized by the total number of generated tokens in 200 candidate responses. The performance boost is computed with respect to the vanilla seq2seq model.
<R> <C> [BOLD] Model <C> [BOLD] Ans. F1 <C> [BOLD] UA. F1 <C> [BOLD] Best F1 <R> <C> No Answer <C> 0.0 <C> 100.0 <C> 32.4 ±0.0 <R> <C> [ITALIC] ZERO-SHOT <C> [ITALIC] ZERO-SHOT <C> [ITALIC] ZERO-SHOT <C> [ITALIC] ZERO-SHOT <R> <C> M-Bert <C> 11.4 <C> [BOLD] 92.6 <C> 37.8 ±2.0 <R> <C> Xlm <C> 13.6 <C> 89.6 <C> 38.2 ±1.8 <R> <C> Xlm-R <C> 26.4 <C> 85.2 <C> 45.5 ±1.4 <R> <C> [ITALIC] TRANSLATE-TEST <C> [ITALIC] TRANSLATE-TEST <C> [ITALIC] TRANSLATE-TEST <C> [ITALIC] TRANSLATE-TEST <R> <C> M-Bert <C> 18.0 <C> 90.1 <C> 41.4 ±2.2 <R> <C> Xlm <C> 16.9 <C> 86.8 <C> 40.1 ±2.0 <R> <C> Xlm-R <C> 21.8 <C> 87.0 <C> 42.9 ±2.1 <R> <C> [ITALIC] TRANSLATE-TRAIN <C> [ITALIC] TRANSLATE-TRAIN <C> [ITALIC] TRANSLATE-TRAIN <C> [ITALIC] TRANSLATE-TRAIN <R> <C> M-Bert <C> 23.4 <C> 87.3 <C> 44.1 ±1.8 <R> <C> Xlm <C> 22.6 <C> 83.4 <C> 43.4 ±2.1 <R> <C> Xlm-R <C> [BOLD] 27.6 <C> 84.5 <C> [BOLD] 46.0 ±1.4 <CAP> Table 5: Mean Best F1 over 26 languages. Best F1 is broken down by Answerable and Unanswerable (UA) F1 scores, with denominators representing their portion of the evaluation set (67.58% and 32.42% respectively). A naive approach predicting exclusively No Answer achieves a lower bound score of 32.42% F1. Xlm-R with Translate-Train outperforms all alternate settings.
<R> <C> B- <C> B- <C> B- <C> I- <C> B- <C> I- <C> B->m+1 1 <CAP> Table 2: Example auto-derived keystroke annotation.
<R> <C> Chunking <C> F1 <C> CCG tagging <C> Accuracy <R> <C> Our model <C> 93.21 <C> Our model <C> 92.41 <R> <C> suzuki:isozaki:2008 <C> 93.88 <C> xu:ea:2015 <C> 93.00 <CAP> Table 4: Baseline model, comparison to existing systems
<R> <C> [BOLD] Test Set Perplexity <C> [BOLD] Test Set Perplexity Mean <C> [BOLD] Test Set Perplexity Median <R> <C> Unsupervised Language Model <C> 4.23 <C> 2.22 <R> <C> Rating <C> 2.94 <C> 2.07 <R> <C> Item <C> 4.48 <C> 2.17 <R> <C> User <C> 2.26 <C> 2.03 <R> <C> User-Item <C> 2.25 <C> 1.98 <CAP> Table 1: Perplexity on test set data for unsupervised RNN language model as well as GCNs with rating, category, user, item, user and item info.
<R> <C> Epoch German→English <C> 1 German→English <C> 1 German→English <R> <C> Data <C> T <C> T-2017 <R> <C> Time (seconds) <C> 365 <C> 305 <R> <C> LR <C> 0.71 <C> 0.71 <R> <C> BLEU (2017) <C> 32,87 <C> 32,66 <R> <C> English→German <C> English→German <C> English→German <R> <C> Data <C> T <C> T-2017 <R> <C> Time (seconds) <C> 372 <C> 308 <R> <C> LR <C> 0.71 <C> 0.71 <R> <C> BLEU (2017) <C> 26,98 <C> 26,80 <CAP> Table 4: Hyper-specialisation on news test sets.
<R> <C> [EMPTY] <C> APbbox <C> APbbox50 <C> APbbox75 <C> APmask <C> APmask50 <C> APmask75 <R> <C> baseline <C> 38.8 <C> 59.3 <C> 42.5 <C> 35.1 <C> 56.2 <C> 37.9 <R> <C> NL <C> 39.6 <C> 60.3 <C> 43.2 <C> 35.8 <C> 57.1 <C> 38.5 <R> <C> NL [ITALIC] p <C> 39.8 <C> 60.4 <C> 43.7 <C> 35.9 <C> 57.3 <C> 38.4 <R> <C> NL [ITALIC] u <C> 40.1 <C> 60.9 <C> 43.8 <C> 36.1 <C> 57.6 <C> 38.7 <R> <C> DNL <C> 40.3 <C> 61.2 <C> 44.1 <C> 36.4 <C> 58.0 <C> 39.1 <CAP> Table 7: Results based on Slow-only baseline using R50 as backbone on Kinetics validation set
<R> <C> [EMPTY] <C> Top-1 Acc <C> Top-5 Acc <R> <C> baseline <C> 74.94 <C> 91.90 <R> <C> NL <C> 75.95 <C> 92.29 <R> <C> NL [ITALIC] p <C> 76.01 <C> 92.28 <R> <C> NL [ITALIC] u <C> 75.76 <C> 92.44 <R> <C> DNL <C> 76.31 <C> 92.69 <CAP> Table 7: Results based on Slow-only baseline using R50 as backbone on Kinetics validation set
<R> <C> [BOLD] Model <C> [BOLD] CER (%) <R> <C> [BOLD] Baseline <C> [BOLD] Baseline <R> <C> MLP  <C> 11.08 <R> <C> MLP+Location Aware  <C> 8.17 <R> <C> MLP  (ours) <C> 7.12 <R> <C> MLP+Location Aware (ours, t=15) <C> 6.87 <R> <C> [BOLD] Proposed <C> [BOLD] Proposed <R> <C> MLP + MA (O=1) <C> 6.43 <R> <C> MLP + MA + C (O=1) <C> 6.04 <R> <C> MLP + MA + C (O=2) <C> 5.85 <R> <C> MLP + MA + C (O=3) <C> 5.59 <CAP> Table 1: MA denotes multiscale aligment, C denotes contextual history, and O denotes how many time-steps are incorporated in the history for the scoring function.
<R> <C> [BOLD] Model <C> [ITALIC] Palindrome  [BOLD] n=60 <C> [ITALIC] Palindrome  [BOLD] n=480 <C> [ITALIC] Palindrome  [BOLD] n=960 <C> [ITALIC] anbn  [BOLD] n=60 <C> [ITALIC] anbn  [BOLD] n=480 <C> [ITALIC] anbn  [BOLD] n=960 <C> [ITALIC] anbncbmam  [BOLD] n=60 <C> [ITALIC] anbncbmam  [BOLD] n=480 <C> [ITALIC] anbncbmam  [BOLD] n=960 <C> [ITALIC] an+ [ITALIC] mbncm  [BOLD] n=60 <C> [ITALIC] an+ [ITALIC] mbncm  [BOLD] n=480 <C> [ITALIC] an+ [ITALIC] mbncm  [BOLD] n=960 <R> <C> 2 [ITALIC] nd Order NSPDA <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <R> <C> 3 [ITALIC] rd Order NSPDA <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <CAP> Table 5: Mean classification error results when using a programmed NSPDA (lower is better).
<R> <C> [BOLD] Learning Algorithm <C> [ITALIC] Palindrome  [BOLD] M1 <C> [ITALIC] Palindrome  [BOLD] M2 <C> [ITALIC] anbn  [BOLD] M1 <C> [ITALIC] anbn  [BOLD] M2 <C> [ITALIC] anbncbmam  [BOLD] M1 <C> [ITALIC] anbncbmam  [BOLD] M2 <C> [ITALIC] an+ [ITALIC] mbncm  [BOLD] M1 <C> [ITALIC] an+ [ITALIC] mbncm  [BOLD] M2 <R> <C> [ITALIC] BPTT <C> 2.02 <C> 1.99 <C> 2.23 <C> 2.55 <C> 2.99 <C> 2.79 <C> 2.99 <C> 1.59 <R> <C> [ITALIC] TBPTT <C> 2.59 <C> 2.81 <C> 1.05 <C> 1.29 <C> 2.97 <C> 2.02 <C> 1.58 <C> 1.11 <R> <C> [ITALIC] RTRL <C> 0.02 <C> 0.19 <C> 0.09 <C> 0.10 <C> 1.85 <C> 0.07 <C> 0.09 <C> 0.01 <R> <C> [ITALIC] UORO <C> 0.00 <C> 0.00 <C> 0.06 <C> 0.01 <C> 0.99 <C> 0.00 <C> 0.09 <C> 0.00 <CAP> Table 5: Mean classification error results when using a programmed NSPDA (lower is better).
<R> <C> [BOLD] RNN Type <C> [ITALIC] Palindrome  [BOLD] Train <C> [ITALIC] Palindrome  [BOLD] Test <C> [ITALIC] anbn  [BOLD] Train <C> [ITALIC] anbn  [BOLD] Test <C> [ITALIC] anbncbmam  [BOLD] Train <C> [ITALIC] anbncbmam  [BOLD] Test <C> [ITALIC] an+ [ITALIC] mbncm  [BOLD] Train <C> [ITALIC] an+ [ITALIC] mbncm  [BOLD] Test <C> [ITALIC] Parenthesis  [BOLD] Train <C> [ITALIC] Parenthesis  [BOLD] Test <R> <C> [ITALIC] RNN <C> 0.00 <C> 78.2 <C> 0.00 <C> 74.11 <C> 0.00 <C> 83.33 <C> 0.00 <C> 73.69 <C> 30.72 <C> 99.96 <R> <C> [ITALIC] LSTM <C> 0.00 <C> 12.58 <C> 0.00 <C> 13.26 <C> 0.00 <C> 14.22 <C> 0.00 <C> 10.56 <C> 48.92 <C> 97.88 <R> <C> [ITALIC] LSTM-p <C> 0.00 <C> 8.69 <C> 0.00 <C> 11.25 <C> 0.00 <C> 13.99 <C> 0.00 <C> 12.88 <C> 49.68 <C> 99.00 <R> <C> [ITALIC] GRU <C> 0.00 <C> 14.99 <C> 0.00 <C> 14.89 <C> 0.00 <C> 19.22 <C> 0.00 <C> 14.00 <C> 43.21 <C> 98.70 <R> <C> [ITALIC] Stack RNN 40+10 <C> 0.00 <C> 4.99 <C> 0.00 <C> 3.01 <C> 0.00 <C> 34.19 <C> 0.00 <C> 58.66 <C> 10.25 <C> 9.38 <R> <C> [ITALIC] Stack RNN 40+10+ rounding <C> 0.00 <C> 0.09 <C> 0.00 <C> 0.89 <C> 0.00 <C> 1.01 <C> 0.00 <C> 0.79 <C> 4.03 <C> 3.968 <R> <C> [ITALIC] listRNN 40+5 <C> 0.00 <C> 0.39 <C> 0.00 <C> 2.29 <C> 0.00 <C> 19.63 <C> 0.00 <C> 1.27 <C> 4.89 <C> 7.45 <R> <C> [ITALIC] MI-RNN <C> 0.00 <C> 75.69 <C> 0.00 <C> 70.26 <C> 0.00 <C> 76.69 <C> 0.00 <C> 73.01 <C> 29.58 <C> 99.92 <R> <C> [ITALIC] MI-LSTM <C> 0.00 <C> 9.99 <C> 0.00 <C> 10.86 <C> 0.00 <C> 13.55 <C> 0.00 <C> 14.22 <C> 47.83 <C> 99.80 <R> <C> [ITALIC] MI-GRU <C> 0.00 <C> 16.22 <C> 0.00 <C> 13.29 <C> 0.00 <C> 20.02 <C> 0.00 <C> 14.83 <C> 42.88 <C> 99.20 <R> <C> 2 [ITALIC] nd Order RNN <C> 0.00 <C> 9.26 <C> 0.00 <C> 8.51 <C> 0.00 <C> 17.52 <C> 0.00 <C> 11.17 <C> 27.89 <C> 37.42 <R> <C> 2 [ITALIC] nd Order RNN reg  [BOLD] (ours) <C> 0.00 <C> 1.88 <C> 0.00 <C> 2.09 <C> 0.00 <C> 2.19 <C> 0.00 <C> 0.99 <C> 21.69 <C> 27.59 <R> <C> [ITALIC] NNPDA <C> 0.00 <C> 7.00 <C> 0.00 <C> 15.25 <C> 0.00 <C> 17.49 <C> 0.00 <C> 55.28 <C> 5.96 <C> 29.21 <R> <C> [ITALIC] NNPDA reg  [BOLD] (ours) <C> 0.00 <C> 4.28 <C> 0.00 <C> 14.20 <C> 0.00 <C> 13.00 <C> 0.00 <C> 41.01 <C> 5.62 <C> 27.09 <R> <C> [ITALIC] NSPDA, M1  [BOLD] (ours) <C> 0.00 <C> 0.00 <C> 0.00 <C> 0.06 <C> 0.00 <C> 0.99 <C> 0.00 <C> 0.09 <C> 0.58 <C> 2.58 <R> <C> [ITALIC] NSPDA, M2  [BOLD] (ours) <C> [BOLD] 0.00 <C> [BOLD] 0.00 <C> [BOLD] 0.00 <C> [BOLD] 0.01 <C> [BOLD] 0.00 <C> [BOLD] 0.00 <C> [BOLD] 0.00 <C> [BOLD] 0.00 <C> [BOLD] 0.01 <C> [BOLD] 0.88 <CAP> Table 5: Mean classification error results when using a programmed NSPDA (lower is better).
<R> <C> [BOLD] Dataset <C> [ITALIC] N <C> [BOLD] Fail <C> [ITALIC] NCF <C> [BOLD] POS-tag Error <C> [BOLD] CP <C> [BOLD] CR <R> <C> WCL v. 1.1 <C> 1537 <C> 9 <C> 1528 <C> 9 <C> 1 <C> 0.9941 <R> <C> Vehicle <C> 150 <C> 3 <C> 147 <C> 3 <C> 1 <C> 0.98 <R> <C> Virus <C> 172 <C> 9 <C> 163 <C> 9 <C> 1 <C> 0.9477 <R> <C> Plant <C> 633 <C> 2 <C> 631 <C> 2 <C> 1 <C> 0.9968 <CAP> Table 3: DLOL Characterization Performance
<R> <C> [EMPTY] <C> [BOLD] dev  [BOLD] acc <C> [BOLD] dev  [BOLD] ora <C> [BOLD] test  [BOLD] acc <C> [BOLD] test  [BOLD] ora <R> <C> IR baseline <C> 13.4 <C> 69.1 <C> 12.7 <C> 70.6 <R> <C> WQ baseline <C> 23.6 <C> 34.4 <C> 24.3 <C> 35.6 <R> <C> Our system <C> 37.0 <C> 76.7 <C> 37.1 <C> 76.6 <CAP> Table 5: Accuracy (acc) and oracle scores (ora) on the development sets (3 random splits of the training data) and the test data.
<R> <C> [EMPTY] <C> [BOLD] Our system <C> [BOLD] acc 37.0 <C> [BOLD] ora 76.7 <R> <C> (a) <C> [BOLD] Rule Ablation <C> [BOLD] Rule Ablation <C> [BOLD] Rule Ablation <R> <C> [EMPTY] <C> join only <C> 10.6 <C> 15.7 <R> <C> [EMPTY] <C> join + count (= WQ baseline) <C> 23.6 <C> 34.4 <R> <C> [EMPTY] <C> join + count + superlative <C> 30.7 <C> 68.6 <R> <C> [EMPTY] <C> all − {⊓,⊔} <C> 34.8 <C> 75.1 <R> <C> (b) <C> [BOLD] Feature Ablation <C> [BOLD] Feature Ablation <C> [BOLD] Feature Ablation <R> <C> [EMPTY] <C> all − features involving predicate <C> 11.8 <C> 74.5 <R> <C> [EMPTY] <C> all − phrase-predicate <C> 16.9 <C> 74.5 <R> <C> [EMPTY] <C> all − lex phrase-predicate <C> 17.6 <C> 75.9 <R> <C> [EMPTY] <C> all − unlex phrase-predicate <C> 34.3 <C> 76.7 <R> <C> [EMPTY] <C> all − missing-predicate <C> 35.9 <C> 76.7 <R> <C> [EMPTY] <C> all − features involving denotation <C> 33.5 <C> 76.8 <R> <C> [EMPTY] <C> all − denotation <C> 34.3 <C> 76.6 <R> <C> [EMPTY] <C> all − phrase-denotation <C> 35.7 <C> 76.8 <R> <C> [EMPTY] <C> all − headword-denotation <C> 36.0 <C> 76.7 <R> <C> (c) <C> [BOLD] Anchor operations to trigger words <C> 37.1 <C> 59.4 <CAP> Table 6: Average accuracy and oracle scores on development data in various system settings.
<R> <C> [EMPTY] <C> Composite  [ITALIC] τ <C> Composite  [ITALIC] ρ <C> Flickr8k  [ITALIC] τ <C> Flickr8k  [ITALIC] ρ <R> <C> BLEU-1 <C> 0.280 <C> 0.353 <C> 0.323 <C> 0.404 <R> <C> BLEU-4 <C> 0.205 <C> 0.352 <C> 0.138 <C> 0.387 <R> <C> ROUGE-L <C> 0.307 <C> 0.383 <C> 0.323 <C> 0.404 <R> <C> METEOR <C> 0.379 <C> 0.469 <C> 0.418 <C> 0.519 <R> <C> CIDEr <C> 0.378 <C> 0.472 <C> 0.439 <C> 0.542 <R> <C> SPICE <C> 0.419 <C> 0.514 <C> 0.449 <C> 0.596 <R> <C> [BOLD] Ours <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> RRS <C> 0.388 <C> 0.479 <C> 0.418 <C> 0.521 <R> <C> WDS <C> 0.433 <C> 0.526 <C> 0.464 <C> 0.572 <R> <C> TIGEr <C> [BOLD] 0.454 <C> [BOLD] 0.553 <C> [BOLD] 0.493 <C> [BOLD] 0.606 <CAP> Table 1: Caption-level correlation between metrics and human grading scores in Composite and Flickr 8K dataset by using Kendall tau and Spearman rho. All p-values < 0.01.
<R> <C> [EMPTY] <C> [BOLD] HC <C> [BOLD] HI <C> [BOLD] HM <C> [BOLD] MM <C> [BOLD] All <R> <C> BLEU-1 <C> 51.20 <C> 95.70 <C> 91.20 <C> 58.20 <C> 74.08 <R> <C> BLEU-4 <C> 53.00 <C> 92.40 <C> 86.70 <C> 59.40 <C> 72.88 <R> <C> ROUGE-L <C> 51.50 <C> 94.50 <C> 92.50 <C> 57.70 <C> 74.05 <R> <C> METEOR <C> [BOLD] 56.70 <C> 97.60 <C> [BOLD] 94.20 <C> 63.40 <C> 77.98 <R> <C> CIDEr <C> 53.00 <C> 98.00 <C> 91.50 <C> 64.50 <C> 76.75 <R> <C> SPICE <C> 52.60 <C> 93.90 <C> 83.60 <C> 48.10 <C> 69.55 <R> <C> TIGEr (ours) <C> 56.00 <C> [BOLD] 99.80 <C> 92.80 <C> [BOLD] 74.20 <C> [BOLD] 80.70 <CAP> Table 2: Accuracy of metrics at matching human judgments on PASCAL-50S with 5 reference captions. The highest accuracy per pair type is shown in bold font. Column titles are explained in Section 4.1.
<R> <C> Model <C> [ITALIC] Nl <C> [ITALIC] Nh <C> [ITALIC] Ne <C> [ITALIC] Se <C> [ITALIC] Stotal <R> <C> W30K <C> 1 <C> 670 <C> 96 <C> 2.91M <C> 3.40M <R> <C> P4K-S <C> 1 <C> 670 <C> 96 <C> 0.38M <C> 0.85M <R> <C> P4K-L <C> 2 <C> 1080 <C> 140 <C> 0.56M <C> 2.70M <R> <C> P4K-G <C> 2 <C> 1080 <C> 280 <C> 1.12M <C> 2.71M <R> <C> P16K-S <C> 1 <C> 670 <C> 96 <C> 1.54M <C> 2.00M <R> <C> P16K-L <C> 1 <C> 670 <C> 160 <C> 2.56M <C> 3.33M <R> <C> P30K <C> 1 <C> 670 <C> 96 <C> 2.91M <C> 3.40M <CAP> Table 2: Parameters for neural language models. W and P refer to word and word-piece models, respectively. Nl, Nh, Ne, Se and Stotal refer to the number of LSTM layers, the number of hidden states in LSTM, the embedding dimension size, the number of parameters in the embedding layer and in total, respectively. The suffixes “S” and “L” indicate small and large models. “G” represents GLSTM. The suffixes 4K, 16K and 30K represent the vocabulary sizes.
<R> <C> Method <C> Val Seen Oracle <C> Val Seen Navigator <C> Val Seen Mixed <C> Val Unseen Oracle <C> Val Unseen Navigator <C> Val Unseen Mixed <C> Test Unseen Oracle <C> Test Unseen Navigator <C> Test Unseen Mixed <R> <C> Baseline (Shortest Path Agent) <C> 8.29 <C> 7.63 <C> 9.52 <C> 8.36 <C> 7.99 <C> 9.58 <C> 8.06 <C> 8.48 <C> 9.76 <R> <C> Baseline (Random Agent) <C> 0.42 <C> 0.42 <C> 0.42 <C> 1.09 <C> 1.09 <C> 1.09 <C> 0.83 <C> 0.83 <C> 0.83 <R> <C> Baseline (Vision Only) <C> 4.12 <C> 5.58 <C> 5.72 <C> 0.85 <C> 1.38 <C> 1.15 <C> 0.99 <C> 1.56 <C> 1.74 <R> <C> Baseline (Dialog Only) <C> 1.41 <C> 1.43 <C> 1.58 <C> 1.68 <C> 1.39 <C> 1.64 <C> 1.51 <C> 1.20 <C> 1.40 <R> <C> Sequence-to-sequence model  <C> 4.48 <C> 5.67 <C> 5.92 <C> 1.23 <C> 1.98 <C> 2.10 <C> 1.25 <C> 2.11 <C> 2.35 <R> <C> CMN (Ours) <C> [BOLD] 5.47 <C> [BOLD] 6.14 <C> [BOLD] 7.05 <C> [BOLD] 2.68 <C> [BOLD] 2.28 <C> [BOLD] 2.97 <C> [BOLD] 2.69 <C> [BOLD] 2.26 <C> [BOLD] 2.95 <CAP> Table 1: Comparison of the performance on Goal Progress (m). Different supervisions of end path are used in training. Oracle indicates planner path, Navigator indicates player path, Mixed indicates trusted path.
<R> <C> [BOLD] Domain <C> [BOLD] Language Model Score ( [ITALIC] lms) <C> [BOLD] Stereotype Score ( [ITALIC] ss) <C> [BOLD] Idealized CAT Score ( [ITALIC] icat) <R> <C> Gender <C> 92.4 <C> 63.9 <C> 66.7 <R> <C> [ITALIC] mother <C> 97.2 <C> 77.8 <C> 43.2 <R> <C> [ITALIC] grandfather <C> 96.2 <C> 52.8 <C> 90.8 <R> <C> Profession <C> 88.8 <C> 62.6 <C> 66.5 <R> <C> [ITALIC] software developer <C> 94.0 <C> 75.9 <C> 45.4 <R> <C> [ITALIC] producer <C> 91.7 <C> 53.7 <C> 84.9 <R> <C> Race <C> 91.2 <C> [BOLD] 61.8 <C> [BOLD] 69.7 <R> <C> [ITALIC] African <C> 91.8 <C> 74.5 <C> 46.7 <R> <C> [ITALIC] Crimean <C> 93.3 <C> 50.0 <C> 93.3 <R> <C> Religion <C> [BOLD] 93.5 <C> 63.8 <C> 67.7 <R> <C> [ITALIC] Bible <C> 85.0 <C> 66.0 <C> 57.8 <R> <C> [ITALIC] Muslim <C> 94.8 <C> 46.6 <C> 88.3 <CAP> Table 5: Domain-wise results of the Ensemble model, along with most and least stereotyped terms.
<R> <C> Task (dataset) <C> MAP <C> VI <R> <C> Sentiment analysis (IMDB) <C> 86.9 <C> [BOLD] 87.0 <R> <C> Paraphrase detection (MSR) <C> 70.0 <C> [BOLD] 71.0 <CAP> Table 1: Classification accuracy of MAP and variational inference
<R> <C> System <C> Recurrent Layers <C> WER% <R> <C> LSTM <C> 1 <C> 10.96 <R> <C> LSTM <C> 2 <C> 9.97 <R> <C> LSTM <C> 4 <C> 9.67 <R> <C> LSTM <C> 6 <C> 9.47 <R> <C> GRU <C> 1 <C> 10.76 <R> <C> GRU <C> 2 <C> 9.47 <R> <C> GRU <C> 4 <C> 9.32 <R> <C> GRU <C> 6 <C> 9.32 <CAP> Table 1: Performance of LSTM and GRU systems.
<R> <C> System <C> Recurrent Layers <C> WER% Baseline <C> WER% Residual Learning <R> <C> LSTM <C> 4 <C> 9.67 <C> 9.53 <R> <C> LSTM <C> 6 <C> 9.47 <C> 9.33 <R> <C> GRU <C> 4 <C> 9.32 <C> 9.23 <R> <C> GRU <C> 6 <C> 9.32 <C> 9.10 <CAP> Table 3: Performance of LSTM/GRU with memory residual connections.
<R> <C> Model <C> dev CER <C> eval CER <R> <C> 2-spkr ASR (1st channel) <C> 22.65 <C> 19.07 <R> <C> BeamformIt Enhancement (2-spkr ASR) <C> 15.23 <C> 12.45 <R> <C> BeamformIt Separation (1-spkr ASR) <C> 77.30 <C> 77.10 <R> <C> MIMO-Speech <C> 17.29 <C> 14.51 <R> <C> + Curriculum Learning (SNRs) <C> [BOLD] 16.34 <C> [BOLD] 13.75 <R> <C> Model <C> dev WER <C> eval WER <R> <C> 2-spkr ASR (1st channel) <C> 34.98 <C> 29.43 <R> <C> BeamformIt Enhancement (2-spkr ASR) <C> 26.61 <C> 21.75 <R> <C> BeamformIt Separation (1-spkr ASR) <C> 98.60 <C> 98.00 <R> <C> MIMO-Speech <C> 13.54 <C> 18.62 <R> <C> + Curriculum Learning (SNRs) <C> [BOLD] 12.59 <C> [BOLD] 17.55 <CAP> Table 1: Performance in terms of average CER and WER [%] on the spatialized anechoic wsj1-2mix corpus.
<R> <C> [EMPTY] <C> [EMPTY] <C> Mean Cosine Sim <C> Mean Cosine Sim <C> Mean Cosine Sim <C> Precision IntCos <C> Precision IntCos <C> Precision IntCos <C> Precision IntCos <C> Precision LRCos <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> complement <C> complement <C> original <C> original <C> [EMPTY] <R> <C> [EMPTY] <C> [EMPTY] <C> Inter <C> IntraL <C> IntraR <C> DensR. <C> SVM <C> DensR. <C> SVM <C> [EMPTY] <R> <C> FT/BATS <C> Inflectional <C> 0.75 <C> 0.48 <C> 0.51 <C> 0.92 <C> 0.93 <C> [BOLD] 0.97 <C> [BOLD] 0.97 <C> [BOLD] 0.97 <R> <C> FT/BATS <C> Derivational <C> 0.63 <C> 0.47 <C> 0.45 <C> 0.74 <C> 0.78 <C> [BOLD] 0.81 <C> 0.80 <C> 0.80 <R> <C> FT/BATS <C> Encyclopedia <C> 0.48 <C> 0.43 <C> 0.55 <C> 0.30 <C> 0.43 <C> 0.41 <C> 0.43 <C> [BOLD] 0.45 <R> <C> FT/BATS <C> Lexicography <C> 0.62 <C> 0.37 <C> 0.38 <C> 0.17 <C> 0.20 <C> 0.21 <C> 0.22 <C> [BOLD] 0.26 <R> <C> FT/BATS <C> Macro Mean <C> 0.62 <C> 0.44 <C> 0.47 <C> 0.53 <C> 0.58 <C> 0.60 <C> 0.60 <C> [BOLD] 0.61 <R> <C> FT/BATS <C> Macro Std <C> 0.12 <C> 0.06 <C> 0.09 <C> 0.34 <C> 0.33 <C> 0.34 <C> 0.33 <C> [BOLD] 0.32 <R> <C> GN/BATS <C> Inflectional <C> 0.63 <C> 0.22 <C> 0.23 <C> [BOLD] 0.88 <C> 0.87 <C> [BOLD] 0.88 <C> [BOLD] 0.88 <C> [BOLD] 0.88 <R> <C> GN/BATS <C> Derivational <C> 0.44 <C> 0.21 <C> 0.20 <C> [BOLD] 0.55 <C> 0.50 <C> 0.51 <C> 0.48 <C> 0.44 <R> <C> GN/BATS <C> Encyclopedia <C> 0.35 <C> 0.29 <C> 0.42 <C> 0.33 <C> [BOLD] 0.35 <C> [BOLD] 0.35 <C> 0.32 <C> 0.34 <R> <C> GN/BATS <C> Lexicography <C> 0.45 <C> 0.17 <C> 0.18 <C> [BOLD] 0.19 <C> 0.17 <C> [BOLD] 0.19 <C> 0.17 <C> 0.18 <R> <C> GN/BATS <C> Macro Mean <C> 0.46 <C> 0.22 <C> 0.26 <C> [BOLD] 0.48 <C> 0.47 <C> [BOLD] 0.48 <C> 0.46 <C> 0.45 <R> <C> GN/BATS <C> Macro Std <C> 0.14 <C> 0.07 <C> 0.12 <C> [BOLD] 0.31 <C> [BOLD] 0.31 <C> 0.32 <C> 0.32 <C> 0.32 <R> <C> FT/GA <C> Micro Mean <C> 0.73 <C> 0.48 <C> 0.53 <C> 0.88 <C> 0.91 <C> [BOLD] 0.93 <C> 0.92 <C> [BOLD] 0.93 <R> <C> FT/GA <C> Macro Mean <C> 0.71 <C> 0.50 <C> 0.53 <C> 0.87 <C> 0.90 <C> [BOLD] 0.91 <C> 0.90 <C> 0.89 <R> <C> FT/GA <C> Macro Std <C> 0.11 <C> 0.05 <C> 0.06 <C> 0.11 <C> [BOLD] 0.08 <C> 0.12 <C> 0.17 <C> 0.23 <R> <C> GN/GA <C> Micro Mean <C> 0.62 <C> 0.31 <C> 0.36 <C> 0.85 <C> 0.87 <C> [BOLD] 0.89 <C> 0.87 <C> 0.88 <R> <C> GN/GA <C> Macro Mean <C> 0.61 <C> 0.30 <C> 0.35 <C> 0.85 <C> 0.86 <C> [BOLD] 0.88 <C> 0.85 <C> 0.87 <R> <C> GN/GA <C> Macro Std <C> 0.10 <C> 0.09 <C> 0.10 <C> 0.08 <C> [BOLD] 0.07 <C> 0.09 <C> 0.11 <C> 0.11 <CAP> Table 4: Left part shows mean cosine similarity. Inter: mean cosine similarity between pairs. IntraL/R: mean cosine similarity within the left/right class. Right part shows precision for word analogy task.
<R> <C> [EMPTY] <C> [BOLD] O <C> [BOLD] C <C> [BOLD] E <C> [BOLD] A <C> [BOLD] N <R> <C> Chance <C> 0.33 <C> 0.33 <C> 0.33 <C> 0.33 <C> 0.33 <R> <C> All Feat <C> [BOLD] 0.39 <C> 0.33 <C> 0.31 <C> 0.33 <C> [BOLD] 0.35 <CAP> Table 1: Baseline accuracy for each trait. Green and bold cells are significantly better than chance (with 95% confidence); red cells are significantly worse than chance.
<R> <C> Category <C> Features <C> Rank <R> <C> Semantic Coherence <C> [ITALIC] BoW mean scene 3 <C> 1 <R> <C> [EMPTY] <C> [ITALIC] INF minimum scene 3 <C> 2 <R> <C> [EMPTY] <C> [ITALIC] SIF 90th percentile scene 3 <C> 5 <R> <C> [EMPTY] <C> [ITALIC] INF maximum scene 2 <C> 7 <R> <C> [EMPTY] <C> [ITALIC] INF median scene 3 <C> 8 <R> <C> [EMPTY] <C> [ITALIC] BoW median scene 3 <C> 9 <R> <C> [EMPTY] <C> [ITALIC] BoW minimum scene 2 <C> 10 <R> <C> [EMPTY] <C> [ITALIC] BoW st. dev. scene 2 <C> 11 <R> <C> [EMPTY] <C> [ITALIC] BoW maximum scene 3 <C> 12 <R> <C> [EMPTY] <C> [ITALIC] INF st. dev. scene 3 <C> 13 <R> <C> [EMPTY] <C> BoW maximum scene 2 <C> 18 <R> <C> [EMPTY] <C> BoW 90th percentile scene 2 <C> 19 <R> <C> [EMPTY] <C> BoW st. dev. scene 3 <C> 20 <R> <C> [EMPTY] <C> BoW 90th percentile scene 3 <C> 21 <R> <C> [EMPTY] <C> INF mean scene 3 <C> 22 <R> <C> [EMPTY] <C> INF 10th percentile scene 3 <C> 23 <R> <C> [EMPTY] <C> BoW 10th percentile scene 2 <C> 24 <R> <C> Lexical Diversity <C> [ITALIC] MATTR <C> 3 <R> <C> [EMPTY] <C> [ITALIC] Brunét’s index <C> 4 <R> <C> [EMPTY] <C> Honoré’s statistic <C> 25 <R> <C> Lexical Density <C> [ITALIC] FUNC/W <C> 6 <R> <C> [EMPTY] <C> [ITALIC] UH/W <C> 14 <R> <C> Syntactic Complexity <C> [ITALIC] Maximum Yngve depth <C> 15 <R> <C> [EMPTY] <C> Mean length sent. (MLS) <C> 16 <R> <C> [EMPTY] <C> Parse tree height <C> 17 <CAP> Table 1: Selected features to model SSPA scores with a linear regression model, including ranking of overall importance for each feature. Italicized features were included in both the 25 feature and 15 feature classification problems.
<R> <C> [EMPTY] <C> 1 <C> 2 <C> 3 <C> 4 <C> 5 <R> <C> 1 <C> [EMPTY] <C> 0.59 <C> 0.63 <C> 0.67 <C> 0.66 <R> <C> 2 <C> [EMPTY] <C> [EMPTY] <C> 0.57 <C> 0.64 <C> 0.65 <R> <C> 3 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 0.64 <C> 0.62 <R> <C> 4 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 0.68 <R> <C> [ITALIC] avg <C> 0.71 <C> 0.68 <C> 0.68 <C> 0.75 <C> 0.74 <CAP> Table 3: Correlation matrix for pairwise correlation agreement; avg refers to agreement of the annotator in the column against the average across the other annotators.
<R> <C> Task <C> [EMPTY] <C> en-fr <C> en-de <C> en-ru <C> en-zh <R> <C> [EMPTY] <C> P <C> 81.9 <C> 82.2 <C> 79.9 <C> 76.7 <R> <C> Train <C> R <C> 69.1 <C> 70.1 <C> 67.8 <C> 67.1 <R> <C> [EMPTY] <C> F1 <C> 74.9 <C> 76.1 <C> 73.3 <C> 71.6 <R> <C> Threshold <C> Threshold <C> 0.58 <C> 0.50 <C> 0.57 <C> 0.64 <R> <C> [EMPTY] <C> P <C> 84.8 <C> 84.1 <C> 81.1 <C> 77.7 <R> <C> Test <C> R <C> 68.6 <C> 70.7 <C> 67.6 <C> 66.4 <R> <C> [EMPTY] <C> F1 <C> 75.8 <C> 76.9 <C> 73.8 <C> 71.6 <CAP> Table 3: Results on the BUCC test set of our approach: Precision, Recall and F-measure (%). We also provide the optimal threshold on the distance.
<R> <C> Threshold <C> #Sents <C> BLEU Mined <C> BLEU Eparl <C> BLEU All <R> <C> Threshold <C> #Sents <C> alone <C> + mined <C> + mined <R> <C> baseline <C> - <C> - <C> 21.87 <C> 25.06 <R> <C> 0.25 <C> 1.0M <C> 4.18 <C> [BOLD] 22.32 <C> [BOLD] 25.07 <R> <C> 0.26 <C> 1.5M <C> 5.17 <C> 22.09 <C> - <R> <C> 0.27 <C> 1.9M <C> 5.92 <C> 21.97 <C> - <R> <C> 0.28 <C> 2.5M <C> 6.48 <C> 22.29 <C> 25.03 <R> <C> 0.29 <C> 3.3M <C> 6.01 <C> 22.10 <C> - <R> <C> 0.30 <C> 4.3M <C> 7.77 <C> 22.24 <C> - <CAP> Table 6: BLEU scores when training on the mined data only, adding it (at different thresholds) to the human translated training corpus (Eparl+NC) and to our best system using filtered Common Crawl.
<R> <C> [BOLD] Dataset <C> [BOLD] Pairs <C> [BOLD] Tokens hi <C> [BOLD] Tokens en <R> <C> IITB train <C> 1,492,827 <C> 22.2M <C> 20.6M <R> <C> IITB train† <C> 923,377 <C> 20.3M <C> 18.9M <R> <C> National News <C> 2,495,129 <C> 41.2M <C> 39.0M <R> <C> Backtranslated <C> 5,653,644 <C> 77.5M <C> 91.9M <R> <C> IITB dev <C> 505 <C> 10,656 <C> 10,174 <R> <C> IITB test <C> 2,507 <C> 49,394 <C> 57,037 <CAP> Table 1: Descriptions of the corpora used, IITB train† is a filtered version of the IITB train corpus.
<R> <C> [EMPTY] <C> [BOLD] ROUGE-1 <C> [BOLD] ROUGE-2 <R> <C> OL <C> 0.290 <C> 0.051 <R> <C> RL <C> 0.248 <C> 0.041 <R> <C> R <C> 0.2052 <C> 0.027 <CAP> Table 1: F-Scores for Benchmark Systems
<R> <C> [EMPTY] <C> Euclidean distance 1↔2 <C> Euclidean distance 1↔3 <C> Euclidean distance 2↔3 <R> <C> BOW <C> 0.40 <C> 0.40 <C> 0.35 <R> <C> CPTW <C> 0.16 <C> 0.49 <C> 0.40 <CAP> Figure 1. Example: Our similarity propagation reduces the Euclidean distance between semantically similar texts compared to bag-of-words with frequency weighting (BOW).
<R> <C> [EMPTY] <C> Method <C> Es→ Fr <C> De→ Fr <R> <C> Cheng et al. Cheng et al. ( 2016a ) <C> pivot <C> 29.79 <C> 23.70 <R> <C> Cheng et al. Cheng et al. ( 2016a ) <C> hard <C> 29.93 <C> 23.88 <R> <C> Cheng et al. Cheng et al. ( 2016a ) <C> soft <C> 30.57 <C> 23.79 <R> <C> Cheng et al. Cheng et al. ( 2016a ) <C> likelihood <C> 32.59 <C> 25.93 <R> <C> Ours <C> sent-beam <C> 31.64 <C> 24.39 <R> <C> Ours <C> word-sampling <C> 33.86 <C> 27.03 <CAP> Table 3: Comparison with previous work on Spanish-French and German-French translation tasks from the Europarl corpus. English is treated as the pivot language. The likelihood method uses 100K parallel source-target sentences, which are not available for other methods.
<R> <C> id <C> qid1 <C> qid2 <C> question1 <C> question2 <C> is_duplicate <R> <C> 130859 <C> 209926 <C> 209927 <C> How do you treat a cat with a  cold ? <C> How can you cure a cat of a  cold ? <C> 1 <R> <C> 82425 <C> 139763 <C> 133638 <C> How much medical evidence is there in support of the claim weed causes cancer? <C> Does weed give you lung cancer? <C> 1 <R> <C> 261370 <C> 377490 <C> 377491 <C> How can an allergy to sawdust be treated? <C> How do you treat sawdust allergy? <C> 1 <R> <C> … <C> … <C> … <C> … <C> … <C> … <CAP> Table 1. Some examples in Quora medical subset
<R> <C> [BOLD] Languages: <C> cmn <C> cym <C> est <C> fin <C> fra <C> heb <C> pol <C> rus <C> spa <C> swa <C> yue <C> [ITALIC] Avg <R> <C> [BOLD] Nouns <C> 84.5 <C> 80.0 <C> 90.0 <C> 87.3 <C> 78.2 <C> 98.2 <C> 90.0 <C> 95.5 <C> 85.5 <C> 80.0 <C> 77.3 <C> 86.0 <R> <C> [BOLD] Adjectives <C> 88.5 <C> 88.5 <C> 61.5 <C> 73.1 <C> 69.2 <C> 100.0 <C> 84.6 <C> 100.0 <C> 69.2 <C> 88.5 <C> 84.6 <C> 82.5 <R> <C> [BOLD] Verbs <C> 88.0 <C> 74.0 <C> 82.0 <C> 76.0 <C> 78.0 <C> 100.0 <C> 74.0 <C> 100.0 <C> 74.0 <C> 76.0 <C> 86.0 <C> 82.5 <R> <C> [BOLD] Adverbs <C> 92.9 <C> 100.0 <C> 57.1 <C> 78.6 <C> 92.9 <C> 100.0 <C> 85.7 <C> 100.0 <C> 85.7 <C> 85.7 <C> 78.6 <C> 87.0 <R> <C> [BOLD] Overall <C> 86.5 <C> 81.0 <C> 82.0 <C> 82.0 <C> 78.0 <C> 99.0 <C> 85.0 <C> 97.5 <C> 80.5 <C> 81.0 <C> 80.5 <C> 84.8 <CAP> Table 2: Inter-translator agreement (% of matched translated words) by independent translators using a randomly selected 100-pair English sample from the Multi-SimLex dataset, and the corresponding 100-pair samples from the other datasets.
<R> <C> [BOLD] Languages: <C> cmn <C> cym <C> eng <C> est <C> fin <C> fra <C> heb <C> pol <C> rus <C> spa <C> swa <C> yue <R> <C> [BOLD] R1: Start <C> 13 <C> 12 <C> 14 <C> 12 <C> 13 <C> 10 <C> 11 <C> 12 <C> 12 <C> 12 <C> 11 <C> 13 <R> <C> [BOLD] R3: End <C> 11 <C> 10 <C> 13 <C> 10 <C> 10 <C> 10 <C> 10 <C> 10 <C> 10 <C> 10 <C> 10 <C> 11 <CAP> Table 3: Number of human annotators. R1 = Annotation Round 1, R3 = Round 3.
<R> <C> [EMPTY] <C> Metrics <C> Joint CBOW <C> Joint SG <C> Seperate CBOW <C> Seperate SG <R> <C> Gender <C> [ITALIC] P1( [ITALIC] Gi| [ITALIC] Gi) <C> 0.909 <C> 0.884 <C> [BOLD] 0.916 <C> 0.884 <R> <C> Gender <C> [ITALIC] P10( [ITALIC] Gi| [ITALIC] Gi) <C> [BOLD] 0.936 <C> 0.927 <C> 0.935 <C> 0.921 <R> <C> Ethnicity <C> [ITALIC] P1( [ITALIC] W| [ITALIC] W) <C> 0.936 <C> [BOLD] 0.946 <C> 0.930 <C> 0.922 <R> <C> Ethnicity <C> [ITALIC] P1( [ITALIC] B| [ITALIC] B) <C> [BOLD] 0.594 <C> 0.456 <C> 0.444 <C> 0.345 <R> <C> Ethnicity <C> [ITALIC] P1( [ITALIC] A| [ITALIC] A) <C> [BOLD] 0.763 <C> 0.721 <C> 0.717 <C> 0.680 <R> <C> Ethnicity <C> [ITALIC] P1( [ITALIC] H| [ITALIC] H) <C> [BOLD] 0.754 <C> [BOLD] 0.754 <C> 0.671 <C> 0.697 <CAP> Table 1. Evaluations of different name embedding variants. CBOW and SG are two word embedding methods. P1(Gi|Gi) is the probability that 1 nearest neighbor (1-NN) is of the same gender while P10(Gi|Gi) is for 10-NN. “W”, “B”, “A”, “H” stand for “White”, “Black”, “API”, “Hispanic”, respectively.
<R> <C> Nationality <C> Wikipedia Data Name# <C> Wikipedia Data  [ITALIC] HMM <C> Wikipedia Data  [ITALIC] Ethnea <C> Wikipedia Data  [ITALIC] Embd <C> Wikipedia Data  [ITALIC] Prism <C> Wikipedia Data  [ITALIC] Prism [ITALIC] w <C> Email/Twitter Data Name# <C> Email/Twitter Data  [ITALIC] HMM <C> Email/Twitter Data  [ITALIC] Ethnea <C> Email/Twitter Data  [ITALIC] Embd <C> Email/Twitter Data  [ITALIC] Prism <C> Email/Twitter Data  [ITALIC] Prism [ITALIC] w <R> <C> GreaterAfrican <C> 11K <C> 0.428 <C> 0.532 <C> 0.480 <C> [BOLD] 0.543 <C> 0.486 <C> 31K <C> 0.269 <C> 0.389 <C> 0.554 <C> [BOLD] 0.645 <C> 0.622 <R> <C> GreaterEuropean <C> 113K <C> 0.863 <C> 0.903 <C> 0.927 <C> [BOLD] 0.932 <C> 0.899 <C> 225K <C> 0.725 <C> 0.815 <C> 0.861 <C> [BOLD] 0.920 <C> 0.902 <R> <C> Asian <C> 24K <C> 0.654 <C> 0.670 <C> 0.711 <C> 0.745 <C> [BOLD] 0.748 <C> 123K <C> 0.674 <C> 0.709 <C> 0.763 <C> [BOLD] 0.910 <C> 0.904 <R> <C> Muslim* <C> 7K <C> 0.380 <C> 0.563 <C> 0.538 <C> [BOLD] 0.615 <C> 0.611 <C> 13K <C> 0.204 <C> 0.374 <C> 0.602 <C> [BOLD] 0.612 <C> 0.533 <R> <C> Africans* <C> 4K <C> 0.285 <C> 0.268 <C> 0.282 <C> [BOLD] 0.314 <C> 0.259 <C> 18K <C> 0.174 <C> 0.288 <C> 0.458 <C> 0.636 <C> [BOLD] 0.659 <R> <C> WestEuropean <C> 49K <C> 0.631 <C> 0.724 <C> 0.709 <C> 0.747 <C> [BOLD] 0.756 <C> 143K <C> 0.553 <C> 0.735 <C> 0.780 <C> 0.873 <C> [BOLD] 0.878 <R> <C> EastEuropean* <C> 9K <C> 0.488 <C> 0.517 <C> 0.466 <C> 0.575 <C> [BOLD] 0.629 <C> 38K <C> 0.301 <C> 0.582 <C> 0.726 <C> 0.794 <C> [BOLD] 0.812 <R> <C> British* <C> 44K <C> 0.611 <C> 0.760 <C> 0.789 <C> [BOLD] 0.794 <C> 0.768 <C> 35K <C> 0.361 <C> 0.578 <C> 0.627 <C> 0.648 <C> [BOLD] 0.689 <R> <C> Jewish* <C> 11K <C> [BOLD] 0.313 <C> 0.111 <C> 0.095 <C> 0.129 <C> 0.183 <C> 9K <C> 0.097 <C> 0.361 <C> 0.301 <C> [BOLD] 0.405 <C> 0.387 <R> <C> GreaterEastAsian <C> 15K <C> 0.637 <C> 0.626 <C> 0.642 <C> 0.690 <C> [BOLD] 0.706 <C> 97K <C> 0.625 <C> 0.656 <C> 0.713 <C> [BOLD] 0.907 <C> 0.895 <R> <C> IndianSubContinent* <C> 9K <C> 0.523 <C> 0.660 <C> 0.768 <C> [BOLD] 0.769 <C> 0.746 <C> 26K <C> 0.438 <C> 0.721 <C> 0.855 <C> [BOLD] 0.912 <C> 0.903 <R> <C> Italian* <C> 14K <C> 0.521 <C> 0.543 <C> 0.595 <C> [BOLD] 0.634 <C> 0.613 <C> 11K <C> 0.233 <C> 0.453 <C> 0.665 <C> 0.713 <C> [BOLD] 0.763 <R> <C> Hispanic* <C> 11K <C> 0.403 <C> [BOLD] 0.600 <C> 0.397 <C> 0.521 <C> 0.538 <C> 69K <C> 0.432 <C> 0.724 <C> 0.676 <C> 0.850 <C> [BOLD] 0.864 <R> <C> Nordic* <C> 5K <C> 0.400 <C> 0.587 <C> [BOLD] 0.713 <C> 0.709 <C> 0.709 <C> 23K <C> 0.303 <C> 0.653 <C> 0.767 <C> [BOLD] 0.783 <C> [BOLD] 0.783 <R> <C> French* <C> 14K <C> 0.428 <C> 0.523 <C> 0.602 <C> 0.600 <C> [BOLD] 0.624 <C> 27K <C> 0.203 <C> 0.426 <C> 0.738 <C> [BOLD] 0.769 <C> 0.750 <R> <C> Germanic* <C> 5K <C> 0.254 <C> 0.410 <C> 0.401 <C> 0.403 <C> [BOLD] 0.412 <C> 13K <C> 0.140 <C> 0.431 <C> 0.582 <C> 0.629 <C> [BOLD] 0.653 <R> <C> Japanese* <C> 8K <C> 0.646 <C> [BOLD] 0.724 <C> 0.456 <C> 0.547 <C> 0.695 <C> 57K <C> 0.674 <C> 0.788 <C> 0.434 <C> 0.928 <C> [BOLD] 0.939 <R> <C> EastAsian* <C> 7K <C> 0.499 <C> 0.455 <C> 0.609 <C> [BOLD] 0.621 <C> 0.549 <C> 40K <C> 0.270 <C> 0.340 <C> 0.723 <C> [BOLD] 0.834 <C> 0.811 <R> <C> Weighted Avg. <C> — <C> 0.492 <C> 0.607 <C> 0.619 <C> 0.648 <C> [BOLD] 0.651 <C> — <C> 0.364 <C> 0.580 <C> 0.642 <C> 0.790 <C> [BOLD] 0.795 <CAP> Table 2. F1 scores on a 13-leaf taxonomy. Existing methods: HMM (Ambekar et al., 2009) and Ethnea (Torvik and Agarwal, 2016); Embd only uses parameters from name embeddings; Prismw is NamePrism with world population as priors. Nationalities on different levels of taxonomy are separated with bold lines. ‘*’ marks leaf nationalities. Weighted Avg. is count-weighted average F1 of leaf nationalities.
<R> <C> [ITALIC] K <C> GloVe+char <C> GloVe <C> BERT <R> <C> 0 <C> 85.7 <C> 85.4 <C> 92.9 <R> <C> 1 <C> 79.8 <C> 79.0 <C> 70.3 <R> <C> 2 <C> 77.5 <C> 77.2 <C> 62.3 <R> <C> 3 <C> 76.7 <C> 76.6 <C> 57.5 <R> <C> 4 <C> 76.3 <C> 75.9 <C> 54.1 <R> <C> 5 <C> 76.4 <C> 75.8 <C> 53.6 <CAP> Table 3: Transferability of the SST results.
<R> <C> [EMPTY] <C> Base <C> C2Q/Q2C <C> Simple Skip <C> Transformer Skip <C> Inside Conv <R> <C> F1 <C> 74.15 <C> 74.34 <C> 74.81 <C> 74.95 <C> [BOLD] 77.03 <R> <C> Has Ans F1 <C> [BOLD] 80.62 <C> 76.30 <C> 76.13 <C> 76.35 <C> 78.47 <R> <C> No Ans F1 <C> 68.21 <C> 72.54 <C> 73.01 <C> 73.66 <C> [BOLD] 73.83 <R> <C> EM <C> 71.09 <C> 71.56 <C> 72.11 <C> 72.07 <C> [BOLD] 74.37 <CAP> Table 2: Performance results for experiments relative to BERT base
<R> <C> dataset <C> relations <C> train <C> dev <C> test <R> <C> CN-OMCS-14 CW <C> 14 <C> 28,952 <C> 3612 <C> 3612 <R> <C> CN-OMCS-14 OW-1 <C> 14+1 <C> 30,960 <C> 3870 <C> 3870 <R> <C> CN-OMCS-14 OW-2 <C> 14+2 <C> 33,088 <C> 4128 <C> 4128 <CAP> Table 3: Dataset details and splits.
<R> <C> [EMPTY] <C> Sentiment <C> Yelp Content <C> Perplexity <C> Sentiment <C> Amazon Content <C> Perplexity <R> <C> Shen et. al  <C> 86.5 <C> 38.3 <C> 27.0 <C> 32. 8 <C> 71.6 <C> 27.3 <R> <C> Our Method <C> 94.4 <C> 77.1 <C> 80.1 <C> 59.5 <C> 77.5 <C> 43.7 <CAP> Table 1: Evaluation results on Yelp and Amazon datasets. For Yelp, the pre-trained classifier had a default accuracy of 97.4% and the pre-trained language model had a default perplexity of 23.5. For Amazon, these values were 82.02% for classification and 25.5 for perplexity.
<R> <C> Model <C> F-F <C> M-M <C> M-F <C> F-M <C> Avg. <R> <C> CDVAE [MCC] <C> 6.56 <C> 5.76 <C> 6.96 <C> 6.27 <C> 6.39 <R> <C> CDVAE-GANSP [SP] <C> 7.09 <C> 6.38 <C> 7.38 <C> 6.93 <C> 6.94 <R> <C> CDVAE-GANMCC [MCC] <C> 7.52 <C> 6.65 <C> 7.87 <C> 7.27 <C> 7.33 <R> <C> CDVAE-GANBOTH [MCC] <C> 7.44 <C> 6.85 <C> 7.90 <C> 7.30 <C> 7.37 <R> <C> CDVAE-CLS [MCC] <C> 6.65 <C> 6.29 <C> 7.02 <C> 6.49 <C> 6.61 <R> <C> CDVAE-CLS-GANSP [SP] <C> 7.23 <C> 6.91 <C> 7.64 <C> 7.08 <C> 7.21 <R> <C> CDVAE-CLS-GANMCC [MCC] <C> 7.71 <C> 6.62 <C> 7.76 <C> 7.05 <C> 7.29 <R> <C> CDVAE-CLS-GANBOTH [MCC] <C> 7.57 <C> 7.13 <C> 8.12 <C> 7.30 <C> 7.53 <CAP> TABLE II: Mean Mel-cepstral distortions [dB] of all non-silent frames in the evaluation set for the compared models.
<R> <C> Settings <C> Entity P <C> Entity R <C> Entity F1 <C> Relation P <C> Relation R <C> Relation F1 <R> <C> Our Model (SPTree) <C> 0.815 <C> 0.821 <C> 0.818 <C> 0.506 <C> 0.529 <C> 0.518 <R> <C> −Entity pretraining (EP) <C> 0.793 <C> 0.798 <C> 0.796 <C> 0.494 <C> 0.491 <C> 0.492* <R> <C> −Scheduled sampling (SS) <C> 0.812 <C> 0.818 <C> 0.815 <C> 0.522 <C> 0.490 <C> 0.505 <R> <C> −Label embeddings (LE) <C> 0.811 <C> 0.821 <C> 0.816 <C> 0.512 <C> 0.499 <C> 0.505 <R> <C> −Shared parameters (Shared) <C> 0.796 <C> 0.820 <C> 0.808 <C> 0.541 <C> 0.482 <C> 0.510 <R> <C> −EP, SS <C> 0.781 <C> 0.804 <C> 0.792 <C> 0.509 <C> 0.479 <C> 0.494* <R> <C> −EP, SS, LE, Shared <C> 0.800 <C> 0.815 <C> 0.807 <C> 0.520 <C> 0.452 <C> 0.484** <CAP> Table 2: Ablation tests on the ACE05 development dataset. * denotes significance at p<0.05, ** denotes p<0.01.
<R> <C> [EMPTY] <C> BLEU-1 <C> BLEU-2 <C> BLEU-3 <C> BLEU-4 <C> METEOR <C> ROUGE-L <C> CIDEr <R> <C> m-RNN (Mao et al.,  2015a ) <C> 0.036 <C> 0.019 <C> 0.000 <C> 0.000 <C> 0.021 <C> 0.084 <C> 0.003 <R> <C> m-LSTM-long <C> 0.310 <C> 0.257 <C> 0.216 <C> 0.169 <C> 0.145 <C> 0.244 <C> 0.696 <R> <C> sgLSTM-NYC-ave <C> 0.237 <C> 0.194 <C> 0.160 <C> 0.133 <C> 0.122 <C> 0.198 <C> [BOLD] 1.270 <R> <C> sgLSTM-GloVe-tfidf-50 <C> [BOLD] 0.417 <C> [BOLD] 0.381 <C> [BOLD] 0.359 <C> [BOLD] 0.339 <C> [BOLD] 0.211 <C> [BOLD] 0.365 <C> 1.010 <R> <C> sgLSTM-GloVe-tfidf-300 <C> 0.281 <C> 0.279 <C> 0.278 <C> 0.276 <C> 0.154 <C> 0.248 <C> 0.177 <CAP> Table 2: Numerical results of the proposed framework compared with other methods based on the testing images in datal.
<R> <C> [EMPTY] <C> [BOLD] Approach <C> [BOLD] HS <C> [BOLD] BP <C> [BOLD] ALM <C> [BOLD] BLM <C> [BOLD] D <C> [BOLD] PE <C> [BOLD] Rank <R> <C> MFD <C> Moral Freq <C> 58.0 <C> 59.5 <C> 68.8 <C> 80.5 <C> 37.0 <C> 68.8 <C> 5.5 <R> <C> MFD <C> Moral Stats <C> 64.8 <C> 67.3 <C> 74.1 <C> 80.2 <C> 39.2 <C> 75.6 <C> 4.0 <R> <C> MFD <C> SIMON <C> 74.0 <C> [BOLD] 79.6 <C> 79.1 <C> 82.9 <C> 78.4 <C> 74.0 <C> 2.3⋆ <R> <C> MoralStrength <C> Moral Freq <C> 59.8 <C> 61.4 <C> 68.4 <C> 80.9 <C> 38.2 <C> 67.9 <C> 5.2 <R> <C> MoralStrength <C> Moral Stats <C> 68.6 <C> 67.9 <C> 76.6 <C> 83.7 <C> 41.3 <C> [BOLD] 77.1 <C> 2.5⋆ <R> <C> MoralStrength <C> SIMON <C> [BOLD] 75.3 <C> 78.7 <C> [BOLD] 79.3 <C> [BOLD] 83.9 <C> [BOLD] 78.8 <C> 75.0 <C> [BOLD] 1.5⋆ <CAP> Table 12: Average F1-Score of the proposed baselines using under-sampling over all datasets. ‘⋆’ marks that the approach statistically outperforms the Moral Freq. with the MFD lexicon baseline. The model with the lowest rank is the one that outperforms the rest.
<R> <C> [EMPTY] <C> Percent Trusted <C> Trusted Only <C> No Corr. <C> Forward <C> Forward Gold <C> Distill. <C> Confusion Matrix <C> GLC (Ours) <R> <C> CIFAR-10 <C> 1 <C> 62.9 <C> 28.3 <C> 28.1 <C> 30.9 <C> 60.4 <C> 31.9 <C> 26.9 <R> <C> CIFAR-10 <C> 5 <C> 39.6 <C> 27.1 <C> 26.6 <C> 25.5 <C> 28.1 <C> 27 <C> 21.9 <R> <C> CIFAR-10 <C> 10 <C> 31.3 <C> 25.9 <C> 25.1 <C> 22.9 <C> 17.8 <C> 24.2 <C> 19.2 <R> <C> Mean <C> Mean <C> 44.6 <C> 27.1 <C> 26.6 <C> 26.4 <C> 35.44 <C> 27.7 <C> [BOLD] 22.7 <R> <C> CIFAR-100 <C> 5 <C> 82.4 <C> 71.1 <C> 73.9 <C> 73.6 <C> 88.3 <C> 74.1 <C> 68.7 <R> <C> CIFAR-100 <C> 10 <C> 67.3 <C> 66 <C> 68.2 <C> 66.1 <C> 62.5 <C> 63.8 <C> 56.6 <R> <C> CIFAR-100 <C> 25 <C> 52.2 <C> 56.9 <C> 56.9 <C> 51.4 <C> 39.7 <C> 50.8 <C> 40.8 <R> <C> Mean <C> Mean <C> 67.3 <C> 64.7 <C> 66.3 <C> 63.7 <C> 63.5 <C> 62.9 <C> [BOLD] 55.4 <CAP> Table 3: Results when obtaining noisy labels by sampling from the softmax distribution of a weak classifier. Percent trusted is the trusted fraction multiplied by 100. Unless otherwise indicated, all values are the percent error. The best average result for each dataset is shown in bold.
<R> <C> [EMPTY] <C> Corruption Type <C> Percent Trusted <C> Trusted Only <C> No Corr. <C> Forward <C> Forward Gold <C> Distill. <C> Confusion Matrix <C> GLC (Ours) <R> <C> MNIST <C> Uniform <C> 5 <C> 37.6 <C> 12.9 <C> 14.5 <C> 13.5 <C> 42.1 <C> 21.8 <C> 10.3 <R> <C> MNIST <C> Uniform <C> 10 <C> 12.9 <C> 12.3 <C> 13.9 <C> 12.3 <C> 9.2 <C> 15.1 <C> 6.3 <R> <C> MNIST <C> Uniform <C> 25 <C> 6.6 <C> 9.3 <C> 11.8 <C> 9.2 <C> 5.8 <C> 11.0 <C> 4.7 <R> <C> 2-10 <C> Flip <C> 5 <C> 37.6 <C> 50.1 <C> 51.7 <C> 41.4 <C> 46.6 <C> 11.7 <C> 3.4 <R> <C> [EMPTY] <C> Flip <C> 10 <C> 12.9 <C> 51.1 <C> 48.8 <C> 36.4 <C> 32.4 <C> 5.6 <C> 2.9 <R> <C> [EMPTY] <C> Flip <C> 25 <C> 6.6 <C> 47.7 <C> 50.2 <C> 37.1 <C> 28.2 <C> 3.8 <C> 2.6 <R> <C> Mean <C> Mean <C> Mean <C> 19.0 <C> 30.6 <C> 31.8 <C> 25.0 <C> 27.4 <C> 11.5 <C> [BOLD] 5.0 <R> <C> SVHN <C> Uniform <C> 0.1 <C> 80.4 <C> 25.5 <C> 26.2 <C> 26.8 <C> 80.9 <C> 25.7 <C> 24.4 <R> <C> SVHN <C> Uniform <C> 1 <C> 79.7 <C> 25.5 <C> 24.2 <C> 24.9 <C> 80.4 <C> 28.2 <C> 28.1 <R> <C> SVHN <C> Uniform <C> 5 <C> 24.3 <C> 25.5 <C> 15.0 <C> 15.7 <C> 24.1 <C> 2.7 <C> 2.8 <R> <C> 2-10 <C> Flip <C> 0.1 <C> 80.4 <C> 51.0 <C> 51.0 <C> 50.9 <C> 89.1 <C> 19.8 <C> 19.4 <R> <C> [EMPTY] <C> Flip <C> 1 <C> 79.7 <C> 51.0 <C> 43.9 <C> 49.5 <C> 86.3 <C> 17.8 <C> 21.7 <R> <C> [EMPTY] <C> Flip <C> 5 <C> 24.3 <C> 51.0 <C> 43.2 <C> 49.0 <C> 17.6 <C> 2.2 <C> 2.2 <R> <C> Mean <C> Mean <C> Mean <C> 61.5 <C> 38.2 <C> 33.9 <C> 36.1 <C> 63.1 <C> [BOLD] 16.1 <C> 16.4 <R> <C> CIFAR-10 <C> Uniform <C> 5 <C> 39.6 <C> 31.9 <C> 9.1 <C> 27.9 <C> 29.7 <C> 22.4 <C> 9.0 <R> <C> CIFAR-10 <C> Uniform <C> 10 <C> 31.3 <C> 31.9 <C> 8.6 <C> 20.6 <C> 18.3 <C> 22.7 <C> 6.9 <R> <C> CIFAR-10 <C> Uniform <C> 25 <C> 17.4 <C> 32.7 <C> 7.7 <C> 27.1 <C> 11.6 <C> 16.7 <C> 6.4 <R> <C> 2-10 <C> Flip <C> 5 <C> 39.6 <C> 53.3 <C> 38.6 <C> 47.8 <C> 29.7 <C> 8.1 <C> 6.6 <R> <C> [EMPTY] <C> Flip <C> 10 <C> 31.3 <C> 53.2 <C> 36.5 <C> 51.0 <C> 18.1 <C> 8.2 <C> 6.2 <R> <C> [EMPTY] <C> Flip <C> 25 <C> 17.4 <C> 52.7 <C> 37.6 <C> 49.5 <C> 11.8 <C> 7.1 <C> 6.1 <R> <C> Mean <C> Mean <C> Mean <C> 29.4 <C> 42.6 <C> 23.0 <C> 37.3 <C> 19.9 <C> 14.2 <C> [BOLD] 6.9 <R> <C> CIFAR-100 <C> Uniform <C> 5 <C> 82.4 <C> 48.8 <C> 47.7 <C> 49.6 <C> 87.5 <C> 53.6 <C> 42.4 <R> <C> CIFAR-100 <C> Uniform <C> 10 <C> 67.3 <C> 48.4 <C> 47.2 <C> 48.9 <C> 61.2 <C> 49.7 <C> 33.9 <R> <C> CIFAR-100 <C> Uniform <C> 25 <C> 52.2 <C> 45.4 <C> 43.6 <C> 46.0 <C> 39.8 <C> 39.6 <C> 27.3 <R> <C> 2-10 <C> Flip <C> 5 <C> 82.4 <C> 62.1 <C> 61.6 <C> 62.6 <C> 87.1 <C> 28.6 <C> 27.1 <R> <C> [EMPTY] <C> Flip <C> 10 <C> 67.3 <C> 61.9 <C> 61.0 <C> 62.2 <C> 61.9 <C> 26.9 <C> 25.8 <R> <C> [EMPTY] <C> Flip <C> 25 <C> 52.2 <C> 59.6 <C> 57.5 <C> 61.4 <C> 40.0 <C> 25.1 <C> 24.7 <R> <C> 2-10 <C> Hierarchical <C> 5 <C> 82.4 <C> 50.9 <C> 51.0 <C> 52.4 <C> 87.1 <C> 45.8 <C> 34.8 <R> <C> [EMPTY] <C> Hierarchical <C> 10 <C> 67.3 <C> 51.9 <C> 50.5 <C> 52.1 <C> 61.7 <C> 38.8 <C> 30.2 <R> <C> [EMPTY] <C> Hierarchical <C> 25 <C> 52.2 <C> 54.3 <C> 47.0 <C> 51.1 <C> 39.7 <C> 29.7 <C> 25.4 <R> <C> Mean <C> Mean <C> Mean <C> 67.3 <C> 53.7 <C> 51.9 <C> 54.0 <C> 62.9 <C> 37.5 <C> [BOLD] 30.2 <CAP> Table 5: Vision dataset results. These differ from the results in the paper by the addition of SVHN. Percent trusted is the trusted fraction multiplied by 100. Unless otherwise indicated, all values are percentages representing the area under the error curve computed at 11 test points. The best mean result is shown in bold.
<R> <C> Method <C> Repeats <C> Skips <C> Error Sentences <C> Error Rate <R> <C> [ITALIC] Tacotron 2 <C> 4 <C> 11 <C> 12 <C> 24% <R> <C> [ITALIC] Transformer TTS <C> 7 <C> 15 <C> 17 <C> 34% <R> <C> [ITALIC] FastSpeech <C> 0 <C> 0 <C> 0 <C> 0% <CAP> Table 3: The comparison of robustness between FastSpeech and other systems on the 50 particularly hard sentences. Each kind of word error is counted at most once per sentence.
<R> <C> Metric <C> NN <R> <C> Success <C> 98% <R> <C> Comprehension <C> 4.11 <R> <C> Naturalness <C> 4.05 <R> <C> # of dialogues: <C> 245 <CAP> Table 3: Human assessment of the NN system. The rating for comprehension/naturalness are both out of 5.
<R> <C> Model <C> Interpretation estimation task <C> New interpretation task <R> <C> SCAIN <C> 0.841 <C> 0.410 <R> <C> No Kalman <C> 0.129 <C> 0.121 <R> <C> Fewer Particle <C> 0.166 <C> 0.127 <CAP> Table 2: Accuracy for the Wikipedia dataset, with the majority choice in terms of confidence taken as correct.
<R> <C> [BOLD] Models <C> [BOLD] Multi-domain  [BOLD] BLEU <C> [BOLD] Multi-domain  [BOLD] Inform <C> [BOLD] Multi-domain  [BOLD] Success <R> <C> [ITALIC] Comparisons <C> [ITALIC] Comparisons <C> [ITALIC] Comparisons <C> [ITALIC] Comparisons <R> <C> Seq2seq <C> 16.7 <C> 65.7 <C> 44.4 <R> <C> HRED <C> 17.5 <C> 70.7 <C> 60.9 <R> <C> Seq2seq + MDBT <C> 13.1 <C> 69.3 <C> 30.0 <R> <C> Seq2seq + TRADE <C> 13.2 <C> 65.9 <C> 34.6 <R> <C> HRED + MDBT <C> 13.1 <C> 68.8 <C> 35.5 <R> <C> HRED + TRADE <C> 13.7 <C> 70.8 <C> 41.8 <R> <C> HRED-MTSS(ours) <C> [BOLD] 18.7 <C> [BOLD] 77.5 <C> [BOLD] 64.9 <R> <C> [ITALIC] State-of-the-art models <C> [ITALIC] State-of-the-art models <C> [ITALIC] State-of-the-art models <C> [ITALIC] State-of-the-art models <R> <C> LaRL + TRADE <C> 12.4 <C> [BOLD] 79.5 <C> 44.7 <R> <C> HDSA + TRADE <C> [BOLD] 20.1 <C> 76.4 <C> [BOLD] 65.9 <R> <C> [ITALIC] Models with manual states <C> [ITALIC] Models with manual states <C> [ITALIC] Models with manual states <C> [ITALIC] Models with manual states <R> <C> Seq2seq + Manual states <C> 17.8 <C> 75.4 <C> 62.8 <R> <C> HRED + Manual states <C> 19.3 <C> 75.2 <C> 66.2 <R> <C> HDSA + Manual states <C> [BOLD] 22.9 <C> [BOLD] 82.3 <C> [BOLD] 75.1 <CAP> Table 3: Performance on the multi-domain environment.
<R> <C> [BOLD] Models <C> [BOLD] Restaurant  [BOLD] Inform <C> [BOLD] Restaurant  [BOLD] Success <C> [BOLD] Hotel  [BOLD] Inform <C> [BOLD] Hotel  [BOLD] Success <C> [BOLD] Train  [BOLD] Inform <C> [BOLD] Train  [BOLD] Success <C> [BOLD] Attraction  [BOLD] Inform <C> [BOLD] Attraction  [BOLD] Success <R> <C> Seq2seq + TRADE <C> 88.6 <C> 57.9 <C> 90.9 <C> 42.4 <C> 72.1 <C> 60.8 <C> 63.9 <C> 55.3 <R> <C> HRED + TRADE <C> [BOLD] 91.8 <C> 74.4 <C> 81.7 <C> 50.5 <C> 76.2 <C> 62.6 <C> 76.8 <C> 65.4 <R> <C> HDSA + TRADE <C> 78.5 <C> 68.6 <C> [BOLD] 91.4 <C> [BOLD] 85.3 <C> 81.4 <C> 80.4 <C> [BOLD] 93.9 <C> [BOLD] 82.1 <R> <C> HRED-MTSS(ours) <C> 87.4 <C> [BOLD] 81.2 <C> 86.8 <C> 81.5 <C> [BOLD] 85.1 <C> [BOLD] 83.4 <C> 86.6 <C> 74.5 <CAP> Table 4: Results on different domains
<R> <C> [BOLD] Distill weights  [ITALIC] α1 <C> [BOLD] Distill weights  [ITALIC] α2 <C> [BOLD] Multi-domain  [BOLD] BLEU <C> [BOLD] Multi-domain  [BOLD] Inform <C> [BOLD] Multi-domain  [BOLD] Success <R> <C> 0.01 <C> 0.005 <C> 17.0 <C> 71.7 <C> 63.5 <R> <C> 0.005 <C> 0.01 <C> [BOLD] 18.9 <C> 73.6 <C> 61.2 <R> <C> 0.005 <C> 0.005 <C> 18.7 <C> [BOLD] 77.5 <C> [BOLD] 64.9 <R> <C> 0.0025 <C> 0.005 <C> 18.1 <C> 73.1 <C> 63.9 <R> <C> 0.01 <C> 0 <C> 17.0 <C> 72.2 <C> 62.0 <R> <C> 0.005 <C> 0 <C> 18.3 <C> 72.2 <C> 63.4 <R> <C> 0 <C> 0.01 <C> 18.2 <C> 77.1 <C> 64.7 <R> <C> 0 <C> 0.005 <C> 18.3 <C> 74.6 <C> 63.2 <R> <C> 0 <C> 0 <C> 17.5 <C> 70.7 <C> 60.9 <CAP> Table 5: Results of adopting different distillation strategies. The last column is the results of a model without distilling.
<R> <C> [EMPTY] <C> TransE MRR <C> TransE MRR <C> TransE Hits@1 <C> TransE Hits@1 <C> DistMult MRR <C> DistMult MRR <C> DistMult Hits@1 <C> DistMult Hits@1 <C> ComplEx MRR <C> ComplEx MRR <C> ComplEx Hits@1 <C> ComplEx Hits@1 <R> <C> Dataset <C> Filter <C> Raw <C> Filter <C> Raw <C> Filter <C> Raw <C> Filter <C> Raw <C> Filter <C> Raw <C> Filter <C> Raw <R> <C> WN18 <C> [BOLD] 0.971 <C> [BOLD] 0.969 <C> [BOLD] 0.956 <C> [BOLD] 0.952 <C> 0.623 <C> 0.622 <C> 0.256 <C> 0.256 <C> [BOLD] 0.991 <C> [BOLD] 0.989 <C> [BOLD] 0.987 <C> [BOLD] 0.983 <R> <C> FB15k <C> 0.883 <C> 0.773 <C> 0.829 <C> 0.650 <C> 0.695 <C> 0.644 <C> 0.463 <C> 0.408 <C> [BOLD] 0.971 <C> [BOLD] 0.840 <C> [BOLD] 0.950 <C> [BOLD] 0.726 <R> <C> WN18RR <C> 0.843 <C> 0.842 <C> 0.735 <C> 0.734 <C> 0.871 <C> 0.866 <C> 0.810 <C> 0.802 <C> [BOLD] 0.894 <C> [BOLD] 0.893 <C> [BOLD] 0.813 <C> [BOLD] 0.813 <R> <C> FB15k-237 <C> [BOLD] 0.955 <C> [BOLD] 0.950 <C> [BOLD] 0.930 <C> [BOLD] 0.921 <C> 0.926 <C> 0.921 <C> 0.880 <C> 0.871 <C> [BOLD] 0.956 <C> [BOLD] 0.950 <C> [BOLD] 0.933 <C> [BOLD] 0.922 <R> <C> FB3M <C> 0.475 <C> 0.464 <C> 0.364 <C> 0.347 <C> 0.620 <C> 0.607 <C> 0.439 <C> 0.432 <C> [BOLD] 0.683 <C> [BOLD] 0.639 <C> [BOLD] 0.460 <C> [BOLD] 0.410 <CAP> Table 1: Evaluation results on relation prediction.
<R> <C> [EMPTY] <C> [BOLD] Avg P <C> [BOLD] Avg R <C> [BOLD] Avg F1 <C> [BOLD] Acc <R> <C> Naive (majority class) <C> 0.269 <C> 0.520 <C> 0.355 <C> 0.520 <R> <C> LR <C> 0.702 <C> 0.697 <C> 0.699 <C> 0.701 <R> <C> SPR <C> 0.794 <C> 0.793 <C> 0.793 <C> 0.792 <R> <C> SPR +Narrative types <C> 0.806 <C> 0.804 <C> 0.805 <C> 0.804 <CAP> Table 1: Test set results for relation classification
<R> <C> Model <C> Run <C> BLEU <C> Challenge sets <R> <C> Random <C> 1 <C> 31.9 <C> 61.5 <R> <C> Random <C> 2 <C> 32.0 <C> 60.2 <R> <C> Random <C> 3 <C> 31.5 <C> 60.0 <R> <C> Random <C> Avg. <C> 31.8 ± 0.2 <C> 60.6 ± 0.7 <R> <C> Partial copy <C> 1 <C> 31.7 <C> 78.4 <R> <C> Partial copy <C> 2 <C> 31.4 <C> 78.8 <R> <C> Partial copy <C> 3 <C> 31.6 <C> 79.0 <R> <C> Partial copy <C> Avg. <C> 31.6 ± 0.1 <C> 78.7 ± 0.2 <CAP> Table 4: Robustness experiments across 3 runs with different data and model random seeds. Each challenge set is weighted equally.
<R> <C> Model <C> Deixis <C> Lex. cohesion <C> Ellipsis (infl.) <C> Ellipsis (VP) <R> <C> Baseline <C> 50.0 (50.0) <C> 45.9 (46.2) <C> 54.0 <C> 28.8 <R> <C> Concat <C> 87.8 (88.8) <C> 86.1 (85.6) <C> [BOLD] 87.6 <C> [BOLD] 88.8 <R> <C> Random <C> 85.4 (87.6) <C> 82.5 (80.0) <C> 82.8 <C> 85.2 <R> <C> Partial copy <C> 89.6 (89.8) <C> [BOLD] 90.1 (89.8) <C> 86.6 <C> 88.6 <R> <C> Context generation <C> 90.6 (90.8) <C> 85.3 (83.2) <C> 85.6 <C> 87.2 <R> <C> DocRepair (Voita et al.,  2019 ) <C> [BOLD] 91.8 <C> 80.6 <C> 86.4 <C> 75.2 <CAP> Table 6: En→De challenge set accuracy, with additional back-translated data. Validation results in parentheses.
<R> <C> Task <C> Base <C> RNN <C> GRU <C> LSTM <C> BERT <R> <C> 1 (Eng) <C> 52.1 <C> 50.1 <C> 50.6 <C> 50.4 <C> [BOLD] 99.8 <R> <C> 2 (Eng) <C> 50.7 <C> 50.2 <C> 50.2 <C> 50.8 <C> [BOLD] 100 <R> <C> 3 (Eng) <C> 63.5 <C> 50.3 <C> 66.1 <C> 63.5 <C> [BOLD] 90.5 <R> <C> 4 (Eng) <C> 51.0 <C> 51.7 <C> 52.7 <C> 51.6 <C> [BOLD] 100 <R> <C> 5 (Eng) <C> 50.6 <C> 50.1 <C> 50.2 <C> 50.2 <C> [BOLD] 100 <R> <C> 6 (Eng) <C> 55.5 <C> 84.4 <C> 82.7 <C> 75.1 <C> [BOLD] 87.5 <R> <C> 7 (Eng) <C> 54.1 <C> 50.9 <C> 53.7 <C> 50.0 <C> [BOLD] 94.6 <R> <C> Avg. <C> 53.9 <C> 55.4 <C> 58.0 <C> 56.2 <C> [BOLD] 96.1 <R> <C> 1 (Pt) <C> 53.9 <C> 50.1 <C> 50.2 <C> 50.0 <C> [BOLD] 99.9 <R> <C> 2 (Pt) <C> 49.8 <C> 50.0 <C> 50.0 <C> 50.0 <C> [BOLD] 99.9 <R> <C> 3 (Pt) <C> 61.7 <C> 50.0 <C> 70.6 <C> 50.1 <C> [BOLD] 78.7 <R> <C> 4 (Pt) <C> 50.9 <C> 50.0 <C> 50.4 <C> 50.0 <C> [BOLD] 100 <R> <C> 5 (Pt) <C> 49.9 <C> 50.1 <C> 50.8 <C> 50.0 <C> [BOLD] 99.8 <R> <C> 6 (Pt) <C> 58.9 <C> 66.4 <C> [BOLD] 79.7 <C> 67.2 <C> 79.1 <R> <C> 7 (Pt) <C> 55.4 <C> 51.1 <C> 51.6 <C> 51.1 <C> [BOLD] 82.7 <R> <C> Avg. <C> 54.4 <C> 52.6 <C> 57.6 <C> 52.6 <C> [BOLD] 91.4 <CAP> Table 2: Results of the experiment (i), accuracy percentage on test data for the English and Portuguese corpora
<R> <C> Beer <C> Appearance S <C> Appearance P <C> Appearance R <C> Appearance F1 <C> Aroma S <C> Aroma P <C> Aroma R <C> Aroma F1 <C> Palate S <C> Palate P <C> Palate R <C> Palate F1 <R> <C> Rnp [lei2016rationalizing] <C> 11.9 <C> 72.0 <C> 46.1 <C> 56.2 <C> 10.7 <C> [BOLD] 70.5 <C> [BOLD] 48.3 <C> [BOLD] 57.3 <C> 10.0 <C> 53.1 <C> 42.8 <C> 47.5 <R> <C> Post-exp <C> 11.9 <C> 64.2 <C> 41.4 <C> 50.4 <C> 10.3 <C> 50.0 <C> 33.1 <C> 39.8 <C> 10.0 <C> 33.0 <C> 26.5 <C> 29.4 <R> <C> Car <C> 11.9 <C> [BOLD] 76.2 <C> [BOLD] 49.3 <C> [BOLD] 59.9 <C> 10.3 <C> 50.3 <C> 33.3 <C> 40.1 <C> 10.2 <C> [BOLD] 56.6 <C> [BOLD] 46.2 <C> [BOLD] 50.9 <CAP> Table 2: Objective performances of selected factual rationales for both (a) beer and (b) hotel review datasets. Each aspect is trained independently. S, P, R, and F1 indicate the sparsity level, precision, recall, and F1 score.
<R> <C> Hotel <C> Location S <C> Location P <C> Location R <C> Location F1 <C> Service S <C> Service P <C> Service R <C> Service F1 <C> Cleanliness S <C> Cleanliness P <C> Cleanliness R <C> Cleanliness F1 <R> <C> Rnp [lei2016rationalizing] <C> 10.9 <C> 43.3 <C> 55.5 <C> 48.6 <C> 11.0 <C> 40.0 <C> 38.2 <C> 39.1 <C> 10.6 <C> 30.5 <C> [BOLD] 36.0 <C> 33.0 <R> <C> Post-exp <C> 8.9 <C> 30.4 <C> 31.8 <C> 31.1 <C> 10.0 <C> 32.5 <C> 28.3 <C> 30.3 <C> 9.2 <C> 23.0 <C> 23.7 <C> 23.3 <R> <C> Car <C> 10.6 <C> [BOLD] 46.6 <C> [BOLD] 58.1 <C> [BOLD] 51.7 <C> 11.7 <C> [BOLD] 40.7 <C> [BOLD] 41.4 <C> [BOLD] 41.1 <C> 9.9 <C> [BOLD] 32.3 <C> 35.7 <C> [BOLD] 33.9 <CAP> Table 2: Objective performances of selected factual rationales for both (a) beer and (b) hotel review datasets. Each aspect is trained independently. S, P, R, and F1 indicate the sparsity level, precision, recall, and F1 score.
<R> <C> System <C> Target speakers (%WER) VCC2TF1 <C> Target speakers (%WER) VCC2TF2 <C> Target speakers (%WER) VCC2TM1 <C> Target speakers (%WER) VCC2TM2 <R> <C> XV <C> 3.25 <C> 2.98 <C> 3.66 <C> 10.57 <R> <C> N10= <C> 9.21 <C> 7.99 <C> 11.79 <C> 9.89 <R> <C> N10× <C> 9.62 <C> 11.52 <C> 8.67 <C> 9.21 <R> <C> N13= <C> 23.31 <C> 21.68 <C> 31.57 <C> 27.37 <R> <C> N13× <C> 32.25 <C> 24.80 <C> 21.41 <C> 26.96 <R> <C> N17= <C> 25.47 <C> 24.39 <C> 33.47 <C> 23.71 <R> <C> N17× <C> 38.08 <C> 31.44 <C> 35.23 <C> 25.88 <R> <C> VCA= [ITALIC] u <C> 25.34 <C> 26.02 <C> 27.37 <C> 25.75 <R> <C> VCA× [ITALIC] u <C> 30.62 <C> 27.51 <C> 23.71 <C> 22.63 <R> <C> TTS [ITALIC] u <C> 7.72 <C> 8.40 <C> 6.23 <C> 7.18 <R> <C> [EMPTY] <C> Source speakers (%WER) <C> Source speakers (%WER) <C> Source speakers (%WER) <C> Source speakers (%WER) <R> <C> [EMPTY] <C> VCC2SF3 <C> VCC2SF4 <C> VCC2SM3 <C> VCC2SM4 <R> <C> S00 <C> 5.69 <C> 4.88 <C> 5.69 <C> 7.32 <CAP> TABLE II: Word error rate for objective evaluation of scenario A
<R> <C> [BOLD] # of topics <C> [BOLD] uniqueness <C> [BOLD] perplexity <C> [ITALIC] ρ×100 <R> <C> 5 <C> 34.05 <C> 9.88 <C> 67.1 <R> <C> 10 <C> 32.23 <C> 9.78 <C> 68.5 <R> <C> 15 <C> 29.57 <C> 9.70 <C> 67.8 <R> <C> 20 <C> 27.12 <C> 9.65 <C> 66.9 <CAP> Table 6: effect of number of topics on Spearman correlation on 50 word pairs from WordSim-353
<R> <C> [ITALIC] pthres <C> [ITALIC] ρ×100 <C> senses captured for  [ITALIC] network <R> <C> 1e-3 <C> 68.3 <C> television, IT <R> <C> 1e-4 <C> 69.1 <C> television, IT, transportation <R> <C> 1e-5 <C> 68.4 <C> mixed senses <CAP> Table 7: Spearman’s correlation ρ×100 on 50 word pairs from WS-353 and the word senses captured for network. The word senses are adjudged qualitatively.
<R> <C> [ITALIC] TASK <C> [ITALIC] N\small XENT <C> [ITALIC] N\small XE+R <C> Δ <R> <C> [ITALIC] summarization <C> 20 <C> 5 <C> 2 <R> <C> [ITALIC] machine translation <C> 25 <C> 5 <C> 3 <R> <C> [ITALIC] image captioning <C> 20 <C> 5 <C> 2 <CAP> Table 2: Best scheduling parameters found by hyper-parameter search of MIXER.
<R> <C> [EMPTY] <C> English <C> German <R> <C> ICD <C> 58.31 <C> 38.47 <R> <C> IFRS <C> 74.76 <C> 49.81 <R> <C> Wikipedia <C> 32.19 <C> 25.55 <CAP> Table 5: Vocabulary overlap (in %) between the ICD, IFRS and Wikipedia evaluation dataset and the development dataset used for domain adaptation.
<R> <C> SMT <C> ICD <C> ICD <C> BLEU inclus. <C> BLEU constr. <C> BLEU exclus. <C> METEOR inclus. <C> METEOR constr. <C> METEOR exclus. <C> chrF3 inclus. <C> chrF3 constr. <C> chrF3 exclus. <R> <C> SMT <C> Wiki  [ITALIC] p=1 <C> Wiki  [ITALIC] p=1 <C> 4.91 <C> 3.56 <C> 3.29 <C> 14.27 <C> 12.01 <C> 12.01 <C> 42.66 <C> 38.97 <C> 38.99 <R> <C> SMT <C> Wiki  [ITALIC] p= [ITALIC] cos( [ITALIC] x, [ITALIC] y) <C> Wiki  [ITALIC] p= [ITALIC] cos( [ITALIC] x, [ITALIC] y) <C> 5.03 <C> 3.42 <C> 3.37 <C> 14.58 <C> 12.04 <C> 12.18 <C> 43.44 <C> 38.99 <C> 39.17 <R> <C> SMT <C> In-dom  [ITALIC] p=1 <C> In-dom  [ITALIC] p=1 <C> 7.1 <C> 8.05 <C> 8.05 <C> 14.23 <C> 14.59 <C> 14.59 <C> 42.61 <C> 43.07 <C> 43.07 <R> <C> SMT <C> IFRS <C> IFRS <C> inclus. <C> constr. <C> exclus. <C> inclus. <C> constr. <C> exclus. <C> inclus. <C> constr. <C> exclus. <R> <C> SMT <C> Wiki  [ITALIC] p=1 <C> Wiki  [ITALIC] p=1 <C> 10.23 <C> 6.54 <C> 6.44 <C> 15.64 <C> 12.4 <C> 12.29 <C> 45.09 <C> 38.35 <C> 38.31 <R> <C> SMT <C> Wiki  [ITALIC] p= [ITALIC] cos( [ITALIC] x, [ITALIC] y) <C> Wiki  [ITALIC] p= [ITALIC] cos( [ITALIC] x, [ITALIC] y) <C> 10.54 <C> 6.52 <C> 6.42 <C> 16.17 <C> 12.39 <C> 12.26 <C> 46.13 <C> 38.45 <C> 38.49 <R> <C> SMT <C> In-dom  [ITALIC] p=1 <C> In-dom  [ITALIC] p=1 <C> 26.72 <C> 29.65 <C> 29.69 <C> 22.47 <C> 24.3 <C> 24.31 <C> 54.44 <C> 57.3 <C> 57.30 <R> <C> SMT <C> Wiki <C> Wiki <C> inclus. <C> constr. <C> exclus. <C> inclus. <C> constr. <C> exclus. <C> inclus. <C> constr. <C> exclus. <R> <C> SMT <C> Wiki  [ITALIC] p=1 <C> Wiki  [ITALIC] p=1 <C> 16.59 <C> 17.03 <C> 17.11 <C> 23.46 <C> 23.48 <C> 23.52 <C> 52.22 <C> 51.62 <C> 51.62 <R> <C> SMT <C> Wiki  [ITALIC] p= [ITALIC] cos( [ITALIC] x, [ITALIC] y) <C> Wiki  [ITALIC] p= [ITALIC] cos( [ITALIC] x, [ITALIC] y) <C> 34.7 <C> 42.68 <C> 42.84 <C> 39.49 <C> 44.91 <C> 44.95 <C> 74.27 <C> 80.62 <C> 80.64 <R> <C> [EMPTY] <C> In-dom  [ITALIC] p=1 <C> [EMPTY] <C> / <C> / <C> / <C> / <C> / <C> / <C> / <C> / <C> / <R> <C> NMT <C> [EMPTY] <C> [EMPTY] <C> ICD <C> IFRS <C> Wiki <C> ICD <C> IFRS <C> Wiki <C> ICD <C> IFRS <C> Wiki <R> <C> NMT <C> Wikipedia <C> 1-gram <C> 6.88 <C> 11.71 <C> 15.74 <C> 12.94 <C> 15.55 <C> 22.16 <C> 35.2 <C> 36.49 <C> 46.86 <R> <C> NMT <C> Wikipedia <C> Lex-align <C> 6.46 <C> 11.90 <C> 13.52 <C> 12.92 <C> 15.66 <C> 20.23 <C> 36.49 <C> 36.32 <C> 44.52 <R> <C> NMT <C> In-domain <C> 1-gram <C> 10.41 <C> 14.66 <C> / <C> 16.69 <C> 18.2 <C> / <C> 43.05 <C> 44.35 <C> / <R> <C> [EMPTY] <C> In-domain <C> Lex-align <C> 10.39 <C> 14.65 <C> / <C> 16.72 <C> 18.19 <C> / <C> 43.09 <C> 44.34 <C> / <CAP> Table 6: Evaluation of term injection into SMT and NMT system (term injection methodology: inclus. = inclusive, constr. = constraint, exclus. = exclusive).
<R> <C> [BOLD] System <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] Weighted F1 Score <R> <C> [BOLD] SOTA  <C> 0.776 <C> - <C> - <R> <C> [BOLD] Lexicons <C> 0.788 <C> 0.798 <C> 0.789 <R> <C> [BOLD] GRU <C> 0.792 <C> 0.798 <C> 0.794 <R> <C> [BOLD] GRU + Lexicons <C> [BOLD] 0.812 <C> [BOLD] 0.815 <C> [BOLD] 0.804 <CAP> TABLE III: Analyzing the impact of changing the feature vector on ABCD corpus. Entries marked with bold are the best performing variants and those marked with - are not available.
<R> <C> [BOLD] Sequence Length <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] Weighted F1 Score <R> <C> [BOLD] 32 <C> 0.810 <C> 0.813 <C> [BOLD] 0.806 <R> <C> [BOLD] 64 <C> [BOLD] 0.812 <C> [BOLD] 0.815 <C> 0.804 <R> <C> [BOLD] 128 <C> 0.805 <C> 0.808 <C> 0.796 <CAP> TABLE IV: Investigating the impact of varying the maximum sequence length on the overall model performance. Entries marked with bold are the best performing variants
<R> <C> Dataset <C> class-1024(3) <C> BO-KN(3) <C> int-KN(3) <C> BO-MKN(3) <C> int-MKN(3) <C> PLRE(3) <R> <C> Small-English Dev <C> 115.64 <C> 99.20 <C> 99.73 <C> 99.95 <C> 95.63 <C> [BOLD] 91.18 <R> <C> Small-English Test <C> 119.70 <C> 103.86 <C> 104.56 <C> 104.55 <C> 100.07 <C> [BOLD] 95.15 <R> <C> Small-Russian Dev <C> 286.38 <C> 281.29 <C> 265.71 <C> 287.19 <C> 263.25 <C> [BOLD] 241.66 <R> <C> Small-Russian Test <C> 284.09 <C> 277.74 <C> 262.02 <C> 283.70 <C> 260.19 <C> [BOLD] 238.96 <CAP> Table 1: Perplexity results on small corpora for all methods.
<R> <C> Task <C> memN2N <C> KB-memN2N <R> <C> 1 <C> 98 <C> [BOLD] 98.1 <R> <C> 2 <C> [BOLD] 100 <C> 95.8 <R> <C> 3 <C> 96.4 <C> [BOLD] 97.1 <R> <C> 4 <C> 82.8 <C> [BOLD] 86.2 <R> <C> 5 <C> 90.1 <C> [BOLD] 93.5 <CAP> Table 1: Mean accuracy % on the DSTC6 dataset333We plan to perform experiments with match-type features in the future.
<R> <C> Task <C> memN2N <C> memN2N (match-type) <C> KB-memN2N <R> <C> 1 <C> 99.9 <C> [BOLD] 100 <C> [BOLD] 100 <R> <C> 2 <C> [BOLD] 100 <C> 98.3 <C> 91.9 <R> <C> 3 <C> [BOLD] 74.9 <C> [BOLD] 74.9 <C> 74.8 <R> <C> 4 <C> 59.5 <C> [BOLD] 100 <C> 57.2 <R> <C> 5 <C> [BOLD] 96.1 <C> 93.4 <C> 92.8 <CAP> Table 2: Mean accuracy % on dialog bAbI tasks; Results for memN2N and memN2N (match-type) are taken from Bordes and Weston (2016).
<R> <C> Models <C> FIGER Precision <C> FIGER Recall <C> FIGER F1 <C> 1k-WFB-g Precision <C> 1k-WFB-g Recall <C> 1k-WFB-g F1 <R> <C> LSTM-CNN-CRF (FIGER) <C> [BOLD] 87.17 <C> 28.95 <C> 43.47 <C> 91.41 <C> 37.13 <C> 52.81 <R> <C> CoreNLP <C> 83.82 <C> 80.99 <C> 82.38 <C> 75.46 <C> 64.12 <C> 69.33 <R> <C> NER Tagger <C> 80.44 <C> 84.01 <C> 82.19 <C> 77.25 <C> 68.52 <C> 72.62 <R> <C> LSTM-CNN-CRF (Wiki-NDS) <C> 86.14 <C> 30.91 <C> 45.49 <C> [BOLD] 92.80 <C> 47.09 <C> 62.48 <R> <C> LSTM-CNN-CRF (Wiki-FbF-w/o-III) <C> 88.07 <C> 44.58 <C> 59.2 <C> 92.55 <C> 65.03 <C> 76.39 <R> <C> LSTM-CNN-CRF (Wiki-FbF) <C> 79.80 <C> [BOLD] 86.32 <C> [BOLD] 82.94 <C> 89.89 <C> [BOLD] 81.98 <C> [BOLD] 85.75 <CAP> Table 4: Performance of the entity detection models on the FIGER and 1k-WFB-g datasets.
<R> <C> [BOLD] Training Datasets <C> [BOLD] FIGER Strict <C> [BOLD] FIGER Ma-F1 <C> [BOLD] FIGER Mi-F1 <C> [BOLD] 1k-WFB-g Strict <C> [BOLD] 1k-WFB-g Ma-F1 <C> [BOLD] 1k-WFB-g Mi-F1 <R> <C> FIGER <C> 25.07 <C> 34.56 <C> 36.47 <C> 27.76 <C> 35.14 <C> 37.31 <R> <C> Wiki-NDS <C> 30.07 <C> 37.89 <C> 38.55 <C> 39.12 <C> 49.28 <C> 51.60 <R> <C> Wiki-FbF <C> [BOLD] 56.31 <C> [BOLD] 70.70 <C> [BOLD] 68.23 <C> [BOLD] 53.34 <C> [BOLD] 68.42 <C> [BOLD] 69.23 <CAP> Table 5: Performance comparison for the FgER task.
<R> <C> [BOLD] Model <C> [BOLD] Case <C> [BOLD] TT-RU <C> [BOLD] RU-TT <R> <C> Baseline <C> insensitive <C> 26.40 <C> 23.90 <R> <C> Baseline <C> sensitive <C> 25.70 <C> 22.82 <R> <C> kk-init-1 <C> insensitive <C> 28.43 <C> 25.72 <R> <C> kk-init-1 <C> sensitive <C> 27.69 <C> 24.68 <R> <C> bt+kk-init-1 <C> insensitive <C> [BOLD] 30.06 <C> 26.40 <R> <C> bt+kk-init-1 <C> sensitive <C> [BOLD] 29.29 <C> 25.36 <R> <C> kk-init-6 <C> insensitive <C> 29.29 <C> [BOLD] 26.47 <R> <C> kk-init-6 <C> sensitive <C> 28.59 <C> [BOLD] 25.45 <R> <C> bt+kk-init-6 <C> insensitive <C> 29.41 <C> 25.94 <R> <C> bt+kk-init-6 <C> sensitive <C> 28.66 <C> 24.87 <CAP> TABLE I: Test scores of all trained models.
<R> <C> system A <C> system B <C> criterion <C> prefer A (%) <C> same (%) <C> prefer B (%) <R> <C> [EMPTY] <C> [EMPTY] <C> coherence <C> [BOLD] 54.16 <C> 13.76 <C> 32.07 <R> <C> INSET (ours) <C> baseline <C> fluency <C> [BOLD] 43.38 <C> 26.98 <C> 29.64 <R> <C> [EMPTY] <C> [EMPTY] <C> informativeness <C> [BOLD] 53.48 <C> 18.79 <C> 27.72 <R> <C> [EMPTY] <C> [EMPTY] <C> coherence <C> 27.87 <C> 15.69 <C> [BOLD] 56.44 <R> <C> INSET (ours) <C> ground truth <C> fluency <C> 21.78 <C> 31.38 <C> [BOLD] 46.84 <R> <C> [EMPTY] <C> [EMPTY] <C> informativeness <C> 27.49 <C> 21.92 <C> [BOLD] 50.59 <R> <C> INSET <C> [EMPTY] <C> coherence <C> 18.50 <C> 23.45 <C> [BOLD] 58.04 <R> <C> w/ keywords <C> ground truth <C> fluency <C> 17.82 <C> 29.78 <C> [BOLD] 52.39 <R> <C> w/ context <C> [EMPTY] <C> informativeness <C> 20.54 <C> 26.13 <C> [BOLD] 53.33 <R> <C> INSET <C> INSET <C> coherence <C> [BOLD] 37.71 <C> 37.62 <C> 24.68 <R> <C> w/ keywords <C> w/ keywords <C> fluency <C> 36.16 <C> [BOLD] 37.87 <C> 25.97 <R> <C> w/ context <C> w/o context <C> informativeness <C> 35.93 <C> [BOLD] 39.86 <C> 24.21 <R> <C> INSET <C> INSET <C> coherence <C> 34.97 <C> 17.06 <C> [BOLD] 47.97 <R> <C> w/ keywords <C> w/o keywords <C> fluency <C> 29.30 <C> 28.04 <C> [BOLD] 42.65 <R> <C> w/ context <C> w/ context <C> informativeness <C> 31.73 <C> 23.24 <C> [BOLD] 45.03 <CAP> Table 3: Human evaluation. “w/(w/o) keywords” and “w/(w/o) context” indicate whether the generation is based on keywords and context, respectively. All numbers are percentages.
<R> <C> [EMPTY] <C> NC <C> IWSLT <R> <C> Dropout <C> 30% <C> 30% <R> <C> Word dropout rate <C> 10% <C> 20% <R> <C> KL annealing steps <C> 80,000 <C> 80,000 <R> <C> KL( [ITALIC] q( [ITALIC] z)|| [ITALIC] p( [ITALIC] z)) on En-De <C> 5.94 <C> 8.01 <CAP> Table 1: Strategies to promote use of latent representation along with the validation KL achieved.
<R> <C> Methods <C> METEOR <C> BLEU-1 <C> BLEU-2 <C> BLEU-3 <C> BLEU-4 <C> CIDEr <R> <C> NIC  <C> 0.237 <C> 0.666 <C> 0.461 <C> 0.329 <C> 0.246 <C> 0.855 <R> <C> CNN-LSTM <C> 0.238 <C> 0.698 <C> 0.525 <C> 0.390 <C> 0.292 <C> 0.889 <R> <C> TPGN <C> [BOLD] 0.243 <C> [BOLD] 0.709 <C> [BOLD] 0.539 <C> [BOLD] 0.406 <C> [BOLD] 0.305 <C> [BOLD] 0.909 <CAP> Table 1: Performance of the proposed TPGN model on the COCO dataset.
<R> <C> [BOLD] Small model <C> [BOLD] Perplexity <R> <C> Baseline w175/w200 <C> 98.82/96.86 <R> <C>  <C> 100.3 <R> <C>  <C> 109.05 <R> <C> c5 with  [ITALIC] n=3 <C> [BOLD] 96.21 <R> <C> c5 with  [ITALIC] n=7 <C> 96.35 <R> <C> [BOLD] Large model <C> [BOLD] Perplexity <R> <C> Baseline w475/w650 <C> 84.38/83.6 <R> <C>  <C> 84.6 <R> <C> c25 with  [ITALIC] n=8 <C> 82.69 <R> <C> c10 with  [ITALIC] n=9( [ITALIC] b) <C> 82.68 <R> <C> c10 with  [ITALIC] n=3+3( [ITALIC] b) <C> [BOLD] 82.04 <CAP> Table 2: Test perplexity results for the best models on PTB. Baseline perplexities are for sizes w175/w200 for a small model and w475/w650 for a large model. n = number of characters added, (b) means backward order. Comparison with other character-level LMs [Kim et al.2016] (we only compare to models without highway layers) and character-word models [Miyamoto and Cho2016] (they do not use dropout and only report results for a small model).
<R> <C> [BOLD] Small model <C> [BOLD] Perplexity <R> <C> Baseline w175/w200 <C> 76.78/76 <R> <C> c10 with  [ITALIC] n=2 <C> 75.23 <R> <C> c10 with  [ITALIC] n=3 <C> [BOLD] 75.04 <R> <C> [BOLD] Large model <C> [BOLD] Perplexity <R> <C> Baseline w475/w650 <C> 70.88/70.69 <R> <C> c25 with  [ITALIC] n=4 <C> 68.79 <R> <C> c25 with  [ITALIC] n=6( [ITALIC] b) <C> [BOLD] 67.64 <CAP> Table 3: Test perplexity results for the best models on CGN. Baseline perplexities are for sizes w175/w200 for a small model and w475/w650 for a large model. n = number of characters added, (b) means backward order.
<R> <C> statistics <C> [ITALIC] SLs <C> [ITALIC] SLw <C> [ITALIC] SLtot <C> [ITALIC] Lv <C> [ITALIC] Lvu <C> [ITALIC] Lu <C> [ITALIC] Ltot <R> <C> # tokens <C> 44 <C> 54 <C> 98 <C> 32 <C> 69 <C> 75 <C> 176 <R> <C> Mean <C> 54.84 <C> 45.10 <C> 49.49 <C> 53.00 <C> 63.79 <C> 46.13 <C> 54.30 <R> <C> Median <C> 55 <C> 44 <C> 49.50 <C> 44 <C> 47 <C> 39 <C> 42.50 <R> <C> Std. Deviation <C> 20.14 <C> 22.37 <C> 21.84 <C> 37.02 <C> 58.61 <C> 49.74 <C> 51.85 <R> <C> Skewness <C> .31 <C> .57 <C> .37 <C> 1.93 <C> 3.32 <C> 5.78 <C> 4.16 <R> <C> Minimum <C> 21 <C> 6 <C> 6 <C> 19 <C> 5 <C> 9 <C> 5 <R> <C> Maximum <C> 100 <C> 113 <C> 113 <C> 173 <C> 328 <C> 414 <C> 414 <CAP> Table 1: Duration of types of laughter in # frames (10 msec.)
<R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> CMN2 <C> CMN2 <C> VAST <C> VAST <R> <C> Sys. <C> Diar. <C> DA <C> EER <C> [ITALIC] Cprim <C> EER <C> [ITALIC] Cprim <R> <C> 1 i <C> N <C> Kaldi <C> 12.6 <C> 0.761 <C> 16.8 <C> 0.676 <R> <C> 2 x <C> N <C> Kaldi <C> 11.6 <C> 0.759 <C> 15.9 <C> 0.713 <R> <C> 3 x <C> N <C> Clust. <C> 8.1 <C> 0.549 <C> 14.3 <C> 0.557 <R> <C> 4 x <C> N <C> Kaldi <C> 7.5 <C> 0.452 <C> [BOLD] 12.1 <C> [BOLD] 0.543 <R> <C> 5 x+ <C> N <C> Kaldi <C> 7.9 <C> 0.558 <C> 15.5 <C> 0.637 <R> <C> 6 x+ <C> Y <C> CORAL+ <C> [BOLD] 5.9 <C> [BOLD] 0.421 <C> 12.7 <C> [BOLD] 0.543 <R> <C> 7 x <C> Y <C> Kaldi <C> 7.3 <C> 0.491 <C> 14.3 <C> 0.571 <R> <C> 8 x <C> N <C> Kaldi <C> 8.1 <C> 0.551 <C> 14.6 <C> 0.601 <R> <C> 9 x <C> N <C> Kaldi <C> 7.5 <C> 0.482 <C> 14.3 <C> 0.533 <R> <C> 10 t <C> N <C> Kaldi <C> 10.5 <C> 0.678 <C> 17.1 <C> 0.720 <R> <C> 11 i <C> N <C> Kaldi <C> 12.4 <C> 0.755 <C> 18.7 <C> 0.700 <R> <C> 12 i <C> N <C> - <C> 16.4 <C> 0.814 <C> 21.3 <C> 0.788 <CAP> Table 2: Sub-system performance on the NIST SRE’18 evaluation set. Performance is measured in terms of EER and min Cprim. We indicate, whether VAST system used diarization (2 systems) and what type of domain adaptation (DA) was utilized (Kaldi PLDA adaptation, CORAL+ or obtaining pseudo labels from the unlabeled set by clustering). Tag ‘i’ indicates an i-vector system, tag ‘t’ indicates a t-vector, tag ‘x’ indicates an x-vector, while tag ‘x+’ indicates an x-vector with attentive pooling.
<R> <C> Explicit spaces <C> Capital letters <C> Initial+Final letters <R> <C> 38.1 <C> 36.2 <C> 36.2 <CAP> Table 1: Word error rate (%) on NIST RT-02 Switchboard-1 test set as a function of symbol inventory, for a 512-wide 5-deep network
<R> <C> Number of layers <C> 512 width <C> 1024 width <R> <C> 3 <C> 48.5 <C> 38.1 <R> <C> 4 <C> 38.3 <C> 34.6 <R> <C> 5 <C> 36.2 <C> 31.9 <R> <C> 6 <C> 34.5 <C> 30.3 <R> <C> 7 <C> 32.5 <C> 30.6 <R> <C> 8 <C> 31.0 <C> 31.3 <R> <C> 9 <C> 31.4 <C> 29.4 <R> <C> 10 <C> 31.5 <C> 29.4 <CAP> Table 3: Word error rate (%) as a function of the number of layers and dimensions on the RT-02 test set
<R> <C> None <C> Iterated CTC <C> Character-Beam <C> Word-Beam <R> <C> 29.4 <C> 27.9 <C> 23.3 <C> 19.2 <CAP> Table 4: Word error rate (%) as a function of post-processing for the RT-02 test set, using a 9-layer 1024-wide network
<R> <C> Language <C> WALS-syntax distance from Spanish (out of a max of 49 features) <R> <C> Spanish (es) <C> 0 <R> <C> Italian (it) <C> 0 <R> <C> Portuguese (pt) <C> 3 <R> <C> English (en) <C> 4 <R> <C> Romanian (ro) <C> 5 <R> <C> Russian (ru) <C> 9 <R> <C> German (de) <C> 10 <R> <C> Finnish (fi) <C> 13 <R> <C> Basque (eu) <C> 15 <R> <C> Korean (ko) <C> 18 <R> <C> Turkish (tr) <C> 23 <R> <C> Japanese (ja) <C> 23 <CAP> Table 1: WALS-syntax distance between Spanish and L1s
<R> <C> Method <C> C→R <C> C→N <C> C→G <C> R→C <C> R→N <C> R→G <C> N→C <C> N→R <C> N→G <C> G→C <C> G→R <C> G→N <C> AVG <R> <C> Non-transfer <C> 67.20 <C> 54.51 <C> 49.01 <C> 65.63 <C> 54.51 <C> 49.01 <C> 65.63 <C> 67.20 <C> 49.01 <C> 65.63 <C> 67.20 <C> 54.51 <C> 59.09 <R> <C> Linear projection Peng and Dredze ( 2017 ) <C> 69.01 <C> 67.02 <C> 57.40 <C> 69.79 <C> 65.87 <C> 57.71 <C> 67.70 <C> 68.77 <C> 51.33 <C> 68.00 <C> 69.65 <C> 61.12 <C> 64.45 <R> <C> Domain mask Peng and Dredze ( 2017 ) <C> 70.76 <C> 63.97 <C> 58.62 <C> 70.18 <C> 64.27 <C> 58.16 <C> 67.93 <C> 69.89 <C> 56.18 <C> 68.87 <C> 69.89 <C> 63.49 <C> 65.18 <R> <C> CD-learning He and Sun ( 2017 ) <C> 71.38 <C> 64.01 <C> 56.72 <C> 72.17 <C> 64.91 <C> 58.14 <C> 68.99 <C> 71.13 <C> 56.27 <C> 70.17 <C> 71.76 <C> 62.06 <C> 65.64 <R> <C> Re-training Lee et al. ( 2017 ) <C> 72.45 <C> 70.55 <C> 59.58 <C> 72.56 <C> 68.59 <C> 60.94 <C> 69.60 <C> 70.08 <C> 56.58 <C> 70.14 <C> 71.90 <C> 66.01 <C> 67.42 <R> <C> Joint-training Yang et al. ( 2017 ) <C> 69.82 <C> 70.49 <C> 63.52 <C> 71.45 <C> 67.03 <C> 67.71 <C> 70.96 <C> 71.43 <C> 60.54 <C> 69.68 <C> 71.55 <C> 68.15 <C> 68.53 <R> <C> La-MMD <C> 73.08 <C> 69.48 <C> 59.86 <C> 72.53 <C> 70.28 <C> 60.16 <C> 71.31 <C> 73.04 <C> 57.94 <C> 69.80 <C> 73.99 <C> 67.19 <C> 68.22 <R> <C> CRF-L2 <C> 73.34 <C> 71.52 <C> 60.17 <C> 72.43 <C> 69.72 <C> 67.61 <C> 69.76 <C> 71.54 <C> 59.96 <C> 69.75 <C> 71.82 <C> 67.30 <C> 68.74 <R> <C> MMD-CRF-L2 <C> 73.05 <C> 72.35 <C> 60.80 <C> 72.65 <C> 69.87 <C> 66.82 <C> 70.25 <C> 71.75 <C> 58.98 <C> 70.48 <C> 73.98 <C> 67.43 <C> 69.03 <R> <C> La-DTL <C> [BOLD] 73.59† <C> [BOLD] 72.91† <C> [BOLD] 64.60† <C> [BOLD] 73.88† <C> [BOLD] 73.01† <C> [BOLD] 70.17† <C> [BOLD] 73.08† <C> [BOLD] 73.11† <C> [BOLD] 62.14† <C> [BOLD] 71.61† <C> [BOLD] 74.21† <C> [BOLD] 71.49† <C> [BOLD] 71.15 <CAP> Table 2: Results (F1-score %) of 12 cross-specialty medical NER tasks. C, R, N, G are short for the department of Cardiology, Respiratory, Neurology, and Gastroenterology, respectively. † indicates La-DTL outperforms the 6 baselines significantly (p<0.05).
<R> <C> [ITALIC] N <C> metric <C> DPP+ [ITALIC] ν [BOLD] θ <C> BPP+ [ITALIC] ν [BOLD] θ <C> DPP− [ITALIC] ν [BOLD] θ <C> Sup. <R> <C> [EMPTY] <C> x-ent <C> 540.02 <C> 540.05 <C> 600.34 <C> ✗ <R> <C> 15 <C> cloze1 <C> 5.76 <C> 5.76 <C> 6.53 <C> ✗ <R> <C> [EMPTY] <C> cloze12 <C> 4.89 <C> 4.89 <C> 5.24 <C> ✗ <R> <C> [EMPTY] <C> x-ent <C> 280.47 <C> 275.36 <C> 335.36 <C> ✗ <R> <C> 25 <C> cloze1 <C> 5.04 <C> 5.25 <C> 6.23 <C> ✗ <R> <C> [EMPTY] <C> cloze12 <C> 4.76 <C> 4.97 <C> 5.43 <C> ✗ <R> <C> [EMPTY] <C> x-ent <C> 222.85 <C> 231.70 <C> 320.05 <C> 1610.37 <R> <C> 50 <C> cloze1 <C> 3.38 <C> 3.16 <C> 4.02 <C> 4.96 <R> <C> [EMPTY] <C> cloze12 <C> 2.73 <C> 2.93 <C> 3.04 <C> 6.95 <R> <C> [EMPTY] <C> x-ent <C> 212.14 <C> 220.42 <C> 380.31 <C> ✗ <R> <C> 57 <C> cloze1 <C> 2.21 <C> 3.08 <C> 3.25 <C> ✗ <R> <C> [EMPTY] <C> cloze12 <C> 2.01 <C> 3.05 <C> 3.41 <C> ✗ <R> <C> [EMPTY] <C> x-ent <C> 271.95 <C> 301.45 <C> 380.02 <C> ✗ <R> <C> 100 <C> cloze1 <C> 2.26 <C> 2.42 <C> 3.03 <C> ✗ <R> <C> [EMPTY] <C> cloze12 <C> 1.96 <C> 2.01 <C> 2.51 <C> ✗ <CAP> Table 1: Cross-entropy in nats per language (lower is better) and expected Euclidean-distance error of the cloze prediction (lower is better). The overall best value for each task is boldfaced. The case N=50 is compared against our supervised baseline. The N=57 row is the case where we allowed N to fluctuate during inference using reversible-jump MCMC; this was the N value selected at the final EM iteration.
<R> <C> [EMPTY] <C> Male <C> Female <R> <C> Parametric <C> 3.54 <C> 3.47 <R> <C> Tacotron 2 <C> 4.10 <C> 4.28 <R> <C> DurIAN <C> 4.11 <C> 4.26 <CAP> Table 1: 5-scale mean opinion score evaluation.
<R> <C> RTF <C> fullband <C> 4band <R> <C> float <C> 1.337 <C> 0.503 <R> <C> int8 <C> 0.387 <C> 0.171 <CAP> Table 3: Real Time Factor (RTF) evaluation of proposed Multiband WaveRNN.
<R> <C> Systems <C> MOS <R> <C> Fullband WaveRNN (float) <C> 4.53 <R> <C> 4-band WaveRNN (int8) <C> 4.58 <R> <C> 4-band WaveRNN (int8) <C> 4.56 <CAP> Table 4: 5-scale mean opinion score (MOS) evaluation of the proposed Multi-band WaveRNN.
<R> <C> [BOLD] Final Parsing Results on Penn Treebank Parser <C> [BOLD] Final Parsing Results on Penn Treebank LR <C> [BOLD] Final Parsing Results on Penn Treebank LP <C> [BOLD] Final Parsing Results on Penn Treebank F1 <R> <C> Durrett and Klein ( 2015 ) <C> – <C> – <C> 91.1 <R> <C> Vinyals et al. ( 2015 ) <C> – <C> – <C> 88.3 <R> <C> Dyer et al. ( 2016 ) <C> – <C> – <C> 89.8 <R> <C> Cross and Huang ( 2016 ) <C> 90.5 <C> 92.1 <C> 91.3 <R> <C> Liu and Zhang ( 2016 ) <C> 91.3 <C> 92.1 <C> 91.7 <R> <C> Best Chart Parser <C> 90.63 <C> 92.98 <C> 91.79 <R> <C> Best Top-Down Parser <C> 90.35 <C> 93.23 <C> 91.77 <CAP> Table 2: Comparison of final test F1 scores on the Penn Treebank. Here we only include scores from single-model parsers trained without external parse data.
<R> <C> Num. LRMs <C> Memory (GB) <C> Time / token (us) <R> <C> 24 <C> 3.4 <C> 405 <R> <C> 12 <C> 2.8 <C> 273 <R> <C> 4 <C> 1.1 <C> 191 <R> <C> 1 <C> 0.50 <C> 155 <R> <C> 0 <C> 0.20 <C> 143 <CAP> Table 1: Profiling a 24-layer TXL training on Enwik8.
<R> <C> Metric <C> High <C> Medium <C> Low <R> <C> Nbm25 <C> 89 <C> 390 <C> 453 <R> <C> Nemb <C> 53 <C> 154 <C> 380 <R> <C> Cbm25 <C> 8.1 <C> 35.8 <C> 41.7 <R> <C> Cemb <C> 4.9 <C> 14.0 <C> 31.6 <R> <C> Pbm25 <C> 97.7 <C> 87.5 <C> 77.6 <R> <C> Pemb <C> 100.0 <C> 94.9 <C> 81.0 <R> <C> Rbm25 <C> 94.5 <C> 79.5 <C> 66.3 <R> <C> Remb <C> 94.3 <C> 86.9 <C> 78.0 <R> <C> F2bm25 <C> 94.2 <C> 79.5 <C> 66.9 <R> <C> F2emb <C> 94.8 <C> 87.2 <C> 76.1 <CAP> Table 1. Retrieval coverage and performance by using similarity thresholds for several confidence levels. The number of documents retrieved by using BM25-based similarity thresholding is denoted as Nbm25, the respective word embedding similarity-based document count as Nemb. The metrics coverage C, precision P, recall R and F2-measure F2 are depicted in percentages.
<R> <C> Metric <C> UA-TFIDF <C> [BOLD] DBSE <C> iitptfidf <R> <C> F2 <C> 54.93 <C> [BOLD] 46.59 <C> 40.08 <R> <C> P <C> 59.18 <C> [BOLD] 45.44 <C> 43.88 <R> <C> R <C> 54.42 <C> [BOLD] 49.32 <C> 39.63 <R> <C> MAP <C> 61.81 <C> [BOLD] 51.19 <C> 50.56 <R> <C> R@5 <C> 61.98 <C> [BOLD] 51.24 <C> 57.02 <R> <C> R@10 <C> 69.42 <C> [BOLD] 61.98 <C> 62.81 <R> <C> R@30 <C> 76.03 <C> [BOLD] 66.94 <C> 75.21 <CAP> Table 2. Retrieval results of our team DBSE for the metrics F2-measure F2, precision P, recall R, mean average precision MAP, recall at 5 R@5, recall at 10 R@10 and recall at 30 R@30, calculated as macro-averaged values and depicted in percentages.
<R> <C> Metric <C> UA_Ex <C> [BOLD] DBSE <C> Baseline <R> <C> A <C> 68.37 <C> [BOLD] 57.14 <C> 52.04 <CAP> Table 3. Entailment classification results of our team DBSE for the accuracy A metric, calculated as macro-averaged value and depicted in percentages.
<R> <C> Model <C> ExactMatch <C> F1 <R> <C> FIFO <C> 24.53 <C> 27.22 <R> <C> Uniform <C> 28.30 <C> 34.39 <R> <C> LIFO <C> 46.23 <C> 50.10 <R> <C> EMR-Independent <C> 38.05 <C> 41.15 <R> <C> EMR-biGRU <C> [BOLD] 52.20 <C> [BOLD] 57.57 <R> <C> EMR-Transformer <C> 48.43 <C> 53.81 <CAP> Table 1: Q&A accuracy on the TriviaQA dataset. model.
<R> <C> Tasks <C> NYT <C> ADE <C> Wiki-DBpedia <R> <C> Metric <C> F1-Measure <C> F1-Measure <C> F1-Measure <R> <C> EL+Lexical <C> 36.8 <C> 61.4 <C> 37.8 <R> <C> EL+LSTM <C> 58.7 <C> 70.3 <C> 65.5 <R> <C> EL+GRU <C> 59.8 <C> 73.2 <C> 67.0 <R> <C> Seq2Seq <C> 64.2 <C> 73.4 <C> 73.5 <R> <C> S+A+W+G <C> [BOLD] 71.4 <C> [BOLD] 79.5 <C> [BOLD] 84.3 <CAP> Table 2: Cross-dataset comparison on triple generations. Seq2Seq denotes the implementation of Seq2Seq without any attention mechanism and pre-trained embeddings; A denotes attention mechanism; W and G denote pre-trained word embeddings for the encoders and KG embeddings for the decoders, respectively.
<R> <C> [BOLD] F  [BOLD] N <C> [BOLD] Any Passage  [BOLD] TF-IDF <C> [BOLD] Any Passage  [BOLD] Trained <C> [BOLD] Any Passage  [BOLD] STS-B <C> [BOLD] Correct Passage  [BOLD] TF-IDF <C> [BOLD] Correct Passage  [BOLD] Trained <C> [BOLD] Correct Passage  [BOLD] STS-B <C> [BOLD] Accuracy(%)  [BOLD] TF-IDF <C> [BOLD] Accuracy(%)  [BOLD] Trained <C> [BOLD] Accuracy(%)  [BOLD] STS-B <R> <C> 1 <C> 228 <C> 258 <C> 288 <C> 196 <C> 229 <C> 234 <C> 52.6 <C> 63.6 <C> 59.2 <R> <C> 2 <C> 294 <C> 324 <C> 347 <C> 264 <C> 293 <C> 304 <C> 57.4 <C> [BOLD] 66.2 <C> 60.6 <R> <C> 3 <C> 324 <C> 358 <C> 368 <C> 290 <C> 328 <C> 337 <C> 59.2 <C> 65.0 <C> 60.2 <R> <C> 5 <C> 350 <C> 391 <C> 398 <C> 319 <C> [BOLD] 370 <C> 366 <C> 61.6 <C> 65.4 <C> 62.8 <R> <C> 7 <C> 356 <C> 411 <C> 411 <C> 328 <C> [BOLD] 390 <C> 384 <C> 59.4 <C> 65.2 <C> 61.8 <R> <C> 10 <C> 373 <C> 423 <C> 420 <C> 354 <C> [BOLD] 405 <C> 396 <C> 60.4 <C> 65.2 <C> 59.4 <CAP> Table 3: Compares (a) The number of correct facts that appears across any four passages (b) The number of correct facts that appears in the passage of the correct hypothesis (c) The accuracy for TF-IDF, BERT model trained on STS-B dataset and BERT model trained on OpenBook dataset. N is the number of facts considered.
<R> <C> [BOLD] Method <C> [BOLD] # Labeled <C> [BOLD] # All <C> [BOLD] AUPRC <C> [BOLD] Prec@0.5 <C> [BOLD] Use Expression <C> [BOLD] Use Text <C> [BOLD] Use Lexicon <C> [BOLD] Use EM <R> <C> URSA <C> 14510 <C> 0 <C> 0.40 <C> 0.52 <C> yes <C> no <C> no <C> no <R> <C> Co-EM <C> 14510 <C> 116895 <C> 0.51 <C> 0.61 <C> yes <C> yes <C> no <C> yes <R> <C> Dist. Sup. <C> 0 <C> 116895 <C> 0.59 <C> 0.63 <C> yes <C> yes <C> yes <C> no <R> <C> [ITALIC] EZLearn <C> 0 <C> 116895 <C> [BOLD] 0.69 <C> [BOLD] 0.86 <C> yes <C> yes <C> yes <C> yes <CAP> Table 1: Comparison of test results between EZLearn and state-of-the-art supervised, semi-supervised, and distantly supervised methods on the CMHGP dataset. We reported the area under the precision-recall curve (AUPRC) and precision at 0.5 recall. EZLearn requires no manually labeled data, and substantially outperforms all other methods. Compared to URSA and co-EM, EZLearn can effectively leverage unlabeled data by exploiting organic supervision from text descriptions and lexicon. EZLearn amounts to initializing with distant supervision (first iteration) and continuing with an EM-like process as in co-training and co-EM, which leads to further significant gains.
<R> <C> Resolve <C> Stand. <C> Pred. <C> Union <C> Inter. <C> Relat. <R> <C> [BOLD] # Classes <C> 623 <C> 329 <C> 603 <C> 351 <C> 601 <R> <C> [BOLD] AUPRC <C> 0.59 <C> 0.64 <C> 0.59 <C> 0.66 <C> [BOLD] 0.69 <CAP> Table 2: Comparison of test results and numbers of unique classes in high-confidence predictions on the Comprehensive Map of Human Gene Expression by EZLearn with various strategies in resolving conflicts between distant supervision and classifier prediction.
<R> <C> Algorithm <C> CIDEr-D <C> Bleu_4 <C> Bleu_3 <C> Bleu_2 <C> Bleu_1 <C> RG_L <C> METEOR <C> SPICE <R> <C> Adaptive  <C> 1.085 <C> 0.332 <C> 0.439 <C> 0.580 <C> 0.742 <C> – <C> 0.266 <C> – <R> <C> MSM  <C> 0.986 <C> 0.325 <C> 0.429 <C> 0.565 <C> 0.730 <C> – <C> 0.251 <C> – <R> <C> Att2in  <C> 1.01 <C> 0.313 <C> – <C> – <C> – <C> – <C> 0.260 <C> – <R> <C> Top-Down  <C> 1.135 <C> 0.362 <C> – <C> – <C> 0.772 <C> 0.564 <C> 0.270 <C> 0.203 <R> <C> TPGN  <C> 0.909 <C> 0.305 <C> 0.406 <C> 0.539 <C> 0.709 <C> – <C> 0.243 <C> – <R> <C> ATPL  <C> 1.013 <C> 0.335 <C> 0.437 <C> 0.572 <C> 0.733 <C> – <C> 0.258 <C> – <R> <C> Attribute-Attention  <C> 1.044 <C> 0.338 <C> 0.443 <C> 0.579 <C> 0.743 <C> 0.549 <C> – <C> – <R> <C> SCN  <C> 1.012 <C> 0.330 <C> 0.433 <C> 0.566 <C> 0.728 <C> – <C> 0.257 <C> – <R> <C> (Semantic TPR)  <C> 1.022 <C> 0.338 <C> 0.443 <C> 0.578 <C> 0.737 <C> 0.546 <C> 0.256 <C> 0.1862 <R> <C> Image Top-Down <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> TP [ITALIC] sgtR Bottom-Up <C> 1.017 <C> 0.338 <C> 0.444 <C> 0.581 <C> 0.737 <C> 0.546 <C> 0.255 <C> 0.186 <R> <C> [BOLD] Semantic Top-Down <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] TP [ITALIC] sgtR Bottom-Up <C> [BOLD] 1.082 <C> [BOLD] 0.349 <C> [BOLD] 0.456 <C> [BOLD] 0.594 <C> [BOLD] 0.752 <C> [BOLD] 0.554 <C> [BOLD] 0.265 <C> [BOLD] 0.196 <CAP> TABLE I: Performance Comparison & Analysis for Different Image Captioning Architectures for Different Metrics
<R> <C> System (scoring method) <C> Accuracy (%) <C> Precision (%) <C> Recall (%) <R> <C> 2 i-vector (SVM) - baseline <C> 57.20 <C> 60.80 <C> 58.00 <R> <C> i-vector (CDS) <C> 56.36 <C> 59.86 <C> 57.19 <R> <C> LDA i-vector (SVM) <C> 60.17 <C> 60.65 <C> 60.91 <R> <C> LDA i-vector (CDS) <C> 58.46 <C> 61.31 <C> 59.14 <R> <C> Siam i-vector (SVM) <C> 61.15 <C> 62.91 <C> 61.56 <R> <C> Siam i-vector (CDS) <C> [BOLD] 63.65 <C> [BOLD] 64.00 <C> [BOLD] 63.88 <CAP> Table 3: i-vector evaluation on DEV set: only TRN set is used for training. Note that scores in this table were not calibrated.
<R> <C> [EMPTY] <C> [BOLD] p@1 <C> [BOLD] p@5 <C> [BOLD] p@10 <C> [BOLD] p@20 <C> [BOLD] MRR <R> <C> [BOLD] baseline <C> 0.293 <C> 0.477 <C> 0.569 <C> 0.669 <C> 16.05 <R> <C> [BOLD] pretrain text <C> 0.284 <C> 0.476 <C> 0.560 <C> 0.640 <C> 17.46 <R> <C> [BOLD] pretrain textNLU <C> 0.290 <C> 0.481 <C> 0.563 <C> 0.638 <C> 17.19 <R> <C> after fine-tuning on 20% QR set for 2 epochs <C> after fine-tuning on 20% QR set for 2 epochs <C> after fine-tuning on 20% QR set for 2 epochs <C> after fine-tuning on 20% QR set for 2 epochs <C> after fine-tuning on 20% QR set for 2 epochs <C> after fine-tuning on 20% QR set for 2 epochs <R> <C> [BOLD] pretrain text <C> 0.307 +4.8%∗ <C> 0.505 +5.9% <C> 0.599 +5.3% <C> 0.689 +3.0% <C> 15.13 <R> <C> [BOLD] pretrain textNLU <C> 0.315 +7.5% <C> 0.513 +7.5% <C> 0.608 +6.9% <C> 0.680 +1.6% <C> 15.18 <CAP> Table 1: Summary of QR experiment results. ∗ relative performance gain in comparison to the "baseline".
<R> <C> [BOLD] Trained on <C> [BOLD] Evaluated on WikiQA <C> [BOLD] Evaluated on WikiQA <C> [BOLD] Evaluated on WikiQA <C> [BOLD] Evaluated on SelQA <C> [BOLD] Evaluated on SelQA <C> [BOLD] Evaluated on SelQA <C> [BOLD] Evaluated on SQuAD <C> [BOLD] Evaluated on SQuAD <C> [BOLD] Evaluated on SQuAD <C> [BOLD] Evaluated on InfoboxQA <C> [BOLD] Evaluated on InfoboxQA <C> [BOLD] Evaluated on InfoboxQA <R> <C> [BOLD] Trained on <C> MAP <C> MRR <C> F1 <C> MAP <C> MRR <C> F1 <C> MAP <C> MRR <C> F1 <C> MAP <C> MRR <C> F1 <R> <C> WikiQA <C> [BOLD] 65.54 <C> [BOLD] 67.41 <C> 13.33 <C> 53.47 <C> 54.12 <C> 8.68 <C> 73.16 <C> 73.72 <C> 11.26 <C> 30.85 <C> 30.85 <C> - <R> <C> SelQA <C> 49.05 <C> 49.64 <C> [BOLD] 24.30 <C> 82.72 <C> 83.70 <C> [BOLD] 48.66 <C> 77.22 <C> 78.04 <C> 44.70 <C> 63.13 <C> 63.13 <C> - <R> <C> SQuAD <C> 58.17 <C> 58.53 <C> 19.35 <C> 81.15 <C> 82.27 <C> 42.88 <C> 88.84 <C> 89.69 <C> [BOLD] 44.93 <C> 63.24 <C> 63.24 <C> - <R> <C> InfoboxQA <C> 45.17 <C> 45.43 <C> - <C> 53.48 <C> 54.25 <C> - <C> 65.27 <C> 65.90 <C> - <C> [BOLD] 79.44 <C> [BOLD] 79.44 <C> - <R> <C> W+S+Q <C> 56.40 <C> 56.51 <C> - <C> [BOLD] 83.19 <C> [BOLD] 84.25 <C> - <C> 88.78 <C> 89.65 <C> - <C> 62.53 <C> 62.53 <C> - <R> <C> W+S+Q+I <C> 60.19 <C> 60.68 <C> - <C> 82.88 <C> 83.97 <C> - <C> [BOLD] 88.92 <C> [BOLD] 89.79 <C> - <C> 70.81 <C> 70.81 <C> - <CAP> Table 3: Results for answer selection and triggering in % trained and evaluated across all corpora splits. The first column shows the training source, and the other columns show the evaluation sources. W: WikiQA, S: SelQA, Q: SQuAD, I: InfoboxQA.
<R> <C> Method <C> Text Ratings  [ITALIC] n <C> Text Ratings  [ITALIC] μ <C> Text Ratings CI <C> Text Ratings  [ITALIC] σ <C> Tabular Ratings  [ITALIC] n <C> Tabular Ratings  [ITALIC] μ <C> Tabular Ratings CI <C> Tabular Ratings  [ITALIC] σ <R> <C> LIME <C> 144 <C> 4.78 <C> 1.47 <C> 1.76 <C> 130 <C> 5.36 <C> 0.63 <C> 1.70 <R> <C> Anchor <C> 133 <C> 3.86 <C> 0.59 <C> 1.79 <C> 175 <C> 4.99 <C> 0.71 <C> 1.38 <R> <C> Prototype <C> 191 <C> 4.45 <C> 1.02 <C> 2.08 <C> 144 <C> 4.20 <C> 0.82 <C> 1.88 <R> <C> DB <C> 224 <C> 3.85 <C> 0.60 <C> 1.81 <C> 144 <C> 4.61 <C> 1.14 <C> 1.86 <R> <C> Composite <C> 240 <C> 4.47 <C> 0.58 <C> 1.70 <C> 192 <C> 5.10 <C> 1.04 <C> 1.42 <CAP> Table 3: User simulatability ratings by data domain, on a scale of 1 to 7. The mean and standard deviation for ratings are given by μ and σ. The 95% confidence interval for the mean is given by CI, as calculated by bootstrap.
<R> <C> word length <C> 2 <C> 3 <C> 4 <C> 5 <C> >5 <R> <C> frequency(%) <C> 27.1 <C> 20.3 <C> 14.5 <C> 10.3 <C> 27.4 <CAP> Table 1: The distribution of sentence length on CIR.
<R> <C> Model <C> Per-word Log-Likelihood <C> Std. Dev. <R> <C> DAP ( [ITALIC] ρ=0.0) <C> -7.22 <C> 0.04 <R> <C> DAP ( [ITALIC] ρ=0.2) <C> -6.47 <C> 0.04 <R> <C> LDA <C> -9.23 <C> 0.02 <R> <C> DTM <C> -9.65 <C> 0.03 <R> <C> CDTM <C> -8.82 <C> 0.03 <CAP> Table 1. Overall comparison of models. Per-word log-likelihoods for documents in the test dataset are computed. Standard deviation in performance computed over the cross-validation sets. While the basic DAP model without regularization performs significantly better than competing model, the RVI approach further increases log-likelihoods.
<R> <C> Word and Character LSTM <C> 92.22 <R> <C> Word Only <C> 91.44 <R> <C> Word and Tag <C> 92.09 <R> <C> Word, Tag, and Character LSTM <C> 92.24 <R> <C> Character Only <C> 92.24 <CAP> Table 1: Development F1 scores on section 22 of the Penn Treebank for different lexical representations.
<R> <C> Component <C> Dimensions <C> Layers <R> <C> Word Embeddings <C> 100 <C> [EMPTY] <R> <C> Character Embeddings <C> 50 <C> [EMPTY] <R> <C> Character LSTM <C> 100 <C> 1 <R> <C> Sentence LSTM <C> 250 <C> 2 <R> <C> Label Feedforward Network <C> 250 <C> 1 <CAP> Table 2: The sizes of the components used in our model.
<R> <C> [BOLD] Model <C> [BOLD] Recall <C> [BOLD] Precision <C> [BOLD] F1 <R> <C> Honk <C> 0.46 <C> 0.34 <C> 0.39 <R> <C> DeepSpeech-finetune <C> 0.267 <C> 0.244 <C> 0.256 <R> <C> DeepSpeech-finetune-prototypical <C> 0.36 <C> 0.33 <C> 0.344 <R> <C> DeepSpeech-finetune-prototypical+metric <C> [BOLD] 0.55 <C> [BOLD] 0.488 <C> [BOLD] 0.51 <CAP> Table 1: Results of all experiments
<R> <C> [BOLD] Model <C> [BOLD] MRR <C> [BOLD] Mean Rank <C> [BOLD] R@10 <R> <C> Answer Prior  <C> 0.3735 <C> 26.50 <C> 53.23 <R> <C> MN-QIH-G  <C> 0.5259 <C> 17.06 <C> 68.88 <R> <C> HCIAE-G-DIS  <C> 0.547 <C> 14.23 <C> 71.55 <R> <C> Frozen-Q-Multi  <C> 0.437 <C> 21.13 <C> 60.48 <R> <C> CoAtt-GAN  <C> 0.5578 <C> 14.4 <C> 71.74 <R> <C> SL(Ours) <C> [BOLD] 0.610 <C> [BOLD] 5.323 <C> [BOLD] 72.68 <R> <C> RL - 1Q,1A(Ours) <C> 0.459 <C> 7.097 <C> 72.34 <R> <C> RL - 1Q,3A(Ours) <C> 0.601 <C> 5.495 <C> 72.48 <R> <C> RL - 3Q,1A(Ours) <C> 0.590 <C> 5.56 <C> 72.61 <CAP> Table 1: Comparison of answer retrieval metrics with previously published work
<R> <C> Test Type → Evaluation → <C> Macmillian Word Definitions (179)  [BOLD] Accuracy <C> Macmillian Word Definitions (179) Rank <C> Macmillian Word Definitions (179) Rank <C> User Concept Descriptions (179)  [BOLD] Accuracy <C> User Concept Descriptions (179) Rank <C> User Concept Descriptions (179) Rank <R> <C> Models ↓ <C> @1/10/100 <C> Median <C> [ITALIC] σ <C> @1/10/100 <C> Median <C> [ITALIC] σ <R> <C> Onelook <C> .19/.41/.65 <C> 5 <C> 24 <C> .04/.21/.40 <C> 10 <C> 26 <R> <C> [BOLD] Onelook,  [ITALIC] corr* <C> .20/.46/.68 <C> 3 <C> 20 <C> .07/ [BOLD] .26/.52 <C> 13 <C> 30 <R> <C> W2V <C> .01/.06/.20 <C> 23 <C> 30 <C> .01/.05/.18 <C> 34 <C> 28 <R> <C> W2V,  [ITALIC] corr* <C> .02/.11/.29 <C> 21 <C> 26 <C> .01/.08/.26 <C> 21 <C> 29 <R> <C> Chance, 3k <C> 10−4/10−3/.03 <C> 50 <C> 29 <C> 10−4/10−3/.03 <C> 50 <C> 29 <R> <C> Fusion, FLM <C> .02/.10/.21 <C> 12 <C> 28 <C> .01/.07/.22 <C> 16 <C> 21 <R> <C> [BOLD] Fusion, mBLM <C> .25/ [BOLD] .55/ [BOLD] .84 <C> 4 <C> 22 <C> [BOLD] .10/.23/ [BOLD] .53 <C> 14 <C> 26 <R> <C> OLD, mBLM <C> [BOLD] .26/.52/.78 <C> 4 <C> 23 <C> .04/.17/.43 <C> 14 <C> 25 <R> <C> WN, BLM <C> .08/.27/.54 <C> 11 <C> 26 <C> .06/.18/.41 <C> 14 <C> 26 <R> <C> MW, mBLM <C> .17/.39/.63 <C> 5 <C> 20 <C> .05/.20/.43 <C> 15 <C> 25 <R> <C> WL, 80k <C> .03/.15/.36 <C> 18 <C> 26 <C> .05/.11/.24 <C> 14 <C> 25 <R> <C> WL,  [ITALIC] corr* <C> .07/.26/.52 <C> 10 <C> 25 <C> .07/.18/.35 <C> 10 <C> 23 <CAP> Table 1: Performance of the various models. Accuracy @n is the fraction of the phrases with the rank of the target word less than or equal to n, in their outputs. σ is the standard deviation. Only the phrases with target words having ranks less than 100 were considered in calculating the median and variance. The 3k cases (OLD, WN, MW, Fusion) were evaluated at a search depth of 11, and the 80k case (WL) at a search depth of 19. *corr indicates the cases where the outputs were truncated to fit in the 3k lexicon, for fair comparison. (Note: Accuracy - higher is better; Rank median - lower is better; Rank variance - lower is better.)
<R> <C> [BOLD] Emoji Embedding Model <C> [ITALIC] ρ [BOLD]  x 100 for each Corpus  [BOLD] Google News <C> [ITALIC] ρ [BOLD]  x 100 for each Corpus  [BOLD] Twitter <R> <C> [ITALIC]  [BOLD] (Sense_Desc.) <C> 49.0 <C> 46.6 <R> <C> [ITALIC]  [BOLD] (Sense_Label) <C> [BOLD] 76.0 <C> [BOLD] 70.2 <R> <C> [ITALIC]  [BOLD] (Sense_Def.) <C> 69.5 <C> 66.9 <R> <C> [ITALIC]  [BOLD] (Sense_All) <C> 71.2 <C> 67.7 <CAP> Table 3: Spearman’s Rank Correlation Results
<R> <C> [BOLD] Word Embedding Model <C> [BOLD] Classification accuracy on testing dataset  [BOLD] N = 12,920 <C> [BOLD] Classification accuracy on testing dataset  [BOLD] N = 12,920 <C> [BOLD] Classification accuracy on testing dataset  [BOLD] N = 2,295 <C> [BOLD] Classification accuracy on testing dataset  [BOLD] N = 2,295 <C> [BOLD] Classification accuracy on testing dataset  [BOLD] N = 2,186 <C> [BOLD] Classification accuracy on testing dataset  [BOLD] N = 2,186 <C> [BOLD] Classification accuracy on testing dataset  [BOLD] N = 308 <C> [BOLD] Classification accuracy on testing dataset  [BOLD] N = 308 <R> <C> [EMPTY] <C> [BOLD] RF <C> [BOLD] SVM <C> [BOLD] RF <C> [BOLD] SVM <C> [BOLD] RF <C> [BOLD] SVM <C> [BOLD] RF <C> [BOLD] SVM <R> <C> Google News + emoji2vec <C> 59.5 <C> 60.5 <C> 54.4 <C> 59.2 <C> 55.0 <C> 59.5 <C> 54.5 <C> 55.2 <R> <C> Google News +  [ITALIC] (Sense_Desc.) <C> 58.7 <C> 61.9 <C> 50.6 <C> 55.0 <C> 49.7 <C> 55.3 <C> 45.4 <C> 50.0 <R> <C> Twitter +  [ITALIC] (Sense_Desc.) <C> 60.2 <C> 62.5 <C> 55.1 <C> 56.7 <C> 53.8 <C> 57.3 <C> 53.5 <C> 53.2 <R> <C> Google News +  [ITALIC] (Sense_Label) <C> 60.3 <C> 63.3 <C> 55.0 <C> [BOLD] 61.8 <C> 56.8 <C> [BOLD] 62.3 <C> 54.2 <C> [BOLD] 59.0 <R> <C> Twitter +  [ITALIC] (Sense_Label) <C> [BOLD] 60.7 <C> [BOLD] 63.6 <C> [BOLD] 57.3 <C> 60.8 <C> [BOLD] 57.5 <C> 61.5 <C> [BOLD] 56.1 <C> 58.4 <R> <C> Google News +  [ITALIC] (Sense_Def.) <C> 59.0 <C> 62.2 <C> 50.3 <C> 55.0 <C> 51.1 <C> 55.2 <C> 48.0 <C> 50.6 <R> <C> Twitter +  [ITALIC] (Sense_Def.) <C> 60.0 <C> 62.4 <C> 53.6 <C> 56.2 <C> 53.7 <C> 56.7 <C> 50.6 <C> 50.6 <R> <C> Google News +  [ITALIC] (Sense_All) <C> 59.1 <C> 62.2 <C> 50.8 <C> 55.1 <C> 50.2 <C> 55.3 <C> 50.0 <C> 50.6 <R> <C> Twitter +  [ITALIC] (Sense_All) <C> 60.3 <C> 62.4 <C> 53.1 <C> 57.6 <C> 54.1 <C> 56.8 <C> 54.5 <C> 50.0 <CAP> Table 4: Accuracy of the Sentiment Analysis task using Emoji Embeddings
<R> <C> [BOLD] Model Freq. <C> [BOLD] 0-4 <C> [BOLD] 5-9 <C> [BOLD] 10-14 <C> [BOLD] 15-19 <C> [BOLD] 20-50 <C> [BOLD] 50+ <R> <C> [BOLD] Russian <C> [BOLD] Russian <C> [BOLD] Russian <C> [BOLD] Russian <C> [BOLD] Russian <C> [BOLD] Russian <C> [BOLD] Russian <R> <C> bilstmword <C> - <C> 0.36 <C> 0.49 <C> 0.61 <C> 0.76 <C> [BOLD] 0.91 <R> <C> bilstmchar <C> 0.16 <C> 0.34 <C> 0.48 <C> 0.59 <C> 0.71 <C> 0.85 <R> <C> cnnchar <C> [BOLD] 0.43 <C> [BOLD] 0.71 <C> [BOLD] 0.77 <C> [BOLD] 0.77 <C> [BOLD] 0.81 <C> 0.81 <R> <C> avemorph <C> 0.03 <C> 0.21 <C> 0.33 <C> 0.40 <C> 0.55 <C> 0.78 <R> <C> bilstmmorph <C> 0.01 <C> 0.24 <C> 0.38 <C> 0.49 <C> 0.65 <C> 0.85 <R> <C> [BOLD] Romanian <C> [BOLD] Romanian <C> [BOLD] Romanian <C> [BOLD] Romanian <C> [BOLD] Romanian <C> [BOLD] Romanian <C> [BOLD] Romanian <R> <C> bilstmword <C> - <C> 0.47 <C> 0.63 <C> 0.71 <C> 0.81 <C> [BOLD] 0.91 <R> <C> bilstmchar <C> 0.02 <C> 0.41 <C> 0.55 <C> 0.62 <C> 0.74 <C> 0.83 <R> <C> cnnchar <C> [BOLD] 0.59 <C> [BOLD] 0.82 <C> [BOLD] 0.81 <C> [BOLD] 0.84 <C> [BOLD] 0.88 <C> 0.84 <R> <C> avemorph <C> 0.05 <C> 0.40 <C> 0.52 <C> 0.61 <C> 0.71 <C> 0.84 <R> <C> bilstmmorph <C> 0.01 <C> 0.38 <C> 0.53 <C> 0.61 <C> 0.72 <C> 0.82 <R> <C> [BOLD] Estonian <C> [BOLD] Estonian <C> [BOLD] Estonian <C> [BOLD] Estonian <C> [BOLD] Estonian <C> [BOLD] Estonian <C> [BOLD] Estonian <R> <C> bilstmword <C> - <C> 0.48 <C> 0.62 <C> 0.70 <C> [BOLD] 0.79 <C> [BOLD] 0.90 <R> <C> bilstmchar <C> 0.13 <C> 0.39 <C> 0.48 <C> 0.55 <C> 0.63 <C> 0.78 <R> <C> cnnchar <C> [BOLD] 0.48 <C> [BOLD] 0.70 <C> [BOLD] 0.75 <C> [BOLD] 0.76 <C> 0.78 <C> 0.78 <R> <C> avemorph <C> 0.07 <C> 0.29 <C> 0.40 <C> 0.47 <C> 0.56 <C> 0.76 <R> <C> bilstmmorph <C> 0.013 <C> 0.36 <C> 0.45 <C> 0.52 <C> 0.60 <C> 0.76 <CAP> Table 4: Semantic evaluation of nearest neighbours using multi-label accuracy on words in different frequency bands.
<R> <C> [EMPTY] <C> Conv 1 <C> Conv 2 <C> Conv 3 <C> Conv 4 <C> Conv 5 <C> Conv 6 <C> Conv 7 <C> Conv 8 <R> <C> Kernel size <C> 1 <C> 2 <C> 3 <C> 4 <C> 5 <C> 6 <C> 7 <C> 7 <R> <C> Output channels <C> 32 <C> 32 <C> 64 <C> 128 <C> 256 <C> 512 <C> 1024 <C> 2048 <CAP> Table 8: One-dimensional convolutional layers used to process character inputs
<R> <C> [BOLD] Rep. <C> [BOLD] 100 <C> [BOLD] 500 <C> [BOLD] 1,000 <C> [BOLD] 2,000 <C> [BOLD] 5,000 <C> [BOLD] Avg. <R> <C> [BOLD] Banksearch <C> [BOLD] Banksearch <C> [BOLD] Banksearch <C> [BOLD] Banksearch <C> [BOLD] Banksearch <C> [BOLD] Banksearch <C> [BOLD] Banksearch <R> <C> FCC <C> 0.723 <C> 0.757 <C> 0.768 <C> 0.765 <C> 0.768 <C> 0.756 <R> <C> EFCC <C> 0.768 <C> 0.778 <C> 0.758 <C> 0.740 <C> 0.759 <C> 0.760 <R> <C> AddFCC <C> [BOLD] 0.775 <C> [BOLD] 0.788 <C> [BOLD] 0.777 <C> [BOLD] 0.784 <C> [BOLD] 0.779 <C> [BOLD] 0.781 <R> <C> [BOLD] WebKB <C> [BOLD] WebKB <C> [BOLD] WebKB <C> [BOLD] WebKB <C> [BOLD] WebKB <C> [BOLD] WebKB <C> [BOLD] WebKB <R> <C> FCC <C> 0.453 <C> 0.472 <C> 0.475 <C> 0.468 <C> 0.475 <C> 0.469 <R> <C> EFCC <C> [BOLD] 0.516 <C> [BOLD] 0.546 <C> [BOLD] 0.545 <C> [BOLD] 0.566 <C> [BOLD] 0.484 <C> [BOLD] 0.532 <R> <C> AddFCC <C> 0.459 <C> 0.493 <C> 0.494 <C> 0.491 <C> 0.471 <C> 0.482 <CAP> Fig. 3: Graphical representation of data in Table 3.
<R> <C> [BOLD] Rep. <C> [BOLD] 100 <C> [BOLD] 500 <C> [BOLD] 1,000 <C> [BOLD] 2,000 <C> [BOLD] 5,000 <C> [BOLD] Avg. <R> <C> [BOLD] SODP <C> [BOLD] SODP <C> [BOLD] SODP <C> [BOLD] SODP <C> [BOLD] SODP <C> [BOLD] SODP <C> [BOLD] SODP <R> <C> FCC <C> 0.195 <C> 0.237 <C> 0.254 <C> 0.256 <C> 0.266 <C> 0.242 <R> <C> AddFCC <C> 0.208 <C> 0.267 <C> 0.276 <C> 0.279 <C> 0.282 <C> 0.262 <R> <C> EFCC <C> 0.233 <C> 0.273 <C> [BOLD] 0.287 <C> 0.283 <C> [BOLD] 0.296 <C> 0.275 <R> <C> EFCC a-1 <C> 0.225 <C> 0.262 <C> 0.279 <C> 0.286 <C> 0.290 <C> 0.268 <R> <C> EFCC a-2 <C> 0.245 <C> 0.246 <C> 0.285 <C> 0.289 <C> 0.269 <C> 0.267 <R> <C> EFCC a-3 <C> 0.248 <C> 0.260 <C> 0.285 <C> [BOLD] 0.294 <C> 0.293 <C> 0.276 <R> <C> EFCC b-1 <C> [BOLD] 0.254 <C> [BOLD] 0.287 <C> 0.275 <C> 0.282 <C> 0.285 <C> [BOLD] 0.277 <R> <C> EFCC b-2 <C> [BOLD] 0.254 <C> 0.249 <C> 0.276 <C> 0.279 <C> 0.291 <C> 0.270 <R> <C> EFCC b-3 <C> 0.249 <C> 0.261 <C> 0.263 <C> 0.278 <C> 0.285 <C> 0.267 <CAP> Fig. 6: Graphical representation of data in Table 6.
<R> <C> [BOLD] Rep. <C> [BOLD] 100 <C> [BOLD] 500 <C> [BOLD] 1000 <C> [BOLD] 2000 <C> [BOLD] 5000 <C> [BOLD] Avg. <R> <C> [BOLD] Banksearch <C> [BOLD] Banksearch <C> [BOLD] Banksearch <C> [BOLD] Banksearch <C> [BOLD] Banksearch <C> [BOLD] Banksearch <C> [BOLD] Banksearch <R> <C> TF-IDF <C> 0.703 <C> 0.737 <C> 0.768 <C> [BOLD] 0.772 <C> 0.758 <C> 0.748 <R> <C> FCC <C> 0.723 <C> 0.757 <C> 0.768 <C> 0.765 <C> [BOLD] 0.768 <C> 0.756 <R> <C> EFCC <C> [BOLD] 0.768 <C> 0.778 <C> 0.758 <C> 0.740 <C> 0.759 <C> 0.760 <R> <C> AFCC <C> 0.767 <C> [BOLD] 0.785 <C> [BOLD] 0.787 <C> 0.757 <C> 0.753 <C> [BOLD] 0.770 <R> <C> [BOLD] WebKB <C> [BOLD] WebKB <C> [BOLD] WebKB <C> [BOLD] WebKB <C> [BOLD] WebKB <C> [BOLD] WebKB <C> [BOLD] WebKB <R> <C> TF-IDF <C> 0.385 <C> 0.438 <C> 0.466 <C> 0.498 <C> 0.513 <C> 0.460 <R> <C> FCC <C> 0.453 <C> 0.472 <C> 0.475 <C> 0.468 <C> 0.475 <C> 0.469 <R> <C> EFCC <C> 0.516 <C> 0.546 <C> 0.545 <C> 0.566 <C> 0.484 <C> 0.532 <R> <C> AFCC <C> [BOLD] 0.528 <C> [BOLD] 0.580 <C> [BOLD] 0.579 <C> [BOLD] 0.589 <C> [BOLD] 0.549 <C> [BOLD] 0.565 <R> <C> [BOLD] SODP <C> [BOLD] SODP <C> [BOLD] SODP <C> [BOLD] SODP <C> [BOLD] SODP <C> [BOLD] SODP <C> [BOLD] SODP <R> <C> TF-IDF <C> [BOLD] 0.244 <C> [BOLD] 0.300 <C> [BOLD] 0.293 <C> [BOLD] 0.307 <C> [BOLD] 0.323 <C> [BOLD] 0.293 <R> <C> FCC <C> 0.195 <C> 0.237 <C> 0.254 <C> 0.256 <C> 0.266 <C> 0.242 <R> <C> EFCC <C> 0.233 <C> 0.273 <C> 0.287 <C> 0.283 <C> 0.296 <C> 0.275 <R> <C> AFCC <C> 0.233 <C> 0.269 <C> 0.292 <C> 0.284 <C> 0.282 <C> 0.272 <CAP> Fig. 7: Graphical representation of data in Table 7.
<R> <C> [EMPTY] <C> Native <C> Transliterable <C> Weighted Average <R> <C> INIT <C> 0.79 <C> 0.38 <C> 0.69 <R> <C> GEN <C> 0.73 <C> 0.17 <C> 0.59 <R> <C> DTIM (n=1) <C> 0.81 <C> 0.44 <C> 0.72 <R> <C> DTIM (n=2) <C> 0.84 <C> 0.50 <C> 0.75 <R> <C> DTIM (n=3) <C> [BOLD] 0.86 <C> [BOLD] 0.60 <C> [BOLD] 0.79 <R> <C> DTIM (n=4) <C> [BOLD] 0.86 <C> [BOLD] 0.60 <C> [BOLD] 0.79 <CAP> Table 2: Clustering Quality (best result in each column highlighted)
<R> <C> Type Metric <C> Image-based representation Mean Rank <C> Image-based representation Mean Rank <C> Image-based representation Hits@10(%) <C> Image-based representation Hits@10(%) <C> Structure-based representation Mean Rank <C> Structure-based representation Mean Rank <C> Structure-based representation Hits@10(%) <C> Structure-based representation Hits@10(%) <R> <C> Metric <C> Raw <C> Filter <C> Raw <C> Filter <C> Raw <C> Filter <C> Raw <C> Filter <R> <C> IKRL (MAX) <C> 59 <C> 52 <C> 79.8 <C> 92.1 <C> 62 <C> 55 <C> 81.0 <C> 92.3 <R> <C> IKRL (AVG) <C> [BOLD] 29 <C> [BOLD] 22 <C> 79.3 <C> 92.9 <C> 43 <C> 36 <C> 80.7 <C> 92.8 <R> <C> IKRL (ATT) <C> [BOLD] 29 <C> [BOLD] 22 <C> [BOLD] 80.2 <C> [BOLD] 93.3 <C> [BOLD] 41 <C> [BOLD] 34 <C> [BOLD] 81.1 <C> [BOLD] 92.9 <CAP> Table 2: Evaluation results on different combination strategies
<R> <C> Metric <C> Mean Rank Raw <C> Mean Rank Filter <C> Hits@10(%) Raw <C> Hits@10(%) Filter <R> <C> TransE <C> 143 <C> 137 <C> 79.9 <C> 91.2 <R> <C> TransR <C> 147 <C> 140 <C> 80.1 <C> 91.7 <R> <C> IKRL (SBR) <C> 41 <C> 34 <C> [BOLD] 81.1 <C> 92.9 <R> <C> IKRL (IBR) <C> 29 <C> 22 <C> 80.2 <C> 93.3 <R> <C> IKRL (UNION) <C> [BOLD] 28 <C> [BOLD] 21 <C> 80.9 <C> [BOLD] 93.8 <CAP> Table 3: Evaluation results on entity prediction
<R> <C> Methods <C> Accuracy(%) <R> <C> TransE <C> 95.0 <R> <C> TransR <C> 95.3 <R> <C> IKRL (MAX) <C> 96.3 <R> <C> IKRL (AVG) <C> 96.6 <R> <C> IKRL (ATT) <C> [BOLD] 96.9 <CAP> Table 4: Evaluation results on triple classification
<R> <C> [BOLD] Model <C> [BOLD] Source  [BOLD] Median <C> [BOLD] Source  [BOLD] Mean <C> [BOLD] Target  [BOLD] Median <C> [BOLD] Target  [BOLD] Mean <R> <C> M1:  Bisk et al. <C> 3.29 <C> 3.47 <C> 3.60 <C> 3.70 <R> <C> M2: Our Replication of  Bisk et al. <C> 3.13 <C> 3.42 <C> 3.29 <C> 3.50 <R> <C> M3:  Tan and Bansal <C> – <C> 2.21 <C> 2.78 <C> 3.07 <R> <C> M4: Restrictive Advice w/o Pre-Trained Model <C> 3.88 <C> 3.83 <C> 3.56 <C> 3.43 <R> <C> M5: 4 Regions Restrictive Advice <C> 2.23 <C> 2.21 <C> 2.18 <C> 2.19 <R> <C> M6: Corrective Advice <C> 2.76 <C> 2.94 <C> 2.72 <C> 3.06 <R> <C> M7: 4 Regions Retry Advice <C> 2.41 <C> 3.02 <C> 2.42 <C> 3.14 <R> <C> M8: 2 Regions Model Self-Generated Advice <C> 3.01 <C> 3.31 <C> 3.08 <C> 3.36 <R> <C> M9: Input-Specific Model Self-Generated Advice <C> 2.87 <C> 3.12 <C> 2.99 <C> 3.26 <CAP> Table 1: Results for our models compared to previous models evaluated as distance from gold prediction normalized by block length for source and target coordinate prediction.
<R> <C> [BOLD] Emotion <C> [BOLD] Interrater Correlation <C> [BOLD] Cohen’s kappa <R> <C> admiration <C> 0.535 <C> 0.468 <R> <C> amusement <C> 0.482 <C> 0.474 <R> <C> anger <C> 0.207 <C> 0.307 <R> <C> annoyance <C> 0.193 <C> 0.192 <R> <C> approval <C> 0.385 <C> 0.187 <R> <C> caring <C> 0.237 <C> 0.252 <R> <C> confusion <C> 0.217 <C> 0.270 <R> <C> curiosity <C> 0.418 <C> 0.366 <R> <C> desire <C> 0.177 <C> 0.251 <R> <C> disappointment <C> 0.186 <C> 0.184 <R> <C> disapproval <C> 0.274 <C> 0.234 <R> <C> disgust <C> 0.192 <C> 0.241 <R> <C> embarrassment <C> 0.177 <C> 0.218 <R> <C> excitement <C> 0.193 <C> 0.222 <R> <C> fear <C> 0.266 <C> 0.394 <R> <C> gratitude <C> 0.645 <C> 0.749 <R> <C> grief <C> 0.162 <C> 0.095 <R> <C> joy <C> 0.296 <C> 0.301 <R> <C> love <C> 0.446 <C> 0.555 <R> <C> nervousness <C> 0.164 <C> 0.144 <R> <C> optimism <C> 0.322 <C> 0.300 <R> <C> pride <C> 0.163 <C> 0.148 <R> <C> realization <C> 0.194 <C> 0.155 <R> <C> relief <C> 0.172 <C> 0.185 <R> <C> remorse <C> 0.178 <C> 0.358 <R> <C> sadness <C> 0.346 <C> 0.336 <R> <C> surprise <C> 0.275 <C> 0.331 <CAP> Table 7: Interrater agreement, as measured by interrater correlation and Cohen’s kappa
<R> <C> context LSTM+kb <C> I like  [BOLD] celebrity-name madly! I like him as well! His performance on Spring Festival Gala is very real! His model on Spring Festival is very comical. But I still like him. r-LSTM <C> I like  [BOLD] celebrity-name madly! I like him as well! His performance on Spring Festival Gala is very real! His model on Spring Festival is very comical. But I still like him. Label <C> I like  [BOLD] celebrity-name madly! I like him as well! His performance on Spring Festival Gala is very real! His model on Spring Festival is very comical. But I still like him. Response <R> <C> 0.7959 <C> [BOLD] 0.8676 <C> 1 <C> Me too. This  [BOLD] uncle is really  [BOLD] handsome! <R> <C> [BOLD] 0.8655 <C> 0.7555 <C> 0 <C> I always feel that he is old-fashioned. <R> <C> 0.3531 <C> 0.1172 <C> 0 <C> So embarrassed tonight. <R> <C> context <C> Is  [BOLD] celebrity-name still a super-star? Her  [BOLD] acting is poor, isn’t it? celebrity-name remains at TV acting level, her cinematic feeling is so weak. <C> Is  [BOLD] celebrity-name still a super-star? Her  [BOLD] acting is poor, isn’t it? celebrity-name remains at TV acting level, her cinematic feeling is so weak. <C> Is  [BOLD] celebrity-name still a super-star? Her  [BOLD] acting is poor, isn’t it? celebrity-name remains at TV acting level, her cinematic feeling is so weak. <R> <C> 0.6772 <C> [BOLD] 0.9078 <C> 1 <C> Comparing with  [BOLD] actresses in the same period, her  [BOLD] acting and  [BOLD] appearance are both good. <R> <C> [BOLD] 0.9278 <C> 0.8217 <C> 0 <C> I like celebrity-name-1, the real  [BOLD] actress. <R> <C> 0.5598 <C> 0.0419 <C> 0 <C> I do not think so, because they just took it out for a walk. <CAP> Table 4: Samples of best response selection. Entities are anonymous during translation.
<R> <C> Model <C> Automatic Data <C> Dev <C> Test <C> [EMPTY] <C> 0.7000 <C> 0.6598 <C> 0.7514 <C> 0.7208 <R> <C> TK <C> [EMPTY] <C> 0.7340 <C> 0.7686 <C> 50k <C> 0.5580 <C> 0.6578 <C> 0.5428 <C> 0.7370 <R> <C> CNN(TK)* <C> 50k <C> 0.7160 <C> [BOLD] 0.7814 <C> 93k <C> 0.7000 <C> 0.6782 <C> 0.6957 <C> [BOLD] 0.7430 <R> <C> CNN(TK)* <C> 93k <C> [BOLD] 0.7380 <C> 0.7614 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 2: Accuracy on QL using all available GS data.
<R> <C> Method <C> Nonce Herbelot and Baroni ( 2017 ) Mean Recip. Rank <C> Nonce Herbelot and Baroni ( 2017 ) Med. Rank <C> Chimera Lazaridou et al. ( 2017 ) 2 Sent. <C> Chimera Lazaridou et al. ( 2017 ) 4 Sent. <C> Chimera Lazaridou et al. ( 2017 ) 6 Sent. <R> <C> word2vec <C> 0.00007 <C> 111012 <C> 0.1459 <C> 0.2457 <C> 0.2498 <R> <C> additive <C> 0.00945 <C> 3381 <C> 0.3627 <C> 0.3701 <C> 0.3595 <R> <C> additive, no stop words <C> 0.03686 <C> 861 <C> 0.3376 <C> 0.3624 <C> [BOLD] 0.4080 <R> <C> nonce2vec <C> 0.04907 <C> 623 <C> 0.3320 <C> 0.3668 <C> 0.3890 <R> <C> [ITALIC] à la carte <C> [BOLD] 0.07058 <C> [BOLD] 165.5 <C> [BOLD] 0.3634 <C> [BOLD] 0.3844 <C> 0.3941 <CAP> Table 1: Comparison with baselines and nonce2vec Herbelot and Baroni (2017) on few-shot embedding tasks. Performance on the chimeras task is measured using the Spearman correlation with human ratings. Note that the additive baseline requires removing stop-words in order to improve with more data.
<R> <C> Method <C> SemEval-2013 Task 12 nouns <C> SemEval-2015 Task 13 adj. <C> SemEval-2015 Task 13 nouns <C> SemEval-2015 Task 13 adv. <C> SemEval-2015 Task 13 verbs <C> SemEval-2015 Task 13 comb. <R> <C> [ITALIC] à la carte (SemCor) <C> 60.0 <C> 72.2 <C> 67.7 <C> 85.2 <C> 60.6 <C> 68.1 <R> <C> [ITALIC] à la carte (glosses) <C> 51.8 <C> 75.3 <C> 62.5 <C> 79.0 <C> 55.8 <C> 64.2 <R> <C> [ITALIC] à la carte (combined) <C> [BOLD] 60.5 <C> 74.1 <C> [BOLD] 70.3 <C> 86.4 <C> 59.4 <C> [BOLD] 69.6 <R> <C> MFS (SemCor) <C> 58.8 <C> [BOLD] 79.5 <C> 60.0 <C> [BOLD] 87.6 <C> [BOLD] 66.7 <C> 66.8 <R> <C> Raganato et al. ( 2017 ) <C> [BOLD] 66.9 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [BOLD] 72.4 <CAP> Table 2: Application of à la carte synset embeddings to two standard WSD tasks. As all systems always return exactly one answer, performance is measured in terms of accuracy. Results due to Raganato et al. (2017), who use a bi-LSTM for this task, are given as the recent state-of-the-art result.
<R> <C> [BOLD] Model <C> [BOLD] SNLI <C> [BOLD] MultiNLI+ <R> <C> Prior work: Baselines <C> Prior work: Baselines <C> Prior work: Baselines <R> <C> 100D LSTM (Yogatama) <C> 80.2 <C> — <R> <C> 300D LSTM (Williams) <C> 82.6 <C> 69.1 <R> <C> 100D Tree-LSTM (Yogatama) <C> 78.5 <C> — <R> <C> 300D SPINN (Williams) <C> 82.2 <C> 67.5 <R> <C> Prior work: Latent Tree Models <C> Prior work: Latent Tree Models <C> Prior work: Latent Tree Models <R> <C> 100D ST-Gumbel (Choi) <C> 81.9 <C> — <R> <C> 300D ST-Gumbel (Williams) <C> 83.3 <C> [BOLD] 69.5 <R> <C> 300D ST-Gumbel† (Williams) <C> [BOLD] 83.7 <C> 67.5 <R> <C> 100D CKY (Maillard) <C> 81.6 <C> — <R> <C> 100D RL-SPINN (Yogatama) <C> 80.5 <C> — <R> <C> 300D RL-SPINN† (Williams) <C> 82.3 <C> 67.4 <R> <C> This work: Latent Tree Models <C> This work: Latent Tree Models <C> This work: Latent Tree Models <R> <C> 100D CKY (Ours) <C> 82.2 <C> 69.1 <R> <C> 100D BSSR (Ours) <C> 83.0 <C> 69.0 <CAP> Table 1: SNLI and MultiNLI (matched) test set accuracy. †: results are for the model variant without the leaf RNN transformation.
<R> <C> System <C> Input Feature <C> Simu Dev <C> Real Dev <C> Simu Test <C> Real Test <R> <C> AM (baseline) <C> Fbank <C> 16.15 <C> 19.24 <C> 23.02 <C> 32.88 <R> <C> BeamformIt+AM <C> STFT <C> 14.32 <C> 12.99 <C> 24.36 <C> 21.21 <R> <C> BF+AM (fixed) <C> STFT <C> 15.23 <C> 15.01 <C> 23.14 <C> 25.64 <R> <C> BF+AM <C> STFT <C> 14.43 <C> 15.19 <C> 22.40 <C> 25.13 <R> <C> BF+AM+Feedback <C> STFT <C> 14.28 <C> 15.10 <C> 22.23 <C> 24.91 <CAP> Table 1: The WER performance (%) of the baseline LSTM acoustic model (AM), BeamformIt-enhanced signal as the input of the AM, joint training of LSTM beamformer and LSTM acoustic model (BF+AM) with or without acoustic feedback.
<R> <C> Models <C> Short Sentences (length <20) <C> Long Sentences (length >30) <R> <C> Sutskever2014Sequence <C> 15.50 <C> 33.46 <R> <C> bahdanau2014neural <C> 14.10 <C> 28.12 <R> <C> Context-In <C> 14.20 <C> 30.50 <R> <C> Context-IO <C> 14.10 <C> 29.50 <R> <C> Context-Attn <C> 13.75 <C> 27.00 <CAP> Table 3: Perplexities of models on sentences of different lengths.
<R> <C> Language <C> Domain <C> Original <C> Refined <C> # rel. data <C> # rel. gold <R> <C> English <C> Environment <C> 26.9 <C> [BOLD] 30.9 <C> 657 <C> 261 <R> <C> [EMPTY] <C> Science <C> 36.7 <C> [BOLD] 41.4 <C> 451 <C> 465 <R> <C> [EMPTY] <C> Food <C> 27.9 <C> [BOLD] 34.1 <C> 1898 <C> 1587 <R> <C> French <C> Environment <C> 23.7 <C> [BOLD] 28.3 <C> 114 <C> 266 <R> <C> [EMPTY] <C> Science <C> 31.8 <C> [BOLD] 33.1 <C> 118 <C> 451 <R> <C> [EMPTY] <C> Food <C> 22.4 <C> [BOLD] 28.9 <C> 598 <C> 1441 <R> <C> Italian <C> Environment <C> [BOLD] 31.0 <C> 30.8 <C> 2 <C> 266 <R> <C> [EMPTY] <C> Science <C> 32.0 <C> [BOLD] 34.2 <C> 4 <C> 444 <R> <C> [EMPTY] <C> Food <C> 16.9 <C> [BOLD] 18.5 <C> 57 <C> 1304 <R> <C> Dutch <C> Environment <C> [BOLD] 28.4 <C> 27.1 <C> 7 <C> 267 <R> <C> [EMPTY] <C> Science <C> 29.8 <C> [BOLD] 30.5 <C> 15 <C> 449 <R> <C> [EMPTY] <C> Food <C> 19.4 <C> [BOLD] 21.8 <C> 61 <C> 1446 <CAP> Table 3: F1 comparison between original (TAXI) and refined taxonomy using domain-specific embeddings.
<R> <C> [BOLD] Sentiment Level <C> [BOLD] Main-AHS <C> [BOLD] Sub-AHS <C> [BOLD] Ar-Twitter <C> [BOLD] ASTD <R> <C> Char-level <C> 0.8941 <C> 0.9164 <C> 0.8131 <C> 0.7419 <R> <C> Ch5gram-level <C> 0.9163 <C> [BOLD] 0.9568 <C> 0.8283 <C> 0.7762 <R> <C> Word-level <C> [BOLD] 0.9424 <C> 0.9510 <C> [BOLD] 0.8810 <C> 0.7641 <R> <C> [ITALIC] Alayba et al., 2018  <C> 0.92 <C> 0.95 <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] Dahou et al., 2016  <C> [EMPTY] <C> [EMPTY] <C> 85.01 <C> [BOLD] 79.07 <R> <C> [ITALIC] Abdulla et al., 2013  <C> [EMPTY] <C> [EMPTY] <C> 87.20 <C> [EMPTY] <CAP> Table 2: Accuracy comparison of the proposed method with different sentiment levels and other models for the same datasets.
<R> <C> [BOLD] Model <C> [BOLD] 5-way Acc. 5-shot <C> [BOLD] 5-way Acc. 10-shot <C> [BOLD] 10-way Acc. 5-shot <C> [BOLD] 10-way Acc. 10-shot <R> <C> Matching Networks (vinyals2016matching) <C> 82.54±0.12 <C> 84.63±0.08 <C> 73.64±0.15 <C> 76.72±0.07 <R> <C> Prototypical Networks (snell2017prototypical) <C> 81.82±0.08 <C> 85.83±0.06 <C> 73.31±0.14 <C> 75.97±0.11 <R> <C> Graph Network (Garcia2017FewShotLW) <C> 84.15±0.16 <C> 87.24±0.09 <C> 75.58±0.12 <C> 78.27±0.10 <R> <C> Relation Network (sung2018learning) <C> 84.41±0.14 <C> 86.93±0.15 <C> 75.28±0.13 <C> 78.61±0.06 <R> <C> SNAIL (mishra2017simple) <C> 84.62±0.16 <C> 87.31±0.11 <C> 75.74±0.07 <C> 79.26±0.09 <R> <C> Induction Networks (ours) <C> [BOLD] 87.16±0.09 <C> [BOLD] 88.49±0.17 <C> [BOLD] 78.27±0.14 <C> [BOLD] 81.64±0.08 <CAP> Table 3: Comparison of mean accuracy (%) on ODIC
<R> <C> [BOLD] Model <C> [BOLD] Image size <C> [BOLD] Learn. rate <C> [BOLD] Epochs <C> [BOLD] Accy. <R> <C> Simple model <C> 64 x 64 <C> 0.0001 <C> 160 <C> 94.12% <R> <C> VGG-16 <C> 64 x 64 <C> 0.0001 <C> 160 <C> 94.78% <R> <C> VGG-16 <C> 224 x 224 <C> 0.001 <C> 10 <C> 83.79% <R> <C> VGG-19 <C> 224 x 224 <C> 0.001 <C> 10 <C> 78.93% <R> <C> GoogleNet <C> 224 x 224 <C> 0.001 <C> 10 <C> 88.21% <R> <C> BoF SVM <C> 64 x 64 <C> [EMPTY] <C> [EMPTY] <C> 69.00% <CAP> Table 2: Results for image classification based on labels.
<R> <C> Evaluator <C> Min <C> Max <C> Avg <C> Median <R> <C> Participant 1 <C> 1 <C> 10 <C> 6.29 <C> 6 <R> <C> Participant 2 <C> 1 <C> 10 <C> 5.01 <C> 4 <R> <C> Participant 3 <C> 1 <C> 10 <C> 4.92 <C> 3 <R> <C> Participant 4 <C> 1 <C> 10 <C> 5.61 <C> 6 <R> <C> Participant 5 <C> 1 <C> 10 <C> 6.40 <C> 8 <CAP> Table 4: Results of evaluation of generated image captions.
<R> <C> [BOLD] Approach <C> [BOLD] Strict <C> [BOLD] Macro <C> [BOLD] Micro <R> <C> PLERen et al. ( 2016b ) <C> 49.44 <C> 68.75 <C> 64.54 <R> <C> C16-1017 <C> [BOLD] 70.43 <C> 75.78 <C> 76.50 <R> <C> AFETRen et al. ( 2016a ) <C> 67.00 <C> 72.70 <C> 73.50 <R> <C> FnetAbhishek et al. ( 2017 ) <C> 60.40 <C> 74.10 <C> 75.70 <R> <C> Our Approach <C> 60.87 <C> [BOLD] 77.75 <C> [BOLD] 76.94 <R> <C> w/o Adaptive thresholds <C> 58.47 <C> 75.84 <C> 75.03 <R> <C> w/o Document-level contexts <C> 58.12 <C> 75.65 <C> 75.11 <CAP> Table 4: Results on the BBN dataset.
<R> <C> [BOLD] Dataset Twitter <C> [BOLD] Dataset SemEval-2018 <C> [BOLD] Model Wu et al. ( 2018 ) <C> [BOLD] Accuracy  [BOLD] 0.735 <C> [BOLD] Precision 0.630 <C> [BOLD] Recall  [BOLD] 0.801 <C> [BOLD] F1-Score  [BOLD] 0.705 <R> <C> Twitter <C> SemEval-2018 <C> ELMo-BiLSTM <C> 0.708 <C> [BOLD] 0.696 <C> 0.697 <C> 0.696 <R> <C> Twitter <C> SemEval-2018 <C> ELMo-BiLSTM-FULL <C> 0.702 <C> 0.689 <C> 0.689 <C> 0.689 <R> <C> Twitter <C> SemEval-2018 <C> ELMo-BiLSTM-AUG <C> 0.658 <C> 0.651 <C> 0.657 <C> 0.651 <R> <C> Twitter <C> Riloff <C> Tay et al. ( 2018 ) <C> 0.823 <C> 0.738 <C> 0.732 <C> 0.732 <R> <C> Twitter <C> Riloff <C> ELMo-BiLSTM <C> 0.842 <C> 0.759 <C> [BOLD] 0.750 <C> [BOLD] 0.759 <R> <C> Twitter <C> Riloff <C> ELMo-BiLSTM-FULL <C> [BOLD] 0.858 <C> [BOLD] 0.778 <C> 0.735 <C> 0.753 <R> <C> Twitter <C> [EMPTY] <C> ELMo-BiLSTM-AUG <C> 0.798 <C> 0.684 <C> 0.708 <C> 0.694 <R> <C> Twitter <C> Ptáček <C> Tay et al. ( 2018 ) <C> 0.864 <C> 0.861 <C> 0.858 <C> 0.860 <R> <C> Twitter <C> Ptáček <C> ELMo-BiLSTM <C> [BOLD] 0.876 <C> 0.868 <C> 0.869 <C> 0.869 <R> <C> Twitter <C> Ptáček <C> ELMo-BiLSTM-FULL <C> 0.872 <C> [BOLD] 0.872 <C> [BOLD] 0.872 <C> [BOLD] 0.872 <R> <C> Twitter <C> [EMPTY] <C> ELMo-BiLSTM-AUG <C> 0.859 <C> 0.859 <C> 0.858 <C> 0.859 <R> <C> Dialog <C> SC-V1 <C> Tay et al. ( 2018 ) <C> 0.632 <C> 0.639 <C> 0.637 <C> 0.632 <R> <C> Dialog <C> SC-V1 <C> ELMo-BiLSTM <C> [BOLD] 0.646 <C> [BOLD] 0.650 <C> [BOLD] 0.646 <C> [BOLD] 0.644 <R> <C> Dialog <C> SC-V1 <C> ELMo-BiLSTM-FULL <C> 0.633 <C> 0.633 <C> 0.633 <C> 0.633 <R> <C> Dialog <C> SC-V2 <C> Tay et al. ( 2018 ) <C> 0.729 <C> 0.729 <C> 0.729 <C> 0.728 <R> <C> Dialog <C> SC-V2 <C> ELMo-BiLSTM <C> 0.748 <C> 0.748 <C> 0.747 <C> 0.747 <R> <C> Dialog <C> SC-V2 <C> ELMo-BiLSTM-FULL <C> [BOLD] 0.760 <C> [BOLD] 0.760 <C> [BOLD] 0.760 <C> [BOLD] 0.760 <R> <C> Reddit <C> SARC 2.0 <C> Khodak et al. ( 2017 ) <C> 0.758 <C> - <C> - <C> - <R> <C> Reddit <C> SARC 2.0 <C> ELMo-BiLSTM <C> [BOLD] 0.773 <C> - <C> - <C> - <R> <C> Reddit <C> SARC 2.0 <C> ELMo-BiLSTM-FULL <C> 0.702 <C> 0.760 <C> 0.760 <C> 0.760 <R> <C> Reddit <C> SARC 2.0 pol <C> Khodak et al. ( 2017 ) <C> 0.765 <C> - <C> - <C> - <R> <C> Reddit <C> SARC 2.0 pol <C> ELMo-BiLSTM <C> [BOLD] 0.785 <C> - <C> - <C> - <R> <C> Reddit <C> SARC 2.0 pol <C> ELMo-BiLSTM-FULL <C> 0.720 <C> 0.720 <C> 0.720 <C> 0.720 <CAP> Table 2: Summary of our obtained results.
<R> <C> Language <C> BLEU <C> METEOR <C> chrF <R> <C> French <C> 0.415 <C> 0.440 <C> [BOLD] 0.586∗ <R> <C> English <C> 0.357 <C> 0.478∗ <C> [BOLD] 0.497 <CAP> Table 3: Correlation of automatic metrics to human judgment of adversarial source and target sentences. “∗” indicates that the correlation is significantly better than the next-best one.
<R> <C> Language pair <C> cs-en <C> de-en <C> fr-en <R> <C> Base <C> 24.11 <C> 24.94 <C> 23.60 <R> <C> [EMPTY] <C> [ITALIC] α=1.0 <C> [ITALIC] α=1.0 <C> [ITALIC] α=1.0 <R> <C> Unconstrained-adv <C> 25.99 <C> 26.24 <C> 25.67 <R> <C> CharSwap-adv <C> 16.46 <C> 17.19 <C> 15.72 <R> <C> [EMPTY] <C> [ITALIC] α=0.5 <C> [ITALIC] α=0.5 <C> [ITALIC] α=0.5 <R> <C> Unconstrained-adv <C> 26.52 <C> 27.26 <C> 24.92 <R> <C> CharSwap-adv <C> 20.41 <C> 20.24 <C> 16.08 <CAP> Table 6: Robustness to CharSwap attacks on the validation set with/without adversarial training (rdb). Lower is better.
<R> <C> Speaker SP01 <C> Set No 35 <C> [ITALIC] Vi /s/ /r/ <C> [ITALIC] Vj /dh/ <C> Set No 34 <C> [ITALIC] Vn /s/ /r/ /dh/ <R> <C> SP02 <C> 22 <C> /d/ <C> /z/ /y/ <C> 21 <C> /d/ /z/ /y/ <R> <C> SP03 <C> 34 <C> /b/ /ch/ <C> /zh/ <C> 33 <C> /b/ /ch/ /zh/ <R> <C> SP03 <C> 31 <C> /zh/ /b/ /ch/ <C> /z/ <C> 30 <C> /zh/ /b/ /ch/ /z/ <R> <C> SP03 <C> 25 <C> /p/ /r/ <C> /ng/ <C> 24 <C> /p/ /r/ /ng/ <R> <C> SP05 <C> 17 <C> /ae/ <C> /eh/ <C> 16 <C> /ae/ /eh/ <R> <C> SP06 <C> 35 <C> /ae/ /ah/ <C> /iy/ <C> 34 <C> /ae/ /ah/ /iy/ <R> <C> SP09 <C> 12 <C> /b/ /w/ /v/ <C> /jh/ /hh/ <C> 11 <C> /b/ /w/ /v/ /jh/ /hh/ <R> <C> SP12 <C> 36 <C> /ah/ <C> /ao/ <C> 34 <C> /ah/ /ao/ <CAP> Table 2: Viseme class merges which improve word recognition
<R> <C> [EMPTY] <C> # <R> <C> Documents <C> 35,194 <R> <C> Sentences <C> 837,914 <R> <C> Tokens <C> 14,819,248 <R> <C> Types, full-forms <C> 521,563 <R> <C> Types, lemmas <C> 446,532 <CAP> Table 2: Basic corpus counts.
<R> <C> Category <C> # Reviews <R> <C> Screen <C> 13,085 <R> <C> Music <C> 12,410 <R> <C> Literature <C> 3,530 <R> <C> Products <C> 3,120 <R> <C> Games <C> 1,765 <R> <C> Restaurants <C> 534 <R> <C> Stage <C> 530 <R> <C> Sports <C> 118 <R> <C> Misc <C> 102 <R> <C> Total <C> 35,194 <CAP> Table 3: Number of reviews across categories.
<R> <C> [EMPTY] <C> Model <C> [ITALIC] Pa <C> [ITALIC] Ra <C> [ITALIC] F1 [ITALIC] a <C> [ITALIC] Pe <C> [ITALIC] Re <C> [ITALIC] F1 [ITALIC] e <R> <C> Env <C> TAXI (DAG) <C> 50.1 <C> 32.7 <C> 39.6 <C> 33.8 <C> 26.8 <C> 29.9 <R> <C> Env <C> TAXI (tree) <C> [BOLD] 67.5 <C> 30.8 <C> 42.3 <C> [BOLD] 41.1 <C> 23.1 <C> 29.6 <R> <C> Env <C> SubSeq <C> - <C> - <C> - <C> - <C> - <C> 22.4 <R> <C> Env <C> TaxoRL (Partial) <C> 51.6 <C> 36.4 <C> 42.7 <C> 37.5 <C> 24.2 <C> 29.4 <R> <C> Env <C> TaxoRL (Full) <C> 47.2 <C> [BOLD] 54.6 <C> [BOLD] 50.6 <C> 32.3 <C> [BOLD] 32.3 <C> [BOLD] 32.3 <R> <C> Sci <C> TAXI (DAG) <C> 61.6 <C> 41.7 <C> 49.7 <C> 38.8 <C> 34.8 <C> 36.7 <R> <C> Sci <C> TAXI (tree) <C> 76.8 <C> 38.3 <C> 51.1 <C> 44.8 <C> 28.8 <C> 35.1 <R> <C> Sci <C> SubSeq <C> - <C> - <C> - <C> - <C> - <C> 39.9 <R> <C> Sci <C> TaxoRL (Partial) <C> [BOLD] 84.6 <C> 34.4 <C> 48.9 <C> [BOLD] 56.9 <C> 33.0 <C> [BOLD] 41.8 <R> <C> Sci <C> TaxoRL (Full) <C> 68.3 <C> [BOLD] 52.9 <C> [BOLD] 59.6 <C> 37.9 <C> [BOLD] 37.9 <C> 37.9 <CAP> Table 2: Results of the hypernymy organization experiment. Our approach outperforms Panchenko et al. (2016); Gupta et al. (2017) when the same hypernym graph is used as input. The precision of partial induction in both metrics is high. The precision of full induction is relatively lower but its recall is much higher.
<R> <C> Model <C> [ITALIC] Pa <C> [ITALIC] Ra <C> [ITALIC] F1 [ITALIC] a <C> [ITALIC] F1 [ITALIC] e <R> <C> [BOLD] Distributional Info <C> 27.1 <C> 24.3 <C> 25.6 <C> 13.8 <R> <C> [BOLD] Path-based Info <C> 27.8 <C> 48.5 <C> 33.7 <C> 27.4 <R> <C> [BOLD] D +  [BOLD] P <C> 36.6 <C> 39.4 <C> 37.9 <C> 28.3 <R> <C> [BOLD] D +  [BOLD] P +  [BOLD] Surface Features <C> 41.3 <C> 49.2 <C> 44.9 <C> 35.6 <R> <C> [BOLD] D +  [BOLD] P +  [BOLD] S + FG <C> [BOLD] 52.9 <C> [BOLD] 58.6 <C> [BOLD] 55.6 <C> [BOLD] 43.8 <CAP> Table 3: Ablation study on the WordNet dataset Bansal et al. (2014). Pe and Re are omitted because they are the same as F1e for each model. We can see that our approach benefits from multiple sources of information which are complementary to each other.
<R> <C> Model <C> F1 <R> <C> Ratinov and Roth ( 2009 ) <C> 86.82 <R> <C> Collobert et al. ( 2011 ) <C> 86.96 <R> <C> Lample et al. ( 2016 ) <C> 90.33 <R> <C> Bi-LSTM <C> 89.34 ± 0.28 <R> <C> 4-layer CNN <C> 89.97 ± 0.20 <R> <C> 5-layer CNN <C> 90.23 ± 0.16 <R> <C> ID-CNN <C> 90.32 ± 0.26 <R> <C> Collobert et al. ( 2011 ) <C> 88.67 <R> <C> Passos et al. ( 2014 ) <C> 90.05 <R> <C> Lample et al. ( 2016 ) <C> 90.20 <R> <C> Bi-LSTM-CRF (re-impl) <C> 90.43 ± 0.12 <R> <C> ID-CNN-CRF <C> [BOLD] 90.54 ± 0.18 <CAP> Table 1: F1 score of models observing sentence-level context. No models use character embeddings or lexicons. Top models are greedy, bottom models use Viterbi inference .
<R> <C> [BOLD] Model <C> [BOLD] English CELEX <C> [BOLD] Dutch CELEX <C> [BOLD] Festival <C> [BOLD] OpenLexique <C> [BOLD] IIT-Guwahat <C> [BOLD] E-Hitz <R> <C> [BOLD] Base <C> 98.5±0.1 <C> 99.47±0.04 <C> 99.990±0.005 <C> 99.98±0.01 <C> 94.9±0.3 <C> 99.83±0.07 <R> <C> Small <C> 98.2±0.2 <C> 99.39±0.04 <C> 99.990±0.004 <C> 99.987±0.007 <C> 95.4±0.3 <C> 99.68±0.06 <R> <C> Base-Softmax <C> 97.7±0.2 <C> 99.24±0.06 <C> 99.984±0.003 <C> 100.00±0.01 <C> 94.7±0.3 <C> 99.71±0.04 <CAP> TABLE III: The accuracy of our proposed model on each evaluation dataset. Model accuracy (%±σ) is reported on a word level which means the entire word must be syllabified correctly.
<R> <C> [EMPTY] <C> single speaker dev93 <C> single speaker eval92 <C> speaker change dev93∗ <C> speaker change eval92∗ <R> <C> i-vector <C> [BOLD] 6.4 <C> 4.7 <C> 10.4 <C> 7.8 <R> <C> M-vector <C> 6.5 <C> [BOLD] 4.2 <C> [BOLD] 7.6 <C> [BOLD] 4.9 <CAP> Table 3: WERs [%] on single speaker utterances (dev93, eval92) and simulated speaker change utterances (dev93∗, eval92∗) from WSJ.
<R> <C> [EMPTY] <C> single speaker dev <C> single speaker test <C> speaker change dev∗ <C> speaker change test∗ <R> <C> i-vector <C> [BOLD] 11.7 <C> 11.2 <C> 16.1 <C> 15.9 <R> <C> M-vector <C> 11.8 <C> [BOLD] 11.0 <C> [BOLD] 14.1 <C> [BOLD] 11.9 <CAP> Table 6: WERs [%] on single speaker utterances (dev, test) and simulated speaker change utterances (dev∗, test∗) from TED-LIUM2.
<R> <C> [EMPTY] <C> [BOLD] S <C> [BOLD] D <C> [BOLD] Q <C> [BOLD] C <C> [BOLD] Total <R> <C> [BOLD] train <C> 925 <C> 378 <C> 395 <C> 3519 <C> 5217 <R> <C> in % <C> 18 <C> 7 <C> 8 <C> 67 <C> [EMPTY] <R> <C> [BOLD] dev <C> 102 <C> 82 <C> 120 <C> 1181 <C> 1485 <R> <C> in % <C> 7 <C> 6 <C> 8 <C> 80 <C> [EMPTY] <R> <C> [BOLD] test <C> 157 <C> 101 <C> 93 <C> 1476 <C> 1827 <R> <C> in % <C> 9 <C> 6 <C> 5 <C> 81 <C> [EMPTY] <CAP> Table 1: Histogram and distribution of examples through classes in the train/dev/test dataset splits. The individual examples belong into 327/38/81 train/dev/test tree-structured discussions.
<R> <C> [EMPTY] <C> #Θ <C> Acc [ITALIC] test <C> macro F1 [ITALIC] dev <C> macro F1 [ITALIC] test <C> F1 [ITALIC] S <C> F1 [ITALIC] Q <C> F1 [ITALIC] D <C> F1 [ITALIC] C <R> <C> Branch-LSTM <C> 453K <C> 84.10 <C> - <C> 49.30 <C> 43.80 <C> 55.00 <C> 7.10 <C> 91.30 <R> <C> FeaturesNN <C> 205K <C> 82.84 <C> 45.46±1e−2 <C> 44.55±2e−2 <C> 40.29 <C> 40.12 <C> 17.69 <C> 80.43 <R> <C> BiLSTM+SelfAtt <C> 28M <C> 83.59 <C> 47.55±6e−3 <C> 46.81±6e−3 <C> 42.21 <C> 45.20 <C> 17.75 <C> 81.92 <R> <C> BERT [ITALIC] base <C> 109M <C> 84.67 <C> 51.40±1e−2 <C> 53.39±3e−2 <C> 43.49 <C> 59.88 <C> 18.42 <C> 90.36 <R> <C> BERT [ITALIC] big− [ITALIC] noprev <C> 335M <C> 84.33 <C> 52.61±2e−2 <C> 52.91±4e−2 <C> 42.37 <C> 55.17 <C> 24.44 <C> 90.15 <R> <C> BERT [ITALIC] big− [ITALIC] nosrc <C> 335M <C> 84.51 <C> 53.72±2e−2 <C> 55.13±3e−3 <C> 43.02 <C> 56.93 <C> 26.53 <C> 90.51 <R> <C> BERT [ITALIC] big <C> 335M <C> 84.08 <C> 56.24±9e−3 <C> 56.70±3e−2 <C> 44.29 <C> 57.07 <C> 35.02 <C> 90.41 <R> <C> BERT [ITALIC] big EXC-N∗ <C> - <C> 85.50 <C> 58.63 <C> 60.28 <C> 48.89 <C> 62.80 <C> 37.50 <C> 91.94 <R> <C> BERT [ITALIC] big TOP-N∗ <C> - <C> 85.22 <C> 62.58 <C> 60.67 <C> 48.25 <C> 62.86 <C> 39.74 <C> 91.83 <R> <C> BERT [ITALIC] big OPT-F1 <C> - <C> 85.39 <C> 62.68 <C> 61.27 <C> 48.03 <C> 62.26 <C> 42.77 <C> 92.01 <R> <C> BERT [ITALIC] big TOP-N [ITALIC] s <C> - <C> 85.50 <C> 61.73 <C> [BOLD] 61.67 <C> 49.11 <C> 64.45 <C> 41.29 <C> 91.84 <CAP> Table 2: Our achieved results. Results for single model were obtained by training at least 10 models and we report mean and standard deviation for these. #Θ denotes the number of parameters. The columns F1S through F1C contain individual F1 scores for problem classes. All ensemble models are optimized for F1-score on dev data. BiLSTM+SelfAtt contains 4.2M parameters without pre-trained BERT embeddings. BERTbig−nosrc and BERTbig−noprev denote ablations with empty source or target post respectively. Note that the accuracy is biased towards different training data prior as shown in Table 1. Our SemEval submissions are denoted with ∗. Winning BLCU-nlp system achieved 61.87 F1 score on test data. More available at http://tinyurl.com/y3m5mskd.
<R> <C> [BOLD] Model <C> [BOLD] Perplexity <C> [BOLD] Perplexity@ [BOLD] U3 <C> [BOLD] Error-Rate <C> [BOLD] Error-Rate@ [BOLD] U3 <R> <C> Backoff N-Gram <C> 64.89 <C> 65.05 <C> - <C> - <R> <C> Modified Kneser-Ney <C> 60.11 <C> 54.75 <C> - <C> - <R> <C> Absolute Discounting N-Gram <C> 56.98 <C> 57.06 <C> - <C> - <R> <C> Witten-Bell Discounting N-Gram <C> 53.30 <C> 53.34 <C> - <C> - <R> <C> RNN <C> 35.63±0.16 <C> 35.30±0.22 <C> 66.34%±0.06 <C> 66.32%±0.08 <R> <C> DCGM-I <C> 36.10±0.17 <C> 36.14±0.26 <C> 66.44%±0.06 <C> 66.57%±0.10 <R> <C> HRED <C> 36.59±0.19 <C> 36.26±0.29 <C> 66.32%±0.06 <C> 66.32%±0.11 <R> <C> HRED + Word2Vec <C> 33.95±0.16 <C> 33.62±0.25 <C> 66.06%±0.06 <C> 66.05%±0.09 <R> <C> RNN + SubTle <C> 27.09±0.13 <C> 26.67±0.19 <C> 64.10%±0.06 <C> 64.07%±0.10 <R> <C> HRED + SubTle <C> 27.14±0.12 <C> 26.60±0.19 <C> 64.10%±0.06 <C> 64.03%±0.10 <R> <C> HRED-Bi. + SubTle <C> [BOLD] 26.81± [BOLD] 0.11 <C> [BOLD] 26.31± [BOLD] 0.19 <C> [BOLD] 63.93%± [BOLD] 0.06 <C> [BOLD] 63.91%± [BOLD] 0.09 <CAP> Table 2: Test set results computed on {U1,U2,U3} and solely on {U3} conditioned on {U1,U2}. Standard deviations are shown for all neural models. Best performances are marked in bold.
<R> <C> Name <C> Squad v1.1 F1 <R> <C> Base 384 <C> 87.99 <R> <C> Position 384 <C> 88.26 <CAP> Table 1: GPU phase 2 bert base performance
<R> <C> [BOLD] LAYER SIZE <C> [BOLD] SPARSITY <C> [BOLD] LAYER TYPE <C> [BOLD] TIME ( [ITALIC] μsec) <C> [BOLD] SPEEDUP <R> <C> 1760 <C> 0% <C> RNN <C> 56 <C> 1 <R> <C> 1760 <C> 95% <C> RNN <C> 20 <C> 2.8 <R> <C> 2560 <C> 95% <C> RNN <C> 29 <C> 1.93 <R> <C> 3072 <C> 95% <C> RNN <C> 48 <C> 1.16 <R> <C> 2560 <C> 0% <C> GRU <C> 313 <C> 1 <R> <C> 2560 <C> 95% <C> GRU <C> 46 <C> 6.80 <R> <C> 3568 <C> 95% <C> GRU <C> 89 <C> 3.5 <CAP> Table 6: GEMM times for recurrent layers with different sparsity
<R> <C> language pair <C> [ITALIC] α <C> [ITALIC] wf <C> [ITALIC] wexact <C> [ITALIC] wstem <C> [ITALIC] wsynonym <C> [ITALIC] wparaphrase <R> <C> *-en <C> 0.85 <C> 0.25 <C> 1.0 <C> 0.6 <C> 0.8 <C> 0.6 <CAP> Table 5: Parameter values of DPMF. *-en represents all the language pairs with English as target language.
<R> <C> [BOLD] Input <C> [BOLD] Accuracy, % <R> <C> Transcript text <C> 99.2 <R> <C> Recognized text <C> 98.1 <R> <C> Audio <C> 97.2 <CAP> Table 1: Results for domain classification. The first row corresponds to evaluation on clean transcripts, the maximum performance achievable with this model.
<R> <C> WikiLarge <C> SARI <C> ADD <C> DEL <C> KEEP <C> FKGL <C> Copy <R> <C> SBMT-SARI <C> 37.94 <C> [BOLD] 05.60 <C> 37.96 <C> 70.27 <C> 8.89 <C> 0.10 <R> <C> DMASS+DCSS <C> 37.01 <C> 05.16 <C> 40.90 <C> 64.96 <C> 9.24 <C> 0.06 <R> <C> PBMT-R <C> 35.92 <C> 05.44 <C> 32.07 <C> 70.26 <C> 10.16 <C> 0.11 <R> <C> Hybrid <C> 28.75 <C> 01.38 <C> [BOLD] 41.45 <C> 43.42 <C> [BOLD] 7.85 <C> [BOLD] 0.04 <R> <C> NTS <C> 33.97 <C> 03.57 <C> 30.02 <C> 68.31 <C> 9.63 <C> 0.11 <R> <C> DRESS <C> 33.30 <C> 02.74 <C> 32.93 <C> 64.23 <C> 8.79 <C> 0.22 <R> <C> DRESS-LS <C> 32.98 <C> 02.57 <C> 30.77 <C> 65.60 <C> 8.94 <C> 0.27 <R> <C> EditNTS <C> 34.94 <C> 03.23 <C> 32.37 <C> 69.22 <C> 9.42 <C> 0.12 <R> <C> LaserTagger <C> 32.31 <C> 03.02 <C> 33.63 <C> 60.27 <C> 9.82 <C> 0.21 <R> <C> FelixPoint <C> 34.37 <C> 02.35 <C> 34.80 <C> 65.97 <C> 9.47 <C> 0.18 <R> <C> FelixInsert <C> 35.79 <C> 04.03 <C> 39.70 <C> 63.64 <C> 8.14 <C> 0.09 <R> <C> Felix <C> [BOLD] 38.13 <C> 03.55 <C> 40.45 <C> [BOLD] 70.39 <C> 8.98 <C> 0.08 <CAP> Table 4: Sentence Simplification results on WikiLarge.
<R> <C> [BOLD] Model <C> [BOLD] F1 <R> <C> BiLSTM-CRF Chiu and Nichols ( 2016 ) <C> 86.28 <R> <C> ID-CNN Strubell et al. ( 2017 ) <C> 86.84 <R> <C> BiLSTM-CRF Strubell et al. ( 2017 ) <C> 86.99 <R> <C> Merge and Label <C> [BOLD] 87.59 <R> <C> LM embeddings or extra data <C> [EMPTY] <R> <C> BiLSTM-CRF lex Ghaddar and Langlais ( 2018 ) <C> 87.95 <R> <C> BiLSTM-CRF with CVT Clark et al. ( 2018 ) <C> 88.81 <R> <C> Merge and Label [BERT] <C> 89.20 <R> <C> BiLSTM-CRF Flair Akbik et al. ( 2018 ) <C> [BOLD] 89.71 <CAP> Table 2: OntoNotes NER
<R> <C> Vocabulary size <C> [ITALIC] eval1 <C> [ITALIC] eval3 <R> <C> 1k <C> 11.87 <C> 18.86 <R> <C> 5k <C> 3.14 <C> 8.20 <R> <C> 10k <C> 1.65 <C> 4.79 <R> <C> 15k <C> 1.30 <C> 3.64 <R> <C> 20k <C> 0.99 <C> 3.10 <R> <C> 25k <C> 0.92 <C> 2.69 <CAP> Table 3: The OOV rates of in-domain (eval1) and out-of-domain (eval3) test sets in CSJ corpus (%).
<R> <C> Fold <C> Train(facts/relations) <C> Test(facts/relations) <C> Average (%) <C> Bias(%) <C> [ITALIC] σ <R> <C> 1 <C> 444422/1210 <C> 77436/135 <C> 61.18 <C> (-1.9, +1.14) <C> 0.0791 <R> <C> 2 <C> 437977/1211 <C> 10942/134 <C> 54.62 <C> (-1.22,+0.78 ) <C> 0.0985 <R> <C> 3 <C> 455313/1210 <C> 50144/135 <C> 55.07 <C> (-0.53 ,+0.53) <C> 0.0705 <R> <C> 4 <C> 424743/1210 <C> 14430/135 <C> 54.54 <C> (-1.42,+0) <C> 0.0768 <R> <C> 5 <C> 416196/1210 <C> 16322/135 <C> 52.4 <C> (-1.5,+1.0) <C> 0.0961 <R> <C> 6 <C> 429286/1211 <C> 12952/134 <C> 55.76 <C> (-0.14,+0.29) <C> 0.0993 <R> <C> 7 <C> 404590/1210 <C> 156120/135 <C> 53.82 <C> (-1.12,+0.84) <C> 0.0806 <R> <C> 8 <C> 447139/1211 <C> 8652/134 <C> 55.63 <C> (-0.23,+0.27 ) <C> 0.0723 <R> <C> 9 <C> 442557/1211 <C> 9955/134 <C> 55.27 <C> (-0.19 ,+0.45) <C> 0.1057 <R> <C> 10 <C> 446054/1211 <C> 9168/134 <C> 56.69 <C> (-0.19 ,+0.41) <C> 0.0845 <R> <C> Average <C> [EMPTY] <C> [EMPTY] <C> 55.5 <C> (-0.84,+0.48) <C> [EMPTY] <CAP> Table 3: Prediction accuracy for unseen facts of FB15K using 10-fold cross validation.
<R> <C> [EMPTY] <C> FB13 HIT@10 <C> FB13 HIT@10 <C> FB13 HIT@3 <C> FB13 HIT@3 <C> FB13 HIT@1 <C> FB13 HIT@1 <C> WN11 HIT@10 <C> WN11 HIT@10 <C> WN11 HIT@3 <C> WN11 HIT@3 <C> WN11 HIT@1 <C> WN11 HIT@1 <R> <C> Metric <C> Raw <C> Filter <C> Raw <C> Filter <C> Raw <C> Filter <C> Raw <C> Filter <C> Raw <C> Filter <C> Raw <C> Filter <R> <C> Rescal <C> 23.86 <C> 24.59 <C> 18.86 <C> 19.80 <C> 5.70 <C> 6.92 <C> 2.64 <C> 2.84 <C> 1.52 <C> 1.65 <C> 0.81 <C> 0.97 <R> <C> TransE <C> 32.65 <C> 32.75 <C> 24.86 <C> 25.16 <C> 17.28 <C> 17.57 <C> 25.84 <C> 30.75 <C> 15.37 <C> 19.47 <C> 7.2 <C> 10.23 <R> <C> TransR <C> 26.83 <C> 27.53 <C> 20.34 <C> 20.94 <C> 14.04 <C> 14.97 <C> 8.38 <C> 8.66 <C> 4.72 <C> 4.84 <C> 2.02 <C> 2.24 <R> <C> TransH <C> 25.02 <C> 25.80 <C> 18.03 <C> 18.78 <C> 6.62 <C> 7.52 <C> 23.87 <C> 29.62 <C> 13.17 <C> 19.13 <C> 4.97 <C> 9.29 <R> <C> TransD <C> 34.77 <C> 35.22 <C> 26.75 <C> 27.77 <C> 18.47 <C> 19.85 <C> 25.99 <C> 26.73 <C> 19.14 <C> 19.84 <C> 7.99 <C> 8.84 <R> <C> DistMult <C> 16.72 <C> 17.82 <C> 14.12 <C> 14.80 <C> 6.27 <C> 7.03 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> Complex <C> 16.51 <C> 17.60 <C> 14.21 <C> 14.80 <C> 6.80 <C> 7.53 <C> 14.68 <C> 15.94 <C> 10.06 <C> 10.99 <C> 6.3 <C> 7.4 <R> <C> TransW <C> [BOLD] 45.12 <C> [BOLD] 47.93 <C> [BOLD] 35.71 <C> [BOLD] 35.94 <C> [BOLD] 29.77 <C> [BOLD] 30.62 <C> 21.15 <C> 23.2 <C> 13.35 <C> 14.26 <C> 4.27 <C> 5.23 <CAP> Table 4: Results on FB13&WN11 for link prediction (%)
<R> <C> [BOLD] Model <C> [BOLD] Val Seen TL (m) <C> [BOLD] Val Seen NE (m) <C> [BOLD] Val Seen SR (%) <C> [BOLD] Val Seen OSR (%) <C> [BOLD] Val Unseen TL (m) <C> [BOLD] Val Unseen NE (m) <C> [BOLD] Val Unseen SR (%) <C> [BOLD] Val Unseen OSR (%) <C> [BOLD] Test (unseen) TL (m) <C> [BOLD] Test (unseen) NE (m) <C> [BOLD] Test (unseen) SR (%) <C> [BOLD] Test (unseen) OSR (%) <R> <C> Shortest <C> 10.19 <C> 0.00 <C> 100 <C> 100 <C> 9.48 <C> 0.00 <C> 100 <C> 100 <C> 9.93 <C> 0.00 <C> 100 <C> 100 <R> <C> Random <C> 9.58 <C> 9.45 <C> 15.9 <C> 21.4 <C> 9.77 <C> 9.23 <C> 16.3 <C> 22.0 <C> 9.93 <C> 9.77 <C> 13.2 <C> 18.3 <R> <C> Teacher-forcing <C> 10.95 <C> 8.01 <C> 27.1 <C> 36.7 <C> 10.67 <C> 8.61 <C> 19.6 <C> 29.1 <C> - <C> - <C> - <C> - <R> <C> Student-forcing <C> 11.33 <C> 6.01 <C> 38.6 <C> 52.9 <C> 8.39 <C> 7.81 <C> 21.8 <C> 28.4 <C> 8.13 <C> 7.85 <C> 20.4 <C> 26.6 <R> <C> [BOLD] Ours <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> XE <C> 11.51 <C> 5.79 <C> 40.2 <C> [BOLD] 54.1 <C> 8.94 <C> 7.97 <C> 21.3 <C> 28.7 <C> 9.37 <C> 7.82 <C> 22.1 <C> 30.1 <R> <C> Model-free RL <C> 10.88 <C> 5.82 <C> 41.9 <C> 53.5 <C> 8.75 <C> 7.88 <C> 21.5 <C> 28.9 <C> 8.83 <C> 7.76 <C> 23.1 <C> 30.2 <R> <C> RPA <C> 8.46 <C> [BOLD] 5.56 <C> [BOLD] 42.9 <C> 52.6 <C> 7.22 <C> [BOLD] 7.65 <C> [BOLD] 24.6 <C> [BOLD] 31.8 <C> 9.15 <C> [BOLD] 7.53 <C> [BOLD] 25.3 <C> [BOLD] 32.5 <CAP> Table 1: Results on both the validation sets and test set in terms of four metrics: Trajectory Length (TL), Navigation Error (NE), Success Rate (SR), and Oracle Success Rate (OSR). We list the best results as reported in [3], of which Student-forcing performs the best. Our RPA method significantly outperforms the previous best results, and it is also noticeable that we gain a larger improvement on the unseen sets, which proves that our RPA method is more generalized.
<R> <C> [BOLD] Reward <C> [BOLD] Val Seen Navigation Error (m) <C> [BOLD] Val Seen Success (%) <C> [BOLD] Val Seen Oracle Success (%) <C> [BOLD] Val Unseen Navigation Error (m) <C> [BOLD] Val Unseen Success (%) <C> [BOLD] Val Unseen Oracle Success (%) <R> <C> [ITALIC] Global Distance <C> 6.17 <C> 35.5 <C> 45.1 <C> 8.20 <C> 19.0 <C> 25.6 <R> <C> [ITALIC] Success <C> 6.21 <C> 37.8 <C> 43.2 <C> 8.17 <C> 21.3 <C> 26.7 <R> <C> [ITALIC] Discounted <C> [BOLD] 5.79 <C> 40.5 <C> 52.8 <C> [BOLD] 7.74 <C> 20.4 <C> 28.5 <R> <C> [ITALIC] Discounted &  [ITALIC] Success <C> 5.82 <C> [BOLD] 41.9 <C> [BOLD] 53.5 <C> 7.88 <C> [BOLD] 21.5 <C> [BOLD] 28.9 <CAP> Table 2: Results of the model-free RL with different reward definitions.
<R> <C> [EMPTY] <C> [BOLD] ACE2005 <C> [BOLD] GENIA <C> [BOLD] KBP2017 <R> <C> Anchor Detector <C> 82.9 <C> 82.7 <C> 83.0 <R> <C> Entire ARNs <C> 74.9 <C> 74.8 <C> 74.6 <R> <C> Δ <C> 8.0 <C> 7.9 <C> 8.4 <CAP> Table 3: F1-scores gap between the anchor detector and the entire ARNs (anchor + region).
<R> <C> [BOLD] Icon <C> [BOLD] Accuracy <C> [BOLD] Cohen  [ITALIC] κ <C> [BOLD] Hellinger distance <C> [BOLD] N(R) <C> [BOLD] N(G) <C> [BOLD] N(Y) <R> <C> Exp. Use <C> 92% <C> 0.76 <C> 0.12 <C> 41 <C> 8 <C> 1 <R> <C> Exp. Collection <C> 88% <C> 0.69 <C> 0.19 <C> 35 <C> 12 <C> 3 <R> <C> Precise Location <C> 84% <C> 0.68 <C> 0.21 <C> 32 <C> 14 <C> 4 <R> <C> Data Retention <C> 80% <C> 0.63 <C> 0.13 <C> 29 <C> 16 <C> 5 <R> <C> Children Privacy <C> 98% <C> 0.95 <C> 0.02 <C> 12 <C> 38 <C> [EMPTY] <CAP> Table 3: Prediction accuracy and κ for icon prediction, with the distribution of icons per color based on OPP-115 labels.
<R> <C> Method <C> # Parameters <C> Error <R> <C> Base Model <C> 1.25×107 <C> 6.2×10−02 <R> <C> KL Loss <C> — <C> 4.7×10−02 <R> <C> Pyramid <C> 1.27×107 <C> 6.4×10−02 <R> <C> Attention <C> 1.26×107 <C> 6.6×10−02 <R> <C> Bidirectional <C> 1.35×107 <C> 6.7×10−02 <R> <C> CNN <C> 1.66×107 <C> 5.8×10−02 <R> <C> DMO-3 <C> 6.14×106 <C> 2.3×10−01 <CAP> Table 1: Comparison of state-of-the-art methods. Each row denotes a base model with only the method applied (i.e., one model component). KL loss was applied to the base model. DMO-3 denotes decoder multi-output predicting 3 timesteps at a time. Reconstruction error is used as error.
<R> <C> [BOLD] System <C> [BOLD] batch1 <C> [BOLD] WMT15  [BOLD] fwd <C> [BOLD] WMT15  [BOLD] rev <C> [BOLD] WMT16  [BOLD] fwd <C> [BOLD] WMT16  [BOLD] rev <C> [BOLD] WMT17  [BOLD] fwd <C> [BOLD] WMT17  [BOLD] rev <C> [BOLD] WMT18  [BOLD] fwd <C> [BOLD] WMT18  [BOLD] rev <R> <C> 10% back-translated, 90% parallel <C> 1.2K <C> 20.4 <C> 34.9 <C> 27.7 <C> 44.4 <C> 25.1 <C> 37.8 <C> 28.5 <C> 46.7 <R> <C> 25% back-translated, 75% parallel <C> 1.2K <C> 20.0 <C> 37.7 <C> 27.5 <C> 47.5 <C> 24.9 <C> 39.8 <C> 27.5 <C> 49.4 <R> <C> 50% back-translated, 50% parallel <C> 1.2K <C> 20.2 <C> 38.3 <C> 28.2 <C> 48.8 <C> 25.9 <C> 40.8 <C> 28.3 <C> 51.3 <R> <C> 75% back-translated, 25% parallel <C> 1.2K <C> 20.9 <C> 39.0 <C> 29.4 <C> 49.7 <C> 26.6 <C> 41.7 <C> 29.6 <C> 52.4 <R> <C> 90% back-translated, 10% parallel <C> 1.2K <C> 21.2 <C> 38.6 <C> 29.0 <C> 49.6 <C> 26.8 <C> 41.5 <C> 29.7 <C> 52.8 <R> <C> 75% back-translated, 25% parallel <C> 1.2K <C> 20.9 <C> 39.0 <C> 29.4 <C> 49.7 <C> 26.6 <C> 41.7 <C> 29.6 <C> 52.4 <R> <C> 75% back-translated, 25% parallel <C> 9K <C> 23.2 <C> 41.2 <C> 31.8 <C> 51.8 <C> 28.7 <C> 44.2 <C> 32.6 <C> 56.3 <R> <C> 75% back-translated, 25% parallel <C> 13K <C> 23.2 <C> 40.9 <C> 31.8 <C> 51.3 <C> 28.6 <C> 44.1 <C> 32.4 <C> 56.2 <R> <C> 75% back-translated, 25% parallel <C> 22K <C> 23.2 <C> 41.2 <C> 31.8 <C> 51.3 <C> 28.7 <C> 44.2 <C> 32.4 <C> 56.2 <R> <C> 75/25, with tuning for WMT18 <C> 22K <C> 23.6 <C> 41.3 <C> 32.5 <C> 51.6 <C> 28.9 <C> 44.0 <C> 33.2 <C> 56.7 <R> <C> Microsoft Marian 2018 (en→de) <C> Microsoft Marian 2018 (en→de) <C> Microsoft Marian 2018 (en→de) <C> Microsoft Marian 2018 (en→de) <C> Microsoft Marian 2018 (en→de) <C> Microsoft Marian 2018 (en→de) <C> Microsoft Marian 2018 (en→de) <C> Microsoft Marian 2018 (en→de) <C> 52.5 <C> 41.6 <R> <C> Edunov et al. ( 2018 ) (en→de) <C> Edunov et al. ( 2018 ) (en→de) <C> Edunov et al. ( 2018 ) (en→de) <C> Edunov et al. ( 2018 ) (en→de) <C> Edunov et al. ( 2018 ) (en→de) <C> Edunov et al. ( 2018 ) (en→de) <C> Edunov et al. ( 2018 ) (en→de) <C> Edunov et al. ( 2018 ) (en→de) <C> 45.8 <C> 46.1 <R> <C> 1 batch size in sentence pairs <C> 1 batch size in sentence pairs <C> 1 batch size in sentence pairs <C> 1 batch size in sentence pairs <C> 1 batch size in sentence pairs <C> 1 batch size in sentence pairs <C> 1 batch size in sentence pairs <C> 1 batch size in sentence pairs <C> 1 batch size in sentence pairs <C> 1 batch size in sentence pairs <CAP> Table 9: Contrastive evaluation (BLEU scores) of performance on genuine German → English (fwd) translation vs. English source restoration from text originally translated from English into German (rev).
<R> <C> Lang. <C> System <C> Dev <C> 2017 <C> 2018 <R> <C> EN-CS <C> Transformer-base <C> 26.7 <C> 22.9 <C> 22.9 <R> <C> EN-CS <C> + Data filtering <C> 27.1 <C> 23.4 <C> 22.6 <R> <C> CS-EN <C> Transformer-base <C> 32.6 <C> 28.8 <C> 30.3 <R> <C> CS-EN <C> + Back-translation <C> 37.3 <C> 31.9 <C> 32.4 <R> <C> EN-CS <C> Base + Back-transl. <C> 28.4 <C> 25.1 <C> 25.1 <R> <C> EN-CS <C> → Transformer-big <C> 29.6 <C> 26.3 <C> 26.2 <R> <C> EN-CS <C> + Ensemble x2 <C> 29.6 <C> 26.5 <C> 26.3 <CAP> Table 11: BLEU score results for EN-CS experiments.
<R> <C> [EMPTY] <C> precision <C> recall <C> f-measure <C> support <R> <C> apiAnswer <C> 0.93 <C> 0.76 <C> 0.83 <C> 24.6 <R> <C> apiQuestion <C> 0.81 <C> 0.66 <C> 0.71 <C> 17.2 <R> <C> clarifAnswer <C> 0.13 <C> 0.07 <C> 0.09 <C> 6.0 <R> <C> clarifQuestion <C> 0.59 <C> 0.41 <C> 0.48 <C> 32.6 <R> <C> confirmation <C> 0.88 <C> 0.8 <C> 0.83 <C> 27.0 <R> <C> docAnswer <C> 0.25 <C> 0.2 <C> 0.22 <C> 3.2 <R> <C> implQuestion <C> 0.52 <C> 0.21 <C> 0.28 <C> 10.6 <R> <C> implStatement <C> 0.0 <C> 0.0 <C> 0.0 <C> 3.0 <R> <C> introduction <C> 0.76 <C> 0.6 <C> 0.63 <C> 4.0 <R> <C> stmnt <C> 0.69 <C> 0.4 <C> 0.51 <C> 49.8 <R> <C> systemQuestion <C> 0.37 <C> 0.22 <C> 0.27 <C> 4.8 <R> <C> avg / total <C> 0.69 <C> 0.5 <C> 0.57 <C> 16.62 <CAP> Table 1. Performance metrics calculated for each speech act type (some speech act types have been abbreviated). Recall that the averages are a weighted average based on the support for each speech type, see Section 8.3.
<R> <C> [BOLD] System <C> 2014 test <C> 2015 progress <C> 2015 test <R> <C> [BOLD] NRC-closed-2014 <C> 95.70 <C> 90.15 <C> - <R> <C> [BOLD] MAC-closed-2015 <C> 96.31 <C> 94.33 <C> 95.54 <R> <C> [BOLD] NRC-open-2015 <C> 96.04 <C> 94.48 <C> 95.65 <CAP> Table 4: Progress test for 2014 and 2015 systems. “progress” is the 2015 test set without group G and X.
<R> <C> [BOLD] Features used <C> [BOLD] Error rate  [BOLD] (frame level) <C> [BOLD] Error rate  [BOLD] (patient level) <R> <C> dFM <C> 17.2 <C> 8.73 <R> <C> dCGD <C> 9.40 <C> 4.93 <R> <C> [ITALIC] T1, [ITALIC] T2 <C> 13.28 <C> 5.35 <R> <C> [ITALIC] Bal1, [ITALIC] T2 <C> 8.65 <C> 5.07 <R> <C> [ITALIC] Bal1, [ITALIC] Bal2, [ITALIC] Bal3 <C> 9.97 <C> 7.89 <R> <C> ModGD,dPPGD,dCGD <C> 7.92 <C> 4.08 <R> <C> dFM,dSTRAIGHT, <C> 8.25 <C> 4.65 <R> <C> dModGD,dPPGD,dCGD <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] T1, [ITALIC] T2, <C> 7.97 <C> 4.08 <R> <C> dModGD,dPPGD,dCGD <C> [EMPTY] <C> [EMPTY] <R> <C> All 10 features <C> 6.16 <C> 4.08 <CAP> Table 2: Results of voice pathology detection using an ANN classifier for various feature sets.
<R> <C> Source <C> Target <C> Pivot BLEU <C> Direct BLEU <C> Δ <C> Source→Pivot Data <C> Pivot→Target Data <C> Direct Data <R> <C> Bs <C> Mk <C> 19.43 <C> 21.51 <C> -2.08 <C> 5661 <C> 25335 <C> 1692 <R> <C> Da <C> Sv <C> 26.79 <C> 28.85 <C> -2.06 <C> 44925 <C> 56646 <C> 17098 <R> <C> Sk <C> Mk <C> 16.71 <C> 18.40 <C> -1.69 <C> 61454 <C> 25335 <C> 11096 <R> <C> Nb <C> Sv <C> 23.68 <C> 25.11 <C> -1.43 <C> 15819 <C> 56646 <C> 7109 <CAP> Table 6: Some examples that direct translation is better than pivot translation through English while the direct training data is less than the pivot training data. Δ means the BLEU gap (the BLUE score of pivot translation minus that of direct translation). The last three columns list the number of training sentence pairs for source→pivot translation, pivot→target translation, and direct translation.
<R> <C> [BOLD] Model Changes <C> [BOLD] Dev WER (%) <R> <C> No changes (full global attention model) <C> 21.81 <R> <C> No sampling <C> 22.05 <R> <C> No dropout <C> 22.17 <R> <C> No input feeding <C> 22.06 <R> <C> No attention <C> 22.98 <R> <C> No attention (rev. unidirectional encoder) <C> 22.65 <R> <C> # of LSTM units - 256 <C> 24.00 <R> <C> # of LSTM units - 50 <C> 32.70 <R> <C> 2-layer LSTM <C> 22.36 <R> <C> 1-layer LSTM <C> 23.67 <R> <C> Rev. unidirectional encoder <C> 22.12 <R> <C> Rev. unidirectional encoder + GRU <C> 23.78 <CAP> Table 2: Ablation study on CMUDict development set.
<R> <C> ASR <C> SF-Type <C> SF-Relevance <C> WER <R> <C> IL5 Universal <C> 0.22 <C> 0.44 <C> 75.9 <R> <C> IL5 Related <C> 0.26 <C> 0.46 <C> 68.5 <R> <C> IL5 Adapt Related <C> 0.34 <C> 0.54 <C> 53.7 <R> <C> [BOLD] IL5 Adapt Universal <C> [BOLD] 0.35 <C> [BOLD] 0.54 <C> [BOLD] 51.6 <R> <C> IL6 Universal <C> 0.34 <C> 0.73 <C> 63.0 <R> <C> IL6 Related <C> 0.35 <C> 0.74 <C> 47.9 <R> <C> IL6 Adapt Related <C> 0.37 <C> 0.77 <C> 44.4 <R> <C> [BOLD] IL6 Adapt Universal <C> [BOLD] 0.37 <C> [BOLD] 0.77 <C> [BOLD] 39.8 <CAP> Table 2: ASR Impact on SF-type Detection
<R> <C> [BOLD] Network <C> [BOLD] Encoder <C> [BOLD] Decoder <R> <C> linear <C> 0.5844 <C> 0.6225 <R> <C> one-layer MLP <C> 0.6187 <C> 0.3559 <R> <C> two-layer MLP <C> 0.6225 <C> 0.1047 <CAP> Table 6: Ablation study with different encoder/decoder networks.
<R> <C> # <C> Scenario <C> Supervision <C> Method <C> De-En test2012 <C> De-En test2013 <C> En-De test2012 <C> En-De test2013 <C> Fr-En test2010 <C> Fr-En test2011 <C> En-Fr test2010 <C> En-Fr test2011 <R> <C> 1 <C> [ITALIC] II <C> Yes <C> 8360031 <C> [EMPTY] <C> [EMPTY] <C> 23.07 <C> 25.40 <C> [EMPTY] <C> [EMPTY] <C> 32.11 <C> 35.22 <R> <C> 2 <C> [ITALIC] II <C> Yes <C> Base <C> 33.68 <C> 35.41 <C> 28.09 <C> 30.48 <C> 36.13 <C> 40.07 <C> 36.43 <C> 37.58 <R> <C> 3 <C> [ITALIC] II <C> No <C> Base <C> 24.42 <C> 25.65 <C> 21.99 <C> 22.72 <C> 25.94 <C> 29.73 <C> 25.32 <C> 27.06 <R> <C> 4 <C> [ITALIC] OO <C> No <C> Base <C> 21.21 <C> 21.66 <C> 10.25 <C> 9.90 <C> 24.28 <C> 28.77 <C> 23.08 <C> 26.08 <R> <C> 5 <C> [ITALIC] IIOO <C> No <C> Base <C> 24.87 <C> 26.00 <C> 21.64 <C> 22.57 <C> 26.05 <C> 30.18 <C> 26.35 <C> 30.12 <R> <C> 6 <C> [ITALIC] IIOO <C> No <C> FT <C> 29.82 <C> 31.57 <C> 26.48 <C> 28.18 <C> 31.23 <C> 35.94 <C> 29.08 <C> 33.67 <R> <C> 7 <C> [ITALIC] IOO <C> No <C> Base <C> 20.94 <C> 21.52 <C> 16.53 <C> 16.80 <C> 25.16 <C> 29.88 <C> 25.18 <C> 28.73 <R> <C> 8 <C> [ITALIC] IOO <C> No <C> FT(original) <C> 22.75 <C> 23.14 <C> 21.09 <C> 21.78 <C> 28.37 <C> 33.57 <C> 26.16 <C> 30.14 <R> <C> 9 <C> [ITALIC] IOO <C> No <C> FT(modified) <C> 24.33 <C> 24.77 <C> 24.43 <C> 25.59 <C> 29.13 <C> 34.38 <C> 26.45 <C> 30.69 <R> <C> 10 <C> [ITALIC] IIO <C> No <C> Base <C> 11.11 <C> 10.30 <C> 11.54 <C> 11.95 <C> 17.88 <C> 20.32 <C> 17.02 <C> 18.16 <R> <C> 11 <C> [ITALIC] IIO <C> No <C> FT+BW(original) <C> 19.91 <C> 20.19 <C> 17.05 <C> 17.23 <C> 26.84 <C> 29.61 <C> 23.18 <C> 25.18 <R> <C> 12 <C> [ITALIC] IIO <C> No <C> FT+BW(modified) <C> 26.12 <C> 27.33 <C> 22.63 <C> 23.72 <C> 27.88 <C> 32.16 <C> 25.42 <C> 28.05 <R> <C> 13 <C> [ITALIC] IO <C> No <C> Base <C> 10.79 <C> 10.77 <C> 11.44 <C> 11.82 <C> 18.00 <C> 20.91 <C> 16.19 <C> 16.84 <R> <C> 14 <C> [ITALIC] IO <C> No <C> BW(original) <C> 8.15 <C> 7.05 <C> 9.28 <C> 9.70 <C> 18.00 <C> 19.52 <C> 16.39 <C> 17.72 <R> <C> 15 <C> [ITALIC] IO <C> No <C> FT+BW(modified) <C> 19.76 <C> 20.22 <C> 18.32 <C> 18.99 <C> 22.59 <C> 26.55 <C> 20.61 <C> 22.79 <CAP> Table 3: The BLEU scores in the different scenarios for En-De and En-Fr language pairs. Base denotes the baseline in the different scenarios; FT denotes fine tuning method; BW denotes batch weighting method. Original denotes the original method for SNMT; modified denotes our modified method for UNMT. #1 and #2 are the results of supervised NMT; others are the results of UNMT. Nin=10, Nout=1 in original batch weighting method, Nin=1, Nout=30 in modified batch weighting method, and selected pseudo in-domain corpus size is set to 20K for fine tuning method in scenario IO and IOO. Note that L2 in-domain data and all L1 out-of-domain data were used in original fine tuning method for scenario IOO.
<R> <C> [BOLD] Vector <C> [BOLD] WS353 <C> [BOLD] SCWS <C> [BOLD] Rank( [ITALIC] L) <R> <C> [ITALIC] V <C> 68.6 <C> 59.8 <C> [EMPTY] <R> <C> ~ [ITALIC] VWordNet <C> 69.1 <C> 62.3 <C> [EMPTY] <R> <C> ~ [ITALIC] VPCA <C> [BOLD] 69.2 <C> [BOLD] 65.3 <C> 5 <R> <C> ~ [ITALIC] VEx−RPCA <C> [BOLD] 69.2 <C> [BOLD] 65.4 <C> 3 <CAP> Table 4: Spearman rank correlation (ρ×100) on Word Sim 353 dataset and SCWS dataset. V refers to the 300-dimensional non-parametric multi-sense skip-gram model Neelakantan et al. (2014), and the results of ~VWordNet are extracted from shi2016real. Rank(L) is the dimensionality of L when reaching the best performance for each type of vectors.
<R> <C> [EMPTY] <C> [BOLD] WMT17  [BOLD] ger-eng <C> [BOLD] WMT17  [BOLD] eng-ger <C> [BOLD] WMT17  [BOLD] chi-eng <C> [BOLD] WAT  [BOLD] eng-jpn <C> [BOLD] WAT  [BOLD] jpn-eng <R> <C> [BOLD] PBMT <C> 28.9 <C> 19.6 <C> 15.8 <C> 33.4 <C> 18.0 <R> <C> [BOLD] FNMT <C> 32.8 <C> 26.1 <C> 20.8 <C> 39.1 <C> 25.3 <R> <C> [BOLD] LNMT <C> 33.7 <C> 26.6 <C> 22.0 <C> 40.4 <C> 26.1 <R> <C> [BOLD] TNMT <C> 35.2 <C> 28.9 <C> 24.8 <C> 44.6 <C> 29.4 <R> <C> [BOLD] LTNMT <C> 35.4 <C> 29.2 <C> 25.4 <C> 44.9 <C> 30.2 <R> <C> [BOLD] Best submissions <C> 35.1 <C> 28.3 <C> 26.4 <C> 43.3 <C> 28.4 <CAP> Table 1: Quality assessment of our NMT systems with and without LMBR posteriors for GRU-based (FNMT, LNMT) and Transformer models (TNMT, LTNMT). Cased BLEU scores reported on 5 translation tasks.The exact PBMT systems used to compute n-gram posteriors for LNMT and LTNMT systems are also reported. The last row shows scores for the best official submissions to each task.
<R> <C> Experiment <C> [ITALIC] S P <C> [ITALIC] S R <C> [ITALIC] S F1 <C> [ITALIC] NS P <C> [ITALIC] NS R <C> [ITALIC] NS F1 <R> <C> disc [ITALIC] ctbl <C> 64.20 <C> 64.95 <C> 64.57 <C> 69.0 <C> 68.30 <C> 68.70 <R> <C> disc [ITALIC] ct+ [ITALIC] ptbl <C> 65.64 <C> 65.86 <C> 65.75 <C> 70.11 <C> 69.91 <C> 70.00 <R> <C> tf-idf [ITALIC] ctbl <C> 63.16 <C> 67.94 <C> 65.46 <C> 70.04 <C> 65.41 <C> 67.64 <R> <C> tf-idf [ITALIC] ct+ [ITALIC] ptbl <C> 65.54 <C> 72.86 <C> 69.01 <C> 73.75 <C> 66.57 <C> 69.98 <R> <C> LSTM [ITALIC] ct <C> 73.25 <C> 58.72 <C> 65.19 <C> 61.47 <C> 75.44 <C> 67.74 <R> <C> LSTM [ITALIC] ct+ [ITALIC] pt <C> 70.54 <C> 71.19 <C> 70.80 <C> 64.65 <C> 74.06 <C> [BOLD] 74.35 <R> <C> LSTM [ITALIC] ct+LSTM [ITALIC] pt <C> 70.89 <C> 67.95 <C> 69.39 <C> 64.94 <C> 68.03 <C> 66.45 <R> <C> LSTM [ITALIC] conditional <C> 76.08 <C> [BOLD] 76.53 <C> [BOLD] 76.30 <C> [BOLD] 72.93 <C> 72.44 <C> 72.68 <R> <C> LSTM [ITALIC] ctas <C> 76.00 <C> 73.18 <C> 74.56 <C> 70.52 <C> 73.52 <C> 71.90 <R> <C> LSTM [ITALIC] ctas+ [ITALIC] ptas <C> 70.44 <C> 67.28 <C> 68.82 <C> 72.52 <C> 75.36 <C> [BOLD] 73.91 <R> <C> LSTM [ITALIC] ctas+LSTM [ITALIC] ptas <C> [BOLD] 77.25 <C> 75.51 <C> [BOLD] 76.36 <C> 72.65 <C> [BOLD] 74.52 <C> [BOLD] 73.57 <R> <C> LSTM [ITALIC] ctas+LSTM [ITALIC] last_ [ITALIC] ptas <C> 73.10 <C> 69.69 <C> 71.36 <C> 74.58 <C> [BOLD] 77.62 <C> [BOLD] 76.07 <R> <C> LSTM [ITALIC] ctaw+LSTM [ITALIC] ptaw <C> 76.74 <C> 69.77 <C> 73.09 <C> 68.63 <C> 75.77 <C> 72.02 <R> <C> LSTM [ITALIC] ctaw+ [ITALIC] s+LSTM [ITALIC] ptaw+ [ITALIC] s <C> 76.42 <C> 71.37 <C> 73.81 <C> 69.50 <C> 74.77 <C> 72.04 <CAP> Table 7: Experimental results for Twitter dataset (bold are best scores)
<R> <C> Experiment <C> [ITALIC] S P <C> [ITALIC] S R <C> [ITALIC] S F1 <C> [ITALIC] NS P <C> [ITALIC] NS R <C> [ITALIC] NS F1 <R> <C> LSTM [ITALIC] ctas <C> 67.08 <C> 27.50 <C> 39.00 <C> 84.32 <C> 96.66 <C> 90.07 <R> <C> LSTM [ITALIC] ctas+LSTM [ITALIC] ptas <C> 62.25 <C> 35.05 <C> 44.85 <C> 85.48 <C> 94.73 <C> 89.87 <CAP> Table 10: Experimental results for Reddit dataset under unbalanced setting
<R> <C> ( [ITALIC] α1, [ITALIC] α2) <C> (0.8,1.2) <C> (1.1,1.2) <C> (0.8,0.9) <C> (0.7,1.3) <R> <C> [ITALIC] Cavg <C> [BOLD] 0.069 <C> 0.075 <C> 0.082 <C> 0.075 <R> <C> EER(%) <C> [BOLD] 6.76 <C> 7.02 <C> 7.17 <C> 7.01 <CAP> TABLE III: Comparison of speed rate changing combination on test-1 data set.
<R> <C> [EMPTY] <C> Pivot <C> Many-to-1 <C> Dev <C> Test <R> <C> (a) <C> [EMPTY] <C> [EMPTY] <C> <1 <C> <1 <R> <C> (b) <C> √ <C> [EMPTY] <C> 20.64 <C> 20.4 <R> <C> (c) <C> √ <C> Early <C> 9.24 <C> 10.42 <R> <C> (d) <C> √ <C> Late <C> 18.22 <C> 19.14 <R> <C> (e) <C> √ <C> E+L <C> 13.29 <C> 14.56 <CAP> Table 4: Zero-resource translation from Spanish (Es) to French (Fr) without finetuning. When pivot is √, English is used as a pivot language.
<R> <C> [EMPTY] <C> Pivot <C> Many-to-1 <C> [EMPTY] <C> Pseudo Parallel Corpus 1k <C> Pseudo Parallel Corpus 10k <C> Pseudo Parallel Corpus 100k <C> Pseudo Parallel Corpus 1m <C> True Parallel Corpus 1k <C> True Parallel Corpus 10k <C> True Parallel Corpus 100k <C> True Parallel Corpus 1m <R> <C> (a) <C> Single-Pair Models <C> Single-Pair Models <C> Dev <C> – <C> – <C> – <C> – <C> – <C> – <C> 11.25 <C> 21.32 <R> <C> (a) <C> [EMPTY] <C> [EMPTY] <C> Test <C> – <C> – <C> – <C> – <C> – <C> – <C> 10.43 <C> 20.35 <R> <C> (b) <C> √ <C> No Finetuning <C> No Finetuning <C> Dev: 20.64, Test: 20.4 <C> Dev: 20.64, Test: 20.4 <C> Dev: 20.64, Test: 20.4 <C> Dev: 20.64, Test: 20.4 <C> – <C> – <C> – <C> – <R> <C> (c) <C> [EMPTY] <C> [EMPTY] <C> Dev <C> 0.28 <C> 10.16 <C> 15.61 <C> 17.59 <C> 0.1 <C> 8.45 <C> 16.2 <C> 20.59 <R> <C> (c) <C> [EMPTY] <C> [EMPTY] <C> Test <C> 0.47 <C> 10.14 <C> 15.41 <C> 17.61 <C> 0.12 <C> 8.18 <C> 15.8 <C> 19.97 <R> <C> (d) <C> √ <C> Early <C> Dev <C> 19.42 <C> 21.08 <C> 21.7 <C> 21.81 <C> 8.89 <C> 16.89 <C> 20.77 <C> 22.08 <R> <C> (d) <C> √ <C> Early <C> Test <C> 19.43 <C> 20.72 <C> 21.23 <C> 21.46 <C> 9.77 <C> 16.61 <C> 20.40 <C> 21.7 <R> <C> (e) <C> √ <C> Early+ <C> Dev <C> 20.89 <C> 20.93 <C> 21.35 <C> 21.33 <C> 14.86 <C> 18.28 <C> 20.31 <C> 21.33 <R> <C> (e) <C> √ <C> Late <C> Test <C> 20.5 <C> 20.71 <C> 21.06 <C> 21.19 <C> 15.42 <C> 17.95 <C> 20.16 <C> 20.9 <CAP> Table 5: Zero-resource translation from Spanish (Es) to French (Fr) with finetuning. When pivot is √, English is used as a pivot language. Row (a) is from Table 4 (b).
<R> <C> [EMPTY] <C> mpi-d5_igraph <C> mpi-d5_intu <C> mpi-d5_union <C> mpi-d5_cqw <C> indri_baseline <R> <C> nDCG@1000 <C> 0.322 <C> [BOLD] 0.341 <C> 0.195 <C> 0.317 <C> 0.293 <R> <C> ERR@1000 <C> 0.147 <C> 0.151 <C> 0.038 <C> 0.145 <C> [BOLD] 0.157 <CAP> Table 2: Results on training data.
<R> <C> [EMPTY] <C> Named entity valid <C> Named entity test <C> Common noun valid <C> Common noun test <C> [EMPTY] <R> <C> Humans (query)  <C> [EMPTY] <C> 52.0 <C> [EMPTY] <C> 64.4 <C> [EMPTY] <R> <C> Humans (context+query)  <C> [EMPTY] <C> [BOLD] 81.6 <C> [EMPTY] <C> [BOLD] 81.6 <C> [EMPTY] <R> <C> LSTMs (context+query)  <C> 51.2 <C> 41.8 <C> 62.6 <C> 56.0 <C> }101mm <R> <C> Memory Networks  <C> 70.4 <C> 66.6 <C> 64.2 <C> 63.0 <C> [EMPTY] <R> <C> psr (single model) <C> 73.8 <C> 68.6 <C> 68.8 <C> 63.4 <C> [EMPTY] <R> <C> [BOLD] psr (avg ensemble) <C> 74.5 <C> 70.6 <C> 71.1 <C> [BOLD] 68.9 <C> [EMPTY] <R> <C> [BOLD] psr (greedy ensemble) <C> 76.2 <C> [BOLD] 71.0 <C> 72.4 <C> 67.5 <C> [EMPTY] <R> <C> GA Reader (ensemble)  <C> 75.5 <C> 71.9 <C> 72.1 <C> 69.4 <C> [EMPTY] <R> <C> EpiReader (ensemble)  <C> 76.6 <C> 71.8 <C> 73.6 <C> 70.6 <C> [EMPTY] <R> <C> [BOLD] IA Reader (ensemble)  <C> 76.9 <C> [BOLD] 72.0 <C> 74.1 <C> [BOLD] 71.0 <C> [EMPTY] <R> <C> [BOLD] AoA Reader (single model)  <C> 77.8 <C> [BOLD] 72.0 <C> 72.2 <C> 69.4 <C> [EMPTY] <R> <C> [BOLD] psr (single model) <C> 80.5 <C> 76.2 <C> 83.2 <C> 80.8 <C> }21mm[ BookTest training data] <R> <C> [BOLD] psr (greedy ensemble) <C> 82.3 <C> [BOLD] 78.4 <C> 85.7 <C> [BOLD] 83.7 <C> [EMPTY] <CAP> Table 2: Results of various architectures on the CBT test datasets.
<R> <C> [BOLD] Grade [t] <C> [BOLD] Challenge % <C> [BOLD] Challenge (# qns) <C> [BOLD] Easy % <C> [BOLD] Easy (# qns) <R> <C> 3 [t] <C> 3.6 <C> (94 qns) <C> 3.4 <C> (176 qns) <R> <C> 4 <C> 9 <C> (233) <C> 11.4 <C> (591) <R> <C> 5 <C> 19.5 <C> (506) <C> 21.2 <C> (1101) <R> <C> 6 <C> 3.2 <C> (84) <C> 3.4 <C> (179) <R> <C> 7 <C> 14.4 <C> (372) <C> 10.7 <C> (557) <R> <C> 8 <C> 41.4 <C> (1072) <C> 41.2 <C> (2139) <R> <C> 9 <C> 8.8 <C> (229) <C> 8.7 <C> (454) <CAP> Table 2: Grade-level distribution of ARC questions
<R> <C> Model <C> wiki <C> bio <R> <C> SVM  <C> 62.01 <C> 78.64 <R> <C> HMM  <C> 63.97 <C> 80.15 <R> <C> CRF + ling  <C> 55.05 <C> [BOLD] 86.79 <R> <C> Our CNN with external attention <C> [BOLD] 67.52 <C> 85.57 <CAP> Table 5: Comparison of our best model with the state of the art
<R> <C> ZHAA <C> Baseline <C> System 1 <C> System 2 <C> System 3 <R> <C> 1.81± 0.20 <C> 2.13±0.14 <C> 2.66±0.11 <C> 3.57±0.12 <C> [BOLD] 3.58±0.10 <CAP> Table 5: Accent similarity test results
<R> <C> [EMPTY] <C> NELL-995 H@1 <C> NELL-995 H@10 <R> <C> ReifKB (Ours) <C> 64.1 <C> 82.4 <R> <C> DistMult* <C> 61.0 <C> 79.5 <R> <C> ComplEx* <C> 61.2 <C> 82.7 <R> <C> ConvE* <C> [BOLD] 67.2 <C> [BOLD] 86.4 <CAP> Table 4: Left: Hits@1 and Hits@10 for KB completion on NELL 995. Starred KB completion methods are transductive, and do not generalize to entities not seen in training. Right: Comparison to MINERVA on several tasks for Hits@1.
<R> <C> [EMPTY] <C> ReifKB (Ours) <C> MINERVA <R> <C> NELL-995 <C> 64.1 <C> [BOLD] 66.3 <R> <C> Grid with seed entity <C> [EMPTY] <C> [EMPTY] <R> <C> 10-hop NSEW <C> 98.9 <C> [BOLD] 99.3 <R> <C> 10-hop NSEW-VH <C> [BOLD] 73.6 <C> 34.4 <R> <C> MetaQA 3-hop <C> [BOLD] 72.3 <C> 41.7 <CAP> Table 4: Left: Hits@1 and Hits@10 for KB completion on NELL 995. Starred KB completion methods are transductive, and do not generalize to entities not seen in training. Right: Comparison to MINERVA on several tasks for Hits@1.
<R> <C> [BOLD] Attribute <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> Baseline: memorize / closest <C> - <C> - <C> 0.675 <R> <C> BluLab (2015) <C> - <C> - <C> 0.791 <R> <C> [ITALIC] Median System (2016) <C> - <C> - <C> 0.724 <R> <C> [ITALIC]  [BOLD] Best (2016) <C> - <C> - <C> [ITALIC]  [BOLD] 0.843 <R> <C> Logistic Regression (LR) <C> 0.738 <C> 0.737 <C> 0.737 <R> <C> LR+Skip-chain <C> 0.743 <C> 0.742 <C> [BOLD] 0.743 <CAP> Table 4: Phase 2: EVENT Document Creation Time Relation extraction measures (baseline precision/recall scores not provided).
<R> <C> Parameter Embedding size <C> value 300 <C> Parameter Epochs <C> value 100/64 <R> <C> LSTMs hidden size <C> 128 <C> Batch sizes <C> 16/64 <R> <C> LSTM layer <C> 1 <C> Optimizer <C> Adam <R> <C> Slot embedding size <C> 128 <C> Learning rate <C> 0.001 <CAP> Table 1: Training hyper-parameters used in all experiments.
<R> <C> Task <C> Metric <C> HeidelTime <C> FastText unaligned <C> FastText aligned w/o Dict. <C> FastText aligned w/ Dict <C> FastText aligned w/ AT <C> BERT unaligned <C> BERT aligned w/ AT <R> <C> EN <C> strict <C> [BOLD] 81.78 <C> 68.36 <C> 69.10 <C> 70.80 <C> 75.63 † <C> 73.09 <C> 74.80 † <R> <C> EN <C> relaxed <C> [BOLD] 90.71 <C> 79.14 <C> 79.03 <C> 81.21 <C> 82.03 † <C> 84.34 <C> 86.61 † <R> <C> EN <C> type <C> [BOLD] 83.27 <C> 72.13 <C> 72.18 <C> 73.32 <C> 72.85 † <C> 75.50 <C> 79.53 † <R> <C> ES <C> strict <C> [BOLD] 85.87 <C> 75.67 <C> 76.53 <C> 77.44 <C> 79.64 † <C> 79.11 <C> 79.55 <R> <C> ES <C> relaxed <C> [BOLD] 90.13 <C> 82.43 <C> 82.45 <C> 82.47 <C> 84.46 † <C> 84.12 <C> 85.71 <R> <C> ES <C> type <C> [BOLD] 87.47 <C> 78.07 <C> 78.46 <C> 78.24 <C> 80.88 † <C> 80.22 <C> 80.11 <R> <C> PT <C> strict <C> 71.59 <C> 70.36 <C> 70.20 <C> 70.48 <C> 72.41 <C> 74.52 <C> [BOLD] 75.47 <R> <C> PT <C> relaxed <C> 81.18 <C> 76.77 <C> 75.86 <C> 76.29 <C> 78.15 <C> 80.75 <C> [BOLD] 81.51 <R> <C> PT <C> type <C> [BOLD] 76.75 <C> 72.29 <C> 71.50 <C> 72.26 <C> 73.84 <C> 75.47 <C> 76.23 <CAP> Table 2: Results for multilingual models trained on English, Spanish and Portuguese data jointly. †highlights aligned models with statistical significant differences to the unaligned model (paired permutation test, p=0.05).
<R> <C> Task <C> Metric <C> HeidelTime -Auto <C> BERT unalign. <C> BERT aligned w/ AT <R> <C> FR <C> strict <C> 52.35 <C> 60.12 <C> [BOLD] 62.58 <R> <C> FR <C> relaxed <C> 72.02 <C> 74.23 <C> [BOLD] 75.46 <R> <C> FR <C> type <C> [BOLD] 68.70 <C> 61.96 <C> 62.07 <R> <C> DE <C> strict <C> 38.87 <C> 63.34 <C> [BOLD] 66.53 <R> <C> DE <C> relaxed <C> 52.11 <C> 76.51 <C> [BOLD] 77.82 <R> <C> DE <C> type <C> 50.15 <C> 66.95 <C> [BOLD] 69.04 <R> <C> CA <C> strict <C> 28.11 <C> 63.24 <C> [BOLD] 64.21 <R> <C> CA <C> relaxed <C> 62.81 <C> 74.95 <C> [BOLD] 77.00 <R> <C> CA <C> type <C> 60.84 <C> 65.66 <C> [BOLD] 67.85 <R> <C> EU <C> strict <C> 22.54 <C> 43.96 <C> [BOLD] 47.87 <R> <C> EU <C> relaxed <C> 26.76 <C> 61.54 <C> [BOLD] 63.83 <R> <C> EU <C> type <C> 23.94 <C> 57.14 <C> [BOLD] 58.51 <CAP> Table 3: Results for the unsupervised cross-lingual setting. We compare to HeidelTime with automatically generated resources, which resembles a similar setting.
<R> <C> [EMPTY] <C> Syntax -Aux <C> Syntax +Aux <C> Phonology -Aux <C> Phonology +Aux <C> Inventory -Aux <C> Inventory +Aux <R> <C> None <C> 69.91 <C> 83.07 <C> 77.92 <C> 86.59 <C> 85.17 <C> 90.68 <R> <C> LMVec <C> 71.32 <C> 82.94 <C> 80.80 <C> 86.74 <C> 87.51 <C> 89.94 <R> <C> MTVec <C> 74.90 <C> 83.31 <C> 82.41 <C> 87.64 <C> 89.62 <C> 90.94 <R> <C> MTCell <C> 75.91 <C> 85.14 <C> 84.33 <C> 88.80 <C> 90.01 <C> 90.85 <R> <C> MTBoth <C> [BOLD] 77.11 <C> [BOLD] 86.33 <C> [BOLD] 85.77 <C> [BOLD] 89.04 <C> [BOLD] 90.06 <C> [BOLD] 91.03 <CAP> Table 1: Accuracy of syntactic, phonological, and inventory features using LM language vectors (LMVec), MT language vectors (MTVec), MT encoder cell averages (MTCell) or both MT feature vectors (MTBoth). Aux indicates auxiliary information of geodesic/genetic nearest neighbors; “None -Aux” is the majority class chance rate, while “None +Aux” is a 3-NN classification.
<R> <C> [BOLD] Model <C> [BOLD] R1 <C> [BOLD] R2 <C> [BOLD] RL <R> <C> BertSumExt <C> 42.02 <C> 24.59 <C> 41.99 <R> <C> TransformerExt <C> 28.75 <C> 14.80 <C> 28.72 <R> <C> BertSumAbs <C> 12.21 <C> 4.36 <C> 12.19 <R> <C> TransformerAbs <C> 6.93 <C> 1.78 <C> 6.88 <CAP> Table 2: ROUGE F1 results on KALIMAT test set
<R> <C> Model <C> Dev PER <C> Dev WER <C> Eval2000 SWB <C> Eval2000 CH <C> Eval2000 full <R> <C> Baseline ( [ITALIC] λ=1.0) <C> - <C> 25.5 <C> 21.5 <C> 33.8 <C> 27.7 <R> <C> Multitask <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] i=3 <C> 17.0 <C> [BOLD] 21.5 <C> 18.6 <C> 30.8 <C> 24.7 <R> <C> [ITALIC] i=4 <C> 15.0 <C> 22.4 <C> - <C> - <C> - <R> <C> [ITALIC] i=5 <C> 13.2 <C> 22.8 <C> - <C> - <C> - <R> <C> Pretrain <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] i=3 <C> 15.2 <C> 22.2 <C> - <C> - <C> - <R> <C> [ITALIC] i=4 <C> 13.5 <C> [BOLD] 22.0 <C> 18.6 <C> 30.7 <C> 24.7 <R> <C> [ITALIC] i=5 <C> 13.1 <C> 23.7 <C> - <C> - <C> - <R> <C> Pretrain + Multitask <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] i=3 <C> 14.9 <C> 21.4 <C> - <C> - <C> - <R> <C> [ITALIC] i=4 <C> 12.9 <C> [BOLD] 21.2 <C> 17.9 <C> 30.7 <C> 24.3 <R> <C> [ITALIC] i=5 <C> 12.8 <C> 23.3 <C> - <C> - <C> - <CAP> Table 4: A comparison of pretraining and multitask learning on our Switchboard development set and evaluation of the tuned models on Eval2000. SWB, CH = Switchboard, CallHome partitions of the Eval2000 corpus. All of the “Multitask” models are trained with λ=0.5. i denotes the position of the phone CTC loss throughout the experiment. PER, WER refer to development set phonetic error rate and word error rate. PER for the “Pretrain” models has been calculated right after pretraining (and before training the subword CTC model).
<R> <C> [BOLD] Entity type <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> company <C> 28.07 <C> 41.03 <C> 33.33 <R> <C> facility <C> 25.00 <C> 23.68 <C> 24.32 <R> <C> geo-loc <C> 53.91 <C> 53.45 <C> 53.68 <R> <C> movie <C> 20.00 <C> 6.67 <C> 10.00 <R> <C> musicartist <C> 14.29 <C> 2.44 <C> 4.17 <R> <C> other <C> 45.78 <C> 28.79 <C> 35.35 <R> <C> person <C> 54.63 <C> 65.50 <C> 59.57 <R> <C> product <C> 27.78 <C> 13.51 <C> 18.18 <R> <C> sportsteam <C> 42.86 <C> 25.71 <C> 32.14 <R> <C> tvshow <C> 0.00 <C> 0.00 <C> 0.00 <R> <C> Overall <C> 45.72 <C> 39.64 <C> 42.46 <R> <C> No types <C> 63.81 <C> 56.28 <C> 59.81 <CAP> Table 2: Results of the USFD W-NUT 2015 system.
<R> <C> K <C> 1 <C> 5 <C> 10 <C> 25 <C> 50 <C> 100 <C> 250 <C> 500 <C> 1000 <R> <C> Epochs <C> 3260 <C> 711 <C> 318 <C> 130 <C> 69 <C> 34 <C> 13 <C> 7 <C> 4 <R> <C> hit@10 <C> 67.05% <C> 68.08% <C> 68.13% <C> 67.63% <C> 69.05% <C> 66.99% <C> 63.95% <C> 60.32% <C> 54.14% <CAP> Table 5: Adapting the number of negative samples k for a 50-dim model for 1 hour of training on Freebase 15k.
<R> <C> task model <C> incident type HAN <C> incident type CNN <C> injury type HAN <C> injury type CNN <C> bodypart HAN <C> bodypart CNN <C> severity HAN <C> severity CNN <R> <C> time/ep. <C> 216.99 <C> 71.47 <C> 32.31 <C> 11.54 <C> 46.33 <C> 16.46 <C> 54.39 <C> 19.76 <R> <C> #params <C> 1,631,966 <C> 1,709,666 <C> 1,551,844 <C> 1,589,664 <C> 1,551,926 <C> 1,590,266 <C> 1,551,762 <C> 1,589,062 <CAP> Table 9: Average runtime per epoch (in secs) on a high-end GPU and number of parameters for the deep learning models.
<R> <C> [EMPTY] <C> BLEU1 Automative <C> BLEU4 Automative <C> METEOR Automative <C> ROUGE-L Automative <C> BLEU1 Baby <C> BLEU4 Baby <C> METEOR Baby <C> ROUGE-L Baby <R> <C> G [ITALIC] a <C> 0.103 <C> 0.047 <C> 0.062 <C> 0.089 <C> 0.104 <C> 0.055 <C> 0.065 <C> 0.068 <R> <C> G [ITALIC] PNa <C> 0.162 <C> 0.090 <C> 0.091 <C> 0.140 <C> 0.153 <C> 0.088 <C> 0.087 <C> 0.195 <R> <C> G [ITALIC] PNar <C> 0.147 <C> 0.082 <C> 0.078 <C> 0.118 <C> 0.133 <C> 0.060 <C> 0.068 <C> 0.102 <R> <C> G [ITALIC] PNa+aspect <C> 0.165 <C> 0.090 <C> 0.093 <C> 0.140 <C> 0.157 <C> 0.088 <C> 0.091 <C> 0.203 <R> <C> AITA-aspect <C> 0.179 <C> 0.094 <C> 0.094 <C> 0.146 <C> 0.157 <C> [BOLD] 0.089 <C> 0.092 <C> 214 <R> <C> AITA <C> [BOLD] 0.184 <C> [BOLD] 0.097 <C> [BOLD] 0.099 <C> [BOLD] 0.148 <C> [BOLD] 0.167 <C> [BOLD] 0.089 <C> [BOLD] 0.094 <C> [BOLD] 0.221 <R> <C> [EMPTY] <C> Beauty <C> Beauty <C> Beauty <C> Beauty <C> Cell Phone <C> Cell Phone <C> Cell Phone <C> Cell Phone <R> <C> G [ITALIC] a <C> 0.133 <C> 0.088 <C> 0.118 <C> 0.218 <C> 0.203 <C> 0.125 <C> 0.130 <C> 0.104 <R> <C> G [ITALIC] PNa <C> 0.235 <C> 0.122 <C> 0.128 <C> 0.257 <C> 0.250 <C> 0.122 <C> 0.150 <C> 0.217 <R> <C> G [ITALIC] PNar <C> 0.194 <C> 0.098 <C> 0.119 <C> 0.205 <C> 0.215 <C> 0.117 <C> 0.136 <C> 0.141 <R> <C> G [ITALIC] PNa+aspect <C> 0.240 <C> 0.122 <C> 0.132 <C> 0.257 <C> 0.251 <C> 0.134 <C> 0.154 <C> 0.223 <R> <C> AITA-aspect <C> 0.240 <C> 0.127 <C> 0.132 <C> 0.257 <C> 0.261 <C> 0.139 <C> 0.184 <C> 0.230 <R> <C> AITA <C> [BOLD] 0.249 <C> [BOLD] 0.129 <C> [BOLD] 0.136 <C> [BOLD] 0.259 <C> [BOLD] 0.267 <C> [BOLD] 0.142 <C> [BOLD] 0.193 <C> [BOLD] 0.244 <R> <C> [EMPTY] <C> Clothing & Jewelry <C> Clothing & Jewelry <C> Clothing & Jewelry <C> Clothing & Jewelry <C> Electronics <C> Electronics <C> Electronics <C> Electronics <R> <C> G [ITALIC] a <C> 0.224 <C> 0.093 <C> 0.091 <C> 0.178 <C> 0.099 <C> 0.048 <C> 0.107 <C> 0.144 <R> <C> G [ITALIC] PNa <C> 0.283 <C> 0.134 <C> 0.118 <C> 0.227 <C> 0.124 <C> 0.069 <C> [BOLD] 0.131 <C> 0.171 <R> <C> G [ITALIC] PNar <C> 0.258 <C> 0.110 <C> 0.101 <C> 0.198 <C> 0.100 <C> 0.053 <C> 0.121 <C> 0.156 <R> <C> G [ITALIC] PNa+aspect <C> 0.298 <C> 0.139 <C> 0.125 <C> 0.241 <C> 0.120 <C> 0.069 <C> 0.126 <C> 0.171 <R> <C> AITA-aspect <C> 0.306 <C> 0.152 <C> 0.138 <C> 0.246 <C> 0.125 <C> 0.069 <C> [BOLD] 0.131 <C> 0.174 <R> <C> AITA <C> [BOLD] 0.316 <C> [BOLD] 0.157 <C> [BOLD] 0.145 <C> [BOLD] 0.263 <C> [BOLD] 0.127 <C> [BOLD] 0.073 <C> [BOLD] 0.131 <C> [BOLD] 0.175 <R> <C> [EMPTY] <C> Health <C> Health <C> Health <C> Health <C> Musical Instruments <C> Musical Instruments <C> Musical Instruments <C> Musical Instruments <R> <C> G [ITALIC] a <C> 0.114 <C> 0.062 <C> 0.091 <C> 0.095 <C> 0.088 <C> 0.054 <C> 0.096 <C> 0.091 <R> <C> G [ITALIC] PNa <C> 0.130 <C> 0.080 <C> 0.089 <C> 0.108 <C> 0.114 <C> 0.110 <C> 0.121 <C> 0.119 <R> <C> G [ITALIC] PNar <C> 0.124 <C> 0.069 <C> 0.086 <C> 0.104 <C> 0.090 <C> 0.072 <C> 0.106 <C> 0.103 <R> <C> G [ITALIC] PNa+aspect <C> 0.133 <C> 0.100 <C> 0.123 <C> 0.175 <C> 0.118 <C> 0.110 <C> 0.130 <C> 0.192 <R> <C> AITA-aspect <C> 0.137 <C> 0.100 <C> 0.121 <C> 0.179 <C> 0.124 <C> 0.110 <C> 0.136 <C> 0.201 <R> <C> AITA <C> [BOLD] 0.142 <C> [BOLD] 0.109 <C> [BOLD] 0.132 <C> [BOLD] 0.194 <C> [BOLD] 0.129 <C> [BOLD] 0.112 <C> [BOLD] 0.141 <C> [BOLD] 0.205 <R> <C> [EMPTY] <C> Sports & Outdoors <C> Sports & Outdoors <C> Sports & Outdoors <C> Sports & Outdoors <C> Tools <C> Tools <C> Tools <C> Tools <R> <C> G [ITALIC] a <C> 0.079 <C> 0.046 <C> 0.042 <C> 0.064 <C> 0.098 <C> 0.059 <C> 0.093 <C> 0.105 <R> <C> G [ITALIC] PNa <C> 0.091 <C> 0.052 <C> 0.079 <C> [BOLD] 0.102 <C> 0.107 <C> 0.077 <C> 0.112 <C> 0.135 <R> <C> G [ITALIC] PNar <C> 0.087 <C> 0.050 <C> 0.071 <C> 0.083 <C> 0.100 <C> 0.072 <C> 0.103 <C> 0.119 <R> <C> G [ITALIC] PNa+aspect <C> 0.091 <C> 0.052 <C> 0.079 <C> [BOLD] 0.102 <C> 0.110 <C> 0.079 <C> 0.110 <C> 0.136 <R> <C> AITA-aspect <C> 0.094 <C> 0.052 <C> 0.080 <C> 0.102 <C> 0.112 <C> 0.079 <C> 0.116 <C> 0.142 <R> <C> AITA <C> [BOLD] 0.097 <C> [BOLD] 0.057 <C> [BOLD] 0.083 <C> [BOLD] 0.102 <C> [BOLD] 0.117 <C> [BOLD] 0.083 <C> [BOLD] 0.120 <C> [BOLD] 0.149 <CAP> Table 3: Overall performance on question generation.
<R> <C> Clothing & Jewelry <C> Clothing & Jewelry Relevance <C> Clothing & Jewelry Aspect <C> Clothing & Jewelry Fluency <R> <C> G [ITALIC] PNa <C> 0.58 <C> 0.62 <C> 2.58 <R> <C> G [ITALIC] PNar <C> 0.47 <C> 0.58 <C> 2.29 <R> <C> G [ITALIC] PNa+aspect <C> 0.66 <C> 0.72 <C> 2.76 <R> <C> AITA <C> [BOLD] 0.80 <C> [BOLD] 0.80 <C> [BOLD] 2.86 <CAP> Table 4: Performance of human evaluation.
<R> <C> Cell Phone <C> Cell Phone Relevance <C> Cell Phone Aspect <C> Cell Phone Fluency <R> <C> G [ITALIC] PNa <C> 0.42 <C> 0.55 <C> 2.79 <R> <C> G [ITALIC] PNar <C> 0.35 <C> 0.41 <C> 2.44 <R> <C> G [ITALIC] PNa+aspect <C> 0.58 <C> 0.63 <C> 2.83 <R> <C> AITA <C> [BOLD] 0.72 <C> [BOLD] 0.72 <C> [BOLD] 2.90 <CAP> Table 4: Performance of human evaluation.
<R> <C> [EMPTY] <C> [BOLD] Model <C> [BOLD] ROUGE-1 <C> [BOLD] ROUGE-L <C> [BOLD] Num words <C> [BOLD] Num sentences <R> <C> [EMPTY] <C> Human average <C> 45.0 <C> 37.7 <C> 13.9 <C> 1.0 <R> <C> [EMPTY] <C> Human maximumd <C> [BOLD] 52.7 <C> [BOLD] 44.1 <C> 17.6 <C> 1.0 <R> <C> [ITALIC] Extractive <C> Extract_1 <C> 27.3 <C> 24.8 <C> 7.9 <C> 1.0 <R> <C> [ITALIC] Extractive <C> Extract_2 <C> 19.5 <C> 16.7 <C> 8.9 <C> 1.0 <R> <C> [ITALIC] Extractive <C> Extract_3 <C> 19.4 <C> 16.4 <C> 9.0 <C> 1.0 <R> <C> [ITALIC] Extractive <C> Extract_4 <C> 20.5 <C> 17.7 <C> 8.7 <C> 1.0 <R> <C> [ITALIC] Extractive <C> Extract_5 <C> 24.6 <C> 20.9 <C> 9.3 <C> 1.0 <R> <C> [ITALIC] Extractive <C> Extract Oracle <C> [BOLD] 36.7 <C> [BOLD] 31.9 <C> 9.7 <C> 1.0 <R> <C> [ITALIC] Abstractive <C> SummAE RNN-RNN <C> [ITALIC] 33.7 <C> [ITALIC] 27.2 <C> [ITALIC] 41.2 <C> [ITALIC] 4.2 <R> <C> [EMPTY] <C> + critica,b <C> 19.8 <C> 17.4 <C> 9.5 <C> 1.0 <R> <C> [EMPTY] <C> + LM pre-trainingb,c <C> 26.4 <C> 21.6 <C> 11.5 <C> 1.0 <R> <C> [EMPTY] <C> SummAE TRF-TRF <C> [ITALIC] 22.4 <C> [ITALIC] 20.3 <C> [ITALIC] 44.1 <C> [ITALIC] 5.1 <R> <C> [EMPTY] <C> + critic <C> 22.9 <C> 19.9 <C> 9.4 <C> 1.0 <R> <C> [EMPTY] <C> + LM pre-training <C> 25.8 <C> 21.2 <C> 10.2 <C> 1.0 <R> <C> [EMPTY] <C> SummAE TRF-RNN <C> [ITALIC] 26.5 <C> [ITALIC] 22.8 <C> [ITALIC] 39.7 <C> [ITALIC] 3.0 <R> <C> [EMPTY] <C> + critic <C> 23.2 <C> 19.9 <C> 10.4 <C> 1.0 <R> <C> [EMPTY] <C> + LM pre-training <C> 26.4 <C> 22.7 <C> 10.7 <C> 1.0 <R> <C> [EMPTY] <C> + LM pre-training <C> 33.4 <C> 27.5 <C> 16.3 <C> 1.0 <R> <C> [EMPTY] <C> - token masking <C> 30.9 <C> 25.1 <C> 16.5 <C> 1.0 <R> <C> [EMPTY] <C> - paragraph shuffling <C> 31.8 <C> 26.5 <C> 14.7 <C> 1.0 <R> <C> [EMPTY] <C> + CPP pre-training <C> 34.4 <C> 28.0 <C> 19.6 <C> 1.0 <R> <C> [EMPTY] <C> + NSSP pre-trainingc,d <C> [BOLD] 36.5 <C> [BOLD] 29.3 <C> 19.4 <C> 1.0 <R> <C> [EMPTY] <C> MeanSum-singlea <C> 15.6 <C> 13.4 <C> 9.5 <C> 1.0 <R> <C> [EMPTY] <C> - token masking <C> 12.0 <C> 10.5 <C> 9.4 <C> 1.0 <CAP> Table 1: ROUGE recall scores and generated summary lengths (number of words and sentences) for estimated human performance, extractive baselines, MeanSum-single, and variants of SummAE. By default, all SummAE models incorporate token masking and paragraph shuffling without critic and any pre-training. TRF stands for Transformer. Numbers in bold denote the best performed models in each category based on ROUGE; numbers in italics denote the models that are not qualified for generating one sentence summaries. Superscript letters, a,b,c,d, denote model pairs that are compared in human evaluations in Table 2.
<R> <C> [EMPTY] <C> [EMPTY] <C> system <C> VAD <C> FEA <C> VOiCES dev MinDCF <C> VOiCES dev PRBEP <C> VOiCES dev EER <C> SITW core-core MinDCF <C> SITW core-core PRBEP <C> SITW core-core EER <R> <C> [EMPTY] <C> 1 <C> x-vector <C> Kaldi <C> FBANK <C> 0.141 <C> 1908.8 <C> 1.23 <C> 0.188 <C> 461.4 <C> 1.80 <R> <C> fixed <C> 2 <C> x-vector <C> Kaldi <C> PLP <C> 0.163 <C> 2204.3 <C> 1.44 <C> 0.191 <C> 464.6 <C> 1.92 <R> <C> [EMPTY] <C> 3 <C> x-vector BIG <C> Kaldi <C> MFCC <C> 0.163 <C> 2186.8 <C> 1.29 <C> 0.177 <C> 430.2 <C> 1.77 <R> <C> [EMPTY] <C> 4 <C> i-vector <C> VAD-NN <C> MFCC+SBN <C> 0.428 <C> 5911.6 <C> 4.46 <C> 0.275 <C> 693.4 <C> 3.19 <R> <C> open <C> 5 <C> x-vector ADAPT <C> Kaldi <C> FBANK <C> 0.146 <C> 1954.0 <C> 1.13 <C> 0.202 <C> 495.8 <C> 1.99 <R> <C> [EMPTY] <C> 6 <C> x-vector ADAPT <C> Kaldi <C> PLP <C> 0.157 <C> 2123.3 <C> 1.31 <C> 0.195 <C> 481.3 <C> 2.11 <R> <C> fixed <C> 1+2+3 <C> PRIMARY 1 <C> [EMPTY] <C> [EMPTY] <C> 0.122 <C> 1647.1 <C> 1.04 <C> 0.17 <C> 427.3 <C> 1.65 <R> <C> fixed <C> 1 <C> CONTRASTIVE 2 <C> [EMPTY] <C> [EMPTY] <C> 0.141 <C> 1908.8 <C> 1.23 <C> 0.188 <C> 461.4 <C> 1.80 <R> <C> open <C> 3+4+5+6 <C> PRIMARY 1 <C> [EMPTY] <C> [EMPTY] <C> 0.119 <C> 1596.1 <C> 1.00 <C> 0.17 <C> 432.1 <C> 1.73 <CAP> Table 3: Development results
<R> <C> [EMPTY] <C> Rank Correlation VQA-HAT <C> Rank Correlation VQA-X <C> Accuracy/% VQA-2.0 <R> <C> Human  <C> 0.623 <C> - <C> 80.62 <R> <C> PJ-X  <C> 0.396 <C> 0.342 <C> - <R> <C> MCB  <C> 0.276 <C> 0.261 <C> 62.27 <R> <C> Attn-MCB,  [ITALIC] α=1 (ours) <C> [BOLD] 0.580 <C> [BOLD] 0.396 <C> 60.51 <R> <C> Attn-MCB (ours) <C> 0.517 <C> 0.375 <C> 62.24 <R> <C> MFB  <C> 0.276 <C> 0.299 <C> 65.22 <R> <C> Attn-MFB (ours) <C> 0.416 <C> 0.335 <C> 65.36 <R> <C> MFH  <C> 0.354 <C> 0.350 <C> 66.17 <R> <C> Attn-MFH (ours) <C> 0.483 <C> 0.376 <C> [BOLD] 66.31 <CAP> Table 1: Evaluation of different VQA models on visual grounding and answer prediction. The reported accuracies are evaluated using the VQA-2.0 test-standard set.
<R> <C> [BOLD] Model <C> [BOLD] %METEOR↑  [BOLD] word <C> [BOLD] %BLEU↑  [BOLD] word <C> [BOLD] %BLEU↑  [BOLD] lemma <C> [BOLD] %BLEU↑  [BOLD] factors <C> [BOLD] #UNK <R> <C> NMT / +UR <C> 62.21 / 63.38 <C> 41.80 / 42.74 <C> 45.10 <C> 51.80 <C> 1111 <R> <C> BPE <C> 62.87 <C> 42.37 <C> 45.96 <C> 53.31 <C> 0 <R> <C> FNMT / +UR <C> [BOLD] 64.10 / 64.81 <C> [BOLD] 43.42 / 44.15 <C> [BOLD] 47.18 <C> [BOLD] 54.24 <C> 604 <CAP> Table 2: Results on IWSLT test15. %BLEU and %METEOR performance of NMT and FNMT systems with and without UNK replacement (UR) are presented. For each system we provide the number of generated UNK tokens in the last column
<R> <C> Model <C> CER <C> PPL <C> #feat (M) <R> <C> KN5 (cutoff=00000) <C> 4.88 <C> 227.92 <C> 49 <R> <C> KN6 (cutoff=000000) <C> 4.87 <C> 227.76 <C> 64 <R> <C> FNN  <C> 4.30 <C> 206.87 <C> 53 <R> <C> TRFs (“w+c+ws+cs+cpw”) <C> TRFs (“w+c+ws+cs+cpw”) <C> TRFs (“w+c+ws+cs+cpw”) <C> TRFs (“w+c+ws+cs+cpw”) <R> <C> 200 classes <C> 4.52 <C> ∼207 <C> 47 <R> <C> 400 classes* <C> 4.32 <C> ∼191 <C> 58 <R> <C> 600 classes <C> 4.35 <C> ∼186 <C> 61 <R> <C> Model Combination <C> Model Combination <C> Model Combination <C> Model Combination <R> <C> FNN + KN6 (W) <C> 4.06 <C> [EMPTY] <C> [EMPTY] <R> <C> FNN + KN6 (S) <C> 4.16 <C> [EMPTY] <C> [EMPTY] <R> <C> FNN + KN6 (Log) <C> 4.11 <C> [EMPTY] <C> [EMPTY] <R> <C> TRF* + FNN (S) <C> 4.09 <C> [EMPTY] <C> [EMPTY] <R> <C> TRF* + FNN (Log) <C> [BOLD] 4.0 <C> [EMPTY] <C> [EMPTY] <CAP> Table 3: The CERs and PPLs on the test set in Chinese speech recognition. “TRF*” denotes the TRF trained with 400 classes.
<R> <C> [EMPTY] <C> [ITALIC] Nbooks <C> [ITALIC] Nchar <C> [ITALIC] Nmin <C> [ITALIC] Nave <C> [ITALIC] b <C> [ITALIC] Nmax <C> [ITALIC] Ncorp <R> <C> en <C> 19,793 <C> 46 <C> 5 <C> 5,899.3 <C> 5,849 <C> 219,990 <C> 2,836,900 <R> <C> fr <C> 1,360 <C> 44 <C> 395 <C> 8,300.7 <C> 17,715 <C> 26,171 <C> 528,314 <R> <C> fi <C> 505 <C> 31 <C> 1,144 <C> 8,872.6 <C> 7,761 <C> 31,623 <C> 811,742 <R> <C> nl <C> 434 <C> 48 <C> 133 <C> 6,747.1 <C> 6,098 <C> 82,246 <C> 443,816 <R> <C> pt <C> 375 <C> 38 <C> 203 <C> 4,675.8 <C> 10,363 <C> 17,818 <C> 246,497 <R> <C> de <C> 327 <C> 30 <C> 153 <C> 7,554.9 <C> 7,259 <C> 113,089 <C> 477,274 <R> <C> es <C> 223 <C> 34 <C> 406 <C> 8,735.1 <C> 15,079 <C> 29,452 <C> 237,874 <R> <C> it <C> 194 <C> 29 <C> 1,083 <C> 9,388.7 <C> 13,954 <C> 29,445 <C> 258,509 <R> <C> sv <C> 56 <C> 34 <C> 1,389 <C> 7,499.8 <C> 5,315 <C> 18,726 <C> 123,806 <R> <C> el <C> 42 <C> 35 <C> 2,047 <C> 6,414.7 <C> 7,613 <C> 17,774 <C> 110,940 <CAP> Table 1: Table of information concerning the data used from the eBooks database. For each language we record the number of books (Nbooks); the number of characters (Nchar), which we take to be the number of letters wik (a, b) (including diacritics and ligatures); the minimum text size (Nmin); the maximum text size (Nmax); and the total corpus size (Ncorp). For reference, we additionally record the regressed point of scaling break, b.
<R> <C> System  [BOLD] InfoRank <C> R-1  [BOLD] 37.6 <C> R-2 15.9 <C> System  [BOLD] InfoFilter <C> R-1  [BOLD] 50.7 <C> R-2  [BOLD] 30.2 <R> <C> Icsisumm <C> 33.3 <C> [BOLD] 16.0 <C> LeadWords <C> 48.0 <C> 27.5 <R> <C> RandomRank <C> 31.9 <C> 8.7 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 4: Performance comparison on single-document summarization (%)
<R> <C> [EMPTY] <C> [BOLD] Speeches 111 Corr. <C> [BOLD] Speeches 111 SRC <C> [BOLD] Speeches 112 Corr. <C> [BOLD] Speeches 112 SRC <C> [BOLD] Speeches 113 Corr. <C> [BOLD] Speeches 113 SRC <C> [BOLD] Tweets 114 Corr. <C> [BOLD] Tweets 114 SRC <R> <C> wordfish <C> 0.52 <C> 0.49 <C> 0.51 <C> 0.51 <C> 0.71 <C> 0.65 <C> 0.79 <C> 0.74 <R> <C> wordshoal <C> 0.62 <C> 0.66 <C> 0.58 <C> 0.51 <C> 0.46 <C> 0.46 <C> — <C> — <R> <C> tbip <C> [BOLD] 0.82 <C> [BOLD] 0.77 <C> [BOLD] 0.85 <C> [BOLD] 0.85 <C> [BOLD] 0.89 <C> [BOLD] 0.86 <C> [BOLD] 0.94 <C> [BOLD] 0.88 <CAP> Table 4: The TBIP learns ideal points most similar to dw-nominate vote ideal points for U.S. senator speeches and tweets. It learns closer ideal points than wordfish and wordshoal in terms of both correlation (Corr.) and Spearman’s rank correlation (SRC). The numbers in the column titles refer to the Senate session of the corpus. wordshoal cannot be applied to tweets because there are no debate labels.
<R> <C> Models <C> Sparse20 <C> Sparse50 <C> Sparse80 <R> <C> NSC(LA) <C> 0.469 <C> 0.428 <C> 0.309 <R> <C> NSC <C> 0.497 <C> 0.408 <C> 0.292 <R> <C> CNN+CSAA <C> 0.497 <C> 0.444 <C> 0.343 <R> <C> RNN+CSAA <C> [BOLD] 0.505 <C> 0.455 <C> 0.364 <R> <C> [BOLD] HCSC <C> [BOLD] 0.505 <C> [BOLD] 0.456 <C> [BOLD] 0.368 <CAP> (a) IMDB Datasets
<R> <C> Models <C> Sparse20 <C> Sparse50 <C> Sparse80 <R> <C> NSC(LA) <C> 0.624 <C> 0.590 <C> 0.523 <R> <C> NSC <C> 0.626 <C> 0.592 <C> 0.511 <R> <C> CNN+CSAA <C> 0.626 <C> 0.605 <C> 0.522 <R> <C> RNN+CSAA <C> 0.633 <C> 0.603 <C> 0.527 <R> <C> [BOLD] HCSC <C> [BOLD] 0.636 <C> [BOLD] 0.608 <C> [BOLD] 0.538 <CAP> (b) Yelp 2013 Datasets
<R> <C> [BOLD] Run <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1 <R> <C> Run-1 <C> 76.08 <C> 70.68 <C> 73.28 <R> <C> Run-2 <C> 76.75 <C> 70.37 <C> 73.42 <R> <C> Run-3 <C> 76.32 <C> 70.25 <C> 73.16 <R> <C> Run-4 <C> 76.16 <C> 70.98 <C> [BOLD] 73.48 <R> <C> Run-5 <C> 75.70 <C> 70.28 <C> 72.89 <R> <C> Run-6 <C> 76.26 <C> 69.90 <C> 72.94 <CAP> Table 6: Official evaluation results on test set, which consider entities at all levels
<R> <C> Category <C> Representation <C> F1 <C> LR precision <C> recall <C> F1 <C> LGBM precision <C> recall <R> <C> [EMPTY] <C> BoW-basic <C> 0.76 <C> 0.81 <C> 0.717 <C> 0.76 <C> 0.77 <C> 0.751 <R> <C> Bag-of-words <C> BoW-parsed <C> 0.764 <C> 0.798 <C> 0.733 <C> 0.755* <C> 0.757 <C> 0.753 <R> <C> [EMPTY] <C> NB-BoW <C> [ITALIC] 0.817 <C> 0.844 <C> 0.794 <C> [ITALIC] 0.77 <C> 0.774 <C> 0.767 <R> <C> [EMPTY] <C> Pretrained <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> word2vec <C> 0.779 <C> 0.793 <C> 0.771 <C> 0.774 <C> 0.77 <C> 0.779 <R> <C> [EMPTY] <C> glove <C> 0.784 <C> 0.789 <C> 0.784 <C> [ITALIC] 0.802 <C> 0.814 <C> 0.791 <R> <C> [EMPTY] <C> fastText <C> 0.78 <C> 0.777 <C> 0.785 <C> 0.761 <C> 0.779 <C> 0.749 <R> <C> Word embedding <C> Recipe54k-trained <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> word2vec <C> 0.78 <C> 0.787 <C> 0.778 <C> 0.779 <C> 0.78 <C> 0.781 <R> <C> [EMPTY] <C> glove <C> 0.783 <C> 0.783 <C> 0.788 <C> 0.783 <C> 0.793 <C> 0.775 <R> <C> [EMPTY] <C> fastText <C> 0.787 <C> 0.778 <C> 0.8 <C> 0.755* <C> 0.76 <C> 0.753 <R> <C> [EMPTY] <C> Recipe1M-trained <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [EMPTY] <C> word2vec <C> [ITALIC] 0.815 <C> 0.815 <C> 0.816 <C> 0.787 <C> 0.823 <C> 0.759 <R> <C> Sentence embedding <C> skip thoughts <C> 0.79 <C> 0.803 <C> 0.779 <C> 0.79 <C> 0.818 <C> 0.767 <R> <C> Paragraph embedding <C> doc2vec <C> 0.745* <C> 0.761 <C> 0.731 <C> 0.763 <C> 0.78 <C> 0.749 <R> <C> [EMPTY] <C> NU only <C> 0.825 <C> 0.83 <C> 0.822 <C> 0.811 <C> 0.81 <C> 0.812 <R> <C> Nutritional properties (NU) <C> NU + NB-BoW <C> [ITALIC] 0.85 <C> 0.859 <C> 0.842 <C> 0.831 <C> 0.823 <C> 0.84 <R> <C> [EMPTY] <C> NU + best word embedding <C> 0.836 <C> 0.851 <C> 0.824 <C> [BOLD] 0.854 <C> 0.852 <C> 0.858 <R> <C> [EMPTY] <C> NU + sentence embedding <C> 0.821 <C> 0.857 <C> 0.79 <C> 0.843 <C> 0.847 <C> 0.839 <CAP> Table 1. Performance of different classification models
<R> <C> Language <C> FastText <C> BPEmb <C> Δ <R> <C> English <C> 62.9 <C> [BOLD] 65.4 <C> 2.5 <R> <C> German <C> 65.5 <C> [BOLD] 66.2 <C> 0.7 <R> <C> Russian <C> [BOLD] 71.2 <C> 70.7 <C> -0.5 <R> <C> French <C> [BOLD] 64.5 <C> 63.9 <C> -0.6 <R> <C> Spanish <C> [BOLD] 66.6 <C> 66.5 <C> -0.1 <R> <C> Chinese <C> 71.0 <C> [BOLD] 72.0 <C> 1.0 <R> <C> Japanese <C> [BOLD] 62.3 <C> 61.4 <C> -0.9 <R> <C> Tibetan <C> 37.9 <C> [BOLD] 41.4 <C> 3.5 <R> <C> Burmese <C> [BOLD] 65.0 <C> 64.6 <C> -0.4 <R> <C> Vietnamese <C> 81.0 <C> 81.0 <C> 0.0 <R> <C> Khmer <C> [BOLD] 61.5 <C> 52.6 <C> -8.9 <R> <C> Thai <C> 63.5 <C> [BOLD] 63.8 <C> 0.3 <R> <C> Lao <C> 44.9 <C> [BOLD] 47.0 <C> 2.1 <R> <C> Malay <C> 75.9 <C> [BOLD] 76.3 <C> 0.4 <R> <C> Tagalog <C> [BOLD] 63.4 <C> 62.6 <C> -1.2 <CAP> Table 2: Entity typing scores for five high-resource languages (top), two high-resource languages without explicit tokenization, and eight medium- to low-resource Asian languages (bottom).
<R> <C> Model <C> PubMed <C> CSAbst. <C> NICTA <R> <C> Jin and Szolovits ( 2018 ) <C> 92.6 <C> 81.3 <C> 84.7 <R> <C> bert +Transformer <C> 89.6 <C> 78.8 <C> 78.4 <R> <C> bert +Transformer+crf <C> 92.1 <C> 78.5 <C> 79.1 <R> <C> Our model <C> [BOLD] 92.9 <C> [BOLD] 83.1 <C> [BOLD] 84.8 <CAP> Table 3: Abstract sentence classification (micro F1).
<R> <C> sent. pos. <C> CNN article <C> Sent-level rouge <C> Individual Oracle <C> Collective Oracle <C> Multiple Collective Oracle <R> <C> sent. pos. <C> Sentences <C> Sent-level rouge <C> Individual Oracle <C> Collective Oracle <C> Multiple Collective Oracle <R> <C> 0 <C> A debilitating, mosquito-borne virus called Chikungunya has made its way to North Carolina, health officials say. <C> 21.2 <C> 1 <C> 1 <C> (0,11,13) : 59.3 (0,13) : 57.5 (11,13) : 57.2 (0,1,13) : 57.1 (1,13) : 56.6 (3,11,13) : 55.0 (13) : 54.5 (0,3,13) : 54.2 (3,13) : 53.4 (1,3,13) : 52.9 (1,11,13) : 52.0 (0,9,13) : 51.3 (0,7,13) : 51.3 (0,12,13) : 51.0 (9,11,13) : 50.4 (1,9,13) : 50.1 (12,13) : 49.3 (7,11,13) : 47.8 (0,10,13) : 47.8 (11,12,13):47.7 (7,13) : 47.6 (9,13) : 47.5 (1,7,13) : 46.9 (3,7,13) : 46.0 (3,12,13) : 46.0 (3,9,13) : 45.9 (10,13) : 45.5 (4,11,13) : 45.3 … <R> <C> 1 <C> It’s the state’s first reported case of the virus. <C> 18.1 <C> 1 <C> 0 <C> (0,11,13) : 59.3 (0,13) : 57.5 (11,13) : 57.2 (0,1,13) : 57.1 (1,13) : 56.6 (3,11,13) : 55.0 (13) : 54.5 (0,3,13) : 54.2 (3,13) : 53.4 (1,3,13) : 52.9 (1,11,13) : 52.0 (0,9,13) : 51.3 (0,7,13) : 51.3 (0,12,13) : 51.0 (9,11,13) : 50.4 (1,9,13) : 50.1 (12,13) : 49.3 (7,11,13) : 47.8 (0,10,13) : 47.8 (11,12,13):47.7 (7,13) : 47.6 (9,13) : 47.5 (1,7,13) : 46.9 (3,7,13) : 46.0 (3,12,13) : 46.0 (3,9,13) : 45.9 (10,13) : 45.5 (4,11,13) : 45.3 … <R> <C> 2 <C> The patient was likely infected in the Caribbean, according to the Forsyth County Department of Public Health. <C> 11.2 <C> 1 <C> 0 <C> (0,11,13) : 59.3 (0,13) : 57.5 (11,13) : 57.2 (0,1,13) : 57.1 (1,13) : 56.6 (3,11,13) : 55.0 (13) : 54.5 (0,3,13) : 54.2 (3,13) : 53.4 (1,3,13) : 52.9 (1,11,13) : 52.0 (0,9,13) : 51.3 (0,7,13) : 51.3 (0,12,13) : 51.0 (9,11,13) : 50.4 (1,9,13) : 50.1 (12,13) : 49.3 (7,11,13) : 47.8 (0,10,13) : 47.8 (11,12,13):47.7 (7,13) : 47.6 (9,13) : 47.5 (1,7,13) : 46.9 (3,7,13) : 46.0 (3,12,13) : 46.0 (3,9,13) : 45.9 (10,13) : 45.5 (4,11,13) : 45.3 … <R> <C> 3 <C> Chikungunya is primarily found in Africa, East Asia and the Caribbean islands, but the Centers for Disease Control and Prevention has been watching the virus,+ for fear that it could take hold in the United States – much like West Nile did more than a decade ago. <C> 35.6 <C> 1 <C> 0 <C> (0,11,13) : 59.3 (0,13) : 57.5 (11,13) : 57.2 (0,1,13) : 57.1 (1,13) : 56.6 (3,11,13) : 55.0 (13) : 54.5 (0,3,13) : 54.2 (3,13) : 53.4 (1,3,13) : 52.9 (1,11,13) : 52.0 (0,9,13) : 51.3 (0,7,13) : 51.3 (0,12,13) : 51.0 (9,11,13) : 50.4 (1,9,13) : 50.1 (12,13) : 49.3 (7,11,13) : 47.8 (0,10,13) : 47.8 (11,12,13):47.7 (7,13) : 47.6 (9,13) : 47.5 (1,7,13) : 46.9 (3,7,13) : 46.0 (3,12,13) : 46.0 (3,9,13) : 45.9 (10,13) : 45.5 (4,11,13) : 45.3 … <R> <C> 4 <C> The virus, which can cause joint pain and arthritis-like symptoms, has been on the U.S. public health radar for some time. <C> 16.7 <C> 1 <C> 0 <C> (0,11,13) : 59.3 (0,13) : 57.5 (11,13) : 57.2 (0,1,13) : 57.1 (1,13) : 56.6 (3,11,13) : 55.0 (13) : 54.5 (0,3,13) : 54.2 (3,13) : 53.4 (1,3,13) : 52.9 (1,11,13) : 52.0 (0,9,13) : 51.3 (0,7,13) : 51.3 (0,12,13) : 51.0 (9,11,13) : 50.4 (1,9,13) : 50.1 (12,13) : 49.3 (7,11,13) : 47.8 (0,10,13) : 47.8 (11,12,13):47.7 (7,13) : 47.6 (9,13) : 47.5 (1,7,13) : 46.9 (3,7,13) : 46.0 (3,12,13) : 46.0 (3,9,13) : 45.9 (10,13) : 45.5 (4,11,13) : 45.3 … <R> <C> 5 <C> About 25 to 28 infected travelers bring it to the United States each year, said Roger Nasci, chief of the CDC’s Arboviral Disease Branch in the Division of Vector-Borne Diseases. <C> 9.7 <C> 0 <C> 0 <C> (0,11,13) : 59.3 (0,13) : 57.5 (11,13) : 57.2 (0,1,13) : 57.1 (1,13) : 56.6 (3,11,13) : 55.0 (13) : 54.5 (0,3,13) : 54.2 (3,13) : 53.4 (1,3,13) : 52.9 (1,11,13) : 52.0 (0,9,13) : 51.3 (0,7,13) : 51.3 (0,12,13) : 51.0 (9,11,13) : 50.4 (1,9,13) : 50.1 (12,13) : 49.3 (7,11,13) : 47.8 (0,10,13) : 47.8 (11,12,13):47.7 (7,13) : 47.6 (9,13) : 47.5 (1,7,13) : 46.9 (3,7,13) : 46.0 (3,12,13) : 46.0 (3,9,13) : 45.9 (10,13) : 45.5 (4,11,13) : 45.3 … <R> <C> 6 <C> ”We haven’t had any locally transmitted cases in the U.S. thus far,” Nasci said. <C> 7.4 <C> 0 <C> 0 <C> (0,11,13) : 59.3 (0,13) : 57.5 (11,13) : 57.2 (0,1,13) : 57.1 (1,13) : 56.6 (3,11,13) : 55.0 (13) : 54.5 (0,3,13) : 54.2 (3,13) : 53.4 (1,3,13) : 52.9 (1,11,13) : 52.0 (0,9,13) : 51.3 (0,7,13) : 51.3 (0,12,13) : 51.0 (9,11,13) : 50.4 (1,9,13) : 50.1 (12,13) : 49.3 (7,11,13) : 47.8 (0,10,13) : 47.8 (11,12,13):47.7 (7,13) : 47.6 (9,13) : 47.5 (1,7,13) : 46.9 (3,7,13) : 46.0 (3,12,13) : 46.0 (3,9,13) : 45.9 (10,13) : 45.5 (4,11,13) : 45.3 … <R> <C> 7 <C> But a major outbreak in the Caribbean this year – with more than 100,000 cases reported – has health officials concerned. <C> 16.4 <C> 1 <C> 0 <C> (0,11,13) : 59.3 (0,13) : 57.5 (11,13) : 57.2 (0,1,13) : 57.1 (1,13) : 56.6 (3,11,13) : 55.0 (13) : 54.5 (0,3,13) : 54.2 (3,13) : 53.4 (1,3,13) : 52.9 (1,11,13) : 52.0 (0,9,13) : 51.3 (0,7,13) : 51.3 (0,12,13) : 51.0 (9,11,13) : 50.4 (1,9,13) : 50.1 (12,13) : 49.3 (7,11,13) : 47.8 (0,10,13) : 47.8 (11,12,13):47.7 (7,13) : 47.6 (9,13) : 47.5 (1,7,13) : 46.9 (3,7,13) : 46.0 (3,12,13) : 46.0 (3,9,13) : 45.9 (10,13) : 45.5 (4,11,13) : 45.3 … <R> <C> 8 <C> Experts say American tourists are bringing Chikungunya back home, and it’s just a matter of time before it starts to spread within the United States. <C> 10.6 <C> 0 <C> 0 <C> (0,11,13) : 59.3 (0,13) : 57.5 (11,13) : 57.2 (0,1,13) : 57.1 (1,13) : 56.6 (3,11,13) : 55.0 (13) : 54.5 (0,3,13) : 54.2 (3,13) : 53.4 (1,3,13) : 52.9 (1,11,13) : 52.0 (0,9,13) : 51.3 (0,7,13) : 51.3 (0,12,13) : 51.0 (9,11,13) : 50.4 (1,9,13) : 50.1 (12,13) : 49.3 (7,11,13) : 47.8 (0,10,13) : 47.8 (11,12,13):47.7 (7,13) : 47.6 (9,13) : 47.5 (1,7,13) : 46.9 (3,7,13) : 46.0 (3,12,13) : 46.0 (3,9,13) : 45.9 (10,13) : 45.5 (4,11,13) : 45.3 … <R> <C> 9 <C> After all, the Caribbean is a popular one with American tourists, and summer is fast approaching. <C> 13.9 <C> 1 <C> 0 <C> (0,11,13) : 59.3 (0,13) : 57.5 (11,13) : 57.2 (0,1,13) : 57.1 (1,13) : 56.6 (3,11,13) : 55.0 (13) : 54.5 (0,3,13) : 54.2 (3,13) : 53.4 (1,3,13) : 52.9 (1,11,13) : 52.0 (0,9,13) : 51.3 (0,7,13) : 51.3 (0,12,13) : 51.0 (9,11,13) : 50.4 (1,9,13) : 50.1 (12,13) : 49.3 (7,11,13) : 47.8 (0,10,13) : 47.8 (11,12,13):47.7 (7,13) : 47.6 (9,13) : 47.5 (1,7,13) : 46.9 (3,7,13) : 46.0 (3,12,13) : 46.0 (3,9,13) : 45.9 (10,13) : 45.5 (4,11,13) : 45.3 … <R> <C> 10 <C> ”So far this year we’ve recorded eight travel-associated cases, and seven of them have come from countries in the Caribbean where we know the virus is being transmitted,” Nasci said. <C> 18.4 <C> 1 <C> 0 <C> (0,11,13) : 59.3 (0,13) : 57.5 (11,13) : 57.2 (0,1,13) : 57.1 (1,13) : 56.6 (3,11,13) : 55.0 (13) : 54.5 (0,3,13) : 54.2 (3,13) : 53.4 (1,3,13) : 52.9 (1,11,13) : 52.0 (0,9,13) : 51.3 (0,7,13) : 51.3 (0,12,13) : 51.0 (9,11,13) : 50.4 (1,9,13) : 50.1 (12,13) : 49.3 (7,11,13) : 47.8 (0,10,13) : 47.8 (11,12,13):47.7 (7,13) : 47.6 (9,13) : 47.5 (1,7,13) : 46.9 (3,7,13) : 46.0 (3,12,13) : 46.0 (3,9,13) : 45.9 (10,13) : 45.5 (4,11,13) : 45.3 … <R> <C> 11 <C> Other states have also reported cases of Chikungunya. <C> 13.4 <C> 0 <C> 1 <C> (0,11,13) : 59.3 (0,13) : 57.5 (11,13) : 57.2 (0,1,13) : 57.1 (1,13) : 56.6 (3,11,13) : 55.0 (13) : 54.5 (0,3,13) : 54.2 (3,13) : 53.4 (1,3,13) : 52.9 (1,11,13) : 52.0 (0,9,13) : 51.3 (0,7,13) : 51.3 (0,12,13) : 51.0 (9,11,13) : 50.4 (1,9,13) : 50.1 (12,13) : 49.3 (7,11,13) : 47.8 (0,10,13) : 47.8 (11,12,13):47.7 (7,13) : 47.6 (9,13) : 47.5 (1,7,13) : 46.9 (3,7,13) : 46.0 (3,12,13) : 46.0 (3,9,13) : 45.9 (10,13) : 45.5 (4,11,13) : 45.3 … <R> <C> 12 <C> The Tennessee Department of Health said the state has had multiple cases of the virus in people who have traveled to the Caribbean. <C> 15.6 <C> 1 <C> 0 <C> (0,11,13) : 59.3 (0,13) : 57.5 (11,13) : 57.2 (0,1,13) : 57.1 (1,13) : 56.6 (3,11,13) : 55.0 (13) : 54.5 (0,3,13) : 54.2 (3,13) : 53.4 (1,3,13) : 52.9 (1,11,13) : 52.0 (0,9,13) : 51.3 (0,7,13) : 51.3 (0,12,13) : 51.0 (9,11,13) : 50.4 (1,9,13) : 50.1 (12,13) : 49.3 (7,11,13) : 47.8 (0,10,13) : 47.8 (11,12,13):47.7 (7,13) : 47.6 (9,13) : 47.5 (1,7,13) : 46.9 (3,7,13) : 46.0 (3,12,13) : 46.0 (3,9,13) : 45.9 (10,13) : 45.5 (4,11,13) : 45.3 … <R> <C> 13 <C> The virus is not deadly, but it can be painful, with symptoms lasting for weeks. <C> 54.5 <C> 1 <C> 1 <C> (0,11,13) : 59.3 (0,13) : 57.5 (11,13) : 57.2 (0,1,13) : 57.1 (1,13) : 56.6 (3,11,13) : 55.0 (13) : 54.5 (0,3,13) : 54.2 (3,13) : 53.4 (1,3,13) : 52.9 (1,11,13) : 52.0 (0,9,13) : 51.3 (0,7,13) : 51.3 (0,12,13) : 51.0 (9,11,13) : 50.4 (1,9,13) : 50.1 (12,13) : 49.3 (7,11,13) : 47.8 (0,10,13) : 47.8 (11,12,13):47.7 (7,13) : 47.6 (9,13) : 47.5 (1,7,13) : 46.9 (3,7,13) : 46.0 (3,12,13) : 46.0 (3,9,13) : 45.9 (10,13) : 45.5 (4,11,13) : 45.3 … <R> <C> 14 <C> Those with weak immune systems, such as the elderly, are more likely to suffer from the virus’ side effects than those who are healthier. <C> 5.5 <C> 0 <C> 0 <C> (0,11,13) : 59.3 (0,13) : 57.5 (11,13) : 57.2 (0,1,13) : 57.1 (1,13) : 56.6 (3,11,13) : 55.0 (13) : 54.5 (0,3,13) : 54.2 (3,13) : 53.4 (1,3,13) : 52.9 (1,11,13) : 52.0 (0,9,13) : 51.3 (0,7,13) : 51.3 (0,12,13) : 51.0 (9,11,13) : 50.4 (1,9,13) : 50.1 (12,13) : 49.3 (7,11,13) : 47.8 (0,10,13) : 47.8 (11,12,13):47.7 (7,13) : 47.6 (9,13) : 47.5 (1,7,13) : 46.9 (3,7,13) : 46.0 (3,12,13) : 46.0 (3,9,13) : 45.9 (10,13) : 45.5 (4,11,13) : 45.3 … <R> <C> Story Highlights <C> Story Highlights <C> Story Highlights <C> Story Highlights <C> Story Highlights <C> Story Highlights <R> <C> • North Carolina reports first case of mosquito-borne virus called Chikungunya • Chikungunya is primarily found in Africa, East Asia and the Caribbean islands • Virus is not deadly, but it can be painful, with symptoms lasting for weeks <C> • North Carolina reports first case of mosquito-borne virus called Chikungunya • Chikungunya is primarily found in Africa, East Asia and the Caribbean islands • Virus is not deadly, but it can be painful, with symptoms lasting for weeks <C> • North Carolina reports first case of mosquito-borne virus called Chikungunya • Chikungunya is primarily found in Africa, East Asia and the Caribbean islands • Virus is not deadly, but it can be painful, with symptoms lasting for weeks <C> • North Carolina reports first case of mosquito-borne virus called Chikungunya • Chikungunya is primarily found in Africa, East Asia and the Caribbean islands • Virus is not deadly, but it can be painful, with symptoms lasting for weeks <C> • North Carolina reports first case of mosquito-borne virus called Chikungunya • Chikungunya is primarily found in Africa, East Asia and the Caribbean islands • Virus is not deadly, but it can be painful, with symptoms lasting for weeks <C> • North Carolina reports first case of mosquito-borne virus called Chikungunya • Chikungunya is primarily found in Africa, East Asia and the Caribbean islands • Virus is not deadly, but it can be painful, with symptoms lasting for weeks <CAP> Table 1: An abridged CNN article (only first 15 out of 31 sentences are shown) and its “story highlights”. The latter are typically written by journalists to allow readers to quickly gather information on stories. Highlights are often used as gold standard abstractive summaries in the summarization literature.
<R> <C> [BOLD] Model <C> sents/sec <R> <C> Petrov and Klein ( 2007 ) <C> 6.2 <R> <C> Zhu et al. ( 2013 ) <C> 89.5 <R> <C> Liu and Zhang ( 2017b ) <C> 79.2 <R> <C> Stern et al. ( 2017a ) <C> 75.5 <R> <C> Shen et al. ( 2018 ) <C> 111.1 <R> <C> Shen et al. ( 2018 )(w/o tree inference) <C> 351 <R> <C> Our (Division) <C> 226.3 <R> <C> Our (Joint) <C> 158.7 <CAP> Table 3: Parsing speed on the PTB dataset.
<R> <C> [EMPTY] <C> [BOLD] Rating <C> [BOLD] Generation <R> <C> Joint <C> 0.904 <C> 0.267 <R> <C> -user <C> 1.254 <C> 0.220 <R> <C> -neighbor <C> 1.162 <C> 0.245 <R> <C> -user,-neighbor <C> 1.342 <C> 0.205 <R> <C> -rating <C> - <C> 0.254 <R> <C> -generation <C> 1.042 <C> - <CAP> Table 2: Feature ablation tests.
<R> <C> [EMPTY] <C> [BOLD] Rating <C> [BOLD] Generation <R> <C> RS-Average <C> 1.280 <C> - <R> <C> RS-Linear <C> 1.234 <C> - <R> <C> RS-Item <C> 1.364 <C> - <R> <C> RS-MF <C> 1.143 <C> - <R> <C> Sum-Opinosis <C> - <C> 0.183 <R> <C> Sum-LSTM-Att <C> - <C> 0.196 <R> <C> Joint <C> [BOLD] 1.023 <C> [BOLD] 0.250 <CAP> Table 3: Final results.
<R> <C> Dataset <C> Desc. <C> No. docs <C> Avg. keywords <C> Avg. doc length <R> <C> 500N-KPCrowd-v1.1  <C> Broadcast news transcriptions <C> 500 <C> 48.92 <C> 408.33 <R> <C> Inspec  <C> Scientific journal papers from Computer Science collected between 1998 and 2002 <C> 2000 <C> 14.62 <C> 128.20 <R> <C> Nguyen2007  <C> Scientific conference papers <C> 209 <C> 11.33 <C> 5201.09 <R> <C> PubMed <C> Full-text papers collected from PubMed Central <C> 500 <C> 15.24 <C> 3992.78 <R> <C> Schutz2008 <C> Full-text papers collected from PubMed Central <C> 1231 <C> 44.69 <C> 3901.31 <R> <C> SemEval2010  <C> Scientific papers from the ACM Digital Library <C> 243 <C> 16.47 <C> 8332.34 <R> <C> SemEval2017  <C> 500 paragraphs selected from 500 ScienceDirect journal articles, evenly distributed among the domains of Computer Science, Material Sciences and Physics <C> 500 <C> 18.19 <C> 178.22 <R> <C> citeulike180  <C> Full-text papers from the CiteULike.org <C> 180 <C> 18.42 <C> 4796.08 <R> <C> fao30  <C> Agricultural documents from two datasets based on Food and Agriculture Organization (FAO) of the UN <C> 30 <C> 33.23 <C> 4777.70 <R> <C> fao780  <C> Agricultural documents from two datasets based on Food and Agriculture Organization (FAO) of the UN <C> 779 <C> 8.97 <C> 4971.79 <R> <C> kdd  <C> Abstracts from the ACM Conference on Knowledge Discovery and Data Mining (KDD) during 2004-2014 <C> 755 <C> 5.07 <C> 75.97 <R> <C> theses100 <C> Full master and Ph.D. theses from the University of Waikato <C> 100 <C> 7.67 <C> 4728.86 <R> <C> wiki20  <C> Computer science technical research reports <C> 20 <C> 36.50 <C> 6177.65 <R> <C> www  <C> Abstracts of WWW conference papers from 2004-2014 <C> 1330 <C> 5.80 <C> 84.08 <CAP> Table 1: Selection of keyword extraction datasets in English language
<R> <C> Method <C> MOSI <C> MOSEI <C> IEMOCAP <R> <C> TFN <C> 74.8 <C> 53.7 <C> 56.8 <R> <C> MARN <C> 74.5 <C> 53.2 <C> 54.2 <R> <C> MFN <C> 74.2 <C> 54.1 <C> 53.5 <R> <C> LR <C> 74.6 <C> 56.6 <C> 53.9 <R> <C> VAE+LR <C> 77.8 <C> 57.4 <C> 54.4 <R> <C> bc-LSTM <C> 75.1 <C> 56.8 <C> 57.7 <R> <C> VAE+bc-LSTM <C> [BOLD] 80.4∗ <C> [BOLD] 58.8∗ <C> [BOLD] 59.6∗ <CAP> Table 2: Trimodal (acoustic, visual, and textual) F1-scores of our method against the baselines (results on MOSI and IEMOCAP are based on the dataset split from poria-EtAl:2017:Long); * signifies statistically significant improvement (p<0.05 with paired t-test) over bc-LSTM.
<R> <C> [EMPTY] <C> English-French En-Fr <C> English-French Fr-En <C> English-French sym <C> Romanian-English Ro-En <C> Romanian-English En-Ro <C> Romanian-English sym <C> English-Czech En-Cz <C> English-Czech Cz-En <C> English-Czech sym <R> <C> words <C> 22.2 <C> 24.2 <C> 15.7 <C> 47.0 <C> 45.5 <C> 40.3 <C> 36.9 <C> 36.3 <C> 29.5 <R> <C> + POS <C> 20.9 <C> 23.9 <C> 15.3 <C> 45.3 <C> 42.9 <C> 36.9 <C> 35.6 <C> 33.7 <C> 28.2 <R> <C> + diag <C> 15.1 <C> 15.8 <C> 12.8 <C> 37.6 <C> 35.7 <C> 32.2 <C> 24.8 <C> 24.5 <C> 21.0 <R> <C> + POS + diag <C> [BOLD] 13.2 <C> [BOLD] 12.1 <C> [BOLD] 10.2 <C> [BOLD] 33.1 <C> [BOLD] 32.2 <C> [BOLD] 27.8 <C> [BOLD] 24.6 <C> [BOLD] 22.9 <C> [BOLD] 19.9 <CAP> Table 2: Alignment error rates using different input features in each language direction and with grow-diag-final-and symmetrization.
<R> <C> [BOLD] NAME <C> [BOLD] ARTICLE  [BOLD] TRAIN <C> [BOLD] ARTICLE  [BOLD] TEST <C> [BOLD] PARAGRAPH  [BOLD] TRAIN <C> [BOLD] PARAGRAPH  [BOLD] TEST <C> [BOLD] LANGUAGE <R> <C> enwiki <C> 7,634,438 <C> 850,457 <C> 41,256,261 <C> 4,583,893 <C> English <R> <C> hudong <C> 1,618,817 <C> 180,278 <C> 53,675,117 <C> 5,999,920 <C> Chinese <R> <C> argiga <C> 3,011,403 <C> 334,764 <C> 27,989,646 <C> 3,116,719 <C> Arabic <R> <C> engiga <C> 8,887,583 <C> 988,513 <C> 116,456,520 <C> 12,969,170 <C> English <R> <C> zhgiga <C> 5,097,198 <C> 567,179 <C> 38,094,390 <C> 4,237,643 <C> Chinese <R> <C> allgiga <C> 16,996,184 <C> 1,89,0456 <C> 182,540,556 <C> 20,323,532 <C> Multi-lingual <CAP> Table 1: Datasets
<R> <C> [BOLD] Model <C> [BOLD] Dev <C> [BOLD] Test <R> <C> Clark and Curran clark2007wide <C> 91.5 <C> 92.0 <R> <C> Lewis et al. lewis2014improved <C> 91.3 <C> 91.6 <R> <C> Lewis et al. lewis2016lstm <C> 94.1 <C> 94.3 <R> <C> Xu et al. xu2015ccg <C> 93.1 <C> 93.0 <R> <C> Xu et al. xu2016expected <C> 93.49 <C> 93.52 <R> <C> Vaswani et al. vaswani2016supertagging <C> 94.24 <C> 94.5 <R> <C> 7-layers + skip output + gating <C> 94.51 <C> 94.67 <R> <C> 7-layers + skip output + gating (no char) <C> 94.33 <C> 94.45 <R> <C> 7-layers + skip output + gating (no dropout) <C> 94.06 <C> 94.0 <R> <C> 9-layers + skip output + gating <C> [BOLD] 94.55 <C> [BOLD] 94.69 <CAP> Table 1: 1-best supertagging accuracy on CCGbank. “skip output” refers to the skip connections to the cell output, “gating” refers to adding a gate to the identity function, “no char” refers to the models that do not use the character-level information, “no dropout” refers to models that do not use dropout.
<R> <C> [BOLD] #Layers <C> [BOLD] Dev <C> [BOLD] Test <R> <C> 3 <C> 94.21 <C> 94.35 <R> <C> 5 <C> 94.51 <C> 94.57 <R> <C> 7 <C> 94.51 <C> 94.67 <R> <C> 9 <C> 94.55 <C> 94.7 <R> <C> 11 <C> 94.43 <C> 94.65 <CAP> Table 3: Accuracy on CCGbank using gated identity connections to cell outputs, with different number of stacked layers.
<R> <C> Recall@ <C> IMP Xu et al. ( 2017 ) <C> Pixel2Graph Newell and Deng ( 2017 ) <C> Graph-RCNN Yang et al. ( 2018 ) <C> Parsed caption (baseline) <C> C-GEARD (ours) <R> <C> 50 <C> 44.8 <C> 68.0 <C> 54.2 <C> 4.1 <C> 15.3 <R> <C> 100 <C> 53.0 <C> 75.2 <C> 59.1 <C> 4.1 <C> 25.2 <CAP> Table 1: Comparison with respect to Recall@50 and Recall@100 on PredCls metric, in %.
<R> <C> Label <C> [ITALIC] r <C> [ITALIC] p <R> <C> Single cand (BiDAF) <C> 0.150 <C> 0.002 <R> <C> Ambiguous (BiDAF) <C> 0.098 <C> 0.044 <R> <C> Word matching (BiDAF) <C> 0.266 <C> 0.000 <R> <C> Knowledge (BiDAF) <C> -0.288 <C> 0.000 <R> <C> Multi sent (BiDAF) <C> -0.120 <C> 0.035 <R> <C> Unsolvable (GA) <C> -0.119 <C> 0.039 <CAP> Table 7: Pearson’s correlation coefficients (r) between the annotation labels and the baseline scores with p < 0.05.
<R> <C> [BOLD] Models <C> [BOLD] Average Positional Error (APE) in mm  [BOLD] Mean <C> [BOLD] Average Positional Error (APE) in mm  [BOLD] Mean w/o Root <C> [BOLD] Average Positional Error (APE) in mm  [BOLD] Root <C> [BOLD] Average Positional Error (APE) in mm  [BOLD] Torso <C> [BOLD] Average Positional Error (APE) in mm  [BOLD] Head <C> [BOLD] Average Positional Error (APE) in mm  [BOLD] LArm <C> [BOLD] Average Positional Error (APE) in mm  [BOLD] RArm <C> [BOLD] Average Positional Error (APE) in mm  [BOLD] LHip <C> [BOLD] Average Positional Error (APE) in mm  [BOLD] RHip <C> [BOLD] Average Positional Error (APE) in mm  [BOLD] LFoot <C> [BOLD] Average Positional Error (APE) in mm  [BOLD] RFoot <R> <C> [BOLD] Lin et. al. <C> 54.9∗∗∗ <C> 50.0 <C> 151.6 <C> 26.6 <C> 35.4 <C> 61.3 <C> 61.6 <C> 32.2 <C> 32.1 <C> 63.3 <C> 63.2 <R> <C> [BOLD] JL2P w/o Curriculum <C> 52.2∗∗∗ <C> 47.9 <C> 139.2 <C> 24.2 <C> 32.5 <C> 57.3 <C> 57.2 <C> 30.6 <C> 30.7 <C> 62.9 <C> 63.2 <R> <C> [BOLD] JL2P w/o L1 <C> 51.7∗∗ <C> 47.0 <C> 145.0 <C> 24.4 <C> 32.8 <C> 58.0 <C> 57.6 <C> 29.9 <C> 30.7 <C> 59.3 <C> 59.8 <R> <C> [BOLD] JL2P w/o Joint Emb. <C> 50.4 <C> 45.7 <C> 143.3 <C> 24.0 <C> [BOLD] 31.0 <C> 55.6 <C> [BOLD] 54.5 <C> 29.7 <C> 29.5 <C> [BOLD] 59.0 <C> 59.5 <R> <C> [BOLD] JL2P <C> [BOLD] 49.5 <C> [BOLD] 45.4 <C> [BOLD] 131.1 <C> [BOLD] 23.0 <C> 31.4 <C> [BOLD] 55.3 <C> 55.0 <C> [BOLD] 28.6 <C> [BOLD] 29.0 <C> 59.2 <C> [BOLD] 58.8 <CAP> Table 1: Average positional error (APE) for JL2P , JL2P w/o Joint Emb., JL2P w/o L1, JL2P w/o Curriculum and Lin et. al.. Lower is better. Our models (JL2P and variants) show consistent increase in accuracy over Lin et. al. across all joints with the addition of components joint embedding, smooth L1 loss and curriculum learning. Two-tailed pairwise t-test between all models and JL2P where ∗∗∗- p<0.001, and ∗∗- p<0.01.
<R> <C> [EMPTY] <C> [EMPTY] <C> [BOLD] Accuracy <C> truthful  [BOLD] P <C> truthful  [BOLD] R <C> truthful  [BOLD] F <C> deceptive  [BOLD] P <C> deceptive  [BOLD] R <C> deceptive  [BOLD] F <R> <C> human <C> judge 1 <C> [BOLD] 61.9% <C> 57.9 <C> 87.5 <C> [BOLD] 69.7 <C> 74.4 <C> 36.3 <C> 48.7 <R> <C> human <C> judge 2 <C> 56.9% <C> 53.9 <C> [BOLD] 95.0 <C> 68.8 <C> [BOLD] 78.9 <C> 18.8 <C> 30.3 <R> <C> human <C> judge 3 <C> 53.1% <C> 52.3 <C> 70.0 <C> 59.9 <C> 54.7 <C> 36.3 <C> 43.6 <R> <C> meta <C> majority <C> 58.1% <C> 54.8 <C> 92.5 <C> 68.8 <C> 76.0 <C> 23.8 <C> 36.2 <R> <C> meta <C> skeptic <C> 60.6% <C> [BOLD] 60.8 <C> 60.0 <C> 60.4 <C> 60.5 <C> [BOLD] 61.3 <C> [BOLD] 60.9 <CAP> Table 2: Performance of three human judges and two meta-judges on a subset of 160 opinions, corresponding to the first fold of our cross-validation experiments in Section 5. Boldface indicates the largest value for each column.
<R> <C> [BOLD] Approach <C> [BOLD] Features <C> [BOLD] Accuracy <C> truthful  [BOLD] P <C> truthful  [BOLD] R <C> truthful  [BOLD] F <C> deceptive  [BOLD] P <C> deceptive  [BOLD] R <C> deceptive  [BOLD] F <R> <C> genre identification <C> pos \textsc  [ITALIC] svm <C> 73.0% <C> 75.3 <C> 68.5 <C> 71.7 <C> 71.1 <C> 77.5 <C> 74.2 <R> <C> psycholinguistic <C> liwc \textsc  [ITALIC] svm <C> 76.8% <C> 77.2 <C> 76.0 <C> 76.6 <C> 76.4 <C> 77.5 <C> 76.9 <R> <C> deception detection <C> liwc \textsc  [ITALIC] svm <C> 76.8% <C> 77.2 <C> 76.0 <C> 76.6 <C> 76.4 <C> 77.5 <C> 76.9 <R> <C> text categorization <C> unigrams \textsc  [ITALIC] svm <C> 88.4% <C> 89.9 <C> 86.5 <C> 88.2 <C> 87.0 <C> 90.3 <C> 88.6 <R> <C> text categorization <C> bigrams+ \textsc  [ITALIC] svm <C> 89.6% <C> 90.1 <C> 89.0 <C> 89.6 <C> 89.1 <C> 90.3 <C> 89.7 <R> <C> text categorization <C> liwc+bigrams+ \textsc  [ITALIC] svm <C> [BOLD] 89.8% <C> 89.8 <C> [BOLD] 89.8 <C> [BOLD] 89.8 <C> [BOLD] 89.8 <C> 89.8 <C> [BOLD] 89.8 <R> <C> text categorization <C> trigrams+ \textsc  [ITALIC] svm <C> 89.0% <C> 89.0 <C> 89.0 <C> 89.0 <C> 89.0 <C> 89.0 <C> 89.0 <R> <C> text categorization <C> unigrams \textsc  [ITALIC] nb <C> 88.4% <C> [BOLD] 92.5 <C> 83.5 <C> 87.8 <C> 85.0 <C> [BOLD] 93.3 <C> 88.9 <R> <C> text categorization <C> bigrams+ \textsc  [ITALIC] nb <C> 88.9% <C> 89.8 <C> 87.8 <C> 88.7 <C> 88.0 <C> 90.0 <C> 89.0 <R> <C> text categorization <C> trigrams+ \textsc  [ITALIC] nb <C> 87.6% <C> 87.7 <C> 87.5 <C> 87.6 <C> 87.5 <C> 87.8 <C> 87.6 <R> <C> human / meta <C> judge 1 <C> [BOLD] 61.9% <C> 57.9 <C> 87.5 <C> [BOLD] 69.7 <C> 74.4 <C> 36.3 <C> 48.7 <R> <C> human / meta <C> judge 2 <C> 56.9% <C> 53.9 <C> [BOLD] 95.0 <C> 68.8 <C> [BOLD] 78.9 <C> 18.8 <C> 30.3 <R> <C> human / meta <C> skeptic <C> 60.6% <C> [BOLD] 60.8 <C> 60.0 <C> 60.4 <C> 60.5 <C> [BOLD] 61.3 <C> [BOLD] 60.9 <CAP> Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments. Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false positive and false negative rates, as suggested by Forman and Scholz Forman:09. Human performance is repeated here for judge 1, judge 2 and the skeptic meta-judge, although they cannot be directly compared since the 160-opinion subset on which they are assessed only corresponds to the first cross-validation fold.
<R> <C> Model <C> Resampled Valid. Set Prec. <C> Resampled Valid. Set Recall <C> Resampled Valid. Set F-1 <C> Original Train. Set Prec. <C> Original Train. Set Recall <C> Original Train. Set F-1 <C> Original Valid. Set Prec. <C> Original Valid. Set Recall <C> Original Valid. Set F-1 <R> <C> IF-1C <C> 0.630 <C> 0.435 <C> 0.481 <C> 0.683 <C> 0.497 <C> 0.545 <C> 0.690 <C> 0.499 <C> 0.548 <R> <C> SFN <C> 0.759 <C> 0.758 <C> 0.758 <C> 0.753 <C> 0.692 <C> 0.707 <C> 0.762 <C> 0.704 <C> 0.717 <CAP> Table 1: Summary of experimental results. All columns contain average scores achieved by 5 separately trained models on resampled training and validation sets. We also present scores achieved by the models on original sets (in the evaluation mode).
<R> <C> [BOLD] Setup <C> [BOLD] Pearson <C> [BOLD] Spearman <C> [BOLD] MAE <C> [BOLD] RMSE <R> <C> Constant <C> - <C> - <C> 1.013 <C> 1.233 <R> <C> BLEU* <C> 0.074 <C> 0.061 <C> 2.264 <C> 2.731 <R> <C> METEOR* <C> 0.095 <C> 0.099 <C> 1.820 <C> 2.129 <R> <C> ROUGE-L* <C> 0.079 <C> 0.072 <C> 1.312 <C> 1.674 <R> <C> CIDEr* <C> 0.061 <C> 0.058 <C> 2.606 <C> 2.935 <R> <C> [0.5pt/2pt]  1 : Base system <C> 0.273 <C> 0.260 <C> 0.948 <C> 1.258 <R> <C> 2 : + errors generated in training system outputs <C> 0.283 <C> 0.268 <C> 0.948 <C> 1.273 <R> <C> 3 : + training references, with generated errors <C> 0.278 <C> 0.261 <C> 0.930 <C> 1.257 <R> <C> 4 : + systems training data, with generated errors <C> [BOLD] 0.330 <C> 0.274 <C> 0.914 <C> 1.226 <R> <C> [0.5pt/2pt]  5 : + test references, with generated errors* <C> [BOLD] 0.331 <C> 0.265 <C> 0.937 <C> 1.245 <R> <C> 6 : + complete datasets, with generated errors* <C> [BOLD] 0.354 <C> [BOLD] 0.287 <C> 0.909 <C> 1.208 <CAP> Table 3: Results using cross-validation over the whole dataset. Setups marked with “*” use human references for the test instances. All setups 1–6 produce significantly better correlations than all metrics (p<0.01). Significant improvements in correlation (p<0.05) over 1 are marked in bold.
<R> <C> Processor <C> Data transfer Unit <C> Data transfer Count <C> Data transfer Time <C> Hidden Layer <C> Output Layer <R> <C> CPU <C> - <C> - <C> - <C> 6.23 <C> 0.04 <R> <C> GPU <C> LM Query <C> 102,172 <C> 5.94 <C> 2.15 <C> 0.06 <R> <C> GPU <C> Frame <C> 518 <C> 0.60 <C> 2.26 <C> 0.03 <CAP> Table 2: Operation times for each RNNLM computation step in seconds.
<R> <C> Label <C> % Occurrence <R> <C> Noun Phrase (NP) <C> 51.7 <R> <C> Verb Phrase (VP) <C> 20.0 <R> <C> Prepositional Phrase (PP) <C> 19.8 <R> <C> Adverbial Phrase (ADVP) <C> 3.7 <R> <C> Subordinate Clause (SBAR) <C> 2.1 <R> <C> Adjective Phrase (ADJP) <C> 1.9 <R> <C> Verb Particles (PRT) <C> 0.5 <R> <C> Conjunctive Phrase (CONJ) <C> 0.06 <R> <C> Interjective Phrase (INTJ) <C> 0.03 <R> <C> List Marker (LST) <C> 0.01 <R> <C> Unlike Coordination Phrase (UCP) <C> 0.002 <CAP> Table 1: Shallow syntactic chunk phrase types from CoNLL 2000 shared task (Tjong:00) and their occurrence % in the training data.
<R> <C> [EMPTY] <C> [BOLD] CCG <C> [BOLD] PTB POS <C> [BOLD] EWT POS <C> [BOLD] Chunk <C> [BOLD] NER <C> [BOLD] Sem. Tagging <C> [BOLD] Gramm. Err. D <C> [BOLD] Prep. Role <C> [BOLD] Prep. Func. <C> [BOLD] Event Fact. <R> <C> ELMo-transformer <C> 92.68 <C> 97.09 <C> 95.13 <C> 92.18 <C> 81.21 <C> 93.78 <C> 30.80 <C> 72.81 <C> 82.24 <C> 70.88 <R> <C> [BOLD] mSynC <C> 92.03 <C> 96.91 <C> 94.64 <C> 96.89 <C> 79.98 <C> 93.03 <C> 30.86 <C> 70.83 <C> 82.67 <C> 70.39 <CAP> Table 3: Test performance of ELMo-transformer Peters:18b vs. mSynC on several linguistic probes from Liu:19. In each case, performance of the best layer from the architecture is reported. Details on the probes can be found in §4.2.1.
<R> <C> input length <C> Proposed system  [ITALIC] sm <C> Proposed system  [ITALIC] sml <C> Proposed system  [ITALIC] smml <C> Frost et al, 2007  [ITALIC] sm <C> Frost et al, 2007  [ITALIC] sml <C> Frost et al, 2007  [ITALIC] smml <R> <C> 12 <C> 0.001 <C> [BOLD] 0.002 <C> [BOLD] 0.002 <C> [BOLD] 0.001 <C> 0.004 <C> 0.003 <R> <C> 24 <C> 0.008 <C> [BOLD] 0.008 <C> [BOLD] 0.01 <C> [BOLD] 0.005 <C> 0.02 <C> 0.02 <R> <C> 48 <C> 0.08 <C> [BOLD] 0.09 <C> [BOLD] 0.11 <C> [BOLD] 0.02 <C> 0.3 <C> 0.3 <R> <C> 72 <C> 0.39 <C> [BOLD] 0.42 <C> [BOLD] 0.52 <C> [BOLD] 0.10 <C> 2.4 <C> 2.5 <R> <C> 96 <C> 1.2 <C> [BOLD] 1.3 <C> [BOLD] 1.6 <C> [BOLD] 0.26 <C> 8.1 <C> 8.7 <CAP> Table 1: Execution times (in seconds) comparing the system presented here with that of Frost et al. (2007) parsing sequences of the token “a” of various lengths (first column) using four highly ambiguous grammars. The best performance for each test case is indicated in boldface.
<R> <C> SST NSE Munkhdalai and Yu ( 2017 ) <C> SST 89.7 <R> <C> [BOLD] IRAM <C> 90.1 <R> <C> BCN + CoVe McCann et al. ( 2017 ) <C> 90.3 <R> <C> bmLSTM Radford et al. ( 2017 ) <C> [BOLD] 91.8 <R> <C> SST-5 <C> SST-5 <R> <C> [BOLD] IRAM <C> 53.7 <R> <C> BCN + CoVe McCann et al. ( 2017 ) <C> 53.7 <R> <C> BCN + ELMo Peters et al. ( 2018 ) <C> [BOLD] 54.7 <R> <C> IMDb <C> IMDb <R> <C> [BOLD] IRAM <C> 91.2 <R> <C> TRNN Dieng et al. ( 2016 ) <C> 93.8 <R> <C> oh-LSTM Johnson and Zhang ( 2016 ) <C> [BOLD] 94.1 <R> <C> Virtual Miyato et al. ( 2016 ) <C> [BOLD] 94.1 <CAP> Table 1: Classification accuracy on the test sets
<R> <C> Separation <C> Target <C> Sound Separation Task <C> Sound Separation Task <C> Sound Separation Task <R> <C> Module <C> Domain <C> Speech <C> Non-speech <C> Mixed <R> <C> TDCN <C> Time <C> 15.4 <C> 7.7 <C> 11.7 <R> <C> TDCN <C> Latent <C> 16.1 <C> 8.2 <C> 12.4 <R> <C> RTDCN <C> Time <C> 15.6 <C> 8.3 <C> 12.0 <R> <C> RTDCN <C> Latent <C> 16.2 <C> 8.4 <C> 12.6 <R> <C> Oracle <C> STFT <C> 13.0 <C> 14.8 <C> 14.5 <R> <C> Masks <C> Latent <C> 34.1 <C> 39.2 <C> 39.5 <CAP> Table 1: Mean SI-SDRi (dB) of best performing models.
<R> <C> [EMPTY] <C> [BOLD] Features <C> [BOLD] Accuracy% <C> [BOLD] f-score% <C> [BOLD] AUC% <R> <C> ANP <C> 13 MFCCs+Δ+ΔΔ <C> 69 <C> 51 <C> 70 <R> <C> VNP <C> 13 MFCCs+Δ+ΔΔ <C> 71 <C> 53 <C> 72 <CAP> TABLE IV: Overall detection performance.
<R> <C> [EMPTY] <C> w/o Punctuation <C> w/o Punctuation <C> w/o Punctuation <C> w/ Punctuation <C> w/ Punctuation <C> w/ Punctuation <R> <C> Model <C> Mean  [ITALIC] F <C> Self-agreement <C> RB-agreement <C> Mean  [ITALIC] F <C> Self-agreement <C> RB-agreement <R> <C> Left-Branching <C> 20.7 <C> - <C> - <C> 18.9 <C> - <C> - <R> <C> Right-Branching <C> [BOLD] 58.5 <C> - <C> - <C> 18.5 <C> - <C> - <R> <C> Balanced-Tree <C> 39.5 <C> - <C> - <C> 22.0 <C> - <C> - <R> <C> ST-Gumbel <C> 36.4 <C> 57.0 <C> 33.8 <C> 21.9 <C> 56.8 <C> [BOLD] 38.1 <R> <C> PRPN <C> 46.0 <C> 48.9 <C> 51.2 <C> 51.6 <C> 65.0 <C> 27.4 <R> <C> Imitation (SbS only) <C> 45.9 <C> 49.5 <C> 62.2 <C> 52.0 <C> [BOLD] 70.8 <C> 20.6 <R> <C> Imitation (SbS + refine) <C> 53.3† <C> [BOLD] 58.2 <C> [BOLD] 64.9 <C> [BOLD] 53.7† <C> 67.4 <C> 21.1 <CAP> Table 1: Parsing performance with and without punctuation. Mean F indicates mean parsing F-score against the Stanford Parser (early stopping by F-score). Self-/RB-agreement indicates self-agreement and agreement with the right-branching baseline across multiple runs. † indicates a statistical difference from the corresponding PRPN baseline with p<0.01, paired one-tailed bootstrap test.333F-score is not normally distributed. It is therefore appropriate to use the non-parametric bootstrap test.
<R> <C> [EMPTY] <C> [BOLD] NC Relations <C> [BOLD] AN Attributes <R> <C> Majority <C> 50.0 <C> 50.0 <R> <C> -Phrase <C> 50.0 <C> 55.66 <R> <C> -Context <C> 45.06 <C> 63.21 <R> <C> -(Context+Phrase) <C> 45.06 <C> 59.43 <R> <C> Full Model <C> 54.3 <C> 65.1 <CAP> Table 8: Accuracy scores of ablations of the phrase, context sentence, and both features from the best models in the NC Relations and AN Attributes tasks (ELMo+Top+biLM and BERT+All+None, respectively).
<R> <C> [BOLD] Model <C> [BOLD] NPMI  [ITALIC] N=5 <C> [BOLD] NPMI 10 <C> [BOLD] NPMI 15 <C> [BOLD] NPMI 20 <C> [BOLD] NPMI 25 <C> [BOLD] NPMI 30 <C> [BOLD] NPMI Mean† <R> <C> [ITALIC] Beer <C> [ITALIC] Beer <C> [ITALIC] Beer <C> [ITALIC] Beer <C> [ITALIC] Beer <C> [ITALIC] Beer <C> [ITALIC] Beer <C> [ITALIC] Beer <R> <C> SAM* <C> 0.046 <C> 0.120 <C> 0.129 <C> 0.243 <C> 0.308 <C> 0.396 <C> 0.207 <R> <C> MASA <C> 0.020 <C> 0.082 <C> 0.130 <C> 0.168 <C> 0.234 <C> 0.263 <C> 0.150 <R> <C> MAA <C> 0.064 <C> [BOLD] 0.189 <C> 0.255 <C> 0.273 <C> 0.332 <C> 0.401 <C> 0.252 <R> <C> [BOLD] MAM <C> [BOLD] 0.083 <C> 0.187 <C> [BOLD] 0.264 <C> [BOLD] 0.348 <C> [BOLD] 0.410 <C> [BOLD] 0.477 <C> [BOLD] 0.295 <R> <C> [ITALIC] Hotel <C> [ITALIC] Hotel <C> [ITALIC] Hotel <C> [ITALIC] Hotel <C> [ITALIC] Hotel <C> [ITALIC] Hotel <C> [ITALIC] Hotel <C> [ITALIC] Hotel <R> <C> SAM* <C> 0.041 <C> 0.103 <C> 0.152 <C> 0.180 <C> 0.233 <C> 0.281 <C> 0.165 <R> <C> MASA <C> 0.043 <C> 0.127 <C> 0.166 <C> 0.295 <C> 0.323 <C> 0.458 <C> 0.235 <R> <C> MAA <C> 0.128 <C> 0.218 <C> [BOLD] 0.352 <C> 0.415 <C> 0.494 <C> 0.553 <C> 0.360 <R> <C> [BOLD] MAM <C> [BOLD] 0.134 <C> [BOLD] 0.251 <C> 0.349 <C> [BOLD] 0.496 <C> [BOLD] 0.641 <C> [BOLD] 0.724 <C> [BOLD] 0.432 <CAP> (b) Average Topic Coherence (NPMI) across different top-N words for each dataset. Each aspect ai is considered as a topic and the masks/attentions are used to compute P(w|ai).
<R> <C> 3 <C> [BOLD] Yahoo <C> [BOLD] Yahoo <C> [BOLD] Yahoo <C> [BOLD] Yahoo <C> [BOLD] Yahoo <C> [BOLD] Yelp <C> [BOLD] Yelp <C> [BOLD] Yelp <C> [BOLD] Yelp <C> [BOLD] Yelp <R> <C> [BOLD] Model <C> [BOLD] NLL <C> ¯¯¯¯¯¯¯¯ [BOLD] NLL <C> [BOLD] KL <C> [BOLD] MI <C> [BOLD] AU <C> [BOLD] NLL <C> ¯¯¯¯¯¯¯¯ [BOLD] NLL <C> [BOLD] KL <C> [BOLD] MI <C> [BOLD] AU <R> <C> 3 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> LSTM-LM* <C> 328.0 <C> – <C> – <C> – <C> – <C> 358.1 <C> – <C> – <C> – <C> – <R> <C> SeqVAE <C> 328.6 <C> – <C> 0.0 <C> 0.0 <C> 0 <C> 358.1 <C> – <C> 0.3 <C> 0.3 <C> 1 <R> <C> SeqVAE + WordDrop <C> 330.7 <C> – <C> 5.4 <C> 3.0 <C> 6 <C> 362.2 <C> – <C> 1.0 <C> 0.8 <C> 1 <R> <C> SkipVAE <C> 328.1 <C> – <C> 4.5 <C> 2.4 <C> 11 <C> 357.4 <C> – <C> 2.5 <C> 1.5 <C> 4 <R> <C> WAE-RNF** <C> 339.0 <C> – <C> 3.0 <C> – <C> – <C> – <C> – <C> – <C> – <C> – <R> <C> SeqVAE + Cyclical <C> 328.6 <C> – <C> 0.0 <C> 0.0 <C> 0 <C> 358.4 <C> – <C> 0.4 <C> 0.3 <C> 1 <R> <C> SeqVAE + Aggressive <C> [BOLD] 326.7 <C> – <C> 5.7 <C> 2.9 <C> 15 <C> [BOLD] 355.9 <C> – <C> 3.8 <C> 2.4 <C> 11 <R> <C> 2 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> SeqVAE + AvgPool <C> 327.8 <C> – <C> 2.4 <C> 1.6 <C> 5 <C> 357.5 <C> – <C> 1.6 <C> 1.2 <C> 5 <R> <C> SeqVAE + AbsPool <C> 327.4 <C> – <C> 3.6 <C> 2.4 <C> 8 <C> 356.6 <C> – <C> 2.0 <C> 1.7 <C> 7 <R> <C> SeqVAE + MaxPool <C> 327.2 <C> – <C> 3.7 <C> 2.5 <C> 9 <C> 356.0 <C> – <C> 3.1 <C> 2.2 <C> 8 <R> <C> 2 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> iVAE <C> – <C> 309.5 <C> 8.0 <C> 4.4 <C> 32 <C> – <C> 348.2 <C> 7.6 <C> 4.6 <C> 32 <R> <C> iVAEMI <C> – <C> 309.1 <C> 11.4 <C> 10.7 <C> 32 <C> – <C> 348.7 <C> 11.6 <C> 11.0 <C> 32 <R> <C> 3 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 1: Experiment results on the Yahoo and Yelp datasets. For the LSTM-LM*, we report the exact negative log likelihood. For the WAE-RNF**, we only show the results on Yahoo reported by Wang and Wang (2019) as their experiments on Yelp were conducted on a different version of the dataset. Also note that the estimated negative log likelihood in iVAE (Fang et al., 2019) cannot be directly compared with the estimated NLL from previous methods since iVAE uses a lower bound on the typical negative ELBO, which could under-estimate its NLL comparing to other methods. For this reason, we use NLL for NLL estimated from negative ELBO in previous methods, and use ¯¯¯¯¯¯¯¯NLL for iVAE. For a more detailed explanation of the difference, please refer to Section B of the appendix.
<R> <C> 3 <C> [BOLD] Yahoo <C> [BOLD] Yahoo <C> [BOLD] Yelp <C> [BOLD] Yelp <R> <C> [EMPTY] <C> [BOLD] W.O. <C> [BOLD] With <C> [BOLD] W.O. <C> [BOLD] With <R> <C> 3 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> SkipVAE <C> 329.1 <C> 328.1 <C> 358.2 <C> 357.4 <R> <C> Aggressive <C> 328.2 <C> 326.7 <C> 356.9 <C> 355.9 <R> <C> MaxPool <C> 328.6 <C> 327.2 <C> 357.6 <C> 356.0 <R> <C> 3 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 3: Estimated NLLs of various models when trained without vs with KL annealing. As we can see, all models benefit from applying KL annealing at the initial stage of learning.
<R> <C> 3  [BOLD] Dataset <C> [BOLD] Pool. <C> [BOLD] Origin <C> [BOLD] Shuffle <C> [BOLD] Diff. <R> <C> 3 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] Yahoo <C> None <C> 328.6 <C> 328.7 <C> 0.1 <R> <C> [BOLD] Yahoo <C> Avg <C> 327.8 <C> 328.0 <C> 0.2 <R> <C> [BOLD] Yahoo <C> Abs <C> 327.4 <C> 327.7 <C> 0.3 <R> <C> [BOLD] Yahoo <C> Max <C> 327.2 <C> 327.3 <C> 0.1 <R> <C> 3 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] Yelp <C> None <C> 358.1 <C> 358.1 <C> 0.0 <R> <C> [BOLD] Yelp <C> Avg <C> 357.5 <C> 357.8 <C> 0.3 <R> <C> [BOLD] Yelp <C> Abs <C> 356.6 <C> 356.8 <C> 0.2 <R> <C> [BOLD] Yelp <C> Max <C> 356.0 <C> 356.7 <C> 0.7 <R> <C> 3 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 4: Estimated negative log likelihoods on test data. Results in the Shuffle column are computed by randomly permuting the input to the encoders.
<R> <C> [BOLD] Id <C> [BOLD] Ver <C> [BOLD] Text Task <C> [BOLD] Text Task <C> [BOLD] Image Task  [ITALIC] m [BOLD] =50 <C> [BOLD] Image Task  [ITALIC] m [BOLD] =50 <C> [BOLD] Image Task  [ITALIC] m [BOLD] =50 <C> [BOLD] Image Task  [ITALIC] m [BOLD] =100 <C> [BOLD] Image Task  [ITALIC] m [BOLD] =100 <C> [BOLD] Image Task  [ITALIC] m [BOLD] =100 <R> <C> [EMPTY] <C> [EMPTY] <C> Bleu <C> Nist <C> r@1 <C> r@2 <C> r@3 <C> r@1 <C> r@2 <C> r@3 <R> <C> 10 <C> V1 <C> 44.42 <C> 5.92 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> 10 <C> V2 <C> 45.21 <C> 6.04 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> 14 <C> V1 <C> 5.79 <C> 0.662 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> 14 <C> V2 <C> 13.65 <C> 1.327 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> 9 <C> V1 <C> 28.37 <C> 1.164 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> 9 <C> V2 <C> 31.47 <C> 2.746 <C> [EMPTY] <C> - <C> - <C> - <C> - <C> - <R> <C> 11 <C> V1 <C> 67.2 <C> 6.23 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> 11 <C> V2 <C> 60.92 <C> 5.54 <C> [EMPTY] <C> - <C> - <C> - <C> - <C> - <R> <C> 6 <C> V1 <C> 4.18 <C> 0.697 <C> 0.18 <C> 0.32 <C> 0.43 <C> 0.11 <C> 0.20 <C> 0.27 <R> <C> 6 <C> V2 <C> 80.56 <C> 8.20 <C> 0.225 <C> 0.28 <C> 0.41 <C> 0.078 <C> 0.127 <C> 0.179 <R> <C> 13 <C> V1 <C> 19.16 <C> 1.92 <C> 0.24 <C> 0.38 <C> 0.49 <C> 0.15 <C> 0.25 <C> 0.34 <R> <C> 13 <C> V2 <C> 86.57 <C> 3.98 <C> 0.176 <C> 0.298 <C> 0.367 <C> 0.083 <C> 0.15 <C> 0.21 <R> <C> 7 <C> V1 <C> 99.79 <C> 6.36 <C> 0.26 <C> 0.40 <C> 0.50 <C> 0.13 <C> 0.23 <C> 0.31 <R> <C> 7 <C> V2 <C> 99.98 <C> 6.37 <C> 0.21 <C> 0.336 <C> 0.43 <C> 0.1197 <C> 0.204 <C> 0.27 <R> <C> 8 <C> V1 <C> 99.87 <C> 2.32 <C> 0.23 <C> 0.37 <C> 0.48 <C> 0.15 <C> 0.25 <C> 0.33 <R> <C> 8 <C> V2 <C> 100 <C> 2.32 <C> 0.17 <C> 0.289 <C> 0.383 <C> 0.10 <C> 0.177 <C> 0.240 <R> <C> 12 <C> V1 <C> 100 <C> 5.21 <C> 0.24 <C> 0.38 <C> 0.49 <C> 0.09 <C> 0.18 <C> 0.26 <R> <C> 12 <C> V2 <C> 100 <C> 5.21 <C> 0.136 <C> 0.229 <C> 0.31 <C> 0.084 <C> 0.144 <C> 0.194 <CAP> Table 7: Best Model’s performance on dialog states described in Table 2 (‘Id’ refers to the ID of the state-type in Table 2) and V1, V2 refer to the two versions of the dataset, m refers to the size of target image set (one correct, rest incorrect) to be ranked by the model and is varied from 50 to 100
<R> <C> [EMPTY] <C> Res14 <C> Lt14 <C> Res15 <R> <C> #instance <C> 800 <C> 800 <C> 685 <R> <C> #multi-op-as <C> 316 <C> 156 <C> 107 <R> <C> #no-op-as <C> 194 <C> 378 <C> 284 <CAP> Table 4: Number of instances, number of multi-aspects and number of no-aspects instances in test set.
<R> <C> [EMPTY] <C> Local <C> Global <R> <C> [EMPTY] <C> (bits/step) <C> (bits/step) <R> <C> Measured <C> 10.78 <C> 2.96 <R> <C> Null <C> 11.41±0.28 <C> 2.98+0.04−0.02 <R> <C> ( [ITALIC] p-value) <C> ≪10−3 <C> 0.02 <R> <C> Greedy Shortest Path <C> 2.11 <C> 2.97 <CAP> Table S6: Exploration habits. Average text-to-text (local) and text-to-past (global) KL Divergence (bits/step) over the reading path. Text-to-past KL is much lower, as Darwin’s reading spreads out to cover topic space and lowers the information-theoretic surprise of subsequent books. Darwin’s reading strategy is simultaneously more exploitative than would be expected of a random reader while also not following a strategy of pure surprise-minimization.
<R> <C> [BOLD] Model <C> | [ITALIC] θ| <C> [BOLD] Inf. Time <C> [BOLD] Test Acc. <R> <C> MTSA <C> 2.9m <C> 1.6 <C> 86.3 <R> <C> MTSA w/o fw&bw masks <C> 2.9m <C> 1.6 <C> 85.3 (-1.0) <R> <C> MTSA w/o token2token <C> 2.5m <C> 1.5 <C> 85.8 (-0.5) <R> <C> MTSA w/o source2token <C> 2.5m <C> 1.4 <C> 84.9 (-1.4) <R> <C> MTSA w/o proposed modules <C> 1.8m <C> 1.1 <C> 84.3 (-2.0) <CAP> Table 3: An ablation study of MTSA on SNLI.
<R> <C> [BOLD] Model <C> [BOLD] Multi-head (Transformer) <C> [BOLD] MTSA <R> <C> [BOLD] Param# <C> 61.38M <C> 61.58M <R> <C> [BOLD] Setup1 <C> 23.64 <C> 24.09 <R> <C> [BOLD] Setup1 <C> [ITALIC] p-value: 0.001 (6 runs) <C> [ITALIC] p-value: 0.001 (6 runs) <R> <C> [BOLD] Setup2 <C> 26.98 <C> 27.21 <R> <C> [BOLD] Setup2 <C> [ITALIC] p-value: 0.080 (3 runs) <C> [ITALIC] p-value: 0.080 (3 runs) <CAP> Table 6: Results for the Transformer with either multi-head self-attention or proposed MTSA. The reported BLEU values for Setup 1 and 2 are the mean of 5 and 3 runs respectively.
<R> <C> [BOLD] Emotion <C> [BOLD] Precision (%) <C> [BOLD] Recall (%) <C> [BOLD] F1-Score (%) <R> <C> happy <C> 95.0 <C> 96.0 <C> 96.0 <R> <C> sad <C> 99.0 <C> 100.0 <C> 99.0 <R> <C> neutral <C> 99.0 <C> 99.0 <C> 99.0 <R> <C> [BOLD] Accuracy (%) <C> [BOLD] 97.8 <C> [BOLD] 97.8 <C> [BOLD] 97.8 <CAP> Table 3: Emotion classification performance from the Emotion Recognition System.
<R> <C> [EMPTY] <C> 1-1 <C> 1-N <C> N-1 <C> N-N <R> <C> [EMPTY] <C> (head/tail) <C> (head/tail) <C> (head/tail) <C> (head/tail) <R> <C> Unstructured(Bordes et al.,  2014b ) <C> 34.5/34.3 <C> 2.5/4.2 <C> 6.1/1.9 <C> 6.6/6.6 <R> <C> SE(Bordes et al.,  2011 ) <C> 35.6/34.9 <C> 62.6/14.6 <C> 17.2/68.3 <C> 37.5/41.3 <R> <C> SME(linear)(Bordes et al.,  2014b ) <C> 35.1/32.7 <C> 53.7/14.9 <C> 19.0/61.6 <C> 40.3/43.3 <R> <C> SME(Bilinear)(Bordes et al.,  2014b ) <C> 30.9/28.2 <C> 69.6/13.1 <C> 19.9/76.0 <C> 38.6/41.8 <R> <C> TransE(Bordes et al.,  2013 ) <C> 43.7/43.7 <C> 65.7/19.7 <C> 18.2/66.7 <C> 47.2/50.0 <R> <C> TransH(Wang et al.,  2014 ) <C> 66.8/65.5 <C> 87.6/39.8 <C> 28.7/83.3 <C> 64.5/67.2 <R> <C> TransD(Ji et al.,  2015 ) <C> 86.1/85.4 <C> 95.5/50.6 <C> 39.8/ [BOLD] 94.4 <C> 78.5/81.2 <R> <C> TransR(Lin et al.,  2015b ) <C> 78.8/79.2 <C> 89.2/37.4 <C> 34.1/90.4 <C> 69.2/72.1 <R> <C> CTransR(Lin et al.,  2015b ) <C> 81.5/80.8 <C> 89.0/38.6 <C> 34.7/90.1 <C> 71.2/73.8 <R> <C> CrossE <C> [BOLD] 88.2†/ [BOLD] 87.7† <C> [BOLD] 95.7†/ [BOLD] 75.1† <C> [BOLD] 64.2†/92.3 <C> [BOLD] 88.1†/ [BOLD] 90.8† <R> <C> CrossE [ITALIC] S <C> 78.6/81.6 <C> 85.1/54.2 <C> 45.3 /85.8 <C> 71.7/76.7 <CAP> Table 6. Hit@10 on FB15k by mapping to different relation types.
<R> <C> [BOLD] Models <C> [BOLD] Embedding <C> [BOLD] F1-Score <R> <C> TigXLNet <C> - <C> [BOLD] 83.29 <R> <C> mBERT <C> +random token embed. <C> 76.01 <R> <C> mBERT <C> +word2vec token embed. <C> 77.51 <R> <C> XLNet <C> +random token embed. <C> 77.83 <R> <C> XLNet <C> +word2vec token embed. <C> 81.62 <CAP> Table 1: Fine-tuning TigXLNet, mBERT and XLNet using Tigrinya sentiment analysis dataset.
<R> <C> [BOLD] Models <C> [BOLD] English Books <C> [BOLD] English DVD <C> [BOLD] English Music <C> [BOLD] German Books <C> [BOLD] German DVD <C> [BOLD] German Music <C> [BOLD] French Books <C> [BOLD] French DVD <C> [BOLD] French Music <C> [BOLD] Japanese Books <C> [BOLD] Japanese DVD <C> [BOLD] Japanese Music <C> [BOLD] Avg. <R> <C> XLNet <C> [BOLD] 92.90 <C> [BOLD] 93.31 <C> [BOLD] 92.02 <C> 85.23 <C> 83.30 <C> 83.89 <C> 73.05 <C> 69.80 <C> 70.12 <C> 83.20 <C> [BOLD] 86.07 <C> 85.24 <C> 83.08 <R> <C> mBERT <C> 92.78 <C> 90.30 <C> 91.88 <C> [BOLD] 88.65 <C> [BOLD] 85.85 <C> [BOLD] 90.38 <C> [BOLD] 91.09 <C> [BOLD] 88.57 <C> [BOLD] 93.67 <C> [BOLD] 84.35 <C> 81.77 <C> [BOLD] 87.53 <C> [BOLD] 88.90 <CAP> Table 3: F1-Score on CLS dataset, note that we have used the same hyper-parameters and same dataset size for all models (all train and the unprocessed dataset of CLS is used for training, and the model is evaluated on the given test set)
<R> <C> Language <C> Message Distribution Emoticons <C> Message Distribution No Emoticons <C> Message Distribution Ratio With <C> Emoticon Distribution Positive <C> Emoticon Distribution Negative <C> Emoticon Distribution Ambiguous <R> <C> Swedish <C> 4064 <C> 14975 <C> 0.21 <C> 0.46 <C> 0.27 <C> 0.27 <R> <C> German <C> 21294 <C> 54869 <C> 0.28 <C> 0.65 <C> 0.16 <C> 0.19 <R> <C> Italian <C> 46931 <C> 158858 <C> 0.23 <C> 0.34 <C> 0.50 <C> 0.16 <R> <C> English <C> 18327 <C> 75869 <C> 0.20 <C> 0.40 <C> 0.30 <C> 0.30 <CAP> Table 3: Postings With and Without Emoticons & Proportions of Emoticon Types
<R> <C> [BOLD] dataset <C> [BOLD] N <C> [BOLD] H <C> [BOLD] features <C> [BOLD] A <C> [BOLD] P <C> [BOLD] R <R> <C> majority class baseline <C> majority class baseline <C> majority class baseline <C> majority class baseline <C> 49.98 <C> [EMPTY] <C> [EMPTY] <R> <C> human baseline <C> human baseline <C> human baseline <C> human baseline <C> 23.83 <C> [EMPTY] <C> [EMPTY] <R> <C> SVM, Naive Bayes, Logistic Regression <C> SVM, Naive Bayes, Logistic Regression <C> SVM, Naive Bayes, Logistic Regression <C> SVM, Naive Bayes, Logistic Regression <C> 65% <C> [EMPTY] <C> [EMPTY] <R> <C> scraped <C> 500 <C> 50,50,50 <C> BFS <C> 80.36 <C> c: 79.3 u: 81.0 <C> c: 75.1 u: 77.4 <R> <C> scraped <C> 800 <C> 60,60,60 <C> BFS <C> 80.2 <C> c: 81.5 u: 75.5 <C> c: 79.4 u: 79.2 <R> <C> Zhu et al’s <C> 800 <C> 50,7 <C> BFS <C> 87.63 <C> c: 85.9 u: 86.0 <C> c: 86.0 u: 86.0 <R> <C> Zhu et al’s <C> 800 <C> 30,30 <C> BFS <C> 86.18 <C> c: 85.3 u: 86.2 <C> c: 87.4 u: 86.2 <R> <C> both <C> 800 <C> 60,60,60 <C> BFS <C> 75.4 <C> c: 72.0 u: 70.6 <C> c: 71.9 u: 75.4 <R> <C> both <C> 500 <C> 50,50,50 <C> BFS <C> 73.94 <C> c: 70.7 u: 71.1 <C> c: 73.5 u: 72.0 <R> <C> scraped <C> 800 <C> 30,30,30 <C> all except LIWC & word embeddings <C> 72.95 <C> c: 71.8 u: 73.0 <C> c: 75.6 u: 73.0 <R> <C> Zhu et al’s <C> 800 <C> 60,60,60 <C> all except LIWC & word embeddings <C> 70.64 <C> c: 89.2 u: 76.3 <C> c: 45.4 u: 69.9 <R> <C> both <C> 500 <C> 40,40,40 <C> all except LIWC & word embeddings <C> 84.67 <C> c: 80.2 u: 79.9 <C> c: 80.4 u: 79.9 <R> <C> both <C> 800 <C> 20,20,20 <C> all except LIWC & word embeddings <C> 88.50 <C> c: 84.6 u: 87.0 <C> c: 80.3 u: 81.7 <R> <C> both <C> 800 <C> 30,30,30 <C> all except LIWC & word embeddings <C> 87.04 <C> c: 86.6 u: 86.0 <C> c: 81.9 u: 85.8 <R> <C> both <C> 800 <C> 50,50,50 <C> all except LIWC & word embeddings <C> 87.24 <C> c: 83.5 u: 84.6 <C> c: 80.8 u: 82.6 <CAP> Table 4: MLP classification results. N = number of epochs, H = number of nodes in each hidden layer, A = accuracy, P = precision, R = recall, BFS = best features set, c = censored, u = uncensored
<R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> Dev. fy <C> Dev. nl <C> Dev. fy-nl <C> Dev. all <C> Test fy <C> Test nl <C> Test fy-nl <C> Test all <C> Total <R> <C> # of Frisian words <C> # of Frisian words <C> # of Frisian words <C> # of Frisian words <C> 9190 <C> 0 <C> 2381 <C> 11,571 <C> 10,753 <C> 0 <C> 1798 <C> 12,551 <C> 24,122 <R> <C> # of Dutch words <C> # of Dutch words <C> # of Dutch words <C> # of Dutch words <C> 0 <C> 4569 <C> 533 <C> 5102 <C> 0 <C> 3475 <C> 306 <C> 3781 <C> 8883 <R> <C> ASR System <C> AM train data <C> LM train data <C> Lang. Tag <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Baseline ASR <C> (1) <C> Orig. <C> No <C> 36.4 <C> 43.7 <C> 48.2 <C> 40.3 <C> 31.5 <C> 39.5 <C> 47.9 <C> 35.2 <C> 37.8 <R> <C> Baseline ASR <C> (1) <C> Orig. <C> Yes <C> 37.9 <C> 48.3 <C> 53.3 <C> 43.3 <C> 32.8 <C> 42.5 <C> 51.9 <C> 37.2 <C> 40.3 <R> <C> ASR_AA <C> (1+2) <C> Orig. <C> No <C> 31.1 <C> 35.2 <C> 42.4 <C> 34.1 <C> 28.6 <C> 31.8 <C> 44.0 <C> 31.2 <C> 32.7 <R> <C> ASR_AA <C> (1+2) <C> Orig. <C> Yes <C> 32.9 <C> 37.8 <C> 48.2 <C> 36.8 <C> 29.5 <C> 34.5 <C> 49.4 <C> 33.1 <C> 35.0 <R> <C> ASR_AA_NL <C> (1+2+3) <C> Orig. <C> No <C> 25.8 <C> 27.4 <C> 36.9 <C> 28.1 <C> 24.8 <C> 24.6 <C> 37.6 <C> 26.4 <C> 27.2 <R> <C> ASR_AA_NL <C> (1+2+3) <C> Orig. <C> Yes <C> 27.4 <C> 30.7 <C> 43.1 <C> 30.9 <C> 25.8 <C> 27.8 <C> 44.1 <C> 28.5 <C> 29.7 <R> <C> ASR_AA_NL-VL <C> (1+2+3+4) <C> Orig. <C> No <C> 26.4 <C> 26.9 <C> 36.2 <C> 28.1 <C> 24.5 <C> 23.2 <C> 38.5 <C> 26.0 <C> 27.1 <R> <C> ASR_AA_NL-VL <C> (1+2+3+4) <C> Orig. <C> Yes <C> 27.7 <C> 30.5 <C> 42.7 <C> 31.0 <C> 25.5 <C> 26.1 <C> 43.5 <C> 27.8 <C> 29.4 <R> <C> ASR_AA_NL-VL_CS-LM <C> (1+2+3+4) <C> Orig.+Gen. <C> No <C> 24.6 <C> 26.7 <C> 33.3 <C> 26.5 <C> 22.5 <C> 22.4 <C> 32.9 <C> 23.8 <C> 25.2 <R> <C> ASR_AA_NL-VL_CS-LM <C> (1+2+3+4) <C> Orig.+Gen. <C> Yes <C> 25.9 <C> 30.2 <C> 38.0 <C> 29.1 <C> 23.5 <C> 26.7 <C> 38.6 <C> 26.0 <C> 27.6 <CAP> Table 2: WER (%) obtained on the development and test set of the FAME! Corpus - Different AM training data is identified with the numbers which are defined in Table 1.
<R> <C> Dev. Ref. word <C> Dev. Hyp. word <C> Dev. Count <C> Test Ref. word <C> Test Hyp. word <C> Test Count <R> <C> en-nl <C> en-fy <C> 26 <C> en-nl <C> en-fy <C> 42 <R> <C> de-fy <C> de-nl <C> 24 <C> dat-nl <C> dat-fy <C> 33 <R> <C> dat-nl <C> dat-fy <C> 18 <C> de-fy <C> de-nl <C> 29 <R> <C> wat-nl <C> wat-fy <C> 16 <C> is-nl <C> is-fy <C> 25 <R> <C> het-nl <C> it-fy <C> 15 <C> ja-nl <C> ja-fy <C> 24 <R> <C> dat-fy <C> dat-nl <C> 15 <C> it-fy <C> ’t-nl <C> 24 <R> <C> it-fy <C> ’t-nl <C> 14 <C> de-nl <C> de-fy <C> 23 <R> <C> is-nl <C> is-fy <C> 14 <C> het-nl <C> it-fy <C> 21 <R> <C> de-nl <C> de-fy <C> 13 <C> ja-fy <C> ja-nl <C> 20 <R> <C> in-fy <C> een-nl <C> 12 <C> dat-fy <C> dat-nl <C> 20 <CAP> Table 3: Most frequent (word-language tag) confusions of the best-performing ASR system on the development and test data
<R> <C> [BOLD] Classifier <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1 <C> [BOLD] # of Predicted Tweets <C> [BOLD] # of Estimated Hateful <R> <C> Supervised Baselines <C> Supervised Baselines <C> Supervised Baselines <C> Supervised Baselines <C> Supervised Baselines <C> Supervised Baselines <R> <C> Logistic Regression <C> 0.088 <C> 0.328 <C> 0.139 <C> [BOLD] 1,380,825 <C> 121,512 <R> <C> LSTMs <C> [BOLD] 0.791 <C> 0.132 <C> 0.228 <C> 62,226 <C> 49,221 <R> <C> The Two-path Weakly Supervised Learning System <C> The Two-path Weakly Supervised Learning System <C> The Two-path Weakly Supervised Learning System <C> The Two-path Weakly Supervised Learning System <C> The Two-path Weakly Supervised Learning System <C> The Two-path Weakly Supervised Learning System <R> <C> LSTMs <C> 0.419 <C> 0.546 <C> 0.474 <C> 483,298 <C> 202,521 <R> <C> Slur Matching <C> 0.565 <C> 0.398 <C> 0.468 <C> 261,183 <C> 147,595 <R> <C> Union <C> 0.422 <C> [BOLD] 0.580 <C> [BOLD] 0.489 <C> 509,897 <C> [BOLD] 214,997 <R> <C> Union* <C> 0.626* <C> 0.258* <C> 0.365* <C> - <C> - <R> <C> Variations of the Two-path Weakly Supervised Learning System <C> Variations of the Two-path Weakly Supervised Learning System <C> Variations of the Two-path Weakly Supervised Learning System <C> Variations of the Two-path Weakly Supervised Learning System <C> Variations of the Two-path Weakly Supervised Learning System <C> Variations of the Two-path Weakly Supervised Learning System <R> <C> Slur Matching Only <C> 0.318 <C> 0.143 <C> 0.197 <C> 166,535 <C> 52,958 <R> <C> LSTMs Only <C> 0.229 <C> 0.303 <C> 0.261 <C> 491,421 <C> 112,535 <CAP> Table 2: Performance of Different Models
<R> <C> Methods <C> xIntent <C> xReact <C> oReact <C> Overall <R> <C> EA-VQ-VAE <C> 23.37 <C> 5.83 <C> 4.87 <C> 11.32 <R> <C> - w/o evidence <C> 21.69 <C> 5.36 <C> 4.48 <C> 10.51 <R> <C> - w/o VQ-VAE <C> 21.87 <C> 5.41 <C> 4.60 <C> 10.63 <R> <C> - w/o SL <C> 21.95 <C> 5.54 <C> 4.57 <C> 10.69 <CAP> Table 5: BLEU score on the Event2Mind dev dataset with different approaches. SL is short for separately learning.
<R> <C> Model/Evals <C> FR Top3 <C> FR Top5 <C> F-Top1 <C> F-Top3 <C> F-Top5 <R> <C> Majority Baseline <C> 30.00 <C> 50.00 <C> 12.44 <C> 43.72 <C> 62.24 <R> <C> NRC Model <C> 30.78 <C> 51.60 <C> 23.10 <C> 47.27 <C> 66.16 <R> <C> GloVe Model <C> 32.71 <C> 53.74 <C> 25.95 <C> 51.29 <C> 68.29 <R> <C> Emoji Model <C> [BOLD] 33.17 <C> [BOLD] 54.06 <C> [BOLD] 26.00 <C> [BOLD] 51.43 <C> [BOLD] 68.53 <R> <C> BERT Model <C> [BOLD] 33.54 <C> [BOLD] 56.00 <C> [BOLD] 26.97 <C> [BOLD] 51.91 <C> [BOLD] 69.38 <CAP> Table 1: Experimental results for all five models. FR represents Font Recall and F represents F-1 score. The results in bold are statistically significant compared to the Majority Baseline.
<R> <C> Layers <C> [BOLD] LSTM 1 <C> [BOLD] LSTM 2 <C> [BOLD] LSTM 3 <C> [BOLD] RNN 1 <C> [BOLD] RNN 2 <C> [BOLD] RNN 3 <C> [BOLD] GRU 1 <C> [BOLD] GRU 2 <C> [BOLD] GRU 3 <R> <C> Size <C> War and Peace Dataset <C> War and Peace Dataset <C> War and Peace Dataset <C> War and Peace Dataset <C> War and Peace Dataset <C> War and Peace Dataset <C> War and Peace Dataset <C> War and Peace Dataset <C> War and Peace Dataset <R> <C> 64 <C> 1.449 <C> 1.442 <C> 1.540 <C> 1.446 <C> 1.401 <C> 1.396 <C> 1.398 <C> [BOLD] 1.373 <C> 1.472 <R> <C> 128 <C> 1.277 <C> 1.227 <C> 1.279 <C> 1.417 <C> 1.286 <C> 1.277 <C> 1.230 <C> [BOLD] 1.226 <C> 1.253 <R> <C> 256 <C> 1.189 <C> [BOLD] 1.137 <C> 1.141 <C> 1.342 <C> 1.256 <C> 1.239 <C> 1.198 <C> 1.164 <C> 1.138 <R> <C> 512 <C> 1.161 <C> 1.092 <C> 1.082 <C> - <C> - <C> - <C> 1.170 <C> 1.201 <C> [BOLD] 1.077 <R> <C> Linux Kernel Dataset <C> Linux Kernel Dataset <C> Linux Kernel Dataset <C> Linux Kernel Dataset <C> Linux Kernel Dataset <C> Linux Kernel Dataset <C> Linux Kernel Dataset <C> Linux Kernel Dataset <C> Linux Kernel Dataset <C> Linux Kernel Dataset <R> <C> 64 <C> 1.355 <C> [BOLD] 1.331 <C> 1.366 <C> 1.407 <C> 1.371 <C> 1.383 <C> 1.335 <C> 1.298 <C> 1.357 <R> <C> 128 <C> 1.149 <C> 1.128 <C> 1.177 <C> 1.241 <C> [BOLD] 1.120 <C> 1.220 <C> 1.154 <C> 1.125 <C> 1.150 <R> <C> 256 <C> 1.026 <C> [BOLD] 0.972 <C> 0.998 <C> 1.171 <C> 1.116 <C> 1.116 <C> 1.039 <C> 0.991 <C> 1.026 <R> <C> 512 <C> 0.952 <C> 0.840 <C> 0.846 <C> - <C> - <C> - <C> 0.943 <C> 0.861 <C> [BOLD] 0.829 <CAP> Figure 1: Left: The test set cross-entropy loss for all models and datasets (low is good). Models in each row have nearly equal number of parameters. The test set has 300,000 characters. The standard deviation, estimated with 100 bootstrap samples, is less than 4×10−3 in all cases. Right: A t-SNE embedding based on the probabilities assigned to test set characters by each model on War and Peace. The color, size, and marker correspond to model type, model size, and number of layers.
<R> <C> Model [ITALIC] n <C> 1 <C> 2 <C> 3 <C> 4 <C> 5 <C> 6 <C> 7 <C> 8 <C> 9 <C> 20 <R> <C> War and Peace Dataset <C> War and Peace Dataset <C> War and Peace Dataset <C> War and Peace Dataset <C> War and Peace Dataset <C> War and Peace Dataset <C> War and Peace Dataset <C> War and Peace Dataset <C> War and Peace Dataset <C> War and Peace Dataset <C> War and Peace Dataset <R> <C> [ITALIC] n-gram <C> 2.399 <C> 1.928 <C> 1.521 <C> 1.314 <C> 1.232 <C> 1.203 <C> [BOLD] 1.194 <C> 1.194 <C> 1.194 <C> 1.195 <R> <C> [ITALIC] n-NN <C> 2.399 <C> 1.931 <C> 1.553 <C> 1.451 <C> 1.339 <C> [BOLD] 1.321 <C> - <C> - <C> - <C> - <R> <C> Linux Kernel Dataset <C> Linux Kernel Dataset <C> Linux Kernel Dataset <C> Linux Kernel Dataset <C> Linux Kernel Dataset <C> Linux Kernel Dataset <C> Linux Kernel Dataset <C> Linux Kernel Dataset <C> Linux Kernel Dataset <C> Linux Kernel Dataset <C> Linux Kernel Dataset <R> <C> [ITALIC] n-gram <C> 2.702 <C> 1.954 <C> 1.440 <C> 1.213 <C> 1.097 <C> 1.027 <C> 0.982 <C> 0.953 <C> 0.933 <C> [BOLD] 0.889 <R> <C> [ITALIC] n-NN <C> 2.707 <C> 1.974 <C> 1.505 <C> 1.395 <C> [BOLD] 1.256 <C> 1.376 <C> - <C> - <C> - <C> - <CAP> Table 2: The test set cross-entropy loss on both datasets for n-gram models (low is good). The standard deviation estimate using 100 bootstrap samples is below 4×10−3 in all cases.
<R> <C> Model <C> En→De <C> En→Fr <R> <C> Transformer (6B)† <C> 28.40 <C> 41.80 <R> <C> Transformer (6B) <C> 28.91 <C> 42.69 <R> <C> Transformer (8B) <C> 28.75 <C> 42.63 <R> <C> Transformer (10B) <C> 28.63 <C> 42.73 <R> <C> Transparent Attn (16B)† <C> 28.04 <C> − <R> <C> [BOLD] Ours (8B) <C> [BOLD] 29.92 <C> [BOLD] 43.27 <CAP> Table 1: The test set performances of WMT14 En→De and En→Fr translation tasks. ‘†’ denotes the performance figures reported in the previous works.
<R> <C> 3  [BOLD] Model <C> [BOLD] Mean Cosine Similarity <R> <C> Word2Vec <C> 0.847 <R> <C> Our Mode <C> 0.795 <R> <C> 3 <C> [EMPTY] <CAP> Table 1: Result Table
<R> <C> Encoder model type <C> Transformer <R> <C> Decoder model type <C> Transformer <R> <C> # Enc. & dec. layers <C> 6 <R> <C> Heads <C> 8 <R> <C> Hidden layer size <C> 512 <R> <C> Word embedding size <C> 512 <R> <C> Batch size <C> 32 <R> <C> Optimizer <C> Adam <R> <C> Learning rate <C> 1.0 <R> <C> Warmup steps <C> 20,000 <R> <C> Maximum training steps <C> 300,000 <R> <C> Validation steps <C> 10,000 <R> <C> Position Encoding <C> True <R> <C> Share Embeddings <C> True <R> <C> Share Decoder Embeddings <C> True <R> <C> Dropout <C> 0.2 (DE-EN) <R> <C> Dropout <C> 0.1 (JA-EN) <CAP> Table 1: Hyperparameters shared across models
<R> <C> [BOLD] Experiment <C> [BOLD] DE−EN  [BOLD] Dev <C> [BOLD] DE−EN  [BOLD] Test <C> [BOLD] JA−EN  [BOLD] Dev <C> [BOLD] JA−EN  [BOLD] Test <R> <C> Teacher Forcing Baseline <C> 35.05 <C> [BOLD] 29.62 <C> 18.00 <C> 19.46 <R> <C> [BOLD] No backprop <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Argmax <C> 23.99 <C> 20.57 <C> 12.88 <C> 15.13 <R> <C> Top-k mix <C> 35.19 <C> 29.42 <C> [BOLD] 18.46 <C> 20.24 <R> <C> Softmax mix  [ITALIC] α=1 <C> 35.07 <C> 29.32 <C> 17.98 <C> 20.03 <R> <C> Softmax mix  [ITALIC] α=10 <C> 35.30 <C> 29.25 <C> 17.79 <C> 19.67 <R> <C> Gumbel Softmax mix  [ITALIC] α=1 <C> [BOLD] 35.36 <C> 29.48 <C> 18.31 <C> 20.21 <R> <C> Gumbel Softmax mix  [ITALIC] α=10 <C> 35.32 <C> 29.58 <C> 17.94 <C> [BOLD] 20.87 <R> <C> Sparsemax mix <C> 35.22 <C> 29.28 <C> 18.14 <C> 20.15 <R> <C> [BOLD] Backprop through model decisions <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Softmax mix  [ITALIC] α=1 <C> 33.25 <C> 27.60 <C> 15.67 <C> 17.93 <R> <C> Softmax mix  [ITALIC] α=10 <C> 27.06 <C> 23.29 <C> 13.49 <C> 16.02 <R> <C> Gumbel Softmax mix  [ITALIC] α=1 <C> 30.57 <C> 25.71 <C> 15.86 <C> 18.76 <R> <C> Gumbel Softmax mix  [ITALIC] α=10 <C> 12.79 <C> 10.62 <C> 13.98 <C> 17.09 <R> <C> Sparsemax mix <C> 24.65 <C> 20.15 <C> 12.44 <C> 16.23 <CAP> Table 2: Experiments with scheduled sampling for Transformer. The table shows BLEU score for the best checkpoint on BLEU, measured on the validation set. The first group of experiments do not have a backpropagation pass through the first decoder. The results from the second group are from model runs with backpropagation pass through the second as well as through the first decoder.
<R> <C> [BOLD] Model <C> [BOLD] RI <C> [BOLD] Loss Function in RC <C> [BOLD] P% <C> [BOLD] R% <C> [BOLD] F1% <R> <C> Baseline+Tag <C> \texttimes <C> Ranking Loss <C> 61.8 <C> 62.7 <C> 61.4 <R> <C> Baseline+Tag <C> \texttimes <C> Cross-entropy Loss <C> 67.7 <C> 57.8 <C> 61.5 <R> <C> Baseline+Tag <C> \texttimes <C> Cross-entropy Loss + Ranking Loss <C> 63.2 <C> 62.1 <C> 61.7 <R> <C> Baseline+MTL+Tag <C> ✓ <C> Ranking Loss <C> 61.3 <C> 65.8 <C> 62.9 <R> <C> Baseline+MTL+Tag <C> ✓ <C> Cross-entropy Loss <C> 61.6 <C> 62.0 <C> 62.0 <CAP> Table 4: Evaluating the effect of the loss function used in relation classification w/o multi-tasking using ACE 2005 Chinese corpus. RC stands for relation classification and RI stands for relation identification.
<R> <C> Method <C> Decoding <C> ppl↓ <C> BLEU↑ <C> distinct-1(%)↑ <C> distinct-2(%)↑ <C> distinct-3(%)↑ <R> <C> MLE <C> BS <C> [BOLD] 19.69 <C> 3.91 <C> 1.20 <C> 5.54 <C> 10.62 <R> <C> MLE <C> diverse BS <C> [BOLD] 19.69 <C> [BOLD] 3.97 <C> 1.20 <C> 5.87 <C> 11.86 <R> <C> LogProb Avg <C> BS <C> 21.73 <C> 3.12 <C> 1.89 <C> 10.84 <C> 22.32 <R> <C> LogProb Avg <C> diverse BS <C> 21.06 <C> 3.16 <C> 1.87 <C> 11.03 <C> 23.37 <R> <C> Logits Avg <C> BS <C> 20.72 <C> 2.43 <C> 1.90 <C> 12.27 <C> 26.85 <R> <C> Logits Avg <C> diverse BS <C> 20.16 <C> 2.43 <C> [BOLD] 1.91 <C> [BOLD] 12.48 <C> [BOLD] 27.98 <R> <C> Human <C> - <C> - <C> - <C> 3.66 <C> 28.89 <C> 60.67 <CAP> Table 2: Automatic evaluation results. “BS” is short for “beam search”. “MLE” uses token-level training as stated in Section 3.1. “LogProb Avg” uses average log-probability as the score for sequence-level training, while “Logits Avg” uses average logits as the score (Equation 5).
<R> <C> [EMPTY] <C> MLE <C> LogProb Avg <C> Logits Avg <R> <C> Mean Rank↓ <C> 44.7 <C> 43.9 <C> [BOLD] 35.1 <CAP> Table 4: Mean rank of groundtruth among 50 context-agnostic distractors. Lower mean rank indicates the model has less label bias. See Section 3.4 for more details.
<R> <C> Method <C> Specificity <C> Sensibleness <C> SSA <R> <C> MLE <C> 0.54 <C> 0.88 <C> 0.71 <R> <C> LogProb Avg <C> 0.68 <C> 1.00 <C> 0.84 <R> <C> Logits Avg <C> [BOLD] 1.06 <C> [BOLD] 1.24 <C> [BOLD] 1.15 <R> <C> Human <C> 1.60 <C> 1.47 <C> 1.53 <CAP> Table 5: Human evaluation results. The scores are averaged over two annotators and 200 dialogue turns, and are in the range of 0 to 2. All methods adopt diverse beam search as the decoding algorithm since it shows slightly better performance on automatic evaluation metrics. “SSA” is the arithmetic mean of specificity score and sensibleness score.
<R> <C> Corpus <C> fold 1 <C> fold 2 <C> fold 3 <C> fold 4 <C> fold 5 <C> average <R> <C> Art <C> 0.78 <C> 0.74 <C> 0.86 <C> 0.87 <C> 0.92 <C> 0.83 <R> <C> Cinema <C> 0.87 <C> 0.90 <C> 0.79 <C> 0.77 <C> 0.82 <C> 0.83 <CAP> Table 1: Accuracy results
<R> <C> [EMPTY] <C> Δ len <C> Δ nov-1 <C> Δ nov-3 <C> Δ rep-1 <C> Δ rep-3 <C> ROUGE-1 <C> ROUGE-L <C> BLEU-1 <R> <C> UniLM <C> -12.11 <C> 27.16 <C> 5.49 <C> -6.87 <C> [BOLD] 0.19 <C> 18.66 <C> [BOLD] 15.49 <C> 16.91 <R> <C> UniLM (no rules) <C> -13.11 <C> 30.16 <C> 5.69 <C> -7.87 <C> -3.77 <C> 18.76 <C> 14.49 <C> 17.14 <R> <C> DAS-naive <C> -10.76 <C> 19.68 <C> 4.58 <C> -10.81 <C> -5.05 <C> 18.19 <C> 13.30 <C> 15.41 <R> <C> DAS <C> [BOLD] -2.72 <C> [BOLD] 19.05 <C> [BOLD] 1.01 <C> [BOLD] -3.42 <C> -1.33 <C> [BOLD] 19.76 <C> 14.92 <C> [BOLD] 17.59 <CAP> Table 5: Results on TL;DR test set for our proposed model in transfer learning scenarios.
<R> <C> Activation function in all MLPs <C> tanh <R> <C> Optimizer <C> Adam <R> <C> Learning rate <C> 0.001 <R> <C> [BOLD] Epochs <C> [BOLD] 100 <R> <C> [BOLD] Dim of lemma embeddings <C> [BOLD] 0 <R> <C> Dim of POS embeddings <C> 32 <R> <C> Dim of NE embeddings <C> 16 <R> <C> Hidden layers in all MLPs <C> 1 <R> <C> Hidden units in LSTM (per direction) <C> 256 <R> <C> Hidden units in edge existence MLP <C> 256 <R> <C> Hidden units in edge label MLP <C> 256 <R> <C> Hidden units in supertagger MLP <C> 1024 <R> <C> Hidden units in lexical label tagger MLP <C> 1024 <R> <C> Layer dropout in LSTMs <C> 0.3 <R> <C> Recurrent dropout in LSTMs <C> 0.4 <R> <C> Input dropout <C> 0.3 <R> <C> Dropout in edge existence MLP <C> 0.0 <R> <C> Dropout in edge label MLP <C> 0.0 <R> <C> Dropout in supertagger MLP <C> 0.4 <R> <C> Dropout in lexical label tagger MLP <C> 0.4 <CAP> Table 6: Common hyperparameters used in all experiments. Deviations from L’19 are highlighted with boldface.
<R> <C> [BOLD] Models <C> Info <C> Rdnd <C> Overall <R> <C> NN-SE <C> 1.36 <C> 1.29 <C> 1.39 <R> <C> [BOLD] NeuSum <C> [BOLD] 1.33 <C> [BOLD] 1.21 <C> [BOLD] 1.34 <CAP> Table 3: Rankings of NeuSum and NN-SE in terms of informativeness (Info), redundancy (Rdnd) and overall quality by human participants (lower is better).
<R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> Number of Categories 30 <C> Number of Categories 50 <C> Number of Categories 70 <C> Number of Categories 90 <C> Number of Categories 110 <R> <C> Category Coverage (%) <C> 40 <C> Random <C> 4.9 <C> 5.5 <C> 6.0 <C> 6.4 <C> 6.7 <R> <C> Category Coverage (%) <C> 40 <C> GloVe <C> 5.6 <C> 6.8 <C> 7.7 <C> 8.3 <C> 8.9 <R> <C> Category Coverage (%) <C> 40 <C> I∗ <C> 25.9 <C> 33.6 <C> 40.2 <C> 44.8 <C> 49.1 <R> <C> Category Coverage (%) <C> 40 <C> I <C> 34.2 <C> 45.2 <C> 55.5 <C> 62.9 <C> 69.2 <R> <C> Category Coverage (%) <C> 60 <C> Random <C> 4.5 <C> 4.9 <C> 5.3 <C> 5.6 <C> 5.8 <R> <C> Category Coverage (%) <C> 60 <C> GloVe <C> 6.7 <C> 7.8 <C> 9.0 <C> 9.7 <C> 10.2 <R> <C> Category Coverage (%) <C> 60 <C> I∗ <C> 27.6 <C> 35.8 <C> 42.4 <C> 47.7 <C> 51.6 <R> <C> Category Coverage (%) <C> 60 <C> I <C> 36.1 <C> 48.4 <C> 59.0 <C> 67.0 <C> 72.8 <R> <C> Category Coverage (%) <C> 80 <C> Random <C> 4.2 <C> 4.6 <C> 4.9 <C> 5.1 <C> 5.3 <R> <C> Category Coverage (%) <C> 80 <C> GloVe <C> 7.6 <C> 8.9 <C> 9.7 <C> 10.4 <C> 11.0 <R> <C> Category Coverage (%) <C> 80 <C> I∗ <C> 30.2 <C> 31.1 <C> 43.2 <C> 48.1 <C> 52.0 <R> <C> Category Coverage (%) <C> 80 <C> I <C> 39.8 <C> 50.7 <C> 60.1 <C> 67.4 <C> 73.2 <R> <C> Category Coverage (%) <C> 100 <C> Random <C> 4.3 <C> 4.6 <C> 4.8 <C> 5.0 <C> 5.1 <R> <C> Category Coverage (%) <C> 100 <C> GloVe <C> 8.4 <C> 9.8 <C> 10.8 <C> 11.4 <C> 12.0 <R> <C> Category Coverage (%) <C> 100 <C> I∗ <C> 30.3 <C> 37.7 <C> 43.4 <C> 48.1 <C> 51.5 <R> <C> Category Coverage (%) <C> 100 <C> I <C> 38.9 <C> 49.9 <C> 59.0 <C> 65.7 <C> 71.3 <CAP> TABLE III: Average Interpretability Scores (%) for λ=5. Results are averaged across 10 independent selections of categories for each category coverage.
<R> <C> [EMPTY] <C> [BOLD] CrowdFlower <C> [BOLD] Legal experts <R> <C> [BOLD] Tagging task (step 1) <C> 0.35 <C> 0.10 <R> <C> [BOLD] Rating Task (step 2) <C> 0.75 <C> 0.24 <CAP> Table 3: Reuse rate in annotation tasks.
<R> <C> [EMPTY] <C> [BOLD] Keyword <C> [BOLD] Keyword in segment  [BOLD] 0.72 <C> [BOLD] Keyword absent 0.00 <R> <C> [BOLD] Online LDA <C> [BOLD] Most Likely <C> 0.21 <C> 0.13 <R> <C> [BOLD] Online LDA <C> [BOLD] Highest Rank <C> 0.48 <C> 0.14 <R> <C> [BOLD] Online LDA <C> [BOLD] Top 30 Rank <C> 0.67 <C> [BOLD] 0.30 <R> <C> [BOLD] Gibbs Sampling- LDA <C> [BOLD] Most Likely <C> 0.00 <C> 0.12 <R> <C> [BOLD] Gibbs Sampling- LDA <C> [BOLD] Highest Rank <C> 0.00 <C> 0.01 <R> <C> [BOLD] Gibbs Sampling- LDA <C> [BOLD] Top 30 Rank <C> 0.00 <C> 0.03 <R> <C> [BOLD] Gibbs Sampling- LDA <C> [BOLD] Concrete Assignment <C> 0.21 <C> 0.19 <CAP> Table 4: F1-scores against CrowdFlower annotations for each method, based on presence or absence of the queried concept keyword in the textual segment.
<R> <C> [EMPTY] <C> Estimate <C> Error <C> 1-95% CI <C> u-95% CI <R> <C> Intercept <C> -0.60 <C> 0.15 <C> -0.90 <C> -0.30 <R> <C> LM_Out <C> 1.90 <C> 0.23 <C> 1.45 <C> 2.37 <R> <C> AddParen <C> -2.03 <C> 0.58 <C> -3.19 <C> -0.95 <R> <C> Arithmetic <C> 0.49 <C> 0.25 <C> -0.01 <C> 0.96 <R> <C> Relational <C> 0.15 <C> 0.26 <C> -0.39 <C> 0.68 <R> <C> RMParen* <C> 1.39 <C> – <C> – <C> – <R> <C> LM_Out:AddParen <C> -0.79 <C> 0.42 <C> -1.63 <C> 0.04 <R> <C> LM_Out:Arithmetic <C> -1.03 <C> 0.37 <C> -1.74 <C> -0.32 <R> <C> LM_Out:Relational <C> 1.64 <C> 0.45 <C> 0.77 <C> 2.50 <R> <C> LM_Out:RMParen* <C> 0.18 <C> – <C> – <C> – <CAP> Table 5. Fixed effects for bayesian mixed effects logistic regression on our human subject study. *Parenthesis removal (RMParen) does not get an independent coefficient estimate in deviation coding, so we calculate the implied coefficient.
<R> <C> Models <C> Test(%acc) <C> N <C> E <C> C <R> <C> LSTM# <C> 53.5 <C> [BOLD] 39.2 <C> 63.1 <C> 53.5 <R> <C> WbW-Attention# <C> 53.9 <C> 30.2 <C> 61.3 <C> 66.5 <R> <C> SE# <C> [BOLD] 56.3 <C> 30.6 <C> 48.3 <C> 71.2 <R> <C> ESIM (our_imp) <C> 59.0 <C> 34.1 <C> 68.3 <C> 65.1 <R> <C> MIMN <C> [BOLD] 66.0 <C> 35.3 <C> [BOLD] 77.9 <C> [BOLD] 73.1 <R> <C> MIMN-memory <C> 61.6 <C> 28.4 <C> 72.7 <C> 70.8 <R> <C> MIMN-gate+ReLU <C> 64.8 <C> 37.5 <C> 77.9 <C> 69.1 <CAP> Table 3: Performance on MPE. Models with # are reported from [14].
<R> <C> Models <C> Valid(%acc) <C> Test(%acc) <R> <C> Majority class⋆ <C> 63.3 <C> 60.3 <R> <C> decomposable attention⋆ <C> 75.4 <C> 72.3 <R> <C> ESIM⋆ <C> 70.5 <C> 70.6 <R> <C> Ngram⋆ <C> 65.0 <C> 70.6 <R> <C> DGEM⋆ <C> 79.6 <C> 77.3 <R> <C> CAFE  <C> - <C> 83.3 <R> <C> MIMN <C> 84.7 <C> [BOLD] 84.0 <R> <C> MIMN-memory <C> 81.3 <C> 82.2 <R> <C> MIMN-gate+ReLU <C> 83.4 <C> 83.5 <CAP> Table 3: Performance on MPE. Models with # are reported from [14].
<R> <C> [BOLD] Method <C> [BOLD] Model <C> [BOLD] Corpus <C> [BOLD] Perplexity <C> [BOLD] Entropy <R> <C> Abs <C> 2-gram <C> BNC <C> 252.15 <C> 7.96 <R> <C> Abs <C> 2-gram <C> EnWiki <C> 251.82 <C> 7.98 <R> <C> Abs <C> 3-gram <C> BNC <C> 156.01 <C> 7.26 <R> <C> Abs <C> 3-gram <C> EnWiki <C> 113.94 <C> 6.83 <R> <C> KNS <C> 2-gram <C> BNC <C> 242.57 <C> 7.91 <R> <C> KNS <C> 2-gram <C> EnWiki <C> 246.43 <C> 7.94 <R> <C> KNS <C> 3-gram <C> BNC <C> 139.46 <C> 7.10 <R> <C> KNS <C> 3-gram <C> EnWiki <C> 104.76 <C> 6.71 <R> <C> MKNS <C> 2-gram <C> BNC <C> 241.18 <C> 7.90 <R> <C> MKNS <C> 2-gram <C> EnWiki <C> 245.96 <C> 7.94 <R> <C> MKNS <C> 3-gram <C> BNC <C> 136.82 <C> 7.08 <R> <C> MKNS <C> 3-gram <C> EnWiki <C> 103.72 <C> 6.69 <R> <C> MDKNSPOMD <C> 2-gram <C> BNC <C> 241.17 <C> 7.90 <R> <C> MDKNSPOMD <C> 2-gram <C> EnWiki <C> 245.98 <C> 7.94 <R> <C> MDKNSPOMD <C> 3-gram <C> BNC <C> 137.50 <C> 7.08 <R> <C> MDKNSPOMD <C> 3-gram <C> EnWiki <C> 104.18 <C> 6.70 <CAP> Table 5: Detailed results of the tested smoothing algorithms.
<R> <C> Layer <C> Output: fmaps × f × T <R> <C> Input window <C> 3×64×48 <R> <C> conv 7×7 <C> 64×64×42 <R> <C> pool 2×1 <C> 64×32×42 <R> <C> conv 3×3 <C> 64×32×40 <R> <C> conv 3×3 <C> 64×32×38 <R> <C> conv 3×3 <C> 64×32×36 <R> <C> pool 2×1 <C> 64×16×36 <R> <C> conv 3×3 <C> 128×16×34 <R> <C> conv 3×3 <C> 128×16×32 <R> <C> conv 3×3 <C> 128×16×30 <R> <C> pool 2×1 <C> 128×8×30 <R> <C> conv 3×3 <C> 256×8×28 <R> <C> conv 3×3 <C> 256×8×26 <R> <C> conv 3×3 <C> 256×8×24 <R> <C> pool 2×2 <C> 256×4×12 <R> <C> conv 3×3 <C> 512×4×10 <R> <C> conv 3×3 <C> 512×4×8 <R> <C> conv 3×3 <C> 512×4×6 <R> <C> pool 2×2 <C> 512×2×3 <R> <C> 3× FC <C> 2048 <R> <C> FC <C> 1024 <R> <C> FC <C> 32000 <CAP> Table 1: CNN architecture.
<R> <C> [EMPTY] <C> SWB XE <C> SWB ST <C> CH XE <C> CH ST <R> <C> Classic 512 CNN  soltau2014joint  <C> 12.6 <C> 10.4 <C> [EMPTY] <C> [EMPTY] <R> <C> IBM 2016 RNN+VGG+LSTM  saon2016ibm  <C> [EMPTY] <C> 8.6 † <C> [EMPTY] <C> 14.4 † <R> <C> MSR 2016 ResNet *  xiong2016microsoft  <C> [EMPTY] <C> 8.9 <C> [EMPTY] <C> [EMPTY] <R> <C> MSR 2016 LACE *  xiong2016microsoft  <C> [EMPTY] <C> 8.6 <C> [EMPTY] <C> [EMPTY] <R> <C> MSR 2016 BLSTM *  xiong2016microsoft  <C> [EMPTY] <C> 8.7 <C> [EMPTY] <C> [EMPTY] <R> <C> VGG (pool, inefficient)  saon2016ibm  <C> 10.2 <C> 9.4 <C> 16.3 <C> 16.0 <R> <C> VGG (no pool)  sercu2016advances  <C> 10.8 <C> 9.7 <C> 17.1 <C> 16.7 <R> <C> VGG-10 + BN (no pool)  sercu2016advances  <C> 10.8 <C> 9.5 <C> 17.0 <C> 16.3 <R> <C> VGG-13 + BN (no pool) <C> 10.3 <C> 9.0 <C> 16.5 <C> 16.4 <R> <C> VGG-13 + BN + pool <C> 9.5 <C> [BOLD] 8.5 <C> 15.1 <C> 15.4 <R> <C> VGG-13 + BN + pool (uncouple CH acwt) <C> [EMPTY] <C> [EMPTY] <C> [BOLD] 14.8 <C> 15.2 <CAP> Table 1: CNN architecture.
<R> <C> [EMPTY] <C> SWB <C> CH <R> <C> IBM 2015 DNN+RNN+CNN  saon2015ibm  <C> 8.8 † <C> 15.3 † <R> <C> IBM 2016 RNN+VGG+LSTM  saon2016ibm  <C> 7.6 † <C> 13.7 † <R> <C> MSR 2016 ResNet  xiong2016microsoft  <C> 8.6 <C> 14.8 <R> <C> MSR 2016 LACE  xiong2016microsoft  <C> 8.3 <C> 14.8 <R> <C> MSR 2016 BLSTM  xiong2016microsoft  <C> 8.7 <C> 16.2 <R> <C> VGG-13 + BN (no pool) <C> 8.1 <C> 15.9 <R> <C> VGG-13 + BN + pool <C> [BOLD] 7.7 <C> 14.5 <R> <C> VGG-13 + BN + pool (uncouple CH acwt) <C> [EMPTY] <C> [BOLD] 14.4 <CAP> Table 1: CNN architecture.
<R> <C> [BOLD] Domain  [BOLD] Methods <C> [BOLD] Laptop  [BOLD] EM <C> [BOLD] F1 <C> [BOLD] Rest.  [BOLD] EM <C> [BOLD] F1 <R> <C> DrQA <C> 28.5 <C> 36.6 <C> 41.6 <C> 50.3 <R> <C> DrQA+CoQA(supervised) <C> 40.4 <C> 51.4 <C> 47.7 <C> 58.5 <R> <C> BERT <C> 38.57 <C> 48.67 <C> 46.87 <C> 55.07 <R> <C> BERT+review <C> 34.53 <C> 43.83 <C> 47.23 <C> 53.7 <R> <C> BERT+CoQA(supervised) <C> 47.1 <C> 58.9 <C> 56.57 <C> 67.97 <R> <C> BERT+Pre-tuning <C> 46.0 <C> 57.23 <C> 54.57 <C> 64.43 <CAP> Table 3: RCRC on EM (Exact Match) and F1.
<R> <C> Ranked Responses <C> Flag <R> <C> 1. "does n’t seem to, no" <C> 1 <R> <C> 2. "you can log in but not transfer files ?" <C> 0 <CAP> Table 5: Example showing the ranked responses from the LSTM. Each utterance is shown after pre-processing steps.
<R> <C> [EMPTY] <C> [EMPTY] <C> Trial Apple Inc. <C> Test Airbus <C> Test GM <C> Test Stock <C> Test Total <R> <C> English (SemEval-2015) <C> # documents <C> 30 <C> 30 <C> 30 <C> 30 <C> 90 <R> <C> English (SemEval-2015) <C> # sentences <C> 463 <C> 446 <C> 430 <C> 459 <C> 1,335 <R> <C> English (SemEval-2015) <C> # tokens <C> 10,343 <C> 9,909 <C> 10,058 <C> 9,916 <C> 29,893 <R> <C> English (SemEval-2015) <C> # event mentions <C> 178 <C> 268 <C> 213 <C> 276 <C> 757 <R> <C> English (SemEval-2015) <C> # event instances <C> 165 <C> 181 <C> 173 <C> 231 <C> 585 <R> <C> English (SemEval-2015) <C> # target entities <C> 6 <C> 13 <C> 12 <C> 13 <C> 38 <R> <C> English (SemEval-2015) <C> # timelines <C> 6 <C> 13 <C> 11 <C> 13 <C> 37 <R> <C> English (SemEval-2015) <C> # event mentions / timeline <C> 29.7 <C> 20.6 <C> 19.4 <C> 21.2 <C> 20.5 <R> <C> English (SemEval-2015) <C> # event instances / timeline <C> 27.5 <C> 13.9 <C> 15.7 <C> 17.8 <C> 15.8 <R> <C> English (SemEval-2015) <C> # docs / timeline <C> 5.7 <C> 5.2 <C> 4.1 <C> 9.1 <C> 6.2 <R> <C> Spanish <C> # documents <C> 30 <C> 30 <C> 30 <C> 30 <C> 90 <R> <C> Spanish <C> # sentences <C> 454 <C> 445 <C> 431 <C> 467 <C> 1,343 <R> <C> Spanish <C> # tokens <C> 10,865 <C> 10,989 <C> 11,058 <C> 11,341 <C> 33,388 <R> <C> Spanish <C> # event mentions <C> 187 <C> 222 <C> 195 <C> 244 <C> 661 <R> <C> Spanish <C> # event instances <C> 149 <C> 163 <C> 147 <C> 212 <C> 522 <R> <C> Spanish <C> # target entities <C> 6 <C> 13 <C> 12 <C> 13 <C> 38 <R> <C> Spanish <C> # timelines <C> 6 <C> 13 <C> 11 <C> 13 <C> 37 <R> <C> Spanish <C> # event mentions / timeline <C> 31.2 <C> 17.1 <C> 17.7 <C> 18.8 <C> 17.9 <R> <C> Spanish <C> # event instances / timeline <C> 24.8 <C> 12.5 <C> 13.4 <C> 16.3 <C> 14.0 <R> <C> Spanish <C> # docs / timeline <C> 5.5 <C> 4.8 <C> 3.7 <C> 8.5 <C> 5.8 <R> <C> Cross-lingual <C> # documents <C> 60 <C> 60 <C> 60 <C> 60 <C> 180 <R> <C> Cross-lingual <C> # sentences <C> 917 <C> 891 <C> 861 <C> 926 <C> 2,678 <R> <C> Cross-lingual <C> # tokens <C> 21,208 <C> 20,898 <C> 21,116 <C> 21,257 <C> 63,271 <R> <C> Cross-lingual <C> # events mentions <C> 364 <C> 490 <C> 408 <C> 520 <C> 1,418 <R> <C> Cross-lingual <C> # event instance <C> 165 <C> 181 <C> 174 <C> 231 <C> 586 <R> <C> Cross-lingual <C> # target entities <C> 6 <C> 13 <C> 12 <C> 13 <C> 38 <R> <C> Cross-lingual <C> # timelines <C> 6 <C> 13 <C> 11 <C> 13 <C> 37 <R> <C> Cross-lingual <C> # events / timeline <C> 60.7 <C> 37.7 <C> 37.1 <C> 40.0 <C> 38.3 <R> <C> Cross-lingual <C> # event chains / timeline <C> 27.5 <C> 13.9 <C> 15.8 <C> 16.2 <C> 15.8 <R> <C> Cross-lingual <C> # docs / timeline <C> 11.5 <C> 10.0 <C> 8.2 <C> 17.6 <C> 12.1 <CAP> Table 1: Counts extracted from the Multilingual and Cross-lingual gold datasets.
<R> <C> System <C> P <C> R <C> F1 <R> <C> SPINOZAVU-RUN-1 <C> 7.95 <C> 1.96 <C> 3.15 <R> <C> SPINOZAVU-RUN-2 <C> 8.16 <C> 0.56 <C> 1.05 <R> <C> WHUNLP_1 <C> 14.10 <C> 4.90 <C> 7.28 <R> <C> [ITALIC] OC_SPINOZA_VU <C> - <C> - <C> 7.12 <R> <C> [ITALIC] WHUNLP_1 <C> 14.59 <C> 5.37 <C> 7.85 <R> <C> [BOLD] BTE <C> [BOLD] 24.56 <C> 4.35 <C> 7.39 <R> <C> [BOLD] DLT <C> 21.00 <C> [BOLD] 11.01 <C> [BOLD] 14.45 <CAP> Table 3: Results on the SemEval-2015 task
<R> <C> Scorer <C> System <C> English P <C> English R <C> English F1 <C> Spanish P <C> Spanish R <C> Spanish F1 <R> <C> SemEval-2015 <C> [BOLD] BTE <C> 24.56 <C> 4.35 <C> 7.39 <C> 12.07 <C> 4.25 <C> 6.29 <R> <C> SemEval-2015 <C> [BOLD] DLT <C> 21.00 <C> 11.01 <C> 14.45 <C> 12.77 <C> 8.60 <C> 10.28 <R> <C> strict-evaluation <C> [BOLD] BTE <C> 24.56 <C> 3.62 <C> 6.32 <C> 12.07 <C> 3.60 <C> 5.55 <R> <C> strict-evaluation <C> [BOLD] DLT <C> 21.00 <C> 9.18 <C> 12.77 <C> 12.77 <C> 7.29 <C> 9.28 <R> <C> relaxed-evaluation <C> [BOLD] BTE <C> 24.12 <C> 5.32 <C> 8.71 <C> 11.55 <C> 5.18 <C> 7.15 <R> <C> relaxed-evaluation <C> [BOLD] DLT <C> 19.39 <C> 12.95 <C> 15.53 <C> 11.47 <C> 9.72 <C> 10.52 <CAP> Table 4: Results on the multilingual task.
<R> <C> Scorer <C> System <C> P <C> R <C> F1 <R> <C> SemEval-2015 <C> [BOLD] BTE <C> 13.98 <C> 4.68 <C> 7.02 <R> <C> SemEval-2015 <C> [BOLD] DLT <C> 14.96 <C> 10.74 <C> 12.50 <R> <C> [EMPTY] <C> [BOLD] CLE <C> 14.96 <C> 10.74 <C> 12.50 <R> <C> strict-evaluation <C> [BOLD] BTE <C> 13.98 <C> 3.12 <C> 5.10 <R> <C> strict-evaluation <C> [BOLD] DLT <C> 14.96 <C> 7.14 <C> 9.67 <R> <C> [EMPTY] <C> [BOLD] CLE <C> 16.59 <C> 8.47 <C> 11.22 <R> <C> relaxed-evaluation <C> [BOLD] BTE <C> 10.13 <C> 8.16 <C> 9.04 <R> <C> relaxed-evaluation <C> [BOLD] DLT <C> 9.75 <C> 17.70 <C> 12.57 <R> <C> [EMPTY] <C> [BOLD] CLE <C> 10.97 <C> 17.70 <C> 13.55 <CAP> Table 5: Results on the cross-lingual task
<R> <C> [BOLD] Methods <C> [BOLD] Speaker (acc) <C> [BOLD] Sentiment (acc) <R> <C> Mel-Features <C> 70.06 <C> 64.63 <R> <C> APC <C> 85.88 <C> 65.97 <R> <C> Base <C> 94.54 <C> 67.38 <R> <C> BaseFT2 <C> [BOLD] 98.05 <C> 68.45 <R> <C> Large <C> 96.26 <C> 70.07 <R> <C> LargeWS <C> 96.40 <C> [BOLD] 71.05 <CAP> Table 2: Comparing different methods with different tasks.
<R> <C> Features <C> Posteriors <C> 30s <C> 10s <C> 3s <R> <C> SDC <C> GMM <C> 5.26 <C> 10.7 <C> 20.9 <R> <C> SDC <C> DNN <C> 4.00 <C> 8.21 <C> 19.5 <R> <C> Bottleneck <C> GMM <C> [BOLD] 2.76 <C> [BOLD] 6.55 <C> [BOLD] 15.9 <R> <C> Bottleneck <C> DNN <C> 3.79 <C> 7.71 <C> 18.2 <R> <C> 5-way fusion <C> 5-way fusion <C> 3.27 <C> 6.67 <C> 17.1 <CAP> Table 3: LRE11 results Cavg
<R> <C> Precision <C> 0.9310 <R> <C> Recall <C> 0.9344 <R> <C> F1 Score <C> 0.9325 <CAP> Table 2: Experiment Results of Coarse Class Classification
<R> <C> System <C> Broadcast conversations Dev <C> Broadcast conversations Test <C> Conference speeches Dev <C> Conference speeches Test <C> User recordings Dev <C> User recordings Test <C> Average relative WER incr. <R> <C> [BOLD] Full system <C> [BOLD] 11.0 <C> [BOLD] 8.1 <C> [BOLD] 14.5 <C> [BOLD] 12.9 <C> 29.4 <C> [BOLD] 22.7 <C> [EMPTY] <R> <C> 6-fold data augmentation (not 9) <C> 11.0 <C> 8.2 <C> 14.9 <C> 13.4 <C> 30.1 <C> 23.0 <C> +1.9% <R> <C> No OOV model <C> 11.3 <C> 8.3 <C> 15.2 <C> 14.0 <C> [BOLD] 29.2 <C> 23.1 <C> +3.3% <R> <C> No RNNLM rescoring <C> 11.8 <C> 9.0 <C> 15.9 <C> 14.0 <C> 31.2 <C> 24.6 <C> +8.5% <R> <C> 2014 system  <C> 18.0 <C> 17.9 <C> 23.7 <C> 26.3 <C> [EMPTY] <C> [EMPTY] <C> +88% <CAP> Table 2: Word error rates for different types of test data, using the full system and with one of the proposed improvements deactivated.
<R> <C> Trait <C> SVM <C> Lasso <R> <C> Excited <C> 0.904 <C> 0.885 <R> <C> Engagement <C> 0.858 <C> 0.850 <R> <C> Smiled <C> 0.845 <C> 0.845 <R> <C> Friendly <C> 0.824 <C> 0.793 <R> <C> Recommend Hiring <C> 0.815 <C> 0.796 <R> <C> Structured Answers <C> 0.812 <C> 0.799 <R> <C> Not Awkward <C> 0.808 <C> 0.787 <R> <C> Overall <C> 0.805 <C> 0.777 <R> <C> No Fillers <C> 0.803 <C> 0.855 <R> <C> Focused <C> 0.791 <C> 0.677 <R> <C> Paused <C> 0.749 <C> 0.749 <R> <C> Authentic <C> 0.688 <C> 0.642 <R> <C> Eye Contact <C> 0.676 <C> 0.622 <R> <C> Calm <C> 0.651 <C> 0.669 <R> <C> Speaking Rate <C> 0.608 <C> 0.546 <R> <C> Not Stressed <C> 0.604 <C> 0.572 <CAP> TABLE V: The average area under the ROC curve.
<R> <C> [BOLD] FLOPs <C> [BOLD] Model <C> [BOLD] CoLA <C> [BOLD] SST-2 <C> [BOLD] MRPC <C> [BOLD] STS-B <C> [BOLD] QQP <C> [BOLD] MNLI-m/mm <C> [BOLD] QNLI <C> [BOLD] RTE <C> [BOLD] Avg. <R> <C> [BOLD] FLOPs <C> [BOLD] Model <C> 8.5k <C> 67k <C> 3.7k <C> 5.7k <C> 364k <C> 393k <C> 108k <C> 2.5k <C> - <R> <C> 4% (2e18) <C> RoBERTa <C> 27.21 <C> 87.58 <C> 63.04 <C> 80.62 <C> 86.76 <C> 76.93/77.69 <C> 85.22 <C> 54.28 <C> 76.40 <R> <C> 4% (2e18) <C> ELECTRA <C> 44.83 <C> 87.72 <C> 74.89 <C> 83.50 <C> 87.38 <C> 77.48/78.08 <C> 85.75 <C> 59.96 <C> 79.57 <R> <C> 4% (2e18) <C> MC-BERT <C> 39.20 <C> 88.50 <C> 76.29 <C> 83.54 <C> 87.31 <C> 77.68/78.27 <C> 85.63 <C> 59.23 <C> 79.89 <R> <C> 8% (4e18) <C> RoBERTa <C> 42.23 <C> 90.70 <C> 71.22 <C> 85.08 <C> 88.15 <C> 79.71/79.78 <C> 87.82 <C> 57.72 <C> 80.06 <R> <C> 8% (4e18) <C> ELECTRA <C> 58.15 <C> 90.15 <C> 80.75 <C> 86.04 <C> 88.64 <C> 80.63/80.93 <C> 88.32 <C> 64.62 <C> 82.76 <R> <C> 8% (4e18) <C> MC-BERT <C> 53.28 <C> 91.11 <C> 80.91 <C> 86.11 <C> 88.51 <C> 80.80/80.95 <C> 88.23 <C> 66.85 <C> 83.23 <R> <C> 16% (8e18) <C> RoBERTa <C> 47.00 <C> 91.56 <C> 76.53 <C> 86.22 <C> 88.68 <C> 81.36/81.55 <C> 88.98 <C> 59.38 <C> 81.83 <R> <C> 16% (8e18) <C> ELECTRA <C> 61.05 <C> 91.56 <C> 82.50 <C> 87.37 <C> 89.14 <C> 82.32/82.28 <C> 89.90 <C> 66.76 <C> 84.22 <R> <C> 16% (8e18) <C> MC-BERT <C> 57.96 <C> 91.96 <C> 82.18 <C> 86.93 <C> 89.11 <C> 82.04/82.30 <C> 89.46 <C> 68.14 <C> 84.28 <R> <C> 32% (1.6e19) <C> RoBERTa <C> 50.69 <C> 92.22 <C> 78.30 <C> 86.72 <C> 89.13 <C> 82.61/82.41 <C> 90.05 <C> 61.03 <C> 82.85 <R> <C> 32% (1.6e19) <C> ELECTRA <C> 61.49 <C> 92.31 <C> 84.12 <C> 88.46 <C> 89.57 <C> 83.85/83.87 <C> 90.80 <C> 67.51 <C> 85.23 <R> <C> 32% (1.6e19) <C> MC-BERT <C> 59.20 <C> 92.67 <C> 83.98 <C> 87.31 <C> 89.37 <C> 83.69/83.58 <C> 90.37 <C> 70.85 <C> 85.46 <R> <C> 64% (3.2e19) <C> RoBERTa <C> 57.40 <C> 93.08 <C> 82.07 <C> 87.72 <C> 89.31 <C> 84.40/84.47 <C> 91.04 <C> 63.24 <C> 84.41 <R> <C> 64% (3.2e19) <C> ELECTRA <C> 65.72 <C> 92.82 <C> 85.22 <C> 88.79 <C> 89.99 <C> 85.42/84.80 <C> 91.31 <C> 69.77 <C> 86.15 <R> <C> 64% (3.2e19) <C> MC-BERT <C> 62.05 <C> 92.41 <C> 85.83 <C> 88.23 <C> 89.75 <C> 85.11/84.73 <C> 91.15 <C> 74.13 <C> 86.63 <R> <C> 100% (5e19) <C> RoBERTa <C> 57.41 <C> 93.15 <C> 83.22 <C> 88.14 <C> 89.24 <C> 84.69/84.63 <C> 91.02 <C> 63.10 <C> 84.65 <R> <C> 100% (5e19) <C> ELECTRA <C> 64.33 <C> 93.38 <C> 84.88 <C> 89.10 <C> 89.96 <C> 86.00/85.29 <C> 91.85 <C> 70.80 <C> 86.52 <R> <C> 100% (5e19) <C> MC-BERT <C> 62.10 <C> 92.34 <C> 85.96 <C> 88.01 <C> 89.65 <C> 85.68/85.24 <C> 91.34 <C> 74.96 <C> 86.82 <CAP> Table 7: The detailed results on the GLUE benchmark (except WNLI).
<R> <C> [BOLD] Dataset <C> [BOLD] Previous Methods (F1-score) Partial CRF <C> [BOLD] Previous Methods (F1-score) CWS-DICT <C> [BOLD] Previous Methods (F1-score) WEB-CWS <C> [BOLD] Ours (F1-score) AT <C> [BOLD] Ours (F1-score) GCNN (PKU) <C> [BOLD] Ours (F1-score) DA <C> [BOLD] Ours (F1-score) GCNN(Mix) <C> [BOLD] Ours (F1-score) GCNN (Target) <C> [BOLD] DAAT <R> <C> DL <C> 92.5 <C> 92.0 <C> [BOLD] 93.5 <C> 90.7 <C> 90.0 <C> 93.6 <C> 93.9 <C> 93.9 <C> [BOLD] 94.1 (+0.6) <R> <C> FR <C> [BOLD] 90.2 <C> 89.1 <C> 89.6 <C> 86.8 <C> 86.0 <C> 92.4 <C> 92.6 <C> 92.6 <C> [BOLD] 93.1 (+2.9) <R> <C> ZX <C> 83.9 <C> 88.8 <C> [BOLD] 89.6 <C> 85.0 <C> 85.4 <C> 90.4 <C> 90.6 <C> 90.7 <C> [BOLD] 90.9 (+1.3) <R> <C> DM <C> [BOLD] 82.8 <C> 81.2 <C> 82.2 <C> 81.0 <C> 82.4 <C> 83.8 <C> 83.9 <C> 84.3 <C> [BOLD] 85.0 (+2.2) <R> <C> PT <C> 85.0 <C> [BOLD] 85.9 <C> 85.1 <C> 85.1 <C> 87.6 <C> 89.1 <C> 89.3 <C> 89.3 <C> [BOLD] 89.6 (+3.7) <CAP> Table 3: The overall results on five datasets. The first block contains the latest cross-domain methods. And the second block reports the results for our implemented methods and DAAT. Numbers in the parentheses indicate absolute improvement than previous SOTA results.
<R> <C> [EMPTY] <C> Exp 1: Object Only Total <C> Exp 1: Object Only Pointing <C> Exp 1: Object Only MissRef <C> Exp 1: Object Only MultRef <C> Exp 2: Object+Attribute Total <C> Exp 2: Object+Attribute Pointing <C> Exp 2: Object+Attribute MissRef <C> Exp 2: Object+Attribute MultRef <R> <C> PoP <C> 66 <C> 71 <C> 57 <C> 51 <C> 69 <C> 77 <C> 57 <C> 46 <R> <C> TRPoP <C> 65 <C> 70 <C> 58 <C> 44 <C> 62 <C> 70 <C> 38 <C> 48 <R> <C> Pipeline <C> 67 <C> 75 <C> 51 <C> 45 <C> 65 <C> 74 <C> 37 <C> 55 <R> <C> CNN <C> 35 <C> 9 <C> 100 <C> 94 <C> - <C> - <C> - <C> - <R> <C> Random <C> 17 <C> 17 <C> 17 <C> 17 <C> 17 <C> 17 <C> 17 <C> 17 <R> <C> Majority <C> 30 <C> 0 <C> 100 <C> 100 <C> 30 <C> 0 <C> 100 <C> 100 <R> <C> Probability <C> 22 <C> 18 <C> 30 <C> 30 <C> 22 <C> 18 <C> 30 <C> 30 <R> <C> AttrRandom <C> - <C> - <C> - <C> - <C> 47 <C> 64 <C> 16 <C> 0 <R> <C> ImgShuffle <C> - <C> - <C> - <C> - <C> 50 <C> 58 <C> 31 <C> 32 <CAP> Table 1: Results. Figure of merit is percentage accuracy. See text for details.
<R> <C> [BOLD] Domain <C> [BOLD] #train <C> [BOLD] #test <C> [BOLD] #slots <C> [BOLD] F1 score <R> <C> MIT Restaurant <C> 7661 <C> 1522 <C> 17 <C> 80.11 <R> <C> MIT Movie <C> 9776 <C> 2444 <C> 25 <C> 87.86 <R> <C> ATIS <C> 4978 <C> 893 <C> 127 <C> 95.51 <CAP> Table 4: The three domains and their basic information (number of training samples, number of samples in the test set and number of different slots). The MIT Restaurant and MIT Movie datasets contain user queries about restaurant and movie information. The Airline Travel Information Services (ATIS) dataset mainly contains questions about flight booking and transport information
<R> <C> [BOLD] Model <C> [BOLD] Corpus <C> [BOLD] Test ppl. <C> [BOLD] EntRate <R> <C> AWD-LSTM (Merity et al.,  2017 ) <C> PTB <C> 58.3 <C> 93.1 <R> <C> CNN-LSTM (Jozefowicz et al.,  2016 ) <C> GBW <C> 29.8 <C> 49.4 <R> <C> Transformer (Vaswani et al.,  2017b ) <C> GBW <C> 28.1 <C> 34.7 <R> <C> Transformer (Radford et al.,  2019 ) <C> WebText <C> 23.7 <C> 61.2 <CAP> Table 1: Entropy rate degradations for generations from popular language models. State-of-the-art performance is usually reported via perplexity (one-step prediction loss), but there is a striking blowup in the entropy rates of these models’ long-term generations.
<R> <C> Model Gen-RNN (30% dropout) <C> Model Gen-RNN (30% dropout) <C> ELBO -52.3 <C> KL 7.1 <C> PPL 41.9 <R> <C> AUTR (no dropout) <C> T = 30 <C> -50.7 <C> 8.0 <C> 37.4 <R> <C> AUTR (no dropout) <C> T = 40 <C> -50.7 <C> 7.8 <C> 37.4 <R> <C> AUTR (30% dropout) <C> T = 30 <C> -51.6 <C> 14.0 <C> 39.9 <R> <C> AUTR (30% dropout) <C> T = 40 <C> -51.5 <C> 13.8 <C> 39.6 <CAP> Table 1: Test set results on the Book Corpus dataset. We report the ELBO, the contribution to the ELBO from the KL divergence term (DKL[qϕ(z|x) || p(z)]), and the perplexity (PPL) on the test set. For the ELBO, higher is better, and for the perplexity, lower is better.
<R> <C> [BOLD] Classifier <C> [BOLD] Accuracy (%) <C> [BOLD] AUROC <R> <C> kNN <C> 82.76 <C> 0.964 <R> <C> LR <C> 86.05 <C> 0.980 <R> <C> SVM <C> 86.92 <C> 0.986 <CAP> Table 3: Cross-validation results of three classifiers.
<R> <C> Model <C> MCD [dB] <C> [ITALIC] F0 RMSE <R> <C> VL <C> 5.85 <C> 15.1 <R> <C> SS <C> 5.71 <C> 15.0 <R> <C> JG <C> 5.57 <C> 14.6 <R> <C> TL <C> 5.79 <C> 15.3 <R> <C> JG+TL <C> 5.63 <C> 14.8 <CAP> Table 3: Objective evaluation using mel-cepstral distortion (MCD) in dB and F0 root mean square error (F0 RMSE) for the multi-speaker modeling task. The errors are calculated on the test set including 440 utterances.
<R> <C> Methods <C> Named Entities <C> Common Nouns <C> Verbs <C> Prepositions <R> <C> Humans (query)(∗) <C> 0.520 <C> 0.644 <C> 0.716 <C> 0.676 <R> <C> Humans (context+query)(∗) <C> [BOLD] 0.816 <C> [BOLD] 0.816 <C> [BOLD] 0.828 <C> 0.708 <R> <C> Maximum frequency (corpus) <C> 0.120 <C> 0.158 <C> 0.373 <C> 0.315 <R> <C> Maximum frequency (context) <C> 0.335 <C> 0.281 <C> 0.285 <C> 0.275 <R> <C> Sliding window <C> 0.168 <C> 0.196 <C> 0.182 <C> 0.101 <R> <C> Word distance model <C> 0.398 <C> 0.364 <C> 0.380 <C> 0.237 <R> <C> Kneser-Ney language model <C> 0.390 <C> 0.544 <C> 0.778 <C> 0.768 <R> <C> Kneser-Ney language model + cache <C> 0.439 <C> 0.577 <C> 0.772 <C> 0.679 <R> <C> Embedding Model (context+query) <C> 0.253 <C> 0.259 <C> 0.421 <C> 0.315 <R> <C> Embedding Model (query) <C> 0.351 <C> 0.400 <C> 0.614 <C> 0.535 <R> <C> Embedding Model (window) <C> 0.362 <C> 0.415 <C> 0.637 <C> 0.589 <R> <C> Embedding Model (window+position) <C> 0.402 <C> 0.506 <C> 0.736 <C> 0.670 <R> <C> LSTMs (query) <C> 0.408 <C> 0.541 <C> 0.813 <C> 0.802 <R> <C> LSTMs (context+query) <C> 0.418 <C> 0.560 <C> [BOLD] 0.818 <C> 0.791 <R> <C> Contextual LSTMs (window context) <C> 0.436 <C> 0.582 <C> 0.805 <C> [BOLD] 0.806 <R> <C> MemNNs (lexical memory) <C> 0.431 <C> 0.562 <C> 0.798 <C> 0.764 <R> <C> MemNNs (window memory) <C> 0.493 <C> 0.554 <C> 0.692 <C> 0.674 <R> <C> MemNNs (sentential memory + PE) <C> 0.318 <C> 0.305 <C> 0.502 <C> 0.326 <R> <C> MemNNs (window memory + self-sup.) <C> [BOLD] 0.666 <C> [BOLD] 0.630 <C> 0.690 <C> 0.703 <CAP> Table 2: Results on CBT test set. (∗)Human results were collected on 10% of the test set.
<R> <C> Methods <C> Named Entities <C> Common Nouns <C> Verbs <C> Prepositions <R> <C> MemNNs (lexical memory) <C> 0.431 <C> 0.562 <C> 0.798 <C> 0.764 <R> <C> MemNNs (window memory) <C> 0.493 <C> 0.554 <C> 0.692 <C> 0.674 <R> <C> MemNNs (sentential memory + PE) <C> 0.318 <C> 0.305 <C> 0.502 <C> 0.326 <R> <C> MemNNs (window memory + self-sup.) <C> [BOLD] 0.666 <C> [BOLD] 0.630 <C> 0.690 <C> 0.703 <R> <C> MemNNs (all windows + self-sup.) <C> 0.648 <C> 0.604 <C> 0.711 <C> 0.693 <R> <C> MemNNs (all windows + all targets + self-sup.) <C> 0.639 <C> 0.602 <C> 0.698 <C> 0.667 <R> <C> MemNNs (LM + self-sup.) <C> 0.638 <C> 0.605 <C> 0.692 <C> 0.647 <CAP> Table 4: Results on CBT test set when considering all windows or targets.
<R> <C> Model D <C> Fluency Score <C> Style Score <C> Style Diversity Score <C> Content Novelty Score <R> <C> L [ITALIC] DIST <C> 7.32 <C> 51.16 <C> 11.40 <C> 24.01 <R> <C> L [ITALIC] STYLE <C> 7.35 <C> 5.40 <C> 9.22 <C> 29.27 <R> <C> L [ITALIC] GAN <C> 6.85 <C> 28.67 <C> 10.35 <C> 26.77 <R> <C> All <C> 7.14 <C> 27.90 <C> 10.52 <C> 25.85 <CAP> Table 4: Ablation study on the various loss terms in the proposed objective function.
<R> <C> [EMPTY] <C> UDpipe <C> Gold <R> <C> ko→ja <C> 14.96 <C> 20.04 <R> <C> ja→ko <C> 37.44 <C> 37.45 <CAP> Table 2: LAS scores when transferring between Korean and Japanese in two tokenization conditions.
<R> <C> [BOLD] Model <C> [BOLD] F1 Score <R> <C> BERT [CLS] <C> 0.81 <R> <C> Seq2Seq Encoder <C> 0.83 <R> <C> No Inter-Sentence Aggregator <C> 0.89 <R> <C> ScopeIt Without Data Augmentation <C> 0.93 <R> <C> ScopeIt <C> [BOLD] 0.94 <CAP> Table 1. Performance for relevance scoping
<R> <C> [BOLD] Dataset <C> [BOLD] Model <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] Fscore <R> <C> 20 <C> Jangada <C> 0.98 <C> 0.971 <C> 0.975 <R> <C> Newsgroup <C> ScopeIt <C> 0.992 <C> 0.999 <C> [BOLD] 0.996 <R> <C> Manually <C> Jangada <C> 0.908 <C> 0.224 <C> 0.359 <R> <C> Annotated <C> ScopeIt <C> 0.995 <C> 0.884 <C> [BOLD] 0.936 <CAP> Table 4. Performance: Jangada Vs ScopeIt
<R> <C> [EMPTY] <C> [BOLD] Depth <C> [BOLD] 1 <C> [BOLD] 2 <C> [BOLD] 3 <C> [BOLD] 4 <C> [BOLD] 5 <C> [BOLD] 6 <R> <C> D1 <C> DS <C> 81.12 <C> 81.45 <C> 81.52 <C> [BOLD] 81.78 <C> 81.07 <C> 80.68 <R> <C> D1 <C> HDS <C> 55.73 <C> 57.08 <C> 60.67 <C> [BOLD] 62.02 <C> 59.10 <C> 58.65 <R> <C> D2 <C> DS <C> 87.20 <C> 87.47 <C> 87.53 <C> [BOLD] 87.55 <C> 87.11 <C> 87.21 <R> <C> D2 <C> HDS <C> 73.93 <C> 74.27 <C> [BOLD] 76.07 <C> 75.73 <C> 75.56 <C> 74.27 <R> <C> D3 <C> DS <C> 78.18 <C> 77.94 <C> 78.69 <C> [BOLD] 78.85 <C> 78.40 <C> 77.88 <R> <C> D3 <C> HDS <C> 59.35 <C> 58.94 <C> 59.43 <C> [BOLD] 60.33 <C> 59.27 <C> 57.80 <R> <C> D4 <C> DS <C> 71.13 <C> 71.10 <C> [BOLD] 71.62 <C> 71.50 <C> 71.16 <C> 70.86 <R> <C> D4 <C> HDS <C> 49.44 <C> 50.00 <C> 50.56 <C> [BOLD] 51.30 <C> 49.81 <C> 49.63 <CAP> Table 8: The accuracy of model depth on the four datasets. ‘D1’: Restaurant-14, ‘D2’: Restaurant-Large, ‘D3’: Restaurant, ‘D4’: Laptop.
<R> <C> [BOLD] Models <C> [BOLD] Rest. <C> [BOLD] Laptop <R> <C> [BOLD] IARM(Majumder et al.,  2018 )* <C> 80.00 <C> 73.80 <R> <C> [BOLD] TNet(Li et al.,  2018a )* <C> 80.79 <C> [BOLD] 76.54 <R> <C> [BOLD] VAE(Xu and Tan,  2018 )* <C> 81.10 <C> 75.34 <R> <C> [BOLD] PBAN(Gu et al.,  2018 )* <C> 81.16 <C> 74.12 <R> <C> [BOLD] AOA(Huang et al.,  2018 )* <C> 81.20 <C> 74.50 <R> <C> [BOLD] MGAN(Fan et al.,  2018 )* <C> 81.25 <C> 75.39 <R> <C> [BOLD] DAuM(Zhu and Qian,  2018 )* <C> 82.32 <C> 74.45 <R> <C> [BOLD] AGDT <C> [BOLD] 82.95 <C> 75.86 <CAP> Table 10: The three-class accuracy of the aspect-term sentiment analysis task on SemEval 2014. ‘*’ refers to citing from the original paper. ‘Rest.’: Restaurant.
<R> <C> Data <C> 2016 <R> <C> WMT+back-trans. <C> 32.6 <R> <C> +Paracrawl-32M <C> 30.1 <R> <C> +Paracrawl-2M <C> 33.2 <R> <C> +Paracrawl-4M <C> 33.5 <R> <C> [BOLD] +Paracrawl-8M <C> [BOLD] 34.0 <R> <C> +Paracrawl-16M <C> 31.9 <R> <C> +Paracrawl-24M <C> 30.3 <R> <C> [BOLD] +Paracrawl-8M-weights <C> [BOLD] 34.2 <R> <C> +Paracrawl-24M-weights <C> 33.4 <CAP> Table 2: Effects of data cleaning, filtering and weighting on BLEU. Evaluated with default shallow Nematus-style RNN model
<R> <C> [BOLD] NMI Scores <C> [BOLD] MS D2V <C> [BOLD] LDA <C> [BOLD] Ward D2V <C> [BOLD] Ward BoW <R> <C> [BOLD] Open Calais <C> 0.303 <C> 0.263 <C> 0.270 <C> 0.202 <R> <C> [BOLD] Google Cloud <C> 0.351 <C> 0.330 <C> 0.322 <C> 0.266 <CAP> Table 2. The NMI scores between the four clustering methods (15 clusters) against the two commercial taxonomy services show that the MS clustering is closest to the output of both commercial classifications.
<R> <C> Task <C> #Action |C| N2N <C> No-Match Features QRN <C> No-Match Features GMemN2N <C> No-Match Features SN <C> No-Match Features N2N <C> With Match Features QRN <C> With Match Features GMemN2N <C> With Match Features SN <C> With Match Features <R> <C> 1: Issuing API Calls <C> 309 <C> 0.1 <C> [BOLD] 0.0 <C> [BOLD] 0.0 <C> [BOLD] 0.0 <C> [BOLD] 0.0 <C> [BOLD] 0.0 <C> [BOLD] 0.0 <C> [BOLD] 0.0 <R> <C> 2: Updating API Calls <C> 307 <C> [BOLD] 0.0 <C> [BOLD] 0.0 <C> [BOLD] 0.0 <C> [BOLD] 0.0 <C> 1.7 <C> [BOLD] 0.0 <C> [BOLD] 0.0 <C> [BOLD] 0.0 <R> <C> 3: Displaying Options <C> 333 <C> 25.1 <C> 12.6 <C> 25.1 <C> [BOLD] 12.0 <C> 25.1 <C> 7.6 <C> 25.1 <C> [BOLD] 5.6 <R> <C> 4: Providing Extra Information <C> 1036 <C> 40.5 <C> [BOLD] 14.3 <C> 42.8 <C> [BOLD] 14.2 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <R> <C> 5: Conducting Full Dialogs <C> 1257 <C> 3.9 <C> [BOLD] 0.7 <C> 3.7 <C> 3.4 <C> 6.6 <C> [BOLD] 0.0 <C> 2.0 <C> 1.0 <R> <C> Average Error Rates (%) <C> [EMPTY] <C> 13.9 <C> [BOLD] 5.5 <C> 14.3 <C> 5.9 <C> 6.7 <C> 1.5 <C> 5.4 <C> [BOLD] 1.3 <R> <C> 1: (OOV) Issuing API Calls <C> 309 <C> 27.7 <C> 6.6 <C> 17.6 <C> [BOLD] 5.6 <C> 3.5 <C> [BOLD] 0.0 <C> [BOLD] 0.0 <C> 1.2 <R> <C> 2: (OOV) Updating API Calls <C> 307 <C> 21.1 <C> [BOLD] 8.4 <C> 21.1 <C> 10.8 <C> 5.5 <C> [BOLD] 0.0 <C> 5.8 <C> 0.8 <R> <C> 3: (OOV) Displaying Options <C> 333 <C> 25.6 <C> 12.4 <C> 24.7 <C> [BOLD] 10.2 <C> 24.8 <C> 7.7 <C> 24.9 <C> [BOLD] 6.0 <R> <C> 4: (OOV) Providing Extra Inform. <C> 1036 <C> 42.4 <C> [BOLD] 14.4 <C> 43.0 <C> 23.3 <C> [BOLD] 0.0 <C> [BOLD] 0.0 <C> [BOLD] 0.0 <C> [BOLD] 0.0 <R> <C> 5: (OOV) Conducting Full Dialogs <C> 1257 <C> 34.5 <C> [BOLD] 13.7 <C> 33.3 <C> 14.6 <C> 22.3 <C> 4.0 <C> 20.6 <C> [BOLD] 2.6 <R> <C> Average Error Rates <C> [EMPTY] <C> 30.3 <C> [BOLD] 11.1 <C> 26.9 <C> 12.9 <C> 11.2 <C> 2.3 <C> 10.3 <C> [BOLD] 2.1 <R> <C> DSTC-2 Real Dialogs <C> 1257 <C> 58.9 <C> 48.9 <C> 52.6 <C> [BOLD] 47.6 <C> 59.0 <C> 49.3 <C> 51.3 <C> [BOLD] 48.7 <CAP> Table 3: Error Rates on Babi Dialog and DSTC2 dialog datasets Bordes & Weston (2017) with and without match features.
<R> <C> Model <C> fn <C> fa <C> wl <R> <C> Heuristic Loss <C> 1719 <C> 1956 <C> 1258 <R> <C> Reward Rescaling <C> 1725 <C> 1994 <C> 1247 <CAP> Table 2: Number of “false new,” “false anaphoric,” and “wrong link” errors produced by the models on the English CoNLL 2012 test set.
<R> <C> Method <C> Accuracy <R> <C> reader-retriever-full <C> [BOLD] 0.30 <R> <C> BERT-large <C> 0.16 <R> <C> BERT-base <C> 0.15 <R> <C> QA-space-only <C> 0.07 <R> <C> {Q}A-space-only <C> 0.21 <CAP> Table 1: Test accuracy on TriviaQA-Open.
<R> <C> Dataset <C> [ITALIC] λt=0 <C> 0< [ITALIC] λt <C> [ITALIC] λt=∞ <R> <C> Pathology <C> 77.4 <C> 91.2 <C> 81.4 <R> <C> Review <C> 80.9 <C> 86.4 <C> 84.3 <CAP> Table 7: The effect of regularization of the transformation layer λt on the performance.
<R> <C> Dataset <C> Target Model <C> Training <C> Original full Pos <C> Original full Neg <C> Original full All <C> Original sampled Pos <C> Original sampled Neg <C> Original sampled All <C> Modified Pos <C> Modified Neg <C> Modified All <R> <C> QQP <C> BiMPM <C> Normal <C> 88.5 <C> 87.8 <C> 88.1 <C> 88.0 <C> 99.4 <C> 93.7 <C> 14.4 <C> 7.8 <C> 11.1 <R> <C> QQP <C> DIIN <C> Normal <C> 91.5 <C> 85.9 <C> 88.7 <C> 89.6 <C> 99.6 <C> 94.6 <C> 31.0 <C> 8.2 <C> 19.6 <R> <C> QQP <C> BERT <C> Normal <C> 90.7 <C> 91.3 <C> 91.0 <C> 89.0 <C> 99.6 <C> 94.3 <C> 33.4 <C> 14.8 <C> 24.1 <R> <C> QQP <C> BiMPM <C> Adversarial <C> 89.6 <C> 88.0 <C> 88.9 <C> 89.4 <C> 99.8 <C> 94.6 <C> 15.0 <C> 27.8 <C> 21.4 <R> <C> QQP <C> DIIN <C> Adversarial <C> 82.1 <C> 91.7 <C> 86.9 <C> 81.2 <C> 99.8 <C> 90.5 <C> 35.0 <C> 72.2 <C> 53.6 <R> <C> QQP <C> BERT <C> Adversarial <C> 87.6 <C> 92.5 <C> 90.1 <C> 86.8 <C> 99.8 <C> 93.3 <C> 53.0 <C> 79.0 <C> 66.0 <R> <C> MRPC <C> BiMPM <C> Normal <C> 90.2 <C> 40.0 <C> 73.4 <C> 87.2 <C> 97.4 <C> 92.3 <C> 3.2 <C> 0.2 <C> 1.7 <R> <C> MRPC <C> DIIN <C> Normal <C> 89.9 <C> 49.5 <C> 76.3 <C> 90.4 <C> 100.0 <C> 95.2 <C> 48.2 <C> 0.4 <C> 24.3 <R> <C> MRPC <C> BERT <C> Normal <C> 93.2 <C> 66.4 <C> 84.2 <C> 94.0 <C> 100.0 <C> 97.0 <C> 45.6 <C> 2.0 <C> 23.8 <R> <C> MRPC <C> BiMPM <C> Adversarial <C> 96.8 <C> 26.3 <C> 73.2 <C> 95.6 <C> 100.0 <C> 97.8 <C> 73.2 <C> 0.6 <C> 36.9 <R> <C> MRPC <C> DIIN <C> Adversarial <C> 85.8 <C> 58.0 <C> 76.5 <C> 82.8 <C> 100.0 <C> 91.4 <C> 59.8 <C> 67.6 <C> 63.7 <R> <C> MRPC <C> BERT <C> Adversarial <C> 95.3 <C> 55.2 <C> 81.9 <C> 95.0 <C> 100.0 <C> 97.5 <C> 81.0 <C> 93.0 <C> 87.0 <CAP> Table 1: Accuracies (%) of target models: “Original full” indicates the full original test set, “original sampled” indicates original examples sampled from the test set (see Sec. 3.3) , and “modified” indicates examples modified by our algorithm. “Pos” and “neg” indicate results on positive examples and negative examples respectively. The “training” column indicates whether the models are normally trained or adversarial trained (see Sec. 4.4).
<R> <C> Method <C> [ITALIC] m <C> PPL <R> <C> Baseline <C> [EMPTY] <C> 53.7 <R> <C> LSM <C> 1 <C> 53.5 <R> <C> LSM <C> 2 <C> 390.3 <R> <C> ARC <C> 0 <C> 66.9 <R> <C> ARC <C> 0.001 <C> 68.0 <R> <C> ARC <C> 0.003 <C> 80.7 <R> <C> ARC <C> 0.01 <C> 106.1 <R> <C> ARC <C> 0.03 <C> 170.4 <R> <C> COS <C> 0 <C> 66.9 <R> <C> COS <C> 0.001 <C> 70.3 <R> <C> COS <C> 0.003 <C> 77.7 <R> <C> COS <C> 0.01 <C> 108.0 <R> <C> COS <C> 0.03 <C> 382.1 <CAP> Table 1: PPL on SWB using margins from face recognition out-of-the-box.
<R> <C> [EMPTY] <C> [ITALIC] b <C> [ITALIC] T <C> Baseline Autoregressive <C> Decoding from an undirected sequence model Uniform <C> Decoding from an undirected sequence model Left2Right <C> Decoding from an undirected sequence model Least2Most <C> Decoding from an undirected sequence model Easy-First <C> Learned <R> <C> En→De <C> 1 <C> [ITALIC] L <C> 25.33 <C> 21.01 <C> 24.27 <C> 23.08 <C> 23.73 <C> 24.10 <R> <C> En→De <C> 4 <C> [ITALIC] L <C> 26.84 <C> 22.16 <C> 25.15 <C> 23.81 <C> 24.13 <C> 24.87 <R> <C> En→De <C> 4 <C> [ITALIC] L* <C> – <C> 22.74 <C> [BOLD] 25.66 <C> 24.42 <C> 24.69 <C> [BOLD] 25.28 <R> <C> En→De <C> 1 <C> 2 [ITALIC] L <C> – <C> 21.16 <C> 24.45 <C> 23.32 <C> 23.87 <C> 24.15 <R> <C> [EMPTY] <C> 4 <C> 2 [ITALIC] L <C> – <C> 21.99 <C> 25.14 <C> 23.81 <C> 24.14 <C> 24.86 <R> <C> De→En <C> 1 <C> [ITALIC] L <C> 29.83 <C> 26.01 <C> 28.34 <C> 28.85 <C> 29.00 <C> 28.47 <R> <C> De→En <C> 4 <C> [ITALIC] L <C> 30.92 <C> 27.07 <C> 29.52 <C> 29.03 <C> 29.41 <C> 29.73 <R> <C> De→En <C> 4 <C> [ITALIC] L* <C> – <C> 28.07 <C> [BOLD] 30.46 <C> 29.84 <C> 30.32 <C> [BOLD] 30.58 <R> <C> De→En <C> 1 <C> 2 [ITALIC] L <C> – <C> 26.24 <C> 28.64 <C> 28.60 <C> 29.12 <C> 28.45 <R> <C> [EMPTY] <C> 4 <C> 2 [ITALIC] L <C> – <C> 26.98 <C> 29.50 <C> 29.02 <C> 29.41 <C> 29.71 <CAP> Table 1: Results (BLEU↑) on WMT’14 En↔De translation using various decoding algorithms and different settings of beam search width (b) and number of iterations (T) as a function of sentence length (L). For each sentence we use 4 most likely sentence lengths. * denotes rescoring generated hypotheses using autoregressive model instead of proposed model.
<R> <C> [EMPTY] <C> fr-en p@1 <C> fr-en p@5 <C> de-en p@1 <C> de-en p@5 <C> es-en p@1 <C> es-en p@5 <C> fi-en p@1 <C> fi-en p@5 <C> ru-en p@1 <C> ru-en p@5 <C> cs-en p@1 <C> cs-en p@5 <R> <C> Random <C> 0.1 <C> 0.5 <C> 0.1 <C> 0.5 <C> 0.1 <C> 0.5 <C> 0.1 <C> 0.5 <C> 0.1 <C> 0.5 <C> 0.1 <C> 0.5 <R> <C> MUSE MUSE <C> 2.5 <C> 7.7 <C> 0.6 <C> 3.5 <C> 3.0 <C> 9.0 <C> 0.0 <C> 0.4 <C> 0.1 <C> 0.7 <C> 0.0 <C> 1.2 <R> <C> MUSE MUSE + normalize <C> 0.7 <C> 3.0 <C> 0.6 <C> 3.3 <C> 0.5 <C> 2.6 <C> 0.0 <C> 0.4 <C> 0.0 <C> 0.5 <C> 0.1 <C> 0.3 <R> <C> vecmap2 vecmap2 <C> 2.4 <C> 6.8 <C> 1.0 <C> 4.5 <C> 1.0 <C> 5.0 <C> 0.0 <C> 0.1 <C> 0.2 <C> 0.9 <C> 0.4 <C> 1.6 <R> <C> Ours <C> [BOLD] 7.3 <C> [BOLD] 16.5 <C> [BOLD] 4.6 <C> [BOLD] 12.0 <C> [BOLD] 8.2 <C> [BOLD] 18.0 <C> [BOLD] 2.7 <C> [BOLD] 7.3 <C> [BOLD] 2.7 <C> [BOLD] 6.9 <C> [BOLD] 3.7 <C> [BOLD] 10.2 <CAP> Table 1: Word alignment average precisions p@1 and 5 when models are trained on 50k sentences of source and target languages.
<R> <C> [EMPTY] <C> fr-en p@1 <C> fr-en p@5 <C> de-en p@1 <C> de-en p@5 <C> es-en p@1 <C> es-en p@5 <C> fi-en p@1 <C> fi-en p@5 <C> ru-en p@1 <C> ru-en p@5 <C> cs-en p@1 <C> cs-en p@5 <R> <C> Random <C> 0.1 <C> 0.5 <C> 0.1 <C> 0.5 <C> 0.1 <C> 0.5 <C> 0.1 <C> 0.5 <C> 0.1 <C> 0.5 <C> 0.1 <C> 0.5 <R> <C> MUSE MUSE <C> 0.8 <C> 4.2 <C> 0.2 <C> 1.3 <C> 1.4 <C> 4.6 <C> 0.1 <C> 0.6 <C> 0.6 <C> 2.1 <C> 0.5 <C> 1.3 <R> <C> MUSE MUSE + normalize <C> 0.2 <C> 1.2 <C> 0.1 <C> 0.8 <C> 0.2 <C> 1.0 <C> 0.2 <C> 1.1 <C> 0.3 <C> 1.1 <C> 0.3 <C> 1.2 <R> <C> vecmap2 vecmap2 <C> 6.1 <C> 14.7 <C> 1.1 <C> 5.0 <C> [BOLD] 29.9 <C> [BOLD] 45.3 <C> 0.5 <C> 2.2 <C> 0.1 <C> 1.2 <C> 0.5 <C> 2.2 <R> <C> Ours <C> [BOLD] 12.7 <C> [BOLD] 26.6 <C> [BOLD] 3.4 <C> [BOLD] 10.0 <C> 14.9 <C> 28.6 <C> [BOLD] 3.0 <C> [BOLD] 8.5 <C> [BOLD] 3.8 <C> [BOLD] 11.1 <C> [BOLD] 4.0 <C> [BOLD] 10.8 <CAP> Table 2: Word alignment average precisions p@1 and 5 when models are trained on one million sentences extracted from different domains between source and target languages.
<R> <C> Model <C> Subj (+/-,o)  [ITALIC] P. <C> Subj (+/-,o)  [ITALIC] R. <C> Subj (+/-,o)  [ITALIC] F1 <C> SA (+,-)  [ITALIC] P. <C> SA (+,-)  [ITALIC] R. <C> SA (+,-)  [ITALIC] F1 <R> <C> zhang2015neural <C> 49.2 <C> 42.1 <C> 45.3 <C> 40.9 <C> 21.6 <C> 27.9 <R> <C> SS +  [ITALIC] emb li2017sentimentscope <C> 50.0 <C> 44.0 <C> 46.8 <C> 37.6 <C> 25.4 <C> 30.2 <R> <C> SA-CRF <C> 44.8 <C> 45.2 <C> 44.9 <C> 35.2 <C> 25.6 <C> 29.3 <R> <C> [BOLD] EI- <C> 49.7 <C> 45.8 <C> 47.6 <C> [BOLD] 43.0 <C> 24.9 <C> 30.2 <R> <C> [BOLD] EI <C> [BOLD] 50.5 <C> [BOLD] 46.5 <C> [BOLD] 48.4 <C> 42.0 <C> [BOLD] 25.6 <C> [BOLD] 31.5 <CAP> Table 3: Results on subjectivity as well as non-neutral sentiment analysis on the Spanish dataset. Subj(+/-,o): subjectivity for all polarities. SA(+,-): sentiment analysis for non-neutral polarities.
<R> <C> Model <C> English  [ITALIC] target <C> English  [ITALIC] sent <C> Dutch  [ITALIC] target <C> Dutch  [ITALIC] sent <C> Russian  [ITALIC] target <C> Russian  [ITALIC] sent <R> <C> SS li2017sentimentscope <C> 46.3 <C> 36.9 <C> 44.6 <C> 33.4 <C> 20.2 <C> 14.5 <R> <C> SS +  [ITALIC] emb li2017sentimentscope <C> 57.1 <C> 48.0 <C> 46.8 <C> 33.5 <C> 35.9 <C> 24.1 <R> <C> SA-CRF <C> 60.8 <C> 51.4 <C> 49.7 <C> 34.0 <C> 54.2 <C> 43.4 <R> <C> [BOLD] EI- <C> 57.7 <C> 48.2 <C> 47.2 <C> 33.7 <C> 52.8 <C> 38.9 <R> <C> [BOLD] EI <C> [BOLD] 62.0 <C> [BOLD] 51.6 <C> [BOLD] 50.0 <C> [BOLD] 34.2 <C> [BOLD] 54.4 <C> [BOLD] 43.4 <CAP> Table 5: F1 scores of targets (target) and their associated sentiment (sent) on SemEval 2016 Restaurant Dataset.
<R> <C> [BOLD] Label <C> t=0 tn <C> t=0 fp <C> t=0 fn <C> t=0 tp <C> t=-0.25 tn <C> t=-0.25 fp <C> t=-0.25 fn <C> t=-0.25 tp <C> LCA tn <C> LCA fp <C> LCA fn <C> LCA tp <R> <C> Architektur & Garten <C> 2062 <C> 0 <C> 4 <C> 13 <C> 2061 <C> 1 <C> 2 <C> 15 <C> 2061 <C> 1 <C> 2 <C> 15 <R> <C> Ganzheitliches Bewusstsein <C> 1959 <C> 8 <C> 45 <C> 67 <C> 1951 <C> 16 <C> 29 <C> 83 <C> 1951 <C> 16 <C> 30 <C> 82 <R> <C> Glaube & Ethik <C> 1986 <C> 3 <C> 31 <C> 59 <C> 1983 <C> 6 <C> 23 <C> 67 <C> 1984 <C> 5 <C> 24 <C> 66 <R> <C> Kinderbuch & Jugendbuch <C> 1783 <C> 8 <C> 80 <C> 208 <C> 1759 <C> 32 <C> 50 <C> 238 <C> 1762 <C> 29 <C> 51 <C> 237 <R> <C> Künste <C> 2061 <C> 0 <C> 6 <C> 12 <C> 2061 <C> 0 <C> 4 <C> 14 <C> 2061 <C> 0 <C> 4 <C> 14 <R> <C> Literatur & Unterhaltung <C> 874 <C> 98 <C> 58 <C> 1049 <C> 801 <C> 171 <C> 31 <C> 1076 <C> 807 <C> 165 <C> 31 <C> 1076 <R> <C> Ratgeber <C> 1799 <C> 20 <C> 110 <C> 150 <C> 1781 <C> 38 <C> 75 <C> 185 <C> 1785 <C> 34 <C> 77 <C> 183 <R> <C> Sachbuch <C> 1701 <C> 40 <C> 148 <C> 190 <C> 1672 <C> 69 <C> 106 <C> 232 <C> 1674 <C> 67 <C> 111 <C> 227 <R> <C> Total <C> 14225 <C> 177 <C> 482 <C> 1748 <C> 14069 <C> 333 <C> 320 <C> 1910 <C> 14085 <C> 317 <C> 330 <C> 1900 <CAP> Table 3: Confusion matrix between label and others for threshold (t) =0 and =-0.25 (true negative: tp, false negative: fn, false positive: fp, true positive: tp)
<R> <C> [BOLD] Method <C> [BOLD] micro F-1 <R> <C> Hsklearn <C> 0.6544 <R> <C> Hsklearn, t=-0.25 <C> [BOLD] 0.6758 <R> <C> Hsklearn, t=-0.2 <C> 0.6749 <R> <C> Hsklearn, LCA normalized <C> 0.6645 <R> <C> Hsklearn, LCA <C> 0.6717 <R> <C> Hsklearn extended <C> 0.6589 <R> <C> Hsklearn extended, t=-0.25 <C> [BOLD] 0.6750 <R> <C> Hsklearn extended, t=-0.2 <C> [BOLD] 0.6765 <R> <C> own imp. <C> 0.6541 <R> <C> own imp., t=-0.25 <C> 0.6704 <R> <C> own imp., t=-0.2 <C> 0.6715 <CAP> Table 4: Preliminary experiments on subtask B, best three values marked in bold
<R> <C> [BOLD] Rank <C> [BOLD] Teamname <C> [BOLD] precision <C> [BOLD] recall <C> [BOLD] micro F-1 <R> <C> 1 <C> EricssonResearch <C> 0.8923 <C> 0.8432 <C> 0.8670 <R> <C> - <C> twistbytes LCA fixing null <C> 0.8536 <C> 0.8790 <C> 0.8661 <R> <C> - <C> twistbytes LCA-labelwise fixing null <C> 0.8536 <C> 0.8763 <C> 0.8648 <R> <C> 2 <C> twistbytes <C> 0.8650 <C> 0.8617 <C> 0.8634 <R> <C> 3 <C> DFKI-SLT <C> 0.8760 <C> 0.8472 <C> 0.8614 <R> <C> 4 <C> Raghavan <C> 0.8777 <C> 0.8383 <C> 0.8575 <R> <C> 5 <C> knowcup <C> 0.8525 <C> 0.8362 <C> 0.8443 <R> <C> 6 <C> fosil-hsmw <C> 0.8427 <C> 0.832 <C> 0.8373 <R> <C> 7 <C> Averbis <C> 0.8609 <C> 0.8083 <C> 0.8337 <R> <C> 8 <C> HSHL1 <C> 0.8244 <C> 0.8159 <C> 0.8201 <R> <C> 9 <C> Comtravo-DS <C> 0.8144 <C> 0.8255 <C> 0.8199 <R> <C> 10 <C> HUIU <C> 0.8063 <C> 0.8072 <C> 0.8067 <R> <C> 11 <C> LT-UHH <C> 0.8601 <C> 0.7481 <C> 0.8002 <CAP> Table 5: Results of subtask A, best micro F-1 score by team
<R> <C> [BOLD] Rank <C> [BOLD] Teamname <C> [BOLD] precision <C> [BOLD] recall <C> [BOLD] micro F-1 <R> <C> 1 <C> twistbytes <C> 0.7072 <C> 0.6487 <C> 0.6767 <R> <C> 2 <C> EricssonResearch <C> 0.7377 <C> 0.6174 <C> 0.6722 <R> <C> 3 <C> knowcup <C> 0.7507 <C> 0.5808 <C> 0.6549 <R> <C> 4 <C> Averbis <C> 0.677 <C> 0.614 <C> 0.644 <R> <C> 5 <C> DFKI-SLT <C> 0.7777 <C> 0.5151 <C> 0.6197 <R> <C> 6 <C> HSHL1 <C> 0.7216 <C> 0.5375 <C> 0.6161 <R> <C> 7 <C> Comtravo-DS <C> 0.7042 <C> 0.5274 <C> 0.6031 <R> <C> 8 <C> LT-UHH <C> 0.8496 <C> 0.3892 <C> 0.5339 <R> <C> 9 <C> NoTeam <C> 0.4166 <C> 0.276 <C> 0.332 <R> <C> 10 <C> DexieDuo <C> 0.0108 <C> 0.0034 <C> 0.0052 <CAP> Table 6: Results of subtask B, best micro F-1 score by team
<R> <C> Model (Ours) <C> en→fr <C> fr→en <C> en→de <C> de→en <R> <C> Text only <C> 49.52 <C> 43.48 <C> 30.10 <C> 32.35 <R> <C> T+V <C> 50.43 <C> 44.10 <C> 31.01 <C> 32.95 <R> <C> T+V+VSE <C> 51.72 <C> 45.73 <C> 32.67 <C> 34.94 <R> <C> T+V+CPT <C> 51.64 <C> 45.55 <C> 33.04 <C> 35.02 <R> <C> T+V+CBT <C> 51.23 <C> 45.21 <C> 32.51 <C> 33.87 <R> <C> T+V+VSE+CBT <C> 51.81 <C> 45.83 <C> 33.01 <C> 34.38 <R> <C> T+V+CPT+CBT <C> 51.85 <C> 45.65 <C> 33.61 <C> 35.85 <R> <C> T+V+VSE+CPT <C> 52.19 <C> [BOLD] 46.10 <C> 33.73 <C> 35.60 <R> <C> Full Model <C> [BOLD] 52.29 <C> 45.98 <C> [BOLD] 33.85 <C> [BOLD] 36.07 <CAP> Table 2: Ablation studies. BLEU comparison of different training objectives.
<R> <C> Model <C> Joint Goal <C> Turn Request <R> <C> SIM <C> 89.5 <C> 97.3 <R> <C> –Var. dropout <C> 88.6 <C> 97.1 <R> <C> –Char. CNN <C> 88.3 <C> 97.0 <R> <C> –Utt. features <C> 87.1 <C> 97.1 <CAP> Table 3: Ablation study of SIM on WoZ. We pick the model with highest joint goal score on development set and report its performance on test set.
<R> <C> [EMPTY] <C> [BOLD] SQuAD <C> [BOLD] NQ <R> <C> Questions <C> 87,599 <C> 74,097 <R> <C> Candidate Sentences <C> 91,707 <C> 239,013 <R> <C> Candidate Paragraphs <C> 18,896 <C> 58,699 <CAP> Table 1: The number of questions and candidates in the constructed datasets ReQA SQuAD and ReQA NQ.
<R> <C> Dataset <C> Data size <C> Happy <C> Sad <C> Angry <R> <C> Training <C> 30160 <C> 5191 <C> 6357 <C> 6027 <R> <C> Validation (Dev) <C> 2755 <C> 180 <C> 151 <C> 182 <R> <C> Testing <C> 5509 <C> 369 <C> 308 <C> 324 <CAP> Table 1: Used datasets.
<R> <C> [EMPTY] <C> RG-1 <C> RG-2 <C> RG-L <C> M.  [ITALIC] F1 <C> B. Acc <R> <C> Full <C> [BOLD] 16.79 <C> [BOLD] 6.98 <C> [BOLD] 16.55 <C> [BOLD] 56.31 <C> [BOLD] 59.30 <R> <C> -I <C> 16.38 <C> 6.70 <C> 16.15 <C> 55.95 <C> 58.57 <R> <C> -A <C> 16.50 <C> 6.81 <C> 16.25 <C> 55.36 <C> 57.14 <R> <C> -R <C> 16.31 <C> 6.64 <C> 16.06 <C> 54.38 <C> 58.34 <R> <C> -C <C> 15.33 <C> 5.72 <C> 15.10 <C> 55.92 <C> 58.50 <CAP> Table 4. Ablation study on the Sports dataset. “Full” indicates our full model. “-I” means the inconsistency loss is removed. “-A” means we replace the attention based classifier with a max pooling based classifier. “-R” means we remove the residual connection in the encoder part and only use one GRU layer as the encoder. “-C” indicates the copy mechanism is removed. RG-* is the F1-Measure of ROUGE-*.
<R> <C> [BOLD] Drop <C> [BOLD] words/sec <C> [BOLD] images/sec <R> <C> [BOLD] Ratio <C> [BOLD] (NMT) <C> [BOLD] (MNIST) <R> <C> 0% <C> 13100 <C> 2489 <R> <C> 90% <C> 14443 <C> 3174 <R> <C> 99% <C> 14740 <C> 3726 <R> <C> 99.9% <C> 14786 <C> 3921 <CAP> Table 1: Training speed with various drop ratios.
<R> <C> Model <C> AIMed Precision <C> AIMed Recall <C> AIMed F score <C> BioInfer Precision <C> BioInfer Recall <C> BioInfer F score <R> <C> PCNNraw  <C> 75.0 <C> 75.7 <C> 75.3 <C> 83.9 <C> 88.1 <C> 85.9 <R> <C> sdpCNN  <C> 64.8 <C> 67.8 <C> 66.0 <C> 73.4 <C> 77.0 <C> 75.2 <R> <C> MCCNN  <C> 76.4 <C> 69.0 <C> 72.4 <C> 81.3 <C> 78.1 <C> 79.6 <R> <C> TK+WE  <C> - <C> - <C> 69.7 <C> - <C> - <C> 74.0 <R> <C> DSTK  <C> 68.9 <C> 73.2 <C> 70.0 <C> 75.7 <C> 76.9 <C> 76.3 <R> <C> BiLSTM  <C> [BOLD] 78.8 <C> 75.2 <C> 76.9 <C> [BOLD] 87.0 <C> 87.4 <C> 87.2 <R> <C> BiLSTMour <C> 75.6 <C> 70.2 <C> 72.8 <C> 83.4 <C> 84.5 <C> 83.9 <R> <C> Ours <C> 77.8 <C> [BOLD] 77.0 <C> [BOLD] 77.4 <C> 85.6 <C> [BOLD] 89.1 <C> [BOLD] 87.3 <R> <C> PCNNraw is the original model in  are based on dataset where the authors did not use the standard number of instances. <C> PCNNraw is the original model in  are based on dataset where the authors did not use the standard number of instances. <C> PCNNraw is the original model in  are based on dataset where the authors did not use the standard number of instances. <C> PCNNraw is the original model in  are based on dataset where the authors did not use the standard number of instances. <C> PCNNraw is the original model in  are based on dataset where the authors did not use the standard number of instances. <C> PCNNraw is the original model in  are based on dataset where the authors did not use the standard number of instances. <C> PCNNraw is the original model in  are based on dataset where the authors did not use the standard number of instances. <CAP> TABLE III: Performance comparison with previous models.
<R> <C> [BOLD] Model <C> [BOLD] Pipeline <C> [BOLD] Joint <R> <C> BiLSTM-CRF <C> 100 <C> 200 <R> <C> + CNN-char <C> 100 <C> 250 <R> <C> + LSTM-char <C> 150 <C> 250 <CAP> Table 1: Optimal number of LSTM units.
<R> <C> Operation <C> Top subjects (2k) No. <C> Top subjects (2k) Perc. <C> Top relations (256) No. <C> Top relations (256) Perc. <C> Top objects (2k) No. <C> Top objects (2k) Perc. <R> <C> Before merging <C> 114,797 <C> 96.20 <C> 115,159 <C> 96.50 <C> 113,398 <C> 95.10 <R> <C> After merging <C> 115,581 <C> 96.86 <C> 116,008 <C> 97.21 <C> 113,962 <C> 95.50 <CAP> Table 4. Merging results of the R-VQA dataset for relation detector. After merging similar elements, the top element candidate in relation facts can cover more training data.
<R> <C> Models <C> Element (Accuracy) Sub. <C> Element (Accuracy) Rel. <C> Element (Accuracy) Obj. <C> Fact (Recall) R@1 <C> Fact (Recall) R@5 <C> Fact (Recall) R@10 <R> <C> V only <C> 3.25 <C> 39.19 <C> 2.11 <C> 0.14 <C> 0.43 <C> 0.72 <R> <C> Q only <C> 56.66 <C> 77.34 <C> 40.76 <C> 23.14 <C> 37.82 <C> 43.16 <R> <C> ours - no merge <C> 65.98 <C> 74.79 <C> 43.61 <C> 25.23 <C> 44.25 <C> 51.26 <R> <C> [BOLD] ours - final <C> [BOLD] 66.47 <C> [BOLD] 78.80 <C> [BOLD] 45.13 <C> [BOLD] 27.39 <C> [BOLD] 46.72 <C> [BOLD] 54.10 <CAP> Table 5. Results for the relation detector.
<R> <C> [BOLD] Method <C> All <C> Obj. <C> Num. <C> Color <C> Loc. <R> <C> 2VIS+BLSTM (Ren et al.,  2015 ) <C> 55.09 <C> 58.17 <C> 44.79 <C> 49.53 <C> 47.34 <R> <C> IMG-CNN (Ma et al.,  2016 ) <C> 58.40 <C> - <C> - <C> - <C> - <R> <C> DDPnet (Noh et al.,  2016 ) <C> 61.16 <C> - <C> - <C> - <C> - <R> <C> SAN (Yang et al.,  2016a ) <C> 61.60 <C> 65.40 <C> 48.60 <C> 57.90 <C> 54.00 <R> <C> AMA (Wu et al.,  2016b ) <C> 61.38 <C> 63.92 <C> 51.83 <C> 57.29 <C> 54.84 <R> <C> QRU (Li and Jia,  2016 ) <C> 62.50 <C> 65.06 <C> 46.90 <C> 60.50 <C> 56.99 <R> <C> [BOLD] RelAtt (ours) <C> [BOLD] 65.15 <C> [BOLD] 67.50 <C> [BOLD] 48.81 <C> [BOLD] 62.64 <C> [BOLD] 58.37 <CAP> Table 7. Evaluation results for our proposed model and compared methods on the COCO QA dataset.
<R> <C> [BOLD] Dataset <C> [BOLD] Method <C> [BOLD] F1 score <C> [BOLD] Recall <C> [BOLD] Accuracy <R> <C> BPE-Dilma <C> LR + w2v <C> 0.57395 <C> 0.6037 <C> 0.5952 <R> <C> BPE-Dilma <C> LR + tfidf <C> [BOLD] 0.64771 <C> 0.6443 <C> [BOLD] 0.7128 <R> <C> BPE-Dilma <C> LR + d2v <C> 0.61354 <C> 0.6071 <C> 0.7256 <R> <C> BPE-Dilma <C> CNN <C> 0.63373 <C> 0.6295 <C> 0.7051 <R> <C> BPE-Dilma <C> RCNN <C> 0.64442 <C> [BOLD] 0.6586 <C> 0.6816 <R> <C> [EMPTY] <C> Hybrid <C> 0.52496 <C> 0.5855 <C> 0.5295 <R> <C> [EMPTY] <C> SotA <C> − <C> − <C> − <R> <C> BPE-Serra <C> LR + w2v <C> 0.35156 <C> 0.4398 <C> 0.3915 <R> <C> BPE-Serra <C> LR + tfidf <C> 0.41105 <C> 0.5546 <C> 0.4475 <R> <C> BPE-Serra <C> LR + d2v <C> 0.50553 <C> 0.6028 <C> 0.5915 <R> <C> BPE-Serra <C> CNN <C> 0.42404 <C> 0.5929 <C> 0.4558 <R> <C> BPE-Serra <C> RCNN <C> 0.52862 <C> 0.5975 <C> 0.6426 <R> <C> [EMPTY] <C> Hybrid <C> [BOLD] 0.57451 <C> [BOLD] 0.6073 <C> [BOLD] 0.7344 <R> <C> [EMPTY] <C> SotA <C> − <C> − <C> − <R> <C> Buscape-1 <C> LR + w2v <C> 0.72324 <C> 0.7250 <C> 0.7250 <R> <C> Buscape-1 <C> LR + tfidf <C> 0.74693 <C> 0.7480 <C> 0.7480 <R> <C> Buscape-1 <C> LR + d2v <C> 0.64276 <C> 0.6465 <C> 0.6465 <R> <C> Buscape-1 <C> CNN <C> 0.67135 <C> 0.6870 <C> 0.6870 <R> <C> Buscape-1 <C> RCNN <C> 0.76542 <C> 0.7654 <C> 0.7654 <R> <C> [EMPTY] <C> Hybrid <C> [BOLD] 0.76681 <C> [BOLD] 0.7695 <C> [BOLD] 0.7695 <R> <C> [EMPTY] <C> SotA <C> 0.8892 <C> − <C> 0.8894 <R> <C> Buscape-2 <C> LR + w2v <C> 0.68146 <C> 0.6903 <C> 0.6910 <R> <C> Buscape-2 <C> LR + tfidf <C> 0.77252 <C> 0.7738 <C> 0.7742 <R> <C> Buscape-2 <C> LR + d2v <C> 0.70175 <C> 0.7027 <C> 0.7030 <R> <C> Buscape-2 <C> CNN <C> 0.70484 <C> 0.7115 <C> 0.7122 <R> <C> Buscape-2 <C> RCNN <C> 0.76563 <C> 0.7658 <C> 0.7657 <R> <C> [EMPTY] <C> Hybrid <C> [BOLD] 0.79171 <C> [BOLD] 0.7930 <C> [BOLD] 0.7934 <R> <C> [EMPTY] <C> SotA <C> 0.8935 <C> − <C> 0.8935 <R> <C> Mercado Livre <C> LR + w2v <C> 0.68616 <C> 0.7048 <C> 0.7066 <R> <C> Mercado Livre <C> LR + tfidf <C> 0.83283 <C> 0.8329 <C> 0.8328 <R> <C> Mercado Livre <C> LR + d2v <C> 0.80894 <C> 0.8093 <C> 0.8097 <R> <C> Mercado Livre <C> CNN <C> 0.77455 <C> 0.7800 <C> 0.7813 <R> <C> Mercado Livre <C> RCNN <C> 0.85612 <C> 0.8561 <C> 0.8563 <R> <C> [EMPTY] <C> Hybrid <C> [BOLD] 0.86141 <C> [BOLD] 0.8614 <C> [BOLD] 0.8614 <R> <C> [EMPTY] <C> SotA <C> 0.9583 <C> − <C> 0.9583 <CAP> TABLE III: Results obtained by each method trained on the distant supervision corpus.
<R> <C> [BOLD] Model <C> [BOLD] PTB NLL <C> [BOLD] PTB PPL <C> [BOLD] BC NLL <C> [BOLD] BC PPL <C> [BOLD] Yahoo NLL <C> [BOLD] Yahoo PPL <C> [BOLD] Yelp NLL <C> [BOLD] Yelp PPL <R> <C> LSTM-LM mikolov2011extensions <C> 101.8 <C> 104.2 <C> 37.4 <C> 64.0 <C> 334.9 <C> 66.2 <C> 362.7 <C> 42.6 <R> <C> LSTM-VAE bowman2016generating <C> 102.0 <C> 104.8 <C> 28.0 <C> 22.5 <C> 337.4 <C> 68.3 <C> 372.2 <C> 47.0 <R> <C> CNN-VAE yang2017improved <C> - <C> - <C> - <C> - <C> 333.9 <C> 65.4 <C> [BOLD] 361.9 <C> [BOLD] 42.3 <R> <C> SA Baseline 1 <C> 95.3 <C> 77.4 <C> 26.1 <C> 18.1 <C> [BOLD] 325.3 <C> [BOLD] 58.7 <C> 362.8 <C> 42.7 <R> <C> SA Baseline 2 <C> 94.8 <C> 75.5 <C> 23.6 <C> 13.7 <C> 331.9 <C> 63.8 <C> 364.3 <C> 43.4 <R> <C> LSTM-SAVAE (Ours) <C> [BOLD] 92.9 <C> [BOLD] 69.5 <C> [BOLD] 23.2 <C> [BOLD] 13.2 <C> 331.2 <C> 63.2 <C> 363.8 <C> 43.1 <CAP> Table 2: Language modeling results on the test sets. Lower is better. For SAVAE, syntax is only provided at training while for SA baseline syntactic information is presented in both training and evaluation. SAVAE has better reconstruction qualities across all four data sets compared to its non-syntax-aware counterpart.
<R> <C> [BOLD] Input <C> [BOLD] Recall @K Top 1 <C> [BOLD] Recall @K Top 3 <C> [BOLD] Recall @K Top 10 <C> [BOLD] Lev <R> <C> (vae) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> syntax  [BOLD] y <C> 0.24 <C> 0.33 <C> 0.41 <C> 3.22 <R> <C> (savae) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> text  [BOLD] x <C> 0.35 <C> 0.49 <C> 0.59 <C> 1.83 <R> <C> syntax  [BOLD] y <C> 0.47 <C> 0.60 <C> 0.69 <C> 1.53 <CAP> Table 3: Evaluation results for generated syntax with different models and inputs on BC data set. Recall @K is the exact match recall rate from top K predictions. Lev is the average Levenshtein distance between gold and inferred syntax tokens.
<R> <C> [BOLD] Dataset <C> [BOLD] F1 Score <R> <C> MsMarco <C> 0.86 <R> <C> HotpotQA <C> 0.88 <R> <C> ReCoRd <C> 0.73 <R> <C> MultiRC <C> 0.75 <R> <C> NewsQA <C> 0.87 <R> <C> DROP <C> 0.85 <R> <C> Micro Average <C> 0.82 <CAP> Table 2: Inter-Annotator agreement F1 scores, averaged for each dataset
<R> <C> [ITALIC]  [BOLD] L <C> [BOLD] Epochs <C> [ITALIC]  [BOLD] d <C> [BOLD] Scores  [BOLD] English <C> [BOLD] Scores  [BOLD] German <C> [BOLD] Scores  [BOLD] Latin <C> [BOLD] Scores  [BOLD] Swedish <C> [BOLD] Scores  [BOLD] Average <R> <C> 5 <C> 20 <C> 5 <C> 0.012 <C> 0.366 <C> 0.391 <C> 0.15 <C> 0.23 <R> <C> 5 <C> 20 <C> 10 <C> -0.144 <C> 0.362 <C> 0.394 <C> 0.114 <C> 0.182 <R> <C> 5 <C> 20 <C> 20 <C> 0.193 <C> 0.402 <C> 0.368 <C> 0.244 <C> 0.302 <R> <C> 5 <C> 20 <C> 50 <C> [BOLD] 0.278 <C> 0.446 <C> 0.315 <C> 0.229 <C> 0.317 <R> <C> 5 <C> 20 <C> 100 <C> 0.226 <C> 0.312 <C> 0.254 <C> 0.186 <C> 0.244 <R> <C> 5 <C> 60 <C> 5 <C> -0.018 <C> 0.354 <C> 0.432 <C> 0.121 <C> 0.222 <R> <C> 5 <C> 60 <C> 10 <C> -0.095 <C> 0.283 <C> 0.379 <C> 0.108 <C> 0.169 <R> <C> 5 <C> 60 <C> 20 <C> 0.212 <C> 0.41 <C> 0.377 <C> 0.324 <C> 0.331 <R> <C> 5 <C> 60 <C> 50 <C> 0.228 <C> 0.464 <C> 0.329 <C> [BOLD] 0.366 <C> [BOLD] 0.347 <R> <C> 5 <C> 60 <C> 100 <C> 0.269 <C> 0.312 <C> 0.276 <C> 0.309 <C> 0.291 <R> <C> 10 <C> 20 <C> 5 <C> 0.011 <C> 0.334 <C> [BOLD] 0.485 <C> 0.218 <C> 0.262 <R> <C> 10 <C> 20 <C> 10 <C> -0.036 <C> 0.351 <C> 0.409 <C> 0.146 <C> 0.218 <R> <C> 10 <C> 20 <C> 20 <C> 0.144 <C> [BOLD] 0.479 <C> 0.397 <C> 0.187 <C> 0.302 <R> <C> 10 <C> 20 <C> 50 <C> 0.198 <C> 0.477 <C> 0.275 <C> 0.267 <C> 0.304 <R> <C> 10 <C> 20 <C> 100 <C> 0.199 <C> 0.349 <C> 0.257 <C> 0.227 <C> 0.258 <CAP> Table 1: Results of the post-evaluation experiments
<R> <C> [EMPTY] <C> [BOLD] GT <C> [BOLD] Sgl <C> [BOLD] Sha <C> [BOLD] Sep <C> [BOLD] Gen <C> [BOLD] Sha 600 <C> [BOLD] Sha 900 <C> [BOLD] Gen 600 <C> [BOLD] Gen 900 <R> <C> DE <C> 4.8±4.6 <C> 7.3±6.0 <C> 8.3±6.0 <C> 15.3±6.0 <C> *5.8±5.3* <C> 13.2±8.90 <C> 12.4±8.0 <C> 15.6±9.40 <C> 12.5±9.30 <R> <C> EL <C> 8.7±6.9 <C> [ITALIC] N/A <C> 11.4±8.3 <C> 22.2±8.30 <C> 11.6±7.10 <C> 16.8±9.70 <C> 16.0±10.2 <C> 14.2±8.7 <C> 14.7±9.80 <R> <C> SP <C> 3.9±4.6 <C> 07.0±10.8 <C> 7.2±6.5 <C> 10.2±8.10 <C> 7.0±9.8 <C> 9.8±7.5 <C> 9.9±8.4 <C> 8.1±6.0 <C> *7.6±5.9* <R> <C> FI <C> 06.9±10.4 <C> 18.6±12.6 <C> 10.3±8.0 <C> 18.1±11.4 <C> 10.4±7.00 <C> 18.2±12.2 <C> 18.4±13.2 <C> *13.2±10.9* <C> 14.0±10.6 <R> <C> FR <C> 11.2±7.30 <C> 25.2±12.6 <C> 30.0±14.3 <C> 54.5±21.9 <C> *19.0±12.9* <C> 40.2±15.8 <C> 37.6±16.2 <C> 32.9±13.2 <C> *27.2±12.2* <R> <C> HU <C> 6.3±6.1 <C> 15.8±9.50 <C> 15.9±10.6 <C> 18.8±9.90 <C> *13.5±8.3* <C> 21.4±10.4 <C> 21.3±13.0 <C> *16.5±10.4* <C> 18.0±10.4 <R> <C> JP <C> 19.0±9.30 <C> 28.8±11.3 <C> 27.2±11.8 <C> 33.7±13.5 <C> 25.1±12.2 <C> 32.5±12.8 <C> 32.2±15.0 <C> 29.9±13.0 <C> 30.9±13.5 <R> <C> NL <C> 14.5±7.40 <C> 33.4±13.8 <C> 31.6±12.5 <C> 49.0±17.4 <C> *22.6±9.6* <C> 37.8±13.5 <C> 30.4±10.2 <C> 32.8±12.3 <C> 28.3±9.8 <R> <C> RU <C> 12.3±15.0 <C> 45.5±24.1 <C> 44.4±21.9 <C> 58.1±24.7 <C> *34.5±21.3* <C> 60.4±18.6 <C> 47.0±20.5 <C> 38.5±20.1 <C> *34.4±17.9* <R> <C> ZH <C> 14.6±11.8 <C> 62.8±18.5 <C> 28.6±15.9 <C> 27.3±14.8 <C> *20.5±13.6* <C> 40.2±15.2 <C> 39.8±18.8 <C> 33.0±15.5 <C> *28.4±15.6* <CAP> Table 2: Left: CERs of ground-truth recordings (GT) and recordings produced by monolingual and the three examined multilingual models. Right: CERs of the recordings synthesized by Gen and Sha trained on just 600 or 900 training examples per language. Best results for the given language are shown in bold; “*” denotes statistical significance (established using paired t-test; p<0.05).
<R> <C> [BOLD] Fluency <C> German <C> [BOLD] Sha 3.0±1.1 <C> [BOLD] Sep 2.6±1.0 <C> [BOLD] Gen *3.4±0.9 <R> <C> [BOLD] Fluency <C> French <C> 2.8±1.0 <C> 2.6±1.0 <C> *3.5±0.9 <R> <C> [BOLD] Fluency <C> Dutch <C> 3.1±0.9 <C> 2.5±1.1 <C> *3.7±1.0 <R> <C> [BOLD] Fluency <C> Russian <C> 2.8±1.0 <C> 2.5±1.0 <C> *3.4±0.9 <R> <C> [BOLD] Fluency <C> Chinese <C> 2.7±1.3 <C> 2.6±1.2 <C> *3.5±1.2 <R> <C> [BOLD] Fluency <C> [BOLD] All <C> 2.9±1.1 <C> 2.5±1.1 <C> *3.5±1.0 <R> <C> [BOLD] Accuracy <C> German <C> 3.3±1.1 <C> 3.1±1.2 <C> *3.7±1.0 <R> <C> [BOLD] Accuracy <C> French <C> 3.1±1.1 <C> 2.7±1.2 <C> *3.7±0.9 <R> <C> [BOLD] Accuracy <C> Dutch <C> 3.4±1.0 <C> 2.5±1.2 <C> *3.9±1.1 <R> <C> [BOLD] Accuracy <C> Russian <C> 3.0±1.2 <C> 2.6±1.2 <C> *3.6±1.0 <R> <C> [BOLD] Accuracy <C> Chinese <C> 2.9±1.4 <C> 2.8±1.4 <C> *3.5±1.2 <R> <C> [BOLD] Accuracy <C> [BOLD] All <C> 3.1±1.2 <C> 2.7±1.2 <C> *3.7±1.1 <R> <C> [BOLD] Word skips <C> [BOLD] Word skips <C> 41/400 <C> 38/400 <C> [BOLD] 11/400 <CAP> Table 3: Mean (with std. dev.) ratings of fluency, naturalness, voice stability (top) and pronunciation accuracy (middle). The bottom row shows the number of sentences with word skips.
<R> <C> Model <C> #Params <C> dev ppl <C> test ppl <R> <C> [ITALIC] Small models <C> [ITALIC] Small models <C> [ITALIC] Small models <C> [ITALIC] Small models <R> <C> Grave et al. ( 2017b ) – LSTM <C> - <C> - <C> 48.7 <R> <C> Bai et al. (2018) – TCN <C> - <C> - <C> 45.2 <R> <C> Dauphin et al. ( 2017 ) – GCNN-8 <C> - <C> - <C> 44.9 <R> <C> Grave et al. ( 2017b ) – LSTM + Neural cache <C> - <C> - <C> 40.8 <R> <C> Merity et al. ( 2018 ) – 4-layer QRNN <C> 151M <C> 32.0 <C> 33.0 <R> <C> Rae et al. ( 2018 ) – LSTM + Hebbian + Cache <C> - <C> 29.7 <C> 29.9 <R> <C> Dai et al. ( 2019 ) – Transformer-XL Standard <C> 151M <C> 23.1 <C> 24.0 <R> <C> All-attention network + adaptive span <C> 133M <C> 19.7 <C> [BOLD] 20.6 <R> <C> Best published result with a large model (Dai et al.,  2019 ) <C> 257M <C> 17.7 <C> [BOLD] 18.3 <CAP> Table 3: Comparison with the state of the art on word level language modeling on WikiText-103. We report perplexity (ppl) for the dev and test sets as well as the number of parameters.
<R> <C> [BOLD] Models <C> [BOLD] Informal to Formal BLEU↑ <C> [BOLD] Informal to Formal Accuracy↑ <C> [BOLD] Informal to Formal Overall↑ <C> [BOLD] Formal to Informal BLEU↑ <C> [BOLD] Formal to Informal Accuracy↑ <C> [BOLD] Formal to Informal Overall↑ <R> <C> CopyNMT <C> 0.263 <C> 0.774 <C> 0.196 <C> 0.280 <C> 0.503 <C> 0.180 <R> <C> TS <C> 0.240 <C> 0.801 <C> 0.184 <C> 0.271 <C> 0.527 <C> 0.179 <R> <C> CP <C> 0.272 <C> 0.749 <C> 0.199 <C> 0.281 <C> 0.487 <C> 0.178 <R> <C> TS+CP <C> 0.259 <C> 0.772 <C> 0.194 <C> 0.271 <C> 0.527 <C> 0.179 <R> <C> CP→TS <C> 0.227 <C> [BOLD] 0.817 <C> 0.178 <C> 0.259 <C> [BOLD] 0.5441 <C> 0.175 <R> <C> TS→CP <C> [BOLD] 0.286 <C> 0.723 <C> [BOLD] 0.205 <C> [BOLD] 0.298 <C> 0.516 <C> [BOLD] 0.189 <CAP> Table 1: Ablation study to demonstrate the improvement of the addition of the loss terms on formality transfer task.
<R> <C> Model <C> Acc.(%) <C> F1(%) <R> <C> AllPositive <C> 66.50 <C> 79.87 <R> <C> Tf-Idf <C> 70.31 <C> 77.62 <R> <C> DSSM <C> 70.09 <C> 80.96 <R> <C> CDSSM <C> 69.80 <C> 80.42 <R> <C> Arc-I <C> 69.60 <C> 80.27 <R> <C> Arc-II <C> 69.90 <C> 80.91 <R> <C> MP-Ind <C> 75.77 <C> 82.66 <R> <C> MP-Cos <C> 75.13 <C> 82.45 <R> <C> MP-Dot <C> [BOLD] 75.94 <C> [BOLD] 83.01 <CAP> Table 1: Results on MSRP.
<R> <C> Model <C> Acc.(%) <C> F1(%) <R> <C> AllPositive <C> 33.33 <C> 50.00 <R> <C> Tf-Idf <C> 82.63 <C> 70.21 <R> <C> DSSM <C> 71.97 <C> 29.88 <R> <C> CDSSM <C> 69.84 <C> 19.97 <R> <C> Arc-I <C> 84.51 <C> 76.79 <R> <C> Arc-II <C> 86.48 <C> 79.57 <R> <C> MP-Ind <C> 73.76 <C> 44.71 <R> <C> MP-Cos <C> 86.65 <C> 79.70 <R> <C> MP-Dot <C> [BOLD] 88.73 <C> [BOLD] 82.86 <CAP> Table 2: Results on the task of paper citation matching.
<R> <C> [BOLD] Word <C> the <C> with <C> for <C> be <C> are <R> <C> [BOLD] Len <C> 0.448 <C> 0.508 <C> 0.509 <C> 0.510 <C> 0.515 <R> <C> [BOLD] Word <C> robotics <C> java <C> snakes <C> musical <C> rfid <R> <C> [BOLD] Len <C> 1.572 <C> 1.576 <C> 1.589 <C> 1.610 <C> 1.878 <CAP> Table 3: The norm of learned word embeddings on the task of paper citation matching.
<R> <C> Lang. <C> Metric <C> Original <C> Shift_Ori <C> Shift_EN <C> De-Align <C> Hybrid_Ori <C> Hybrid_EN <R> <C> ES <C> MWEAT–Diff <C> 3.6918 <C> 0.3090 <C> 0.3324 <C> 3.5748 <C> [BOLD] 0.3090 <C> 2.2494 <R> <C> ES <C> MWEAT–p-value <C> 0.0000 <C> 0.1130 <C> 0.0010 <C> 0.0010 <C> [BOLD] 0.7330 <C> 0.0020 <R> <C> FR <C> MWEAT–Diff <C> 2.3437 <C> 0.2446 <C> 0.3882 <C> 2.3436 <C> [BOLD] 0.2446 <C> 1.1758 <R> <C> FR <C> MWEAT–p-value <C> 0.0000 <C> 0.1470 <C> 0.0010 <C> 0.0020 <C> [BOLD] 0.5290 <C> 0.0910 <R> <C> ES <C> Word Similarity <C> [BOLD] 0.7392 <C> 0.7363 <C> 0.7359 <C> [BOLD] 0.7392 <C> 0.7358 <C> 0.7356 <R> <C> FR <C> Word Similarity <C> [BOLD] 0.7294 <C> 0.7218 <C> 0.7218 <C> 0.7156 <C> 0.7218 <C> 0.7218 <CAP> Table 1: Analyses on Spanish and French monolingual embeddings before and after bias mitigation. Results show that the original Spanish and French embeddings exhibit strong bias and Hybrid_Ori significantly reduces the bias in the embedding to to an insignificant level (p-value > 0.05).
<R> <C> Task <C> Metric <C> Original <C> Shift_Ori <C> Shift_EN <C> De-Align <C> Hyrid_Ori <C> Hyrid_EN <R> <C> Word Translation <C> Word Translation <C> Word Translation <C> Word Translation <C> Word Translation <C> Word Translation <C> Word Translation <C> Word Translation <R> <C> EN→ES <C> P@1/P@5 <C> 79.2/89.0 <C> [BOLD] 80.7/90.3 <C> [BOLD] 80.7/90.3 <C> 76.5/88.9 <C> [BOLD] 80.7/90.3 <C> [BOLD] 80.7/90.3 <R> <C> ES→EN <C> P@1/P@5 <C> 79.2/89.0 <C> 79.2/89.0 <C> 79.2/89.0 <C> [BOLD] 80.1/90.7 <C> 79.2/89.0 <C> 79.2/89.0 <R> <C> EN→FR <C> P@1/P@5 <C> 78.2/89.4 <C> [BOLD] 79.9/91.1 <C> [BOLD] 79.9/91.1 <C> 74.3/87.8 <C> [BOLD] 79.9/91.1 <C> [BOLD] 79.9/91.1 <R> <C> FR→EN <C> P@1/P@5 <C> [BOLD] 76.1/88.1 <C> [BOLD] 76.1/88.1 <C> [BOLD] 76.1/88.1 <C> 74.4/87.2 <C> [BOLD] 76.1/88.1 <C> [BOLD] 76.1/88.1 <R> <C> Word Pair Translation <C> Word Pair Translation <C> Word Pair Translation <C> Word Pair Translation <C> Word Pair Translation <C> Word Pair Translation <C> Word Pair Translation <C> Word Pair Translation <R> <C> EN→ES <C> ASD <C> 0.1082 <C> 0.0961 <C> 0.0961 <C> 0.0827 <C> [BOLD] 0.0755 <C> 0.0772 <R> <C> EN→ES <C> F_MRR <C> 0.2073 <C> 0.2507 <C> 0.2507 <C> 0.2919 <C> 0.3450 <C> 0.3150 <R> <C> EN→ES <C> M_MRR <C> 0.6940 <C> 0.6766 <C> 0.6766 <C> 0.6775 <C> 0.6398 <C> 0.6696 <R> <C> EN→ES <C> MRR Diff <C> 0.4867 <C> 0.4259 <C> 0.4259 <C> 0.3856 <C> [BOLD] 0.2949 <C> 0.3546 <R> <C> EN→FR <C> ASD <C> 0.1208 <C> 0.1048 <C> 0.1082 <C> 0.0892 <C> [BOLD] 0.0735 <C> 0.0805 <R> <C> EN→FR <C> F_MRR <C> 0.1663 <C> 0.2101 <C> 0.1943 <C> 0.2679 <C> 0.3128 <C> 0.2975 <R> <C> EN→FR <C> M_MRR <C> 0.6549 <C> 0.6313 <C> 0.6419 <C> 0.6610 <C> 0.6393 <C> 0.6467 <R> <C> EN→FR <C> MRR Diff <C> 0.4886 <C> 0.4212 <C> 0.4476 <C> 0.3931 <C> [BOLD] 0.3265 <C> 0.3492 <CAP> Table 2: Results on word translation and word pair translation based on Spanish-English and French-English bilingual embeddings. We find that the original bilingual embeddings have a large discrepancy between the two genders, indicated by the average cosine similarity difference (ASD) and mean reciprocal ranks (MRR) difference. After applying the mitigation methods, both ASD and MRR difference drop.
<R> <C> Models <C> Fisher text <C> PPL <R> <C> Baseline LM <C> x <C> 74.15 <R> <C> Baseline LM <C> o <C> 72.81 <R> <C> Proposed Conversational LM <C> x <C> 67.03 <R> <C> Proposed Conversational LM <C> o <C> 64.30 <CAP> Table 2: Perplexities on a held-out set of our proposed conversational context LM and baselines.
<R> <C> POS <C> All <C> Chosen <R> <C> NOUN <C> 58 <C> 49 <R> <C> ADJECTIVE <C> 40 <C> 34 <R> <C> VERB <C> 15 <C> 13 <R> <C> PARTICLE <C> 12 <C> 12 <R> <C> PRE <C> 6 <C> 6 <R> <C> PREPOSITION <C> 5 <C> 5 <R> <C> NUMBER <C> 4 <C> 1 <R> <C> total <C> 140 <C> 120 <CAP> Table 1. ”POS tags incident and chosen as preferential e.g. in text synthesis. The official dictionary often relates tokens to more than one POS tag. For the text highlighting Plugin, for example, a token has to have an established tag to have a defined color. On the Chosen column, the tokens were regarded only once by choosing the first occurrence of [’PRE’, ’VERB’, ’PREPOSITION’, ’PARTICLE’, ’ADJECTIVE’, ’NOUN’, ’NUMBER’] in the official dictionary.
<R> <C> Language <C> average value lower <C> average value loss <C> average value pc upper <C> average value upper <C> relative difference to loss lower <C> relative difference to loss pc upper <C> relative difference to loss upper <R> <C> Arabic <C> 0.66925 <C> 0.67257 <C> [BOLD] 0.67312 <C> 0.68143 <C> 0.00182 <C> [BOLD] 0.00029 <C> 0.00587 <R> <C> Basque <C> [BOLD] 0.58260 <C> 0.58318 <C> 0.58389 <C> 0.62543 <C> [BOLD] 0.00035 <C> 0.00038 <C> 0.02732 <R> <C> Catalan <C> 0.58009 <C> 0.58793 <C> [BOLD] 0.58931 <C> 0.60644 <C> 0.00424 <C> [BOLD] 0.00069 <C> 0.00961 <R> <C> Chinese <C> [BOLD] 0.56515 <C> 0.56711 <C> 0.57156 <C> 0.62921 <C> [BOLD] 0.00121 <C> 0.00302 <C> 0.03984 <R> <C> Czech <C> [BOLD] 0.57521 <C> 0.58357 <C> 0.59401 <C> 0.62883 <C> [BOLD] 0.00476 <C> 0.00685 <C> 0.02662 <R> <C> English <C> 0.55267 <C> 0.56383 <C> [BOLD] 0.56884 <C> 0.59494 <C> 0.00633 <C> [BOLD] 0.00294 <C> 0.01767 <R> <C> Greek <C> 0.56123 <C> 0.57443 <C> [BOLD] 0.57983 <C> 0.61256 <C> 0.00731 <C> [BOLD] 0.00296 <C> 0.02256 <R> <C> Hungarian <C> [BOLD] 0.46495 <C> 0.46672 <C> 0.46873 <C> 0.48797 <C> [BOLD] 0.00097 <C> 0.00114 <C> 0.01165 <R> <C> Italian <C> 0.62033 <C> 0.62612 <C> [BOLD] 0.62767 <C> 0.64356 <C> 0.00307 <C> [BOLD] 0.00082 <C> 0.00883 <R> <C> Turkish <C> [BOLD] 0.60143 <C> 0.60215 <C> 0.60660 <C> 0.63560 <C> [BOLD] 0.00060 <C> 0.00329 <C> 0.02139 <R> <C> Bulgarian <C> 0.61415 <C> 0.62257 <C> [BOLD] 0.62433 <C> 0.64497 <C> 0.00456 <C> [BOLD] 0.00086 <C> 0.01233 <R> <C> Danish <C> 0.67350 <C> 0.67904 <C> [BOLD] 0.68119 <C> 0.69436 <C> 0.00291 <C> [BOLD] 0.00108 <C> 0.00916 <R> <C> Dutch <C> 0.69201 <C> 0.70600 <C> [BOLD] 0.71105 <C> 0.74008 <C> 0.00709 <C> [BOLD] 0.00251 <C> 0.01862 <R> <C> German <C> [BOLD] 0.54581 <C> 0.54755 <C> 0.55080 <C> 0.58182 <C> [BOLD] 0.00104 <C> 0.00208 <C> 0.02033 <R> <C> Japanese <C> [BOLD] 0.60515 <C> 0.60515 <C> [BOLD] 0.60515 <C> 0.60654 <C> [BOLD] 0.00000 <C> [BOLD] 0.00000 <C> 0.00115 <R> <C> Portuguese <C> 0.58880 <C> 0.60063 <C> [BOLD] 0.60185 <C> 0.61780 <C> 0.00651 <C> [BOLD] 0.00067 <C> 0.00867 <R> <C> Slovene <C> 0.56155 <C> 0.56860 <C> [BOLD] 0.57135 <C> 0.60373 <C> 0.00396 <C> [BOLD] 0.00153 <C> 0.01979 <R> <C> Spanish <C> 0.58247 <C> 0.59119 <C> [BOLD] 0.59277 <C> 0.61273 <C> 0.00487 <C> [BOLD] 0.00089 <C> 0.01197 <R> <C> Swedish <C> 0.57543 <C> 0.58636 <C> [BOLD] 0.58933 <C> 0.61104 <C> 0.00585 <C> [BOLD] 0.00153 <C> 0.01383 <R> <C> Average <C> 0.59009 <C> 0.59656 <C> [BOLD] 0.59954 <C> 0.62416 <C> 0.00355 <C> [BOLD] 0.00176 <C> 0.01513 <CAP> Table 1: Average value of the different bounds and the loss, and of the relative differences from each bound to the loss, on CoNLL-XI (first block) and CoNLL-X (second block) datasets during 100,000 transitions. For each language, we show in boldface the average value and relative difference of the bound that is closer to the loss.
<R> <C> [EMPTY] <C> static <C> static <C> dynamic monotonic <C> dynamic monotonic <C> dynamic non-monotonic lower <C> dynamic non-monotonic lower <C> dynamic non-monotonic pc upper <C> dynamic non-monotonic pc upper <C> dynamic non-monotonic upper <C> dynamic non-monotonic upper <R> <C> Language <C> UAS <C> LAS <C> UAS <C> LAS <C> UAS <C> LAS <C> UAS <C> LAS <C> UAS <C> LAS <R> <C> Arabic <C> 80.67 <C> 66.51 <C> 82.76∗ <C> 68.48∗ <C> 83.29∗ <C> 69.14∗ <C> 83.18∗ <C> 69.05∗ <C> [BOLD] 83.40† <C> [BOLD] 69.29† <R> <C> Basque <C> 76.55 <C> 66.05 <C> [BOLD] 77.49† <C> [BOLD] 67.31† <C> 74.61 <C> 65.31 <C> 74.69 <C> 65.18 <C> 74.27 <C> 64.78 <R> <C> Catalan <C> 90.52 <C> 85.09 <C> [BOLD] 91.37∗ <C> [BOLD] 85.98∗ <C> 90.51 <C> 85.35 <C> 90.40 <C> 85.30 <C> 90.44 <C> 85.35 <R> <C> Chinese <C> 84.93 <C> 80.80 <C> 85.82 <C> 82.15 <C> 86.55∗ <C> [BOLD] 82.53∗ <C> 86.29∗ <C> 82.27∗ <C> [BOLD] 86.60∗ <C> 82.51∗ <R> <C> Czech <C> 78.49 <C> 61.77 <C> 80.21∗ <C> 63.52∗ <C> 81.32† <C> 64.89† <C> 81.33† <C> 64.81† <C> [BOLD] 81.49† <C> [BOLD] 65.18† <R> <C> English <C> 85.35 <C> 84.29 <C> 87.47∗ <C> 86.55∗ <C> 88.44† <C> 87.37† <C> 88.23† <C> 87.22† <C> [BOLD] 88.50† <C> [BOLD] 87.55† <R> <C> Greek <C> 79.47 <C> 69.35 <C> 80.76 <C> 70.43 <C> 80.90 <C> 70.46 <C> 80.84 <C> 70.34 <C> [BOLD] 81.02∗ <C> [BOLD] 70.49∗ <R> <C> Hungarian <C> 77.65 <C> 68.32 <C> [BOLD] 78.84∗ <C> [BOLD] 70.16∗ <C> 78.67∗ <C> 69.83∗ <C> 78.47∗ <C> 69.66∗ <C> 78.65∗ <C> 69.74∗ <R> <C> Italian <C> 84.06 <C> 79.79 <C> 84.30 <C> 80.17 <C> 84.38 <C> 80.30 <C> [BOLD] 84.64 <C> [BOLD] 80.52 <C> 84.47 <C> 80.32 <R> <C> Turkish <C> [BOLD] 81.28 <C> 70.97 <C> 81.14 <C> [BOLD] 71.38 <C> 80.65 <C> 71.15 <C> 80.80 <C> 71.29 <C> 80.60 <C> 71.07 <R> <C> Bulgarian <C> 89.13 <C> 85.30 <C> 90.45∗ <C> 86.86∗ <C> 91.36† <C> 87.88† <C> 91.33† <C> 87.89† <C> [BOLD] 91.73† <C> [BOLD] 88.26† <R> <C> Danish <C> 86.00 <C> 81.49 <C> 86.91∗ <C> [BOLD] 82.75∗ <C> 86.83∗ <C> 82.63∗ <C> 86.89∗ <C> 82.74∗ <C> [BOLD] 86.94∗ <C> 82.68∗ <R> <C> Dutch <C> 81.54 <C> 78.46 <C> 82.07 <C> 79.26 <C> 82.78∗ <C> 79.64∗ <C> 82.80∗ <C> 79.68∗ <C> [BOLD] 83.02† <C> [BOLD] 79.92† <R> <C> German <C> 86.97 <C> 83.91 <C> [BOLD] 87.95∗ <C> [BOLD] 85.17∗ <C> 87.31 <C> 84.37 <C> 87.18 <C> 84.22 <C> 87.48 <C> 84.54 <R> <C> Japanese <C> 93.63 <C> 92.20 <C> 93.67 <C> 92.33 <C> [BOLD] 94.02 <C> [BOLD] 92.68 <C> [BOLD] 94.02 <C> [BOLD] 92.68 <C> 93.97 <C> 92.66 <R> <C> Portuguese <C> 86.55 <C> 82.61 <C> [BOLD] 87.45∗ <C> 83.62∗ <C> 87.17∗ <C> 83.47∗ <C> 87.12∗ <C> 83.45∗ <C> 87.40∗ <C> [BOLD] 83.71∗ <R> <C> Slovene <C> 76.76 <C> 63.53 <C> 77.86 <C> 64.43 <C> 80.39† <C> 67.04† <C> [BOLD] 80.56† <C> [BOLD] 67.10† <C> 80.47† <C> [BOLD] 67.10† <R> <C> Spanish <C> 79.20 <C> 76.00 <C> 80.12∗ <C> 77.24∗ <C> [BOLD] 81.36∗ <C> [BOLD] 78.30∗ <C> 81.12∗ <C> 77.99∗ <C> 81.33∗ <C> 78.16∗ <R> <C> Swedish <C> 87.43 <C> 81.77 <C> 88.05∗ <C> 82.77∗ <C> 88.20∗ <C> 83.02∗ <C> 88.09∗ <C> 82.87∗ <C> [BOLD] 88.36∗ <C> [BOLD] 83.16∗ <R> <C> Average <C> 83.48 <C> 76.75 <C> 84.46 <C> 77.92 <C> 84.67 <C> 78.18 <C> 84.63 <C> 78.12 <C> [BOLD] 84.74 <C> [BOLD] 78.24 <CAP> Table 3: Parsing accuracy (UAS and LAS, including punctuation) of the Covington non-projective parser with static, and dynamic monotonic and non-monotonic oracles on CoNLL-XI (first block) and CoNLL-X (second block) datasets. For the dynamic non-monotonic oracle, we show the performance with the three loss expressions, where lower stands for the lower bound |U(c,tG)|, pc upper for the upper bound |U(c,tG)|+npc(A∪I(c,tG)), and upper for the upper bound |U(c,tG)|+nc(A∪I(c,tG)). For each language, we run five experiments with the same setup but different seeds and report the averaged accuracy. Best results for each language are shown in boldface. Statistically significant improvements (α=.05) of both dynamic oracles are marked with ∗ if they are only over the static oracle, and with † if they are over the opposite dynamic oracle too.
<R> <C> [BOLD] Model <C> [ITALIC] τ (50) <C> [ITALIC] τ (100) <R> <C> Contextual TM <C> 0.1632 <C> 0.1381 <R> <C> Combined TM <C> 0.1644 <C> [BOLD] 0.1409 <R> <C> Neural-ProdLDA <C> [BOLD] 0.1658 <C> 0.1285 <R> <C> LDA <C> -0.0246 <C> -0.0757 <CAP> Table 1: NPMI Coherences on W1 data set.
<R> <C> [BOLD] Lang <C> [BOLD] Mat25↑ <C> [BOLD] KL25↓ <C> [BOLD] CD25↑ <C> [BOLD] Mat50↑ <C> [BOLD] KL50↓ <C> [BOLD] CD50↑ <R> <C> IT <C> 75.67 <C> 0.16 <C> 0.84 <C> 62.00 <C> 0.21 <C> 0.75 <R> <C> FR <C> 79.00 <C> 0.14 <C> 0.86 <C> 63.33 <C> 0.19 <C> 0.77 <R> <C> PT <C> 78.00 <C> 0.14 <C> 0.85 <C> 68.00 <C> 0.19 <C> 0.79 <R> <C> DE <C> 79.33 <C> 0.15 <C> 0.85 <C> 64.33 <C> 0.20 <C> 0.77 <R> <C> Uni <C> 4.00 <C> 0.75 <C> — <C> 2.00 <C> 0.85 <C> — <CAP> Table 2: Match, KL-divergence, and centroid similarity for 25 and 50 topics on various languages on W2.
<R> <C> System <C> P <C> R <C> F <R> <C> SuperAttn+Cate ( [ITALIC] n=50) <C> 0.56 <C> 0.49 <C> 0.52 <R> <C> No-Relation-Align <C> 0.46 <C> 0.40 <C> 0.43 <CAP> Table 3: Supervised attention impact on development set
<R> <C> [BOLD] Model <C> [BOLD] Retrieval Statistics  [BOLD] MRR <C> [BOLD] Retrieval Statistics  [BOLD] MRR@10 <C> [BOLD] Retrieval Statistics  [BOLD] nDCG <C> [BOLD] Unique ~ [ITALIC] q % [BOLD]  dev. <R> <C> S2S-M256 <C> 0.0368 <C> 0.0147 <C> 0.1943 <C> 66.3 <R> <C> S2S-M512 <C> 0.0393 <C> 0.0165 <C> 0.1980 <C> 71.1 <R> <C> S2S <C> 0.0520 <C> 0.0266 <C> 0.2147 <C> 70.6 <R> <C> S2S [ITALIC] rmrep <C> 0.0540 <C> 0.0284 <C> 0.2152 <C> [BOLD] 80.4 <R> <C> MESD-M512 <C> 0.0367 <C> 0.0147 <C> 0.1941 <C> 72.6 <R> <C> MESD <C> 0.0509 <C> 0.0248 <C> 0.2141 <C> 68.6 <R> <C> MSQG-M512 <C> 0.0450 <C> 0.0210 <C> 0.2056 <C> 72.0 <R> <C> MSQG [ITALIC] mult <C> 0.0513 <C> 0.0256 <C> 0.2142 <C> 61.4 <R> <C> MSQG <C> 0.0560 <C> 0.0287 <C> 0.2209 <C> 66.9 <R> <C> MSQG [ITALIC] sharedh <C> 0.0569 <C> 0.0298 <C> 0.2220 <C> 67.0 <R> <C> MSQG [ITALIC] sharedh, [ITALIC] rmrep <C> [BOLD] 0.0704 <C> [BOLD] 0.0441 <C> [BOLD] 0.2337 <C> 70.3 <CAP> Table 3: Full results, comparing models constructed with M256, M512, and Mattn256. M512 has the most number of parameters among the three considered.
<R> <C> [BOLD] Evaluation Set <C> [BOLD] Size <C> [BOLD] Noise <C> [BOLD] Reverb. <C> [BOLD] Spont. <C> [BOLD] Baseline <C> [BOLD] Stage 2 <C> [BOLD] Stage 1 <C> [BOLD] Two- <R> <C> [EMPTY] <C> [min.] <C> [EMPTY] <C> [EMPTY] <C> [BOLD] Speech <C> [EMPTY] <C> [BOLD] only <C> [BOLD] only <C> [BOLD] Staged <R> <C> DiSCo Planned Clean <C> 55 <C> n <C> n <C> n <C> 9.03 <C> 9.23 <C> 8.95 <C> [BOLD] 8.89 <R> <C> DiSCo Spontaneous Clean <C> 115 <C> n <C> n <C> y <C> 10.25 <C> 10.06 <C> [BOLD] 9.90 <C> 9.94 <R> <C> DiSCo Planned Mix <C> 87 <C> y <C> n <C> n <C> 11.64 <C> 11.67 <C> [BOLD] 10.80 <C> 10.83 <R> <C> DiSCo Spontaneous Mix <C> 66 <C> y <C> n <C> y <C> 19.48 <C> 18.80 <C> 17.54 <C> [BOLD] 17.41 <R> <C> General German Broadcasts <C> 61 <C> p <C> p <C> p <C> 12.31 <C> 11.87 <C> 11.49 <C> [BOLD] 11.24 <R> <C> Challenging Broadcast Radio <C> 52 <C> y <C> p <C> y <C> 23.43 <C> 23.20 <C> 22.69 <C> [BOLD] 22.02 <R> <C> Challenging Broadcast TV <C> 53 <C> y <C> p <C> y <C> 17.78 <C> 17.44 <C> 17.28 <C> [BOLD] 17.02 <R> <C> Spoken QALD-7 <C> 15 <C> p <C> y <C> p <C> 20.59 <C> 19.70 <C> 18.34 <C> [BOLD] 17.72 <R> <C> Humanities (Interaction) <C> 49 <C> y <C> y <C> y <C> 66.50 <C> 64.37 <C> 47.81 <C> [BOLD] 47.13 <CAP> Table 3: Word error rates on several in-house evaluation sets from different domains. Legend: y: yes; n: no, p: partly
<R> <C> [EMPTY] <C> R-1 <C> R-2 <C> R-L <R> <C> [BOLD] Extractive <C> [BOLD] Extractive <C> [BOLD] Extractive <C> [BOLD] Extractive <R> <C> Lead-3 <C> 40.00 <C> 17.50 <C> 36.20 <R> <C> SummaRuNNer  <C> 39.60 <C> 16.20 <C> 35.30 <R> <C> Refresh  <C> 40.00 <C> 18.20 <C> 36.60 <R> <C> Rnes w/o coherence  <C> 41.25 <C> 18.87 <C> 37.75 <R> <C> BanditSum  <C> 41.50 <C> 18.70 <C> 37.60 <R> <C> Latent  <C> 41.05 <C> 18.77 <C> 37.54 <R> <C> rnn-ext+RL  <C> 41.47 <C> 18.72 <C> 37.76 <R> <C> NeuSum  <C> 41.59 <C> 19.01 <C> 37.98 <R> <C> [BOLD] Abstractive <C> [BOLD] Abstractive <C> [BOLD] Abstractive <C> [BOLD] Abstractive <R> <C> Pointer-Generator  <C> 39.53 <C> 17.28 <C> 36.38 <R> <C> KIGN+Prediction-guide  <C> 38.95 <C> 17.12 <C> 35.68 <R> <C> Multi-Task(EG+QG)  <C> 39.81 <C> 17.64 <C> 36.54 <R> <C> RL+pg+cbdec  <C> 40.66 <C> 17.87 <C> 37.06 <R> <C> Saliency+Entail.  <C> 40.43 <C> 18.00 <C> 37.10 <R> <C> Inconsistency loss  <C> 40.68 <C> 17.97 <C> 37.13 <R> <C> Bottom-up  <C> 41.22 <C> 18.68 <C> 38.34 <R> <C> rnn-ext+abs+RL  <C> 40.04 <C> 17.61 <C> 37.59 <R> <C> [BOLD] Mixed Extractive-Abstractive <C> [BOLD] Mixed Extractive-Abstractive <C> [BOLD] Mixed Extractive-Abstractive <C> [BOLD] Mixed Extractive-Abstractive <R> <C> [BOLD] EditNet <C> 41.42 <C> 19.03 <C> 38.36 <CAP> Table 1: Quality evaluation using ROUGE F-measure (ROUGE-1, ROUGE-2, ROUGE-L) on CNN/DailyMail non-annonymized dataset
<R> <C> [EMPTY] <C> [BOLD] Logistic Regression TF <C> [BOLD] Logistic Regression TF-Norm <C> [BOLD] Logistic Regression Binary <C> [BOLD] Logistic Regression TF-IDF <C> [BOLD] Random Forest TF <C> [BOLD] Random Forest TF-Norm <C> [BOLD] Random Forest Binary <C> [BOLD] Random Forest TF-IDF <C> [BOLD] Support Vector Machines TF <C> [BOLD] Support Vector Machines TF-Norm <C> [BOLD] Support Vector Machines Binary <C> [BOLD] Support Vector Machines TF-IDF <R> <C> Task 1 <C> 83.0 <C> 83.0 <C> 80.0 <C> 83.0 <C> 81.0 <C> 77.5 <C> 78.0 <C> 81.0 <C> 81.0 <C> 83.0 <C> 78.0 <C> [BOLD] 85.5 <R> <C> Task 2 <C> 79.0 <C> 82.5 <C> 77.0 <C> 79.5 <C> 76.0 <C> 75.5 <C> 74.0 <C> 76.0 <C> 81.5 <C> 82.5 <C> 73.5 <C> [BOLD] 83.0 <R> <C> Task 3 <C> 82.0 <C> 81.5 <C> 75.5 <C> 81.0 <C> 76.5 <C> 76.5 <C> 74.0 <C> 74.5 <C> 80.5 <C> 81.0 <C> 76.0 <C> [BOLD] 83.5 <R> <C> Task 4 <C> 87.0 <C> [BOLD] 87.5 <C> [BOLD] 87.5 <C> 87.0 <C> 81.5 <C> 80.0 <C> 81.5 <C> 81.0 <C> 85.0 <C> 86.0 <C> 84.0 <C> 85.5 <R> <C> Task 5 <C> 80.5 <C> 82.5 <C> [BOLD] 83.5 <C> 83.0 <C> 75.0 <C> 75.5 <C> 74.5 <C> 75.0 <C> [BOLD] 83.5 <C> 81.5 <C> 80.0 <C> 81.0 <R> <C> Average <C> 82.3 <C> 83.4 <C> 80.7 <C> 82.7 <C> 78.0 <C> 77.0 <C> 76.4 <C> 77.5 <C> 82.3 <C> 82.8 <C> 78.3 <C> [BOLD] 83.7 <CAP> TABLE I: Accuracy (in %) of the baseline models using different combinations of classifiers and vector representations on the five tasks.
<R> <C> [ITALIC]  [BOLD] Feature <C> [ITALIC]  [BOLD] Value <R> <C> unique tokens <C> 102 <R> <C> total tokens <C> 37,921,928 <R> <C> papers <C> 799 <CAP> Table 1: Some quantitative information about the dataset.
<R> <C> [ITALIC]  [BOLD] Model <C> [ITALIC]  [BOLD] Training CE <C> [ITALIC]  [BOLD] Validation CE <C> [ITALIC]  [BOLD] Training BPC <C> [ITALIC]  [BOLD] Validation BPC <R> <C> Char-LSTM <C> 0.99 <C> 1.15 <C> 1.43 <C> 1.66 <R> <C> Transformer <C> 1.05 <C> 1.16 <C> 1.51 <C> 1.67 <R> <C> Transformer-XL <C> [BOLD] 0.48 <C> [BOLD] 0.71 <C> [BOLD] 0.69 <C> [BOLD] 1.02 <CAP> Table 4: Best training and validation cross entropy (CE) errors and bits-per-character (BPC) calculations on trained models.
<R> <C> # SEG <C> Arabic-to-English tst11 <C> Arabic-to-English tst12 <C> Arabic-to-English tst13 <C> Arabic-to-English tst14 <C> Arabic-to-English AVG. <C> English-to-Arabic tst11 <C> English-to-Arabic tst12 <C> English-to-Arabic tst13 <C> English-to-Arabic tst14 <C> English-to-Arabic AVG. <R> <C> UNSEG <C> 25.7 <C> 28.2 <C> 27.3 <C> 23.9 <C> 26.3 <C> 15.8 <C> 17.1 <C> 18.1 <C> 15.5 <C> 16.6 <R> <C> MORPH <C> 29.2 <C> 33.0 <C> 32.9 <C> 28.3 <C> 30.9 <C> 16.5 <C> 18.8 <C> 20.4 <C> 17.2 <C> [BOLD] 18.2 <R> <C> cCNN <C> 29.0 <C> 32.0 <C> 32.5 <C> 28.0 <C> 30.3 <C> 14.3 <C> 12.8 <C> 13.6 <C> 12.6 <C> 13.3 <R> <C> CHAR <C> 28.8 <C> 31.8 <C> 32.5 <C> 27.8 <C> 30.2 <C> 15.3 <C> 17.1 <C> 18.0 <C> 15.3 <C> 16.4 <R> <C> BPE <C> 29.7 <C> 32.5 <C> 33.6 <C> 28.4 <C> [BOLD] 31.1 <C> 17.5 <C> 18.0 <C> 20.0 <C> 16.6 <C> 18.0 <CAP> Table 1: Results of comparing several segmentation strategies.
<R> <C> SEG <C> UNSEG <C> MORPH <C> CHAR <C> cCNN <C> BPE <R> <C> ACC <C> 90.9 <C> 96.2 <C> 95.9 <C> 95.8 <C> 94.9 <CAP> Table 2: POS tagging with various segmentations
<R> <C> [EMPTY] <C> [BOLD] Dataset <C> [BOLD] P <C> [BOLD] P’ <C> [BOLD] R <C> [BOLD] R’ <C> [BOLD] F1 <C> [BOLD] F1’ <R> <C> [EMPTY] <C> [ITALIC] crowd <C> 0.565 <C> 0.632 <C> 0.743 <C> 0.754 <C> [BOLD] 0.642 <C> [BOLD] 0.687 <R> <C> [ITALIC] cause <C> [ITALIC] expert <C> [BOLD] 0.672 <C> [BOLD] 0.711 <C> 0.604 <C> 0.616 <C> 0.638 <C> 0.658 <R> <C> relation <C> [ITALIC] baseline <C> 0.436 <C> 0.474 <C> [BOLD] 0.844 <C> [BOLD] 0.842 <C> 0.575 <C> 0.606 <R> <C> [EMPTY] <C> [ITALIC] single <C> 0.495 <C> 0.545 <C> 0.473 <C> 0.478 <C> 0.483 <C> 0.658 <R> <C> [EMPTY] <C> [ITALIC] crowd <C> 0.823 <C> 0.843 <C> 0.891 <C> 0.902 <C> 0.854 <C> 0.869 <R> <C> [ITALIC] treat <C> [ITALIC] expert <C> [BOLD] 0.834 <C> [BOLD] 0.863 <C> 0.833 <C> 0.84 <C> 0.832 <C> 0.85 <R> <C> relation <C> [ITALIC] baseline <C> 0.767 <C> 0.811 <C> [BOLD] 0.968 <C> [BOLD] 0.968 <C> [BOLD] 0.856 <C> [BOLD] 0.882 <R> <C> [EMPTY] <C> [ITALIC] single <C> 0.774 <C> 0.819 <C> 0.856 <C> 0.866 <C> 0.811 <C> 0.84 <CAP> Table 3: Model evaluation results over sentences with expert annotation. Crowd scores are shown at 0.5 negative/positive sentence-relation score threshold.
<R> <C> [EMPTY] <C> [BOLD] Dataset <C> [BOLD] P <C> [BOLD] P’ <C> [BOLD] R <C> [BOLD] R’ <C> [BOLD] F1 <C> [BOLD] F1’ <R> <C> [ITALIC] cause <C> [ITALIC] crowd <C> [BOLD] 0.538 <C> [BOLD] 0.61 <C> 0.79 <C> 0.802 <C> [BOLD] 0.64 <C> [BOLD] 0.692 <R> <C> relation <C> [ITALIC] baseline <C> 0.475 <C> 0.53 <C> [BOLD] 0.889 <C> [BOLD] 0.887 <C> 0.619 <C> 0.663 <R> <C> [ITALIC] treat <C> [ITALIC] crowd <C> [BOLD] 0.876 <C> [BOLD] 0.913 <C> [BOLD] 0.887 <C> [BOLD] 0.898 <C> [BOLD] 0.88 <C> [BOLD] 0.904 <R> <C> relation <C> [ITALIC] baseline <C> 0.808 <C> 0.858 <C> 0.678 <C> 0.673 <C> 0.736 <C> 0.754 <CAP> Table 4: Model evaluation results over 3,984 sentences. Crowd scores are shown at 0.5 sentence-relation score threshold.
<R> <C> Model <C> Discriminative MRR <C> Discriminative R@1 <C> Discriminative R@5 <C> Discriminative R@10 <C> Discriminative Mean <C> Generative MRR <C> Generative R@1 <C> Generative R@5 <C> Generative R@10 <C> Generative Mean <R> <C> HCIAE-D-NP-ATT <C> 0.6222 <C> 48.48 <C> 78.75 <C> 87.59 <C> 4.81 <C> - <C> - <C> - <C> - <C> - <R> <C> HCIAE-G-DIS <C> - <C> - <C> - <C> - <C> - <C> 0.5467 <C> 44.35 <C> 65.28 <C> 71.55 <C> 14.23 <R> <C> HCIAE-GAN1 <C> 0.2177 <C> 8.82 <C> 32.97 <C> 52.14 <C> 18.53 <C> 0.5298 <C> 43.12 <C> 62.74 <C> 68.58 <C> 16.25 <R> <C> HCIAE-GAN2 <C> 0.6050 <C> 46.20 <C> 77.92 <C> 87.20 <C> 4.97 <C> 0.5459 <C> 44.33 <C> 65.05 <C> 71.40 <C> 14.34 <CAP> Table 4: Adversarial training results on VisDial dataset.
<R> <C> Models <C> Quora-i@ ROUGE-1 <C> Quora-i@ ROUGE-2 <C> Quora-i@ BLEU <C> Quora-i@ METEOR <C> Quora-ii@ ROUGE-1 <C> Quora-ii@ ROUGE-2 <C> Quora-ii@ BLEU <C> Quora-ii@ METEOR <R> <C> Seq2Seq <C> 58.77 <C> 31.47 <C> 36.55 <C> 26.28 <C> 47.22 <C> 20.72 <C> 26.06 <C> 20.35 <R> <C> Residual LSTM <C> 59.21 <C> 32.43 <C> 37.38 <C> 28.17 <C> 48.55 <C> 22.48 <C> 27.32 <C> 22.37 <R> <C> VAE-SVG-eq <C> - <C> - <C> - <C> 25.50 <C> - <C> - <C> - <C> 22.20 <R> <C> Pointer-generator <C> 61.96 <C> 36.07 <C> 40.55 <C> 30.21 <C> 51.98 <C> 25.16 <C> 30.01 <C> 24.31 <R> <C> RL-ROUGE <C> 63.35 <C> 37.33 <C> 41.83 <C> 30.96 <C> 54.50 <C> 27.50 <C> 32.54 <C> 25.67 <R> <C> RbM-SL (ours) <C> [BOLD] 64.39 <C> [BOLD] 38.11 <C> [BOLD] 43.54 <C> [BOLD] 32.84 <C> [BOLD] 57.34 <C> [BOLD] 31.09 <C> [BOLD] 35.81 <C> [BOLD] 28.12 <R> <C> RbM-IRL (ours) <C> 64.02 <C> 37.72 <C> 43.09 <C> 31.97 <C> 56.86 <C> 29.90 <C> 34.79 <C> 26.67 <CAP> Table 2: Performances on Quora datasets.
<R> <C> Models <C> Twitter ROUGE-1 <C> Twitter ROUGE-2 <C> Twitter BLEU <C> Twitter METEOR <R> <C> Seq2Seq <C> 30.43 <C> 14.61 <C> 30.54 <C> 12.80 <R> <C> Residual LSTM <C> 32.50 <C> 16.86 <C> 33.90 <C> 13.65 <R> <C> Pointer-generator <C> 38.31 <C> 21.22 <C> 40.37 <C> 17.62 <R> <C> RL-ROUGE <C> 40.16 <C> 22.99 <C> 42.73 <C> 18.89 <R> <C> RbM-SL (ours) <C> 41.87 <C> 24.23 <C> 44.67 <C> 19.97 <R> <C> RbM-IRL (ours) <C> [BOLD] 42.15 <C> [BOLD] 24.73 <C> [BOLD] 45.74 <C> [BOLD] 20.18 <CAP> Table 3: Performances on Twitter corpus.
<R> <C> Models <C> Quora-i@ Relevance <C> Quora-i@ Fluency <C> Quora-ii@ Relevance <C> Quora-ii@ Fluency <R> <C> Pointer-generator <C> 3.23 <C> 4.55 <C> 2.34 <C> 2.96 <R> <C> RL-ROUGE <C> 3.56 <C> 4.61 <C> 2.58 <C> 3.14 <R> <C> RbM-SL (ours) <C> [BOLD] 4.08 <C> 4.67 <C> [BOLD] 3.20 <C> 3.48 <R> <C> RbM-IRL (ours) <C> 4.07 <C> [BOLD] 4.69 <C> 2.80 <C> [BOLD] 3.53 <R> <C> Reference <C> 4.69 <C> 4.95 <C> 4.68 <C> 4.90 <CAP> Table 3: Performances on Twitter corpus.
<R> <C> [EMPTY] <C> PR100 <C> BPD1000.6 <C> BPD1001.0 <C> BPD1.0 <R> <C> Recall <C> 0.455 <C> 0.249 <C> 0.264 <C> 0.727 <R> <C> Precision <C> 0.407 <C> 0.396 <C> 0.410 <C> 0.256 <R> <C> Fscore <C> 0.429 <C> 0.303 <C> 0.318 <C> 0.359 <CAP> Table 2: Averaged performances of the BPD algorithms BPD100θ (θ=0.6 or θ=1.0) and BPDθ (θ=1.0) and the PageRank algorithm PR100 on the 533 text documents of DUC 2002 [26]. The Precision, Recall, and F-score values are obtained by averaging over the results of individual text documents. The inverse temperature of BPD is fixed to be β=8.0.
<R> <C> Model# <C> BERT <C> [ITALIC] L <C> [BOLD] ^sdim <C> Span <C> 5-fold CV on train <C> test <C> runtime(s) <R> <C> 1 <C> BASE <C> 1 <C> 32 <C> Meanpooling <C> 0.5247±0.0379 <C> 0.4891 <C> 232.8 <R> <C> 2 <C> BASE <C> 4 <C> 32 <C> Meanpooling <C> 0.4699±0.0431 <C> 0.4270 <C> 317.3 <R> <C> 3 <C> LARGE <C> 4 <C> 32 <C> Meanpooling <C> 0.4041±0.0532 <C> 0.3819 <C> 358.3 <R> <C> 4 <C> LARGE <C> 8 <C> 32 <C> Meanpooling <C> 0.3783±0.0468 <C> 0.3519 <C> 372.2 <R> <C> 5 <C> LARGE <C> 12 <C> 32 <C> Meanpooling <C> 0.3879±0.0461 <C> 0.3546 <C> 415.4 <R> <C> 6 <C> LARGE <C> 8 <C> 8 <C> Meanpooling <C> 0.3758±0.0430 <C> 0.3490 <C> 436.2 <R> <C> 7 <C> LARGE <C> 8 <C> 16 <C> Meanpooling <C> 0.3736±0.0465 <C> 0.3488 <C> 415.0 <R> <C> 8 <C> LARGE <C> 8 <C> 64 <C> Meanpooling <C> 0.3780±0.0441 <C> 0.3518 <C> 447.6 <R> <C> 9 <C> LARGE <C> 8 <C> 16 <C> Attention <C> 0.3582±0.0435 <C> 0.3349 <C> 828.2 <CAP> Table 1: Results of Feature-based Aproach.
<R> <C> Model# <C> Based Model <C> method <C> [ITALIC] Ltuning <C> 5-fold CV on train <C> test <C> runtime(s) <R> <C> 10 <C> #9 <C> premature <C> 12 <C> 0.3033±0.0367 <C> 0.2795 <C> 6909.5 <R> <C> 11 <C> #9 <C> mature <C> 12 <C> 0.3042±0.0352 <C> 0.2856 <C> 7627.7 <R> <C> 12 <C> #9 <C> mature <C> 8 <C> 0.3110±0.0352 <C> 0.2876 <C> 8928.1 <R> <C> 13 <C> #9 <C> mature <C> 16 <C> 0.3185±0.0465 <C> 0.2820 <C> 7763.4 <R> <C> 14 <C> #9 <C> mature <C> 24 <C> 0.3169±0.0440 <C> 0.2843 <C> 8695.4 <CAP> Table 2: Results of Fine-tuning Aproach.
<R> <C> [BOLD] Sys <C> [BOLD] Acc (%) <R> <C>  <C> 97.24 <R> <C>  <C> 97.35 <R> <C>  NN <C> 96.36 <R> <C>  NN+WE <C> 97.20 <R> <C> [BOLD] BLSTM-RNN <C> 96.61 <R> <C> [BOLD] BLSTM-RNN+WE(10m) <C> 96.61 <R> <C> [BOLD] BLSTM-RNN+WE(100m) <C> 97.10 <R> <C> [BOLD] BLSTM-RNN+WE(all) <C> 97.26 <R> <C> [BOLD] BLSTM-RNN+WE(all)+suffix2 <C> [BOLD] 97.40 <CAP> Table 2: POS tagging accuracies on WSJ test set.
<R> <C> Feature∖Task Case <C> KM <C> Lin <C> NN <C> KM <C> Lin <C> NN <C> KM 0.25 <C> Lin 0.87 <C> NN 0.88 <C> KM 0.15 <C> Lin 0.84 <C> NN 0.86 <C> KM <C> Lin <C> NN <C> KM 0.2 <C> Lin 0.86 <C> NN 0.87 <R> <C> Gender <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 0.5 <C> 0.96 <C> 0.96 <C> 0.32 <C> 0.9 <C> 0.91 <C> 0.28 <C> 0.86 <C> 0.87 <C> [BOLD] 0.48 <C> 0.97 <C> 0.97 <C> 0.39 <C> 0.92 <C> 0.93 <R> <C> Mood <C> 0.6 <C> 0.98 <C> 0.98 <C> 0.3 <C> 0.98 <C> 0.96 <C> 0.41 <C> 0.91 <C> 0.85 <C> 0.28 <C> [BOLD] 0.99 <C> [BOLD] 0.99 <C> 0.26 <C> 0.9 <C> 0.88 <C> 0.37 <C> 0.95 <C> 0.93 <R> <C> Number <C> 0.46 <C> 0.97 <C> 0.97 <C> 0.49 <C> 0.97 <C> 0.97 <C> 0.48 <C> 0.93 <C> 0.92 <C> [BOLD] 0.48 <C> 0.92 <C> 0.93 <C> 0.46 <C> [BOLD] 0.99 <C> [BOLD] 0.99 <C> [BOLD] 0.47 <C> 0.96 <C> 0.95 <R> <C> Person <C> 0.32 <C> 0.96 <C> 0.95 <C> 0.33 <C> 0.97 <C> 0.97 <C> 0.39 <C> 0.96 <C> 0.95 <C> 0.25 <C> [BOLD] 0.99 <C> 0.96 <C> 0.31 <C> 0.93 <C> 0.93 <C> 0.32 <C> 0.96 <C> 0.95 <R> <C> Tense <C> [BOLD] 0.64 <C> [BOLD] 1.0 <C> [BOLD] 0.99 <C> 0.25 <C> 0.99 <C> [BOLD] 0.99 <C> [BOLD] 0.55 <C> [BOLD] 0.97 <C> [BOLD] 0.97 <C> 0.34 <C> 0.97 <C> 0.96 <C> 0.21 <C> 0.98 <C> 0.96 <C> 0.4 <C> [BOLD] 0.98 <C> [BOLD] 0.97 <R> <C> VerbForm <C> 0.2 <C> 0.88 <C> 0.87 <C> 0.39 <C> [BOLD] 1.0 <C> 0.96 <C> 0.33 <C> [BOLD] 0.97 <C> 0.94 <C> 0.2 <C> [BOLD] 0.99 <C> [BOLD] 0.99 <C> 0.33 <C> 0.98 <C> 0.98 <C> 0.29 <C> 0.96 <C> 0.95 <R> <C> Average <C> 0.44 <C> 0.96 <C> 0.95 <C> 0.38 <C> 0.98 <C> 0.97 <C> 0.39 <C> 0.93 <C> 0.92 <C> 0.28 <C> 0.94 <C> 0.94 <C> 0.34 <C> 0.96 <C> 0.95 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 3: Weighted F1 Scores for each language and feature; scores averaged across all layers. KM=K-Means clustering, Lin=Linear classifer, NN=3-layer Neural Network with ReLU activations. Bold reflects highest score in each column. Red indicates score ≤ random baseline (see Table 8).
<R> <C> Feature∖Task Case <C> KM <C> Lin <C> NN <C> KM <C> Lin <C> NN <C> KM 0.21 <C> Lin 0.6 <C> NN 0.64 <C> KM 0.12 <C> Lin 0.28 <C> NN 0.29 <C> KM <C> Lin <C> NN <C> KM 0.16 <C> Lin 0.44 <C> NN 0.47 <R> <C> Gender <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 0.44 <C> 0.62 <C> 0.61 <C> 0.25 <C> 0.48 <C> 0.52 <C> 0.26 <C> 0.4 <C> 0.43 <C> 0.43 <C> 0.74 <C> 0.73 <C> 0.34 <C> 0.56 <C> 0.58 <R> <C> Mood <C> 0.46 <C> 0.84 <C> 0.84 <C> 0.17 <C> 0.69 <C> 0.65 <C> 0.35 <C> 0.74 <C> 0.8 <C> 0.2 <C> 0.87 <C> 0.92 <C> 0.18 <C> 0.65 <C> 0.59 <C> 0.27 <C> 0.76 <C> 0.76 <R> <C> Number <C> 0.39 <C> 0.63 <C> 0.61 <C> 0.39 <C> 0.7 <C> 0.62 <C> 0.4 <C> 0.58 <C> 0.55 <C> 0.38 <C> 0.52 <C> 0.53 <C> 0.34 <C> 0.66 <C> 0.66 <C> 0.38 <C> 0.62 <C> 0.6 <R> <C> Person <C> 0.4 <C> 0.94 <C> 0.94 <C> 0.33 <C> 0.79 <C> 0.8 <C> 0.27 <C> 0.74 <C> 0.74 <C> 0.27 <C> 0.79 <C> 0.79 <C> 0.24 <C> 0.73 <C> 0.63 <C> 0.3 <C> 0.8 <C> 0.78 <R> <C> Tense <C> 0.49 <C> 0.76 <C> 0.76 <C> 0.21 <C> 0.66 <C> 0.65 <C> 0.44 <C> 0.76 <C> 0.7 <C> 0.29 <C> 0.53 <C> 0.52 <C> 0.2 <C> 0.68 <C> 0.64 <C> 0.33 <C> 0.68 <C> 0.65 <R> <C> VerbForm <C> 0.2 <C> 0.51 <C> 0.56 <C> 0.22 <C> 0.57 <C> 0.56 <C> 0.27 <C> 0.58 <C> 0.53 <C> 0.2 <C> 0.46 <C> 0.45 <C> 0.2 <C> 0.47 <C> 0.47 <C> 0.22 <C> 0.52 <C> 0.51 <R> <C> Average <C> 0.39 <C> 0.74 <C> 0.74 <C> 0.29 <C> 0.67 <C> 0.65 <C> 0.31 <C> 0.64 <C> 0.64 <C> 0.25 <C> 0.55 <C> 0.56 <C> 0.27 <C> 0.65 <C> 0.62 <C> 0.3 <C> 0.65 <C> 0.64 <CAP> Table 8: Random baselines for weighted F1 Scores for each language and feature; scores averaged across all layers. Compare against Table 3.
<R> <C> 2-3 5-6 8-9 Type <C> WN18RR Baseline <C> WN18RR GPFL <C> FB15K-237 Baseline <C> FB15K-237 GPFL <C> DBpedia3.8 Baseline <C> DBpedia3.8 GPFL <R> <C> All <C> 195.89 <C> 3.63 <C> 703.16 <C> 9.45 <C> 1285.62 <C> 69.41 <R> <C> CAR <C> 0.84 <C> 0.84 <C> 2.99 <C> 2.81 <C> 21.54 <C> 21.49 <R> <C> len=1 <C> 11.09 <C> 0.36 <C> 52.63 <C> 0.56 <C> 499.91 <C> 13.27 <R> <C> len=2 <C> 33.09 <C> 0.54 <C> 123.47 <C> 0.61 <C> 537.36 <C> 15.44 <R> <C> len=3 <C> 150.85 <C> 1.87 <C> 524.05 <C> 5.46 <C> 294.48 <C> 22.84 <CAP> Table 3. Runtimes of rule evaluation approaches measured in various rule groups and reported in seconds.
<R> <C> Russian–English (medium) Dataset Size <C> Russian–English (medium) <C> Beam Size 10 <C> Beam Size 50 <C> Beam Size 75 <C> Beam Size 100 <C> Beam Size 150 <R> <C> [EMPTY] <C> baseline <C> 24.9 <C> 23.8 <C> 23.6 <C> 23.3 <C> 22.5 <R> <C> 100% <C> reward <C> 26.5 <C> 26.6 <C> 26.5 <C> 26.5 <C> 26.5 <R> <C> [EMPTY] <C> [ITALIC] γ <C> 0.716 <C> 0.643 <C> 0.640 <C> 0.633 <C> 0.617 <R> <C> [EMPTY] <C> baseline <C> 22.8 <C> 21.4 <C> 20.8 <C> 20.4 <C> 19.2 <R> <C> 50% <C> reward <C> 24.7 <C> 25.0 <C> 24.9 <C> 24.9 <C> 25.0 <R> <C> [EMPTY] <C> [ITALIC] γ <C> 0.697 <C> 0.645 <C> 0.638 <C> 0.636 <C> 0.646 <R> <C> [EMPTY] <C> baseline <C> 17.0 <C> 16.2 <C> 15.8 <C> 15.6 <C> 15.1 <R> <C> 10% <C> reward <C> 17.6 <C> 18.0 <C> 18.0 <C> 18.0 <C> 18.1 <R> <C> [EMPTY] <C> [ITALIC] γ <C> 0.892 <C> 0.835 <C> 0.773 <C> 0.750 <C> 0.800 <CAP> Table 5: Varying the size of the Russian–English training dataset results in different optimal word reward scores (γ). In all settings, the tuned score alleviates the beam problem. As the datasets get smaller, using a tuned larger beam improves the BLEU score over a smaller tuned beam. This suggests that lower-resource systems are more susceptible to the beam problem.
<R> <C> [EMPTY] <C> CNN/DailyMail <C> Big-Patent <C> Newsroom <R> <C> Accuracy (%) <C> 98.5 <C> 97.5 <C> 98.2 <R> <C> Δ <C> – <C> -1% <C> -0.3% <CAP> Table 3: Cross-domain transferrability
<R> <C> [BOLD] Feature Set <C> [BOLD] Random Forest <C> [BOLD] JRip <C> [BOLD] SMO <R> <C> factual <C> 0.86 <C> 0.82 <C> 0.73 <R> <C> structural <C> 0.81 <C> 0.74 <C> 0.54 <R> <C> semantic <C> 0.83 <C> 0.75 <C> 0.84 <R> <C> content (tf-idf) <C> [BOLD] 0.92 <C> [BOLD] 0.92 <C> [BOLD] 0.94 <CAP> Table 2: F1-scores for different classifiers and feature sets.
<R> <C> [EMPTY] <C> Model <C> Satire P <C> Satire R <C> Satire F1 <C> Publication P <C> Publication R <C> Publication F1 <R> <C> dev <C> majority class <C> 00.0 <C> 00.0 <C> 00.0 <C> 29.5 <C> 54.3 <C> 38.3 <R> <C> dev <C> no adv <C> 98.9 <C> [BOLD] 52.6 <C> [BOLD] 68.7 <C> 44.6 <C> 56.2 <C> 49.7 <R> <C> dev <C> adv,  [ITALIC] λ=0.2 <C> [BOLD] 99.3 <C> 50.8 <C> 67.2 <C> 31.2 <C> 55.4 <C> 40.0 <R> <C> dev <C> adv,  [ITALIC] λ=0.3 <C> 97.3 <C> 48.9 <C> 65.0 <C> 31.1 <C> 54.8 <C> 39.6 <R> <C> dev <C> adv,  [ITALIC] λ=0.5 <C> 99.1 <C> 50.8 <C> 67.2 <C> 31.7 <C> 55.2 <C> 40.3 <R> <C> dev <C> adv,  [ITALIC] λ=0.7 <C> 86.7 <C> 44.1 <C> 58.4 <C> [BOLD] 26.9 <C> [BOLD] 00.0 <C> [BOLD] 00.0 <R> <C> test <C> majority class <C> 00.0 <C> 00.0 <C> 00.0 <C> 29.1 <C> 53.9 <C> 37.8 <R> <C> test <C> no adv. <C> 99.0 <C> [BOLD] 50.1 <C> [BOLD] 66.5 <C> 44.2 <C> 55.7 <C> 49.3 <R> <C> test <C> adv,  [ITALIC] λ=0.2 <C> [BOLD] 99.4 <C> 49.4 <C> 66.0 <C> [BOLD] 30.8 <C> 54.8 <C> 39.5 <R> <C> test <C> adv,  [ITALIC] λ=0.7 <C> 85.0 <C> 42.5 <C> 56.6 <C> 31.3 <C> [BOLD] 00.0 <C> [BOLD] 00.0 <CAP> Table 2: Results on dev and independent test data.
<R> <C> Method <C> average  [ITALIC] F1 <R> <C> Structured (syntactic) <C> 38.1 <R> <C> Structured (sentential) <C> 38.7 <R> <C> Structured (syntactic + sentential) <C> 40.1 <R> <C> Structured + Joint (syntactic) <C> 43.6 <R> <C> Structured + Joint (sentential) <C> 44.1 <R> <C> Structured + Joint (syntactic + sentential) <C> [BOLD] 45.8 <CAP> Table 3: Impact of different MCCNN channels on the development set.
<R> <C> [BOLD] Model <C> P <C> R <C> [ITALIC] F1 <R> <C> tmChem <C> 72.56 <C> 78.37 <C> 75.35 <R> <C> CRFSuite <C> [BOLD] 81.93 <C> 78.38 <C> 80.12 <R> <C> BiLSTM-CRF + LSTM-char <C> 79.72 <C> [BOLD] 84.42 <C> [BOLD] 82.01 <R> <C> BiLSTM-CNN-CRF <C> 83.76 <C> 85.01 <C> 84.38 <R> <C> EBC-CRF <C> [BOLD] 84.30 <C> [BOLD] 87.11 <C> [BOLD] 85.68 <CAP> Table 5: NER scores on full BioSemantics test set (Akhondi et al., 2014). Results in the first 3 rows were reported in Habibi et al. (2017). BiLSTM-CRF + LSTM-char denotes the BiLSTM-CRF model with additional LSTM-based character-level word embeddings (Lample et al., 2016). Recall that our models use the OSCAR4 tokenizer and pre-trained ChemPatent word embeddings.
<R> <C> [BOLD] Entity label <C> [BOLD] Count† # <C> [BOLD] Count† % <C> [BOLD] BiLSTM-CNN-CRF P <C> [BOLD] BiLSTM-CNN-CRF R <C> [BOLD] BiLSTM-CNN-CRF  [ITALIC] F1 <C> [BOLD] +ELMo P <C> [BOLD] +ELMo R <C> [BOLD] +ELMo  [ITALIC] F1 <C> Δ [ITALIC] F1 <R> <C> B (Abbreviation) <C> 6,558 <C> 5.78 <C> 85.90 <C> 87.02 <C> 86.46 <C> 85.78 <C> 89.98 <C> 87.83 <C> +1.37 <R> <C> C (CAS Number) <C> 13 <C> 0.01 <C> 54.55 <C> 46.15 <C> 50.00 <C> 57.14 <C> 61.54 <C> 59.26 <C> +9.26 <R> <C> D (Trademark) <C> 2,290 <C> 2.01 <C> 62.58 <C> 61.79 <C> 62.18 <C> 66.44 <C> 71.40 <C> 68.83 <C> +6.65 <R> <C> F (Formula) <C> 7,935 <C> 6.99 <C> 86.05 <C> 86.81 <C> 86.42 <C> 83.07 <C> 90.91 <C> 86.82 <C> +0.40 <R> <C> G (Generic) <C> 51,313 <C> 45.20 <C> 81.45 <C> 84.56 <C> 82.98 <C> 83.84 <C> 84.44 <C> 84.14 <C> +1.16 <R> <C> M (IUPAC) <C> 39,896 <C> 35.14 <C> 88.40 <C> 87.77 <C> 88.09 <C> 87.25 <C> 91.20 <C> 89.18 <C> +1.09 <R> <C> MOA (Mode of Action) <C> 1,137 <C> 1.00 <C> 68.97 <C> 63.32 <C> 66.02 <C> 67.62 <C> 72.74 <C> 70.08 <C> +4.06 <R> <C> R (Registry #) <C> 96 <C> 0.08 <C> 55.68 <C> 51.04 <C> 53.26 <C> 65.82 <C> 54.17 <C> 59.43 <C> +6.17 <R> <C> T (Target) <C> 4,290 <C> 3.78 <C> 77.77 <C> 77.32 <C> 77.55 <C> 77.21 <C> 82.68 <C> 79.85 <C> +2.30 <R> <C> [BOLD] Micro Avg. <C> 113,528 <C> 100.0 <C> 83.76 <C> 85.01 <C> 84.38 <C> 84.30 <C> 87.11 <C> 85.68 <C> +1.30 <CAP> Table 6: F1 score with respect to each entity label. “Count†” denotes gold-entity counts in test sets.“+ELMo” denotes scores obtained by EBC-CRF.
<R> <C> [BOLD] Entity label <C> [BOLD] Count† # <C> [BOLD] Count† % <C> [BOLD] BiLSTM-CNN-CRF P <C> [BOLD] BiLSTM-CNN-CRF R <C> [BOLD] BiLSTM-CNN-CRF  [ITALIC] F1 <C> [BOLD] +ELMo P <C> [BOLD] +ELMo R <C> [BOLD] +ELMo  [ITALIC] F1 <C> Δ [ITALIC] F1 <R> <C> 1 (chemClass) <C> 1,476 <C> 12.36 <C> 78.35 <C> 66.46 <C> 71.92 <C> 81.96 <C> 75.75 <C> 78.73 <C> +6.81 <R> <C> 2 (chemClassbiomolecule) <C> 951 <C> 7.96 <C> 71.86 <C> 70.50 <C> 71.17 <C> 76.27 <C> 78.76 <C> 77.50 <C> +6.33 <R> <C> 3 (chemClassmarkush) <C> 38 <C> 0.32 <C> 42.86 <C> 47.37 <C> 45.00 <C> 42.86 <C> 47.37 <C> 45.00 <C> +0.00 <R> <C> 4 (chemClassmixture) <C> 387 <C> 3.24 <C> 76.49 <C> 59.69 <C> 67.05 <C> 74.18 <C> 64.60 <C> 69.06 <C> +2.01 <R> <C> 5 (chemClassmixture-part) <C> 161 <C> 1.35 <C> 71.00 <C> 44.10 <C> 54.41 <C> 78.10 <C> 50.93 <C> 61.65 <C> +7.24 <R> <C> 6 (chemClasspolymer) <C> 609 <C> 5.10 <C> 81.40 <C> 72.82 <C> 76.87 <C> 89.20 <C> 84.07 <C> 86.56 <C> +13.74 <R> <C> 7 (chemCompound) <C> 6,988 <C> 58.53 <C> 89.02 <C> 92.01 <C> 90.49 <C> 91.01 <C> 94.58 <C> 92.76 <C> +2.27 <R> <C> 8 (chemCompoundmixture-part) <C> 904 <C> 7.57 <C> 90.02 <C> 81.86 <C> 85.75 <C> 90.63 <C> 85.62 <C> 88.05 <C> +2.27 <R> <C> 9 (chemCompoundprophetics) <C> 426 <C> 3.57 <C> 18.52 <C> 2.35 <C> 4.17 <C> 77.75 <C> 79.58 <C> 78.65 <C> +74.48 <R> <C> [BOLD] Micro Avg. <C> 11,940 <C> 100.0 <C> 85.12 <C> 80.36 <C> 82.67 <C> 87.41 <C> 87.53 <C> 87.47 <C> +4.80 <CAP> Table 6: F1 score with respect to each entity label. “Count†” denotes gold-entity counts in test sets.“+ELMo” denotes scores obtained by EBC-CRF.
<R> <C> LAMA <C> Relation <C> B <C> B-adv <C> [ITALIC] open domain sourced context B-gen <C> [ITALIC] open domain sourced context DrQA <C> [ITALIC] open domain sourced context B-ret <C> B-ora <R> <C> Google-RE <C> & birth-place 16.1 <C> 14.5 <C> 8.5 <C> [BOLD] 48.6 <C> 43.5 <C> [ITALIC] 70.6 <C> [EMPTY] <R> <C> Google-RE <C> & birth-date 1.4 <C> 1.4 <C> 1.4 <C> 42.9 <C> [BOLD] 43.1 <C> [ITALIC] 98.1 <C> [EMPTY] <R> <C> Google-RE <C> & death-place 14.0 <C> 12.6 <C> 6.0 <C> [BOLD] 38.4 <C> 35.8 <C> [ITALIC] 65.1 <C> [EMPTY] <R> <C> Google-RE <C> Total <C> 10.5 <C> 9.5 <C> 5.3 <C> [BOLD] 43.3 <C> 40.8 <C> [ITALIC] 78.0 <R> <C> T-REx <C> 1-1 <C> 74.5 <C> 74.5 <C> 71.3 <C> 55.2 <C> [BOLD] 81.2 <C> [ITALIC] 91.1 <R> <C> T-REx <C> [ITALIC] N-1 <C> 34.2 <C> 33.8 <C> 32.7 <C> 30.4 <C> [BOLD] 47.5 <C> [ITALIC] 67.3 <R> <C> T-REx <C> [ITALIC] N- [ITALIC] M <C> 24.3 <C> 23.6 <C> 23.8 <C> 15.4 <C> [BOLD] 32.0 <C> [ITALIC] 52.4 <R> <C> T-REx <C> Total <C> 32.3 <C> 31.8 <C> 31.1 <C> 25.8 <C> [BOLD] 43.1 <C> [ITALIC] 62.6 <R> <C> SQuAD <C> [EMPTY] <C> 17.4 <C> 17.4 <C> 15.8 <C> [BOLD] 37.5 <C> 34.3 <C> [ITALIC] 61.7 <R> <C> [ITALIC] weighted average <C> [ITALIC] weighted average <C> 30.5 <C> 30.0 <C> 29.0 <C> 27.2 <C> [BOLD] 42.8 <C> [ITALIC] 63.6 <CAP> Table 2: Mean precision at one (P@1) for the DrQA baseline, BERT-large on context-free cloze questions (B) and on adversarial (B-adv), generated (B-gen), retrieved (B-ret) and oracle (B-ora) context-enriched questions on the relational LAMA probe. The fully unsupervised B-ret is competitive with the supervised DrQA system and is dramatically better than the context-free baseline. We weight the average per number of relations (3 for Google-RE, 41 for T-REx and we consider SQuAD as a single contribution). Pairwise sign tests per relation show statistically significant differences (p-value below 1e-5) between: B-ret and all other results; B-ora and all other results.
<R> <C> [EMPTY] <C> IMDB <C> CoNLL-2003 <R> <C> Setting <C> Accuracy <C> F1 Score <R> <C> Single <C> 91.99 <C> 91.93 <R> <C> 1) Only pseudo-tags <C> 89.84 <C> 92.20 <R> <C> 2) Random distinct vectors <C> 92.06 <C> 92.21 <R> <C> 3) Random noise <C> 92.38 <C> 92.32 <R> <C> SingleEns <C> [BOLD] 92.91 <C> [BOLD] 92.37 <CAP> Table 3: Comparison of proposed method (pseudo-tags + corresponding distinct vectors) with other settings. Pseudo-tags and distinct vectors appear to complement each other.
<R> <C> [BOLD] Lang <C> [BOLD] Method <C> [BOLD] Prec <C> [BOLD] Recall <C> [BOLD] F-1 <R> <C> English <C> [ITALIC] Morf-Base <C> 0.740 <C> 0.623 <C> 0.677 <R> <C> English <C> [ITALIC] Morf-Cat <C> 0.673 <C> 0.587 <C> 0.627 <R> <C> English <C> [ITALIC] AGMorph <C> 0.696 <C> 0.604 <C> 0.647 <R> <C> English <C> [ITALIC] Lee (M2) <C> 0.825 <C> 0.525 <C> 0.642 <R> <C> English <C> Model -C <C> 0.555 <C> 0.792 <C> 0.653 <R> <C> English <C> Model -T <C> 0.831 <C> 0.664 <C> 0.738 <R> <C> English <C> Model -A <C> 0.810 <C> 0.713 <C> 0.758 <R> <C> [EMPTY] <C> Full model <C> 0.807 <C> 0.722 <C> [BOLD] 0.762 <R> <C> Turkish <C> [ITALIC] Morf-Base <C> 0.827 <C> 0.362 <C> 0.504 <R> <C> Turkish <C> [ITALIC] Morf-Cat <C> 0.522 <C> 0.607 <C> 0.561 <R> <C> Turkish <C> [ITALIC] AGMorph <C> 0.878 <C> 0.466 <C> 0.609 <R> <C> Turkish <C> [ITALIC] Lee (M2) <C> 0.787 <C> 0.355 <C> 0.489 <R> <C> Turkish <C> Model -C <C> 0.516 <C> 0.652 <C> 0.576 <R> <C> Turkish <C> Model -T <C> 0.665 <C> 0.521 <C> 0.584 <R> <C> Turkish <C> Model -A <C> 0.668 <C> 0.543 <C> 0.599 <R> <C> [EMPTY] <C> Full model <C> 0.743 <C> 0.520 <C> [BOLD] 0.612 <R> <C> Arabic <C> [ITALIC] Morf-Base <C> 0.807 <C> 0.204 <C> 0.326 <R> <C> Arabic <C> [ITALIC] Morf-Cat <C> 0.774 <C> 0.726 <C> 0.749 <R> <C> Arabic <C> [ITALIC] AGMorph <C> 0.672 <C> 0.761 <C> 0.713 <R> <C> Arabic <C> [ITALIC] Poon et al. <C> 0.885 <C> 0.692 <C> 0.777 <R> <C> Arabic <C> [ITALIC] Lee (M2) <C> - <C> - <C> [BOLD] 0.820 <R> <C> Arabic <C> Model -C <C> 0.626 <C> 0.912 <C> 0.743 <R> <C> Arabic <C> Model -T <C> 0.774 <C> 0.807 <C> 0.790 <R> <C> Arabic <C> Model -A <C> 0.775 <C> 0.808 <C> 0.791 <R> <C> [EMPTY] <C> Full model <C> 0.770 <C> 0.831 <C> 0.799 <CAP> Table 5: Results on unsupervised morphological segmentation; scores are calculated across all segmentation points in the test data. Baselines are in italics. -C=without cosine features, -T=without transformation features, -A=without affix correlation features. Numbers on Arabic for Poon et al. and Lee (M2) are reported directly from their papers.
<R> <C> Model <C> Parameters <C> Embed. dim. <C> SentEval Downstream <C> SentEval Probing <C> SentEval Avg. <C> SentEval Δ <R> <C> [ITALIC] Bag-of-words (BoW) weak baselines <C> [ITALIC] Bag-of-words (BoW) weak baselines <C> [ITALIC] Bag-of-words (BoW) weak baselines <C> [ITALIC] Bag-of-words (BoW) weak baselines <C> [ITALIC] Bag-of-words (BoW) weak baselines <C> [ITALIC] Bag-of-words (BoW) weak baselines <C> [ITALIC] Bag-of-words (BoW) weak baselines <R> <C> GloVe <C> – <C> 300 <C> 65.50 <C> 62.42 <C> 63.96 <C> -12.02 <R> <C> fastText <C> – <C> 300 <C> 68.57 <C> 63.16 <C> 65.87 <C> -10.11 <R> <C> [ITALIC] Supervised and semi-supervised <C> [ITALIC] Supervised and semi-supervised <C> [ITALIC] Supervised and semi-supervised <C> [ITALIC] Supervised and semi-supervised <C> [ITALIC] Supervised and semi-supervised <C> [ITALIC] Supervised and semi-supervised <C> [ITALIC] Supervised and semi-supervised <R> <C> InferSent <C> 38M <C> 4096 <C> 76.46 <C> 72.58 <C> 74.52 <C> -1.46 <R> <C> USE <C> 147M <C> 512 <C> [BOLD] 79.13 <C> 66.70 <C> 72.91 <C> -3.06 <R> <C> Sentence Transformers <C> 125M <C> 768 <C> 77.59 <C> 63.22 <C> 70.40 <C> -5.57 <R> <C> [ITALIC] Unsupervised <C> [ITALIC] Unsupervised <C> [ITALIC] Unsupervised <C> [ITALIC] Unsupervised <C> [ITALIC] Unsupervised <C> [ITALIC] Unsupervised <C> [ITALIC] Unsupervised <R> <C> Transformer-small <C> 82M <C> 768 <C> 72.69 <C> [BOLD] 74.27 <C> 73.48 <C> -2.50 <R> <C> Transformer-base <C> 125M <C> 768 <C> 72.22 <C> 73.38 <C> 72.80 <C> -3.18 <R> <C> DeCLUTR-small (ours) <C> 82M <C> 768 <C> 76.80 <C> 73.84 <C> 75.32 <C> -0.66 <R> <C> DeCLUTR-base (ours) <C> 125M <C> 768 <C> 78.16 <C> 73.80 <C> [BOLD] 75.98 <C> – <CAP> Table 1: Results on the downstream and probing tasks from the test set of the SentEval benchmark. USE: Google’s Universal Sentence Encoder. Transformer-small and Transformer-base are pretrained DistilRoBERTa and RoBERTa-base models respectively, using mean pooling. DeCLUTR-small and DeCLUTR-base are pretrained DistilRoBERTa and RoBERTa-base models respectively after continued pretraining with our method. Bold: best scores. Δ: difference to our methods (DeCLUTR-base) average score.
<R> <C> Method <C> F1 <C> Precision <C> Recall <R> <C> Perspective baseline <C> 0.57 <C> 0.54 <C> 0.60 <R> <C> Sentence length baseline <C> 0.47 <C> 0.44 <C> 0.51 <R> <C> BERT ensemble <C> 0.66 <C> 0.63 <C> 0.69 <CAP> Table 1: Performance of baselines on development set.
<R> <C> Method <C> MSE <R> <C> M2P2 without alignment loss <C> 0.018 <R> <C> M2P2 without reference models <C> 0.015 <R> <C> M2P2-LSTM <C> 0.032 <R> <C> M2P2-Acoustic <C> 0.017 <R> <C> M2P2-Visual <C> 0.019 <R> <C> M2P2-Language <C> 0.016 <R> <C> M2P2 <C> [BOLD] 0.012 <CAP> Table 3: Ablation study results. All improvements are statistically significant (p-val<0.01).
<R> <C> # paragraphs <C> # tokens <C> # types <R> <C> 2,532,361 <C> 100,872,713 <C> 156,663 <CAP> Table 1: Training data sizes.
<R> <C> [EMPTY] <C> [BOLD] Senti <C> [BOLD] NER <C> [BOLD] POS <C> [BOLD] Parse <R> <C> [ITALIC] ρ(d, p) <C> -0.68 <C> 0.76 <C> 0.66 <C> -0.76 <R> <C> [ITALIC] ρ(p, s) <C> 0.33 <C> 0.75 <C> 0.75 <C> -0.45 <R> <C> [ITALIC] ρ(s, d) <C> -0.16 <C> 0.81 <C> 0.51 <C> 0.67 <CAP> Table 4: Curricula correlations across feature groups.
<R> <C> [BOLD] Model <C> [BOLD] Dev  [BOLD] EM <C> [BOLD] Dev  [BOLD] F1 <C> [BOLD] Test  [BOLD] EM <C> [BOLD] Test  [BOLD] F1 <R> <C> [ITALIC] Regular Track <C> [ITALIC] Regular Track <C> [ITALIC] Regular Track <C> [ITALIC] Regular Track <C> [ITALIC] Regular Track <R> <C> Joint SAN <C> 69.3 <C> 72.2 <C> 68.7 <C> 71.4 <R> <C> U-Net <C> 70.3 <C> 74.0 <C> 69.2 <C> 72.6 <R> <C> RMR + ELMo + Verifier <C> 72.3 <C> 74.8 <C> 71.7 <C> 74.2 <R> <C> [ITALIC] BERT Track <C> [ITALIC] BERT Track <C> [ITALIC] BERT Track <C> [ITALIC] BERT Track <C> [ITALIC] BERT Track <R> <C> Human <C> - <C> - <C> 86.8 <C> 89.5 <R> <C> BERT + DAE + AoA† <C> - <C> - <C> 85.9 <C> 88.6 <R> <C> BERT + NGM + SST† <C> - <C> - <C> 85.2 <C> 87.7 <R> <C> BERT + CLSTM + MTL + V† <C> - <C> - <C> 84.9 <C> 88.2 <R> <C> SemBERT† <C> - <C> - <C> 84.8 <C> 87.9 <R> <C> Insight-baseline-BERT† <C> - <C> - <C> 84.8 <C> 87.6 <R> <C> BERT + MMFT + ADA† <C> - <C> - <C> 83.0 <C> 85.9 <R> <C> BERTLARGE <C> - <C> - <C> 82.1 <C> 84.8 <R> <C> Baseline <C> 84.1 <C> 86.8 <C> - <C> - <R> <C> [BOLD] SG-Net <C> [BOLD] 85.1 <C> [BOLD] 87.9 <C> - <C> - <R> <C> [BOLD] +Verifier <C> [BOLD] 85.6 <C> [BOLD] 88.3 <C> [BOLD] 85.2 <C> [BOLD] 87.9 <CAP> Table 1: Exact Match (EM) and F1 scores (%) on SQuAD 2.0 dataset for single models. Our model is in boldface. † refers to unpublished work. Besides published works, we also list competing systems on the SQuAD leaderboard at the time of submitting SG-Net (May 14, 2019). Our model is significantly better than the baseline BERT with p-value < 0.01.
<R> <C> [BOLD] Model <C> [BOLD] RACE-M <C> [BOLD] RACE-H <C> [BOLD] RACE <R> <C> [ITALIC] Human Performance <C> [ITALIC] Human Performance <C> [ITALIC] Human Performance <C> [ITALIC] Human Performance <R> <C> Turkers <C> 85.1 <C> 69.4 <C> 73.3 <R> <C> Ceiling <C> 95.4 <C> 94.2 <C> 94.5 <R> <C> [ITALIC] Leaderboard <C> [ITALIC] Leaderboard <C> [ITALIC] Leaderboard <C> [ITALIC] Leaderboard <R> <C> DCMN <C> 77.6 <C> 70.1 <C> 72.3 <R> <C> BERTLARGE <C> 76.6 <C> 70.1 <C> 72.0 <R> <C> OCN <C> 76.7 <C> 69.6 <C> 71.7 <R> <C> Baseline <C> 78.4 <C> 70.4 <C> 72.6 <R> <C> [BOLD] SG-Net <C> [BOLD] 78.8 <C> [BOLD] 72.2 <C> [BOLD] 74.2 <CAP> Table 2: Accuracy (%) on RACE test set for single models. Our model is significantly better than the baseline BERT with p-value < 0.01.
<R> <C> [BOLD] Model <C> [BOLD] EM <C> [BOLD] F1 <R> <C> baseline <C> 84.1 <C> 86.8 <R> <C> + Vanilla attention only <C> 84.2 <C> 86.9 <R> <C> + Syntax-guided attention only <C> 84.4 <C> 87.2 <R> <C> + Dual contextual attention <C> [BOLD] 85.1 <C> [BOLD] 87.9 <R> <C> Concatenation <C> 84.5 <C> 87.6 <R> <C> Bi-attention <C> 84.9 <C> 87.8 <CAP> Table 3: Ablation study on potential components and aggregation methods on SQuAD 2.0 dev set.
<R> <C> Word Embedding <C> Model <C> MAP MeSH terms <C> MAP Clinical topics <R> <C> Random <C> DeepMatch <C> 0.1826 <C> 0.2723 <R> <C> [EMPTY] <C> Delta <C> 0.3276 <C> 0.4054 <R> <C> [EMPTY] <C> GRAPHENE <C> 0.3629 <C> 0.4362 <R> <C> Word2Vec <C> DeepMatch <C> 0.33326 <C> 0.4155 <R> <C> [EMPTY] <C> Delta <C> 0.3521 <C> 0.4645 <R> <C> [EMPTY] <C> GRAPHENE <C> 0.4131 <C> 0.5903 <R> <C> Glove <C> DeepMatch <C> 0.3589 <C> 0.4375 <R> <C> [EMPTY] <C> Delta <C> 0.3842 <C> 0.4856 <R> <C> [EMPTY] <C> GRAPHENE <C> 0.4276 <C> 0.5967 <CAP> Table 4. Performance with different choices of pre-trained word embeddings on deep neural IR models
<R> <C> Model <C> NDCG.20 <C> MAP <C> Prec.10 <R> <C> DeepMatch <C> 0.1767 <C> 0.1265 <C> 0.1923 <R> <C> Delta <C> 0.1134 <C> 0.0843 <C> 0.1243 <R> <C> GRAPHENE <C> 0.1221 <C> 0.0752 <C> 0.1385 <CAP> Table 5. Performance without pre-train via MeSH terms on deep neural IR models
<R> <C> [EMPTY] <C> PTB-YM UAS <C> PTB-YM LAS <C> CTB UAS <C> CTB LAS <C> Parsing (sec/sent) <C> Complexity <R> <C> [ITALIC] PScorer <C> 93.05 <C> [EMPTY] <C> 87.73 <C> [EMPTY] <C> 0.011 <C> O( [ITALIC] n3) <R> <C> [ITALIC] PBase <C> 93.13 <C> 92.05 <C> 87.23 <C> 85.95 <C> 0.004 <C> O( [ITALIC] n) <R> <C> [ITALIC] PCH <C> 93.58 <C> 92.64 <C> 87.82 <C> 86.54 <C> 0.012 <C> O( [ITALIC] n) <R> <C> [ITALIC] PBase, [ITALIC] Global <C> 93.95 <C> 92.84 <C> 88.67 <C> 87.27 <C> 0.222 <C> O( [ITALIC] n3)+ <R> <C> [ITALIC] PCH, [ITALIC] Global <C> [BOLD] 94.33 <C> [BOLD] 93.37 <C> 88.89 <C> [BOLD] 87.58 <C> 0.235 <C> O( [ITALIC] n3)+ <R> <C> ZM2014 <C> 93.57 <C> 92.48 <C> 87.96 <C> 86.34 <C> [EMPTY] <C> O( [ITALIC] n3)+ <R> <C> Dyer2015 <C> [EMPTY] <C> [EMPTY] <C> 87.2 <C> 85.7 <C> 0.010 <C> O( [ITALIC] n) <R> <C> Zhang2016 <C> 93.31 <C> 92.23 <C> 87.65 <C> 86.17 <C> [EMPTY] <C> O( [ITALIC] n3)+ <R> <C> Wang2016 <C> 93.51 <C> 92.45 <C> 87.55 <C> 86.23 <C> 0.038 <C> O( [ITALIC] n3) <R> <C> Kiperwasser2016 <C> [EMPTY] <C> [EMPTY] <C> 87.6 <C> 86.1 <C> [EMPTY] <C> O( [ITALIC] n3) <R> <C> Wu2016 <C> [EMPTY] <C> [EMPTY] <C> 87.33 <C> 85.97 <C> [EMPTY] <C> O( [ITALIC] n) <R> <C> Cheng2016 <C> [EMPTY] <C> [EMPTY] <C> 88.1 <C> 85.7 <C> [EMPTY] <C> O( [ITALIC] n2)+ <R> <C> Sheng2014 <C> 93.37 <C> [EMPTY] <C> [BOLD] 89.16 <C> [EMPTY] <C> 11.78 <C> O( [ITALIC] n4)+ <R> <C> LZ2014 <C> 93.12 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> O( [ITALIC] n3) <R> <C> Zhu2015 <C> 93.83 <C> [EMPTY] <C> 85.7 <C> [EMPTY] <C> [EMPTY] <C> O( [ITALIC] n)+ <R> <C> Zhou2016 <C> 93.61 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 0.062 <C> O( [ITALIC] n)+ <CAP> Table 3. The results of the parsers. The different systems are taken from: ZM2014(Zhang and McDonald [31]); Dyer2015(Dyer et al. [10]); Zhang2016(Zhang et al. [35]); Wang2016(Wang et al. [28]); Kiperwasser2016(Kiperwasser et al. [15]); Wu2016(Wu et al. [29]); Cheng2016(Cheng et al. [5]); Sheng2014(Sheng et al. [22]); LZ2014 (Le and Zuidema [16]); Zhu2015(Zhu et al. [37]); Zhou2016 (Zhou et al. [36])
<R> <C> [EMPTY] <C> PTB-YM ROOT <C> PTB-YM 1 <C> PTB-YM 2 <C> PTB-YM 3-6 <C> PTB-YM 7-… <R> <C> [ITALIC] PBase <C> 95.32 <C> 97.03 <C> 94.63 <C> 91.35 <C> 86.88 <R> <C> [ITALIC] PBase, [ITALIC] Global <C> 95.51 <C> 97.28 <C> 95.17 <C> 92.49 <C> [BOLD] 89.12(+2.23) <R> <C> [ITALIC] PCH <C> 95.97 <C> 97.17 <C> 94.94 <C> 91.91 <C> 88.13 <R> <C> [ITALIC] PCH, [ITALIC] Global <C> 96.30 <C> 97.45 <C> 95.50 <C> 93.01 <C> 89.62 <R> <C> [EMPTY] <C> CTB5 <C> CTB5 <C> CTB5 <C> CTB5 <C> CTB5 <R> <C> [EMPTY] <C> ROOT <C> 1 <C> 2 <C> 3-6 <C> 7-… <R> <C> [ITALIC] PBase <C> 81.82 <C> 95.82 <C> 89.11 <C> 87.18 <C> 83.86 <R> <C> [ITALIC] PBase, [ITALIC] Global <C> 82.29 <C> 96.28 <C> 90.21 <C> 88.63 <C> 85.37 <R> <C> [ITALIC] PCH <C> 82.75 <C> 96.05 <C> 89.63 <C> 87.75 <C> 84.23 <R> <C> [ITALIC] PCH, [ITALIC] Global <C> 82.25 <C> 96.39 <C> 90.42 <C> 88.79 <C> [BOLD] 85.37(+1.51) <CAP> Table 4. F1 score of binned distance for PTB-YM and CTB5
<R> <C> [BOLD] Ablation <C> [BOLD] Exact Match <C> [BOLD] Zero-shot Retrieval Recall@5 <R> <C> [EMPTY] <C> 38.2 <C> 38.5 <R> <C> REALM retriever+Baseline encoder <C> 37.4 <C> 38.5 <R> <C> Baseline retriever+REALM encoder <C> 35.3 <C> 13.9 <R> <C> Baseline (ORQA) <C> 31.3 <C> 13.9 <R> <C> with random uniform masks <C> 32.3 <C> 24.2 <R> <C> with random span masks <C> 35.3 <C> 26.1 <R> <C> 30× stale MIPS <C> 28.7 <C> 15.1 <CAP> Table 2: Ablation experiments on NQ’s development set.
<R> <C> # Sent. <C> es→ en <C> en → fr <C> es → fr <R> <C> 0 <C> 31.53 <C> 30.46 <C> 29.52 <R> <C> 1K <C> 32.64 <C> 30.29 <C> 30.23 <R> <C> 10K <C> 32.92 <C> 30.93 <C> 31.51 <R> <C> 50K <C> 33.29 <C> 31.57 <C> 32.40 <R> <C> 100K <C> 33.35 <C> 31.63 <C> 32.45 <CAP> Table 7: Effect of the data size of source-to-target parallel corpora (Bridge Corpora) used in Likelihood.
<R> <C> [EMPTY] <C> subjectivity <C> sentiment <C> named entity <R> <C> TP <C> 5,676 <C> 6,764 <C> 6,345 <R> <C> FP <C> 1,504 <C> 1,005 <C> 0,469 <R> <C> TN <C> 1,917 <C> 1,565 <C> 2,841 <R> <C> FN <C> 0,903 <C> 0,666 <C> 0,345 <CAP> Table 2: Detailed evaluation of BiRNN trained with 1m data for Transformer trained on 0.1m parallel sentence pairs.
<R> <C> System <C> IAM CER(%) <C> IAM WER(%) <C> RIMES CER(%) <C> RIMES WER(%) <R> <C> Baseline <C> 17.4 <C> 25.5 <C> 12.0 <C> 19.1 <R> <C> + LN <C> 13.1 <C> 22.9 <C> 9.7 <C> 15.8 <R> <C> + LN + Focal Loss <C> 11.4 <C> 21.1 <C> 7.3 <C> 13.5 <R> <C> + LN + Focal Loss + Beam Search <C> [BOLD] 8.1 <C> [BOLD] 16.7 <C> [BOLD] 3.5 <C> [BOLD] 9.6 <CAP> Table 3: Effect of Layer Normalization, Focal Loss & Beam Search on Model Performance
<R> <C> Model <C> [BOLD] Spanish  [BOLD] Intent acc. <C> [BOLD] Spanish  [BOLD] Intent acc. <C> [BOLD] Spanish  [BOLD] Slot F1 <C> [BOLD] Spanish  [BOLD] Slot F1 <C> [BOLD] Thai  [BOLD] Intent acc. <C> [BOLD] Thai  [BOLD] Intent acc. <C> [BOLD] Thai  [BOLD] Slot F1 <C> [BOLD] Thai  [BOLD] Slot F1 <R> <C> Model <C> [BOLD] LVM <C> [BOLD] CRF <C> [BOLD] LVM <C> [BOLD] CRF <C> [BOLD] LVM <C> [BOLD] CRF <C> [BOLD] LVM <C> [BOLD] CRF <R> <C> Vanilla BiLSTM <C> 46.36 <C> 44.13 <C> 15.64 <C> 11.32 <C> 35.12 <C> 33.57 <C> 5.82 <C> 5.24 <R> <C> +  [ITALIC] noise (N) <C> 72.97 <C> 66.95 <C> 46.56 <C> 20.27 <C> 40.37 <C> 37.53 <C> 10.66 <C> 6.51 <R> <C> +  [ITALIC] refinement (R) <C> 87.69 <C> 88.23 <C> 61.63 <C> 42.62 <C> 59.40 <C> 59.28 <C> 21.84 <C> 16.53 <R> <C> +  [ITALIC] noise & refinement <C> 89.21 <C> 88.79 <C> 64.04 <C> 43.98 <C> 70.81 <C> 64.48 <C> 29.54 <C> 17.46 <R> <C> +  [ITALIC] N & R & delexicalization <C> [BOLD] 90.20 <C> 89.98 <C> [BOLD] 65.79 <C> 47.70 <C> [BOLD] 73.43 <C> 69.62 <C> [BOLD] 32.24 <C> 23.11 <R> <C> Zero-shot SLU ‡ <C> 46.64 <C> 46.64 <C> 15.41 <C> 15.41 <C> 35.64 <C> 35.64 <C> 12.11 <C> 12.11 <R> <C> Multi. CoVe w/ auto <C> 53.89 <C> 53.89 <C> 19.25 <C> 19.25 <C> 70.70 <C> 70.70 <C> 35.62 <C> 35.62 <R> <C> Translate Train † <C> [ITALIC] 85.39 <C> [ITALIC] 85.39 <C> [ITALIC] 72.87 <C> [ITALIC] 72.87 <C> [ITALIC] 95.85 <C> [ITALIC] 95.85 <C> [ITALIC] 55.43 <C> [ITALIC] 55.43 <CAP> Table 1: Results on different models including baseline models, where N refers to the Gaussian noise injection and R refers to the cross-lingual embeddings refinement. ‡ We implemented Upadhyay et al. (2018) model and evaluated with our test set. † Schuster et al. (2019) translated English data to Spanish and Thai with a trained supervised machine translation system and it is considered as our upper bound result.
<R> <C> Model <C> Spanish Intent <C> Spanish Slot <C> Thai Intent <C> Thai Slot <R> <C> Our model <C> [BOLD] 90.20 <C> [BOLD] 65.79 <C> [BOLD] 73.43 <C> [BOLD] 32.24 <R> <C> [ITALIC] - LVM <C> 85.85 <C> 61.86 <C> 66.01 <C> 25.22 <R> <C> [ITALIC] - LVM + MLP <C> 86.02 <C> 62.34 <C> 66.56 <C> 28.35 <CAP> Table 2: Ablation Study on LVM models. {- LVM} means removing LVM and {- LVM + MLP} means replacing LVM with a Multi-Layer Perceptron which has the same size as the LVM.
<R> <C> Datasets <C> Training <C> Validation <C> Testing <C> Classes <C> Ave length <C> Content <R> <C> AG <C> 96,000 <C> 24,000 <C> 7,600 <C> 4 <C> 43 <C> Title+description fields <R> <C> DBPedia <C> 448,000 <C> 112,000 <C> 70,000 <C> 14 <C> 52 <C> Title+abstract of article <R> <C> Yelp <C> 520,000 <C> 130,000 <C> 50,000 <C> 5 <C> 149 <C> Food reviews <R> <C> Yahoo <C> 1,120,000 <C> 280,000 <C> 60,000 <C> 10 <C> 105 <C> Title+question+best answer <R> <C> Amazon <C> 2,880,000 <C> 720,000 <C> 400,000 <C> 2 <C> 86 <C> Title+content <CAP> Table 2: Descriptions of the training, validation, and testing samples split of the five datasets.
<R> <C> [BOLD] Dataset <C> [BOLD] Dev2016  [BOLD] MAP <C> [BOLD] Dev2016  [BOLD] Accuracy <R> <C> Qatar Living Forum <C> [BOLD] 0.6311 <C> 0.7078 <R> <C> Qatar Living Forum+Ext <C> 0.6269 <C> [BOLD] 0.7131 <R> <C> Google News <C> 0.6113 <C> 0.6996 <R> <C> Doha News <C> 0.5769 <C> 0.6844 <CAP> Table 2: Semantic vectors trained on different unannotated datasets as the only features for subtask A: training on train2016-part1, testing on dev2016.
<R> <C> [EMPTY] <C> recall <C> precision <C> F1-score <R> <C> CRF baseline <C> 48.09 <C> 71.37 <C> 57.47 <R> <C> auto NE <C> 48.09 <C> 71.37 <C> 57.47 <R> <C> CRF baseline <C> 52.17 <C> 74.71 <C> 61.44 <R> <C> ref NE <C> 52.17 <C> 74.71 <C> 61.44 <R> <C> DNN <C> 51.36 <C> 66.78 <C> 58.06 <R> <C> DNN <C> 61.14 <C> 67.77 <C> 64.29 <R> <C> with unlabeled data <C> 61.14 <C> 67.77 <C> 64.29 <CAP> Table 1: Event trigger word identification results using different systems.
<R> <C> Data sets <C> sample boundary COCO <C> sample boundary EMNLP <C> classification error COCO <C> classification error EMNLP <R> <C> baseline <C> — <C> — <C> 0.325 <C> 0.312 <R> <C> [ITALIC] c=0.8 <C> 0.010 <C> 0.002 <C> 0.374 <C> 0.334 <R> <C> [ITALIC] c=0.5 <C> 0.160 <C> 0.018 <C> 0.400 <C> 0.360 <R> <C> [ITALIC] c=0.2 <C> 1.000 <C> 1.000 <C> 0.408 <C> 0.370 <CAP> Figure 5: The influence of acceptance ratio.
<R> <C> Training Method <C> GRID PER <C> TCD-TIMIT PER-61 4.2pt <C> TCD-TIMIT PER-39 <R> <C> ASR-Mod. Clean-Audio <C> 5.8 <C> 46.7 <C> 40.6 <R> <C> ASR-Mod. Mixed-Audio <C> 49.4 <C> 78.4 <C> 71.3 <R> <C> ASR-Mod. Mixed-A/V <C> 49.9 <C> 77.2 <C> 70.9 <R> <C> ASR-Mod. Visual <C> 29.4 <C> 78.6 <C> 74.7 <R> <C> Joint-Mod. Joint loss <C> 15.4 <C> 53.1 <C> 47.7 <R> <C> Joint-Mod. Alt. 2 full <C> 16.0 <C> 45.6 <C> 41.2 <R> <C> Joint-Mod. Alt. 2 full freeze <C> 18.7 <C> 3.0pt [BOLD] 44.3 <C> 3.0pt [BOLD] 40.0 <R> <C> Joint-Mod. Alt. <C> −0.5pt [BOLD] 13.9 <C> 44.9 <C> 40.6 <R> <C> Joint-Mod. Alt. freeze <C> 18.1 <C> 61.3 <C> 55.5 <R> <C> Joint-Mod. PIT Alt. <C> 43.3 <C> 67.1 <C> 62.4 <CAP> Table 1: Results on GRID and TCD-TIMIT, the first part of the table contains the results by the ASR baseline models, while in the second part, the results obtained by the joint models trained with the various training strategies are reported. All the results are computed on the test set.
<R> <C> [BOLD] Relation Type <C> [BOLD] NN w/ Representation of Same Type (%)  [BOLD] BioELMo <C> [BOLD] NN w/ Representation of Same Type (%)  [BOLD] ELMo <C> [BOLD] NN w/ Representation of Same Type (%)  [BOLD] BioBERT-tog <C> [BOLD] NN w/ Representation of Same Type (%)  [BOLD] BioBERT <C> [BOLD] NN w/ Representation of Same Type (%)  [BOLD] BERT-tog <C> [BOLD] NN w/ Representation of Same Type (%)  [BOLD] BERT <C> [BOLD] NN w/ Representation of Same Type (%)  [BOLD] Biomed w2v <R> <C> [BOLD] disease-symptom <C> [BOLD] 54.2 <C> 52.1 <C> 44.5 <C> 38.8 <C> 34.2 <C> 37.0 <C> 40.9 <R> <C> [BOLD] disease-drug <C> 32.8 <C> [BOLD] 34.4 <C> 26.1 <C> 17.9 <C> 27.7 <C> 22.6 <C> 23.6 <R> <C> [BOLD] number-indication <C> 70.5 <C> 63.9 <C> 47.0 <C> 45.3 <C> 48.1 <C> 49.5 <C> [BOLD] 74.4 <R> <C> [BOLD] synonyms <C> [BOLD] 63.6 <C> 56.4 <C> 60.8 <C> 55.8 <C> 56.4 <C> 52.8 <C> 51.7 <R> <C> [BOLD] All <C> [BOLD] 57.5 <C> 53.3 <C> 47.1 <C> 42.1 <C> 43.3 <C> 42.5 <C> 49.5 <R> <C> [BOLD] Subset Accuracy (%) <C> [BOLD] 73.9 <C> 62.8 <C> 71.4 <C> 65.0 <C> 65.8 <C> 64.5 <C> 69.7 <CAP> Table 3: Average proportion of nearest neighbor (NN) representations that belong to the same type for different embeddings, averaged over three random seeds. Biomed w2v performs best for number-indication relations, probably because it uses a vocabulary of over 5M tokens, in which about 100k are numbers. Subset accuracy denotes the probing task performance in the subset of MedNLI test set used for this analysis.
<R> <C> Settings Dev <C> Settings F1 <C> DrQA 4.5 <C> BERTserini 19.3 <C> Ours w/o hist. 24.0 <C> Ours  [BOLD] 26.9‡ <R> <C> Dev <C> HEQ-Q <C> 0.0 <C> 14.1 <C> 15.2 <C> [BOLD] 17.5 <R> <C> Dev <C> HEQ-D <C> 0.0 <C> [BOLD] 0.2 <C> [BOLD] 0.2 <C> [BOLD] 0.2 <R> <C> Dev <C> Rt MRR <C> 0.1151 <C> 0.1767 <C> 0.4012 <C> [BOLD] 0.4286‡ <R> <C> Dev <C> Rr MRR <C> [EMPTY] <C> [EMPTY] <C> 0.4472 <C> [BOLD] 0.5209‡ <R> <C> Dev <C> Rt Recall <C> 0.2000 <C> 0.2656 <C> 0.5271 <C> [BOLD] 0.5714‡ <R> <C> Test <C> F1 <C> 6.3 <C> 26.0 <C> 26.3 <C> [BOLD] 29.4‡ <R> <C> Test <C> HEQ-Q <C> 0.1 <C> 20.4 <C> 20.7 <C> [BOLD] 24.1 <R> <C> Test <C> HEQ-D <C> 0.0 <C> 0.1 <C> 0.4 <C> [BOLD] 0.6 <R> <C> Test <C> Rt MRR <C> 0.1574 <C> 0.1784 <C> 0.1979 <C> [BOLD] 0.2246‡ <R> <C> Test <C> Rr MRR <C> [EMPTY] <C> [EMPTY] <C> 0.2702 <C> [BOLD] 0.3127‡ <R> <C> Test <C> Rt Recall <C> 0.2253 <C> 0.2507 <C> 0.2859 <C> [BOLD] 0.3141‡ <CAP> Table 3. Main evaluation results. “Rt” and “Rr” refers to “Retriever” and “Reranker”. ‡ means statistically significant improvement over the strongest baseline with p<0.05.
<R> <C> class <C> stage 1 <C> stage 2 <C> Δ <R> <C> ORG <C> 9099 <C> 9137 <C> 38 <R> <C> PER <C> 2229 <C> 2214 <C> -15 <R> <C> LOC <C> 2039 <C> 2022 <C> -17 <R> <C> DATE <C> 956 <C> 955 <C> -1 <R> <C> PRO <C> 4458 <C> 4446 <C> -12 <R> <C> EVENT <C> 93 <C> 93 <C> 0 <R> <C> TOTAL <C> 18874 <C> 18863 <C> -11 <CAP> Table 1: Counts (second and third column) and their differences (fourth column) in named entity classes after the first and second annotation stages.
<R> <C> [EMPTY] <C> All Labels  [ITALIC] RP@5 <C> All Labels  [ITALIC] nDCG@5 <C> All Labels Micro- [ITALIC] F1 <C> Frequent  [ITALIC] RP@5 <C> Frequent  [ITALIC] nDCG@5 <C> Few  [ITALIC] RP@5 <C> Few  [ITALIC] nDCG@5 <C> Zero  [ITALIC] RP@5 <C> Zero  [ITALIC] nDCG@5 <R> <C> Exact Match <C> 0.097 <C> 0.099 <C> 0.120 <C> 0.219 <C> 0.201 <C> 0.111 <C> 0.074 <C> 0.194 <C> 0.186 <R> <C> Logistic Regression <C> 0.710 <C> 0.741 <C> 0.539 <C> 0.767 <C> 0.781 <C> 0.508 <C> 0.470 <C> 0.011 <C> 0.011 <R> <C> bigru-att <C> 0.758 <C> 0.789 <C> 0.689 <C> 0.799 <C> 0.813 <C> 0.631 <C> 0.580 <C> 0.040 <C> 0.027 <R> <C> han <C> 0.746 <C> 0.778 <C> 0.680 <C> 0.789 <C> 0.805 <C> 0.597 <C> 0.544 <C> 0.051 <C> 0.034 <R> <C> cnn-lwan <C> 0.716 <C> 0.746 <C> 0.642 <C> 0.761 <C> 0.772 <C> 0.613 <C> 0.557 <C> 0.036 <C> 0.023 <R> <C> bigru-lwan <C> [BOLD] 0.766 <C> [BOLD] 0.796 <C> [BOLD] 0.698 <C> [BOLD] 0.805 <C> [BOLD] 0.819 <C> [BOLD] 0.662 <C> [BOLD] 0.618 <C> 0.029 <C> 0.019 <R> <C> z-cnn-lwan <C> 0.684 <C> 0.717 <C> 0.618 <C> 0.730 <C> 0.745 <C> 0.495 <C> 0.454 <C> 0.321 <C> 0.264 <R> <C> z-bigru-lwan <C> 0.718 <C> 0.752 <C> 0.652 <C> 0.764 <C> 0.780 <C> 0.561 <C> 0.510 <C> [BOLD] 0.438 <C> [BOLD] 0.345 <R> <C> ensemble-lwan <C> [BOLD] 0.766 <C> [BOLD] 0.796 <C> [BOLD] 0.698 <C> [BOLD] 0.805 <C> [BOLD] 0.819 <C> [BOLD] 0.662 <C> [BOLD] 0.618 <C> [BOLD] 0.438 <C> [BOLD] 0.345 <R> <C> max-hss <C> 0.737 <C> 0.773 <C> 0.671 <C> 0.784 <C> 0.803 <C> 0.463 <C> 0.443 <C> 0.039 <C> 0.028 <R> <C> lw-han <C> 0.721 <C> 0.761 <C> 0.669 <C> 0.766 <C> 0.790 <C> 0.412 <C> 0.402 <C> 0.039 <C> 0.026 <CAP> Table 2: Results on eurlex57k for all, frequent (>50 training instances), few-shot (1 to 50 instances), and zero-shot labels. All the differences between the best (bold) and other methods are statistically significant (p<0.01).
<R> <C> [BOLD] Method <C> [BOLD] P@1 <C> [BOLD] P@10 <C> [BOLD] P@20 <C> [BOLD] P@30 <R> <C> DNN <C> 6.53 <C> 28.29 <C> 38.83 <C> 53.79 <R> <C> +ngram <C> 7.25 <C> 30.76 <C> 41.57 <C> 56.49 <R> <C> att-BiLSTM <C> 7.34 <C> 30.95 <C> 41.56 <C> 56.02 <CAP> Table 1. The offline comparison results of different methods in large-scale, real-world search logs of a widely used commercial web search engine.
<R> <C> [BOLD] Model <C> [BOLD] SQuAD 1.1 (dev.) EM <C> [BOLD] SQuAD 1.1 (dev.) F1 <C> [BOLD] MNLI (dev.) Acc. (m) <C> [BOLD] MNLI (dev.) Acc. (mm) <C> [BOLD] CoNLL NER Dev. F1 <C> [BOLD] CoNLL NER Test F1 <R> <C> Ours, BPE <C> 80.6 <C> 88.2 <C> 81.4 <C> 82.4 <C> 94.0 <C> 90.2 <R> <C> Ours, Unigram LM <C> 81.8 <C> 89.3 <C> 82.8 <C> 82.9 <C> 94.3 <C> 90.4 <R> <C> BERT \textsc  [ITALIC] base <C> 80.5 <C> 88.5 <C> 84.6 <C> 83.4 <C> 96.4 <C> 92.4 <CAP> Table 3: Fine-tuning results. Metrics are averaged across 5 fine-tuning seeds; due to computational constraints we did not pretrain more than once per tokenization. We include fine-tuning results for a transformer with a comparable architecture, BERT\textscbase, for reference, although we note that a direct comparison cannot be made due to BERT\textscbase using both a larger pretraining corpus and a larger subword vocabulary.
<R> <C> System <C> Sarcasticness <C> Creativity <C> Humor <C> Grammaticality <R> <C> State-of-the-art Mishra et al. ( 2019 ) <C> 1.63 <C> 1.60 <C> 1.50 <C> 1.46 <R> <C> Human Generated <C> [BOLD] 3.57 <C> 3.16 <C> [BOLD] 3.18 <C> 3.98 <R> <C> Reversal of Valence (RV) <C> 3.00 <C> 2.80 <C> 2.72 <C> [BOLD] 4.29 <R> <C> No Reversal of Valence (NoRV) <C> 1.79 <C> 2.28 <C> 2.09 <C> 3.91 <R> <C> No Semantic Incongruity (NSI) <C> 3.04 <C> 2.99 <C> 2.90 <C> 3.68 <R> <C> Full Model (FM) <C> 3.23* <C> [BOLD] 3.24 <C> 3.08* <C> 3.69 <CAP> Table 2: Average scores for generated sarcasm from all systems as judged by the Turkers. The scale ranges from 1 (not at all) to 5 (very). For creativity and grammaticality, our models are comparable to human annotation and significantly better than the state-of-the-art (p<0.001). For sarcasticness and humor, the full model is ranked 2nd by a small margin against the human generated message (denoted by *).
<R> <C> Aspect <C> FM vs Human win% <C> FM vs Human lose% <C> FM vs MTS2019 win% <C> FM vs MTS2019 lose% <R> <C> Sarcasticness <C> 34.0 <C> [BOLD] 55.3 <C> [BOLD] 90.0 <C> 6.0 <R> <C> Creativity <C> [BOLD] 48.0 <C> 36.0 <C> [BOLD] 95.3 <C> 4.0 <R> <C> Humor <C> 40.6 <C> [BOLD] 48.0 <C> [BOLD] 90.0 <C> 4.0 <R> <C> Grammaticality <C> 26.6 <C> [BOLD] 56.6 <C> [BOLD] 98.0 <C> 1.3 <CAP> Table 3: Pairwise comparison between the full model (FM) and human generated sarcasm, and between the full model (FM) and the state-of-the-art model in abhijit. Win % (lose %) is the percentage of the FM gets a higher (lower) average score compared to the other method for the 150 human-rated sentences. The rest are ties.
<R> <C> [EMPTY] <C> SIM <C> GRO1 <C> GRO2 <R> <C> [ITALIC] Nearest Neighbor <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> KNeighbors <C> 67.89 <C> 74.06 <C> 61.25 <R> <C> NearestCentroidOvR <C> 65.81 <C> 83.44 <C> 63.87 <R> <C> [ITALIC] Linear Models <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> SGD <C> 79.49 <C> 87.93 <C> 65.27 <R> <C> PassiveAggressive <C> 63.32 <C> 84.70 <C> 65.04 <R> <C> LogisticRegression <C> 76.61 <C> 86.17 <C> 61.95 <R> <C> [ITALIC] Ensemble Methods <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> RandomForest <C> 80.84 <C> 87.91 <C> 65.79 <R> <C> ExtraTrees <C> 67.89 <C> 74.06 <C> 61.25 <R> <C> AdaBoost <C> 79.49 <C> 87.93 <C> 65.27 <R> <C> GradientBoosting <C> 80.83 <C> [BOLD] 92.58 <C> 65.68 <R> <C> [ITALIC] Bayesian <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> GaussianNB <C> 63.85 <C> 84.44 <C> 65.25 <R> <C> MultinomialNB <C> 75.35 <C> 87.02 <C> 67.64 <R> <C> [ITALIC] Neural <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> MLP <C> [BOLD] 82.74 <C> 91.58 <C> [BOLD] 70.87 <CAP> TABLE I: Results comparison grounded and simulated data (sample-based f-score, n=17, k=5, p=0.5, |W|=100)
<R> <C> [BOLD] Models <C> [BOLD] H-1 <C> [BOLD] H-2 <C> [BOLD] H-3 <C> [BOLD] Majority Voting <R> <C> ViLBERT-Fr <C> 31 <C> 29 <C> 38 <C> 20 <R> <C> ViLBERT-Ra <C> [BOLD] 69 <C> [BOLD] 71 <C> [BOLD] 62 <C> [BOLD] 80 <CAP> Table 4: Percent of rationales preferred by three human judges. Here, H stands for Human.
<R> <C> [BOLD] Model <C> [BOLD] Evaluation Results  [BOLD] rPPL↓ <C> [BOLD] Evaluation Results  [BOLD] BLEU↑ <C> [BOLD] Evaluation Results  [BOLD] wKL↓ <C> [BOLD] Evaluation Results  [BOLD] NLL↓ <C> [BOLD] Regularization Terms KL( [ITALIC] z) <C> [BOLD] Regularization Terms KL( [ITALIC] c) <C> [BOLD] Regularization Terms Var <C> [BOLD] Regularization Terms Lmi <R> <C> Test Set <C> - <C> 100.0 <C> [BOLD] 0.14 <C> - <C> - <C> - <C> - <C> - <R> <C> LSTM-LM (Mikolov et al.,  2010 ) <C> - <C> - <C> - <C> 100.53 <C> - <C> - <C> - <C> - <R> <C> AE (Vincent et al.,  2010 ) <C> 730.81 <C> [BOLD] 10.88 <C> 0.58 <C> - <C> - <C> - <C> - <C> - <R> <C> VAE (Kingma and Welling,  2013 ) <C> 686.18 <C> 3.12 <C> 0.50 <C> ≤100.85 <C> 5.76 <C> - <C> - <C> - <R> <C> DAE <C> 797.17 <C> 3.93 <C> 0.58 <C> - <C> - <C> - <C> - <C> - <R> <C> DVAE <C> 744.07 <C> 1.56 <C> 0.55 <C> ≤101.07 <C> - <C> 3.87 <C> - <C> 0.17 <R> <C> DI-VAE (Zhao et al.,  2018b ) <C> 310.29 <C> 4.53 <C> 0.24 <C> ≤108.90 <C> - <C> 24.78 <C> - <C> 1.14 <R> <C> [ITALIC] semi-VAE (Kingma et al.,  2014 ) <C> 494.52 <C> 2.71 <C> 0.43 <C> ≤100.67 <C> 4.96 <C> 1.96 <C> - <C> 0.09 <R> <C> [ITALIC] semi-VAE + Lmi <C> 260.28 <C> 5.08 <C> 0.20 <C> ≤107.30 <C> 0.045 <C> 24.04 <C> - <C> 1.12 <R> <C> GM-VAE <C> 983.50 <C> 2.34 <C> 0.72 <C> ≤ [BOLD] 99.44 <C> 3.41 <C> 0.00 <C> 0.001 <C> 0.00 <R> <C> GM-VAE +Lmi <C> 287.07 <C> 6.26 <C> 0.25 <C> ≤103.16 <C> 9.13 <C> 28.38 <C> 13.10 <C> 1.30 <R> <C> DGM-VAE <C> 257.68 <C> 8.17 <C> 0.19 <C> ≤104.26 <C> 41.48 <C> 4.76 <C> 787.03 <C> 0.22 <R> <C> DGM-VAE + Lmi <C> [BOLD] 247.37 <C> [BOLD] 8.67 <C> [BOLD] 0.18 <C> ≤105.73 <C> 25.48 <C> 19.73 <C> 203.34 <C> 0.91 <CAP> Table 1: Language generation results on PTB. β=0.8 for DGM-VAE.
<R> <C> [BOLD] Model <C> [BOLD] Unsupervised DD  [BOLD] MI <C> [BOLD] Unsupervised DD  [BOLD] BLEU↑ <C> [BOLD] Unsupervised DD  [BOLD] act↑ <C> [BOLD] Unsupervised DD  [BOLD] em↑ <R> <C> DI-VAE <C> 1.20 <C> 3.05 <C> 0.18 <C> 0.09 <R> <C> [ITALIC] semi-VAE <C> 0.03 <C> 4.06 <C> 0.02 <C> 0.08 <R> <C> [ITALIC] semi-VAE + Lmi <C> 1.21 <C> 3.69 <C> 0.21 <C> 0.14 <R> <C> GM-VAE <C> 0.00 <C> 2.03 <C> 0.08 <C> 0.02 <R> <C> GM-VAE + Lmi <C> 1.41 <C> 2.96 <C> 0.19 <C> 0.09 <R> <C> DGM-VAE <C> 0.53 <C> [BOLD] 7.63 <C> 0.11 <C> 0.09 <R> <C> DGM-VAE + Lmi <C> 1.32 <C> 7.39 <C> [BOLD] 0.23 <C> [BOLD] 0.16 <CAP> Table 2: Results of interpretable language generation on DD. Mutual information (MI), BLEU and homogeneity with actions (act) and emotions (em) are shown. The larger↑, the better.
<R> <C> [BOLD] Model <C> [BOLD] Supervised DD  [BOLD] NLL↓ <C> [BOLD] Supervised DD  [BOLD] ACCact ↑ <C> [BOLD] Supervised DD  [BOLD] ACCem ↑ <C> [BOLD] Supervised DD Var <R> <C> DVAE <C> ≤ [BOLD] 48.71 <C> 0.79 <C> 0.63 <C> - <R> <C> CM-VAE <C> ≤52.24 <C> 0.79 <C> 0.63 <C> 0.00 <R> <C> DCM-VAE <C> ≤48.78 <C> [BOLD] 0.80 <C> [BOLD] 0.73 <C> 139.16 <CAP> Table 6: Results of supervised interpretable generation on DD. Negative log-likelihood (NLL), classification accuracy (ACC) on emotion (em) and action (act) and the variance of parameters (Var) are shown.
<R> <C> [EMPTY] <C> Intent <C> Entity <C> Combined <R> <C> [BOLD] HERMIT <C> [BOLD] 87.55±0.63 <C> 84.74±1.18 <C> [BOLD] 86.25±0.66 <R> <C> – SA <C> 87.03±0.74 <C> 84.35±1.15 <C> 85.81±0.81 <R> <C> – SA/CN <C> 87.09±0.78 <C> 82.43±1.42 <C> 84.97±0.72 <R> <C> – SA/CRF <C> 83.57±0.75 <C> [BOLD] 84.77±1.06 <C> 84.09±0.79 <R> <C> – SA/CN/CRF <C> 83.78±1.10 <C> 82.22±1.41 <C> 83.10±1.06 <CAP> Table 5: Ablation study of HERMIT on the NLU-BM.
<R> <C> Model <C> Rec. <C> Rec. [ITALIC] w <C> Prec. <C> [ITALIC] F1 <C> [ITALIC] Fw1 <R> <C> [ITALIC] dd-CRP clustering models <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> 1. baseline <C> 0.14 <C> 0.27 <C> 0.33 <C> 0.20 <C> 0.30 <R> <C> 2. offline <C> 0.32 <C> 0.47 <C> 0.27 <C> 0.29 <C> 0.34 <R> <C> 3. online <C> 0.34 <C> 0.55 <C> 0.26 <C> 0.29 <C> 0.35 <R> <C> [ITALIC] Top systems from Trec-2014 TTG <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> 4. TTGPKUICST2  <C> 0.37 <C> 0.58 <C> 0.46 <C> 0.35 <C> 0.46 <R> <C> 5. EM50  <C> 0.29 <C> 0.48 <C> 0.42 <C> 0.25 <C> 0.38 <R> <C> 6. hltcoeTTG1  <C> 0.40 <C> 0.59 <C> 0.34 <C> 0.28 <C> 0.37 <CAP> Table 1: Performance of Models in the TREC 2014 TTG Task. Weighted recall and F1 are indicated as Rec.w and Fw1.
<R> <C> TextC <C> SentiC (acc) <C> CNN <C> performance 82.38 <C> lr 0.2 <C> hidden 20 <C> batch 5 <C> sentLen 60 <C> filter_size 3 <C> margin – <R> <C> TextC <C> SentiC (acc) <C> GRU <C> [BOLD] 86.32 <C> 0.1 <C> 30 <C> 50 <C> 60 <C> – <C> – <R> <C> TextC <C> SentiC (acc) <C> LSTM <C> 84.51 <C> 0.2 <C> 20 <C> 40 <C> 60 <C> – <C> – <R> <C> TextC <C> RC (F1) <C> CNN <C> 68.02 <C> 0.12 <C> 70 <C> 10 <C> 20 <C> 3 <C> – <R> <C> TextC <C> RC (F1) <C> GRU <C> [BOLD] 68.56 <C> 0.12 <C> 80 <C> 100 <C> 20 <C> – <C> – <R> <C> TextC <C> RC (F1) <C> LSTM <C> 66.45 <C> 0.1 <C> 80 <C> 20 <C> 20 <C> – <C> – <R> <C> SemMatch <C> TE (acc) <C> CNN <C> 77.13 <C> 0.1 <C> 70 <C> 50 <C> 50 <C> 3 <C> – <R> <C> SemMatch <C> TE (acc) <C> GRU <C> [BOLD] 78.78 <C> 0.1 <C> 50 <C> 80 <C> 65 <C> – <C> – <R> <C> SemMatch <C> TE (acc) <C> LSTM <C> 77.85 <C> 0.1 <C> 80 <C> 50 <C> 50 <C> – <C> – <R> <C> SemMatch <C> AS (MAP & MRR) <C> CNN <C> ( [BOLD] 63.69,65.01) <C> 0.01 <C> 30 <C> 60 <C> 40 <C> 3 <C> 0.3 <R> <C> SemMatch <C> AS (MAP & MRR) <C> GRU <C> (62.58,63.59) <C> 0.1 <C> 80 <C> 150 <C> 40 <C> – <C> 0.3 <R> <C> SemMatch <C> AS (MAP & MRR) <C> LSTM <C> (62.00,63.26) <C> 0.1 <C> 60 <C> 150 <C> 45 <C> – <C> 0.1 <R> <C> SemMatch <C> QRM (acc) <C> CNN <C> [BOLD] 71.50 <C> 0.125 <C> 400 <C> 50 <C> 17 <C> 5 <C> 0.01 <R> <C> SemMatch <C> QRM (acc) <C> GRU <C> 69.80 <C> 1.0 <C> 400 <C> 50 <C> 17 <C> - <C> 0.01 <R> <C> SemMatch <C> QRM (acc) <C> LSTM <C> 71.44 <C> 1.0 <C> 200 <C> 50 <C> 17 <C> - <C> 0.01 <R> <C> SeqOrder <C> PQA (hit@10) <C> CNN <C> 54.42 <C> 0.01 <C> 250 <C> 50 <C> 5 <C> 3 <C> 0.4 <R> <C> SeqOrder <C> PQA (hit@10) <C> GRU <C> [BOLD] 55.67 <C> 0.1 <C> 250 <C> 50 <C> 5 <C> – <C> 0.3 <R> <C> SeqOrder <C> PQA (hit@10) <C> LSTM <C> 55.39 <C> 0.1 <C> 300 <C> 50 <C> 5 <C> – <C> 0.3 <R> <C> ContextDep <C> POS tagging (acc) <C> CNN <C> 94.18 <C> 0.1 <C> 100 <C> 10 <C> 60 <C> 5 <C> – <R> <C> ContextDep <C> POS tagging (acc) <C> GRU <C> 93.15 <C> 0.1 <C> 50 <C> 50 <C> 60 <C> – <C> – <R> <C> ContextDep <C> POS tagging (acc) <C> LSTM <C> 93.18 <C> 0.1 <C> 200 <C> 70 <C> 60 <C> – <C> – <R> <C> ContextDep <C> POS tagging (acc) <C> Bi-GRU <C> 94.26 <C> 0.1 <C> 50 <C> 50 <C> 60 <C> – <C> – <R> <C> ContextDep <C> POS tagging (acc) <C> Bi-LSTM <C> [BOLD] 94.35 <C> 0.1 <C> 150 <C> 5 <C> 60 <C> – <C> – <CAP> Table 1: Best results or CNN, GRU and LSTM in NLP tasks
<R> <C> Class <C> #Entities <C> #Coarse-Grained entities <R> <C> SportsTeam <C> 352006 <C> 320835 (91.1%) <R> <C> Company <C> 70208 <C> 55524 (79.1%) <R> <C> Settlement <C> 478906 <C> 246163 (51.4%) <R> <C> Activity <C> 19464 <C> 8824 (45.3%) <R> <C> Event <C> 76029 <C> 19418 (25.5%) <CAP> Figure 1: Exempt of a subgraph from DBpedia
<R> <C> Signer <C> Tandem HMM 1 <C> Tandem HMM 2 <C> Tandem HMM 3 <C> Tandem HMM 4 <C> Tandem HMM  [BOLD] Mean <C> Rescoring SCRF 1 <C> Rescoring SCRF 2 <C> Rescoring SCRF 3 <C> Rescoring SCRF 4 <C> Rescoring SCRF  [BOLD] Mean <C> 1st-pass SCRF 1 <C> 1st-pass SCRF 2 <C> 1st-pass SCRF 3 <C> 1st-pass SCRF 4 <C> 1st-pass SCRF  [BOLD] Mean <R> <C> No adapt. <C> 45.9 <C> 45.3 <C> 37.4 <C> 42.5 <C> [BOLD] 42.8 <C> 47.4 <C> 48.8 <C> 38.9 <C> 43.7 <C> [BOLD] 44.7 <C> 44.7 <C> 46.7 <C> 27.5 <C> 38.6 <C> [BOLD] 39.4 <R> <C> Forced align. <C> 69.8 <C> 71.5 <C> 60.4 <C> 63.9 <C> [BOLD] 66.4 <C> 70.5 <C> 74.0 <C> 61.8 <C> 65.5 <C> [BOLD] 68.0 <C> 75.6 <C> 75.1 <C> 63.5 <C> 64.5 <C> [BOLD] 69.7 <R> <C> Ground truth <C> 78.0 <C> 87.0 <C> 68.4 <C> 78.6 <C> [BOLD] 78.0 <C> 77.6 <C> 86.5 <C> 70.5 <C> 78.6 <C> [BOLD] 78.3 <C> 84.8 <C> 89.4 <C> 75.1 <C> 81.6 <C> [BOLD] 82.7 <CAP> Table 1: Letter accuracies (%) on four test signers.
<R> <C> Method <C> test all <C> control all <C> control context <R> <C> Baselines  <C> Baselines  <C> Baselines  <C> Baselines  <R> <C> Random in context <C> 1.6 <C> 0 <C> [EMPTY] <R> <C> Random cap. in context <C> 7.3 <C> 0 <C> [EMPTY] <R> <C> [ITALIC] n-gram <C> 0.1 <C> 19.1 <C> [EMPTY] <R> <C> [ITALIC] n-gram + cache <C> 0.1 <C> 19.1 <C> [EMPTY] <R> <C> LSTM <C> 0 <C> [BOLD] 21.9 <C> [EMPTY] <R> <C> Memory network <C> 0 <C> 8.5 <C> [EMPTY] <R> <C> Our context-restricted non-stopword baselines <C> Our context-restricted non-stopword baselines <C> Our context-restricted non-stopword baselines <C> Our context-restricted non-stopword baselines <R> <C> Random <C> 5.6 <C> 0.3 <C> 2.2 <R> <C> First <C> 3.8 <C> 0.1 <C> 1.1 <R> <C> Last <C> 6.2 <C> 0.9 <C> 6.5 <R> <C> Most frequent <C> 11.7 <C> 0.4 <C> 8.1 <R> <C> Our context-restricted language model baselines <C> Our context-restricted language model baselines <C> Our context-restricted language model baselines <C> Our context-restricted language model baselines <R> <C> [ITALIC] n-gram <C> 10.7 <C> 2.2 <C> 15.6 <R> <C> [ITALIC] n-gram + cache <C> 11.8 <C> 2.2 <C> 15.6 <R> <C> LSTM <C> 9.2 <C> 2.4 <C> 16.9 <R> <C> Our neural reader results <C> Our neural reader results <C> Our neural reader results <C> Our neural reader results <R> <C> Stanford Reader <C> 21.7 <C> 7.0 <C> 49.3 <R> <C> Modified Stanford Reader <C> 32.1 <C> 7.4 <C> 52.3 <R> <C> AS Reader <C> 41.4 <C> 8.5 <C> 60.2 <R> <C> AS Reader + features <C> 44.5 <C> 8.6 <C> 60.6 <R> <C> GA Reader <C> 45.4 <C> 8.8 <C> 62.5 <R> <C> GA Reader + features <C> [BOLD] 49.0 <C> 9.3 <C> [BOLD] 65.6 <R> <C> Human <C> [ITALIC] 86.0∗ <C> [ITALIC] 36.0† <C> - <CAP> Table 1: Accuracies on test and control datasets, computed over all instances (“all”) and separately on those in which the answer is in the context (“context”). The first section is from lambada:16. ∗Estimated from 100 randomly-sampled dev instances. †Estimated from 100 randomly-sampled control instances.
<R> <C> [BOLD] Model <C> [BOLD] Word Similarity  [BOLD] SL <C> [BOLD] Word Similarity  [BOLD] SV <C> [BOLD] Word Similarity  [BOLD] WS <C> [BOLD] Word Similarity  [BOLD] RG <C> [BOLD] Word Similarity  [BOLD] RW <C> [BOLD] Word Similarity  [BOLD] SCWS <C> [BOLD] Word Similarity  [BOLD] MC <C> [BOLD] Word Similarity  [BOLD] MEN <R> <C> [BOLD] GloVe <C> 0.41 <C> 0.28 <C> 0.74 <C> 0.77 <C> 0.54 <C> 0.64 <C> 0.80 <C> 0.80 <R> <C> ⊕ Affect <C> 0.49 <C> 0.39 <C> [BOLD] 0.77 <C> 0.79 <C> 0.59 <C> 0.67 <C> 0.80 <C> 0.84 <R> <C> + Retrofitting <C> 0.53 <C> 0.37 <C> 0.73 <C> 0.81 <C> 0.52 <C> 0.66 <C> 0.82 <C> 0.82 <R> <C> + Retrofitting ∗ c-strength <C> 0.53 <C> 0.36 <C> 0.74 <C> 0.81 <C> 0.52 <C> 0.66 <C> 0.82 <C> 0.82 <R> <C> + Retrofitting ∗ i-strength <C> 0.56 <C> 0.38 <C> 0.64 <C> 0.80 <C> 0.44 <C> 0.62 <C> 0.80 <C> 0.78 <R> <C> + Retrofitting ⊕ Affect <C> 0.60 <C> 0.46 <C> 0.76 <C> 0.81 <C> [BOLD] 0.61 <C> [BOLD] 0.69 <C> 0.81 <C> [BOLD] 0.85 <R> <C> + Counterfitting <C> 0.58 <C> 0.47 <C> 0.65 <C> 0.80 <C> 0.56 <C> 0.61 <C> 0.78 <C> 0.77 <R> <C> + Counterfitting ⊕ Affect <C> [BOLD] 0.62 <C> [BOLD] 0.53 <C> 0.70 <C> [BOLD] 0.84 <C> [BOLD] 0.61 <C> 0.64 <C> [BOLD] 0.84 <C> 0.80 <R> <C> [BOLD] Word2Vec <C> 0.45 <C> 0.36 <C> 0.70 <C> 0.76 <C> 0.59 <C> 0.67 <C> 0.80 <C> 0.78 <R> <C> ⊕ Affect <C> 0.49 <C> 0.42 <C> 0.67 <C> 0.81 <C> 0.59 <C> 0.66 <C> 0.85 <C> 0.79 <R> <C> + Retrofitting <C> 0.55 <C> 0.45 <C> [BOLD] 0.74 <C> 0.82 <C> [BOLD] 0.62 <C> [BOLD] 0.70 <C> 0.83 <C> 0.80 <R> <C> + Retrofitting ∗ c-strength <C> 0.55 <C> 0.44 <C> 0.73 <C> 0.82 <C> 0.62 <C> [BOLD] 0.70 <C> 0.83 <C> 0.80 <R> <C> + Retrofitting ∗ i-strength <C> 0.58 <C> 0.47 <C> 0.71 <C> 0.83 <C> 0.57 <C> 0.69 <C> 0.85 <C> 0.80 <R> <C> + Retrofitting ⊕ Affect <C> 0.59 <C> 0.49 <C> 0.71 <C> [BOLD] 0.84 <C> [BOLD] 0.62 <C> [BOLD] 0.70 <C> [BOLD] 0.86 <C> [BOLD] 0.82 <R> <C> + Counterfitting <C> 0.56 <C> 0.51 <C> 0.66 <C> 0.75 <C> 0.61 <C> 0.64 <C> 0.75 <C> 0.73 <R> <C> + Counterfitting ⊕ Affect <C> [BOLD] 0.60 <C> [BOLD] 0.54 <C> 0.64 <C> 0.82 <C> 0.60 <C> 0.64 <C> 0.82 <C> 0.76 <R> <C> [BOLD] Paragram <C> 0.69 <C> 0.54 <C> [BOLD] 0.73 <C> 0.78 <C> 0.59 <C> 0.68 <C> 0.80 <C> 0.78 <R> <C> ⊕ Affect <C> 0.71 <C> 0.59 <C> 0.70 <C> 0.77 <C> [BOLD] 0.60 <C> 0.67 <C> 0.76 <C> [BOLD] 0.79 <R> <C> + Retrofitting <C> 0.68 <C> 0.55 <C> [BOLD] 0.73 <C> 0.79 <C> 0.59 <C> 0.68 <C> 0.81 <C> 0.78 <R> <C> + Retrofitting ∗ c-strength <C> 0.69 <C> 0.55 <C> [BOLD] 0.73 <C> 0.79 <C> 0.59 <C> [BOLD] 0.69 <C> 0.81 <C> 0.78 <R> <C> + Retrofitting ∗ i-strength <C> 0.68 <C> 0.56 <C> 0.71 <C> 0.80 <C> 0.58 <C> 0.68 <C> [BOLD] 0.84 <C> 0.77 <R> <C> + Retrofitting ⊕ Affect <C> 0.71 <C> 0.58 <C> 0.70 <C> 0.80 <C> 0.59 <C> 0.67 <C> 0.78 <C> [BOLD] 0.79 <R> <C> + Counterfitting <C> 0.74 <C> 0.63 <C> 0.69 <C> [BOLD] 0.81 <C> [BOLD] 0.60 <C> 0.66 <C> 0.82 <C> 0.74 <R> <C> + Counterfitting ⊕ Affect <C> [BOLD] 0.75 <C> [BOLD] 0.66 <C> 0.68 <C> [BOLD] 0.81 <C> [BOLD] 0.60 <C> 0.65 <C> 0.82 <C> 0.76 <CAP> Table 4: Intrinsic Evaluation: Word Similarity–We report the Spearman’s correlation coefficient (ρ). The results show that Aff2Vec variants improve performance consistently.
<R> <C> [BOLD] Model <C> [BOLD] FFP-Prediction  [BOLD] MSE ( [ITALIC] X10−3) <C> [BOLD] FFP-Prediction  [BOLD] MSE ( [ITALIC] X10−3) <C> [BOLD] FFP-Prediction  [BOLD] MSE ( [ITALIC] X10−3) <C> [BOLD] Personality Detection  [BOLD] Acc. (%) <C> [BOLD] Personality Detection  [BOLD] Acc. (%) <C> [BOLD] Personality Detection  [BOLD] Acc. (%) <C> [BOLD] Personality Detection  [BOLD] Acc. (%) <C> [BOLD] Personality Detection  [BOLD] Acc. (%) <C> [BOLD] SA  [BOLD] Acc. (%) <C> [BOLD] EMO-INT  [BOLD] Pearson’s  [ITALIC] ρ ( [ITALIC] X10−2) <C> [BOLD] EMO-INT  [BOLD] Pearson’s  [ITALIC] ρ ( [ITALIC] X10−2) <C> [BOLD] EMO-INT  [BOLD] Pearson’s  [ITALIC] ρ ( [ITALIC] X10−2) <C> [BOLD] EMO-INT  [BOLD] Pearson’s  [ITALIC] ρ ( [ITALIC] X10−2) <R> <C> [EMPTY] <C> [BOLD] FOR <C> [BOLD] FRU <C> [BOLD] POL <C> [BOLD] EXT <C> [BOLD] NEU <C> [BOLD] AGR <C> [BOLD] CON <C> [BOLD] OPEN <C> [BOLD] DAN <C> [BOLD] ANG <C> [BOLD] FEA <C> [BOLD] JOY <C> [BOLD] SAD <R> <C> [BOLD] GloVe <C> 27.59 <C> 32.40 <C> 21.89 <C> [BOLD] 56.08 <C> 55.25 <C> 56.06 <C> [BOLD] 57.32 <C> 59.14 <C> 83.1 <C> 70.98 <C> 71.19 <C> 65.85 <C> 73.30 <R> <C> ⊕ Affect <C> 27.72 <C> 28.76 <C> 22.02 <C> 51.47 <C> 57.41 <C> 56.09 <C> 55.06 <C> [BOLD] 62.08 <C> 84.3 <C> 70.91 <C> 71.72 <C> 66.26 <C> [BOLD] 73.58 <R> <C> + Retrofitting <C> 27.44 <C> 29.35 <C> 21.75 <C> 55.79 <C> 59.67 <C> 55.59 <C> 56.89 <C> 59.67 <C> 82.7 <C> 72.10 <C> 71.86 <C> [BOLD] 67.11 <C> 73.14 <R> <C> + Retrofitting ⊕ Affect <C> 28.33 <C> [BOLD] 27.91 <C> 22.24 <C> 55.01 <C> 56.43 <C> [BOLD] 57.48 <C> 53.04 <C> 61.12 <C> 83.7 <C> [BOLD] 72.38 <C> [BOLD] 72.53 <C> 66.29 <C> 72.76 <R> <C> + Counterfitting <C> [BOLD] 25.66 <C> 29.20 <C> 22.90 <C> 55.11 <C> 58.32 <C> 55.41 <C> 53.89 <C> 60.36 <C> 84.2 <C> 70.45 <C> 68.95 <C> 65.27 <C> 72.63 <R> <C> + Counterfitting ⊕ Affect <C> 28.89 <C> 32.46 <C> [BOLD] 21.64 <C> 52.12 <C> [BOLD] 60.03 <C> 56.53 <C> 54.93 <C> 59.51 <C> [BOLD] 84.4 <C> 70.20 <C> 70.43 <C> 65.81 <C> 72.37 <R> <C> [BOLD] Word2Vec <C> 25.86 <C> 27.88 <C> 21.56 <C> [BOLD] 56.08 <C> 58.19 <C> 56.59 <C> 55.18 <C> 61.41 <C> 83.3 <C> 68.86 <C> 71.24 <C> 65.23 <C> 72.60 <R> <C> ⊕ Affect <C> 25.39 <C> 28.16 <C> 22.99 <C> 53.54 <C> 57.97 <C> 55.17 <C> 54.12 <C> 59.31 <C> 83.4 <C> 69.29 <C> [BOLD] 71.92 <C> 64.49 <C> [BOLD] 72.63 <R> <C> + Retrofitting <C> 27.81 <C> 29.05 <C> 21.85 <C> 54.33 <C> 56.65 <C> [BOLD] 57.39 <C> 54.65 <C> 60.03 <C> 82.5 <C> 70.12 <C> 71.42 <C> [BOLD] 67.96 <C> 72.02 <R> <C> + Retrofitting ⊕ Affect <C> [BOLD] 25.08 <C> [BOLD] 27.08 <C> 21.64 <C> 53.74 <C> [BOLD] 59.61 <C> 56.34 <C> [BOLD] 56.93 <C> 59.7 <C> 83.3 <C> [BOLD] 70.65 <C> 71.90 <C> 66.36 <C> 72.20 <R> <C> + Counterfitting <C> 28.28 <C> 27.12 <C> 22.95 <C> 54.55 <C> 57.61 <C> 57.09 <C> 54.1 <C> 58.5 <C> 83.3 <C> 68.64 <C> 70.13 <C> 63.36 <C> 70.67 <R> <C> + Counterfitting ⊕ Affect <C> 27.73 <C> 29.67 <C> [BOLD] 21.52 <C> 51.28 <C> 58.86 <C> 56.66 <C> 53.22 <C> [BOLD] 61.62 <C> [BOLD] 83.5 <C> 69.38 <C> 70.31 <C> 64.94 <C> 71.37 <R> <C> [BOLD] Baselines <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C>  <C> – <C> – <C> – <C> [BOLD] 58.09 <C> 59.38 <C> 56.71 <C> 57.30 <C> [BOLD] 62.68 <C> – <C> – <C> – <C> – <C> – <R> <C> ENRON Trainable <C> 31.61 <C> 43.90 <C> 26.27 <C> – <C> – <C> – <C> – <C> – <C> – <C> – <C> – <C> – <C> – <R> <C> Re(Glove) <C> – <C> – <C> – <C> – <C> – <C> – <C> – <C> – <C> 82.2 <C> – <C> – <C> – <C> – <R> <C> Re(w2v) <C> – <C> – <C> – <C> – <C> – <C> – <C> – <C> – <C> 82.4 <C> – <C> – <C> – <C> – <CAP> Table 5: Extrinsic Evaluation: Results for FFP–Prediction, Personality Detection, Sentiment Analysis and WASSA Emotional Intensity task for Aff2Vec variants for GloVe and Word2Vec embeddings. We report the Mean Squared Error (MSE) for FFP–Prediction, Accuracy (% ACC) for Personality Detection and Sentiment Analysis (SA) and Person’s ρ for the WASSA Emo-Int Task (EMO-INT)
<R> <C> [BOLD] Model <C> [BOLD] PN@10 (%)  [BOLD] V <C> [BOLD] PN@10 (%)  [BOLD] A <C> [BOLD] PN@10 (%)  [BOLD] D <C> [BOLD] GN@10 ( [ITALIC] X10−2)  [BOLD] V <C> [BOLD] GN@10 ( [ITALIC] X10−2)  [BOLD] A <C> [BOLD] GN@10 ( [ITALIC] X10−2)  [BOLD] D <R> <C> [BOLD] GloVe <C> 23.21 <C> 22.15 <C> 27.07 <C> 83.91 <C> 79.19 <C> 74.19 <R> <C> ⊕ Affect <C> [BOLD] 16.46 <C> 19.65 <C> [BOLD] 19.42 <C> [BOLD] 72.56 <C> [BOLD] 69.00 <C> 64.02 <R> <C> + Retrofitting <C> 22.55 <C> 21.82 <C> 26.5 <C> 82.15 <C> 78.68 <C> 72.53 <R> <C> + Retrofitting ∗ c-strength <C> 22.07 <C> 21.63 <C> 26.14 <C> 80.85 <C> 78.12 <C> 71.86 <R> <C> + Retrofitting ∗ i-strength <C> 23.05 <C> 21.77 <C> 26.66 <C> 83.14 <C> 78.76 <C> 72.65 <R> <C> + Retrofitting ⊕ Affect <C> 19.68 <C> [BOLD] 18.16 <C> 22.88 <C> 73.45 <C> 71.56 <C> 66.55 <R> <C> + Counterfitting <C> 22.68 <C> 22.2 <C> 26.46 <C> 83.31 <C> 78.78 <C> 72.54 <R> <C> + Counterfitting ⊕ Affect <C> 16.75 <C> 19.99 <C> 19.99 <C> 73.89 <C> 69.55 <C> [BOLD] 63.93 <R> <C> [BOLD] Word2Vec <C> 24.66 <C> 22.19 <C> 27.41 <C> 85.81 <C> 79.23 <C> 74.25 <R> <C> ⊕ Affect <C> 20.62 <C> [BOLD] 17.83 <C> 23.19 <C> [BOLD] 74.78 <C> [BOLD] 71.64 <C> 67.32 <R> <C> + Retrofitting <C> 23.75 <C> 22.25 <C> 26.94 <C> 84.65 <C> 79.36 <C> 73.00 <R> <C> + Retrofitting ∗ c-strength <C> 23.33 <C> 22.01 <C> 26.58 <C> 83.39 <C> 78.71 <C> 72.24 <R> <C> + Retrofitting ∗ i-strength <C> 23.90 <C> 22.30 <C> 27.13 <C> 85.34 <C> 79.46 <C> 73.12 <R> <C> + Retrofitting ⊕ Affect <C> 20.61 <C> 18.54 <C> 23.6 <C> 75.71 <C> 72.47 <C> 67.61 <R> <C> + Counterfitting <C> 23.47 <C> 22.48 <C> 26.72 <C> 84.62 <C> 79.14 <C> 72.29 <R> <C> + Counterfitting ⊕ Affect <C> [BOLD] 20.34 <C> 18.17 <C> [BOLD] 23.01 <C> 74.83 <C> 71.94 <C> [BOLD] 66.62 <R> <C> [BOLD] Paragram <C> 25.16 <C> 22.55 <C> 28.05 <C> 88.34 <C> 80.73 <C> 75.49 <R> <C> ⊕ Affect <C> 20.81 <C> 21.29 <C> 23.45 <C> 81.83 <C> 75.27 <C> 69.79 <R> <C> + Retrofitting <C> 25.69 <C> 22.8 <C> 28.48 <C> 89.67 <C> 81.25 <C> 76.05 <R> <C> + Retrofitting ∗ c-strength <C> 25.46 <C> 22.64 <C> 28.22 <C> 89.06 <C> 80.95 <C> 75.58 <R> <C> + Retrofitting ∗ i-strength <C> 25.69 <C> 22.84 <C> 28.43 <C> 89.85 <C> 81.26 <C> 75.93 <R> <C> + Retrofitting ⊕ Affect <C> 23.38 <C> [BOLD] 20.34 <C> 25.99 <C> 83.17 <C> 76.51 <C> 71.83 <R> <C> + Counterfitting <C> 24.86 <C> 22.76 <C> 27.88 <C> 88.27 <C> 80.68 <C> 75.18 <R> <C> + Counterfitting ⊕ Affect <C> [BOLD] 20.31 <C> 21.5 <C> [BOLD] 23.03 <C> [BOLD] 81.40 <C> [BOLD] 75.05 <C> [BOLD] 69.10 <CAP> Table 6: Polarity-Noise@k (PN@10) and Granularity-Noise@k (GN@10) where k=10 for GloVe and Word2Vec variants. Note that lower the number, better this qualitative metric.
<R> <C> GoldPredicted <C> man-made <C> organic <C> animal <C> Pref. <C> No Pref. <C> Total <R> <C> man-made <C> 128 <C> 9 <C> 5 <C> 142 <C> 124 <C> 266 <R> <C> organic <C> 0 <C> 48 <C> 1 <C> 49 <C> 19 <C> 68 <R> <C> animal <C> 9 <C> 14 <C> 33 <C> 56 <C> 72 <C> 128 <CAP> Table 2: Confusion matrix for experiment 3: rows report gold categories, columns report subjects’ responses (the first 3 columns count cases with significant preferences for one macro-category only).
<R> <C> [EMPTY] <C> a1–a2 <C> a1–a3 <C> a2–a3 <R> <C> Inst. labelled <C> 24 <C> 19 <C> 24 <R> <C> Inst. unlab. <C> 33 <C> 27 <C> 29 <R> <C> Graph labelled <C> 66 <C> 69 <C> 66 <R> <C> Graph unlabelled <C> 90 <C> 93 <C> 92 <CAP> Table 1: F1 scores in % for agreement between annotators on different levels. a1, a2, and a3 are different annotators.
<R> <C> [BOLD] Locale <C> [BOLD] Model <C> [BOLD] Overall <C> [BOLD] Locale-specific <C> [BOLD] Locale-independent <C> [BOLD] Single-locale <C> [BOLD] Small <R> <C> US <C> single <C> 70.21 <C> 54.39 <C> 69.90 <C> – <C> 8.18 <R> <C> US <C> union <C> 70.21 <C> 54.39 <C> 69.90 <C> – <C> 8.18 <R> <C> US <C> constrained <C> 74.25 <C> 76.08 <C> 74.02 <C> – <C> 38.30 <R> <C> US <C> universal <C> [BOLD] 82.64 <C> 88.20 <C> [BOLD] 81.92 <C> – <C> [BOLD] 61.79 <R> <C> US <C> universal + adv <C> 11.13 <C> [BOLD] 97.51 <C> 0.00 <C> – <C> 5.38 <R> <C> GB <C> single <C> 56.02 <C> 62.81 <C> 55.09 <C> 37.81 <C> 0.00 <R> <C> GB <C> union <C> 66.61 <C> 78.74 <C> 64.96 <C> 48.19 <C> 36.54 <R> <C> GB <C> constrained <C> 67.82 <C> 76.83 <C> 66.60 <C> 50.51 <C> 38.04 <R> <C> GB <C> universal <C> 80.06 <C> [BOLD] 88.37 <C> 78.93 <C> [BOLD] 83.60 <C> 57.96 <R> <C> GB <C> universal + adv <C> [BOLD] 80.52 <C> 85.88 <C> [BOLD] 79.79 <C> 82.22 <C> [BOLD] 59.52 <R> <C> CA <C> single <C> 43.43 <C> 3.57 <C> 43.68 <C> 0.00 <C> 0.24 <R> <C> CA <C> union <C> 61.04 <C> 10.71 <C> 61.35 <C> 0.65 <C> 30.78 <R> <C> CA <C> constrained <C> 76.46 <C> 67.85 <C> 76.51 <C> 39.17 <C> 55.66 <R> <C> CA <C> universal <C> [BOLD] 94.00 <C> [BOLD] 75.00 <C> [BOLD] 94.12 <C> 97.74 <C> [BOLD] 77.09 <R> <C> CA <C> universal + adv <C> 35.21 <C> 71.42 <C> 34.98 <C> [BOLD] 98.87 <C> 36.69 <R> <C> IN <C> single <C> 56.25 <C> 0.00 <C> 60.46 <C> 0.00 <C> 0.00 <R> <C> IN <C> union <C> 45.93 <C> 0.00 <C> 49.38 <C> 0.00 <C> 17.96 <R> <C> IN <C> constrained <C> 62.64 <C> 44.71 <C> 63.98 <C> 25.94 <C> 58.64 <R> <C> IN <C> universal <C> [BOLD] 88.09 <C> [BOLD] 87.01 <C> [BOLD] 88.17 <C> 80.00 <C> [BOLD] 68.47 <R> <C> IN <C> universal + adv <C> 22.30 <C> [BOLD] 87.01 <C> 17.46 <C> [BOLD] 82.97 <C> 10.50 <CAP> Table 4: Domain classification accuracy over different domain categories and different locales.
<R> <C> [EMPTY] <C> De-En bleu <C> De-En meteor <C> De-En rep <C> De-En drop <C> Ja-En bleu <C> Ja-En meteor <C> Ja-En rep <C> Ja-En drop <C> Ro-En bleu <C> Ro-En meteor <C> Ro-En rep <C> Ro-En drop <R> <C> softmax <C> 29.51 <C> 31.43 <C> 3.37 <C> 5.89 <C> 20.36 <C> 23.83 <C> 13.48 <C> 23.30 <C> 29.67 <C> 32.05 <C> 2.45 <C> 5.59 <R> <C> softmax + CovPenalty <C> 29.69 <C> 31.53 <C> 3.47 <C> 5.74 <C> 20.70 <C> 24.12 <C> 14.12 <C> 22.79 <C> 29.81 <C> 32.15 <C> 2.48 <C> 5.49 <R> <C> softmax + CovVector <C> 29.63 <C> 31.54 <C> 2.93 <C> 5.65 <C> 21.53 <C> 24.50 <C> 11.07 <C> 22.18 <C> [BOLD] 30.08 <C> [BOLD] 32.22 <C> 2.42 <C> 5.47 <R> <C> sparsemax <C> 29.73 <C> 31.54 <C> 3.18 <C> 5.90 <C> 21.28 <C> 24.25 <C> 13.09 <C> 22.40 <C> 29.97 <C> 32.12 <C> 2.19 <C> 5.60 <R> <C> sparsemax + CovPenalty <C> 29.83 <C> 31.60 <C> 3.24 <C> 5.79 <C> [BOLD] 21.64 <C> 24.49 <C> 13.36 <C> 21.91 <C> 30.07 <C> 32.20 <C> 2.20 <C> 5.47 <R> <C> sparsemax + CovVector <C> 29.22 <C> 31.18 <C> 3.13 <C> 6.15 <C> 21.35 <C> [BOLD] 24.74 <C> [BOLD] 10.11 <C> [BOLD] 21.25 <C> 29.30 <C> 31.84 <C> 2.18 <C> 5.87 <R> <C> csoftmax ( [ITALIC] c=0.2) <C> 29.39 <C> 31.33 <C> 3.29 <C> 5.86 <C> 20.71 <C> 24.00 <C> 12.38 <C> 22.73 <C> 29.39 <C> 31.83 <C> 2.37 <C> 5.64 <R> <C> csparsemax ( [ITALIC] c=0.2) <C> [BOLD] 29.85 <C> [BOLD] 31.76 <C> [BOLD] 2.67 <C> [BOLD] 5.23 <C> 21.31 <C> 24.51 <C> 11.40 <C> 21.59 <C> 29.77 <C> 32.10 <C> [BOLD] 1.98 <C> [BOLD] 5.44 <CAP> Table 1: BLEU, METEOR, REP and DROP scores on the test sets for different attention transformations.
<R> <C> [EMPTY] <C> bleu <C> meteor <R> <C> constant,  [ITALIC] f=2 <C> 29.66 <C> 31.60 <R> <C> constant,  [ITALIC] f=3 <C> 29.64 <C> 31.56 <R> <C> guided, <C> 29.56 <C> 31.45 <R> <C> predicted,  [ITALIC] c=0 <C> 29.78 <C> 31.60 <R> <C> predicted,  [ITALIC] c=0.2 <C> [BOLD] 29.85 <C> [BOLD] 31.76 <CAP> Table 2: Impact of various fertility strategies for the csparsemax attention model (De-En).
<R> <C> [BOLD] Models <C> [BOLD] Accuracy <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1 Score <R> <C> CNN model-A <C> [BOLD] 82.31% <C> 0.893 <C> 0.784 <C> [BOLD] 0.835 <R> <C> CNN model-B <C> 76.35% <C> 0.597 <C> 0.906 <C> 0.719 <R> <C> Decision Tree <C> 63.40% <C> 0.925 <C> 0.584 <C> 0.716 <R> <C> SVM <C> 59.33% <C> 0.220 <C> [BOLD] 0.943 <C> 0.356 <R> <C> XGBoost <C> 54.90% <C> 0.1463 <C> 0.8478 <C> 0.2469 <R> <C> Logistic-1 <C> 57.44% <C> 0.873 <C> 0.546 <C> 0.672 <R> <C> Logistic-2 <C> 54.56% <C> [BOLD] 0.954 <C> 0.525 <C> 0.677 <CAP> Table 5: Metrics for the Different Machine Learning Models
<R> <C> [BOLD] German-English System <C> [BOLD] German-English test11 <C> [BOLD] German-English test12 <C> [BOLD] German-English test13 <C> [BOLD] German-English Avg. <R> <C> Baseline <C> 35.0 <C> 30.3 <C> 27.1 <C> 30.8 <R> <C> OSM [ITALIC] pos <C> 35.3 <C> 30.5 <C> 27.1 <C> 31.0 <R> <C> OSM [ITALIC] mkcls <C> 35.1 <C> 30.1 <C> 26.8 <C> 30.7 <R> <C> OSM [ITALIC] neural <C> 35.8 <C> 31.5 <C> 27.0 <C> 31.4 <R> <C> Lex.reo [ITALIC] neural <C> 35.5 <C> 31.1 <C> 27.2 <C> 31.3 <R> <C> Lex [ITALIC] neural <C> 35.3 <C> 30.8 <C> 26.9 <C> 31.0 <R> <C> [BOLD] English-German <C> [BOLD] English-German <C> [BOLD] English-German <C> [BOLD] English-German <C> [BOLD] English-German <R> <C> Baseline <C> 25.7 <C> 21.7 <C> 23.4 <C> 23.6 <R> <C> OSM [ITALIC] pos <C> 25.9 <C> 21.9 <C> 23.8 <C> 23.9 <R> <C> OSM [ITALIC] mkcls <C> 25.8 <C> 21.8 <C> 23.4 <C> 23.7 <R> <C> OSM [ITALIC] neural <C> 26.1 <C> 22.1 <C> 24.2 <C> 24.1 <R> <C> Lex.reo [ITALIC] neural <C> 26.1 <C> 22.4 <C> 23.7 <C> 24.1 <R> <C> Lex [ITALIC] neural <C> 26.0 <C> 22.2 <C> 23.7 <C> 24.0 <CAP> Table 2: Comparing performance of Neural Reordering Models against N-gram-based Models. Quality measured in cased-bleu [35]
<R> <C> [EMPTY] <C> Method <C> AAE fraction in summary <C> ROUGE-1 Recall <C> ROUGE-1 Precision <C> ROUGE-1 F-score <C> ROUGE-L Recall <C> ROUGE-L Precision <C> ROUGE-L F-score <R> <C> Keyword: “funny” <C> Collection containing keyword <C> 0.11 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> 2-9 <C> TF-IDF <C> 0.04 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> [EMPTY] <C> TF-IDF-balanced <C> 0.10 <C> 0.76 <C> 0.79 <C> 0.78 <C> 0.77 <C> 0.73 <C> 0.75 <R> <C> 2-9 <C> Hybrid-TF-IDF <C> 0.04 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> [EMPTY] <C> Hybrid-TF-IDF-balanced <C> 0.02 <C> 0.89 <C> 0.39 <C> 0.54 <C> 0.78 <C> 0.21 <C> 0.33 <R> <C> 2-9 <C> LexRank <C> 0.04 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> [EMPTY] <C> LexRank-balanced <C> 0.22 <C> 0.53 <C> 0.55 <C> 0.54 <C> 0.41 <C> 0.36 <C> 0.38 <R> <C> 2-9 <C> TextRank <C> 0.06 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> [EMPTY] <C> TextRank-balanced <C> 0.04 <C> 0.94 <C> 0.23 <C> 0.43 <C> 0.92 <C> 0.15 <C> 0.25 <R> <C> 2-9 <C> SummRuNNer <C> 0.03 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> [EMPTY] <C> SummRuNNer-balanced <C> 0.10 <C> 0.61 <C> 0.51 <C> 0.56 <C> 0.45 <C> 0.41 <C> 0.43 <R> <C> 2-9 <C> Centroid-Word2Vec <C> 0.02 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> [EMPTY] <C> Centroid-Word2Vec-balanced <C> 0.10 <C> 0.68 <C> 0.67 <C> 0.67 <C> 0.57 <C> 0.49 <C> 0.53 <R> <C> Keyword: “twitter” <C> Collection containing keyword <C> 0.10 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> 2-9 <C> TF-IDF <C> 0.10 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> [EMPTY] <C> TF-IDF-balanced <C> 0.16 <C> 0.72 <C> 0.76 <C> 0.74 <C> 0.71 <C> 0.69 <C> 0.70 <R> <C> 2-9 <C> Hybrid-TF-IDF <C> 0.11 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> [EMPTY] <C> Hybrid-TF-IDF-balanced <C> 0.08 <C> 0.85 <C> 0.46 <C> 0.59 <C> 0.69 <C> 0.34 <C> 0.45 <R> <C> 2-9 <C> LexRank <C> 0.04 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> [EMPTY] <C> LexRank-balanced <C> 0.22 <C> 0.49 <C> 0.53 <C> 0.51 <C> 0.33 <C> 0.28 <C> 0.30 <R> <C> 2-9 <C> TextRank <C> 0.08 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> [EMPTY] <C> TextRank-balanced <C> 0.12 <C> 0.96 <C> 0.63 <C> 0.76 <C> 0.93 <C> 0.61 <C> 0.73 <R> <C> 2-9 <C> SummRuNNer <C> 0.14 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> [EMPTY] <C> SummRuNNer-balanced <C> 0.26 <C> 0.57 <C> 0.53 <C> 0.55 <C> 0.42 <C> 0.39 <C> 0.40 <R> <C> 2-9 <C> Centroid-Word2Vec <C> 0.06 <C> - <C> - <C> - <C> - <C> - <C> - <R> <C> [EMPTY] <C> Centroid-Word2Vec-balanced <C> 0.12 <C> 0.64 <C> 0.66 <C> 0.65 <C> 0.51 <C> 0.44 <C> 0.47 <CAP> Table 1: TwitterAAE Evaluation 2. The performance of our model for keywords “twitter” and “funny”, using different blackbox algorithms. The size of generated summary is 50. The ROUGE scores are computed for summaries generated by our model A-balanced against summaries generated by A.
<R> <C> [EMPTY] <C> Intent Prec <C> Intent Rec <C> Intent F1 <C> Entity Prec <C> Entity Rec <C> Entity F1 <R> <C> Rasa <C> 0.863 <C> 0.863 <C> 0.863 <C> 0.859 <C> 0.694 <C> 0.768 <R> <C> Dialogflow <C> 0.870 <C> 0.859 <C> 0.864 <C> 0.782 <C> 0.709 <C> 0.743 <R> <C> LUIS <C> 0.855 <C> 0.855 <C> 0.855 <C> 0.837 <C> 0.725 <C> [BOLD] 0.777 <R> <C> Watson <C> 0.884 <C> 0.881 <C> [BOLD] 0.882 <C> 0.354 <C> 0.787 <C> [BOLD] 0.488 <CAP> Table 2: Overall Scores for Intent and Entity
<R> <C> [EMPTY] <C> Prec <C> Rec <C> F1 <R> <C> Rasa <C> 0.862 <C> 0.787 <C> 0.822 <R> <C> Dialogflow <C> 0.832 <C> 0.791 <C> 0.811 <R> <C> LUIS <C> 0.848 <C> 0.796 <C> 0.821 <R> <C> Watson <C> 0.540 <C> 0.838 <C> [BOLD] 0.657 <CAP> Table 2: Overall Scores for Intent and Entity
<R> <C> FB-DB <C> Concat <C> MRR 2.1 <C> HITS@1 1.7 <C> HITS@10 2.7 <R> <C> FB-DB <C> Ensemble <C> 40.1 <C> 34.3 <C> 50.2 <R> <C> FB-DB <C> PoE- [ITALIC] lrni <C> [BOLD] 72.1 <C> [BOLD] 66.6 <C> [BOLD] 82.0 <R> <C> FB-Yago <C> Concat <C> 0.18 <C> 0.18 <C> 0.04 <R> <C> FB-Yago <C> Ensemble <C> 47.6 <C> 42.3 <C> 57.5 <R> <C> FB-Yago <C> PoE- [ITALIC] lrni <C> [BOLD] 63.5 <C> [BOLD] 57.3 <C> [BOLD] 74.6 <CAP> Table 6: Performance comparison for P=80%.
<R> <C> [BOLD] Method <C> [BOLD] 2012 Dataset  [BOLD] MAP <C> [BOLD] 2012 Dataset  [BOLD] P5 <C> [BOLD] 2012 Dataset  [BOLD] P10 <C> [BOLD] 2008 Dataset  [BOLD] MAP <C> [BOLD] 2008 Dataset  [BOLD] P5 <C> [BOLD] 2008 Dataset  [BOLD] P10 <R> <C> English Monolingual <C> 0.3218 <C> 0.56 <C> 0.522 <C> 0.1609 <C> 0.248 <C> 0.236 <R> <C> Dictionary <C> 0.1691 <C> 0.2048 <C> 0.2048 <C> 0.084 <C> 0.1464 <C> 0.137 <R> <C> Chinnakotla et.al [clef:07] <C> 0.2236 <C> 0.3347 <C> 0.3388 <C> 0.11 <C> 0.15 <C> 0.147 <R> <C> Google Translate <C> 0.3566 <C> 0.576 <C> 0.522 <C> 0.178 <C> 0.255 <C> 0.24 <CAP> Table 3: Performance Results for the Baseline approaches
<R> <C> [BOLD] Method <C> [BOLD] # Translations <C> [BOLD] 2012 Dataset  [BOLD] MAP <C> [BOLD] 2012 Dataset  [BOLD] P5 <C> [BOLD] 2012 Dataset  [BOLD] P10 <C> [BOLD] 2008 Dataset  [BOLD] MAP <C> [BOLD] 2008 Dataset  [BOLD] P5 <C> [BOLD] 2008 Dataset  [BOLD] P10 <R> <C> WE <C> 1word <C> 0.2533 <C> [BOLD] 0.3920 <C> [BOLD] 0.3840 <C> 0.1284 <C> [BOLD] 0.175 <C> [BOLD] 0.163 <R> <C> WE <C> 2words <C> [BOLD] 0.2568 <C> 0.3840 <C> 0.3720 <C> [BOLD] 0.129 <C> 0.167 <C> 0.154 <R> <C> WE <C> 3words <C> 0.2379 <C> 0.384 <C> 0.3520 <C> 0.127 <C> 0.166 <C> 0.152 <R> <C> [EMPTY] <C> 5words <C> 0.2053 <C> 0.328 <C> 0.32 <C> 0.119 <C> 0.145 <C> 0.143 <R> <C> WE weighted <C> 3words <C> 0.2802 <C> [BOLD] 0.436 <C> 0.392 <C> 0.138 <C> 0.191 <C> 0.187 <R> <C> WE weighted <C> 5words <C> [BOLD] 0.2808 <C> 0.408 <C> [BOLD] 0.408 <C> [BOLD] 0.14 <C> [BOLD] 0.218 <C> [BOLD] 0.209 <R> <C> WE weighted <C> 7words <C> 0.2804 <C> 0.428 <C> 0.402 <C> 0.136 <C> 0.21 <C> 0.196 <R> <C> SIM Vec <C> Sum - 15words <C> 0.2508 <C> 0.364 <C> 0.362 <C> 0.1276 <C> [BOLD] 0.2137 <C> [BOLD] 0.1968 <R> <C> SIM Vec <C> Sum - 20words <C> [BOLD] 0.2562 <C> [BOLD] 0.368 <C> [BOLD] 0.368 <C> [BOLD] 0.1282 <C> 0.2108 <C> 0.196 <R> <C> SIM Vec <C> Sum - 25words <C> 0.2493 <C> 0.359 <C> 0.343 <C> 0.1268 <C> 0.187 <C> 0.1823 <R> <C> SIM Vec <C> Max - 10words <C> 0.2733 <C> 0.4120 <C> 0.382 <C> 0.138 <C> 0.23 <C> 0.225 <R> <C> SIM Vec <C> Max - 15words <C> [BOLD] 0.2835 <C> 0.408 <C> 0.4 <C> [BOLD] 0.144 <C> 0.2416 <C> 0.237 <R> <C> SIM Vec <C> Max - 20words <C> 0.2830 <C> 0.4120 <C> 0.392 <C> 0.14 <C> [BOLD] 0.2471 <C> 0.238 <R> <C> SIM Vec <C> Max - 25words <C> 0.2812 <C> [BOLD] 0.424 <C> [BOLD] 0.394 <C> 0.137 <C> 0.24 <C> [BOLD] 0.24 <CAP> Table 4: Performance Results when Queries are translated using proposed Word Embedding based methods: for WE and WE weighted, # Translations per query term are shown, while for SIM Vec, # Translations for the complete query are shown.
<R> <C> [BOLD] Query in Hindi <C> [BOLD] Translation in English <C> [BOLD] Translation Method <C> [BOLD] Translations <C> [BOLD] MAP <C> [BOLD] P5 <C> [BOLD] P10 <R> <C> shriilaMkaaii raaShTriiya krikeTa Tiima para hamalaa <C> Sri Lankan national cricket team attack <C> Sum <C> Sri^1 Lankan^1 cricket^0.34 team^0.34 sport^0.32 <C> 0.3738 <C> 0.6 <C> 0.6 <R> <C> shriilaMkaaii raaShTriiya krikeTa Tiima para hamalaa <C> Sri Lankan national cricket team attack <C> Max <C> Sri^1 Lankan^1 team^0.35 assault^0.33 attack^0.32 <C> 0.51 <C> 0.8 <C> 0.9 <R> <C> iraaka kaa prathama chunaava <C> Iraq‘s first election <C> Sum <C> Iraqi^1 choice^0.37 unfashionable^0.32 predictable^0.31 <C> 0.08 <C> 0 <C> 0 <R> <C> iraaka kaa prathama chunaava <C> Iraq‘s first election <C> Max <C> Iraqi^1 elections ^0.334 first^0.332 election^0.33 <C> 0.4 <C> 0.8 <C> 0.6 <R> <C> miga durghaTanaa pashchima baMgaala <C> MiG crash in West Bengal <C> Sum <C> MiG^1 West ^1 Bengal ^1 oriental^0.34 venomous ^0.33 exotic ^0.33 <C> 0.18 <C> 0.2 <C> 0.2 <R> <C> miga durghaTanaa pashchima baMgaala <C> MiG crash in West Bengal <C> Max <C> MiG ^1 West ^1 Bengal ^1 accident ^0.36 mishap ^0.33 crash ^0.31 <C> 0.4 <C> 0.8 <C> 0.5 <CAP> Table 6: Example queries to illustrate the ‘Max’ and ‘Sum’ functions for SIM Vec
<R> <C> [BOLD] Query in Hindi <C> [BOLD] Translation in English <C> [BOLD] Translation Method <C> [BOLD] Translations <C> [BOLD] MAP <C> [BOLD] P5 <C> [BOLD] P10 <R> <C> gorakhaalaiMDa kii maaMga <C> Demand of Gorkhaland <C> DT <C> Gorkhaland <C> 0.197 <C> 0.2 <C> 0.4 <R> <C> gorakhaalaiMDa kii maaMga <C> Demand of Gorkhaland <C> WE + DT Weighted <C> Gorkhaland^1 demand^0.51 demands^0.49 <C> 0.88 <C> 1 <C> 1 <R> <C> abhiyukta ajamala kasaaba <C> Accused Ajmal Kasab <C> DT <C> Ajmal Kasab accused <C> 0.32 <C> 0.2 <C> 0.2 <R> <C> abhiyukta ajamala kasaaba <C> Accused Ajmal Kasab <C> WE + DT Weighted <C> Ajmal^1 Kasab^1 murder^0.26 criminal^0.25 murderer^0.25 complainant^0.24 accusedˆ0.2 <C> 0.66 <C> 0.8 <C> 0.8 <R> <C> 2003 aashiyaana kapa vijetaa <C> 2003 ASEAN Cup winner <C> DT <C> 2003 ASEAN cup champion victor <C> 0.24 <C> 0.4 <C> 0.3 <R> <C> 2003 aashiyaana kapa vijetaa <C> 2003 ASEAN Cup winner <C> WE + DT Weighted <C> 2003^1 ASEAN^1 tournament^0.8 cup^0.2 winners^0.52 winner^0.48 championship^0.1 victor^0.1 <C> 0.4 <C> 0.6 <C> 0.5 <CAP> Table 8: Example queries to illustrate the hybrid model with word Embeddings and Dictionary
<R> <C> [BOLD] Snippet <C> [BOLD] Model <C> [BOLD] Macro  [BOLD] P <C> [BOLD] Macro  [BOLD] R <C> [BOLD] Macro  [BOLD] F <C> [BOLD] Weighted  [BOLD] P <C> [BOLD] Weighted  [BOLD] R <C> [BOLD] Weighted  [BOLD] F <R> <C> - <C> Base <C> 0.1 <C> 1.2 <C> 0.2 <C> 1.3 <C> 11.5 <C> 2.4 <R> <C> 32 <C> mlr <C> 18.7 <C> 11.6 <C> 13.1 <C> 28.9 <C> 31.4 <C> 28.3 <R> <C> 32 <C> mlp <C> 19.1 <C> 9.8 <C> 10.3 <C> [BOLD] 31.7 <C> 32.1 <C> 28.0 <R> <C> 32 <C> lstm <C> 13.7 <C> 9.7 <C> 9.5 <C> 29.1 <C> 32.2 <C> 28.6 <R> <C> 32 <C> cnn <C> 9.9 <C> 7.8 <C> 7.3 <C> 24.6 <C> 29.2 <C> 24.7 <R> <C> 64 <C> mlr <C> [BOLD] 20.6 <C> 12.3 <C> 13.9 <C> 29.9 <C> 32.1 <C> 29.0 <R> <C> 64 <C> mlp <C> 17.9 <C> 9.5 <C> 9.8 <C> 31.2 <C> 32.7 <C> 27.9 <R> <C> 64 <C> lstm <C> 13.3 <C> 10.3 <C> 10.2 <C> 30.3 <C> 33.9 <C> 30.4 <R> <C> [EMPTY] <C> cnn <C> 9.8 <C> 7.8 <C> 7.4 <C> 25.0 <C> 29.9 <C> 25.4 <R> <C> 96 <C> mlr <C> 20.4 <C> [BOLD] 13.3 <C> [BOLD] 14.6 <C> 30.3 <C> 32.0 <C> 29.3 <R> <C> 96 <C> mlp <C> 16.9 <C> 9.5 <C> 9.8 <C> 30.2 <C> 32.6 <C> 27.8 <R> <C> 96 <C> lstm <C> 14.0 <C> 10.5 <C> 10.3 <C> 30.6 <C> 34.5 <C> 30.7 <R> <C> 96 <C> cnn <C> 10.2 <C> 7.1 <C> 6.9 <C> 25.2 <C> 29.4 <C> 24.4 <R> <C> 128 <C> mlr <C> 19.6 <C> 12.1 <C> 12.9 <C> 30.0 <C> 31.7 <C> 28.2 <R> <C> 128 <C> mlp <C> 18.9 <C> 9.9 <C> 10.3 <C> 31.4 <C> 32.9 <C> 28.0 <R> <C> 128 <C> lstm <C> 14.4 <C> 10.5 <C> 10.5 <C> 31.3 <C> [BOLD] 35.1 <C> [BOLD] 31.1 <R> <C> 128 <C> cnn <C> 8.8 <C> 7.8 <C> 7.1 <C> 24.8 <C> 30.2 <C> 25.0 <CAP> Table 3: Macro and weighted F-scores over 5 runs.
<R> <C> [ITALIC] k <C> min words <C> mean words <C> max words <R> <C> 1 <C> 9 <C> 103.6 <C> 4771 <R> <C> 2 <C> 30 <C> 207.3 <C> 4825 <R> <C> 3 <C> 53 <C> 311.6 <C> 4926 <R> <C> 4 <C> 81 <C> 415.5 <C> 5411 <R> <C> 5 <C> 107 <C> 516.8 <C> 6106 <R> <C> 6 <C> 136 <C> 620.1 <C> 6822 <R> <C> 7 <C> 164 <C> 722.0 <C> 6490 <R> <C> 8 <C> 219 <C> 824.3 <C> 6939 <R> <C> 9 <C> 227 <C> 927.8 <C> 7644 <R> <C> 10 <C> 259 <C> 1031.6 <C> 8302 <R> <C> 11 <C> 297 <C> 1133.9 <C> 8180 <R> <C> 12 <C> 315 <C> 1235.8 <C> 9015 <R> <C> 13 <C> 374 <C> 1338.4 <C> 8605 <R> <C> 14 <C> 371 <C> 1440.2 <C> 8862 <R> <C> 15 <C> 422 <C> 1542.5 <C> 10014 <R> <C> 16 <C> 407 <C> 1646.6 <C> 9455 <R> <C> 17 <C> 475 <C> 1749.9 <C> 9921 <R> <C> 18 <C> 462 <C> 1853.8 <C> 10577 <R> <C> 19 <C> 530 <C> 1956.8 <C> 10950 <R> <C> 20 <C> 551 <C> 2059.6 <C> 11263 <CAP> Table 3: Document lengths for the TripAdvisor benchmarks
<R> <C> Noising scheme <C> Validation <C> Test <R> <C> none <C> 94.3 <C> 123.6 <R> <C> blank <C> 85.0 <C> 110.7 <R> <C> unigram <C> 85.2 <C> 111.3 <R> <C> bigram Kneser-Ney <C> 84.5 <C> 110.6 <CAP> Table 3: Perplexity on Text8 with different noising schemes.
<R> <C> Inputs <C> #Hidden units=256 <C> #Hidden units=512 <C> #Hidden units=1024 <R> <C> W <C> 24.50 <C> 23.63 <C> 23.29 <R> <C> W + PST <C> 24.38 <C> 23.03 <C> 22.59 <CAP> Table 5: Test Set Perplexity for sentence topic prediction using Thought vector (W=Word, PST=PrevSentThought)
<R> <C> CER <C> dev_android <C> dev_ios <C> dev_mic <C> test_android <C> test_ios <C> test_mic <C> Training time in hours <R> <C> Mono <C> 47.08 <C> 43.37 <C> 47.33 <C> 45.40 <C> 44.81 <C> 44.28 <C> 0.5 <R> <C> tri1 <C> 26.61 <C> 22.94 <C> 26.55 <C> 26.08 <C> 24.79 <C> 25.36 <C> 1 <R> <C> tri2 <C> 24.59 <C> 21.47 <C> 24.59 <C> 23.82 <C> 22.69 <C> 23.37 <C> 2 <R> <C> tri3(LDA+MLLT) <C> 22.24 <C> 18.86 <C> 22.47 <C> 21.00 <C> 19.77 <C> 21.10 <C> 2.5 <R> <C> Chain-TDNN <C> 10.43 <C> 9.10 <C> 11.84 <C> 9.59 <C> 8.81 <C> 10.87 <C> 15 <CAP> Table 1: Baseline system results and training time
<R> <C> Model <C> Avg time <C> Avg tokens <R> <C> NSP <C> 0.260 ±0.002 <C> 56209 <R> <C> NSP-G(500) <C> 0.079±0.000 <C> 9643 <R> <C> NSP-G(104) <C> 0.252±0.000 <C> 6981 <R> <C> NSP-G(all) <C> 4.416±0.029 <C> [BOLD] 6336 <R> <C> NSP-GC(500) <C> 0.074±0.000 <C> 9643 <R> <C> NSP-GC(104) <C> 0.069±0.000 <C> 6981 <R> <C> NSP-GC(all) <C> [BOLD] 0.067± [BOLD] 0.000 <C> [BOLD] 6336 <CAP> Table 1: Prediction time (in seconds) and number of permissible tokens per query on average, for our baseline neural semantic parser (NSP) and various models using grammar integration with caching (NSP-GC) or without (NSP-G).
<R> <C> Task <C> Train <C> Valid <C> Test <C> Total <R> <C> Dialogue <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> – HH (Human-Human) <C> 131438 <C> 7801 <C> 6634 <C> 145873 <R> <C> – HB (Human-Bot) <C> 60000 <C> 0 <C> 0 <C> 60000 <R> <C> Feedback <C> 60000 <C> 1000 <C> 1000 <C> 62000 <R> <C> Satisfaction <C> 1000 <C> 500 <C> 1000 <C> 2500 <CAP> Table 1: The number of examples used in our experiments by task and split. Note that the HH Dialogue examples come from the PersonaChat dataset, HB Dialogue and Feedback examples were collected during deployment, and an additional 40k Satisfaction training examples were collected for the analysis in Section 5.1.
<R> <C> Subset <C> Q1 Slope <C> Q1  [ITALIC] r2 <C> Median (Q2) Slope <C> Median (Q2)  [ITALIC] r2 <C> Q3 Slope <C> Q3  [ITALIC] r2 <R> <C> [EMPTY] <C> (chars./year) <C> [EMPTY] <C> (chars./year) <C> [EMPTY] <C> (chars./year) <C> [EMPTY] <R> <C> All <C> -2.53 <C> 0.916 <C> -5.20 <C> 0.926 <C> -8.32 <C> 0.862 <R> <C> Sep–Dec <C> -5.29 <C> 0.812 <C> -7.85 <C> 0.894 <C> -9.97 <C> 0.889 <R> <C> Resampled <C> -2.54 <C> 0.918 <C> -5.25 <C> 0.927 <C> -8.23 <C> 0.860 <R> <C> URLs removed <C> -2.42 <C> 0.933 <C> -4.63 <C> 0.922 <C> -7.51 <C> 0.887 <R> <C> English only <C> -3.38 <C> 0.910 <C> -5.95 <C> 0.881 <C> -8.35 <C> 0.785 <R> <C> 2010–2012 <C> -5.19 <C> 0.842 <C> -8.07 <C> 0.910 <C> -10.4 <C> 0.938 <R> <C> Trending topics <C> -3.41 <C> 0.153 <C> -6.83 <C> 0.294 <C> -7.61 <C> 0.440 <R> <C> Outside US <C> -5.57 <C> 0.838 <C> -8.15 <C> 0.904 <C> -10.4 <C> 0.909 <CAP> Table 1: Slopes of utterance length quartiles temporal regression lines
<R> <C> bound <C> dataset <C> ROUGE-1 F <R> <C> human <C> DUC ’03 <C> 0.56235 <R> <C> extractive <C> [EMPTY] <C> 0.45497 <R> <C> model fit <C> [EMPTY] <C> 0.40873 <R> <C> prediction <C> [EMPTY] <C> 0.39294 <R> <C> human <C> DUC ’04 <C> 0.55221 <R> <C> extractive <C> [EMPTY] <C> 0.45199 <R> <C> model fit <C> [EMPTY] <C> 0.40963 <R> <C> prediction <C> [EMPTY] <C> 0.40662 <CAP> Figure 7: Upper bounds on ROUGE-1 F scores: agreement between manual summaries, greedily computed best extractive summaries, best model fit on the train set (using the best C value) and the test scores of the pairwise model.
<R> <C> removed <C> ROUGE-1 F <R> <C> group <C> [EMPTY] <R> <C> none <C> 0.40662 <R> <C> basic <C> [BOLD] 0.38681 <R> <C> all except basic <C> [BOLD] 0.39723 <R> <C> location <C> [BOLD] 0.39782 <R> <C> sent+doc <C> 0.39901 <R> <C> cap+stop+len <C> 0.40273 <R> <C> minmax <C> 0.40721 <CAP> Figure 8: Effects of removing different feature groups on the DUC ’04 dataset. Bold font marks significant difference (p≤0.05) when compared to the full pariwise model. The most important are basic similarity features including all words (similar to [22]). The last feature group actually lowered the score but is included in the model because we only found this out later on DUC ’04 dataset.
<R> <C> Measure <C> [ITALIC] P <C> [ITALIC] R <C> [ITALIC] F1 <C> 0/1 Acc <R> <C> 0/1 (bin.) <C> 88.86 <C> 89.27 <C> 89.00 <C> 27.32 <R> <C> [ITALIC] #FP (bin.) <C> 90.43 <C> 90.34 <C> 90.32 <C> 27.91 <R> <C> [ITALIC] F1 (bin.) <C> 90.33 <C> 90.43 <C> 90.32 <C> 28.69 <R> <C> [ITALIC] F1 <C> [BOLD] 90.92 <C> [BOLD] 90.82 <C> [BOLD] 90.82 <C> [BOLD] 29.18 <CAP> Table 1: Experimental results on the test set.
<R> <C> [ITALIC] fgivenTE <C> [ITALIC] fclaimTE <C> Dev(%) <C> Test(%) <R> <C> [ITALIC] fESIMG1 <C> [ITALIC] fESIMC1 <C> 67.27 <C> 71.2 <R> <C> [ITALIC] fESIMG1 <C> [ITALIC] fBERTC1 <C> 62.23 <C> 69.12 <R> <C> [ITALIC] fESIMG1 <C> [ITALIC] fESIMC2 <C> 66.54 <C> 69.57 <R> <C> [ITALIC] fESIMG1 <C> [ITALIC] fBERTC2 <C> 59.71 <C> 67.39 <R> <C> [ITALIC] fBERTG1 <C> [ITALIC] fESIMC1 <C> 67.99 <C> 71.56 <R> <C> [ITALIC] fBERTG1 <C> [ITALIC] fBERTC1 <C> 67.62 <C> 69.38 <R> <C> [ITALIC] fBERTG1 <C> [ITALIC] fESIMC2 <C> 62.95 <C> 69.2 <R> <C> [ITALIC] fBERTG1 <C> [ITALIC] fBERTC2 <C> 68.35 <C> 67.93 <R> <C> [ITALIC] fESIMG2 <C> [ITALIC] fESIMC1 <C> 68.34 <C> 67.21 <R> <C> [ITALIC] fESIMG2 <C> [ITALIC] fBERTC1 <C> 59.35 <C> 66.49 <R> <C> [ITALIC] fESIMG2 <C> [ITALIC] fESIMC2 <C> 66.55 <C> 66.3 <R> <C> [ITALIC] fESIMG2 <C> [ITALIC] fBERTC2 <C> 58.63 <C> 64.3 <R> <C> [ITALIC] fBERTG2 <C> [ITALIC] fESIMC1 <C> [BOLD] 73.38 <C> [BOLD] 76.63 <R> <C> [ITALIC] fBERTG2 <C> [ITALIC] fBERTC1 <C> 72.66 <C> 75.36 <R> <C> [ITALIC] fBERTG2 <C> [ITALIC] fESIMC2 <C> 70.50 <C> 73.55 <R> <C> [ITALIC] fBERTG2 <C> [ITALIC] fBERTC2 <C> 73.02 <C> 70.29 <CAP> Table 5: shows the accuracy on dev and test set of QUAREL for various choice of fgivenTE and fclaimTE. Here, G1,G2,C1 and C2 respectively represents TrainQUARELGiven, TrainQUARELGiven∪TrainSNLI, TrainQUARELClaim, TrainQUARELClaim∪TrainSNLI.
<R> <C> [BOLD] Type of speech <C> [BOLD] #corpora <C> [BOLD] #F <C> [BOLD] #M <R> <C> Elicited <C> 41 <C> 1782 <C> 1596 <R> <C> [EMPTY] <C> [EMPTY] <C> 52.8% <C> 47.2% <R> <C> Non-elicited <C> 5 <C> 1268 <C> 1426 <R> <C> [EMPTY] <C> [EMPTY] <C> 47.1% <C> 52.9% <R> <C> Non-elicited <C> 4 <C> 67 <C> 143 <R> <C> (without Librispeech) <C> [EMPTY] <C> 31.9% <C> 68.1% <CAP> Table 3: Speaker gender distribution in data depending on the type of speech. NB: the two last lines refer to the non-elicited corpora, the only difference is that the last line does not take Librispeech into account.
<R> <C> [BOLD] Task <C> [BOLD] #corpora <C> [BOLD] #F <C> [BOLD] #M <R> <C> ASR <C> 12 <C> 2523 <C> 2615 <R> <C> [EMPTY] <C> [EMPTY] <C> 49.1% <C> 50.9% <R> <C> TTS <C> 10 <C> 124 <C> 70 <R> <C> [EMPTY] <C> [EMPTY] <C> 63.9% <C> 36.1% <R> <C> [EMPTY] <C> 25 <C> 403 <C> 337 <R> <C> [EMPTY] <C> [EMPTY] <C> 54.5% <C> 45.5% <CAP> Table 5: Speaker gender representation in data depending on the task. ASR stands for Automatic Speech Recognition, TTS stands for Text To Speech, and NA accounts for the corpora for which no task was explicitly cited.
<R> <C> [BOLD] Dataset <C> [BOLD] w2g <C> [BOLD] w2gm <C> [BOLD] GM_ [BOLD] KL (Ours) <R> <C> SL <C> 14.29 <C> 19.77 <C> [BOLD] 22.96 <R> <C> WS <C> 47.63 <C> 58.35 <C> [BOLD] 64.79 <R> <C> WS-S <C> 49.43 <C> 59.22 <C> [BOLD] 65.48 <R> <C> WS-R <C> 47.85 <C> 56.90 <C> [BOLD] 64.67 <R> <C> MEN <C> 42.61 <C> 55.96 <C> [BOLD] 57.04 <R> <C> MC <C> 43.01 <C> 39.21 <C> 41.05 <R> <C> RG <C> 27.13 <C> 49.68 <C> [BOLD] 51.87 <R> <C> YP <C> 12.05 <C> 28.74 <C> 21.50 <R> <C> MT-287 <C> 51.41 <C> 61.25 <C> [BOLD] 64.00 <R> <C> MT-771 <C> 41.38 <C> 50.58 <C> [BOLD] 51.68 <R> <C> RW <C> 18.43 <C> 12.65 <C> 12.96 <CAP> Table 3: Spearman correlation results on word similarity datasets.
<R> <C> [BOLD] Dataset <C> [BOLD] Metric <C> [BOLD] w2g <C> [BOLD] w2gm <C> [BOLD] GM_ [BOLD] KL (Ours) <R> <C> Turney and Mohammad ( 2015 ) <C> Precision <C> 51.69 <C> 53.47 <C> [BOLD] 54.25 <R> <C> [EMPTY] <C> F1 <C> 65.41 <C> 66.27 <C> [BOLD] 66.32 <R> <C> Baroni et al. ( 2012 ) <C> Precision <C> 57.18 <C> 66.42 <C> [BOLD] 67.55 <R> <C> [EMPTY] <C> F1 <C> 63.72 <C> 70.72 <C> [BOLD] 71.49 <R> <C> Kotlerman et al. ( 2010 ) <C> Precision <C> 66.12 <C> 69.89 <C> [BOLD] 70.00 <R> <C> [EMPTY] <C> F1 <C> 46.07 <C> 46.40 <C> [BOLD] 47.48 <CAP> Table 4: Results on entailment datasets
<R> <C> Model <C> Metric 1 <C> Metric 2 <C> Metric 3 <R> <C> SVR <C> 62.14 <C> 54.59 <C> 62.34 <R> <C> SLSTM <C> 72.89 <C> 61.55 <C> 68.64 <R> <C> ELSTM <C> 73.20 <C> 61.98 <C> 69.24 <CAP> Table 1: Results
<R> <C> [BOLD] Model <C> [BOLD] Baseline <C> [ITALIC] β=1 <C> [ITALIC] β=0.5 <C> [ITALIC] β=0 <R> <C> [BOLD] BLEU <C> 20.11 <C> 20.41 <C> 20.94 <C> [BOLD] 21.32 <CAP> Table 1: BLEU scores with different β on English-German dev set with greedy search. Baseline is trained only with cross entropy.
<R> <C> # of topics <C> [ITALIC] β <C> Topic Coherences <R> <C> 20 <C> 0.75 <C> [BOLD] 0.567 <R> <C> 30 <C> 0.75 <C> 0.555 <R> <C> 40 <C> 0.75 <C> 0.553 <R> <C> 50 <C> 0.75 <C> 0.547 <R> <C> 20 <C> 1.00 <C> 0.563 <R> <C> 30 <C> 1.00 <C> 0.564 <R> <C> 40 <C> 1.00 <C> 0.552 <R> <C> 50 <C> 1.00 <C> 0.558 <CAP> Figure 2: Average topic coherences found by lda2vec in the Twenty Newsgroups dataset are given. The topic coherence has been demonstrated to correlate with human evaluations of topic models [Röder et al.2015]. The number of topics chosen is given, as well as the negative sampling exponent parameter β. Compared to β=1.00, β=0.75 draws more rare words as negative samples. The best topic coherences are found in models n=20 topics and a β=0.75.
<R> <C> [BOLD] %Training Edges <C> [BOLD] 15% <C> [BOLD] 25% <C> [BOLD] 35% <C> [BOLD] 45% <C> [BOLD] 55% <C> [BOLD] 65% <C> [BOLD] 75% <C> [BOLD] 85% <C> [BOLD] 95% <R> <C> [BOLD] MMB <C> 54.7 <C> 57.1 <C> 59.5 <C> 61.9 <C> 64.9 <C> 67.8 <C> 71.1 <C> 72.6 <C> 75.9 <R> <C> [BOLD] DeepWalk <C> 56.0 <C> 63.0 <C> 70.2 <C> 75.5 <C> 80.1 <C> 85.2 <C> 85.3 <C> 87.8 <C> 90.3 <R> <C> [BOLD] LINE <C> 55.0 <C> 58.6 <C> 66.4 <C> 73.0 <C> 77.6 <C> 82.8 <C> 85.6 <C> 88.4 <C> 89.3 <R> <C> [BOLD] node2vec <C> 55.9 <C> 62.4 <C> 66.1 <C> 75.0 <C> 78.7 <C> 81.6 <C> 85.9 <C> 87.3 <C> 88.2 <R> <C> [BOLD] Naive combination <C> 72.7 <C> 82.0 <C> 84.9 <C> 87.0 <C> 88.7 <C> 91.9 <C> 92.4 <C> 93.9 <C> 94.0 <R> <C> [BOLD] TADW <C> 86.6 <C> 88.2 <C> 90.2 <C> 90.8 <C> 90.0 <C> 93.0 <C> 91.0 <C> 93.4 <C> 92.7 <R> <C> [BOLD] CENE <C> 72.1 <C> 86.5 <C> 84.6 <C> 88.1 <C> 89.4 <C> 89.2 <C> 93.9 <C> 95.0 <C> 95.9 <R> <C> [BOLD] CANE <C> 86.8 <C> 91.5 <C> 92.2 <C> 93.9 <C> 94.6 <C> 94.9 <C> 95.6 <C> 96.6 <C> 97.7 <R> <C> [BOLD] WANE <C> 86.1 <C> 90.9 <C> 92.3 <C> 93.1 <C> 93.4 <C> 94.5 <C> 95.1 <C> 95.4 <C> 95.9 <R> <C> [BOLD] WANE- [ITALIC] wc <C> 88.7 <C> 92.1 <C> 92.9 <C> 94.4 <C> 94.8 <C> 95.1 <C> 95.7 <C> 96.5 <C> 97.4 <R> <C> [BOLD] WANE- [ITALIC] ww <C> [BOLD] 91.7 <C> [BOLD] 93.3 <C> [BOLD] 94.1 <C> [BOLD] 95.7 <C> [BOLD] 96.2 <C> [BOLD] 96.9 <C> [BOLD] 97.5 <C> [BOLD] 98.2 <C> [BOLD] 99.1 <CAP> Table 1: AUC scores for link prediction on the Cora dataset.
<R> <C> [BOLD] Baseline Models <C> 1% <C> 3% <C> 7% <C> 10% <R> <C> [BOLD] Text Only <C> 33.0 <C> 43.0 <C> 57.1 <C> 62.8 <R> <C> [BOLD] Naive Combination <C> 67.4 <C> 70.6 <C> 75.1 <C> 77.4 <R> <C> [BOLD] TADW <C> 72.1 <C> 77.0 <C> 79.1 <C> 81.3 <R> <C> [BOLD] CENE <C> [BOLD] 73.8 <C> 79.1 <C> 81.5 <C> 84.5 <R> <C> [BOLD] CANE <C> 72.6 <C> 78.2 <C> 80.4 <C> 83.4 <R> <C> [BOLD] WANE- [ITALIC] ww (ours) <C> 73.4 <C> [BOLD] 79.6 <C> [BOLD] 82.7 <C> [BOLD] 85.1 <CAP> Table 4: Semi-supervised vertex classification results on the Cora dataset.
<R> <C> [BOLD] Model <C> [BOLD] MR* <C> [BOLD] Ohsumed* <R> <C> TextGCN <C> 53.15 <C> 47.24 <R> <C> TextING <C> 64.43 <C> 57.11 <R> <C> # Words in Training <C> 465 <C> 7,009 <R> <C> # New Words in Test <C> 18,299 <C> 7,148 <CAP> Table 3: Accuracy (%) of TextGCN and TextING on MR and Ohsumed, where MR uses 40 labelled documents (0.5% of full training data) and Ohsumed uses 460 labelled documents (13.7% of full training data).
<R> <C> Model <C> LAPTOP <C> REST <C> TWITTER <R> <C> DE-CNN <C> 81.59 <C> - <C> - <R> <C> TAG <C> [BOLD] 85.20 <C> [BOLD] 84.48 <C> 73.47 <R> <C> SPAN <C> 83.35 <C> 82.38 <C> [BOLD] 75.28 <CAP> Table 3: F1 comparison of different approaches for target extraction.
<R> <C> Model <C> LAPTOP <C> REST <C> TWITTER <R> <C> MGAN <C> 75.39 <C> - <C> - <R> <C> TNet <C> 76.54 <C> - <C> - <R> <C> TAG <C> 71.42 <C> 81.80 <C> 59.76 <R> <C> SPAN <C> [BOLD] 81.39 <C> [BOLD] 89.95 <C> [BOLD] 75.16 <CAP> Table 5: Accuracy comparison of different approaches for polarity classification.
<R> <C> [BOLD] Method <C> [BOLD] 2-class <C> [BOLD] 3-class <R> <C> Neural Attention (Rocktäschel et al.,  2015 ) <C> * <C> [BOLD] 83.5 <R> <C> EOP classifier (Bowman et al.,  2015 ) <C> 75.0 <C> * <R> <C> skip-thoughts <C> 87.7 <C> 81.5 <R> <C> order-embeddings (symmetric) <C> 79.3 <C> * <R> <C> order-embeddings <C> [BOLD] 88.6 <C> * <CAP> Table 3: Test accuracy (%) on SNLI.
<R> <C> sources <C> 1 <C> Model GloVe <C> RG 81.7 <C> MC 80.8 <C> WS 64.3 <C> RW 38.4 <C> SCWS 54.0 <C> MEN 74.3 <C> SL 37.4 <C> GL 70.5 <C> SE 39.9 <C> DV 87.7 <C> SA 73.4 <C> MR 70.0 <R> <C> sources <C> 2 <C> CBOW <C> 76.0 <C> 82.2 <C> 69.8 <C> 53.4 <C> 53.4 <C> 78.2 <C> 44.2 <C> 75.2 <C> 39.1 <C> 87.4 <C> 73.6 <C> 71.0 <R> <C> sources <C> 3 <C> HLBL <C> 35.3 <C> 49.3 <C> 35.7 <C> 19.1 <C> 47.7 <C> 30.7 <C> 22.1 <C> 16.6 <C> 34.8 <C> 72.0 <C> 62.6 <C> 61.6 <R> <C> sources <C> 4 <C> Huang <C> 51.3 <C> 58.8 <C> 58.0 <C> 36.4 <C> 63.7 <C> 56.1 <C> 21.7 <C> 8.3 <C> 35.2 <C> 76.0 <C> 64.8 <C> 60.9 <R> <C> sources <C> 5 <C> CW <C> 29.9 <C> 34.3 <C> 28.4 <C> 15.3 <C> 39.8 <C> 25.7 <C> 15.6 <C> 4.7 <C> 34.6 <C> 75.6 <C> 62.7 <C> 61.4 <R> <C> ablation <C> 6 <C> CONC (-GloVe) <C> 75.0 <C> 79.0 <C> 70.0 <C> 55.3 <C> 62.9 <C> 77.7 <C> 41.5 <C> 64.0 <C> 38.7 <C> 82.9 <C> 72.1 <C> 69.1 <R> <C> ablation <C> 7 <C> CONC (-CBOW) <C> 80.8 <C> 81.0 <C> 65.2 <C> 46.0 <C> 56.3 <C> 74.9 <C> 37.3 <C> 70.0 <C> 38.8 <C> 86.0 <C> 71.6 <C> 69.9 <R> <C> ablation <C> 8 <C> CONC (-HLBL) <C> 83.0 <C> 84.0 <C> 71.9 <C> 53.4 <C> 61.4 <C> 80.1∗ <C> 41.6 <C> 72.7 <C> 39.5 <C> 84.9 <C> 71.0 <C> 69.4 <R> <C> ablation <C> 9 <C> CONC (-Huang) <C> 83.0 <C> 84.0 <C> 71.6 <C> 48.8 <C> 60.8 <C> 80.1∗ <C> 41.9 <C> 72.8 <C> 40.0 <C> 86.7 <C> 71.2 <C> 69.1 <R> <C> ablation <C> 10 <C> CONC (-CW) <C> 82.9 <C> 84.0 <C> 71.9 <C> 53.3 <C> 61.6 <C> 80.2∗ <C> 41.6 <C> 72.6 <C> 39.6 <C> 84.9 <C> 72.3 <C> 69.9 <R> <C> [EMPTY] <C> 11 <C> SVD (-GloVe) <C> 78.6 <C> 79.9 <C> 68.4 <C> 53.9 <C> 61.6 <C> 77.5 <C> 40.1 <C> 61.7 <C> 38.5 <C> 84.1 <C> 71.6 <C> 69.8 <R> <C> [EMPTY] <C> 12 <C> SVD (-CBOW) <C> 80.5 <C> 81.2 <C> 64.4 <C> 45.3 <C> 55.3 <C> 74.2 <C> 35.7 <C> 70.9 <C> 38.7 <C> 86.7 <C> 73.4 <C> 69.1 <R> <C> [EMPTY] <C> 13 <C> SVD (-HLBL) <C> 82.7 <C> 83.6 <C> 70.3 <C> 52.6 <C> 60.1 <C> 79.9∗ <C> 39.6 <C> 73.5 <C> 39.8 <C> 87.3 <C> 73.2 <C> 70.4 <R> <C> [EMPTY] <C> 14 <C> SVD (-Huang) <C> 82.5 <C> 85.0 <C> 70.3 <C> 48.6 <C> 59.8 <C> 79.9∗ <C> 39.9 <C> 73.7 <C> 40.0 <C> 87.3 <C> 73.5 <C> 70.8 <R> <C> [EMPTY] <C> 15 <C> SVD (-CW) <C> 82.5 <C> 83.9 <C> 70.4 <C> 52.5 <C> 60.1 <C> 80.0∗ <C> 39.7 <C> 73.3 <C> 39.8 <C> 87.2 <C> 73.1 <C> 70.7 <R> <C> [EMPTY] <C> 16 <C> Proposed (-GloVe) <C> 79.8 <C> 79.7 <C> 71.1 <C> 54.7 <C> 62.3 <C> 78.2 <C> 46.1 <C> 84.2∗ <C> 39.8 <C> 85.4 <C> 72.2 <C> 70.2 <R> <C> [EMPTY] <C> 17 <C> Proposed (-CBOW) <C> 80.9 <C> 82.1 <C> 67.4 <C> 58.7∗ <C> 58.7 <C> 75.7 <C> 45.2 <C> 85.2∗ <C> 40.1 <C> 87.1 <C> 73.8 <C> 70.1 <R> <C> [EMPTY] <C> 18 <C> Proposed (-HLBL) <C> 82.1 <C> 86.1 <C> 71.3 <C> 58.3∗ <C> 62.1 <C> 81.9∗ <C> 34.8 <C> 86.3∗ <C> 40.3 <C> 87.7 <C> 73.7 <C> 71.1 <R> <C> [EMPTY] <C> 19 <C> Proposed (-Huang) <C> 81.2 <C> 85.2 <C> 73.1 <C> 55.1 <C> 63.7 <C> 81.4∗ <C> 42.3 <C> 82.6∗ <C> 41.1 <C> 87.5 <C> 73.9 <C> 71.2 <R> <C> [EMPTY] <C> 20 <C> Proposed (-CW) <C> 83.1 <C> 84.8 <C> 72.5 <C> 58.5 <C> 62.3 <C> 81.1∗ <C> 43.5 <C> 88.4∗ <C> 41.9 <C> 87.8 <C> 71.6 <C> 71.1 <R> <C> ensemble <C> 21 <C> CONC <C> 82.9 <C> 84.1 <C> 71.9 <C> 53.3 <C> 61.5 <C> 80.2∗ <C> 41.6 <C> 72.9 <C> 39.6 <C> 84.9 <C> 72.4 <C> 69.9 <R> <C> ensemble <C> 22 <C> SVD <C> 82.7 <C> 83.9 <C> 70.4 <C> 52.6 <C> 60.0 <C> 79.9∗ <C> 39.7 <C> 73.4 <C> 39.7 <C> 87.2 <C> 73.4 <C> 70.7 <R> <C> ensemble <C> 23 <C> 1TON <C> 80.7 <C> 80.7 <C> 74.5 <C> 60.1∗ <C> 61.6 <C> 73.5 <C> 46.4 <C> 76.8 <C> 42.3∗ <C> 87.6 <C> 73.8 <C> 70.3 <R> <C> ensemble <C> 24 <C> 1TON+ <C> 82.7 <C> 85.0 <C> 75.3 <C> 61.6∗ <C> 60.2 <C> 74.1 <C> 46.3 <C> 77.0 <C> 40.1 <C> 83.9 <C> 73.9 <C> 69.2 <R> <C> ensemble <C> 25 <C> Proposed <C> [BOLD] 83.4 <C> [BOLD] 86.2 <C> [BOLD] 75.7∗ <C> [BOLD] 62.8∗ <C> [BOLD] 63.8 <C> [BOLD] 82.2∗ <C> [BOLD] 48.7 <C> [BOLD] 89.9∗ <C> [BOLD] 43.1∗ <C> [BOLD] 88.7∗ <C> [BOLD] 74.0 <C> [BOLD] 71.3 <CAP> Table 1: Results on word similarity, analogy, relation and short-text classification tasks. For each task, the best performing method is shown in bold. Statistically significant improvements over the best individual source embedding are indicated by an asterisk.
<R> <C> Model <C> BLEU Greedy <C> BLEU Beam <C> ROUGE Greedy <C> ROUGE Beam <C> METEOR Greedy <C> METEOR Beam <C> Human Ranking COPA <C> Human Ranking WSC-G <C> Human Ranking NC <R> <C> L2E-Seq2Seq <C> [BOLD] 0.55 <C> 0.37 <C> 18.8 <C> 18.3 <C> 7.4 <C> 7.6 <C> [BOLD] 0.412 <C> [BOLD] 0.409 <C> 0.454 <R> <C> L2EC-Seq2Seq <C> 0.40 <C> [BOLD] 0.47 <C> [BOLD] 19.9 <C> [BOLD] 19.7 <C> [BOLD] 8.6 <C> [BOLD] 8.8 <C> — <C> — <C> 0.433 <R> <C> L2E-LM <C> 0.25 <C> 0.20 <C> 15.9 <C> 16.8 <C> 6.1 <C> 6.7 <C> 0.515 <C> 0.572 <C> 0.479 <R> <C> L2EC-LM <C> 0.36 <C> 0.38 <C> 17.0 <C> 17.7 <C> 6.7 <C> 7.3 <C> — <C> — <C> [BOLD] 0.432 <R> <C> LM-1B† <C> 0.18 <C> — <C> 16.9 <C> — <C> 7.1 <C> — <C> 0.526 <C> 0.484 <C> 0.454 <R> <C> L2W† <C> 0.00 <C> 0.00 <C> 14.0 <C> 13.9 <C> 6.7 <C> 6.8 <C> 0.511 <C> 0.523 <C> 0.625 <R> <C> L2WC <C> 0.13 <C> 0.14 <C> 12.8 <C> 12.7 <C> 5.7 <C> 5.7 <C> — <C> — <C> 0.546 <R> <C> OpenSubtitle† <C> 0.04 <C> 0.0 <C> 13.0 <C> 13.4 <C> 1.9 <C> 3.7 <C> 0.827 <C> 0.823 <C> 0.811 <R> <C> Reference <C> 100 <C> 100 <C> 100 <C> 100 <C> 100 <C> 100 <C> [BOLD] 0.266 <C> [BOLD] 0.238 <C> [BOLD] 0.267 <CAP> Table 4: BLUE, ROGUE, METEOR are evaluated on News Commentary test data. Any model with C in the name is evaluated with full context. Models with † are pre-trained models from other work. Only L2E-Seq2Seq uses the Transformer architecture, the rest LSTM. In human ranking, we report the average rank across participants. Top ranking is 0 and lowest ranking is 1.
<R> <C> [BOLD] Model <C> [BOLD] Diversity  [BOLD] Distinct-2 / 3 / 4 <C> [BOLD] F.Inc <C> [BOLD] Fact-Inclusion  [BOLD] F.Per <C> [BOLD] F.Hal <C> [BOLD] Agreement  [BOLD] F.Inc /  [BOLD] F.Per <R> <C> M-1 <C> .004 / .006 / .010 <C> 0.41 <C> 0.01 <C> 0.40 <C> 0.99 / 0.99 <R> <C> M-2 <C> .010 / .019 / .031 <C> 0.43 <C> 0.01 <C> 0.42 <C> 0.97 / 0.99 <R> <C> M-3 <C> .001 / .001 / .002 <C> 0.06 <C> 0.04 <C> 0.02 <C> 0.99 / 0.99 <R> <C> M-4 <C> .054 / .010 / .156 <C> 0.51 <C> 0.09 <C> 0.42 <C> 0.98 / 0.98 <R> <C> S2S-1 <C> .012 / .022 / .036 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> N/A / N/A <R> <C> S2S-2 <C> .012 / .022 / .035 <C> 0.54 <C> 0.04 <C> 0.50 <C> 0.97 / 0.99 <R> <C> S2S-3 <C> .026 / .043 / .061 <C> 0.79 <C> 0.16 <C> 0.63 <C> 0.97 / 0.97 <R> <C> S2SC-1 <C> .039 / .069 / .104 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> N/A / N/A <R> <C> S2SC-2 <C> .035 / .067 / .109 <C> 0.73 <C> 0.36 <C> 0.37 <C> 0.99 / 0.99 <R> <C> S2SC-3* <C> .058 / .111 / .178 <C> 0.73 <C> 0.55 <C> 0.18 <C> 0.98 / 0.96 <R> <C> M-S2S <C> .035 / .065 / .104 <C> 0.47 <C> 0.05 <C> 0.42 <C> 0.96 / 0.98 <R> <C> [BOLD] DeepCopy <C> [BOLD] .059 / .121 / .201 <C> 0.62 <C> 0.23 <C> 0.39 <C> 0.95 / 0.97 <R> <C> G.Truth <C> 0.35 / 0.66 / 0.84 <C> 0.76 <C> 0.49 <C> 0.27 <C> 0.93 / 0.96 <CAP> Table 2: Lexical diversity and fact inclusion analysis results. Model names are abbreviated according to Table 1. F.Inc denotes the ratio of responses that include factual information. F.Per and F.Hal denote the ratio of responses where the included fact is consistent with the persona or a hallucinated one, respectively. Agreement column corresponds to Cohen’s κ statistic measuring inter-rater agreement on binary factual evaluation metrics for F.Inc and F.Per. * indicates the ORACLE model.
<R> <C> [EMPTY] <C> JZR <C> JZR (limited) <C> ISRI <R> <C> JZR <C> 0 <C> 63 <C> 126 <R> <C> JZR (limited) <C> 0 <C> 0 <C> 120 <R> <C> ISRI <C> 224 <C> 281 <C> 0 <CAP> Table 4: One-to-one comparison of extractors. The number in the cell shows how many times the extractor in that row performed better than the extractor in the column.
<R> <C> [ITALIC] δ <C> # min <C> evaluations mean <C> with max <C> BTS-4 % correctly selected <C> # min <C> evaluations mean <C> with max <C> BTS-8 % correctly selected <R> <C> 0.05 <C> 28 <C> 282 <C> 1392 <C> 100 <C> 88 <C> 315 <C> 1128 <C> 100 <R> <C> 0.1 <C> 24 <C> 144 <C> 520 <C> 100 <C> 56 <C> 178 <C> 784 <C> 100 <R> <C> 0.2 <C> 24 <C> 76 <C> 280 <C> 98 <C> 32 <C> 106 <C> 352 <C> 99 <CAP> Table 3: Number of evaluations of required to select a TDSA model at a range of confidence levels across 500 runs of BTS selecting batches of 4 and 8 models.
<R> <C> Random (per-task) LSTM+CNN+SA <C> Wh: 0.09 Wh: 0.81 <C> Wh: 0.09 Wh: 0.81 <C> Y/N: 0.50 Y/N 0.52 <C> Y/N: 0.50 Y/N 0.52 <R> <C> CL setups: <C> I) Wh→Y/N <C> I) Wh→Y/N <C> II) Y/N→Wh <C> II) Y/N→Wh <R> <C> [EMPTY] <C> Wh <C> Y/N <C> Y/N <C> Wh <R> <C> Random (both tasks) <C> 0.04 <C> 0.25 <C> 0.25 <C> 0.04 <R> <C> Naive <C> 0.00 <C> 0.61 <C> 0.00 <C> 0.81 <R> <C> EWC <C> 0.25 <C> 0.51 <C> 0.00 <C> 0.83 <R> <C> Rehearsal <C> 0.75 <C> 0.51 <C> 0.51 <C> 0.80 <R> <C> Cumulative <C> 0.81 <C> 0.74 <C> 0.74 <C> 0.81 <CAP> Table 1: Mean accuracy over 3 runs: Trained on each task independently (first two rows; per-task label space Y) vs. CL setups (single-head label space over all Y).
<R> <C> Sampling <C> Model <C> Temp. Valid. <C> Rep. Valid. <C> Rep. Test <C> Prompts <C> Interactive <R> <C> Rephrases <C> Independent <C> 0.979 <C> 0.810 <C> [BOLD] 0.806 <C> 0.171 <C> 0.307 <R> <C> Rephrases <C> Seq2Tree <C> 0.979 <C> 0.801 <C> 0.794 <C> 0.180 <C> 0.231 <R> <C> Rephrases <C> SentenceRec <C> 0.976 <C> 0.814 <C> [BOLD] 0.807 <C> 0.159 <C> 0.255 <R> <C> Prompts <C> Independent <C> 0.976 <C> 0.804 <C> 0.773 <C> 0.184 <C> 0.370 <R> <C> Prompts <C> Seq2Tree <C> 0.987 <C> 0.819 <C> 0.789 <C> 0.176 <C> 0.321 <R> <C> Prompts <C> SentenceRec <C> 0.980 <C> 0.828 <C> 0.776 <C> 0.179 <C> 0.360 <R> <C> Interactive <C> Independent <C> 0.976 <C> 0.782 <C> 0.709 <C> 0.179 <C> 0.337 <R> <C> Interactive <C> Seq2Tree <C> 0.975 <C> 0.802 <C> 0.734 <C> [BOLD] 0.196 <C> 0.454 <R> <C> Interactive <C> SentenceRec <C> 0.980 <C> 0.820 <C> 0.771 <C> [BOLD] 0.195 <C> [BOLD] 0.465 <CAP> Table 1: Success of trained models over various training and test distributions. Each group of three rows corresponds to a distribution over top-level commands used during training. “Rephrases”, “Prompts”, and “Interactive” as in Figure 2. In the columns, “Temp” refers to the templates distribution, “Rep.” to rephrases (from the template distribution), and “Prompts” and “Interactive” as before.
<R> <C> Model <C> Node <C> Test P/R/F Rephrases <C> Test P/R/F Prompts <C> Test P/R/F Interactive <R> <C> Ind. <C> INT <C> 97/95/96 <C> 75/77/76 <C> 75/83/79 <R> <C> Ind. <C> CAT <C> 92/90/91 <C> 44/56/50 <C> 52/66/58 <R> <C> Ind. <C> SPAN <C> 94/91/93 <C> 53/42/47 <C> 58/51/54 <R> <C> SRec <C> INT <C> 97/96/97 <C> 75/77/76 <C> 86/84/85 <R> <C> SRec <C> CAT <C> 92/91/92 <C> 42/54/47 <C> 59/66/62 <R> <C> SRec <C> SPAN <C> 94/93/94 <C> 51/43/46 <C> 68/56/61 <CAP> Table 2: Per-node Precision, Recall and F1 for models trained with interactive sampling for all node types
<R> <C> [BOLD] # Datapoints <C> [BOLD] Model <C> [BOLD] en-es <C> [BOLD] es-en <C> [BOLD] en-fr <C> [BOLD] fr-en <C> [BOLD] en-de <C> [BOLD] de-en <C> [BOLD] en-ru <C> [BOLD] ru-en <C> [BOLD] en-zh <C> [BOLD] zh-en <R> <C> * <C> MUSE(U) <C> 81.7 <C> 83.3 <C> 82.3 <C> 82.1 <C> 74.0 <C> 72.2 <C> 44.0 <C> 59.1 <C> 32.5† <C> 31.4† <R> <C> * <C> Vecmap(U)++ <C> 82.2† <C> 84.5† <C> 82.5† <C> 83.6† <C> 75.2† <C> [BOLD] 74.2† <C> 48.5† <C> 65.1† <C> 0.0 <C> 0.0 <R> <C> 50 <C> MUSE(IR) <C> 0.3 <C> 82.7 <C> 0.5 <C> 1.6 <C> 31.9 <C> 72.7† <C> 0.1 <C> 0.0 <C> 0.3 <C> 0.3 <R> <C> 50 <C> GeoMM <C> 0.3 <C> 1.9 <C> 0.3 <C> 1.0 <C> 0.3 <C> 0.3 <C> 0.0 <C> 0.6 <C> 0.0 <C> 0.0 <R> <C> 50 <C> RCSLS <C> 0.1 <C> 0.4 <C> 0.0 <C> 0.3 <C> 0.1 <C> 0.1 <C> 0.1 <C> 0.1 <C> 0.0 <C> 0.0 <R> <C> 50 <C> BLISS (R) <C> 82.1† <C> 83.6† <C> [BOLD] 82.8† <C> 83.0† <C> 75.1† <C> 72.7† <C> 39.3† <C> 61.0† <C> 32.6† <C> 32.5† <R> <C> 500 <C> MUSE(IR) <C> 81.6 <C> 83.5† <C> 82.1 <C> 82.0 <C> 73.1 <C> 72.7 <C> 40.3 <C> 62 <C> 34.5 <C> 32.2 <R> <C> 500 <C> GeoMM <C> 31.9 <C> 46.6 <C> 34.4 <C> 44.7 <C> 13.5 <C> 14.7 <C> 10.6 <C> 20.5 <C> 3.9 <C> 2.9 <R> <C> 500 <C> RCSLS <C> 22.9 <C> 44.9 <C> 22.4 <C> 43.5 <C> 9.9 <C> 10.2 <C> 7.9 <C> 19.6 <C> 6.6 <C> 7.1 <R> <C> 500 <C> BLISS(R) <C> 82.3† <C> 83.4 <C> 82.3† <C> 82.9† <C> 74.7† <C> 73.1† <C> 41.6† <C> 63.0† <C> 36.3† <C> 35.1† <R> <C> 5000 <C> MUSE(IR) <C> 81.9 <C> 82.8 <C> 82.2 <C> 82.1 <C> 75.2 <C> 72.4 <C> 50.4 <C> 63.7 <C> 39.2 <C> 36.3 <R> <C> 5000 <C> GeoMM <C> 79.7 <C> 82.7 <C> 79.9 <C> 83.2 <C> 71.7 <C> 70.6 <C> 49.7 <C> [BOLD] 65.5† <C> [BOLD] 43.7† <C> 40.1 <R> <C> 5000 <C> RCSLS <C> 80.9 <C> 82.9 <C> 80.4 <C> 82.5 <C> 72.5 <C> 70.9 <C> 51.3 <C> 63.8 <C> 42.5 <C> 41.9 <R> <C> 5000 <C> BLISS(R) <C> [BOLD] 82.4†  <C> [BOLD] 84.9† <C> 82.6† <C> [BOLD] 83.9† <C> [BOLD] 75.7† <C> 72.5† <C> [BOLD] 52.1† <C> 65.2 <C> 42.5 <C> [BOLD] 42.8† <CAP> Таблица 5: Performance with different levels of supervision. † marks the best performance at a given level of supervision, while bold marks the best for a language pair.
<R> <C> [BOLD] Corpus Name <C> [BOLD] Properties Word2vec Glove  [BOLD] Size <C> [BOLD] Properties Word2vec Glove  [BOLD] Voc <C> [BOLD] Properties Word2vec Glove  [BOLD] Cov <C> [BOLD] Properties Word2vec Glove  [BOLD] Sem <C> [BOLD] Properties Word2vec Glove  [BOLD] Sync <C> [BOLD] Properties Word2vec Glove  [BOLD] Overall <C> [BOLD] Properties Word2vec Glove  [BOLD] Sem <C> [BOLD] Properties Word2vec Glove  [BOLD] Sync <C> [BOLD] Properties Word2vec Glove  [BOLD] Overall <R> <C> Tweets_100M <C> 100M <C> 44911 <C> 48.52 <C> 59.52 <C> 42.07 <C> 42.92 <C> 77.27 <C> 34.00 <C> 36.11 <R> <C> Tweets_200M <C> 200M <C> 67154 <C> 49.03 <C> 59.52 <C> 42.07 <C> 42.92 <C> 69.76 <C> 34.84 <C> 36.68 <R> <C> Tweets_300M <C> 300M <C> 78954 <C> 49.03 <C> 60.21 <C> 43.24 <C> 44.17 <C> 68.97 <C> 35.82 <C> 37.57 <R> <C> Tweets_400M <C> 400M <C> 87529 <C> 49.03 <C> 60.77 <C> 44.12 <C> 44.87 <C> 68.18 <C> 37.30 <C> 38.93 <R> <C> Tweets_500M <C> 500M <C> 96055 <C> 49.03 <C> 60.77 <C> 44.32 <C> 45.05 <C> 69.17 <C> 38.22 <C> 39.86 <R> <C> News_250M <C> 250M <C> 79034 <C> 49.03 <C> 54.74 <C> 45.30 <C> 45.79 <C> 37.15 <C> 33.57 <C> 33.67 <R> <C> News_500M <C> 500M <C> 111606 <C> 49.03 <C> 63.64 <C> 50.19 <C> 50.90 <C> 43.48 <C> 40.55 <C> 40.70 <R> <C> News_750M <C> 750M <C> 141772 <C> 49.03 <C> 73.12 <C> 51.32 <C> 52.47 <C> 62.06 <C> 44.50 <C> 45.43 <R> <C> News_1B <C> 1B <C> 157522 <C> 49.03 <C> 76.48 <C> 52.56 <C> 53.82 <C> 59.29 <C> 45.68 <C> 46.40 <R> <C> News_1.25B <C> 1.25B <C> 171143 <C> 49.03 <C> 76.88 <C> 52.39 <C> 53.68 <C> 66.40 <C> 45.86 <C> 46.94 <R> <C> Lyrics_5B <C> 5B <C> 41901 <C> 44.91 <C> 21.9 <C> 8.78 <C> 9.41 <C> 7.14 <C> 1.76 <C> 2.02 <R> <C> Lyrics_10B <C> 10B <C> 45714 <C> 44.91 <C> 25.71 <C> 9.72 <C> 10.48 <C> 12.14 <C> 1.35 <C> 1.87 <R> <C> Lyrics_15B <C> 15B <C> 49294 <C> 44.91 <C> 22.62 <C> 9.34 <C> 9.98 <C> 9.76 <C> 1.30 <C> 1.71 <R> <C> Lyrics_20B <C> 20B <C> 53213 <C> 44.92 <C> 24.41 <C> 9.61 <C> 10.32 <C> 9.00 <C> 1.62 <C> 1.97 <R> <C> Lyrics_24B <C> 24B <C> 71590 <C> 47.92 <C> 26.29 <C> 9.77 <C> 10.59 <C> 10.34 <C> 2.02 <C> 2.43 <R> <C> Review_500M <C> 500M <C> 59745 <C> 48.74 <C> 85.38 <C> 48.97 <C> 50.90 <C> 84.19 <C> 49.11 <C> 50.98 <R> <C> Review_1B <C> 1B <C> 67547 <C> 49.03 <C> 87.15 <C> 49.93 <C> 51.90 <C> 89.13 <C> 53.09 <C> 54.99 <R> <C> Review_1.5B <C> 1.5B <C> 71623 <C> 49.03 <C> 88.14 <C> 52.04 <C> 53.04 <C> 90.32 <C> 55.73 <C> 57.56 <R> <C> Review_2B <C> 2B <C> 75084 <C> 49.03 <C> [BOLD] 88.14 <C> 54.11 <C> 55.91 <C> [BOLD] 90.32 <C> 57.64 <C> 59.36 <R> <C> Review_2.5B <C> 2.5B <C> 82682 <C> 49.03 <C> 83.20 <C> 56.82 <C> 58.21 <C> 86.96 <C> 57.35 <C> 58.91 <R> <C> Misch_12B <C> 12B <C> 441415 <C> 49.03 <C> 79.23 <C> [BOLD] 59.22 <C> 59.48 <C> 81.42 <C> 59.77 <C> 60.92 <R> <C> Misch_24B <C> 24B <C> 1234020 <C> 49.03 <C> 79.23 <C> 57.34 <C> 57.96 <C> 84.39 <C> 55.81 <C> 57.32 <R> <C> Misch_36B <C> 36B <C> 1730038 <C> 49.04 <C> 80.26 <C> 58.66 <C> 57.97 <C> 79.72 <C> 56.99 <C> 58.19 <R> <C> Misch_48B <C> 48B <C> 1739498 <C> 49.04 <C> 81.11 <C> 58.66 <C> 59.05 <C> 84.45 <C> 59.32 <C> 60.65 <R> <C> Misch_60B <C> [BOLD] 60B <C> [BOLD] 1748730 <C> 49.04 <C> 82.39 <C> 59.04 <C> [BOLD] 59.68 <C> 85.04 <C> [BOLD] 59.99 <C> [BOLD] 61.32 <R> <C> Text8Corpus <C> 17M <C> 71290 <C> [BOLD] 91.21 <C> 39.25 <C> 40.61 <C> 40.05 <C> 40.40 <C> 22.00 <C> 29.65 <CAP> Table 3: Word analogy task accuracy scores
<R> <C> Compared Models <C> Task <C> t <C> p <C> Verdict <R> <C> Crawl42 vs Text8_gv <C> Tweets <C> 5.26 <C> 0.00077 <C> [ITALIC] H0 rejected <R> <C> Crawl840 vs Lyric_gv <C> Tweets <C> 5.04 <C> 0.00092 <C> [ITALIC] H0 rejected <R> <C> Crawl840 vs Item_cb <C> Lyrics <C> 4.11 <C> 0.00341 <C> [ITALIC] H0 rejected <R> <C> Crawl42 vs Text8_cb <C> Lyrics <C> 3.921 <C> 0.0042 <C> [ITALIC] H0 rejected <R> <C> Item_cb vs Text8_gv <C> Movies <C> 5.36 <C> 0.00068 <C> [ITALIC] H0 rejected <R> <C> Item_gv vs WikiDep <C> Movies <C> 5.11 <C> 0.00081 <C> [ITALIC] H0 rejected <R> <C> Crawl840 vs Lyric_gv <C> Phones <C> 8.37 <C> 0.000011 <C> [ITALIC] H0 rejected <R> <C> Crawl42 vs Lyric_cb <C> Phones <C> 7.56 <C> 0.000025 <C> [ITALIC] H0 rejected <CAP> Table 4: Comparing highest and lowest scores of each experiment
<R> <C> Compared Models <C> t <C> p <C> Verdict <R> <C> Tweets_500M_gv vs Tweets_500M_cb <C> 1.94 <C> 0.088 <C> [ITALIC] Hm0 not rejected <R> <C> Tweets_400M_gv vs Tweets_400M_cb <C> 1.97 <C> 0.084 <C> [ITALIC] Hm0 not rejected <R> <C> Tweets_300M_gv vs Tweets_300M_cb <C> 2.33 <C> 0.048 <C> [ITALIC] Hm0 rejected <R> <C> Tweets_200M_gv vs Tweets_200M_cb <C> 2.16 <C> 0.062 <C> [ITALIC] Hm0 not rejected <R> <C> Tweets_100M_gv vs Tweets_100M_cb <C> 2.51 <C> 0.036 <C> [ITALIC] Hm0 rejected <R> <C> Tweets_500M_gv vs Tweets_100M_gv <C> 2.14 <C> 0.064 <C> [ITALIC] Hs0 not rejected <R> <C> Tweets_500M_cb vs Tweets_100M_cb <C> 1.66 <C> 0.135 <C> [ITALIC] Hs0 not rejected <CAP> Table 9: Assessing Hs0 and Hm0 with Tweet models on tweets
<R> <C> Compared Models <C> Task <C> t <C> p <C> Verdict <R> <C> Tweets_500M_cb vs Misch_500M_cb <C> Tweets <C> 2.89 <C> 0.02 <C> [ITALIC] Ht0 rejected <R> <C> Tweets_500M_gv vs Misch_500M_gv <C> Tweets <C> 3.24 <C> 0.011 <C> [ITALIC] Ht0 rejected <R> <C> Item_500M_cb vs Misch_500M_cb <C> Movies <C> 4.56 <C> 0.0018 <C> [ITALIC] Ht0 rejected <R> <C> Item_500M_gv vs Misch_500M_gv <C> Movies <C> 4.27 <C> 0.0027 <C> [ITALIC] Ht0 rejected <R> <C> Item_500M_cb vs Misch_500M_cb <C> Phones <C> 4.12 <C> 0.0033 <C> [ITALIC] Ht0 rejected <R> <C> Item_500M_gv vs Misch_500M_gv <C> Phones <C> 5.02 <C> 0.001 <C> [ITALIC] Ht0 rejected <CAP> Table 13: Assessing Ht0 hypothesis
<R> <C> [BOLD] Corpus Name <C> [BOLD] Training <C> [BOLD] Size <C> [BOLD] Voc <C> [BOLD] Dim <C> [BOLD] CV <C> [BOLD] F1 <R> <C> Tweets_w2v <C> W2v <C> 250,000 <C> 12875 <C> 100 <C> 0.727 <C> 0.726 <R> <C> Tweets_sswe <C> SSWE <C> 250,000 <C> 14576 <C> 100 <C> [BOLD] 0.738 <C> 0.729 <R> <C> Lyrics_w2v <C> W2v <C> 1.2M <C> 16350 <C> 100 <C> 0.685 <C> 0.664 <R> <C> Lyrics_sswe <C> SSWE <C> 1.2M <C> 17905 <C> 100 <C> 0.692 <C> 0.673 <CAP> Table 15: SSWE vs WE on Tweets and Lyrics
<R> <C> [BOLD] Experiment <C> [BOLD] Generated Score <C> [BOLD] Generated Fleiss  [ITALIC] κ <C> [BOLD] Gold Score <C> [BOLD] Gold Fleiss  [ITALIC] κ <R> <C> Is this question well-formed? <C> 85.8% <C> 0.65 <C> 93.3% <C> 0.54 <R> <C> Is this question relevant? <C> 78.7% <C> 0.36 <C> 83.3% <C> 0.41 <R> <C> (among  [ITALIC] well-formed) <C> 81.1% <C> 0.39 <C> 83.3% <C> 0.40 <R> <C> Does the span  [ITALIC] partially contain the answer? <C> 85.3% <C> 0.45 <C> 81.1% <C> 0.43 <R> <C> (among  [ITALIC] well-formed) <C> 87.6% <C> 0.48 <C> 82.1% <C> 0.42 <R> <C> (among  [ITALIC] well-formed and relevant) <C> 94.9% <C> 0.41 <C> 92.9% <C> 0.44 <R> <C> Does the span  [ITALIC] completely contain the answer? <C> 74.1% <C> 0.36 <C> 70.0% <C> 0.37 <R> <C> (among  [ITALIC] well-formed) <C> 76.9% <C> 0.36 <C> 70.2% <C> 0.39 <R> <C> (among  [ITALIC] well-formed and relevant) <C> 85.4% <C> 0.30 <C> 80.0% <C> 0.42 <CAP> Table 3: Human evaluations demonstrate the high individual QA quality of our pipeline’s outputs. All interannotator agreement scores (Fleiss κ) show “fair” to “substantial” agreement Landis and Koch (1977).
<R> <C> Ablation <C> F1 <R> <C> Full model (Dynapro) <C> [BOLD] 71.9 <R> <C> No attribute aware representation <C> 69.5 <R> <C> No transition prediction <C> 66.3 <R> <C> No sequential modeling in transition module <C> 68.8 <R> <C> No sequential modeling in attribute classification <C> 68.2 <R> <C> No class prediction <C> 53.8 <R> <C> CLS instead of attribute-aware representation <C> 70.9 <R> <C> Full procedural input <C> 61.0 <CAP> Table 3: Ablation study of different components in Dynapro by comparing F1 score on ProPara Document Level task (dev set).
<R> <C> Baseline <C> Binary Acc <C> Binary F1 <C> 7-class Acc <C> 7-class F1 <C> Regression MAE <R> <C> [ITALIC] Embeddingonly <C> 64.6 <C> 0.60 <C> 48.17 <C> 0.43 <C> 0.595 <R> <C> [ITALIC] Emb+ [ITALIC] Lex <C> 62.8 <C> 0.57 <C> 45.6 <C> 0.41 <C> 0.61 <R> <C> [ITALIC] Emb+ [ITALIC] Vader <C> 64.2 <C> 0.59 <C> 45.4 <C> 0.42 <C> 0.61 <R> <C> [ITALIC] Emb+ [ITALIC] ELMO <C> 65.5 <C> 0.61 <C> 47.5 <C> 0.44 <C> 0.589 <R> <C> [ITALIC] Emb+ [ITALIC] Lex+ [ITALIC] Vader <C> 64.2 <C> 0.59 <C> 47.5 <C> 0.44 <C> 0.59 <R> <C> [ITALIC] Emb+ [ITALIC] ELMO+ [ITALIC] Lex <C> 64.6 <C> 0.58 <C> 48.7 <C> 0.45 <C> 0.576 <R> <C> [ITALIC] Emb+ [ITALIC] ELMO+ [ITALIC] Vader <C> [BOLD] 66.4 <C> [BOLD] 0.63 <C> [BOLD] 48.9  <C> [BOLD] 0.44 <C> [BOLD] 0.577 <R> <C> [ITALIC] Allfeatures <C> 66 <C> 0.62 <C> 47.9 <C> 0.43 <C> 0.58 <CAP> Table 3: Text Ablation Study
<R> <C> [BOLD] Model <C> [BOLD] Mean Opinion Score (MOS) <R> <C> Deep Voice 3 (Griffin-Lim) <C> 3.62±0.31 <R> <C> Deep Voice 3 (WORLD) <C> 3.63±0.27 <R> <C> Deep Voice 3 (WaveNet) <C> 3.78±0.30 <R> <C> Tacotron (WaveNet) <C> 3.78±0.34 <R> <C> Deep Voice 2 (WaveNet) <C> 2.74±0.35 <CAP> Table 2: Mean Opinion Score (MOS) ratings with 95% confidence intervals using different waveform synthesis methods. We use the crowdMOS toolkit (Ribeiro et al., 2011); batches of samples from these models were presented to raters on Mechanical Turk. Since batches contained samples from all models, the experiment naturally induces a comparison between the models.
<R> <C> [BOLD] Model <C> [BOLD] MOS (VCTK) <C> [BOLD] MOS (LibriSpeech) <R> <C> Deep Voice 3 (Griffin-Lim) <C> 3.01±0.29 <C> 2.37±0.24 <R> <C> Deep Voice 3 (WORLD) <C> 3.44±0.32 <C> 2.89±0.38 <R> <C> Deep Voice 2 (WaveNet) <C> 3.69±0.23 <C> - <R> <C> Tacotron (Griffin-Lim) <C> 2.07±0.31 <C> - <R> <C> Ground truth <C> 4.69±0.04 <C> 4.51±0.18 <CAP> Table 3: MOS ratings with 95% confidence intervals for audio clips from neural TTS systems on multi-speaker datasets. We also use crowdMOS toolkit; batches of samples including ground truth were presented to human raters. Multi-speaker Tacotron implementation and hyperparameters are based on Arık et al. (2017), which is a proof-of-concept implementation. Deep Voice 2 and Tacotron systems were not trained for the LibriSpeech dataset due to prohibitively long time required to optimize hyperparameters.
<R> <C> [BOLD] Methods <C> [BOLD] BBN Acc <C> [BOLD] BBN Ma-F1 <C> [BOLD] BBN Mi-F1 <C> [BOLD] OntoNotes Acc <C> [BOLD] OntoNotes Ma-F1 <C> [BOLD] OntoNotes Mi-F1 <C> [BOLD] Wiki Acc <C> [BOLD] Wiki Ma-F1 <C> [BOLD] Wiki Mi-F1 <R> <C> DZET+bert <C> 0.214 <C> 0.481 <C> 0.509 <C> 0.231 <C> 0.276 <C> 0.281 <C> 0.285 <C> 0.551 <C> 0.560 <R> <C> OTYPER [ITALIC] Wiki <C> 0.270 <C> 0.495 <C> 0.503 <C> 0.316 <C> 0.345 <C> 0.321 <C> – <C> – <C> – <R> <C> OTYPER [ITALIC] BBN <C> – <C> – <C> – <C> 0.025 <C> 0.051 <C> 0.054 <C> 0.053 <C> 0.115 <C> 0.115 <R> <C> OTYPER [ITALIC] OntoNotes <C> 0.236 <C> 0.511 <C> 0.479 <C> – <C> – <C> – <C> 0.004 <C> 0.156 <C> 0.168 <R> <C> [BOLD] MZET <C> [BOLD] 0.293 <C> [BOLD] 0.606 <C> [BOLD] 0.687 <C> [BOLD] 0.337 <C> [BOLD] 0.423 <C> [BOLD] 0.437 <C> [BOLD] 0.319 <C> [BOLD] 0.555 <C> [BOLD] 0.579 <CAP> Table 3: Evaluation on 3 benchmark datasets. OTYPER is designed for open type typing, which means, training on a dataset, but testing on other datasets. Here OTYPERWiki is training on Wiki data, but testing on BBN and OntoNotes. OTYPERBBN and OTYPEROntoNotes are similar with OTYPERWiki, but training on BBN and OntoNote, respectively.
<R> <C> [EMPTY] <C> [BOLD] All <C> [BOLD] Seen <C> [BOLD] Unseen <R> <C> [BOLD] Discourse Ordering <C> [BOLD] Discourse Ordering <C> [BOLD] Discourse Ordering <C> [BOLD] Discourse Ordering <R> <C> Random <C> 0.31 <C> 0.29 <C> 0.35 <R> <C> Majority <C> 0.48 <C> 0.51 <C> 0.44 <R> <C> GRU <C> 0.35 <C> 0.56 <C> 0.10 <R> <C> Transformer <C> 0.34 <C> 0.56 <C> 0.09 <R> <C> [BOLD] Text Structuring <C> [BOLD] Text Structuring <C> [BOLD] Text Structuring <C> [BOLD] Text Structuring <R> <C> Random <C> 0.29 <C> 0.29 <C> 0.30 <R> <C> Majority <C> 0.27 <C> 0.45 <C> 0.06 <R> <C> GRU <C> 0.39 <C> 0.63 <C> 0.13 <R> <C> Transformer <C> 0.36 <C> 0.59 <C> 0.12 <R> <C> [BOLD] Lexicalization <C> [BOLD] Lexicalization <C> [BOLD] Lexicalization <C> [BOLD] Lexicalization <R> <C> Random <C> 39.49 <C> 40.46 <C> 33.79 <R> <C> Majority <C> 44.82 <C> 45.65 <C> 39.43 <R> <C> GRU <C> 37.43 <C> 49.26 <C> 23.63 <R> <C> Transformer <C> 38.12 <C> 48.14 <C> 24.15 <R> <C> [BOLD] Referring Expression Generation <C> [BOLD] Referring Expression Generation <C> [BOLD] Referring Expression Generation <C> [BOLD] Referring Expression Generation <R> <C> [ITALIC] OnlyNames <C> 0.51 <C> 0.53 <C> 0.50 <R> <C> NeuralREG <C> 0.39 <C> 0.70 <C> 0.07 <CAP> Table 1: Accuracy of Discourse Ordering, Text Structuring and Referring Expression models, as well as BLEU score of Lexicalization approaches.
<R> <C> Dataset/Model <C> D_B/PAR <C> D_B/PAR+INV <C> SVM/PAR <R> <C> RMSE Score <C> 5.37 <C> 4.58 <C> 5.22 <CAP> Table 4: Test set results for MMSE score prediction, ‘D_B’ indicates our DistilBERT embedding model with LASSO linear model.
<R> <C> [BOLD] System <C> Dev  [BOLD] Accuracy <C> Dev  [BOLD] EER <C> Test  [BOLD] Accuracy <C> Test  [BOLD] EER <R> <C> DNN x-vector <C> 90.64 <C> 2.81 <C> 89.95 <C> 3.32 <R> <C> CLSTM <C> 91.20 <C> 2.55 <C> 90.82 <C> 2.91 <CAP> Table 1: Performance results for DNN x-vector and CLSTM.
<R> <C> [BOLD] TSM <C> [BOLD] TTDA <C> Dev  [BOLD] Accuracy <C> Dev  [BOLD] EER <C> Test  [BOLD] Accuracy <C> Test  [BOLD] EER <R> <C> Yes <C> No <C> 92.26 <C> 2.38 <C> 91.07 <C> 2.68 <R> <C> No <C> Yes <C> 93.58 <C> 1.84 <C> 92.69 <C> 2.19 <R> <C> Yes <C> Yes <C> 93.47 <C> 1.90 <C> 93.06 <C> 2.09 <CAP> Table 3: Performance results for CLSTM with TTDA.
<R> <C> (%) <C> Conversation EER <C> Conversation CER <C> Conversation Comb. <C> Amazon Review EER <C> Amazon Review CER <C> Amazon Review Comb. <R> <C> OSVM <C> 63.6 <C> - <C> - <C> 47.6 <C> - <C> - <R> <C> LSTM AutoEnc. <C> 48.0 <C> 78.4 <C> 79.5 <C> 45.4 <C> 29.3 <C> 38.6 <R> <C> Vanilla CNN <C> 26.4 <C> 76.8 <C> 77.6 <C> 47.7 <C> 34.4 <C> 42.8 <R> <C> Proto. Network <C> 26.9 <C> 32.5 <C> 44.5 <C> 46.5 <C> [BOLD] 7.3 <C> 47.6 <R> <C> O-Proto (L [ITALIC] in+L [ITALIC] gt) <C> 27.6 <C> 33.3 <C> 46.2 <C> 47.8 <C> 7.4 <C> 48.9 <R> <C> O-Proto (L [ITALIC] in+L [ITALIC] ood) <C> 24.5 <C> 30.1 <C> 41.2 <C> 24.7 <C> 9.7 <C> 30.1 <R> <C> O-Proto (all) <C> [BOLD] 24.1 <C> [BOLD] 29.6 <C> [BOLD] 40.8 <C> [BOLD] 24.0 <C> 9.1 <C> [BOLD] 29.1 <R> <C> Proto. with bilstm <C> 25.0 <C> 32.5 <C> 42.6 <C> 45.1 <C> [BOLD] 6.8 <C> 46.0 <R> <C> O-Proto with bilstm <C> [BOLD] 22.0 <C> [BOLD] 30.5 <C> [BOLD] 39.8 <C> [BOLD] 21.9 <C> 9.0 <C> [BOLD] 27.1 <CAP> Figure 2: Various β for Amazon data
<R> <C> # <C> [BOLD] Fusion Method <C> [BOLD] image to text R@1 <C> [BOLD] image to text R@5 <C> [BOLD] image to text R@10 <C> [BOLD] image to text Med r <C> [BOLD] text to image R@1 <C> [BOLD] text to image R@5 <C> [BOLD] text to image R@10 <C> [BOLD] text to image Med r <R> <C> 2.1 <C> Global Max Pooling <C> 21.4 <C> 50.0 <C> 64.3 <C> 5.0 <C> 19.6 <C> 47.4 <C> 62.3 <C> 6.0 <R> <C> 2.2 <C> Global Max Pooling (w/o random drop) <C> 22.0 <C> 50.3 <C> 65.2 <C> 5.0 <C> 17.9 <C> 47.6 <C> [BOLD] 63.5 <C> 6.0 <R> <C> 2.3 <C> Element-wise Adding <C> 21.2 <C> 50.6 <C> 64.4 <C> 5.0 <C> [BOLD] 20.8 <C> [BOLD] 49.4 <C> 63.1 <C> 6.0 <R> <C> 2.4 <C> Element-wise Adding (w/o random drop) <C> 21.8 <C> 50.5 <C> 65.6 <C> 5.0 <C> 17.2 <C> 46.3 <C> 62.0 <C> 6.0 <R> <C> 2.5 <C> Neural Net Fuser <C> 24.7 <C> 53.2 <C> 68.7 <C> 5.0 <C> 17.8 <C> 47.8 <C> 63.0 <C> 6.0 <R> <C> 2.6 <C> Neural Net Fuser (w/o random drop) <C> [BOLD] 25.6 <C> 54.0 <C> 67.8 <C> 5.0 <C> 20.2 <C> [BOLD] 49.4 <C> 62.7 <C> 6.0 <R> <C> 2.7 <C> Attention Fuser (from scratch) <C> 14.9 <C> 39.4 <C> 54.3 <C> 9.0 <C> 12.1 <C> 33.2 <C> 47.9 <C> 12.0 <R> <C> 2.8 <C> Attention Fuser (w/o random drop) <C> 22.8 <C> 51.7 <C> 66.6 <C> 5.0 <C> 19.3 <C> 47.2 <C> 62.8 <C> 6.0 <R> <C> 2.9 <C> Attention Fuser <C> 24.3 <C> [BOLD] 54.8 <C> [BOLD] 70.6 <C> 5.0 <C> 18.2 <C> 46.8 <C> 62.7 <C> 6.0 <CAP> Table 2. Comparing different multi-textual-source feature fusion methods.
<R> <C> Methods <C> F1 <R> <C> simple RNN  <C> 0.9411 <R> <C> CNN-CRF  <C> 0.9435 <R> <C> LSTM  <C> 0.9485 <R> <C> RNN-SOP  <C> 0.9489 <R> <C> Deep LSTM  <C> 0.9508 <R> <C> RNN-EM  <C> 0.9525 <R> <C> Bi-RNN with ranking loss  <C> 0.9547 <R> <C> Sequential CNN  <C> 0.9561 <R> <C> Encoder-labeler Deep LSTM  <C> 0.9566 <R> <C> BiLSTM-LSTM (focus)  <C> 0.9579 <R> <C> Neural Sequence Chunking  <C> [BOLD] 0.9586 <R> <C> DCMTL (Ours) <C> 0.9583–––––––∗ <CAP> Table 2: Comparison with published results on the ATIS dataset.
<R> <C> Models <C> Precision <C> Recall <C> F1 <R> <C> Basic BiLSTM-CRF <C> 0.4330 <C> 0.4275 <C> 0.4302 <R> <C> * Basic BiLSTM-CRF (cond. SEG) <C> 0.7948 <C> 0.7953 <C> 0.7950 <R> <C> * Basic BiLSTM-CRF (cond. NE) <C> 0.8985 <C> 0.8986 <C> 0.8985 <R> <C> Vanilla Multi-task <C> 0.3990 <C> 0.3941 <C> 0.3965 <R> <C> Hierarchy Multi-task <C> 0.4417 <C> 0.4494 <C> 0.4455 <R> <C> ** DCMTL ( [BOLD] - cascade) <C> 0.4654 <C> 0.4613 <C> 0.4633 <R> <C> ** DCMTL ( [BOLD] - residual) <C> 0.4923 <C> 0.4760 <C> 0.4840 <R> <C> DCMTL (full) <C> [BOLD] 0.5281 <C> [BOLD] 0.4941 <C> [BOLD] 0.5105 <CAP> Table 3: Results for slot filling task on the ECSA dataset. Columns with highlighted boldface are the best performance. Rows with * prefix are just results for our case study. Rows with ** prefix are results for ablation test.
<R> <C> Section <C> Algorithm <C> Public Elementary <C> Public Middle <C> Licensed Elementary <C> Licensed Middle <C> All Test Questions <C> Vector Type <C> Delta <R> <C> 4.1 <C> Multivex <C> 59.7 <C> 60.6 <C> 51.1 <C> 49.0 <C> 51.8 <C> sparse <C> 0.0 <R> <C> 4.1 <C> Lucene <C> 55.8 <C> 52.5 <C> 48.7 <C> 47.3 <C> 49.1 <C> sparse <C> −2.7 <R> <C> 4.2 <C> SVD 1 <C> 55.5 <C> 51.5 <C> 46.8 <C> 45.6 <C> 47.6 <C> dense <C> −4.3 <R> <C> 4.2 <C> SVD 2 <C> 56.2 <C> 51.8 <C> 48.3 <C> 46.9 <C> 48.8 <C> dense <C> −3.1 <R> <C> 4.3 <C> Word2vec 1 <C> 49.9 <C> 49.7 <C> 41.7 <C> 42.2 <C> 43.7 <C> dense <C> −8.2 <R> <C> 4.3 <C> Word2vec 2 <C> 51.9 <C> 52.6 <C> 45.3 <C> 44.8 <C> 46.5 <C> dense <C> −5.4 <CAP> Table 4: Accuracy of various algorithms on the test question sets. Delta is the accuracy of an algorithm on all test questions minus the accuracy of Multivex.
<R> <C> [EMPTY] <C> Laptop Dataset <C> Restaurant Dataset <R> <C> CRF <C> 74.01 <C> 69.56 <R> <C> IHS_RD <C> 74.55 <C> - <R> <C> NLANGP <C> - <C> 72.34 <R> <C> WDEmb <C> 75.16 <C> - <R> <C> LSTM <C> 75.71 <C> 70.35 <R> <C> BiLSTM-CNN-CRF <C> 77.8 <C> 72.5 <R> <C> RNCRF <C> 78.42 <C> - <R> <C> CMLA <C> 77.80 <C> - <R> <C> MIN <C> 77.58 <C> 73.44 <R> <C> THA & STN <C> 79.52 <C> 73.61 <R> <C> BERT <C> 77.19 <C> 71.52 <R> <C> DE-CNN <C> 81.59 <C> 74.37 <R> <C> DAN- - <C> 78.28 <C> 70.43 <R> <C> DAN- <C> 76.68 <C> 72.94 <R> <C> DAN <C> 80.24 <C> 73.35 <R> <C> Ctrl- - <C> 79.47 <C> 71.15 <R> <C> Ctrl- <C> 81.66 <C> 73.77 <R> <C> Ctrl <C> [BOLD] 82.73* <C> [BOLD] 75.64* <CAP> Table 3: Comparison results in F1 score: results are averaged scores of 5 runs. *indicates the result is statistically significant at the level of 0.01.
<R> <C> Order <C> WMT16 Ro → En BLEU <C> WMT16 Ro → En Ribes <C> WMT16 Ro → En Meteor <C> WMT16 Ro → En TER <C> WMT18 En → Tr BLEU <C> WMT18 En → Tr Ribes <C> WMT18 En → Tr Meteor <C> WMT18 En → Tr TER <C> KFTT En → Ja BLEU <C> KFTT En → Ja Ribes <C> KFTT En → Ja Meteor <C> KFTT En → Ja TER <R> <C> RND <C> 20.20 <C> 79.35 <C> 41.00 <C> 63.20 <C> 03.04 <C> 55.45 <C> 19.12 <C> 90.60 <C> 17.09 <C> 70.89 <C> 35.24 <C> 70.11 <R> <C> L2R <C> 31.82 <C> 83.37 <C> 52.19 <C> 50.62 <C> 14.85 <C> 69.20 <C> 33.90 <C> [BOLD] 71.56 <C> 30.87 <C> 77.72 <C> 48.57 <C> 59.92 <R> <C> R2L <C> 31.62 <C> 83.18 <C> 52.09 <C> 50.20 <C> 14.38 <C> 68.87 <C> 33.33 <C> 71.91 <C> 30.44 <C> [BOLD] 77.95 <C> 47.91 <C> 61.09 <R> <C> ODD <C> 30.11 <C> 83.09 <C> 50.68 <C> 50.79 <C> 13.64 <C> 68.85 <C> 32.48 <C> 72.84 <C> 28.59 <C> 77.01 <C> 46.28 <C> 60.12 <R> <C> BLT <C> 24.38 <C> 81.70 <C> 45.67 <C> 55.38 <C> 08.72 <C> 65.70 <C> 27.40 <C> 77.76 <C> 21.50 <C> 73.97 <C> 40.23 <C> 64.39 <R> <C> SYN <C> 29.62 <C> 82.65 <C> 50.25 <C> 52.14 <C> – <C> – <C> – <C> – <C> – <C> – <C> – <C> – <R> <C> CF <C> 30.25 <C> 83.22 <C> 50.71 <C> 50.72 <C> 12.04 <C> 67.61 <C> 31.18 <C> 74.75 <C> 28.91 <C> 77.06 <C> 46.46 <C> 61.56 <R> <C> RF <C> 30.23 <C> 83.29 <C> 50.72 <C> 51.73 <C> 12.10 <C> 67.44 <C> 30.72 <C> 73.40 <C> 27.35 <C> 76.40 <C> 45.15 <C> 62.14 <R> <C> SAO <C> [BOLD] 32.47 <C> [BOLD] 84.10 <C> [BOLD] 53.00 <C> [BOLD] 49.02 <C> [BOLD] 15.18 <C> [BOLD] 70.06 <C> [BOLD] 34.60 <C> [BOLD] 71.56 <C> [BOLD] 31.91 <C> 77.56 <C> [BOLD] 49.66 <C> [BOLD] 59.80 <CAP> Table 3: Results of translation experiments for three language pairs in different decoding orders. Scores are reported on the test set with four widely used evaluation metrics (BLEU↑, Meteor↑, TER↓ and Ribes↑). We do not report models trained with SYN order on En-Tr and En-Ja due to the lack of reliable dependency parsers. The statistical significance analysis6 between the outputs of SAO and L2R are conducted using BLEU score as the metric, and the p-values are ≤0.001 for all three language pairs.
<R> <C> Model Variants <C> dev <C> test <R> <C> Baseline L2R <C> 32.53 <C> 31.82 <R> <C> SAO default <C> [BOLD] 33.60 <C> [BOLD] 32.47 <R> <C> no bootstrap <C> 32.86 <C> 31.88 <R> <C> no bootstrap, no noise <C> 32.64 <C> 31.72 <R> <C> bootstrap from R2L order <C> 33.12 <C> 32.02 <R> <C> bootstrap from SYN order <C> 33.09 <C> 31.93 <R> <C> stern2019insertion - Uniform <C> 29.99 <C> 28.52 <R> <C> stern2019insertion - Binary <C> 32.27 <C> 30.66 <CAP> Table 5: Ablation study for machine translation on WMT16 Ro-En. Results of stern2019insertion are based on greedy decoding with the EOS penalty.
<R> <C> Model <C> Training (b/s) <C> Decoding (ms/s) <R> <C> L2R <C> 4.21 <C> 12.3 <R> <C> SAO ( [ITALIC] b=1) <C> 1.12 <C> 12.5 <R> <C> SAO ( [ITALIC] b=8) <C> 0.58 <C> 12.8 <CAP> Table 6: Comparison of the L2R order with SAO on running time, where b/s is batches per second and ms/s is ms per sentence. All experiments are conducted on 8 Nvidia V100 GPUs with 2000 tokens per GPU. We also compare beam sizes of 1 and 8 for SAO to search the best orders during training. We report the decoding speed of all three models based on greedy decoding.
<R> <C> [BOLD] Model <C> [BOLD] Unlabeled  [ITALIC] F1 <R> <C> Right Branching <C> 40.7 <R> <C> †DIORA <C> 60.6 <R> <C> ‡PRPN <C> 52.4 <R> <C> ‡PaLM-U <C> 42.0 <CAP> Table 3: Unlabeled unsupervised parsing F1 on WSJ-40. ‡ trains on the training split of WSJ, while † trains on AllNLI (htut2018grammar). The PRPN result is taken from Drozdov:2019.
<R> <C> Random <C> [BOLD] % Left Splits 39.3 <C> [BOLD] % Left Splits ±10.5 <C> [BOLD] % Right Splits 41.2 <C> [BOLD] % Right Splits ±8.8 <R> <C> PaLM-U <C> 1.1 <C> [EMPTY] <C> 85.6 <C> [EMPTY] <R> <C> Gold <C> 6.5 <C> [EMPTY] <C> 52.7 <C> [EMPTY] <CAP> Table 4: Percentage of left and right splits. The first row shows the numbers averaging over 25 differently randomly initialized PaLM models, without training. ± indicates standard deviation.
<R> <C> [EMPTY] <C> [BOLD] BLEU-2 <C> [BOLD] BLEU-4 <C> [BOLD] Entropy <C> [BOLD] Dist-1 <C> [BOLD] Dist-2 <R> <C> [BOLD] Test Set <C> [BOLD] - <C> [BOLD] - <C> 6.15 <C> 0.077 <C> 0.414 <R> <C> [BOLD] DED <C> 3.96 <C> 0.85 <C> 5.55 <C> 0.044 <C> 0.275 <R> <C> [BOLD] VED <C> 3.26 <C> 0.59 <C> 5.45 <C> 0.053 <C> 0.204 <R> <C> [BOLD] WED-D <C> [BOLD] 4.05 <C> [BOLD] 0.98 <C> 5.53 <C> 0.042 <C> 0.272 <R> <C> [BOLD] WED-S <C> 3.72 <C> 0.69 <C> [BOLD] 5.59 <C> [BOLD] 0.066 <C> [BOLD] 0.309 <CAP> Table 2: Results on dialog generation, where VED/WED hyperparameters for each model were chosen by Table 1.
<R> <C> LSTM Layers <C> 1 <R> <C> WE/LSTM size <C> 32 <R> <C> Attention <C> Bilinear <R> <C> Batch size <C> 64 <R> <C> Optimizer <C> SGD <R> <C> LR <C> 0.125 <R> <C> Max gradient norm <C> 5 <R> <C> Dropout <C> 0.1 <R> <C> Initialization <C> Uniform(-0.1, 0.1) <CAP> Table 1: Model details.
<R> <C> [ITALIC] ns <C> Δ [ITALIC] TS <C> Δ [ITALIC] WP <C> [ITALIC] J( [ITALIC] θ) [ITALIC] TS <C> [ITALIC] J( [ITALIC] θ) [ITALIC] WP <R> <C> 5 <C> 0.792 <C> 0.741 <C> 3.724 <C> 3.950 <R> <C> 10 <C> 0.801 <C> 0.657 <C> 3.476 <C> 3.831 <R> <C> 15 <C> 0.816 <C> 0.613 <C> 3.581 <C> 3.678 <R> <C> 25 <C> 0.715 <C> 0.674 <C> 3.246 <C> 3.497 <R> <C> 35 <C> 0.691 <C> 0.671 <C> 3.034 <C> 3.359 <R> <C> 50 <C> 0.697 <C> 0.640 <C> 3.003 <C> 3.268 <CAP> Table 1: Effect of the number of Hidden States on L(θH).
<R> <C> (%) <C> (%) <C> (%) <C> NED <C> Cov. <C> Matching <C> Matching <C> Matching <C> Grouping <C> Grouping <C> Grouping <C> Type <C> Type <C> Type <C> Token <C> Token <C> Token <C> Boundary <C> Boundary <C> Boundary <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> NED <C> Cov. <C> P <C> R <C> F <C> P <C> R <C> F <C> P <C> R <C> F <C> P <C> R <C> F <C> P <C> R <C> F <R> <C> Eng. <C> JHU <C> JHU <C> 21.9 <C> 16.3 <C> 39.4 <C> 1.6 <C> 3.1 <C> 21.4 <C> 84.6 <C> 33.3 <C> 6.2 <C> 1.9 <C> 2.9 <C> 5.5 <C> 0.4 <C> 0.8 <C> 44.1 <C> 4.7 <C> 8.6 <R> <C> Eng. <C> (A) <C> (4) BNF-1st, MR-0  [ITALIC] ψ=(7,50) <C> 87.5 <C> 100 <C> 1.4 <C> 0.5 <C> 0.8 <C> 3.6 <C> 18.7 <C> 6 <C> 4.2 <C> [BOLD] 11.9 <C> [BOLD] 6.2 <C> [BOLD] 8.3 <C> [BOLD] 15.7 <C> [BOLD] 10.9 <C> 35.2 <C> [BOLD] 84.6 <C> [BOLD] 49.8 <R> <C> Tso. <C> JHU <C> JHU <C> 12 <C> 16.2 <C> 69.1 <C> 0.3 <C> 0.5 <C> 52.1 <C> 77.4 <C> 62.2 <C> 3.2 <C> 1.4 <C> 2 <C> 2.6 <C> 0.5 <C> 0.8 <C> 22.3 <C> 5.6 <C> 8.9 <R> <C> Tso. <C> (B) <C> (8) BNF-2nd, MR-1  [ITALIC] ψ=(9,50) <C> 69.1 <C> 95 <C> 5.9 <C> [BOLD] 0.5 <C> [BOLD] 0.9 <C> 10.7 <C> 26.8 <C> 15.3 <C> 1.5 <C> 3.9 <C> 2.2 <C> 2.3 <C> 6.6 <C> 3.4 <C> 17.1 <C> 59.1 <C> 26.6 <R> <C> Tso. <C> (C) <C> (5) BNF-1st, MR-1  [ITALIC] ψ=(13,300) <C> 60.2 <C> 96.1 <C> 9.7 <C> 0.4 <C> 0.8 <C> 13.5 <C> 12.7 <C> 13.1 <C> 1.8 <C> [BOLD] 4.7 <C> [BOLD] 2.5 <C> [BOLD] 3.9 <C> [BOLD] 9.1 <C> [BOLD] 5.4 <C> 21.2 <C> [BOLD] 62.1 <C> [BOLD] 31.6 <CAP> Table 2: Comparison of three typical example token sets selected out of all shown in Fig.4 with the JHU baseline. Those better than JHU baseline are in bold.
<R> <C> [BOLD] Word hilton <C> [BOLD] Entity Type /building/hotel <C> [BOLD] Sim 0.58 <C> [BOLD] Word located <C> [BOLD] Entity Type /location <C> [BOLD] Sim 0.47 <R> <C> hilton <C> /building/restaurant <C> 0.46 <C> located <C> /location/city <C> 0.44 <R> <C> hilton <C> /person/actor <C> 0.37 <C> located <C> /building <C> 0.40 <R> <C> gpx2 <C> /biology <C> 0.69 <C> directed <C> /person/director <C> 0.60 <R> <C> gpx2 <C> /product/software <C> 0.56 <C> directed <C> /art/film <C> 0.55 <R> <C> jrun <C> /product/software <C> 0.64 <C> in <C> /date <C> 0.58 <R> <C> jrun <C> /product/weapon <C> 0.23 <C> in <C> /location/city <C> 0.54 <R> <C> dammstadt <C> /location/city <C> 0.45 <C> won <C> /award <C> 0.53 <R> <C> dammstadt <C> /location/railway <C> 0.44 <C> won <C> /event/sports_event <C> 0.53 <CAP> Table 1: Topmost similar entity types to a few single-word mentions (left table) and non-entity words (right table).
<R> <C> [BOLD] Model <C> [BOLD] BC <C> [BOLD] BN <C> [BOLD] MZ <C> [BOLD] NW <C> [BOLD] TC <C> [BOLD] WB <R> <C>  <C> 78.66 <C> 87.29 <C> 82.45 <C> 85.50 <C> 67.27 <C> 72.56 <R> <C>  <C> 78.88 <C> 87.39 <C> 82.46 <C> 87.60 <C> 72.68 <C> 76.17 <R> <C>  <C> 85.23 <C> 89.93 <C> 84.45 <C> 88.39 <C> 72.39 <C> 78.38 <R> <C> [BOLD] This work <C> [BOLD] 86.33 <C> [BOLD] 90.46 <C> [BOLD] 85.91 <C> [BOLD] 89.75 <C> [BOLD] 75.41 <C> [BOLD] 80.39 <CAP> Table 6: Per-genre F1 scores on OntoNotes (numbers taken from chiu2015named). BC = broadcast conversation, BN = broadcast news, MZ = magazine, NW = newswire, TC = telephone conversation, WB = blogs and newsgroups.
<R> <C> [EMPTY] <C> [BOLD] Dataset <C> [BOLD] Female Count <C> [BOLD] Male Count <C> [BOLD] Female Pct <C> [BOLD] Male Pct <R> <C> [EMPTY] <C> Census <C> 67,698 <C> 41,475 <C> 62% <C> 38% <R> <C> Train <C> CoNLL 2003 <C> 1,810 <C> 2,506 <C> 42% <C> 58% <R> <C> Train <C> OntoNotes5 <C> 2,758 <C> 3,832 <C> 42% <C> 58% <R> <C> Dev <C> CoNLL 2003 <C> 962 <C> 1,311 <C> 42% <C> 58% <R> <C> Dev <C> OntoNotes5 <C> 1,159 <C> 1,524 <C> 43% <C> 57% <R> <C> Test <C> CoNLL 2003 <C> 879 <C> 1,228 <C> 42% <C> 58% <R> <C> Test <C> OntoNotes5 <C> 828 <C> 1,068 <C> 44% <C> 56% <CAP> Table 4: Percentage of female and male names from the census data appearing in CoNLL 2003 and OntoNotes datasets with their corresponding counts. Both datasets fail to reflect the variety of female names.
<R> <C> T <C> uni-MS <C> Self <C> uni-Self <C> Tri <C> Tri-D <R> <C> B <C> 79.46 <C> 79.60 <C> 79.46 <C> [BOLD] 79.61 <C> 79.51 <R> <C> D <C> 82.32 <C> [BOLD] 82.49 <C> 82.35 <C> 82.35 <C> 82.35 <R> <C> E <C> 84.93 <C> 84.97 <C> 84.93 <C> [BOLD] 84.99 <C> 84.93 <R> <C> K <C> 87.17 <C> 87.18 <C> 87.17 <C> 87.15 <C> [BOLD] 87.23 <CAP> Table 4: Classification accuracies (%) for semi-supervised methods on Chen2012.
<R> <C> T <C> uni-MS <C> Self <C> PL <C> Att <R> <C> B <C> 79.46 <C> 79.60 <C> 79.57 <C> [BOLD] 79.68 <R> <C> D <C> 82.32 <C> 82.49 <C> 82.71 <C> [BOLD] 82.96 <R> <C> E <C> 84.93 <C> 84.97 <C> [BOLD] 85.30 <C> [BOLD] 85.30 <R> <C> K <C> 87.17 <C> 87.18 <C> 87.30 <C> [BOLD] 87.48 <CAP> Table 5: Classification accuracies (%) across different steps of the proposed method, evaluated on Chen2012.
<R> <C> [BOLD] Model-Dataset <C> [BOLD] Full <C> [BOLD] Easy <C> [BOLD] Hard <R> <C> [BOLD] Level A <C> [BOLD] Level A <C> [BOLD] Level A <C> [BOLD] Level A <R> <C> BERT- [ITALIC] OLID <C> 0.914 <C> 0.987 <C> 0.524 <R> <C> + [ITALIC] SOLID <C> [BOLD] 0.916 <C> [BOLD] 0.989 <C> [BOLD] 0.531 <R> <C> FastText- [ITALIC] OLID <C> 0.865 <C> 0.944 <C> 0.538 <R> <C> + [ITALIC] SOLID <C> [BOLD] 0.871 <C> [BOLD] 0.949 <C> [BOLD] 0.539 <R> <C> [BOLD] Level B <C> [BOLD] Level B <C> [BOLD] Level B <C> [BOLD] Level B <R> <C> BERT- [ITALIC] OLID <C> 0.568 <C> 0.637 <C> 0.516 <R> <C> + [ITALIC] SOLID <C> [BOLD] 0.627 <C> [BOLD] 0.722 <C> [BOLD] 0.572 <R> <C> FastText- [ITALIC] OLID <C> 0.374 <C> 0.457 <C> 0.314 <R> <C> + [ITALIC] SOLID <C> [BOLD] 0.494 <C> [BOLD] 0.597 <C> [BOLD] 0.434 <R> <C> [BOLD] Level C <C> [BOLD] Level C <C> [BOLD] Level C <C> [BOLD] Level C <R> <C> BERT- [ITALIC] OLID <C> 0.635 <C> 0.638 <C> [BOLD] 0.632 <R> <C> + [ITALIC] SOLID <C> [BOLD] 0.670 <C> [BOLD] 0.721 <C> 0.619 <R> <C> FastText- [ITALIC] OLID <C> 0.372 <C> 0.364 <C> 0.381 <R> <C> + [ITALIC] SOLID <C> [BOLD] 0.537 <C> [BOLD] 0.542 <C> [BOLD] 0.522 <CAP> Table 8: Experiments on the SOLID Analysis dataset, as well as on the easy and on the hard subsets thereof. Shown are macro-F1 scores.
<R> <C> Language <C> Family <C> Avg. Accuracy <C> Lexemes <C> Forms <C> Avg. Forms/Lexeme <R> <C> Albanian <C> Indo-European <C> 0.83 <C> 537 <C> 26993 <C> 50.4 <R> <C> Arabic <C> Semitic <C> 0.63 <C> 3559 <C> 89879 <C> 25.5 <R> <C> Armenian <C> Indo-European <C> 0.95 <C> 4614 <C> 144841 <C> 31.4 <R> <C> [BOLD] Basque <C> [BOLD] Isolate <C> [BOLD] 0.01 <C> [BOLD] 26 <C> [BOLD] 10382 <C> [BOLD] 441.9 <R> <C> Bulgarian <C> Slavic <C> 0.94 <C> 2042 <C> 36007 <C> 17.7 <R> <C> Czech <C> Slavic <C> 0.92 <C> 4470 <C> 61251 <C> 13.8 <R> <C> Danish <C> Germanic <C> 0.65 <C> 2580 <C> 19968 <C> 7.8 <R> <C> Dutch <C> Germanic <C> 0.94 <C> 3932 <C> 20680 <C> 5.3 <R> <C> English <C> Germanic <C> 0.95 <C> 9915 <C> 40210 <C> 4.1 <R> <C> Estonian <C> Uralic <C> 0.79 <C> 817 <C> 31711 <C> 38.9 <R> <C> French <C> Romance <C> 0.86 <C> 5378 <C> 195638 <C> 37.4 <R> <C> German <C> Germanic <C> 0.92 <C> 14739 <C> 69190 <C> 4.7 <R> <C> Hebrew <C> Semitic <C> 0.78 <C> 492 <C> 11240 <C> 23.3 <R> <C> Hindi <C> Indo-Aryan <C> 0.74 <C> 254 <C> 26404 <C> 104.0 <R> <C> Irish <C> Celtic <C> 0.85 <C> 6527 <C> 69551 <C> 10.7 <R> <C> Italian <C> Romance <C> 0.99 <C> 6495 <C> 269908 <C> 41.9 <R> <C> Latvian <C> Baltic <C> 0.97 <C> 5347 <C> 60146 <C> 11.9 <R> <C> Persian <C> Iranian <C> 0.70 <C> 271 <C> 26336 <C> 98.3 <R> <C> Polish <C> Slavic <C> 0.93 <C> 8317 <C> 106914 <C> 13.0 <R> <C> Portuguese <C> Romance <C> 0.98 <C> 2621 <C> 138372 <C> 52.9 <R> <C> Romanian <C> Romance <C> 0.78 <C> 3409 <C> 51670 <C> 15.3 <R> <C> Russian <C> Slavic <C> 0.95 <C> 19991 <C> 243748 <C> 12.2 <R> <C> Spanish <C> Romance <C> 0.97 <C> 3904 <C> 232676 <C> 59.9 <R> <C> Swedish <C> Germanic <C> 0.89 <C> 6451 <C> 43118 <C> 6.7 <R> <C> Turkish <C> Turkic <C> 0.85 <C> 2697 <C> 150477 <C> 55.9 <R> <C> Ukrainian <C> Slavic <C> 0.86 <C> 1426 <C> 13844 <C> 9.8 <R> <C> [BOLD] Urdu <C> [BOLD] Indo-Aryan <C> [BOLD] 0.38 <C> [BOLD] 180 <C> [BOLD] 5581 <C> [BOLD] 31.0 <R> <C> [BOLD] Welsh <C> [BOLD] Celtic <C> [BOLD] 0.41 <C> [BOLD] 179 <C> [BOLD] 9083 <C> [BOLD] 50.8 <CAP> Table 3: Accuracy per language.
<R> <C> [EMPTY] <C> Content P <C> Content R <C> Content F <C> Content + Social P <C> Content + Social R <C> Content + Social F <R> <C> es ♢ <C> 92.64 <C> 95.69 <C> 94.14 <C> 93.55 <C> 95.89 <C> [BOLD] 94.70 <R> <C> pt ♠ <C> 89.81 <C> 92.58 <C> 91.17 <C> 94.87 <C> 92.52 <C> [BOLD] 93.68 <R> <C> ca ♢ <C> 81.14 <C> 87.19 <C> 84.06 <C> 85.22 <C> 90.17 <C> [BOLD] 87.62 <R> <C> en <C> 77.42 <C> 76.18 <C> [BOLD] 76.79 <C> 77.86 <C> 70.53 <C> 74.01 <R> <C> gl ♠ <C> 56.93 <C> 52.93 <C> 54.85 <C> 65.15 <C> 50.35 <C> [BOLD] 56.80 <R> <C> eu <C> 92.41 <C> 76.29 <C> [BOLD] 83.58 <C> 94.41 <C> 68.01 <C> 79.06 <R> <C> amb <C> 100.00 <C> 89.56 <C> [BOLD] 94.49 <C> 100.00 <C> 85.54 <C> 92.21 <R> <C> und <C> 66.67 <C> 10.98 <C> 18.85 <C> 45.06 <C> 28.54 <C> [BOLD] 34.95 <R> <C> avg <C> 82.13 <C> 72.67 <C> 74.74 <C> 82.01 <C> 72.69 <C> [BOLD] 76.63 <CAP> Table 1: Experimental results. ♢/♠ are similar pairs.
<R> <C> R <C> T <C> S <C> B <R> <C> T <C> 0.0 <C> 221591 <C> 69178 <R> <C> S <C> 221591 <C> 0.0 <C> 68772 <R> <C> B <C> 69178 <C> 68772 <C> 0.0 <CAP> Table 6. Mean squared error between RT, RS, and RB.
<R> <C> [BOLD] Dialogs <C> #  [BOLD] turns <C> [BOLD] avg words per turn <C> #  [BOLD] Cycle <C> [BOLD] avg turns per cycle <R> <C> Original <C> 61 <C> 6.5 <C> 6 <C> 2.5 <R> <C> Structure <C> 65 <C> 6.5 <C> 5 <C> 2.6 <R> <C> Content <C> 108 <C> 4.2 <C> 9 <C> 6.9 <R> <C> Structure+Content <C> 75 <C> 5.7 <C> 6 <C> 2.8 <CAP> Table 9. Structure Metrics calculated from dialogs used as example. The original and the most similar based on structure, content and structure and content respectively.
<R> <C> [EMPTY] <C> Model <C> Test accuracy (%) <R> <C> 1 <C> VQA baseline <C> 44.68 <R> <C> 2 <C> VQA + MFB baseline <C> 44.94 <R> <C> 3 <C> VTQA (EF+LF+AR) <C> 46.86 <CAP> Table 1: Our VTQA model significantly outperforms (p< 0.001) the strong baseline VQA model (we do not apply MFB to our VTQA model, since it does not work for the VTQA model).
<R> <C> Model <C> Fully aaa quantized <C> dev clean <C> dev other <C> test clean <C> test other <R> <C> Full-precision <C> ✗ <C> 5.6 <C> [BOLD] 14.2 <C> 5.5 <C> [BOLD] 14.8 <R> <C> Post-training quant <C> ✓ <C> 5.6 <C> 14.6 <C> 5.6 <C> 15.1 <R> <C> Quant-aware training <C> ✓ <C> [BOLD] 5.4 <C> 14.5 <C> 5.5 <C> 15.2 <CAP> Table 3: Quantization results of our proposed model (no positional encodings or decoder-side convolutions).
<R> <C> Task <C> Dev Charge <C> Dev Charge <C> Dev Article <C> Dev Article <C> Dev Term <C> Test Charge <C> Test Charge <C> Test Article <C> Test Article <C> Test Term <R> <C> Metrics <C> MiF <C> MaF <C> MiF <C> MaF <C> Dis <C> MiF <C> MaF <C> MiF <C> MaF <C> Dis <R> <C> TextCNN <C> 93.8 <C> 74.6 <C> 92.8 <C> [BOLD] 70.5 <C> 1.586 <C> 93.9 <C> 72.2 <C> 93.5 <C> 67.0 <C> 1.539 <R> <C> DPCNN <C> 94.7 <C> 72.2 <C> 93.9 <C> 68.8 <C> 1.448 <C> 94.9 <C> 72.1 <C> 94.6 <C> 69.4 <C> 1.390 <R> <C> LSTM <C> 94.7 <C> 71.2 <C> 93.9 <C> 66.5 <C> 1.456 <C> 94.3 <C> 66.0 <C> 94.7 <C> 70.7 <C> 1.467 <R> <C> BERT <C> 94.5 <C> 66.3 <C> 93.5 <C> 64.7 <C> [BOLD] 1.421 <C> 94.7 <C> 71.3 <C> 94.3 <C> 66.9 <C> 1.342 <R> <C> FactLaw <C> 79.5 <C> 25.4 <C> 79.8 <C> 24.9 <C> 1.721 <C> 76.9 <C> 35.0 <C> 78.1 <C> 30.8 <C> 1.683 <R> <C> TopJudge <C> [BOLD] 94.8 <C> [BOLD] 76.3 <C> [BOLD] 94.0 <C> 69.6 <C> 1.438 <C> [BOLD] 97.6 <C> [BOLD] 76.8 <C> [BOLD] 96.9 <C> [BOLD] 70.9 <C> [BOLD] 1.335 <R> <C> Gating Network <C> - <C> - <C> - <C> - <C> 1.604 <C> - <C> - <C> - <C> - <C> 1.553 <CAP> Table 4: Experimental results of judgment prediction on C-LJP. In this table, MiF and MaF denotes micro-F1 and macro-F1, and Dis denotes the log distance between prediction and ground truth.
<R> <C> Model <C> Dev <C> Test <R> <C> TF-IDF <C> 52.9 <C> 53.3 <R> <C> TextCNN <C> 62.5 <C> [BOLD] 69.9 <R> <C> BiDAF <C> 63.3 <C> 68.6 <R> <C> BERT <C> [BOLD] 64.3 <C> 66.8 <R> <C> ABCNN <C> 62.7 <C> [BOLD] 69.9 <R> <C> SMASH RNN <C> 64.2 <C> 65.8 <CAP> Table 5: Experimental results of SCM. The evaluation metric is accuracy.
<R> <C> [EMPTY] <C> KD-Questions Single <C> KD-Questions All <C> CA-Questions Single <C> CA-Questions All <C> All Single <C> All <R> <C> Unskilled Humans <C> 76.9 <C> 71.1 <C> 62.5 <C> 58.0 <C> 70.0 <C> 64.2 <R> <C> Skilled Humans <C> 80.6 <C> 77.5 <C> 86.8 <C> 84.7 <C> 84.1 <C> 81.1 <R> <C> BiDAF <C> 36.7 <C> 20.6 <C> 37.2 <C> 22.2 <C> 38.3 <C> 22.0 <R> <C> BERT <C> [BOLD] 38.0 <C> 21.2 <C> 38.9 <C> [BOLD] 23.7 <C> 39.7 <C> [BOLD] 22.3 <R> <C> Co-matching <C> 35.8 <C> 20.2 <C> 35.8 <C> 20.3 <C> 38.1 <C> 21.2 <R> <C> HAF <C> 36.6 <C> [BOLD] 21.4 <C> [BOLD] 42.5 <C> 19.8 <C> [BOLD] 42.6 <C> 21.2 <CAP> Table 6: Experimental results of JEC-QA. The evaluation metrics is accuracy. The performance of unskilled and skilled humans is collected from original paper.
<R> <C> A <C> role <C> B 78.2 <C> C 74.1 <C> D 78.7 <C> E 74.5 <C> avg 76.4 <C> plr 86.1 <R> <C> A <C> fxn <C> 81.5 <C> 84.3 <C> 88.0 <C> 81.5 <C> 83.8 <C> 90.3 <R> <C> B <C> role <C> [EMPTY] <C> 73.1 <C> 74.5 <C> 71.8 <C> 74.4 <C> 82.9 <R> <C> B <C> fxn <C> [EMPTY] <C> 77.3 <C> 81.0 <C> 74.1 <C> 78.5 <C> 83.8 <R> <C> C <C> role <C> [EMPTY] <C> [EMPTY] <C> 73.6 <C> 72.7 <C> 73.4 <C> 80.1 <R> <C> C <C> fxn <C> [EMPTY] <C> [EMPTY] <C> 83.3 <C> 80.6 <C> 81.4 <C> 88.0 <R> <C> D <C> role <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 73.1 <C> 75.0 <C> 84.7 <R> <C> D <C> fxn <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 81.0 <C> 83.3 <C> 91.7 <R> <C> E <C> role <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 73.0 <C> 83.3 <R> <C> E <C> fxn <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 79.3 <C> 86.1 <CAP> Table 7: Pairwise interannotator agreement rates, each annotator’s average agreement rate with others (“avg”), and each annotator’s rate of agreeing with the label chosen by the plurality of annotators (“plr”). Tokens for which there is no plurality (6 for both role and function) are included and counted as disagreement for all annotators. Figures are exact label match percentages.
<R> <C> [EMPTY] <C> [BOLD] 20ng <C> [BOLD] hs <C> [BOLD] ns200 <C> [BOLD] odp <C> [BOLD] rl <C> [BOLD] rest <C> [BOLD] sem <C> [BOLD] sent <R> <C> 1K training instances <C> 1K training instances <C> 1K training instances <C> 1K training instances <C> 1K training instances <C> 1K training instances <C> 1K training instances <C> 1K training instances <C> 1K training instances <R> <C> no wgt <C> 0.554 <C> [BOLD] 0.613 <C> 0.448 <C> [BOLD] 0.234 <C> [BOLD] 0.324 <C> 0.415 <C> [BOLD] 0.577 <C> [BOLD] 0.694 <R> <C> TF-IDF <C> 0.653 <C> 0.534 <C> 0.436 <C> 0.184 <C> 0.265 <C> 0.403 <C> 0.465 <C> 0.634 <R> <C> KLD <C> 0.668 <C> 0.543 <C> 0.440 <C> 0.179 <C> 0.264 <C> 0.400 <C> 0.452 <C> 0.620 <R> <C> TF-CR <C> [BOLD] 0.783 <C> 0.566 <C> [BOLD] 0.456 <C> 0.196 <C> 0.268 <C> [BOLD] 0.426 <C> 0.474 <C> 0.665 <R> <C> 2K training instances <C> 2K training instances <C> 2K training instances <C> 2K training instances <C> 2K training instances <C> 2K training instances <C> 2K training instances <C> 2K training instances <C> 2K training instances <R> <C> no wgt <C> 0.591 <C> [BOLD] 0.620 <C> 0.473 <C> [BOLD] 0.264 <C> [BOLD] 0.333 <C> 0.443 <C> [BOLD] 0.592 <C> [BOLD] 0.698 <R> <C> TF-IDF <C> 0.730 <C> 0.558 <C> 0.460 <C> 0.217 <C> 0.274 <C> 0.410 <C> 0.483 <C> 0.636 <R> <C> KLD <C> 0.734 <C> 0.578 <C> 0.463 <C> 0.209 <C> 0.263 <C> 0.431 <C> 0.489 <C> 0.631 <R> <C> TF-CR <C> [BOLD] 0.836 <C> 0.588 <C> [BOLD] 0.481 <C> 0.239 <C> 0.278 <C> [BOLD] 0.452 <C> 0.512 <C> 0.690 <R> <C> 5K training instances <C> 5K training instances <C> 5K training instances <C> 5K training instances <C> 5K training instances <C> 5K training instances <C> 5K training instances <C> 5K training instances <C> 5K training instances <R> <C> no wgt <C> 0.626 <C> [BOLD] 0.634 <C> 0.503 <C> 0.290 <C> [BOLD] 0.364 <C> 0.460 <C> [BOLD] 0.606 <C> 0.707 <R> <C> TF-IDF <C> 0.645 <C> 0.545 <C> 0.476 <C> 0.266 <C> 0.308 <C> 0.408 <C> 0.503 <C> 0.633 <R> <C> KLD <C> 0.646 <C> 0.590 <C> 0.491 <C> 0.262 <C> 0.296 <C> 0.459 <C> 0.537 <C> 0.642 <R> <C> TF-CR <C> [BOLD] 0.811 <C> 0.600 <C> [BOLD] 0.516 <C> [BOLD] 0.296 <C> 0.332 <C> [BOLD] 0.479 <C> 0.562 <C> [BOLD] 0.708 <R> <C> 10K training instances <C> 10K training instances <C> 10K training instances <C> 10K training instances <C> 10K training instances <C> 10K training instances <C> 10K training instances <C> 10K training instances <C> 10K training instances <R> <C> no wgt <C> 0.596 <C> [BOLD] 0.641 <C> 0.519 <C> 0.301 <C> [BOLD] 0.370 <C> 0.475 <C> [BOLD] 0.612 <C> 0.713 <R> <C> TF-IDF <C> 0.440 <C> 0.536 <C> 0.490 <C> 0.295 <C> 0.303 <C> 0.399 <C> 0.502 <C> 0.636 <R> <C> KLD <C> 0.475 <C> 0.606 <C> 0.524 <C> 0.300 <C> 0.318 <C> 0.475 <C> 0.549 <C> 0.651 <R> <C> TF-CR <C> [BOLD] 0.696 <C> 0.614 <C> [BOLD] 0.541 <C> [BOLD] 0.351 <C> 0.347 <C> [BOLD] 0.490 <C> 0.585 <C> [BOLD] 0.716 <R> <C> 40K training instances <C> 40K training instances <C> 40K training instances <C> 40K training instances <C> 40K training instances <C> 40K training instances <C> 40K training instances <C> 40K training instances <C> 40K training instances <R> <C> no wgt <C> 0.705 <C> [BOLD] 0.656 <C> 0.538 <C> 0.312 <C> 0.418 <C> 0.495 <C> [BOLD] 0.634 <C> 0.718 <R> <C> TF-IDF <C> 0.893 <C> 0.548 <C> 0.493 <C> 0.335 <C> 0.367 <C> 0.390 <C> 0.528 <C> 0.648 <R> <C> KLD <C> 0.860 <C> 0.632 <C> 0.570 <C> 0.349 <C> 0.368 <C> 0.505 <C> 0.577 <C> 0.660 <R> <C> TF-CR <C> [BOLD] 0.930 <C> 0.635 <C> [BOLD] 0.575 <C> [BOLD] 0.423 <C> [BOLD] 0.427 <C> [BOLD] 0.513 <C> 0.632 <C> [BOLD] 0.736 <R> <C> 90K training instances <C> 90K training instances <C> 90K training instances <C> 90K training instances <C> 90K training instances <C> 90K training instances <C> 90K training instances <C> 90K training instances <C> 90K training instances <R> <C> no wgt <C> 0.705 <C> [BOLD] 0.661 <C> 0.544 <C> 0.325 <C> 0.422 <C> 0.503 <C> 0.635 <C> 0.721 <R> <C> TF-IDF <C> 0.893 <C> 0.556 <C> 0.507 <C> 0.354 <C> 0.379 <C> 0.390 <C> 0.532 <C> 0.647 <R> <C> KLD <C> 0.860 <C> 0.643 <C> 0.586 <C> 0.362 <C> 0.364 <C> 0.516 <C> 0.577 <C> 0.663 <R> <C> TF-CR <C> [BOLD] 0.930 <C> 0.648 <C> [BOLD] 0.595 <C> [BOLD] 0.458 <C> [BOLD] 0.444 <C> [BOLD] 0.522 <C> [BOLD] 0.638 <C> [BOLD] 0.748 <CAP> Table 1. Comparison of results using different weighting schemes for varying sizes of training data.
<R> <C> [BOLD] Genre <C> [BOLD] GEN <C> [BOLD] GNZ <C> [BOLD] STA <C> [BOLD] EVT <R> <C> Impl. Information <C> 0.84 <C> 0.02 <C> 0.13 <C> 0.01 <R> <C> Microtexts <C> 0.64 <C> 0.05 <C> 0.24 <C> 0.02 <R> <C> Report <C> 0.03 <C> 0.04 <C> 0.54 <C> 0.39 <R> <C> TED Talk <C> 0.12 <C> 0.03 <C> 0.49 <C> 0.36 <R> <C> Fiction <C> 0.02 <C> 0.05 <C> 0.39 <C> 0.54 <CAP> Table 1: Distribution of the most frequent Semantic Clause Types among different genres (expressed as percentages)
<R> <C> [EMPTY] <C> [EMPTY] <C> 0→es <C> pt→es orig <C> pt→es ciph <C> ar→es orig <C> ar→es ciph <R> <C> 50 <C> acc <C> 0.00 <C> 0.48 <C> 0.09 <C> 0.04 <C> 0.02 <R> <C> 50 <C> ED <C> 5.42 <C> 0.85 <C> 3.25 <C> 4.06 <C> 4.62 <R> <C> 200 <C> acc <C> 0.38 <C> 0.62 <C> 0.54 <C> 0.54 <C> 0.56 <R> <C> 200 <C> ED <C> 1.37 <C> 0.57 <C> 0.95 <C> 0.87 <C> 0.93 <CAP> Table 5: Results for ciphering. “0→es” and “orig” are original results, copied from Tab. 3; “ciph” is the result after the cipher has been applied.
<R> <C> [BOLD] Model <C> [BOLD] train <C> [BOLD] #M <C> [BOLD] en <C> [BOLD] nl <C> [BOLD] es <C> [BOLD] de <C> [BOLD] Avg <R> <C> Lample et al. ( 2016 ) <C> each <C> N <C> 90.74 <C> 81.74 <C> 85.75 <C> 78.76 <C> 84.25 <R> <C> Akbik et al. ( 2018 ) <C> each <C> N <C> [BOLD] 93.18 <C> 90.44 <C> - <C> [BOLD] 88.27 <C> - <R> <C> mBERT† <C> each <C> N <C> 91.97 <C> 90.94 <C> 87.38 <C> 82.82 <C> 88.28 <R> <C> mBERT† <C> en <C> 1 <C> 91.97 <C> 77.57 <C> 74.96 <C> 69.56 <C> 78.52 <R> <C> XLM-RBase <C> each <C> N <C> 92.25 <C> 90.39 <C> 87.99 <C> 84.60 <C> 88.81 <R> <C> XLM-RBase <C> en <C> 1 <C> 92.25 <C> 78.08 <C> 76.53 <C> 69.60 <C> 79.11 <R> <C> XLM-RBase <C> all <C> 1 <C> 91.08 <C> 89.09 <C> 87.28 <C> 83.17 <C> 87.66 <R> <C> [BOLD] XLM-R <C> each <C> N <C> 92.92 <C> [BOLD] 92.53 <C> [BOLD] 89.72 <C> 85.81 <C> 90.24 <R> <C> [BOLD] XLM-R <C> en <C> 1 <C> 92.92 <C> 80.80 <C> 78.64 <C> 71.40 <C> 80.94 <R> <C> [BOLD] XLM-R <C> all <C> 1 <C> 92.00 <C> 91.60 <C> 89.52 <C> 84.60 <C> 89.43 <CAP> Table 2: Results on named entity recognition on CoNLL-2002 and CoNLL-2003 (F1 score). Results with † are from Wu and Dredze (2019). Note that mBERT and XLM-R do not use a linear-chain CRF, as opposed to Akbik et al. (2018) and Lample et al. (2016).
<R> <C> [BOLD] Model <C> [BOLD] #lgs <C> [BOLD] MNLI-m/mm <C> [BOLD] QNLI <C> [BOLD] QQP <C> [BOLD] SST <C> [BOLD] MRPC <C> [BOLD] STS-B <C> [BOLD] Avg <R> <C> BERTLarge† <C> 1 <C> 86.6/- <C> 92.3 <C> 91.3 <C> 93.2 <C> 88.0 <C> 90.0 <C> 90.2 <R> <C> XLNetLarge† <C> 1 <C> 89.8/- <C> 93.9 <C> 91.8 <C> 95.6 <C> 89.2 <C> 91.8 <C> 92.0 <R> <C> RoBERTa† <C> 1 <C> 90.2/90.2 <C> 94.7 <C> 92.2 <C> 96.4 <C> 90.9 <C> 92.4 <C> 92.8 <R> <C> XLM-R <C> 100 <C> 88.9/89.0 <C> 93.8 <C> 92.3 <C> 95.0 <C> 89.5 <C> 91.2 <C> 91.8 <CAP> Table 4: GLUE dev results. Results with † are from Liu et al. (2019). We compare the performance of XLM-R to BERTLarge, XLNet and RoBERTa on the English GLUE benchmark.
<R> <C> [BOLD] Model <C> [BOLD] D <C> [BOLD] #vocab <C> [BOLD] en <C> [BOLD] fr <C> [BOLD] de <C> [BOLD] ru <C> [BOLD] zh <C> [BOLD] sw <C> [BOLD] ur <C> [BOLD] Avg <R> <C> [ITALIC] Monolingual baselines <C> [ITALIC] Monolingual baselines <C> [ITALIC] Monolingual baselines <C> [ITALIC] Monolingual baselines <C> [ITALIC] Monolingual baselines <C> [ITALIC] Monolingual baselines <C> [ITALIC] Monolingual baselines <C> [ITALIC] Monolingual baselines <C> [ITALIC] Monolingual baselines <C> [ITALIC] Monolingual baselines <C> [ITALIC] Monolingual baselines <R> <C> BERT <C> Wiki <C> 40k <C> 84.5 <C> 78.6 <C> 80.0 <C> 75.5 <C> 77.7 <C> 60.1 <C> 57.3 <C> 73.4 <R> <C> BERT <C> CC <C> 40k <C> 86.7 <C> 81.2 <C> 81.2 <C> 78.2 <C> 79.5 <C> 70.8 <C> 65.1 <C> 77.5 <R> <C> [ITALIC] Multilingual models (cross-lingual transfer) <C> [ITALIC] Multilingual models (cross-lingual transfer) <C> [ITALIC] Multilingual models (cross-lingual transfer) <C> [ITALIC] Multilingual models (cross-lingual transfer) <C> [ITALIC] Multilingual models (cross-lingual transfer) <C> [ITALIC] Multilingual models (cross-lingual transfer) <C> [ITALIC] Multilingual models (cross-lingual transfer) <C> [ITALIC] Multilingual models (cross-lingual transfer) <C> [ITALIC] Multilingual models (cross-lingual transfer) <C> [ITALIC] Multilingual models (cross-lingual transfer) <C> [ITALIC] Multilingual models (cross-lingual transfer) <R> <C> XLM-7 <C> Wiki <C> 150k <C> 82.3 <C> 76.8 <C> 74.7 <C> 72.5 <C> 73.1 <C> 60.8 <C> 62.3 <C> 71.8 <R> <C> XLM-7 <C> CC <C> 150k <C> 85.7 <C> 78.6 <C> 79.5 <C> 76.4 <C> 74.8 <C> 71.2 <C> 66.9 <C> 76.2 <R> <C> [ITALIC] Multilingual models (translate-train-all) <C> [ITALIC] Multilingual models (translate-train-all) <C> [ITALIC] Multilingual models (translate-train-all) <C> [ITALIC] Multilingual models (translate-train-all) <C> [ITALIC] Multilingual models (translate-train-all) <C> [ITALIC] Multilingual models (translate-train-all) <C> [ITALIC] Multilingual models (translate-train-all) <C> [ITALIC] Multilingual models (translate-train-all) <C> [ITALIC] Multilingual models (translate-train-all) <C> [ITALIC] Multilingual models (translate-train-all) <C> [ITALIC] Multilingual models (translate-train-all) <R> <C> XLM-7 <C> Wiki <C> 150k <C> 84.6 <C> 80.1 <C> 80.2 <C> 75.7 <C> 78 <C> 68.7 <C> 66.7 <C> 76.3 <R> <C> XLM-7 <C> CC <C> 150k <C> [BOLD] 87.2 <C> [BOLD] 82.5 <C> [BOLD] 82.9 <C> [BOLD] 79.7 <C> [BOLD] 80.4 <C> [BOLD] 75.7 <C> [BOLD] 71.5 <C> [BOLD] 80.0 <CAP> Table 5: Multilingual versus monolingual models (BERT-BASE). We compare the performance of monolingual models (BERT) versus multilingual models (XLM) on seven languages, using a BERT-BASE architecture. We choose a vocabulary size of 40k and 150k for monolingual and multilingual models.
<R> <C> [EMPTY] <C> [BOLD] BestPlan <C> [BOLD] StrongNeural <R> <C> Expressed <C> 417 <C> 360 <R> <C> Omitted <C> 6 <C> 41 <R> <C> Wrong-lexicalization <C> 17 <C> 39 <R> <C> Over-generation <C> 3 <C> 29 <CAP> Table 2: Semantic faithfulness of each system regarding 440 RDF triplets from 139 input sets in the seen part of the manually evaluated test set.
<R> <C> dataset <C> dataset <C> number of sentences <C> average #corr/sent. <C> RB <C> over non-robust sent f-BLEU <C> over non-robust sent f-METEOR <C> NR <R> <C> WI+loc <C> A <C> 9K <C> 3.4 <C> 17.77 <C> 46.75 <C> 65.29 <C> 2.12 <R> <C> WI+loc <C> B <C> 10K <C> 2.6 <C> 21.17 <C> 54.72 <C> 70.80 <C> 2.39 <R> <C> WI+loc <C> C <C> 5.9K <C> 1.8 <C> 29.07 <C> 63.46 <C> 76.63 <C> 2.73 <R> <C> WI+loc <C> N <C> 500 <C> 1.8 <C> 28.80 <C> 64.79 <C> 77.35 <C> 3.23 <R> <C> nucle <C> nucle <C> 21.3K <C> 2.0 <C> 20.69 <C> 59.97 <C> 74.6 <C> 2.92 <R> <C> fce <C> fce <C> 20.7K <C> 2.4 <C> 20.48 <C> 50.45 <C> 67.49 <C> 2.43 <R> <C> jfleg <C> jfleg <C> 1.3K <C> 3.8 <C> 12.42 <C> 42.05 <C> 61.99 <C> 2.18 <R> <C> Lang8 <C> Lang8 <C> 149.5K <C> 2.4 <C> 16.06 <C> 37.15 <C> 58.89 <C> 2.20 <R> <C> ALL∖Lang8 <C> ALL∖Lang8 <C> 69K <C> 2.4 <C> 20.94 <C> 54.65 <C> 70.64 <C> 2.55 <R> <C> ALL <C> ALL <C> 218.5K <C> 2.4 <C> 17.60 <C> 42.65 <C> 62.59 <C> 2.55 <CAP> Table 1: Aggregate results across all datasets. As expected, the NMT system’s performance deteriorates as input noise increases. For all metrics except NR, higher scores are better.
<R> <C> Configuration <C> Parameter Setting <C> Parameter Setting <C> Parameter Setting <C> Parameter Setting <C> Wiki-102 Linear <C> Wiki-102 Linear <C> Wiki-102 S-Shaped Curve <C> Wiki-102 S-Shaped Curve <C> Wiki-102 Exponential Increase <C> Wiki-102 Exponential Increase <C> Wiki-102 Static <C> Wiki-102 Static <C> Penn-Treebank Linear <C> Penn-Treebank Linear <C> Penn-Treebank S-Shaped Curve <C> Penn-Treebank S-Shaped Curve <C> Penn-Treebank Exponential Increase <C> Penn-Treebank Exponential Increase <C> Penn-Treebank Static <C> Penn-Treebank Static <R> <C> [EMPTY] <C> [ITALIC] ϵs <C> [ITALIC] ϵe <C> [ITALIC] γs <C> [ITALIC] γe <C> Valid <C> Test <C> Valid <C> Test <C> Valid <C> Test <C> Valid <C> Test <C> Valid <C> Test <C> Valid <C> Test <C> Valid <C> Test <C> Valid <C> Test <R> <C> No Sampling <C> - <C> - <C> - <C> - <C> 140.28 <C> 128.78 <C> 140.28 <C> 128.78 <C> 140.28 <C> 128.78 <C> 140.28 <C> 128.78 <C> 76.25 <C> 71.81 <C> 76.25 <C> 71.81 <C> 76.25 <C> 71.81 <C> 76.25 <C> 71.81 <R> <C> TPRS-1 <C> 0 <C> 0 <C> 0 <C> 0.2 <C> 143.78 <C> 131.18 <C> 137.69 <C> 127.05 <C> 137.63 <C> 136.88 <C> 136.31 <C> 126.49 <C> 81.42 <C> 76.58 <C> 83.40 <C> 81.22 <C> 77.56 <C> 73.02 <C> 81.45 <C> 80.36 <R> <C> TPRS-2 <C> 0 <C> 0 <C> 0 <C> 0.3 <C> 154.93 <C> 144.02 <C> 146.92 <C> 137.07 <C> 137.11 <C> 125.41 <C> 137.10 <C> 125.95 <C> 96.91 <C> 94.30 <C> 88.79 <C> 86.17 <C> 77.63 <C> 72.80 <C> 91.73 <C> 84.48 <R> <C> TPRS-3 <C> 0 <C> 0 <C> 0 <C> 0.5 <C> 159.11 <C> 148.41 <C> 148.10 <C> 139.58 <C> 138.46 <C> 127.97 <C> 138.53 <C> 129.87 <C> 97.62 <C> 94.78 <C> 88.46 <C> 86.05 <C> 76.60 <C> 72.77 <C> 98.31 <C> 95.23 <R> <C> NNRS-1 <C> 0 <C> 0 <C> 0 <C> 0.2 <C> 142.53 <C> 130.45 <C> 137.08 <C> 126.82 <C> 136.81 <C> 136.11 <C> 135.06 <C> 136.02 <C> 80.91 <C> 76.70 <C> 83.17 <C> 79.62 <C> 76.00 <C> 72.19 <C> 82.23 <C> 80.03 <R> <C> NNRS-2 <C> 0 <C> 0 <C> 0 <C> 0.3 <C> 154.50 <C> 143.13 <C> 149.69 <C> 136.98 <C> 136.83 <C> 125.53 <C> 136.34 <C> 125.84 <C> 96.66 <C> 93.18 <C> 88.68 <C> 85.29 <C> 77.12 <C> 73.05 <C> 92.12 <C> 84.29 <R> <C> NNRS-3 <C> 0 <C> 0 <C> 0 <C> 0.5 <C> 158.30 <C> 147.13 <C> 148.02 <C> 137.34 <C> 137.56 <C> 127.61 <C> 132.62 <C> 121.50 <C> 96.95 <C> 93.15 <C> 88.21 <C> 84.47 <C> 75.91 <C> 72.46 <C> 97.55 <C> 94.79 <R> <C> SS-1 <C> 0 <C> 0.2 <C> 0 <C> 0 <C> 139.31 <C> 128.50 <C> 143.82 <C> 131.08 <C> 137.72 <C> 126.46 <C> 135.55 <C> 122.61 <C> 83.63 <C> 80.03 <C> 82.33 <C> 79.02 <C> 76.71 <C> 73.32 <C> 74.50 <C> 70.20 <R> <C> SS-2 <C> 0 <C> 0.3 <C> 0 <C> 0 <C> 137.78 <C> 126.00 <C> 138.39 <C> 126.80 <C> 135.89 <C> 125.03 <C> 131.29 <C> 121.52 <C> 94.74 <C> 79.82 <C> 84.42 <C> 80.03 <C> 76.43 <C> 73.46 <C> 74.56 <C> 70.25 <R> <C> SS-3 <C> 0 <C> 0.5 <C> 0 <C> 0 <C> 135.14 <C> 124.29 <C> 136.88 <C> 125.41 <C> 136.98 <C> 125.96 <C> 131.28 <C> 121.51 <C> 92.48 <C> 88.92 <C> 85.87 <C> 82.37 <C> 76.41 <C> 73.15 <C> 74.11 <C> 69.48 <R> <C> SS-4 <C> 0 <C> 0.8 <C> 0 <C> 0 <C> 140.97 <C> 130.00 <C> 141.13 <C> 129.99 <C> 138.09 <C> 127.23 <C> 134.32 <C> 122.86 <C> 92.94 <C> 88.85 <C> 87.29 <C> 84.27 <C> 76.22 <C> 72.98 <C> 74.02 <C> 70.23 <R> <C> SS-NNRS-1 <C> 0 <C> 0.2 <C> 0 <C> 0.2 <C> 139.32 <C> 128.50 <C> 141.57 <C> 129.67 <C> 137.73 <C> 126.47 <C> 135.55 <C> 122.60 <C> 83.62 <C> 80.03 <C> 81.99 <C> 77.86 <C> 76.71 <C> 73.31 <C> 74.50 <C> 70.20 <R> <C> SS-NNRS-2 <C> 0 <C> 0.3 <C> 0 <C> 0.3 <C> 137.78 <C> 126.01 <C> 147.34 <C> 135.61 <C> 136.02 <C> 125.73 <C> 137.73 <C> 126.47 <C> 95.39 <C> 92.18 <C> 87.97 <C> 84.64 <C> 75.43 <C> 72.46 <C> 74.64 <C> 70.43 <R> <C> SS-NNRS-3 <C> 0 <C> 0.5 <C> 0 <C> 0.2 <C> 135.14 <C> 124.29 <C> 149.00 <C> 137.97 <C> 135.82 <C> 124.72 <C> 130.95 <C> 120.76 <C> 95.90 <C> 92.96 <C> 88.56 <C> 84.96 <C> 74.83 <C> 70.95 <C> 72.89 <C> 69.06 <R> <C> SS-NNRS-4 <C> 0.2 <C> 0.5 <C> 0.2 <C> 0.5 <C> 150.97 <C> 138.59 <C> 146.01 <C> 134.67 <C> 135.88 <C> 126.02 <C> 123.84 <C> 121.98 <C> 96.78 <C> 93.20 <C> 88.56 <C> 87.06 <C> 76.41 <C> 73.15 <C> 74.42 <C> 69.88 <R> <C> SS-NNRS-5 <C> 0 <C> 0.5 <C> 0 <C> 0.5 <C> 136.22 <C> 125.70 <C> 151.39 <C> 138.44 <C> 137.02 <C> 125.84 <C> 132.17 <C> 121.86 <C> 97.12 <C> 93.90 <C> 90.37 <C> 86.25 <C> 76.02 <C> 72.31 <C> 74.08 <C> 70.79 <R> <C> SS-NNRS-6 <C> 0 <C> 0.8 <C> 0 <C> 0.2 <C> 148.96 <C> 130.01 <C> 141.14 <C> 129.99 <C> 138.09 <C> 127.24 <C> 134.32 <C> 122.89 <C> 95.91 <C> 92.87 <C> 81.89 <C> 78.54 <C> 74.74 <C> 70.64 <C> 74.02 <C> 70.23 <R> <C> SS-NNRS-7 <C> 0.2 <C> 0.8 <C> 0.2 <C> 0.5 <C> 155.17 <C> 148.21 <C> 149.58 <C> 137.83 <C> 135.45 <C> 124.82 <C> 131.09 <C> 121.43 <C> 96.77 <C> 93.64 <C> 88.27 <C> 85.04 <C> 76.35 <C> 72.74 <C> 74.43 <C> 70.12 <CAP> Table 1: Perplexity for Transition Probability Sampling (TPRS), Scheduled Sampling (SS) and Nearest-Neighbor Replacement Sampling (NNRS) with linear, s-shaped curve and exponential sampling functions for a standard 2-hidden layer LSTM
<R> <C> BoW <C> Seq2Seq <C> InferSent <C> Fu et al. ( 2018 ) <C> ADNet <R> <C> 80.82 <C> 74.68 <C> [BOLD] 83.17 <C> 78.88 <C> 81.38 <CAP> Table 3: F1 scores on the task of paraphrase detection using the SentEval toolkit (Conneau et al., 2017)
<R> <C> beam size=3 <C> beam size=3 Bleu3 <C> beam size=3 Rouge <C> beam size=3 Meteor <C> beam size=3 CIDEr <R> <C> enc-dec <C> 19.58 <C> 29.23 <C> 33.02 <C> 4.65 <R> <C> enc-attn-dec <C> 19.73 <C> 28.94 <C> 32.98 <C> 4.96 <R> <C> h-attn <C> 20.53 <C> 29.82 <C> 33.81 <C> 6.84 <R> <C> h-attn-rank <C> [BOLD] 20.78 <C> [BOLD] 29.82 <C> [BOLD] 33.94 <C> [BOLD] 7.38 <R> <C> h-(gd)attn-rank <C> 21.02 <C> 29.53 <C> 34.12 <C> 7.51 <CAP> Table 1: Story generation evaluation.
<R> <C> Model <C> Laptops 3-way <C> Laptops Binary <C> Restaurants 3-way <C> Restaurants Binary <R> <C> TD-LSTM <C> 62.38 <C> 79.31 <C> 69.73 <C> 84.41 <R> <C> AT-LSTM <C> 65.83 <C> 78.25 <C> 74.37 <C> 84.74 <R> <C> ATAE-LSTM <C> 60.34 <C> 74.20 <C> 70.71 <C> 84.52 <R> <C> AF-LSTM <C> 68.81 <C> 83.58 <C> 75.44 <C> 87.78 <R> <C> CNN <C> 68.65 <C> 85.50 <C> 77.95 <C> 89.50 <R> <C> PF-CNN <C> [BOLD] 70.06 <C> [BOLD] 86.35 <C> [BOLD] 79.20 <C> 90.15 <R> <C> PG-CNN <C> 69.12 <C> 86.14 <C> 78.93 <C> [BOLD] 90.58 <CAP> Table 2: Comparisons results with baselines. We use accuracy to measure the performance. Performances of baselines are cited from Tay et al. (2018).
<R> <C> Dataset <C> Method <C> B <C> R <C> M <C> C <R> <C> MSR- VTT <C> RecNet <C> 39.1 <C> 59.3 <C> 26.6 <C> 42.7 <R> <C> MSR- VTT <C> HRL <C> 41.3 <C> 61.7 <C> 28.7 <C> 48.0 <R> <C> MSR- VTT <C> Dense Cap <C> 41.4 <C> 61.1 <C> 28.3 <C> 48.9 <R> <C> MSR- VTT <C> HACA <C> 43.5 <C> 61.8 <C> [BOLD] 29.5 <C> 49.7 <R> <C> MSR- VTT <C> MM-TGM <C> 44.3 <C> - <C> 29.3 <C> 49.2 <R> <C> [EMPTY] <C> Ours <C> [BOLD] 44.6 <C> [BOLD] 62.6 <C> [BOLD] 29.5 <C> [BOLD] 49.8 <R> <C> MSVD <C> SCN <C> 50.2 <C> - <C> 33.4 <C> 77.7 <R> <C> MSVD <C> TDDF <C> 45.8 <C> 69.7 <C> 33.3 <C> 73.0 <R> <C> [EMPTY] <C> LSTM-TSA <C> 52.8 <C> - <C> 33.5 <C> 74 <R> <C> [EMPTY] <C> RecNet <C> 52.3 <C> 69.8 <C> 34.1 <C> 80.3 <R> <C> [EMPTY] <C> MM-TGM <C> 48.7 <C> - <C> 34.3 <C> 80.4 <R> <C> [EMPTY] <C> Ours <C> [BOLD] 52.9 <C> [BOLD] 72.0 <C> [BOLD] 35.5 <C> [BOLD] 86.1 <CAP> Table 2: Evaluation results of video captioning, where B, R, M, C denote BLEU4, ROUGE, METEOR, CIDEr, respectively, and Ours denotes L-HOCA-UBT.
<R> <C> Method <C> Space (G) <C> Training Time (s) <R> <C> HOCA-U <C> 4.1 <C> 16418 <R> <C> HOCA-UBT <C> 9.7 <C> 37040 <R> <C> L-HOCA-UBT <C> 5.6 <C> 24615 <CAP> Table 3: Computing cost of different methods, where the “space” denotes the memory space requirement and the “training time” denotes the total time for training. We evaluate them on MSR-VTT. Note that the metrics belong to the whole model, not only the attention module.
<R> <C> [BOLD] No. <C> [BOLD] Features <C> [BOLD] # of features <C> [BOLD] Naive Bayes  [BOLD] Presence <C> [BOLD] Naive Bayes  [BOLD] Frequency <C> [BOLD] SVM  [BOLD] Presence <C> [BOLD] SVM  [BOLD] Frequency <R> <C> 1 <C> Unigram(Non-Neg) <C> 1042107 <C> 0.807 <C> 0.677 <C> [BOLD] 0.859 <C> 0.747 <R> <C> 2 <C> Unigram(Neg) <C> 1042107 <C> 0.795 <C> 0.684 <C> [BOLD] 0.843 <C> 0.753 <R> <C> 3 <C> Bigram <C> 1040507 <C> 0.744 <C> 0.687 <C> [BOLD] 0.815 <C> 0.781 <R> <C> 4 <C> Trigram <C> 1038907 <C> 0.704 <C> 0.672 <C> [BOLD] 0.741 <C> 0.734 <R> <C> 5 <C> Adjective/Adverb <C> 503952 <C> 0.737 <C> 0.699 <C> 0.772 <C> [BOLD] 0.773 <R> <C> 6 <C> Adjective <C> 104164 <C> 0.774 <C> 0.716 <C> [BOLD] 0.802 <C> 0.792 <R> <C> 7 <C> Polarized Bigram <C> 625734 <C> 0.776 <C> 0.708 <C> [BOLD] 0.794 <C> 0.783 <R> <C> 8 <C> Polarized Unigram <C> 156671 <C> 0.558 <C> 0.628 <C> 0.560 <C> [BOLD] 0.735 <R> <C> 9 <C> Adjective/Adverb Trigram <C> 438120 <C> 0.653 <C> 0.639 <C> [BOLD] 0.702 <C> [BOLD] 0.702 <CAP> Table 2: Average five-fold cross-validation accuracies. Boldface: best performance for a given setting(row). Neg means apply negation tagging and Non-Neg means without negation tagging
<R> <C> Strategy <C> ShARC <C> Daily Dialog <R> <C> one step greedy <C> 22.9 <C> 09.3 <R> <C> lowest entropy <C> 40.3 <C> 16.8 <R> <C> highest probability <C> [BOLD] 50.9 <C> 16.4 <R> <C> left-to-right <C> 46.2 <C> [BOLD] 23.8 <CAP> Table 6: BLEU-4 using various sequence generation strategies for BiSon on ShARC and Daily Dialog.
<R> <C> Dataset <C> [ITALIC] α1 <C> [ITALIC] α2 <C> [ITALIC] α3 <R> <C> ShARC <C> 92.6±3.1 <C> 5.2±2.4 <C> 2.2±1.8 <R> <C> DD <C> 97.0±2.5 <C> 2.3±2.3 <C> 0.7±0.4 <R> <C> [EMPTY] <C> [EMPTY] <C> ¯ [ITALIC] α2 <C> ¯ [ITALIC] α3 <R> <C> ShARC <C> - <C> 71.7±13.7 <C> 28.3±13.7 <R> <C> DD <C> - <C> 70.2±14.5 <C> 29.8±14.5 <CAP> Table 7: Average attention weights and standard deviation when predicting from left-to-right on both ShARC and Daily Dialog (DD) for different parts of the sequence, where α1 is for the input sequence x, α2/¯α2 is for the already produced sequence y and α3/¯α3 is for the sequence of remaining placeholder tokens p. αk are the normalized attention weights across all three parts, whereas ¯αk normalizes over the second and third part.
<R> <C> Method <C> BC5CDR Pre <C> BC5CDR Rec <C> BC5CDR F1 <C> NCBI-Disease Pre <C> NCBI-Disease Rec <C> NCBI-Disease F1 <C> LaptopReview Pre <C> LaptopReview Rec <C> LaptopReview F1 <R> <C> AutoNER w/ Original Dict <C> 82.79 <C> 70.40 <C> 76.09 <C> 53.14 <C> 63.54 <C> 57.87 <C> 69.96 <C> 49.85 <C> 58.21 <R> <C> AutoNER w/ Tailored Dict <C> 84.57 <C> 70.22 <C> 76.73 <C> 77.30 <C> 58.54 <C> 66.63 <C> Not Applicable <C> Not Applicable <C> Not Applicable <R> <C> AutoNER w/ Tailored Dict & Phrases <C> 88.96 <C> 81.00 <C> [BOLD] 84.8 <C> 79.42 <C> 71.98 <C> [BOLD] 75.52 <C> 72.27 <C> 59.79 <C> [BOLD] 65.44 <CAP> Table 4: Ablation Experiments for Dictionary Refinement. The dictionary for the LaptopReview dataset contains no alias, so the corpus-aware dictionary tailoring is not applicable.
<R> <C> PTB Constituents model <C> PTB Constituents <C> PTB Constituents LP <C> PTB Constituents LR <C> PTB Constituents F1 <R> <C> Costa et al. ( 2001 ) <C> PoS <C> 57.8 <C> 64.9 <C> 61.1 <R> <C> Henderson ( 2003 ) <C> PoS <C> 83.3 <C> 84.3 <C> 83.8 <R> <C> Henderson ( 2003 ) <C> [EMPTY] <C> 88.8 <C> 89.5 <C> 89.1 <R> <C> Henderson ( 2004 ) <C> [EMPTY] <C> 89.8 <C> 90.4 <C> 90.1 <R> <C> Vinyals et al. ( 2015 ) seq2seq <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> <70 <R> <C> Vinyals et al. ( 2015 ) attn <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 88.3 <R> <C> Vinyals et al. ( 2015 ) seq2seq <C> semisup <C> [EMPTY] <C> [EMPTY] <C> 90.5 <CAP> Table 1: Some neural network parsing results on Penn Treebank WSJ. LP/LR/F1: labelled constituent precision/recall/F-measure. UAS/LAS: unlabelled/labelled dependency accuracy.  * results reported in (Yazdani and Henderson, 2015).
<R> <C> CoNLL09 Dependencies model (transition-based) <C> CoNLL09 Dependencies <C> CoNLL09 Dependencies UAS <C> CoNLL09 Dependencies LAS <R> <C> Titov and Henderson ( 2007a )* <C> [EMPTY] <C> 91.44 <C> 88.65 <R> <C> Chen and Manning ( 2014 )* <C> [EMPTY] <C> 89.17 <C> 86.49 <R> <C> Yazdani and Henderson ( 2015 ) <C> [EMPTY] <C> 90.75 <C> 88.14 <R> <C> Stanford Dependencies <C> Stanford Dependencies <C> Stanford Dependencies <C> Stanford Dependencies <R> <C> model (transition-based) <C> [EMPTY] <C> UAS <C> LAS <R> <C> Chen and Manning ( 2014 ) <C> [EMPTY] <C> 91.80 <C> 89.60 <R> <C> Dyer et al. ( 2015 ) <C> [EMPTY] <C> 93.10 <C> 90.90 <R> <C> Andor et al. ( 2016 ) <C> [EMPTY] <C> 94.61 <C> 92.79 <R> <C> Kiperwasser and Goldberg ( 2016 ) <C> [EMPTY] <C> 93.9 <C> 91.9 <R> <C> Mohammadshahi and Henderson ( 2019 ) <C> BERT <C> 95.63 <C> 93.81 <CAP> Table 1: Some neural network parsing results on Penn Treebank WSJ. LP/LR/F1: labelled constituent precision/recall/F-measure. UAS/LAS: unlabelled/labelled dependency accuracy.  * results reported in (Yazdani and Henderson, 2015).
<R> <C> [EMPTY] <C> [BOLD] Experiment 1  [BOLD] Dev <C> [BOLD] Experiment 1  [BOLD] Test <C> [BOLD] Experiment 2  [BOLD] Dev <C> [BOLD] Experiment 2  [BOLD] Test <R> <C> Baseline <C> 300.2 <C> 291.5 <C> 2146.7 <C> 2188.0 <R> <C> BiCVM_add <C> 226.3 <C> 228.1 <C> 1890.9 <C> 1994.1 <R> <C> BiCVM_bi <C> 225.3 <C> 217.7 <C> 1977.4 <C> 2102.9 <R> <C> BiSkip <C> 224.9 <C> 220.9 <C> 1712.1 <C> 1851.5 <R> <C> BiCCA_skip <C> 249.5 <C> 241.4 <C> 2033.6 <C> 2156.6 <R> <C> BiCCA_cbow <C> 247.8 <C> 241.4 <C> 1956.8 <C> 2145.2 <R> <C> BiCCAonBiSkip <C> 257.5 <C> 248.9 <C> 1988.9 <C> 2117.8 <R> <C> Bi-CS_skip <C> 223.2 <C> 214.5 <C> 1895.2 <C> 2016.6 <R> <C> Bi-CS_cbow <C> [BOLD] 204.7 <C> [BOLD] 193.6 <C> [BOLD] 1588.3 <C> [BOLD] 1697.0 <CAP> Table 3: Experiment 1 - PPLs on the development and the test set
<R> <C> [BOLD] POS Tag <C> [BOLD] Man. Tag <C> [BOLD] Syst. Tag <C> [BOLD] Diff. & Conf. <R> <C> [BOLD] NOUN <C> 522 <C> 538 <C> 16 ADJVERB <R> <C> [BOLD] VERB <C> 286 <C> 259 <C> 27 NOUNPRON <R> <C> [BOLD] ADJ <C> 169 <C> 141 <C> 28 NOUNVERB <R> <C> [BOLD] PRON <C> 104 <C> 118 <C> 14 ADJADV <R> <C> [BOLD] ADV <C> 93 <C> 63 <C> 30 VERBADV <R> <C> [BOLD] SYM <C> 59 <C> 60 <C> 1 NUM <R> <C> [BOLD] CONJ <C> 58 <C> 49 <C> 9 NOUNVERB <R> <C> [BOLD] DET <C> 54 <C> 53 <C> 1 VERB <R> <C> [BOLD] ADP <C> 54 <C> 49 <C> 5 PRT <R> <C> [BOLD] PRT <C> 21 <C> 18 <C> 3 ADJ <R> <C> [BOLD] DEM <C> 10 <C> 11 <C> 1 NOUN <R> <C> [BOLD] NUM <C> 9 <C> 9 <C> 0 <R> <C> [BOLD] INTF <C> 3 <C> 6 <C> 3 VERB <R> <C> [BOLD] RDP <C> 1 <C> 1 <C> 0 <R> <C> [BOLD] UN <C> 0 <C> 606 <C> 606 <R> <C> [BOLD] K’s \alpha (Interval) <C> 0.7522 <C> 0.7522 <C> 0.7522 <CAP> Table 6: Agreement Analysis between manual tagged and system tagged POS tags
<R> <C> Rank 1 <C> Type Movie <C> Mean ± std 98,387 ± <C> Mean ± std 322,166 <C> Median 14,352 <R> <C> 2 <C> TelevisionShow <C> 97,098 ± <C> 309,172 <C> 9,765 <R> <C> 3 <C> VideoGame <C> 51,236 ± <C> 166,802 <C> 11,852 <R> <C> 4 <C> CreativeWork <C> 50,024 ± <C> 213,490 <C> 5,716 <R> <C> 5 <C> InformationEntity <C> 49,704 ± <C> 212,816 <C> 5,634 <R> <C> 6 <C> Software <C> 43,657 ± <C> 149,499 <C> 9,582 <R> <C> 7 <C> MusicGroup <C> 38,883 ± <C> 133,336 <C> 5,400 <R> <C> 8 <C> Artist <C> 35,607 ± <C> 122,032 <C> 4,116 <R> <C> 9 <C> DesignedArtifact <C> 29,830 ± <C> 82,081 <C> 7,191 <R> <C> 10 <C> Book <C> 18,248 ± <C> 109,400 <C> 3,126 <R> <C> 11 <C> WrittenWork <C> 14,227 ± <C> 86,637 <C> 1,801 <R> <C> 12 <C> Person <C> 13,772 ± <C> 77,791 <C> 1,568 <R> <C> 13 <C> MusicalWork <C> 10,443 ± <C> 25,009 <C> 3,523 <R> <C> 14 <C> Athlete <C> 9,415 ± <C> 41,887 <C> 1,545 <R> <C> 15 <C> Organization <C> 9,003 ± <C> 45,140 <C> 1,816 <R> <C> 16 <C> Company <C> 7,624 ± <C> 21,371 <C> 2,566 <R> <C> 17 <C> OfficeHolder <C> 3,763 ± <C> 16,167 <C> 958 <R> <C> 18 <C> ArchitecturalStructure <C> 3,189 ± <C> 16,978 <C> 1,042 <R> <C> 19 <C> Building <C> 3,180 ± <C> 20,106 <C> 987 <R> <C> 20 <C> Infrastructure <C> 2,813 ± <C> 6,769 <C> 1,085 <R> <C> 21 <C> Place <C> 2,339 ± <C> 12,649 <C> 827 <R> <C> 22 <C> EducationalInstitution <C> 1,799 ± <C> 3,031 <C> 862 <R> <C> 23 <C> PopulatedPlace <C> 1,743 ± <C> 9,081 <C> 694 <R> <C> 24 <C> School <C> 1,137 ± <C> 1,426 <C> 747 <CAP> Table 7: “Popularity” (number of pageviews) of each entity in our dataset, aggregated per entity type. Ranked in descending order.
<R> <C> [EMPTY] <C> Pre-training <C> Fine-tuning <R> <C> [BOLD] Max Steps <C> 1 [ITALIC] M <C> - <R> <C> [BOLD] Max Epochs <C> - <C> 5 or 10 a <R> <C> [BOLD] Learning Rate <C> 1e-4 <C> {2e-5, 3e-5, 4e-5, 5e-5} <R> <C> [BOLD] Batch Size <C> 256 <C> 32 <R> <C> [BOLD] Warm-up Ratio <C> 0.01 <C> 0.06 <R> <C> [BOLD] Sequence Length <C> 512 <C> 512 <R> <C> [BOLD] Learning Rate Decay <C> Linear <C> Linear <R> <C> [BOLD] Adam  [ITALIC] ϵ <C> 1e-6 <C> 1e-6 <R> <C> [BOLD] Adam ( [ITALIC] β1,  [ITALIC] β2) <C> (0.9, 0.999) <C> (0.9, 0.999) <R> <C> [BOLD] Clip Norm <C> 1.0 <C> 1.0 <R> <C> [BOLD] Dropout <C> 0.1 <C> 0.1 <R> <C> [BOLD] Weight Decay <C> 0.01 <C> 0.01 <CAP> Table 1: Hyperparameters for the pre-training and fine-tuning.
<R> <C> [EMPTY] <C> Steps <C> MNLI-m/mm <C> QNLI <C> QQP <C> SST <C> CoLA <C> MRPC <C> RTE <C> STS <C> Avg. <R> <C> BERT-A <C> 1 [ITALIC] M <C> 84.77/84.95 <C> 91.78 <C> 90.99 <C> 92.75 <C> 56.77 <C> 87.90 <C> 71.70 <C> [BOLD] 89.60 <C> 83.47 <R> <C> BERT-R <C> 1 [ITALIC] M <C> 86.01/85.86 <C> 92.20 <C> 91.17 <C> 92.86 <C> 56.97 <C> 88.62 <C> 70.34 <C> 88.87 <C> 83.66 <R> <C> TUPE-A <C> 1 [ITALIC] M <C> 85.96/85.96 <C> 91.86 <C> 91.11 <C> [BOLD] 93.08 <C> 63.36 <C> 88.54 <C> 71.54 <C> 88.93 <C> 84.48 <R> <C> TUPE-R <C> 1 [ITALIC] M <C> [BOLD] 86.26/86.12 <C> [BOLD] 92.48 <C> [BOLD] 91.32 <C> [BOLD] 93.08 <C> [BOLD] 63.72 <C> [BOLD] 89.98 <C> [BOLD] 71.77 <C> 89.33 <C> [BOLD] 84.90 <R> <C> TUPE-A\tiny mid <C> 300 [ITALIC] k <C> 84.71/84.75 <C> 90.97 <C> 91.08 <C> 92.30 <C> 62.48 <C> 87.66 <C> 68.91 <C> 88.32 <C> 83.46 <R> <C> TUPE-R\tiny mid <C> 300 [ITALIC] k <C> 84.97/85.15 <C> 91.28 <C> 91.17 <C> 92.75 <C> 62.66 <C> 87.74 <C> 69.85 <C> 88.74 <C> 83.81 <R> <C> TUPE-A\tiny tie-cls <C> 1 [ITALIC] M <C> 86.03/85.79 <C> 91.97 <C> 91.12 <C> 92.86 <C> 59.02 <C> 88.86 <C> 69.64 <C> 88.78 <C> 83.79 <R> <C> BERT-A [ITALIC] d <C> 1 [ITALIC] M <C> 85.43/85.23 <C> 91.14 <C> 91.00 <C> 92.63 <C> 60.32 <C> 87.82 <C> 71.58 <C> 87.55 <C> 83.63 <CAP> Table 2: GLUE scores. All settings are pre-trained by BERT-Base (110M) model with 16GB data. TUPE-A\tiny mid (TUPE-R\tiny mid) is the intermediate 300k-step checkpoint of TUPE-A (TUPE-R). TUPE-A\tiny tie-cls removes the reset function from TUPE-A. BERT-Ad uses different projection matrices for words and positions, based on BERT-A.
<R> <C> [EMPTY] <C> [BOLD] Method <C> [BOLD] P <C> [BOLD] R <C> [ITALIC] F1 <R> <C> [BOLD] Subtask 1 <C> Random Baseline <C> 0.283 <C> 0.503 <C> 0.362 <R> <C> [BOLD] Subtask 1 <C> Majority Baseline <C> 0.000 <C> 0.000 <C> 0.000 <R> <C> [BOLD] Subtask 1 <C> Cos <C> [BOLD] 0.841 <C> 0.672 <C> 0.747 <R> <C> [BOLD] Subtask 1 <C> [BOLD] LexNET+Cos <C> 0.754 <C> [BOLD] 0.777 <C> [BOLD] 0.765 <R> <C> [BOLD] Subtask 2 <C> Random Baseline <C> 0.073 <C> 0.201 <C> 0.106 <R> <C> [BOLD] Subtask 2 <C> Majority Baseline <C> 0.000 <C> 0.000 <C> 0.000 <R> <C> [BOLD] Subtask 2 <C> Dist <C> 0.469 <C> 0.371 <C> 0.411 <R> <C> [BOLD] Subtask 2 <C> [BOLD] LexNET <C> [BOLD] 0.480 <C> [BOLD] 0.418 <C> [BOLD] 0.445 <CAP> Table 2: Performance scores on the test set in each subtask, of the selected methods and the baselines.
<R> <C> [EMPTY] <C> white <C> black <C> red <C> green <C> yellow <C> blue <C> brown <C> pink <C> purple <C> orange <C> grey <R> <C> overall <C> 11.9 <C> 12.2 <C> 11.7 <C> 12.0 <C> 11.0 <C> 9.4 <C> 9.6 <C> 8.6 <C> 4.2 <C> 4.2 <C> 4.6 <R> <C> voted <C> 22.7 <C> 18.4 <C> 13.4 <C> 12.1 <C> 10.0 <C> 6.4 <C> 6.3 <C> 5.3 <C> 2.1 <C> 1.5 <C> 1.3 <CAP> Table 1: Percentage of terms marked as being associated with each colour.
<R> <C> [BOLD] majority class size one <C> [BOLD] majority class size two <C> [BOLD] majority class size three <C> [BOLD] majority class size four <C> [BOLD] majority class size five <C> [BOLD] majority class size ≥ two <C> [BOLD] majority class size ≥ three <R> <C> 15.1 <C> 52.9 <C> 22.4 <C> 7.3 <C> 2.1 <C> 84.9 <C> 32.0 <CAP> Table 3: Percentage of terms in different majority classes.
<R> <C> [BOLD] CLEVR <C> Synonym <C> Q.Type 50.0 <C> GRU (Lang. Only) 66.3±1.4 <C> GRU-CNN 60.9±10.6 <C> BERT (Variant I ; Variant II) 76.2±10.2 ; 80.2±16.1 <C> NS-CL  [BOLD] 100.0± [BOLD] 0.0 <C> VCML  [BOLD] 100.0± [BOLD] 0.0 <R> <C> [BOLD] CLEVR <C> Same-kind <C> 50.0 <C> 64.7±5.1 <C> 61.5±6.6 <C> 75.4±5.4 ; 80.1±10.0 <C> 92.3±4.9 <C> [BOLD] 99.3± [BOLD] 1.0 <R> <C> [BOLD] GQA <C> Synonym <C> 50.0 <C> 80.8±1.0 <C> 76.2±0.8 <C> 76.2±2.4 ; 83.1±1.5 <C> 81.2±2.8 <C> [BOLD] 91.1± [BOLD] 1.7 <R> <C> [BOLD] GQA <C> Same-kind <C> 50.0 <C> 56.3±2.3 <C> 57.3±5.3 <C> 59.5±2.7 ; 68.2±4.0 <C> 66.8±4.1 <C> [BOLD] 69.1± [BOLD] 1.7 <R> <C> [BOLD] CUB <C> Hypernym <C> 50.0 <C> 74.3±5.2 <C> 76.7±8.8 <C> 75.6±1.2 ; 61.7±10.3 <C> 80.1±7.3 <C> [BOLD] 94.8± [BOLD] 1.3 <R> <C> [BOLD] CUB <C> Meronym <C> 50.0 <C> 80.1±5.9 <C> 78.1±4.8 <C> 63.1±3.2 ; 72.9±9.9 <C> [BOLD] 97.7± [BOLD] 1.1 <C> 92.5±1.0 <CAP> Table 6: Metaconcept generalization evaluation on the CLEVR, GQA and CUB dataset. (Two variants of BERT are shown here; see Section 4.2 for details.)
<R> <C> [BOLD] Augmentation <C> [BOLD] Nonsense errors % <C> [BOLD] Legal errors% <R> <C> No-Augmentation <C> 32.93 <C> 67.07 <R> <C> No-Augmentation + LM <C> 24.34 <C> 75.66 <R> <C> Rep-Phonestream <C> 24.99 <C> 75.11 <R> <C> Rep-Phonestream + LM <C> 20.25 <C> 79.75 <CAP> Table 3: Error type differences between the Rep-Phonestream MMDA trained system and the baseline system on WSJ (dev & test combined). “Nonsense errors” are substitutions or insertions that result in non-legal English words, e.g. CASINO substituted with ACCINO . “Legal errors” are errors that result legal English words, e.g. BOEING substituted with BOLDING.
<R> <C> System ID <C> Description <C> ARC-easy #Correct <C> ARC-easy Acc. <C> ARC-challenge #Correct <C> ARC-challenge Acc. <R> <C> [ITALIC] S1 <C> BERT <C> 1721 <C> 72.4 <C> 566 <C> 48.3 <R> <C> [ITALIC] S2 <C> Reading Strategies <C> 1637 <C> 68.9 <C> 496 <C> 42.3 <CAP> Table 1: Performance of two systems devlin2019bert; sun2018improving on the ARC question-answering dataset Clark2018ThinkYH. ARC-easy & ARC-challenge have 2376 & 1172 instances, respectively. Acc.: accuracy as a percentage.
<R> <C> Features (dim.) <C> CNN SV <C> CNN SV <C> CNN SV <C> CNN MV <C> CNN MV <C> CNN MV <C> Attentive CNN SV <C> Attentive CNN SV <C> Attentive CNN SV <C> Attentive CNN MV <C> Attentive CNN MV <C> Attentive CNN MV <R> <C> [EMPTY] <C> [ITALIC] μ <C> min <C> max <C> [ITALIC] μ <C> min <C> max <C> [ITALIC] μ <C> min <C> max <C> [ITALIC] μ <C> min <C> max <R> <C> logMel (26) <C> 51.07 <C> 48.78 <C> 52.99 <C> 51.64 <C> 50.73 <C> 52.78 <C> 52.64 <C> 51.27 <C> 53.53 <C> 51.70 <C> 51.16 <C> 52.58 <R> <C> MFCC (13) <C> [BOLD] 52.35 <C> 51.22 <C> 52.97 <C> [BOLD] 53.01 <C> 52.37 <C> 53.97 <C> [ITALIC]  [BOLD] 53.19 <C> 52.84 <C> 54.21 <C> 52.72 <C> 52.31 <C> 53.45 <R> <C> eGeMAPS (25) <C> 51.84 <C> 50.93 <C> 53.98 <C> 52.82 <C> 52.15 <C> 54.25 <C> 52.31 <C> 51.16 <C> 54.16 <C> [ITALIC]  [BOLD] 53.19 <C> 52.57 <C> [ITALIC] 54.31 <R> <C> Prosody (7) <C> 49.17 <C> 48.46 <C> 50.06 <C> 48.76 <C> 48.16 <C> 49.65 <C> 48.69 <C> 47.71 <C> 49.70 <C> 49.02 <C> 48.16 <C> 50.25 <CAP> Table 2: CNN prediction results on scripted sessions (weighted accuracy).
<R> <C> [EMPTY] <C> [BOLD] Validation <C> [BOLD] Validation  [BOLD] Top 3 <C> [BOLD] Validation  [BOLD] Top 3 <C> [BOLD] Validation  [BOLD] Top 5 <C> [BOLD] Validation  [BOLD] Top 5 <C> [BOLD] Test <C> [BOLD] Test  [BOLD] Top 3 <C> [BOLD] Test  [BOLD] Top 3 <C> [BOLD] Test  [BOLD] Top 5 <C> [BOLD] Test  [BOLD] Top 5 <R> <C> [EMPTY] <C> [BOLD] MLR <C> [BOLD] Micro F1 <C> [BOLD] TL <C> [BOLD] Micro F1 <C> [BOLD] TL <C> [BOLD] MLR <C> [BOLD] Micro F1 <C> [BOLD] TL <C> [BOLD] Micro F1 <C> [BOLD] TL <R> <C> [BOLD] Only Narrative <C> [BOLD] Only Narrative <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Top N [ITALIC] b <C> 85.23 <C> 29.70 <C> 3 <C> 31.50 <C> 5 <C> 85.66 <C> 29.70 <C> 3 <C> 28.40 <C> 5 <R> <C> Random N [ITALIC] b <C> 51.49 <C> 4.20 <C> [ITALIC] 71 <C> 5.40 <C> [ITALIC] 71 <C> 52.12 <C> 4.20 <C> [ITALIC] 71 <C> 6.36 <C> [ITALIC] 71 <R> <C> Features [ITALIC] b <C> 85.23 <C> 36.90 <C> 40 <C> 36.80 <C> 48 <C> 85.66 <C> 37.30 <C> 47 <C> 37.30 <C> 52 <R> <C> CNN-FE  [ITALIC] b <C> 86.10 <C> 37.70 <C> 37 <C> 37.60 <C> 46 <C> 86.51 <C> 36.90 <C> 58 <C> 36.70 <C> 65 <R> <C> HAN <C> 90.94 <C> 38.89 <C> 38 <C> 38.79 <C> 51 <C> 90.85 <C> 38.25 <C> 41 <C> 38.29 <C> 49 <R> <C> HAN† <C> 91.32 <C> 38.88 <C> 54 <C> 39.04 <C> 61 <C> 91.09 <C> 38.43 <C> 57 <C> 38.74 <C> 64 <R> <C> [BOLD] Narrative + Review <C> [BOLD] Narrative + Review <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Concat <C> 93.36 <C> 42.46 <C> 60 <C> 42.30 <C> 67 <C> 93.19 <C> 41.96 <C> 57 <C> 41.74 <C> 65 <R> <C> Concat† <C> 93.31 <C> 42.78 <C> 60 <C> 42.80 <C> 67 <C> 93.31 <C> 41.84 <C> 60 <C> 41.79 <C> 65 <R> <C> Gate <C> 93.39 <C> [BOLD] 43.60 <C> 53 <C> 42.89 <C> 62 <C> 93.23 <C> 42.28 <C> 53 <C> 42.04 <C> 59 <R> <C> Gate† <C> 93.32 <C> 42.74 <C> 60 <C> 42.29 <C> 66 <C> 93.10 <C> 41.81 <C> 59 <C> 41.67 <C> 66 <R> <C> Gate‡ <C> 93.50 <C> 43.36 <C> 61 <C> [BOLD] 43.06 <C> [BOLD] 68 <C> [BOLD] 93.42 <C> [BOLD] 42.92 <C> 60 <C> [BOLD] 42.34 <C> 66 <R> <C> Gate†‡ <C> [BOLD] 93.56 <C> 42.74 <C> [BOLD] 63 <C> 42.52 <C> [BOLD] 68 <C> 93.38 <C> 41.90 <C> [BOLD] 61 <C> 41.97 <C> [BOLD] 68 <CAP> Table 1: Results obtained on the validation and test set using different methodologies on the narratives and after adding reviews with the narratives. MLR and TL stand for multi-label rank and tags learned respectively. b: baseline, †: sentence level prediction enabled, ‡: skip connection enabled.
<R> <C> Data set <C> #students <C> #texts <C> #Sim <R> <C> [ITALIC] Ttrain <C> 5418 <C> 70432 <C> 934720 <R> <C> [ITALIC] Tval <C> 989 <C> 12997 <C> 173536 <R> <C> [ITALIC] Tanalyze <C> 3688 <C> 47666 <C> [EMPTY] <R> <C> Total <C> 10095 <C> 131,095 <C> 1108256 <CAP> Table 1: Data set overview. The table lists the number of students and texts, as well as the number of problem instances #Sim for training the Siamese neural network.
<R> <C> Cluster <C> #students <R> <C> C1 <C> 603 <R> <C> C2 <C> 720 <R> <C> C3 <C> 884 <R> <C> C4 <C> 969 <R> <C> C5 <C> 512 <CAP> Table 2: The number of students in each cluster.
<R> <C> [BOLD] Model <C> [BOLD] n <C> [BOLD] R-1 <C> [BOLD] R-2 <C> [BOLD] R-L <R> <C> LEAD <C> 3 <C> 31.40 <C> 8.68 <C> 29.42 <R> <C> LEAD <C> 4 <C> 31.87 <C> 8.93 <C> 29.91 <R> <C> LEAD <C> 5 <C> 32.02 <C> 9.53 <C> [BOLD] 30.07 <R> <C> MIDDLE <C> 3 <C> 28.04 <C> 6.57 <C> 26.13 <R> <C> MIDDLE <C> 4 <C> 30.08 <C> 7.96 <C> 28.10 <R> <C> MIDDLE <C> 5 <C> 29.91 <C> 8.12 <C> 27.97 <R> <C> LONGEST <C> 3 <C> [BOLD] 32.46 <C> 10.27 <C> 29.92 <R> <C> LONGEST <C> 4 <C> 32.19 <C> [BOLD] 10.35 <C> 29.91 <R> <C> LONGEST <C> 5 <C> 31.61 <C> 10.21 <C> 29.55 <R> <C> LONGER -THAN <C> 10 <C> 28.31 <C> 9.69 <C> 26.72 <R> <C> LONGER -THAN <C> 20 <C> 29.36 <C> 10.23 <C> 27.59 <R> <C> LONGER -THAN <C> 30 <C> 29.61 <C> 10.28 <C> 27.71 <R> <C> MOST-ACTIVE <C> [EMPTY] <C> 26.54 <C> 8.55 <C> 24.57 <R> <C> [EMPTY] <C> 26.54 <C> 8.55 <C> 24.57 <C> [EMPTY] <R> <C> -PERSON <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 3: Baselines for the dialogues summarization
<R> <C> [BOLD] Style <C> [BOLD] Training <C> [BOLD] Q1  [ITALIC] μx <C> [BOLD] Q1  [ITALIC] SD <C> [BOLD] Q2  [ITALIC] μx <C> [BOLD] Q2  [ITALIC] SD <C> [BOLD] Q3  [ITALIC] μx <C> [BOLD] Q3  [ITALIC] SD <C> [BOLD] Q4  [ITALIC] μx <C> [BOLD] Q4  [ITALIC] SD <C> [BOLD] Q5  [ITALIC] μx <C> [BOLD] Q5  [ITALIC] SD <C> [BOLD] Q6  [ITALIC] μx <C> [BOLD] Q6  [ITALIC] SD <C> [BOLD] Q7  [ITALIC] μx <C> [BOLD] Q7  [ITALIC] SD <C> [BOLD] Q8  [ITALIC] μx <C> [BOLD] Q8  [ITALIC] SD <R> <C> authoritarian <C> both <C> [BOLD] 3.35 <C> 1.08 <C> 2.65 <C> 1.26 <C> 3.33 <C> 1.06 <C> [BOLD] 3.02 <C> 1.10 <C> [BOLD] 3.08 <C> 0.99 <C> 3.03 <C> 1.01 <C> 3.16 <C> 0.99 <C> 3.10 <C> 1.03 <R> <C> authoritarian <C> peer only <C> 2.97 <C> 1.18 <C> 2.14 <C> 1.17 <C> [BOLD] 3.44 <C> 1.13 <C> 2.66 <C> 1.15 <C> 2.82 <C> 1.08 <C> [BOLD] 3.10 <C> 1.11 <C> [BOLD] 3.03 <C> 1.12 <C> 3.11 <C> 1.13 <R> <C> authoritarian <C> master only <C> 3.34 <C> 1.07 <C> [BOLD] 2.89 <C> 1.28 <C> 3.30 <C> 1.05 <C> 3.00 <C> 1.12 <C> 3.07 <C> 1.05 <C> 3.04 <C> 1.02 <C> 3.12 <C> 1.02 <C> [BOLD] 3.07 <C> 1.04 <R> <C> authoritative <C> both <C> 3.43 <C> 1.08 <C> 3.09 <C> 1.31 <C> 3.28 <C> 1.12 <C> 3.08 <C> 1.16 <C> 3.08 <C> 1.05 <C> 3.02 <C> 1.05 <C> [BOLD] 3.22 <C> 1.06 <C> [BOLD] 3.13 <C> 1.05 <R> <C> authoritative <C> peer only <C> 3.41 <C> 1.14 <C> [BOLD] 3.23 <C> 1.35 <C> 3.37 <C> 1.21 <C> 3.13 <C> 1.16 <C> 3.07 <C> 1.08 <C> 3.08 <C> 1.09 <C> 3.24 <C> 1.10 <C> 3.16 <C> 1.13 <R> <C> authoritative <C> master only <C> [ITALIC]  [BOLD] 3.47 <C> 1.03 <C> 3.15 <C> 1.33 <C> [BOLD] 3.41 <C> 1.08 <C> [BOLD] 3.17 <C> 1.11 <C> [ITALIC]  [BOLD] 3.16 <C> 1.02 <C> [BOLD] 3.16 <C> 1.04 <C> 3.28 <C> 1.01 <C> 3.24 <C> 1.06 <R> <C> master <C> both <C> 3.38 <C> 1.07 <C> [BOLD] 2.79 <C> 1.28 <C> 3.29 <C> 1.06 <C> 3.07 <C> 1.12 <C> [BOLD] 3.09 <C> 1.04 <C> [BOLD] 3.10 <C> 1.06 <C> 3.22 <C> 1.02 <C> 3.14 <C> 1.05 <R> <C> master <C> master only <C> [BOLD] 3.40 <C> 1.07 <C> 2.61 <C> 1.30 <C> [BOLD] 3.34 <C> 1.11 <C> [BOLD] 3.10 <C> 1.15 <C> [BOLD] 3.09 <C> 1.02 <C> 3.04 <C> 1.03 <C> [BOLD] 3.17 <C> 1.04 <C> [BOLD] 3.11 <C> 1.06 <R> <C> neglecting <C> both <C> [BOLD] 3.45 <C> 1.11 <C> [ITALIC]  [BOLD] 3.28 <C> 1.32 <C> [BOLD] 3.34 <C> 1.11 <C> [ITALIC]  [BOLD] 3.28 <C> 1.12 <C> [ITALIC]  [BOLD] 3.16 <C> 1.06 <C> 3.12 <C> 1.04 <C> 3.21 <C> 1.08 <C> 3.22 <C> 1.07 <R> <C> neglecting <C> peer only <C> 3.36 <C> 1.07 <C> 3.02 <C> 1.37 <C> 3.31 <C> 1.11 <C> 3.12 <C> 1.15 <C> 3.09 <C> 1.02 <C> [BOLD] 3.14 <C> 1.04 <C> 3.19 <C> 1.02 <C> [BOLD] 3.14 <C> 1.06 <R> <C> neglecting <C> master only <C> 3.28 <C> 1.13 <C> 2.87 <C> 1.35 <C> [BOLD] 3.34 <C> 1.12 <C> 3.09 <C> 1.14 <C> 3.05 <C> 1.05 <C> 3.07 <C> 1.06 <C> [BOLD] 3.15 <C> 1.06 <C> 3.18 <C> 1.08 <R> <C> permissive <C> both <C> [BOLD] 3.23 <C> 1.18 <C> 2.67 <C> 1.38 <C> 3.59 <C> 1.06 <C> [BOLD] 3.06 <C> 1.13 <C> [BOLD] 3.08 <C> 1.08 <C> [ITALIC]  [BOLD] 3.30 <C> 1.04 <C> 3.21 <C> 1.07 <C> 3.30 <C> 1.10 <R> <C> permissive <C> peer only <C> 3.05 <C> 1.19 <C> [BOLD] 2.87 <C> 1.39 <C> 3.25 <C> 1.18 <C> 2.88 <C> 1.13 <C> 2.88 <C> 1.09 <C> 3.00 <C> 1.08 <C> 2.99 <C> 1.11 <C> 3.04 <C> 1.14 <R> <C> permissive <C> master only <C> 3.09 <C> 1.23 <C> 2.32 <C> 1.24 <C> [ITALIC]  [BOLD] 3.64 <C> 1.11 <C> 2.88 <C> 1.15 <C> 2.91 <C> 1.12 <C> 3.07 <C> 1.15 <C> [ITALIC]  [BOLD] 2.98 <C> 1.13 <C> [ITALIC]  [BOLD] 3.04 <C> 1.12 <CAP> Table 2: Mean and standard deviation.
<R> <C> XW1 <C> [BOLD] en <C> [BOLD] de <C> [BOLD] es <C> [BOLD] it <C> [BOLD] hr <C> [BOLD] ru <C> [BOLD] tr <R> <C> [BOLD] en <C> – <C> 1.28 <C> 1.63 <C> 1.62 <C> 1.59 <C> 1.49 <C> 1.32 <R> <C> [BOLD] de <C> 1.55 <C> – <C> 1.28 <C> 1.45 <C> 1.41 <C> 1.03 <C> 1.29 <R> <C> [BOLD] es <C> 1.45 <C> 1.25 <C> – <C> 1.28 <C> 1.21 <C> 1.31 <C> 1.09 <R> <C> [BOLD] it <C> 1.18 <C> 1.10 <C> 1.28 <C> – <C> 1.29 <C> 0.61 <C> 1.09 <R> <C> [BOLD] hr <C> 1.57 <C> 1.62 <C> 1.59 <C> 1.62 <C> – <C> 1.62 <C> 1.63 <R> <C> [BOLD] ru <C> 1.41 <C> 1.12 <C> 1.20 <C> 1.38 <C> 1.46 <C> – <C> 1.29 <R> <C> [BOLD] tr <C> 1.23 <C> 1.21 <C> 1.06 <C> 1.26 <C> 1.24 <C> 1.04 <C> – <CAP> Table 7: XWEAT T1 effect sizes for cross-lingual embedding spaces. Rows denote the target set language, column the attribute set language.
<R> <C> XW9 <C> [BOLD] en <C> [BOLD] de <C> [BOLD] es <C> [BOLD] it <C> [BOLD] hr <C> [BOLD] ru <C> [BOLD] tr <R> <C> [BOLD] en <C> – <C> 1.12 <C> 1.66 <C> 1.61 <C> −0.59∗ <C> 1.76 <C> 1.65 <R> <C> [BOLD] de <C> 1.74 <C> – <C> 1.68 <C> 1.66 <C> −1.39 <C> 1.46 <C> 1.57 <R> <C> [BOLD] es <C> 1.64 <C> 1.48 <C> – <C> 1.79 <C> −1.34 <C> 1.75 <C> 1.37 <R> <C> [BOLD] it <C> 1.62 <C> 0.19∗ <C> 1.47 <C> – <C> −1.63 <C> 1.87 <C> 1.74 <R> <C> [BOLD] hr <C> 1.54 <C> 1.89 <C> 1.87 <C> 0.96∗ <C> – <C> 1.73 <C> 1.59 <R> <C> [BOLD] ru <C> 1.82 <C> 1.54 <C> 1.64 <C> 1.72 <C> −0.84∗ <C> – <C> 0.80∗ <R> <C> [BOLD] tr <C> 1.88 <C> 0.98∗ <C> 1.88 <C> 1.70 <C> −1.80 <C> 0.58∗ <C> – <CAP> Table 12: XWEAT T9 effect sizes for cross-lingual embedding spaces. Rows denote the target set language, column the attribute set language.
<R> <C> Method <C> [ITALIC] MCTest-160 accuracy (%) Single (112) <C> [ITALIC] MCTest-160 accuracy (%) Multiple (128) <C> [ITALIC] MCTest-160 accuracy (%) All <C> [ITALIC] MCTest-500 accuracy (%) Single (272) <C> [ITALIC] MCTest-500 accuracy (%) Multiple (328) <C> [ITALIC] MCTest-500 accuracy (%) All <R> <C> Richardson et al. (2013) + RTE <C> 76.78 <C> 62.50 <C> 69.16 <C> 68.01 <C> 59.45 <C> 63.33 <R> <C> Sachan et al. (2015) <C> - <C> - <C> - <C> 67.65 <C> 67.99 <C> 67.83 <R> <C> Wang et al. (2015) <C> [BOLD] 84.22 <C> 67.85 <C> [BOLD] 75.27 <C> 72.05 <C> 67.94 <C> 69.94 <R> <C> Attentive Reader <C> 48.1 <C> 44.7 <C> 46.3 <C> 44.4 <C> 39.5 <C> 41.9 <R> <C> Neural Reasoner <C> 48.4 <C> 46.8 <C> 47.6 <C> 45.7 <C> 45.6 <C> 45.6 <R> <C> HABCNN-TE <C> 63.3 <C> 62.9 <C> 63.1 <C> 54.2 <C> 51.7 <C> 52.9 <R> <C> Parallel-Hierarchical <C> 79.46 <C> [BOLD] 70.31 <C> 74.58 <C> [BOLD] 74.26 <C> [BOLD] 68.29 <C> [BOLD] 71.00 <CAP> Table 1: Experimental results on MCTest.
<R> <C> Question type (# questions) <C> Baseline Models Majority <C> Baseline Models QRN <C> Baseline Models EntNet <C> Baseline Models Rule-based <C> Baseline Models Feature-based <C> Our Models ProLocal <C> Our Models ProGlobal <C> Human Upper Bound <R> <C> Cat-1 (750) <C> 51.01 <C> 52.37 <C> 51.62 <C> 57.14 <C> 58.64 <C> 62.65 <C> 62.95 <C> 91.67 <R> <C> Cat-2 (601) <C> - <C> 15.51 <C> 18.83 <C> 20.33 <C> 20.82 <C> 30.50 <C> 36.39 <C> 87.66 <R> <C> Cat-3 (823) <C> - <C> 10.92 <C> 7.77 <C> 2.4 <C> 9.66 <C> 10.35 <C> 35.9 <C> 62.96 <R> <C> macro-avg <C> - <C> 26.26 <C> 26.07 <C> 26.62 <C> 29.7 <C> 34.50 <C> 45.08 <C> 80.76 <R> <C> micro-avg <C> [EMPTY] <C> 26.49 <C> 25.96 <C> 26.24 <C> 29.64 <C> 33.96 <C> 45.37 <C> 79.69 <CAP> Table 3: Model accuracy on the end task (test partition of ProPara). Questions are (Section 5.1): (Cat-1) Is ei created (destroyed, moved)? (Cat-2) When is ei created (…)? (Cat-3) Where is ei created (…)?
<R> <C> Evaluation method <C> Text-to-Speech Words (%) <C> Text-to-Speech Phones (%) <C> Speech-to-Text Words (%) <C> Speech-to-Text Letters (%) <R> <C> PROB <C> 63.80 <C> (90.51) <C> 76.23 <C> (95.22) <R> <C> PROB1/2 <C> 65.25 <C> (90.98) <C> 76.66 <C> (95.36) <R> <C> PROB1/3 <C> 65.60 <C> (91.14) <C> [BOLD] 76.83 <C> [BOLD] (95.41) <R> <C> PROB1/4 <C> 65.81 <C> [BOLD] (91.20) <C> 76.74 <C> (95.38) <R> <C> PROB1/5 <C> 65.87 <C> (91.19) <C> 76.69 <C> (95.37) <R> <C> PROB1/6 <C> [BOLD] 65.92 <C> (91.19) <C> 76.65 <C> (95.37) <R> <C> PROB1/7 <C> 65.89 <C> (91.19) <C> 76.58 <C> (95.35) <R> <C> PROB1/8 <C> 65.88 <C> (91.17) <C> 76.51 <C> (95.33) <R> <C> PROB1/9 <C> 65.88 <C> (91.16) <C> 76.51 <C> (95.33) <R> <C> PROB1/10 <C> 65.88 <C> (91.15) <C> 76.52 <C> (95.33) <R> <C> CONDF <C> 66.21 <C> (91.13) <C> [BOLD] 76.26 <C> [BOLD] (95.25) <R> <C> CONDF1/2 <C> 66.29 <C> (91.15) <C> 76.17 <C> (95.21) <R> <C> CONDF1/3 <C> 66.28 <C> (91.14) <C> 76.07 <C> (95.19) <R> <C> CONDF1/4 <C> [BOLD] 66.31 <C> [BOLD] (91.14) <C> 76.02 <C> (95.17) <R> <C> CONDF1/5 <C> 66.27 <C> (91.11) <C> 76.00 <C> (95.16) <R> <C> CONDR <C> 63.26 <C> (90.14) <C> 75.80 <C> (95.11) <R> <C> CONDR1/2 <C> 64.76 <C> (90.61) <C> 75.94 <C> (95.17) <R> <C> CONDR1/3 <C> 65.20 <C> (90.77) <C> [BOLD] 76.00 <C> [BOLD] (95.17) <R> <C> CONDR1/4 <C> [BOLD] 65.41 <C> [BOLD] (90.84) <C> 75.95 <C> (95.15) <R> <C> CONDR1/5 <C> 65.46 <C> (90.85) <C> 75.93 <C> (95.14) <R> <C> CONDL <C> 65.98 <C> (91.12) <C> 76.05 <C> (95.19) <R> <C> CONDL1/2 <C> 66.52 <C> (91.31) <C> 76.20 <C> (95.23) <R> <C> CONDL1/3 <C> [BOLD] 66.61 <C> [BOLD] (91.33) <C> [BOLD] 76.21 <C> [BOLD] (95.24) <R> <C> CONDL1/4 <C> 66.54 <C> (91.31) <C> 76.17 <C> (95.23) <R> <C> CONDL1/5 <C> 66.51 <C> (91.29) <C> 76.12 <C> (95.22) <R> <C> CONDRL <C> 65.50 <C> (90.98) <C> 76.30 <C> (95.27) <R> <C> CONDRL1/2 <C> 66.24 <C> (91.20) <C> [BOLD] 76.36 <C> [BOLD] (95.27) <R> <C> CONDRL1/3 <C> [BOLD] 66.33 <C> [BOLD] (91.23) <C> 76.32 <C> (95.26) <R> <C> CONDRL1/4 <C> 66.28 <C> (91.19) <C> 76.23 <C> (95.24) <R> <C> CONDRL1/5 <C> 66.26 <C> (91.16) <C> 76.16 <C> (95.22) <R> <C> CONDALL <C> 65.66 <C> (91.02) <C> 76.38 <C> (95.29) <R> <C> CONDALL1/2 <C> 66.30 <C> (91.23) <C> [BOLD] 76.39 <C> [BOLD] (95.28) <R> <C> CONDALL1/3 <C> [BOLD] 66.39 <C> [BOLD] (91.25) <C> 76.36 <C> (95.27) <R> <C> CONDALL1/4 <C> 66.35 <C> (91.22) <C> 76.26 <C> (95.24) <R> <C> CONDALL1/5 <C> 66.31 <C> (91.18) <C> 76.17 <C> (95.21) <CAP> Table 7: Experiments applying the “magic root” suggested by the good performance of the PFSP method of Polyákova and Bonafonte (2008) to our probabilistically justified methods. The degree of the applied root is shown as the inverse exponent of the method name.
<R> <C> system alignment <C> HACM S <C> HACM N <C> HAEM S <C> HAEM N <C> Nematus – <R> <C> low <C> 5 <C> 5 <C> 5 <C> 5 <C> [EMPTY] <R> <C> medium <C> 5 <C> 5 <C> 5 <C> 3 <C> [EMPTY] <R> <C> high <C> 3 <C> 3 <C> 3 <C> 2 <C> 1 <CAP> Table 2: Number of single models that we train for each language. N=Naive alignment, S=Smart alignment. E.g. for each language at the medium setting, there are 3 HAEM models trained on data aligned with naive alignment.
<R> <C> Noise <C> Inv <C> BL <C> A Inv <C> A BL <C> B Inv <C> B BL <C> C Inv <C> C BL <C> D Inv <C> D BL <R> <C> 1 <C> 16.36 <C> 18.14 <C> 6.54 <C> 7.57 <C> 12.71 <C> 14.09 <C> 11.45 <C> 13.10 <C> 22.47 <C> 24.80 <R> <C> 2 <C> 15.56 <C> 17.39 <C> 5.90 <C> 6.58 <C> 11.69 <C> 13.28 <C> 11.12 <C> 13.51 <C> 21.79 <C> 23.96 <R> <C> 3 <C> 14.24 <C> 14.67 <C> 5.45 <C> 5.08 <C> 10.76 <C> 12.44 <C> 9.75 <C> 9.84 <C> 19.93 <C> 19.30 <R> <C> 4 <C> 13.61 <C> 13.84 <C> 5.08 <C> 5.29 <C> 9.73 <C> 9.97 <C> 9.49 <C> 9.56 <C> 19.49 <C> 19.90 <R> <C> 5 <C> 13.41 <C> 13.02 <C> 5.12 <C> 5.34 <C> 9.52 <C> 9.42 <C> 9.55 <C> 8.67 <C> 19.33 <C> 18.65 <R> <C> 6 <C> 12.62 <C> 12.60 <C> 4.80 <C> 4.61 <C> 9.04 <C> 8.86 <C> 8.76 <C> 8.59 <C> 18.16 <C> 18.21 <R> <C> 6* <C> 11.85 <C> 11.99 <C> 4.52 <C> 4.76 <C> 8.76 <C> 8.76 <C> 7.79 <C> 8.57 <C> 16.84 <C> 16.99 <CAP> Table 1: Average word error rate (WER%) on Aurora-4 dataset on all test conditions, including seen and unseen noise and unseen microphone. First column is the number of noise conditions used for the training. The last row is a preliminary experiment with layer-wise pre-training close to state-of-the-art model and a corresponding invariance training starting with a pretrained model.
<R> <C> [BOLD] Method <C> [BOLD] BLEU <C> [ITALIC]  [BOLD] distinct-1 <C> [ITALIC]  [BOLD] distinct-2 <R> <C> Seq2Seq <C> 0.97 <C> 0.008 <C> 0.062 <R> <C> Seq2Seq & Hard-Attention <C> 1.18 <C> 0.009 <C> 0.064 <R> <C> Random Hard-Attention <C> 1.15 <C> 0.008 <C> 0.064 <R> <C> Self-Attention & Min <C> 1.12 <C> 0.009 <C> 0.071 <R> <C> Self-Attention & Max <C> [BOLD] 1.26 <C> [BOLD] 0.009 <C> [BOLD] 0.076 <R> <C> Seq2Seq using MMI <C> [BOLD] 3.38 <C> 0.010 <C> 0.119 <R> <C> Self-Attention & Max using MMI <C> 2.67 <C> [BOLD] 0.012 <C> [BOLD] 0.171 <CAP> Table 2: Automatic evaluation result
<R> <C> [BOLD] Method <C> [ITALIC]  [BOLD] Good (1) <C> [ITALIC]  [BOLD] Mediocre (2) <C> [ITALIC]  [BOLD] Bad (3) <C> [BOLD] Average <R> <C> Seq2Seq <C> 7 <C> 126 <C> 66 <C> 2.296 <R> <C> Seq2Seq & Hard-Attention <C> 6 <C> 126 <C> 67 <C> 2.307 <R> <C> Random Hard-Attention <C> 5 <C> 127 <C> 67 <C> 2.312 <R> <C> Self-Attention & Min <C> 10 <C> 124 <C> 65 <C> 2.276 <R> <C> Self-Attention & Max <C> 13 <C> 123 <C> 63 <C> [BOLD] 2.251 <R> <C> Seq2Seq using MMI <C> 10 <C> 18 <C> 171 <C> 2.809 <R> <C> Self-Attention & Max using MMI <C> 28 <C> 27 <C> 144 <C> [BOLD] 2.583 <CAP> Table 3: Human evaluation result
<R> <C> [BOLD] Decomposition <C> [BOLD] Stage 1 −log [ITALIC] p( [ITALIC] z∗) <C> [BOLD] Stage 2 −log [ITALIC] p( [ITALIC] x| [ITALIC] z∗) <R> <C> Summary <C> 4.20 <C> 5.09 <R> <C> Keyword <C> 6.92 <C> 4.23 <R> <C> Compression <C> 5.05 <C> 3.64 <R> <C> SRL Action Plan <C> 2.72 <C> 3.95 <R> <C> NER Entity Anonymization <C> 3.32 <C> 4.75 <R> <C> Coreference Anonymization <C> 3.15 <C> 4.55 <CAP> Table 1: Negative log likelihood of generating stories using different decompositions (lower is easier for the model). Stage 1 is the generation of the intermediate representation z∗, and Stage 2 is the generation of the story x conditioned upon z∗. Entity generation is with a word-based vocabulary to be consistent with the other models.
<R> <C> [BOLD] Model <C> [BOLD] # Unique Verbs <C> [BOLD] % Diverse Verbs <R> <C> Human Stories <C> 34.0 <C> 76.5 <R> <C> Fusion <C> 10.3 <C> 61.1 <R> <C> Summary <C> 12.4 <C> 60.6 <R> <C> Keyword <C> 9.1 <C> 58.2 <R> <C> Compression <C> 10.3 <C> 54.3 <R> <C> SRL <C> 14.4 <C> 62.5 <R> <C> + verb-attention <C> 15.9 <C> 64.9 <CAP> Table 2: Action Generation. Generating the SRL structure improves verb diversity and reduces repetition.
<R> <C> [BOLD] Model <C> [BOLD] First Mentions  [BOLD] Rank 10 <C> [BOLD] First Mentions  [BOLD] Rank 50 <C> [BOLD] First Mentions  [BOLD] Rank 100 <C> [BOLD] Subsequent Mentions  [BOLD] Rank 10 <C> [BOLD] Subsequent Mentions  [BOLD] Rank 50 <C> [BOLD] Subsequent Mentions  [BOLD] Rank 100 <R> <C> Word-Based <C> 42.3 <C> 25.4 <C> 17.2 <C> 48.1 <C> 38.4 <C> 28.8 <R> <C> BPE <C> 48.1 <C> 20.3 <C> 25.5 <C> 52.5 <C> 50.7 <C> 48.8 <R> <C> Character-level <C> 64.2 <C> 51.0 <C> 35.6 <C> 66.1 <C> 55.0 <C> 51.2 <R> <C> No story <C> 50.3 <C> 40.0 <C> 26.7 <C> 54.7 <C> 51.3 <C> 30.4 <R> <C> Left story context <C> 59.1 <C> 49.6 <C> 33.3 <C> 62.9 <C> 53.2 <C> 49.4 <R> <C> Full story <C> 64.2 <C> 51.0 <C> 35.6 <C> 66.1 <C> 55.0 <C> 51.2 <CAP> Table 3: Accuracy at choosing the correct reference string for a mention, discriminating against 10, 50 and 100 random distractors. We break out results for the first mention of an entity (requiring novelty to produce an appropriate name in the context) and subsequent references (typically pronouns, nominal references, or shorter forms of names). We compare the effect of sub-word modelling and providing longer contexts.
<R> <C> [BOLD] Group <C> [BOLD] Method <C> [BOLD] 5-class accuracy <C> [BOLD] 2-class accuracy <C> [BOLD] Reported in <R> <C> Baseline <C> SVM <C> 40.7 <C> 79.4 <C> RNN <R> <C> Baseline <C> Naïve Bayes <C> 41.0 <C> 81.8 <C> RNN <R> <C> CNNs <C> 1-layer convolution <C> 37.4 <C> 77.1 <C> CNNNLP <R> <C> CNNs <C> Deep CNN <C> 48.5 <C> 86.8 <C> CNNNLP <R> <C> CNNs <C> Non-static <C> 48.0 <C> 87.2 <C> cnn2 <R> <C> CNNs <C> Multichannel <C> 47.4 <C> [BOLD] 88.1 <C> cnn2 <R> <C> RNNs <C> Basic <C> 43.2 <C> 82.4 <C> RNN <R> <C> RNNs <C> Matrix-vector <C> 44.4 <C> 82.9 <C> RNN <R> <C> RNNs <C> Tensor <C> 45.7 <C> 85.4 <C> RNN <R> <C> RNNs <C> Tree LSTM (variant 1) <C> 48.0 <C> – <C> lstm3 <R> <C> RNNs <C> Tree LSTM (variant 2) <C> 50.6 <C> 86.9 <C> lstm1 <R> <C> RNNs <C> Tree LSTM (variant 3) <C> 49.9 <C> 88.0 <C> lstm2 <R> <C> [EMPTY] <C> Deep RNN <C> 49.8 <C> 86.6† <C> deepRNN <R> <C> Recurrent <C> LSTM <C> 45.8 <C> 86.7 <C> lstm1 <R> <C> Recurrent <C> bi-LSTM <C> 49.1 <C> 86.8 <C> lstm1 <R> <C> Vector <C> Word vector avg. <C> 32.7 <C> 80.1 <C> RNN <R> <C> Vector <C> Paragraph vector <C> 48.7 <C> 87.8 <C> paravec <R> <C> TBCNNs <C> c-TBCNN <C> 50.4 <C> 86.8† <C> Our implementation <R> <C> TBCNNs <C> d-TBCNN <C> [BOLD] 51.4 <C> 87.9† <C> Our implementation <CAP> Table 2: Accuracy of sentiment prediction (in percentage). For 2-class prediction, “†” remarks indicate that the network is transferred directly from that of 5-class.
<R> <C> [BOLD] Method <C> [BOLD] Acc. (%) <C> [BOLD] Reported in <R> <C> SVM <C> 95.0 <C> QC <R> <C> 10k features + 60 rules <C> 95.0 <C> QC <R> <C> CNN-non-static <C> 93.6 <C> cnn2 <R> <C> CNN-mutlichannel <C> 92.2 <C> cnn2 <R> <C> RNN <C> 90.2 <C> AdaCNN <R> <C> Deep-CNN <C> 93.0 <C> CNNNLP <R> <C> Ada-CNN <C> 92.4 <C> AdaCNN <R> <C> c-TBCNN <C> 94.8 <C> Our implementation <R> <C> d-TBCNN <C> [BOLD] 96.0 <C> Our implementation <CAP> Table 3: Accuracy of 6-way question classification.
<R> <C> Task <C> Keyword-based model Precision <C> Keyword-based model Recall <C> Keyword-based model F-score <C> BERT-based model Precision <C> BERT-based model Recall <C> BERT-based model F-score <R> <C> Is about COVID-19 <C> 0.36 <C> 1.00 <C> 0.54 <C> 0.82 <C> 0.87 <C> [BOLD] 0.84 <R> <C> Topic: Infection status <C> 0.09 <C> 0.53 <C> 0.16 <C> 0.43 <C> 0.81 <C> [BOLD] 0.56 <R> <C> Topic: Prevention <C> 0.05 <C> 0.73 <C> 0.10 <C> 0.19 <C> 0.73 <C> [BOLD] 0.30 <R> <C> Topic: Medical information <C> 0.17 <C> 0.70 <C> 0.27 <C> 0.27 <C> 0.91 <C> [BOLD] 0.41 <R> <C> Topic: Economic <C> 0.06 <C> 0.36 <C> 0.10 <C> 0.14 <C> 0.84 <C> [BOLD] 0.24 <R> <C> Topic: Education <C> 0.06 <C> 1.00 <C> [BOLD] 0.11 <C> 0.05 <C> 0.60 <C> 0.09 <R> <C> Topic: Art and Sport <C> 0.06 <C> 0.41 <C> 0.10 <C> 0.08 <C> 0.94 <C> [BOLD] 0.14 <R> <C> Topic: Others <C> 0.52 <C> 0.07 <C> 0.13 <C> 0.87 <C> 0.79 <C> [BOLD] 0.83 <CAP> Table 4: Topic classification results. Each line stands for one task. We use F-score to evaluate.
<R> <C> [EMPTY] <C> LR <C> GBDT <C> TextCNN <C> Bi-Transformer <C> MaLSTM <C> MAN <C> Our <R> <C> [BOLD] Accuracy <C> 0.8297 <C> 0.8628 <C> 0.8772 <C> 0.8813 <C> 0.8825 <C> 0.8808 <C> [BOLD] 0.8899 <R> <C> [BOLD] AUC <C> 0.8808 <C> 0.9287 <C> 0.9312 <C> 0.9335 <C> 0.9375 <C> 0.9365 <C> [BOLD] 0.9444 <CAP> Table 1: ASAG performance comparison on a real-world K-12 dataset.
<R> <C> Model <C> [ITALIC] kn <C> [ITALIC] km <C> [ITALIC] kr <C> [ITALIC] λ <R> <C> TSR-TXT <C> 300 <C> 500 <C> 5 <C> 5⋅104 <R> <C> TSR-CNN <C> 300 <C> 300 <C> 5 <C> 70⋅104 <R> <C> TSR-HCA <C> 300 <C> 500 <C> 5 <C> 10⋅104 <CAP> Table 3: Optimized hyperparameter values used in final evaluation.
<R> <C> Models <C> Info <C> Conc <C> Read <R> <C> [BOLD] Baselines <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Length <C> 2.33 <C> 2.93 <C> 2.28 <R> <C> Popularity <C> 2.38 <C> 2.35 <C> 3.05 <R> <C> User <C> 3.13 <C> 3.10 <C> 3.75 <R> <C> LexRank <C> 3.05 <C> 2.70 <C> 3.03 <R> <C> [BOLD] State-of-the-art <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Chang et al. ( 2013 ) <C> 3.43 <C> 3.50 <C> 3.70 <R> <C> Li et al. ( 2015a ) <C> [BOLD] 3.70 <C> [BOLD] 3.90 <C> [BOLD] 4.15 <R> <C> [BOLD] Our models <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> topic only <C> 3.33 <C> 3.03 <C> 3.35 <R> <C> topic+disc <C> 3.25 <C> 3.15 <C> 3.55 <R> <C> topic+disc+rel <C> 3.35 <C> 3.28 <C> 3.73 <CAP> Table 8: Overall human ratings on summaries produced by varying summarization systems. Info, Conc, Read are short forms of informativeness, conciseness, and readability, respectively. The ratings are given in a 1-5 Likert scale. Higher scores indicate better ratings. The best score in each setting is highlighted in bold.
<R> <C> captain <C> officer <C> S 8.22 <C> P 8.17 <R> <C> celery <C> food <C> 9.3 <C> 9.43 <R> <C> horn <C> bull <C> 1.12 <C> 0.94 <R> <C> wing <C> airplane <C> 1.03 <C> 0.84 <R> <C> prince <C> royalty <C> 9.85 <C> 4.71 <R> <C> autumn <C> season <C> 9.77 <C> 3.69 <R> <C> kid <C> parent <C> 0.52 <C> 8.00 <R> <C> discipline <C> punishment <C> 7.7 <C> 3.32 <CAP> Table 3: Example word pairs from the HyperLex development set. S is the human-annotated score in the HyperLex dataset. P is the predicted score using the SDSN+SDF+AS model.
<R> <C> [BOLD] System <C> [ITALIC]  [BOLD] CandGen <C> [BOLD] NCBI (Disease)  [BOLD] P <C> [BOLD] NCBI (Disease)  [BOLD] R <C> [BOLD] NCBI (Disease)  [BOLD] F1 <C> [BOLD] CDR (Disease)  [BOLD] P <C> [BOLD] CDR (Disease)  [BOLD] R <C> [BOLD] CDR (Disease)  [BOLD] F1 <C> [BOLD] CDR (Chemical)  [BOLD] P <C> [BOLD] CDR (Chemical)  [BOLD] R <C> [BOLD] CDR (Chemical)  [BOLD] F1 <R> <C> TaggerOne∗ <C> [EMPTY] <C> 83.5 <C> 79.6 <C> [BOLD] 81.5 <C> 83.1 <C> 76.4 <C> 79.6 <C> [BOLD] 92.4 <C> 84.7 <C> [BOLD] 88.4 <R> <C> Majority Vote <C> [ITALIC] Hand-tuned <C> [BOLD] 84.5 <C> 75.5 <C> 79.8 <C> [BOLD] 85.4 <C> 67.6 <C> 75.5 <C> 89.8 <C> 83.1 <C> 86.3 <R> <C> ♠ CRF/emb <C> [ITALIC] Hand-tuned <C> 78.7 <C> 78.0 <C> 78.4 <C> 83.1 <C> 77.2 <C> [BOLD] 80.1 <C> 89.6 <C> 84.0 <C> 86.7 <R> <C> ♠ LSTM-CRF/emb <C> [ITALIC] Hand-tuned <C> 81.6 <C> [BOLD] 80.1 <C> 80.8 <C> 81.6 <C> [BOLD] 78.6 <C> [BOLD] 80.1 <C> 89.8 <C> 85.5 <C> 87.6 <R> <C> Majority Vote <C> [ITALIC] Noun Phrase <C> 84.4 <C> 51.7 <C> 64.1 <C> 76.4 <C> 67.3 <C> 71.5 <C> 86.2 <C> 86.1 <C> 86.2 <R> <C> ♠ CRF/emb <C> [ITALIC] Noun Phrase <C> 56.5 <C> 64.4 <C> 60.2 <C> 81.5 <C> 75.81 <C> 78.5 <C> 89.2 <C> 86.7 <C> 87.9 <R> <C> ♠ LSTM-CRF/emb <C> [ITALIC] Noun Phrase <C> 64.7 <C> 69.7 <C> 67.1 <C> 80.7 <C> 77.6 <C> 79.1 <C> 88.3 <C> [BOLD] 88.3 <C> 88.3 <R> <C> Lexicon Baseline <C> [EMPTY] <C> 36.3 <C> 77.6 <C> 49.5 <C> 40.8 <C> 77.1 <C> 53.3 <C> 62.0 <C> 81.2 <C> 70.3 <CAP> Table 2: Best SwellShark results compared against supervised baselines. Here we add additional labeling functions to improve performance and report scores for both hand-tuned and automatic candidate generators. ♠ indicates the highest scoring model after scaling with additional (≤ 100K) unlabeled documents. * TaggerOne scores are for NER only.
<R> <C> [EMPTY] <C> [BOLD] Ans <C> [BOLD] Email <C> [BOLD] Newsg <C> [BOLD] Rev <C> [BOLD] Webl <R> <C> % unk tag <C> 0.25 <C> 0.80 <C> 0.31 <C> 0.06 <C> 0.0 <R> <C> % OOV <C> 8.53 <C> 10.56 <C> 10.34 <C> 6.84 <C> 8.45 <R> <C> % UWT <C> 2.91 <C> 3.47 <C> 2.43 <C> 2.21 <C> 1.46 <R> <C> Accuracy on OOV tokens <C> Accuracy on OOV tokens <C> Accuracy on OOV tokens <C> Accuracy on OOV tokens <C> Accuracy on OOV tokens <C> Accuracy on OOV tokens <R> <C> Src <C> 54.26 <C> 57.48 <C> [BOLD] 61.80 <C> 59.26 <C> [BOLD] 80.37 <R> <C> Tri <C> [BOLD] 55.53 <C> [BOLD] 59.11 <C> 61.36 <C> [BOLD] 61.16 <C> 79.32 <R> <C> Asym <C> 52.86 <C> 56.78 <C> 56.58 <C> 59.59 <C> 76.84 <R> <C> MT-Tri <C> 52.88 <C> 57.22 <C> 57.28 <C> 58.99 <C> 77.77 <R> <C> Accuracy on unknown word-tag (UWT) tokens <C> Accuracy on unknown word-tag (UWT) tokens <C> Accuracy on unknown word-tag (UWT) tokens <C> Accuracy on unknown word-tag (UWT) tokens <C> Accuracy on unknown word-tag (UWT) tokens <C> Accuracy on unknown word-tag (UWT) tokens <R> <C> Src <C> [BOLD] 17.68 <C> [BOLD] 11.14 <C> [BOLD] 17.88 <C> [BOLD] 17.31 <C> [BOLD] 24.79 <R> <C> Tri <C> 16.88 <C> 10.04 <C> 17.58 <C> 16.35 <C> 23.65 <R> <C> Asym <C> 17.16 <C> 10.43 <C> 17.84 <C> 16.92 <C> 22.74 <R> <C> MT-Tri <C> 16.43 <C> 11.08 <C> 17.29 <C> 16.72 <C> 23.13 <R> <C> FLORS* <C> 17.19 <C> 15.13 <C> 21.97 <C> 21.06 <C> 21.65 <CAP> Table 5: Accuracy scores on dev sets for OOV and unknown word-tag (UWT) tokens.
<R> <C> Core Number <C> Number of words in core <R> <C> 0 <C> 0 <R> <C> 1 <C> 206 <R> <C> 2 <C> 1482 <R> <C> 3 <C> 3014 <R> <C> 4 <C> 4276 <R> <C> 5 <C> 4841 <R> <C> 6 <C> 4813 <R> <C> 7 <C> 4742 <R> <C> 8 <C> 2753 <R> <C> 9 <C> 1000 <R> <C> 10 <C> 0 <CAP> Table 1: Distribution of words amongst cores
<R> <C> [EMPTY] <C> Yelp13 <C> Yelp14 <C> Elec. <C> Games <C> Foods <R> <C> PMF <C> 0.985 <C> 1.053 <C> 1.411 <C> 1.297 <C> 1.251 <R> <C> CTR <C> 0.975 <C> 1.013 <C> 1.284 <C> 1.147 <C> 1.139 <R> <C> ConvMF+ <C> 0.917 <C> 0.954 <C> 1.241 <C> 1.092 <C> 1.084 <R> <C> DeepCoNN <C> 0.880 <C> 0.910 <C> 1.232 <C> 1.130 <C> 0.985 <R> <C> NARRE <C> 0.879 <C> 0.906 <C> 1.215 <C> 1.112 <C> 0.986 <R> <C> TARMF <C> 0.875 <C> 0.909 <C> 1.147 <C> 1.043 <C> 1.019 <R> <C> NRPA <C> [BOLD] 0.872 <C> [BOLD] 0.897 <C> [BOLD] 1.047 <C> [BOLD] 1.014 <C> [BOLD] 0.953 <CAP> Table 2. Comparisons between NRPA and baselines.
<R> <C> [BOLD] Test Dataset <C> [BOLD] Input <C> [BOLD] BLEU <C> [BOLD] ROGUE-1 <C> [BOLD] ROGUE-2 <C> [BOLD] ROGUE-L <R> <C> SQuAD/HarvestingQA <C> Best Hypothesis <C> [BOLD] 60.26 <C> [BOLD] 82.43 <C> [BOLD] 70.61 <C> [BOLD] 78.21 <R> <C> SQuAD/HarvestingQA <C> Confnet <C> 55.38 <C> 81.60 <C> 68.02 <C> 76.68 <R> <C> SQuAD/HarvestingQA <C> Clean Confnet <C> 55.92 <C> 81.39 <C> 67.79 <C> 76.78 <R> <C> Freebase <C> Best Hypothesis <C> [BOLD] 43.21 <C> 71.37 <C> 51.72 <C> 64.98 <R> <C> Freebase <C> Confnet <C> 41.86 <C> 72.42 <C> 51.84 <C> [BOLD] 65.78 <R> <C> Freebase <C> Clean Confnet <C> 42.89 <C> [BOLD] 72.54 <C> [BOLD] 52.77 <C> 66.39 <R> <C> NewsQA <C> Best Hypothesis <C> 49.98 <C> 75.82 <C> 59.59 <C> 72.65 <R> <C> NewsQA <C> Confnet <C> 53.45 <C> [BOLD] 76.45 <C> 60.32 <C> 72.78 <R> <C> NewsQA <C> Clean Confnet <C> [BOLD] 56.86 <C> 76.07 <C> [BOLD] 61.18 <C> [BOLD] 73.12 <CAP> Table 1: Top section shows the scores on 1000 SQuAD/HarvestingQA test samples. Bottom 2 section shows the scores for cross-dataset evaluation on a Knowledge-Base(Freebase) dataset and a machine comprehension(NewsQA) dataset. For each section, the top row displays the score on the best hypothesis of the confusion network, the middle row displays the scores on the confusion network, while the bottom row displays the results on the pruned clean confusion network
<R> <C> model <C> dev WER <C> test WER <C> char/sec <R> <C> pyramidal <C> 15.83 <C> 16.16 <C> 1.1k <R> <C> LSTM/NiN <C> 14.57 <C> 14.70 <C> 1.1k <R> <C> stacked hybrid <C> 16.38 <C> 17.48 <C> 2.4k <R> <C> interleaved hybrid <C> 15.29 <C> 16.71 <C> 1.5k <CAP> Table 1: Comparison to baselines. Training speed (char/sec) was measured on a GTX 1080 Ti GPU.
<R> <C> model <C> dev <C> test <R> <C> add (trig.) <C> diverged <C> diverged <R> <C> concat (trig.) <C> 30.27 <C> 38.60 <R> <C> concat (emb.) <C> 29.81 <C> 31.74 <R> <C> stacked hybrid <C> 16.38 <C> 17.48 <R> <C> interleaved hybrid <C> 15.29 <C> 16.71 <CAP> Table 2: WER results on position modeling.
<R> <C> model <C> dev <C> test <R> <C> stacked hybrid <C> 16.38 <C> 17.48 <R> <C> + local masking <C> 15.42 <C> 16.17 <R> <C> + Gauss mask (init. small) <C> 16.05 <C> 16.96 <R> <C> + Gauss mask (init. large) <C> 14.90 <C> 15.89 <R> <C> interleaved hybrid <C> 15.29 <C> 16.71 <R> <C> + local masking <C> 15.44 <C> 16.19 <R> <C> + Gauss mask (init. small) <C> 16.43 <C> 16.89 <R> <C> + Gauss mask (init. large) <C> 15.00 <C> 15.82 <CAP> Table 3: WER results on attention biasing.
<R> <C> Approach <C> BLEU <R> <C> SMT  <C> 24.9 <R> <C> SMT  <C> 26.9 <R> <C> SMT with our text-labels <C> 22.3 <R> <C> SMT with our sense-labels <C> 24.0 <CAP> Table 4: BLEU@4 in % on sentences of Detailed Descriptions of the TACoS Multi-Level [55] corpus, see Section 5.2.
<R> <C> Corpus <C> Clause <C> NLP <C> Labels <C> WSD <R> <C> TACoS Multi-Level  <C> 0.96 <C> 0.86 <C> 0.91 <C> 0.75 <R> <C> Movie Description (ours) <C> 0.89 <C> 0.62 <C> 0.86 <C> 0.7 <CAP> Table 7: Semantic parser accuracy for TACoS Multi-Level and our new corpus. Discussion in Section 6.2.
<R> <C> Model <C> Seed <C> Update <C> WER (eval) SI <C> WER (eval) SD <R> <C> HDNN- [ITALIC] H512 [ITALIC] L10 <C> [EMPTY] <C> [EMPTY] <C> 27.2 <C> 26.5 <R> <C> HDNN- [ITALIC] H256 [ITALIC] L10 <C> CE <C> [EMPTY] <C> 28.6 <C> 27.9 <R> <C> HDNN- [ITALIC] H512 [ITALIC] L15 <C> [EMPTY] <C> [EMPTY] <C> 27.1 <C> 26.4 <R> <C> HDNN- [ITALIC] H256 [ITALIC] L15 <C> [EMPTY] <C> [ITALIC] θg <C> 28.4 <C> 27.6 <R> <C> HDNN- [ITALIC] H512 [ITALIC] L10 <C> [EMPTY] <C> [EMPTY] <C> 24.9 <C> [BOLD] 24.1 <R> <C> HDNN- [ITALIC] H256 [ITALIC] L10 <C> [EMPTY] <C> [EMPTY] <C> 26.0 <C> [BOLD] 25.0 <R> <C> HDNN- [ITALIC] H512 [ITALIC] L15 <C> sMBR <C> [EMPTY] <C> 24.7 <C> 24.0 <R> <C> HDNN- [ITALIC] H256 [ITALIC] L15 <C> [EMPTY] <C> [EMPTY] <C> 25.9 <C> 24.9 <R> <C> HDNN- [ITALIC] H512 [ITALIC] L10 <C> [EMPTY] <C> { [ITALIC] θh, [ITALIC] θg, [ITALIC] θc} <C> 24.9 <C> 24.5 <R> <C> HDNN- [ITALIC] H256 [ITALIC] L10 <C> [EMPTY] <C> [EMPTY] <C> 26.0 <C> 25.4 <CAP> Table 4: Results of unsupervised speaker adaptation. Here, we only updated θg using the CE criterion, while the speaker-independent (SI) models were trained by either CE or sMBR. SD denotes speaker-dependent models.
<R> <C> [EMPTY] <C> Speed (tokens/sec) <R> <C> Zhang et al. ( 2019 ) <C> 617 <R> <C> Ours <C> [BOLD] 1076 <CAP> Table 1: Parsing speed on the AMR 2.0 test set.
<R> <C> Model <C> [ITALIC] dev93 <C> [ITALIC] eval92 <R> <C> Train on target <C> 23.71 <C> 18.32 <R> <C> + Speed Perturbation <C> 21.67 <C> [EMPTY] <R> <C> + Spectral Masking <C> 22.77 <C> [EMPTY] <R> <C> + Both <C> [BOLD] 20.55 <C> [BOLD] 16.14 <CAP> Table 1: Performance (measured by PER in %) of data augmentation techniques.
<R> <C> Model <C> [ITALIC] dev93 <C> [ITALIC] eval92 <R> <C> Train on Target + DataAug <C> 20.55 <C> 16.14 <R> <C> Pretrain + Finetune <C> 13.60 <C> [EMPTY] <R> <C> + DataAug <C> 13.01 <C> [EMPTY] <R> <C> Pretrain + Finetune + LIN <C> 10.72 <C> [EMPTY] <R> <C> + DataAug <C> [BOLD] 10.09 <C> [BOLD] 7.32 <CAP> Table 2: Performance (PER in %) of domain adaptation techniques. DataAug: Speed perturbation + Spectral masking. “Train on Target + DataAug” is taken from Table 1.
<R> <C> Model <C> [ITALIC] dev93 <C> [ITALIC] eval92 <R> <C> Teacher (5 Bi-LSTM layers, Pretrain <C> [EMPTY] <C> [EMPTY] <R> <C> + Finetune + LIN + DataAug) <C> 5.36 <C> [EMPTY] <R> <C> + word decode <C> 3.68 <C> [EMPTY] <R> <C> Pretrain + Finetune + LIN + DataAug <C> 10.09 <C> 7.32 <R> <C> + Unsup KD (phone decode) <C> 8.37 <C> [EMPTY] <R> <C> + Unsup KD (word decode) <C> [BOLD] 8.25 <C> [BOLD] 6.04 <R> <C> + Self-training <C> 9.50 <C> [EMPTY] <CAP> Table 3: Performance (measured by PER in %) of knowledge distillation using unsupervised data. “Pretrain + Finetune + LIN + DataAug” is taken from Table 2.
<R> <C> Method <C> Accuracy <R> <C> Slope (Lin et al.,  2012 ) <C> 0.58 <R> <C> R square (Lin et al.,  2012 ) <C> 0.57 <R> <C> Lin et al. ( 2012 ) <C> 0.64 <R> <C> Origin characteristic <C> 0.76 <R> <C> Semantic-based entity discovery (Wu et al.,  2016 ) <C> 0.64 <R> <C> Salience-assisted entity discovery <C> 0.78 <R> <C> OSS <C> [BOLD] 0.83 <CAP> Table 4. Results of the Novel Entity Classification task in terms of accuracy. After filtering out “naught” entities, there are about 98% of the mentions in the golden test collections that are either in-KB entities or out-of-KB entities. The binary classification results are listed here.
<R> <C> Models <C> Relevance <C> Fluency <R> <C> VAE-SVG <C> 3.75 <C> 4.07 <R> <C> MC-WGAN <C> [BOLD] 4.09 <C> [BOLD] 4.22 <R> <C> Reference <C> 4.88 <C> 4.95 <CAP> Table 4: Human evaluation results on Quora dataset.
<R> <C> [EMPTY] <C> [BOLD]  Tran Acc <C> [BOLD] Span Acc <C> [BOLD] Span F1 <R> <C> Att-biLSTM <C> 66.6 <C> 58.7 <C> 59.2 <R> <C> Photon <C> [BOLD] 87.6 <C> [BOLD] 83.6 <C> [BOLD] 84.7 <CAP> Table 2: Translatability prediction accuracy (“Tran Acc”) and the confusing spans prediction accuracy and F1 on our SpiderUTran dataset (%).
<R> <C> [BOLD] Model <C> [BOLD] Acc <C> [BOLD] AUC <C> [BOLD] F1 <R> <C> Baseline <C> 0.63 <C> 0.63 <C> 0.63 <R> <C> Baseline+Domain Sentences <C> 0.69 <C> 0.70 <C> 0.69 <R> <C> Baseline+Clinical Sentiment <C> 0.72 <C> 0.72 <C> 0.72 <CAP> Table 2: Results (in ascending order)
<R> <C> [EMPTY] <C> [BOLD] S2 <C> [BOLD] NYT <R> <C> ResNet <C> 47.8±0.23 <C> 100.0±0.15 <R> <C> + AdaDyn <C> 48.0±0.45 <C> 97.9±0.26 <R> <C> + StatCond <C> 46.9±0.13 <C> 97.3±0.16 <R> <C> + AdaDyn + StatCond (Ours) <C> 46.7±0.09 <C> 97.1±0.14 <CAP> TABLE III: Ablation study of the dynamic function fϕ. Results are in micro perplexity.
<R> <C> [EMPTY] <C> Acc <C> P <C> R <C> F1 <R> <C> Shutova et al. (2016) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> linguistic <C> - <C> 67 <C> 76 <C> 71 <R> <C> multimodal <C> - <C> 65 <C> [BOLD] 87 <C> [BOLD] 75 <R> <C> FFN skip-gram <C> 71.2 <C> 70.4 <C> 71.8 <C> 70.5 <R> <C> FFN attribute <C> 68.5 <C> 66.7 <C> 71.0 <C> 68.3 <R> <C> SSN skip-gram <C> [BOLD] 74.8 <C> [BOLD] 73.6 <C> [BOLD] 76.1 <C> [BOLD] 74.2 <R> <C> SSN attribute <C> 69.7 <C> 68.8 <C> 69.7 <C> 68.8 <R> <C> SSN fusion <C> 70.8 <C> 70.1 <C> 70.9 <C> 69.9 <CAP> Table 4: System performance on the Mohammad et al. dataset (moh) in terms of accuracy (Acc), precision (P), recall (R) and F-score (F1).
<R> <C> [EMPTY] <C> [BOLD] Control Total <C> [BOLD] Control Qual. <C> [BOLD] Patients Total <C> [BOLD] Patients Qual. <R> <C> [BOLD] Nouns <C> 934 <C> 226 <C> 242 <C> 90 <R> <C> [BOLD] Adjectives <C> 573 <C> 371 <C> 204 <C> 127 <R> <C> [BOLD] Verbs <C> 699 <C> 60 <C> 204 <C> 34 <R> <C> [BOLD] Adverbs <C> 166 <C> 104 <C> 86 <C> 50 <CAP> Table 6: Experiment 2: Counts of nouns, verbs, and their modifiers, across the two groups. Qual. = Qualified.
<R> <C> [BOLD] Word <C> [ITALIC] αtr <C> [ITALIC] αcn <C> [BOLD] Best Ind. <R> <C> save <C> 6.2 10−1 <C> 1.6 10−3 <C> kNN <R> <C> note <C> 3.8 10−1 <C> 4.7 10−2 <C> kNN <R> <C> march <C> 1.4 10−2 <C> 1.9 10−3 <C> kNN <R> <C> present <C> 6.8 10−2 <C> 2.2 10−2 <C> Naive Bayes <R> <C> jam <C> 1.0 10−2 <C> 6.0 10−3 <C> Naive Bayes <R> <C> ring <C> 3.8 10−9 <C> 2.8 10−4 <C> Naive Bayes <R> <C> just <C> 1.8 10−4 <C> 1.6 10−2 <C> Naive Bayes <R> <C> bear <C> 3.3 10−5 <C> 8.0 10−3 <C> C4.5 <R> <C> rock <C> <1.0 10−10 <C> 9.7 10−9 <C> Naive Bayes <R> <C> close <C> <1.0 10−10 <C> 8.0 10−2 <C> Naive Bayes <CAP> Table 3: Results from characterizing ambiguous words with the traditional approach (second column) and with the CN approach (third column). The p-value (αtr for the traditional approach and αcn for the CN-based method) was computed considering as null model a classifier based on the most common sense. The best classifier algorithm in the fourth column refers to the traditional approach.
<R> <C> [BOLD] Models <C> [BOLD] Relevance  [BOLD] BLEU <C> [BOLD] Relevance  [BOLD] RG-L <C> [BOLD] Diversity  [BOLD]  Dist-1 <C> [BOLD] Diversity  [BOLD]  Dist-2 <C> [BOLD] Diversity  [BOLD]  Ent-4 <R> <C> Vanilla Seq2Seq Model <C> 7.64 <C> 26.68 <C> 0.010 <C> 0.034 <C> 3.370 <R> <C> NQG Du et al. ( 2017 ) <C> 13.97 <C> 31.75 <C> 0.017 <C> 0.068 <C> 6.518 <R> <C> With 1 Layer Reasoning, no RL <C> 16.13 <C> 32.24 <C> 0.053 <C> 0.171 <C> 7.862 <R> <C> With 2 Layer Reasoning, no RL <C> 17.85 <C> 33.06 <C> 0.062 <C> 0.216 <C> 8.285 <R> <C> With 3 Layer Reasoning, no RL <C> 17.42 <C> 32.88 <C> 0.061 <C> 0.205 <C> 8.247 <R> <C> With Dynamic Reasoning, no RL <C> 19.10 <C> 33.57 <C> 0.064 <C> 0.220 <C> 8.304 <R> <C> Reinforced Dynamic Reasoning (ReDR) <C> [BOLD] 19.69 <C> [BOLD] 34.05 <C> [BOLD] 0.069 <C> [BOLD] 0.225 <C> [BOLD] 8.367 <CAP> Table 2: Quantitative evaluation for conversational question generation using CoQA dataset.
<R> <C> [EMPTY] <C> [BOLD] NQG <C> [BOLD] ReDR <C> [BOLD] Human <R> <C> Naturalness <C> 1.94 <C> 1.92 <C> 2.14 <R> <C> Relevance <C> 1.16 <C> 2.02 <C> 2.82 <R> <C> Coherence <C> 1.12 <C> 1.94 <C> 2.94 <R> <C> Richness <C> 1.16 <C> 2.30 <C> 2.54 <R> <C> Answerability <C> 1.18 <C> 1.86 <C> 2.96 <CAP> Table 3: Human evaluation results on CoQA. “Human” in the table means the original human-created questions in CoQA.
<R> <C> Dataset <C> #Events <C> #Event-Event Relations <R> <C> TimeBank <C> 7,935 <C> 3,481 <R> <C> TempEval 2010 <C> 5,688 <C> 3,308 <R> <C> TempEval 2013 <C> 11,145 <C> 5,272 <R> <C> TimeBank-Dense <C> 1,729 <C> 8,130 <R> <C> Hong et al. ( 2016 ) <C> 863 <C> 25,610 <R> <C> [BOLD] UDS-T <C> [BOLD] 32,302 <C> [BOLD] 70,368 <CAP> Table 1: Number of total events, and event-event temporal relations captured in various corpora
<R> <C> Data <C> EU Para <C> UN Data <C> GlobalVoices <C> JRCAcquis <C> EU Bookshops <C> OpenSub <C> Medline <R> <C> Transformer <C> 34.04 <C> 49.88 <C> 42.28 <C> 52.22 <C> 20.78 <C> 25.54 <C> 50.95 <R> <C> +Fine Tune <C> 37.10 <C> 54.81 <C> 45.24 <C> 60.07 <C> 22.29 <C> 27.31 <C> 57.58 <R> <C> MetaMT <C> 39.02 <C> 55.13 <C> 47.73 <C> 61.04 <C> 22.74 <C> 29.43 <C> 59.06 <R> <C> -enc-proj <C> 38.32 <C> 55.01 <C> 46.95 <C> 60.93 <C> 22.25 <C> 28.97 <C> 58.77 <R> <C> -dec-proj <C> 37.34 <C> 54.92 <C> 45.73 <C> 60.52 <C> 21.02 <C> 28.35 <C> 58.22 <CAP> Table 2: Performance Comparison (BLEU-4)
<R> <C> [BOLD] Hyperparameter <C> [BOLD] ConveRT <C> [BOLD] BERT <C> [BOLD] Vanilla <R> <C> Dimensionality of the input subword embeddings <C> 512 <C> 768 <C> 32 <R> <C> Size of minibatches during training <C> 16 <C> 16 <C> 64 <R> <C> The learning rate for the SGD optimizer <C> 0.01 <C> 0.01 <C> 0.1 <R> <C> Keep probability of elements in the sub-word embedding <C> 0.5 <C> 0.9 <C> 0.5 <R> <C> Keep probability of elements in the sub-word feature embeddings <C> 0.6 <C> 0.6 <C> 0.5 <R> <C> The size of the subword-CNN filters <C> (128, 64) <C> (128, 64) <C> (100, 100, 100) <R> <C> Width of the subword CNN filters <C> (1, 5) <C> (1, 5) <C> (8, 4, 1) <R> <C> Activation function for subword CNN <C> swish <C> swish <C> swish <CAP> Table 2: The final hyper-parameters used for different subword representations; swish refers to swish activation taken from swish.
<R> <C> Task <C> Corpus <C> Sg <C> Mu <C> BiMu <C> BiMu-Sg <R> <C> [BOLD] SCWS <C> RU-EN <C> 54.8 <C> 57.3 <C> [BOLD] 59.5 <C> 4.7_0.9^9.8 <R> <C> [BOLD] SCWS <C> CZ-EN <C> 51.2 <C> 54.0 <C> [BOLD] 55.3 <C> 4.1_-0.6^8.8 <R> <C> [BOLD] SCWS <C> FR-EN <C> 58.8 <C> 60.4 <C> [BOLD] 60.5 <C> 1.7_-2.6^5.9 <R> <C> [BOLD] SCWS <C> FR-EN (NC) <C> 47.2 <C> 52.4 <C> [BOLD] 54.3 <C> 7.1_2.2^12.0 <R> <C> [BOLD] SCWS <C> RU-EN (NC) <C> 47.3 <C> [BOLD] 54.0 <C> [BOLD] 54.0 <C> 6.7_0.6^12.8 <R> <C> [BOLD] SCWS <C> CZ-EN (NC) <C> 47.7 <C> [BOLD] 52.1 <C> 51.9 <C> 4.2_-2.0^10.3 <R> <C> [BOLD] SCWS <C> DE-EN (NC) <C> 48.5 <C> 52.9 <C> [BOLD] 54.0 <C> 5.5_-0.6^11.6 <R> <C> [BOLD] SCWS <C> ES-EN (NC) <C> 47.2 <C> 53.2 <C> [BOLD] 54.5 <C> 7.3_1.1^13.3 <R> <C> [BOLD] Similarity <C> RU-EN <C> 37.8 <C> 41.2 <C> [BOLD] 46.3 <C> [EMPTY] <R> <C> [BOLD] Similarity <C> CZ-EN <C> 39.5 <C> 36.9 <C> [BOLD] 41.9 <C> [EMPTY] <R> <C> [BOLD] Similarity <C> FR-EN <C> [BOLD] 46.3 <C> 42.0 <C> 43.5 <C> [EMPTY] <R> <C> [BOLD] Similarity <C> FR-EN (NC) <C> 17.9 <C> 26.0 <C> [BOLD] 27.6 <C> [EMPTY] <R> <C> [BOLD] Similarity <C> RU-EN (NC) <C> 19.3 <C> 27.3 <C> [BOLD] 28.4 <C> [EMPTY] <R> <C> [BOLD] Similarity <C> CZ-EN (NC) <C> 15.8 <C> [BOLD] 26.6 <C> 25.4 <C> [EMPTY] <R> <C> [BOLD] Similarity <C> DE-EN (NC) <C> 20.7 <C> 28.4 <C> [BOLD] 30.8 <C> [EMPTY] <R> <C> [BOLD] Similarity <C> ES-EN (NC) <C> 19.9 <C> 27.2 <C> [BOLD] 31.2 <C> [EMPTY] <R> <C> [BOLD] Qvec <C> RU-EN <C> 55.8 <C> 56.0 <C> [BOLD] 56.5 <C> [EMPTY] <R> <C> [BOLD] Qvec <C> CZ-EN <C> [BOLD] 56.6 <C> 56.5 <C> 55.9 <C> [EMPTY] <R> <C> [BOLD] Qvec <C> FR-EN <C> 57.5 <C> 57.1 <C> [BOLD] 57.6 <C> [EMPTY] <R> <C> [BOLD] POS <C> RU-EN <C> [BOLD] 93.5 <C> 93.2 <C> 93.3 <C> [EMPTY] <R> <C> [BOLD] POS <C> CZ-EN <C> [BOLD] 94.0 <C> 93.7 <C> [BOLD] 94.0 <C> [EMPTY] <R> <C> [BOLD] POS <C> FR-EN <C> [BOLD] 94.1 <C> 93.8 <C> 94.0 <C> [EMPTY] <CAP> Table 2: Results, per-row best in bold. Sg and Mu are trained on the English part of the parallel corpora. In BiMu-Sg, we report the difference between BiMu and Sg, together with the 95% CI of that difference. The Similarity scores are averaged over 12 benchmarks described in § 5.1. For POS tagging, we report the accuracy.
<R> <C> [BOLD] KB/Word  [BOLD] Embedding <C> [BOLD] RG-65  [ITALIC] ρ <C> [BOLD] RG-65  [ITALIC] r <C> [BOLD] WSS-353  [ITALIC] ρ <C> [BOLD] WSS-353  [ITALIC] r <C> [BOLD] SimLex-999  [ITALIC] ρ <C> [BOLD] SimLex-999  [ITALIC] r <R> <C> node2vec <C> 0.88 <C> 0.86 <C> 0.67 <C> 0.70 <C> 0.36 <C> 0.39 <R> <C> DeepWalk <C> 0.86 <C> 0.86 <C> 0.69 <C> 0.70 <C> 0.35 <C> 0.38 <R> <C> w2v-gn <C> 0.75 <C> 0.77 <C> 0.77 <C> 0.76 <C> 0.44 <C> 0.45 <R> <C> GloVe <C> 0.76 <C> 0.75 <C> 0.66 <C> 0.66 <C> 0.37 <C> 0.39 <CAP> Table 1: Pearson (r) and Spearman (ρ) correlation results on three word similarity datasets.
<R> <C> Models <C> Avg <C> Std dev <C> Max <C> p-value <R> <C> BERT-BiLSTM-CRF (baseline) <C> 68.68 <C> 1.07 <C> 70.79 <C> [EMPTY] <R> <C> GLYNN <C> 69.2 <C> 1.11 <C> [BOLD] 71.81 <C> .03 <R> <C> GLYNN + dropout .5 <C> 69.01 <C> 1.11 <C> 71.58 <C> .18 <R> <C> strided <C> 68.67 <C> .97 <C> 70.7 <C> .97 <R> <C> Glyce (Glyce) <C> [EMPTY] <C> [EMPTY] <C> 67.6 <C> [EMPTY] <R> <C> latticeLSTM (latticeLSTM) <C> [EMPTY] <C> [EMPTY] <C> 58.79 <C> [EMPTY] <CAP> Table 5: Results on Full Weibo
<R> <C> [BOLD] Task <C> [BOLD] Dataset <C> [BOLD] Metrics <C> [BOLD] E-SVM <C> [BOLD] CNN-M <C> [BOLD] BERT-Base <C> [BOLD] BioBERT <C> [BOLD] SCIBERT <C> [BOLD] BERT-MK <R> <C> Entity <C> 2010 i2b2/VA <C> Acc <C> - <C> - <C> 96.76 <C> 97.43 <C> [BOLD] 97.74 <C> 97.70 <R> <C> Typing <C> JNLPBA <C> Acc <C> - <C> - <C> 94.12 <C> 94.37 <C> [BOLD] 94.60 <C> 94.55 <R> <C> [EMPTY] <C> BC5CDR <C> Acc <C> - <C> - <C> 98.78 <C> 99.27 <C> 99.38 <C> [BOLD] 99.54 <R> <C> [EMPTY] <C> Average <C> Acc <C> - <C> - <C> 96.55 <C> 97.02 <C> 97.24 <C> [BOLD] 97.26 <R> <C> Relation <C> 2010 i2b2/VA <C> P <C> - <C> 73.1 <C> 72.6 <C> 76.1 <C> 74.8 <C> [BOLD] 77.6 <R> <C> Classification <C> [EMPTY] <C> R <C> - <C> 66.7 <C> 65.7 <C> 71.3 <C> 71.6 <C> [BOLD] 72.0 <R> <C> [EMPTY] <C> [EMPTY] <C> F <C> - <C> 69.7 <C> 69.2 <C> 73.6 <C> 73.1 <C> [BOLD] 74.7 <R> <C> [EMPTY] <C> GAD <C> P <C> 79.21 <C> - <C> 74.28 <C> 76.43 <C> 77.47 <C> [BOLD] 81.67 <R> <C> [EMPTY] <C> [EMPTY] <C> R <C> 89.25 <C> - <C> 85.11 <C> 87.65 <C> 85.94 <C> [BOLD] 92.79 <R> <C> [EMPTY] <C> [EMPTY] <C> F <C> 83.93 <C> - <C> 79.33 <C> 81.66 <C> 81.45 <C> [BOLD] 86.87 <R> <C> [EMPTY] <C> EU-ADR <C> P <C> - <C> - <C> 75.45 <C> 81.05 <C> 78.42 <C> [BOLD] 84.43 <R> <C> [EMPTY] <C> [EMPTY] <C> R <C> - <C> - <C> [BOLD] 96.55 <C> 93.90 <C> 90.09 <C> 91.17 <R> <C> [EMPTY] <C> [EMPTY] <C> F <C> - <C> - <C> 84.71 <C> 87.00 <C> 85.51 <C> [BOLD] 87.49 <R> <C> [EMPTY] <C> Average <C> P <C> - <C> - <C> 74.11 <C> 77.86 <C> 76.90 <C> [BOLD] 81.23 <R> <C> [EMPTY] <C> [EMPTY] <C> R <C> - <C> - <C> 82.45 <C> 84.23 <C> 82.54 <C> [BOLD] 85.32 <R> <C> [EMPTY] <C> [EMPTY] <C> F <C> - <C> - <C> 77.75 <C> 80.75 <C> 80.02 <C> [BOLD] 83.02 <CAP> Table 3: Experimental results on the entity typing and relation classification tasks. Accuracy (Acc), Precision, Recall, and F1 scores are used to evaluate the model performance. The results given in previous work are underlined. E-SVM is short for Ensemble SVM [Bhasuran and Natarajan2018], which achieves SOTA performance in GAD. CNN-M stands for CNN using multi-pooling [He, Guan, and Dai2019], and this model is the SOTA model in 2010 i2b2/VA.
<R> <C> [BOLD] Dataset <C> [BOLD] MedERNIE P <C> [BOLD] MedERNIE R <C> [BOLD] MedERNIE F <C> [BOLD] BERT-MK P <C> [BOLD] BERT-MK R <C> [BOLD] BERT-MK F <R> <C> 2010 i2b2/VA <C> 76.6 <C> 71.1 <C> 73.8 <C> 77.6 <C> 72.0 <C> 74.7 <R> <C> GAD <C> 81.28 <C> 91.86 <C> 86.23 <C> 81.67 <C> 92.79 <C> 86.87 <R> <C> Average <C> 78.94 <C> 81.48 <C> 80.02 <C> [BOLD] 79.64 <C> [BOLD] 82.40 <C> [BOLD] 80.79 <CAP> Table 4: TransE vs. KG-Transformer. Since the EU-ADR dataset is too small, the model comparison on this dataset is not listed in this table.
<R> <C> [ITALIC] Er=  [BOLD] SNR (dB) <C> [ITALIC] Er= DeepBeam S1 <C> -10 18.5 <C> 0 22.0 <C> 10 26.5 <C> 20 28.4 <R> <C> [BOLD] SNR (dB) <C> DeepBeam S2 <C> 17.1 <C> 20.3 <C> 25.9 <C> 27.4 <R> <C> [BOLD] SNR (dB) <C> DeepBeam S3 <C> 15.3 <C> 19.5 <C> 24.1 <C> 27.6 <R> <C> [BOLD] SNR (dB) <C> DeepBeam S4 <C> 14.1 <C> 19.0 <C> 23.1 <C> 28.5 <R> <C> [BOLD] SNR (dB) <C> GRAB <C> 2.48 <C> 12.5 <C> 21.6 <C> 25.4 <R> <C> [BOLD] SNR (dB) <C> CLOSEST <C> -5.13 <C> 3.38 <C> 14.9 <C> 24.8 <R> <C> [BOLD] SNR (dB) <C> MVDR <C> 8.41 <C> 12.9 <C> 22.6 <C> 26.7 <R> <C> [BOLD] SNR (dB) <C> IVA <C> 10.3 <C> 13.3 <C> 16.8 <C> 19.2 <R> <C> [BOLD] DRR (dB) <C> DeepBeam S1 <C> 3.45 <C> 8.97 <C> 11.2 <C> 11.5 <R> <C> [BOLD] DRR (dB) <C> DeepBeam S2 <C> 7.38 <C> 11.9 <C> 12.6 <C> 11.5 <R> <C> [BOLD] DRR (dB) <C> DeepBeam S3 <C> 5.60 <C> 4.85 <C> 8.43 <C> 9.78 <R> <C> [BOLD] DRR (dB) <C> DeepBeam S4 <C> 2.11 <C> 6.68 <C> 7.10 <C> 9.31 <R> <C> [BOLD] DRR (dB) <C> GRAB <C> -0.83 <C> 1.70 <C> 3.63 <C> 3.68 <R> <C> [BOLD] DRR (dB) <C> CLOSEST <C> 8.56 <C> 7.32 <C> 7.67 <C> 8.44 <R> <C> [BOLD] DRR (dB) <C> MVDR <C> -2.17 <C> -3.47 <C> -3.42 <C> -4.13 <R> <C> [BOLD] DRR (dB) <C> IVA <C> -8.92 <C> -8.77 <C> -8.81 <C> -8.99 <R> <C> S1: seen speaker, seen noise; S2: seen speaker, unseen noise; <C> S1: seen speaker, seen noise; S2: seen speaker, unseen noise; <C> S1: seen speaker, seen noise; S2: seen speaker, unseen noise; <C> S1: seen speaker, seen noise; S2: seen speaker, unseen noise; <C> S1: seen speaker, seen noise; S2: seen speaker, unseen noise; <C> S1: seen speaker, seen noise; S2: seen speaker, unseen noise; <R> <C> S3: unseen speaker, seen noise; S4: unseen speaker, unseen noise. <C> S3: unseen speaker, seen noise; S4: unseen speaker, unseen noise. <C> S3: unseen speaker, seen noise; S4: unseen speaker, unseen noise. <C> S3: unseen speaker, seen noise; S4: unseen speaker, unseen noise. <C> S3: unseen speaker, seen noise; S4: unseen speaker, unseen noise. <C> S3: unseen speaker, seen noise; S4: unseen speaker, unseen noise. <CAP> Table 1: Simulated Data Evaluation Results.
<R> <C> [BOLD] Noise Type  [BOLD] SNR (dB) <C> [BOLD] Noise Type DeepBeam <C> N1  [BOLD] 20.1 <C> N2  [BOLD] 20.0 <C> N3  [BOLD] 16.9 <C> N4  [BOLD] 19.6 <C> N5  [BOLD] 18.7 <R> <C> [BOLD] SNR (dB) <C> GRAB <C> 18.9 <C> 17.4 <C> 12.4 <C> 18.5 <C> 17.4 <R> <C> [BOLD] SNR (dB) <C> CLOSEST <C> 10.0 <C> 10.0 <C> 10.0 <C> 10.0 <C> 10.0 <R> <C> [BOLD] SNR (dB) <C> MVDR <C> 10.8 <C> 16.5 <C> 7.72 <C> 14.0 <C> 13.4 <R> <C> [BOLD] SNR (dB) <C> IVA <C> 11.7 <C> 9.74 <C> 6.83 <C> 12.4 <C> 15.9 <R> <C> [BOLD] MOS <C> DeepBeam <C> [BOLD] 3.83 <C> [BOLD] 3.72 <C> [BOLD] 3.63 <C> [BOLD] 4.09 <C> [BOLD] 4.20 <R> <C> [BOLD] MOS <C> GRAB <C> 3.10 <C> 3.06 <C> 2.93 <C> 3.71 <C> 3.45 <R> <C> [BOLD] MOS <C> CLOSEST <C> 2.74 <C> 2.68 <C> 3.02 <C> 3.55 <C> 3.50 <R> <C> [BOLD] MOS <C> MVDR <C> 2.05 <C> 2.40 <C> 2.28 <C> 2.71 <C> 2.62 <R> <C> [BOLD] MOS <C> IVA <C> 1.73 <C> 2.03 <C> 1.75 <C> 1.78 <C> 2.08 <R> <C> N1: cell phone; N2: CombBind machine; N3:paper shuffle; <C> N1: cell phone; N2: CombBind machine; N3:paper shuffle; <C> N1: cell phone; N2: CombBind machine; N3:paper shuffle; <C> N1: cell phone; N2: CombBind machine; N3:paper shuffle; <C> N1: cell phone; N2: CombBind machine; N3:paper shuffle; <C> N1: cell phone; N2: CombBind machine; N3:paper shuffle; <C> N1: cell phone; N2: CombBind machine; N3:paper shuffle; <R> <C> N4: door slide; N5: footsteps. <C> N4: door slide; N5: footsteps. <C> N4: door slide; N5: footsteps. <C> N4: door slide; N5: footsteps. <C> N4: door slide; N5: footsteps. <C> N4: door slide; N5: footsteps. <C> N4: door slide; N5: footsteps. <CAP> Table 2: Realworld Data Evaluation Results.
<R> <C> Model <C> Len <C> NLL <C> Fail (%) <C> Dup (%) <R> <C> Oracle <C> 10(30) <C> 2.43 <C> − <C> 11.2 <R> <C> LSTM <C> 10(30) <C> 3.85 <C> 54.1 <C> 8.6 <R> <C> SeqGAN <C> 10(30) <C> 0.67 <C> 2.6 <C> 93.0 <R> <C> LeakGAN <C> 10(30) <C> 8.25† <C> 52.0 <C> 0.0 <R> <C> TDTD <C> 10(30) <C> [BOLD] 3.58 <C> 0.0 <C> 25.7 <R> <C> Oracle <C> 15(45) <C> 2.63 <C> − <C> 1.3 <R> <C> LSTM <C> 15(45) <C> 6.39 <C> 66.2 <C> 0.0 <R> <C> SeqGAN <C> 15(45) <C> 7.41† <C> 93.7 <C> 0.0 <R> <C> LeakGAN <C> 15(45) <C> 6.42† <C> 78.2 <C> 0.0 <R> <C> TDTD <C> 15(45) <C> [BOLD] 3.86 <C> 0.0 <C> 16.7 <R> <C> Oracle <C> 20(60) <C> 2.85 <C> − <C> 0.3 <R> <C> LSTM <C> 20(60) <C> 7.55 <C> 67.8 <C> 0.0 <R> <C> SeqGAN <C> 20(60) <C> 7.88† <C> 94.2 <C> 0.0 <R> <C> LeakGAN <C> 20(60) <C> 6.79† <C> 71.1 <C> 0.0 <R> <C> TDTD <C> 20(60) <C> [BOLD] 4.32 <C> 0.0 <C> 11.8 <CAP> Table 2: Results on synthetic datasets. “Len” means number of nodes in the tree, and the number in bracket is the length of brackets sequence. “Fail” denotes the percentage of ill-formed generated samples, which have unmatched brackets and cannot be converted into a tree. “Dup” is the percentage of duplicate samples in all the generated samples. Failed samples are not counted in the NLL score. † means that the performance falls after a few iterations during the training, in which case we perform early-stop.
<R> <C> Algorithm <C> Weighted  [ITALIC] DKL <R> <C> GLA <C> 0.92 <R> <C> Perceptron <C> 0.80 <R> <C> MaxEnt <C> 0.53 <CAP> Table 13: Weighted, by number of sentences with a given input pattern in the test data, KL-divergence between the true distribution and the predicted distributions. KL-divergence is averaged over ten runs. KL-divergence is measured in bits and smaller values are better than larger values.
<R> <C> Predictor <C> [ITALIC] DKL <C> Accuracy <R> <C> Uniform distribution over word order labels <C> 1.53 <C> 16.7% <R> <C> Predict most likely single word order for input <C> 5.54 <C> 69.4% <R> <C> GLA prediction <C> 0.92 <C> 59.7% <R> <C> MaxEnt <C> 0.53 <C> 66.5% <R> <C> Perceptron prediction <C> 0.80 <C> 67.0% <CAP> Table 17: KL-divergence between the true distribution over word orders in a held out test set of 1,000 sentences and five strategies for predicting a distribution over word orders: a uniform distribution over word order labels, always predicting the most frequent word order label for a given input pattern (note that this strategy gives the upper bound on accuracy), and using the GLA, MaxEnt, and perceptron algorithms to learn models and make predictions.
<R> <C> [EMPTY] <C> MUC P <C> MUC R <C> MUC F1 <C> [ITALIC] B3 P <C> [ITALIC] B3 R <C> [ITALIC] B3 F1 <C> CEAF [ITALIC] e P <C> CEAF [ITALIC] e R <C> CEAF [ITALIC] e F1 <C> CoNLL F1 <R> <C> [EMPTY] <C> Cross-document Event Coreference Resolution (CD) <C> Cross-document Event Coreference Resolution (CD) <C> Cross-document Event Coreference Resolution (CD) <C> Cross-document Event Coreference Resolution (CD) <C> Cross-document Event Coreference Resolution (CD) <C> Cross-document Event Coreference Resolution (CD) <C> Cross-document Event Coreference Resolution (CD) <C> Cross-document Event Coreference Resolution (CD) <C> Cross-document Event Coreference Resolution (CD) <C> Cross-document Event Coreference Resolution (CD) <R> <C> Lemma <C> 75.1 <C> 55.4 <C> 63.8 <C> 71.7 <C> 39.6 <C> 51.0 <C> 36.2 <C> 61.1 <C> 45.5 <C> 53.4 <R> <C> hdp-lex <C> 75.5 <C> 63.5 <C> 69.0 <C> 65.6 <C> 43.7 <C> 52.5 <C> 34.8 <C> 60.2 <C> 44.1 <C> 55.2 <R> <C> Agglomerative <C> 78.3 <C> 59.2 <C> 67.4 <C> 73.2 <C> 40.2 <C> 51.9 <C> 30.2 <C> 65.6 <C> 41.4 <C> 53.6 <R> <C> ddcrp <C> 79.6 <C> 58.2 <C> 67.1 <C> 78.1 <C> 39.6 <C> 52.6 <C> 31.8 <C> [BOLD] 69.4 <C> 43.6 <C> 54.4 <R> <C> hddcrp∗ <C> 77.5 <C> 66.4 <C> 71.5 <C> 69.0 <C> [BOLD] 48.1 <C> [BOLD] 56.7 <C> 38.2 <C> 63.0 <C> 47.6 <C> 58.6 <R> <C> hddcrp <C> [BOLD] 80.3 <C> [BOLD] 67.1 <C> [BOLD] 73.1 <C> [BOLD] 78.5 <C> 40.6 <C> 53.5 <C> [BOLD] 38.6 <C> 68.9 <C> [BOLD] 49.5 <C> [BOLD] 58.7 <R> <C> [EMPTY] <C> Within-document Event Coreference Resolution (WD) <C> Within-document Event Coreference Resolution (WD) <C> Within-document Event Coreference Resolution (WD) <C> Within-document Event Coreference Resolution (WD) <C> Within-document Event Coreference Resolution (WD) <C> Within-document Event Coreference Resolution (WD) <C> Within-document Event Coreference Resolution (WD) <C> Within-document Event Coreference Resolution (WD) <C> Within-document Event Coreference Resolution (WD) <C> Within-document Event Coreference Resolution (WD) <R> <C> Lemma <C> 60.9 <C> 30.2 <C> 40.4 <C> 78.9 <C> 57.3 <C> 66.4 <C> 63.6 <C> 69.0 <C> 66.2 <C> 57.7 <R> <C> hdp-lex <C> 50.0 <C> 39.1 <C> 43.9 <C> 74.7 <C> 67.6 <C> 71.0 <C> 66.2 <C> 71.4 <C> 68.7 <C> 61.2 <R> <C> Agglomerative <C> 61.9 <C> 39.2 <C> 48.0 <C> 80.7 <C> 67.6 <C> 73.5 <C> 65.6 <C> 76.0 <C> 70.4 <C> 63.9 <R> <C> ddcrp <C> 71.2 <C> 36.4 <C> 48.2 <C> 85.4 <C> 64.9 <C> 73.8 <C> 61.8 <C> 76.1 <C> 68.2 <C> 63.4 <R> <C> hddcrp∗ <C> 58.1 <C> [BOLD] 42.8 <C> 49.3 <C> 78.4 <C> [BOLD] 68.7 <C> 73.2 <C> [BOLD] 67.6 <C> 74.5 <C> 70.9 <C> 64.5 <R> <C> hddcrp <C> [BOLD] 74.3 <C> 41.7 <C> [BOLD] 53.4 <C> [BOLD] 85.6 <C> 67.3 <C> [BOLD] 75.4 <C> 65.1 <C> [BOLD] 79.8 <C> [BOLD] 71.7 <C> [BOLD] 66.8 <CAP> Table 3: Within- and cross-document coreference results on the ECB+ corpus
<R> <C> [BOLD] QA  [BOLD] Task <C> [BOLD] #totalfold <C> [BOLD] #answer-  [BOLD] able <C> [BOLD] #Predicted  [BOLD] Top1 <C> [BOLD] #Predicted  [BOLD] Top2 <C> [BOLD] #Predicted  [BOLD] Top3 <C> [BOLD] Setting <R> <C> NED <C> 324.3 <C> 294.2 <C> 245.2 <C> 270.9 <C> 284.3 <C> Logistic Regression <R> <C> NED <C> 324.3 <C> 309.1 <C> 249.5 <C> 270.5 <C> 283.6 <C> [BOLD] Random Forest <R> <C> RL <C> 324.3 <C> 153.1 <C> 90.3 <C> 118.9 <C> 134.4 <C> Logistic Regression <R> <C> RL <C> 324.3 <C> 231.7 <C> 91 <C> 116.6 <C> 134.3 <C> [BOLD] Random Forest <R> <C> CL <C> 324.3 <C> 76 <C> 68.1 <C> 76 <C> – <C> Logistic Regression <R> <C> CL <C> 324.3 <C> 76 <C> 68.8 <C> 76 <C> – <C> [BOLD] Random Forest <R> <C> QB <C> 324.3 <C> 175.4 <C> 162.7 <C> 175.4 <C> – <C> [BOLD] Logistic Regression <R> <C> QB <C> 324.3 <C> 175.4 <C> 159.5 <C> 175.4 <C> – <C> Random Forest <CAP> Table 5: Evaluation of the Impact of Supervised Learning Models. For NED and RL, Random Forest performs slightly better than Logistic Regression.
<R> <C> [BOLD] Subband Analysis <C> [BOLD] ERROR [%]  [BOLD] Maj. Voting <C> [BOLD] ERROR [%]  [BOLD] Stack. Gen. <R> <C> Level-4 wavelet decomposition <C> 43.7 <C> [BOLD] 31.8 <R> <C> Level-4 wavelet packet decomposition <C> 45.1 <C> [BOLD] 33.1 <R> <C> DCT (16 uniform-width bands) <C> 44 <C> [BOLD] 32.6 <R> <C> 16-channel CMFB <C> 42.4 <C> [BOLD] 31.2 <R> <C> [BOLD] Composite Waveform  <C> [BOLD] 36.7 <C> [BOLD] 36.7 <CAP> TABLE I: Errors obtained with different subband decompositions [53] (listed in the left column) and aggregation schemes for subband classification in quiet condition.
<R> <C> [BOLD] Model <C> [BOLD] MRR <C> [BOLD] NDCG@10 <C> [BOLD] NCG@100 <R> <C> [BOLD] Non-neural baselines <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> BM25+RM3 run with best NDCG@10 <C> 0.807 <C> 0.549 <C> 0.559 <R> <C> Non-neural run with best NDCG@10 <C> 0.872 <C> 0.561 <C> 0.560 <R> <C> [BOLD] Neural baselines <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> DeepCT run with best NDCG@10 <C> 0.872 <C> 0.554 <C> 0.498 <R> <C> BERT-based document expansion + reranking run with best NCG@10 <C> 0.899 <C> 0.646 <C> 0.637 <R> <C> BERT-based document expansion + reranking run with best NDCG@10 <C> 0.961 <C> 0.726 <C> 0.580 <R> <C> [BOLD] Our models <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Conformer-Kernel <C> 0.845 <C> 0.554 <C> 0.464 <R> <C> Conformer-Kernel + learned BM25 <C> 0.906 <C> 0.603 <C> 0.533 <R> <C> Conformer-Kernel + learned BM25 + ORCAS field <C> 0.898 <C> 0.620 <C> 0.547 <CAP> Table 1: Full retrieval results based on the TREC 2019 Deep Learning track test set.
<R> <C> Method <C> CIDEr↑ <C> CIDErBtw↓ <C> BLEU3↑ <C> BLEU4↑ <C> METEOR↑ <C> ROUGE-L↑ <C> SPICE↑ <C> R@1↑ <C> R@5 ↑ <C> R@10↑ <R> <C> FC [rennie2017self] <C> 97.90 <C> 83.35 <C> 41.81 <C> 31.58 <C> 25.22 <C> 53.34 <C> 17.99 <C> 15.44 <C> 40.36 <C> 55.08 <R> <C> FC+CIDErBtw (ours) <C> 98.82 <C> 83.22 <C> 42.03 <C> 31.79 <C> 25.46 <C> 53.48 <C> 18.29 <C> 16.24 <C> 41.54 <C> 56.64 <R> <C> Att2in [rennie2017self] <C> 110.04 <C> 83.19 <C> 46.36 <C> 35.75 <C> 26.79 <C> 56.18 <C> 19.91 <C> 17.44 <C> 43.88 <C> 58.02 <R> <C> Att2in+CIDErBtw (ours) <C> 110.97 <C> 82.42 <C> 46.63 <C> 36.0 <C> 27.03 <C> 56.30 <C> 20.01 <C> 17.98 <C> 44.72 <C> 58.62 <R> <C> UpDown [updown] <C> 111.25 <C> 79.46 <C> 45.64 <C> 35.93 <C> 27.54 <C> 56.24 <C> 20.54 <C> 20.10 <C> 47.58 <C> 61.92 <R> <C> UpDown+CIDErBtw (ours) <C> 112.77 <C> 78.34 <C> 46.35 <C> 36.10 <C> 27.69 <C> 56.36 <C> 20.68 <C> 20.92 <C> 49.72 <C> 63.98 <R> <C> Transformer [vaswani2017attention] <C> 110.13 <C> 80.98 <C> 44.80 <C> 34.46 <C> 26.98 <C> 55.30 <C> 20.18 <C> 21.52 <C> 49.88 <C> 64.70 <R> <C> Transformer+CIDErBtw (ours) <C> 112.44 <C> [BOLD] 75.35 <C> 45.44 <C> 35.01 <C> 27.59 <C> 55.66 <C> 20.74 <C> 21.84 <C> 50.48 <C> 65.04 <R> <C> FC+SCST [rennie2017self] <C> 104.43 <C> 90.09 <C> 43.10 <C> 31.59 <C> 25.46 <C> 54.33 <C> 18.67 <C> 11.44 <C> 33.16 <C> 48.04 <R> <C> FC+SCST+CIDErBtw (ours) <C> 104.76 <C> 89.41 <C> 43.25 <C> 31.72 <C> 25.60 <C> 54.35 <C> 18.58 <C> 11.74 <C> 33.62 <C> 48.32 <R> <C> Att2in+SCST [rennie2017self] <C> 117.96 <C> 87.40 <C> 47.22 <C> 35.31 <C> 27.17 <C> 56.92 <C> 20.57 <C> 16.00 <C> 41.55 <C> 56.66 <R> <C> Att2in+SCST+CIDErBtw (ours) <C> 118.48 <C> 87.21 <C> 47.33 <C> 35.41 <C> 27.27 <C> 56.94 <C> 20.77 <C> 16.82 <C> 42.26 <C> 57.72 <R> <C> UpDown+SCST [updown] <C> 121.94 <C> 86.82 <C> 48.82 <C> 36.12 <C> 27.95 <C> 57.61 <C> 21.29 <C> 18.50 <C> 46.34 <C> 61.70 <R> <C> UpDown+SCST+CIDErBtw (ours) <C> 123.02 <C> 86.42 <C> 48.98 <C> 36.39 <C> 28.12 <C> 57.78 <C> 21.44 <C> 19.68 <C> 47.30 <C> 62.78 <R> <C> Transformer+SCST [vaswani2017attention] <C> 125.13 <C> 86.68 <C> 50.26 <C> 38.04 <C> 27.96 <C> 58.60 <C> 22.30 <C> 23.38 <C> 54.34 <C> 68.44 <R> <C> Transformer+SCST+CIDErBtw (ours) <C> [BOLD] 128.11 <C> 84.70 <C> [BOLD] 51.29 <C> [BOLD] 39.0 <C> [BOLD] 29.12 <C> [BOLD] 59.24 <C> 22.92 <C> 24.46 <C> 55.22 <C> 69.02 <R> <C> +CIDErBtwReward (ours) <C> 127.78 <C> 82.74 <C> 50.97 <C> 38.52 <C> 29.09 <C> 58.82 <C> [BOLD] 22.96 <C> [BOLD] 26.46 <C> [BOLD] 57.98 <C> [BOLD] 71.28 <R> <C> Stack-Cap [gu2018stack] <C> 120.4 <C> 88.7 <C> 47.9 <C> 36.1 <C> 27.4 <C> 56.9 <C> 20.9 <C> 21.9 <C> 49.7 <C> 63.7 <R> <C> DiscCap [luo2018discriminability] <C> 120.1 <C> 89.2 <C> 48.5 <C> 36.1 <C> 27.7 <C> 57.8 <C> 21.4 <C> 21.6 <C> 50.3 <C> 65.4 <R> <C> VisPara-Cap [liu2019generating] <C> 86.9 <C> - <C> - <C> 27.1 <C> - <C> - <C> 21.1 <C> 26.3 <C> 57.2 <C> 70.8 <R> <C> CL-Cap [contrastive] <C> 114.2 <C> 81.3 <C> 46.0 <C> 35.3 <C> 27.1 <C> 55.9 <C> 19.7 <C> 24.1 <C> 52.5 <C> 67.5 <R> <C> PSST [vered2019joint] <C> 111.9 <C> - <C> - <C> 32.2 <C> 26.4 <C> 54.4 <C> 20.6 <C> 45.3† <C> 79.4† <C> 89.9† <CAP> Table 1: Comparison of caption accuracy and distinctiveness on MSCOCO test split: (top) baseline models trained with MLE using standard or our weighted XE loss; (middle) models trained with SCST using standard or our weighted loss/reward; (bottom) SOTA methods for generating distinctive/discriminative captions. CIDEr, BLEU3/4, METEOR, ROUGE-L, and SPICE measure caption accuracy, while CIDErBtw and R@K measure distinctiveness. ↑ or ↓ show whether higher or lower scores are better for each metric. CIDErBtw could not be computed for some models because the captions are not publicly available. Our self-retrieval results (R@K) and those of [gu2018stack, luo2018discriminability, liu2019generating, contrastive] use the pre-trained VSE++ model and the same protocol. † Note that [vered2019joint] reports self-retrieval results using a different retrieval model/protocol – they use their own model for retrieval – which makes it not directly comparable.
<R> <C> [BOLD] Data <C> [BOLD] Translation to English  [BOLD] BL-FT <C> [BOLD] Translation to English  [BOLD] ML-Scratch <C> [BOLD] Translation to English  [BOLD] ML-Scratch <C> [BOLD] Translation to English  [BOLD] ML-FT <C> [BOLD] Translation to English  [BOLD] ML-FT <C> [BOLD] Translation from English  [BOLD] BL-FT <C> [BOLD] Translation from English  [BOLD] ML-Scratch <C> [BOLD] Translation from English  [BOLD] ML-Scratch <C> [BOLD] Translation from English  [BOLD] ML-FT <C> [BOLD] Translation from English  [BOLD] ML-FT <R> <C> [EMPTY] <C> →en <C> N→1 <C> N↔N <C> N→1 <C> N↔N <C> en→ <C> 1→N <C> N↔N <C> 1→N <C> N↔N <R> <C> >10M <C> 2.60 <C> 3.99 <C> 2.51 <C> [BOLD] 4.37 <C> 3.19 <C> [BOLD] 1.67 <C> 0.64 <C> -0.6 <C> 2.20 <C> -0.90 <R> <C> 1M-10M <C> 3.70 <C> 5.70 <C> 5.06 <C> [BOLD] 6.40 <C> 4.62 <C> [BOLD] 3.40 <C> 2.40 <C> 1.7 <C> 1.76 <C> 1.40 <R> <C> 100k-1M <C> 5.49 <C> 7.28 <C> 7.04 <C> [BOLD] 8.13 <C> 6.47 <C> 4.17 <C> [BOLD] 4.31 <C> 4.97 <C> 2.9 <C> 2.00 <R> <C> 7k-30k <C> 10.80 <C> 14.63 <C> 13.77 <C> [BOLD] 18.03 <C> 14.57 <C> 7.27 <C> [BOLD] 8.07 <C> 7.90 <C> 7.6 <C> 0.90 <R> <C> All <C> 4.94 <C> 6.91 <C> 6.15 <C> [BOLD] 7.91 <C> 6.14 <C> [BOLD] 3.67 <C> 3.31 <C> 2.66 <C> 3.0 <C> 1.81 <CAP> Table 1: Multilingual Finetuning on 25 languages comparing to bilingual models. Numbers are the improvement in BLEU compared to bilingual training from scratch.
<R> <C> ctx <C> aa <C> tag <C> cut <C> ffl1 <C> ffl2 <C> [BOLD] all (600) s@1 <C> [BOLD] all (600) s@2 <C> [BOLD] all (600) s@ 3 <C> [BOLD] all (600) s@ 4 <C> [BOLD] nominal (397) s@1 <C> [BOLD] nominal (397) s@2 <C> [BOLD] nominal (397) s@ 3 <C> [BOLD] nominal (397) s@ 4 <C> [BOLD] pronominal (203) s@1 <C> [BOLD] pronominal (203) s@2 <C> [BOLD] pronominal (203) s@ 3 <C> [BOLD] pronominal (203) s@ 4 <R> <C> darkgreen ✓ <C> darkgreen ✓ <C> darkgreen ✓ <C> darkgreen ✓ <C> darkgreen ✓ <C> darkgreen ✓ <C> 24.17 <C> 43.67 <C> 54.50 <C> 63.00 <C> 29.47 <C> 50.63 <C> 62.47 <C> 72.04 <C> 13.79 <C> 30.05 <C> 38.92 <C> 45.32 <R> <C> darkred ✗ <C> darkgreen ✓ <C> darkgreen ✓ <C> darkgreen ✓ <C> darkgreen ✓ <C> darkgreen ✓ <C> 29.67 <C> 52.50 <C> 66.00 <C> [BOLD] 75.00 <C> 33.50 <C> 58.19 <C> 72.04 <C> [BOLD] 80.86 <C> 22.17 <C> 41.38 <C> [BOLD] 54.19 <C> [BOLD] 63.55 <R> <C> darkgreen ✓ <C> darkred ✗ <C> darkgreen ✓ <C> darkgreen ✓ <C> darkgreen ✓ <C> darkgreen ✓ <C> 22.83 <C> 39.00 <C> 52.00 <C> 61.33 <C> 22.42 <C> 41.31 <C> 54.66 <C> 64.48 <C> 23.65 <C> 34.48 <C> 46.80 <C> 55.17 <R> <C> darkgreen ✓ <C> darkgreen ✓ <C> darkred ✗ <C> darkred ✗ <C> darkgreen ✓ <C> darkgreen ✓ <C> 38.33 <C> 54.83 <C> 63.17 <C> 69.33 <C> 46.60 <C> [BOLD] 64.48 <C> 72.54 <C> 79.09 <C> 22.17 <C> 35.96 <C> 44.83 <C> 50.25 <R> <C> darkgreen ✓ <C> darkgreen ✓ <C> darkred ✗ <C> darkred ✗ <C> darkgreen ✓ <C> darkgreen ✓ <C> [BOLD] 43.83 <C> [BOLD] 56.33 <C> [BOLD] 66.33 <C> 73.00 <C> [BOLD] 51.89 <C> [BOLD] 64.48 <C> [BOLD] 73.55 <C> 79.85 <C> 28.08 <C> 40.39 <C> 52.22 <C> 59.61 <R> <C> darkgreen ✓ <C> darkgreen ✓ <C> darkred ✗ <C> darkred ✗ <C> darkgreen ✓ <C> darkgreen ✓ <C> 38.17 <C> 52.50 <C> 61.33 <C> 68.67 <C> 43.07 <C> 57.43 <C> 65.49 <C> 72.04 <C> 28.57 <C> [BOLD] 42.86 <C> 53.20 <C> 62.07 <R> <C> darkgreen ✓ <C> darkgreen ✓ <C> darkgreen ✓ <C> darkred ✗ <C> darkgreen ✓ <C> darkgreen ✓ <C> 30.17 <C> 48.00 <C> 57.83 <C> 67.33 <C> 30.73 <C> 50.88 <C> 61.21 <C> 71.54 <C> [BOLD] 29.06 <C> 42.36 <C> 51.23 <C> 59.11 <R> <C> darkred ✗ <C> darkred ✗ <C> darkred ✗ <C> darkred ✗ <C> darkgreen ✓ <C> darkgreen ✓ <C> 26.33 <C> 40.50 <C> 50.67 <C> 58.67 <C> 28.46 <C> 41.81 <C> 52.14 <C> 59.70 <C> 22.17 <C> 37.93 <C> 47.78 <C> 56.65 <R> <C> darkgreen ✓ <C> darkgreen ✓ <C> darkgreen ✓ <C> darkgreen ✓ <C> darkred ✗ <C> darkgreen ✓ <C> 21.33 <C> 41.17 <C> 53.17 <C> 60.33 <C> 23.43 <C> 47.36 <C> 60.45 <C> 69.52 <C> 17.24 <C> 29.06 <C> 38.92 <C> 42.36 <R> <C> darkgreen ✓ <C> darkgreen ✓ <C> darkgreen ✓ <C> darkgreen ✓ <C> darkgreen ✓ <C> darkred ✗ <C> 12.00 <C> 24.67 <C> 33.50 <C> 41.50 <C> 13.35 <C> 27.20 <C> 37.28 <C> 45.84 <C> 9.36 <C> 19.70 <C> 26.11 <C> 33.00 <R> <C> PS [ITALIC] BL <C> PS [ITALIC] BL <C> PS [ITALIC] BL <C> PS [ITALIC] BL <C> PS [ITALIC] BL <C> PS [ITALIC] BL <C> 27.67 <C> - <C> - <C> - <C> 30.48 <C> - <C> - <C> - <C> 22.17 <C> - <C> - <C> - <R> <C> TAG [ITALIC] BL <C> TAG [ITALIC] BL <C> TAG [ITALIC] BL <C> TAG [ITALIC] BL <C> TAG [ITALIC] BL <C> TAG [ITALIC] BL <C> 38.43 <C> - <C> - <C> - <C> 40.10 <C> - <C> - <C> - <C> [BOLD] 35.17 <C> - <C> - <C> - <CAP> Table 5: Results table for the ARRAU-AA test set. Refer to text for explanation of duplicated rows.
<R> <C> Subreddit <C> Frequency <R> <C> r/politics <C> 245308 <R> <C> r/worldnews <C> 122884 <R> <C> r/The_Donald <C> 80042 <R> <C> r/todayilearned <C> 59892 <R> <C> r/news <C> 59166 <R> <C> r/technology <C> 54860 <R> <C> r/science <C> 46452 <R> <C> r/Conservative <C> 30823 <R> <C> r/POLITIC <C> 28310 <R> <C> r/conspiracy <C> 28293 <R> <C> r/india <C> 27892 <R> <C> r/environment <C> 26816 <R> <C> r/atheism <C> 25999 <R> <C> r/programming <C> 24020 <R> <C> r/Libertarian <C> 23711 <CAP> Table 3: Subreddit Frequency.
<R> <C> NoContext <C> Coref-GRU <C> MHQA-GRN <C> Entity-GCN <R> <C> 59.70 <C> 56.00 <C> 62.80 <C> 64.80 <CAP> Table 2: The results of our no-context baseline compared with Coref-GRU Dhingra et al. (2018), MHQA-GRN Song et al. (2018), and Entity-GCN De Cao et al. (2018) on the WikiHop dev set.
<R> <C> [BOLD] model <C> [BOLD] BLEU-1 <C> [BOLD] BLEU-2 <C> [BOLD] BLEU-3 <C> [BOLD] BLEU-4 <C> [BOLD] METEOR <C> [BOLD] ROUGE-L <R> <C> Seq2SeqAtt <C> 42.90 <C> 25.97 <C> 17.68 <C> 12.49 <C> 16.85 <C> 39.59 <R> <C> Seq2SeqAtt (Du) <C> 43.09 <C> 25.96 <C> 17.50 <C> 12.28 <C> 16.62 <C> 39.75 <R> <C> Transformer (Scialom) <C> 43.33 <C> 26.27 <C> 18.32 <C> 13.23 <C> - <C> 40.22 <R> <C> Our model <C> [BOLD] 45.08 <C> [BOLD] 27.98 <C> [BOLD] 19.38 <C> [BOLD] 13.90 <C> [BOLD] 18.12 <C> [BOLD] 40.77 <CAP> Table 2: Experimental results comparing with previous works.
<R> <C> [BOLD] model <C> [BOLD] BLEU-1 <C> [BOLD] BLEU-2 <C> [BOLD] BLEU-3 <C> [BOLD] BLEU-4 <C> [BOLD] METEOR <C> [BOLD] ROUGE-L <R> <C> Baseline <C> 43.39 <C> 26.75 <C> 18.48 <C> 13.17 <C> 17.40 <C> 40.57 <R> <C> Baseline+Type <C> 44.59 <C> 27.25 <C> 18.76 <C> 13.41 <C> 17.54 <C> 40.41 <R> <C> Baseline+CopyLoss <C> 44.45 <C> 27.62 <C> 19.08 <C> 13.65 <C> 17.90 <C> 40.16 <R> <C> Baseline+CopyLoss+Type <C> [BOLD] 45.08 <C> [BOLD] 27.98 <C> [BOLD] 19.38 <C> [BOLD] 13.90 <C> [BOLD] 18.12 <C> [BOLD] 40.77 <R> <C> Upper Bound <C> 48.42 <C> 30.28 <C> 21.12 <C> 15.27 <C> 19.43 <C> 43.82 <CAP> Table 3: Ablation study of our model.
<R> <C> [BOLD] Type <C> [BOLD] Total <C> [BOLD] Predict <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] Fscore <R> <C> what <C> 6707 <C> 8542 <C> 0.63 <C> 0.80 <C> 0.71 <R> <C> who <C> 1421 <C> 1298 <C> 0.35 <C> 0.32 <C> 0.34 <R> <C> how <C> 1476 <C> 1258 <C> 0.59 <C> 0.50 <C> 0.54 <R> <C> where <C> 455 <C> 350 <C> 0.19 <C> 0.15 <C> 0.17 <R> <C> when <C> 647 <C> 346 <C> 0.27 <C> 0.15 <C> 0.19 <R> <C> yes/no <C> 868 <C> 65 <C> 0.09 <C> 0.01 <C> 0.01 <R> <C> others <C> 303 <C> 18 <C> < 0.01 <C> < 0.01 <C> < 0.01 <CAP> Table 4: Prediction results of different question types.
<R> <C> [EMPTY] <C> [BOLD] Fluency <C> [BOLD] Relevance <C> [BOLD] Answer-ability <R> <C> Baseline <C> 3.78 <C> 3.73 <C> 3.54 <R> <C> Our model <C> [BOLD] 4.23 <C> [BOLD] 4.23 <C> [BOLD] 4.20 <R> <C> Golden <C> 4.69 <C> 4.44 <C> 4.48 <R> <C> Spearman <C> 0.59 <C> 0.67 <C> 0.65 <CAP> Table 6: Human evaluation results of different models.
<R> <C> Model <C> EM <C> F1 <R> <C> MemNet <C> 12.8 <C> 16.8 <R> <C> -  [ITALIC] seen <C> 21.0 <C> 25.0 <R> <C> - ten-cands <C> 23.9 <C> 26.3 <R> <C> - ten-cands ( [ITALIC] seen) <C> 55.7 <C> 57.5 <R> <C> - pointer <C> 27.5 <C> 34.7 <R> <C> - anonymization <C> 13.3 <C> 18.1 <R> <C> - attention-feat <C> 27.8 <C> 33.9 <R> <C> - attention-feat (+emb-pretr) <C> 30.3 <C> 36.7 <R> <C> - attention-feat-only (+emb-pretr) <C> 32.2 <C> 38.7 <R> <C> - best-window <C> 30.3 <C> 36.8 <R> <C> - best-window (+emb-pretr) <C> 29.0 <C> 35.4 <R> <C> QueryClassifier <C> 10.7 <C> 15.1 <R> <C> -  [ITALIC] seen <C> 16.8 <C> 21.4 <R> <C> random <C> 2.7 <C> 6.9 <R> <C> max-freq <C> 14.6 <C> 19.0 <R> <C> sim-window† <C> 35.7 <C> 43.7 <R> <C> Stanford Attentive Reader† <C> [EMPTY] <C> [EMPTY] <R> <C> - anonymization <C> 34.0 <C> 41.3 <R> <C> - no-anonymization <C> 10.8 <C> 15.8 <R> <C> Gated-attention reader† <C> [EMPTY] <C> [EMPTY] <R> <C> - anonymization <C> 42.2 <C> 50.0 <R> <C> - no-anonymization <C> 38.2 <C> 45.6 <R> <C> QANet <C> 39.5 <C> 46.5 <R> <C> BiDAF <C> 44.7 <C> 52.8 <CAP> Table 1: CliCR results. Results indicated with † are for models from Šuster and Daelemans (2018). Results are reported for the test split with those questions not having an answer in the passage removed. The use of pretrained word embeddings for the model variants in the upper part of the table is indicated with +emb-pretr, with randomly initialized word embeddings used otherwise. In the bottom part of the table, all neural models and sim-window use pretrained embeddings. Finally, for QANet, the data splits are smaller in size to prune out long sequences which led to memory issues.
<R> <C> Training Language <C> Single Example  [ITALIC] Cmin [ITALIC] nxe↓ <C> Single Example  [ITALIC] MTWV↑ <C> Multiple Examples  [ITALIC] Cmin [ITALIC] nxe↓ <C> Multiple Examples  [ITALIC] MTWV↑ <R> <C> Portuguese (PT) <C> [BOLD] 0.6771 <C> [BOLD] 0.3786 <C> [BOLD] 0.6478 <C> 0.3963 <R> <C> Spanish (ES) <C> 0.6776 <C> 0.3754 <C> 0.6501 <C> [BOLD] 0.3967 <R> <C> Russian (RU) <C> 0.7035 <C> 0.3184 <C> 0.6767 <C> 0.3383 <R> <C> French (FR) <C> 0.7021 <C> 0.333 <C> 0.6757 <C> 0.3511 <R> <C> German (GE) <C> 0.7503 <C> 0.2643 <C> 0.7257 <C> 0.2919 <R> <C> PT-ES-RU <C> 0.6330 <C> 0.4305 <C> 0.6023 <C> 0.4478 <R> <C> PT-ES-RU-FR-GE <C> [BOLD] 0.6204 <C> [BOLD] 0.4358 <C> [BOLD] 0.5866 <C> [BOLD] 0.4580 <CAP> TABLE III: Performance of the DTW based template matching approach in SWS 2013 using monolingual and multilingual bottleneck features for single and multiple examples per query using all evaluation queries.
<R> <C> Training Language(s) <C> T1 Queries  [ITALIC] Cmin [ITALIC] nxe↓ <C> T1 Queries  [ITALIC] MTWV↑ <C> T2 Queries  [ITALIC] Cmin [ITALIC] nxe↓ <C> T2 Queries  [ITALIC] MTWV↑ <C> T3 Queries  [ITALIC] Cmin [ITALIC] nxe↓ <C> T3 Queries  [ITALIC] MTWV↑ <R> <C> Portuguese (PT) <C> [BOLD] 0.5582 <C> [BOLD] 0.4671 <C> [BOLD] 0.6814 <C> [BOLD] 0.3048 <C> [BOLD] 0.8062 <C> [BOLD] 0.1915 <R> <C> Spanish (ES) <C> 0.5788 <C> 0.4648 <C> 0.7074 <C> 0.2695 <C> 0.8361 <C> 0.1612 <R> <C> Russian (RU) <C> 0.6119 <C> 0.4148 <C> 0.7285 <C> 0.2434 <C> 0.8499 <C> 0.1385 <R> <C> French (FR) <C> 0.6266 <C> 0.4242 <C> 0.7462 <C> 0.2086 <C> 0.8522 <C> 0.1249 <R> <C> German (GE) <C> 0.6655 <C> 0.3481 <C> 0.7786 <C> 0.1902 <C> 0.8533 <C> 0.1038 <R> <C> PT-ES-RU <C> 0.4828 <C> 0.5459 <C> 0.6218 <C> 0.3626 <C> 0.7849 <C> 0.2057 <R> <C> PT-ES-RU-FR-GE <C> [BOLD] 0.4606 <C> [BOLD] 0.5663 <C> [BOLD] 0.6013 <C> [BOLD] 0.3605 <C> [BOLD] 0.7601 <C> [BOLD] 0.2138 <CAP> TABLE IV: Performance of the DTW based template matching approach in QUESST 2014 using monolingual and multilingual bottleneck features for different types of queries in evaluation set.
<R> <C> # of layers frozen <C> T1 Queries  [ITALIC] Cmin [ITALIC] nxe↓ <C> T1 Queries  [ITALIC] MTWV↑ <C> T2 Queries  [ITALIC] Cmin [ITALIC] nxe↓ <C> T2 Queries  [ITALIC] MTWV↑ <C> T3 Queries  [ITALIC] Cmin [ITALIC] nxe↓ <C> T3 Queries  [ITALIC] MTWV↑ <R> <C> 3 <C> 0.3881 <C> 0.6395 <C> 0.5238 <C> 0.4362 <C> 0.6254 <C> 0.3669 <R> <C> 2 <C> [BOLD] 0.3796 <C> [BOLD] 0.6499 <C> 0.5158 <C> 0.4433 <C> 0.6278 <C> 0.3617 <R> <C> 1 <C> 0.3888 <C> 0.6309 <C> [BOLD] 0.5124 <C> [BOLD] 0.4513 <C> [BOLD] 0.6148 <C> [BOLD] 0.3793 <R> <C> 0 <C> 0.4268 <C> 0.6190 <C> 0.5338 <C> 0.4338 <C> 0.6591 <C> 0.3646 <CAP> TABLE VIII: Performance of the End-to-End neural network based approach in QUESST 2014 for different types of queries in evaluation set. Different number of layers in the feature extractor block were frozen to train with limited data.
<R> <C> # of layers frozen <C> Single Example  [ITALIC] Cmin [ITALIC] nxe↓ <C> Single Example  [ITALIC] MTWV↑ <C> Multiple Examples  [ITALIC] Cmin [ITALIC] nxe↓ <C> Multiple Examples  [ITALIC] MTWV↑ <R> <C> 3 <C> 0.5788 <C> 0.4633 <C> 0.5521 <C> 0.4888 <R> <C> 2 <C> 0.5705 <C> 0.4708 <C> 0.5539 <C> [BOLD] 0.4914 <R> <C> 1 <C> [BOLD] 0.5607 <C> [BOLD] 0.4719 <C> [BOLD] 0.5429 <C> 0.4894 <R> <C> 0 <C> 0.5718 <C> 0.4597 <C> 0.5593 <C> 0.4738 <R> <C> Bottleneck <C> 0.6204 <C> 0.4358 <C> 0.5866 <C> 0.4580 <CAP> TABLE IX: Performance of the DTW based template matching approach using multilingual bottleneck features which are fine tuned using CNN based loss function. The experiments were performed using evaluation queries in SWS 2013 for single and multiple examples per query.
<R> <C> AUC <C> MSE <C> ACC <C> F1 <R> <C> Linear Regression <C> Linear Regression <C> Linear Regression <C> Linear Regression <R> <C> 0.693 <C> 0.038 <C> 0.817 <C> 0.55 <R> <C> Logistics Regression <C> Logistics Regression <C> Logistics Regression <C> Logistics Regression <R> <C> 0.723 <C> 0.197 <C> 0.711 <C> 0.56 <R> <C> Random Forest Regression <C> Random Forest Regression <C> Random Forest Regression <C> Random Forest Regression <R> <C> 0.69 <C> 0.036 <C> 0.819 <C> 0.55 <R> <C> Random Forest Classifier <C> Random Forest Classifier <C> Random Forest Classifier <C> Random Forest Classifier <R> <C> 0.69 <C> 0.036 <C> 0.818 <C> 0.54 <CAP> Table 2: Performance of the clickbait classifiers using different prediction models
<R> <C> GROUPS <C> Linear Regression AUC <C> Linear Regression MSE <C> Linear Regression ACC <C> Linear Regression F1 <C> Logistics Regression AUC <C> Logistics Regression MSE <C> Logistics Regression ACC <C> Logistics Regression F1 <R> <C> Post-related (175 features) <C> 0.694 <C> 0.038 <C> 0.694 <C> 0.55 <C> 0.753 <C> 0.162 <C> 0.762 <C> 0.61 <R> <C> Target-related (2 features) <C> 0.5 <C> 0.062 <C> 0.749 <C> 0 <C> 0.565 <C> 0.299 <C> 0.562 <C> 0.4 <R> <C> Relation (3 features) <C> 0.5 <C> 0.06 <C> 0.749 <C> 0 <C> 0.56 <C> 0.278 <C> 0.593 <C> 0.38 <R> <C> [EMPTY] <C> Random Forest Regression <C> Random Forest Regression <C> Random Forest Regression <C> Random Forest Regression <C> Random Forest Classifier <C> Random Forest Classifier <C> Random Forest Classifier <C> Random Forest Classifier <R> <C> GROUPS <C> AUC <C> MSE <C> ACC <C> F1 <C> AUC <C> MSE <C> ACC <C> F1 <R> <C> Post-related (175 features) <C> 0.693 <C> 0.036 <C> 0.818 <C> 0.55 <C> 0.742 <C> 0.152 <C> 0.778 <C> 0.6 <R> <C> Target-related (2 features) <C> 0.537 <C> 0.067 <C> 0.734 <C> 0.21 <C> 0.571 <C> 0.229 <C> 0.665 <C> 0.39 <R> <C> Relation (3 features) <C> 0.564 <C> 0.062 <C> 0.748 <C> 0.28 <C> 0.608 <C> 0.215 <C> 0.681 <C> 0.42 <CAP> Table 3: Performance of the clickbait classifiers using different prediction models with different feature groups.
<R> <C> [EMPTY] <C> MR <C> Yelp <C> Ag’s news <C> 20Newsgroups <C> Yahoo!answer <C> Sogou news <R> <C> SVM + Unigram † <C> 76.92 <C> 56.63 <C> 91.78 <C> 93.05 <C> 71.32 <C> 81.70 <R> <C> SVM + Bigram † <C> 78.08 <C> 59.10 <C> 91.70 <C> 93.23 <C> 70.75 <C> 84.62 <R> <C> CNN † <C> 80.87 <C> 65.11 <C> 92.82 <C> 95.60 <C> 72.45 <C> 84.41 <R> <C> Bi-GRU † <C> 80.61 <C> 64.76 <C> 92.41 <C> 95.72 <C> 73.91 <C> 84.81 <R> <C> RCNN † <C> 81.13 <C> 65.89 <C> 92.50 <C> 95.81 <C> 74.17 <C> 85.04 <R> <C> Self-attentive LSTM † <C> 81.20 <C> 65.83 <C> [BOLD] 93.08 <C> [BOLD] 96.43 <C> 74.30 <C> 84.99 <R> <C> MV-RNN ‡  <C> 79.0 <C> - <C> - <C> - <C> - <C> - <R> <C> ClassifyLDA-EM ‡  <C> - <C> - <C> - <C> 93.60 <C> - <C> - <R> <C> Char-conv ‡  <C> - <C> - <C> 91.45 <C> - <C> 71.20 <C> - <R> <C> VDCNN ‡  <C> - <C> - <C> 91.33 <C> - <C> 73.43 <C> - <R> <C> TCNN-SM ‡  <C> - <C> - <C> 92.01 <C> - <C> - <C> - <R> <C> Region.emb ‡  <C> - <C> - <C> 92.8 <C> - <C> 73.7 <C> - <R> <C> ID-LSTM ‡  <C> 81.6 <C> - <C> 92.2 <C> - <C> - <C> - <R> <C> CRNN w/o NTL <C> 81.80 <C> 66.18 <C> 92.54 <C> 96.13 <C> 74.66 <C> 85.20 <R> <C> CRNN <C> [BOLD] 82.03 <C> [BOLD] 66.51 <C> 93.05 <C> 96.31 <C> [BOLD] 75.05 <C> [BOLD] 85.55 <CAP> Table 3: The classification accuracy on several benchmark data sets. The best result on each data is marked in bold.. Note that “CRNN w/o NTL” means CRNN is implemented without the neural tensor layer (NTL). † marks results re-implemented in this paper. ‡ marks results reported in the original paper.
<R> <C> Filter size <C> Ag’s news <C> Yelp <C> Yahoo!answer <R> <C> {1} <C> 92.75 <C> 65.57 <C> 78.44 <R> <C> {3} <C> 92.69 <C> 66.03 <C> 78.45 <R> <C> {1, 2, 3} <C> [BOLD] 92.96 <C> 66.14 <C> [BOLD] 78.50 <R> <C> {3, 4, 5} <C> 92.81 <C> [BOLD] 66.53 <C> 78.26 <CAP> Table 4: Accuracies on the development sets of three datasets with different filter sizes.
<R> <C> Model <C> Scales <C> Attention Context Window <C> Layers <C> Test PPL <C> Mem (GB) <R> <C> [BOLD] Ablation for number of scales <C> [BOLD] Ablation for number of scales <C> [BOLD] Ablation for number of scales <C> [BOLD] Ablation for number of scales <C> [BOLD] Ablation for number of scales <C> [BOLD] Ablation for number of scales <R> <C> Vanilla <C> 1 <C> 0:512 <C> 12 <C> 16.95 <C> 20.98 <R> <C> Vanilla <C> 1 <C> 0:512 <C> 14 <C> 16.54 <C> 23.78 <R> <C> Top-down <C> 4,1 <C> 0:512 <C> 8,12 <C> 15.87 <C> 25.45 <R> <C> Top-down <C> 16,4,1 <C> 0:512 <C> 4,8,12 <C> 15.76 <C> 25.23 <R> <C> Top-down <C> 64,16,4,1 <C> 0:512 <C> 2,4,8,12 <C> 16.00 <C> 22.62 <R> <C> [BOLD] Ablation for capacity at different scales <C> [BOLD] Ablation for capacity at different scales <C> [BOLD] Ablation for capacity at different scales <C> [BOLD] Ablation for capacity at different scales <C> [BOLD] Ablation for capacity at different scales <C> [BOLD] Ablation for capacity at different scales <R> <C> Top-down <C> 64,16,4,1 <C> 0:512 <C> 16,12,6,1 <C> 22.14 <C> 10.61 <R> <C> Top-down <C> 64,16,4,1 <C> 0:512 <C> 10,10,8,5 <C> 17.89 <C> 13.59 <R> <C> Top-down <C> 64,16,4,1 <C> 0:512 <C> 8,8,7,7 <C> 17.16 <C> 16.87 <R> <C> Top-down <C> 64,16,4,1 <C> 0:512 <C> 7,7,8,8 <C> 16.86 <C> 18.22 <R> <C> Top-down <C> 64,16,4,1 <C> 0:512 <C> 5,5,9,9 <C> 16.57 <C> 19.47 <R> <C> Top-down <C> 64,16,4,1 <C> 0:512 <C> 3,3,10,10 <C> 16.31 <C> 20.71 <R> <C> Top-down <C> 64,16,4,1 <C> 0:512 <C> 2,2,11,11 <C> 16.12 <C> 22.05 <R> <C> Top-down <C> 64,16,4,1 <C> 0:512 <C> 1,1,12,12 <C> 15.95 <C> 23.40 <R> <C> [BOLD] Ablation for downsampler <C> [BOLD] Ablation for downsampler <C> [BOLD] Ablation for downsampler <C> [BOLD] Ablation for downsampler <C> [BOLD] Ablation for downsampler <C> [BOLD] Ablation for downsampler <R> <C> Top-down (Avg-Pool) <C> 64,16,4,1 <C> 0:512 <C> 7,7,8,8 <C> 16.86 <C> 18.22 <R> <C> Top-down (Conv) <C> 64,16,4,1 <C> 0:512 <C> 7,7,8,8 <C> 17.95 <C> 18.45 <R> <C> [BOLD] Ablation for deep & narrow networks <C> [BOLD] Ablation for deep & narrow networks <C> [BOLD] Ablation for deep & narrow networks <C> [BOLD] Ablation for deep & narrow networks <C> [BOLD] Ablation for deep & narrow networks <C> [BOLD] Ablation for deep & narrow networks <R> <C> Vanilla (256/1024) <C> 1 <C> 0:512 <C> 30 <C> 22.15 <C> 22.07 <R> <C> Vanilla (512/2048) <C> 1 <C> 0:512 <C> 20 <C> 17.70 <C> 22.77 <R> <C> [BOLD] Ablation for attention context window <C> [BOLD] Ablation for attention context window <C> [BOLD] Ablation for attention context window <C> [BOLD] Ablation for attention context window <C> [BOLD] Ablation for attention context window <C> [BOLD] Ablation for attention context window <R> <C> Vanilla <C> 1 <C> 0:512 <C> 12 <C> 16.95 <C> 23.78 <R> <C> Vanilla <C> 1 <C> 0:256 <C> 12 <C> 17.00 <C> 23.78 <R> <C> Vanilla <C> 1 <C> 0:128 <C> 12 <C> 17.39 <C> 23.78 <R> <C> Vanilla <C> 1 <C> 0:64 <C> 12 <C> 17.88 <C> 23.78 <R> <C> Vanilla <C> 1 <C> 0:16 <C> 12 <C> 19.01 <C> 23.78 <R> <C> Vanilla <C> 1 <C> 0:8 <C> 12 <C> 20.19 <C> 23.78 <R> <C> Retina <C> 16,4,1 <C> 0:8,8:256, 256:512 <C> 12 <C> 16.81 <C> 23.95 <R> <C> Retina <C> 16,4,1 <C> 0:16,16:256, 256:512 <C> 12 <C> 17.07 <C> 23.95 <R> <C> Retina <C> 16,4,1 <C> 0:128,64:256, 128:512 <C> 12 <C> 17.08 <C> 23.95 <CAP> Table 4: Model ablations for the number of scales, number of transformer layers per scale, type of downsampling function, skinny and deep networks and different local attention/retina attention masks. All reported results are on our BookCorpus test set. The attention context window column indicates what attention heads at each scale look at - for example, "0:512" implies all scales see the entire history while "0:8,8:256,256:512" indicates that the finest scale sees only the previous 8 tokens, the subsequent scale from 8-256 and so on.
<R> <C> Model <C> Layers <C> N-gram PPL <C> GPT-2 PPL <C> Ref BLEU <C> N-gram Repeat <C> Time (s/seq) <R> <C> Vanilla <C> 14 <C> 22.81 <C> 7.30 <C> 7.45 <C> 0.75/0.45/0.29/0.20 <C> 1.35 <R> <C> Top-down <C> 2,4,8,12 <C> 21.17 <C> 7.69 <C> 6.80 <C> 0.73/0.41/0.26/0.16 <C> 0.93 <R> <C> Bottom-up <C> 8,9,9 <C> 24.54 <C> 8.53 <C> 7.04 <C> 0.75/0.45/0.28/0.18 <C> 1.36 <R> <C> Retina <C> 12 <C> 20.91 <C> 8.64 <C> 7.37 <C> 0.73/0.42/0.26/0.17 <C> 1.72 <CAP> Table 5: Sample based evaluation of models on the BookCorpus. The N-gram repeat column reports 1/2/3/4-gram repeat fractions with a within of 256 tokens
<R> <C> [BOLD] Data <C> [BOLD] Model <C> [BOLD] Resolution Task  [BOLD] EM(%) <C> [BOLD] Resolution Task  [BOLD] BLEU(%) <C> [BOLD] Resolution Task  [BOLD] F1(%) <C> [BOLD] Resolution Task  [BOLD] Prec.(%) <C> [BOLD] Resolution Task  [BOLD] Rec.(%) <C> [BOLD] Dialogue Task  [BOLD] Succ.F1(%) <C> [BOLD] Dialogue Task  [BOLD] Prec.(%) <C> [BOLD] Dialogue Task  [BOLD] Rec.(%) <R> <C> Complete <C> TSCP <C> - <C> - <C> - <C> - <C> - <C> 86.30 <C> 89.60 <C> 83.23 <R> <C> Ellipsis <C> TSCP <C> - <C> - <C> - <C> - <C> - <C> 84.56 <C> 87.25 <C> 82.02 <R> <C> Ellipsis <C> Our Model <C> 60.83 <C> 78.89 <C> 95.64 <C> 97.79 <C> 93.58 <C> [BOLD] 85.33 <C> 88.69 <C> 82.21 <R> <C> Co-reference <C> TSCP <C> - <C> - <C> - <C> - <C> - <C> 82.17 <C> 88.91 <C> 76.38 <R> <C> Co-reference <C> Our Model <C> 68.56 <C> 83.98 <C> 96.61 <C> 98.09 <C> 95.18 <C> [BOLD] 86.00 <C> 90.46 <C> 81.95 <R> <C> Mixed <C> TSCP <C> - <C> - <C> - <C> - <C> - <C> 83.25 <C> 86.91 <C> 79.89 <R> <C> Mixed <C> Our Model <C> 66.47 <C> 83.63 <C> 96.26 <C> 98.16 <C> 94.44 <C> [BOLD] 85.97 <C> 87.98 <C> 84.05 <CAP> Table 4: Results of the multi-task learning model. This table is split into two parts: performance of resolution for the integrated GECOR on the left side and performance of dialogue task on the right side.
<R> <C> [EMPTY] <C> MCD [dB] CycleGAN-Joint <C> MCD [dB] CycleGAN-Separate <R> <C> Neutral→Angry <C> 10.87 <C> 8.83 <R> <C> Neutral→Sad <C> 9.41 <C> 8.27 <R> <C> Neutral→Surprise <C> 10.43 <C> 9.05 <R> <C> Overall mean <C> 10.23 <C> [BOLD] 8.71 <CAP> Table 2: A comparison of the MCD results between CycleGAN-Joint and CycleGAN-Separate for three different emotion combinations.
<R> <C> [EMPTY] <C> RMSE [Hz] Baseline <C> RMSE [Hz] CycleGAN-Joint <C> RMSE [Hz] CycleGAN-Separate <C> PCC Baseline <C> PCC CycleGAN-Joint <C> PCC CycleGAN-Separate <R> <C> Neutral→Angry <C> 71.09 <C> 64.55 <C> 67.44 <C> 0.75 <C> 0.81 <C> 0.78 <R> <C> Neutral→Sad <C> 62.99 <C> 57.46 <C> 48.33 <C> 0.66 <C> 0.68 <C> 0.74 <R> <C> Neutral→Surprise <C> 77.89 <C> 73.16 <C> 74.14 <C> 0.75 <C> 0.79 <C> 0.76 <R> <C> Overall mean <C> 70.62 <C> 65.05 <C> [BOLD] 63.03 <C> 0.72 <C> [BOLD] 0.76 <C> [BOLD] 0.76 <CAP> Table 3: A comparison of the RMSE and PCC results of the baseline, CycleGAN-Joint and CycleGAN-Separate for three different emotion combinations (neutral-to-angry, neutral-to-sad and neutral-to-surprise).
<R> <C> [BOLD] Method  [BOLD] Projection <C> [BOLD] Random  [BOLD] PCA <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] (SubSpace) <C> [BOLD] Autoencoder <C> [EMPTY] <C> [EMPTY] <R> <C> Time (sec) <C> 35 <C> 66 <C> 608 <CAP> Table 12: Time complexity for dimensionality reduction of word topic vectors to a 2000-dimension dense representation using various reduction techniques on 20NewsGroup
<R> <C> model <C> sentiment accuracy <C> distinct-1 <C> distinct-2 <C> bleu-1 <C> bleu-2 <C> average <R> <C> seq2seq-att <C> 41.2 <C> 76.3 <C> 86.3 <C> [BOLD] 14.7 <C> 1.22 <C> 35.1 <R> <C> seq2seq-att-sent <C> 75.9 <C> 76.1 <C> 84.2 <C> [BOLD] 14.7 <C> [BOLD] 1.29 <C> 38.9 <R> <C> dual-decoder <C> [BOLD] 91.0 <C> [BOLD] 87.7 <C> [BOLD] 94.1 <C> 14.3 <C> 1.25 <C> [BOLD] 39.7 <CAP> Table 1: Experimental results of different models on generating responses with sentiment.
<R> <C> System <C> BLEU Score <R> <C> STUMABA-D <C> 79.43 <R> <C> Pipeline <C> 70.99 <R> <C> TBDIL <C> [BOLD] 80.49 <CAP> Table 13: Test results.
<R> <C> Train. \Inf. <C> Sentences 1 <C> Sentences 10 <C> Tokens 128 <C> Tokens 512 <R> <C> 128 toks. <C> 69.1 <C> 76.8 <C> 75.7 <C> 77.2 <R> <C> 256 toks. <C> 69.2 <C> 77.1 <C> 75.7 <C> 78.4 <R> <C> 384 toks. <C> 67.6 <C> 76.9 <C> 75.5 <C> 78.5 <R> <C> 512 toks. <C> 66.7 <C> 77.1 <C> 74.7 <C> 78.7 <R> <C> 1 sent. <C> 69.1 <C> 73.6 <C> 73.0 <C> 72.9 <R> <C> 5 sent. <C> 69.6 <C> 76.9 <C> 75.7 <C> 77.9 <R> <C> 10 sent. <C> 68.6 <C> 77.1 <C> 75.3 <C> 78.1 <CAP> Table 2: Average dev. F1 score for models that are trained and evaluated across a range of segment lengths.
<R> <C> Gold <C> Pretrained <C> TF <C> # Epochs <C> Avg. F1 <R> <C> N <C> Y <C> N <C> 3 <C> 78.7 <R> <C> N <C> Y <C> Y <C> 11 <C> 77.8 <R> <C> N <C> N <C> N <C> 15 <C> 78.0 <R> <C> N <C> N <C> Y <C> 2 <C> 78.7 <R> <C> Y <C> Y <C> N <C> 32 <C> 91.4 <R> <C> Y <C> Y <C> Y <C> 14 <C> 91.2 <R> <C> Y <C> N <C> N <C> 6 <C> 91.7 <R> <C> Y <C> N <C> Y <C> 5 <C> 91.5 <CAP> Table 3: Average dev. F1 score for models that (do not) use gold spans, pretrained scorers for coreference resolution, and teacher forcing (TF).
<R> <C> [EMPTY] <C> Camel <C> TaPEm <R> <C> Average rank violation (the smaller the better) <C> 3.8374 <C> 0.4519 <CAP> Table 5. Average rank violation comparisons.
<R> <C> Methods <C> METEOR <C> BLEU-1 <C> BLEU-2 <C> BLEU-3 <C> BLEU-4 <C> CIDEr <C> SPICE <R> <C> NIC  <C> – <C> 0.666 <C> 0.451 <C> 0.304 <C> 0.203 <C> – <C> – <R> <C> CNN-LSTM <C> 0.238 <C> 0.698 <C> 0.525 <C> 0.390 <C> 0.292 <C> 0.889 <C> – <R> <C> TPGN <C> [BOLD] 0.243 <C> [BOLD] 0.709 <C> [BOLD] 0.539 <C> [BOLD] 0.406 <C> [BOLD] 0.305 <C> [BOLD] 0.909 <C> [BOLD] 0.18 <CAP> Table 1: Performance of the proposed TPGN model on the COCO dataset.
<R> <C> [BOLD] Algorithm <C> [BOLD] drama <C> [BOLD] comedy <C> [BOLD] thriller <C> [BOLD] romance <C> [BOLD] action <C> [BOLD] crime <C> [BOLD] adventure <C> [BOLD] horror <R> <C> LDA <C> 0.5544 <C> 0.5856 <C> 0.8158 <C> 0.8173 <C> 0.8745 <C> 0.8685 <C> 0.8765 <C> 0.9063 <R> <C> paragraph2vec <C> 0.6367 <C> 0.6767 <C> 0.7958 <C> 0.7919 <C> 0.8193 <C> 0.8537 <C> 0.8524 <C> 0.8699 <R> <C> word2vec <C> 0.7172 <C> 0.7449 <C> 0.8102 <C> 0.8204 <C> 0.8627 <C> 0.8692 <C> 0.8768 <C> 0.9231 <R> <C> HDV <C> [BOLD] 0.7274 <C> [BOLD] 0.7487 <C> [BOLD] 0.8201 <C> [BOLD] 0.8233 <C> [BOLD] 0.8814 <C> [BOLD] 0.8728 <C> [BOLD] 0.8854 <C> [BOLD] 0.9872 <CAP> Table 1: Movie classification accuracy
<R> <C> [EMPTY] <C> Company∗ AUC <C> Company∗ LogLoss <C> Criteo AUC <C> Criteo LogLoss <R> <C> LR <C> 0.8640 <C> 0.02648 <C> 0.7686 <C> 0.47762 <R> <C> FM <C> 0.8678 <C> 0.02633 <C> 0.7892 <C> 0.46077 <R> <C> FNN <C> 0.8683 <C> 0.02629 <C> 0.7963 <C> 0.45738 <R> <C> IPNN <C> 0.8664 <C> 0.02637 <C> 0.7972 <C> 0.45323 <R> <C> OPNN <C> 0.8658 <C> 0.02641 <C> 0.7982 <C> 0.45256 <R> <C> PNN∗ <C> 0.8672 <C> 0.02636 <C> 0.7987 <C> 0.45214 <R> <C> LR & DNN <C> 0.8673 <C> 0.02634 <C> 0.7981 <C> 0.46772 <R> <C> FM & DNN <C> 0.8661 <C> 0.02640 <C> 0.7850 <C> 0.45382 <R> <C> DeepFM <C> [BOLD] 0.8715 <C> [BOLD] 0.02618 <C> [BOLD] 0.8007 <C> [BOLD] 0.45083 <CAP> Table 2: Performance on CTR prediction.
<R> <C> [BOLD] Model <C> [BOLD] AUC <R> <C> SESA (Random word embeddings) <C> 0.82 <R> <C> SESA (Pretrained word embeddings) <C> [BOLD] 0.86 <R> <C> Logistic Regression <C> 0.78 <R> <C> Gradient Boosting (500 trees) <C> 0.85 <R> <C> Gradient Boosting (1000 trees) <C> [BOLD] 0.86 <CAP> Table 2: The AUC of the SESA with an LSTM encoder and a linear projector versus the baseline feature-based systems.
<R> <C> [EMPTY] <C> B → E 5 layers <C> B → E 8 layers <C> B → E 10 layers <C> B → E 12 layers (full) <C> A → K 5 layers <C> A → K 8 layers <C> A → K 10 layers <C> A → K 12 layers (full) <R> <C> BERT <C> 70.9 <C> 75.9 <C> 80.6 <C> 78.8 <C> 71.2 <C> 74.9 <C> 81.2 <C> 78.8 <R> <C> Fine-tuned BERT <C> 74.6 <C> 76.5 <C> 84.2 <C> 84.2 <C> 74.0 <C> 76.3 <C> 80.8 <C> 81.9 <R> <C> PERL (Ours) <C> 81.1 <C> 83.2 <C> [BOLD] 88.2 <C> 87.0 <C> 77.7 <C> 80.2 <C> [BOLD] 84.7 <C> 84.2 <CAP> Table 4: Classification accuracy with reduced-size encoders.
<R> <C> [EMPTY] <C> B → E <C> K → D <C> E → K <C> D → B <R> <C> BERT <C> 78.8 <C> 77.7 <C> 85.1 <C> 81.0 <R> <C> Fine-tuned BERT <C> 84.2 <C> 79.8 <C> 89.2 <C> 84.1 <R> <C> High-MI, No Target <C> 76.2 <C> 76.4 <C> 84.9 <C> 83.7 <R> <C> Random-Frequent <C> 79.7 <C> 76.8 <C> 85.5 <C> 81.7 <R> <C> PERL (Ours) <C> [BOLD] 87.0 <C> [BOLD] 84.6 <C> [BOLD] 90.6 <C> [BOLD] 85.0 <R> <C> Oracle <C> 88.9 <C> 85.6 <C> 91.5 <C> 86.7 <CAP> Table 5: Impact of PERL’s pivot selection method.
<R> <C> [EMPTY] <C> B → E <C> K → D <C> A → B <C> I → E <R> <C> No fine-tuning <C> No fine-tuning <C> No fine-tuning <C> No fine-tuning <C> No fine-tuning <R> <C> BERT <C> 78.8 <C> 77.7 <C> 70.9 <C> 75.4 <R> <C> Source data only <C> Source data only <C> Source data only <C> Source data only <C> Source data only <R> <C> Fine-tuned BERT <C> 80.7 <C> 79.8 <C> 69.4 <C> 81.0 <R> <C> PERL <C> 79.6 <C> 82.2 <C> 69.8 <C> 84.4 <R> <C> Target data only <C> Target data only <C> Target data only <C> Target data only <C> Target data only <R> <C> Fine-tuned BERT <C> 82.0 <C> 80.9 <C> 71.6 <C> 81.1 <R> <C> PERL <C> 86.9 <C> 83.0 <C> 71.8 <C> 84.2 <R> <C> Source and target data <C> Source and target data <C> Source and target data <C> Source and target data <C> Source and target data <R> <C> Fine-tuned BERT <C> 84.2 <C> 79.8 <C> 72.9 <C> 81.5 <R> <C> PERL <C> [BOLD] 87.0 <C> [BOLD] 84.6 <C> [BOLD] 77.1 <C> [BOLD] 87.1 <CAP> Table 6: Impact of fine-tuning data selection.
<R> <C> Class Plane <C> # of data 2803 <C> Class Speedboat <C> # of data 900 <R> <C> Guitar <C> 207 <C> Piano <C> 1899 <R> <C> Drum <C> 259 <C> Dog <C> 264 <R> <C> Dam <C> 584 <C> Baseball <C> 1708 <R> <C> Soccer <C> 2077 <C> [EMPTY] <C> [EMPTY] <CAP> Table 1: Number of training data in different classes.
<R> <C> 0.5 times <C> Original <C> 2 times <C> 3 times <CAP> Table 3: Generated images by inputting different volumes of sounds. The numbers in the table is the relative loudness to the original sound.
<R> <C> Model <C> Average Score <R> <C> Conditional GAN with spectral norm <C> 1.90 <R> <C> + Hinge Loss <C> 2.74 <R> <C> + Projection Discriminator <C> 3.16 <R> <C> + Auxiliary Classifier <C> 3.70 <CAP> Table 5: Human scores on different models
<R> <C> RNNs on the Testing Dataset Model <C> RNNs on the Testing Dataset Architecture <C> RNNs on the Testing Dataset Features <C> RNNs on the Testing Dataset validation <C> RNNs on the Testing Dataset validation <C> RNNs on the Testing Dataset validation <C> RNNs on the Testing Dataset testing <C> RNNs on the Testing Dataset testing <C> RNNs on the Testing Dataset testing <C> RNNs on the Testing Dataset testing <C> RNNs on the Testing Dataset testing <R> <C> Model <C> Architecture <C> Features <C> ¯¯¯¯¯¯¯¯¯¯¯ [ITALIC] sens <C> ¯¯¯¯¯¯¯¯¯¯ [ITALIC] spec <C> ¯¯¯¯¯¯¯ [ITALIC] F1 <C> ¯¯¯¯¯¯¯¯¯¯¯ [ITALIC] sens <C> ¯¯¯¯¯¯¯¯¯¯ [ITALIC] spec <C> ¯¯¯¯¯¯¯ [ITALIC] F1 <C> F1 CI <C> p-value <R> <C> 1 <C> ⟨15⟩ <C> all <C> 0.696 <C> 0.945 <C> 0.712 <C> 0.667 <C> 0.931 <C> 0.676 <C> ± 0.026 <C> 0.024 <R> <C> 2 <C> ⟨15⟩ <C> all-{ [ITALIC] B} <C> 0.545 <C> 0.957 <C> 0.591 <C> 0.522 <C> 0.948 <C> 0.559 <C> ± 0.046 <C> 0.000 <R> <C> 3 <C> ⟨15⟩ <C> all - { [ITALIC] T,  [ITALIC] P,  [ITALIC] D} <C> 0.677 <C> 0.949 <C> 0.697 <C> 0.654 <C> 0.935 <C> 0.666 <C> ± 0.031 <C> 0.012 <R> <C> 4 <C> ⟨15⟩ <C> all-{ [ITALIC] T,  [ITALIC] P,  [ITALIC] D,  [ITALIC] W} <C> 0.703 <C> 0.948 <C> 0.718 <C> 0.686 <C> 0.935 <C> 0.690 <C> ± 0.018 <C> 0.076 <R> <C> 5 <C> ⟨15⟩ <C> all-{ [ITALIC] T,  [ITALIC] P,  [ITALIC] D,  [ITALIC] Sp} <C> 0.689 <C> 0.945 <C> 0.706 <C> 0.672 <C> 0.931 <C> 0.683 <C> ± 0.013 <C> 0.007 <R> <C> 6 <C> ⟨15⟩ <C> all- { [ITALIC] T, [ITALIC] P, [ITALIC] D,  [ITALIC] E} <C> 0.686 <C> 0.949 <C> 0.706 <C> 0.662 <C> 0.936 <C> 0.675 <C> ± 0.023 <C> 0.012 <R> <C> 7 <C> ⟨15⟩ <C> all - { [ITALIC] T,  [ITALIC] P,  [ITALIC] D,  [ITALIC] Sp,  [ITALIC] W,  [ITALIC] E} <C> 0.726 <C> 0.943 <C> [BOLD] 0.734 <C> 0.706 <C> 0.928 <C> [BOLD] 0.704 <C> ± 0.012 <C> — <CAP> Table 4: Average performance of the proposed model (RNN) for seven different sets of features using an architecture with a single single Bi-LSTM layer and 15 hidden units (indicated as ⟨15⟩ with this notation explained in B). For each set of features, we run ten trials and report the average sensitivity, specificity, and F1-score for both the validation and testing data. For the testing data, we also report the confidence intervals at a 95% level of significance for the F1-score. We also report the p-value of a single-tail t-test for each model against the best model of the table. The test is performed between the F1-score on the testing data for each of the two models. A small p-value gives evidence to reject the hypothesis that the models have the same F1-score, indicating that the best model is statistically significantly better. In this experiment, we tested two different hypotheses. First, we examine our intuition that some features are not useful for prediction on the presence of others. As the preliminary studies show several features are not useful for the ED task as long as the other features are present in the model. Furthermore, removing such features improves the performance by eliminating the noise they add to the task. We tested this hypothesis with the experiments for models 3 to 7 using model 1 as the baseline. Where model 7, with all the features excluded except the contextual word and sentence embeddings, showed the best performance. Secondly, we tested the hypothesis that the exclusion of contextual embeddings significantly impacts the overall performance, even more than any other feature. The second experiment provides evidence to support our second hypothesis.
<R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> proposed model model 7 (all-{ [ITALIC] T, [ITALIC] P, [ITALIC] D, [ITALIC] Sp, [ITALIC] W,  [ITALIC] E}) <C> baseline models model 8 ({ [ITALIC] W, [ITALIC] E}) <C> baseline models model 9 ({ [ITALIC] W}) <R> <C> proposed models <C> model 2 <C> (all - { [ITALIC] B}) <C> 0.000 <C> 0.264 <C> 0.344 <R> <C> proposed models <C> model 7 <C> (all - { [ITALIC] T, [ITALIC] P, [ITALIC] D, [ITALIC] Sp, [ITALIC] W,  [ITALIC] E}) <C> — <C> 0.000 <C> 0.000 <R> <C> baseline models <C> model 8 <C> ({ [ITALIC] W, [ITALIC] E}) <C> — <C> — <C> 0.384 <CAP> Table 6: P-values of single-tail t-tests between the best two baseline models (models 8 and 9) and two of the proposed models (models 2 and 7). For each model, we compute the p-values of the t-test against each of all the other models. A high p-value implies that there is not enough evidence to reject the null hypothesis that the models have the same performance. Here we measure performance based on the F1-score on the testing data. For example, the t-test between models 8 and 9 has a p-value of 0.384. This high p-value indicates that the difference in performance of these two models is not statistically significant. In other words, the model without the contextual word embeddings is close in performance to the baseline model. In contrast, the model with contextual word embeddings is statistically significantly better than all the others (having a p-value of 0.0 for all the t-tests). This result highlights the significant impact that the contextual embeddings have on the performance of the ED task.
<R> <C> RNNs on the Validation Dataset: Assessing Feature Contribution Models <C> RNNs on the Validation Dataset: Assessing Feature Contribution Architecture <C> RNNs on the Validation Dataset: Assessing Feature Contribution Features <C> RNNs on the Validation Dataset: Assessing Feature Contribution train <C> RNNs on the Validation Dataset: Assessing Feature Contribution train <C> RNNs on the Validation Dataset: Assessing Feature Contribution train <C> RNNs on the Validation Dataset: Assessing Feature Contribution validation <C> RNNs on the Validation Dataset: Assessing Feature Contribution validation <C> RNNs on the Validation Dataset: Assessing Feature Contribution validation <C> RNNs on the Validation Dataset: Assessing Feature Contribution validation <C> RNNs on the Validation Dataset: Assessing Feature Contribution validation <R> <C> Models <C> Architecture <C> Features <C> ¯¯¯¯¯¯¯¯¯¯¯ [ITALIC] sens <C> ¯¯¯¯¯¯¯¯¯¯ [ITALIC] spec <C> ¯¯¯¯¯¯¯ [ITALIC] F1 <C> ¯¯¯¯¯¯¯¯¯¯¯ [ITALIC] sens <C> ¯¯¯¯¯¯¯¯¯¯ [ITALIC] spec <C> ¯¯¯¯¯¯¯ [ITALIC] F1 <C> F1 CI <C> p-value <R> <C> 1 <C> ⟨15⟩ <C> all <C> 0.757 <C> 0.977 <C> 0.765 <C> 0.709 <C> 0.952 <C> 0.729 <C> ± 0.026 <C> 0.326 <R> <C> 2 <C> ⟨15⟩ <C> all-{ [ITALIC] B} <C> 0.707 <C> 0.980 <C> 0.728 <C> 0.528 <C> 0.963 <C> 0.578 <C> ± 0.072 <C> 0.001 <R> <C> 3 <C> ⟨15⟩ <C> all-{ [ITALIC] S} <C> 0.671 <C> 0.995 <C> 0.710 <C> 0.571 <C> 0.971 <C> 0.626 <C> ± 0.061 <C> 0.003 <R> <C> 4 <C> ⟨15⟩ <C> all-{ [ITALIC] B, [ITALIC] S} <C> 0.781 <C> 0.997 <C> 0.790 <C> 0.594 <C> 0.935 <C> 0.637 <C> ± 0.023 <C> 0.000 <R> <C> 5 <C> ⟨15⟩ <C> all-{ [ITALIC] T, [ITALIC] P} <C> 0.752 <C> 0.976 <C> 0.761 <C> 0.667 <C> 0.957 <C> 0.698 <C> ± 0.037 <C> 0.026 <R> <C> 6 <C> ⟨15⟩ <C> all-{ [ITALIC] D} <C> 0.747 <C> 0.980 <C> 0.758 <C> 0.718 <C> 0.944 <C> 0.733 <C> ± 0.017 <C> 0.430 <R> <C> 7 <C> ⟨15⟩ <C> all-{ [ITALIC] E} <C> 0.761 <C> 0.977 <C> 0.769 <C> 0.697 <C> 0.950 <C> 0.721 <C> ± 0.034 <C> 0.182 <R> <C> 8 <C> ⟨15⟩ <C> all-{ [ITALIC] W} <C> 0.629 <C> 0.973 <C> 0.644 <C> 0.707 <C> 0.954 <C> 0.727 <C> ± 0.049 <C> 0.341 <R> <C> 9 <C> ⟨15⟩ <C> all-{ [ITALIC] Sp} <C> 0.748 <C> 0.979 <C> 0.759 <C> 0.721 <C> 0.946 <C> [BOLD] 0.735 <C> ± 0.018 <C> — <CAP> Table 9: Average performance of the proposed model (RNN) for nine different sets of features using the best architecture (single Bi-LSTM layer with fifteen hidden units). For each set of features, we run five trials and report the average sensitivity, specificity, and F1-score in both the training and validation data. For the validation data, we also report the confidence intervals at a 95% level of significance for the F1-score. We also report for each model the p-value of a single-tail t-test against the best model of the table. The test is performed between the F1-score in the validation data for each of the two models. A small p-value gives evidence to reject the hypothesis that the models have the same F1-score, indicating that the best model of the table is statistically significantly better. In these experiments, we examine nine different sets of features. We use the complete set of features for comparison, and remove each different input to measure its impact. We removed T and P together because they are semantically the same input (i.e., the simplified and the detailed versions of Part-Of-Speech). And we remove B and S inputs together because those are the two contextual-embedding-related inputs. The contextual-embedding-related features show the most significant impact. The results show that features D, E, W, and Sp have a negligible impact on the performance. We can conclude from these results that, in the presence of all the other features, we can remove each of these features without harming the performance. On the other hand, the T and P features have a small but statistically significant effect when removed. We conducted further experiments to find additional evidence for these findings.
<R> <C> [BOLD] Language <C> [BOLD] Mono (m) <C> [BOLD] CS (m) <C> [BOLD] Total (h) <C> [BOLD] Total (%) <C> [BOLD] Word tokens <C> [BOLD] Word types <R> <C> English <C> 754.96 <C> 121.81 <C> 14.61 <C> 69.26 <C> 194 426 <C> 7 908 <R> <C> isiZulu <C> 92.75 <C> 57.41 <C> 2.50 <C> 11.86 <C> 24 412 <C> 6 789 <R> <C> isiXhosa <C> 65.13 <C> 23.83 <C> 1.48 <C> 7.03 <C> 13 825 <C> 5 630 <R> <C> Sesotho <C> 44.65 <C> 34.04 <C> 1.31 <C> 6.22 <C> 22 226 <C> 2 321 <R> <C> Setswana <C> 36.92 <C> 34.46 <C> 1.19 <C> 5.64 <C> 21 409 <C> 1 525 <R> <C> [BOLD] Total <C> 994.43 <C> 271.54 <C> 21.10 <C> 100 <C> 276 290 <C> 24 170 <CAP> Table 1: Duration in minutes (min) and hours (h) as well as word type and token counts for the unbalanced corpus.
<R> <C> [EMPTY] <C> MT02(dev) <C> MT04(%) <C> MT05(%) <R> <C> MERT <C> 34.61 <C> 31.76 <C> 28.85 <R> <C> MIRA <C> 35.31 <C> 32.25 <C> 29.37 <R> <C> PL(1) <C> 34.20 <C> 31.70 <C> 28.90 <R> <C> PL(2) <C> 34.31 <C> 31.83 <C> 29.10 <R> <C> PL(3) <C> 34.39 <C> 32.05 <C> 29.20 <R> <C> PL(4) <C> 34.40 <C> 32.13 <C> 29.46+ <R> <C> PL(5) <C> 34.46 <C> 32.19+ <C> 29.42+ <R> <C> PL(6) <C> 34.37 <C> 32.16 <C> 29.30 <R> <C> PL(7) <C> 34.39 <C> 32.20+ <C> 29.32 <R> <C> PL(8) <C> 34.70 <C> 32.19+ <C> 29.10 <R> <C> PL(9) <C> 34.30 <C> 32.07 <C> 29.22 <R> <C> PL(10) <C> 34.30 <C> 32.14 <C> 29.19 <CAP> Table 2: PL(k): Plackett-Luce model optimizing the ground-truth permutation with length k. The significant symbols (+ at 0.05 level) are compared with MERT. The bold font numbers signifies better results compared to M(1) system.
<R> <C> [EMPTY] <C> First 6 months #Node <C> First 6 months #Edge <C> First 6 months #Doc <C> Last 6 months #Node <C> Last 6 months #Edge <C> Last 6 months #Doc <C> 1 year #Node <C> 1 year #Edge <C> 1 year #Doc <R> <C> Chinese <C> 3,592 <C> 17,435 <C> 8,394 <C> 3,171 <C> 12,862 <C> 8,933 <C> 7,360 <C> 33,892 <C> 17,327 <R> <C> English <C> 5,078 <C> 28,326 <C> 114,159 <C> 2,948 <C> 43,473 <C> 72,578 <C> 8,852 <C> 85,125 <C> 186,737 <R> <C> Time <C> 161.27s <C> 161.27s <C> 161.27s <C> 480.96s <C> 480.96s <C> 480.96s <C> 979.72s <C> 979.72s <C> 979.72s <CAP> Table 1: Run time of the decipherment algorithm on different sizes of streaming data.
<R> <C> [EMPTY] <C> [BOLD] Countries (AUC-PR) DistMult <C> [BOLD] Countries (AUC-PR) ComplEx <C> [BOLD] Countries (AUC-PR) ConvE <C> [BOLD] Countries (AUC-PR) RotatE <R> <C> S1 <C> [BOLD] 1.00± [BOLD] 0.00 <C> 0.97±0.02 <C> [BOLD] 1.00± [BOLD] 0.00 <C> [BOLD] 1.00± [BOLD] 0.00 <R> <C> S2 <C> 0.72±0.12 <C> 0.57±0.10 <C> 0.99±0.01 <C> [BOLD] 1.00± [BOLD] 0.00 <R> <C> S3 <C> 0.52±0.07 <C> 0.43±0.07 <C> 0.86±0.05 <C> [BOLD] 0.95± [BOLD] 0.00 <CAP> Table 6: Results on the Countries datasets. Other results are taken from (Dettmers et al., 2017).
<R> <C> [ITALIC] N <C> MDP Size # States <C> MDP Size # Transitions <C> Approach in  # Binary Variables <C> Approach in  # Real Variables <C> Approach in  # States <C> Approach in  Time (s) <C> Proposed Approach # Binary Variables <C> Proposed Approach # Real Variables <C> Proposed Approach # States <C> Proposed Approach # Sentences <C> Proposed Approach Time (s) <R> <C> 10 <C> 100 <C> 208 <C> 309 <C> 56 <C> 9 <C> 0.11 <C> 242 <C> 56 <C> 9 <C> 3 <C> 1.39 <R> <C> 20 <C> 400 <C> 788 <C> 1,369 <C> 156 <C> 19 <C> [BOLD] 24.86 <C> 1,022 <C> 156 <C> 39 <C> 3 <C> [BOLD] 4.43 <R> <C> 30 <C> 900 <C> 1,768 <C> 2,652 <C> 900 <C> – <C> [BOLD] time-out <C> 2,273 <C> 291 <C> 29 <C> 3 <C> [BOLD] 1.54 <R> <C> 40 <C> 1,600 <C> 3,148 <C> 4,732 <C> 1,600 <C> – <C> [BOLD] time-out <C> 4,231 <C> 506 <C> 79 <C> 3 <C> [BOLD] 17.45 <R> <C> 50 <C> 2,500 <C> 4,928 <C> 7,412 <C> 2,500 <C> – <C> [BOLD] time-out <C> 6,669 <C> 756 <C> 99 <C> 3 <C> [BOLD] 32.33 <CAP> TABLE II: Experimental results for the warehouse robot planning
<R> <C> [BOLD] Model <C> Rg [BOLD] -4 <C> Bleu <R> <C> Wiseman et al. ( 2018 ) <C> 38.6 <C> 34.8 <R> <C> Liu et al. ( 2018 ) <C> 41.7 <C> 44.7 <R> <C> This work (Seq2seq) <C> 39.3 <C> 42.5 <R> <C> †This work (AttExp) <C> 40.0 <C> 43.1 <R> <C> †This work (AdaDec) <C> 40.6 <C> 43.6 <R> <C> †This work (AdaDec+AttExp) <C> 41.1 <C> 44.1 <CAP> Table 4: Data-to-text generation performance in Rouge-4 and Bleu on the Wikibio test set (§4.3). † indicates the models using retrieved exemplars.
<R> <C> Quantity Trigger List Prediction <C> Accuracy <R> <C> All features <C> [BOLD] 95.3 <R> <C> No Neighborhood features <C> 42.5 <R> <C> No Quantity features <C> 93.2 <R> <C> Variable Trigger List Prediction <C> Accuracy <R> <C> All features <C> [BOLD] 75.5 <R> <C> No Variable features <C> 58.6 <R> <C> No Neighborhood features <C> 70.3 <R> <C> Equation Tree Prediction <C> Accuracy <R> <C> All features <C> [BOLD] 78.9 <R> <C> No Neighborhood features <C> 64.3 <R> <C> No Connecting Text features <C> 70.2 <R> <C> No Number features <C> 77.6 <R> <C> No Lexicon <C> 72.7 <R> <C> No Projectivity <C> 72.8 <R> <C> Conform with Syntactic Parse <C> 70.2 <R> <C> Lexicon as Features <C> 74.5 <CAP> Table 3: Performance of system components
<R> <C> [BOLD] Model <C> [BOLD] BLEU1 <C> [BOLD] BLEU2 <C> [BOLD] BLEU3 <C> [BOLD] BLEU4 <R> <C> COCO <C> 13.97 <C> 6.13 <C> 2.85 <C> 1.39 <R> <C> SIND <C> 13.39 <C> 2.99 <C> 0.82 <C> 0.18 <R> <C> DenseCap <C> 20.77 <C> 9.26 <C> 4.15 <C> 1.90 <R> <C> Ours <C> [BOLD] 20.87 <C> 8.71 <C> 3.58 <C> 1.41 <CAP> Table 3: Performances of the generated image narratives with human-written image narratives as ground truth.
<R> <C> [BOLD] Model <C> [BOLD] # Overall <C> [BOLD] # Yes <C> [BOLD] # No <R> <C> Ours <C> 1,000 <C> [BOLD] 664 <C> 336 <R> <C> VQG <C> 1,000 <C> 217 <C> 783 <R> <C> Overall <C> 2,000 <C> 881 <C> 1119 <CAP> Table 7: Evaluation results on whether the generated questions allow for multiple responses.
<R> <C> [BOLD] Model <C> [BOLD] SARI <C> [BOLD] Oracle <R> <C> Hybrid <C> 33.27 <C> – <R> <C> DRESS <C> 36.00 <C> – <R> <C> DMASS <C> 34.35 <C> – <R> <C> S2S <C> 36.32 <C> – <R> <C> S2S-Loss <C> 36.03 <C> – <R> <C> S2S-FA <C> 36.47 <C> 54.01 <R> <C> S2S-Cluster-FA <C> [BOLD] 37.22 <C> 50.36 <R> <C> S2S-Diverse-FA <C> 35.36 <C> 52.65 <R> <C> S2S-All-FAS <C> 36.30 <C> 50.40 <R> <C> S2S-All-FA <C> [BOLD] 37.11 <C> 50.40 <CAP> Table 4: Comparison of our models to baselines and state-of-the-art models using SARI. We also include oracle SARI scores (Oracle), given a perfect reranker. S2S-All-FA is significantly better than the DMASS and Hybrid baselines using a student t-test (p<0.05).
<R> <C> [BOLD] Model <C> [BOLD] Len <C> [BOLD] FKGL <C> [BOLD] TER <C> [BOLD] Ins <C> [BOLD] Edit <R> <C> Complex <C> 23.1 <C> 11.14 <C> 0 <C> 0 <C> – <R> <C> Hybrid <C> 12.4 <C> 7.82 <C> 0.49 <C> 0.01 <C> – <R> <C> DRESS <C> 14.4 <C> 7.60 <C> 0.44 <C> 0.07 <C> – <R> <C> DMASS <C> 15.1 <C> 7.40 <C> 0.59 <C> 0.28 <C> – <R> <C> S2S <C> 16.1 <C> 7.91 <C> 0.41 <C> 0.23 <C> – <R> <C> S2S-Loss <C> 16.4 <C> 8.11 <C> 0.40 <C> 0.31 <C> – <R> <C> S2S-FA <C> 7.6 <C> 6.42 <C> 0.73 <C> 0.01 <C> 7.28 <R> <C> S2S-Cluster-FA <C> 9.1 <C> 6.49 <C> 0.68 <C> 0.05 <C> 7.55 <R> <C> S2S-Diverse-FA <C> 7.5 <C> 5.97 <C> 0.78 <C> 0.07 <C> 8.22 <R> <C> S2S-All-FAS <C> 9.1 <C> 5.37 <C> 0.68 <C> 0.05 <C> 7.56 <R> <C> S2S-All-FA <C> 10.8 <C> 6.42 <C> 0.61 <C> 0.07 <C> 7.56 <R> <C> Reference <C> 12.8 <C> 6.90 <C> 0.67 <C> 0.42 <C> – <CAP> Table 5: Average sentence length, FKGL, TER score compared to input, and number of insertions. We also calculate average edit distance (Edit) between candidate sentences for applicable models.
<R> <C> [BOLD] Logic template <C> [BOLD] Example <C> [BOLD] Count <R> <C> 1. (¬({goal\,}) \coloneq {state\,})∧ ({goal\,} \coloneq {action\,}({state\,})) <C> If it snows tonight then wake me up early because I want to arrive to work on time <C> 65 <R> <C> 2. ({goal\,} \coloneq {action\,}({state\,}))∧ (¬({goal\,}) \coloneq ¬({action\,}({state\,}))) <C> If I am walking to a meeting then remind me who else is there because I want to be prepared for the meeting <C> 50 <R> <C> 3. ({goal\,} \coloneq {action\,} [ITALIC] h)∧ ({action\,} [ITALIC] h \coloneq {action\,}({state\,})) <C> If we are approaching Fall then remind me to buy flower bulbs because I want to make sure I have a pretty Spring garden. <C> 17 <R> <C> 4. ({goal\,} \coloneq {state\,})∧ (¬({goal\,}) \coloneq {action\,}({state\,})) <C> If I am at the grocery store but I have a trip coming up in the next week then remind me not to buy perishables because they will go bad while I am away <C> 5 <R> <C> 5. other <C> If tomorrow is a holiday then ask me if I want to disable or change my alarms because I don’t want to wake up early if I don’t need to go to work early. <C> 23 <CAP> Table 5: Different reasoning templates of the statements that we uncovered, presumably reflecting how humans logically reason. ∧, ¬, \coloneq indicate logical and, negation, and implication, respectively. actionh is an action that is hidden in the main utterance and action(state) indicates performing the action when the state holds.
<R> <C> [BOLD] Model <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> BiLSTM <C> 0.903 <C> 0.912 <C> 0.908 <R> <C> NegEx <C> 0.664 <C> 0.944 <C> 0.780 <R> <C> NegEx - Stanford <C> 0.944 <C> 0.912 <C> [BOLD] 0.928 <CAP> Table 5: Comparison of BiLSTM, NegEx and NegEx-Stanford for negation detection. All algorithms predicted whether a given medical entity was negated or affirmed.
<R> <C> Increment <C> System <C> Announce <C> Quotes <C> Playlist <C> Alarms <C> Chat <R> <C> 0% <C> Baseline <C> 41.05 <C> 107.44 <C> 51.33 <C> 22.75 <C> 30.86 <R> <C> 10% <C> Annotation <C> 34.05 <C> 77.02 <C> 40.61 <C> 16.01 <C> 34.03 <R> <C> 10% <C> + SSL <C> [BOLD] 27.16 <C> [BOLD] 67.45 <C> [BOLD] 33.32 <C> [BOLD] 15.12 <C> [BOLD] 26.73 <R> <C> 20% <C> Annotation <C> 31.78 <C> 70.02 <C> 37.89 <C> 14.28 <C> 34.72 <R> <C> 20% <C> + SSL <C> [BOLD] 24.75 <C> [BOLD] 59.98 <C> [BOLD] 32.38 <C> [BOLD] 13.37 <C> [BOLD] 28.78 <R> <C> 50% <C> Annotation <C> 29.11 <C> 57.95 <C> 31.11 <C> 11.65 <C> 31.70 <R> <C> 50% <C> + SSL <C> [BOLD] 23.71 <C> [BOLD] 50.43 <C> [BOLD] 25.78 <C> [BOLD] 10.97 <C> [BOLD] 24.98 <R> <C> 80% <C> Annotation <C> 28.08 <C> 50.70 <C> 26.92 <C> 11.20 <C> 31.19 <R> <C> 80% <C> + SSL <C> [BOLD] 24.85 <C> [BOLD] 43.40 <C> [BOLD] 23.38 <C> [BOLD] 10.62 <C> [BOLD] 24.54 <R> <C> 100% <C> Annotation <C> 28.36 <C> 50.82 <C> 26.13 <C> 10.90 <C> 30.30 <R> <C> 100% <C> + SSL <C> [BOLD] 23.40 <C> [BOLD] 41.40 <C> [BOLD] 21.90 <C> [BOLD] 10.40 <C> [BOLD] 22.73 <CAP> Table 2: The impact of SSL on NLU Performance for five functionalities. Numbers are reported in SER.
<R> <C> Incr. <C> Select <C> Annc. <C> Quotes <C> Playlist <C> Alarms <C> Chat <R> <C> 10% <C> ALL <C> 27.16 <C> 67.45 <C> 33.32 <C> 15.12 <C> 26.73 <R> <C> 10% <C> Para <C> [BOLD] 28.14 <C> [BOLD] 67.10 <C> [BOLD] 33.82 <C> [BOLD] 14.91 <C> [BOLD] 27.59 <R> <C> 10% <C> Rand <C> 29.25 <C> 68.07 <C> 34.56 <C> 15.45 <C> 28.72 <R> <C> 10% <C> Sub-m. <C> 28.95 <C> 68.04 <C> 34.44 <C> 15.05 <C> 28.35 <R> <C> 10% <C> Uniq <C> 28.96 <C> 67.42 <C> 34.29 <C> 15.00 <C> 27.90 <R> <C> 20% <C> ALL <C> 25.75 <C> 59.98 <C> 32.38 <C> 13.37 <C> 28.78 <R> <C> 20% <C> Para <C> [BOLD] 26.59 <C> 61.37 <C> 32.34 <C> 13.51 <C> [BOLD] 28.84 <R> <C> 20% <C> Rand <C> 27.84 <C> 61.35 <C> 33.37 <C> 13.55 <C> 29.87 <R> <C> 20% <C> Sub-m. <C> 27.05 <C> [BOLD] 60.75 <C> 32.11 <C> [BOLD] 13.33 <C> 29.62 <R> <C> 20% <C> Uniq <C> 26.92 <C> 60.97 <C> [BOLD] 31.70 <C> 13.57 <C> 29.22 <R> <C> 50% <C> ALL <C> 23.71 <C> 50.43 <C> 25.78 <C> 10.97 <C> 24.98 <R> <C> 50% <C> Para <C> [BOLD] 24.40 <C> 50.41 <C> [BOLD] 25.38 <C> 11.11 <C> [BOLD] 26.16 <R> <C> 50% <C> Rand <C> 24.99 <C> 51.62 <C> 26.18 <C> 11.17 <C> 27.57 <R> <C> 50% <C> Sub-m. <C> 24.69 <C> 50.43 <C> 25.76 <C> [BOLD] 11.04 <C> 26.56 <R> <C> 50% <C> Uniq <C> 24.64 <C> [BOLD] 50.23 <C> 25.63 <C> 11.19 <C> 27.06 <R> <C> 80% <C> ALL <C> 28.08 <C> 43.40 <C> 23.38 <C> 10.62 <C> 24.54 <R> <C> 80% <C> Para <C> [BOLD] 24.52 <C> [BOLD] 44.00 <C> 23.41 <C> [BOLD] 10.61 <C> [BOLD] 25.31 <R> <C> 80% <C> Rand <C> 24.86 <C> 44.87 <C> 24.13 <C> 10.74 <C> 26.58 <R> <C> 80% <C> Sub-m. <C> 24.85 <C> 44.25 <C> [BOLD] 23.38 <C> 10.61 <C> 25.72 <R> <C> 80% <C> Uniq <C> 25.35 <C> 44.03 <C> 23.42 <C> 10.63 <C> 25.79 <R> <C> 100% <C> ALL <C> 23.40 <C> 41.40 <C> 21.90 <C> 10.40 <C> 22.73 <R> <C> 100% <C> Para <C> [BOLD] 23.41 <C> 42.28 <C> 21.98 <C> 10.41 <C> [BOLD] 22.80 <R> <C> 100% <C> Rand <C> 24.12 <C> 43.05 <C> 22.47 <C> [BOLD] 10.35 <C> 24.20 <R> <C> 100% <C> Sub-m. <C> 23.86 <C> 42.61 <C> 22.17 <C> 10.43 <C> 23.57 <R> <C> 100% <C> Uniq <C> 23.51 <C> [BOLD] 42.06 <C> [BOLD] 21.65 <C> 10.48 <C> 23.01 <CAP> Table 3: SER score of five functionalities for each annotation increment using different selection schemes. The best selection mechanism is shown in bold letters.
<R> <C> Lexical Category <C> [ITALIC] M1 (Informed) <C> [ITALIC] M1 (Misinformed) <C> z-score ( [ITALIC] Z1) <C> p-value ( [ITALIC] Z1) <R> <C> function <C> 33.90 <C> 29.32 <C> 7.25 <C>  <R> <C> pronoun <C> 7.97 <C> 6.53 <C> 4.89 <C>  <R> <C> ipron <C> 3.26 <C> 3.03 <C> 1.23 <C> .2 <R> <C> ppron <C> 4.71 <C> 3.49 <C> 5.39 <C>  <R> <C> Analytic <C> 69.83 <C> 76.01 <C> -4.82 <C>  <R> <C> social <C> 6.49 <C> 5.05 <C> 5.45 <C>  <R> <C> family <C> .34 <C> .20 <C> 2.24 <C> .03 <R> <C> friend <C> .17 <C> .17 <C> -.03 <C> .97 <R> <C> Authentic <C> 25.12 <C> 16.43 <C> 6.78 <C>  <R> <C> Tone <C> 35.42 <C> 37.59 <C> -1.45 <C> .15 <R> <C> informal <C> 4.89 <C> 5.16 <C> -1.63 <C> .10 <R> <C> swear <C> .51 <C> .34 <C> 1.86 <C> .06 <CAP> Table 5: This table shows the summary of our analyses across all the linguistic dimensions described above using LIWC. The first column shows the lexical category. The second and third columns show the test statistic (M1) as the mean of the LIWC indices for informed and misinformed communities respectively. The fourth and fifth columns display the z-score and p-value for the independent z-test for the difference in means.
<R> <C> Measure <C> Span <C> Parameters for best correlation  [ITALIC] ϵ <C> Parameters for best correlation  [ITALIC] δ <C> Parameters for best correlation Type <C> Parameters for best correlation Correlation <R> <C> PMI <C> 5w <C> 0.05 <C> 1 <C> C <C> 91.3 <R> <C> PMI <C> 25w <C> 0.40 <C> 1 <C> D <C> 85.3 <R> <C> PMI <C> 50w <C> 0.50 <C> 1 <C> D <C> 82.0 <R> <C> CWCD <C> 5w <C> 0.99 <C> 0.9 <C> D <C> 83.6 <R> <C> CWCD <C> 25w <C> 0.50 <C> 0.9 <C> D <C> 76.0 <R> <C> CWCD <C> 50w <C> 0.50 <C> 0.9 <C> D <C> 74.4 <R> <C> CSA <C> 5w <C> 0.1 <C> 0.0005 <C> A <C> 98.9 <R> <C> CSA <C> 25w <C> 0.05 <C> 0.0005 <C> A <C> 96.7 <R> <C> CSA <C> 50w <C> 0.1 <C> 0.0005 <C> A <C> 94.9 <R> <C> Dice <C> 5w <C> 0.1 <C> 0.005 <C> A <C> 96.1 <R> <C> Dice <C> 25w <C> 0.05 <C> 0.005 <C> A <C> 93.0 <R> <C> Dice <C> 50w <C> 0.1 <C> 0.0005 <C> A <C> 91.3 <R> <C> Ochiai <C> 5w <C> 0.1 <C> 0.1 <C> A <C> 97.4 <R> <C> Ochiai <C> 25w <C> 0.1 <C> 0.01 <C> A <C> 95.5 <R> <C> Ochiai <C> 50w <C> 0.1 <C> 0.005 <C> A <C> 94.5 <R> <C> LLR <C> 5w <C> 0.05 <C> 0.0005 <C> A <C> 97.3 <R> <C> LLR <C> 25w <C> 0.05 <C> 0.0005 <C> A <C> 94.8 <R> <C> LLR <C> 50w <C> 0.1 <C> 0.0005 <C> A <C> 92.6 <R> <C> TTest <C> 5w <C> 0.05 <C> 0.0005 <C> A <C> 94.2 <R> <C> TTest <C> 25w <C> 0.05 <C> 0.0005 <C> A <C> 90.9 <R> <C> TTest <C> 50w <C> 0.1 <C> 0.0005 <C> A <C> 88.8 <R> <C> SCI <C> 5w <C> 0.05 <C> 0.0005 <C> A <C> 82.7 <R> <C> SCI <C> 25w <C> 0.05 <C> 0.0005 <C> A <C> 75.9 <R> <C> SCI <C> 50w <C> 0.1 <C> 0.0005 <C> A <C> 73.1 <CAP> Table 5: Best performing (ϵ,δ)-pairs for different measures on sim data
<R> <C> [EMPTY] <C> [BOLD] SQuAD <C> [BOLD] Trivia <C> [BOLD] QuAC <C> [BOLD] QA-SRL <C> [BOLD] QA-ZRE <C> [BOLD] MNLI <C> [BOLD] SNLI <R> <C> BERT <C> 85.4 <C> 72.5 <C> 60.0 <C> 85.0 <C> 88.2 <C> 81.1 <C> 88.0 <R> <C> ELMo <C> 78.3 <C> 57.1 <C> 54.3 <C> 67.3 <C> 88.5 <C> 69.1 <C> 77.9 <CAP> Table 3: Results on multiple tasks for BERT and ELMo trained with a random training curriculum for 200,000 iterations.
<R> <C> [BOLD] Methods <C> [BOLD] R-1 10% <C> [BOLD] R-2 10% <C> [BOLD] R-SU4 10% <C> [BOLD] R-1 40% <C> [BOLD] R-2 40% <C> [BOLD] R-SU4 40% <C> [BOLD] R-1 70% <C> [BOLD] R-2 70% <C> [BOLD] R-SU4 70% <R> <C> SWR <C> 56.82 <C> 48.18 <C> 47.03 <C> 76.48 <C> 71.74 <C> 70.39 <C> 88.21 <C> 86.17 <C> 85.19 <R> <C> SWR_NSE <C> 55.02 <C> 46.08 <C> 45.15 <C> [BOLD] 71.98 <C> [BOLD] 66.76 <C> [BOLD] 65.42 <C> [BOLD] 85.94 <C> [BOLD] 83.18 <C> [BOLD] 81.83 <R> <C> SWR_NAS <C> [BOLD] 51.74 <C> [BOLD] 40.87 <C> [BOLD] 40.13 <C> 73.52 <C> 68.26 <C> 67.12 <C> 87.82 <C> 84.95 <C> 84.12 <R> <C> SWR_NSC <C> 56.36 <C> 47.34 <C> 46.44 <C> 75.68 <C> 70.35 <C> 69.03 <C> 87.54 <C> 84.49 <C> 83.54 <R> <C> SWR_NSP <C> 56.77 <C> 48.12 <C> 46.97 <C> 76.39 <C> 71.59 <C> 70.25 <C> 88.14 <C> 86.04 <C> 85.07 <R> <C> TextRank <C> 49.22 <C> 36.07 <C> 35.29 <C> 71.16 <C> 65.47 <C> 63.94 <C> 86.04 <C> 83.52 <C> 82.29 <CAP> Table 5: ROUGE (%) comparison with different features removed
<R> <C> [EMPTY] <C> Pearson ( [ITALIC] ρp) Ours <C> Pearson ( [ITALIC] ρp) Expert Resp. <C> Spearman ( [ITALIC] ρs) Ours <C> Spearman ( [ITALIC] ρs) Expert Resp. <R> <C> 2005 <C> 0.81 <C> 0.81 <C> [BOLD] 0.79 <C> 0.77 <R> <C> 2006 <C> [BOLD] 0.74 <C> 0.60 <C> [BOLD] 0.69 <C> 0.40 <CAP> Table 1: Correlations to the original Pyramid scores, for our crowdsourced method and for expert Responsiveness method, for DUC ’05 and ’06.
<R> <C> Model <C> R@1 <C> R@10 <C> R@50 <R> <C> REDIAL <C> 2.3±0.2 <C> 12.9±0.7 <C> 28.7±0.9 <R> <C> [BOLD] KBRD (D) <C> 2.7±0.2 <C> 14.0±0.6 <C> 30.6±0.7 <R> <C> [BOLD] KBRD (K) <C> 2.6±0.2 <C> 14.4±0.9 <C> 31.0±1.2 <R> <C> [BOLD] KBRD <C> [BOLD] 3.0±0.2 <C> [BOLD] 16.3±0.3 <C> [BOLD] 33.8±0.7 <CAP> Table 2: Evaluation of the recommender system. We report the results of Recall@1, Recall@10 and Recall@50 of the models (p≪0.01). KBRD (D) stands for only incorporating the dialog contents. KBRD (K) stands for only incorporating knowledge. The results demonstrate that both the interaction with the dialog system and the external knowledge are helpful for the improvement of model performance, and our proposed model reaches the best performance on the three metrics.
<R> <C> Model <C> PPL <C> Dist-3 <C> Dist-4 <C> CSTC <R> <C> REDIAL <C> 28.1 <C> 0.11 <C> 0.13 <C> 1.73 <R> <C> Transformer <C> 18.0 <C> 0.27 <C> 0.39 <C> - <R> <C> [BOLD] KBRD <C> [BOLD] 17.9 <C> [BOLD] 0.30 <C> [BOLD] 0.45 <C> [BOLD] 1.99 <CAP> Table 3: Automatic and human evaluation of dialog generation. For automatic evaluation, we evaluate the perplexity (PPL) and distinct n-gram (Dist-3 and Dist-4 refer to distinct 3-gram and 4-gram respectively) of the generated dialogs. For human evaluation, we ask human annotators to evaluate the consistency (CSTC) of the generated utterances with the dialog history. Our proposed method performs the best in all evaluations compared with the baselines.
<R> <C> Model Options <C> PKU <C> MSR <R> <C> without pretraining <C> 94.7 <C> 96.7 <R> <C> with pretraining <C> [BOLD] 95.7 <C> [BOLD] 97.3 <CAP> Table 4: Test performances with or without pretraining character embeddings. “without pretraining” means that the character embeddings are randomly initialized.
<R> <C> Models <C> teacher PKU <C> teacher MSR <C> student PKU <C> student MSR <R> <C> WE-CONV-SEG <C> 95.7 <C> 97.4 <C> 96.5 <C> 98.0 <R> <C> worse teacher <C> 95.4 <C> 97.1 <C> 96.4 <C> 97.9 <R> <C> better teacher <C> 96.5 <C> 98.0 <C> 96.5 <C> 98.0 <CAP> Table 6: Performances of student models and teacher models. A previous trained model maybe reused in following so that there are some
<R> <C> Models <C> PKU <C> MSR <R> <C> WE-CONV-SEG <C> [BOLD] 96.5 <C> [BOLD] 98.0 <R> <C> -word emb <C> 96.1 <C> 97.6 <R> <C> -word feature <C> 95.7 <C> 97.3 <CAP> Table 7: Performances of our models with different word feature options. “-word emb” denotes the model in which word features and the vocabulary are used but the pretrained word embeddings are not. “-word feature” denotes the model that uses no word feature, i.e. CONV-SEG.
<R> <C> System <C> [BOLD] Eval-1 with Train set Intent <C> [BOLD] Eval-1 with Train set Slot <C> [BOLD] Eval-1 with Train set Sentence <C> [BOLD] Eval-2 with Train-R set Intent <C> [BOLD] Eval-2 with Train-R set Slot <C> [BOLD] Eval-2 with Train-R set Sentence <R> <C> [BOLD] Rasa <C> 96.42 <C> 85.40 <C> 65.76 <C> 93.84 <C> 78.58 <C> 52.25 <R> <C> [BOLD] LUIS <C> 95.99 <C> 79.47 <C> 50.57 <C> 94.46 <C> 72.51 <C> 35.53 <R> <C> [BOLD] Watson Assistant <C> 96.56 <C> - <C> - <C> 95.03 <C> - <C> - <R> <C> [BOLD] Dialogflow <C> 95.56 <C> 74.62 <C> 46.16 <C> 93.60 <C> 65.23 <C> 36.68 <R> <C> [BOLD] Bert-Joint <C> [BOLD] 97.6 <C> [BOLD] 90.0 <C> [BOLD] 77.1 <C> [BOLD] 96.13 <C> [BOLD] 83.04 <C> [BOLD] 65.23 <CAP> Table 3: Overall scores for Intent and Slot
<R> <C> [BOLD] method <C> [BOLD] task type <C> [BOLD] WERs(%)  [BOLD] avg <C> [BOLD] WERs(%)  [BOLD] bus <C> [BOLD] WERs(%)  [BOLD] caf <C> [BOLD] WERs(%)  [BOLD] ped <C> [BOLD] WERs(%)  [BOLD] str <R> <C> [EMPTY] <C> development set <C> development set <C> development set <C> development set <C> development set <C> development set <R> <C> baseline <C> simu <C> 57.01 <C> 50.49 <C> 65.96 <C> 49.19 <C> 62.39 <R> <C> baseline <C> real <C> 58.77 <C> 71.47 <C> 63.30 <C> 43.30 <C> 57.03 <R> <C> CycleGAN <C> simu <C> 66.99 <C> 65.94 <C> 69.47 <C> 64.38 <C> 68.17 <R> <C> CycleGAN <C> real <C> 62.11 <C> 78.01 <C> 62.67 <C> 52.35 <C> 55.40 <R> <C> proposed <C> simu <C> [BOLD] 50.46 <C> [BOLD] 41.45 <C> [BOLD] 60.88 <C> [BOLD] 45.18 <C> [BOLD] 54.34 <R> <C> proposed <C> real <C> [BOLD] 43.07 <C> [BOLD] 50.18 <C> [BOLD] 47.33 <C> [BOLD] 32.19 <C> [BOLD] 32.59 <R> <C> [EMPTY] <C> evaluation set <C> evaluation set <C> evaluation set <C> evaluation set <C> evaluation set <C> evaluation set <R> <C> baseline <C> simu <C> 72.50 <C> 66.29 <C> 76.69 <C> 75.18 <C> 71.85 <R> <C> baseline <C> real <C> 81.09 <C> 94.20 <C> 83.49 <C> 80.83 <C> 65.84 <R> <C> CycleGAN <C> simu <C> 86.21 <C> 90.21 <C> 85.26 <C> 85.88 <C> 83.88 <R> <C> CycleGAN <C> real <C> 89.92 <C> 97.66 <C> 90.46 <C> 89.97 <C> 81.58 <R> <C> proposed <C> simu <C> [BOLD] 65.78 <C> [BOLD] 55.34 <C> [BOLD] 72.45 <C> [BOLD] 71.27 <C> [BOLD] 64.05 <R> <C> proposed <C> real <C> [BOLD] 70.21 <C> [BOLD] 82.31 <C> [BOLD] 74.90 <C> [BOLD] 70.89 <C> [BOLD] 52.75 <CAP> Table 2: WERs for noisy-to-clean speech domain adaptation with the CHiME4 challenge corpus.
<R> <C> [BOLD] Customer <C> [BOLD] Data Subset <C> [BOLD] Structured <C> [BOLD] Text <C> [BOLD] Combined Model <R> <C> Existing Customers <C> Structured <C> 1.00 <C> [EMPTY] <C> [EMPTY] <R> <C> Existing Customers <C> Text <C> 0.29 <C> 1.00 <C> [EMPTY] <R> <C> Existing Customers <C> Combined Model <C> 0.97 <C> 0.43 <C> 1.00 <R> <C> New Customers <C> Structured <C> 1.00 <C> [EMPTY] <C> [EMPTY] <R> <C> New Customers <C> Text <C> 0.17 <C> 1.00 <C> [EMPTY] <R> <C> New Customers <C> Combined Model <C> 0.97 <C> 0.30 <C> 1.00 <R> <C> All Customers <C> Structured <C> 1.00 <C> [EMPTY] <C> [EMPTY] <R> <C> All Customers <C> Text <C> 0.31 <C> 1.00 <C> [EMPTY] <R> <C> All Customers <C> Combined Model <C> 0.97 <C> 0.43 <C> 1.00 <CAP> Table 3: Probability Correlations
<R> <C> [EMPTY] <C> [BOLD] Amazon  [BOLD] BERT-FT: 94.00 <C> [BOLD] Amazon  [BOLD] BERT-FT: 94.00 <C> [BOLD] Amazon  [BOLD] Adapter: 94.25 <C> [BOLD] Amazon  [BOLD] Adapter: 94.25 <C> [BOLD] Yelp  [BOLD] BERT-FT: 91.67 <C> [BOLD] Yelp  [BOLD] BERT-FT: 91.67 <C> [BOLD] Yelp  [BOLD] Adapter: 93.50 <C> [BOLD] Yelp  [BOLD] Adapter: 93.50 <C> [BOLD] IMDB  [BOLD] BERT-FT: 92.33 <C> [BOLD] IMDB  [BOLD] BERT-FT: 92.33 <C> [BOLD] IMDB  [BOLD] Adapter: 90.50 <C> [BOLD] IMDB  [BOLD] Adapter: 90.50 <R> <C> [EMPTY] <C> [BOLD] BOW <C> [BOLD] CNN-R <C> [BOLD] CNN-S <C> [BOLD] CNN-NS <C> [BOLD] BOW <C> [BOLD] CNN-R <C> [BOLD] CNN-S <C> [BOLD] CNN-NS <C> [BOLD] BOW <C> [BOLD] CNN-R <C> [BOLD] CNN-S <C> [BOLD] CNN-NS <R> <C> [BOLD] Default <C> 79.2 <C> 91.1 <C> 94.7 <C> 95.9 <C> 81.3 <C> 92.7 <C> 95.2 <C> 95.8 <C> 89.3 <C> 93.2 <C> 96.6 <C> 96.8 <R> <C> [BOLD] ConcatFT <C> - <C> [BOLD] 94.0 <C> [BOLD] 95.7 <C> [BOLD] 96.8 <C> - <C> [BOLD] 96.2 <C> [BOLD] 97.2 <C> [BOLD] 98.3 <C> - <C> [BOLD] 97.0 <C> [BOLD] 98.3 <C> [BOLD] 98.4 <R> <C> [BOLD] Concat <C> 89.6 <C> 93.2 <C> 95.3 <C> 96.4 <C> 89.0 <C> 96.5 <C> 97.1 <C> 98.3 <C> 89.3 <C> 96.2 <C> 98.1 <C> 98.3 <R> <C> [BOLD] KCCA <C> 89.1 <C> 91.5 <C> 94.3 <C> 95.8 <C> 88.5 <C> 91.5 <C> 91.9 <C> 96.2 <C> 88.3 <C> 94.1 <C> 97.9 <C> 97.2 <R> <C> [BOLD] CCA <C> 50.9 <C> 79.1 <C> 83.6 <C> 81.3 <C> 50.3 <C> 71.5 <C> 67.8 <C> 69.4 <C> 51.0 <C> 80.8 <C> 83.3 <C> 85.0 <CAP> Table 2: Test accuracy for Amazon, Yelp and IMDB datasets. BOW, CNN-R, CNN-S and CNN-NS refers to Bag of Words, text CNN with random initialised, static and non-static word embeddings. Best results in boldface.
<R> <C> [EMPTY] <C> [BOLD] MR <C> [BOLD] MPQA <C> [BOLD] SUBJ <C> [BOLD] TREC <R> <C> [BOLD] CNN-non-static <C> 80.93 <C> 88.38 <C> 89.25 <C> 92.98 <R> <C> [BOLD] BERT No-FT <C> 83.26 <C> 87.44 <C> 95.96 <C> 88.06 <R> <C> [BOLD] Adapter BERT <C> 85.55 <C> 90.40 <C> 97.40 <C> 96.55 <R> <C> [BOLD] BERT FT <C> 86.22 <C> 90.47 <C> 96.95 <C> 96.40 <R> <C> [BOLD] Concat <C> 85.60 <C> 90.06 <C> 95.92 <C> 96.64 <R> <C> [BOLD] CCA <C> 85.41 <C> 77.22 <C> 94.55 <C> 84.28 <R> <C> [BOLD] ConcatFT <C> [BOLD] 87.15 <C> [BOLD] 91.19 <C> [BOLD] 97.60 <C> [BOLD] 97.06 <CAP> Table 3: Test accuracy for medium-sized datasets. Best results on the datasets are highlighted in boldface.
<R> <C> Size <C> [BOLD] CNN <C> [BOLD] No-FT <C> [BOLD] FT <C> [BOLD] Concat <C> [BOLD] CCA <C> [BOLD] ConcatFT <R> <C> 100 <C> 76.23 <C> 75.69 <C> 76.85 <C> 73.28 <C> 55.78 <C> [BOLD] 78.76 <R> <C> 200 <C> 74.95 <C> 77.31 <C> 79.41 <C> 77.74 <C> 61.73 <C> [BOLD] 80.02 <R> <C> 500 <C> 78.39 <C> 78.75 <C> 80.38 <C> 80.10 <C> 58.87 <C> [BOLD] 80.96 <R> <C> 2000 <C> 78.30 <C> 80.78 <C> 82.75 <C> 80.89 <C> 79.53 <C> [BOLD] 83.94 <R> <C> 4000 <C> 79.29 <C> 81.96 <C> 83.88 <C> 83.85 <C> 83.37 <C> [BOLD] 84.82 <R> <C> 6000 <C> 80.14 <C> 82.80 <C> 84.72 <C> 85.08 <C> 85.28 <C> [BOLD] 86.06 <R> <C> 8528 <C> 80.93 <C> 83.26 <C> 86.22 <C> 85.60 <C> 85.41 <C> [BOLD] 87.15 <CAP> Table 4: Test accuracy for the MR dataset with different training dataset sizes. FT and No-FT refers to BERT fine-tuned and not fine-tuned respectively.
<R> <C> Model <C> Top-n Accuracy (%)  [ITALIC] n=1 <C> Top-n Accuracy (%)  [ITALIC] n=2 <C> Top-n Accuracy (%)  [ITALIC] n=3 <C> Top-n Accuracy (%)  [ITALIC] n=4 <C> Top-n Accuracy (%)  [ITALIC] n=5 <R> <C> LR <C> 18.3 <C> 32.1 <C> 42.6 <C> 52.2 <C> 60.8 <R> <C> CNN <C> [BOLD] 34.0 <C> 50.3 <C> [BOLD] 61.5 <C> 70.0 <C> 77.4 <R> <C> RNN <C> 33.3 <C> [BOLD] 50.6 <C> 61.4 <C> [BOLD] 70.5 <C> [BOLD] 77.6 <CAP> Table 3: Top-n accuracies for each model. LR: logistic regression. CNN: convolutional neural network. RNN: recurrent neural network.
<R> <C> [EMPTY] <C> NEGLM <C> NEGLM-B <C> NCE <R> <C> PTB <C> [BOLD] 98.35 <C> 100.69 <C> 104.33 <R> <C> WMT <C> [BOLD] 65.84 <C> [BOLD] 65.62 <C> 69.28 <CAP> Table 2: Perplexity results on test sets.
<R> <C> [EMPTY] <C> Std. <C> C-C <C> +coverage Std. <C> +coverage C-C <R> <C> ROUGE-1 <C> 35.77 <C> 34.10 <C> 38.82 <C> 36.22 <R> <C> ROUGE-2 <C> 15.28 <C> 14.02 <C> 16.81 <C> 14.69 <R> <C> ROUGE-L <C> 32.54 <C> 31.15 <C> 35.71 <C> 33.60 <R> <C> METEOR <C> 15.03 <C> 13.73 <C> 16.73 <C> 14.57 <R> <C> METEOR⋆ <C> 16.33 <C> 14.96 <C> 18.14 <C> 15.84 <R> <C> # of tokens <C> 56.10 <C> 50.47 <C> 59.87 <C> 50.30 <CAP> Table 1: The summary quality measured in ROUGE-X and METEOR. Standard: Standard abstractive beam search for decoding. C-C: copy-controlled decoding. METEOR⋆: METEOR +stem +synonym +paraphrase.
<R> <C> [EMPTY] <C> FF <C> LSTM <C> GRU <C> Zoneout LSTM <R> <C> Master <C> 17.00±0.23 <C> 15.30±0.13 <C> 15.66±0.19 <C> 21.73±0.26 <R> <C> Master + RPL <C> 17.09±0.26 <C> 15.29±0.21 <C> 15.71±0.14 <C> 27.81±0.40 <R> <C> Folds <C> 17.14±0.09 <C> 14.98±0.10 <C> 15.12±0.13 <C> 20.98±0.19 <R> <C> Folds + RPL <C> 17.27±0.10 <C> 14.94±0.12 <C> 15.27±0.13 <C> 28.73±0.20 <R> <C> Master + Folds <C> 17.04±0.10 <C> 14.84±0.14 <C> 15.22±0.11 <C> 20.81±0.19 <R> <C> Master + Folds + RPL <C> 17.17±0.09 <C> 14.84±0.12 <C> 15.22±0.09 <C> 28.17±0.24 <CAP> Table 1: DNN Phone Error Rate [%]
<R> <C> [EMPTY] <C> [EMPTY] <C> WEAT T8 (gender bias, science vs. art) Explicit <C> WEAT T8 (gender bias, science vs. art) Explicit <C> WEAT T8 (gender bias, science vs. art) Explicit <C> WEAT T8 (gender bias, science vs. art) Implicit <C> WEAT T8 (gender bias, science vs. art) Implicit <C> WEAT T8 (gender bias, science vs. art) SemQ <C> WEAT T8 (gender bias, science vs. art) SemQ <C> WEAT T1 (sentiment, flowers vs. insects) Explicit <C> WEAT T1 (sentiment, flowers vs. insects) Explicit <C> WEAT T1 (sentiment, flowers vs. insects) Explicit <C> WEAT T1 (sentiment, flowers vs. insects) Implicit <C> WEAT T1 (sentiment, flowers vs. insects) Implicit <C> WEAT T1 (sentiment, flowers vs. insects) SemQ <C> WEAT T1 (sentiment, flowers vs. insects) SemQ <R> <C> Model <C> Model <C> WEAT <C> ECT <C> BAT <C> KM <C> SVM <C> SL <C> WS <C> WEAT <C> ECT <C> BAT <C> KM <C> SVM <C> SL <C> WS <R> <C> [BOLD] FT <C> Distributional <C> 1.30 <C> 73.5 <C> 41.0 <C> 100 <C> 100 <C> 38.2 <C> 73.8 <C> 1.67 <C> 46.2 <C> 56.1 <C> 95.7 <C> 100 <C> 38.2 <C> 73.0 <R> <C> [EMPTY] <C> GBDD <C> 0.96 <C> 84.7 <C> 33.9 <C> 62.9 <C> [BOLD] 50.0 <C> [BOLD] 38.4 <C> [BOLD] 73.8 <C> 0.08* <C> 96.2 <C> 41.7 <C> 56.0 <C> 53.1 <C> 38.1 <C> 72.9 <R> <C> [EMPTY] <C> BAM <C> 0.10* <C> 71.8 <C> 38.4 <C> 99.8 <C> 100 <C> 37.7 <C> 70.4 <C> 1.57 <C> 50.3 <C> 56.0 <C> 95.7 <C> 100 <C> 37.4 <C> 71.5 <R> <C> [EMPTY] <C> DBN <C> [BOLD] 0.05* <C> 79.1 <C> [BOLD] 33.6 <C> 99.8 <C> 100 <C> 34.1 <C> 65.1 <C> 0.18* <C> 79.8 <C> 45 <C> 95.7 <C> 100 <C> 35.09 <C> 68.6 <R> <C> [EMPTY] <C> GBDD ∘ BAM <C> 0.18* <C> [BOLD] 94.4 <C> 38.7 <C> 65.1 <C> 65.3 <C> 37.7 <C> 70.2 <C> 0.42* <C> 89.3 <C> 48.1 <C> 75.0 <C> 91.4 <C> 37.3 <C> 71.3 <R> <C> [EMPTY] <C> BAM ∘ GBDD <C> 0.57* <C> 90.3 <C> 34.6 <C> [BOLD] 60.1 <C> [BOLD] 50.0 <C> 36.4 <C> 72.6 <C> [BOLD] 0.07* <C> 94.4 <C> 42.4 <C> 56.9 <C> [BOLD] 51.3 <C> 37.9 <C> 68.4 <R> <C> [EMPTY] <C> GBDD ∘ DBN <C> 0.11* <C> 81.5 <C> 37.4 <C> 65.8 <C> 50.3 <C> 33.9 <C> 64.6 <C> -0.08* <C> 95.9 <C> [BOLD] 41.9 <C> [BOLD] 54.6 <C> [BOLD] 52.0 <C> 34.9 <C> 68.4 <R> <C> [BOLD] CBOW <C> Distributional <C> 0.81* <C> -24.0 <C> 45.6 <C> 90.6 <C> 93.4 <C> 34.7 <C> 59.4 <C> 1.13 <C> 78.1 <C> 50.2 <C> 62.6 <C> 93.9 <C> [BOLD] 34.7 <C> [BOLD] 59.4 <R> <C> [EMPTY] <C> GBDD <C> 0.38* <C> 50.9 <C> 43.4 <C> 59.5 <C> [BOLD] 50.0 <C> [BOLD] 34.8 <C> [BOLD] 59.8 <C> -0.07* <C> 90.7 <C> [BOLD] 41.1 <C> 55.7 <C> 51.9 <C> [BOLD] 34.7 <C> [BOLD] 59.4 <R> <C> [EMPTY] <C> BAM <C> 0.14* <C> 36.8 <C> 51.1 <C> 95.1 <C> 89.4 <C> 33.4 <C> 59.2 <C> 0.44* <C> 82.4 <C> 50.7 <C> 60.9 <C> 94.4 <C> 34.4 <C> 59.3 <R> <C> [EMPTY] <C> DBN <C> 0.45* <C> 4.7 <C> 57.5 <C> 97.4 <C> 98.4 <C> 33.9 <C> 52.2 <C> 0.60 <C> 82.5 <C> 46 <C> 85.7 <C> 90.8 <C> 33.4 <C> 53.4 <R> <C> [EMPTY] <C> GBDD ∘ BAM <C> [BOLD] 0.00* <C> [BOLD] 69.4 <C> 50.3 <C> [BOLD] 52.7 <C> 68.8 <C> 33.4 <C> 59.3 <C> [BOLD] -0.04* <C> [BOLD] 91.3 <C> 48.7 <C> 60.7 <C> 68.1 <C> 34.5 <C> 59.2 <R> <C> [EMPTY] <C> BAM ∘ GBDD <C> 0.09* <C> 65.6 <C> 42.7 <C> 62.6 <C> [BOLD] 50.0 <C> 33.2 <C> 56.9 <C> -0.17* <C> 89.2 <C> 45.3 <C> 55.6 <C> [BOLD] 51.1 <C> 33.2 <C> 57 <R> <C> [EMPTY] <C> GBDD ∘ DBN <C> 0.38* <C> -3.5 <C> 57.6 <C> 61.9 <C> 50.3 <C> 34.0 <C> 52.1 <C> -0.15* <C> 90.5 <C> 41.3 <C> [BOLD] 55.4 <C> 52.6 <C> 33.4 <C> 53.3 <R> <C> [BOLD] GloVe <C> Distributional <C> 1.28 <C> 84.1 <C> 36.3 <C> 100 <C> 100 <C> [BOLD] 36.9 <C> [BOLD] 60.5 <C> 1.38 <C> 76.2 <C> 40.5 <C> 94.1 <C> 100 <C> [BOLD] 36.9 <C> 60.5 <R> <C> [EMPTY] <C> GBDD <C> 0.95 <C> 89.7 <C> 29.1 <C> 57.4 <C> 50.6 <C> [BOLD] 36.9 <C> 59.6 <C> 0.44* <C> [BOLD] 92.4 <C> 32.7 <C> 55.6 <C> 54.5 <C> 36.8 <C> [BOLD] 60.7 <R> <C> [EMPTY] <C> BAM <C> 1.08 <C> 89.7 <C> 27.8 <C> 96 <C> 100 <C> 36.2 <C> 59.5 <C> 0.96 <C> 82.1 <C> 39.2 <C> 90.7 <C> 100 <C> 34.4 <C> 56.4 <R> <C> [EMPTY] <C> DBN <C> 0.83* <C> 81.5 <C> 30.8 <C> 100 <C> 100 <C> 35.9 <C> 58.6 <C> 0.55 <C> 77.6 <C> 34.8 <C> 95.3 <C> 100 <C> 36.7 <C> 59.1 <R> <C> [EMPTY] <C> GBDD ∘ BAM <C> 0.98 <C> 94.7 <C> [BOLD] 25.8 <C> 63.6 <C> 79.1 <C> 36.6 <C> 59.3 <C> 0.40* <C> 90.7 <C> 36.5 <C> 57.7 <C> 76.5 <C> 34.2 <C> 56.4 <R> <C> [EMPTY] <C> BAM ∘ GBDD <C> 0.78* <C> 97.1 <C> 36.9 <C> [BOLD] 53.9 <C> [BOLD] 50.0 <C> 36.3 <C> 59.2 <C> 0.65 <C> 87.3 <C> 44.1 <C> [BOLD] 55.5 <C> [BOLD] 51.2 <C> 35.5 <C> 58.6 <R> <C> [EMPTY] <C> GBDD ∘ DBN <C> [BOLD] 0.51* <C> [BOLD] 97.4 <C> 28.2 <C> 59.5 <C> [BOLD] 50.0 <C> 35.8 <C> 58.4 <C> [BOLD] -0.03* <C> 89.7 <C> [BOLD] 30.3 <C> 57.4 <C> 52.1 <C> 36.5 <C> 59.1 <CAP> Table 2: Main results on two bias test sets, WEAT T8 and T1 for three en distributional spaces debiased with three models – GBDD, BAM, and DebiasNet (DBN) – and their compositions. We quantify the explicit bias (Explicit): WEAT, ECT, and BAT evaluation measures; implicit bias (Implicit): clustering with KMeans++ (KM) and classification with SVM; and the preservation of semantic quality (SemQ): word similarity scores on SimLex (SL) and WordSim-353 (WS). Asterisks (*) indicate insignificant (α=0.05) bias effects for the WEAT evaluation measure.
<R> <C> Test set <C> Embeddings <C> Spearman <C> Pearson <R> <C> WordSim-353 <C> GloVe vectors reported in (Pennington et al.,  2014 ) <C> 0.658 <C> [EMPTY] <R> <C> WordSim-353 <C> GloVe embedding  [ITALIC] V∗\small{GloVe} <C> 0.601 <C> 0.603 <R> <C> WordSim-353 <C> GloVe embedding  [ITALIC] V∗\small{GloVe} with Equation  6  imposed <C> 0.641 <C> 0.637 <R> <C> WordSim-353 <C> [ITALIC] V=Λ [ITALIC] V∗\small{GloVe} optimized over Λ=diag( [ITALIC] λ1,..., [ITALIC] λd) <C> 0.679 <C> 0.760 <R> <C> WordSim-353 <C> word2vec embedding  [ITALIC] V∗\small{w2v} <C> 0.700 <C> 0.652 <R> <C> [EMPTY] <C> word2vec embedding  [ITALIC] V∗\small{w2v} with Equation  6  imposed <C> 0.645 <C> 0.588 <R> <C> [EMPTY] <C> [ITALIC] V=Λ [ITALIC] V∗\small{w2v} optimized over Λ=diag( [ITALIC] λ1,..., [ITALIC] λd) <C> 0.797 <C> 0.838 <R> <C> SimLex-999 <C> GloVe embedding  [ITALIC] V∗\small{GloVe} <C> 0.371 <C> 0.389 <R> <C> SimLex-999 <C> GloVe embedding  [ITALIC] V∗\small{GloVe} with Equation  6  imposed <C> 0.402 <C> 0.421 <R> <C> SimLex-999 <C> [ITALIC] V=Λ [ITALIC] V∗\small{GloVe} optimized over Λ=diag( [ITALIC] λ1,..., [ITALIC] λd) <C> 0.560 <C> 0.582 <R> <C> SimLex-999 <C> word2vec embedding  [ITALIC] V∗\small{w2v} <C> 0.441 <C> 0.453 <R> <C> [EMPTY] <C> word2vec embedding  [ITALIC] V∗\small{w2v} with Equation  6  imposed <C> 0.475 <C> 0.480 <R> <C> [EMPTY] <C> [ITALIC] V=Λ [ITALIC] V∗\small{w2v} optimized over Λ=diag( [ITALIC] λ1,..., [ITALIC] λd) <C> 0.583 <C> 0.617 <CAP> Table 1: Evaluation task scores g(D,V) corresponding to WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) test sets. The base GloVe embedding V∗ is as described in the caption of Figure 3; the word2vec embedding is as described in the caption of Figure 4.
<R> <C> Model <C> Sent2Vec uni. <C> Sent2Vec uni.+bi. <C> Sent2Vec uni.+bi+tri. <C> CBOW (char.) <C> CBOW (char.)+bi. <C> CBOW (char.)+bi.+tri. <C> CBOW <C> Skip-gram (char.) <C> Skip-gram <R> <C> Embedding Dimensions <C> 300 <C> 300 <C> 300 <C> 300 <C> 300 <C> 300 <C> 300 <C> 300 <C> 300 <R> <C> Max Vocab. Size <C> 750k <C> 750k <C> 750k <C> 750k <C> 750k <C> 750k <C> 750k <C> 750k <C> 750k <R> <C> Minimum Word Count <C> 5 <C> 5 <C> 5 <C> 5 <C> 5 <C> 5 <C> 5 <C> 5 <C> 5 <R> <C> Initial Learning Rate <C> 0.2 <C> 0.2 <C> 0.2 <C> 0.05 <C> 0.05 <C> 0.05 <C> 0.05 <C> 0.05 <C> 0.05 <R> <C> Epochs <C> 9 <C> 9 <C> 9 <C> 9 <C> 9 <C> 9 <C> 5 <C> 15 <C> 15 <R> <C> Subsampling hyper-param. <C> 1×10−5 <C> 5×10−5 <C> 5×10−6 <C> 1×10−4 <C> 1×10−4 <C> 1×10−4 <C> 1×10−4 <C> 1×10−4 <C> 1×10−4 <R> <C> Word-Ngrams Bucket Size <C> - <C> 2M <C> 4M <C> - <C> 2M <C> 4M <C> - <C> - <C> - <R> <C> Char-Ngrams Bucket Size <C> - <C> - <C> - <C> 2M <C> 2M <C> 2M <C> - <C> 2M <C> - <R> <C> Word-Ngrams Dropped per context <C> - <C> 4 <C> 4 <C> - <C> 2 <C> 2 <C> - <C> - <C> - <R> <C> Window Size <C> - <C> - <C> - <C> 10 <C> 10 <C> 10 <C> 10 <C> 5 <C> 5 <R> <C> Number of negatives sampled <C> 10 <C> 10 <C> 10 <C> 5 <C> 5 <C> 5 <C> 5 <C> 5 <C> 5 <R> <C> Max Char-Ngram Size <C> - <C> - <C> - <C> 6 <C> 6 <C> 6 <C> - <C> 6 <C> - <R> <C> Min Char-Ngram Size <C> - <C> - <C> - <C> 3 <C> 3 <C> 3 <C> - <C> 3 <C> - <R> <C> Percentage at which training is halted (For CBOW models only) <C> - <C> - <C> - <C> 75% <C> 80% <C> 80% <C> 60% <C> - <C> - <CAP> Table 3: Training parameters for all non-GloVe models
<R> <C> Model <C> WikiSQL (SQL) BLEU-4 <C> WikiSQL (SQL) ROUGE-2 <C> WikiSQL (SQL) ROUGE-L <C> ATIS (lambda-calculus) BLEU-4 <C> ATIS (lambda-calculus) ROUGE-2 <C> ATIS (lambda-calculus) ROUGE-L <C> CoNaLa (Python) BLEU-4 <C> CoNaLa (Python) ROUGE-2 <C> CoNaLa (Python) ROUGE-L <R> <C> Code-NN <C> 6.7 <C> 9.7 <C> 30.9 <C> 37.1 <C> 43.28 <C> 59.4 <C> 8.1 <C> 12.2 <C> 26.1 <R> <C> P-G <C> 25.7 <C> 29.2 <C> 50.1 <C> 41.9 <C> 47.3 <C> 60.5 <C> 10.0 <C> 13.8 <C> 28.0 <R> <C> Tree2Seq <C> 22.0 <C> 22.0 <C> 43.4 <C> 40.1 <C> 47.2 <C> 60.9 <C> 6.6 <C> 9.2 <C> 25.2 <R> <C> Graph2Seq <C> 17.6 <C> 24.3 <C> 45.7 <C> 34.6 <C> 41.8 <C> 58.3 <C> 10.4 <C> 14.1 <C> 28.2 <R> <C> T2S+CP <C> 31.0 <C> 36.8 <C> 54.5 <C> 39.0 <C> 43.7 <C> 58.4 <C> 13.3 <C> 18.5 <C> 31.5 <R> <C> [BOLD] TAG(B) <C> [BOLD] 35.8 <C> 41.0 <C> 57.8 <C> [BOLD] 42.4 <C> [BOLD] 47.4 <C> 61.2 <C> [BOLD] 14.1 <C> 19.4 <C> 31.8 <R> <C> [BOLD] TAG(R) <C> 35.2 <C> [BOLD] 41.1 <C> [BOLD] 58.1 <C> 40.6 <C> 47.1 <C> [BOLD] 61.5 <C> 12.6 <C> [BOLD] 19.7 <C> [BOLD] 32.2 <CAP> Table 2: Comparisons with baseline models on different test sets.
<R> <C> System <C> Data <C> BUS <C> CAF <C> PED <C> STR <C> Avg. <R> <C> SI <C> Real <C> 24.77 <C> 16.12 <C> 13.39 <C> 17.27 <C> 17.84 <R> <C> SI <C> Simu <C> 18.07 <C> 21.44 <C> 14.68 <C> 16.70 <C> 17.72 <R> <C> SIT <C> Real <C> 22.91 <C> 15.63 <C> 12.77 <C> 16.66 <C> [BOLD] 16.95 <R> <C> SIT <C> Simu <C> 16.64 <C> 20.23 <C> 13.53 <C> 15.96 <C> [BOLD] 16.54 <CAP> Table 1: The ASR WER (%) performance of SI and SIT DNN acoustic models on real and simulated development set of CHiME-3.
<R> <C> System <C> BUS <C> CAF <C> PED <C> STR <C> Avg. <R> <C> SA SI <C> 22.76 <C> 15.56 <C> 11.52 <C> 15.37 <C> 16.25 <R> <C> SA SIT <C> 21.42 <C> 14.79 <C> 11.11 <C> 14.70 <C> [BOLD] 15.46 <CAP> Table 2: The ASR WER (%) performance of SA SI and SA SIT DNN acoustic models after CRT unsupervised speaker adaptation on real development set of CHiME-3.
<R> <C> [BOLD] Model <C> [BOLD] Cycle <C> [BOLD] Translate <R> <C> [ITALIC] SSNT0 <C> 43.35 <C> 37.42 <R> <C> [ITALIC] SSNT-CIP1 <C> [BOLD] 47.34 <C> [BOLD] 38.29 <CAP> Table 1: BLEU score on cycle-consistency and translation for WMT, across baseline and informative models. Greedy unrolling and α=0.1
<R> <C> Dataset <C> 1-gram <C> 2-gram <C> 3-gram <C> 4-gram <R> <C> FEVER <C> 40874 <C> 179525 <C> 315025 <C> 387093 <R> <C> Fakeddit <C> 61141 <C> 507512 <C> 767281 <C> 755929 <CAP> Table 3: Unique n-grams for FEVER and Fakeddit for equal sample size (FEVER’s total dataset size).
<R> <C> [EMPTY] <C> Stan-ford <C> CMU <C> ELMO <C> FLAIR <C> BERT <R> <C> ALL-O <C> 88.13 <C> 89.78 <C> 92.39 <C> 92.83 <C> 91.62 <R> <C> ALL-C <C> 88.73 <C> 90.39 <C> 93.21 <C> [BOLD] 93.79 <C> 92.33 <R> <C> PER-O <C> 93.31 <C> 95.74 <C> 97.07 <C> 97.49 <C> 96.14 <R> <C> PER-C <C> 93.94 <C> 96.49 <C> 97.81 <C> [BOLD] 98.08 <C> 96.88 <R> <C> ORG-O <C> 84.23 <C> 86.90 <C> 90.68 <C> 91.34 <C> 90.61 <R> <C> ORG-C <C> 84.89 <C> 87.53 <C> 91.61 <C> [BOLD] 92.64 <C> 91.44 <R> <C> LOC-O <C> 90.83 <C> 92.02 <C> 93.87 <C> 94.01 <C> 92.85 <R> <C> LOC-C <C> 91.58 <C> 92.62 <C> [BOLD] 94.92 <C> 94.72 <C> 93.59 <R> <C> MISC-O <C> 79.10 <C> 77.31 <C> 82.31 <C> 82.89 <C> 80.81 <R> <C> MISC-C <C> 79.37 <C> 77.58 <C> 82.47 <C> [BOLD] 84.40 <C> 81.10 <CAP> Table 4: Results for selected models on the original (designated as ending ’…-O’) and re-annotated / corrected (’…-C’) CoNLL 2003 test set concerning NE classes (ALL comprise PER, ORG, LOC, MISC). The given metric is a multilabel-F1 score (percentages).
<R> <C> [BOLD] Method <C> [ITALIC] σ <C> [BOLD] thresh <C> [BOLD] # Clusters <C> [BOLD] Error ↓ <C> [BOLD] Purity ↑ <C> [BOLD] Entropy ↓ <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> ( [ITALIC] NNE+ [ITALIC] NDC)| [ITALIC] V| <C> [EMPTY] <C> [EMPTY] <R> <C> Word2Vec <C> 0.2 <C> 0.04 <C> 750 <C> 0.716 <C> 0.88 <C> 0.14 <R> <C> Word2Vec + Roget <C> 0.7 <C> 0.04 <C> 750 <C> 0.033 <C> 0.94 <C> 0.09 <R> <C> Eigenword <C> 2.0 <C> 0.07 <C> 200 <C> 0.655 <C> 0.84 <C> 0.25 <R> <C> Eigenword + MSW <C> 1.0 <C> 0.08 <C> 200 <C> 0.042 <C> 0.95 <C> 0.01 <R> <C> GloCon <C> 3.0 <C> 0.09 <C> 100 <C> 0.691 <C> 0.98 <C> 0.03 <R> <C> GloCon + Roget <C> 0.9 <C> 0.06 <C> 750 <C> 0.048 <C> 0.94 <C> 0.02 <R> <C> Glove <C> 9.0 <C> 0.09 <C> 200 <C> 0.657 <C> 0.72 <C> 0.33 <R> <C> Glove + Roget <C> 11.0 <C> 0.01 <C> 1000 <C> 0.070 <C> 0.91 <C> 0.10 <CAP> Table 2: Clustering evaluation after parameter optimization minimizing error using grid search.
<R> <C> [BOLD] Method <C> [BOLD] Accuracy <C> [BOLD] Coverage <R> <C> MS Thes Lookup <C> 0.70 <C> 0.57 <R> <C> Roget Thes Lookup <C> 0.63 <C> 0.99 <R> <C> WordNet Thes Lookup <C> 0.43 <C> 1.00 <R> <C> Combined Thes Lookup <C> 0.90 <C> 1.00 <R> <C> Word2Vec <C> 0.36 <C> 1.00 <R> <C> Word2Vec+CombThes <C> 0.67 <C> 1.00 <R> <C> Eigenwords <C> 0.23 <C> 1.00 <R> <C> Eigenwords+CombThes <C> 0.12 <C> 1.00 <R> <C> GloCon <C> 0.07 <C> 1.00 <R> <C> GloCon+CombThes <C> 0.05 <C> 1.00 <R> <C> GloVe <C> 0.33 <C> 1.00 <R> <C> GloVe+CombThes <C> 0.58 <C> 1.00 <R> <C> Thes Lookup+W2V+CombThes <C> 0.96 <C> 1.00 <CAP> Table 4: Clustering evaluation using SimLex-999 with 120 word pairs having similarity score over 8.
<R> <C> [BOLD] Model <C> F1 <C> UAS <C> LAS <R> <C> separate constituent <C> 93.98 <C> − <C> − <R> <C> converted dependency <C> 93.98 <C> 95.38 <C> 94.06 <R> <C> separate dependency <C> − <C> 95.80 <C> 94.40 <R> <C> joint span  [ITALIC] λH = 1.0 <C> 93.89 <C> − <C> − <R> <C> joint span  [ITALIC] λH = 0.0 <C> − <C> 95.90 <C> 94.50 <R> <C> joint span  [ITALIC] λH = 0.8 <C> [BOLD] 93.98 <C> [BOLD] 95.99 <C> 94.53 <R> <C> converted dependency <C> [BOLD] 93.98 <C> 95.70 <C> [BOLD] 94.60 <CAP> Table 1: PTB dev set performance of joint span syntactic parsing. The converted means the corresponding dependency syntactic parsing results are from the corresponding constituent parse tree using head rules.
<R> <C> Name <C> PSD <C> Training Speed Seq./GPU <C> Training Speed fr./s. <C> WER (%) swbd <C> WER (%) callhm <R> <C> Mod. CTC <C> × <C> 5 <C> 1027 <C> 32.0 <C> 42.5 <R> <C> Mod. CTC <C> √ <C> [BOLD] 30 <C> [BOLD] 5851 <C> [BOLD] 24.9 <C> [BOLD] 36.5 <CAP> Table 3: Performance and Speed Comparison with or w/o PSD.
<R> <C> Morphological Processing <C> Morphological Processing MAP <C> Morphological Processing %M <C> Morphological Processing P@5 <C> Morphological Processing P@10 <R> <C> Mono <C> 0.384 <C> [EMPTY] <C> 0.640 <C> 0.605 <R> <C> SPLIT:2G <C> 0.223 <C> 58.2 <C> 0.362 <C> 0.363 <R> <C> STEM:2G <C> 0.247 <C> 65.0 <C> 0.412 <C> 0.401 <R> <C> RAG:2G <C> 0.245 <C> 63.8 <C> 0.380 <C> 0.389 <R> <C> AG:2G <C> 0.268123 <C> 69.8 <C> 0.412 <C> 0.411 <R> <C> AG: ITD <C> [BOLD] 0.269123 <C> 70.0 <C> 0.432 <C> 0.420 <CAP> Table 3: SPLIT (5-gram truncation), STEM, RAG (Rule-based affix generation), and AG are the morphological processing-based methods. :2G and :ITD indicate disambiguation functions. 1/2/3 show that the MAP improvements over baselines are statistically significant.
<R> <C> Model <C> Acc. <C> [ITALIC] F1 <C> [ITALIC] P <C> [ITALIC] R <R> <C> Random guess <C> 61.8 <C> 25.4 <C> 26.0 <C> 25.0 <R> <C> Logistic regression w/ (uni+bi)-gram <C> 80.5 <C> 50.9 <C> 73.0 <C> 39.0 <R> <C> DNN w/ (uni+bi)-gram <C> 76.6 <C> 56.5 <C> 54.4 <C> 58.8 <R> <C> CNN w/o context <C> 77.8 <C> 57.8 <C> 56.8 <C> 58.9 <R> <C> RNN w/o context <C> 83.3 <C> 63.9 <C> 72.5 <C> 57.1 <R> <C> RNN w/ context (non-hierarchical) <C> 83.7 <C> 65.7 <C> 72.6 <C> 60.0 <R> <C> RNN w/ context (hierarchical) <C> 85.1 <C> 69.2 <C> 74.6 <C> 64.6 <R> <C> + static attention <C> [BOLD] 89.2 <C> [BOLD] 78.4 <C> [BOLD] 81.5 <C> [BOLD] 75.6 <CAP> Table 1. Model performance (in %). Here, LSTM units are used in RNN, but omitted in the table for brevity. The context size is 8, chosen by validation (deferred to Figure 2).
<R> <C> Method <C> MR <R> <C> VLAWE ( [ITALIC] k=2) <C> 93.0 <R> <C> VALWE (PCA) <C> 93.2 <R> <C> VLAWE (full,  [ITALIC] k=10) <C> 93.3 <CAP> Table 2: Performance results (in %) of the full VLAWE representation (with k=10) versus two compact versions of VLAWE, obtained either by setting k=2 or by applying PCA.
<R> <C> [ITALIC]  [BOLD] Debatepedia extended data set  [BOLD] Topic <C> [ITALIC]  [BOLD] Debatepedia extended data set  [BOLD] #argum <C> [ITALIC]  [BOLD] Debatepedia extended data set  [BOLD] #pairs <R> <C> [ITALIC] Violent games/aggressiveness <C> 17 <C> 23 <R> <C> [ITALIC] China one-child policy <C> 11 <C> 14 <R> <C> [ITALIC] Consider coca as a narcotic <C> 17 <C> 22 <R> <C> [ITALIC] Child beauty contests <C> 13 <C> 17 <R> <C> [ITALIC] Arming Libyan rebels <C> 13 <C> 15 <R> <C> [ITALIC] Random alcohol breath tests <C> 11 <C> 14 <R> <C> [ITALIC] Osama death photo <C> 22 <C> 24 <R> <C> [ITALIC] Privatizing social security <C> 12 <C> 13 <R> <C> [ITALIC] Internet access as a right <C> 15 <C> 17 <R> <C> [ITALIC] Ground zero mosque <C> 11 <C> 12 <R> <C> [ITALIC] Mandatory military service <C> 15 <C> 17 <R> <C> [ITALIC] No fly zone over Libya <C> 18 <C> 19 <R> <C> [ITALIC] Airport security profiling <C> 12 <C> 13 <R> <C> [ITALIC] Solar energy <C> 18 <C> 19 <R> <C> [ITALIC] Natural gas vehicles <C> 16 <C> 17 <R> <C> [ITALIC] Use of cell phones/driving <C> 16 <C> 16 <R> <C> [ITALIC] Marijuana legalization <C> 23 <C> 25 <R> <C> [ITALIC] Gay marriage as a right <C> 10 <C> 10 <R> <C> [ITALIC] Vegetarianism <C> 14 <C> 13 <R> <C> [BOLD] TOTAL <C> 310 <C> [BOLD] 320 <CAP> Table 2: Debatepedia extended data set
<R> <C> Model <C> # params <C> % acc <C> AUC <R> <C> LDA (75 topics) <C> - <C> 63.2 <C> 0.684 <R> <C> LSA (300 topics) <C> - <C> 66.2 <C> - <R> <C> TF-IDF <C> - <C> 68.8 <C> 0.754 <R> <C> 2-gram TF-IDF <C> - <C> 70.1 <C> 0.766 <R> <C> Unidir. RCNN <C> 0.6M <C> 79.0 <C> 0.870 <R> <C> Unidir. LSTM <C> 3.3M <C> 78.9 <C> 0.872 <R> <C> [BOLD] Bi-dir. RCNN <C> [BOLD] 1.7M <C> [BOLD] 79.4 <C> [BOLD] 0.872 <R> <C> [BOLD] Bi-dir. LSTM <C> [BOLD] 14.6M <C> [BOLD] 79.4 <C> [BOLD] 0.875 <CAP> Tabuľka 1: Comparison of test set performance of multiple classification models. Baseline models (first four) use various text representations that are classified by boosted decision trees (1,000 trees) Freund and Schapire (1995).
<R> <C> Approach <C> Metrics BLEU-2 <C> Metrics RHYTHM <R> <C> [ITALIC] baseline <C> 26.726 <C> 0.824 <R> <C> [ITALIC] AS2S <C> 27.458 <C> 0.866 <R> <C> [ITALIC] HieAS2S-tile <C> [BOLD] 29.991 <C> [BOLD] 0.892 <R> <C> [ITALIC] HieAS2S-concat <C> 28.171 <C> 0.876 <R> <C> [ITALIC] Positive-groundtruth <C> 29.095 <C> 1.000 <R> <C> [ITALIC] Negative-groundtruth <C> 12.062 <C> 1.000 <CAP> TABLE I: The BLEU-2 scores and RHYTHM scores of different approaches
<R> <C> Method <C> Poeticness <C> Fluency <C> Meaning <C> Coherence <C> Overall <R> <C> AS2S <C> 3.62 <C> 3.07 <C> 2.73 <C> 3.12 <C> 3.13 <R> <C> Ours <C> 3.87 <C> 3.24 <C> 2.85 <C> 3.24 <C> 3.30 <R> <C> Human-written <C> 4.07 <C> 3.43 <C> 3.58 <C> 3.71 <C> 3.69 <CAP> TABLE II: The results of human evaluation
<R> <C> [EMPTY] <C> GloVe <R> <C> Original <C> 0.797 <R> <C> Attract-Repel <C> 0.817 <R> <C> DFFN <C> 0.829 <R> <C> AuxGAN <C> 0.836 <R> <C> WGAN-postspec <C> 0.838 <CAP> Table 4: DST results for English.
<R> <C> [BOLD] lan- <C> [EMPTY] <C> [BOLD] proxy <C> [BOLD] proxy <C> [BOLD] ge- <C> [BOLD] lan-  [BOLD] guage- <R> <C> [BOLD] guage <C> [ITALIC] m <C> [BOLD] worst <C> [BOLD] best <C> [BOLD] neric <C> [BOLD] specific <R> <C> cs <C> 4 <C> 81.6 <C> [BOLD] 82.5 <C> [BOLD] 82.5 <C> [BOLD] 82.5 <R> <C> en <C> 4 <C> 76.4 <C> [BOLD] 82.9 <C> 80.7† <C> 81.7† <R> <C> es <C> 2 <C> 76.1 <C> [BOLD] 80.3 <C> [BOLD] 80.3 <C> – <R> <C> fi <C> 2 <C> 52.5 <C> [BOLD] 80.6 <C> 80.5 <C> – <R> <C> fr <C> 4 <C> 74.9 <C> [BOLD] 78.6 <C> [BOLD] 78.6 <C> [BOLD] 78.6 <R> <C> it <C> 3 <C> 84.4 <C> [BOLD] 85.5 <C> [BOLD] 85.5 <C> – <R> <C> ko <C> 2 <C> 35.5 <C> 43.9 <C> [BOLD] 44.0 <C> – <R> <C> pt <C> 2 <C> 74.6 <C> 77.4 <C> [BOLD] 77.6 <C> – <R> <C> ru <C> 3 <C> 82.6 <C> [BOLD] 83.7 <C> 82.9 <C> – <R> <C> sv <C> 2 <C> 73.7 <C> [BOLD] 74.7 <C> [BOLD] 74.7 <C> – <CAP> Table 3: PUD Test Set Results: Statistically significant differences between proxy-best and our best method are marked with †
<R> <C> \em match\/( [ITALIC] q, [ITALIC] q′) <C> BNDS MRR <C> BNDS PARTIAL <C> NEWS MRR <C> NEWS PARTIAL <R> <C> instantiation <C> MRR <C> MRR <C> MRR <C> MRR <R> <C> STR <C> 0.028 <C> 0.374 <C> 0.226 <C> 0.355 <R> <C> BOW <C> 0.031 <C> 0.442 <C> 0.243 <C> 0.457 <R> <C> SEM <C> 0.081 <C> 0.589 <C> 0.256 <C> 0.491 <CAP> Table 1: Predictiveness
<R> <C> Query Budget <C> Acc. (Imp.) <R> <C> 0 (Pure Weak Supervision) <C> 75.6 <R> <C> 100 (0.2%) <C> 76.3(+0.7) <R> <C> 200 (0.4%) <C> 76.7(+1.1) <R> <C> 500 (0.9%) <C> 77.7(+2.1) <R> <C> 1,000 (1.8%) <C> 78.6(+3.0) <R> <C> 2,500 (4.4%) <C> 79.0(+3.4) <R> <C> 10,000 (17.7%) <C> 79.8(+4.2) <R> <C> 56,355(All) <C> 80.3 <R> <C> [BOLD] Previous Weak Supervision Methods <C> Acc. <R> <C> NSM+MML (liang2016neural) <C> 70.7 <R> <C> NSM+MAPO (liang2016neural; liang2018memory) <C> 72.6 <R> <C> [BOLD] Previous Full Supervision Methods <C> Acc. <R> <C> STAMP (sun2018semantic) <C> 74.4 <R> <C> TranX+AP (yin2018tranx) <C> 78.6 <R> <C> Coarse2Fine (dong2018coarse) <C> 78.5 <R> <C> TypeSQL+TC (yu2018typesql) <C> 82.6 <CAP> Table 3: Wassp with varying budget and previous fully- or weakly-supervised methods on WikiSQL. All methods use table contents during training and are evaluated on the test set.
<R> <C> [EMPTY] <C> F1 <R> <C> GraphFlow (2-His) <C> [BOLD] 78.3 <R> <C> – PreQues <C> 78.2 <R> <C> – PreAns <C> 77.7 <R> <C> – PreAnsLoc <C> 76.6 <R> <C> – BERT <C> 70.2 <R> <C> – GF <C> 68.8 <R> <C> – TempConn <C> 69.9 <R> <C> GraphFlow (1-His) <C> 78.2 <R> <C> GraphFlow (0-His) <C> 76.7 <CAP> Table 3: Ablation study: model performance (in %) on the CoQA dev. set.
<R> <C> test-time inputs <C> Trained on R <C> Trained on R+1 <C> Trained on R+L <C> Trained on R+L+S <R> <C> reference <C> 24.7 <C> 24.3 <C> 24.8 <C> 24.4 <R> <C> oracle <C> 15.8 <C> 16.8 <C> 16.3 <C> 15.9 <R> <C> 1-best <C> 11.8 <C> 13.3 <C> 12.4 <C> 12.0 <R> <C> lattice <C> 9.3 <C> 7.1 <C> [BOLD] 13.7 <C> [BOLD] 14.1 <CAP> Table 5: BLEU scores on Callhome/Evltest (1 reference). All models are pre-trained on Fisher/Train references (R), and potentially fine-tuned on Callhome/Train. The best result using 1-best or lattice inputs is in bold. Statistically significant improvement over 1-best/R+1 is in bold.
<R> <C> preference (%) TPCW-GST <C> preference (%) neutral <C> preference (%) TPSE-GST <C> p-value 3-point <C> p-value 7-point <R> <C> 25.1% <C> 45.4% <C> 29.4% <C> 0.063 <C> 0.054 <CAP> Table 2: Subjective preference (%) of 8 raters on 260 audiobook phrases for TPCW-GST vs TPSE-GST. t-test p-values are given for both a 3-point and 7-point rating system.
<R> <C> model <C> accuracy <R> <C> U-model <C> 82.03 <R> <C> I-model <C> 81.25 <R> <C> FSM <C> 81.25 <R> <C> RB <C> 32.81 <CAP> Table 1: Accuracy measure of each model
<R> <C> Iter <C> BiLSTM+Softmax P <C> BiLSTM+Softmax R <C> BiLSTM+Softmax F1 <C> BiLSTM+CRF P <C> BiLSTM+CRF R <C> BiLSTM+CRF F1 <R> <C> 0 <C> 67.60 <C> 79.14 <C> 72.91 <C> 84.14 <C> 66.49 <C> 74.28 <R> <C> 1 <C> 68.47 <C> 85.61 <C> 76.08 <C> 67.94 <C> 86.77 <C> 76.21 <R> <C> 2 <C> 68.92 <C> 86.54 <C> 76.73 <C> 68.73 <C> 88.59 <C> 77.41 <R> <C> 3 <C> 68.86 <C> 87.78 <C> 77.18 <C> 68.69 <C> 88.91 <C> 77.51 <R> <C> 4 <C> 69.13 <C> 88.11 <C> 77.47 <C> 70.26 <C> 88.18 <C> 78.21 <R> <C> 5 <C> 69.13 <C> 88.00 <C> 77.43 <C> 69.48 <C> 88.78 <C> 77.95 <R> <C> 6 <C> 68.91 <C> 88.59 <C> 77.52 <C> 70.09 <C> 88.79 <C> 78.34 <R> <C> 7 <C> 68.44 <C> 88.38 <C> 77.15 <C> 70.35 <C> 89.63 <C> 78.83 <R> <C> 8 <C> 68.26 <C> 89.29 <C> 77.37 <C> 69.73 <C> 89.41 <C> 78.36 <R> <C> 9 <C> 68.01 <C> 89.02 <C> 77.11 <C> 69.30 <C> 89.88 <C> 78.26 <R> <C> 10 <C> 68.37 <C> 89.66 <C> [BOLD] 77.58 <C> 72.29 <C> 88.92 <C> [BOLD] 79.75 <CAP> Table 2: Performance of iterative refinement (E5, E6).
<R> <C> Task Method <C> Sentence Retrieval R@1 <C> Sentence Retrieval R@5 <C> Sentence Retrieval R@10 <C> Sentence Retrieval Med  [ITALIC] r <C> Image Retrieval R@1 <C> Image Retrieval R@5 <C> Image Retrieval R@10 <C> Image Retrieval Med  [ITALIC] r <C> 1K test images <R> <C> Random Ranking <C> 0.1 <C> 0.6 <C> 1.1 <C> 631 <C> 0.1 <C> 0.5 <C> 1.0 <C> 500 <C> 1K test images <R> <C> STV ( 2015 ) <C> 33.8 <C> 67.7 <C> 82.1 <C> 3 <C> 25.9 <C> 60.0 <C> 74.6 <C> 4 <C> 1K test images <R> <C> DVSA ( 2015 ) <C> 38.4 <C> 69.9 <C> 80.5 <C> 1 <C> 27.4 <C> 60.2 <C> 74.8 <C> 3 <C> 1K test images <R> <C> GMM-FV ( 2015 ) <C> 39.0 <C> 67.0 <C> 80.3 <C> 3 <C> 24.2 <C> 59.3 <C> 76.0 <C> 4 <C> 1K test images <R> <C> MM-ENS ( 2015 ) <C> 39.4 <C> 67.9 <C> 80.9 <C> 2 <C> 25.1 <C> 59.8 <C> 76.6 <C> 4 <C> 1K test images <R> <C> m-RNN ( 2014 ) <C> 41.0 <C> 73.0 <C> 83.5 <C> 2 <C> 29.0 <C> 42.2 <C> 77.0 <C> 3 <C> 1K test images <R> <C> m-CNN ( 2015 ) <C> 42.8 <C> 73.1 <C> 84.1 <C> 2 <C> 32.6 <C> 68.6 <C> 82.8 <C> 3 <C> 1K test images <R> <C> HM-LSTM ( 2017 ) <C> 43.9 <C> - <C> 87.8 <C> 2 <C> 36.1 <C> - <C> 86.7 <C> 3 <C> 1K test images <R> <C> SPE ( 2016 ) <C> 50.1 <C> 79.7 <C> 89.2 <C> - <C> 39.6 <C> 75.2 <C> 86.9 <C> - <C> 1K test images <R> <C> VQA-A ( 2016 ) <C> 50.5 <C> 80.1 <C> 89.7 <C> - <C> 37.0 <C> 70.9 <C> 82.9 <C> - <C> 1K test images <R> <C> 2WayNet ( 2017 ) <C> 55.8 <C> 75.2 <C> - <C> - <C> 39.7 <C> 63.3 <C> - <C> - <C> 1K test images <R> <C> sm-LSTM ( 2017 ) <C> 53.2 <C> 83.1 <C> 91.5 <C> 1 <C> 40.7 <C> 75.8 <C> 87.4 <C> 2 <C> 1K test images <R> <C> RRF-Net ( 2017 ) <C> 56.4 <C> 85.3 <C> 91.5 <C> - <C> 43.9 <C> 78.1 <C> 88.6 <C> - <C> 1K test images <R> <C> VSE++ ( 2017 ) <C> 64.6 <C> 90.0 <C> 95.7 <C> 1 <C> 52.0 <C> 84.3 <C> 92.0 <C> 1 <C> 1K test images <R> <C> SCAN ( 2018 ) <C> 72.7 <C> 94.8 <C> 98.4 <C> - <C> 58.8 <C> 88.4 <C> 94.8 <C> - <C> 1K test images <R> <C> Ours <C> 47.5 <C> 81.0 <C> 91.0 <C> 2 <C> 48.4 <C> 84.3 <C> 91.5 <C> 2 <C> [EMPTY] <R> <C> GMM-FV ( 2015 ) <C> 17.3 <C> 39.0 <C> 50.2 <C> 10 <C> 10.8 <C> 28.3 <C> 40.1 <C> 17 <C> 5K test images <R> <C> DVSA ( 2015 ) <C> 16.5 <C> 39.2 <C> 52.0 <C> 9 <C> 10.7 <C> 29.6 <C> 42.2 <C> 14 <C> 5K test images <R> <C> VQA-A ( 2016 ) <C> 23.5 <C> 50.7 <C> 63.6 <C> - <C> 16.7 <C> 40.5 <C> 53.8 <C> - <C> 5K test images <R> <C> VSE++ ( 2017 ) <C> 41.3 <C> 71.1 <C> 81.2 <C> 2 <C> 30.3 <C> 59.4 <C> 72.4 <C> 4 <C> 5K test images <R> <C> SCAN ( 2018 ) <C> 50.4 <C> 82.2 <C> 90.0 <C> - <C> 38.6 <C> 69.3 <C> 80.4 <C> - <C> 5K test images <R> <C> Ours <C> 23.8 <C> 53.7 <C> 67.3 <C> 4 <C> 25.6 <C> 55.1 <C> 68.4 <C> 3 <C> 5K test images <CAP> Table 1: Image and sentence retrieval results on MS-COCO. “Sentence Retrieval” denotes using an image as query to search for the relevant sentences, and “Image Retrieval” denotes using a sentence to ﬁnd the relevant image. R@K is Recall@K (high is good). Med r is the median rank (low is good).
<R> <C> Models <C> WER <C> RTF <C> Latency <R> <C> hybrid ASR <C> 10.9 <C> 0.15 <C> 385 ms <R> <C> att. transducer  [ITALIC] τ=2, [ITALIC] w=4 <C> 10.3 <C> 0.19 <C> 300 ms <CAP> Table 3: Results on 10 thousands hours of Liulishuo L2 English speech
<R> <C> [BOLD] Model <C> [BOLD] Avg <C> [BOLD] LID (Accuracy)  [BOLD] SPA-ENG <C> [BOLD] LID (Accuracy)  [BOLD] HIN-ENG <C> [BOLD] LID (Accuracy)  [BOLD] NEP-ENG <C> [BOLD] LID (Accuracy)  [BOLD] MSA-EA <C> [BOLD] POS (Accuracy)  [BOLD] SPA-ENG <C> [BOLD] POS (Accuracy)  [BOLD] HIN-ENG <C> [BOLD] NER (Micro F1)  [BOLD] SPA-ENG <C> [BOLD] NER (Micro F1)  [BOLD] HIN-ENG <C> [BOLD] NER (Micro F1)  [BOLD] MSA-EA <C> [BOLD] SA (Accuracy)  [BOLD] SPA-ENG <R> <C> BiLSTM <C> 73.20 <C> 94.16 <C> 92.34 <C> 93.29 <C> 74.26 <C> 94.80 <C> 81.84 <C> 44.92 <C> 48.36 <C> 62.64 <C> 45.39 <R> <C> ELMo <C> 78.64 <C> 98.12 <C> 96.21 <C> 96.19 <C> 80.54 <C> 96.30 <C> 88.42 <C> 53.80 <C> 65.83 <C> 61.00 <C> 49.97 <R> <C> ML-BERT <C> 82.93 <C> 98.53 <C> 96.44 <C> 96.57 <C> 84.14 <C> 97.00 <C> 89.28 <C> 63.56 <C> 75.96 <C> 67.61 <C> 60.20 <CAP> Table 5: Baseline results on the test set of the LinCE benchmark.
<R> <C> Category Exact match <C> Classifier 13 <C> Classifier (100.0%) <C> Neural net 13 <C> Neural net (100.0%) <R> <C> Paraphrasing <C> 32 <C> (78.1%) <C> 39 <C> (95.1%) <R> <C> Partial clue <C> 14 <C> (73.7%) <C> 17 <C> (89.5%) <R> <C> Multiple sentences <C> 1 <C> (50.0%) <C> 1 <C> (50.0%) <R> <C> Coreference errors <C> 4 <C> (50.0%) <C> 3 <C> (37.5%) <R> <C> Ambiguous / hard <C> 2 <C> (11.8%) <C> 1 <C> (5.9%) <R> <C> All <C> 66 <C> (66.0%) <C> 74 <C> (74.0%) <CAP> Table 6: The per-category performance of our two systems.
<R> <C> [ITALIC] approach <C> [ITALIC] methods <C> [ITALIC] evaluating age (EN) <C> [ITALIC] evaluating gender (EN) <C> [ITALIC] evaluating age (SP) <C> [ITALIC] evaluating gender (SP) <R> <C> Textual <C> T1: BoW (2k) <C> 0.394 <C> 0.741 <C> 0.481 <C> 0.601 <R> <C> Textual <C> T2: BoW (10k) <C> 0.409 <C> 0.755 <C> [BOLD] 0.505 <C> [BOLD] 0.703 <R> <C> Visual <C> V3: LL-CNN (all-imgs) <C> 0.349 <C> 0.526 <C> 0.481 <C> 0.524 <R> <C> Visual <C> V4: LL-CNN AVG (all-imgs) <C> 0.390 <C> 0.700 <C> 0.380 <C> 0.650 <R> <C> Multimodal <C> M3: T1+V4 <C> 0.414 <C> 0.775 <C> 0.451 <C> 0.685 <R> <C> Multimodal <C> M6: T2+V4 <C> [BOLD] 0.423 <C> [BOLD] 0.778 <C> 0.433 <C> 0.642 <CAP> Table 5: Comparison among methods based on textual, visual and multimodal approaches for performing author profiling task.
<R> <C> system sampled (∈) or generated <C> style intensity neural <C> style intensity ngram <C> style intensity count <C> relevancy BLEU1 <C> relevancy BLEU2 <C> relevancy BLEU3 <C> relevancy BLEU4 <C> diversity entropy4 <C> diversity distinct1 <C> diversity distinc2 <R> <C> target = arXiv <C> target = arXiv <C> target = arXiv <C> target = arXiv <C> target = arXiv <C> target = arXiv <C> target = arXiv <C> target = arXiv <C> target = arXiv <C> target = arXiv <C> target = arXiv <R> <C> Rand (∈Dstyle) <C> 0.99 <C> 0.85 <C> 1.00 <C> 12.1 <C> 1.7 <C> 0.6 <C> 0.34 <C> 9.4 <C> 0.13 <C> 0.56 <R> <C> Retrieval (∈Dstyle) <C> 0.84 <C> 0.77 <C> 0.59 <C> 15.5 <C> 2.3 <C> 0.8 <C> 0.49 <C> 7.6 <C> 0.06 <C> 0.19 <R> <C> Human (∈Dtest) <C> 0.43 <C> 0.47 <C> 0.35 <C> 29.0 <C> 16.3 <C> 10.6 <C> 7.44 <C> 8.6 <C> 0.31 <C> 0.81 <R> <C> S2S+LM <C> 0.36 <C> [BOLD] 0.48 <C> [BOLD] 0.34 <C> 14.2 <C> 2.5 <C> 0.7 <C> 0.41 <C> [BOLD] 9.4 <C> [BOLD] 0.11 <C> [BOLD] 0.54 <R> <C> MTask <C> 0.13 <C> 0.13 <C> 0.10 <C> 16.5 <C> 2.9 <C> 1.2 <C> 0.66 <C> 5.7 <C> 0.04 <C> 0.13 <R> <C> +Lconv (SpaceFusion) <C> 0.27 <C> 0.41 <C> 0.17 <C> [BOLD] 18.1 <C> 3.9 <C> 1.4 <C> 0.75 <C> 6.9 <C> 0.04 <C> 0.13 <R> <C> +Lstyle (StyleFusion) <C> [BOLD] 0.40 <C> 0.46 <C> 0.21 <C> 17.9 <C> [BOLD] 4.4 <C> [BOLD] 1.6 <C> [BOLD] 0.83 <C> 7.9 <C> 0.05 <C> 0.20 <R> <C> target = Holmes <C> target = Holmes <C> target = Holmes <C> target = Holmes <C> target = Holmes <C> target = Holmes <C> target = Holmes <C> target = Holmes <C> target = Holmes <C> target = Holmes <C> target = Holmes <R> <C> Rand (∈Dstyle) <C> 0.60 <C> 0.48 <C> 1.00 <C> 13.1 <C> 1.9 <C> 0.6 <C> 0.37 <C> 9.0 <C> 0.15 <C> 0.62 <R> <C> Retrieval (∈Dstyle) <C> 0.20 <C> 0.21 <C> 0.10 <C> 10.7 <C> 1.7 <C> 0.7 <C> 0.45 <C> 6.5 <C> 0.04 <C> 0.15 <R> <C> Human (∈Dtest) <C> 0.46 <C> 0.43 <C> 0.67 <C> 26.5 <C> 13.7 <C> 9.2 <C> 6.65 <C> 9.3 <C> 0.16 <C> 0.60 <R> <C> S2S+LM <C> 0.50 <C> 0.44 <C> [BOLD] 0.59 <C> 16.3 <C> 3.0 <C> 0.8 <C> 0.44 <C> [BOLD] 8.7 <C> [BOLD] 0.07 <C> [BOLD] 0.38 <R> <C> MTask <C> 0.17 <C> 0.22 <C> 0.14 <C> 19.5 <C> 4.5 <C> 1.5 <C> 0.73 <C> 6.9 <C> 0.03 <C> 0.12 <R> <C> +Lconv (SpaceFusion) <C> 0.39 <C> 0.33 <C> 0.22 <C> 18.9 <C> 4.6 <C> 1.5 <C> [BOLD] 0.76 <C> 7.7 <C> 0.04 <C> 0.17 <R> <C> +Lstyle (StyleFusion) <C> [BOLD] 0.55 <C> [BOLD] 0.48 <C> 0.47 <C> [BOLD] 20.6 <C> [BOLD] 5.1 <C> [BOLD] 1.6 <C> 0.73 <C> 7.8 <C> 0.04 <C> 0.17 <CAP> Table 6: Performance of each model on automatic measures. The highest score for generative models in each column is in bold for each target. the ”count” metric is normalized by the value of the targeted style dataset. Note that the unit for BLEU is percentage.
<R> <C> [BOLD] Model Tf-idf <C> [BOLD] F1@10 0.270 <C> [BOLD] Model TopicRank <C> [BOLD] F1@10 0.154 <R> <C> TextRank <C> 0.097 <C> KeyCluster <C> 0.140 <R> <C> SingleRank <C> 0.256 <C> CopyRNN <C> 0.164 <R> <C> ExpandRank <C> 0.269 <C> CorrRNN <C> 0.173 <CAP> Table 6: Performance on DUC-2001. CopyRNN and CorrRNN are supervised, and they are trained on scientific publications but evaluated on news.
<R> <C> Model <C> Dev WER <C> Test WER <R> <C> No-rescoring <C> 8.75 <C> 8.77 <R> <C> Base XLNet 4-gram, no mems <C> 8.54 <C> 8.73 <R> <C> Base XLNet 4-gram, with mems <C> 8.46 <C> 8.82 <R> <C> Fine-tuned XLNet 4-gram <C> 8.32 <C> 8.47 <R> <C> Fine-tuned XLNet 10-gram <C> 8.29 <C> 8.44 <R> <C> RNNLM 4-gram <C> 6.77 <C> 7.25 <R> <C> Lattice Oracle Best Path <C> 1.81 <C> 1.69 <CAP> Table 1: The pre-trained and fine-tuned transformer models make improvements on the lattice, but they’re still not as good as the pre-trained RNNLM for TED-LIUM. This is most likely due to using a very large XLNet, limited GPU resources, and only 25MB worth of TED-LIUM text.
<R> <C> [BOLD] Model <C> | [ITALIC] θ| <C> [BOLD] PPL <R> <C> LSTM (large) (Zaremba et al.,  2014 ) <C> 66m <C> 78.4 <R> <C> Character CNN (Kim et al.,  2015 ) <C> 19m <C> 78.9 <R> <C> Variational LSTM ( Gal & Ghahramani ) <C> 20m <C> 78.6 <R> <C> Variational LSTM ( Gal & Ghahramani ) <C> 66m <C> 73.4 <R> <C> Pointer Sentinel-LSTM ( Merity et al. ) <C> 21m <C> 70.9 <R> <C> Variational RHN (Zilly et al.,  2016 ) <C> 23m <C> 65.4 <R> <C> Neural Net Search (Zoph & Le,  2016 ) <C> 25m <C> 64.0 <R> <C> Kernel NN ( [ITALIC] λ=0.8) <C> 5m <C> 84.3 <R> <C> Kernel NN ( [ITALIC] λ learned as parameter) <C> 5m <C> 76.8 <R> <C> Kernel NN (gated  [ITALIC] λ) <C> 5m <C> 73.6 <R> <C> Kernel NN (gated  [ITALIC] λ) <C> 20m <C> 69.2 <R> <C> + variational dropout <C> 20m <C> 65.5 <R> <C> + variational dropout, 4 RNN layers <C> 20m <C> [BOLD] 63.8 <CAP> Table 1: Comparison with state-of-the-art results on PTB. |θ| denotes the number of parameters. Following recent work (Press & Wolf, 2016), we share the input and output word embedding matrix. We report the test perplexity (PPL) of each model. Lower number is better.
<R> <C> [BOLD] Model <C> [BOLD] Fine <C> [BOLD] Binary <R> <C> RNN (Socher et al. ( 2011 )) <C> 43.2 <C> 82.4 <R> <C> RNTN (Socher et al. ( 2013 )) <C> 45.7 <C> 85.4 <R> <C> DRNN (Irsoy & Cardie ( 2014 )) <C> 49.8 <C> 86.8 <R> <C> RLSTM (Tai et al. ( 2015 )) <C> 51.0 <C> 88.0 <R> <C> DCNN (Kalchbrenner et al. ( 2014 )) <C> 48.5 <C> 86.9 <R> <C> CNN-MC (Kim ( 2014 )) <C> 47.4 <C> 88.1 <R> <C> Bi-LSTM (Tai et al. ( 2015 )) <C> 49.1 <C> 87.5 <R> <C> LSTMN (Cheng et al. ( 2016 )) <C> 47.9 <C> 87.0 <R> <C> PVEC (Le & Mikolov ( 2014 )) <C> 48.7 <C> 87.8 <R> <C> DAN (Iyyer et al. ( 2014 )) <C> 48.2 <C> 86.8 <R> <C> DMN (Kumar et al. ( 2016 )) <C> 52.1 <C> 88.6 <R> <C> Kernel NN,  [ITALIC] λ=0.5 <C> 51.2 <C> 88.6 <R> <C> Kernel NN, gated  [ITALIC] λ <C> [BOLD] 53.2 <C> [BOLD] 89.9 <CAP> Table 2: Classification accuracy on Stanford Sentiment Treebank. Block I: recursive networks; Block II: convolutional or recurrent networks; Block III: other baseline methods. Higher number is better.
<R> <C> [EMPTY] <C> Target Reference  [BOLD] R-1 <C> Target Reference  [BOLD] R-2 <C> Target Reference  [BOLD] R-3 <C> Target Reference  [BOLD] R-4 <C> Target Reference  [BOLD] R-L <C> Lead-3 Reference  [BOLD] R-1 <C> Lead-3 Reference  [BOLD] R-2 <C> Lead-3 Reference  [BOLD] R-3 <C> Lead-3 Reference  [BOLD] R-4 <C> Lead-3 Reference  [BOLD] R-L <R> <C> Extractive Oracle Grusky et al. ( 2018 ) <C> 93.36 <C> 83.19 <C> - <C> - <C> 93.36 <C> - <C> - <C> - <C> - <C> - <R> <C> Lead-3 Baseline <C> 40.24 <C> 17.53 <C> 9.94 <C> 6.50 <C> 36.49 <C> - <C> - <C> - <C> - <C> - <R> <C> [ITALIC] Abstractive Models <C> [ITALIC] Abstractive Models <C> [ITALIC] Abstractive Models <C> [ITALIC] Abstractive Models <C> [ITALIC] Abstractive Models <C> [ITALIC] Abstractive Models <C> [ITALIC] Abstractive Models <C> [ITALIC] Abstractive Models <C> [ITALIC] Abstractive Models <C> [EMPTY] <C> [EMPTY] <R> <C> Model Hsu et al. ( 2018 ) <C> 40.68 <C> 17.97 <C> 10.43 <C> 6.97 <C> 37.13 <C> 69.66 <C> 62.60 <C> 60.33 <C> 58.72 <C> 68.42 <R> <C> Model Gehrmann et al. ( 2018 ) <C> 41.53 <C> 18.77 <C> 10.68 <C> 6.98 <C> 38.39 <C> 52.25 <C> 39.03 <C> 33.40 <C> 29.61 <C> 50.21 <R> <C> Model Jiang and Bansal ( 2018 ) <C> 40.05 <C> 17.66 <C> 10.34 <C> 6.99 <C> 36.73 <C> 62.32 <C> 52.93 <C> 49.95 <C> 47.98 <C> 60.72 <R> <C> Model Chen and Bansal ( 2018 ) <C> 40.88 <C> 17.81 <C> 9.79 <C> 6.19 <C> 38.54 <C> 55.87 <C> 41.30 <C> 34.69 <C> 29.88 <C> 53.83 <R> <C> Model See et al. ( 2017 ) <C> 39.53 <C> 17.29 <C> 10.05 <C> 6.75 <C> 36.39 <C> 58.15 <C> 47.60 <C> 44.11 <C> 41.82 <C> 56.34 <R> <C> Model Kryściński et al. ( 2018 ) <C> 40.23 <C> 17.30 <C> 9.33 <C> 5.70 <C> 37.76 <C> 57.22 <C> 42.30 <C> 35.26 <C> 29.95 <C> 55.13 <R> <C> Model Li et al. ( 2018 ) <C> 40.78 <C> 17.70 <C> 9.76 <C> 6.19 <C> 38.34 <C> 56.45 <C> 42.36 <C> 35.97 <C> 31.39 <C> 54.51 <R> <C> Model Pasunuru and Bansal ( 2018 ) <C> 40.44 <C> 18.03 <C> 10.56 <C> 7.12 <C> 37.02 <C> 62.81 <C> 53.57 <C> 50.25 <C> 47.99 <C> 61.27 <R> <C> Model Zhang et al. ( 2018 ) <C> 39.75 <C> 17.32 <C> 10.11 <C> 6.83 <C> 36.54 <C> 58.82 <C> 47.55 <C> 44.07 <C> 41.84 <C> 56.83 <R> <C> Model Guo et al. ( 2018 ) <C> 39.81 <C> 17.64 <C> 10.40 <C> 7.08 <C> 36.49 <C> 56.42 <C> 45.88 <C> 42.39 <C> 40.11 <C> 54.59 <R> <C> [ITALIC] Extractive Models <C> [ITALIC] Extractive Models <C> [ITALIC] Extractive Models <C> [ITALIC] Extractive Models <C> [ITALIC] Extractive Models <C> [ITALIC] Extractive Models <C> [ITALIC] Extractive Models <C> [ITALIC] Extractive Models <C> [ITALIC] Extractive Models <C> [EMPTY] <C> [EMPTY] <R> <C> Model Dong et al. ( 2018 ) <C> 41.41 <C> 18.69 <C> 10.87 <C> 7.22 <C> 37.61 <C> 73.10 <C> 66.98 <C> 65.49 <C> 64.66 <C> 72.05 <R> <C> Model Wu and Hu ( 2018 ) <C> 41.25 <C> 18.87 <C> 11.05 <C> 7.38 <C> 37.75 <C> 78.68 <C> 74.74 <C> 73.74 <C> 73.12 <C> 78.08 <R> <C> Model Zhou et al. ( 2018 ) <C> 41.59 <C> 19.00 <C> 11.13 <C> 7.45 <C> 38.08 <C> 69.32 <C> 61.00 <C> 58.51 <C> 56.98 <C> 67.85 <CAP> Table 6: ROUGE (R-) scores computed for different models on the test set of the CNN/DM dataset. Left: Scores computed with the original reference summaries. Right: Scores computed with Lead-3 used as the reference.
<R> <C> Category <C> Count <C> % <C> Examples <R> <C> Entertainment <C> 128,803 <C> 33.3 <C> #AnimalCrossing, #BTS, #IHeartAwards, #NFLDraft, #Oscars <R> <C> News <C> 30,952 <C> 8.0 <C> #Breaking, #News <R> <C> Support Trump/GOP <C> 23,386 <C> 6.1 <C> #MAGA, #Trump2020, #KAG <R> <C> Social solidarity <C> 21,464 <C> 5.6 <C> #StayHome, #SocialDistancing <R> <C> Science/Technology <C> 21,138 <C> 5.5 <C> #AI, #BigData, #ML <R> <C> Business <C> 19,837 <C> 5.1 <C> #Business, #Marketing <R> <C> Democrats <C> 16,656 <C> 4.3 <C> #DemDebate, #Biden <R> <C> Attack Trump/GOP <C> 13,467 <C> 3.5 <C> #TrumpVirus, #Resist <R> <C> Support DNC <C> 13,203 <C> 3.4 <C> #YangGang, #Bernie2020 <R> <C> Inspiration <C> 12,770 <C> 3.3 <C> #Success, #Wisdom, #Leadership <R> <C> Conspiracy theories <C> 11,964 <C> 3.1 <C> #Qanon, #wwg1wga <R> <C> Republican <C> 11,896 <C> 3.1 <C> #TCOT, #GOP <R> <C> Issues <C> 10,258 <C> 2.7 <C> #EarthDay, #BlackHistoryMonth <R> <C> Generic <C> 9,233 <C> 2.4 <C> #Love, #Win, #Truth <R> <C> Political <C> 8,501 <C> 2.2 <C> #Impeachment, #2020Census <R> <C> Health care <C> 7,090 <C> 1.8 <C> #Health, #MedicareForAll, #MentalHealth <R> <C> China <C> 6,734 <C> 1.7 <C> #China, #Wuhan, #CCP <R> <C> Foreign issues <C> 6,409 <C> 1.7 <C> #WWIII, #Iran <R> <C> Blame media <C> 3,410 <C> 0.9 <C> #FakeNews <R> <C> Attack DNC <C> 3,192 <C> 0.8 <C> #MeToo (accusing Biden of harassment) <CAP> Table 3: Categorization of the 200 most frequent hashtags (excluding COVID19 hashtags) by sampled group.
<R> <C> Measure <C> LEX <C> POS <C> DEP <R> <C> RAW <C> 26.61 <C> 25.35 <C> 24.68 <R> <C> PPMI <C> 25.00 <C> 25.51 <C> 24.48 <R> <C> LMI <C> 27.15 <C> 25.19 <C> 24.41 <R> <C> LOG <C> 23.01 <C> 25.55 <C> 24.47 <R> <C> ENT <C> 22.39 <C> 25.55 <C> 24.52 <CAP> Table 1: Semantic word analogy detection accuracy using word representations learnt by the proposed method from relational graphs with different pattern types and weighting measures.
<R> <C> Method <C> capital-common <C> capital-world <C> city-in-state <C> family (gender) <C> currency <C> Overall Accuracy <R> <C> SVD+LEX <C> 11.43 <C> 5.43 <C> 0 <C> 9.52 <C> 0 <C> 3.84 <R> <C> SVD+POS <C> 4.57 <C> 9.06 <C> 0 <C> 29.05 <C> 0 <C> 6.57 <R> <C> SVD+DEP <C> 5.88 <C> 3.02 <C> 0 <C> 0 <C> 0 <C> 1.11 <R> <C> CBOW <C> 8.49 <C> 5.26 <C> 4.95 <C> 47.82 <C> 2.37 <C> 10.58 <R> <C> skip-gram <C> 9.15 <C> 9.34 <C> 5.97 <C> [BOLD] 67.98 <C> 5.29 <C> 14.86 <R> <C> GloVe <C> 4.24 <C> 4.93 <C> 4.35 <C> 65.41 <C> 0 <C> 11.89 <R> <C> Prop+LEX <C> [BOLD] 22.87 <C> [BOLD] 31.42 <C> [BOLD] 15.83 <C> 61.19 <C> 25.0 <C> [BOLD] 26.61 <R> <C> Prop+POS <C> 22.55 <C> 30.82 <C> 14.98 <C> 60.48 <C> 20.0 <C> 25.35 <R> <C> Prop+DEP <C> 20.92 <C> 31.40 <C> 15.27 <C> 56.19 <C> 20.0 <C> 24.68 <CAP> Table 2: Comparison of the proposed method (denoted by Prop) against prior work on word representation learning.
<R> <C> Model <C> AUC <R> <C> Average (word2vec) <C> 0.75 <R> <C> Average (GloVe) <C> 0.78 <R> <C> Average (Paragram) <C> 0.75 <R> <C> WR (word2vec) <C> 0.74 <R> <C> WR (GloVe) <C> [BOLD] 0.79 <R> <C> WR (Paragram) <C> 0.75 <R> <C> Infersent <C> [BOLD] 0.91 <R> <C> BiLSTM-NMTmax <C> 0.90 <R> <C> BiLSTM-NMTavg <C> 0.85 <R> <C> dbow (WIKI) <C> [BOLD] 0.91 <R> <C> dbow (AP) <C> 0.90 <CAP> Table 1: AUC Scores for cqadupstack
<R> <C> [BOLD] Model <C> [BOLD] Accuracy <R> <C> Aristo <C> 77.4 <R> <C> Lucene <C> 80.0 <R> <C> TableILP <C> 31.8 <R> <C> AS Reader <C> 74.1 <R> <C> GA Reader <C> 73.8 <R> <C> Humans <C> 87.8 ± 0.045 <CAP> Table 2: Test set accuracy of existing models on the multiple choice version of SciQ.
<R> <C> [EMPTY] <C> [BOLD] Average number of node <C> [BOLD] Sentences with <C> [BOLD] BLEU <R> <C> [EMPTY] <C> [BOLD] expansions per sentence <C> [BOLD] search errors <C> [BOLD] score <R> <C> Exhaustive enumeration <C> 652.3K <C> 0% <C> 21.7 <R> <C> Depth-first search with admissible pruning <C> 3.0K <C> 0% <C> 21.7 <R> <C> Beam search (beam=20) <C> 250.5 <C> 20.3% <C> 21.9 <R> <C> Beam search (beam=4) <C> 64.8 <C> 41.9% <C> 21.9 <R> <C> Greedy decoding <C> 18.0 <C> 67.9% <C> 22.1 <CAP> Table 1: BPE-level SMT lattice rescoring with different search strategies. The BLEU score does not benefit from less search errors due to modeling errors.
<R> <C> [BOLD] Models <C> [BOLD] AA-Sentiment Precision <C> [BOLD] AA-Sentiment Recall <C> [BOLD] AA-Sentiment ROC Area <C> [BOLD] AA-Sentiment Accuracy <C> [BOLD] AA-Sentiment F1 Score <C> [BOLD] HA-Sentiment Precision <C> [BOLD] HA-Sentiment Recall <C> [BOLD] HA-Sentiment ROC Area <C> [BOLD] HA-Sentiment Accuracy <C> [BOLD] HA-Sentiment F1 Score <R> <C> T-LSTM <C> 0.921 <C> [BOLD] 0.901 <C> 0.931 <C> 0.866 <C> 0.911 <C> 0.708 <C> 0.825 <C> 0.774 <C> 0.707 <C> 0.762 <R> <C> E-LSTM <C> 0.944 <C> 0.86 <C> 0.933 <C> 0.855 <C> 0.900 <C> 0.816 <C> 0.825 <C> 0.855 <C> 0.794 <C> 0.820 <R> <C> ATT-E-LSTM <C> 0.948 <C> 0.890 <C> 0.954 <C> 0.879 <C> 0.918 <C> 0.825 <C> 0.868 <C> 0.878 <C> 0.820 <C> 0.846 <R> <C> BiE-LSTM <C> 0.961 <C> 0.891 <C> 0.966 <C> 0.890 <C> 0.924 <C> 0.822 <C> 0.881 <C> 0.898 <C> 0.824 <C> 0.850 <R> <C> MATT-BiE-LSTM <C> [BOLD] 0.972 <C> 0.895 <C> [BOLD] 0.975 <C> [BOLD] 0.900 <C> [BOLD] 0.932 <C> [BOLD] 0.831 <C> 0.872 <C> 0.890 <C> 0.826 <C> 0.851 <R> <C> WATT-BiE-LSTM <C> 0.949 <C> 0.895 <C> 0.960 <C> 0.883 <C> 0.921 <C> 0.830 <C> [BOLD] 0.889 <C> [BOLD] 0.899 <C> [BOLD] 0.834 <C> [BOLD] 0.859 <CAP> Table 3. Twitter Sentiment Analysis.
<R> <C> Model <C> DBP15KZH-EN HR1 <C> DBP15KZH-EN HR10 <C> DBP15KZH-EN MRR <C> DBP15KJA-EN HR1 <C> DBP15KJA-EN HR10 <C> DBP15KJA-EN MRR <C> DBP15KFR-EN HR1 <C> DBP15KFR-EN HR10 <C> DBP15KFR-EN MRR <R> <C> MuGNN <C> 49.40 <C> 84.40 <C> 61.10 <C> 50.10 <C> 85.70 <C> 62.10 <C> 49.50 <C> 87.00 <C> 62.10 <R> <C> BootEA <C> 62.94 <C> 84.75 <C> 70.30 <C> 62.23 <C> 85.39 <C> 70.10 <C> 65.30 <C> 87.44 <C> 73.10 <R> <C> JAPE <C> 41.18 <C> 74.46 <C> 49.00 <C> 36.25 <C> 68.50 <C> 47.60 <C> 32.39 <C> 66.68 <C> 43.00 <R> <C> GCNs <C> 41.25 <C> 74.38 <C> 55.80 <C> 39.91 <C> 74.46 <C> 55.20 <C> 37.29 <C> 74.49 <C> 53.40 <R> <C> MultiKE <C> 50.87 <C> 57.61 <C> 53.20 <C> 39.30 <C> 48.85 <C> 42.60 <C> 63.94 <C> 71.19 <C> 66.50 <R> <C> JarKA-r <C> 57.18 <C> 70.44 <C> 61.80 <C> 50.63 <C> 60.36 <C> 54.30 <C> 53.92 <C> 60.40 <C> 56.30 <R> <C> JarKA-a <C> 58.64 <C> 83.89 <C> 67.10 <C> 55.74 <C> 83.23 <C> 65.10 <C> 59.25 <C> 85.74 <C> 68.60 <R> <C> JarKA(M1) <C> 68.59 <C> 86.56 <C> 74.90 <C> 62.65 <C> 82.79 <C> 69.70 <C> 68.43 <C> 87.86 <C> 75.10 <R> <C> JarKA(M2) <C> 69.32 <C> 87.37 <C> 75.50 <C> 63.01 <C> 83.37 <C> 70.00 <C> [BOLD] 70.87 <C> 87.05 <C> 76.50 <R> <C> JarKA(M3) <C> [BOLD] 70.58 <C> [BOLD] 87.81 <C> [BOLD] 76.60  <C> [BOLD] 64.58 <C> [BOLD] 85.50 <C> [BOLD] 70.80  <C> 70.41 <C> [BOLD] 88.81 <C> [BOLD] 76.80 <R> <C> JarKA-IT <C> 66.39 <C> 87.29 <C> 73.40 <C> 60.08 <C> 84.45 <C> 68.20 <C> 68.31 <C> 88.33 <C> 75.40 <CAP> Table 2: Overall performance of entity alignment (%).
<R> <C> Method Full sentence <C> Method Full sentence <C> Latency (#tokens) 9.75 (± 2.69) <C> BLEU 34.53 <C> RIBES 84.03 <R> <C> Wait-k Ma et al. ( 2018 ) <C> k=3 <C> 3.00 (± 0.00) <C> 31.06 <C> 82.46 <R> <C> [EMPTY] <C> k=5 <C> 5.00 (± 0.00) <C> 33.29 <C> 83.45 <R> <C> Ours <C> [ITALIC] α=0.00 <C> 4.32 (± 3.14) <C> 28.01 <C> 81.78 <R> <C> [EMPTY] <C> [ITALIC] α=0.01 <C> 4.29 (± 3.16) <C> 30.42 <C> 82.60 <R> <C> [EMPTY] <C> [ITALIC] α=0.03 <C> 2.88 (± 2.95) <C> 26.47 <C> 80.51 <R> <C> [EMPTY] <C> [ITALIC] α=0.05 <C> 0.80 (± 1.96) <C> 22.60 <C> 77.86 <CAP> Table 2: Latency and automatic evaluation scores with small_parallel_enja. Latencies are shown by averages and standard deviations (in parentheses) in the number of tokens.
<R> <C> Dataset Correlation <C> 20NG P <C> 20NG S <C> NYT P <C> NYT S <C> Genomics P <C> Genomics S <R> <C> NPMI <C> 0.74 <C> 0.74 <C> 0.72 <C> 0.71 <C> 0.62 <C> 0.65 <R> <C> GloVe-50d <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> WETC [ITALIC] PW <C> 0.82 <C> 0.77 <C> 0.73 <C> 0.71 <C> 0.65 <C> 0.65 <R> <C> WETC [ITALIC] C <C> 0.81 <C> 0.77 <C> 0.73 <C> 0.71 <C> 0.65 <C> 0.65 <R> <C> GloVe-300d <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> WETC [ITALIC] PW <C> 0.77 <C> 0.75 <C> 0.78 <C> 0.76 <C> 0.68 <C> 0.70 <R> <C> WETC [ITALIC] C <C> 0.80 <C> 0.75 <C> 0.78 <C> 0.76 <C> 0.68 <C> 0.70 <R> <C> Word2Vec <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> WETC [ITALIC] PW <C> 0.29 <C> 0.23 <C> 0.53 <C> 0.59 <C> 0.56 <C> 0.55 <R> <C> WETC [ITALIC] C <C> 0.31 <C> 0.23 <C> 0.55 <C> 0.59 <C> 0.56 <C> 0.55 <R> <C> FastText <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> WETC [ITALIC] PW <C> 0.40 <C> 0.61 <C> 0.63 <C> 0.67 <C> 0.62 <C> 0.62 <R> <C> WETC [ITALIC] C <C> 0.48 <C> 0.61 <C> 0.64 <C> 0.67 <C> 0.63 <C> 0.62 <R> <C> LexVec <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> WETC [ITALIC] PW <C> 0.37 <C> 0.57 <C> 0.79 <C> 0.80 <C> 0.65 <C> 0.64 <R> <C> WETC [ITALIC] C <C> 0.47 <C> 0.57 <C> 0.81 <C> 0.80 <C> 0.65 <C> 0.64 <CAP> Table 2: NPMI and WETC correlation with human gold standard (P: Pearson, S: Spearman)
<R> <C> [BOLD] Model <C> [BOLD] ROUGE  [BOLD] R-1 <C> [BOLD] ROUGE  [BOLD] R-2 <C> [BOLD] ROUGE  [BOLD] R-L <R> <C> 1-g shuf (w/o attn) <C> 23.01 <C> 5.51 <C> 20.07 <R> <C> 2-g shuf (w/o attn) <C> 22.36 <C> 5.18 <C> 19.60 <R> <C> 1-g shuf <C> 27.22 <C> 7.63 <C> 23.55 <R> <C> 2-g shuf <C> 27.72 <C> 7.55 <C> 23.43 <R> <C> 1-g shuf + InferSent <C> 28.12 <C> 7.75 <C> 24.81 <R> <C> 2-g shuf + InferSent <C> [BOLD] 28.42 <C> [BOLD] 7.82 <C> [BOLD] 24.95 <CAP> Table 2: Ablation study. We find that using attention, shuffling bigrams, and incorporating sentence embeddings all improve our ROUGE scores. All length countdowns settings are the same is in the main model.
<R> <C> Architecture <C> Perplexity <R> <C> Seq2Seq <C> 75.44 <R> <C> D-NTMS <C> 74.07 <R> <C> HRED <C> 73.33 <R> <C> LM <C> 69.36 <R> <C> NTM-LM <C> [BOLD] 68.50 <CAP> Table 1: Word-level perplexity evaluation on proposed model and two selected baselines.
<R> <C> [EMPTY] <C> Accuracy (%) <R> <C> 1. Baseline: always-new <C> 31.08 <R> <C> 2. Baseline: shallow features <C> 45.34 <R> <C> 3. Modi et al. ( 2017 ) <C> 62.65 <R> <C> 4. EntityNLM <C> [BOLD] 74.23 <R> <C> 5.  [ITALIC] Human prediction <C> 77.35 <CAP> Table 3: Entity prediction accuracy on the test set of the InScript corpus.
<R> <C> Encoder and Decoder Model <C> Colors Prec. <C> Colors Rec. <C> Colors Acc. <C> Colors Exact <C> GEO Prec. <C> GEO Rec. <C> GEO Acc. <C> GEO Exact <C> WSJ10 Prec. <C> WSJ10 Rec. <C> WSJ10 Acc. <C> WSJ10 Exact <C> English to Mandarin BLEU <R> <C> LLA-LSTM <C> 90 <C> 62.50 <C> [BOLD] 50.90 <C> 0 <C> 97.00 <C> 92.65 <C> 88.31 <C> [BOLD] 50.4 <C> [BOLD] 76.81 <C> 56.90 <C> 42.49 <C> 2.51 <C> [BOLD] 11.71 <R> <C> LLA-LSTM (No Adversary) <C> 90 <C> 62.50 <C> 49.48 <C> 0 <C> [BOLD] 97.08 <C> [BOLD] 92.68 <C> [BOLD] 89.80 <C> 47.2 <C> 73.42 <C> [BOLD] 57.85 <C> 43.67 <C> 2.76 <C> 11.40 <R> <C> LSTM <C> 90 <C> 62.50 <C> 49.48 <C> 0 <C> 93.66 <C> 91.18 <C> 88.60 <C> 37.6 <C> 65.94 <C> 56.74 <C> [BOLD] 44.48 <C> 2.76 <C> 9.95 <CAP> Table 1: Comparison of the models on the test sets. The metrics used are mean precision (Prec.), mean recall (Rec.), mean accuracy (Acc.), mean number of exact matches (Exact.), and corpus-level BLEU Papineni . (2002).
<R> <C> [BOLD] Model <C> [BOLD] Query <C> [BOLD] Denotation  [BOLD] Relaxed <C> [BOLD] Denotation  [BOLD] Strict <R> <C> [BOLD] Development Results <C> [BOLD] Development Results <C> [BOLD] Development Results <C> [BOLD] Development Results <R> <C> seq2seq-0 <C> 28.7±1.7 <C> 48.8±1.4 <C> 43.2±1.8 <R> <C> [0.5pt/1pt]seq2seq-h <C> 35.1±2.2 <C> 59.4±2.4 <C> 56.7±3.2 <R> <C> [0.5pt/1pt]s2s+anon <C> 37.6±0.7 <C> 61.6±0.7 <C> 60.6±0.7 <R> <C> [0.5pt/1pt]Full-0 <C> 36.3±0.5 <C> 61.5±0.8 <C> 61.0±0.9 <R> <C> [0.5pt/1pt]Full <C> 37.5±0.9 <C> 63.0±0.7 <C> [BOLD] 62.5±0.9 <R> <C> [0.5pt/1pt]– turn-level enc. <C> 37.4±1.5 <C> 62.1±2.5 <C> 61.4±2.7 <R> <C> [0.5pt/1pt]– batch re-weight <C> 36.4±0.6 <C> 61.8±0.3 <C> 61.5±0.4 <R> <C> [0.5pt/1pt]– input pos. embs. <C> 33.3±0.2 <C> 57.9±0.8 <C> 57.4±0.8 <R> <C> [0.5pt/1pt]– query segments <C> 36.0±0.9 <C> 59.5±1.3 <C> 58.3±1.4 <R> <C> [0.5pt/1pt]– anon. scoring <C> 35.7±0.5 <C> 60.8±1.1 <C> 60.0±1.0 <R> <C> [0.5pt/1pt]– pre-processing <C> 26.4±6.1 <C> 53.3±8.6 <C> 53.0±8.5 <R> <C> Full-Gold <C> 42.1±0.8 <C> 66.6±0.7 <C> 66.1±0.7 <R> <C> [BOLD] Test Results <C> [BOLD] Test Results <C> [BOLD] Test Results <C> [BOLD] Test Results <R> <C> seq2seq-0 <C> 35.7±1.5 <C> 56.4±1.1 <C> 53.8±1.0 <R> <C> [0.5pt/1pt]seq2seq-h <C> 42.2±2.0 <C> 66.6±3.2 <C> 65.8±3.4 <R> <C> [0.5pt/1pt]s2s+anon <C> 44.0±1.2 <C> 69.3±1.0 <C> 68.6±1.1 <R> <C> [0.5pt/1pt]Full-0 <C> 43.1±1.3 <C> 67.8±1.6 <C> 67.2±1.6 <R> <C> [0.5pt/1pt]Full <C> 43.6±1.0 <C> 69.3±0.8 <C> [BOLD] 69.2±0.8 <R> <C> Full-Gold <C> 47.4±1.3 <C> 72.3±0.5 <C> 72.0±0.5 <CAP> Table 2: Mean and standard deviation development and test results, including ablations on the Full model.
<R> <C> [BOLD] Model <C> [BOLD] Query <C> [BOLD] Denotation  [BOLD] Relaxed <C> [BOLD] Denotation  [BOLD] Strict <R> <C> [BOLD] Development Results <C> [BOLD] Development Results <C> [BOLD] Development Results <C> [BOLD] Development Results <R> <C> s2s+anon <C> 44.4±1.2 <C> 69.9±0.3 <C> 68.9±0.3 <R> <C> [0.5pt/1pt]Full <C> 42.8±0.2 <C> 68.8±0.2 <C> 68.4±0.2 <R> <C> Full-Gold <C> 47.5±0.2 <C> 71.5±0.4 <C> 70.7±0.6 <R> <C> [BOLD] Test Results <C> [BOLD] Test Results <C> [BOLD] Test Results <C> [BOLD] Test Results <R> <C> s2s+anon <C> 43.9±0.3 <C> 67.4±1.0 <C> 67.2±1.0 <R> <C> [0.5pt/1pt]Full <C> 44.3±0.2 <C> 66.3±0.3 <C> 66.3±0.4 <R> <C> Full-Gold <C> 47.2±0.3 <C> 68.2±0.5 <C> 67.9±0.4 <CAP> Table 3: Results on the original split of the data.
<R> <C> [BOLD] Models <C> [BOLD] H <C> [BOLD] L <C> [BOLD] A <C> [BOLD] # of Parameters <C> [BOLD] URL <R> <C> [BOLD] BERTbase <C> 768 <C> 12 <C> 12 <C> 110M <C> https://github.com/google-research/bert <R> <C> [BOLD] ERNIE 1.0base <C> 768 <C> 12 <C> 12 <C> 110M <C> https://github.com/PaddlePaddle/ERNIE <R> <C> [BOLD] ERNIE 2.0base <C> 768 <C> 12 <C> 12 <C> 110M <C> [EMPTY] <R> <C> [BOLD] ERNIE 2.0large <C> 1024 <C> 24 <C> 16 <C> 340M <C> [EMPTY] <CAP> Table 4: The hyper-parameters of pre-trained language models. We denote the number of layers as L, the hidden size as H, and the number of self-attention heads as A. Besides, we provide the URL to download these pre-trained models.
<R> <C> [BOLD] Train Dataset <C> [BOLD] Model <C> [BOLD] Intrinsic Evaluation  [BOLD] Train <C> [BOLD] Intrinsic Evaluation  [BOLD] Train <C> [BOLD] Intrinsic Evaluation  [BOLD] Valid <C> [BOLD] Intrinsic Evaluation  [BOLD] Valid <C> [BOLD] Intrinsic Evaluation  [BOLD] Valid <C> [BOLD] Intrinsic Evaluation  [BOLD] Valid <C> [BOLD] Extrinsic Evaluation  [BOLD] Eval Dataset <C> [BOLD] Extrinsic Evaluation  [BOLD] Train <C> [BOLD] Extrinsic Evaluation  [BOLD] Valid <C> [BOLD] Extrinsic Evaluation  [BOLD] Test <R> <C> [BOLD] Train Dataset <C> [BOLD] Model <C> [BOLD] Loss <C> [BOLD] BLEU <C> [BOLD] Loss <C> [BOLD] BLEU <C> [BOLD] GLEU <C> [BOLD] KL <C> [BOLD] Eval Dataset <C> [BOLD] Train <C> [BOLD] Valid <C> [BOLD] Test <R> <C> WikiAtomicEdits <C> Baseline <C> [BOLD] 57.76 <C> [BOLD] 0.21 <C> [BOLD] 26.2 <C> [BOLD] 0.81 <C> [BOLD] 0.79 <C> - <C> Wikipedia Edits <C> [BOLD] 0.634 <C> 0.629 <C> 0.634 <R> <C> WikiAtomicEdits <C> EVE <C> 67.75 <C> 0.20 <C> 41.4 <C> 0.77 <C> 0.75 <C> 1.66 <C> Wikipedia Edits <C> 0.632 <C> [BOLD] 0.629 <C> [BOLD] 0.642 <R> <C> WikiAtomicEdits <C> HEVE <C> 66.92 <C> [BOLD] 0.21 <C> 37.07 <C> 0.79 <C> 0.78 <C> 1.86 <C> Wikipedia Edits <C> 0.592 <C> 0.592 <C> 0.594 <R> <C> WikiEditsMix <C> Baseline <C> [BOLD] 256.49 <C> [BOLD] 0.20 <C> 360.37 <C> 0.62 <C> [BOLD] 0.62 <C> - <C> Wikipedia Edits <C> 0.608 <C> 0.615 <C> 0.609 <R> <C> WikiEditsMix <C> EVE <C> 267.47 <C> [BOLD] 0.20 <C> 384.25 <C> 0.60 <C> 0.62 <C> 24.17 <C> Wikipedia Edits <C> 0.588 <C> 0.591 <C> 0.598 <R> <C> WikiEditsMix <C> HEVE <C> 278.61 <C> [BOLD] 0.20 <C> [BOLD] 352.35 <C> 0.58 <C> 0.61 <C> 10.55 <C> Wikipedia Edits <C> [BOLD] 0.622 <C> [BOLD] 0.618 <C> [BOLD] 0.614 <R> <C> Lang 8 <C> Baseline <C> [BOLD] 28.96 <C> 0.12 <C> [BOLD] 16.11 <C> [BOLD] 0.66 <C> [BOLD] 0.59 <C> - <C> WI + Locness <C> [BOLD] 0.657 <C> [BOLD] 0.617 <C> [BOLD] 0.617 <R> <C> Lang 8 <C> EVE <C> 38.18 <C> 0.10 <C> 29.91 <C> 0.56 <C> 0.46 <C> 0.57 <C> WI + Locness <C> 0.332 <C> 0.241 <C> 0.241 <R> <C> Lang 8 <C> HEVE <C> 50.49 <C> [BOLD] 0.16 <C> 49.42 <C> 0.56 <C> 0.46 <C> 0.69 <C> WI + Locness <C> 0.566 <C> 0.495 <C> 0.495 <R> <C> QT21 De-En <C> Baseline <C> [BOLD] 48.88 <C> [BOLD] 0.13 <C> [BOLD] 50.77 <C> [BOLD] 0.57 <C> [BOLD] 0.49 <C> - <C> QT21 De-En MQM <C> 0.773 <C> 0.752 <C> 0.762 <R> <C> QT21 De-En <C> EVE <C> 60.37 <C> 0.11 <C> 64.66 <C> 0.50 <C> 0.42 <C> 7.38 <C> QT21 De-En MQM <C> 0.174 <C> 0.112 <C> 0.163 <R> <C> QT21 De-En <C> HEVE <C> 57.41 <C> 0.12 <C> 60.13 <C> 0.53 <C> 0.45 <C> 8.62 <C> QT21 De-En MQM <C> [BOLD] 0.839 <C> [BOLD] 0.816 <C> [BOLD] 0.825 <CAP> Table 2: Result of the intrinsic and extrinsic evaluations on our datasets, as defined by the PEER framework. For the intrinsic evaluation, validation BLEU and GLEU scores are computed over the beam search generated output.
<R> <C> [BOLD] Model <C> AvgScore <C> Fleiss’ kappa <R> <C> PCFG-Trans <C> 1.42 <C> 0.50 <R> <C> NQG++ <C> 2.18 <C> 0.46 <CAP> Table 2: Human evaluation results.
<R> <C> Model <C> Overall <C> Exist <C> Count <C> Compare integers equal <C> Compare integers less <C> Compare integers more <C> Query attribute size <C> Query attribute color <C> Query attribute material <C> Query attribute shape <C> Compare attribute size <C> Compare attribute color <C> Compare attribute material <C> Compare attribute shape <R> <C> Human  <C> 92.60 <C> 96.60 <C> 86.70 <C> 79.00 <C> 87.00 <C> 91.00 <C> 97.00 <C> 95.00 <C> 94.00 <C> 94.00 <C> 94.00 <C> 98.00 <C> 96.00 <C> 96.00 <R> <C> CNN-LSTM  <C> 52.30 <C> 65.20 <C> 43.70 <C> 57.00 <C> 72.00 <C> 69.00 <C> 59.00 <C> 32.00 <C> 58.00 <C> 48.00 <C> 54.00 <C> 54.00 <C> 51.00 <C> 53.00 <R> <C> +MCB  <C> 51.40 <C> 63.40 <C> 42.10 <C> 57.00 <C> 71.00 <C> 68.00 <C> 59.00 <C> 32.00 <C> 57.00 <C> 48.00 <C> 51.00 <C> 52.00 <C> 50.00 <C> 51.00 <R> <C> +SA  <C> 68.50 <C> 71.10 <C> 52.2 <C> 60.00 <C> 82.00 <C> 74.00 <C> 87.00 <C> 81.00 <C> 88.00 <C> 85.00 <C> 52.00 <C> 55.00 <C> 51.00 <C> 51.00 <R> <C> NMN  <C> 72.10 <C> 79.30 <C> 52.50 <C> 61.20 <C> 77.90 <C> 75.20 <C> 84.20 <C> 68.90 <C> 82.60 <C> 80.20 <C> 80.70 <C> 74.40 <C> 77.60 <C> 79.30 <R> <C> N2NMN  <C> 83.70 <C> 85.70 <C> 68.50 <C> 73.80 <C> 89.70 <C> 87.70 <C> 93.10 <C> 84.50 <C> 91.50 <C> 90.60 <C> 92.60 <C> 82.80 <C> 89.60 <C> 90.00 <R> <C> FiLM  <C> 97.7 <C> 99.1 <C> 94.3 <C> 96.8 <C> 96.8 <C> 96.8 <C> 99.1 <C> 99.1 <C> 99.1 <C> 99.1 <C> 99.1 <C> 99.1 <C> 99.1 <C> 99.1 <R> <C> QGHC(ours) <C> 86.30 <C> 78.10 <C> 91.17 <C> 67.30 <C> 87.14 <C> 83.28 <C> 93.65 <C> 87.86 <C> 86.75 <C> 90.70 <C> 86.24 <C> 87.24 <C> 86.75 <C> 86.93 <CAP> Table 4: Comparisons of question answering accuracy of the proposed approach and the state-of-the-art methods on the CLVER dataset.
<R> <C> Words <C> Human judgment <C> Binary <C> Cosine <R> <C> dollar – buck <C> 0.92 <C> 0.56 <C> 0.13 <R> <C> seafood – sea <C> 0.75 <C> 0.62 <C> 0.24 <R> <C> money – salary <C> 0.79 <C> 0.58 <C> 0.41 <R> <C> car – automobile <C> 0.98 <C> 0.69 <C> 0.60 <CAP> Table 4: Semantic similarity for some pairs of words, evaluated by human or computed with binary/real-valued vectors.
<R> <C> Environment <C> [ITALIC] T60 <R> <C> Home <C> 0.55 <R> <C> Office <C> 0.4–0.7 <R> <C> Mall <C> 1.7–3.2 <R> <C> Airport <C> 3+ <CAP> Table 1: T60 for various acoustic environments.
<R> <C> X <C> [ITALIC] nA, [ITALIC] XnA <C> [ITALIC] nB, [ITALIC] XnB <C> [ITALIC] nA, [ITALIC] B, [ITALIC] XnA, [ITALIC] B <C> [ITALIC] μmin <C> [ITALIC] μmax <R> <C> Apple <C> 1.66⋅10−1 <C> 2.36⋅10−1 <C> 2.71⋅10−1 <C> 1.02⋅10−1 <C> 3.34⋅10−1 <R> <C> Parsley <C> 1.21⋅10−2 <C> 4.52⋅10−2 <C> 3.19⋅10−2 <C> 1.31⋅10−2 <C> 5.95⋅10−2 <R> <C> Yam <C> 2.88⋅10−3 <C> 3.48⋅10−3 <C> 4.76⋅10−3 <C> 1.15⋅10−3 <C> 7.30⋅10−3 <R> <C> Elderberry <C> 2.16⋅10−3 <C> 3.95⋅10−3 <C> 4.57⋅10−3 <C> 1.25⋅10−3 <C> 6.49⋅10−3 <R> <C> Olive <C> 5.22⋅10−2 <C> 2.13⋅10−1 <C> 2.90⋅10−1 <C> 6.56⋅10−2 <C> 2.12⋅10−1 <R> <C> Raisin <C> 3.49⋅10−2 <C> 3.83⋅10−2 <C> 1.04⋅10−1 <C> 1.45⋅10−3 <C> 9.69⋅10−2 <R> <C> Almond <C> 9.01⋅10−2 <C> 1.10⋅10−1 <C> 2.55⋅10−1 <C> 6.21⋅10−3 <C> 2.35⋅10−1 <R> <C> Lentils <C> 1.42⋅10−2 <C> 1.69⋅10−2 <C> 4.39⋅10−2 <C> 1.38⋅10−3 <C> 4.10⋅10−2 <CAP> Table 1: Individual and joint probabilities for the words “A = Fruits” and “B = Vegetables”, with respect to the exemplars “X = Apple, Parsley, Yam, Elderberry, Olive, Raisin, Almond and Lentils.” For “Apple, Parsley, Yam, Elderberry” data can be modeled using only ‘interference effects’, i.e., formula (15), whereas for “Olive, Raisin, Almond, Lentils” the values of nAB,XnAB lie outside of the ‘interference interval’ [μmin,μmax].
<R> <C> LM <C> rescore <C> dev Vit <C> dev CN <C> eval Vit <C> eval CN <R> <C> ng4 <C> - <C> 23.8 <C> 23.5 <C> 24.2 <C> 23.9 <R> <C> +uni-rnn <C> 100-best <C> 21.7 <C> - <C> 22.1 <C> - <R> <C> +uni-rnn <C> lattice <C> 21.7 <C> 21.5 <C> 21.9 <C> 21.7 <CAP> Table 1: Baseline WER results on AMI corpus
<R> <C> [BOLD] Model <C> [BOLD] RTE <C> [BOLD] SNLI <C> [BOLD] MNLI <C> [BOLD] QNLI <R> <C> [ITALIC] BERTBASE <C> 66.4 <C> 90.4 <C> 86.7 <C> 90.5 <CAP> Table 4: NLI results (Accuracy)
<R> <C> System <C> BLEU <C> Meteor <C> PPL <R> <C> PBMT <C> 9.31 <C> 32.30 <C> 478.4 <R> <C> PBMT+Tok-Farasa <C> 9.51 <C> 33.38 <C> 335.5 <R> <C> PBMT+Tok-MADAMIRA <C> 9.63 <C> 32.90 <C> 342.5 <R> <C> NMT <C> 9.91 <C> 30.55 <C> 2.275 <R> <C> NMT Large <C> 9.92 <C> 30.46 <C> 2.214 <R> <C> NMT+UNK Replace <C> 10.12 <C> 31.84 <C> 2.275 <R> <C> NMT+charCNN <C> 10.65 <C> 32.43 <C> 2.239 <R> <C> NMT+charCNN+UNK Repl. <C> 10.86 <C> 33.61 <C> 2.239 <CAP> Table 3: Results on WIT3. Differences in BLEU scores in the first block are not statistically significant (at p<0.05); differences between the two blocks are significant; the difference between small and large NMT models is not significant; differences between word and character NMT models are significant. Perplexity (PPL) scores are computed from the PBMT language model and the NMT decoder’s classification loss, respectively.
<R> <C> Model <C> R-1-f <C> R-1-r <C> R-2-f <C> R-2-r <C> R-L-f <C> R-L-r <R> <C> [EMPTY] <C> New York Times <C> New York Times <C> New York Times <C> New York Times <C> New York Times <C> New York Times <R> <C> First Sentence <C> 11.64 <C> [BOLD] 34.67 <C> 2.28 <C> 7.43 <C> 7.19 <C> [BOLD] 31.39 <R> <C> Encoder-Decoder <C> 23.02 <C> 21.90 <C> 11.84 <C> 11.44 <C> 21.23 <C> 21.31 <R> <C> summ-hieratt  <C> - <C> 29.60 <C> - <C> 8.17 <C> - <C> 26.05 <R> <C> Universal Transformer w/ smoothing (ours) <C> 25.60 <C> 23.90 <C> 12.92 <C> 12.42 <C> 23.66 <C> 25.27 <R> <C> Universal Transformer (ours) <C> [BOLD] 26.86 <C> 25.33 <C> [BOLD] 13.48 <C> [BOLD] 13.01 <C> [BOLD] 24.84 <C> 24.38 <R> <C> [EMPTY] <C> Rossiya Segodnya <C> Rossiya Segodnya <C> Rossiya Segodnya <C> Rossiya Segodnya <C> Rossiya Segodnya <C> Rossiya Segodnya <R> <C> First Sentence <C> 24.08 <C> [BOLD] 45.58 <C> 10.57 <C> 21.30 <C> 16.70 <C> [BOLD] 41.67 <R> <C> Encoder-Decoder <C> 39.10 <C> 38.31 <C> 22.13 <C> [BOLD] 21.75 <C> 36.34 <C> 36.34 <R> <C> Universal Transformer w/ smoothing (ours) <C> 39.31 <C> 37.10 <C> 21.82 <C> 20.66 <C> 36.32 <C> 35.37 <R> <C> Universal Transformer (ours) <C> [BOLD] 39.75 <C> 37.62 <C> [BOLD] 22.15 <C> 21.04 <C> [BOLD] 36.81 <C> 35.91 <CAP> Table 1: ROUGE-1,2,L F1 and recall scores, on NYT corpus and RIA corpus.
<R> <C> Configuration <C> Accuracy <R> <C> Best <C> 80.1 <R> <C> [ITALIC] H=3 <C> 75.9 <R> <C> [ITALIC] H=1 <C> 75.9 <R> <C> Sentence-level batching <C> 72.4 <R> <C> glove initialization <C> 67.6 <R> <C> No character encoding <C> 65.6 <CAP> Table 2: Ablation of the best model on Penn WSJ.
<R> <C> Classifier <C> SemEval2010 <C> KBP37 <R> <C> CR-CNN  <C> 84.1 <C> - <R> <C> RNN  <C> 79.6 <C> 58.8 <R> <C> Supervised Ranking CNN <C> [BOLD] 84.39 <C> [BOLD] 61.26 <CAP> Table 1: F1-scores for testing datasets.
<R> <C> Languages <C> Error@N=50 <C> Error @N=1000 <R> <C> None <C> 0.275 <C> 0.118 <R> <C> 10 Languages <C> [BOLD] 0.194 <C> 0.114 <R> <C> Spanish <C> 0.233 <C> 0.111 <R> <C> Spanish, French <C> 0.225 <C> [BOLD] 0.109 <R> <C> Spanish, French, Bengali <C> 0.228 <C> 0.111 <R> <C> Bengali <C> 0.241 <C> 0.113 <CAP> Table 2: The first column represents the languages through which we generate backtranslations. Points are the median of 3 runs. † Spanish, French, German, Afrikaans, Russian, Czech, Estonian, Haitian Creole, Bengali.
<R> <C> Feature <C> True Label <C> Prediction <R> <C> Avg. Sentence <C> 0.6 <C> 0.63 <R> <C> Last Sent <C> 0.21 <C> 0.24 <R> <C> First Sent <C> 0.03 <C> 0.07 <R> <C> Max <C> 0 <C> 0.02 <R> <C> Min <C> 0 <C> 0.01 <R> <C> Len <C> -0.04 <C> 0.04 <R> <C> [ITALIC] ModelAccuracy <C> 92% <C> 95% <CAP> Table 4: Logistic Regression Coefficients estimating the effect of a 1 standard deviation increase in a summary statistic computed over the distribution of sentence level sentiments. One feature, for example, is the average sentiment of sentences in a given review. The experiment design is described further in the appendix.
<R> <C> Systems <C> Metric <C> EER% Cough <C> EER% Laugh <C> EER% “Wei” <R> <C> i-vector <C> Cosine <C> 19.96 <C> 23.03 <C> 12.72 <R> <C> [EMPTY] <C> LDA <C> 23.55 <C> 24.24 <C> 12.90 <R> <C> [EMPTY] <C> PLDA <C> 23.33 <C> 24.30 <C> 13.77 <R> <C> d-vector <C> Cosine <C> 11.19 <C> 13.62 <C> 10.66 <R> <C> [EMPTY] <C> LDA <C> 12.37 <C> 13.41 <C> 10.75 <R> <C> [EMPTY] <C> PLDA <C> 10.99 <C> 13.76 <C> 10.06 <CAP> Table 2: EER(%) results with the i-vector and d-vector systems.
<R> <C> Model <C> High <C> Low <C> [ITALIC] ρ <R> <C> Baseline <C> 0.27 <C> 0.26 <C> 0.08 <R> <C> Add <C> 0.59 <C> 0.59 <C> 0.04 <R> <C> Kintsch <C> 0.47 <C> 0.45 <C> 0.09 <R> <C> Multiply <C> 0.42 <C> 0.28 <C> 0.17 <R> <C> [BOLD] Categorical <C> [BOLD] 0.84 <C> [BOLD] 0.79 <C> [BOLD] 0.17 <R> <C> UpperBound <C> 4.94 <C> 3.25 <C> 0.40 <CAP> Table 3: Selected model means for High and Low similarity items and correlation coefficients with human judgements, first experiment [Mitchell and Lapata2008]. p<0.05 for each ρ.
<R> <C> [ITALIC] Stimulus−Reaction <C> [ITALIC] Ngram freq. <C> [ITALIC] Relations <R> <C> yellow−colour <C> 241 <C> − <R> <C> Russia−country <C> 110 <C> hyponymy <R> <C> morning−good <C> 445 <C> − <R> <C> mouth−face <C> 6 <C> part meronymy <R> <C> medicine−clinic <C> 0 <C> domain <R> <C> public−social <C> 0 <C> synonymy <R> <C> help−find <C> 33 <C> − <R> <C> most−outstanding <C> 27 <C> − <R> <C> ask−answer <C> 0 <C> antonymy <R> <C> here−there <C> 18 <C> antonymy <R> <C> write−letter <C> 218 <C> − <R> <C> impression−emotion <C> 0 <C> hyponymy <CAP> Figure 1: Usage of various semantic relations across gender.
<R> <C> [EMPTY] <C> Train <C> Validation <C> Test <R> <C> Pages <C> 90 <C> 10 <C> 25 <R> <C> Records <C> 872 <C> 96 <C> 253 <R> <C> Lines <C> 2759 <C> 311 <C> 757 <R> <C> Words <C> 28346 <C> 3155 <C> 8026 <R> <C> Out of vocabulary words: 5.57 % <C> Out of vocabulary words: 5.57 % <C> Out of vocabulary words: 5.57 % <C> Out of vocabulary words: 5.57 % <CAP> TABLE II: Marriage Records dataset distribution
<R> <C> Tasks <C> Single-Task LSTM <C> Single-Task Bi-LSTM <C> Multi-Task FS <C> Multi-Task SSP <C> Multi-Task PSP <C> Multi-Task CS <C> Multi-Task Our <R> <C> Books <C> 80.3 <C> 82.0 <C> 84.5 <C> 87.0 <C> 84.7 <C> 84.0 <C> 87.3 <R> <C> Electronics <C> 78.5 <C> 80.0 <C> 86.7 <C> 86.5 <C> 85.5 <C> 85.5 <C> 88.3 <R> <C> DVD <C> 78.3 <C> 84.0 <C> 86.5 <C> 86.3 <C> 86.0 <C> 86.5 <C> 86.0 <R> <C> Kitchen <C> 81.7 <C> 83.5 <C> 86.0 <C> 88.7 <C> 88.0 <C> 87.3 <C> 87.5 <R> <C> Apparel <C> 83.0 <C> 85.7 <C> 85.7 <C> 86.3 <C> 87.5 <C> 87.0 <C> 88.7 <R> <C> Camera <C> 87.3 <C> 88.3 <C> 86.7 <C> 88.0 <C> 87.7 <C> 86.3 <C> 90.0 <R> <C> Health <C> 84.5 <C> 85.3 <C> 89.3 <C> 90.0 <C> 89.0 <C> 90.0 <C> 90.3 <R> <C> Music <C> 77.5 <C> 78.7 <C> 82.5 <C> 81.0 <C> 81.3 <C> 84.0 <C> 84.7 <R> <C> Toys <C> 84.0 <C> 85.7 <C> 88.5 <C> 87.7 <C> 88.5 <C> 87.5 <C> 89.7 <R> <C> Video <C> 80.3 <C> 83.7 <C> 86.7 <C> 85.5 <C> 84.5 <C> 85.7 <C> 86.0 <R> <C> Baby <C> 85.7 <C> 85.7 <C> 88.0 <C> 87.7 <C> 87.3 <C> 88.3 <C> 89.0 <R> <C> Magazines <C> 92.3 <C> 92.5 <C> 91.5 <C> 93.0 <C> 93.3 <C> 91.0 <C> 90.3 <R> <C> Software <C> 84.5 <C> 86.7 <C> 87.0 <C> 88.3 <C> 89.7 <C> 87.5 <C> 91.0 <R> <C> Sports <C> 79.3 <C> 81.0 <C> 85.5 <C> 86.5 <C> 86.7 <C> 87.7 <C> 88.7 <R> <C> IMDB <C> 79.5 <C> 83.7 <C> 82.3 <C> 85.3 <C> 84.5 <C> 82.5 <C> 84.7 <R> <C> MR <C> 73.5 <C> 73.7 <C> 71.0 <C> 72.7 <C> 72.5 <C> 73.3 <C> 76.3 <R> <C> Avg <C> 81.9 <C> 83.0 <C> 85.5 <C> 86.3 <C> 86.1 <C> 85.9 <C> [BOLD] 87.4 <CAP> Table 1: The results of text classification experiment.
<R> <C> Domain bc <C> Task POS <C> Bi-LSTM 89.56 <C> PSP 94.21 <C> SSP  [BOLD] 94.67 <C> CS 94.17 <C> Our 94.61 <C> Task NER <C> Bi-LSTM 92.15 <C> PSP 94.26 <C> SSP  [BOLD] 94.50 <C> CS 93.03 <C> Our  [BOLD] 94.50 <R> <C> bc <C> CHUNK <C> 85.60 <C> 89.20 <C> 89.81 <C> 86.24 <C> [BOLD] 91.19 <C> SRL <C> 97.58 <C> 98.07 <C> 98.11 <C> 97.80 <C> [BOLD] 98.21 <R> <C> bn <C> POS <C> 92.26 <C> 94.67 <C> 95.17 <C> 92.49 <C> [BOLD] 95.28 <C> NER <C> 88.46 <C> 93.19 <C> 93.11 <C> 89.70 <C> [BOLD] 93.68 <R> <C> bn <C> CHUNK <C> 87.23 <C> 90.80 <C> 91.06 <C> 86.02 <C> [BOLD] 91.34 <C> SRL <C> 95.56 <C> [BOLD] 97.94 <C> 97.71 <C> 97.66 <C> 97.35 <R> <C> mz <C> POS <C> 79.77 <C> 91.22 <C> 92.80 <C> 84.61 <C> [BOLD] 92.88 <C> NER <C> 89.69 <C> 92.09 <C> 93.22 <C> 91.23 <C> [BOLD] 93.37 <R> <C> mz <C> CHUNK <C> 83.79 <C> 89.85 <C> 90.36 <C> 81.81 <C> [BOLD] 91.04 <C> SRL <C> 97.75 <C> 97.52 <C> 97.86 <C> 97.96 <C> [BOLD] 98.16 <R> <C> nw <C> POS <C> 94.35 <C> 95.11 <C> 94.80 <C> [BOLD] 96.23 <C> 95.02 <C> NER <C> 93.27 <C> 94.30 <C> 93.62 <C> [BOLD] 95.48 <C> 93.94 <R> <C> nw <C> CHUNK <C> 90.51 <C> 92.19 <C> 91.00 <C> 92.25 <C> [BOLD] 92.29 <C> SRL <C> 96.45 <C> 97.05 <C> 96.73 <C> 96.83 <C> [BOLD] 97.81 <R> <C> wb <C> POS <C> 89.40 <C> 92.69 <C> [BOLD] 93.24 <C> 90.08 <C> 92.71 <C> NER <C> 96.20 <C> 96.48 <C> 96.51 <C> [BOLD] 98.17 <C> 96.58 <R> <C> wb <C> CHUNK <C> 87.62 <C> 91.39 <C> 91.56 <C> 88.00 <C> [BOLD] 92.52 <C> SRL <C> 95.13 <C> 95.17 <C> 96.17 <C> 95.84 <C> [BOLD] 96.36 <R> <C> tc <C> POS <C> 92.75 <C> 93.88 <C> 94.63 <C> 94.24 <C> [BOLD] 94.99 <C> NER <C> 95.25 <C> 95.32 <C> 95.37 <C> [BOLD] 97.21 <C> 95.95 <R> <C> tc <C> CHUNK <C> 88.05 <C> 90.41 <C> 90.23 <C> 87.35 <C> [BOLD] 91.38 <C> SRL <C> 98.32 <C> 98.80 <C> 98.84 <C> 98.81 <C> [BOLD] 99.05 <R> <C> pt <C> POS <C> 97.26 <C> 97.73 <C> 97.54 <C> [BOLD] 98.81 <C> 97.68 <C> NER <C> / <C> / <C> / <C> / <C> / <R> <C> pt <C> CHUNK <C> 93.86 <C> 95.33 <C> 94.45 <C> 95.47 <C> [BOLD] 95.91 <C> SRL <C> 96.38 <C> 96.35 <C> 96.36 <C> 96.20 <C> [BOLD] 97.42 <R> <C> [EMPTY] <C> POS <C> 90.76 <C> 94.22 <C> 94.69 <C> 92.95 <C> [BOLD] 94.74 <C> NER <C> 92.50 <C> 94.27 <C> 94.39 <C> 94.14 <C> [BOLD] 94.67 <R> <C> Avg <C> CHUNK <C> 88.09 <C> 91.31 <C> 91.21 <C> 88.16 <C> [BOLD] 92.24 <C> SRL <C> 96.74 <C> 97.27 <C> 97.40 <C> 97.30 <C> [BOLD] 97.77 <CAP> Table 3: Resultant accuracy for the sequence labeling dataset, regarding every combination of (domain, problem) as tasks, accumulating to 7 * 4 - 1 = 27 separate tasks (accurately 27 tasks, since pt has no NER annotation).
<R> <C> Ratio <C> Quoted + Context Imb. F1 <C> Quoted + Context Bal. F1 <C> Quoted Imb. F1 <C> Quoted Bal. F1 <R> <C> 1:1 <C> 0.542 <C> [BOLD] 0.708 <C> 0.554 <C> [BOLD] 0.690 <R> <C> 1:20 <C> 0.670 <C> 0.574 <C> 0.620 <C> 0.518 <R> <C> 2:20 <C> 0.682 <C> 0.619 <C> 0.640 <C> 0.554 <R> <C> 3:20 <C> [BOLD] 0.684 <C> 0.654 <C> 0.646 <C> 0.585 <R> <C> 4:20 <C> 0.678 <C> 0.632 <C> [BOLD] 0.650 <C> 0.640 <R> <C> 5:20 <C> 0.668 <C> 0.626 <C> 0.645 <C> 0.582 <R> <C> 10:20 <C> 0.672 <C> 0.656 <C> 0.641 <C> 0.640 <R> <C> 15:20 <C> 0.665 <C> 0.641 <C> 0.641 <C> 0.593 <R> <C> 20:20 <C> 0.674 <C> 0.621 <C> 0.645 <C> 0.597 <CAP> Table 4: The impact of different train-set positive:negative ratios. All the models are BERTL. The first row is based on the balanced dataset, and the rest are based on the imbalanced dataset with different oversampling ratios. Model selection again used the procedure in Appendix B.
<R> <C> [BOLD] Ref. <C> [BOLD] Base <C> [BOLD] Sec_Att <C> [BOLD] Agg <C> [BOLD] Agg_VAE <R> <C> SS <C> 30.0 <C> 72.5 <C> 77.0 <C> 78.0 <R> <C> US <C> 30.0 <C> 59.0 <C> 66.0 <C> 70.0 <CAP> Table 2: MUSHRA medians for different PT approaches. Labels for reference: Same speaker (SS), Unseen speaker (US)
<R> <C> Corpus <C> Models ELMo <C> Models ELMo5.5B <C> Models TransformerXL <C> Models BERT-large <C> Models RoBERTa [ITALIC] LARGE <C> Models K-Apdater <R> <C> LAMA-Google-RE <C> 2.2 <C> 3.1 <C> 1.8 <C> 12.1 <C> 4.8 <C> 7.0 <R> <C> LAMA-UHN-Google-RE <C> 2.3 <C> 2.7 <C> 1.3 <C> 6.5 <C> 2.5 <C> 3.7 <R> <C> LAMA-T-REx <C> 0.2 <C> 0.3 <C> 19.5 <C> 33.9 <C> 27.1 <C> 29.1 <R> <C> LAMA-UHN-T-REx <C> 0.2 <C> 0.2 <C> 12.6 <C> 26.2 <C> 20.1 <C> 23.0 <CAP> Table 5: P@1 on LAMA and LAMA-UHN across Google-RE and T-REx corpora.
<R> <C> Method <C> Embeddings <C> En→De <C> De→En <C> En→It <C> It→En <C> En→Pt <C> Pt→En <R> <C> nBOW <C> conceptNet <C> 39.4 <C> 41.5 <C> 54.3 <C> 46.9 <C> 40.4 <C> 24.3 <R> <C> nBOW <C> muse <C> 42.2 <C> 32.2 <C> 38.3 <C> 28.8 <C> 42.5 <C> 34.0 <R> <C> nBOW <C> iclr <C> 48.6 <C> 35.8 <C> 57.4 <C> 44.6 <C> 60.2 <C> 49.1 <R> <C> EMD <C> conceptNet <C> 86.4 <C> 82.9 <C> 90.7 <C> 90.4 <C> 78.4 <C> 81.2 <R> <C> EMD <C> muse <C> 82.7 <C> 86.6 <C> 90.4 <C> 91.4 <C> 90.6 <C> 92.9 <R> <C> EMD <C> iclr <C> 80.7 <C> 86.3 <C> 89.6 <C> 91.9 <C> 91.9 <C> 93.8 <R> <C> sEMD <C> conceptNet <C> [BOLD] 86.8 <C> 84.1 <C> [BOLD] 91.3 <C> 90.8 <C> 79.3 <C> 81.1 <R> <C> sEMD <C> muse <C> 83.9 <C> [BOLD] 87.1 <C> 91.1 <C> 91.7 <C> 91.3 <C> 93.4 <R> <C> sEMD <C> iclr <C> 82.0 <C> [BOLD] 87.1 <C> 90.3 <C> [BOLD] 92.2 <C> [BOLD] 92.3 <C> [BOLD] 94.2 <CAP> Table 1: Europarl Sentence Retrieval; P@1 scores.
<R> <C> Method <C> Embeddings <C> En→Fr <C> En→It <C> En→De <C> En→Es <C> Fr→En <C> It→En <C> De→En <C> Es→En <C> Rank <R> <C> EMD UB <C> cNet <C> 90.5 <C> 81.2 <C> 90.1 <C> 89.0 <C> 91.5 <C> 91.5 <C> 91.5 <C> 91.5 <C> [EMPTY] <R> <C> EMD <C> cNet <C> [BOLD] 86.5 <C> [BOLD] 74.2 <C> 86.9 <C> 76.3 <C> 79.0 <C> 70.2 <C> 76.6 <C> 70.0 <C> 2.6 <R> <C> EMD <C> muse <C> 81.1 <C> 63.5 <C> 85.0 <C> 66.4 <C> 70.3 <C> 48.7 <C> 42.8 <C> 49.9 <C> 8.6 <R> <C> [EMPTY] <C> iclr <C> 78.7 <C> 59.6 <C> 81.9 <C> 51.5 <C> 66.2 <C> 63.5 <C> 70.2 <C> 40.9 <C> 9.3 <R> <C> sEMD <C> cNet <C> 84.8 <C> 71.7 <C> 86.0 <C> 73.4 <C> 76.7 <C> 72.3 <C> 76.0 <C> 72.3 <C> 3.6 <R> <C> sEMD <C> muse <C> 77.2 <C> 63.6 <C> 82.4 <C> 59.9 <C> 63.5 <C> 49.6 <C> 27.9 <C> 42.5 <C> 10.0 <R> <C> [EMPTY] <C> iclr <C> 78.0 <C> 60.8 <C> 81.3 <C> 52.0 <C> 65.4 <C> 63.0 <C> 70.3 <C> 37.4 <C> 9.9 <R> <C> nBOW <C> cNet <C> 74.5 <C> 65.7 <C> 80.7 <C> 73.5 <C> 76.3 <C> 69.0 <C> 77.9 <C> 67.8 <C> 6.7 <R> <C> nBOW <C> muse <C> 77.1 <C> 63.0 <C> 77.3 <C> 67.4 <C> 73.1 <C> 63.1 <C> 65.2 <C> 69.2 <C> 8.6 <R> <C> [EMPTY] <C> iclr <C> 79.2 <C> 66.0 <C> 81.2 <C> 28.7 <C> 71.2 <C> 64.5 <C> 55.6 <C> 27.3 <C> 9.1 <R> <C> MLP <C> LASER <C> 78.0 <C> 70.2 <C> 86.2 <C> [BOLD] 79.3 <C> [BOLD] 80.1 <C> [BOLD] 74.1 <C> 80.7 <C> 69.6 <C> 2.8 <R> <C> ADAN <C> cNet <C> 82.3 <C> 70.3 <C> 86.0 <C> 75.6 <C> 80.0 <C> 72.3 <C> [BOLD] 80.8 <C> [BOLD] 76.4 <C> [BOLD] 2.5 <R> <C> ADAN <C> muse <C> 78.0 <C> 66.9 <C> [BOLD] 87.7 <C> 74.6 <C> 72.0 <C> 65.6 <C> 77.6 <C> 64.5 <C> 5.3 <R> <C> [EMPTY] <C> iclr <C> 59.8 <C> 46.9 <C> 69.3 <C> 57.7 <C> 61.5 <C> 49.5 <C> 58.8 <C> 43.0 <C> 11.6 <CAP> Table 2: Reuters Classification IDF. UB: upper bound.
<R> <C> [BOLD] Dataset <C> [BOLD] Best Test Set  [BOLD] Accuracy (%) <C> [BOLD] Best Test Set  [BOLD] Accuracy (%) <C> [BOLD] Best Reported  [BOLD] Accuracy (%) <C> [BOLD] Lowest Reported  [BOLD] Non-zero Accuracy (%) <R> <C> [BOLD] Dataset <C> [BOLD] w/o term frequency <C> [BOLD] w/ term frquency <C> [BOLD] Accuracy (%) <C> [BOLD] Non-zero Accuracy (%) <R> <C> AAAC Problem A <C> 61.54 <C> 69.23 <C> [BOLD] 84.62 <C> 15.38 <R> <C> AAAC Problem B <C> [BOLD] 61.54 <C> [BOLD] 61.54 <C> 53.85 <C> 7.69 <R> <C> AAAC Problem C <C> [BOLD] 100.00 <C> [BOLD] 100.00 <C> [BOLD] 100.00 <C> 33.33 <R> <C> AAAC Problem G <C> [BOLD] 100.00 <C> [BOLD] 100.00 <C> 75.00 <C> 25.00 <R> <C> AAAC Problem H <C> [BOLD] 100.00 <C> [BOLD] 100.00 <C> [BOLD] 100.00 <C> 33.33 <R> <C> AAAC Problem I <C> [BOLD] 100.00 <C> [BOLD] 100.00 <C> [BOLD] 100.00 <C> 25.00 <R> <C> AAAC Problem J <C> [BOLD] 100.00 <C> [BOLD] 100.00 <C> [BOLD] 100.00 <C> 50.00 <R> <C> AAAC Problem K <C> [BOLD] 100.00 <C> [BOLD] 100.00 <C> 75.00 <C> 25.00 <R> <C> AAAC Problem L <C> [BOLD] 100.00 <C> [BOLD] 100.00 <C> [BOLD] 100.00 <C> 25.00 <R> <C> AAAC Problem M <C> 45.83 <C> 54.17 <C> [BOLD] 87.50 <C> 16.67 <R> <C> Average across ten problems <C> 86.89 <C> [BOLD] 88.49 <C> 87.60 <C> 25.64 <R> <C> PAN12 Problem A <C> [BOLD] 100.00 <C> [BOLD] 100.00 <C> [BOLD] 100.00 <C> 33.33 <R> <C> PAN12 Problem C <C> 75.00 <C> 75.00 <C> [BOLD] 100.00 <C> 12.50 <R> <C> PAN12 Problem I <C> [BOLD] 92.86 <C> 85.71 <C> [BOLD] 92.86 <C> 35.71 <R> <C> Average across three problems <C> 89.29 <C> 86.90 <C> [BOLD] 97.62 <C> 27.18 <CAP> Table 9: Classification performance of word network features on competition datasets. Along with our best test set accuracy values (second and third columns), we report here the best and lowest reported accuracy values on the test set. Second column shows our results using pure word network features, and third column shows the results using word network features + raw term frequency of words. Best results for different problems (i.e., on different rows) are boldfaced.
<R> <C> Feature Excluded <C> Precision <C> Recall <C> Accuracy <C> F1 <R> <C> None <C> 0.7022 <C> 0.7718 <C> 0.7195 <C> 0.7313 <R> <C> Special Words <C> 0.6213 <C> 0.7758 <C> 0.6501 <C> 0.6877 <R> <C> Action Words <C> 0.6614 <C> [BOLD] 0.8061 <C> 0.6960 <C> 0.7246 <R> <C> [BOLD] Now Words <C> [BOLD] 0.7167 <C> 0.8033 <C> [BOLD] 0.7410 <C> [BOLD] 0.7547 <R> <C> POS <C> 0.6986 <C> 0.6990 <C> 0.6980 <C> 0.6961 <R> <C> Syntactic <C> 0.6876 <C> 0.7572 <C> 0.7046 <C> 0.7173 <CAP> Table 4. Results of ablation experiment using SVM.
<R> <C> Strategy <C> Precision <C> Recall <C> Accuracy <C> F1 <R> <C> w/o rules <C> 0.7167 <C> [BOLD] 0.8033 <C> [BOLD] 0.7410 <C> [BOLD] 0.7547 <R> <C> w/ rules <C> [BOLD] 0.7544 <C> 0.6337 <C> 0.7114 <C> 0.6859 <CAP> Table 5. Results with or without additional rules.
<R> <C> [EMPTY] <C> [BOLD] Classifier <C> [BOLD] Pos <C> [BOLD] Neg <C> [BOLD] Macro <R> <C> [EMPTY] <C> [EMPTY] <C> [BOLD] F1 <C> [BOLD] F1 <C> [BOLD] F <R> <C> 1 <C> [BOLD] SVM <C> 0.66 <C> 0.60 <C> 0.64 <R> <C> 2 <C> [BOLD] NRC <C> 0.58 <C> 0.69 <C> 0.64 <R> <C> 3 <C> [BOLD] Stanford <C> 0.54 <C> 0.73 <C> 0.67 <R> <C> 4 <C> [BOLD] AutoSlog (ASlog) <C> 0.11 <C> 0.68 <C> 0.53 <R> <C> 5 <C> [BOLD] Retrained Stanford <C> 0.53 <C> 0.73 <C> 0.67 <R> <C> 6 <C> [BOLD] NRC, ASlog <C> 0.60 <C> 0.78 <C> 0.71 <R> <C> 7 <C> [BOLD] Stanford, ASlog <C> 0.55 <C> 0.76 <C> 0.70 <R> <C> 8 <C> [BOLD] NRC, ASlog, Stanford <C> 0.64 <C> [BOLD] 0.79 <C> 0.74 <R> <C> 9 <C> [BOLD] NRC, ASlog, SVM <C> [BOLD] 0.70 <C> 0.78 <C> [BOLD] 0.75 <CAP> Table 4: Test Set Results
<R> <C> Label <C> [BOLD] OntoNotes  [BOLD] % <C> [BOLD] OntoNotes  [BOLD] F-1 <C> [BOLD] Wiki(gold)  [BOLD] % <C> [BOLD] Wiki(gold)  [BOLD] Prec. <C> [BOLD] Wiki(gold)  [BOLD] Rec. <C> [BOLD] Wiki(gold)  [BOLD] F-1 <R> <C> [BOLD] person <C> 14 <C> 90 <C> 23 <C> 79 <C> 59 <C> 66 <R> <C> [BOLD] location <C> 14 <C> 93 <C> 37 <C> 62 <C> 47 <C> 54 <R> <C> [BOLD] organization <C> 24 <C> 85 <C> 26 <C> 45 <C> 16 <C> 23 <R> <C> [BOLD] event <C> 1 <C> 70 <C> 2 <C> 81 <C> 17 <C> 28 <R> <C> [BOLD] product <C> 1 <C> 56 <C> 2 <C> 44 <C> 4 <C> 8 <R> <C> [BOLD] building <C> 1 <C> 65 <C> 4 <C> 81 <C> 17 <C> 11 <R> <C> [BOLD] art <C> 2 <C> 54 <C> 0 <C> 0 <C> 0 <C> 0 <CAP> Table 2: Performance of our model from the NER classifier evaluated on OntoNotes, and the 112 subclass Wikidata linking step evaluated on Wiki(gold). The first column denotes the percentage breakdown per class type. The precision, recall, and F-1 scores are shown for Wiki(gold). For OntoNotes the precision and recall are identical for each category, therefore we only quote F-1. All values are quoted as a percentage and rounded to the nearest whole number. Since the table only shows 7 categories, the percentages will not sum to 100.
<R> <C> [BOLD] Model <C> [BOLD] MRR@10 Dev <C> [BOLD] MRR@10 Eval <R> <C> [BOLD] Other approaches <C> [BOLD] Other approaches <C> [BOLD] Other approaches <R> <C> BM25 <C> 0.165 <C> 0.167 <R> <C> Single CKNRM (Dai et al.,  2018 ) model <C> 0.247 <C> 0.247 <R> <C> Ensemble of 8 CKNRM (Dai et al.,  2018 ) models <C> 0.290 <C> 0.271 <R> <C> IRNet (a proprietary deep neural model) <C> 0.278 <C> 0.281 <R> <C> BERT (Nogueira and Cho,  2019 ) <C> 0.365 <C> 0.359 <R> <C> [BOLD] Duet variants <C> [BOLD] Duet variants <C> [BOLD] Duet variants <R> <C> Single Duet v2 w/o IDF weighting for interaction matrix <C> 0.163 <C> - <R> <C> Single Duet v2 w/ Tanh non-linearity (instead of ReLU) <C> 0.179 <C> - <R> <C> Single Duet v2 w/o MLP to combine local and distributed scores <C> 0.208 <C> - <R> <C> Single Duet v2 model <C> 0.243 <C> 0.245 <R> <C> Ensemble of 8 Duet v2 models <C> 0.252 <C> 0.253 <CAP> Table 1: Comparison of the different Duet variants and other state-of-the-art approaches from the public MS MARCO leaderboard. The update Duet model—referred to as Duet v2—benefits significantly from the modifications proposed in this paper.
<R> <C> EER(%) <C> [ITALIC] ϵ=0.1 FGSM <C> [ITALIC] ϵ=0.1 PGD <C> [ITALIC] ϵ=1 FGSM <C> [ITALIC] ϵ=1 PGD <C> [ITALIC] ϵ=5 FGSM <C> [ITALIC] ϵ=5 PGD <R> <C> LCNN-big <C> 4.691 <C> [BOLD] 6.256 <C> 36.504 <C> [BOLD] 54.382 <C> 48.457 <C> [BOLD] 93.119 <R> <C> LCNN-small <C> 7.613 <C> [BOLD] 17.419 <C> 34.670 <C> [BOLD] 73.649 <C> 48.375 <C> [BOLD] 89.845 <R> <C> SENet12 <C> 7.737 <C> [BOLD] 13.896 <C> 24.936 <C> [BOLD] 62.681 <C> 51.626 <C> [BOLD] 87.220 <CAP> Table 5: White-box attack performance of the FGSM and the PGD method.
<R> <C> Layer Name <C> Neurons <C> Activation Func <R> <C> Hidden 1 <C> 200 <C> ReLU <R> <C> Hidden 2 <C> 500 <C> ReLU <R> <C> Hidden 3 <C> 500 <C> ReLU <R> <C> Hidden 4 <C> 450 <C> ReLU <R> <C> Hidden 5 <C> 400 <C> ReLU <R> <C> Hidden 6 <C> 400 <C> ReLU <R> <C> Hidden 7 <C> 350 <C> ReLU <R> <C> Hidden 8 <C> 300 <C> ReLU <R> <C> Hidden 9 <C> 300 <C> ReLU <R> <C> Hidden 10 <C> 250 <C> ReLU <R> <C> Hidden 11 <C> 200 <C> ReLU <R> <C> Hidden 12 <C> 200 <C> ReLU <R> <C> Hidden 13 <C> 150 <C> ReLU <R> <C> Hidden 14 <C> 100 <C> ReLU <R> <C> Hidden 15 <C> 100 <C> ReLU <R> <C> Hidden 16 <C> 50 <C> ReLU <R> <C> Hidden 17 <C> 25 <C> ReLU <R> <C> Output <C> 15 <C> Softmax <R> <C> Trainable Parameters: 1,501,115 <C> Trainable Parameters: 1,501,115 <C> Trainable Parameters: 1,501,115 <CAP> Tabelle 11: FFNN basic model structure
<R> <C> Models <C> F1 <R> <C> berant-EtAl:2013:EMNLP <C> 35.7 <R> <C> berant2014semantic <C> 39.9 <R> <C> berant2015imitation <C> 49.7 <R> <C> reddy2016transforming <C> 50.3 <R> <C> yao2014information <C> 33.0 <R> <C> bast2015more <C> 49.4 <R> <C> bordesquestion <C> 39.2 <R> <C> dong2015question <C> 40.8 <R> <C> yih2015semantic <C> 52.5 <R> <C> xu2016question <C> 53.3 <R> <C> cheng2017learning <C> 49.4 <R> <C> npr <C> 50.1 <R> <C> + granker <C> 50.2 <R> <C> + lexicon encoding on granker <C> 51.7 <R> <C> + lexicon encoding on parser and granker <C> 52.5 <CAP> Table 2: WebQuestions results.
<R> <C> # annotations <C> Δ score <C> Δ rank <C> [ITALIC] ρ <C> [ITALIC] r <R> <C> 3 [ITALIC] N <C> 0.11 <C> 397 <C> 0.85 <C> 0.85 <R> <C> 5 [ITALIC] N <C> 0.10 <C> 363 <C> 0.87 <C> 0.88 <R> <C> 20 [ITALIC] N <C> 0.08 <C> 264 <C> 0.93 <C> 0.93 <CAP> Table 1: Differences in final outcomes of BWS and RS, for different total numbers of annotations.
<R> <C> [ITALIC] t\scriptsize rank= <C> (a) Mean hit rate 80 <C> (a) Mean hit rate 100 <C> (b) Mean cosine sim. 80 <C> (b) Mean cosine sim. 100 <C> (c) % with prototypes 80 <C> (c) % with prototypes 100 <C> (d) Mean # of prototypes 80 <C> (d) Mean # of prototypes 100 <R> <C> [ITALIC] t\scriptsize evd=4 <C> 26% <C> 22% <C> 0.39 <C> 0.39 <C> 8.93% <C> 9.52% <C> 4.20 <C> 4.16 <R> <C> [ITALIC] t\scriptsize evd=6 <C> 31% <C> 26% <C> 0.43 <C> 0.43 <C> 5.13% <C> 5.47% <C> 3.29 <C> 3.30 <R> <C> [ITALIC] t\scriptsize evd=10 <C> 36% <C> 31% <C> 0.45 <C> 0.45 <C> 2.91% <C> 3.14% <C> 2.25 <C> 2.29 <CAP> Table 4: Overview of the influence of the hyperparameters on prototype extraction.
<R> <C> [EMPTY] <C> (a) No comp. splitting Splits <C> (a) No comp. splitting BLEU <C> (a) No comp. splitting MTR <C> (b) OOV only Splits <C> (b) OOV only BLEU <C> (b) OOV only MTR <C> (c) Rare: c(w)<20 Splits <C> (c) Rare: c(w)<20 BLEU <C> (c) Rare: c(w)<20 MTR <C> (d) All words Splits <C> (d) All words BLEU <C> (d) All words MTR <R> <C> Moses splitter <C> 0 <C> 17.6 <C> 25.5 <C> 226 <C> 17.6 <C> 25.7A <C> 231 <C> 17.6 <C> 25.7 <C> 244 <C> 17.9 <C> 25.8A <R> <C> This work <C> 0 <C> 17.6 <C> 25.5 <C> 317 <C> 17.6 <C> 25.8A <C> 744 <C> [BOLD] 18.2ABC <C> [BOLD] 26.1ABC <C> 1616 <C> 17.7 <C> 26.3A <CAP> Table 8: Translation results for various integration methods.
<R> <C> [BOLD] Rate <C> [BOLD] BLEU-4 0\% <C> [BOLD] BLEU-4 25\% <C> [BOLD] BLEU-4 50\% <C> [BOLD] BLEU-4 75\% <C> [BOLD] BLEU-4 100\% <C> [BOLD] Accuracy (%) 0\% <C> [BOLD] Accuracy (%) 25\% <C> [BOLD] Accuracy (%) 50\% <C> [BOLD] Accuracy (%) 75\% <C> [BOLD] Accuracy (%) 100\% <R> <C> Noun Masking <C> Noun Masking <C> Noun Masking <C> Noun Masking <C> Noun Masking <C> Noun Masking <C> Noun Masking <C> Noun Masking <C> Noun Masking <C> Noun Masking <C> Noun Masking <R> <C> NMT <C> 26.9 <C> 20.2 <C> 13.0 <C> 8.5 <C> 4.1 <C> 70.2 <C> 53.7 <C> 35.4 <C> 15.6 <C> 10.1 <R> <C> VMT <C> 29.1 <C> 24.7 <C> 19.3 <C> 16.9 <C> 14.3 <C> 76.4 <C> 65.6 <C> 50.8 <C> 43.2 <C> 39.7 <R> <C> Verb Masking <C> Verb Masking <C> Verb Masking <C> Verb Masking <C> Verb Masking <C> Verb Masking <C> Verb Masking <C> Verb Masking <C> Verb Masking <C> Verb Masking <C> Verb Masking <R> <C> NMT <C> 26.9 <C> 23.3 <C> 15.4 <C> 11.6 <C> 7.2 <C> 65.1 <C> 57.4 <C> 40.9 <C> 33.6 <C> 19.8 <R> <C> VMT <C> 29.1 <C> 26.8 <C> 22.0 <C> 19.3 <C> 16.5 <C> 70.4 <C> 63.6 <C> 54.2 <C> 48.7 <C> 40.5 <CAP> Table 6: Video-guided machine translation on English\rightarrowChinese with different noun/verb masking rates. We evaluate the results using the BLEU-4 score and noun/verb recovery accuracy.
<R> <C> Identity Attribute <C> Emotion Categories Fear <C> Emotion Categories Anger <C> Emotion Categories Joy <C> Emotion Categories Sadness <C> Emotion Categories Neutral <C> Total <R> <C> Male <C> 1050 <C> 1050 <C> 1050 <C> 1050 <C> 120 <C> 4320 <R> <C> Female <C> 1050 <C> 1050 <C> 1050 <C> 1050 <C> 120 <C> 4320 <R> <C> African-American <C> 700 <C> 700 <C> 700 <C> 700 <C> 120 <C> 2920 <R> <C> Caucasian <C> 700 <C> 700 <C> 700 <C> 700 <C> 120 <C> 2920 <R> <C> None <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 2920 <C> 2920 <R> <C> (SS-1): Biased Subsample with (Female, Fear)↑, (Male, Anger)↑ <C> (SS-1): Biased Subsample with (Female, Fear)↑, (Male, Anger)↑ <C> (SS-1): Biased Subsample with (Female, Fear)↑, (Male, Anger)↑ <C> (SS-1): Biased Subsample with (Female, Fear)↑, (Male, Anger)↑ <C> (SS-1): Biased Subsample with (Female, Fear)↑, (Male, Anger)↑ <C> (SS-1): Biased Subsample with (Female, Fear)↑, (Male, Anger)↑ <C> (SS-1): Biased Subsample with (Female, Fear)↑, (Male, Anger)↑ <R> <C> Male <C> 500 <C> 1050 <C> 1050 <C> 1050 <C> 120 <C> 3770 <R> <C> Female <C> 1050 <C> 500 <C> 1050 <C> 1050 <C> 120 <C> 3770 <R> <C> African-American <C> 450 <C> 550 <C> 700 <C> 700 <C> 120 <C> 2520 <R> <C> Caucasian <C> 550 <C> 500 <C> 700 <C> 700 <C> 120 <C> 2570 <R> <C> None <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 1450 <C> 1450 <R> <C> (SS-2): Biased Subsample with (Caucasian, Fear)↑ <C> (SS-2): Biased Subsample with (Caucasian, Fear)↑ <C> (SS-2): Biased Subsample with (Caucasian, Fear)↑ <C> (SS-2): Biased Subsample with (Caucasian, Fear)↑ <C> (SS-2): Biased Subsample with (Caucasian, Fear)↑ <C> (SS-2): Biased Subsample with (Caucasian, Fear)↑ <C> (SS-2): Biased Subsample with (Caucasian, Fear)↑ <R> <C> Male <C> 850 <C> 1050 <C> 1050 <C> 1050 <C> 120 <C> 3770 <R> <C> Female <C> 850 <C> 1050 <C> 1050 <C> 1050 <C> 120 <C> 3770 <R> <C> African-American <C> 300 <C> 700 <C> 700 <C> 700 <C> 120 <C> 2520 <R> <C> Caucasian <C> 700 <C> 700 <C> 700 <C> 700 <C> 120 <C> 2570 <R> <C> None <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 1450 <C> 1450 <CAP> Table 2: Distribution of emotion categories (primary task labels) into social identity attributes (gender and ethnicity).
<R> <C> Method Name <C> Sampled <C> Acc ( [ITALIC] A) <C> Fear vs. Gender  [ITALIC] αfemale <C> Fear vs. Gender Fairness <C> Fear vs. Gender  [ITALIC] γ <C> Fear vs. Race  [ITALIC] αwhite <C> Fear vs. Race Fairness <C> Fear vs. Race  [ITALIC] γ <C> Anger vs. Gender  [ITALIC] αmale <C> Anger vs. Gender Fairness <C> Anger vs. Gender  [ITALIC] γ <R> <C> ‘Bias-Agnstc-L2’ <C> SS-1 <C> 0.8337 <C> 1.0000 <C> 0.0000 <C> 0.0000 <C> 0.3076 <C> 0.2103 <C> 0.1679 <C> 0.6187 <C> 0.2169 <C> 0.1721 <R> <C> ‘Bias-Agnstc-L2’ <C> SS-2 <C> 0.7930 <C> 0.9800 <C> 0.0194 <C> 0.0189 <C> 0.2777 <C> 0.1928 <C> 0.1551 <C> 0.6919 <C> 0.3081 <C> 0.2218 <R> <C> ‘Bias-Agnstc-L2’ <C> None <C> 0.8620 <C> 0.7914 <C> 0.1650 <C> 0.1384 <C> 0.2436 <C> 0.1842 <C> 0.1517 <C> 0.3806 <C> 0.2357 <C> 0.1850 <R> <C> ‘Bias-Agnstc’ <C> None <C> 0.8237 <C> 0.7971 <C> 0.1617 <C> 0.1351 <C> 0.3659 <C> 0.2320 <C> 0.1810 <C> 0.6884 <C> 0.2145 <C> 0.1701 <R> <C> ‘Bias-Agnstc-DWE’ <C> None <C> 0.7380 <C> 1.0000 <C> 0.0000 <C> 0.0000 <C> 0.2636 <C> 0.1914 <C> 0.1521 <C> 0.7598 <C> 0.1825 <C> 0.1463 <R> <C> ‘Bias-Agnstc-Gndr-Ftr’ <C> None <C> 0.7900 <C> 1.0000 <C> 0.0000 <C> 0.0000 <C> 0.1667 <C> 0.1389 <C> 0.1181 <C> 0.6098 <C> 0.2425 <C> 0.1855 <R> <C> ‘Bias-Agnstc-Race-Ftr’ <C> None <C> 0.7800 <C> 1.0000 <C> 0.0000 <C> 0.0000 <C> 0.1877 <C> 0.1524 <C> 0.1274 <C> 0.6740 <C> 0.2197 <C> 0.1714 <R> <C> ‘Bias-Awr-Gndr’ <C> None <C> [BOLD] 0.9430 <C> [BOLD] 0.5023 <C> [BOLD] 0.2499 <C> [BOLD] 0.1986 <C> 0.2036 <C> 0.1540 <C> 0.1320 <C> [BOLD] 0.5817 <C> [BOLD] 0.2433 <C> [BOLD] 0.1935 <R> <C> ‘Bias-Awr-Race’ <C> None <C> 0.9400 <C> 0.7914 <C> 0.1650 <C> 0.1403 <C> 0.3734 <C> 0.2339 <C> [BOLD] 0.1873 <C> 0.6194 <C> 0.2357 <C> 0.1884 <R> <C> ‘Bias-Awr-Joint’ <C> None <C> 0.9100 <C> 0.5582 <C> 0.1955 <C> 0.1609 <C> [BOLD] 0.3774 <C> [BOLD] 0.2349 <C> 0.1867 <C> 0.5940 <C> 0.2411 <C> 0.1906 <CAP> Table 3: Comparison of emotion classification correctness and fairness (along with a harmonic mean of the two denoted by γ) for bias-agnostic and bias-aware approaches. Fairness (lack of bias) is measured by associating two emotions classes (fear and anger) with gender (male/female) and race (black/white). Results show that the bias-aware models output more socially acceptable responses, specifically, a) not every woman is fearful, b) not all Caucasians are phobic, and c) not all men are angry, as evident respectively from the fairness (F) values of ‘Fear vs. Gender’, ‘Fear vs. Race’, and ‘Anger vs. Gender’.
<R> <C> Distribution <C> Word2vec Male <C> Word2vec Female <C> Debiased Male <C> Debiased Female <R> <C> Male Stereotypes <C> 0.7545 <C> 0.2454 <C> 0.7437 <C> 0.2562 <R> <C> Female Stereotypes <C> 0.7151 <C> 0.2848 <C> 0.6959 <C> 0.3040 <CAP> Table 2: Percentage of gendered tokens in the follow-up distribution from a language model after trigger male/female stereotypical profession is provided as a starting token. We only examine following distributions which contained gender-specific terms and omit gender-neutral distributions.
<R> <C> [BOLD] MSA-EGY Baseline Data set <C> [BOLD] MSA-EGY Baseline MADAMIRA-MSA <C> [BOLD] MSA-EGY Baseline MADAMIRA-EGY <R> <C> ARZTest <C> 77.23 <C> 72.22 <R> <C> [BOLD] SPA-ENG Baseline <C> [BOLD] SPA-ENG Baseline <C> [BOLD] SPA-ENG Baseline <R> <C> Dataset <C> TreeTagger SPA <C> TreeTagger ENG <R> <C> Spanglish <C> 44.61 <C> 75.87 <R> <C> Bangor <C> 45.95 <C> 64.05 <CAP> Table 3: POS tagging accuracy (%) for monolingual baseline taggers
<R> <C> Method <C> Dev Acc <C> Test Acc <R> <C> Siamese-CNN <C> - <C> 79.60 <R> <C> Multi-Perspective CNN <C> - <C> 81.38 <R> <C> Siamese-LSTM <C> - <C> 82.58 <R> <C> Multi-Perspective-LSTM <C> - <C> 83.21 <R> <C> L.D.C <C> - <C> 85.55 <R> <C> BiMPM <C> 88.69 <C> 88.17 <R> <C> FFNNword <C> 85.07 <C> 84.35 <R> <C> FFNNchar <C> 86.01 <C> 85.06 <R> <C> DecAttword <C> 86.04 <C> 85.27 <R> <C> DecAttglove <C> 87.42 <C> 86.52 <R> <C> DecAttchar <C> 87.78 <C> 86.84 <R> <C> DecAttparalex−char <C> 87.80 <C> 87.77 <R> <C> pt-DecAttword <C> 88.44 <C> 87.54 <R> <C> pt-DecAttchar <C> [BOLD] 88.89 <C> [BOLD] 88.40 <CAP> Table 2: Results on the Quora development and test sets in terms of accuracy. The first six rows are taken from (Wang et al., 2017).
<R> <C> Word embedding <C> Similarity task WS353 <C> Similarity task Similarity <C> Similarity task Relatedness <C> Similarity task M. Turk <C> Similarity task MEN <C> Analogy task 3CosAdd <C> Analogy task 3CosMul <R> <C> GloVe <C> 48.7 <C> 50.9 <C> 53.7 <C> 54.1 <C> 17.6 <C> 32.1 <C> 28.5 <R> <C> SGNS <C> 67.2 <C> 70.3 <C> 67.9 <C> [BOLD] 59.9∗ <C> [BOLD] 25.1∗ <C> 30.4 <C> 27.8 <R> <C> PU-learning <C> [BOLD] 68.3∗ <C> [BOLD] 71.8∗ <C> [BOLD] 68.2∗ <C> 57.0 <C> 22.7 <C> [BOLD] 32.6∗ <C> [BOLD] 32.3∗ <CAP> Table 2: Performance of the best SGNS, GloVe, PU-Learning models, trained on the text8 corpus. Results show that our proposed model is better than SGNS and GloVe. Star indicates it is significantly better than the second best algorithm in the same column according to Wilcoxon signed-rank test. (p<0.05)
<R> <C> Dutch (nl) Word embedding <C> Similarity task WS353 <C> Similarity task Similarity <C> Similarity task Relatedness <C> Similarity task M. Turk <C> Similarity task MEN <C> Analogy task 3CosAdd <C> Analogy task 3CosMul <R> <C> GloVe <C> 35.4 <C> 35.0 <C> 41.7 <C> 44.3 <C> 11 <C> 21.2 <C> 20.2 <R> <C> SGNS <C> 51.9 <C> 52.9 <C> 53.5 <C> [BOLD] 49.8∗ <C> 15.4 <C> 22.1 <C> 23.6 <R> <C> PU-learning <C> [BOLD] 53.7∗ <C> [BOLD] 53.4∗ <C> [BOLD] 55.1∗ <C> 46.7 <C> [BOLD] 16.4∗ <C> [BOLD] 23.5∗ <C> [BOLD] 24.7∗ <R> <C> Danish (da) <C> Similarity task <C> Similarity task <C> Similarity task <C> Similarity task <C> Similarity task <C> Analogy task <C> Analogy task <R> <C> Word embedding <C> WS353 <C> Similarity <C> Relatedness <C> M. Turk <C> MEN <C> 3CosAdd <C> 3CosMul <R> <C> GloVe <C> 25.7 <C> 18.4 <C> 40.3 <C> 49.0 <C> 16.4 <C> [BOLD] 25.8∗ <C> [BOLD] 24.3∗ <R> <C> SGNS <C> 49.7 <C> 47.1 <C> 52.1 <C> 51.5 <C> 22.4 <C> 22.0 <C> 21.2 <R> <C> PU-learning <C> [BOLD] 53.5∗ <C> [BOLD] 49.5∗ <C> [BOLD] 59.3∗ <C> [BOLD] 51.7∗ <C> [BOLD] 22.7∗ <C> 22.6 <C> 22.8 <R> <C> Czech (cs) <C> Similarity task <C> Similarity task <C> Similarity task <C> Similarity task <C> Similarity task <C> Analogy task <C> Analogy task <R> <C> Word embedding <C> WS353 <C> Similarity <C> Relatedness <C> M. Turk <C> MEN <C> 3CosAdd <C> 3CosMul <R> <C> GloVe <C> 34.3 <C> 23.2 <C> 48.9 <C> 36.5 <C> 16.2 <C> 8.9 <C> 8.6 <R> <C> SGNS <C> 51.4 <C> 42.7 <C> 61.1 <C> 44.2 <C> 21.3 <C> [BOLD] 10.4∗ <C> 9.8 <R> <C> PU-learning <C> [BOLD] 54.0∗ <C> [BOLD] 45.4∗ <C> [BOLD] 65.3∗ <C> [BOLD] 46.2∗ <C> [BOLD] 21.7∗ <C> 9.9 <C> [BOLD] 10.1∗ <R> <C> English (en) <C> Similarity task <C> Similarity task <C> Similarity task <C> Similarity task <C> Similarity task <C> Analogy task <C> Analogy task <R> <C> Word embedding <C> WS353 <C> Similarity <C> Relatedness <C> M. Turk <C> MEN <C> 3CosAdd <C> 3CosMul <R> <C> GloVe <C> 47.9 <C> 52.1 <C> 49.5 <C> 58.8 <C> 19.1 <C> 34.3 <C> 32.6 <R> <C> SGNS <C> 65.7 <C> [BOLD] 67.1∗ <C> 66.5 <C> [BOLD] 62.8∗ <C> [BOLD] 26.1∗ <C> 31.2 <C> 27.4 <R> <C> PU-learning <C> [BOLD] 67.0∗ <C> 66.7 <C> [BOLD] 69.6∗ <C> 59.4 <C> 22.4 <C> [BOLD] 39.2∗ <C> [BOLD] 38.8∗ <CAP> Table 4: Performance of SGNS, GloVe, and the proposed PU-Learning model in four different languages. Results show that the proposed PU-Learning model outperforms SGNS and GloVe in most cases when the size of corpus is relatively small (around 50 million tokens). Star indicates it is significant better than the second best algorithm in the same column according to Wilcoxon signed-rank test. (p<0.05).
<R> <C> Task <C> [EMPTY] <C> Data proportion 1/4 <C> Data proportion 2/4 <C> Data proportion 3/4 <C> Data proportion 4/4 <R> <C> Speech/ <C> MLP <C> 70.8 <C> 74.6 <C> 80.1 <C> 81.2 <R> <C> Music <C> [BOLD] s2sL <C> [BOLD] 75.2 <C> [BOLD] 79.3 <C> [BOLD] 82.7 <C> [BOLD] 85.1 <R> <C> Neutral/ <C> MLP <C> 86.3 <C> 88.0 <C> 90.5 <C> 91.1 <R> <C> Sad <C> [BOLD] s2sL <C> [BOLD] 90.4 <C> [BOLD] 91.2 <C> [BOLD] 92.1 <C> [BOLD] 92.9 <CAP> Table 2: F1 for imbalanced data classification. Note: EB is Eusboost and MM is MWMOTE.
<R> <C> E2E Model <C> WER (%) <R> <C> word-based CTC <C> 9.84 <R> <C> mixed (OOV: single-letter) CTC <C> 20.10 <R> <C> mixed(OOV: word + single-letter) CTC <C> 10.17 <R> <C> mixed (OOV: word + double-letter) CTC <C> 9.58 <R> <C> mixed (OOV: word + triple-letter) CTC <C> 9.32 <R> <C> mixed (OOV: word + triple-letter) attention CTC <C> 8.65 <CAP> Table 4: WERs of the vanilla word-based CTC and the CTC with mixed units.
<R> <C> [EMPTY] <C> En <C> No <C> Fr <C> De <C> Ja <C> Avg. <R> <C> Bow <C> 71.1 <C> 55.6 <C> 63.3 <C> [BOLD] 73.2 <C> 60.6 <C> 64.2 <R> <C> Cnn <C> 68.1 <C> 52.8 <C> [BOLD] 63.4 <C> 72.4 <C> 61.1 <C> 63.6 <R> <C> ULMFiT <C> 69.4 <C> 55.7 <C> 60.8 <C> 69.3 <C> [BOLD] 63.7 <C> 63.8 <R> <C> Hcnn <C> 68.3 <C> 55.7 <C> 61.9 <C> 71.2 <C> 61.0 <C> 63.6 <R> <C> Han <C> [BOLD] 72.1 <C> [BOLD] 61.4 <C> 63.2 <C> [BOLD] 73.2 <C> 61.9 <C> [BOLD] 66.4 <CAP> Table 2: Accuracy of non-hierarchical (Bow, Cnn, ULMFiT) and hierarchical (Hcnn, Han) models on document-level sentiment datasets.
<R> <C> [EMPTY] <C> noun-noun <C> verb-verb <C> adjective-adjective <R> <C> [1.0,0.9] <C> 96 <C> 48 <C> 48 <R> <C> [0.9,0.8] <C> 96 <C> 48 <C> 48 <R> <C> [0.8,0.7] <C> 96 <C> 48 <C> 48 <R> <C> [0.7,0.6] <C> 96 <C> 48 <C> 48 <R> <C> [0.6,0.4] <C> 96 <C> 48 <C> 48 <R> <C> total <C> 480 <C> 240 <C> 240 <CAP> Table 1: Number of MWE pairs with different cosine similarities in three sets.
<R> <C> Item <C> 1 qwk (%) <C> 2 qwk (%) <C> 3 qwk (%) <C> 4 qwk (%) <C> 5 qwk (%) <C> 6 qwk (%) <C> 7 qwk (%) <C> 8 qwk (%) <C> Avg qwk(%) <R> <C> BERT <C> 79.20 <C> 67.99 <C> [BOLD] 71.52 <C> 80.08 <C> 80.59 <C> 80.53 <C> 78.51 <C> 59.58 <C> 74.75 <R> <C> XLNet <C> 77.69 <C> 68.06 <C> 69.29 <C> 80.62 <C> 78.33 <C> 79.37 <C> 78.67 <C> 62.68 <C> 74.34 <R> <C> LSTM  <C> 77.50 <C> 68.70 <C> 68.30 <C> 79.50 <C> [BOLD] 81.80 <C> 81.30 <C> 80.50 <C> 59.40 <C> 74.63 <R> <C> BERT Ensemble <C> 80.21 <C> 67.21 <C> 70.82 <C> 81.56 <C> 80.63 <C> 81.47 <C> 80.42 <C> 59.74 <C> 75.26 <R> <C> XLNet Ensemble <C> 80.49 <C> 68.59 <C> 70.09 <C> 79.56 <C> 79.94 <C> 80.54 <C> 80.02 <C> 59.76 <C> 74.87 <R> <C> BERT + XLNet Ensemble <C> 80.78 <C> [BOLD] 69.67 <C> 70.31 <C> [BOLD] 81.90 <C> 80.82 <C> 81.45 <C> 80.67 <C> 60.46 <C> 75.76 <R> <C> LSTM (+CNN) Ensemble  <C> [BOLD] 82.10 <C> 68.80 <C> 69.40 <C> 80.50 <C> 80.70 <C> [BOLD] 81.90 <C> [BOLD] 80.80 <C> [BOLD] 64.40 <C> [BOLD] 76.08 <R> <C> EASE (Bag of Words)  <C> 78.10 <C> 62.10 <C> 63.00 <C> 74.90 <C> 78.20 <C> 77.10 <C> 72.70 <C> 53.40 <C> 69.90 <R> <C> H1-H2 Agreement <C> 72.08 <C> 81.23 <C> 76.90 <C> 85.10 <C> 75.27 <C> 77.59 <C> 72.09 <C> 62.03 <C> 75.29 <CAP> Table 2. qwks for the different items
<R> <C> [EMPTY] <C> SP <C> Tran.non <C> Fac.non <C> Both.non <R> <C> log(Pauto) <C> 21.3807 <C> [BOLD] 12.2571 <C> 13.8311 <C> 13.6604 <R> <C> WER <C> 0.8033 <C> 0.8841 <C> [BOLD] 0.7061 <C> 0.8334 <CAP> Table 1: Log perplexity and WER for real data
<R> <C> [EMPTY] <C> cs-en <C> de-en <C> fi-en <C> ro-en <C> ru-en <C> tr-en <C> Avg. <R> <C> SentBLEU Bojar et al. ( 2016 ) <C> 0.557 <C> 0.448 <C> 0.484 <C> 0.499 <C> 0.502 <C> 0.532 <C> 0.504 <R> <C> Blend Ma et al. ( 2017 ) <C> 0.709 <C> 0.601 <C> 0.584 <C> 0.636 <C> 0.633 <C> [BOLD] 0.675 <C> 0.640 <R> <C> DPMFcomb Bojar et al. ( 2016 ) <C> [BOLD] 0.713 <C> 0.584 <C> 0.598 <C> 0.627 <C> 0.615 <C> 0.663 <C> 0.633 <R> <C> ReVal Bojar et al. ( 2016 ) <C> 0.577 <C> 0.528 <C> 0.471 <C> 0.547 <C> 0.528 <C> 0.531 <C> 0.530 <R> <C> SVR with Skip-Thought <C> 0.665 <C> 0.571 <C> 0.609 <C> [BOLD] 0.677 <C> 0.608 <C> 0.599 <C> 0.622 <R> <C> SVR with InferSent <C> 0.679 <C> 0.604 <C> 0.617 <C> 0.640 <C> 0.644 <C> 0.630 <C> 0.636 <R> <C> SVR with InferSent + Skip-Thought <C> 0.686 <C> [BOLD] 0.611 <C> [BOLD] 0.633 <C> 0.660 <C> [BOLD] 0.649 <C> 0.646 <C> [BOLD] 0.648 <CAP> Table 2: Segment-level Pearson correlation of metric scores and DA human evaluations scores for to-English language pairs in WMT-2016 (newstest2016).
<R> <C> XNLI <C> rnd <C> fr 66.9 <C> vi 67.1 <C> zh 65.7 <C> ru 59.1 <C> ar 50.8 <C> hi 48.5 <C> avg 59.7 <R> <C> XNLI <C> scr <C> 71.1 <C> 58.0 <C> 62.8 <C> 63.8 <C> 62.1 <C> 47.5 <C> 60.8 <R> <C> XNLI <C> vec <C> 75.2 <C> 71.8 <C> 70.5 <C> 71.1 <C> 68.5 <C> 62.8 <C> 70.0 <R> <C> UD <C> rnd <C> 71.0 <C> 33.2 <C> 25.5 <C> 60.3 <C> 19.7 <C> 13.4 <C> 37.2 <R> <C> UD <C> scr <C> 63.8 <C> 17.0 <C> 13.6 <C> 61.0 <C> 15.5 <C> 11.2 <C> 30.3 <R> <C> UD <C> vec <C> 78.0 <C> 38.3 <C> 30.1 <C> 67.2 <C> 40.6 <C> 38.6 <C> 48.8 <CAP> Table 3: Comparison between random initialization (rnd) of language specific parameters and initialization using aligned fastText vectors (vec) and bilingual BERT trained from scratch (scr) for 400 hours.
<R> <C> [EMPTY] <C> fr <C> vi <C> zh <C> ru <C> ar <C> hi <C> avg <R> <C> mBERT <C> 92.1 <C> 62.2 <C> 85.1 <C> 93.1 <C> 83.6 <C> 91.3 <C> 84.6 <R> <C> ℝ𝔸𝕄𝔼ℕ \textsc  [ITALIC] base <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> + BERT <C> 92.2 <C> 63.3 <C> 85.0 <C> 93.3 <C> 83.9 <C> 92.2 <C> 85.0 <R> <C> + RoBERTa <C> 92.8 <C> 65.3 <C> 86.0 <C> 93.7 <C> 84.9 <C> 92.5 <C> 85.9 <R> <C> ℝ𝔸𝕄𝔼ℕ \textsc  [ITALIC] large <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> + BERT <C> 92.8 <C> 64.7 <C> 86.2 <C> 93.9 <C> 84.9 <C> 92.0 <C> 85.7 <R> <C> + RoBERTa <C> 93.0 <C> 66.4 <C> 87.3 <C> 94.0 <C> 85.3 <C> 92.8 <C> 86.5 <CAP> Table 4: Evaluation in supervised UD parsing. The scores are LAS.
<R> <C> Dataset <C> F1 Score <C> Accuracy <R> <C> All IND Baseline <C> 0.337 <C> 0.765 <R> <C> OLID <C> 0.302 <C> 0.523 <CAP> Table 4: Accuracy and macro F1 results on the sub-task C:BERT based on an all “IND” baseline and OLID dataset.
<R> <C> Method <C> F1 Score <C> Accuracy <R> <C> BERT+Bi-GRU <C> 0.833 <C> 0.906 <R> <C> BERT+Bi-GRU+ BERT Hidden Layers Concatenated <C> 0.819 <C> 0.915 <R> <C> BERT+Bi-LSTM <C> 0.797 <C> 0.883 <R> <C> BERT+Bi-LSTM + BERT Hidden Layers Concatenated <C> 0.837 <C> 0.915 <CAP> Table 8: Accuracy and macro F1 results on the sub-task A: Greek based on different techniques
<R> <C> Metric <C> DSTC2 Spearman <C> DSTC2 p-value <C> DSTC2 Pearson <C> p-value <C> Restaurants Spearman <C> Restaurants p-value <C> Restaurants Pearson <C> Restaurants p-value <R> <C> Bleu 1 <C> -0.317 <C> <0.005 <C> 0.583 <C> <0.005 <C> 0.069 <C> 0.494 <C> 0.277 <C> 0.005 <R> <C> Bleu 2 <C> -0.318 <C> <0.005 <C> 0.526 <C> <0.005 <C> 0.091 <C> 0.366 <C> 0.166 <C> 0.099 <R> <C> Bleu 3 <C> -0.318 <C> <0.005 <C> 0.500 <C> <0.005 <C> 0.109 <C> 0.280 <C> 0.223 <C> 0.026 <R> <C> Bleu 4 <C> -0.318 <C> <0.005 <C> 0.461 <C> <0.005 <C> 0.105 <C> 0.296 <C> 0.255 <C> 0.010 <R> <C> METEOR <C> 0.295 <C> <0.005 <C> 0.582 <C> <0.005 <C> 0.353 <C> <0.005 <C> 0.489 <C> <0.005 <R> <C> ROUGE_L <C> 0.294 <C> <0.005 <C> 0.448 <C> <0.005 <C> 0.346 <C> <0.005 <C> 0.382 <C> <0.005 <R> <C> Skip Thought <C> 0.528 <C> <0.005 <C> 0.086 <C> 0.397 <C> 0.284 <C> <0.005 <C> 0.364 <C> <0.005 <R> <C> Embedding Average <C> 0.295 <C> <0.005 <C> 0.485 <C> <0.005 <C> 0.423 <C> <0.005 <C> 0.260 <C> 0.009 <R> <C> Vector Extrema <C> 0.299 <C> <0.005 <C> 0.624 <C> <0.005 <C> 0.446 <C> <0.005 <C> 0.287 <C> <0.005 <R> <C> Greedy Matching <C> 0.295 <C> <0.005 <C> 0.572 <C> <0.005 <C> 0.446 <C> <0.005 <C> 0.325 <C> <0.005 <R> <C> Human <C> 0.810 <C> <0.005 <C> 0.984 <C> <0.005 <C> 0.653 <C> <0.005 <C> 0.857 <C> <0.005 <CAP> Table 3: Correlation of automated metrics with human evaluations scores
<R> <C> [BOLD] Model <C> [BOLD] SST-2 acc. <C> [BOLD] SST-2 skip(%) <C> [BOLD] SST-2 test-time <C> [BOLD] SST-2 speedup <C> [BOLD] IMDB acc. <C> [BOLD] IMDB skip(%) <C> [BOLD] IMDB test-time <C> [BOLD] IMDB speedup <C> [BOLD] AGNews acc. <C> [BOLD] AGNews skip(%) <C> [BOLD] AGNews test-time <C> [BOLD] AGNews speedup <C> [BOLD] Yelp acc. <C> [BOLD] Yelp skip(%) <C> [BOLD] Yelp test-time <C> [BOLD] Yelp speedup <R> <C> LSTM-Jump learning-to-skim <C> LSTM-Jump learning-to-skim <C> LSTM-Jump learning-to-skim <C> LSTM-Jump learning-to-skim <C> LSTM-Jump learning-to-skim <C> LSTM-Jump learning-to-skim <C> LSTM-Jump learning-to-skim <C> LSTM-Jump learning-to-skim <C> LSTM-Jump learning-to-skim <C> LSTM-Jump learning-to-skim <C> LSTM-Jump learning-to-skim <C> LSTM-Jump learning-to-skim <C> LSTM-Jump learning-to-skim <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] Baseline <C> - <C> - <C> - <C> - <C> 89.1 <C> 0 <C> 1243 <C> 1x <C> 88.1 <C> 0 <C> 81.7 <C> 1x <C> - <C> - <C> - <C> - <R> <C> LSTM-Jump <C> - <C> - <C> - <C> - <C> 89.4 <C> - <C> 769 <C> 1.6x <C> 89.3 <C> - <C> 74.2 <C> 1.1x <C> - <C> - <C> - <C> - <R> <C> skim-RNN skim-rnn <C> skim-RNN skim-rnn <C> skim-RNN skim-rnn <C> skim-RNN skim-rnn <C> skim-RNN skim-rnn <C> skim-RNN skim-rnn <C> skim-RNN skim-rnn <C> skim-RNN skim-rnn <C> skim-RNN skim-rnn <C> skim-RNN skim-rnn <C> skim-RNN skim-rnn <C> skim-RNN skim-rnn <C> skim-RNN skim-rnn <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] Baseline <C> 86.4 <C> 0 <C> - <C> 1x <C> 91.1 <C> 0 <C> - <C> 1x <C> 93.5 <C> 0 <C> - <C> 1x <C> - <C> - <C> - <C> - <R> <C> Skim-RNN-1 <C> 85.6 <C> 62.3 <C> - <C> 1.5x <C> 88.7 <C> 63.2 <C> - <C> 1.5x <C> [BOLD] 93.6 <C> 30.3 <C> - <C> 1x <C> - <C> - <C> - <C> - <R> <C> Skim-RNN-2 <C> [BOLD] 86.4 <C> 68.0 <C> - <C> 1.7x <C> 90.9 <C> 90.7 <C> - <C> [BOLD] 2.7x <C> 92.5 <C> 10.6 <C> - <C> 0.8x <C> - <C> - <C> - <C> - <R> <C> skim-RNN (Re-implemented) <C> skim-RNN (Re-implemented) <C> skim-RNN (Re-implemented) <C> skim-RNN (Re-implemented) <C> skim-RNN (Re-implemented) <C> skim-RNN (Re-implemented) <C> skim-RNN (Re-implemented) <C> skim-RNN (Re-implemented) <C> skim-RNN (Re-implemented) <C> skim-RNN (Re-implemented) <C> skim-RNN (Re-implemented) <C> skim-RNN (Re-implemented) <C> skim-RNN (Re-implemented) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] Baseline <C> 85.5 <C> 0 <C> 4 <C> 1x <C> 90.6 <C> 0 <C> 794 <C> 1x <C> 92.4 <C> 0 <C> 45 <C> 1x <C> [BOLD] 67.1 <C> 0 <C> 1027 <C> 1x <R> <C> Skim-RNN-Re <C> 84.2 <C> 15 <C> 5 <C> 0.9x <C> 90.9 <C> 71.9 <C> 620 <C> 1.3x <C> 92.1 <C> 41 <C> 38 <C> 1.2x <C> 60.5 <C> 81 <C> 873 <C> 1.2x <R> <C> [EMPTY] <C> acc <C> removal(%) <C> test-time <C> speedup <C> acc <C> removal(%) <C> test-time <C> speedup <C> acc <C> removal(%) <C> test-time <C> speedup <C> acc <C> removal(%) <C> test-time <C> speedup <R> <C> LSTM Classifier <C> LSTM Classifier <C> LSTM Classifier <C> LSTM Classifier <C> LSTM Classifier <C> LSTM Classifier <C> LSTM Classifier <C> LSTM Classifier <C> LSTM Classifier <C> LSTM Classifier <C> LSTM Classifier <C> LSTM Classifier <C> LSTM Classifier <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] Baseline <C> 85.5 <C> 0 <C> 4 <C> 1x <C> 90.6 <C> 0 <C> 794 <C> 1x <C> 92.4 <C> 0 <C> 45 <C> 1x <C> [BOLD] 67.1 <C> 0 <C> 1027 <C> 1x <R> <C> Bag-of-Words <C> 84.5 <C> 11 <C> 4 <C> 1x <C> 90.0 <C> 37 <C> 486 <C> 1.6x <C> 92.6 <C> 8 <C> 35 <C> [BOLD] 1.3x <C> 56.6 <C> 20 <C> 805 <C> 1.3x <R> <C> Our framework <C> 83.2 <C> 21 <C> 3 <C> 1.3x <C> 90.3 <C> 47 <C> 446 <C> [BOLD] 1.8x <C> 93.0 <C> 8 <C> 36 <C> [BOLD] 1.3x <C> 64.3 <C> 20 <C> 824 <C> 1.3x <R> <C> [EMPTY] <C> [BOLD] 86.4 <C> 0 <C> 4 <C> 1x <C> 91.9 <C> 0 <C> 822 <C> 1x <C> 93.0 <C> 0 <C> 43 <C> 1x <C> 66.3 <C> 0 <C> 994 <C> 1x <R> <C> BCN Classifier <C> BCN Classifier <C> BCN Classifier <C> BCN Classifier <C> BCN Classifier <C> BCN Classifier <C> BCN Classifier <C> BCN Classifier <C> BCN Classifier <C> BCN Classifier <C> BCN Classifier <C> BCN Classifier <C> BCN Classifier <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] Baseline <C> 85.7 <C> 0 <C> 9 <C> 1x <C> 91.0 <C> 0 <C> 1546 <C> 1x <C> 92.3 <C> 0 <C> 59 <C> 1x <C> 66.5 <C> 0 <C> 3487 <C> 1x <R> <C> Bag-of-Words <C> 78.8 <C> 25 <C> 5.34 <C> 1.7x <C> 91.5 <C> 8 <C> 1258 <C> 1.2x <C> 92.6 <C> 8 <C> 45 <C> [BOLD] 1.3x <C> 59.7 <C> 45 <C> 2183 <C> [BOLD] 1.6x <R> <C> Our framework <C> 82.6 <C> 35 <C> 4.6 <C> [BOLD] 2x <C> 92.0 <C> 8 <C> 1297 <C> 1.2x <C> 93.1 <C> 8 <C> 46 <C> [BOLD] 1.3x <C> 64.8 <C> 45 <C> 2179 <C> [BOLD] 1.6x <R> <C> [EMPTY] <C> 85.3 <C> 0 <C> 9 <C> 1x <C> [BOLD] 92.1 <C> 0 <C> 1618 <C> 1x <C> [BOLD] 93.2 <C> 0 <C> 57 <C> 1x <C> 66.3 <C> 0 <C> 3448 <C> 1x <CAP> Table 2: Performance and speedup on the test datasets. Test-times are measured in seconds. Skip(%) denotes skip or skim rate. For each classifier, our framework has two rows of results. Top row denotes the best speedup and the bottom row denotes the best test accuracy achieved. Overall best accuracies and best speedups are boldfaced. In the same experimental settings with LSTM, our framework achieves better speedup with competitive accuracy. Being generic, using BCN classifier performances enhance more.
<R> <C> Source <C> Decoding <C> CoNLL-2014 Prec. <C> CoNLL-2014 Rec. <C> CoNLL-2014  [ITALIC] F0.5 <C> JFLEG GLEU+ <R> <C> Revision <C> single-shot <C> 60.4 <C> 19.2 <C> 42.2 <C> 54.5 <R> <C> [EMPTY] <C> iterative <C> 58.3 <C> 25.1 <C> 46.1 <C> 56.6 <R> <C> +finetune <C> single-shot <C> 67.7 <C> 28.1 <C> 52.8 <C> 57.9 <R> <C> [EMPTY] <C> iterative <C> 64.5 <C> 36.2 <C> 55.8 <C> 62.0 <R> <C> RTT <C> single-shot <C> 47.1 <C> 21.4 <C> 38.0 <C> 52.5 <R> <C> [EMPTY] <C> iterative <C> 47.1 <C> 21.4 <C> 38.0 <C> 52.5 <R> <C> +finetune <C> single-shot <C> 66.7 <C> 31.8 <C> 54.7 <C> 59.0 <R> <C> [EMPTY] <C> iterative <C> 64.4 <C> 38.4 <C> 56.7 <C> 62.1 <CAP> Table 3: Comparing iterative decoding to single-shot decoding for two models, trained on all Wikipedia revisions data and on all round-trip translation (RTT) data.
<R> <C> [EMPTY] <C> Model <C> CoNLL-2014 Precision <C> CoNLL-2014 Recall <C> CoNLL-2014  [ITALIC] F0.5 <C> JFLEG GLEU+ <R> <C> chollampatt2018multilayer <C> MLConv [ITALIC] embed <C> 60.9 <C> 23.7 <C> 46.4 <C> 51.3 <R> <C> [EMPTY] <C> Ensemble (4) +EO +LM +SpellCheck <C> 65.5 <C> 33.1 <C> 54.8 <C> 57.5 <R> <C> junczys2018approaching <C> Single Transformer <C> [EMPTY] <C> [EMPTY] <C> 53.0 <C> 57.9 <R> <C> [EMPTY] <C> Ensemble (4) <C> 63.0 <C> 38.9 <C> 56.1 <C> 58.5 <R> <C> [EMPTY] <C> Ensemble (4) +LM <C> 61.9 <C> 40.2 <C> 55.8 <C> 59.9 <R> <C> grundkiewicz2018near <C> Hybrid PBMT +NMT +LM <C> 66.8 <C> 34.5 <C> 56.3 <C> 61.5 <R> <C> Best Single Model <C> Best Single Model <C> 65.5 <C> 37.1 <C> 56.8 <C> 61.6 <R> <C> Best Ensemble <C> Best Ensemble <C> 66.7 <C> 43.9 <C> [BOLD] 60.4 <C> [BOLD] 63.3 <CAP> Table 9: Comparison of recent state-of-the-art models (top) and our best single-system and ensemble models (bottom) on the CoNLL-2014 and JFLEG datsets. Only systems trained with publicly available Lang-8 and CoNLL datasets are reported.
<R> <C> Error Type <C> Revisions Pre-trained <C> Revisions Fine-tuned <C> Revisions Ensemble <C> Round-trip Translations Pre-trained <C> Round-trip Translations Fine-tuned <C> Round-trip Translations Ensemble <R> <C> Adjective <C> 16.9 <C> 29.4 <C> 36.6 <C> 14.4 <C> 27.8 <C> 37.9 <R> <C> Adverb <C> 31.5 <C> 39.7 <C> 43.5 <C> 21.7 <C> 33.3 <C> 44.6 <R> <C> Determiner <C> 31.3 <C> 57.2 <C> 59.4 <C> 27.4 <C> 57.7 <C> 59.5 <R> <C> Morphology <C> 64.5 <C> 66.1 <C> 66.1 <C> 38.7 <C> 59.3 <C> 62.0 <R> <C> Noun <C> 24.1 <C> 28.6 <C> 33.2 <C> 8.6 <C> 27.5 <C> 32.4 <R> <C> Orthography <C> 69.4 <C> 57.1 <C> 69.6 <C> 19.2 <C> 58.6 <C> 57.9 <R> <C> Preposition <C> 33.0 <C> 49.2 <C> 55.6 <C> 30.3 <C> 52.7 <C> 61.9 <R> <C> Pronoun <C> 34.9 <C> 34.1 <C> 44.6 <C> 24.4 <C> 41.7 <C> 50.1 <R> <C> Punctuation <C> 26.7 <C> 29.5 <C> 36.4 <C> 29.8 <C> 18.4 <C> 33.3 <R> <C> Spelling <C> 60.6 <C> 69.2 <C> 66.7 <C> 51.0 <C> 58.5 <C> 62.5 <R> <C> Verb <C> 36.1 <C> 47.1 <C> 43.2 <C> 20.7 <C> 45.2 <C> 43.2 <R> <C> Word Order <C> 45.5 <C> 33.3 <C> 52.1 <C> 34.8 <C> 42.9 <C> 45.5 <CAP> Table 11: F0.5 across error categories on the CoNLL-2014 test set.
<R> <C> [EMPTY] <C> WER <C> # params (M) <C> #M frames <R> <C> Classic 512  <C> 13.2 <C> 41.2 <C> 1200 <R> <C> Classic 256 ReLU (A+S) <C> 13.8 <C> 58.7 <C> 290 <R> <C> VCX (6 conv) (A+S) <C> 13.1 <C> 36.9 <C> 290 <R> <C> VDX (8 conv) (A+S) <C> 12.3 <C> 38.4 <C> 170 <R> <C> WDX (10 conv) (A+S) <C> 12.2 <C> 41.3 <C> 140 <R> <C> VDX (8 conv) (S) <C> 11.9 <C> 38.4 <C> 340 <R> <C> WDX (10 conv) (S) <C> [BOLD] 11.8 <C> 41.3 <C> 320 <CAP> Table 5: Results on Hub5’00 SWB after training on the 262-hour SWB-1 dataset. We obtain 14.5% relative improvement over our baseline adaptation of the classical CNN and 10.6% relative improvement over [17]. (A+S) means Adadelta + SGD finetuning. (S) means the model was trained from random initialization using SGD. The last column gives the number of frames til convergence.
<R> <C> Track <C> Enh. <C> DER (%) Dev <C> DER (%) Eval <C> JER (%) Dev <C> JER (%) Eval <R> <C> Track 1 <C> no <C> 23.70 <C> 25.99 <C> 56.20 <C> 59.51 <R> <C> Track 2 <C> no <C> 46.33 <C> 50.12 <C> 69.26 <C> 72.1 <R> <C> Track 2 <C> yes <C> 38.26 <C> 40.86 <C> 62.59 <C> 66.60 <R> <C> Track 3 <C> no <C> 59.73 <C> 50.85 <C> 68.00 <C> 65.91 <R> <C> Track 4 <C> no <C> 87.55 <C> 83.41 <C> 88.08 <C> 85.12 <R> <C> Track 4 <C> yes <C> 82.49 <C> 77.34 <C> 83.6 <C> 80.42 <CAP> Table 2: Baseline performance (measured by DER and JER) on dev and eval sets for all tracks. The Enh. column indicates whether or not speech enhancement was applied prior to SAD.
<R> <C> [EMPTY] <C> More Captions <C> More Images <R> <C> B-1** <C> 55.167 <C> 54.243 <R> <C> B-2* <C> 33.567 <C> 32.814 <R> <C> B-3 <C> 20.633 <C> 20.300 <R> <C> B-4 <C> 13.133 <C> 13.014 <R> <C> METEOR <C> 13.105 <C> 13.096 <R> <C> CIDEr <C> 21.428 <C> 20.418 <R> <C> CIDEr-D <C> 16.350 <C> 15.550 <R> <C> 20cmProportion <C> [EMPTY] <C> [EMPTY] <R> <C> Unique** <C> 14.8% <C> 9.96% <R> <C> 20cmTraining <C> [EMPTY] <C> [EMPTY] <R> <C> Perplexity** <C> 14.69 <C> 16.01 <R> <C> 20cmValidation <C> [EMPTY] <C> [EMPTY] <R> <C> Perplexity* <C> 25.86 <C> 25.33 <CAP> Table 3: Evaluations for the NIC models trained on subsets of Flickr30k containing more captions (5 captions per image, 1/5 the total number of images) and more images (1 caption per image, all training images). Significance for all 3 statistical tests that there was a true difference between the subsetting techniques: **p
<R> <C> [BOLD] Model <C> [BOLD] 200 <C> [BOLD] 200 (re) <C> [BOLD] 500 <C> [BOLD] 500 (re) <R> <C> EM-Local <C> 81.4 <C> 82.1 <C> 83.0 <C> 82.8 <R> <C> EM-Global <C> 85.6 <C> [BOLD] 84.6 <C> 85.5 <C> [BOLD] 85.4 <R> <C> Cat-Local-Alt <C> 83.3 <C> 82.9 <C> 84.8 <C> 84.1 <R> <C> Cat-Global-Alt <C> [BOLD] 86.4 <C> 83.1 <C> [BOLD] 87.1 <C> 85.0 <R> <C> Cat-Local <C> 83.2 <C> 82.5 <C> 85.3 <C> 84.8 <R> <C> Cat-Global <C> 85.4 <C> 82.8 <C> 86.1 <C> 84.5 <R> <C> VQ-Local-Alt <C> 82.9 <C> 81.1 <C> 84.8 <C> 81.4 <R> <C> VQ-Global-Alt <C> 84.7 <C> 79.6 <C> 85.9 <C> 82.9 <R> <C> VQ-Local <C> 82.6 <C> 78.7 <C> 83.6 <C> 81.3 <R> <C> VQ-Global <C> 83.0 <C> 76.8 <C> 85.4 <C> 82.0 <CAP> Table 6: Effect of alternating optimization on AG News classification with 200 and 500 labels. The “(re)” affix denotes reembedding. Accuracies are on development set with column highs in bold.
<R> <C> [BOLD] Neutral Segments <C> [BOLD] Neutral Segments <C> [BOLD] Neutral Segments Non-Gtd <C> [BOLD] Neutral Segments Gated <R> <C> [BOLD] Sent <C> HierNet <C> 4.67 <C> 36.60 <R> <C> [BOLD] Sent <C> MilNet <C> 39.61 <C> 44.60 <R> <C> [EMPTY] <C> [EMPTY] <C> Non-Gtd <C> Gated <R> <C> [BOLD] EDU <C> HierNet <C> 2.39 <C> 55.38 <R> <C> [BOLD] EDU <C> MilNet <C> 52.10 <C> 56.60 <CAP> Table 4: F1 scores for neutral segments (Yelp’13).
<R> <C> [BOLD] Method <C> Yelp <C> IMDB <R> <C> GICF <C> 86.3 <C> 86.0 <R> <C> GICF \textsc  [ITALIC] HN <C> 92.9 <C> 86.5 <R> <C> GICF \textsc  [ITALIC] MN <C> 93.2 <C> 91.0 <R> <C> MilNet <C> 94.0 <C> 91.9 <CAP> Table 4: F1 scores for neutral segments (Yelp’13).
<R> <C> [BOLD] Method <C> [BOLD] Informativeness <C> [BOLD] Polarity <C> [BOLD] Coherence <R> <C> HierNet [ITALIC] sent <C> 43.7 <C> 33.6 <C> 43.5 <R> <C> MilNet [ITALIC] sent <C> [BOLD] 45.7 <C> [BOLD] 36.7 <C> [BOLD] 44.6 <R> <C> Unsure <C> 10.7 <C> 29.6 <C> 11.8 <R> <C> HierNet [ITALIC] edu <C> 34.2† <C> 28.0† <C> [BOLD] 48.4 <R> <C> MilNet [ITALIC] edu <C> [BOLD] 53.3 <C> [BOLD] 61.1 <C> 45.0 <R> <C> Unsure <C> 12.5 <C> 11.0 <C> 6.6 <R> <C> MilNet [ITALIC] sent <C> 35.7† <C> 33.4† <C> [BOLD] 70.4† <R> <C> MilNet [ITALIC] edu <C> [BOLD] 55.0 <C> [BOLD] 51.5 <C> 23.7 <R> <C> Unsure <C> 9.3 <C> 15.2 <C> 5.9 <R> <C> Lead <C> 34.0 <C> 19.0† <C> [BOLD] 40.3 <R> <C> Random <C> 22.9† <C> 19.6† <C> 17.8† <R> <C> MilNet [ITALIC] edu <C> [BOLD] 37.4 <C> [BOLD] 46.9 <C> 33.3 <R> <C> Unsure <C> 5.7 <C> 14.6 <C> 8.6 <CAP> Table 6: Human evaluation results (in percentages). † indicates that the system in question is significantly different from MilNet (sign-test, p<0.01).
<R> <C> Dataset <C> Resolution (w×h) <C> ImgNum <C> PairNum <C> Max1 <C> Min1 <C> Avg1 <C> SPP1 <R> <C> AMT 20K <C> 280×280 <C> 14764 <C> 31751 <C> 13 <C> 1 <C> 5.28 <C> 1 <R> <C> OID:Aircraft <C> 1200×7562 <C> 7541 <C> 9400 <C> 21 <C> 5 <C> 8.22 <C> 13 <CAP> Table 1: Comparison of our AMT 20K dataset with existing datasets
<R> <C> Correlation coefficient (LOOCV) TF-IDF <C> Correlation coefficient (LOOCV) 0.284 <R> <C> fastText (Wiki) <C> 0.335 <R> <C> fastText (CC) <C> 0.359 <R> <C> fastText (VK) <C> 0.420 <CAP> Table 1: Predictive power of the models measured as Pearson correlation between real and predicted outcomes. The results were computed using leave-one-out cross-validation.
<R> <C> Model <C> Librispeech test_clean <C> Librispeech test_other <C> AISHELL-2 test_android <C> AISHELL-2 test_ios <C> AISHELL-2 test_mic <C> HKUST <C> CN12000h test_set 1 <C> CN12000h test_set 2 <C> CN12000h test_set 3 <R> <C> Transformer <C> [BOLD] 2.78 <C> [BOLD] 7.57 <C> 6.12 <C> 5.74 <C> 6.31 <C> [BOLD] 21.88 <C> [BOLD] 5.52 <C> [BOLD] 4.61 <C> [BOLD] 22.27 <R> <C> CIF-based model <C> 2.86 <C> 8.08 <C> [BOLD] 6.09 <C> [BOLD] 5.68 <C> [BOLD] 6.20 <C> 22.80 <C> 5.54 <C> 5.01 <C> 23.01 <CAP> Table 1: Results of accuracy performance for the models compared in this work. The evaluation metric uses word error rate (WER) for Librispeech, and uses character error rate (CER) for the three Mandarin datasets, and keep the same for the later table 3 and 4.
<R> <C> Vector <C> deu <C> spa <C> fra <C> ita <C> por <C> swe <R> <C> fastText <C> 31.6 <C> 54.8 <C> 56.7 <C> 50.2 <C> 55.5 <C> 43.9 <R> <C> ELMos <C> ELMos <C> ELMos <C> ELMos <C> ELMos <C> ELMos <C> ELMos <R> <C> Layer 0 <C> 19.7 <C> 41.5 <C> 41.1 <C> 36.9 <C> 44.6 <C> 27.5 <R> <C> Layer 1 <C> 24.4 <C> 46.4 <C> 47.6 <C> 44.2 <C> 48.3 <C> 36.3 <R> <C> Layer 2 <C> 19.9 <C> 40.5 <C> 41.9 <C> 38.1 <C> 42.5 <C> 30.9 <R> <C> Rosita <C> Rosita <C> Rosita <C> Rosita <C> Rosita <C> Rosita <C> Rosita <R> <C> Layer 0 <C> 37.9 <C> [BOLD] 56.6 <C> [BOLD] 58.2 <C> 57.5 <C> [BOLD] 56.6 <C> 50.6 <R> <C> Layer 1 <C> [BOLD] 40.3 <C> 56.3 <C> 57.2 <C> [BOLD] 58.1 <C> 56.5 <C> [BOLD] 53.7 <R> <C> Layer 2 <C> 38.8 <C> 51.1 <C> 52.7 <C> 53.6 <C> 50.7 <C> 50.8 <CAP> Table 6: Crosslingual alignment results (precision at 1) from decontextual probe. Layers 0, 1, and 2 denote the character CNN, first LSTM, and second LSTM layers in the language models respectively.
<R> <C> Character CNNs Char embedding size <C> Character CNNs 16 <R> <C> (# Window Size, # Filters) <C> (1, 32), (2, 32), (3, 68), (4, 128), (5, 256), 6, 512), (7, 1024) <R> <C> Activation <C> Relu <R> <C> Word-level LSTM <C> Word-level LSTM <R> <C> LSTM size <C> 2048 <R> <C> # LSTM layers <C> 2 <R> <C> LSTM projection size <C> 256 <R> <C> Use skip connections <C> Yes <R> <C> Inter-layer dropout rate <C> 0.1 <R> <C> Training <C> Training <R> <C> Batch size <C> 128 <R> <C> Unroll steps (Window Size) <C> 20 <R> <C> # Negative samples <C> 64 <R> <C> # Epochs <C> 10 <R> <C> Adagrad Duchi et al. ( 2011 ) lrate <C> 0.2 <R> <C> Adagrad initial accumulator value <C> 1.0 <CAP> Table 7: Language model hyperparameters.
<R> <C> target <C> | [ITALIC] Dτ|=0 +eng <C> | [ITALIC] Dτ|=0 +rel. <C> | [ITALIC] Dτ|=100 mono <C> | [ITALIC] Dτ|=100 +eng <C> | [ITALIC] Dτ|=100 +rel. <C> | [ITALIC] Dτ|=500 mono <C> | [ITALIC] Dτ|=500 +eng <C> | [ITALIC] Dτ|=500 +rel. <C> | [ITALIC] Dτ|=1000 mono <C> | [ITALIC] Dτ|=1000 +eng <C> | [ITALIC] Dτ|=1000 +rel. <R> <C> ara <C> 10.31 <C> [BOLD] 20.47 <C> 62.50 <C> 73.39 <C> [BOLD] 73.43 <C> 76.15 <C> [BOLD] 79.55 <C> 79.16 <C> 79.43 <C> 81.38 <C> [BOLD] 81.49 <R> <C> heb <C> 23.76 <C> [BOLD] 24.89 <C> 64.53 <C> 74.86 <C> [BOLD] 75.69 <C> 79.27 <C> 82.35 <C> [BOLD] 82.92 <C> 82.59 <C> 84.59 <C> [BOLD] 84.70 <R> <C> hrv <C> 48.69 <C> [BOLD] 67.67 <C> 63.49 <C> 79.21 <C> [BOLD] 82.00 <C> 80.80 <C> 84.92 <C> [BOLD] 85.89 <C> 84.14 <C> 86.27 <C> [BOLD] 86.66 <R> <C> rus <C> 38.69 <C> [BOLD] 73.24 <C> 59.51 <C> 75.63 <C> [BOLD] 79.29 <C> 77.38 <C> 83.16 <C> [BOLD] 84.60 <C> 82.90 <C> 85.68 <C> [BOLD] 86.99 <R> <C> nld <C> 61.68 <C> [BOLD] 72.90 <C> 57.12 <C> 74.90 <C> [BOLD] 77.01 <C> 75.19 <C> [BOLD] 82.42 <C> 81.33 <C> 81.41 <C> [BOLD] 84.93 <C> 83.23 <R> <C> deu <C> 51.18 <C> [BOLD] 68.66 <C> 60.26 <C> 72.52 <C> [BOLD] 73.45 <C> 72.94 <C> [BOLD] 77.88 <C> 77.68 <C> 76.46 <C> [BOLD] 78.67 <C> 78.57 <R> <C> spa <C> 55.85 <C> [BOLD] 75.88 <C> 64.97 <C> 80.86 <C> [BOLD] 81.55 <C> 79.67 <C> [BOLD] 84.88 <C> 84.63 <C> 82.97 <C> 86.69 <C> [BOLD] 86.81 <R> <C> ita <C> 59.71 <C> [BOLD] 78.12 <C> 69.17 <C> [BOLD] 84.63 <C> 83.51 <C> 82.96 <C> [BOLD] 88.96 <C> 87.91 <C> 87.03 <C> [BOLD] 90.22 <C> 89.32 <R> <C> cmn <C> [BOLD] 8.16 <C> 5.34 <C> 53.36 <C> [BOLD] 63.63 <C> 61.47 <C> 71.94 <C> 74.88 <C> [BOLD] 74.98 <C> 77.42 <C> [BOLD] 79.07 <C> 78.96 <R> <C> jpn <C> 4.12 <C> [BOLD] 11.66 <C> 72.37 <C> [BOLD] 80.94 <C> 80.24 <C> 86.20 <C> [BOLD] 87.74 <C> [BOLD] 87.74 <C> 88.74 <C> 89.08 <C> [BOLD] 89.32 <CAP> Table 9: LAS for UD parsing with additional simulated low-resource and zero-target-treebank settings.
<R> <C> Semantic structures <C> Pre-trained BERT <C> LayoutLM with BERT initialization <C> LayoutLM from scratch <C> Pre-trained LayoutLM <R> <C> author <C> 0.9275 <C> 0.9268 <C> 0.8991 <C> [BOLD] 0.9423 <R> <C> footer <C> 0.9290 <C> 0.9597 <C> 0.9426 <C> [BOLD] 0.9826 <R> <C> section <C> 0.9356 <C> 0.9535 <C> 0.9470 <C> [BOLD] 0.9694 <R> <C> title <C> 0.9999 <C> 0.9999 <C> 0.9999 <C> [BOLD] 0.9999 <R> <C> abstract <C> 0.9121 <C> 0.9095 <C> 0.9378 <C> [BOLD] 0.9537 <R> <C> list <C> 0.7782 <C> 0.8400 <C> 0.8257 <C> [BOLD] 0.8699 <R> <C> paragraph <C> 0.9292 <C> 0.9713 <C> 0.9758 <C> [BOLD] 0.9849 <R> <C> reference <C> [BOLD] 0.9679 <C> 0.9602 <C> 0.9552 <C> 0.9643 <R> <C> caption <C> 0.9529 <C> 0.9406 <C> 0.9383 <C> [BOLD] 0.9788 <R> <C> equation <C> 0.6319 <C> 0.8611 <C> 0.8526 <C> [BOLD] 0.9346 <R> <C> figure <C> 0.7839 <C> 0.9705 <C> 0.9893 <C> [BOLD] 0.9941 <R> <C> table <C> 0.8136 <C> 0.7869 <C> 0.8097 <C> [BOLD] 0.8175 <R> <C> Average <C> 0.8801 <C> 0.9233 <C> 0.9228 <C> [BOLD] 0.9493 <CAP> Table 1: The performance of LayoutLM and BERT on the DocBank test set.
<R> <C> [BOLD] Metric <C> [EMPTY] <C> [BOLD] Dev <C> [BOLD] Test <R> <C> Tokens <C> [EMPTY] <C> 291,746 <C> 216,473 <R> <C> Sentences <C> [EMPTY] <C> 15,084 <C> 11,623 <R> <C> Span <C> Precision <C> 93.42 <C> 93.04 <R> <C> [EMPTY] <C> Recall <C> 94.21 <C> 94.34 <R> <C> [EMPTY] <C> F1 <C> 93.81 <C> 93.69 <R> <C> Frame <C> Precision <C> 93.47 <C> 93.20 <R> <C> [EMPTY] <C> Recall <C> 94.16 <C> 94.08 <R> <C> [EMPTY] <C> F1 <C> 93.81 <C> 93.64 <R> <C> Type <C> Precision <C> 85.56 <C> 85.67 <R> <C> [EMPTY] <C> Recall <C> 86.20 <C> 86.49 <R> <C> [EMPTY] <C> F1 <C> 85.88 <C> 86.08 <R> <C> Role <C> Precision <C> 70.21 <C> 69.59 <R> <C> [EMPTY] <C> Recall <C> 69.11 <C> 69.20 <R> <C> [EMPTY] <C> F1 <C> 69.65 <C> 69.39 <R> <C> Label <C> Precision <C> 96.51 <C> 95.02 <R> <C> [EMPTY] <C> Recall <C> 94.97 <C> 90.70 <R> <C> [EMPTY] <C> F1 <C> 95.73 <C> 92.81 <R> <C> Slot <C> Precision <C> 80.00 <C> 79.81 <R> <C> [EMPTY] <C> Recall <C> 79.90 <C> 80.10 <R> <C> [EMPTY] <C> F1 <C> 79.95 <C> 79.96 <R> <C> Combined <C> Precision <C> 87.46 <C> 87.20 <R> <C> [EMPTY] <C> Recall <C> 87.79 <C> 87.91 <R> <C> [EMPTY] <C> F1 <C> 87.63 <C> 87.55 <CAP> Table 3: Evaluation on dev and test corpora, model chosen on the Slot-F1 metric on dev corpus.
<R> <C> BERT network embedding token size <C> BERT network 64 <C> PPO network  [ITALIC] c1 <C> PPO network 0.5 <R> <C> num of attention head <C> 4 <C> [ITALIC] c2 <C> 0.01 <R> <C> num of hidden layers <C> 3 <C> [ITALIC] ϵ <C> 0.2 <CAP> TABLE I: Hyperparameters of lamBERT models. The parameters are shown separately to the BERT part and the PPO part.
<R> <C> [BOLD] Model <C> ST_CMDS <C> AISHELL-1 <C> AISHELL-2 <C> AIDATANG <C> MagicData <C> HKUST <R> <C> TDNN  <C> - <C> 8.7 <C> - <C> 7.2 <C> - <C> 32.7 <R> <C> Chain-Model  <C> - <C> 7.5 <C> - <C> 5.6 <C> - <C> 28.1 <R> <C> MS-Attn  <C> - <C> - <C> 8.5 <C> - <C> - <C> - <R> <C> SpeechBERT  <C> - <C> 7.4 <C> - <C> - <C> - <C> 21.0 <R> <C> SAN-M  <C> - <C> 6.4 <C> - <C> - <C> - <C> - <R> <C> wav2letter (w/o. WSP) <C> 4.5 <C> 11.7 <C> 12.5 <C> 12.9 <C> 7.4 <C> 35.7 <R> <C> wav2letter (w. WSP) <C> 2.4 <C> 7.1 <C> 10.0 <C> 9.2 <C> 6.7 <C> 29.3 <R> <C> Speech Transformer (w/o. WSP) <C> 4.4 <C> 6.7 <C> 7.4 <C> 7.8 <C> 3.6 <C> 23.5 <R> <C> Speech Transformer (w. WSP) <C> [BOLD] 2.1 <C> [BOLD] 5.9 <C> [BOLD] 5.9 <C> [BOLD] 4.9 <C> [BOLD] 3.3 <C> [BOLD] 20.0 <CAP> Table 2: Performance of different ASR models on six public test datasets in terms of CER (%).
<R> <C> [BOLD] Method/Iteration <C> 4 <C> 8 <C> 12 <R> <C> Liao et al.  <C> 17.3 <C> 16.8 <C> 16.5 <R> <C> WSP ( [ITALIC] γ=0) <C> 16.1 <C> 15.0 <C> 14.2 <R> <C> WSP ( [ITALIC] γ=0.5%) <C> 15.4 <C> 14.4 <C> 13.6 <R> <C> WSP ( [ITALIC] γ=1.0%) <C> 15.3 <C> 14.2 <C> [BOLD] 13.3 <R> <C> WSP ( [ITALIC] γ=2.0%) <C> 15.6 <C> 14.9 <C> 14.7 <CAP> Table 3: The performance of the pre-trained wav2letter model with different data filtering techniques over the AISHELL-1 development set in terms of CER (%).
<R> <C> [BOLD] Dataset <C> [BOLD] WSP? <C> [BOLD] Insertion <C> [BOLD] Deletion <C> [BOLD] Substitution <R> <C> AISHELL-1 <C> No <C> 0.1 <C> 0.2 <C> 6.4 <R> <C> AISHELL-1 <C> Yes <C> 0.1 <C> 0.2 <C> 5.7 <R> <C> HKUST <C> No <C> 2.6 <C> 3.6 <C> 17.3 <R> <C> HKUST <C> Yes <C> 2.7 <C> 2.6 <C> 14.7 <CAP> Table 4: Error analysis on AISHELL-1 and HKUST in terms of CER (%).
<R> <C> Training Language <C> Develop Accuracy <C> Accuracy on test languages DE1 <C> Accuracy on test languages EN1,2 <C> Accuracy on test languages ES1,2 <C> Accuracy on test languages FR1,2 <C> Accuracy on test languages IT1 <C> Accuracy on test languages RU2 <C> Accuracy on test languages ZH2 <C> Accuracy on test languages JA <C> Average lang1 <C> Average lang2 <R> <C> [BOLD] MultiCCA word embeddings, aggregation by convolutional network <C> [BOLD] MultiCCA word embeddings, aggregation by convolutional network <C> [BOLD] MultiCCA word embeddings, aggregation by convolutional network <C> [BOLD] MultiCCA word embeddings, aggregation by convolutional network <C> [BOLD] MultiCCA word embeddings, aggregation by convolutional network <C> [BOLD] MultiCCA word embeddings, aggregation by convolutional network <C> [BOLD] MultiCCA word embeddings, aggregation by convolutional network <C> [BOLD] MultiCCA word embeddings, aggregation by convolutional network <C> [BOLD] MultiCCA word embeddings, aggregation by convolutional network <C> [BOLD] MultiCCA word embeddings, aggregation by convolutional network <C> [BOLD] MultiCCA word embeddings, aggregation by convolutional network <C> [BOLD] MultiCCA word embeddings, aggregation by convolutional network <R> <C> German <C> 92.2 <C> (93.7) <C> 55.95 <C> [ITALIC] 73.23 <C> 71.55 <C> [ITALIC] 63.98 <C> 44.83 <C> 55.45 <C> 60.18 <C> 71.68 <C> 60.20 <R> <C> English <C> 93.9 <C> [BOLD] 81.2 <C> (92.2) <C> [ITALIC] 72.50 <C> 72.38 <C> [BOLD] 69.38 <C> [ITALIC] 60.80 <C> [BOLD] 74.73 <C> [BOLD] 67.63 <C> [BOLD] 77.52 <C> [BOLD] 73.44 <R> <C> Spanish <C> 95.3 <C> 55.8 <C> 74.0 <C> (94.45) <C> 65.63 <C> 58.35 <C> 45.53 <C> 41.63 <C> 43.40 <C> 69.63 <C> 67.58 <R> <C> French <C> 91.5 <C> 53.7 <C> 64.8 <C> 65.40 <C> (92.05) <C> 61.15 <C> 40.75 <C> 38.35 <C> 37.75 <C> 67.43 <C> 64.83 <R> <C> Italian <C> 85.6 <C> 49.2 <C> 53.7 <C> 58.68 <C> 62.25 <C> (85.55) <C> 35.58 <C> 32.13 <C> 45.30 <C> 61.87 <C> 64.83 <R> <C> Russian <C> 86.8 <C> 40.3 <C> 72.5 <C> 41.03 <C> 44.60 <C> 42.70 <C> (85.65) <C> 42.38 <C> 39.68 <C> 48.22 <C> 57.30 <R> <C> Chinese <C> 90.8 <C> 48.7 <C> 56.0 <C> 35.53 <C> 53.58 <C> 47.18 <C> 40.45 <C> (87.30) <C> 50.63 <C> 48.19 <C> 46.55 <R> <C> Japanese <C> 87.3 <C> 52.7 <C> 54.9 <C> 54.28 <C> 48.30 <C> 44.33 <C> 40.85 <C> 44.78 <C> (85.35) <C> 50.89 <C> 48.52 <R> <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on Europarl <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on Europarl <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on Europarl <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on Europarl <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on Europarl <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on Europarl <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on Europarl <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on Europarl <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on Europarl <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on Europarl <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on Europarl <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on Europarl <R> <C> German <C> 94.3 <C> (92.03) <C> 71.52 <C> [BOLD] 75.50 <C> [BOLD] 75.45 <C> 56.45 <C> - <C> - <C> - <C> 74.15 <C> - <R> <C> English <C> 90.7 <C> 71.83 <C> (88.40) <C> 66.65 <C> 72.83 <C> 60.73 <C> - <C> - <C> - <C> 72.09 <C> - <R> <C> Spanish <C> 88.2 <C> 71.05 <C> 62.70 <C> (88.28) <C> 62.67 <C> 57.93 <C> - <C> - <C> - <C> 68.53 <C> - <R> <C> French <C> 90.6 <C> [ITALIC] 78.42 <C> [BOLD] 76.00 <C> 70.70 <C> (89.75) <C> 63.70 <C> - <C> - <C> - <C> [ITALIC] 75.71 <C> - <R> <C> Italian <C> 83.1 <C> 66.22 <C> 67.15 <C> 67.07 <C> 65.07 <C> (82.88) <C> - <C> - <C> - <C> 69.68 <C> - <R> <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on United Nations <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on United Nations <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on United Nations <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on United Nations <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on United Nations <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on United Nations <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on United Nations <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on United Nations <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on United Nations <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on United Nations <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on United Nations <C> [BOLD] Joint sentence embeddings BiLSTM + max pooling, trained on United Nations <R> <C> English <C> 91.3 <C> - <C> (88.83) <C> 69.50 <C> [ITALIC] 74.52 <C> - <C> [BOLD] 61.42 <C> [ITALIC] 71.97 <C> - <C> - <C> [BOLD] 73.25 <R> <C> Spanish <C> 86.8 <C> - <C> 61.65 <C> (87.67) <C> 61.62 <C> - <C> 45.10 <C> 59.88 <C> - <C> - <C> 63.18 <R> <C> French <C> 90.5 <C> - <C> [ITALIC] 75.35 <C> 71.80 <C> (89.55) <C> - <C> 59.55 <C> 69.08 <C> - <C> - <C> 73.07 <R> <C> Russian <C> 83.8 <C> - <C> 68.53 <C> 65.18 <C> 65.90 <C> - <C> (81.60) <C> 59.65 <C> - <C> - <C> 68.17 <R> <C> Chinese <C> 90.4 <C> - <C> 66.30 <C> 64.78 <C> 63.82 <C> - <C> 54.57 <C> 87.10 <C> - <C> - <C> 67.31 <CAP> Table 4: Baseline classification accuracies for zero-shot transfer on the test set of the proposed Multilingual Document Classification Corpus. All classifiers were trained on 1 000 news stories and model selection is performed on the Dev corpus of the training language. The same system is then applied to all test languages. Underlined scores indicate the best result on each transfer language for each group, bold scores the overall best accuracy, and italic ones the second best results.
<R> <C> Train <C> Accuracy on test languages DE <C> Accuracy on test languages EN <C> Accuracy on test languages ES <C> Accuracy on test languages FR <C> Accuracy on test languages IT <C> Avg <R> <C> [BOLD] Joint sentence embeddings (Europarl) <C> [BOLD] Joint sentence embeddings (Europarl) <C> [BOLD] Joint sentence embeddings (Europarl) <C> [BOLD] Joint sentence embeddings (Europarl) <C> [BOLD] Joint sentence embeddings (Europarl) <C> [BOLD] Joint sentence embeddings (Europarl) <C> [BOLD] Joint sentence embeddings (Europarl) <R> <C> DE <C> (92.03) <C> 76.48 <C> 76.95 <C> 76.72 <C> 66.27 <C> 77.69 <R> <C> EN <C> 81.17 <C> (88.40) <C> 70.75 <C> 77.80 <C> 62.35 <C> 76.09 <R> <C> ES <C> 77.38 <C> 67.58 <C> (88.28) <C> 67.92 <C> 64.07 <C> 73.05 <R> <C> FR <C> 82.78 <C> 76.72 <C> 76.97 <C> (89.75) <C> 64.07 <C> 78.06 <R> <C> IT <C> 77.10 <C> 72.70 <C> 72.60 <C> 76.97 <C> (82.88) <C> 76.45 <CAP> Table 5: Baseline classification accuracies for targeted transfer on the test set of the proposed MLDoc. All classifiers were trained on 1 000 news stories and model selection is performed on the Dev corpus of the target language. Each entry corresponds to a specifically optimized system.
<R> <C> [BOLD] No. <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F-core <R> <C> 1 <C> 82.19 <C> 82.88 <C> 82.53 <R> <C> 2 <C> 97.65 <C> 41.94 <C> 58.68 <CAP> Table 2: Performance of Action Analysis
<R> <C> [BOLD] Dataset <C> [BOLD] Gender <C> [BOLD] Anonymization  [BOLD] Enroll <C> [BOLD] Anonymization  [BOLD] Trial <C> [BOLD] Development  [BOLD] EER (%) <C> [BOLD] Development  [BOLD] Cminllr <C> [BOLD] Development  [BOLD] Cllr <C> [BOLD] Test  [BOLD] EER (%) <C> [BOLD] Test  [BOLD] Cminllr <C> [BOLD] Test  [BOLD] Cllr <R> <C> LibriSpeech <C> Female <C> original <C> original <C> 8.67 <C> 0.304 <C> 42.86 <C> 7.66 <C> 0.183 <C> 26.79 <R> <C> LibriSpeech <C> Female <C> original <C> anonymized <C> 50.14 <C> 0.996 <C> 144.11 <C> 47.26 <C> 0.995 <C> 151.82 <R> <C> LibriSpeech <C> Female <C> anonymized <C> anonymized <C> 36.79 <C> 0.894 <C> 16.35 <C> 32.12 <C> 0.839 <C> 16.27 <R> <C> LibriSpeech <C> Male <C> original <C> original <C> 1.24 <C> 0.034 <C> 14.25 <C> 1.11 <C> 0.041 <C> 15.30 <R> <C> LibriSpeech <C> Male <C> original <C> anonymized <C> 57.76 <C> 0.999 <C> 168.99 <C> 52.12 <C> 0.999 <C> 166.66 <R> <C> LibriSpeech <C> Male <C> anonymized <C> anonymized <C> 34.16 <C> 0.867 <C> 24.72 <C> 36.75 <C> 0.903 <C> 33.93 <R> <C> VCTK <C> Female <C> original <C> original <C> 2.86 <C> 0.100 <C> 1.13 <C> 4.89 <C> 0.169 <C> 1.50 <R> <C> (different) <C> Female <C> original <C> anonymized <C> 49.97 <C> 0.989 <C> 166.03 <C> 48.05 <C> 0.998 <C> 146.93 <R> <C> (different) <C> Female <C> anonymized <C> anonymized <C> 26.11 <C> 0.760 <C> 8.41 <C> 31.74 <C> 0.847 <C> 11.53 <R> <C> (different) <C> Male <C> original <C> original <C> 1.44 <C> 0.052 <C> 1.16 <C> 2.07 <C> 0.072 <C> 1.82 <R> <C> (different) <C> Male <C> original <C> anonymized <C> 53.95 <C> 1.000 <C> 167.51 <C> 53.85 <C> 1.000 <C> 167.82 <R> <C> (different) <C> Male <C> anonymized <C> anonymized <C> 30.92 <C> 0.839 <C> 23.80 <C> 30.94 <C> 0.834 <C> 23.84 <CAP> Table 3: Speaker verifiability achieved by the pretrained ASVeval model. The primary baseline is used for anonymization.
<R> <C> [EMPTY] <C> B-1 <C> B-2 <C> B-3 <C> B-4 <C> SVM0 <C> SVM0.5 <C> SVM0.6 <C> SVM0.7 <C> SVM0.8 <C> SVM0.99 <C> SVM0.7 with np <C> B-LSTM & CRF <R> <C> [BOLD] P <C> 21.01 <C> 30.45 <C> 32.44 <C> 40.22 <C> 47.97 <C> 48.43 <C> 47.84 <C> 48.81 <C> 47.82 <C> 46.09 <C> 47.95 <C> [BOLD] 50.33 <R> <C> [BOLD] R <C> 16.34 <C> 11.20 <C> 16.49 <C> 33.28 <C> 36.35 <C> 37.47 <C> 37.19 <C> 38.12 <C> 36.81 <C> 35.14 <C> 40.26 <C> [BOLD] 40.49 <R> <C> [BOLD] F1 <C> 18.38 <C> 16.37 <C> 21.87 <C> 36.42 <C> 41.36 <C> 42.25 <C> 41.85 <C> 42.80 <C> 41.60 <C> 39.87 <C> 43.77 <C> [BOLD] 44.87 <CAP> TABLE II: Experimental results (precision, recall and F-score) for ATE using distant supervision. The labels of the columns indicate the model used for ATE. In case of the SVM classifier, the subscript indicates the value used for sentence selection.
<R> <C> System <C> #Questions Generated <C> Avg. #Questions Per Sentence <C> Grammaticality <C> Relevance <R> <C> H&S <C> 381 <C> 3.81 <C> 3.49 <C> 4.23 <R> <C> NQG <C> 100 <C> 1 <C> 3.48 <C> 3.28 <R> <C> QPP&QAP <C> — <C> — <C> 3.9 <C> 4.03 <R> <C> Syn-QG <C> 654 <C> [BOLD] 6.54 <C> [BOLD] 3.93 <C> [BOLD] 4.34 <CAP> Table 5: Comparison of human evaluation with H&S Heilman and Smith (2009), NQG Du et al. (2017) and QPP&QAP Zhang and Bansal (2019)
<R> <C> [EMPTY] <C> [BOLD] Harm. (dB) <C> [BOLD] Aper. (dB) <C> [BOLD] V/UV (acc.) <R> <C> [BOLD] HTS (M1) <C> 4.24 <C> 0.89 <C> 96.86 <R> <C> [BOLD] NPSS* (M1) <C> 4.40 <C> 1.02 <C> 97.39 <R> <C> [BOLD] HTS (F1) <C> 4.28 <C> 1.50 <C> 96.93 <R> <C> [BOLD] NPSS* (F1) <C> 4.29 <C> 1.62 <C> 97.54 <R> <C> [BOLD] HTS (F2) <C> 4.21 <C> 1.27 <C> 98.40 <R> <C> [BOLD] NPSS* (F2) <C> 4.43 <C> 1.47 <C> 98.51 <CAP> Table 1: Quantitative results for each of the voices. The first two columns show average Mel-Cepstral Distortion (MCD) for harmonic and aperiodic components respectively. The final column shows the accuracy of the voiced/unvoiced decision prediction.
<R> <C> [EMPTY] <C> Jin et al. ( 2019 ) <C> Alzantot et al. ( 2018 ) <R> <C> Semantic Preservation <C> 4.06 <C> 4.11 <R> <C> Grammatical Error % <C> 0 <C> 0 <R> <C> Non-suspicion Score <C> 58.8 <C> 56.9 <R> <C> Attack Success % <C> 10.3 <C> [BOLD] 10.9 <R> <C> Perturbed Word % <C> 10.8 <C> [BOLD] 9.5 <R> <C> Num Queries <C> [BOLD] 28 <C> 4,160 <CAP> Table 6: Comparison of Alzantot et al. (2018) and Jin et al. (2019), with the same constraints, attacking BERT fine-tuned on the MR dataset. Results across 1,000 examples.
<R> <C> Systems <C> MT03 <C> MT04 <C> MT05 <C> Average <C> Parameters # <R> <C> RNNsearch \textsc  [ITALIC] default <C> 29.02 <C> 31.25 <C> 28.32 <C> 29.53 <C> 31M <R> <C> RNNsearch \textsc  [ITALIC] best <C> 30.28 <C> 31.72 <C> 28.52 <C> 30.17 <C> 46M <R> <C> Arc-I \textsc  [ITALIC] loc <C> 28.98 <C> 32.02 <C> 29.53* <C> 30.18 <C> 54M <R> <C> Arc-I \textsc  [ITALIC] hyb <C> 30.14 <C> 32.70* <C> 29.40* <C> 30.75 <C> 54M <R> <C> Arc-II <C> [BOLD] 31.27* <C> 33.02* <C> [BOLD] 30.63* <C> 31.64 <C> 42M <R> <C> Arc-III <C> 30.15 <C> [BOLD] 33.46* <C> 29.49* <C> 31.03 <C> 53M <R> <C> Arc-IV <C> 29.88 <C> 32.00 <C> 28.76 <C> 30.21 <C> 48M <R> <C> Moses <C> 31.61 <C> 33.48 <C> 30.75 <C> 31.95 <C> – <CAP> Table 1: BLEU-4 scores (%) of NMT baselines: RNNsearch\textscdefault and RNNsearch\textscbest, DeepMemory architectures (Arc-I, II, III and IV), and phrase-based SMT system (Moses). The “*” indicates that the results are significantly (p<0.05) better than those of the RNNsearch\textscbest.
<R> <C> [BOLD] Model <C> [BOLD] Description <C> [BOLD] A <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F <R> <C> SVM <C> Baseline <C> 0.579 <C> 0.347 <C> 0.579 <C> 0.430 <R> <C> DNN <C> Baseline <C> 0.756 <C> 0.809 <C> 0.756 <C> 0.781 <R> <C> RNN <C> Baseline <C> 0.802 <C> 0.827 <C> 0.802 <C> 0.814 <R> <C> CNN <C> Baseline <C> 0.702 <C> 0.918 <C> 0.702 <C> 0.796 <R> <C> BiLSTM <C> Baseline <C> 0.840 <C> 0.880 <C> 0.840 <C> 0.859 <R> <C> LSTM+DNN <C> Word emb. <C> 0.873 <C> 0.892 <C> 0.873 <C> 0.882 <R> <C> LSTM+DNN <C> Semantic emb. <C> 0.917 <C> 0.922 <C> 0.925 <C> 0.917 <R> <C> LSTM+RNN <C> Word emb. <C> 0.860 <C> 0.875 <C> 0.860 <C> 0.855 <R> <C> LSTM+RNN <C> Semantic emb. <C> 0.891 <C> 0.904 <C> 0.891 <C> 0.891 <R> <C> BiLSTM+CNN <C> Word emb. <C> 0.847 <C> 0.938 <C> 0.847 <C> 0.890 <R> <C> BiLSTM+CNN <C> Semantic emb. <C> 0.902 <C> 0.933 <C> 0.902 <C> 0.917 <CAP> Table 2: Performance evaluation on test dataset using Accuracy (A), Precision (P), Recall (R) and F1-Score (F). Our state-of-the-art models and their baselines are marked in gray.
<R> <C> [BOLD] SemEval 15  [BOLD] Model <C> [BOLD] SemEval 15  [BOLD] F1 <C> [BOLD] SemEval 16  [BOLD] Acc. <C> [BOLD] SemEval 16  [BOLD] Model <C> [BOLD] SemEval 16  [BOLD] MAP <C> [BOLD] SemEval 16  [BOLD] F1 <C> [BOLD] SemEval 16  [BOLD] Acc. <R> <C> DFFN <C> [BOLD] 61.22* <C> [BOLD] 75.24* <C> DFFN <C> [BOLD] 82.34* <C> [BOLD] 66.22* <C> [BOLD] 76.67 <R> <C> JAIST <C> 57.29 <C> 72.67 <C> Kelp <C> 79.19 <C> 64.36 <C> 75.11 <R> <C> HITSZ-ICRC <C> 56.44 <C> 69.43 <C> ConvKN <C> 78.71 <C> 63.55 <C> 74.95 <R> <C> DFFN  [ITALIC] w/o HCF <C> 56.04 <C> 69.73 <C> DFFN  [ITALIC] w/o HCF <C> 74.36 <C> 60.22 <C> 72.88 <R> <C> DFFN  [ITALIC] w/o CNN <C> 51.65 <C> 67.12 <C> DFFN  [ITALIC] w/o CNN <C> 70.21 <C> 56.77 <C> 68.65 <R> <C> ICRC-HIT <C> 53.82 <C> 73.18 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 1: Overall Results of DFFN on SemEval 2015 and SemEval 2016 datasets. Results marked with a ∗ were found to be statistically significant with respect to the nearest baseline at 95% confidence level (α=0.05) when tested using paired two-tailed t-test.
<R> <C> Model <C> Precision <C> Recall <C> F1-Score <R> <C> [ITALIC] shuf 1 <C> [ITALIC] shuf 1 <C> [ITALIC] shuf 1 <C> [ITALIC] shuf 1 <R> <C> SEM(dev) <C> 92.96 <C> 87.84 <C> 90.33 <R> <C> LSTM-CRF+CamemBERTOSCAR-BASE-SWM(dev) <C> 93.77 <C> 94.00 <C> 93.89 <R> <C> SEM(test) <C> 91.88 <C> 87.14 <C> 89.45 <R> <C> LSTM-CRF+CamemBERTOSCAR-BASE-SWM(test) <C> [BOLD] 92.59 <C> [BOLD] 93.96 <C> [BOLD] 93.27 <R> <C> [ITALIC] shuf 2 <C> [ITALIC] shuf 2 <C> [ITALIC] shuf 2 <C> [ITALIC] shuf 2 <R> <C> SEM(dev) <C> 91.67 <C> 85.96 <C> 88.73 <R> <C> LSTM-CRF+CamemBERTOSCAR-BASE-SWM(dev) <C> 93.15 <C> 94.21 <C> 93.68 <R> <C> SEM(test) <C> 90.57 <C> 87.76 <C> 89.14 <R> <C> LSTM-CRF+CamemBERTOSCAR-BASE-SWM(test) <C> [BOLD] 92.63 <C> [BOLD] 94.31 <C> [BOLD] 93.46 <R> <C> [ITALIC] shuf 3 <C> [ITALIC] shuf 3 <C> [ITALIC] shuf 3 <C> [ITALIC] shuf 3 <R> <C> SEM(dev) <C> 92.53 <C> 88.75 <C> 90.60 <R> <C> LSTM-CRF+CamemBERTOSCAR-BASE-SWM(dev) <C> 94.85 <C> 95.82 <C> 95.34 <R> <C> SEM(test) <C> 90.68 <C> 85.00 <C> 87.74 <R> <C> LSTM-CRF+CamemBERTOSCAR-BASE-SWM(test) <C> [BOLD] 91.30 <C> [BOLD] 92.67 <C> [BOLD] 91.98 <CAP> Table 2: Results on the test set for the best development set scores.
<R> <C> [BOLD] Method <C> [BOLD] Coherence <C> [BOLD] N.R. <C> [BOLD] Readability <R> <C> Lead+Δ <C> 2.32 <C> 2.74 <C> 2.71 <R> <C> Cent.+Δ <C> 2.63 <C> 2.84 <C> 3.29 <R> <C> Δ+Cov. <C> 2.30 <C> 3.53 <C> 2.92 <R> <C> Δ+Text. <C> 3.18 <C> 3.75 <C> 3.34 <R> <C> Δ+Δ <C> 2.23 <C> 2.57 <C> 2.57 <R> <C> Our Model <C> [BOLD] 3.76 <C> [BOLD] 3.92 <C> [BOLD] 4.08 <CAP> Table 6: Human evaluation results on 20 samples from the DUC 2002 and DUC 2004 datasets.
<R> <C> [BOLD] Models <C> [BOLD] OE(-) <C> [BOLD] HL (-) <C> [BOLD] Macro F1(+) <C> [BOLD] Micro F1(+) <R> <C> ML-KNN <C> 77.3 <C> 0.094 <C> 23.6 <C> 38.1 <R> <C> Binary Relevance <C> 74.4 <C> 0.083 <C> 24.7 <C> 41.8 <R> <C> Classifier Chains <C> 67.5 <C> 0.107 <C> 29.9 <C> 44.3 <R> <C> Label Powerset <C> 56.2 <C> 0.096 <C> 37.7 <C> 50.3 <R> <C> MLP <C> 71.5 <C> 0.081 <C> 29.8 <C> 45.8 <R> <C> CNN <C> 37.9 <C> 0.099 <C> 32.5 <C> 49.3 <R> <C> LSTM <C> 30.5 <C> 0.089 <C> 33.0 <C> 53.9 <R> <C> [BOLD] HAN (Proposal) <C> 25.9 <C> 0.079 <C> 52.1 <C> 61.0 <R> <C> [BOLD] +LCM (Proposal) <C> [BOLD] 22.6 <C> [BOLD] 0.074 <C> [BOLD] 54.4 <C> [BOLD] 64.5 <CAP> Table 2: The comparisons between our approach and the baselines on the test set. The OE and HL denotes one-error and hamming loss respectively, the implemented approach HAN and LCM denotes the hierarchical attention network and the label correlation mechanism respectively. “+” represents that higher scores are better and “-” represents that lower scores are better. It can be seen that the proposed approach significantly outperforms the baselines.
<R> <C> [BOLD] Most Styles <C> [BOLD] % of Samples <C> [BOLD] F1 <R> <C> Rock <C> 30.4 <C> 75.8 <R> <C> Independent Music <C> 30.0 <C> 64.8 <R> <C> Pop <C> 26.2 <C> 67.1 <R> <C> Folk Music <C> 21.9 <C> 73.7 <R> <C> Electronic Music <C> 13.9 <C> 61.8 <R> <C> [BOLD] Least styles <C> [BOLD] % of Samples <C> [BOLD] F1 <R> <C> Jazz <C> 4.3 <C> 37.5 <R> <C> Heavy Metal Music <C> 3.9 <C> 55.6 <R> <C> Hip-Hop <C> 3.1 <C> 7.5 <R> <C> Post-punk <C> 2.5 <C> 17.1 <R> <C> Dark Wave <C> 1.3 <C> 17.4 <CAP> Table 5: The performance of the proposed method on most and fewest styles.
<R> <C> [EMPTY] <C> SciTail (dev) <C> SciTail (test) <R> <C> HCRN <C> 79.4 <C> 80.0 <R> <C> Decomp-Att <C> 75.4 <C> 72.3 <R> <C> AWE-Decomp-Att <C> [BOLD] 77.1 <C> [BOLD] 74.3 <R> <C> DeIsTe <C> 82.4 <C> 82.1 <R> <C> AWE-DeIsTe <C> [BOLD] 85.1 <C> [BOLD] 84.2 <CAP> Table 1: Dev and test accuracies (%) on SciTail
<R> <C> [EMPTY] <C> SNLI (dev) <C> SNLI (test) <R> <C> Decomp-Att <C> 86.4 <C> 86.8 <R> <C> AWE-Decomp-Att <C> [BOLD] 87.7 <C> [BOLD] 87.1 <R> <C> DeIsTe <C> 83.3 <C> 82.2 <R> <C> AWE-DeIsTe <C> [BOLD] 85.1 <C> [BOLD] 84.1 <CAP> Table 2: Dev and test accuracies (%) on SNLI
<R> <C> [EMPTY] <C> SciTail (dev) <C> SciTail (test) <C> SNLI (dev) <C> SNLI (test) <R> <C> DeIsTe (baseline) <C> 82.4 <C> 82.1 <C> 83.3 <C> 82.2 <R> <C> AWE-DeIsTe + Asymmetric Embeddings on SNLI <C> (84.0) <C> (83.6) <C> [BOLD] 84.6 <C> 83.7 <R> <C> AWE-DeIsTe + Asymmetric Embeddings on SciTail <C> [BOLD] 84.4 <C> [BOLD] 84.2 <C> (83.2) <C> ( [BOLD] 84.1) <CAP> Table 3: Cross-data evaluation of asymmetric word embeddings. “()” indicates the cross-data results, i.e. models are trained/tested on one dataset but the added asymmetric embeddings are trained from a different dataset. The results of AWE-DeIsTe without “()” are using the asymmetric word embeddings trained from the same dataset.
<R> <C> System <C> W&I+L test <C> W&I+L dev <C> CoNLL 14 test No W&I+L <C> CoNLL 14 test With W&I+L <R> <C> including ensembles <C> including ensembles <C> including ensembles <C> including ensembles <C> including ensembles <R> <C> lichtarge2019corpora <C> – <C> – <C> 60.40 <C> – <R> <C> zhao2019improving <C> – <C> – <C> 61.15 <C> – <R> <C> xu2019erroneous <C> 67.21 <C> 55.37 <C> – <C> 63.20 <R> <C> choe2019neural <C> 69.06 <C> 52.79 <C> 57.50 <C> – <R> <C> grundkiewicz2019neural <C> [BOLD] 69.47 <C> 53.00 <C> [BOLD] 61.30 <C> [BOLD] 64.16 <R> <C> no ensembles <C> no ensembles <C> no ensembles <C> no ensembles <C> no ensembles <R> <C> lichtarge2019corpora <C> – <C> – <C> [BOLD] 56.80 <C> – <R> <C> xu2019erroneous <C> [BOLD] 63.94 <C> 52.29 <C> – <C> 60.90 <R> <C> choe2019neural <C> 63.05 <C> 47.75 <C> – <C> – <R> <C> grundkiewicz2019neural <C> – <C> 50.01 <C> – <C> – <R> <C> no ensembles <C> no ensembles <C> no ensembles <C> no ensembles <C> no ensembles <R> <C> Our work – synthetic pretrain <C> 51.16 <C> 32.76 <C> 41.85 <C> 44.12 <R> <C> Our work – finetuned base single GPU <C> 67.18 <C> 52.80 <C> 59.87 <C> – <R> <C> Our work – finetuned <C> 69.00 <C> 53.30 <C> 60.76 <C> 63.40 <CAP> Table 5: Comparison of systems on two English GEC datasets. CoNLL 2014 Test Set is divided into two system groups (columns): those who do not train on W&I+L training data and those who do.
<R> <C> System <C> P <C> R <C> [ITALIC] F0.5 <R> <C> boyd2018using <C> 51.99 <C> 29.73 <C> 45.22 <R> <C> Our work – synthetic pretrain <C> 67.45 <C> 26.35 <C> 51.41 <R> <C> Our work – finetuned base single GPU <C> 78.11 <C> 59.13 <C> 73.40 <R> <C> Our work – finetuned <C> 78.21 <C> 59.94 <C> 73.71 <CAP> Table 6: Results on on Falko-Merlin Test Set (German).
<R> <C> System <C> P <C> R <C> [ITALIC] F0.5 <R> <C> rozovskaya2019grammar <C> 38.0 <C> 7.5 <C> 21.0 <R> <C> Our work – synthetic pretrain <C> 47.76 <C> 26.08 <C> 40.96 <R> <C> Our work – finetuned base single GPU <C> 59.13 <C> 26.05 <C> 47.15 <R> <C> Our work – finetuned <C> 63.26 <C> 27.50 <C> 50.20 <CAP> Table 8: Results on on RULEC-GEC Test Set (Russian).
<R> <C> Method <C> Substitution decipher 20% <C> Substitution decipher 40% <C> Substitution decipher 60% <C> Substitution decipher 80% <C> Substitution decipher 100% <C> Order recover <R> <C> No transfer (copy) <C> 56.4 <C> 21.4 <C> 6.3 <C> 4.5 <C> 0 <C> 5.1 <R> <C> Unigram matching <C> 74.3 <C> 48.1 <C> 17.8 <C> 10.7 <C> 1.2 <C> - <R> <C> Variational auto-encoder <C> 79.8 <C> 59.6 <C> 44.6 <C> 34.4 <C> 0.9 <C> 5.3 <R> <C> Aligned auto-encoder <C> 81.0 <C> 68.9 <C> 50.7 <C> 45.6 <C> 7.2 <C> 5.2 <R> <C> Cross-aligned auto-encoder <C> [BOLD] 83.8 <C> [BOLD] 79.1 <C> [BOLD] 74.7 <C> [BOLD] 66.1 <C> [BOLD] 57.4 <C> [BOLD] 26.1 <R> <C> Parallel translation <C> 99.0 <C> 98.9 <C> 98.2 <C> 98.5 <C> 97.2 <C> 64.6 <CAP> Table 4: Bleu scores of word substitution decipherment and word order recovery.
<R> <C> Model <C> i2b2 Precision <C> i2b2 Recall <C> i2b2 F1-score <C> MIMIC Precision <C> MIMIC Recall <C> MIMIC F1-score <R> <C> Nottingham <C> [BOLD] 99.000 <C> 96.680 <C> 97.680 <C> - <C> - <C> - <R> <C> MIST <C> 95.288 <C> 75.691 <C> 84.367 <C> 97.739 <C> 97.164 <C> 97.450 <R> <C> CRF <C> 98.560 <C> 96.528 <C> 97.533 <C> 99.060 <C> 98.987 <C> 99.023 <R> <C> ANN <C> 98.320 <C> 97.380 <C> 97.848 <C> [BOLD] 99.208 <C> 99.251 <C> [BOLD] 99.229 <R> <C> CRF + ANN <C> 97.920 <C> [BOLD] 97.835 <C> [BOLD] 97.877 <C> 98.820 <C> [BOLD] 99.398 <C> 99.108 <CAP> Table 3: Performance (%) on the PHI as defined in the HIPAA. We evaluated the systems based on the detection of PHI token versus non-PHI token (i.e., binary HIPAA token-based evaluation). The best performance for each metric on each dataset is highlighted in bold. Nottingham is the best performing system from the 2014 i2b2/UTHealth shared task Track 1. MIST, the MITRE Identification Scrubber Toolkit, is a freely available de-identification program. CRF is the model based on Conditional Random Field, ANN is the model based on Artificial Neural Network, and CRF+ANN is the result obtained by combining the outputs of the CRF model and the ANN model. The Nottingham system could not be run on the MIMIC dataset, as it is not publicly available.
<R> <C> Aspect/Class <C> mjr <C> Single Feature Group emt <C> Single Feature Group hrm <C> Single Feature Group syn <C> Single Feature Group swr <C> Single Feature Group usr <C> Single Feature Group frm <C> Single Feature Group cue <C> Single Feature Group pol <C> Single Feature Group ngr <C> Single Feature Group glv <C> All Features R <C> All Features P <C> All Features F1 <C> Size <R> <C> [BOLD] I: Intention <C> [BOLD] I: Intention <C> [BOLD] I: Intention <C> [BOLD] I: Intention <C> [BOLD] I: Intention <C> [BOLD] I: Intention <C> [BOLD] I: Intention <C> [BOLD] I: Intention <C> [BOLD] I: Intention <C> [BOLD] I: Intention <C> [BOLD] I: Intention <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> No trolling <C> 69.7 <C> 69.4 <C> 69.2 <C> 68.3 <C> 67.3 <C> 66.2 <C> 69.6 <C> 69.6 <C> 69.6 <C> 64.2 <C> 71.9 <C> 76.0 <C> 60.8 <C> 67.5 <C> 53 <R> <C> Trolling <C> 0.0 <C> 1.0 <C> 2.9 <C> 4.2 <C> 32.2 <C> 7.4 <C> 0.0 <C> 0.0 <C> 0.0 <C> 41.2 <C> 27.4 <C> 41.8 <C> 49.0 <C> 45.1 <C> 38 <R> <C> Playing <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 9 <R> <C> Accuracy <C> 53.5 <C> 53.4 <C> 53.4 <C> 52.5 <C> 54.6 <C> 51.1 <C> 53.5 <C> 53.5 <C> 53.5 <C> 52.8 <C> 57.9 <C> - <C> - <C> 56.4 <C> - <R> <C> [BOLD] D: hidden <C> [BOLD] D: hidden <C> [BOLD] D: hidden <C> [BOLD] D: hidden <C> [BOLD] D: hidden <C> [BOLD] D: hidden <C> [BOLD] D: hidden <C> [BOLD] D: hidden <C> [BOLD] D: hidden <C> [BOLD] D: hidden <C> [BOLD] D: hidden <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Hidden <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 12 <R> <C> Exposed <C> 0.0 <C> 0.0 <C> 0.5 <C> 0.0 <C> 32.5 <C> 5.7 <C> 0.0 <C> 0.0 <C> 0.0 <C> 51.2 <C> 31.8 <C> 49.8 <C> 53.0 <C> 49.8 <C> 35 <R> <C> None <C> 69.9 <C> 69.3 <C> 69.5 <C> 69.6 <C> 70.3 <C> 69.2 <C> 69.7 <C> 69.7 <C> 69.7 <C> 66.5 <C> 72.0 <C> 76.3 <C> 61.5 <C> 76.3 <C> 53 <R> <C> Accuracy <C> 53.9 <C> 53.4 <C> 53.6 <C> 53.5 <C> 57.5 <C> 53.1 <C> 53.9 <C> 53.9 <C> 53.9 <C> 56.8 <C> 58.9 <C> - <C> - <C> 57.9 <C> - <R> <C> [BOLD] R: Interpretation <C> [BOLD] R: Interpretation <C> [BOLD] R: Interpretation <C> [BOLD] R: Interpretation <C> [BOLD] R: Interpretation <C> [BOLD] R: Interpretation <C> [BOLD] R: Interpretation <C> [BOLD] R: Interpretation <C> [BOLD] R: Interpretation <C> [BOLD] R: Interpretation <C> [BOLD] R: Interpretation <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> No trolling <C> 0.0 <C> 4.0 <C> 0.5 <C> 1.0 <C> 4.7 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 50.8 <C> 38.7 <C> 63.0 <C> 56.3 <C> 63.0 <C> 34 <R> <C> Playing <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 6 <R> <C> Trolling <C> 74.2 <C> 73.6 <C> 74.1 <C> 73.8 <C> 74.2 <C> 74.2 <C> 74.2 <C> 74.2 <C> 74.2 <C> 74.3 <C> 76.3 <C> 74.3 <C> 72.8 <C> 74.3 <C> 60 <R> <C> Accuracy <C> 59.2 <C> 58.4 <C> 59.2 <C> 58.5 <C> 59.2 <C> 59.2 <C> 59.2 <C> 59.2 <C> 59.2 <C> 64.4 <C> 64.9 <C> - <C> - <C> 66.3 <C> - <R> <C> [BOLD] R: Interpretation <C> [BOLD] R: Interpretation <C> [BOLD] R: Interpretation <C> [BOLD] R: Interpretation <C> [BOLD] R: Interpretation <C> [BOLD] R: Interpretation <C> [BOLD] R: Interpretation <C> [BOLD] R: Interpretation <C> [BOLD] R: Interpretation <C> [BOLD] R: Interpretation <C> [BOLD] R: Interpretation <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Frustrate <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 30.0 <C> 19.4 <C> 30.3 <C> 28.0 <C> 28.0 <C> 12 <R> <C> Troll <C> 0.0 <C> 2.7 <C> 0.9 <C> 4.5 <C> 28.4 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 33.6 <C> 27.3 <C> 36.3 <C> 34.5 <C> 34.5 <C> 24 <R> <C> Follow <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 18.8 <C> 6.1 <C> 66.8 <C> 14.5 <C> 14.5 <C> 3 <R> <C> Praise <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 5.8 <C> 0.0 <C> 25.0 <C> 3.8 <C> 3.8 <C> 3 <R> <C> Neutralize <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 37.0 <C> 26.8 <C> 38.3 <C> 43.8 <C> 43.8 <C> 9 <R> <C> Normal <C> 0.0 <C> 2.8 <C> 0.0 <C> 0.0 <C> 1.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 0.0 <C> 41.2 <C> 35.4 <C> 37.5 <C> 42.3 <C> 42.3 <C> 20 <R> <C> Engage <C> 43.8 <C> 44.5 <C> 43.9 <C> 43.2 <C> 42.7 <C> 43.7 <C> 43.7 <C> 43.7 <C> 43.7 <C> 40.5 <C> 49.9 <C> 42.0 <C> 45.5 <C> 45.5 <C> 29 <R> <C> Accuracy <C> 36.0 <C> 28.4 <C> 27.9 <C> 27.9 <C> 30.1 <C> 28.1 <C> 28.1 <C> 28.1 <C> 28.1 <C> 36.0 <C> 38.0 <C> - <C> - <C> 37.5 <C> - <R> <C> [BOLD] All Tasks Combined <C> [BOLD] All Tasks Combined <C> [BOLD] All Tasks Combined <C> [BOLD] All Tasks Combined <C> [BOLD] All Tasks Combined <C> [BOLD] All Tasks Combined <C> [BOLD] All Tasks Combined <C> [BOLD] All Tasks Combined <C> [BOLD] All Tasks Combined <C> [BOLD] All Tasks Combined <C> [BOLD] All Tasks Combined <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Total Accuracy <C> 52.5 <C> 48.4 <C> 48.5 <C> 48.1 <C> 50.4 <C> 47.9 <C> 48.7 <C> 48.7 <C> 48.7 <C> 52.5 <C> 54.9 <C> - <C> - <C> 54.5 <C> - <CAP> Table 2: Experiments Results. Below the “mjr” header, we report F1 scores the the majority class prediction we report F1 scores for the four aspects of trolling: Intention, Intentions Disclosure, Interpretation, and Response strategy. Also, below the “Single Feature Group” header, we report F1 scores as before, when the feature group indicated in the column headers is the only feature group used for classifier. The column headers abbreviations stand for: Emoticons, Harmful Vocabulary, Emotion Synsets, Swearing Vocabulary, Swearing Vocabulary in Usernames, Framenet, Politeness cues, n-grams (actual n-grams and n-grams appended with their corresponding part of speech tag) and Glove embeddings in that order. Below the “All Features” header we report Recall, Precision and F1 score, respectively, when all features are use for prediction. All experiments are performed using a logistic regression classifier per task. The last column reports the class distribution in percentage per task. The last row of each trolling aspect reports accuracy (the percentage of instances correctly classified). The last row in the table reports total accuracy, the percentage of correctly classified instances considering all aspects.
<R> <C> [BOLD] Method <C> [BOLD] p <C> [BOLD] r <C> [BOLD] f1 <R> <C> hs <C> 74.15 <C> 72.46 <C> 73.24 <R> <C> augmented hs + cng† <C> 76.28 <C> [BOLD] 72.72 <C> [BOLD] 74.40 <R> <C> context hs + cng† <C> [BOLD] 76.61 <C> 72.15 <C> 74.26 <R> <C> ws <C> 76.43 <C> 67.77 <C> 71.78 <R> <C> augmented ws + cng† <C> [BOLD] 78.17 <C> 72.20 <C> [BOLD] 75.01 <R> <C> context ws + cng† <C> 77.90 <C> [BOLD] 72.26 <C> 74.91 <CAP> (a) Racism
<R> <C> [BOLD] Method <C> [BOLD] p <C> [BOLD] r <C> [BOLD] f1 <R> <C> hs <C> 76.04 <C> 68.84 <C> 72.24 <R> <C> augmented hs + cng† <C> 80.07 <C> [BOLD] 69.28 <C> [BOLD] 74.26 <R> <C> context hs + cng† <C> [BOLD] 80.29 <C> 68.52 <C> 73.92 <R> <C> ws <C> 81.75 <C> 57.37 <C> 67.38 <R> <C> augmented ws + cng† <C> 85.61 <C> [BOLD] 65.91 <C> [BOLD] 74.44 <R> <C> context ws + cng† <C> [BOLD] 85.80 <C> 65.41 <C> 74.18 <CAP> (b) Sexism
<R> <C> [BOLD] Method <C> [BOLD] w-tox <C> [BOLD] w-att <R> <C> hs <C> 88.65 <C> 86.28 <R> <C> hs + cng† <C> 89.29 <C> 87.32 <R> <C> augmented hs + cng† <C> 89.31 <C> 87.33 <R> <C> context hs + cng† <C> [BOLD] 89.35 <C> [BOLD] 87.44 <R> <C> ws <C> 85.49 <C> 84.35 <R> <C> ws + cng† <C> 87.12 <C> 85.80 <R> <C> augmented ws + cng† <C> 87.02 <C> 85.75 <R> <C> context ws + cng† <C> [BOLD] 87.16 <C> [BOLD] 85.81 <CAP> Table 5: Macro f1 scores on the two Wikipedia datasets. The current state of the art method for these datasets is hs. † denotes the methods we propose. Our best method (context hs + cng) outperforms all the other methods.
<R> <C> Model <C> Test set 1 P <C> Test set 1 R <C> Test set 1 F <C> Test set 2 P <C> Test set 2 R <C> Test set 2 F <R> <C> Multi-label <C> 10.83 <C> 29.72 <C> 15.87 <C> 13.51 <C> 40.49 <C> 20.26 <R> <C> Basic seq2seq <C> 26.03 <C> 13.52 <C> 17.80 <C> 30.97 <C> 23.70 <C> 26.85 <R> <C> Proposal <C> 29.57 <C> 17.30 <C> 21.83 <C> 38.22 <C> 30.18 <C> 33.73 <CAP> Table 4: Automatic evaluation results of different models on the two test datasets. Multi-label is introduced in Section 4.3. Test set 1 is the subset of the large dataset collected from the Internet, which is homogeneous to the training set. Test set 2 is the test set extracted from the prescription text book.
<R> <C> [BOLD] Systems <C> [BOLD] PE <C> [BOLD] PE + LSA <R> <C> [BOLD] ALL <C> 40.54 <C> 40.90 <CAP> Table 4: Translation performance (BELU score) with normal positional encodings and normal positional encodings with LSA model on NIST Zh-En dataset.
<R> <C> Feature Groups <C> Dnevnik <C> bTV vs. <C> Dnevnik vs. <R> <C> [EMPTY] <C> Ne!Novinite <C> bTV Duplex <C> Bazikileaks <R> <C> [BOLD] Credibility + Linguistic + Semantic <C> [BOLD] 99.36 <C> 62.04 <C> [BOLD] 85.53 <R> <C> [BOLD] Credibility + Semantic <C> 92.67 <C> [BOLD] 75.91 <C> 82.99 <R> <C> Linguistic + Credibility <C> 96.02 <C> 59.12 <C> [ITALIC] 61.94 <R> <C> [BOLD] Semantic <C> 98.95 <C> 61.31 <C> 71.01 <R> <C> Linguistic <C> 95.71 <C> [ITALIC] 56.93 <C> 73.25 <R> <C> Credibility <C> [ITALIC] 83.25 <C> 62.04 <C> 79.85 <R> <C> Baseline (majority class) <C> 52.60 <C> 50.36 <C> 50.86 <CAP> Table 2: Accuracy for different feature group combinations.
<R> <C> [ITALIC] k <C> RG <C> MC <C> WS <C> RW <C> SCWS <C> MEN <C> SL <C> SE <C> DV <C> TR <C> MR <C> CR <C> SUBJ <R> <C> 2 <C> [BOLD] 78.63 <C> 79.17 <C> 59.68 <C> 41.53 <C> [BOLD] 57.09 <C> 70.42 <C> [BOLD] 34.76 <C> 37.21 <C> 75.34 <C> 72.43 <C> 68.38 <C> 79.19 <C> 82.20 <R> <C> ≤3 <C> 77.51 <C> [BOLD] 79.92 <C> 59.61 <C> [BOLD] 41.58 <C> 56.69 <C> [BOLD] 70.92∗ <C> 34.65 <C> [BOLD] 37.42 <C> [BOLD] 75.96∗ <C> [BOLD] 72.92∗ <C> [BOLD] 68.71 <C> [BOLD] 79.52∗ <C> 82.35 <R> <C> ≤4 <C> 75.85 <C> 72.66 <C> 59.75 <C> 41.23 <C> 56.74 <C> 70.32 <C> 34.51 <C> 37.01 <C> 74.92 <C> 72.37 <C> 67.87 <C> 78.18 <C> 82.25 <R> <C> ≤5 <C> 75.19 <C> 74.63 <C> [BOLD] 60.54∗ <C> 40.84 <C> 56.92 <C> 70.50 <C> 34.67 <C> 37.21 <C> 74.76 <C> 72.21 <C> 68.48 <C> 77.18 <C> [BOLD] 82.60∗ <CAP> Table 2: The results on word similarity, analogy, relation classification and short-text classification tasks reported by the word embeddings learnt using k-way co-occurrences for different k values.
<R> <C> [BOLD] Method <C> [BOLD] BEA-19 <C> [BOLD] CoNLL-14 <R> <C> [BOLD] 6 Layers <C> [BOLD] 6 Layers <C> [BOLD] 6 Layers <R> <C> [BOLD] Transformer <C> 57.1 <C> 53.8 <R> <C> [BOLD] Ours <C> [BOLD] 58.4 <C> [BOLD] 54.7 <R> <C> -w/o pseudo future context modeling <C> 57.6 <C> 54.0 <R> <C> -w/o encoder-decoder sharing <C> 58.2 <C> 54.5 <R> <C> [BOLD] 12 Layers <C> [BOLD] 12 Layers <C> [BOLD] 12 Layers <R> <C> [BOLD] Transformer- 12 layers <C> 56.3 <C> 52.9 <R> <C> [BOLD] Ours-12 layers <C> [BOLD] 59.2 <C> [BOLD] 55.3 <R> <C> -w/o pseudo future context modeling <C> 58.6 <C> 54.2 <R> <C> -w/o encoder-decoder sharing <C> 58.8 <C> 54.5 <CAP> Table 2: Ablation study result for comparing the contribution of the pseudo future context modeling method and the encoder-decoder sharing mechanism in the proposed PBD approach.
<R> <C> [BOLD] Method <C> [BOLD] Spell <C> [BOLD] OCR <R> <C> [BOLD] LSTM-soft <C> 46.3 <C> 79.9 <R> <C> [BOLD] LSTM-hard <C> 52.2 <C> 58.4 <R> <C> [BOLD] ribeiro2018local <C> 54.1 <C> 81.8 <R> <C> [BOLD] PIE <C> 67.0 <C> 87.6 <R> <C> [BOLD] 2 Layers <C> [BOLD] 2 Layers <C> [BOLD] 2 Layers <R> <C> [BOLD] Transformer <C> 67.6 <C> 84.5 <R> <C> [BOLD] Ours <C> [BOLD] 69.2 <C> [BOLD] 88.7 <R> <C> [BOLD] 4 Layers <C> [BOLD] 4 Layers <C> [BOLD] 4 Layers <R> <C> [BOLD] Transformer-4 layers <C> 67.1 <C> 85.4 <R> <C> [BOLD] Ours-4 layers <C> [BOLD] 70.4 <C> [BOLD] 89.6 <CAP> Table 3: The performance (accuracy) of different compared models on the spell and OCR correction tasks.
<R> <C> [BOLD] Metric <C> [ITALIC] r <C> [ITALIC] ρ <R> <C> GLEU+ <C> 0.549 <C> 0.401 <R> <C> GLEU0 <C> 0.542 <C> 0.555 <R> <C> M2 <C> 0.358 <C> 0.429 <R> <C> I-measure <C> -0.051 <C> -0.005 <R> <C> BLEU <C> -0.125 <C> -0.225 <CAP> Table 2: Correlation between automatic metrics and the human ranking.
<R> <C> query <C> CTC  [ITALIC] random <C> CTC  [ITALIC] entropy <C> CTC  [ITALIC] pCTC <C> CTC  [ITALIC] EGL <C> CER  [ITALIC] random <C> CER  [ITALIC] entropy <C> CER  [ITALIC] pCTC <C> CER  [ITALIC] EGL <C> WER  [ITALIC] random <C> WER  [ITALIC] entropy <C> WER  [ITALIC] pCTC <C> WER  [ITALIC] EGL <R> <C> 10% <C> 35.97 <C> [BOLD] 32.88 <C> 33.10 <C> 33.24 <C> 12.70 <C> 11.59 <C> 11.73 <C> [BOLD] 11.36 <C> 31.47 <C> 28.29 <C> 28.62 <C> [BOLD] 28.15 <R> <C> 20% <C> 31.31 <C> 29.48 <C> 29.44 <C> [BOLD] 28.14 <C> 10.84 <C> 10.20 <C> 10.17 <C> [BOLD] 9.59 <C> 26.57 <C> 25.29 <C> 25.29 <C> [BOLD] 23.63 <R> <C> 30% <C> 28.80 <C> 27.27 <C> 27.02 <C> [BOLD] 26.43 <C> 9.81 <C> 9.44 <C> 9.22 <C> [BOLD] 9.12 <C> 24.08 <C> 23.32 <C> 22.88 <C> [BOLD] 22.50 <R> <C> 40% <C> 27.23 <C> 25.62 <C> 25.59 <C> [BOLD] 25.31 <C> 9.28 <C> 8.75 <C> 8.68 <C> [BOLD] 8.67 <C> 22.75 <C> 21.51 <C> 21.37 <C> [BOLD] 21.26 <CAP> Table 1: Performance metrics at various query percentages (smaller is better, best in bold)
<R> <C> [EMPTY] <C> [BOLD] PDTB-based Data Set <C> [BOLD] SEW-based Data Set <R> <C> [BOLD] Source <C> Penn Discourse <C> Simple English <R> <C> [EMPTY] <C> Treebank Corpus <C> Wikipedia Corpus <R> <C> [BOLD] # of pairs of articles <C> 378 <C> 1988 <R> <C> [BOLD] # of positive pairs <C> 194 <C> 944 <R> <C> [BOLD] # of negative pairs <C> 184 <C> 944 <R> <C> [BOLD] Discourse Annotation <C> Manually Annotated <C> Extracted using <R> <C> [EMPTY] <C> [EMPTY] <C> End-to-End parser  <CAP> Table 1: Summary of the two data sets.
<R> <C> [EMPTY] <C> en-p <C> en-np <C> de-p <C> de-np <C> es-p <C> es-np <R> <C> # Multilingual Editors <C> 3,832 <C> 7,784 <C> 1,631 <C> 1,640 <C> 996 <C> 1,510 <R> <C> # Article Edit Sessions <C> 200,883 <C> 36,959 <C> 112,788 <C> 7,334 <C> 63,947 <C> 5,609 <R> <C> # Edits <C> 298,868 <C> 51,665 <C> 151,014 <C> 9,111 <C> 104,341 <C> 7,757 <R> <C> # Edited paragraphs <C> 1,447,692 <C> 230,893 <C> 816,647 <C> 27,656 <C> 554,762 <C> 25,340 <CAP> Table 1: Number of editors, article edit sessions, edits, and edit paragraphs. There is more activity in the English edition (en) than in either the German (de) or Spanish (es) edition. In all three language editions there are more primary editors (p) than non-primary editors (np) and primary editors are more active than non-primary editors.
<R> <C> [BOLD] Language <C> [BOLD] Data <C> [BOLD] Model  [BOLD] GLAD <C> [BOLD] Model  [BOLD] GCE <C> [BOLD] Model  [BOLD] G-SAT <R> <C> English <C> Dev <C> 88.4±0.6 <C> 89.0±1.0 <C> [BOLD] 89.0±0.6 <R> <C> English <C> Test <C> 84.6±0.7 <C> 85.1±1.1 <C> [BOLD] 87.6±0.6 * <R> <C> Italian <C> Dev <C> 73.2±1.3 <C> 72.1±0.9 <C> [BOLD] 76.6±1.4 * <R> <C> Italian <C> Test <C> 76.3±1.2 <C> 77.2±1.7 <C> [BOLD] 79.4±1.5 * <R> <C> German <C> Dev <C> 52.3±1.4 <C> 52.1±1.0 <C> [BOLD] 56.4±1.0 * <R> <C> German <C> Test <C> 59.3±1.9 <C> 59.8±1.2 <C> [BOLD] 62.4±1.4 * <CAP> Table 2: Joint Goal performance on WOZ2.0 test data: all models trained without pre-trained embeddings. * indicates statistically significant [22] than both GLAD and GCE, using Wilcoxon signed-rank test (with p<0.05).
<R> <C> [BOLD] Language <C> [BOLD] Model  [BOLD] GLAD <C> [BOLD] Model  [BOLD] GCE <C> [BOLD] Model  [BOLD] G-SAT <R> <C> English <C> 97.0±0.3 <C> 96.8±0.3 <C> [BOLD] 97.2±0.2 <R> <C> Italian <C> [BOLD] 95.9±0.2 <C> [BOLD] 95.9±0.2 <C> 95.8±0.2 <R> <C> German <C> 94.7±0.4 <C> 94.4±0.4 <C> [BOLD] 94.8±0.4 <CAP> Table 3: Turn Request performance on WOZ2.0 test data: all models trained without pre-trained embeddings.
<R> <C> Dataset <C> Random <C> GloVe <C> GloVe+ Char <C> GloVe+ CoVe-S <C> GloVe+ CoVe-M <C> GloVe+ CoVe-L <C> GloVe+ Char+CoVe-L <R> <C> SST-2 <C> 84.2 <C> 88.4 <C> 90.1 <C> 89.0 <C> 90.9 <C> 91.1 <C> [BOLD] 91.2 <R> <C> SST-5 <C> 48.6 <C> 53.5 <C> 52.2 <C> 54.0 <C> 54.7 <C> 54.5 <C> [BOLD] 55.2 <R> <C> IMDb <C> 88.4 <C> 91.1 <C> 91.3 <C> 90.6 <C> 91.6 <C> 91.7 <C> [BOLD] 92.1 <R> <C> TREC-6 <C> 88.9 <C> 94.9 <C> 94.7 <C> 94.7 <C> 95.1 <C> 95.8 <C> [BOLD] 95.8 <R> <C> TREC-50 <C> 81.9 <C> 89.2 <C> 89.8 <C> 89.6 <C> 89.6 <C> 90.5 <C> [BOLD] 91.2 <R> <C> SNLI <C> 82.3 <C> 87.7 <C> 87.7 <C> 87.3 <C> 87.5 <C> 87.9 <C> [BOLD] 88.1 <R> <C> SQuAD <C> 65.4 <C> 76.0 <C> 78.1 <C> 76.5 <C> 77.1 <C> 79.5 <C> [BOLD] 79.9 <CAP> Table 2: CoVe improves validation performance. CoVe has an advantage over character n-gram embeddings, but using both improves performance further. Models benefit most by using an MT-LSTM trained with MT-Large (CoVe-L). Accuracy is reported for classification tasks, and F1 is reported for SQuAD.
<R> <C> Model <C> EM <C> F1 <R> <C> LR (Rajpurkar et al.,  2016 ) <C> 40.0 <C> 51.0 <R> <C> DCR (Yu et al.,  2017 ) <C> 62.5 <C> 72.1 <R> <C> hM-LSTM+AP (Wang and Jiang,  2017 ) <C> 64.1 <C> 73.9 <R> <C> DCN+Char (Xiong et al.,  2017 ) <C> 65.4 <C> 75.6 <R> <C> BiDAF (Seo et al.,  2017 ) <C> 68.0 <C> 77.3 <R> <C> R-NET (Wang et al.,  2017 ) <C> 71.1 <C> 79.5 <R> <C> [BOLD] DCN+Char+CoVe  [ [BOLD] Ours] <C> [BOLD] 71.3 <C> [BOLD] 79.9 <CAP> Table 3: Exact match and F1 validation scores for single-model question answering.
<R> <C> Dataset <C> GloVe+Char+ Skip-Thought <C> GloVe+Char+ CoVe-L <R> <C> SST-2 <C> 88.7 <C> [BOLD] 91.2 <R> <C> SST-5 <C> 52.1 <C> [BOLD] 55.2 <R> <C> TREC-6 <C> 94.2 <C> [BOLD] 95.8 <R> <C> TREC-50 <C> 89.6 <C> [BOLD] 91.2 <R> <C> SNLI <C> 86.0 <C> [BOLD] 88.1 <CAP> Table 5: Classification validation accuracies with skip-thought and CoVe.
<R> <C> Dataset <C> # Relations <C> # Examples <C> Neg. examples <R> <C> SemEval <C> 19 <C> 10,717 <C> 17.4% <R> <C> TACRED <C> 42 <C> 106,264 <C> 79.5% <CAP> Table 1: Comparison of datasets used for evaluation
<R> <C> Masking Strategy <C> each edit P@1 <C> each edit R@1 <C> each edit F0.5@1 <C> last edit P@1 <C> last edit R@1 <C> last edit F0.5@1 <R> <C> # origin <C> 0.632 <C> 0.853 <C> 0.667 <C> 0.592 <C> 0.824 <C> 0.627 <R> <C> # target <C> 0.66 <C> 0.887 <C> 0.696 <C> 0.614 <C> 0.855 <C> 0.651 <R> <C> single <C> [BOLD] 0.763 <C> [BOLD] 0.931 <C> [BOLD] 0.790 <C> [BOLD] 0.767 <C> [BOLD] 0.920 <C> [BOLD] 0.794 <CAP> Table 1: Sentence-level evaluation with different masking strategies for single edit pairs. Subword predictions are merged in sentence generation.
<R> <C> Scenario I  [ITALIC] DS→ [ITALIC] DT <C> Scenario I  [ITALIC] DS→ [ITALIC] DT <C> DecNet Base <C> Transferred up to Encod <C> Transferred up to conv 7 <C> Transferred up to conv 8 <C> Transferred up to conv 9 <C> U-net Base <C> Transferred up to Encod <C> Transferred up to conv 7 <C> Transferred up to conv 8 <C> Transferred up to conv 9 <R> <C> Test  [ITALIC] DS <C> Loss <C> [BOLD] 0.2888 <C> 0.3212 <C> 0.3296 <C> 0.3711 <C> 0.4434 <C> [BOLD] 0.2269 <C> 0.3034 <C> 0.3203 <C> 0.3431 <C> 0.2464 <R> <C> Test  [ITALIC] DS <C> Dice <C> [BOLD] 0.6584 <C> 0.5957 <C> 0.5744 <C> 0.5768 <C> 0.5891 <C> [BOLD] 0.6884 <C> 0.5818 <C> 0.5777 <C> 0.6274 <C> 0.6213 <R> <C> Test  [ITALIC] DT <C> Loss <C> 0.4999 <C> 0.3573 <C> [BOLD] 0.3252 <C> 0.4100 <C> 0.4513 <C> 0.4805 <C> [BOLD] 0.3129 <C> 0.3600 <C> 0.3963 <C> 0.4627 <R> <C> Test  [ITALIC] DT <C> Dice <C> 0.5011 <C> [BOLD] 0.5779 <C> 0.5760 <C> 0.5131 <C> 0.5622 <C> 0.4664 <C> [BOLD] 0.6306 <C> 0.5074 <C> 0.5558 <C> 0.3808 <R> <C> Scenario II  [ITALIC] DT→ [ITALIC] DS <C> Scenario II  [ITALIC] DT→ [ITALIC] DS <C> DecNet <C> Transferred up to <C> Transferred up to <C> Transferred up to <C> Transferred up to <C> U-net <C> Transferred up to <C> Transferred up to <C> Transferred up to <C> Transferred up to <R> <C> [EMPTY] <C> [EMPTY] <C> Base <C> Encod <C> conv 7 <C> conv 8 <C> conv 9 <C> Base <C> Encod <C> conv 7 <C> conv 8 <C> conv 9 <R> <C> Test  [ITALIC] DT <C> Loss <C> [BOLD] 0.3286 <C> 0.4423 <C> 0.4981 <C> 0.4856 <C> 0.4332 <C> [BOLD] 0.3736 <C> 0.5100 <C> 0.5777 <C> 0.5591 <C> 0.6015 <R> <C> Test  [ITALIC] DT <C> Dice <C> [BOLD] 0.6570 <C> 0.4685 <C> 0.3977 <C> 0.3611 <C> 0.4732 <C> [BOLD] 0.5901 <C> 0.4052 <C> 0.2931 <C> 0.3032 <C> 0.2301 <R> <C> Test  [ITALIC] DS <C> Loss <C> 0.4831 <C> [BOLD] 0.2571 <C> 0.2997 <C> 0.2986 <C> 0.2908 <C> 0.4253 <C> [BOLD] 0.2635 <C> 0.3363 <C> 0.3296 <C> 0.3270 <R> <C> Test  [ITALIC] DS <C> Dice <C> 0.5299 <C> [BOLD] 0.6378 <C> 0.5816 <C> 0.5659 <C> 0.5921 <C> 0.5283 <C> [BOLD] 0.6211 <C> 0.5263 <C> 0.5361 <C> 0.5268 <CAP> Table 1: Quantitative results of each scenario. Negative knowledge transferring can be seen in the two first columns for both models.
<R> <C> [BOLD] Model <C> [BOLD] Newsela  [BOLD] BLEU <C> [BOLD] Newsela  [BOLD] SARI <C> [BOLD] WikiSmall  [BOLD] BLEU <C> [BOLD] WikiSmall  [BOLD] SARI <C> [BOLD] WikiLarge  [BOLD] BLEU <C> [BOLD] WikiLarge  [BOLD] SARI <R> <C> Pbmt-R <C> 18.19 <C> 15.77 <C> 46.31 <C> 15.97 <C> 81.11 <C> 38.56 <R> <C> Hybrid <C> 14.46 <C> [BOLD] 30.00 <C> [BOLD] 53.94 <C> [BOLD] 30.46 <C> 48.97 <C> 31.40 <R> <C> Sbmt-Sari <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 73.08 <C> [BOLD] 39.96 <R> <C> Dress <C> 23.21 <C> 27.37 <C> 34.53 <C> 27.48 <C> 77.18 <C> 37.08 <R> <C> Dress-Ls <C> 24.30 <C> 26.63 <C> 36.32 <C> 27.24 <C> 80.12 <C> 37.27 <R> <C> LstmLstm-B <C> 24.38 <C> 27.66 <C> 50.53 <C> 17.67 <C> 88.81 <C> 34.22 <R> <C> NseLstm-B <C> [BOLD] 26.31 <C> 27.42 <C> 53.42 <C> 17.47 <C> [BOLD] 92.02 <C> 33.43 <R> <C> LstmLstm-S <C> 23.50 <C> 28.67 <C> 31.32 <C> 28.04 <C> 81.95 <C> 35.45 <R> <C> NseLstm-S <C> 22.62 <C> 29.58 <C> 29.72 <C> 29.75 <C> 80.43 <C> 36.88 <CAP> Table 2: Model performance using automatic evaluation measures (BLEU and SARI).
<R> <C> [BOLD] Dataset ID <C> [BOLD] Chunk size <C> [BOLD] Psychotic <C> [BOLD] Control <C> [BOLD] Depressive <C> [BOLD] Total <R> <C> belabBERT-505 <C> 505 <C> 294 <C> 274 <C> 24 <C> 592 <R> <C> belabBERT-220 <C> 220 <C> 625 <C> 589 <C> 52 <C> 1266 <R> <C> RobBERT-505 <C> 505 <C> 499 <C> 127 <C> 41 <C> 1012 <R> <C> RobBERT-220 <C> 220 <C> 1096 <C> 1043 <C> 92 <C> 2231 <R> <C> [BOLD] Full <C> — <C> 76 <C> 59 <C> 6 <C> 141 <CAP> Table 3.1: Total amount of samples after chunking with different chunk lengths and different tokenizers
<R> <C> [EMPTY] <C> N1 Texts <C> N2 Texts <C> N3 Texts <C> N4 Texts <C> N5 Texts <R> <C> N1 Templates <C> 3.536 <C> 2.342 <C> 0.295 <C> 0.230 <C> 0.146 <R> <C> N2 Templates <C> 8.141 <C> 6.110 <C> 4.130 <C> 0.922 <C> 0.146 <R> <C> N3 Templates <C> 15.954 <C> 14.308 <C> 5.900 <C> 2.765 <C> 0.582 <R> <C> N4 Templates <C> 65.214 <C> 60.081 <C> 51.327 <C> 37.327 <C> 4.803 <R> <C> N5 Templates <C> 299.178 <C> 306.059 <C> 323.599 <C> 267.972 <C> 256.477 <CAP> Table 1: Grammatical Templates in Japanese, with hyphens denoting words to be filled in. Note that some grammatical templates may impose requirements of some properties (e.g. part of speech or form) on the missing words.
<R> <C> [EMPTY] <C> [BOLD] Gene Prec <C> [BOLD] Gene Rec <C> [BOLD] Gene F1 <C> [BOLD] Chemical Prec <C> [BOLD] Chemical Rec <C> [BOLD] Chemical F1 <C> [BOLD] Disease Prec <C> [BOLD] Disease Rec <C> [BOLD] Disease F1 <C> [BOLD] Total Prec <C> [BOLD] Total Rec <C> [BOLD] Total F1 <R> <C> SciSpacy (BIONLP13CG) <C> [BOLD] 91.48 <C> [BOLD] 82.06 <C> [BOLD] 86.51 <C> 64.66 <C> 39.81 <C> 49.28 <C> 8.11 <C> 2.75 <C> 4.11 <C> 76.36 <C> 53.59 <C> 62.98 <R> <C> SciSpacy (BC5CDR) <C> - <C> - <C> - <C> [BOLD] 86.97 <C> 51.86 <C> 64.69 <C> [BOLD] 80.31 <C> 59.65 <C> 68.46 <C> [BOLD] 82.40 <C> 54.57 <C> 65.66 <R> <C> Ours <C> 82.14 <C> 74.68 <C> 78.23 <C> 82.93 <C> [BOLD] 75.22 <C> [BOLD] 78.89 <C> 75.73 <C> [BOLD] 68.42 <C> [BOLD] 71.89 <C> 81.29 <C> [BOLD] 73.65 <C> [BOLD] 77.28 <CAP> Table 1: Performance comparison on three major biomedical entity types in COVID-19 corpus.
<R> <C> [BOLD] Setting <C> [BOLD] Accuracy <R> <C> Random <C> 24.89 <R> <C> Train for 3 epochs <C> – <R> <C> + window & title.bg & pass.ngram <C> 29.62 <R> <C> + passage.bg & passage <C> 39.35 <R> <C> – title.bg <C> 39.69 <R> <C> + passage.bg^2 <C> 40.26 <R> <C> + title.bg^2 <C> 40.30 <R> <C> + bigger window <C> 36.54 <R> <C> + paragraph split <C> 42.23 <R> <C> + Slavic pre-training <C> 33.270034181541966 <R> <C> Train for 1 epoch best <C> 40.25826053930877 <R> <C> Train for 2 epochs best <C> 41.89137865552602 <CAP> Table 4: Accuracy on the Bulgarian testset: ablation study when sequentially adding/removing different model components.
<R> <C> [BOLD] #docs <C> [BOLD] Overall <C> [BOLD] biology-12th <C> [BOLD] philosophy-12th <C> [BOLD] geography-12th <C> [BOLD] history-12th <C> [BOLD] history-quiz <R> <C> Random <C> Random <C> Random <C> Random <C> Random <C> Random <C> Random <R> <C> 0 <C> 24.88619119878604 <C> 26.08695652173913 <C> 24.444444444444443 <C> 24.18300653594771 <C> 25.871559633027523 <C> 24.02912621359223 <R> <C> Window Small <C> Window Small <C> Window Small <C> Window Small <C> Window Small <C> Window Small <C> Window Small <R> <C> title.bulgarian, passage.bulgarian <C> title.bulgarian, passage.bulgarian <C> title.bulgarian, passage.bulgarian <C> title.bulgarian, passage.bulgarian <C> title.bulgarian, passage.bulgarian <C> title.bulgarian, passage.bulgarian <C> title.bulgarian, passage.bulgarian <R> <C> 1 <C> 39.95442461071022 <C> 40.274599542334094 <C> 40.63492063492063 <C> 34.967320261437905 <C> 42.988929889298895 <C> 41.99029126213592 <R> <C> 2 <C> 40.22028104823395 <C> 40.274599542334094 <C> 40.63492063492063 <C> 35.947712418300654 <C> 42.61992619926199 <C> 42.71844660194175 <R> <C> 5 <C> 40.22028104823395 <C> 38.901601830663616 <C> 40.63492063492063 <C> 38.071895424836605 <C> 41.51291512915129 <C> 42.47572815533981 <R> <C> 10 <C> 38.663121914166354 <C> 40.503432494279174 <C> 39.84126984126984 <C> 35.45751633986928 <C> 39.298892988929886 <C> 38.83495145631068 <R> <C> 20 <C> 36.84010634257501 <C> 37.52860411899314 <C> 39.04761904761905 <C> 33.8235294117647 <C> 38.745387453874535 <C> 34.70873786407767 <R> <C> title.bulgarian, passage.ngram <C> title.bulgarian, passage.ngram <C> title.bulgarian, passage.ngram <C> title.bulgarian, passage.ngram <C> title.bulgarian, passage.ngram <C> title.bulgarian, passage.ngram <C> title.bulgarian, passage.ngram <R> <C> 1 <C> 28.940372199012533 <C> 29.06178489702517 <C> 32.06349206349206 <C> 27.287581699346404 <C> 27.490774907749078 <C> 28.398058252427184 <R> <C> 2 <C> 29.092290163311812 <C> 29.06178489702517 <C> 33.333333333333336 <C> 25.0 <C> 28.782287822878228 <C> 29.12621359223301 <R> <C> 5 <C> 29.05431067223699 <C> 27.45995423340961 <C> 32.06349206349206 <C> 26.633986928104576 <C> 30.627306273062732 <C> 27.66990291262136 <R> <C> 10 <C> 29.624003038359287 <C> 29.06178489702517 <C> 32.53968253968254 <C> 26.96078431372549 <C> 30.07380073800738 <C> 29.12621359223301 <R> <C> 20 <C> 29.43410558298519 <C> 31.80778032036613 <C> 32.698412698412696 <C> 26.633986928104576 <C> 28.59778597785978 <C> 27.184466019417474 <R> <C> title.bulgarian, passage.ngram, passage, passage.bulgarian <C> title.bulgarian, passage.ngram, passage, passage.bulgarian <C> title.bulgarian, passage.ngram, passage, passage.bulgarian <C> title.bulgarian, passage.ngram, passage, passage.bulgarian <C> title.bulgarian, passage.ngram, passage, passage.bulgarian <C> title.bulgarian, passage.ngram, passage, passage.bulgarian <C> title.bulgarian, passage.ngram, passage, passage.bulgarian <R> <C> 1 <C> 38.32130649449297 <C> 38.215102974828376 <C> 40.0 <C> 34.47712418300654 <C> 39.48339483394834 <C> 40.04854368932039 <R> <C> 2 <C> 39.080896315989364 <C> 37.07093821510298 <C> 40.317460317460316 <C> 34.47712418300654 <C> 40.59040590405904 <C> 44.1747572815534 <R> <C> 5 <C> 39.3467527535131 <C> 40.961098398169334 <C> 39.84126984126984 <C> 34.64052287581699 <C> 41.32841328413284 <C> 41.262135922330096 <R> <C> 10 <C> 38.62514242309153 <C> 40.503432494279174 <C> 40.63492063492063 <C> 33.49673202614379 <C> 40.40590405904059 <C> 38.83495145631068 <R> <C> 20 <C> 36.53627041397645 <C> 38.672768878718536 <C> 37.93650793650794 <C> 31.372549019607842 <C> 37.45387453874539 <C> 38.592233009708735 <R> <C> passage.ngram, passage, passage.bulgarian^2 <C> passage.ngram, passage, passage.bulgarian^2 <C> passage.ngram, passage, passage.bulgarian^2 <C> passage.ngram, passage, passage.bulgarian^2 <C> passage.ngram, passage, passage.bulgarian^2 <C> passage.ngram, passage, passage.bulgarian^2 <C> passage.ngram, passage, passage.bulgarian^2 <R> <C> 1 <C> 39.68856817318648 <C> 40.274599542334094 <C> 40.63492063492063 <C> 35.130718954248366 <C> 42.06642066420664 <C> 41.262135922330096 <R> <C> 2 <C> 40.25826053930877 <C> 39.816933638443935 <C> 40.95238095238095 <C> 35.947712418300654 <C> 42.61992619926199 <C> 42.96116504854369 <R> <C> 5 <C> 39.57462969996202 <C> 39.588100686498855 <C> 39.36507936507937 <C> 37.254901960784316 <C> 40.95940959409594 <C> 41.50485436893204 <R> <C> 10 <C> 38.70110140524117 <C> 41.18993135011441 <C> 39.523809523809526 <C> 35.78431372549019 <C> 39.298892988929886 <C> 38.349514563106794 <R> <C> 20 <C> 37.143942271173565 <C> 39.359267734553775 <C> 37.77777777777778 <C> 35.294117647058826 <C> 38.37638376383764 <C> 34.95145631067961 <R> <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <R> <C> 1 <C> 39.840486137485755 <C> 40.274599542334094 <C> 40.79365079365079 <C> 35.130718954248366 <C> 42.25092250922509 <C> 41.74757281553398 <R> <C> 2 <C> 40.29624003038359 <C> 40.274599542334094 <C> 40.63492063492063 <C> 36.111111111111114 <C> 42.80442804428044 <C> 42.71844660194175 <R> <C> 5 <C> 40.25826053930877 <C> 39.130434782608695 <C> 40.63492063492063 <C> 38.39869281045752 <C> 41.14391143911439 <C> 42.47572815533981 <R> <C> 10 <C> 38.73908089631599 <C> 40.503432494279174 <C> 39.682539682539684 <C> 35.62091503267974 <C> 39.48339483394834 <C> 39.077669902912625 <R> <C> 20 <C> 37.06798328902393 <C> 37.75743707093822 <C> 39.04761904761905 <C> 34.64052287581699 <C> 38.56088560885609 <C> 34.95145631067961 <R> <C> Window Big <C> Window Big <C> Window Big <C> Window Big <C> Window Big <C> Window Big <C> Window Big <R> <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <R> <C> 1 <C> 31.21914166350171 <C> 28.37528604118993 <C> 33.96825396825397 <C> 29.41176470588235 <C> 30.81180811808118 <C> 33.25242718446602 <R> <C> 2 <C> 33.11811621724269 <C> 31.57894736842105 <C> 37.46031746031746 <C> 31.209150326797385 <C> 33.94833948339483 <C> 29.854368932038835 <R> <C> 5 <C> 36.0425370300038 <C> 35.69794050343249 <C> 38.095238095238095 <C> 33.8235294117647 <C> 37.82287822878229 <C> 34.22330097087379 <R> <C> 10 <C> 36.53627041397645 <C> 37.29977116704806 <C> 36.03174603174603 <C> 33.98692810457516 <C> 39.298892988929886 <C> 36.650485436893206 <R> <C> 20 <C> 35.62476262818078 <C> 34.55377574370709 <C> 39.682539682539684 <C> 31.045751633986928 <C> 38.37638376383764 <C> 33.737864077669904 <R> <C> Paragraph <C> Paragraph <C> Paragraph <C> Paragraph <C> Paragraph <C> Paragraph <C> Paragraph <R> <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <C> title.bulgarian^2, passage.ngram, passage, passage.bulgarian^2 <R> <C> 1 <C> 41.815419673376375 <C> 41.41876430205949 <C> 42.06349206349206 <C> 38.071895424836605 <C> 40.95940959409594 <C> 48.54368932038835 <R> <C> 2 <C> 42.23319407519939 <C> 42.5629290617849 <C> 43.17460317460318 <C> 35.62091503267974 <C> 42.988929889298895 <C> 49.271844660194176 <R> <C> 5 <C> 41.58754272692746 <C> 43.24942791762014 <C> 40.317460317460316 <C> 38.72549019607843 <C> 40.03690036900369 <C> 48.05825242718446 <R> <C> 10 <C> 39.46069122673756 <C> 40.961098398169334 <C> 38.41269841269841 <C> 36.928104575163395 <C> 39.85239852398524 <C> 42.71844660194175 <R> <C> 20 <C> 37.52373718192176 <C> 39.130434782608695 <C> 37.61904761904762 <C> 34.64052287581699 <C> 38.56088560885609 <C> 38.592233009708735 <R> <C> Slavic BERT <C> Slavic BERT <C> Slavic BERT <C> Slavic BERT <C> Slavic BERT <C> Slavic BERT <C> Slavic BERT <R> <C> 1 <C> 33.19407519939233 <C> 30.892448512585812 <C> 33.17460317460318 <C> 28.758169934640524 <C> 32.28782287822878 <C> 43.44660194174757 <R> <C> 2 <C> 33.270034181541966 <C> 31.57894736842105 <C> 31.904761904761905 <C> 31.209150326797385 <C> 35.239852398523986 <C> 37.62135922330097 <R> <C> 5 <C> 31.14318268135207 <C> 30.205949656750573 <C> 30.158730158730158 <C> 29.248366013071895 <C> 30.99630996309963 <C> 36.650485436893206 <R> <C> 10 <C> 30.421572350930496 <C> 29.290617848970253 <C> 29.682539682539684 <C> 29.73856209150327 <C> 31.918819188191883 <C> 31.796116504854368 <R> <C> 20 <C> 29.661982529434106 <C> 28.60411899313501 <C> 29.365079365079364 <C> 28.431372549019606 <C> 32.103321033210335 <C> 29.854368932038835 <CAP> Table 5: Evaluation results for the Bulgarian multiple-choice reading comprehension task: comparison of various indexing and query strategies.
<R> <C> Model <C> BLEU-1 c5 <C> BLEU-1 c40 <C> BLEU-2 c5 <C> BLEU-2 c40 <C> BLEU-3 c5 <C> BLEU-3 c40 <C> BLEU-4 c5 <C> BLEU-4 c40 <C> METEOR c5 <C> METEOR c40 <C> ROUGE-L c5 <C> ROUGE-L c40 <C> CIDEr-D c5 <C> CIDEr-D c40 <R> <C> SCN-LSTM <C> [BOLD] 0.740 <C> [BOLD] 0.917 <C> [BOLD] 0.575 <C> [BOLD] 0.839 <C> [BOLD] 0.436 <C> [BOLD] 0.739 <C> [BOLD] 0.331 <C> [BOLD] 0.631 <C> [BOLD] 0.257 <C> [BOLD] 0.348 <C> [BOLD] 0.543 <C> [BOLD] 0.696 <C> [BOLD] 1.003 <C> [BOLD] 1.013 <R> <C> ATT <C> 0.731 <C> 0.900 <C> 0.565 <C> 0.815 <C> 0.424 <C> 0.709 <C> 0.316 <C> 0.599 <C> 0.250 <C> 0.335 <C> 0.535 <C> 0.682 <C> 0.943 <C> 0.958 <R> <C> OV <C> 0.713 <C> 0.895 <C> 0.542 <C> 0.802 <C> 0.407 <C> 0.694 <C> 0.309 <C> 0.587 <C> 0.254 <C> 0.346 <C> 0.530 <C> 0.682 <C> 0.943 <C> 0.946 <R> <C> MSR Cap <C> 0.715 <C> 0.907 <C> 0.543 <C> 0.819 <C> 0.407 <C> 0.710 <C> 0.308 <C> 0.601 <C> 0.248 <C> 0.339 <C> 0.526 <C> 0.680 <C> 0.931 <C> 0.937 <CAP> Table 2: Comparison to published state-of-the-art image captioning models on the blind test set as reported by the COCO test server. SCN-LSTM is our model. ATT refers to ATT VC [54], OV refers to OriolVinyals [48], and MSR Cap refers to MSR Captivator [9].
<R> <C> Model <C> B-4 <C> M <C> C <R> <C> S2VT  <C> − <C> 0.292 <C> − <R> <C> LSTM-E  <C> 0.453 <C> 0.310 <C> − <R> <C> GRU-RCN  <C> 0.479 <C> 0.311 <C> 0.678 <R> <C> h-RNN  <C> 0.499 <C> 0.326 <C> 0.658 <R> <C> LSTM-R <C> 0.448 <C> 0.310 <C> 0.640 <R> <C> LSTM-C <C> 0.445 <C> 0.309 <C> 0.644 <R> <C> LSTM-CR <C> 0.469 <C> 0.317 <C> 0.688 <R> <C> LSTM-T <C> 0.473 <C> 0.324 <C> 0.699 <R> <C> LSTM-CRT <C> 0.475 <C> 0.316 <C> 0.647 <R> <C> LSTM-CRT2 <C> 0.469 <C> 0.326 <C> 0.706 <R> <C> SCN-LSTM <C> 0.502 <C> 0.334 <C> 0.770 <R> <C> SCN-LSTM Ensemble of 5 <C> [BOLD] 0.511 <C> [BOLD] 0.335 <C> [BOLD] 0.777 <CAP> Table 3: Results on BLEU-4 (B-4), METEOR (M) and CIDEr-D (C) metrices compared to other state-of-the-art results and baselines on Youtube2Text.
<R> <C> Model <C> Ans.-Ans. <C> HDL <C> Plagiarism <C> Postediting <C> Ques.-Ques. <R> <C> Sent2Vec <C> 0.57739 <C> [ITALIC]  [BOLD] 0.75061 <C> 0.80068 <C> [ITALIC]  [BOLD] 0.82857 <C> [ITALIC]  [BOLD] 0.73035 <R> <C> [BOLD] WISSE <C> [BOLD] 0.65560 <C> 0.70102 <C> [BOLD] 0.80607 <C> 0.82161 <C> 0.70410 <R> <C> D2V (400d) <C> 0.41123 <C> 0.69169 <C> 0.60488 <C> 0.75547 <C> -0.02245 <R> <C> Skip-toughs <C> 0.23199 <C> 0.49643 <C> 0.48636 <C> 0.17749 <C> 0.33446 <R> <C> W2V (300d-average) <C> 0.50311 <C> 0.66362 <C> 0.72347 <C> 0.73935 <C> 0.16586 <R> <C> Binary TF–IDF (BoW) <C> 0.41133 <C> 0.54073 <C> 0.69601 <C> 0.82615 <C> 0.03844 <R> <C> Bold values indicates the best result for WISSE. <C> Bold values indicates the best result for WISSE. <C> Bold values indicates the best result for WISSE. <C> Bold values indicates the best result for WISSE. <C> Bold values indicates the best result for WISSE. <C> Bold values indicates the best result for WISSE. <R> <C> Values in bold italics indicate best state-of-the-art results. <C> Values in bold italics indicate best state-of-the-art results. <C> Values in bold italics indicate best state-of-the-art results. <C> Values in bold italics indicate best state-of-the-art results. <C> Values in bold italics indicate best state-of-the-art results. <C> Values in bold italics indicate best state-of-the-art results. <CAP> Table 6: WISSE and state-of-the-art sentence embedding models on SemEval STS datasets
<R> <C> R <C> System <C> ALL <C> Ans.-Ans. <C> HDL <C> Plagiarism <C> Postediting <C> Ques.-Ques. <R> <C> 1 <C> Samsung Pol. <C> 0.77807 <C> 0.69235 <C> 0.82749 <C> 0.84138 <C> 0.83516 <C> 0.68705 <R> <C> 2 <C> UWB <C> 0.75731 <C> 0.62148 <C> 0.81886 <C> 0.82355 <C> 0.82085 <C> 0.70199 <R> <C> 3 <C> MayoNLPTeam <C> 0.75607 <C> 0.61426 <C> 0.77263 <C> 0.805 <C> 0.8484 <C> 0.74705 <R> <C> 4 <C> Samsung Pol. <C> 0.75468 <C> 0.69235 <C> 0.82749 <C> 0.81288 <C> 0.83516 <C> 0.58567 <R> <C> 5 <C> NaCTeM <C> 0.74865 <C> 0.60237 <C> 0.8046 <C> 0.81478 <C> 0.82858 <C> 0.69367 <R> <C> 6 <C> ECNU <C> 0.75079 <C> 0.56979 <C> 0.81214 <C> 0.82503 <C> 0.82342 <C> 0.73116 <R> <C> 7 <C> UMD-TTIC-UW <C> 0.74201 <C> 0.66074 <C> 0.79457 <C> 0.81541 <C> 0.80939 <C> 0.61872 <R> <C> 9 <C> SimiHawk <C> 0.73774 <C> 0.59237 <C> 0.81419 <C> 0.80566 <C> 0.82179 <C> 0.65048 <R> <C> 8 <C> Sent2Vec <C> 0.73836 <C> 0.57739 <C> 0.75061 <C> 0.80068 <C> 0.82857 <C> 0.73035 <R> <C> 10 <C> [BOLD] WISSE <C> 0.73768 <C> 0.655600 <C> 0.70102 <C> 0.80607 <C> 0.82065 <C> 0.70410 <R> <C> 23 <C> UWB <C> 0.72622 <C> 0.64442 <C> 0.79352 <C> 0.82742 <C> 0.81209 <C> 0.53383 <R> <C> – <C> D2V (400d) <C> 0.50206 <C> 0.41123 <C> 0.69169 <C> 0.60488 <C> 0.75547 <C> -0.02245 <R> <C> – <C> Skip-toughs <C> 0.27148 <C> 0.23199 <C> 0.49643 <C> 0.48636 <C> 0.17749 <C> 0.33446 <R> <C> – <C> W2V (300d-avg) <C> 0.56007 <C> 0.50311 <C> 0.66362 <C> 0.72347 <C> 0.73935 <C> 0.16586 <R> <C> 110 <C> STS (BoW) <C> 0.51334 <C> 0.41133 <C> 0.54073 <C> 0.69601 <C> 0.82615 <C> 0.03844 <CAP> Table 7: Ranking for the SemEval STS 2016 task (this is a modified version derived from Agirre et al. (2016))
<R> <C> LDA-DP <C> Target <C> Target <C> Opinion 510.15 ± <C> Opinion 0.08 <C> Overall <C> Overall <R> <C> ILDA <C> 594.81 ± <C> 13.61 <C> 519.84 ± <C> 0.43 <C> 556.03 ± <C> 6.22 <R> <C> TOTM <C> 592.91 ± <C> 13.86 <C> [BOLD] 137.42 ± <C> 0.28 <C> [BOLD] 285.42 ± <C> 3.23 <CAP> Table 4: Test Perplexity on Electronic Product Tweets
<R> <C> [ITALIC] Sent140 Tweets <C> Accuracy <C> Precision <C> Recall <C> F1-score <R> <C> LDA-DP <C> 57.3 <C> 56.1 <C> 90.1 <C> 69.2 <R> <C> ILDA <C> 54.1 <C> 56.9 <C> 55.3 <C> 55.9 <R> <C> TOTM <C> [BOLD] 65.0 <C> [BOLD] 61.7 <C> [BOLD] 90.2 <C> [BOLD] 73.3 <R> <C> [ITALIC] SemEval Tweets <C> Accuracy <C> Precision <C> Recall <C> F1-score <R> <C> LDA-DP <C> 52.1 <C> 65.0 <C> 58.3 <C> 61.4 <R> <C> ILDA <C> 46.8 <C> 60.7 <C> 53.6 <C> 56.3 <R> <C> TOTM <C> [BOLD] 73.3 <C> [BOLD] 84.0 <C> [BOLD] 74.9 <C> [BOLD] 79.0 <CAP> Table 5: Sentiment Classification Results (%)
<R> <C> [EMPTY] <C> [ITALIC] Electronic Product Tweets Negativity <C> [ITALIC] Electronic Product Tweets Negativity <C> [ITALIC] Electronic Product Tweets Positivity <C> [ITALIC] Electronic Product Tweets Positivity <C> [ITALIC] Sent140 Tweets Negativity <C> [ITALIC] Sent140 Tweets Negativity <C> [ITALIC] Sent140 Tweets Positivity <C> [ITALIC] Sent140 Tweets Positivity <C> [ITALIC] SemEval Tweets Negativity <C> [ITALIC] SemEval Tweets Negativity <C> [ITALIC] SemEval Tweets Positivity <C> [ITALIC] SemEval Tweets Positivity <R> <C> No lexicon <C> 17.82 ± <C> 1.26 <C> 17.39 ± <C> 0.45 <C> 22.63 ± <C> 0.96 <C> 32.31 ± <C> 1.98 <C> 15.24 ± <C> 1.45 <C> 21.03 ± <C> 3.85 <R> <C> MPQA <C> [BOLD] 23.91 ± <C> 0.49 <C> 31.96 ± <C> 0.09 <C> 24.10 ± <C> 0.49 <C> [BOLD] 42.65 ± <C> 1.02 <C> 16.88 ± <C> 0.31 <C> 29.47 ± <C> 0.99 <R> <C> SentiStrength <C> 23.19 ± <C> 0.08 <C> [BOLD] 35.69 ± <C> 0.33 <C> [BOLD] 24.29 ± <C> 1.07 <C> 41.26 ± <C> 1.53 <C> [BOLD] 16.94 ± <C> 0.78 <C> [BOLD] 32.17 ± <C> 2.07 <CAP> Table 6: Sentiment Evaluations for the Sentiment Priors (in unit of 0.01)
<R> <C> Model <C> Model <C> Multi- lingual <C> BLEU (↑) Fisher-CallHome <C> BLEU (↑) Fisher-CallHome <C> BLEU (↑) Fisher-CallHome <C> BLEU (↑) Fisher-CallHome <C> BLEU (↑) Fisher-CallHome <C> BLEU (↑) Librispeech <C> BLEU (↑) ST-TED <C> BLEU (↑) ST-TED <R> <C> [EMPTY] <C> [EMPTY] <C> Multi- lingual <C> dev <C> dev2 <C> test <C> devtest <C> evltest <C> test <C> test <C> tst2013 <R> <C> E2E-ST <C> (E-B-1) Bi-ST <C> – <C> 40.4 <C> 41.4 <C> 41.5 <C> 14.1 <C> 14.2 <C> 15.7 <C> 16.0 <C> 12.5 <R> <C> E2E-ST <C> (E-B-2) + ASR-PT <C> – <C> 43.5 <C> 45.1 <C> 44.7 <C> 15.6 <C> 16.4 <C> 16.3 <C> 17.1 <C> 13.1 <R> <C> E2E-ST <C> (E-B-3) + MT-PT <C> – <C> 44.4 <C> 45.1 <C> 45.2 <C> 15.6 <C> 15.4 <C> 16.8 <C> 17.4 <C> 13.5 <R> <C> E2E-ST <C> (E-Mc-1) Multi-ST <C> M2Mc <C> 44.1 <C> 45.4 <C> 45.2 <C> 16.4 <C> 16.2 <C> 17.3 <C> 17.7 <C> [BOLD] 14.8 <R> <C> E2E-ST <C> (E-Mc-2) + ASR-PT <C> M2Mc <C> [BOLD] 46.3 <C> [BOLD] 47.1 <C> [BOLD] 46.3 <C> [BOLD] 17.3 <C> [BOLD] 17.2 <C> [BOLD] 17.6 <C> [BOLD] 18.6 <C> 14.6 <R> <C> Pipe-ST <C> Best system <C> – <C> 37.9 <C> 40.3 <C> 39.2 <C> 17.6 <C> 17.2 <C> 16.7 <C> 18.5 <C> 14.0 <CAP> Table 5: Results of the end-to-end ST systems with pre-training
<R> <C> [BOLD] Dataset <C> [BOLD] LSTM  [BOLD] type <C> [BOLD] basic-LSTM∗ <C> [BOLD] LDA+LSTM∗ 50 <C> [BOLD] LDA+LSTM∗ 100 <C> [BOLD] LDA+LSTM∗ 150 <C> [BOLD] LCLM∗ <C> [BOLD] Topic-RNN 50 <C> [BOLD] Topic-RNN 100 <C> [BOLD] Topic-RNN 150 <C> [BOLD] TDLM∗ 50 <C> [BOLD] TDLM∗ 100 <C> [BOLD] TDLM∗ 150 <C> [BOLD] TCNLM 50 <C> [BOLD] TCNLM 100 <C> [BOLD] TCNLM 150 <R> <C> APNEWS <C> small <C> 64.13 <C> 57.05 <C> 55.52 <C> 54.83 <C> 54.18 <C> 56.77 <C> 54.54 <C> 54.12 <C> 53.00 <C> 52.75 <C> 52.65 <C> 52.75 <C> 52.63 <C> [BOLD] 52.59 <R> <C> APNEWS <C> large <C> 58.89 <C> 52.72 <C> 50.75 <C> 50.17 <C> 50.63 <C> 53.19 <C> 50.24 <C> 50.01 <C> 48.96 <C> 48.97 <C> 48.21 <C> 48.07 <C> 47.81 <C> [BOLD] 47.74 <R> <C> IMDB <C> small <C> 72.14 <C> 69.58 <C> 69.64 <C> 69.62 <C> 67.78 <C> 68.74 <C> 67.83 <C> 66.45 <C> 63.67 <C> 63.45 <C> 63.82 <C> 63.98 <C> 62.64 <C> [BOLD] 62.59 <R> <C> IMDB <C> large <C> 66.47 <C> 63.48 <C> 63.04 <C> 62.78 <C> 67.86 <C> 63.02 <C> 61.59 <C> 60.14 <C> 58.99 <C> 59.04 <C> 58.59 <C> 57.06 <C> 56.38 <C> [BOLD] 56.12 <R> <C> BNC <C> small <C> 102.89 <C> 96.42 <C> 96.50 <C> 96.38 <C> 87.47 <C> 94.66 <C> 93.57 <C> 93.55 <C> 87.42 <C> 85.99 <C> 86.43 <C> 87.98 <C> 86.44 <C> [BOLD] 86.21 <R> <C> BNC <C> large <C> 94.23 <C> 88.42 <C> 87.77 <C> 87.28 <C> 80.68 <C> 85.90 <C> 84.62 <C> 84.12 <C> 82.62 <C> 81.83 <C> 80.58 <C> 80.29 <C> 80.14 <C> [BOLD] 80.12 <CAP> Table 2: Test perplexities of different models on APNEWS, IMDB and BNC. (∗) taken from Lau et al. (2017).
<R> <C> Method <C> F1 <R> <C> Bordes et al., 2014b <C> 29.7 <R> <C> Bordes et al., 2014a <C> 39.2 <R> <C> Yang et al., 2014 <C> 41.3 <R> <C> Dong et al., 2015 <C> 40.8 <R> <C> Bordes et al., 2015 <C> 42.2 <R> <C> [BOLD] ours <C> [BOLD] 42.6 <CAP> Table 1: The evaluation results on WEBQUESTIONS.
<R> <C> Method <C> Valid <C> Test <R> <C> LSTM (grave2016efficient) <C> - <C> 48.7 <R> <C> Temporal CNN (bai2018convolutional) <C> - <C> 45.2 <R> <C> GCNN (dauphin2016language) <C> - <C> 37.2 <R> <C> LSTM + Hebbian (rae2018fast) <C> 34.1 <C> 34.3 <R> <C> 4 layer QRNN (merity2018analysis) <C> 32.0 <C> 33.0 <R> <C> [BOLD] 4 layer QRNN + Ours <C> [BOLD] 30.6 <C> [BOLD] 31.6 <R> <C> + post process (rae2018fast) <C> + post process (rae2018fast) <C> + post process (rae2018fast) <R> <C> LSTM + Hebbian + Cache + MbPA (rae2018fast) <C> 29.0 <C> 29.2 <R> <C> [BOLD] 4 layer QRNN + Ours + dynamic evaluation <C> [BOLD] 27.2 <C> [BOLD] 28.0 <CAP> Table 3: Perplexities on validation and test sets on the Wikitext-103 dataset.
<R> <C> Dataset <C> Sentiment <C> Train <C> Test <C> Total <R> <C> Sentiment140 <C> Negative <C> 100 <C> 25 <C> 125 <R> <C> Sentiment140 <C> Positive <C> 100 <C> 25 <C> 125 <R> <C> Sentiment140 <C> Total <C> 200 <C> 50 <C> 250 <CAP> Table 3: Details about our Twitter Sentiment Classification dataset, composed of incorrect and correct data.
<R> <C> [BOLD] Model <C> [BOLD] F1-score (micro, %)  [ITALIC] Inc <C> [BOLD] F1-score (micro, %)  [ITALIC] Corr <C> [BOLD] F1-score (micro, %)  [ITALIC] Inc+Corr <R> <C> Rasa (spacy) <C> 44.00 <C> 54.00 <C> 54.00 <R> <C> Rasa (tensorflow) <C> 53.06 <C> 60.00 <C> 59.18 <R> <C> Dialogflow <C> 30.00 <C> 40.00 <C> 42.00 <R> <C> SAP Conversational AI <C> 59.18 <C> 65.31 <C> 59.18 <R> <C> Semantic Hashing <C> 72.00 <C> 70.00 <C> 72.00 <R> <C> BERT <C> 72.00 <C> 76.00 <C> 74.00 <R> <C> Stacked DeBERT (ours) <C> [BOLD] 80.00 <C> [BOLD] 82.00 <C> [BOLD] 80.00 <CAP> Table 6: F1-micro scores for Twitter Sentiment Classification task on Kaggle’s Sentiment140 Corpus. Note that: (Inc) is the original dataset, with naturally incorrect tweets, (Corr) is the corrected version of the dataset and (Inc+Corr) contains both.
<R> <C> [BOLD] Model <C> [BOLD] F1-score (micro, %) Complete <C> [BOLD] F1-score (micro, %) gtts-witai <C> [BOLD] F1-score (micro, %) macsay-witai <R> <C> [ITALIC] iBLEU score <C> 0.00 <C> 0.44 <C> 0.50 <R> <C> Rasa (spacy) <C> 92.45 <C> 91.51 <C> 86.79 <R> <C> Rasa (tensorflow) <C> [BOLD] 99.06 <C> 92.89 <C> 91.51 <R> <C> Dialogflow <C> 96.23 <C> 87.74 <C> 81.13 <R> <C> SAP Conversational AI <C> 95.24 <C> 94.29 <C> 94.29 <R> <C> Semantic Hashing <C> [BOLD] 99.06 <C> 95.28 <C> 91.51 <R> <C> BERT <C> 98.11 <C> 96.23 <C> 94.34 <R> <C> Stacked DeBERT (ours) <C> [BOLD] 99.06 <C> [BOLD] 97.17 <C> [BOLD] 96.23 <CAP> Table 7: F1-micro scores for original sentences and sentences imbued with STT error in the Chatbot Corpus. The noise level is represented by the iBLEU score (See Eq. (5)).
<R> <C> [EMPTY] <C> da <C> nl <C> de <C> el <C> it <C> pt <C> es <C> sv <C> tk <C> mg <R> <C> Random <C> 23.2 <C> 30.5 <C> 27.1 <C> 23.2 <C> 25.9 <C> 24.3 <C> 26.9 <C> 21.6 <C> 36.9 <C> 34.5 <R> <C> BiLSTM <C> 61.8 <C> 62.1 <C> 60.5 <C> 70.1 <C> 73.6 <C> 67.6 <C> 63.6 <C> 57.2 <C> 44.0 <C> 63.4 <R> <C> BiLSTM-Crf <C> 46.3 <C> 47.7 <C> 53.2 <C> 35.1 <C> 41.2 <C> 44.1 <C> 25.5 <C> 54.9 <C> 43.1 <C> 41.4 <R> <C> MiniTagger <C> 77.0 <C> 72.5 <C> 75.9 <C> 75.7 <C> 67.3 <C> 75.1 <C> 73.5 <C> 77.7 <C> 49.8 <C> 67.2 <R> <C> Distant +CCA <C> 73.5 <C> 64.5 <C> 57.7 <C> 53.1 <C> 59.5 <C> 67.8 <C> 63.5 <C> 66.0 <C> 57.2 <C> 49.7 <R> <C> Distant +Cluster <C> 70.4 <C> 61.7 <C> 65.9 <C> 65.5 <C> 64.8 <C> 66.9 <C> 68.4 <C> 64.1 <C> 51.7 <C> 50.2 <R> <C> BiLSTM-Debias +CCA <C> 73.2 <C> 72.8 <C> 72.5 <C> 71.2 <C> 70.7 <C> 72.1 <C> 71.1 <C> 73.1 <C> 49.2 <C> 65.9 <R> <C> BiLSTM-Debias +Cluster <C> 72.5 <C> 70.1 <C> 71.2 <C> 68.7 <C> 69.1 <C> 72.5 <C> 70.6 <C> 73.3 <C> 48.7 <C> 64.5 <R> <C> Joint +CCA <C> 81.1 <C> [BOLD] 82.3 <C> 76.1 <C> 77.5 <C> 75.9 <C> [BOLD] 82.1 <C> 79.7 <C> [BOLD] 78.1 <C> [BOLD] 72.6 <C> 75.3 <R> <C> Joint +Cluster <C> [BOLD] 81.9 <C> 81.5 <C> [BOLD] 78.9 <C> [BOLD] 80.1 <C> [BOLD] 81.9 <C> 76.7 <C> [BOLD] 81.2 <C> 78.0 <C> 70.4 <C> [BOLD] 75.7 <CAP> Table 1: POS tagging accuracy on over the ten target languages, showing first approaches using only the gold data; next methods using only distant cross-lingual supervision, and lastly joint multi-task learning. English is used as the source language and columns correspond to a specific target language.
<R> <C> Process <C> Basic <C> Secondary <R> <C> Inheritance <C> 1161 <C> 2356 <R> <C> Derivation <C> 82 <C> 183 <R> <C> Cognate <C> 303 <C> 483 <R> <C> Borrowing <C> 18 <C> 84 <R> <C> None of these <C> 42566 <C> 65969 <CAP> Table 5: Sources of foreign color words. The colors are not mutually exclusive.
<R> <C> [EMPTY] <C> Embed size <C> [BOLD] De-En 64 <C> [BOLD] De-En 128 <C> [BOLD] De-En 256 <C> [BOLD] Tr-En 64 <C> [BOLD] Tr-En 128 <C> [BOLD] Tr-En 256 <C> [BOLD] Tr-En-morph 64 <C> [BOLD] Tr-En-morph 128 <C> [BOLD] Tr-En-morph 256 <R> <C> [EMPTY] <C> Model size (M) <C> 8±1 <C> 11±1 <C> 17±1 <C> 11±1 <C> 17±1 <C> 28±1 <C> 13±1 <C> 21±1 <C> 36±1 <R> <C> 4L <C> BASE-4L <C> 28.97 <C> 29.99 <C> 30.43 <C> [BOLD] 19.80 <C> 20.26 <C> 20.99 <C> 18.90 <C> 18.81 <C> 20.08 <R> <C> 4L <C> DenseNMT-4L-1 <C> [BOLD] 30.11 <C> [BOLD] 30.80 <C> 31.26 <C> 19.21 <C> 20.08 <C> 21.36 <C> 18.83 <C> 20.16 <C> 21.43 <R> <C> 4L <C> DenseNMT-4L-2 <C> 29.77 <C> 30.01 <C> [BOLD] 31.40 <C> 19.59 <C> [BOLD] 20.86 <C> [BOLD] 21.48 <C> [BOLD] 19.04 <C> [BOLD] 20.19 <C> [BOLD] 21.57 <R> <C> 8L <C> BASE-8L <C> 30.15 <C> 30.91 <C> 31.51 <C> 20.40 <C> 21.60 <C> 21.92 <C> 20.21 <C> 20.76 <C> 22.62 <R> <C> 8L <C> DenseNMT-8L-1 <C> [BOLD] 30.91 <C> [BOLD] 31.54 <C> 32.08 <C> 21.82 <C> [BOLD] 22.20 <C> 23.20 <C> 21.20 <C> 21.73 <C> 22.60 <R> <C> 8L <C> DenseNMT-8L-2 <C> 30.70 <C> 31.17 <C> [BOLD] 32.26 <C> [BOLD] 21.93 <C> 21.98 <C> [BOLD] 23.25 <C> [BOLD] 21.73 <C> [BOLD] 22.44 <C> [BOLD] 23.45 <CAP> Table 1: BLEU score on IWSLT German-English and Turkish-English translation tasks. We compare models using different embedding sizes, and keep the model size consistent within each column.
<R> <C> [EMPTY] <C> [BOLD] De-En <C> [BOLD] Tr-En <C> [BOLD] Tr-En-morph <R> <C> BASE <C> 30.43 <C> 20.99 <C> 20.08 <R> <C> DenseENC-4L <C> 30.72 <C> 21.32 <C> 21.24 <R> <C> DenseDEC-4L <C> 31.23 <C> 21.04 <C> 21.06 <R> <C> DenseAtt-4L <C> 31.05 <C> 21.35 <C> 21.08 <R> <C> DenseNMT-4L-1 <C> 31.26 <C> 21.36 <C> 21.43 <R> <C> DenseNMT-4L-2 <C> 31.40 <C> 21.48 <C> 21.57 <CAP> Table 2: Ablation study for encoder block, decoder block, and attention block in DenseNMT.
<R> <C> Methods <C> [BOLD] Movie ACC <C> [BOLD] Movie F1_pos <C> [BOLD] Movie F1_neg <C> [BOLD] Laptop ACC <C> [BOLD] Laptop F1_pos <C> [BOLD] Laptop F1_neg <C> [BOLD] Restaurant ACC <C> [BOLD] Restaurant F1_pos <C> [BOLD] Restaurant F1_neg <R> <C> NBSVM-uni Wang and Manning ( 2012 ) <C> 0.6791 <C> 0.6663 <C> 0.6910 <C> 0.7637 <C> 0.8216 <C> 0.6500 <C> 0.7949 <C> 0.8478 <C> 0.6858 <R> <C> NBSVM-bi Wang and Manning ( 2012 ) <C> 0.6416 <C> 0.6438 <C> 0.6394 <C> 0.7784 <C> 0.8320 <C> [BOLD] 0.6749 <C> 0.7154 <C> 0.7834 <C> 0.5853 <R> <C> CNN Kim ( 2014 ) <C> 0.6667 <C> 0.6467 <C> 0.6844 <C> 0.7737 <C> 0.8381 <C> 0.6245 <C> 0.8329 <C> 0.8841 <C> 0.7007 <R> <C> Adaptation Goldberger and Ben-Reuven ( 2017 ) <C> 0.6682 <C> 0.6708 <C> 0.6656 <C> 0.7272 <C> 0.7936 <C> 0.5981 <C> 0.8285 <C> 0.8872 <C> 0.6422 <R> <C> Forward Patrini et al. ( 2017 ) <C> 0.6864 <C> 0.6753 <C> 0.6969 <C> 0.7547 <C> 0.8170 <C> 0.6282 <C> 0.8329 <C> 0.8882 <C> 0.6695 <R> <C> Backward Patrini et al. ( 2017 ) <C> 0.6651 <C> 0.6160 <C> 0.6830 <C> 0.7124 <C> 0.7834 <C> 0.5723 <C> 0.7890 <C> 0.8485 <C> 0.6521 <R> <C> Masking Han et al. ( 2018a ) <C> 0.6708 <C> 0.6631 <C> 0.6782 <C> 0.7188 <C> 0.7787 <C> 0.6144 <C> 0.8219 <C> 0.8789 <C> 0.6639 <R> <C> Co-teaching Han et al. ( 2018b ) <C> 0.6150 <C> 0.5980 <C> 0.6306 <C> 0.7145 <C> 0.7867 <C> 0.5686 <C> 0.7978 <C> 0.8575 <C> 0.6515 <R> <C> NetAb (Our method) <C> [BOLD] 0.7047 <C> [BOLD] 0.7076 <C> [BOLD] 0.7017 <C> [BOLD] 0.7928 <C> [BOLD] 0.8487 <C> 0.6711 <C> [BOLD] 0.8593 <C> [BOLD] 0.9056 <C> [BOLD] 0.7241 <CAP> Table 2: Accuracy (ACC) of both classes, F1 (F1_pos) of positive class and F1 (F1_neg) of negative class on clean test data/sentences. Training data are real noisy-labeled sentences.
<R> <C> methods <C> methods <C> English corpus P <C> English corpus R <C> English corpus F1 <C> Chinese corpus P <C> Chinese corpus R <C> Chinese corpus F1 <R> <C> string containing <C> string containing <C> [BOLD] 95.20 <C> 2.39 <C> 4.66 <C> [BOLD] 99.17 <C> 78.47 <C> 87.61 <R> <C> set containing <C> set containing <C> 94.45 <C> 5.64 <C> 10.66 <C> 97.95 <C> 86.66 <C> 91.96 <R> <C> feature vector <C> whole entity <C> 73.72 <C> 81.77 <C> 77.54 <C> 79.51 <C> 69.47 <C> 74.15 <R> <C> feature vector <C> sum of words <C> 87.31 <C> 88.95 <C> 88.12 <C> 93.35 <C> [BOLD] 94.78 <C> 94.06 <R> <C> projection <C> whole entity <C> 70.96 <C> 83.47 <C> 76.71 <C> 78.19 <C> 64.75 <C> 70.84 <R> <C> learning <C> sum of words <C> 83.30 <C> 87.22 <C> 85.21 <C> 85.32 <C> 86.84 <C> 86.08 <R> <C> simple RNN <C> whole entity <C> 76.51 <C> 80.78 <C> 78.59 <C> 81.42 <C> 62.59 <C> 70.78 <R> <C> simple RNN <C> sum of words <C> 88.52 <C> 89.92 <C> 89.22 <C> 80.11 <C> 63.56 <C> 70.88 <R> <C> term <C> whole entity <C> 29.81 <C> 59.62 <C> 39.75 <C> 78.19 <C> 64.75 <C> 70.84 <R> <C> embedding <C> sum of words <C> 42.63 <C> 85.25 <C> 56.83 <C> 38.78 <C> 77.56 <C> 51.73 <R> <C> our model <C> our model <C> 89.30 <C> [BOLD] 91.42 <C> [BOLD] 90.35 <C> 96.09 <C> 94.44 <C> [BOLD] 95.26 <CAP> TABLE II: Comparative Results of Our Model and Reference Methods
<R> <C> [ITALIC] n [BOLD] -gram <C> [ITALIC] m1 <C> [ITALIC] m2 <C> [ITALIC] s <R> <C> phantom <C> 34 <C> 1 <C> 0.945 <R> <C> Amy <C> 9 <C> 0 <C> 0.909 <R> <C> , who <C> 8 <C> 0 <C> 0.900 <R> <C> my mother <C> 7 <C> 0 <C> 0.889 <R> <C> else happened <C> 5 <C> 0 <C> 0.857 <R> <C> going to show you <C> 0 <C> 6 <C> 0.125 <R> <C> going to show <C> 0 <C> 6 <C> 0.125 <R> <C> hemisphere <C> 0 <C> 5 <C> 0.143 <R> <C> Is <C> 0 <C> 5 <C> 0.143 <R> <C> ’m going to show <C> 0 <C> 5 <C> 0.143 <CAP> Table 2: Examples discovered by n-gram analysis
<R> <C> [EMPTY] <C> estimate <C> S.E. <C> t statistic <R> <C> Intercept <C> 0.08567 <C> 0.04321 <C> [ITALIC] t(1152)=1.983 <R> <C> Load (high) <C> 0.16715 <C> 0.04740 <C> [ITALIC] t(1152)=3.527 <R> <C> Domain (words) <C> −0.11373 <C> 0.04149 <C> [ITALIC] t(1152)=−2.742 <R> <C> Input entropy <C> −0.14094 <C> 0.05494 <C> [ITALIC] t(1152)=−2.565 <R> <C> Load * Input entropy <C> −0.49568 <C> 0.05622 <C> [ITALIC] t(1152)=−8.816 <R> <C> Domain * Input ent. <C> −0.09968 <C> 0.04596 <C> [ITALIC] t(1152)=−2.169 <CAP> Table 4: The best-fit linear mixed effects model for change in entropy, see Section 4.3.
<R> <C> [EMPTY] <C> estimate <C> S.E. <C> t statistic <R> <C> Intercept <C> 0.31193 <C> 0.03127 <C> [ITALIC] t(1152)=9.976 <R> <C> Load (high) <C> 0.13083 <C> 0.03672 <C> [ITALIC] t(1152)=3.563 <R> <C> Domain (words) <C> −0.05145 <C> 0.02001 <C> [ITALIC] t(1152)=−2.571 <R> <C> Input entropy <C> −0.37040 <C> 0.03901 <C> [ITALIC] t(1152)=−9.496 <R> <C> Load * Input ent. <C> −0.24258 <C> 0.04563 <C> [ITALIC] t(1152)=−5.316 <CAP> Table 6: The best-fit linear mixed effects model for estimates, see Section 4.5.
<R> <C> Language <C> Model <C> [ITALIC] tgt_ [ITALIC] size = 100 Accuracy <C> [ITALIC] tgt_ [ITALIC] size = 100 F1-Macro <C> [ITALIC] tgt_ [ITALIC] size = 100 F1-Micro <C> [ITALIC] tgt_ [ITALIC] size=1000 Accuracy <C> [ITALIC] tgt_ [ITALIC] size=1000 F1-Macro <C> [ITALIC] tgt_ [ITALIC] size=1000 F1-Micro <R> <C> sv <C> Baseline <C> 15.11 <C> 8.36 <C> 10.37 <C> 68.64 <C> 76.36 <C> 76.50 <R> <C> sv <C> Ours <C> [BOLD] 29.47 <C> [BOLD] 54.09 <C> [BOLD] 54.36 <C> [BOLD] 71.32 <C> [BOLD] 84.42 <C> [BOLD] 84.46 <R> <C> bg <C> Baseline <C> [BOLD] 29.05 <C> 14.32 <C> 29.62 <C> [BOLD] 59.20 <C> [BOLD] 67.22 <C> [BOLD] 67.12 <R> <C> bg <C> Ours <C> 27.81 <C> [BOLD] 40.97 <C> [BOLD] 42.43 <C> 39.25 <C> 60.23 <C> 60.84 <R> <C> hu <C> Baseline <C> 21.97 <C> 13.30 <C> 16.67 <C> [BOLD] 50.75 <C> 58.68 <C> 62.79 <R> <C> hu <C> Ours <C> [BOLD] 33.32 <C> [BOLD] 54.88 <C> [BOLD] 54.69 <C> 45.90 <C> [BOLD] 74.05  <C> [BOLD] 73.38 <R> <C> pt <C> Baseline <C> 18.91 <C> 7.10 <C> 10.33 <C> 74.22 <C> 81.62 <C> 81.87 <R> <C> pt <C> Ours <C> [BOLD] 58.82 <C> [BOLD] 73.67 <C> [BOLD] 74.07 <C> [BOLD] 76.26 <C> [BOLD] 87.13 <C> [BOLD] 87.22 <CAP> Table 3: Token-wise accuracy and F1 scores on mono-lingual experiments
<R> <C> Language <C> Model <C> [ITALIC] tgt_ [ITALIC] size = 100 Accuracy <C> [ITALIC] tgt_ [ITALIC] size = 100 F1-Macro <C> [ITALIC] tgt_ [ITALIC] size = 100 F1-Micro <C> [ITALIC] tgt_ [ITALIC] size=1000 Accuracy <C> [ITALIC] tgt_ [ITALIC] size=1000 F1-Macro <C> [ITALIC] tgt_ [ITALIC] size=1000 F1-Micro <R> <C> da/sv <C> Baseline <C> [BOLD] 66.06 <C> 73.95 <C> 74.37 <C> [BOLD] 82.26 <C> [BOLD] 87.88 <C> [BOLD] 87.91 <R> <C> da/sv <C> Ours <C> 63.22 <C> [BOLD] 78.75 <C> [BOLD] 78.72 <C> 77.43 <C> 87.56 <C> 87.52 <R> <C> ru/bg <C> Baseline <C> [BOLD] 52.76  <C> 58.41 <C> 58.23 <C> [BOLD] 71.90 <C> 77.89 <C> 77.97 <R> <C> ru/bg <C> Ours <C> 46.89 <C> [BOLD] 64.46  <C> [BOLD] 64.75 <C> 67.56 <C> [BOLD] 82.06 <C> [BOLD] 82.11 <R> <C> fi/hu <C> Baseline <C> [BOLD] 51.74 <C> 68.15 <C> 66.82 <C> 61.80 <C> 75.96 <C> 76.16 <R> <C> fi/hu <C> Ours <C> 45.41 <C> [BOLD] 68.63 <C> [BOLD] 68.07 <C> [BOLD] 63.93 <C> [BOLD] 85.06 <C> [BOLD] 84.12 <R> <C> es/pt <C> Baseline <C> [BOLD] 79.40 <C> 86.03 <C> 86.14 <C> [BOLD] 85.85  <C> 91.91 <C> 91.93 <R> <C> es/pt <C> Ours <C> 77.75 <C> [BOLD] 88.42 <C> [BOLD] 88.44 <C> 85.02 <C> [BOLD] 92.35 <C> [BOLD] 92.37 <CAP> Table 4: Token-wise accuracy and F1 scores on cross-lingual experiments
<R> <C> [BOLD] Model <C> [BOLD] All <C> [BOLD] Seq <C> [BOLD] Pos 1 <C> [BOLD] Pos 2 <C> [BOLD] Pos 3 <R> <C> FP <C> 34.1 <C> 7.2 <C> 52.6 <C> 25.6 <C> [BOLD] 25.9 <R> <C> NP <C> 39.4 <C> 10.8 <C> 58.9 <C> 35.9 <C> 24.6 <R> <C> DynSP <C> 42.0 <C> 10.2 <C> 70.9 <C> 35.8 <C> 20.1 <R> <C> FP+ <C> 33.2 <C> 7.7 <C> 51.4 <C> 22.2 <C> 22.3 <R> <C> NP+ <C> 40.2 <C> 11.8 <C> 60.0 <C> 35.9 <C> 25.5 <R> <C> DynSP* <C> 44.7 <C> 12.8 <C> 70.4 <C> 41.1 <C> 23.6 <R> <C> CAMP <C> 45.0 <C> 11.7 <C> 71.3 <C> 42.8 <C> 21.9 <R> <C> CAMP + TU <C> [BOLD] 45.5 <C> 12.7 <C> [BOLD] 71.1 <C> [BOLD] 43.2 <C> 22.5 <R> <C> CAMP + TU + LM <C> [BOLD] 45.5 <C> [BOLD] 13.2 <C> 70.3 <C> 42.6 <C> 24.8 <CAP> Table 4: Accuracies of all systems on SequentailQA; the models in the top section of the table treat questions independently, while those in the middle consider sequential context. Our method in the bottom section also consider sequential context and outperforms existing ones both in terms of overall accuracy as well as sequence accuracy
<R> <C> [BOLD] models <C> CAMP <C> + TU <C> + TU + LM <R> <C> Controller <C> 83.5 <C> 83.5 <C> 84.8 <R> <C> SELECT-col <C> 82.5 <C> 83.4 <C> 83.7 <R> <C> WHERE-col <C> 35.0 <C> 35.9 <C> 36.6 <R> <C> Operation <C> 69.7 <C> 69.7 <C> 70.2 <R> <C> Value <C> 21.2 <C> 21.2 <C> 21.5 <CAP> Table 5: Accuracy for each module in different settings.
<R> <C> [BOLD] Pair:  [BOLD] BWESG+add <C> [BOLD] ES-EN  [ITALIC] d=100 <C> [BOLD] ES-EN  [ITALIC] d=200 <C> [BOLD] ES-EN  [ITALIC] d=300 <C> [BOLD] IT-EN  [ITALIC] d=100 <C> [BOLD] IT-EN  [ITALIC] d=200 <C> [BOLD] IT-EN  [ITALIC] d=300 <C> [BOLD] NL-EN  [ITALIC] d=100 <C> [BOLD] NL-EN  [ITALIC] d=200 <C> [BOLD] NL-EN  [ITALIC] d=300 <R> <C> [BOLD] Length-Ratio <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] cs:16 <C> [BOLD] 0.794 <C> [BOLD] 0.767 <C> 0.752 <C> [BOLD] 0.817 <C> 0.789 <C> 0.794 <C> 0.778 <C> 0.769 <C> 0.767 <R> <C> [ITALIC] cs:48 <C> 0.752 <C> 0.758 <C> [BOLD] 0.764 <C> 0.814 <C> [BOLD] 0.831 <C> [BOLD] 0.814 <C> [BOLD] 0.797 <C> [BOLD] 0.789 <C> [BOLD] 0.775 <R> <C> Mikolov <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] cs:4 <C> 0.742 <C> 0.739 <C> 0.725 <C> 0.733 <C> 0.706 <C> 0.692 <C> 0.692 <C> 0.700 <C> 0.700 <R> <C> [ITALIC] cs:8 <C> 0.767 <C> 0.750 <C> 0.747 <C> 0.767 <C> 0.747 <C> 0.744 <C> 0.694 <C> 0.697 <C> 0.672 <R> <C> [ITALIC] cs:16 <C> 0.769 <C> 0.744 <C> 0.747 <C> 0.758 <C> 0.755 <C> 0.758 <C> 0.725 <C> 0.700 <C> 0.689 <R> <C> [ITALIC] cs:48 <C> 0.678 <C> 0.642 <C> 0.669 <C> 0.714 <C> 0.714 <C> 0.747 <C> 0.725 <C> 0.711 <C> 0.708 <R> <C> [ITALIC] cs:60 <C> 0.636 <C> 0.658 <C> 0.656 <C> 0.725 <C> 0.725 <C> 0.742 <C> 0.722 <C> 0.728 <C> 0.722 <R> <C> [BOLD] BiCVM <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> iterations:200 <C> 0.547 <C> 0.567 <C> 0.539 <C> 0.636 <C> 0.664 <C> 0.642 <C> 0.586 <C> 0.567 <C> 0.581 <CAP> Table 8: SWTC results: Comparison of BWESG with (1) the BWE induction model from Mikolov:2013arxiv relying on SGNS, (2) BiCVM: the BWE induction model from Hermann:2014 initially developed for parallel sentence-aligned data. All models were trained on the same document-aligned training Wikipedia data with exactly the same vocabularies.
<R> <C> [EMPTY] <C> [BOLD] Top 3  [BOLD] F1 <C> [BOLD] Top 3  [BOLD] TR <C> [BOLD] Top 3  [BOLD] TL <C> [BOLD] Top 5  [BOLD] F1 <C> [BOLD] Top 5  [BOLD] TR <C> [BOLD] Top 5  [BOLD] TL <R> <C> Plot Synopses <C> 29.3 <C> 8.04 <C> 28 <C> 38.7 <C> 15.70 <C> 35 <R> <C> Scripts <C> 29.8 <C> 5.16 <C> 19 <C> 37.0 <C> 9.27 <C> 26 <CAP> Table 4: Evaluation of predictions using plot synopses and scripts
<R> <C> [EMPTY] <C> Model <C> Layer Norm <C> Dev <C> Test 2014 <C> Test 2015 <R> <C> En→De <C> Baseline <C> [EMPTY] <C> 21.57 <C> 21.33 <C> [BOLD] 23.45 <R> <C> En→De <C> Baseline† <C> [EMPTY] <C> 21.4 <C> 21.16 <C> 22.1 <R> <C> En→De <C> PAG <C> [EMPTY] <C> 21.52 <C> 21.35 <C> 22.21 <R> <C> En→De <C> PAG <C> [EMPTY] <C> [BOLD] 22.12 <C> [BOLD] 21.93 <C> 22.83 <R> <C> En→De <C> rPAG <C> [EMPTY] <C> 21.81 <C> 21.71 <C> 22.45 <R> <C> En→De <C> rPAG <C> [EMPTY] <C> 21.67 <C> 21.81 <C> 22.73 <R> <C> En→Cs <C> Baseline <C> [EMPTY] <C> 17.68 <C> 19.27 <C> 16.98 <R> <C> En→Cs <C> PAG <C> [EMPTY] <C> 17.44 <C> 18.72 <C> 16.99 <R> <C> En→Cs <C> PAG <C> [EMPTY] <C> [BOLD] 18.78 <C> [BOLD] 20.9 <C> [BOLD] 18.59 <R> <C> En→Cs <C> rPAG <C> [EMPTY] <C> 17.83 <C> 19.54 <C> 17.79 <R> <C> En→Fi <C> Baseline <C> [EMPTY] <C> 11.19 <C> - <C> 10.93 <R> <C> En→Fi <C> PAG <C> [EMPTY] <C> 11.51 <C> - <C> 11.13 <R> <C> En→Fi <C> PAG <C> [EMPTY] <C> [BOLD] 12.67 <C> - <C> [BOLD] 11.84 <R> <C> En→Fi <C> rPAG <C> [EMPTY] <C> 11.50 <C> - <C> 10.59 <CAP> Table 1: The results of different models on WMT’15 task on English to German, English to Czech and English to Finnish language pairs. We report BLEU scores of each model computed via the multi-blue.perl script. The best-score of each model for each language pair appears in bold-face. We use newstest2013 as our development set, newstest2014 as our "Test 2014" and newstest2015 as our "Test 2015" set. (†) denotes the results of the baseline that we trained using the hyperparameters reported in Chung et al. (2016) and the code provided with that paper. For our baseline, we only report the median result, and do not have multiple runs of our models.
<R> <C> Model <C> Model <C> Model <C> F-score Arabic Wikipedia Title <C> F-score Arabic Wikipedia Title <C> F-score Arabic Poetry <C> F-score Arabic Poetry <R> <C> [EMPTY] <C> Embedding <C> Classifier <C> Micro [%] <C> Macro [%] <C> Micro [%] <C> Macro [%] <R> <C> [EMPTY] <C> [EMPTY] <C> Majority Class <C> 21.67 <C> 2.97 <C> 47.06 <C> 5.33 <R> <C> Word <C> Unigram <C> SVM <C> 45.47 <C> 26.60 <C> 52.80 <C> 34.83 <R> <C> level <C> AraVec <C> CNN <C> 45.02 <C> 25.05 <C> 69.28 <C> 41.95 <R> <C> Character <C> One-hot <C> CLCNN <C> 42.76 <C> 18.71 <C> 68.24 <C> 37.72 <R> <C> level <C> [BOLD] AraDIC <C> CLCNN (− CB loss) <C> 47.47 <C> 26.85 <C> 74.86 <C> 45.61 <R> <C> [EMPTY] <C> [EMPTY] <C> CLCNN (+ CB loss) <C> 49.49 <C> 30.55 <C> 74.03 <C> 48.65 <R> <C> [EMPTY] <C> [EMPTY] <C> BiGRU (− CB loss) <C> 55.71 <C> 39.04 <C> 78.93 <C> 59.88 <R> <C> [EMPTY] <C> [EMPTY] <C> BiGRU (+ CB loss) <C> [BOLD] 57.76 <C> [BOLD] 44.54 <C> [BOLD] 79.53 <C> [BOLD] 65.00 <CAP> Table 2: Classification results of our model and other baselines. Majority Class: Due to high class-imbalance in both of our datasets, we examine the performance of majority class classifier. CNN + AraVec: Sentence classifier CNN Sagheer and Sukkar (2018); Kim (2014) using AraVec’s word embeddings Soliman et al. (2017). SVM: an SVM with unigrams, stemming, and document term matrix with TF-IDF scores as features. CLCNN: character level CNN with one hot encoding as inputsZhang et al. (2015). AraDIC: our proposed end-to-end framework of character encoder, CLCNN and BiGRU classifiers, trained with and without class-balanced softmax loss (CB loss). We report two evaluation metrics, the macro and micro F-scores.
<R> <C> [BOLD] Language <C> [BOLD] Wikipedia Dumps <C> [BOLD] Train <C> [BOLD] Test <R> <C> [ITALIC] Arabic <C> 3406732 <C> 4570 <C> 1163 <R> <C> [ITALIC] Chinese <C> 8067971 <C> 2593 <C> 1011 <R> <C> [ITALIC] Dutch <C> 11860559 <C> 2169 <C> 683 <R> <C> [ITALIC] English <C> 30000002 <C> 3584 <C> 1102 <R> <C> [ITALIC] French <C> 26024881 <C> 1410 <C> 534 <R> <C> [ITALIC] Italian <C> 15338617 <C> 4588 <C> 512 <R> <C> [ITALIC] Russian <C> 16671224 <C> 2555 <C> 835 <R> <C> [ITALIC] Spanish <C> 22328668 <C> 1553 <C> 646 <R> <C> [ITALIC] Turkish <C> 3622336 <C> 1008 <C> 121 <CAP> Table 2: Size of the data partitions (# sentences).
<R> <C> Class <C> Precision <C> Recall <C> F1-Score <C> Support <R> <C> (0) Negative <C> 0.47 <C> 0.50 <C> 0.49 <C> 289 <R> <C> (1) Neutral <C> 0.31 <C> 0.18 <C> 0.23 <C> 22 <R> <C> (2) Positive <C> 0.96 <C> 0.96 <C> 0.96 <C> 4215 <R> <C> Average / Total <C> 0.93 <C> 0.93 <C> 0.93 <C> 4526 <CAP> Table 6. Statistical Report on Sentiment Classification using Bidirectional LSTM.
<R> <C> [BOLD] Model <C> [BOLD] Original <C> AddSent <C> AddOneSent <R> <C> BiDAF-S DBLP:journals/corr/SeoKFH16 <C> 75.5 <C> 34.3 <C> 45.7 <R> <C> ReasoNet-S shen2017reasonet <C> 78.2 <C> 39.4 <C> 50.3 <R> <C> Reinforced Mnemonic Reader-S hu2017reinforced <C> 78.5 <C> 46.6 <C> 56.0 <R> <C> QANet-S yu2018qanet <C> 83.8 <C> 45.2 <C> 55.7 <R> <C> GQA-S lewis2018generative <C> 83.7 <C> 47.3 <C> 57.8 <R> <C> FusionNet-E huang2017fusionnet <C> 83.6 <C> 51.4 <C> 60.7 <R> <C> BERT-S devlin2018bert <C> 88.5 <C> 51.0 <C> 63.4 <R> <C> BERT-S + QAInfomax <C> [BOLD] 88.6 <C> [BOLD] 54.5 † <C> [BOLD] 64.9 † <CAP> Table 1: F-measure on AdversarialSquad (S: single, E: ensemble). † indicates the significant improvement over baselines with p-value <0.05.
<R> <C> [BOLD] Function <C> AddSent <C> AddOneSent <R> <C> Mean <C> [BOLD] 52.2 <C> [BOLD] 63.7 <R> <C> Max <C> 52.0 <C> 63.3 <R> <C> Sample <C> [BOLD] 52.2 <C> 63.0 <CAP> Table 3: Different summarization functions for GC .
<R> <C> Court <C> Supreme Court <C> Supreme Adm. Court <C> Constitutional Court <C> Rest <R> <C> Supreme Court cites <C> 153 242 <C> 804 <C> 80 658 <C> 112 287 <R> <C> Supreme Administrative Court cites <C> 1 342 <C> 90 217 <C> 14 756 <C> 20 709 <R> <C> Constitutional Court cites <C> 8 486 <C> 2 877 <C> 137 308 <C> 45 689 <CAP> Table 2: References sorted by categories, unlinked
<R> <C> [BOLD] Metric <C> [ITALIC] DeFactoNLP <C> [ITALIC] Baseline <C> [ITALIC] Best <R> <C> Label Accuracy <C> 0.5136 <C> 0.4884 <C> 0.6821 <R> <C> Evidence F1 <C> 0.4277 <C> 0.1826 <C> 0.6485 <R> <C> FEVER Score <C> 0.3833 <C> 0.2745 <C> 0.6421 <CAP> Table 3: System Performance
<R> <C> [EMPTY] <C> [BOLD] Ubuntu data <C> [BOLD] Ubuntu data <C> [BOLD] Ubuntu data <C> [BOLD] Ubuntu data <C> [BOLD] Ubuntu data <C> [BOLD] Tieba data <C> [BOLD] Tieba data <C> [BOLD] Tieba data <R> <C> [EMPTY] <C> R2@1 <C> R5@1 <C> R10@1 <C> R10@2 <C> R10@5 <C> MAP <C> MRR <C> P@1 <R> <C> Random <C> 0.500 <C> 0.200 <C> 0.100 <C> 0.200 <C> 0.500 <C> 0.642 <C> 0.695 <C> 0.524 <R> <C> Cosine <C> 0.681 <C> 0.470 <C> 0.383 <C> 0.482 <C> 0.686 <C> 0.597 <C> 0.662 <C> 0.553 <R> <C> Translation <C> 0.721 <C> 0.502 <C> [BOLD] 0.393 <C> 0.507 <C> 0.727 <C> 0.710 <C> 0.760 <C> 0.658 <R> <C> DeepMatch [ITALIC] topic <C> 0.593 <C> 0.345 <C> 0.248 <C> 0.376 <C> 0.693 <C> 0.677 <C> 0.725 <C> 0.594 <R> <C> MLP <C> 0.651 <C> 0.362 <C> 0.256 <C> 0.380 <C> 0.703 <C> 0.653 <C> 0.712 <C> 0.550 <R> <C> CNTN <C> 0.743 <C> 0.489 <C> 0.349 <C> 0.512 <C> 0.797 <C> 0.731 <C> [BOLD] 0.797 <C> 0.670 <R> <C> LSTM <C> 0.725 <C> 0.494 <C> 0.361 <C> 0.529 <C> 0.801 <C> 0.732 <C> [BOLD] 0.797 <C> 0.670 <R> <C> Arc1 <C> 0.665 <C> 0.372 <C> 0.221 <C> 0.360 <C> 0.684 <C> 0.698 <C> 0.771 <C> 0.640 <R> <C> Arc2 <C> 0.736 <C> 0.508 <C> 0.380 <C> 0.534 <C> 0.777 <C> 0.708 <C> 0.783 <C> 0.660 <R> <C> TACNTN <C> [BOLD] 0.759 <C> [BOLD] 0.520 <C> 0.382 <C> [BOLD] 0.544 <C> [BOLD] 0.809 <C> [BOLD] 0.749 <C> [BOLD] 0.804 <C> [BOLD] 0.688 <CAP> Table 2: Evaluation results on the Ubuntu data and the Tieba data
<R> <C> Model <C> MSRP Acc.(%) <C> MSRP F1(%) <C> SICK  [ITALIC] r <C> SICK  [ITALIC] ρ <C> MSRvid  [ITALIC] r <C> MSRvid  [ITALIC] ρ <C> STSbenchmark  [ITALIC] r <C> STSbenchmark  [ITALIC] ρ <R> <C> MaLSTM <C> 66.95 <C> 73.95 <C> 0.7824 <C> 0.71843 <C> 0.7325 <C> 0.7193 <C> 0.5739 <C> 0.5558 <R> <C> Multi-scale MaLSTM <C> [BOLD] 74.09 <C> [BOLD] 82.18 <C> [BOLD] 0.8168 <C> [BOLD] 0.74226 <C> [BOLD] 0.8236 <C> [BOLD] 0.8188 <C> [BOLD] 0.6839 <C> [BOLD] 0.6575 <R> <C> HCTI <C> 73.80 <C> 80.85 <C> 0.8408 <C> 0.7698 <C> [BOLD] 0.8848 <C> [BOLD] 0.8763 <C> [BOLD] 0.7697 <C> [BOLD] 0.7549 <R> <C> Multi-scale HCTI <C> [BOLD] 74.03 <C> [BOLD] 81.76 <C> [BOLD] 0.8437 <C> [BOLD] 0.7729 <C> 0.8763 <C> 0.8686 <C> 0.7269 <C> 0.7033 <CAP> Table 4. A comparison among different supervised learning models in terms of accuracy, F1 score, Pearson’s r and Spearman’s ρ on various test sets.
<R> <C> [BOLD] Features <C> [EMPTY] <C> [BOLD] Generic  [ITALIC] cross-val <C> [BOLD] Same-Topic  [ITALIC] Politics <C> [BOLD] Same-Topic  [ITALIC] Personal <C> [BOLD] Cross-Topic  [ITALIC] Pol-Pers <C> [BOLD] Cross-Topic  [ITALIC] Pers-Pol <R> <C> [BOLD] uni/bi-grams <C> Acc. <C> 0.51 <C> 0.58 <C> 0.40 <C> 0.31 <C> 0.36 <R> <C> [BOLD] uni/bi-grams <C> Macro-F1 <C> 0.50 <C> 0.53 <C> 0.39 <C> 0.21 <C> 0.29 <R> <C> [BOLD] uni/bi-grams + annotations <C> Acc. <C> 0.63 <C> 0.64 <C> 0.56 <C> 0.47 <C> 0.50 <R> <C> [BOLD] uni/bi-grams + annotations <C> Macro-F1 <C> 0.63 <C> 0.62 <C> 0.54 <C> 0.37 <C> 0.44 <CAP> Table 3: Experimental results of a baseline logistic regression model showing the impact of adding the corpus annotation features, and the impact of changing the topic from training to testing.
<R> <C> [BOLD] Data <C> [BOLD] Hyperparameters  [ITALIC] α <C> [BOLD] Hyperparameters  [ITALIC] β <C> [BOLD] Hyperparameters  [BOLD] gen. mode <C> [BOLD] ASR  [BOLD] CER (%) <C> [BOLD] TTS  [BOLD] Mel <C> [BOLD] TTS  [BOLD] Raw <C> [BOLD] TTS  [BOLD] Acc (%) <R> <C> Paired (80 utt/spk) <C> - <C> - <C> - <C> 26.47 <C> 10.213 <C> 13.175 <C> 98.6 <R> <C> + Unpaired (remaining) <C> 0.25 <C> 1 <C> greedy <C> 23.03 <C> 9.137 <C> 12.863 <C> 98.7 <R> <C> + Unpaired (remaining) <C> 0.5 <C> 1 <C> greedy <C> 20.91 <C> 9.312 <C> 12.882 <C> 98.6 <R> <C> + Unpaired (remaining) <C> 0.25 <C> 1 <C> beam 5 <C> 22.55 <C> 9.359 <C> 12.767 <C> 98.6 <R> <C> + Unpaired (remaining) <C> 0.5 <C> 1 <C> beam 5 <C> 19.99 <C> 9.198 <C> 12.839 <C> 98.6 <CAP> Table 2: Experiment result for multi-speaker test set.
<R> <C> [EMPTY] <C> [BOLD] Trad 3 <C> [BOLD] Trad 5 <C> [BOLD] SDT <C> [BOLD] DDT1 <C> [BOLD] DDT2 <R> <C> Efficiency <C> 3,000 <C> 5,000 <C> 3,907 <C> 3,206 <C> 3,608 <R> <C> Imprv. <C> [EMPTY] <C> [EMPTY] <C> 22% <C> 36% <C> 28% <R> <C> Accuracy <C> 0.612 <C> 0.653 <C> 0.624 <C> 0.630 <C> 0.643 <R> <C> Loss <C> [EMPTY] <C> [EMPTY] <C> 4.4 pp <C> 3.5 pp <C> 1.0 pp <CAP> Table 2: Comparison of results of five methods with respect to their efficiency and accuracy. The number of crowd workers engaged (i.e., efficiency or costs) and the accuracy of their sentiment labeling (Cohen’s Kappa IRR rate) compared to the gold standard established by experts are given for each method. For the first two methods, each tweet is analyzed by the same fixed number of crowd workers, i.e., 3 crowd workers (Trad 3) or 5 crowd workers (Trad 5). For the methods that use the decision tree (DT), the number of crowd workers engaged depends on the content of the tweet and result in significant improvements (Improv.) in efficiency with respect to the 5 crowd-worker models (row 2), without much loss of accuracy (row 4, given in percent points, pp).
<R> <C> %  [ITALIC] p-value ≥0.01 <C> %  [ITALIC] p-value ≥0.01  [ITALIC] Nc <C> %  [ITALIC] p-value ≥0.01  [ITALIC] Nl <C> %  [ITALIC] p-value ≥0.01  [ITALIC] NSw <C> %  [ITALIC] p-value ≥0.01  [ITALIC] NSc <C> %  [ITALIC] p-value ≥0.01  [ITALIC] NSl <R> <C> [ITALIC] Nw <C> 95.80 <C> 96.00 <C> 80.43 <C> 71.85 <C> 67.86 <R> <C> [ITALIC] Nc <C> [EMPTY] <C> 100.00 <C> 88.62 <C> 98.40 <C> 92.01 <R> <C> [ITALIC] Nl <C> [EMPTY] <C> [EMPTY] <C> 92.01 <C> 99.00 <C> 98.60 <R> <C> [ITALIC] NSw <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 90.41 <C> 88.42 <R> <C> [ITALIC] NSc <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 100.00 <CAP> Table 2: Percentage of distributions that can be drawn from the same distributions for each comparison. The percentage values were obtained using the two–sample Kolmogorov–Smirnov test (p–value ≥0.01).
<R> <C> Model <C> BLEU <C> Perplexity <R> <C> Baseline <C> 25.07 <C> 4.60 <R> <C> StyleEQ <C> [BOLD] 30.04 <C> [BOLD] 3.33 <CAP> Table 4: Test set reconstruction BLEU score and perplexity (in nats).
<R> <C> phoneme context <C> acoustic model <C> vtln <C> sat <C> sMBR <C> *wer [%] dev <C> *wer [%] dev <C> *wer [%] test <C> *wer [%] test <R> <C> phoneme context <C> acoustic model <C> vtln <C> sat <C> sMBR <C> clean <C> other <C> clean <C> other <R> <C> mono <C> GMM <C> no <C> no <C> no <C> 24.3 <C> 52.6 <C> 24.1 <C> 56.1 <R> <C> tri <C> GMM <C> no <C> no <C> no <C> 12.1 <C> 34.5 <C> 12.9 <C> 36.9 <R> <C> tri <C> GMM <C> yes <C> no <C> no <C> 12.0 <C> 35.1 <C> 11.2 <C> 36.4 <R> <C> tri <C> GMM <C> no <C> yes <C> no <C> 8.0 <C> 21.9 <C> 8.6 <C> 22.9 <R> <C> tri <C> GMM <C> yes <C> yes <C> no <C> 7.6 <C> 22.0 <C> 8.4 <C> 23.1 <R> <C> tri <C> LSTM <C> yes <C> yes <C> no <C> 4.0 <C> 9.6 <C> 4.4 <C> 10.0 <R> <C> tri <C> LSTM <C> yes <C> yes <C> yes <C> 3.4 <C> 8.3 <C> 3.8 <C> 8.8 <CAP> Table 1: gmmhmm and hybrid dnn/hmm results on LibriSpeech with 12k cart labels and evaluated with the official 4-gram lm.
<R> <C> # of cart labels <C> *wer [%] dev <C> *wer [%] dev <C> *wer [%] test <C> *wer [%] test <R> <C> # of cart labels <C> clean <C> other <C> clean <C> other <R> <C> 9001 <C> 6.2 <C> 14.9 <C> 5.8 <C> 15.9 <R> <C> 12001 <C> 4.0 <C> 9.6 <C> 4.4 <C> 10.0 <R> <C> 20001 <C> 4.9 <C> 11.3 <C> 5.4 <C> 12.3 <CAP> Table 2: Hybrid dnn/hmm results on LibriSpeech with different numbers of cart labels. For all systems the official 4-gram word lm is used.
<R> <C> [EMPTY] <C> Normal Act <C> Normal Val <C> Adversarial Act <C> Adversarial Val <R> <C> [BOLD] MuSE to IEMOCAP <C> [BOLD] MuSE to IEMOCAP <C> [BOLD] MuSE to IEMOCAP <C> [BOLD] MuSE to IEMOCAP <C> [BOLD] MuSE to IEMOCAP <R> <C> Unimodal (A) <C> 0.419 <C> 0.376 <C> [BOLD] 0.448 <C> [BOLD] 0.401 <R> <C> Unimodal (L) <C> 0.401 <C> 0.433 <C> [BOLD] 0.436 <C> 0.447 <R> <C> Multimodal (A+L) <C> 0.422 <C> 0.431 <C> [BOLD] 0.459 <C> [BOLD] 0.472 <R> <C> [BOLD] MuSE to MSP-Improv <C> [BOLD] MuSE to MSP-Improv <C> [BOLD] MuSE to MSP-Improv <C> [BOLD] MuSE to MSP-Improv <C> [BOLD] MuSE to MSP-Improv <R> <C> Unimodal (A) <C> 0.404 <C> 0.368 <C> [BOLD] 0.431 <C> 0.372 <CAP> Table 4. Performance (in UAR) predicting activation (left) and valence (right) in non-adversarial and adversarial (for best lambda value) setups across datasets when trained on MuSE. Bold signifies significantly different performance (paired t-test, α<0.05).
<R> <C> [ITALIC] Tmin <C> | [ITALIC] F| <C> CoTK’s Perplexity <C> Original Perplexity <R> <C> 1 <C> 30765 <C> 14.17 <C> 14.15 <R> <C> 2 <C> 19227 <C> 14.11 <C> 13.95 <R> <C> 4 <C> 12555 <C> [BOLD] 14.10 <C> 13.74 <R> <C> 10 <C> 8044 <C> 14.22 <C> 13.39 <R> <C> 40 <C> 4062 <C> 15.30 <C> 12.69 <R> <C> 160 <C> 1900 <C> 17.13 <C> 11.49 <CAP> Table 6: Perplexity under different sets of the frequent vocabulary. In each row, the words appearing less than Tmin times in the training data are regarded as rare words. The results of CoTK’s perplexity are comparable while those of the original perplexity are not.
<R> <C> [EMPTY] <C> [EMPTY] <C> [BOLD] Quora 50K <C> [BOLD] Quora 50K <C> [BOLD] Quora 50K <C> [BOLD] Quora 100K <C> [BOLD] Quora 100K <C> [BOLD] Quora 100K <C> [BOLD] Quora 150K <C> [BOLD] Quora 150K <C> [BOLD] Quora 150K <R> <C> [BOLD] Model <C> Measure <C> BLEU <C> METEOR <C> TER <C> BLEU <C> METEOR <C> TER <C> BLEU <C> METEOR <C> TER <R> <C> [BOLD] Unsupervised (baseline) <C> - <C> 8.3 <C> 12.2 <C> 83.7 <C> 10.6 <C> 14.3 <C> 79.9 <C> 11.4 <C> 14.5 <C> 78.0 <R> <C> [BOLD] VAE-S (baseline) <C> Avg <C> 11.9 <C> 17.4 <C> 77.7 <C> 13.0 <C> 18.4 <C> 76.8 <C> 14.2 <C> 19.0 <C> 74.8 <R> <C> [BOLD] VAE-S (baseline) <C> best BLEU <C> 15.8 <C> 20.1 <C> 69.4 <C> 17.5 <C> 21.6 <C> 67.1 <C> 19.8 <C> 22.6 <C> 63.9 <R> <C> [BOLD] VAE-S (baseline) <C> best METEOR <C> 15.6 <C> 21.1 <C> 71.5 <C> 17.5 <C> 22.7 <C> 69.5 <C> 19.7 <C> 23.8 <C> 66.9 <R> <C> [BOLD] VAE-SVG (ours) <C> Avg <C> 13.8 <C> 18.7 <C> 68.2 <C> 18.6 <C> 21.9 <C> 60.6 <C> 25.0 <C> 25.1 <C> 52.5 <R> <C> [BOLD] VAE-SVG (ours) <C> best BLEU <C> 17.1 <C> 21.3 <C> 63.1 <C> 22.5 <C> 24.6 <C> 55.7 <C> 30.3 <C> 28.5 <C> 47.3 <R> <C> [BOLD] VAE-SVG (ours) <C> best METEOR <C> 17.1 <C> [BOLD] 22.2 <C> 63.8 <C> 22.4 <C> [BOLD] 25.5 <C> 55.6 <C> 30.3 <C> 29.2 <C> 47.1 <R> <C> [BOLD] VAE-SVG-eq (ours) <C> Avg <C> 13.9 <C> 18.8 <C> 67.1 <C> 19.0 <C> 21.7 <C> 60.0 <C> 26.2 <C> 25.7 <C> 52.1 <R> <C> [BOLD] VAE-SVG-eq (ours) <C> best BLEU <C> [BOLD] 17.4 <C> 21.4 <C> [BOLD] 61.9 <C> [BOLD] 22.9 <C> 24.7 <C> 55.0 <C> 31.4 <C> 29.0 <C> 46.8 <R> <C> [BOLD] VAE-SVG-eq (ours) <C> best METEOR <C> 17.3 <C> [BOLD] 22.2 <C> 62.6 <C> [BOLD] 22.9 <C> [BOLD] 25.5 <C> [BOLD] 54.9 <C> 32.0 <C> 30.0 <C> 46.1 <R> <C> [BOLD] VAE-SVG-eq (ours) <C> Avg (beam=10) <C> - <C> - <C> - <C> - <C> - <C> - <C> 37.1 <C> 32.0 <C> 40.8 <R> <C> [BOLD] VAE-SVG-eq (ours) <C> best BLEU (beam=10) <C> - <C> - <C> - <C> - <C> - <C> - <C> 38.0 <C> 32.9 <C> 40.0 <R> <C> [BOLD] VAE-SVG-eq (ours) <C> best METEOR (beam=10) <C> - <C> - <C> - <C> - <C> - <C> - <C> [BOLD] 38.3 <C> [BOLD] 33.6 <C> [BOLD] 39.5 <CAP> Table 3: Results on Quora dataset. Higher BLEU and METEOR score is better, whereas lower TER score is better.
<R> <C> [BOLD] Dataset <C> [BOLD] Input <C> [BOLD] Relevance <C> [BOLD] Readability <R> <C> MSCOCO <C> Ground Truth <C> 3.38 <C> 4.68 <R> <C> [EMPTY] <C> System Output <C> 3.0 <C> 3.84 <R> <C> Quora <C> Ground Truth <C> 4.82 <C> 4.94 <R> <C> [EMPTY] <C> System Output <C> 3.57 <C> 4.08 <CAP> Table 6: Our Human evaluation results for paraphrase generation.
<R> <C> Model <C> Accuracy <C> RMSE <R> <C> Guess1 <C> 33.8 <C> 3.74 <R> <C> LSTM <C> 36.8 <C> 3.47 <R> <C> SoftCount <C> 50.2 (49.2) <C> [BOLD] 2.37 (2.45) <R> <C> UpDown <C> 52.7 (51.5) <C> 2.64 (2.69) <R> <C> IRLC <C> [BOLD] 57.7 (56.1) <C> [BOLD] 2.37 (2.45) <CAP> Table 2: HowMany-QA test set performance. Values in parentheses apply to models trained without caption grounding.
<R> <C> Method Random Selection <C> Precision [ITALIC] macro 0.5 <C> Recall [ITALIC] macro 0.5 <C> F1 [ITALIC] macro 0.35 <C> F1 [ITALIC] macro 0.35 <R> <C> Majority Class <C> 0.49 <C> 0.5 <C> 0.5 <C> 0.5 <R> <C> Tweet2vec <C> 0.49 <C> 0.5 <C> 0.5 <C> 0.5 <R> <C> Network Features <C> 0.49 <C> 0.5 <C> 0.5 <C> 0.5 <R> <C> [EMPTY] <C> Theme-based Features <C> Theme-based Features <C> [BOLD] - [ITALIC] themes <C> + [ITALIC] themes <R> <C> Emotions <C> 0.59 <C> 0.5 <C> 0.5 <C> 0.87 <R> <C> Sentiment <C> 0.49 <C> 0.5 <C> 0.5 <C> 0.64 <R> <C> Bad & Sexual Cues <C> 0.49 <C> 0.5 <C> 0.5 <C> 0.79 <R> <C> Stance cues <C> 0.91 <C> 0.53 <C> 0.56 <C> 0.86 <R> <C> Bias Cues <C> 0.73 <C> 0.51 <C> 0.51 <C> 0.87 <R> <C> LIWC <C> 0.69 <C> 0.51 <C> 0.51 <C> 0.85 <R> <C> Morality <C> 0.90 <C> 0.62 <C> 0.68 <C> 0.86 <R> <C> Profiling Features <C> Profiling Features <C> Profiling Features <C> Profiling Features <C> Profiling Features <R> <C> Stylistic <C> 0.92 <C> 0.84 <C> 0.88 <C> 0.88 <R> <C> NLI <C> 0.96 <C> 0.88 <C> 0.91 <C> 0.91 <R> <C> [BOLD] All Features <C> 0.96 <C> 0.93 <C> 0.94 <C> 0.94 <CAP> Table 2: Classification results. We report the results of each feature set independently.
<R> <C> Language <C> Method <C> Entity AL <C> Entity ACPP <C> Entity ARCPP <C> Category AL <C> Category ACPP <C> Category ARCPP <R> <C> French <C> MultiWiBi <C> 8.24 <C> 2.96 <C> 0.49 <C> 8.92 <C> 3.6 <C> [BOLD] 0.56 <R> <C> French <C> Char TFIDF <C> 11.08 <C> [BOLD] 5.08 <C> 0.49 <C> 8.36 <C> [BOLD] 3.76 <C> 0.49 <R> <C> Italian <C> MultiWiBi <C> 7.36 <C> 2.68 <C> 0.45 <C> 14.84 <C> 3.72 <C> 0.27 <R> <C> Italian <C> Char TFIDF <C> 8.32 <C> [BOLD] 4.88 <C> [BOLD] 0.61 <C> 8.32 <C> [BOLD] 4.52 <C> [BOLD] 0.57 <R> <C> Spanish <C> MultiWiBi <C> 7.04 <C> 3.08 <C> [BOLD] 0.55 <C> 12.08 <C> 4.08 <C> 0.36 <R> <C> Spanish <C> Char TFIDF <C> 12.8 <C> [BOLD] 5.0 <C> 0.48 <C> 12.76 <C> [BOLD] 5.28 <C> [BOLD] 0.48 <R> <C> Arabic <C> MultiWiBi <C> 8.96 <C> 2.12 <C> 0.31 <C> 14.64 <C> 4.12 <C> 0.31 <R> <C> Arabic <C> Char TFIDF <C> 7.48 <C> [BOLD] 5.88 <C> [BOLD] 0.81 <C> 6.96 <C> [BOLD] 5.04 <C> [BOLD] 0.74 <R> <C> Hindi <C> MultiWiBi <C> 7.72 <C> 1.88 <C> 0.27 <C> 7.4 <C> 1.8 <C> 0.36 <R> <C> Hindi <C> Char TFIDF <C> 10.28 <C> [BOLD] 4.92 <C> [BOLD] 0.47 <C> 8.0 <C> [BOLD] 2.44 <C> [BOLD] 0.38 <R> <C> Chinese <C> MultiWiBi <C> 7.4 <C> 2.56 <C> 0.47 <C> 8.0 <C> 4.43 <C> 0.63 <R> <C> Chinese <C> Char TFIDF <C> 6.32 <C> [BOLD] 3.92 <C> [BOLD] 0.68 <C> 6.95 <C> [BOLD] 4.48 <C> [BOLD] 0.68 <CAP> Table 4: Comparison of average path length (AL), average length of correct path prefix (ACPP), and average ratio of CPP to path lengths (ARCPP).
<R> <C> Task <C> TSQ <C> Eng QT <C> Spa QT <R> <C> # Tokens <C> 57.2K <C> 42.5K <C> 16.6K <R> <C> # Test Questions <C> 271 <C> 381 <C> 261 <R> <C> Avg. # Judgments per TQ <C> 55.72 <C> 28.60 <C> 16.28 <R> <C> Accuracy <C> 0.98 <C> 0.98 <C> 0.97 <R> <C> Avg. Acc of SJ per TQ <C> 0.88 <C> 0.89 <C> 0.87 <R> <C> Avg. Agrmnt of SJ wrt MV <C> 0.89 <C> 0.90 <C> 0.87 <R> <C> Accuracy(1+1) <C> 0.89 <C> 0.92 <C> 0.91 <R> <C> Accuracy(2+1) <C> 0.94 <C> 0.92 <C> 0.92 <R> <C> Accuracy(3+1) <C> 0.94 <C> 0.96 <C> 0.96 <R> <C> Accuracy(4+1) <C> 0.96 <C> 0.95 <C> 0.96 <CAP> Table 2: Accuracy and Agreement measurements per task.
<R> <C> Setting <C> [ITALIC] F1 <R> <C> Add <C> 32.7241147628∗ <R> <C> AddMul <C> 32.5742784297 <R> <C> Affine <C> 31.6653789348 <R> <C> Disjoint <C> 27.8694132587 <R> <C> Tied <C> 30.7020859959 <R> <C> MaLOPa <C> 30.6903287165 <R> <C> Fenda <C> 31.4313323055 <R> <C> Doc2Vec <C> 26.7319113957 <CAP> Table 7: Results of section 4.4. Add is significantly outperforming the best baseline Fenda(p=$0.044286491784080818$) and AddMul borderline significant (p=$0.08163768966141785$) against Fenda , the best-performing domain-adaptive baseline model under paired t-test. The difference between Addand AddMul against other baseline models are also significant under paired t-test. Hyperparameters are regularization strength C and transfer setting.
<R> <C> Setting <C> [ITALIC] F1 E <C> [ITALIC] F1 E+I+R <C> [ITALIC] F1 E+R <C> [ITALIC] F1 E+I <R> <C> Add <C> 24.3328051165 <C> 30.1534183525 <C> 25.9690487907 <C> 32.6839251657 <R> <C> AddMul <C> 22.8737340439 <C> 30.2506606426 <C> 27.8310546096 <C> 33.0636945739 <R> <C> Affine <C> 30.8249878622 <C> 28.3745130702 <C> 26.554399144 <C> 33.0876984225 <R> <C> Disjoint <C> 27.6358589692 <C> 29.2908998428 <C> 26.2468863835 <C> 25.7771756349 <R> <C> Tied <C> 27.2349753625 <C> 31.2022182115 <C> 25.2450905502 <C> 30.8631942751 <CAP> Table 8: Breakdown on different transfer settings.
<R> <C> [ITALIC] f <C> MAP <C> P@50 <C> P@100 <C> P@200 <C> P@300 <R> <C> [ITALIC] f0 <C> 0.42 <C> 0.40 <C> 0.44 <C> 0.42 <C> 0.38 <R> <C> [ITALIC] f1 <C> 0.58 <C> [BOLD] 0.70 <C> 0.60 <C> 0.53 <C> [BOLD] 0.44 <R> <C> [ITALIC] f2 <C> 0.48 <C> 0.56 <C> 0.52 <C> 0.49 <C> 0.42 <R> <C> [ITALIC] f3 <C> [BOLD] 0.59 <C> 0.68 <C> [BOLD] 0.63 <C> [BOLD] 0.55 <C> [BOLD] 0.44 <R> <C> [ITALIC] f4 <C> 0.56 <C> 0.40 <C> 0.48 <C> 0.50 <C> 0.42 <CAP> Table 3: Ranking results of scoring functions.
<R> <C> Language <C> % valid <C> BLEU <C> Relation length Gold <C> Relation length Auto <R> <C> French <C> 81.6% <C> 0.47 <C> 3.6 <C> 2.5 <R> <C> Hindi <C> 64.9% <C> 0.38 <C> 4.1 <C> 2.8 <R> <C> Russian <C> 63.5% <C> 0.62 <C> 1.8 <C> 1.7 <CAP> Таблица 1: % of valid relations and BLEU score of the extracted relations across languages with the average relation phrase length (in words).
<R> <C> Hyperparameter <C> Value <R> <C> |N|, |P| <C> 10, 20 <R> <C> [ITALIC] n <C> 60 <R> <C> [ITALIC] d <C> 300 <R> <C> [ITALIC] α <C> 10 <R> <C> #layers of  [ITALIC] f(1),  [ITALIC] f(2),  [ITALIC] f(3) <C> 6, 6, 4 <R> <C> non-linear activation <C> relu <CAP> Table 1: Hyper-parameters and values
<R> <C> [BOLD] Length <C> 1-3 <C> 4-6 <C> 7-10 <R> <C> Real Input <C> 0.439 <C> 0.518 <C> 0.566 <R> <C> TDGPN <C> 0.69 <C> 0.726 <C> 0.748 <CAP> Table 3: Average similarity scores between the output response and the target response in Real list.
<R> <C> [BOLD] Model <C> [BOLD] # Updates Pre-training <C> [BOLD] # Updates Fine-tuning <R> <C> [ITALIC] No adaptation <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] Out-domain <C> 28,750 <C> - <R> <C> [BOLD] In-domain <C> 4,047 <C> - <R> <C> [ITALIC] Baselines <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] FT <C> 28,750 <C> 2,160 <R> <C> [BOLD] BT <C> 56,350 <C> 246,510 <R> <C> [BOLD] MDL <C> 21,080 <C> - <R> <C> [ITALIC] Proposed <C> [EMPTY] <C> [EMPTY] <R> <C> [BOLD] VA (Linear) <C> 28,750 <C> 1,988 <R> <C> [BOLD] VA (LLM) <C> 28,750 <C> 2,556 <R> <C> [BOLD] BT + VA (LLM) <C> 56,350 <C> 91,212 <CAP> Table 6: The number of updates until convergence in En \rightarrow Ja translation with the 100k training set and 2000k monolingual corpus in the target domain.
<R> <C> [BOLD] Model <C> [BOLD] Measure ROC-AUC <C> [BOLD] Measure F1 <R> <C> HHN <C> [BOLD] 0.67224 <C> [BOLD] 0.53155 <R> <C> TextCNN(T&C) <C> 0.66185 <C> 0.51734 <R> <C> TextRNN(T&C) <C> 0.65026 <C> 0.49115 <R> <C> TextSAN(T&C) <C> 0.64851 <C> 0.48759 <R> <C> FastText(T&C) <C> 0.64489 <C> 0.47466 <CAP> Table 2: The experimental results in the ideal situation.
<R> <C> [BOLD] Model <C> [BOLD] Measure ROC-AUC <C> [BOLD] Measure F1 <R> <C> FedCNN <C> [BOLD] 0.65694 <C> [BOLD] 0.50881 <R> <C> TextCNN(T) <C> 0.63943 <C> 0.47120 <R> <C> TextCNN(C) <C> 0.61256 <C> 0.44583 <R> <C> FedRNN <C> [BOLD] 0.64627 <C> [BOLD] 0.47744 <R> <C> TextRNN(T) <C> 0.63306 <C> 0.47102 <R> <C> TextRNN(C) <C> 0.60779 <C> 0.42497 <R> <C> FedSAN <C> [BOLD] 0.64888 <C> [BOLD] 0.48603 <R> <C> TextSAN(T) <C> 0.63555 <C> 0.46201 <R> <C> TextSAN(C) <C> 0.59554 <C> 0.40951 <R> <C> FedFastText <C> [BOLD] 0.64224 <C> [BOLD] 0.46843 <R> <C> FastText(T) <C> 0.63287 <C> 0.45195 <R> <C> FastText(C) <C> 0.59350 <C> 0.39497 <R> <C> FedHHN <C> [BOLD] 0.66835 <C> [BOLD] 0.52898 <CAP> Table 3: The experimental results in the Data Island.
<R> <C> [BOLD] Model Name <C> English Dataset P@100 <C> English Dataset P@200 <C> English Dataset P@300 <C> English Dataset  [BOLD] Mean <C> English Dataset Δ [ITALIC] Base <C> Chinese Dataset P@100 <C> Chinese Dataset P@200 <C> Chinese Dataset P@300 <C> Chinese Dataset  [BOLD] Mean <C> Chinese Dataset Δ [ITALIC] Base <R> <C> [ITALIC] ACNN <C> 96.70 <C> 92.61 <C> 91.72 <C> 93.68 <C> – <C> 89.08 <C> 86.89 <C> 84.52 <C> 86.83 <C> – <R> <C> [ITALIC] ACNN(Coh) <C> 97.39 <C> 93.78 <C> 90.69 <C> 93.96 <C> +0.3 <C> 95.86 <C> 94.86 <C> 93.04 <C> 94.59 <C> +7.8 <R> <C> [ITALIC] ACNN(Sem) <C> 97.62 <C> 95.87 <C> 94.12 <C> 95.87 <C> +2.2 <C> 95.97 <C> 94.61 <C> 93.53 <C> 94.70 <C> +8.1 <R> <C> [ITALIC] ACNN+ILP <C> 97.87 <C> 94.36 <C> 93.16 <C> 95.13 <C> +1.5 <C> 93.75 <C> 92.18 <C> 90.10 <C> 92.01 <C> +5.2 <R> <C> [ITALIC] ACNN(Coh)+ILP <C> 97.73 <C> 94.51 <C> 91.29 <C> 94.51 <C> +0.8 <C> 97.09 <C> 96.18 <C> 94.01 <C> 95.76 <C> +9.0 <R> <C> [ITALIC] ACNN(Sem)+ILP <C> [BOLD] 98.17 <C> [BOLD] 96.6 <C> [BOLD] 95.48 <C> [BOLD] 96.75 <C> [BOLD] +3.1 <C> [BOLD] 97.73 <C> [BOLD] 96.40 <C> [BOLD] 94.43 <C> [BOLD] 96.18 <C> [BOLD] +9.4 <R> <C> [ITALIC] APCNN <C> 100 <C> 98.97 <C> 97.41 <C> 98.79 <C> – <C> 92.96 <C> 91.75 <C> 91.08 <C> 91.93 <C> – <R> <C> [ITALIC] APCNN(Coh) <C> 100 <C> 99.57 <C> 97.33 <C> 98.97 <C> +0.2 <C> 98.88 <C> 96.00 <C> 94.98 <C> 96.62 <C> +4.7 <R> <C> [ITALIC] APCNN(Sem) <C> 100 <C> 100 <C> 97.95 <C> 99.32 <C> +0.5 <C> 100 <C> 96.97 <C> 93.42 <C> 96.80 <C> +4.9 <R> <C> [ITALIC] APCNN+ILP <C> 100 <C> 99.13 <C> 97.55 <C> 98.89 <C> +0.1 <C> 96.06 <C> 95.15 <C> 94.63 <C> 95.28 <C> +3.4 <R> <C> [ITALIC] APCNN(Coh)+ILP <C> 100 <C> 100 <C> 98.03 <C> 99.34 <C> +0.6 <C> 99.07 <C> 96.17 <C> [BOLD] 95.16 <C> 96.79 <C> +4.9 <R> <C> [ITALIC] APCNN(Sem)+ILP <C> [BOLD] 100 <C> [BOLD] 100 <C> [BOLD] 98.39 <C> [BOLD] 99.46 <C> [BOLD] +0.7 <C> [BOLD] 100 <C> [BOLD] 97.67 <C> 94.25 <C> [BOLD] 97.31 <C> [BOLD] +5.4 <CAP> Table 2: Summary P@N(%) scores of our approach on two datasets with ACNN and APCNN as base models. ΔBase indicates the difference between mentioned model and the base NRE model (ACNN in the top and APCNN in the bottom). And the name with +ILP means that we perform ILP over the model’s outputs as an extra post-processing.
<R> <C> source <C> all <C> >=2 <C> >=3 <C> >=4 <C> >=5 <R> <C> W2V all <C> 29013 <C> 28910 <C> 27299 <C> 20732 <C> 12843 <R> <C> W2V freq >100,000 <C> 11730 <C> 11627 <C> 10373 <C> 7881 <C> 5758 <R> <C> BNC <C> 7692 <C> 7491 <C> 6681 <C> 5926 <C> 4925 <CAP> Table 1: Sizes of the extracted parallel datasets
<R> <C> source <C> all Correct <C> all False positive <C> all No output <C> >=2 Correct <C> >=2 False positive <C> >=2 No output <C> >=3 Correct <C> >=3 False positive <C> >=3 No output <C> >=4 Correct <C> >=4 False positive <C> >=4 No output <C> >=5 Correct <C> >=5 False positive <C> >=5 No output <R> <C> W2V all <C> 0,510 <C> 0,350 <C> 0,140 <C> 0,500 <C> 0,375 <C> 0,125 <C> 0,520 <C> 0,325 <C> 0,155 <C> 0,490 <C> 0,390 <C> 0,120 <C> 0,525 <C> 0,390 <C> 0,085 <R> <C> W2V freq >100,000 <C> 0,515 <C> 0,305 <C> 0,180 <C> 0,540 <C> 0,310 <C> 0,150 <C> 0,510 <C> 0,340 <C> 0,150 <C> 0,540 <C> 0,315 <C> 0,145 <C> 0,515 <C> 0,330 <C> 0,155 <R> <C> BNC <C> [BOLD] 0,580 <C> 0,285 <C> 0,135 <C> 0,555 <C> 0,300 <C> 0,145 <C> 0,570 <C> [BOLD] 0,245 <C> 0,185 <C> 0,550 <C> 0,310 <C> 0,140 <C> 0,550 <C> 0,315 <C> 0,135 <CAP> Table 2: Results of the NMT models trained on different datasets
<R> <C> [BOLD] Model <C> [BOLD] R@1  [BOLD] Dev <C> [BOLD] R@1  [BOLD] Test <C> [BOLD] R@5  [BOLD] Dev <C> [BOLD] R@5  [BOLD] Test <C> [BOLD] R@10  [BOLD] Dev <C> [BOLD] R@10  [BOLD] Test <C> [BOLD] Upper Bound  [BOLD] Dev <C> [BOLD] Upper Bound  [BOLD] Test <R> <C> VisualBERT w/o COCO Pre-training <C> 68.07 <C> - <C> 83.98 <C> - <C> 86.24 <C> - <C> 86.97 <C> 87.45 <R> <C> VisualBERT (Li et al.,  2019 ) <C> 70.40 <C> 71.33 <C> 84.49 <C> [BOLD] 84.98 <C> 86.31 <C> [BOLD] 86.51 <C> [EMPTY] <C> [EMPTY] <R> <C> Ours L1-H2-abs <C> 69.8 <C> [BOLD] 71.36 <C> 84.22 <C> 84.76 <C> 86.21 <C> 86.49 <C> 86.97 <C> 87.45 <CAP> Table 3: Comparison with VisualBERT on Flickr30K Entities grounding
<R> <C> [BOLD] GEC model <C> [EMPTY] <R> <C> Model Architecture <C> Transformer (big) <R> <C> Number of epochs <C> 30 <R> <C> Max tokens <C> 4096 <R> <C> Optimizer <C> Adam <R> <C> [EMPTY] <C> ( [ITALIC] β1=0.9, [ITALIC] β2=0.98, [ITALIC] ϵ=1×10−8) <R> <C> Learning rate <C> 3×10−5 <R> <C> Min learning rate <C> 1×10−6 <R> <C> Loss function <C> label smoothed cross-entropy <R> <C> [EMPTY] <C> ( [ITALIC] ϵls=0.1) <R> <C> [EMPTY] <C> Szegedy et al. ( 2016 ) <R> <C> Dropout <C> 0.3 <R> <C> Gradient Clipping <C> 0.1 <R> <C> Beam search <C> 5 <R> <C> [BOLD] GED model <C> [EMPTY] <R> <C> Model Architecture <C> BERT-Base (cased) <R> <C> Number of epochs <C> 3 <R> <C> Batch size <C> 32 <R> <C> Max sentence length <C> 128 <R> <C> Optimizer <C> Adam <R> <C> [EMPTY] <C> ( [ITALIC] β1=0.9, [ITALIC] β2=0.999, [ITALIC] ϵ=1×10−8) <R> <C> Learning rate <C> 4e−5 <R> <C> Dropout <C> 0.1 <CAP> Table 1: Hyperparameters values of GEC model and Fine-tuned BERT.
<R> <C> [EMPTY] <C> BEA-test (ERRANT)  [BOLD] P <C> BEA-test (ERRANT)  [BOLD] R <C> BEA-test (ERRANT)  [BOLD] F0.5 <C> CoNLL-14 (M2)  [BOLD] P <C> CoNLL-14 (M2)  [BOLD] R <C> CoNLL-14 (M2)  [BOLD] F0.5 <C> FCE-test (M2)  [BOLD] P <C> FCE-test (M2)  [BOLD] R <C> FCE-test (M2)  [BOLD] F0.5 <C> JFLEG  [BOLD] GLEU <R> <C> w/o BERT <C> 51.5 <C> 43.2 <C> 49.6 <C> 59.2 <C> 31.2 <C> 50.2 <C> 61.7 <C> 46.4 <C> 57.9 <C> 52.7 <R> <C> BERT-init <C> 55.1 <C> 43.7 <C> 52.4 <C> 61.3 <C> 31.5 <C> 51.4 <C> 62.4 <C> 46.9 <C> 58.5 <C> 53.0 <R> <C> BERT-fuse <C> 57.5 <C> [BOLD] 44.9 <C> 54.4 <C> 62.3 <C> 31.3 <C> 52.0 <C> 64.0 <C> 47.6 <C> 59.8 <C> 54.1 <R> <C> BERT-fuse mask <C> 57.1 <C> 44.7 <C> 54.1 <C> 62.9 <C> 32.2 <C> 52.8 <C> 64.3 <C> 48.1 <C> 60.2 <C> 54.2 <R> <C> BERT-fuse GED <C> [BOLD] 58.1 <C> 44.8 <C> [BOLD] 54.8 <C> [BOLD] 63.6 <C> [BOLD] 33.0 <C> [BOLD] 53.6 <C> [BOLD] 65.0 <C> [BOLD] 49.6 <C> [BOLD] 61.2 <C> [BOLD] 54.4 <R> <C> w/o BERT <C> 66.1 <C> 59.9 <C> 64.8 <C> 68.5 <C> 44.8 <C> 61.9 <C> 56.5 <C> 48.1 <C> 54.9 <C> 61.0 <R> <C> BERT-fuse <C> 66.6 <C> 60.0 <C> 65.2 <C> 68.3 <C> [BOLD] 45.7 <C> 62.1 <C> 59.7 <C> [BOLD] 48.5 <C> [BOLD] 57.0 <C> 61.2 <R> <C> BERT-fuse mask <C> 67.0 <C> 60.0 <C> 65.4 <C> 68.8 <C> 45.3 <C> 62.3 <C> 59.7 <C> 47.1 <C> 56.6 <C> 61.2 <R> <C> BERT-fuse GED <C> [BOLD] 67.1 <C> [BOLD] 60.1 <C> [BOLD] 65.6 <C> [BOLD] 69.2 <C> 45.6 <C> [BOLD] 62.6 <C> [BOLD] 59.8 <C> 46.9 <C> 56.7 <C> 61.3 <R> <C> Lichtarge et al. ( 2019 ) <C> - <C> - <C> - <C> 65.5 <C> 37.1 <C> 56.8 <C> - <C> - <C> - <C> [BOLD] 61.6 <R> <C> Awasthi et al. ( 2019 ) <C> - <C> - <C> - <C> 66.1 <C> 43.0 <C> 59.7 <C> - <C> - <C> - <C> 60.3 <R> <C> Kiyono et al. ( 2019 ) <C> 65.5 <C> 59.4 <C> 64.2 <C> 67.9 <C> 44.1 <C> 61.3 <C> - <C> - <C> - <C> 59.7 <R> <C> BERT-fuse GED + R2L <C> 72.3 <C> [BOLD] 61.4 <C> 69.8 <C> [BOLD] 72.6 <C> [BOLD] 46.4 <C> [BOLD] 65.2 <C> 62.8 <C> 48.8 <C> 59.4 <C> 62.0 <R> <C> Lichtarge et al. ( 2019 ) <C> - <C> - <C> - <C> 66.7 <C> 43.9 <C> 60.4 <C> - <C> - <C> - <C> [BOLD] 63.3 <R> <C> Grundkiewicz et al. ( 2019 ) <C> 72.3 <C> 60.1 <C> 69.5 <C> - <C> - <C> 64.2 <C> - <C> - <C> - <C> 61.2 <R> <C> Kiyono et al. ( 2019 )∗ <C> [BOLD] 74.7 <C> 56.7 <C> [BOLD] 70.2 <C> 72.4 <C> 46.1 <C> 65.0 <C> - <C> - <C> - <C> 61.4 <CAP> Table 2: Results of our GEC models. The top group shows the results of the single models without using pseudo-data and/or ensemble. The second group shows the results of the single models using pseudo-data. The third group shows ensemble models using pseudo-data. Bold indicates the highest score in each column. * reports the state-of-the-art scores for BEA test and CoNLL 2014 for two separate models: models with and without SED. We filled out a single line with the results from such two separate models.
<R> <C> EMOTION <C> Valence <C> Arousal <C> Dominance <R> <C> AFRAID <C> 2.25 <C> 5.12 <C> 2.71 <R> <C> AMUSED <C> 7.05 <C> 4.27 <C> 5.93 <R> <C> ANGRY <C> 2.53 <C> 6.02 <C> 4.11 <R> <C> ANNOYED <C> 2.80 <C> 5.29 <C> 4.08 <R> <C> DONT_CARE <C> 3.53 <C> 4.27 <C> 3.62 <R> <C> HAPPY <C> 8.47 <C> 6.05 <C> 7.21 <R> <C> INSPIRED <C> 6.89 <C> 5.56 <C> 7.30 <R> <C> SAD <C> 2.10 <C> 3.49 <C> 3.84 <CAP> Table 6: Valence, Arousal and Dominance scores for emotion labels, as provided by [34].
<R> <C> [EMPTY] <C> method <C> Sim <C> Rel <C> MEN <C> M.Turk <C> Rare <C> S999 <R> <C> [EMPTY] <C> CBOW <C> 0.388 <C> 0.438 <C> 0.383 <C> 0.579 <C> 0.050 <C> 0.075 <R> <C> [EMPTY] <C> SGNS <C> 0.674 <C> 0.654 <C> 0.561 <C> 0.608 <C> 0.027 <C> 0.215 <R> <C> [EMPTY] <C> GloVe <C> 0.431 <C> 0.466 <C> 0.421 <C> 0.508 <C> 0.118 <C> 0.096 <R> <C> [EMPTY] <C> fastText <C> 0.655 <C> 0.609 <C> 0.636 <C> 0.623 <C> 0.059 <C> 0.223 <R> <C> [EMPTY] <C> tail-cut <C> 0.762 <C> 0.667 <C> 0.682 <C> 0.649 <C> 0.121 <C> 0.212 <R> <C> [EMPTY] <C> LCA  [BOLD] Nflat <C> 0.749 <C> 0.680 <C> 0.671 <C> 0.668 <C> 0.127 <C> 0.218 <R> <C> [EMPTY] <C> LCA  [BOLD] Nskip <C> 0.741 <C> 0.657 <C> 0.672 <C> 0.640 <C> 0.135 <C> 0.211 <R> <C> [EMPTY] <C> SCA+MEN <C> 0.743 <C> 0.665 <C> 0.770 <C> 0.636 <C> 0.136 <C> 0.210 <R> <C> [EMPTY] <C> SCA+M.Turk <C> 0.741 <C> 0.658 <C> 0.672 <C> 0.798 <C> 0.136 <C> 0.211 <CAP> Table 4: Comparisons using the text8 corpus.
<R> <C> Words <C> [ITALIC] v <C> [ITALIC] u <C> [ITALIC] δ <R> <C> network <C> 1.00000 <C> 0.32680 <C> 0.67320 <R> <C> international <C> 0.57339 <C> 0.49485 <C> 0.07855 <R> <C> computer <C> 0.56474 <C> 0.48814 <C> 0.07660 <R> <C> system <C> 0.52506 <C> 0.52577 <C> -0.00072 <R> <C> software <C> 0.50420 <C> 0.68041 <C> -0.17621 <R> <C> use <C> 0.40142 <C> 1.00000 <C> -0.59858 <CAP> Table 1: An optimal micro cluster of an actor: ”Abdullah Mohd Zin”
<R> <C> Method <C> R. Ch. AUC-ROC <C> R. Ch. O.A. <R> <C> DL-T <C> 0.31 <C> 0.24 <R> <C> DL-BERT <C> 0.30 <C> 0.27 <R> <C> DL-T-noAtt <C> 0.29 <C> 0.24 <R> <C> DL-T-basic <C> 0.28 <C> 0.23 <R> <C> DL-CNN <C> 0.28 <C> 0.13 <R> <C> LR <C> 0.24 <C> 0.21 <CAP> Table 4: Relative change w.r.t popularity model of AUC-ROC and Overall Accuracy: transformer model (DL-T), BERT model (DL-BERT), transformer without special context-aware attention (DL-T-noAtt) and without both special attention and modified loss (DL-T-basic), CNN model (DL-CNN) and LR model (LR).
<R> <C> [EMPTY] <C> tokyo (2nd gen 20-qubit system) <C> poughkeepsie (3rd gen 20-qubit system) <R> <C> Mean of Two-qubit (CNOT) error rates ×10−2 <C> 2.84 <C> 2.25 <R> <C> best <C> 1.47 <C> 1.11 <R> <C> worst <C> 7.12 <C> 6.11 <R> <C> Mean of Single-qubit error rates ×10−3 <C> 1.99 <C> 1.07 <R> <C> best <C> 0.64 <C> 0.52 <R> <C> worst <C> 6.09 <C> 2.77 <CAP> Table 1: the performance specifications of two IBMQ 20-qubit systems named Tokyo and Poughkeepsie
<R> <C> Accents (Abbr.) <C> No. of utterances <C> Proportion (%) <C> Total Duration1 <C> Total Duration2 <C> Comp. rate (%) <R> <C> AR <C> 112 <C> 2.27 <C> 0:34:32 <C> 0:29:11 <C> 84.5 <R> <C> BP <C> 459 <C> 9.32 <C> 2:34:24 <C> 2:09:58 <C> 84.2 <R> <C> FR <C> 284 <C> 5.77 <C> 1:31:05 <C> 1:18:44 <C> 86.4 <R> <C> GE <C> 325 <C> 6.6 <C> 1:36:04 <C> 1:22:18 <C> 85.7 <R> <C> HI <C> 348 <C> 7.07 <C> 1:56:10 <C> 1:36:31 <C> 83.1 <R> <C> MA <C> 282 <C> 5.73 <C> 1:30:37 <C> 1:16:06 <C> 84.0 <R> <C> RU <C> 236 <C> 4.79 <C> 1:11:13 <C> 0:59:54 <C> 84.1 <R> <C> Note: duration1 and duration2 are the duration before and after silence removal <C> Note: duration1 and duration2 are the duration before and after silence removal <C> Note: duration1 and duration2 are the duration before and after silence removal <C> Note: duration1 and duration2 are the duration before and after silence removal <C> Note: duration1 and duration2 are the duration before and after silence removal <C> Note: duration1 and duration2 are the duration before and after silence removal <CAP> Tab. 1: Summary of selected accents in FAE corpus
<R> <C> [EMPTY] <C> DRSS  [BOLD] Ws <C> DRSS  [BOLD] Wsc <C> DRSS  [BOLD] Wt <C> DRSS  [BOLD] Wtc <C> DRSS-Adv  [BOLD] Ws <C> DRSS-Adv  [BOLD] Wsc <C> DRSS-Adv  [BOLD] Wt <C> DRSS-Adv  [BOLD] Wtc <R> <C> [BOLD] Ws <C> 1.000 <C> 0.242 <C> 0.101 <C> 0.205 <C> 1.000 <C> 0.090 <C> 0.055 <C> 0.062 <R> <C> [BOLD] Wsc <C> 0.242 <C> 1.000 <C> 0.008 <C> 0.247 <C> 0.090 <C> 1.000 <C> 0.024 <C> 0.221 <R> <C> [BOLD] Wt <C> 0.101 <C> 0.008 <C> 1.000 <C> 0.127 <C> 0.055 <C> 0.024 <C> 1.000 <C> 0.043 <R> <C> [BOLD] Wtc <C> 0.205 <C> 0.247 <C> 0.127 <C> 1.000 <C> 0.062 <C> 0.221 <C> 0.043 <C> 1.000 <CAP> Table 6. Correlation Matrices on SNLI→Fict.
<R> <C> Method <C> Yahoo Rec↓ <C> Yahoo PPL↓ <C> Yahoo KL <C> PTB Rec↓ <C> PTB PPL↓ <C> PTB KL <C> SNLI Rec↓ <C> SNLI PPL↓ <C> SNLI KL <R> <C> LSTM-LM <C> - <C> 60.75 <C> - <C> - <C> 100.47 <C> - <C> - <C> 21.44 <C> - <R> <C> VAE <C> 329.08 <C> 61.60 <C> 0.00 <C> 102.31 <C> 106.59 <C> 0.00 <C> 33.36 <C> 22.22 <C> 0.007 <R> <C> +anneal <C> 328.62 <C> 60.34 <C> 0.00 <C> 102.18 <C> 105.87 <C> 0.00 <C> 32.33 <C> 20.19 <C> 1.05 <R> <C> +cyclic <C> 328.78 <C> 61.35 <C> 0.06 <C> 102.07 <C> 105.36 <C> 0.00 <C> 31.71 <C> 19.07 <C> 1.65 <R> <C> +aggressive <C> 323.08 <C> 57.12 <C> 4.94 <C> 100.81 <C> 96.46 <C> 1.85 <C> 32.91 <C> 21.31 <C> 0.47 <R> <C> +FBP <C> 326.58 <C> 59.68 <C> 8.08 <C> 98.06 <C> 89.91 <C> 5.91 <C> 31.91 <C> 19.41 <C> 1.99 <R> <C> +pretraining+FBP <C> 316.17 <C> 52.39 <C> 16.17 <C> 94.88 <C> 76.21 <C> 7.47 <C> 30.62 <C> 17.22 <C> 2.83 <R> <C> GVAM <C> 350.14 <C> 79.28 <C> 0.00 <C> 102.20 <C> 105.94 <C> 0.00 <C> 30.90 <C> 17.68 <C> 0.38 <R> <C> DVAM (K=128) <C> 303.65 <C> 44.36 <C> 1.88 <C> 79.94 <C> 38.38 <C> 2.21 <C> 16.08 <C> 4.46 <C> 2.33 <R> <C> DVAM (K=512) <C> [BOLD] 259.68 <C> [BOLD] 25.83 <C> 2.60 <C> [BOLD] 64.79 <C> [BOLD] 19.22 <C> 3.13 <C> [BOLD] 11.06 <C> [BOLD] 2.82 <C> 2.58 <CAP> Table 2: Results of language modelling on Yahoo, PTB and SNLI Datasets.
<R> <C> [EMPTY] <C> [ITALIC] clef11  [ITALIC] piv <C> [ITALIC] clef11  [ITALIC] piv <C> [ITALIC] clef11  [ITALIC] bm25 <C> [ITALIC] clef11  [ITALIC] bm25 <C> [ITALIC] clef12  [ITALIC] piv <C> [ITALIC] clef12  [ITALIC] piv <C> [ITALIC] clef12  [ITALIC] bm25 <C> [ITALIC] clef12  [ITALIC] bm25 <R> <C> [EMPTY] <C> [ITALIC] MAP <C> [ITALIC] P@10 <C> [ITALIC] MAP <C> [ITALIC] P@10 <C> [ITALIC] MAP <C> [ITALIC] P@10 <C> [ITALIC] MAP <C> [ITALIC] P@10 <R> <C> [ITALIC] NoEmb-NoSim <C> 0.1096 <C> 0.2300 <C> 0.1552 <C> 0.3100 <C> 0.0978 <C> 0.1381 <C> 0.1083 <C> 0.1571 <R> <C> [ITALIC] NoEmb-Leacock <C> 0.1085 <C> 0.2267 <C> 0.1505 <C> 0.2933 <C> 0.0927 <C> 0.1429 <C> 0.1064 <C> [BOLD] 0.1667 <R> <C> [ITALIC] FEmb_Eq 2 <C> 0.1089 <C> [BOLD] 0.2333 <C> 0.1608 <C> [BOLD] 0.3167 <C> 0.0934 <C> 0.1429 <C> 0.1119 <C> 0.1524 <R> <C> [ITALIC] HEmb_Eq 2 <C> 0.1111 <C> 0.2100 <C> 0.1640*\dagger <C> 0.3133 <C> 0.0987 <C> [BOLD] 0.1524 <C> 0.1140* <C> 0.1619 <R> <C> [ITALIC] WEmb_Eq 2 <C> [BOLD] 0.1137* <C> 0.2267 <C> [BOLD] 0.1654*\dagger <C> 0.3133 <C> [BOLD] 0.1012 <C> 0.1476 <C> [BOLD] 0.1154* <C> 0.1571 <CAP> Table 1: Experimental results for clef11 and clef12 collections
<R> <C> [BOLD] Method <C> [BOLD] Model <C> [ITALIC] α <C> [BOLD] ASNQ MAP <C> [BOLD] ASNQ nDCG@10 <C> [BOLD] ASNQ P@1 <C> [BOLD] ASNQ MRR <C> [BOLD] GPD MAP <C> [BOLD] GPD nDCG@10 <C> [BOLD] GPD P@1 <C> [BOLD] GPD MRR <C> [BOLD] Cost reduction per batch <R> <C> Monolithic transformer (MT) <C> 4 layers TandA <C> – <C> 31.5 <C> 30.8 <C> 25.9 <C> 30.8 <C> 38.9 <C> 50.1 <C> 40.8 <C> 54.0 <C> −67% <R> <C> Monolithic transformer (MT) <C> 6 layers TandA <C> – <C> 60.2 <C> 58.7 <C> 47.2 <C> 59.2 <C> 51.4 <C> 64.1 <C> 56.1 <C> 67.6 <C> −50% <R> <C> Monolithic transformer (MT) <C> 8 layers TandA <C> – <C> 63.9 <C> 62.2 <C> 49.2 <C> 62.4 <C> 56.3 <C> 68.7 <C> 61.2 <C> 70.4 <C> −33% <R> <C> Monolithic transformer (MT) <C> 10 layers TandA <C> – <C> 65.3 <C> 64.5 <C> 52.0 <C> 64.1 <C> 57.2 <C> 71.3 <C> 64.9 <C> 72.7 <C> −20% <R> <C> 2-12 <C> \textsc  [ITALIC] TandA \textsc  [ITALIC] base <C> – <C> 65.5 <C> 65.1 <C> 52.1 <C> 64.7 <C> [BOLD] 58.0 <C> [BOLD] 72.2 <C> [BOLD] 67.5 <C> 76.8 <C> [ITALIC] baseline <R> <C> Sequential Ranker (SR) <C> MT models, 4 to 12 layers, in sequence <C> 0.3 <C> 65.4 <C> 65.1 <C> 52.1 <C> 64.8 <C> 55.8 <C> 70.2 <C> 66.2 <C> 74.3 <C> +53% <R> <C> Sequential Ranker (SR) <C> MT models, 4 to 12 layers, in sequence <C> 0.4 <C> 64.9 <C> 64.2 <C> 51.6 <C> 64.2 <C> 53.8 <C> 69.6 <C> 65.6 <C> 73.0 <C> +18% <R> <C> Sequential Ranker (SR) <C> MT models, 4 to 12 layers, in sequence <C> 0.5 <C> 64.6 <C> 63.4 <C> 50.8 <C> 63.5 <C> 52.2 <C> 68.4 <C> 63.0 <C> 72.3 <C> −10% <R> <C> Cascade transformer (CT) <C> 4 layers CT <C> 0.0 <C> 22.0 <C> 19.3 <C> 10.2 <C> 18.3 <C> 32.7 <C> 38.9 <C> 35.2 <C> 42.6 <C> −67% <R> <C> Cascade transformer (CT) <C> 6 layers CT <C> 0.0 <C> 49.1 <C> 47.2 <C> 32.7 <C> 47.7 <C> 44.8 <C> 56.0 <C> 47.3 <C> 58.5 <C> −50% <R> <C> Cascade transformer (CT) <C> 8 layers CT <C> 0.0 <C> 62.8 <C> 61.5 <C> 48.7 <C> 61.9 <C> 53.8 <C> 71.7 <C> 61.2 <C> 69.1 <C> −33% <R> <C> Cascade transformer (CT) <C> 10 layers CT <C> 0.0 <C> 65.6 <C> 65.1 <C> 53.0 <C> 65.2 <C> 55.8 <C> 72.0 <C> 63.1 <C> 72.1 <C> −20% <R> <C> 2-12 <C> Full CT (12 layers) <C> 0.0 <C> [BOLD] 66.3 <C> [BOLD] 66.1 <C> [BOLD] 53.2 <C> [BOLD] 65.4 <C> 57.8 <C> 71.9 <C> [BOLD] 67.5 <C> [BOLD] 76.9 <C> −0% <R> <C> [EMPTY] <C> Full CT (12 layers) <C> 0.3 <C> 65.3 <C> 65.3 <C> 52.9 <C> 65.3 <C> 55.7 <C> 69.8 <C> 66.2 <C> 75.1 <C> −37% <R> <C> [EMPTY] <C> Full CT (12 layers) <C> 0.4 <C> 64.8 <C> 65.0 <C> 52.5 <C> 64.8 <C> 52.8 <C> 68.6 <C> 65.6 <C> 74.3 <C> −45% <R> <C> [EMPTY] <C> Full CT (12 layers) <C> 0.5 <C> 64.1 <C> 65.0 <C> 52.4 <C> 64.5 <C> 50.2 <C> 66.1 <C> 62.4 <C> 72.9 <C> −51% <CAP> Table 3: Comparison of Cascade Transformers with other models on the ASNQ and GPD datasets. “Monolithic transformer” refers to a single transformer model trained independently; “sequential ranker” (ST) is a sequence of monolithic transformer models of size 4,6,…,12 trained independently; and “Cascade Transformer” (CT) is the approach we propose. This can train models that equal or outperform the state of the art when no drop is applied (i.e., α=0.0); with drop, they obtain the same performance with 37% to 51% fewer operations.
<R> <C> No. <C> Features Used <C> P <C> R <C> FM <C> AUC <R> <C> 1 <C> Linguistic <C> 0.58 <C> 0.60 <C> 0.56 <C> 0.60 <R> <C> 2 <C> Linguistic & Discretisation <C> 0.81 <C> 0.70 <C> 0.74 <C> 0.84 <R> <C> 3 <C> Linguistic & Discretisation & Other <C> 0.84 <C> 0.70 <C> 0.76 <C> 0.87 <R> <C> 4 <C> Linguistic & Other & User Rating (no discretisation) <C> 0.82 <C> 0.69 <C> 0.75 <C> 0.86 <R> <C> 5 <C> Linguistic & Other & User Rating (with discretisation) <C> 0.82 <C> 0.72 <C> 0.77 <C> 0.88 <R> <C> 6 <C> All features (Answer and User Rating with discretisation) <C> 0.88 <C> 0.85 <C> 0.86 <C> 0.94 <CAP> Table 2: Results for best answer prediction using different sets of features (Cases 1 to 6) for all SE websites. Columns show macro average precision (P), recall (R), F-Measure (FM) and Area-Under-Curve (AUC) for all 21 SE websites using 10-fold validation.
<R> <C> [BOLD] Method <C> [BOLD] Bas. <C> [BOLD] Blo. <C> [BOLD] Cal. <C> [BOLD] Hou. <C> [BOLD] Pub. <C> [BOLD] Rec. <C> [BOLD] Res. <C> [BOLD] Soc. <C> [BOLD] Avg. <C> [BOLD] ATIS <R> <C> Att <C> 80.1 <C> 55.4 <C> 61.9 <C> 53.4 <C> 60.2 <C> 64.4 <C> 71.1 <C> 76.8 <C> 65.4 <C> 78.6 <R> <C> Att+Pseudo(Q) <C> 80.1 <C> 59.4 <C> 60.1 <C> 52.9 <C> 59.6 <C> 66.2 <C> 73.8 <C> 79.0 <C> 66.4 <C> 78.8 <R> <C> Att+Pseudo(LF) <C> 83.9 <C> 60.4 <C> 64.3 <C> 54.5 <C> 58.4 <C> 69.0 <C> 70.5 <C> 77.3 <C> 67.3 <C> 77.9 <R> <C> Att+Pseudo(LF+Q) <C> 80.6 <C> 59.1 <C> 61.9 <C> 57.1 <C> 62.7 <C> 65.3 <C> 73.2 <C> [BOLD] 79.8 <C> 67.5 <C> 78.3 <R> <C> Att+Dual <C> 82.6 <C> 60.2 <C> 72.0 <C> 58.7 <C> 66.5 <C> 73.6 <C> 74.1 <C> 79.3 <C> 70.9 <C> 79.5 <R> <C> Att+Dual+Q <C> 83.9 <C> 60.7 <C> 70.2 <C> [BOLD] 60.8 <C> 69.6 <C> 71.3 <C> [BOLD] 76.2 <C> [BOLD] 79.8 <C> 71.6 <C> 79.7 <R> <C> Att+Dual+LF <C> 83.4 <C> 61.4 <C> 71.4 <C> 59.3 <C> 70.2 <C> 73.1 <C> 75.3 <C> 78.6 <C> 71.6 <C> 80.4 <R> <C> Att+Dual+LF+Q <C> [BOLD] 85.4 <C> [BOLD] 62.9 <C> [BOLD] 73.2 <C> 59.3 <C> [BOLD] 72.0 <C> [BOLD] 75.5 <C> 75.6 <C> 79.5 <C> [BOLD] 72.9 <C> [BOLD] 81.7 <R> <C> AttPtr <C> 81.1 <C> 58.1 <C> 63.1 <C> 48.7 <C> 55.3 <C> 69.4 <C> 68.4 <C> 77.4 <C> 65.2 <C> 84.8 <R> <C> AttPtr+Pseudo(Q) <C> 82.1 <C> 59.6 <C> 61.3 <C> 47.6 <C> 57.1 <C> 72.2 <C> 69.9 <C> 78.4 <C> 66.0 <C> 85.0 <R> <C> AttPtr+Pseudo(LF) <C> 82.4 <C> 59.1 <C> 62.5 <C> 54.5 <C> 63.4 <C> 71.3 <C> 69.6 <C> 77.6 <C> 67.5 <C> 86.2 <R> <C> AttPtr+Pseudo(LF+Q) <C> 81.3 <C> 59.4 <C> 65.5 <C> 49.2 <C> 58.4 <C> 72.7 <C> 72.0 <C> 78.6 <C> 67.1 <C> 85.0 <R> <C> AttPtr+Dual <C> 81.8 <C> 60.2 <C> 68.5 <C> 57.1 <C> 65.2 <C> 72.2 <C> 74.1 <C> 79.0 <C> 69.8 <C> 86.2 <R> <C> AttPtr+Dual+Q <C> 81.6 <C> 60.7 <C> 69.6 <C> 61.4 <C> 68.9 <C> 74.1 <C> [BOLD] 79.8 <C> 80.1 <C> 72.0 <C> 86.6 <R> <C> AttPtr+Dual+LF <C> 82.6 <C> [BOLD] 62.2 <C> 68.5 <C> [BOLD] 62.4 <C> 69.6 <C> 73.1 <C> 77.4 <C> 79.4 <C> 71.9 <C> [BOLD] 87.3 <R> <C> AttPtr+Dual+LF+Q <C> [BOLD] 83.6 <C> [BOLD] 62.2 <C> [BOLD] 72.6 <C> 61.9 <C> [BOLD] 71.4 <C> [BOLD] 75.0 <C> 76.5 <C> [BOLD] 80.4 <C> [BOLD] 73.0 <C> 86.8 <CAP> Table 4: Semi-supervised learning experiments. We keep 50% of the training set as labeled data randomly, and leave the rest as unpaired queries(Q) and logical forms(LF) to simulate unsupervised dataset.
<R> <C> [BOLD] Methodology <C> [BOLD] Median AP <C> [BOLD] Mean AP <R> <C> Stacking with auxiliary features <C> [BOLD] 0.526 <C> [BOLD] 0.546 <R> <C> Best standalone system (VGG + selective search)Ren et al.  <C> 0.450 <C> 0.454 <R> <C> Oracle Voting baseline (1 or more systems must agree) <C> 0.367 <C> 0.353 <CAP> Table 3: Results on 2015 ImageNet object detection task using the official ImageNet scorer.
<R> <C> Pruned <C> Pre-train Loss <C> MNLI 392k <C> QQP 363k <C> QNLI 108k <C> SST-2 67k <C> CoLA 8.5k <C> AVG <R> <C> 0 <C> 1.82 <C> 83.1|0.25 <C> 90.5|0.10 <C> 91.1|0.12 <C> 92.1|0.06 <C> 79.1|0.26 <C> 87.2|15.7 <R> <C> 10 <C> 1.82 <C> 83.3|0.21 <C> 90.4|0.10 <C> 91.0|0.12 <C> 91.6|0.07 <C> 79.4|0.30 <C> 87.2|16.0 <R> <C> 20 <C> 1.83 <C> 83.3|0.24 <C> 90.5|0.11 <C> 91.1|0.11 <C> 91.6|0.05 <C> 79.1|0.30 <C> 87.1|16.0 <R> <C> 30 <C> 1.86 <C> 83.3|0.23 <C> 90.2|0.12 <C> 90.7|0.12 <C> 91.9|0.06 <C> 79.5|0.31 <C> 87.1|16.9 <R> <C> 40 <C> 1.93 <C> 83.0|0.25 <C> 90.1|0.12 <C> 90.4|0.12 <C> 91.5|0.06 <C> 78.4|0.23 <C> 86.7|15.6 <R> <C> 50 <C> 2.03 <C> 82.6|0.27 <C> 89.8|0.13 <C> 90.2|0.13 <C> 90.9|0.07 <C> 77.4|0.30 <C> 86.2|18.0 <R> <C> 60 <C> 2.25 <C> 81.8|0.32 <C> 89.4|0.16 <C> 89.3|0.16 <C> 91.4|0.07 <C> 75.9|0.44 <C> 85.6|23.0 <R> <C> 70 <C> 2.62 <C> 79.5|0.40 <C> 88.6|0.18 <C> 88.4|0.21 <C> 90.1|0.10 <C> 72.7|0.47 <C> 83.9|27.1 <R> <C> 80 <C> 3.44 <C> 75.9|0.49 <C> 86.9|0.24 <C> 85.3|0.29 <C> 88.1|0.12 <C> 69.1|0.61 <C> 81.1|34.8 <R> <C> 90 <C> 5.83 <C> 64.8|0.76 <C> 81.1|0.36 <C> 71.7|0.52 <C> 80.3|0.25 <C> 69.1|0.61 <C> 73.4|49.8 <R> <C> [EMPTY] <C> [EMPTY] <C> Information Deletion <C> Information Deletion <C> Information Deletion <C> Information Deletion <C> Information Deletion <C> [EMPTY] <R> <C> 0 <C> 1.82 <C> 83.0|0.20 <C> 90.6|0.06 <C> 90.0|0.10 <C> 92.1|0.03 <C> 80.6|0.18 <C> 87.3|11.6 <R> <C> 10 <C> 1.82 <C> 82.8|0.01 <C> 90.5|0.05 <C> 90.5|0.09 <C> 92.2|0.05 <C> 80.8|0.16 <C> 87.4|07.2 <R> <C> 20 <C> 1.83 <C> 82.9|0.01 <C> 90.5|0.05 <C> 90.5|0.09 <C> 91.5|0.05 <C> 80.3|0.16 <C> 87.2|07.3 <R> <C> 30 <C> 1.86 <C> 82.3|0.01 <C> 90.6|0.04 <C> 90.5|0.10 <C> 90.8|0.05 <C> 80.0|0.18 <C> 86.9|07.7 <R> <C> 40 <C> 1.93 <C> 82.2|0.19 <C> 90.5|0.05 <C> 90.1|0.10 <C> 92.0|0.05 <C> 79.0|0.17 <C> 86.7|11.1 <R> <C> 50 <C> 2.03 <C> 82.5|0.19 <C> 90.3|0.05 <C> 90.2|0.10 <C> 91.2|0.05 <C> 77.9|0.19 <C> 86.4|11.6 <R> <C> 60 <C> 2.25 <C> 81.9|0.20 <C> 90.1|0.05 <C> 89.5|0.10 <C> 90.8|0.05 <C> 76.4|0.23 <C> 85.7|12.6 <R> <C> 70 <C> 2.62 <C> 80.8|0.01 <C> 90.2|0.01 <C> 88.7|0.10 <C> 90.3|0.06 <C> 74.4|0.28 <C> 84.9|09.3 <R> <C> 80 <C> 3.44 <C> 78.6|0.01 <C> 89.3|0.02 <C> 86.0|0.02 <C> 88.8|0.07 <C> 70.0|0.45 <C> 82.5|11.5 <R> <C> 90 <C> 5.83 <C> 72.9|0.01 <C> 87.5|0.02 <C> 76.8|0.06 <C> 83.0|0.09 <C> 69.1|0.61 <C> 77.9|15.7 <R> <C> [EMPTY] <C> [EMPTY] <C> Pruned after Downstream Fine-tuning <C> Pruned after Downstream Fine-tuning <C> Pruned after Downstream Fine-tuning <C> Pruned after Downstream Fine-tuning <C> Pruned after Downstream Fine-tuning <C> [EMPTY] <R> <C> 0 <C> - <C> 82.6|0.15 <C> 90.6|0.06 <C> 90.1|0.10 <C> 92.1|0.04 <C> 78.7|0.25 <C> 86.8|12.0 <R> <C> 10 <C> - <C> 82.9|0.19 <C> 90.6|0.06 <C> 90.3|0.10 <C> 91.6|0.05 <C> 79.0|0.11 <C> 86.9|10.3 <R> <C> 20 <C> - <C> 82.7|0.15 <C> 90.6|0.07 <C> 90.2|0.07 <C> 92.0|0.04 <C> 79.0|0.22 <C> 86.9|10.7 <R> <C> 30 <C> - <C> 82.7|0.23 <C> 90.4|0.07 <C> 89.7|0.07 <C> 91.6|0.04 <C> 78.5|0.23 <C> 86.6|12.8 <R> <C> 40 <C> - <C> 82.7|0.25 <C> 90.5|0.11 <C> 89.9|0.12 <C> 91.7|0.05 <C> 78.8|0.17 <C> 86.7|13.9 <R> <C> 50 <C> - <C> 82.6|0.19 <C> 90.3|0.08 <C> 89.7|0.11 <C> 90.8|0.06 <C> 78.0|0.22 <C> 86.3|13.0 <R> <C> 60 <C> - <C> 81.8|0.22 <C> 90.2|0.10 <C> 89.3|0.12 <C> 90.6|0.06 <C> 76.1|0.31 <C> 85.6|16.4 <R> <C> 70 <C> - <C> 80.5|0.30 <C> 89.4|0.14 <C> 86.2|0.19 <C> 88.2|0.07 <C> 69.5|0.58 <C> 82.7|25.8 <R> <C> 80 <C> - <C> 73.7|0.53 <C> 87.8|0.12 <C> 80.4|0.21 <C> 86.4|0.07 <C> 69.1|0.59 <C> 79.5|30.5 <R> <C> 90 <C> - <C> 58.7|0.86 <C> 82.5|0.26 <C> 65.2|0.52 <C> 81.5|0.16 <C> 69.1|0.61 <C> 71.4|47.9 <R> <C> [EMPTY] <C> [EMPTY] <C> Random Pruning <C> Random Pruning <C> Random Pruning <C> Random Pruning <C> Random Pruning <C> [EMPTY] <R> <C> 0 <C> 1.82 <C> 83.3|0.26 <C> 90.5|0.10 <C> 90.6|0.15 <C> 92.4|0.07 <C> 78.7|0.18 <C> 87.1|15.3 <R> <C> 10 <C> 2.09 <C> 82.0|0.27 <C> 90.1|0.12 <C> 90.3|0.13 <C> 92.3|0.05 <C> 77.0|0.32 <C> 86.3|18.0 <R> <C> 20 <C> 2.46 <C> 80.6|0.32 <C> 89.8|0.12 <C> 88.5|0.14 <C> 91.1|0.07 <C> 73.5|0.39 <C> 84.7|20.8 <R> <C> 30 <C> 2.98 <C> 79.1|0.36 <C> 89.2|0.14 <C> 86.9|0.23 <C> 89.3|0.10 <C> 71.8|0.47 <C> 83.3|25.9 <R> <C> 40 <C> 3.76 <C> 75.4|0.45 <C> 88.2|0.16 <C> 84.5|0.23 <C> 88.6|0.09 <C> 69.3|0.57 <C> 81.2|30.3 <R> <C> 50 <C> 4.73 <C> 71.6|0.60 <C> 86.6|0.20 <C> 81.5|0.28 <C> 85.0|0.10 <C> 69.1|0.61 <C> 78.8|35.8 <R> <C> 60 <C> 5.63 <C> 70.4|0.60 <C> 85.2|0.24 <C> 71.7|0.45 <C> 81.5|0.21 <C> 69.1|0.61 <C> 75.6|42.3 <R> <C> 70 <C> 6.22 <C> 64.1|0.76 <C> 81.4|0.34 <C> 63.0|0.62 <C> 80.6|0.20 <C> 69.1|0.61 <C> 71.6|50.3 <R> <C> 80 <C> 6.87 <C> 58.8|0.84 <C> 76.6|0.46 <C> 61.1|0.64 <C> 80.6|0.23 <C> 69.1|0.61 <C> 69.3|55.6 <R> <C> 90 <C> 7.37 <C> 49.8|0.98 <C> 74.3|0.51 <C> 60.2|0.65 <C> 75.1|0.33 <C> 69.1|0.61 <C> 65.7|61.4 <CAP> Table 1: Pre-training development losses and GLUE task development accuracies for various levels of pruning. Each development accuracy is accompanied on its right by the achieved training loss, evaluated on the entire training set. Averages are summarized in Figure 1. Pre-training losses are omitted for models pruned after downstream fine-tuning because it is not clear how to measure their performance on the pre-training task in a fair way.
<R> <C> [BOLD] token bin <C> [BOLD] r <C> [BOLD] r2 <C> [BOLD] p <R> <C> 1-3 <C> 0.001 <C> 0.000 <C> 9.92×10−01 <R> <C> 4-6 <C> -0.243 <C> 0.059 <C> 8.32×10−02 <R> <C> 7-9 <C> -0.327 <C> 0.107 <C> 1.79×10−02 <R> <C> 10-12 <C> -0.386 <C> 0.149 <C> 4.70×10−03 <R> <C> 13-15 <C> -0.344 <C> 0.118 <C> 1.25×10−02 <R> <C> 16-18 <C> -0.364 <C> 0.133 <C> 7.90×10−03 <R> <C> 19-21 <C> -0.344 <C> 0.118 <C> 1.26×10−02 <R> <C> 22-24 <C> -0.350 <C> 0.122 <C> 1.11×10−02 <R> <C> 25-27 <C> -0.349 <C> 0.122 <C> 1.11×10−02 <R> <C> 28-33 <C> -0.347 <C> 0.121 <C> 1.17×10−02 <R> <C> 34-39 <C> -0.354 <C> 0.125 <C> 1.01×10−02 <R> <C> 40-99 <C> -0.298 <C> 0.089 <C> 3.97×10−02 <CAP> Table 2: Full results for each sentence-length bin for non-projective algorirthms where token bin is the sentence length range, r is the Pearson coefficient of the correlation between δUAS and the EMD of each algorithm (e.g. as shown in Figure 6), r2 is the squared Pearson coefficient which gives an indication of how much variation in the data this correlation accounts for, and finally p is the p-value for a given correlation.
<R> <C> [BOLD] Model / Features <C> Sub-Task A –  [ITALIC] 8 labels  [BOLD] F1 <C> Sub-Task A –  [ITALIC] 8 labels  [BOLD] Prec. <C> Sub-Task A –  [ITALIC] 8 labels  [BOLD] Recall <C> Sub-Task B –  [ITALIC] 343 labels  [BOLD] F1 <C> Sub-Task B –  [ITALIC] 343 labels  [BOLD] Prec. <C> Sub-Task B –  [ITALIC] 343 labels  [BOLD] Recall <R> <C> (1) BERT-German + Metadata + Author <C> 87.20 <C> 88.76 <C> 85.70 <C> 64.70 <C> 83.78 <C> 52.70 <R> <C> (2) BERT-German + Metadata <C> 86.90 <C> 89.65 <C> 84.30 <C> 63.96 <C> 83.94 <C> 51.67 <R> <C> (3) BERT-German + Author <C> 86.84 <C> 89.02 <C> 84.75 <C> 64.41 <C> 82.02 <C> 53.03 <R> <C> (4) BERT-German <C> 86.65 <C> 89.65 <C> 83.86 <C> 60.51 <C> 83.44 <C> 47.47 <R> <C> (5) BERT-Base-Multilingual-Cased <C> 83.94 <C> 86.31 <C> 81.70 <C> 54.08 <C> 82.63 <C> 40.19 <R> <C> (6) Author <C> 61.99 <C> 75.59 <C> 52.54 <C> 32.13 <C> 72.39 <C> 20.65 <R> <C> (7) Baseline <C> 77.00 <C> 79.00 <C> 74.00 <C> 45.00 <C> 67.00 <C> 34.00 <R> <C> Results of best model (1) on test set <C> 88.00 <C> 85.00 <C> 86.00 <C> 78.00 <C> 52.00 <C> 62.00 <CAP> Table 2: Evaluation scores (micro avg.) on validation set with respect to the features used for classification. The model with BERT-German, metadata and author embeddings yields the highest F1-scores on both tasks and was accordingly submitted to the GermEval 2019 competition. The scores in the last row are the result on the test set as reported by Remus et al., 2019.
<R> <C> [EMPTY] <C> [BOLD] Label <C> [BOLD] Sentences <C> [BOLD] Paragraphs <C> [BOLD] Words <C> [BOLD] Characters <R> <C> [BOLD] Corpus <C> [BOLD] LISSS <C> 230 <C> 26 <C> 4 577 <C> 25 643 <R> <C> Anger <C> A <C> 43 <C> 7 <C> 961 <C> 5 428 <R> <C> Love <C> L <C> 43 <C> 8 <C> 941 <C> 5 166 <R> <C> Fear <C> F <C> 46 <C> 5 <C> 940 <C> 5 173 <R> <C> Happiness <C> H <C> 46 <C> 2 <C> 889 <C> 5 143 <R> <C> Sadness/Pain <C> S <C> 41 <C> 7 <C> 846 <C> 4 728 <CAP> Table 1: Corpus LISSS of literary sentences classified in 5 emotions.
<R> <C> Model <C> 28k <C> 59k <C> 120k <R> <C> LSTM () <C> 57.9 <C> 62.5 <C> 65.9 <R> <C> LSTM-AE () <C> 59.9 <C> 64.6 <C> 68.5 <R> <C> LSTM-ADAE () <C> 62.5 <C> 66.8 <C> 70.9 <R> <C> CNN (random) <C> 58.7 <C> 62.7 <C> 65.6 <R> <C> CNN (Glove) <C> 60.3 <C> 64.1 <C> 66.8 <R> <C> DeConv-AE <C> 62.1 <C> 65.5 <C> 68.7 <R> <C> LSTM-LVM <C> 64.7 <C> 67.5 <C> 71.1 <R> <C> DeConv-LVM <C> [BOLD] 67.2 <C> [BOLD] 69.3 <C> [BOLD] 72.2 <CAP> Table 3: Semi-supervised recognizing textual entailment accuracy on SNLI dataset, in percentage. For direct comparison with [Kim et al.2017]. The number of labeled examples is set as 28k, 59k or 120k.
<R> <C> [BOLD] Section <C> [BOLD] F1 <R> <C> First 1K sentences <C> [BOLD] 0.720 <R> <C> First 5K sentences <C> 0.685 <R> <C> First 10K sentences <C> 0.698 <R> <C> Last 1K sentences <C> 0.672 <R> <C> Full Book <C> 0.708 <CAP> Table 5: Weighted F1 score on the test set obtained when using different sections from each book as input.
<R> <C> system <C> P <C> R <C> acc. <R> <C> On  [BOLD] uncorrected SICK <C> On  [BOLD] uncorrected SICK <C> On  [BOLD] uncorrected SICK <C> On  [BOLD] uncorrected SICK <R> <C> majority baseline <C> – <C> – <C> 56.36 <R> <C> hypothesis-only baseline <C> – <C> – <C> 56.87 <R> <C> Poliak et al. ( 2018 ) <C> – <C> – <C> 56.87 <R> <C> MonaLog (this work) <C> MonaLog (this work) <C> MonaLog (this work) <C> MonaLog (this work) <R> <C> MonaLog + all transformations <C> 83.75 <C> 70.66 <C> 77.19 <R> <C> Hybrid: MonaLog + BERT <C> 83.09 <C> 85.46 <C> 85.38 <R> <C> ML/DL-based systems <C> ML/DL-based systems <C> ML/DL-based systems <C> ML/DL-based systems <R> <C> BERT (base, uncased) <C> 86.81 <C> 85.37 <C> 86.74 <R> <C> Yin and Schütze ( 2017 ) <C> – <C> – <C> [BOLD] 87.1 <R> <C> Beltagy et al. ( 2016 ) <C> – <C> – <C> 85.1 <R> <C> Logic-based systems <C> Logic-based systems <C> Logic-based systems <C> Logic-based systems <R> <C> Bjerva et al. ( 2014 ) <C> 93.6 <C> 60.6 <C> 81.6 <R> <C> Abzianidze  <C> 97.95 <C> 58.11 <C> 81.35 <R> <C> Martínez-Gómez et al. ( 2017 ) <C> 97.04 <C> 63.64 <C> 83.13 <R> <C> Yanaka et al. ( 2018 ) <C> 84.2 <C> 77.3 <C> 84.3 <R> <C> On  [BOLD] corrected SICK <C> On  [BOLD] corrected SICK <C> On  [BOLD] corrected SICK <C> On  [BOLD] corrected SICK <R> <C> MonaLog + existential trans. <C> 89.43 <C> 71.53 <C> 79.11 <R> <C> MonaLog + pass2act <C> 89.42 <C> 72.18 <C> 80.25 <R> <C> MonaLog + all transformations <C> 89.91 <C> 74.23 <C> 81.66 <R> <C> Hybrid: MonaLog + BERT <C> 85.65 <C> 87.33 <C> [BOLD] 85.95 <R> <C> BERT (base, uncased) <C> 84.62 <C> 84.27 <C> 85.00 <CAP> Table 3: Performance on the SICK test set, original SICK above and corrected SICK below. P / R for MonaLog averaged across three labels. Results involving BERT are averaged across six runs; same for later experiments.
<R> <C> [EMPTY] <C> E P <C> E R <C> C P <C> C R <C> N P <C> N R <R> <C> uncorr. SICK <C> 97.75 <C> 46.74 <C> 80.06 <C> 70.24 <C> 73.43 <C> 94.99 <R> <C> corr. SICK <C> 98.50 <C> 50.46 <C> 95.02 <C> 73.60 <C> 76.22 <C> 98.63 <CAP> Table 4: Results of MonaLog per relation. C: contradiction; E: entailment; N: neutral.
<R> <C> [BOLD] Attack Type <C> [BOLD] ACD <C> [BOLD] Agglomerative Occlusion <R> <C> Saliency (Papernot et al.,  2016 ) <C> 0.762 <C> 0.259 <R> <C> Gradient attack <C> 0.662 <C> 0.196 <R> <C> FGSM (Goodfellow et al.,  2014 ) <C> 0.590 <C> 0.131 <R> <C> Boundary (Brendel et al.,  2017 ) <C> 0.684 <C> 0.155 <R> <C> DeepFool (Moosavi Dezfooli et al.,  2016 ) <C> 0.694 <C> 0.202 <CAP> Table 2: Correlation between pixel ranks for different adversarial attacks. ACD achieves consistently high correlation across different attack types, indicating that ACD hierarchies are largely robust to adversarial attacks. Using occlusion in place of CD produces substantially less stable hierarchies.
<R> <C> 2* [BOLD] Methods (lr)2-3 (lr)4-5 <C> CIFAR-10 (WRN-28- [ITALIC] k) 4K,  [ITALIC] k=2 <C> CIFAR-10 (WRN-28- [ITALIC] k) Full,  [ITALIC] k=10 <C> ImageNet (ResNet-50) 10% <C> ImageNet (ResNet-50) Full <R> <C> Uniform <C> 82.60±0.17 <C> 95.55±0.15 <C> 56.36/79.45 <C> 76.51/93.20 <R> <C> SPCL <C> 81.09±0.22 <C> 93.66±0.12 <C> - <C> - <R> <C> BatchWeight <C> 79.61±0.50 <C> 94.11±0.18 <C> - <C> - <R> <C> MentorNet <C> 83.11±0.62 <C> 94.92±0.34 <C> - <C> - <R> <C> DAT <C> 83.63± 0.29 <C> 96.31± 0.13 <C> [BOLD] 56.81/ [BOLD] 79.51 <C> [BOLD] 77.23/ [BOLD] 93.57 <R> <C> retrained DAT <C> [BOLD] 85.56±0.20 <C> [BOLD] 97.91±0.12 <C> - <C> - <CAP> Table 1: Results for image classification accuracy (left) and multilingual MT BLEU (right). For MT, the statistical significance is indicated with ∗ (p < 0.005) and † (p < 0.0001). DAT outperforms the best baseline in all settings. For both image classification and NMT, DAT performs better than other intelligent data selection methods.
<R> <C> [BOLD] Methods <C> [BOLD] aze <C> [BOLD] bel <C> [BOLD] glg <C> [BOLD] slk <R> <C> Uniform <C> 10.31 <C> 17.21 <C> 26.05 <C> 27.44 <R> <C> SPCL <C> 9.07 <C> 16.99 <C> 23.64 <C> 21.44 <R> <C> Related <C> 10.34 <C> 15.31 <C> 27.41 <C> 25.92 <R> <C> TCS <C> 11.18 <C> 16.97 <C> 27.28 <C> 27.72 <R> <C> DAT <C> 10.74 <C> 17.24 <C> 27.32 <C> [BOLD] 28.20∗ <R> <C> TCS+DAT <C> [BOLD] 11.84∗ <C> [BOLD] 17.74† <C> [BOLD] 27.78 <C> 27.74 <CAP> Table 1: Results for image classification accuracy (left) and multilingual MT BLEU (right). For MT, the statistical significance is indicated with ∗ (p < 0.005) and † (p < 0.0001). DAT outperforms the best baseline in all settings. For both image classification and NMT, DAT performs better than other intelligent data selection methods.
<R> <C> Methods <C> Precision <C> Recall <C> F1 <R> <C> Bi-directional LSTM <C> 72.4 <C> 70.5 <C> 71.4 <R> <C> Word-CNN <C> 72.8 <C> 70.3 <C> 71.5 <R> <C> 3-layer Transformer <C> 73.3 <C> 72.6 <C> 73.1 <R> <C> 12-layer BERT <C> [BOLD] 77.5 <C> [BOLD] 77.4 <C> [BOLD] 77.3 <CAP> Table 2: Accuracy of Dialog Act Prediction
<R> <C> Dialog-Act <C> Methods <C> Delexicalized BLEU <C> Delexicalized Inform <C> Delexicalized Request <C> Delexicalized Entity F1 <C> Restored BLEU <R> <C> None <C> LSTM Budzianowski et al. ( 2018 ) <C> 18.8 <C> 71.2 <C> 60.2 <C> 54.8 <C> 15.1 <R> <C> None <C> 3-layer Transformer Vaswani et al. ( 2017 ) <C> 19.1 <C> 71.1 <C> 59.9 <C> 55.1 <C> 15.2 <R> <C> Tree Act <C> SC-LSTM Wen et al. ( 2015 ) <C> 20.5 <C> 74.5 <C> 62.5 <C> 57.7 <C> 16.6 <R> <C> Tree Act <C> 3-layer Transformer-out <C> 19.9 <C> 74.4 <C> 61.1 <C> 57.4 <C> 16.0 <R> <C> Tree Act <C> 3-layer Transformer-in <C> 20.2 <C> 73.8 <C> 62.1 <C> 57.3 <C> 16.2 <R> <C> Graph Act (Predicted) <C> 3-layer Transformer-out <C> 22.5 <C> 80.8 <C> 64.8 <C> 64.2 <C> 19.3 <R> <C> Graph Act (Predicted) <C> 3-layer Transformer-in <C> 22.7 <C> 80.4 <C> 65.1 <C> 64.6 <C> 19.9 <R> <C> Graph Act (Predicted) <C> Straight DSA (44 heads) + 2 x SA <C> 22.6 <C> 80.3 <C> 67.1 <C> 65.0 <C> 20.0 <R> <C> Graph Act (Predicted) <C> 2-layer HDSA (7/27 heads) + SA <C> 23.2 <C> [BOLD] 82.9 <C> [BOLD] 69.1 <C> 65.1 <C> 20.3 <R> <C> Graph Act (Predicted) <C> 3-layer HDSA (10/7/27 heads) <C> [BOLD] 23.6 <C> [BOLD] 82.9 <C> 68.9 <C> [BOLD] 65.7 <C> [BOLD] 20.6 <R> <C> Graph Act (Groundtruth) <C> 3-layer Transformer-in <C> 29.1 <C> 85.5 <C> 72.6 <C> 83.8 <C> 25.1 <R> <C> Graph Act (Groundtruth) <C> Straight DSA (44 heads) + 2 x SA <C> 29.6 <C> 86.4 <C> 75.6 <C> 84.1 <C> 25.5 <R> <C> Graph Act (Groundtruth) <C> 3-layer HDSA (10/7/27 heads) <C> [BOLD] 30.4 <C> [BOLD] 87.9 <C> [BOLD] 78.0 <C> [BOLD] 86.2 <C> [BOLD] 26.2 <CAP> Table 3: Empirical Results on MultiWOZ Response Generation, we experiment with three forms of dialog act, namely none, one-hot and hierarchical.
<R> <C> Language <C> IMS-CUB 1 <C> IMS-CUB 2-S <C> IMS-CUB 2-V <R> <C> Basque <C> [BOLD] 25.00 <C> 18.75 <C> 12.50 <R> <C> Bulgarian <C> 97.73 <C> [BOLD] 98.19 <C> 97.28 <R> <C> English <C> 96.39 <C> [BOLD] 98.80 <C> [BOLD] 98.80 <R> <C> Finnish <C> [BOLD] 99.04 <C> 98.47 <C> 98.85 <R> <C> German <C> 91.49 <C> [BOLD] 93.39 <C> 91.99 <R> <C> Kannada <C> 91.47 <C> [BOLD] 92.89 <C> 91.00 <R> <C> Maltese <C> 79.17 <C> 79.17 <C> [BOLD] 85.42 <R> <C> Navajo <C> 0.00 <C> 75.00 <C> [BOLD] 100.00 <R> <C> Persian <C> 95.56 <C> 94.81 <C> [BOLD] 95.56 <R> <C> Portuguese <C> 93.81 <C> [BOLD] 93.87 <C> 93.74 <R> <C> Russian <C> 92.15 <C> 93.02 <C> [BOLD] 93.19 <R> <C> Spanish <C> 92.91 <C> 92.71 <C> [BOLD] 93.52 <R> <C> Swedish <C> 93.48 <C> [BOLD] 93.69 <C> 93.27 <R> <C> Turkish <C> 93.90 <C> 95.30 <C> [BOLD] 95.68 <CAP> Table 2: Accuracy of our morphological inflection components on the development sets produced by the first three steps in our pipeline. We list both development and test languages.
<R> <C> [EMPTY] <C> CRAE <C> CDL <C> CTR <C> PMF <R> <C> [ITALIC] CiteULike <C> [BOLD] 46.60 <C> 21.14 <C> 31.47 <C> 17.85 <R> <C> [ITALIC] Netflix <C> [BOLD] 48.69 <C> 6.90 <C> 17.17 <C> 11.74 <CAP> Table 3: BLEU score for two datasets
<R> <C> [BOLD] Train set size <C> [BOLD] 250 <C> [BOLD] 500 <C> [BOLD] 1,000 <C> [BOLD] 5,000 <C> [BOLD] 10,000 <R> <C> [BOLD] Plain <C> 0.533 <C> 0.686 <C> 0.859 <C> 0.931 <C> 0.98 <R> <C> [BOLD] Online <C> 0.966 <C> 0.974 <C> 0.968 <C> 0.99.5 <C> 1.0 <R> <C> [BOLD] Difference <C> 81% <C> 41% <C> 13% <C> 7% <C> 2% <CAP> Table 2: The LWP task - The accuracy on test data using plain training and online training on different amounts of train data.
<R> <C> Languages <C> Accuracy in binary detection <C> [ITALIC] h <C> Pearson correlation <C> Kendall- [ITALIC] τ <C> Kendall- [ITALIC] τ-p-value <R> <C> English <C> 70.3 <C> 0.4 <C> 0.314 <C> 0.292 <C> 0.034 <R> <C> German <C> 75.0 <C> 0.5 <C> 0.432 <C> 0.315 <C> 0.009 <R> <C> Swedish <C> 77.4 <C> 0.4 <C> 0.316 <C> 0.383 <C> 0.0115 <R> <C> Latin <C> 60.0 <C> 0.15 <C> 0.162 <C> 0.161 <C> 0.222 <R> <C> Average <C> 70.7 <C> [EMPTY] <C> 0.306 <C> [EMPTY] <C> [EMPTY] <CAP> Table 2: The summary of results for the detection and quantification of lexical-semantics changes in English, German, Swedish, and Latin in SemEval 2020.
<R> <C> [BOLD] Method <C> [BOLD] Mean Accuracy <R> <C> [ITALIC] AvgW2 [ITALIC] V <C> [BOLD] 0.886 <R> <C> [ITALIC] D2 [ITALIC] V <C> 0.623 <R> <C> [ITALIC] AvgGloVe <C> 0.684 <R> <C> [ITALIC] IDFWVw2 [ITALIC] v <C> 0.847 <CAP> Table 1: Impacts of the choice of word vector approaches on the classification accuracy.
<R> <C> Setup <C> Trans <C> A-Delib <C> C-Delib <R> <C> Baseline <C> [BOLD] 39.8 <C> [BOLD] 37.6 <C> 36.4 <R> <C> Cond-AvgPool <C> 39.7 <C> 36.0 † <C> 36.2 <R> <C> Attn-AvgPool <C> 39.7 <C> [BOLD] 37.6 <C> [BOLD] 37.4 † <R> <C> Attn-Emb <C> 39.7 <C> 37.0 † <C> 37.3 † <R> <C> Attn-Conv <C> [BOLD] 39.8 <C> 37.2 <C> 37.0 <CAP> Table 1: BLEU scores for the test set: bold highlights our best results. † indicates a system is significantly different from its text-only counterpart (p-value ≤ 0.05).
<R> <C> [BOLD] Team <C> isomorphic <C> non-isomorphic <R> <C> SFS <C> 23.1 <C> 17.9 <R> <C> IIITH <C> 23.1 <C> 25.8 <R> <C> MELODI-Primary <C> 13.0 <C> 54.8 <R> <C> MELODI-Contrast <C> 13.6 <C> 53.6 <R> <C> [ITALIC] Naive Baseline <C> [ITALIC] 13.8 <C> [ITALIC] 40.6 <CAP> Table 2: Results for the participating systems; the baseline outputs the same paraphrases for all compounds.
<R> <C> [EMPTY] <C> [BOLD] RACE-M <C> [BOLD] RACE-H <C> [BOLD] RACE <R> <C> Random <C> 24.6 <C> 25.0 <C> 24.9 <R> <C> Sliding Window <C> 37.3 <C> 30.4 <C> 32.2 <R> <C> Stanford AR <C> 44.2 <C> 43.0 <C> 43.3 <R> <C> GA <C> 43.7 <C> 44.2 <C> 44.1 <R> <C> ElimiNet <C> - <C> - <C> 44.7 <R> <C> HAF <C> 45.3 <C> 47.9 <C> 47.2 <R> <C> MUSIC <C> 51.5 <C> 45.7 <C> 47.4 <R> <C> Hier-Co-Matching <C> [BOLD] 55.8∗ <C> [BOLD] 48.2∗ <C> [BOLD] 50.4∗ <R> <C> - Hier-Aggregation <C> 54.2 <C> 46.2 <C> 48.5 <R> <C> - Co-Matching <C> 50.7 <C> 45.6 <C> 46.4 <R> <C> Turkers <C> 85.1 <C> 69.4 <C> 73.3 <R> <C> Ceiling <C> 95.4 <C> 94.2 <C> 94.5 <CAP> Table 2: Experiment Results. ∗ means it’s significant to the models ablating either the hierarchical aggregation or co-matching state.
<R> <C> Model <C> Task Specific <C> SemEval2013 <C> Turney2012(5) <C> Turney2012(10) <R> <C> SUM <C> False <C> 65.46 <C> 39.58 <C> 19.79 <R> <C> RAE <C> False <C> 51.75 <C> 22.99 <C> 14.81 <R> <C> FCT(LM) <C> False <C> 67.22 <C> [BOLD] 42.59 <C> [BOLD] 27.55 <R> <C> GRU(PPDB) <C> False <C> [BOLD] 71.29 <C> 41.44 <C> 26.37 <R> <C> Tree-RNN <C> True <C> 71.50 <C> 40.95 <C> 27.20 <R> <C> FCT <C> True <C> 68.84 <C> 41.90 <C> 33.80 <R> <C> GRU <C> True <C> [BOLD] 73.44 <C> [BOLD] 48.88 <C> [BOLD] 39.23 <CAP> Table 1: Performances of our models and baselines on SemEval2013, Turney2012(5) and Turney2012(10). Models are split into task-specific ones and task-unspecific ones for comparison.
<R> <C> Evaluation set <C> Prec. <C> Rec. <C> F1 <R> <C> Drugs (DDI-2013) <C> 0.90 <C> 0.87 <C> 0.88 <R> <C> Drugs (NLM-DailyMed) <C> 0.83 <C> 0.85 <C> 0.84 <R> <C> Supplements-500 <C> 0.82 <C> 0.58 <C> 0.68 <CAP> Table 2: The RoBERTa-DDI model (trained on drug-drug interaction labels) is evaluated on two DDI evaluation sets (first two rows) and our supplement interaction evaluation set (last row).
<R> <C> Test dataset <C> Num. pairwise instances <C> RoBERTa-DDI (Trained on DDI-2013 and NLM-DailyMed) <C> RoBERTa-DDI (Trained on DDI-2013 only) <R> <C> DDI-2013 (All) <C> 5688 <C> 0.88 <C> 0.89 <R> <C> DDI-2013 (DrugBank) <C> 5251 <C> 0.89 <C> 0.90 <R> <C> DDI-2013 (Medline) <C> 437 <C> 0.73 <C> 0.77 <R> <C> NLM-DailyMed <C> 927 <C> 0.84 <C> 0.70 <R> <C> All <C> 6615 <C> 0.87 <C> 0.85 <CAP> Table 3: F1-scores of RoBERTa-DDI trained using different training data. Test data contains all pairwise combinations of entities in test sentences.
<R> <C> Test Set <C> F1 <C> F1 +truecaser <C> \delta <R> <C> All (micro) <C> 83.4 <C> [BOLD] 84.4 <C> 1.0 <R> <C> bc <C> 81.8 <C> [BOLD] 83.7 <C> 1.9 <R> <C> bn <C> 87.5 <C> [BOLD] 88.6 <C> 1.1 <R> <C> mz <C> 79.5 <C> [BOLD] 81.0 <C> 1.5 <R> <C> nw <C> 85.8 <C> [BOLD] 86.6 <C> 0.8 <R> <C> tc <C> 69.6 <C> [BOLD] 69.9 <C> 0.3 <R> <C> wb <C> 76.1 <C> [BOLD] 76.6 <C> 0.5 <CAP> Table 7: Scores on uncased Ontonotes test set from an uncased model, broken down by sub-genre. The leftmost column is a standard BiLSTM-CRF, the middle column is our proposed approach, and the rightmost column is the difference. In all cases, the truecaser outperforms the original model, with the greatest improvements in bc (broadcast news) and mz (magazine).
<R> <C> TC Train <C> NER F1 <C> Char F1 <R> <C> Fixed pretrained <C> 46.9 <C> 30.6 <R> <C> Fine-tuned pretrained <C> 46.3 <C> [BOLD] 52.3 <R> <C> From scratch <C> [BOLD] 47.7 <C> 36.2 <CAP> Table 8: Results on WNUT17 with BERT uncased, trained and tested on uncased (U) data, varying the Truecaser (TC) training paradigm.
<R> <C> Labeling rate <C> | [ITALIC] U| <C> Method <C> Dev F1 <C> Test F1 <C> Test EM <R> <C> 0.1 <C> 50K <C> SL <C> 0.4262 <C> 0.3815 <C> 0.2492 <R> <C> 0.1 <C> 50K <C> Context <C> 0.5046 <C> 0.4515 <C> 0.2966 <R> <C> 0.1 <C> 50K <C> Context + domain <C> 0.5139 <C> 0.4575 <C> 0.3036 <R> <C> 0.1 <C> 50K <C> Gen <C> 0.5049 <C> 0.4553 <C> 0.3018 <R> <C> 0.1 <C> 50K <C> Gen + GAN <C> 0.4897 <C> 0.4373 <C> 0.2885 <R> <C> 0.1 <C> 50K <C> Gen + dual <C> 0.5036 <C> 0.4555 <C> 0.3005 <R> <C> 0.1 <C> 50K <C> Gen + domain <C> 0.5234 <C> 0.4703 <C> 0.3145 <R> <C> 0.1 <C> 50K <C> Gen + domain + adv <C> [BOLD] 0.5313 <C> [BOLD] 0.4802 <C> [BOLD] 0.3218 <R> <C> 0.2 <C> 50K <C> SL <C> 0.5134 <C> 0.4674 <C> 0.3163 <R> <C> 0.2 <C> 50K <C> Context <C> 0.5652 <C> 0.5132 <C> 0.3573 <R> <C> 0.2 <C> 50K <C> Context + domain <C> 0.5672 <C> 0.5200 <C> 0.3581 <R> <C> 0.2 <C> 50K <C> Gen <C> 0.5643 <C> 0.5159 <C> 0.3618 <R> <C> 0.2 <C> 50K <C> Gen + GAN <C> 0.5525 <C> 0.5037 <C> 0.3470 <R> <C> 0.2 <C> 50K <C> Gen + dual <C> 0.5720 <C> 0.5192 <C> 0.3612 <R> <C> 0.2 <C> 50K <C> Gen + domain <C> 0.5749 <C> 0.5216 <C> 0.3658 <R> <C> 0.2 <C> 50K <C> Gen + domain + adv <C> [BOLD] 0.5867 <C> [BOLD] 0.5394 <C> [BOLD] 0.3781 <R> <C> 0.5 <C> 50K <C> SL <C> 0.6280 <C> 0.5722 <C> 0.4187 <R> <C> 0.5 <C> 50K <C> Context <C> 0.6300 <C> 0.5740 <C> 0.4195 <R> <C> 0.5 <C> 50K <C> Context + domain <C> 0.6307 <C> 0.5791 <C> 0.4237 <R> <C> 0.5 <C> 50K <C> Gen <C> 0.6237 <C> 0.5717 <C> 0.4155 <R> <C> 0.5 <C> 50K <C> Gen + GAN <C> 0.6110 <C> 0.5590 <C> 0.4044 <R> <C> 0.5 <C> 50K <C> Gen + dual <C> 0.6368 <C> 0.5746 <C> 0.4163 <R> <C> 0.5 <C> 50K <C> Gen + domain <C> [BOLD] 0.6378 <C> 0.5826 <C> 0.4261 <R> <C> 0.5 <C> 50K <C> Gen + domain + adv <C> 0.6375 <C> [BOLD] 0.5831 <C> [BOLD] 0.4267 <R> <C> 0.9 <C> 50K <C> SL <C> [BOLD] 0.6611 <C> 0.6070 <C> 0.4534 <R> <C> 0.9 <C> 50K <C> Context <C> 0.6560 <C> 0.6028 <C> 0.4507 <R> <C> 0.9 <C> 50K <C> Context + domain <C> 0.6553 <C> [BOLD] 0.6105 <C> 0.4557 <R> <C> 0.9 <C> 50K <C> Gen <C> 0.6464 <C> 0.5970 <C> 0.4445 <R> <C> 0.9 <C> 50K <C> Gen + GAN <C> 0.6396 <C> 0.5874 <C> 0.4317 <R> <C> 0.9 <C> 50K <C> Gen + dual <C> 0.6511 <C> 0.5892 <C> 0.4340 <R> <C> 0.9 <C> 50K <C> Gen + domain <C> [BOLD] 0.6611 <C> 0.6102 <C> [BOLD] 0.4573 <R> <C> 0.9 <C> 50K <C> Gen + domain + adv <C> 0.6585 <C> 0.6043 <C> 0.4497 <R> <C> 0.1 <C> 5M <C> SL <C> 0.4262 <C> 0.3815 <C> 0.2492 <R> <C> 0.1 <C> 5M <C> Context <C> 0.5140 <C> 0.4641 <C> 0.3014 <R> <C> 0.1 <C> 5M <C> Context + domain <C> 0.5166 <C> 0.4599 <C> 0.3083 <R> <C> 0.1 <C> 5M <C> Gen <C> 0.5099 <C> 0.4619 <C> 0.3103 <R> <C> 0.1 <C> 5M <C> Gen + domain <C> 0.5301 <C> 0.4703 <C> 0.3227 <R> <C> 0.1 <C> 5M <C> Gen + domain + adv <C> [BOLD] 0.5442 <C> [BOLD] 0.4840 <C> [BOLD] 0.3270 <R> <C> 0.9 <C> 5M <C> SL <C> 0.6611 <C> 0.6070 <C> 0.4534 <R> <C> 0.9 <C> 5M <C> Context <C> 0.6605 <C> 0.6026 <C> 0.4473 <R> <C> 0.9 <C> 5M <C> Context + domain <C> 0.6642 <C> 0.6066 <C> 0.4548 <R> <C> 0.9 <C> 5M <C> Gen <C> 0.6647 <C> 0.6065 <C> [BOLD] 0.4600 <R> <C> 0.9 <C> 5M <C> Gen + domain <C> [BOLD] 0.6726 <C> 0.6092 <C> 0.4599 <R> <C> 0.9 <C> 5M <C> Gen + domain + adv <C> 0.6670 <C> [BOLD] 0.6102 <C> 0.4531 <CAP> Table 2: Performance with various labeling rates, unlabeled data sizes |U|, and methods. “Dev” denotes the development set, and “test” denotes the test set. F1 and EM are two metrics.
<R> <C> BLEU <C> BLEU-1 <C> METEOR <C> chrF <R> <C> 16.48 <C> 56.0 <C> 13.97 <C> 26.84 <CAP> Table 4: Automatic evaluation of the neural networks based on the gold standard
<R> <C> Model <C> Dataset <C> UWA <C> Neutral <C> Joy <C> Sadness <C> Anger <R> <C> SA-BiLSTM <C> [ITALIC] Friends <C> 59.6 <C> 90.1 <C> 68.8 <C> 30.6 <C> 49.1 <R> <C> SA-BiLSTM <C> [ITALIC] EmotionPush <C> 55.0 <C> 94.2 <C> 70.5 <C> 31.0 <C> 24.3 <R> <C> SA-BiLSTM <C> [ITALIC] Average <C> 57.3 <C> 92.1 <C> 69.6 <C> 30.8 <C> 36.7 <CAP> Table 2: Experimental results of Friends and EmotionPush in the test sets.
<R> <C> [BOLD] Previous conversation turns <C> [BOLD] seq2seq+A <C> [BOLD] Nseq2seq+A <R> <C> [ITALIC] N=1 <C> 13.84±0.02 <C> 13.71±0.03 <R> <C> [ITALIC] N=2 <C> 13.49±0.03 <C> 13.40±0.04 <R> <C> [ITALIC] N=3 <C> 13.44±0.05 <C> 13.31±0.03 <R> <C> [ITALIC] N=5 <C> - <C> 13.14±0.03 <R> <C> [ITALIC] N=7 <C> - <C> 13.08±0.03 <CAP> Table 1: Results on OpenSubtitles dataset. Perplexity vs. number of previous conversation turns.
<R> <C> [BOLD] Evaluation Tasks <C> [BOLD] Evaluation Tasks  [BOLD] GPred-Task <C> [BOLD] Evaluation Tasks  [BOLD] GDPred-Task <C> [BOLD] Evaluation Tasks  [BOLD] PLen-Task <C> [BOLD] Evaluation Tasks  [BOLD] SC-Task <R> <C> [BOLD] base-BoW Model <C> 96.8 <C> 79 <C> 34 <C> 39.6 <R> <C> [BOLD] weighted-BoW Model <C> [BOLD] 97.5 <C> [BOLD] 80.05 <C> 33.4 <C> [BOLD] 44.3 <R> <C> [BOLD] base-Seq2seq Model <C> 76.6 <C> 75.8 <C> 70.7 <C> 15.3 <R> <C> [BOLD] bi-Seq2seq Model <C> 84.9 <C> 76.2 <C> [BOLD] 71.9 <C> 21.7 <CAP> Table 1: Evaluation (Embedding Probing) task accuracies for the embedding models. The best result for each task is displayed in bold font.
<R> <C> [EMPTY] <C> mod-1 comp <C> mod-1 rel <C> mod-1 macro <C> mod-2 comp <C> mod-2 rel <C> mod-2 macro <C> mod-3 comp <C> mod-3 rel <C> mod-3 macro <R> <C> separate <C> 0.737 <C> 0.589 <C> 0.663 <C> 0.741 <C> 0.590 <C> 0.665 <C> 0.737 <C> 0.593 <C> 0.665 <R> <C> MST <C> 0.748 <C> [BOLD] 0.666 <C> 0.707 <C> 0.751 <C> 0.675 <C> 0.713 <C> 0.788 <C> [BOLD] 0.689 <C> 0.739 <R> <C> ILP <C> [ITALIC]  [BOLD] 0.770 <C> [BOLD] 0.666 <C> [BOLD] 0.718 <C> [ITALIC]  [BOLD] 0.778 <C> [BOLD] 0.678 <C> [BOLD] 0.728 <C> [BOLD] 0.793 <C> 0.688 <C> [BOLD] 0.740 <CAP> Table 4: Performance on the student essays corpus (comp: component classification; rel: relation classification; Bold: the best performance in column; Underline: the performance is significantly better than separate baseline (p<0.05); Italic: the performance is significantly better than MST (p<0.05).)
<R> <C> Model <C> EN  [ITALIC] R1 [ITALIC] R <C> EN  [ITALIC] R1 [ITALIC] P <C> EN  [ITALIC] RLR <C> EN  [ITALIC] RLP <C> ET  [ITALIC] R1 [ITALIC] R <C> ET  [ITALIC] R1 [ITALIC] P <C> ET  [ITALIC] RLR <C> ET  [ITALIC] RLP <R> <C> No pre-training <C> 20.36 <C> 33.51 <C> 17.68 <C> 29.03 <C> 26.44 <C> 34.23 <C> 25.31 <C> 32.74 <R> <C> Embeddings <C> 21.09 <C> 33.36 <C> 18.23 <C> 28.72 <C> 28.42 <C> 35.94 <C> 27.02 <C> 34.16 <R> <C> Encoder ( 2.1 ) <C> 21.25 <C> 34.1 <C> 18.45 <C> 29.5 <C> [BOLD] 29.28 <C> [BOLD] 37.04 <C> [BOLD] 27.88 <C> [BOLD] 35.24 <R> <C> Decoder ( 2.2 ) <C> 20.11 <C> 31.1 <C> 17.43 <C> 26.87 <C> 25.12 <C> 32.6 <C> 23.89 <C> 30.99 <R> <C> Enc.+dec. <C> 20.72 <C> 33.93 <C> 18.04 <C> 29.43 <C> 27.18 <C> 34.58 <C> 25.79 <C> 32.78 <R> <C> Distant all <C> 20.32 <C> 31.54 <C> 17.59 <C> 27.25 <C> 26.17 <C> 34.49 <C> 24.96 <C> 32.87 <R> <C> Enc.+dec.+dist. ( 2.3 ) <C> [BOLD] 21.34 <C> [BOLD] 34.81 <C> [BOLD] 18.53 <C> [BOLD] 30.14 <C> 27.74 <C> 35.46 <C> 26.35 <C> 33.67 <CAP> Table 2: Recall and precision of ROUGE-1 and ROUGE-L on the test sets. Best scores in bold. Results with statistically significant differences (95% confidence) compared to No pre-training underlined.
<R> <C> [BOLD] Input DAs <C> [BOLD] Generator mode <C> [BOLD] Lexicalizer <C> [BOLD] BLEU <C> [BOLD] NIST <C> [BOLD] METEOR <C> [BOLD] ROUGE-L <C> [BOLD] CIDEr <C> [BOLD] SER <R> <C> Delexicalized <C> Word forms <C> Random <C> 15.51‡ <C> 3.7352 <C> 18.60 <C> 35.00 <C> 1.3922 <C> [BOLD] 00.70 <R> <C> Delexicalized <C> Word forms <C> Most frequent <C> 20.28‡ <C> 4.5192 <C> 22.69 <C> 40.92 <C> 1.9399 <C> [BOLD] 00.70 <R> <C> Delexicalized <C> Word forms <C> RNN LM <C> 20.74∗ <C> 4.5096 <C> 22.61 <C> 40.72 <C> 1.9924 <C> [BOLD] 00.70 <R> <C> 2-9[0.5pt/2pt] <C> Lemma-tag <C> Random <C> 19.66† <C> 4.4884†‡ <C> 22.19 <C> 41.42 <C> 1.8844 <C> 01.85 <R> <C> [EMPTY] <C> Lemma-tag <C> Most frequent <C> 21.21†‡ <C> 4.6900†‡ <C> 23.07 <C> 42.62 <C> 2.0983 <C> 01.85 <R> <C> [EMPTY] <C> Lemma-tag <C> RNN LM <C> [BOLD] 21.96∗†‡ <C> [BOLD] 4.7720∗†‡ <C> [BOLD] 23.32 <C> [BOLD] 42.95 <C> [BOLD] 2.1783 <C> 01.85 <R> <C> Lexicalized <C> Word forms <C> Random <C> 14.70 <C> 3.7595 <C> 18.29 <C> 35.64 <C> 1.3712 <C> 02.30 <R> <C> Lexicalized <C> Word forms <C> Most frequent <C> 19.73 <C> 4.5618 <C> 22.45 <C> 41.71 <C> 1.9473 <C> 02.30 <R> <C> Lexicalized <C> Word forms <C> RNN LM <C> 20.48∗ <C> 4.6060∗‡ <C> 22.55 <C> 41.66 <C> 2.0192 <C> 02.30 <R> <C> 2-9[0.5pt/2pt] <C> Lemma-tag <C> Random <C> 18.92† <C> 4.3501† <C> 21.76 <C> 40.55 <C> 1.8014 <C> 03.08 <R> <C> [EMPTY] <C> Lemma-tag <C> Most frequent <C> 19.44 <C> 4.4453 <C> 22.22 <C> 41.26 <C> 1.8801 <C> 03.08 <R> <C> [EMPTY] <C> Lemma-tag <C> RNN LM <C> 20.42∗ <C> 4.5460∗ <C> 22.56 <C> 41.73 <C> 1.9796 <C> 03.08 <CAP> Table 3: Automatic metrics results. See Section 4.2 for metrics; scores are averaged over 5 different random initializations, all scores except for NIST and CIDEr are percentages. ∗ = significantly better than the corresponding most frequent baseline lexicalizer, † = significantly better than the corresponding word forms mode, ‡ = significantly better than the corresponding (de)lexicalized input DAs. Significance was assessed using pairwise bootstrap resampling Koehn (2004), p<0.01.
<R> <C> [EMPTY] <C> [BOLD] Features <C> Labeled by- [BOLD] Article Accuracy <C> Labeled by- [BOLD] Article Prec. <C> Labeled by- [BOLD] Article Rec. <C> Labeled by- [BOLD] Article F1 <C> Labeled by- [BOLD] Publisher Accuracy <C> Labeled by- [BOLD] Publisher Prec. <C> Labeled by- [BOLD] Publisher Rec. <C> Labeled by- [BOLD] Publisher F1 <R> <C> 1 <C> BoW (TFiDF) <C> 67.8 <C> 53.8 <C> 89.1 <C> 67.1 <C> 56.7 <C> 55.1 <C> 72.5 <C> 62.6 <R> <C> 2 <C> BoW (NB-TFiDF) <C> 69.6 <C> 56.1 <C> 80.7 <C> 66.2 <C> 57.1 <C> 56.4 <C> 61.9 <C> 59.0 <R> <C> 3 <C> ↰ + Char trigrams <C> 74.0 <C> 62.5 <C> 73.5 <C> 67.6 <C> 54.8 <C> 54.3 <C> 60.8 <C> 57.4 <R> <C> 4 <C> ↰ + Bias <C> 75.2 <C> 67.7 <C> 62.6 <C> 65.1 <C> 54.5 <C> 55.0 <C> 50.4 <C> 52.6 <R> <C> 5 <C> ↰ + Lexical <C> 75.2 <C> 67.0 <C> 64.7 <C> 65.8 <C> 52.3 <C> 52.3 <C> 51.5 <C> 51.9 <R> <C> 6 <C> ↰ + Vocab. Richness <C> 75.8 <C> 67.1 <C> 67.6 <C> 67.4 <C> 50.9 <C> 50.8 <C> 52.5 <C> 51.7 <R> <C> 7 <C> ↰ + Readability <C> 76.0 <C> 66.4 <C> 70.6 <C> 68.4 <C> 51.6 <C> 51.5 <C> 53.9 <C> 52.7 <CAP> Table 1: An incremental analysis showing the performance of different feature combinations, evaluated on the validation sets labeled by article and by publisher.
<R> <C> [BOLD] Accuracy(%)  [BOLD] (Averaged over 5 runs)  [BOLD] Datasets <C> [BOLD] Accuracy(%)  [BOLD] (Averaged over 5 runs)  [BOLD] MRDA <C> [BOLD] Accuracy(%)  [BOLD] (Averaged over 5 runs)  [BOLD] MRDA <C> [BOLD] Accuracy(%)  [BOLD] (Averaged over 5 runs)  [BOLD] MRDA <C> [BOLD] Accuracy(%)  [BOLD] (Averaged over 5 runs)  [BOLD] ATIS <C> [BOLD] Accuracy(%)  [BOLD] (Averaged over 5 runs)  [BOLD] ATIS <C> [BOLD] Accuracy(%)  [BOLD] (Averaged over 5 runs)  [BOLD] ATIS <C> [BOLD] Accuracy(%)  [BOLD] (Averaged over 5 runs)  [BOLD] SWDA <C> [BOLD] Accuracy(%)  [BOLD] (Averaged over 5 runs)  [BOLD] SWDA <C> [BOLD] SWDA <R> <C> [BOLD] Perturb(%) <C> [BOLD] BiLSTM-wp <C> [BOLD] BiLSTM-w <C> [BOLD] SGNN <C> [BOLD] BiLSTM-wp <C> [BOLD] BiLSTM-w <C> [BOLD] SGNN <C> [BOLD] BiLSTM-wp <C> [BOLD] BiLSTM-w <C> [BOLD] SGNN <R> <C> 0 <C> 79.23 <C> 78.14 <C> 87.22 <C> 91.73 <C> 92.04 <C> 93.51 <C> 72.92 <C> 72.94 <C> 76.21 <R> <C> Perturbation operation:  [ITALIC] drop <C> Perturbation operation:  [ITALIC] drop <C> Perturbation operation:  [ITALIC] drop <C> Perturbation operation:  [ITALIC] drop <C> Perturbation operation:  [ITALIC] drop <C> Perturbation operation:  [ITALIC] drop <C> Perturbation operation:  [ITALIC] drop <C> Perturbation operation:  [ITALIC] drop <C> Perturbation operation:  [ITALIC] drop <C> [EMPTY] <R> <C> 20 <C> 69.46±1.1 <C> 74.96±0.7 <C> 85.43±0.3 <C> 81.95±2.2 <C> 80.15±2.0 <C> 91.05±0.2 <C> 65.05±4.2 <C> 64.95±4.1 <C> 70.76±1.1 <R> <C> 40 <C> 72.34±2.9 <C> 55.17±3.1 <C> 84.62±0.22 <C> 71.69±2.9 <C> 65.88±2.1 <C> 91.86±0.3 <C> 61.24±4.1 <C> 64.95±5.2 <C> 67.79±1.2 <R> <C> 60 <C> 69.81±4.7 <C> 42.25±4.2 <C> 83.27±0.25 <C> 59.16±4.8 <C> 56.25±3.9 <C> 90.12±0.4 <C> 57.48±5.7 <C> 58.77±5.8 <C> 63.21±1.3 <R> <C> Perturbation operation:  [ITALIC] swap <C> Perturbation operation:  [ITALIC] swap <C> Perturbation operation:  [ITALIC] swap <C> Perturbation operation:  [ITALIC] swap <C> Perturbation operation:  [ITALIC] swap <C> Perturbation operation:  [ITALIC] swap <C> Perturbation operation:  [ITALIC] swap <C> Perturbation operation:  [ITALIC] swap <C> Perturbation operation:  [ITALIC] swap <C> [EMPTY] <R> <C> 20 <C> 78.25±1.1 <C> 71.34±2.2 <C> 86.74±0.1 <C> 86.12±1.7 <C> 85.05±1.8 <C> 92.05±0.3 <C> 66.27±3.3 <C> 64.52±2.1 <C> 70.84±0.3 <R> <C> 40 <C> 75.91±3.9 <C> 69.22±2.1 <C> 86.39±0.2 <C> 82.06±2.7 <C> 78.04±2.9 <C> 91.15±0.2 <C> 62.67±5.8 <C> 54.93±2.4 <C> 67.22±0.3 <R> <C> 60 <C> 69.22±3.8 <C> 66.91±4.0 <C> 85.99±0.2 <C> 72.34±3.3 <C> 68.54±4.1 <C> 91.27±0.3 <C> 59.20±4.8 <C> 47.93±4.3 <C> 64.48±0.3 <R> <C> Perturbation operation:  [ITALIC] all <C> Perturbation operation:  [ITALIC] all <C> Perturbation operation:  [ITALIC] all <C> Perturbation operation:  [ITALIC] all <C> Perturbation operation:  [ITALIC] all <C> Perturbation operation:  [ITALIC] all <C> Perturbation operation:  [ITALIC] all <C> Perturbation operation:  [ITALIC] all <C> Perturbation operation:  [ITALIC] all <C> [EMPTY] <R> <C> 20 <C> 72.96±1.3 <C> 73.39±2.3 <C> 86.71±0.4 <C> 80.40±1.7 <C> 83.55±2.1 <C> 92.83±0.2 <C> 60.49±4.3 <C> 64.28±3.3 <C> 68.96±0.2 <R> <C> 40 <C> 70.32±3.4 <C> 63.04±4.3 <C> 85.31±0.5 <C> 71.62±3.1 <C> 75.81±2.6 <C> 90.71±0.3 <C> 54.96±4.1 <C> 59.46±4.6 <C> 65.44±0.4 <R> <C> 60 <C> 67.64±5.7 <C> 55.50±5.3 <C> 84.21±0.5 <C> 61.10±6.1 <C> 66.58±5.6 <C> 88.35±0.3 <C> 49.62±6.7 <C> 51.85±6.3 <C> 64.97±0.5 <CAP> Table 3: Comparison of projection based models vs BiLSTMs subject to various types and amounts of perturbations. BiLSTM-wp and BiLSTM-w refer to models with word-piece and word-only tokenization respectively.
<R> <C> [BOLD] LSH Proj.Dim ( [BOLD] K) <C> [BOLD] Character Perturbations 5% <C> [BOLD] Character Perturbations 10% <R> <C> 840 <C> 10.08 <C> 24.33 <R> <C> 980 <C> 15.48 <C> 31.24 <R> <C> 1120 <C> 18.83 <C> 33.65 <R> <C> 1260 <C> 19.71 <C> 39.01 <R> <C> 1400 <C> 27.76 <C> 54.08 <CAP> Table 4: Avg. changes in word projections (bits) for different Char Perturbation % in enwik9 corpus.
<R> <C> [EMPTY] <C> GPT-2 PPL <C> [ITALIC] ρ <C> [ITALIC] ρ4 <C> [ITALIC] ρ2 <C> length <R> <C> ours <C> 118 <C> 0.86 <C> 0.21 <C> 0.13 <C> 8.2 <R> <C> [ITALIC] n-gram <C> 224 <C> 0.83 <C> 0.19 <C> 0.12 <C> 8,2 <R> <C> ml <C> 944 <C> 1.0 <C> 0.97 <C> 0.72 <C> 9.2 <R> <C> data <C> 111 <C> 1.0 <C> 0.97 <C> 0.74 <C> 8.9 <CAP> Table 3: Results summarized including a data sample.
<R> <C> Model <C> WER (%) 9000 senones <C> WER (%) 9000 senones <C> WER (%) 27000 senones <C> WER (%) 27000 senones <R> <C> Model <C> CH <C> SWB <C> CH <C> SWB <R> <C> Baseline <C> 21.4 <C> 9.9 <C> 20.5 <C> 10.6 <R> <C> Spatial smoothing <C> 19.2 <C> 9.3 <C> 19.5 <C> 9.2 <CAP> Table 2: Accuracy improvements from spatial smoothing on the NIST 2000 CTS test set. The model is a six layer BLSTM, using i-vectors and 40 dimensional filterbank features, and a dimension of 512 in each direction of each layer.
<R> <C> [EMPTY] <C> [BOLD] CH  [BOLD] ASR <C> [BOLD] CH  [BOLD] Human <C> [BOLD] SWB  [BOLD] ASR <C> [BOLD] SWB  [BOLD] Human <R> <C> sub <C> 6.5 <C> 4.1 <C> 3.3 <C> 2.6 <R> <C> del <C> 3.3 <C> 6.5 <C> 1.8 <C> 2.7 <R> <C> ins <C> 1.4 <C> 0.7 <C> 0.7 <C> 0.7 <R> <C> all <C> 11.1 <C> 11.3 <C> 5.9 <C> 5.9 <CAP> Table 13: Overall substitution, deletion and insertion rates.
<R> <C> speaker (human) <C> easy <C> difficult <R> <C> random <C> 0.92 <C> 0.81 <R> <C> discriminative <C> 1.0 <C> 0.97 <CAP> Table 1: Accuracy performance of a human listener with a human speaker producing either random or discriminative caption on the easy and difficult splits.
<R> <C> Weights learned with RL <C> joint <C> human <C> Δ <R> <C> reranker <C> 0.88 <C> 0.92 <C> -0.04 <R> <C> reranker + speaker ResNet <C> 0.92 <C> 0.90 <C> +0.02 <R> <C> reranker + both agent ResNets <C> 0.96 <C> 0.88 <C> +0.08 <CAP> Table 5: Referential success for PoE with gold captions when updating different components during training with joint listener.
<R> <C> [EMPTY] <C> Task A: Hillary Clinton Favg <C> Task A: Hillary Clinton Ranking <C> Task B: Donald Trump Favg <C> Task B: Donald Trump Ranking <R> <C> experiment1 for Hillary Clinton <C> [BOLD] 63.75 <C> 3 <C> 53.46 <C> 2 <R> <C> experiment1 for Donald Trump <C> 61.25 <C> 4 <C> [BOLD] 55.51 <C> 2 <R> <C> experiment2 for Hillary Clinton <C> [BOLD] 71.21 <C> 1 <C> 69.59 <C> 1 <R> <C> experiment2 for Donald Trump <C> 68.29 <C> 1 <C> [BOLD] 74.49 <C> 1 <R> <C> Systems in the official competition <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> INF-UFRGS <C> - <C> - <C> 42.32 <C> 3 <R> <C> LitisMind <C> 42.08 <C> 17 <C> 44.66 <C> 2 <R> <C> pkudbblab <C> 64.41 <C> 2 <C> [BOLD] 56.28 <C> 1 <R> <C> PKULCWM <C> 62.26 <C> 3 <C> - <C> - <R> <C> TakeLab <C> [BOLD] 67.12 <C> 1 <C> - <C> - <CAP> Table 6: Results of task A and B
<R> <C> [BOLD] Model <C> [BOLD] Iteration <C> [BOLD] 1 Shot <C> [BOLD] 5 Shot <R> <C> w/o DMM <C> 3 <C> 81.79 <C> 90.19 <R> <C> w/o QIM <C> 3 <C> 82.37 <C> 90.57 <R> <C> DMIN <C> 1 <C> 82.70 <C> 90.92 <R> <C> DMIN <C> 2 <C> 82.95 <C> 91.18 <R> <C> DMIN <C> 3 <C> [BOLD] 83.46 <C> [BOLD] 91.75 <CAP> Table 3: Ablation study of accuracy (%) on ODIC in a 5-way setup.
<R> <C> [BOLD] Dataset <C> [BOLD] Method <C> [BOLD] AUC <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1-Score <C> [BOLD] P@100 <C> [BOLD] P@200 <R> <C> NYT <C> PCNN <C> 0.3296 <C> 0.3830 <C> 0.4020 <C> 0.3923 <C> 0.77 <C> 0.72 <R> <C> NYT <C> PCNN+ATT <C> 0.3424 <C> 0.3588 <C> 0.4564 <C> 0.4018 <C> 0.75 <C> 0.75 <R> <C> NYT <C> BGWA <C> 0.3670 <C> 0.3994 <C> 0.4451 <C> 0.4210 <C> 0.76 <C> 0.74 <R> <C> NYT <C> CNN+RL <C> 0.3735 <C> 0.4201 <C> 0.4389 <C> 0.4293 <C> 0.79 <C> 0.73 <R> <C> NYT <C> PA-T <C> 0.3572 <C> 0.3779 <C> 0.4586 <C> 0.4143 <C> 0.78 <C> 0.72 <R> <C> NYT <C> PA-MR <C> 0.3635 <C> 0.4091 <C> 0.4410 <C> 0.4244 <C> 0.79 <C> 0.78 <R> <C> NYT <C> PA-TMR <C> [BOLD] 0.3939 <C> [BOLD] 0.4320 <C> [BOLD] 0.4615 <C> [BOLD] 0.4463 <C> [BOLD] 0.83 <C> [BOLD] 0.79 <R> <C> GDS <C> PCNN <C> 0.7798 <C> 0.6804 <C> 0.8673 <C> 0.7626 <C> 0.88 <C> 0.90 <R> <C> GDS <C> PCNN+ATT <C> 0.8034 <C> 0.7250 <C> 0.8474 <C> 0.7814 <C> 0.94 <C> 0.93 <R> <C> GDS <C> BGWA <C> 0.8148 <C> 0.7725 <C> 0.7162 <C> 0.8385 <C> 0.99 <C> 0.98 <R> <C> GDS <C> CNN+RL <C> 0.8554 <C> 0.7680 <C> [BOLD] 0.9132 <C> 0.8343 <C> 1.0 <C> 0.96 <R> <C> GDS <C> PA-T <C> 0.8512 <C> 0.7925 <C> 0.8969 <C> 0.8414 <C> 0.96 <C> 0.94 <R> <C> GDS <C> PA-MR <C> 0.8571 <C> 0.8011 <C> 0.8947 <C> [BOLD] 0.8453 <C> 0.97 <C> 0.94 <R> <C> GDS <C> PA-TMR <C> [BOLD] 0.8646 <C> [BOLD] 0.8058 <C> 0.8641 <C> 0.8339 <C> [BOLD] 1.0 <C> [BOLD] 0.98 <CAP> TABLE IV: Performance Comparison
<R> <C> Features <C> SI-SDR <C> eSTOI <R> <C> Noisy speech <C> 7.5 <C> 68.3 <R> <C> Mimic - hard targets <C> 1.6 <C> 72.6 <R> <C> Joint training <C> 0.6 <C> 47.0 <CAP> Table 1: Speech enhancement scores for the state-of-the-art architecture trained from scratch without the parallel clean speech data from the CHiME-4 corpus. Evaluation is done on channel 5 of the simulated et05 data. The joint training is done with an identical setup to the mimic system.
<R> <C> Features <C> SI-SDR <C> eSTOI <R> <C> Noisy speech <C> 7.5 <C> 68.3 <R> <C> AECNN-T <C> 11.5 <C> 77.0 <R> <C> + Mimic loss <C> 11.9 <C> 79.1 <R> <C> AECNN-T-SM <C> 11.7 <C> 78.9 <R> <C> + Mimic loss <C> 11.9 <C> 79.8 <R> <C> Joint training <C> 11.7 <C> 79.5 <CAP> Table 2: Speech enhancement scores for the state-of-the-art system trained with the parallel data available in the CHiME-4 corpus. Evaluation is done on channel 5 of the simulation et05 data. Mimic loss is applied to the AECNN model trained with time-domain mapping loss only, as well as time-domain and spectral magnitude mapping losses. The joint training system is done with an identical setup to the mimic system with all three losses.
<R> <C> [EMPTY] <C> models <C> BLEU <C> NIST <C> TER <R> <C> [ITALIC] en-hi <C> BL <C> 18.52 <C> 5.813 <C> 64.33 <R> <C> [EMPTY] <C> BL+RO <C> 22.72 <C> [BOLD] 6.035 <C> [BOLD] 59.85 <R> <C> [EMPTY] <C> BL+RO+FACT <C> [BOLD] 22.83 <C> 5.994 <C> 60.17 <CAP> Table 4: Effect of source reordering (RO) and factor (FACT)
<R> <C> [BOLD] Model <C> [BOLD] PER (%) <C> [BOLD] WER (%) <R> <C> Baseline <C> Baseline <C> Baseline <R> <C> Enc-Dec LSTM (2 lyr) Yao and Zweig ( 2015 ) <C> 7.63 <C> 28.61 <R> <C> Bi-LSTM (3 lyr) Yao and Zweig ( 2015 ) <C> 5.45 <C> 23.55 <R> <C> Att Enc-Dec with Global MLP Scorer (ours) <C> 5.96 <C> 25.55 <R> <C> Att Enc-Dec with  [ITALIC] local-m (ours) Luong et al. ( 2015 ) <C> 5.64 <C> 24.32 <R> <C> Proposed <C> Proposed <C> Proposed <R> <C> Att Enc-Dec + Unconst (exp) (2 [ITALIC] σ = 2) <C> 5.45 <C> [BOLD] 23.15 <R> <C> Att Enc-Dec + Unconst (exp) (2 [ITALIC] σ = 3) <C> [BOLD] 5.43 <C> 23.19 <CAP> Table 2: Results from baseline and proposed method on G2P task with CMUDict test set
<R> <C> [BOLD] Model <C> [BOLD] BLEU <R> <C> [BOLD] BTEC English to France <C> [BOLD] BTEC English to France <R> <C> Baseline <C> Baseline <R> <C> Att Enc-Dec with Global MLP Scorer <C> 49.0 <R> <C> Att Enc-Dec with  [ITALIC] local-m (ours) Luong et al. ( 2015 ) <C> 50.4 <R> <C> Proposed <C> Proposed <R> <C> Att Enc-Dec + Unconst (exp) (2 [ITALIC] σ = 4) <C> [BOLD] 51.2 <R> <C> Att Enc-Dec + Unconst (exp) (2 [ITALIC] σ = 6) <C> 51.1 <R> <C> [BOLD] BTEC Indonesian to English <C> [BOLD] BTEC Indonesian to English <R> <C> Baseline <C> Baseline <R> <C> Att Enc-Dec with Global MLP Scorer <C> 38.2 <R> <C> Att Enc-Dec with  [ITALIC] local-m (ours) Luong et al. ( 2015 ) <C> 39.8 <R> <C> Proposed <C> Proposed <R> <C> Att Enc-Dec + Unconst (exp) (2 [ITALIC] σ = 4) <C> 40.9 <R> <C> Att Enc-Dec + Unconst (exp) (2 [ITALIC] σ = 6) <C> [BOLD] 41.8 <CAP> Table 3: Results from baseline and proposed method on English-to-France and Indonesian-to-English translation tasks.
<R> <C> Model <C> IoU=0.3 <C> IoU=0.5 <C> IoU=0.7 <C> mIoU <R> <C> 3D ConvNet without fine-tuning as visual feature extractor <C> 3D ConvNet without fine-tuning as visual feature extractor <C> 3D ConvNet without fine-tuning as visual feature extractor <C> 3D ConvNet without fine-tuning as visual feature extractor <C> 3D ConvNet without fine-tuning as visual feature extractor <R> <C> CTRL <C> - <C> 23.63 <C> 8.89 <C> - <R> <C> ACL-K <C> - <C> 30.48 <C> 12.20 <C> - <R> <C> QSPN <C> 54.70 <C> 35.60 <C> 15.80 <C> - <R> <C> SAP <C> - <C> 27.42 <C> 13.36 <C> - <R> <C> SM-RL <C> - <C> 24.36 <C> 11.17 <C> - <R> <C> RWM-RL <C> - <C> 36.70 <C> - <C> - <R> <C> MAN <C> - <C> 46.53 <C> 22.72 <C> - <R> <C> DEBUG <C> 54.95 <C> 37.39 <C> 17.69 <C> 36.34 <R> <C> VSLBase <C> 61.72 <C> 40.97 <C> 24.14 <C> 42.11 <R> <C> VSLNet <C> [BOLD] 64.30 <C> [BOLD] 47.31 <C> [BOLD] 30.19 <C> [BOLD] 45.15 <R> <C> 3D ConvNet with fine-tuning on Charades dataset <C> 3D ConvNet with fine-tuning on Charades dataset <C> 3D ConvNet with fine-tuning on Charades dataset <C> 3D ConvNet with fine-tuning on Charades dataset <C> 3D ConvNet with fine-tuning on Charades dataset <R> <C> ExCL <C> 65.10 <C> 44.10 <C> 23.30 <C> - <R> <C> VSLBase <C> 68.06 <C> 50.23 <C> 30.16 <C> 47.15 <R> <C> VSLNet <C> [BOLD] 70.46 <C> [BOLD] 54.19 <C> [BOLD] 35.22 <C> [BOLD] 50.02 <CAP> Table 2: Results (%) of “R@n,IoU=μ” and “mIoU” compared with the state-of-the-art on Charades-STA.
<R> <C> Model <C> IoU=0.3 <C> IoU=0.5 <C> IoU=0.7 <C> mIoU <R> <C> TGN <C> 45.51 <C> 28.47 <C> - <C> - <R> <C> ABLR <C> 55.67 <C> 36.79 <C> - <C> 36.99 <R> <C> RWM-RL <C> - <C> 36.90 <C> - <C> - <R> <C> QSPN <C> 45.30 <C> 27.70 <C> 13.60 <C> - <R> <C> ExCL∗ <C> 63.00 <C> [BOLD] 43.60 <C> 24.10 <C> - <R> <C> DEBUG <C> 55.91 <C> 39.72 <C> - <C> 39.51 <R> <C> VSLBase <C> 58.18 <C> 39.52 <C> 23.21 <C> 40.56 <R> <C> VSLNet <C> [BOLD] 63.16 <C> 43.22 <C> [BOLD] 26.16 <C> [BOLD] 43.19 <CAP> Table 3: Results (%) of “R@n,IoU=μ” and “mIoU” compared with the state-of-the-art on ActivityNet Caption.
<R> <C> Module <C> IoU=0.3 <C> IoU=0.5 <C> IoU=0.7 <C> mIoU <R> <C> BiLSTM + CAT <C> 61.18 <C> 43.04 <C> 26.42 <C> 42.83 <R> <C> CMF + CAT <C> 63.49 <C> 44.87 <C> 27.07 <C> 44.01 <R> <C> BiLSTM + CQA <C> 65.08 <C> 46.94 <C> 28.55 <C> 45.18 <R> <C> CMF + CQA <C> 68.06 <C> 50.23 <C> 30.16 <C> 47.15 <CAP> Table 5: Comparison between models with alternative modules in VSLBase on Charades-STA.
<R> <C> Module <C> CAT <C> CQA <C> Δ <R> <C> BiLSTM <C> 26.42 <C> 28.55 <C> +2.13 <R> <C> CMF <C> 27.07 <C> 30.16 <C> +3.09 <R> <C> Δ <C> +0.65 <C> +1.61 <C> - <CAP> Table 6: Performance gains (%) of different modules over “R@1,IoU=0.7” on Charades-STA.
<R> <C> Model <C> [BOLD] SQuAD EM <C> [BOLD] SQuAD F1 <C> [BOLD] ParallelQA EM <C> [BOLD] ParallelQA F1 <R> <C> BiDAF <C> 67.70 <C> 77.30 <C> 35.29 <C> 42.52 <R> <C> DrQA <C> 69.64 <C> 78.76 <C> 39.22 <C> 47.23 <R> <C> R-Net <C> 71.07 <C> 79.51 <C> 41.18 <C> 50.38 <CAP> Table 3: Performance on SQuAD vs ParallelQA
<R> <C> Country <C> Total number of HealthMap news articles <C> Total number of unique words <C> Total number of unique location names or (country, state) pairs <R> <C> China <C> 11,209 <C> 21,879 <C> 30 <R> <C> India <C> 1,204 <C> 17,160 <C> 30 <R> <C> U.S. <C> 9,872 <C> 59,687 <C> 51 <CAP> Table 2: Country-wise distribution of the total number of HealthMap news articles along with unique words and location names extracted from all the corresponding articles.
<R> <C> [BOLD] Model <C> [BOLD] F1 <R> <C> chieu2002named <C> 88.31 <R> <C> florian2003named <C> 88.76 <R> <C> ando2005framework <C> 89.31 <R> <C> collobert2011natural‡ <C> 89.59 <R> <C> huang2015bidirectional‡ <C> 90.10 <R> <C> chiu2015named‡ <C> 90.77 <R> <C> ratinov2009design <C> 90.80 <R> <C> lin2009phrase <C> 90.90 <R> <C> passos-kumar-mccallum:2014:W14-16 <C> 90.90 <R> <C> lample:2016:NAACL‡ <C> 90.94 <R> <C> luo-EtAl:2015:EMNLP2 <C> 91.20 <R> <C> [BOLD] This paper <C> [BOLD] 91.21 <CAP> Table 5: NER F1 score of our model on test data set from CoNLL-2003. For the purpose of comparison, we also list F1 scores of previous top-performance systems. ‡ marks the neural models.
<R> <C> Constraints <C> F1 score <C> Sparsity <C> # of cons <R> <C> Baseline <C> 94.44 <C> [EMPTY] <C> [EMPTY] <R> <C> Only-one <C> 94.62 <C> 0% <C> 3 <R> <C> Hierarchical <C> 94.55 <C> 56.25% <C> 16 <R> <C> Pairwise <C> 95.23 <C> 43.19% <C> 609 <R> <C> All <C> [BOLD] 95.39 <C> 32.96% <C> 628 <R> <C> All DD <C> 94.60 <C> 0% <C> 628 <CAP> Table 1: Set of constraints learned and F1 scores. The last row depicts the result of inference using all constraints as hard constraints.
<R> <C> [BOLD] Method <C> [BOLD] ROUGE-1 <C> [BOLD] ROUGE-2 <C> [BOLD] ROUGE-L <R> <C> MLE+xls+mt <C> 41.25 <C> 22.40 <C> 37.93 <R> <C> +Share <C> 41.36 <C> 22.43 <C> 37.95 <R> <C> +Tag <C> 41.45 <C> 22.47 <C> 38.04 <CAP> Table 3: Effect of sharing decoder layers and adding task-specific tags
<R> <C> [BOLD] Metric <C> [BOLD] Model v. MLE <C> [BOLD] Win (%) <C> [BOLD] Lose (%) <C> [BOLD] Tie (%) <R> <C> Relevance <C> RL-rouge <C> 25.3 <C> 15.3 <C> 59.3 <R> <C> Relevance <C> RL-xsim <C> 36.0 <C> 31.3 <C> 32.7 <R> <C> Fluency <C> RL-rouge <C> 13.3 <C> 17.3 <C> 69.3 <R> <C> Fluency <C> RL-xsim <C> 37.3 <C> 28.7 <C> 34.0 <CAP> Table 4: Results showing preferences of human evaluators towards the summaries generated by the mentioned RL methods vs ones from the pre-trained model (MLE-xls+mt+dis referred in short as MLE)
<R> <C> [EMPTY] <C> [BOLD] CROATIAN CO <C> [BOLD] CROATIAN SIN <C> [BOLD] CROATIAN SHU <C> [BOLD] CROATIAN SYL <C> [BOLD] CROATIAN GR <C> [BOLD] ENGLISH CO <C> [BOLD] ENGLISH SIN <C> [BOLD] ENGLISH SHU <C> [BOLD] ENGLISH SYL <C> [BOLD] ENGLISH GR <R> <C> [ITALIC] N <C> 23359 <C> 23359 <C> 23359 <C> 2634 <C> 34 <C> 10930 <C> 10930 <C> 10930 <C> 2599 <C> 26 <R> <C> [ITALIC] K <C> 71860 <C> 70155 <C> 86214 <C> 18849 <C> 491 <C> 50299 <C> 52221 <C> 58920 <C> 6053 <C> 333 <R> <C> [ITALIC] L <C> 4.01 <C> 1.81 <C> 3.74 <C> 1.86 <C> 1.58 <C> 3.47 <C> 1.96 <C> 0.45 <C> 1.88 <C> 1.51 <R> <C> [ITALIC] C <C> 0.167 <C> 0.120 <C> 0.182 <C> 0.255 <C> 0.636 <C> 0.286 <C> 0.153 <C> 0.295 <C> 0.057 <C> 0.838 <R> <C> [ITALIC] T <C> 0.004 <C> 0.003 <C> 0.013 <C> 0.120 <C> 0.522 <C> 0.009 <C> 0.014 <C> 0.016 <C> 0.020 <C> 0.654 <R> <C> [ITALIC] ω <C> 2 <C> 2 <C> 2 <C> 17 <C> 1 <C> 3 <C> 3 <C> 1 <C> 54 <C> 1 <CAP> Table 1: The standard network measures for ten layers.
<R> <C> [EMPTY] <C> Sentence Retrival (Image to Text) R@1 <C> Sentence Retrival (Image to Text) R@5 <C> Sentence Retrival (Image to Text) R@10 <C> Sentence Retrival (Image to Text) Med r <C> Image Retrival (Text to Image) R@1 <C> Image Retrival (Text to Image) R@5 <C> Image Retrival (Text to Image) R@10 <C> Image Retrival (Text to Image) Med r <R> <C> Random <C> 0.1 <C> 0.5 <C> 1.0 <C> 631 <C> 0.1 <C> 0.5 <C> 1.0 <C> 500 <R> <C> Socher-decaf  <C> 4.5 <C> 18.0 <C> 28.6 <C> 32 <C> 6.1 <C> 18.5 <C> 29.0 <C> 29 <R> <C> Socher-avg-rcnn  <C> 6.0 <C> 22.7 <C> 34.0 <C> 23 <C> 6.6 <C> 21.6 <C> 31.7 <C> 25 <R> <C> DeViSE-avg-rcnn  <C> 4.8 <C> 16.5 <C> 27.3 <C> 28 <C> 5.9 <C> 20.1 <C> 29.6 <C> 29 <R> <C> DeepFE-decaf  <C> 5.9 <C> 19.2 <C> 27.3 <C> 34 <C> 5.2 <C> 17.6 <C> 26.5 <C> 32 <R> <C> DeepFE-rcnn  <C> 12.6 <C> 32.9 <C> 44.0 <C> 14 <C> 9.7 <C> 29.6 <C> [BOLD] 42.5 <C> [BOLD] 15 <R> <C> Ours-m-RNN-decaf <C> [BOLD] 14.5 <C> [BOLD] 37.2 <C> [BOLD] 48.5 <C> [BOLD] 11 <C> [BOLD] 11.5 <C> [BOLD] 31.0 <C> 42.4 <C> [BOLD] 15 <CAP> Table 3: Results of R@K and median rank (Med r) for Flickr8K dataset. Note DeepFE-rcnn uses more sophisticated image features than we do
<R> <C> Pair <C> Seg <C> English translation <C> Acou. sim. <C> Transl. sim. <R> <C> a <C> 1 <C> (to) tell (them) (to) send (me) (my)  [BOLD] baptism act <C> 0.93 <C> 0.125 <R> <C> a <C> 2 <C> (we) (are) going (to) need (the) sacrament (of)  [BOLD] baptism paper <C> 0.93 <C> 0.125 <R> <C> b <C> 3 <C> (not) (now) (now) (then) (he) cant anymore <C> 0.88 <C> 0 <R> <C> b <C> 4 <C> yes well (its) good well yeah <C> 0.88 <C> 0 <R> <C> c <C> 5 <C> okay (this) (the) address  [BOLD] two thousand two hundred <C> 0.86 <C> 0.600 <R> <C> c <C> 6 <C> [BOLD] two thousand two hundred <C> 0.86 <C> 0.600 <CAP> Table 1: English translations for the utterance pairs shown in Figure 1. The acoustic (DTW) similarity and translation (Jaccard) similarity for each pair is also shown. Stop words (in parentheses) are not used to compute translation similarity. Matching content words in bold.
<R> <C> Task <C> CAN+IQA <C> EncDec+IQA <C> DMN+ <C> MemN2N <C> EncDec <R> <C> Task 1 <C> [BOLD] 0 <C> 6 <C> 8 <C> 8 <C> 8 <R> <C> Task 4 <C> [BOLD] 0 <C> 8 <C> 8 <C> 8 <C> 8 <R> <C> Task 7 <C> [BOLD] 2 <C> 7 <C> 8 <C> 8 <C> 8 <CAP> Table 7. Performance comparison of various models from the number of failed datasets for each task in the IQA setting. Each task has eight datasets with different RIQA.
<R> <C> Metrics <C> [ITALIC] σ=0.5 QDisc <C> [ITALIC] σ=0.5 DRate(%) <C> [ITALIC] σ=1.0 QDisc <C> [ITALIC] σ=1.0 DRate(%) <C> [ITALIC] σ=2.0 QDisc <C> [ITALIC] σ=2.0 DRate(%) <R> <C> BS-1 <C> 0.01287 <C> 2.55 <C> 0.01509 <C> 3.29 <C> 0.01063 <C> 3.15 <R> <C> BS-2 <C> 0.02384 <C> 9.41 <C> 0.01699 <C> 4.27 <C> 0.01146 <C> 1.71 <R> <C> BS-3 <C> 2.090 ×10−8 <C> <0.01 <C> 6.045 ×10−6 <C> 0.19 <C> 3.878 ×10−4 <C> 0.05 <CAP> Table 1: Lower-bound of QDisc and DRate w.r.t. BLEU-NSBLEU on synthetic data with different σs.
<R> <C> Metrics <C> MSCOCO QDisc <C> MSCOCO DRate(%) <C> MSCOCO Self-Ratio <C> MSCOCO Ref-Ratio <C> WMT QDisc <C> WMT DRate(%) <C> WMT Self-Ratio <C> WMT Ref-Ratio <R> <C> BS-3 <C> 0.090 <C> 9.0 <C> 0.152 <C> 0.81 <C> 0.117 <C> 11.7 <C> 0.242 <C> 0.88 <R> <C> BS-4 <C> 0.162 <C> 16.2 <C> 0.274 <C> 1.46 <C> 0.211 <C> 21.1 <C> 0.437 <C> 1.59 <R> <C> BS-5 <C> 0.211 <C> 21.1 <C> 0.350 <C> 1.99 <C> 0.258 <C> 25.8 <C> 0.528 <C> 1.97 <R> <C> CN-3 <C> 1.07×10−6 <C> 0.053 <C> 0.0092 <C> 0.087 <C> 3.45×10−7 <C> 0.098 <C> 0.0125 <C> 0.358 <R> <C> CN-4 <C> 1.18×10−6 <C> 0.079 <C> 0.0095 <C> 0.125 <C> 3.25×10−7 <C> 0.103 <C> 0.0116 <C> 0.489 <R> <C> CN-5 <C> 1.33×10−6 <C> 0.098 <C> 0.0082 <C> 0.207 <C> 2.57×10−7 <C> 0.079 <C> 0.0089 <C> 0.689 <CAP> Table 2: Estimation of QDisc, DRate, Self-Ratio, and Ref-Ratio on real text data.
<R> <C> System # <C> Training Data <C> PCA( [ITALIC] σ=80) <C> Sparsity( [ITALIC] λ=0.1) <C> Non-Enhanced Soft-Targets <R> <C> 0 <C> AMI (Baseline WER  [BOLD] 32.4%) <C> - <C> - <C> - <R> <C> 1 <C> AMI(SE-0) <C> 31.9 <C> 31.6 <C> 32.0 <R> <C> 2 <C> ICSI(FP-1) + AMI(SE-0) <C> 31.2 <C> 31.6 <C> 32.5 <R> <C> 3 <C> LIB100(FP-1) + AMI(SE-0) <C> 31.2 <C> 31.6 <C> 32.4 <R> <C> 4 <C> LIB100(FP-2) + AMI(SE-0) <C> 31.0 <C> 31.8 <C> 32.4 <R> <C> 5 <C> LIB100(FP-2) + ICSI(FP-2) + AMI(SE-0) <C> [BOLD] 30.9 <C> 31.7 <C> 32.4 <CAP> Table 1: Performance of various systems (in WER%) when additional untranscribed training data is used. System 0 is hard-targets based baseline DNN. In paranthesis, SE-0 denotes supervised enhancement of DNN outputs from system 0 and FP-n shows forward pass using system n.
<R> <C> [BOLD] Domain <C> [BOLD] train <C> [BOLD] dev <C> [BOLD] test <R> <C> WMT17 <C> 5,919,142 <C> 2,169 <C> 3,004 <R> <C> IWSLT17 <C> 206,112 <C> 2,385 <C> 1,138 <R> <C> Selection <C> 1035 corr / 1042 mark <C> [EMPTY] <C> 1,043 <CAP> Table 4: Data sizes (en-de), official splits from WMT17 and IWSLT17. Our target-domain data is a subset of selected talks from IWSLT2017 training data totalling 3,120 sentences.
<R> <C> Models <C> [BOLD] SNIPS Slot ( [ITALIC] F1) <C> [BOLD] SNIPS Intent ( [ITALIC] Acc) <C> [BOLD] ATIS Slot ( [ITALIC] F1) <C> [BOLD] ATIS Intent ( [ITALIC] Acc) <R> <C> Joint GRU Zhang and Wang ( 2016 ) <C> – <C> – <C> 95.49 <C> 98.10 <R> <C> Self-Attention, Intent GateLi et al. ( 2018 ) <C> – <C> – <C> 96.52 <C> 98.77 <R> <C> Bi-model Wang et al. ( 2018 ) <C> – <C> – <C> [BOLD] 96.89 <C> 98.99 <R> <C> Attention Bi-RNN Liu and Lane ( 2016 ) * <C> 87.80 <C> 96.70 <C> 95.98 <C> 98.21 <R> <C> Joint Seq2Seq Hakkani-Tür et al. ( 2016 ) * <C> 87.30 <C> 96.90 <C> 94.20 <C> 92.60 <R> <C> Slot-Gated (Intent Atten.) Goo et al. ( 2018 ) <C> 88.30 <C> 96.80 <C> 95.20 <C> 94.10 <R> <C> Slot-Gated (Full Atten.) Goo et al. ( 2018 ) <C> 88.80 <C> 97.00 <C> 94.80 <C> 93.60 <R> <C> CAPSULE-NLUZhang et al. ( 2018a ) <C> 91.80 <C> 97.70 <C> 95.20 <C> 95.00 <R> <C> Dilated CNN, Label-Recurrent Gupta et al. ( 2019 ) <C> 93.11 <C> 98.29 <C> 95.54 <C> 98.10 <R> <C> Sentence-State LSTM Zhang et al. ( 2018b ) † <C> 95.80 <C> 98.30 <C> 95.65 <C> 98.21 <R> <C> BiLSTMs + EMLoL Siddhant et al. ( 2018 ) <C> 93.29 <C> 98.83 <C> 95.62 <C> 97.42 <R> <C> BiLSTMs + EMLo Siddhant et al. ( 2018 ) <C> 93.90 <C> [BOLD] 99.29 <C> 95.42 <C> 97.30 <R> <C> Joint BERT Chen et al. ( 2019 ) <C> 97.00 <C> 98.60 <C> 96.10 <C> 97.50 <R> <C> CM-Net (Ours) <C> [BOLD] 97.15 <C> [BOLD] 99.29 <C> 96.20 <C> [BOLD] 99.10 <CAP> Table 3: Results on test sets of the SNIPS and ATIS, where our CM-Net achieves state-of-the-art performances in most cases. “*” indicates that results are retrieved from Slot-Gated Goo et al. (2018), and “†” indicates our implementation.
<R> <C> Models <C> [BOLD] SNIPS Slot ( [ITALIC] F1) <C> [BOLD] SNIPS Intent ( [ITALIC] Acc) <R> <C> BiLSTMs + EMLoL <C> 93.29 <C> 98.83 <R> <C> BiLSTMs + EMLo <C> 93.90 <C> 99.29 <R> <C> Joint BERT <C> 97.00 <C> 98.60 <R> <C> CM-Net + BERT <C> [BOLD] 97.31 <C> [BOLD] 99.32 <CAP> Table 5: Results on the SNIPS benchmark with the assistance of pre-trained language model, where we establish new state-of-the-art results on the SNIPS.
<R> <C> Prop. <C> Holder <C> P 56.4 <C> R 34.3 <C> F1 42.4 <R> <C> Prop. <C> Target <C> 28.2 <C> 36.4 <C> 31.3 <R> <C> Prop. <C> P. Exp. <C> 28.1 <C> 36.4 <C> 31.3 <R> <C> Binary <C> Holder <C> 55.8 <C> 35.1 <C> 43.5 <R> <C> Binary <C> Target <C> 38.4 <C> 40.4 <C> 39.1 <R> <C> Binary <C> P. Exp. <C> 66.3 <C> 57.7 <C> 61.5 <CAP> Table 4: Precision (P), recall (R), and Micro F1 for holders, targets, and polar expressions. Prop. refers to proportional overlap and Binary refers to binary overlap.
<R> <C> [BOLD] COCO <C> [BOLD] I→T <C> Ours (Trip) 0.4790 <C> Struc (GT, Text)  [BOLD] 0.4817 <C> Struc (NNΩ, Text) 0.4635 <C> Struc (NNΩ, Img) 0.4752 <R> <C> [BOLD] COCO <C> [BOLD] T→I <C> 0.4611 <C> [BOLD] 0.4867 <C> 0.4594 <C> 0.4604 <CAP> Table 2: We show retrieval results for image to text (I→T) and text to image (T→I) on COCO using [52]’s loss vs. ours. GT requires multiple Ground Truth captions per image, while NN uses Nearest Neighbors. The best method per row is shown in bold, while the best method which does not require a set of neighboring text is underlined.
<R> <C> [EMPTY] <C> LLGPar <C> LGPar <C> LTPar <C> BerkeleyParser <C> TurboParser <C> Mate-tool <C> ZPar <R> <C> on Dev <C> [BOLD] 93.16 <C> 93.00 <C> 92.77 <C> 92.84 <C> 92.86 <C> 92.58 <C> 92.42 <R> <C> on Test <C> 92.42 <C> 92.43 <C> 92.01 <C> [BOLD] 92.85 <C> 92.63 <C> 92.48 <C> 92.12 <CAP> Table 2: UAS of different parsers trained on all training data (40K)
<R> <C> Epochs <C> NCE <C> IS <C> Training Time [Hours] <R> <C> 1 <C> 97 <C> 60 <C> 1 <R> <C> 5 <C> 58 <C> 47.5 <C> 4 <R> <C> 10 <C> 53 <C> 45 <C> 8 <R> <C> 20 <C> 49 <C> 44 <C> 14 <R> <C> 50 <C> 46.1 <C> 43.7 <C> 34 <CAP> Table 3: The test perplexities of an LSTM-2048-512 trained with different losses versus number of epochs. The model needs about 40 minutes per epoch. First epoch is a bit slower because we slowly increase the number of workers.
<R> <C> [EMPTY] <C> Precision <C> Recall <C> F1 <R> <C> Positive <C> 0.01 <C> 0.11 <C> 0.02 <R> <C> Decision Tree <C> 0.87 <C> 0.86 <C> 0.87 <R> <C> MLP <C> 0.90 <C> 0.91 <C> 0.90 <CAP> Table 2: Results on pivotal events classification.
<R> <C> Model <C> Param (Zh-En) <C> IWSLT <C> WMT Zh-En <R> <C> Deng et al. ( 2018 ) <C> - <C> 33.1 <C> - <R> <C> Hassan et al. ( 2018 ) <C> - <C> - <C> 24.2 <R> <C> Self-attention baseline <C> 292M <C> 34.4 <C> 23.8 <R> <C> LightConv <C> 285M <C> 34.8 <C> 24.3 <R> <C> DynamicConv <C> 296M <C> [BOLD] 35.2 <C> [BOLD] 24.4 <CAP> Table 2: Machine translation accuracy in terms of BLEU on IWSLT and WMT Zh-En.
<R> <C> [BOLD] W <C> [BOLD] BLOCK-RECURRENCE <C> [BOLD] PER <R> <C> 15 <C> No <C> 34.3 <R> <C> 15 <C> Yes <C> 20.6 <CAP> Table 1: Impact of maintaining recurrent state of transducer across blocks on the PER. This table shows that maintaining the state of the transducer across blocks leads to much better results. For this experiment, a block size (W) of 15 frames was used. The reported number is the median of three different runs.
<R> <C> [BOLD] # of layers in encoder / transducer <C> [BOLD] 1 <C> [BOLD] 2 <C> [BOLD] 3 <C> [BOLD] 4 <R> <C> [BOLD] 2 <C> [EMPTY] <C> 19.2 <C> 18.9 <C> 18.8 <R> <C> [BOLD] 3 <C> [EMPTY] <C> 18.5 <C> [BOLD] 18.2 <C> 19.4 <CAP> Table 2: Impact of architecture on PER. Table shows the PER on the dev set as function of the number of layers (2 or 3) in the encoder and the number of layers in the transducer (1-4).
<R> <C> Model <C> Dataset <C> BLEU <C> ROUGE-1 <C> ROUGE-2 <C> Coherence(L) <R> <C> Standard <C> Hotel Review <C> 0.241 <C> 0.571 <C> 0.302 <C> 1.92 <R> <C> Hierarchical <C> Hotel Review <C> 0.267 <C> 0.590 <C> 0.330 <C> 1.71 <R> <C> Hierarchical+Attention <C> Hotel Review <C> 0.285 <C> 0.624 <C> 0.355 <C> 1.57 <R> <C> Standard <C> Wikipedia <C> 0.178 <C> 0.502 <C> 0.228 <C> 2.75 <R> <C> Hierarchical <C> Wikipedia <C> 0.202 <C> 0.529 <C> 0.250 <C> 2.30 <R> <C> Hierarchical+Attention <C> Wikipedia <C> 0.220 <C> 0.544 <C> 0.291 <C> 2.04 <CAP> Table 3: Results for three models on two datasets. As with coherence score L, smaller values signifies better performances.
<R> <C> Approche <C> Issue Précision <C> Issue Rappel <C> Issue F1 <R> <C> Cas clinique entier <C> 0.5285 <C> 0.5199 <C> 0.5241 <R> <C> Empan de l’issue obtenue par étiquetage ( [ITALIC] run-1) <C> 0.5198 <C> 0.4918 <C> 0.5054 <R> <C> Traitement linguistique de  [ITALIC] décès et quatre dernières phrases <C> [BOLD] 0.5985 <C> [BOLD] 0.5831 <C> [BOLD] 0.5906 <CAP> Table 9: Scores de précision, rappel et F-mesure pour l’issue sur le jeu de test. Le première approche considère l’ensemble du cas clinique. Une seconde approche utilise uniquement l’empan "Issue" obtenu par étiquetage automatique. Enfin, une dernière approche utilise un traitement linguistique pour la détection de l’issue "décès", tandis que les quatre dernières phrases sont considérés pour prédire les autres issues.
<R> <C> [BOLD] Rating Type <C> [BOLD] Agreement <R> <C> [BOLD] Fine-grained <C> 0.13 <R> <C> [BOLD] Coarse-grained <C> 0.22 <CAP> Table 7: Cohen’s Kappa scores on the overall quality ratings between the expert and interactive evaluation.
<R> <C> [BOLD] Feature <C> [BOLD] Micro-accuracy  [BOLD] TC2014 <C> [BOLD] Micro-accuracy  [BOLD] TC2015 <C> [BOLD] Micro-accuracy  [BOLD] Diff. <C> [BOLD] Macro-accuracy  [BOLD] TC2014 <C> [BOLD] Macro-accuracy  [BOLD] TC2015 <C> [BOLD] Macro-accuracy  [BOLD] Diff. <C> [BOLD] MSE  [BOLD] TC2014 <C> [BOLD] MSE  [BOLD] TC2015 <C> [BOLD] MSE  [BOLD] Diff. <R> <C> content <C> 0.503 <C> [BOLD] 0.588 <C> [BOLD] +16.9% <C> 0.188 <C> 0.264 <C> [BOLD] +40.4% <C> 1404.002 <C> [BOLD] 1148.264 <C> [BOLD] -18.2% <R> <C> description <C> 0.322 <C> 0.325 <C> +0.9% <C> 0.096 <C> 0.095 <C> -1.0% <C> 1870.311 <C> 1868.584 <C> -0.1% <R> <C> name <C> 0.232 <C> 0.232 <C> +0.0% <C> 0.086 <C> 0.081 <C> -5.8% <C> 2186.904 <C> 2190.848 <C> +0.2% <R> <C> offset <C> 0.267 <C> 0.233 <C> -12.7% <C> 0.048 <C> 0.039 <C> -18.8% <C> 2096.595 <C> 2173.044 <C> +3.6% <R> <C> tlang <C> [BOLD] 0.568 <C> 0.536 <C> -5.6% <C> 0.107 <C> 0.088 <C> -17.8% <C> [BOLD] 1156.279 <C> 1262.012 <C> +9.1% <R> <C> tz <C> 0.304 <C> 0.318 <C> +4.6% <C> 0.123 <C> 0.118 <C> -4.1% <C> 2013.270 <C> 1946.919 <C> -3.3% <R> <C> ulang <C> 0.547 <C> 0.525 <C> -4.0% <C> 0.076 <C> 0.069 <C> -9.2% <C> 1354.614 <C> 1468.346 <C> +8.4% <R> <C> uloc <C> 0.438 <C> 0.499 <C> +13.9% <C> [BOLD] 0.374 <C> [BOLD] 0.370 <C> -1.1% <C> 1669.383 <C> 1434.115 <C> -14.1% <CAP> TABLE III: Classification results with a Maximum Entropy classifier on a single feature for all the countries in TC2014 and TC2015. The last column, “Diff.”, shows the relative difference in performance for each of these datasets.
<R> <C> [BOLD] Theme <C> CInd-D <C> CInd-LL <R> <C> IncompSent <C> 2.37 <C> 3.39 <R> <C> ImpAnaphora <C> 4.61 <C> 5.80 <R> <C> PNAnaphora <C> 9.39 <C> 11.0 <R> <C> AdvAnaphora1 <C> 3.59 <C> 2.93 <R> <C> AdvAnaphora2 <C> 9.95 <C> 3.74 <R> <C> StructConn <C> 3.70 <C> 0.92 <R> <C> CEQAnswer <C> 0.37 <C> 2.59 <R> <C> [BOLD] Total <C> [BOLD] 33.35 <C> [BOLD] 26.74 <CAP> Table 4: Percentage of sentences with a predicted theme in the CInd datasets.
<R> <C> Models <C> [BOLD] Accuracy <C> Precision <C> Recall <C> [ITALIC] F1 score <R> <C> CNN-Kim(kim2014convolutional) <C> 0.5785 <C> 0.6371 <C> 0.6745 <C> 0.6553 <R> <C> CNN-CR(qu2019user) 1 <C> 0.6354 <C> 0.7108 <C> 0.6952 <C> 0.7029 <R> <C> CRNN ( [ITALIC] v1) w/ LSTM <C> 0.6668* <C> 0.7238 <C> 0.7297 <C> 0.7267 <R> <C> CRNN ( [ITALIC] v1) w/ GRU <C> 0.6543* <C> 0.7056 <C> 0.7065 <C> 0.7061 <R> <C> CRNN ( [ITALIC] v2) w/ LSTM <C> 0.6731**** <C> 0.7315 <C> 0.7315 <C> 0.7315 <R> <C> CRNN ( [ITALIC] v2) w/ GRU <C> 0.6734** <C> 0.7280 <C> 0.7334 <C> 0.7307 <R> <C> CRNN ( [ITALIC] v3) w/ LSTM <C> [BOLD] 0.6822**** <C> 0.7254 <C> [BOLD] 0.7422 <C> [BOLD] 0.7337 <R> <C> CRNN ( [ITALIC] v3) w/ GRU <C> 0.6733*** <C> [BOLD] 0.7358 <C> 0.7215 <C> 0.7286 <CAP> Table 2. Performance of CNN-Kim, CNN-CR, and CRNN.2
<R> <C> # of ref DAs <C> % <C> Mean accuracy CRNN ( [ITALIC] v3) <C> Mean accuracy CNN-CR <C> Avg. num. of pred DAs CRNN ( [ITALIC] v3) <C> Avg. num. of pred DAs CNN-CR <R> <C> 1 <C> 36.9 <C> [BOLD] 0.7704** <C> 0.7126 <C> 1.44 <C> 1.44 <R> <C> 2 <C> 42.8 <C> [BOLD] 0.6641*** <C> 0.6232 <C> [BOLD] 2.02** <C> 1.89 <R> <C> 3 <C> 16.7 <C> [BOLD] 0.5596* <C> 0.5177 <C> [BOLD] 2.56*** <C> 2.37 <R> <C> ≥4 <C> 3.6 <C> [BOLD] 0.5618 <C> 0.5339 <C> [BOLD] 2.68 <C> 2.74 <CAP> Table 3. Mean accuracy and the average number of predicted DAs grouped by the number of reference DAs.2 The percentage indicates the frequency of each DA group in the test set.
<R> <C> [EMPTY] <C> WSJ (WER) <C> AISHELL-1 (CER) <R> <C> Batch processing <C> Batch processing <C> [EMPTY] <R> <C> biLSTM  <C> 6.7 <C> 9.2 <R> <C> uniLSTM <C> 8.4 <C> 11.8 <R> <C> Transformer  <C> 4.9 <C> 6.7 <R> <C> CBP Enc. + Batch Dec.  <C> 6.0 <C> 7.6 <R> <C> Online processing <C> Online processing <C> [EMPTY] <R> <C> CBP Enc. + median Dec.  <C> 9.9 <C> 25.0 <R> <C> — [ITALIC] with past frames <C> 7.9 <C> 24.2 <R> <C> CBP Enc. + Proposed Dec. <C> 8.8 <C> 18.7 <R> <C> — [ITALIC] with past frames <C> [BOLD] 6.6 <C> [BOLD] 9.7 <CAP> Table 1: Word error rates (WERs) in the WSJ and AISHELL-1 evaluation task.
<R> <C> [EMPTY] <C> (a) Standard HLM vs. Homogeneous variance <C> (a) Standard HLM vs. Homogeneous variance <C> (b) Percolation vs. Homogeneous variance <C> (b) Percolation vs. Homogeneous variance <C> (c) Homogeneous variance vs. Heterogeneous variance <C> (c) Homogeneous variance vs. Heterogeneous variance <R> <C> [EMPTY] <C> mixture model <C> mixture model <C> mixture model <C> mixture model <C> mixture model <C> mixture model <R> <C> Study <C> elpd_diff <C> SE <C> elpd_diff <C> SE <C> elpd_diff <C> SE <R> <C> 1 <C> -0.29 <C> 1.67 <C> 29.55 <C> 6.97 <C> 0.57 <C> 1.09 <R> <C> 2 <C> 56.98 <C> 13.57 <C> 76.34 <C> 14.26 <C> 15.20 <C> 6.07 <R> <C> 3 <C> 97.62 <C> 16.10 <C> 112.40 <C> 17.43 <C> 57.12 <C> 11.11 <R> <C> 4 <C> 71.29 <C> 14.08 <C> 84.78 <C> 14.12 <C> 19.66 <C> 8.77 <R> <C> 5 <C> 112.74 <C> 18.17 <C> 120.45 <C> 18.56 <C> 63.28 <C> 18.12 <R> <C> 6 <C> 66.84 <C> 12.59 <C> 85.97 <C> 13.88 <C> 43.58 <C> 12.18 <R> <C> 7 <C> 72.45 <C> 13.76 <C> 80.93 <C> 14.72 <C> 80.92 <C> 14.41 <R> <C> 8 <C> 88.50 <C> 14.60 <C> 90.22 <C> 14.77 <C> 40.17 <C> 11.87 <R> <C> 9 <C> 78.35 <C> 14.21 <C> 108.10 <C> 16.04 <C> 26.21 <C> 7.76 <R> <C> 10 <C> 90.08 <C> 14.14 <C> 105.23 <C> 15.02 <C> 33.59 <C> 11.95 <CAP> Table 1: Comparison of the 10 sets of hierarchical models using PSIS-LOO. Shown are the differences in ˆelpd between (a) the standard hierarchical model and the homogeneous variance mixture model; (b) the feature percolation model and the homogeneous variance mixture model; and (c) the homogeneous vs. heterogeneous variance mixture model. Also shown are standard errors for each comparison. If the difference in ˆelpd is positive, this is evidence in favour of the second model. The pairwise model comparisons are transitive. These comparisons show that the heterogeneous variance mixture model has the best predictive performance.
<R> <C> [EMPTY] <C> T1 <C> T2 <C> T11 <C> T15 <C> T16 <R> <C> MemN2N <C> 0.0 <C> 83.0 <C> 84.0 <C> 0.0 <C> 44.0 <R> <C> QA <C> 0.7 <C> 2.7 <C> 0.0 <C> 0.0 <C> 9.8 <R> <C> QA + AE <C> 70.9 <C> 55.1 <C> [BOLD] 100.0 <C> 24.6 <C> [BOLD] 100.0 <R> <C> QA + AE + ST <C> [BOLD] 100.0 <C> [BOLD] 85.3 <C> [BOLD] 100.0 <C> [BOLD] 100.0 <C> [BOLD] 100.0 <CAP> Table 3: Test accuracy on bAbI tasks with auto-encoding (AE) and structure tweak (ST)
<R> <C> [BOLD] System <C> [BOLD] R-1 <C> [BOLD] R-2 <C> [BOLD] R-L <R> <C> RNN <C> 21.50 <C> 8.90 <C> 18.60 <R> <C> RNN-context <C> 29.90 <C> 17.40 <C> 27.20 <R> <C> CopyNet <C> 34.40 <C> 21.60 <C> 31.30 <R> <C> RNN-distract <C> 35.20 <C> 22.60 <C> 32.50 <R> <C> DRGD <C> 36.99 <C> 24.15 <C> 34.21 <R> <C> [BOLD] AC-ABS <C> [BOLD] 37.51 <C> [BOLD] 24.68 <C> [BOLD] 35.02 <CAP> Table 4: ROUGE-F1 on LCSTS
<R> <C> [BOLD] Entity/Action (freq. in test set) <C> [BOLD] MaxEnt <C> [BOLD] BiLSTM <C> [BOLD] BiLSTM + CRF <R> <C> Action (3519) <C> 83.87 <C> 85.95 <C> 86.89 <R> <C> Amount (886) <C> 68.25 <C> 81.59 <C> 82.34 <R> <C> Conc. (273) <C> 56.84 <C> 65.36 <C> 76.36 <R> <C> Device (408) <C> 49.14 <C> 58.73 <C> 64.02 <R> <C> Gen.-Measure (91) <C> 05.88 <C> 06.45 <C> 25.68 <R> <C> Location (1007) <C> 61.07 <C> 69.57 <C> 73.53 <R> <C> Meas.-Type (50) <C> 15.38 <C> 18.75 <C> 21.62 <R> <C> Mention (37) <C> 43.37 <C> 52.31 <C> 57.97 <R> <C> Method (177) <C> 37.97 <C> 30.60 <C> 38.21 <R> <C> Modifier (720) <C> 50.86 <C> 56.90 <C> 59.34 <R> <C> Numerical (129) <C> 39.70 <C> 47.84 <C> 49.80 <R> <C> Reagent (2486) <C> 60.54 <C> 71.34 <C> 74.55 <R> <C> Seal (43) <C> 49.52 <C> 54.05 <C> 66.67 <R> <C> Size (69) <C> 19.35 <C> 24.82 <C> 26.92 <R> <C> Speed (200) <C> 74.88 <C> 85.31 <C> 91.00 <R> <C> Temperature (469) <C> 80.69 <C> 86.68 <C> 91.90 <R> <C> Time (708) <C> 83.68 <C> 92.69 <C> 93.94 <R> <C> pH (21) <C> 41.86 <C> 53.66 <C> 70.00 <R> <C> Macro-avg F1 <C> 49.23 <C> 58.81 <C> 64.44 <R> <C> Micro-avg F1 <C> 68.03 <C> 74.99 <C> 78.03 <CAP> Table 4: F1 scores for segmenting and classifying entities and action triggers compared across the various models.
<R> <C> [BOLD] Layer (Transfer) <C> [BOLD] Channels <C> [BOLD] Filter <C> [BOLD] Stride <C> [BOLD] Act. <R> <C> 1D Conv (✓) <C> 128 <C> 80 <C> 4 <C> LReLU <R> <C> 1D Conv (✓) <C> 128 <C> 25 <C> 2 <C> LReLU <R> <C> 1D Conv (✓) <C> 128 <C> 10 <C> 1 <C> LReLU <R> <C> 1D Conv (✓) <C> 128 <C> 5 <C> 1 <C> LReLU <R> <C> NIN (✓) <C> [128,128] <C> 1 <C> 1 <C> LReLU×2 <R> <C> NIN (×) <C> [128,N] <C> 1 <C> 1 <C> Tanh Identity <CAP> Table 1: Layer setting details for convolutional and NIN layers. Sorted from the input layer to the output layer.
<R> <C> Characterization tasks <C> [ITALIC] Dis2Vec (Exposures) <C> [ITALIC] Dis2Vec (Transmission methods) <C> [ITALIC] Dis2Vec (Transmission agents) <C> [ITALIC] Dis2Vec (Symptoms) <C> [ITALIC] Dis2Vec (full vocabulary) <R> <C> Symptoms <C> 0.597 <C> 0.581 <C> 0.165 <C> 0.883 <C> [BOLD] 0.945 <R> <C> Exposures <C> 0.554 <C> 0.557 <C> 0.315 <C> 0.416 <C> [BOLD] 0.597 <R> <C> Transmission methods <C> 0.748 <C> 0.768 <C> 0.517 <C> 0.455 <C> [BOLD] 0.794 <R> <C> Transmission agents <C> 0.446 <C> 0.459 <C> 0.467 <C> 0.457 <C> [BOLD] 0.516 <CAP> Table 5: Comparative performance evaluation of Dis2Vec with full vocabulary against each of the 6 conditions of Dis2Vec with a truncated vocabulary across the 4 characterization tasks where the truncated vocabulary consists of disease names and all possible terms related to a particular taxonomical category. We use equation 8 as the accuracy metric in this table.
<R> <C> Taxonomy <C> # of entities <C> # of concepts <C> # of  [ITALIC] isA relations <C> precision <R> <C> Chinese WikiTaxonomy <C> 581,616 <C> 79,470 <C> 1,317,956 <C> 97.6% <R> <C> Bigcilin <C> 9,000,000 <C> 70,000 <C> 10,000,000 <C> 90.0% <R> <C> Probase-Tran <C> 404,910 <C> 151,933 <C> 1,819,273 <C> 54.5% <R> <C> [BOLD] CN-Probase <C> [BOLD] 15,066,667 <C> 270,025 <C> [BOLD] 32,925,306 <C> 95.0% <CAP> TABLE I: Comparisons with other taxonomies. ‘-’ represents results that are not provided.
<R> <C> Method <C> C <C> M <C> B-4 <R> <C> Image-Flat  <C> 11.06 <C> 12.82 <C> 7.71 <R> <C> Regions-Hierarchical  <C> 13.52 <C> 15.95 <C> 8.69 <R> <C> RTT-GAN  <C> 20.36 <C> 18.39 <C> 9.21 <R> <C> CapG-RevG  <C> 20.93 <C> 18.62 <C> 9.43 <R> <C> LSTM-ATT <C> 20.17 <C> 17.72 <C> 8.72 <R> <C> CAE-LSTM <C> [BOLD] 25.15 <C> [BOLD] 18.82 <C> [BOLD] 9.67 <CAP> Table 1: Performance of our CAE-LSTM and other state-of-the-art methods on Stanford, where C, M, and B-4 are short for CIDEr, METEOR, and BLEU-4. All values are reported as percentage (%).
<R> <C> [EMPTY] <C> MUC Prec. <C> MUC Rec. <C> MUC F1 <C> B3 Prec. <C> B3 Rec. <C> B3 F1 <C> CEAF [ITALIC] ϕ4 Prec. <C> CEAF [ITALIC] ϕ4 Rec. <C> CEAF [ITALIC] ϕ4 F1 <C> Avg. F1 <R> <C> Our model (ensemble) <C> [BOLD] 81.2 <C> [BOLD] 73.6 <C> [BOLD] 77.2 <C> [BOLD] 72.3 <C> [BOLD] 61.7 <C> [BOLD] 66.6 <C> [BOLD] 65.2 <C> [BOLD] 60.2 <C> [BOLD] 62.6 <C> [BOLD] 68.8 <R> <C> Our model (single) <C> 78.4 <C> 73.4 <C> 75.8 <C> 68.6 <C> 61.8 <C> 65.0 <C> 62.7 <C> 59.0 <C> 60.8 <C> 67.2 <R> <C> Clark and Manning ( 2016a ) <C> 79.2 <C> 70.4 <C> 74.6 <C> 69.9 <C> 58.0 <C> 63.4 <C> 63.5 <C> 55.5 <C> 59.2 <C> 65.7 <R> <C> Clark and Manning ( 2016b ) <C> 79.9 <C> 69.3 <C> 74.2 <C> 71.0 <C> 56.5 <C> 63.0 <C> 63.8 <C> 54.3 <C> 58.7 <C> 65.3 <R> <C> Wiseman et al. ( 2016 ) <C> 77.5 <C> 69.8 <C> 73.4 <C> 66.8 <C> 57.0 <C> 61.5 <C> 62.1 <C> 53.9 <C> 57.7 <C> 64.2 <R> <C> Wiseman et al. ( 2015 ) <C> 76.2 <C> 69.3 <C> 72.6 <C> 66.2 <C> 55.8 <C> 60.5 <C> 59.4 <C> 54.9 <C> 57.1 <C> 63.4 <R> <C> Clark and Manning ( 2015 ) <C> 76.1 <C> 69.4 <C> 72.6 <C> 65.6 <C> 56.0 <C> 60.4 <C> 59.4 <C> 53.0 <C> 56.0 <C> 63.0 <R> <C> Martschat and Strube ( 2015 ) <C> 76.7 <C> 68.1 <C> 72.2 <C> 66.1 <C> 54.2 <C> 59.6 <C> 59.5 <C> 52.3 <C> 55.7 <C> 62.5 <R> <C> Durrett and Klein ( 2014 ) <C> 72.6 <C> 69.9 <C> 71.2 <C> 61.2 <C> 56.4 <C> 58.7 <C> 56.2 <C> 54.2 <C> 55.2 <C> 61.7 <R> <C> Björkelund and Kuhn ( 2014 ) <C> 74.3 <C> 67.5 <C> 70.7 <C> 62.7 <C> 55.0 <C> 58.6 <C> 59.4 <C> 52.3 <C> 55.6 <C> 61.6 <R> <C> Durrett and Klein ( 2013 ) <C> 72.9 <C> 65.9 <C> 69.2 <C> 63.6 <C> 52.5 <C> 57.5 <C> 54.3 <C> 54.4 <C> 54.3 <C> 60.3 <CAP> Table 1: Results on the test set on the English data from the CoNLL-2012 shared task. The final column (Avg. F1) is the main evaluation metric, computed by averaging the F1 of MUC, B3, and CEAFϕ4. We improve state-of-the-art performance by 1.5 F1 for the single model and by 3.1 F1.
<R> <C> [EMPTY] <C> Avg. F1 <C> Δ <R> <C> Our model (joint mention scoring) <C> 67.7 <C> [EMPTY] <R> <C> w/ rule-based mentions <C> 66.7 <C> -1.0 <R> <C> w/ oracle mentions <C> 85.2 <C> +17.5 <CAP> Table 3: Comparisons of of various mention proposal methods with our model on the development data. The rule-based mentions are derived from the mention detector from Raghunathan et al. (2010), resulting in a 1 F1 drop in performance. The oracle mentions are from the labeled clusters and improve our model by over 17.5 F1.
<R> <C> Model <C> holistic <C> TraAtt <C> RegAtt <C> ConAtt <C> [BOLD] SalAtt <R> <C> VGG19 <C> 59.52 <C> 58.39 <C> 59.76 <C> 59.24 <C> [BOLD] 59.86 <R> <C> ResNet <C> 60.30 <C> 59.07 <C> 60.45 <C> 60.25 <C> [BOLD] 60.62 <CAP> Table 1: Comparison of different models using two CNN features on COCO-VQA validation set, where SalAtt is our proposed model. (accuracy in %)
<R> <C> [EMPTY] <C> [BOLD] XWN  [ITALIC] Precision <C> [BOLD] XWN  [ITALIC] Recall <C> [BOLD] XWN  [ITALIC] F1-Score <C> [BOLD] SemCor  [ITALIC] Precision <C> [BOLD] SemCor  [ITALIC] Recall <C> [BOLD] SemCor  [ITALIC] F1-Score <R> <C> Random <C> 71.82 <C> 72.04 <C> 71.93 <C> 61.52 <C> 62.52 <C> 62.02 <R> <C> FO Tagging <C> [BOLD] 89.68 <C> [BOLD] 89.74 <C> [BOLD] 89.71 <C> [BOLD] 86.10 <C> [BOLD] 86.36 <C> [BOLD] 86.23 <R> <C> SuperSense Tagging <C> - <C> - <C> - <C> 76.65 <C> 77.71 <C> 77.18 <CAP> Table 2: Evaluation results
<R> <C> Def. <C> Len. <C> Grammar G1 <C> Grammar G2 <C> Grammar G3 <C> Grammar G4 <C> Grammar G5 <C> Grammar G6 <C> Grammar G7 <R> <C> [ITALIC] HN <C> 8 <C> 0.12 <C> 0.12 <C> 0.87 <C> 0.87 <C> 0.88 <C> 0.85 <C> 0.86 <R> <C> [ITALIC] HN <C> 10 <C> 0.10 <C> 0.10 <C> 0.88 <C> 0.89 <C> 0.90 <C> 0.88 <C> 0.82 <R> <C> [ITALIC] HN <C> 12 <C> 0.08 <C> 0.08 <C> 0.88 <C> 0.91 <C> 0.92 <C> 0.90 <C> 0.76 <R> <C> [ITALIC] HN <C> 14 <C> 0.07 <C> 0.07 <C> 0.88 <C> 0.92 <C> 0.93 <C> 0.92 <C> 0.70 <R> <C> [ITALIC] DN <C> 8 <C> 2.51 <C> 2.51 <C> 1.13 <C> 1.16 <C> 1.00 <C> 1.00 <C> 1.17 <R> <C> [ITALIC] DN <C> 10 <C> 3.00 <C> 3.00 <C> 1.18 <C> 1.16 <C> 1.00 <C> 1.00 <C> 1.31 <R> <C> [ITALIC] DN <C> 12 <C> 3.50 <C> 3.50 <C> 1.24 <C> 1.18 <C> 1.00 <C> 1.00 <C> 1.51 <R> <C> [ITALIC] DN <C> 14 <C> 4.00 <C> 4.00 <C> 1.30 <C> 1.22 <C> 1.00 <C> 1.00 <C> 1.75 <CAP> TABLE II: Entropy & avg. edit distance for Tomita grammars.
<R> <C> Tokens <C> [ITALIC] Baseline <C> [BOLD] Ambig. 41.0 <C> [BOLD] Unseen 26.6 <C> [BOLD] All 31.0 <R> <C> Tokens <C> [ITALIC] Lemming <C> 38.2 <C> 48.3 <C> 50.6 <R> <C> Tokens <C> [ITALIC] HMAM <C> 41.4 <C> 50.2 <C> 52.1 <R> <C> Tokens <C> [ITALIC] Lematus 0-ch <C> 39.9 <C> 43.7 <C> 46.8 <R> <C> Tokens <C> [ITALIC] Lematus 20-ch <C> 38.4 <C> 42.8 <C> 45.8 <R> <C> Types <C> [ITALIC] Baseline <C> 45.0 <C> 26.6 <C> 32.4 <R> <C> Types <C> [ITALIC] Lemming <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Types <C> [ITALIC] HMAM <C> 41.8 <C> 53.7 <C> 56.3 <R> <C> Types <C> [ITALIC] Lematus 0-ch <C> 42.5 <C> 53.7 <C> 55.1 <R> <C> Types <C> [ITALIC] Lematus 20-ch <C> 43.1 <C> 51.7 <C> 54.9 <CAP> Table 2: Average type level lemmatization exact match accuracy on five development languages in type and token based training data scenarios. Colour-scale is computed over the whole Ambig. column and over all but Baseline rows for the other columns.
<R> <C> [EMPTY] <C> BLEU ↑ <C> Meteor ↑ <C> TER ↓ <R> <C> ∙LIUMCVC_MNMT_C <C> 33.4 <C> 54.0 <C> 48.5 <R> <C> ∙NICT_NMTrerank_C <C> 31.9 <C> 53.9 <C> 48.1 <R> <C> ∙LIUMCVC_NMT_C <C> 33.2 <C> 53.8 <C> 48.2 <R> <C> ∙UvA-TiCC_IMAGINATION_U <C> 33.3 <C> 53.5 <C> 47.5 <R> <C> UvA-TiCC_IMAGINATION_C <C> 30.2 <C> 51.2 <C> 50.8 <R> <C> CUNI_NeuralMonkeyTextualMT_U <C> 31.1 <C> 51.0 <C> 50.7 <R> <C> OREGONSTATE_2NeuralTranslation_C <C> 31.0 <C> 50.6 <C> 50.7 <R> <C> DCU-ADAPT_MultiMT_C <C> 29.8 <C> 50.5 <C> 52.3 <R> <C> CUNI_NeuralMonkeyMultimodalMT_U <C> 29.5 <C> 50.2 <C> 52.5 <R> <C> CUNI_NeuralMonkeyTextualMT_C <C> 28.5 <C> 49.2 <C> 54.3 <R> <C> OREGONSTATE_1NeuralTranslation_C <C> 29.7 <C> 48.9 <C> 51.6 <R> <C> CUNI_NeuralMonkeyMultimodalMT_C <C> 25.8 <C> 47.1 <C> 56.3 <R> <C> SHEF_ShefClassInitDec_C <C> 25.0 <C> 44.5 <C> 53.8 <R> <C> SHEF_ShefClassProj_C <C> 24.2 <C> 43.4 <C> 55.9 <R> <C> Baseline (text-only NMT) <C> 19.3 <C> 41.9 <C> 72.2 <R> <C> AFRL-OHIOSTATE-MULTIMODAL_U <C> 6.5 <C> 20.2 <C> 87.4 <CAP> Table 4: Official results for the WMT17 Multimodal Machine Translation task on the English-German Multi30K 2017 test data. Systems with grey background indicate use of resources that fall outside the constraints provided for the shared task.
<R> <C> [EMPTY] <C> BLEU ↑ <C> Meteor ↑ <C> TER ↓ <R> <C> ∙LIUMCVC_MNMT_C <C> 55.9 <C> 72.1 <C> 28.4 <R> <C> ∙NICT_NMTrerank_C <C> 55.3 <C> 72.0 <C> 28.4 <R> <C> DCU-ADAPT_MultiMT_C <C> 54.1 <C> 70.1 <C> 30.0 <R> <C> LIUMCVC_NMT_C <C> 53.3 <C> 70.1 <C> 31.7 <R> <C> OREGONSTATE_2NeuralTranslation_C <C> 51.9 <C> 68.3 <C> 32.7 <R> <C> OREGONSTATE_1NeuralTranslation_C <C> 51.0 <C> 67.2 <C> 33.6 <R> <C> CUNI_NeuralMonkeyMultimodalMT_C <C> 49.9 <C> 67.2 <C> 34.3 <R> <C> CUNI_NeuralMonkeyTextualMT_C <C> 50.3 <C> 67.0 <C> 33.6 <R> <C> Baseline (text-only NMT) <C> 44.3 <C> 63.1 <C> 39.6 <R> <C> *SHEF_ShefClassInitDec_C <C> 45.0 <C> 62.8 <C> 38.4 <R> <C> SHEF_ShefClassProj_C <C> 43.6 <C> 61.5 <C> 40.5 <CAP> Table 6: Results for the Multimodal Translation task on the English-French Multi30K Test 2017 data.
<R> <C> [EMPTY] <C> BLEU ↑ <C> Meteor ↑ <C> TER ↓ <R> <C> ∙LIUMCVC_MNMT_C <C> 45.9 <C> 65.9 <C> 34.2 <R> <C> ∙NICT_NMTrerank_C <C> 45.1 <C> 65.6 <C> 34.7 <R> <C> ∙DCU-ADAPT_MultiMT_C <C> 44.5 <C> 64.1 <C> 35.2 <R> <C> OREGONSTATE_2NeuralTranslation_C <C> 44.1 <C> 63.8 <C> 36.7 <R> <C> LIUMCVC_NMT_C <C> 43.6 <C> 63.4 <C> 37.4 <R> <C> CUNI_NeuralMonkeyTexutalMT_C <C> 43.0 <C> 62.5 <C> 38.2 <R> <C> CUNI_NeuralMonkeyMultimodalMT_C <C> 42.9 <C> 62.5 <C> 38.2 <R> <C> OREGONSTATE_1NeuralTranslation_C <C> 41.2 <C> 61.6 <C> 37.8 <R> <C> SHEF_ShefClassInitDec_C <C> 37.2 <C> 57.3 <C> 42.4 <R> <C> *SHEF_ShefClassProj_C <C> 36.8 <C> 57.0 <C> 44.5 <R> <C> Baseline (text-only NMT) <C> 35.1 <C> 55.8 <C> 45.8 <CAP> Table 7: Results for the Multimodal Translation task on the English-French Ambiguous COCO dataset.
<R> <C> [EMPTY] <C> dev <C> test <R> <C> Majority baseline <C> 50.0 <C> 50.0 <R> <C> State-of-the-art <C> – <C> 73.5 <R> <C> training data <C> [EMPTY] <C> [EMPTY] <R> <C> SNLI <C> 47.1 <C> 46.0 <R> <C> SciTail <C> 60.5 <C> 60.2 <CAP> Table 5: Train on different TE datasets, test accuracy on two-way RTE-5. State-of-the-art refers to Iftene and Moruz (2009)
<R> <C> Approaches <C> 10% <C> 20% <C> 50 <C> 100 <R> <C> Lead <C> 62.9 <C> 72.8 <C> 63.2 <C> 73.6 <R> <C> TextRank <C> 65.2 <C> 76.6 <C> 66.1 <C> 75.4 <R> <C> ILP <C> 66.7 <C> 79.5 <C> 66.9 <C> 79.7 <R> <C> SummCoder <C> 68.8 <C> 82.3 <C> 69.0 <C> 82.5 <R> <C> Ours <C> 71.1 <C> 85.7 <C> 72.6 <C> 86.3 <CAP> Table 4: Evaluation results on SogouCA dataset
<R> <C> No. <C> Model <C> R-L <C> C-F1 <C> Output <R> <C> - <C> Reference <C> - <C> - <C> watch and learn how to tie thread to a hook to help with fly tying as explained by out expert in this free how - to video on fly tying tips and techniques . <R> <C> 8 <C> Ground-truth text + Action Feat. <C> 54.9 <C> 48.9 <C> learn from our expert how to attach thread to fly fishing for fly fishing in this free how - to video on fly tying tips and techniques . <R> <C> 5a <C> Text-only (Ground-truth) <C> 53.9 <C> 47.4 <C> learn from our expert how to tie a thread for fly fishing in this free how - to video on fly tying tips and techniques . <R> <C> 9 <C> ASR output + Action Feat. <C> 46.3 <C> 34.7 <C> learn how to tie a fly knot for fly fishing in this free how-to video on fly tying tips and techniques . <R> <C> 5c <C> ASR output <C> 46.1 <C> 34.7 <C> learn tips and techniques for fly fishing in this free fishing video on techniques for and making fly fishing nymphs . <R> <C> 7 <C> Action Features + RNN <C> 46.3 <C> 34.9 <C> learn about the equipment needed for fly tying , as well as other fly fishing tips from our expert in this free how - to video on fly tying tips and techniques . <R> <C> 6 <C> Action Features only <C> 38.5 <C> 24.8 <C> learn from our expert how to do a double half hitch knot in this free video clip about how to use fly fishing . <R> <C> 2b <C> Next Neighbor <C> 31.8 <C> 17.9 <C> use a sheep shank knot to shorten a long piece of rope . learn how to tie sheep shank knots for shortening rope in this free knot tying video from an eagle scout . <R> <C> 1 <C> Random Baseline <C> 27.5 <C> 8.3 <C> learn tips on how to play the bass drum beat variation on the guitar in this free video clip on music theory and guitar lesson . <CAP> Table A2: Example outputs of ground-truth text-and-video with hierarchical attention (8), text-only with ground-truth (5a), text-only with ASR output (5c), ASR output text-andv-video with hierarchical attention (9), action features with RNN (7) and action features only (6) models compared with the reference, the topic-based next neighbor (2b) and random baseline (1). Arranged in the order of best to worst summary in this table.
<R> <C> [EMPTY] <C> % of Labeled Data 10 <C> % of Labeled Data 20 <C> % of Labeled Data 30 <R> <C> Supervised CRF <C> 86.07 <C> 87.69 <C> 88.64 <R> <C> Self-trained CRF <C> 86.34 <C> 87.73 <C> 88.64 <R> <C> Semi-supervised CRF <C> 87.72 <C> 88.75 <C> 89.12 <CAP> Table 2: Comparison of training results. Slot/Value F-score in %.
<R> <C> Approach <C> Precision <C> Recall <C> F1-score <R> <C> KEA <C> 0.146 <C> 0.296 <C> 0.179 <R> <C> Maui <C> 0.191 <C> 0.375 <C> 0.235 <R> <C> KpMiner <C> 0.165 <C> 0.357 <C> 0.220 <R> <C> PositionRank <C> 0.202 <C> 0.362 <C> 0.240 <R> <C> SurfKE <C> [BOLD] 0.220 <C> [BOLD] 0.390 <C> [BOLD] 0.260 <CAP> Table 1: The comparison of SurfKE with baselines.
<R> <C> Dim.  [ITALIC] d <C> Algorithm <C> ws-sim <C> ws-rel <C> ws-full <C> simlex <C> men <R> <C> [ITALIC] d=100 <C> SGD-SGNS <C> 0.719 <C> 0.570 <C> 0.662 <C> 0.288 <C> 0.645 <R> <C> [ITALIC] d=100 <C> SVD-SPPMI <C> 0.722 <C> 0.585 <C> 0.669 <C> 0.317 <C> [BOLD] 0.686 <R> <C> [ITALIC] d=100 <C> RO-SGNS <C> [BOLD] 0.729 <C> [BOLD] 0.597 <C> [BOLD] 0.677 <C> [BOLD] 0.322 <C> 0.683 <R> <C> [ITALIC] d=200 <C> SGD-SGNS <C> 0.733 <C> 0.584 <C> 0.677 <C> 0.317 <C> 0.664 <R> <C> [ITALIC] d=200 <C> SVD-SPPMI <C> 0.747 <C> 0.625 <C> 0.694 <C> 0.347 <C> [BOLD] 0.710 <R> <C> [ITALIC] d=200 <C> RO-SGNS <C> [BOLD] 0.757 <C> [BOLD] 0.647 <C> [BOLD] 0.708 <C> [BOLD] 0.353 <C> 0.701 <R> <C> [ITALIC] d=500 <C> SGD-SGNS <C> 0.738 <C> 0.600 <C> 0.688 <C> 0.350 <C> 0.712 <R> <C> [ITALIC] d=500 <C> SVD-SPPMI <C> 0.765 <C> 0.639 <C> 0.707 <C> 0.380 <C> [BOLD] 0.737 <R> <C> [ITALIC] d=500 <C> RO-SGNS <C> [BOLD] 0.767 <C> [BOLD] 0.654 <C> [BOLD] 0.715 <C> [BOLD] 0.383 <C> 0.732 <CAP> Table 2: Comparison of the methods in terms of the semantic similarity task. Each entry represents the Spearman’s correlation between predicted similarities and the manually assessed ones.
<R> <C> Method <C> ModCloth dataset <C> RTR dataset <R> <C> Majority Class <C> 0.6892 <C> 0.7396 <R> <C> TF-IDF LR <C> 0.7899 <C> 0.8033 <R> <C> Mean GloVe LR <C> 0.7124 <C> 0.7471 <R> <C> ULMFit Fine-Tuned <C> [BOLD] 0.8269 <C> [BOLD] 0.8420 <R> <C> BERT Fine-Tuned <C> 0.8113 <C> - * <CAP> Table 2. Micro-F1 scores on the test set for the text classification problem on two different datasets. * We stopped fine tuning BERT on the RTR dataset after the first epoch, which already took 5 hours on a NVIDIA Tesla K80.
<R> <C> Model <C> Maximum Likelihood B-4 <C> Maximum Likelihood M <C> Maximum Likelihood R-L <C> Maximum Likelihood C <C> Maximum Likelihood S <C> REINFORCE (CIDEr) B-4 <C> REINFORCE (CIDEr) M <C> REINFORCE (CIDEr) R-L <C> REINFORCE (CIDEr) C <C> REINFORCE (CIDEr) S <R> <C> LSTM-A Yao et al. ( 2017 ) <C> 35.2 <C> 26.9 <C> 55.8 <C> 108.8 <C> 20.0 <C> 35.5 <C> 27.3 <C> 56.8 <C> 118.3 <C> 20.8 <R> <C> StackCap Gu et al. ( 2018 ) <C> 35.2 <C> 26.5 <C> - <C> 109.1 <C> - <C> 36.1 <C> 27.4 <C> 56.9 <C> 120.4 <C> 20.9 <R> <C> FC Vinyals et al. ( 2015 ) <C> 32.9 <C> 25.0 <C> [BOLD] 54.0 <C> 95.4 <C> 17.9 <C> 32.8 <C> 25.0 <C> 54.2 <C> 104.0 <C> [BOLD] 18.5 <R> <C> FC + HSG <C> [BOLD] 33.2 <C> [BOLD] 25.5 <C> 53.9 <C> [BOLD] 96.1 <C> [BOLD] 18.3 <C> [BOLD] 33.9 <C> [BOLD] 25.9 <C> [BOLD] 54.8 <C> [BOLD] 107.5 <C> 18.4 <R> <C> Up-Down Anderson et al. ( 2018 ) <C> [BOLD] 36.0 <C> 27.0 <C> 56.3 <C> 113.1 <C> 20.4 <C> 36.3 <C> 27.5 <C> 56.8 <C> 120.7 <C> 21.4 <R> <C> Up-Down + HSG <C> 35.6 <C> [BOLD] 27.3 <C> [BOLD] 56.7 <C> [BOLD] 113.9 <C> [BOLD] 20.6 <C> [BOLD] 37.4 <C> [BOLD] 28.0 <C> [BOLD] 57.7 <C> [BOLD] 124.0 <C> [BOLD] 21.5 <CAP> Table 1: Automatic evaluation comparisons with various baseline caption decoders on the Karpathy test set. “HSG” denotes trained with hidden state guidance, B-4, M, R-L, C and S are short hands for BLEU-4, METEOR, ROUGE-L, CIDEr and SPICE. All captions are generated with beam size 5.
<R> <C> English-German Model <C> English-German Precision <C> English-German Recall <C> English-German Matched Brackets <C> English-German Cross Brackets <C> English-German Tag Accuracy <R> <C> Recurrent <C> 0.38 <C> 0.38 <C> 0.42 <C> 0.31 <C> 0.46 <R> <C> Transformer <C> 0.31 <C> 0.31 <C> 0.35 <C> 0.28 <C> 0.40 <R> <C> German-English <C> German-English <C> German-English <C> German-English <C> German-English <C> German-English <R> <C> Model <C> Precision <C> Recall <C> Matched Brackets <C> Cross Brackets <C> Tag Accuracy <R> <C> Recurrent <C> 0.12 <C> 0.12 <C> 0.30 <C> 0.80 <C> 0.32 <R> <C> Transformer <C> 0.11 <C> 0.11 <C> 0.28 <C> 0.77 <C> 0.31 <CAP> Table 4: Average parse tree similarity (PARSEVAL scores) between word occurrences and their nearest neighbors. Note that the apparent identity of precision and recall values is due to rounding and the very close number of constituents in the corresponding parse tree of words (gold parse trees) and the corresponding parse trees of their nearest neighbors (candidate parse trees).
<R> <C> [BOLD] Model <C> Layers <C> BLEU <R> <C> Transformer <C> [EMPTY] <C> 27.31 <R> <C> [ITALIC] w/ Dynamic Fusion <C> Embedding <C> 27.94 <R> <C> [ITALIC] w/ Dynamic Fusion <C> 1st-5th <C> 28.42 <R> <C> [ITALIC] w/ Dynamic Fusion <C> Output <C> 28.34 <R> <C> [ITALIC] w/ Dynamic Fusion <C> All <C> 28.77 <R> <C> [ITALIC] w/ Knowledge Distillation <C> Embedding <C> 28.21 <R> <C> [ITALIC] w/ Knowledge Distillation <C> 1st-5th <C> 28.01 <R> <C> [ITALIC] w/ Knowledge Distillation <C> Output <C> 28.68 <R> <C> [ITALIC] w/ Knowledge Distillation <C> All <C> 28.22 <CAP> Table 5: The comparison of employing dynamic fusion mechanism and knowledge distillation paradigm on different layers on the EN→DE task.
<R> <C> [EMPTY] <C> Categorical Cross Entropy training Pos <C> Categorical Cross Entropy training Neu <C> Categorical Cross Entropy training Neg <C> Ordinal Cross Entropy training Pos <C> Ordinal Cross Entropy training Neu <C> Ordinal Cross Entropy training Neg <R> <C> [EMPTY] <C> HI_EN,TW <C> HI_EN,TW <C> HI_EN,TW <C> HI_EN,TW <C> HI_EN,TW <C> HI_EN,TW <R> <C> CSGen <C> 0.52 <C> 0.62 <C> 0.38 <C> 0.55 <C> 0.63 <C> 0.34 <R> <C> Gold <C> 0.48 <C> 0.63 <C> 0.24 <C> 0.50 <C> 0.62 <C> 0.35 <R> <C> [EMPTY] <C> HI_EN,FB <C> HI_EN,FB <C> HI_EN,FB <C> HI_EN,FB <C> HI_EN,FB <C> HI_EN,FB <R> <C> CSGen <C> 0.59 <C> 0.73 <C> 0.56 <C> 0.62 <C> 0.71 <C> 0.55 <R> <C> Gold <C> 0.60 <C> 0.74 <C> 0.54 <C> 0.60 <C> 0.71 <C> 0.44 <R> <C> [EMPTY] <C> ES_EN <C> ES_EN <C> ES_EN <C> ES_EN <C> ES_EN <C> ES_EN <R> <C> CSGen <C> 0.38 <C> 0.53 <C> 0.37 <C> 0.48 <C> 0.50 <C> 0.42 <R> <C> Gold <C> 0.47 <C> 0.44 <C> 0.41 <C> 0.40 <C> 0.53 <C> 0.43 <R> <C> [EMPTY] <C> BN_EN <C> BN_EN <C> BN_EN <C> BN_EN <C> BN_EN <C> BN_EN <R> <C> CSGen <C> 0.63 <C> 0.49 <C> 0.58 <C> 0.55 <C> 0.47 <C> 0.59 <R> <C> Gold <C> 0.55 <C> 0.51 <C> 0.65 <C> 0.37 <C> 0.49 <C> 0.61 <CAP> Table 3: F1 score for each class prediction. Blue: CSGen is better than Gold.
<R> <C> Models <C> [ITALIC] model1 <C> [ITALIC] model2 <C> [ITALIC] model3 <R> <C> Training loss <C> 0.82 <C> 1.01 <C> 1.72 <R> <C> Number of epochs <C> 50 <C> 30 <C> 19 <R> <C> BLEU score <C> 24.5/9.0/3.2/1.3 <C> 26.0/9.7/3.6/1.5 <C> 26.4/10.1/3.8/1.6 <R> <C> METEOR score <C> 23.0 <C> 23.9 <C> 23.9 <CAP> Table 1: Results for the generated stories. The loss is calculated over the training set and the METEOR and BLEU score are calucated over the test set.
<R> <C> [BOLD] %training labels <C> [BOLD] 10% <C> [BOLD] 30% <C> [BOLD] 50% <C> [BOLD] 70% <R> <C> [BOLD] LINE <C> 53.9 <C> 56.7 <C> 58.8 <C> 60.1 <R> <C> [BOLD] TADW <C> 71.0 <C> 71.4 <C> 75.9 <C> 77.2 <R> <C> [BOLD] CANE <C> 81.6 <C> 82.8 <C> 85.2 <C> 86.3 <R> <C> [BOLD] DMTE <C> 81.8 <C> 83.9 <C> 86.3 <C> 87.9 <R> <C> [BOLD] WANE <C> 81.9 <C> 83.9 <C> 86.4 <C> 88.1 <R> <C> [BOLD] GANE-OT <C> 82.0 <C> 84.1 <C> 86.6 <C> 88.3 <R> <C> [BOLD] GANE-AP <C> [BOLD] 82.3 <C> [BOLD] 84.2 <C> [BOLD] 86.7 <C> [BOLD] 88.5 <CAP> Table 4: Test Macro-F1 scores for multi-label node classification on Cora.
<R> <C> Period (AH) 1–50 <C> Texts 43 <C> Sentences 33K <C> Words 462K <C> Period 751–800 <C> Texts 407 <C> Sentences 3,980K <C> Words 112M <R> <C> 51–100 <C> 17 <C> 33K <C> 546K <C> 801–850 <C> 226 <C> 2,105K <C> 47M <R> <C> 101–150 <C> 48 <C> 122K <C> 3M <C> 851–900 <C> 240 <C> 3,396K <C> 98M <R> <C> 151–200 <C> 120 <C> 549K <C> 11M <C> 901–950 <C> 288 <C> 2,248K <C> 55M <R> <C> 201–250 <C> 325 <C> 1,487K <C> 39M <C> 951–1000 <C> 122 <C> 1,536K <C> 45M <R> <C> 251–300 <C> 504 <C> 1,779K <C> 39M <C> 1001–1050 <C> 112 <C> 858K <C> 29M <R> <C> 301–350 <C> 495 <C> 2,539K <C> 65M <C> 1051–1100 <C> 103 <C> 1,533K <C> 37M <R> <C> 351–400 <C> 568 <C> 2,850K <C> 59M <C> 1101–1150 <C> 82 <C> 1,713K <C> 47M <R> <C> 401–450 <C> 458 <C> 2,075K <C> 46M <C> 1151–1200 <C> 81 <C> 482K <C> 14M <R> <C> 451–500 <C> 481 <C> 3,350K <C> 102M <C> 1201–1250 <C> 211 <C> 2,666K <C> 66M <R> <C> 501–550 <C> 266 <C> 1,919K <C> 41M <C> 1251–1300 <C> 100 <C> 1,004K <C> 37M <R> <C> 551–600 <C> 443 <C> 3,392K <C> 101M <C> 1301–1350 <C> 201 <C> 1,713K <C> 36M <R> <C> 601–650 <C> 291 <C> 2,361K <C> 67M <C> 1351–1400 <C> 53 <C> 1,537K <C> 34M <R> <C> 651–700 <C> 313 <C> 2,216K <C> 63M <C> 1401–1450 <C> 90 <C> 2,714K <C> 56M <R> <C> 701–750 <C> 456 <C> 9,597K <C> 186M <C> [BOLD] Total <C> 7144 <C> 62M <C> 1,537M <CAP> Table 4: Number of texts, sentences, and words in each 50-year time period in the openiti complete corpus. ah refers to the Islamic calendar period.
<R> <C> Method <C> Average Score <C> Average Rank (1-6) <R> <C> Causal <C> 49.71 <C> 4.10 <R> <C> LM <C> 35.95 <C> 3.39 <R> <C> PMI <C> 34.92 <C> 3.02 <CAP> Table 1: Average Annotator Scores in Pairwise annotation experiment
<R> <C> Method <C> Average Score <C> Average Rank (1-3) <R> <C> Causal <C> 60.12 <C> 2.19 <R> <C> LM <C> 57.40 <C> 2.12 <R> <C> PMI <C> 44.26 <C> 1.68 <CAP> Table 3: Average Annotator Scores in Chain annotation experiment
<R> <C> Word embedding dimension <C> 100 <R> <C> POS tag embedding dimension <C> 25 <R> <C> Relation embedding dimension <C> 25 <R> <C> Hidden units in  [ITALIC] ScoreU <C> 100 <R> <C> Hidden units in  [ITALIC] ScoreL <C> 100 <R> <C> LSTM Dimensions (tree) <C> 200 <R> <C> LSTM Layers (tree) <C> 2 <R> <C> BI-LSTM Dimensions <C> 100+100 <R> <C> BI-LSTM Layers <C> 2 <R> <C> Mini-batch size <C> 50 <R> <C> [ITALIC] α (for word dropout) <C> 0.25 <R> <C> [ITALIC] paug (for exploration training) <C> 0.1 <R> <C> [ITALIC] g <C> [ITALIC] tanh <CAP> Table 1: Hyper-parameter values used in experiments
<R> <C> [BOLD] Test Set <C> [BOLD] Test Set <C> [BOLD] Dictionary definitions  [BOLD] Seen (500 WN defs) <C> [BOLD] Dictionary definitions  [BOLD] Seen (500 WN defs) <C> [BOLD] Dictionary definitions  [BOLD] Seen (500 WN defs) <C> [BOLD] Dictionary definitions  [BOLD] Unseen (500 WN defs) <C> [BOLD] Dictionary definitions  [BOLD] Unseen (500 WN defs) <C> [BOLD] Dictionary definitions  [BOLD] Unseen (500 WN defs) <C> [BOLD] Concept descriptions (200) <C> [BOLD] Concept descriptions (200) <C> [BOLD] Concept descriptions (200) <R> <C> Unsup. <C> W2V add <C> - <C> - <C> - <C> 923 <C> .04/.16 <C> 163 <C> 339 <C> .07/.30 <C> 150 <R> <C> models <C> W2V mult <C> - <C> - <C> - <C> 1000 <C> .00/.00 <C> 10* <C> 1000 <C> .00/.00 <C> 27* <R> <C> [EMPTY] <C> OneLook <C> [BOLD] 0 <C> [BOLD] .89/.91 <C> [BOLD] 67 <C> - <C> - <C> - <C> [BOLD] 18.5 <C> [BOLD] .38/.58 <C> 153 <R> <C> [EMPTY] <C> RNN cosine <C> 12 <C> .48/.73 <C> 103 <C> 22 <C> .41/.70 <C> 116 <C> 69 <C> .28/.54 <C> 157 <R> <C> [EMPTY] <C> RNN w2v cosine <C> 19 <C> .44/.70 <C> 111 <C> 19 <C> .44/.69 <C> 126 <C> 26 <C> [BOLD] .38/.66 <C> 111 <R> <C> [EMPTY] <C> RNN ranking <C> 18 <C> .45/.67 <C> 128 <C> 24 <C> .43/.69 <C> 103 <C> 25 <C> .34/.66 <C> 102 <R> <C> NLMs <C> RNN w2v ranking <C> 54 <C> .32/.56 <C> 155 <C> 33 <C> .36/.65 <C> 137 <C> 30 <C> .33/.69 <C> [BOLD] 77 <R> <C> [EMPTY] <C> BOW cosine <C> 22 <C> .44/.65 <C> 129 <C> 19 <C> .43/.69 <C> 103 <C> 50 <C> .34/.60 <C> 99 <R> <C> [EMPTY] <C> BOW w2v cosine <C> 15 <C> .46/.71 <C> 124 <C> [BOLD] 14 <C> [BOLD] .46/ .71 <C> 104 <C> 28 <C> .36/.66 <C> 99 <R> <C> [EMPTY] <C> BOW ranking <C> 17 <C> .45/.68 <C> 115 <C> 22 <C> .42/.70 <C> [BOLD] 95 <C> 32 <C> .35/.69 <C> 101 <R> <C> [EMPTY] <C> BOW w2v rankng <C> 55 <C> .32/.56 <C> 155 <C> 36 <C> .35/.66 <C> 138 <C> 38 <C> .33/ [BOLD] .72 <C> 85 <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [ITALIC] median rank accuracy@10/100 rank variance <C> [ITALIC] median rank accuracy@10/100 rank variance <C> [ITALIC] median rank accuracy@10/100 rank variance <C> [ITALIC] median rank accuracy@10/100 rank variance <C> [ITALIC] median rank accuracy@10/100 rank variance <C> [ITALIC] median rank accuracy@10/100 rank variance <CAP> Table 1: Performance of different reverse dictionary models in different evaluation settings. *Low variance in mult models is due to consistently poor scores, so not highlighted.
<R> <C> [BOLD] Model <C> [BOLD] Restaurant Acc <C> [BOLD] Restaurant F1 <C> [BOLD] Laptop Acc <C> [BOLD] Laptop F1 <C> [BOLD] Twitter Acc <C> [BOLD] Twitter F1 <R> <C> No pretrained embeddings <C> No pretrained embeddings <C> No pretrained embeddings <C> No pretrained embeddings <C> No pretrained embeddings <C> [EMPTY] <C> [EMPTY] <R> <C> SVM <C> 80.16 <C> - <C> 70.49 <C> - <C> 63.40 <C> 63.30 <R> <C> AdaRNN† <C> - <C> - <C> - <C> - <C> 66.30 <C> 65.90 <R> <C> GloVe embeddings <C> GloVe embeddings <C> GloVe embeddings <C> GloVe embeddings <C> GloVe embeddings <C> [EMPTY] <C> [EMPTY] <R> <C> IAN <C> 78.60 <C> - <C> 72.10 <C> - <C> - <C> - <R> <C> TNet <C> 80.69 <C> 71.27 <C> 76.54 <C> 71.75 <C> 74.97 <C> 73.60 <R> <C> MGAN <C> 81.25 <C> 71.94 <C> 75.39 <C> 72.47 <C> 72.54 <C> 70.81 <R> <C> AEN <C> 80.98 <C> 72.14 <C> 73.51 <C> 69.04 <C> 72.83 <C> 69.81 <R> <C> CDT† <C> 82.30 <C> 74.02 <C> 77.19 <C> 72.99 <C> 74.66 <C> 73.66 <R> <C> TD-GAT† <C> 81.20 <C> - <C> 74.00 <C> - <C> - <C> - <R> <C> RGAT† <C> 83.45–––––– <C> 76.43–––––– <C> 77.60–––––– <C> 73.47–––––– <C> 74.80 <C> 74.05–––––– <R> <C> BERT embeddings <C> BERT embeddings <C> BERT embeddings <C> BERT embeddings <C> BERT embeddings <C> [EMPTY] <C> [EMPTY] <R> <C> PT <C> 84.95 <C> 76.96 <C> 78.07 <C> 75.08 <C> - <C> - <R> <C> SPC <C> 84.46 <C> 76.98 <C> 78.99 <C> 75.03 <C> 73.55 <C> 72.14 <R> <C> AEN <C> 83.12 <C> 73.76 <C> 79.93 <C> 76.31 <C> 74.71 <C> 73.13 <R> <C> TD-GAT† <C> 83.00 <C> - <C> 80.10 <C> - <C> - <C> - <R> <C> RGAT† <C> [BOLD] 86.59 <C> [BOLD] 80.51 <C> [BOLD] 81.25 <C> [BOLD] 78.55 <C> [BOLD] 75.84 <C> [BOLD] 74.65 <CAP> Table 2: Performance comparison of different models on the benchmark datasets. The best performance are bold-typed, † denotes that the model requires dependency tree as input. Results underlined indicate that the proposed method is significantly better than baseline at significance level p<0.01.
<R> <C> [BOLD] Model <C> [BOLD] Restaurant Acc <C> [BOLD] Restaurant F1 <C> [BOLD] Laptop Acc <C> [BOLD] Laptop F1 <R> <C> Transformer (G) <C> 80.32 <C> 72.10 <C> 72.85 <C> 69.45 <R> <C> GAT (G) <C> 82.41 <C> 75.10 <C> 76.75 <C> 71.87 <R> <C> GAT (G)-Ratt <C> 82.68 <C> 75.39 <C> 77.10 <C> 72.15 <R> <C> RGAT (G) <C> 83.45 <C> 76.43 <C> 77.60 <C> 73.47 <R> <C> Transformer (B) <C> 84.89 <C> 77.90 <C> 79.33 <C> 76.08 <R> <C> GAT (B) <C> 85.70 <C> 78.80 <C> 79.97 <C> 77.10 <R> <C> GAT (B)-Ratt <C> 86.13 <C> 79.61 <C> 80.55 <C> 77.67 <R> <C> RGAT (B) <C> [BOLD] 86.59 <C> [BOLD] 80.51 <C> [BOLD] 81.25 <C> [BOLD] 78.55 <CAP> Table 3: An ablation study on the restaurant and laptop datasets. The best performance are bold-typed. G,B denote GloVe and BERT, respectively. Ratt refers to relational attention.
<R> <C> [BOLD] Type of replacements <C> [BOLD] Replacements <C> [BOLD] Occurrences in corpus <C> [BOLD] Mean accuracy <R> <C> Monosemous children <C> 25.48 <C> 383,498 <C> 63.31% <R> <C> + Monosemous relatives <C> 30.09 <C> 769,947 <C> 70.86% <R> <C> + First relatives <C> 44.89 <C> 2,295,686 <C> 77.35% <R> <C> + Word determiners <C> 51.26 <C> 2,296,514 <C> 77.72% <R> <C> + All determiners <C> 102.04 <C> 2,309,640 <C> 77.64% <R> <C> + Other words <C> 103.04 <C> 2,473,255 <C> [BOLD] 77.96% <CAP> Table 2: Strategies for generating replacements, each built by adding new elements to the previous step, with the resulting number of replacements (average per word), their occurrences in the corpus (total) and the mean accuracy of disambiguation.
<R> <C> [EMPTY] <C> WebQuestionsSP KB <C> WebQuestionsSP Text <C> WebQuestionsSP 50% KB <C> WebQuestionsSP 50% KB + Text <R> <C> KV-Mem* <C> 46.7 <C> 23.2 <C> 32.7 <C> 31.6 <R> <C> GraftNet* <C> 66.4 <C> 24.9 <C> 48.2 <C> 49.7 <R> <C> PullNet (Ours) <C> [BOLD] 68.1 <C> 24.8 <C> [BOLD] 50.3 <C> [BOLD] 51.9 <R> <C> GraftNet <C> [BOLD] 67.8 <C> [BOLD] 25.3 <C> 47.7 <C> 49.9 <R> <C> NSM <C> 69.0 (F1) <C> – <C> – <C> – <CAP> Table 5: Hits@1 on WebQuestionsSP compared to baseline models. Number below the double line are from original papers: GraftNet Sun et al. (2018), and NSM Liang et al. (2017) (which only reports F1 in their paper). * Reimplemented or different data.
<R> <C> [EMPTY] <C> Complex WebQuestions (dev) KB <C> Complex WebQuestions (dev) Text <C> Complex WebQuestions (dev) 50% KB <C> Complex WebQuestions (dev) 50% KB + Text <R> <C> KV-Mem* <C> 21.1 <C> 7.4 <C> 14.8 <C> 15.2 <R> <C> GraftNet* <C> 32.8 <C> 10.6 <C> 26.1 <C> 26.9 <R> <C> PullNet (Ours) <C> [BOLD] 47.2 <C> [BOLD] 13.1 <C> [BOLD] 31.5 <C> [BOLD] 33.7 <CAP> Table 6: Hits@1 on Complex WebQuestions (dev) compared to baseline models. * Reimplemented or different data.
<R> <C> [EMPTY] <C> en <C> fr <C> es <C> de <C> el <C> bg <C> ru <C> tr <C> ar <C> vi <C> th <C> zh <C> hi <C> sw <C> ur <R> <C> Premise <C> 21.7 <C> 24.1 <C> 22.1 <C> 21.1 <C> 21.0 <C> 20.9 <C> 19.6 <C> 16.8 <C> 20.7 <C> 27.6 <C> 22.1 <C> 21.8 <C> 23.2 <C> 18.7 <C> 24.1 <R> <C> Hypothesis <C> 10.7 <C> 12.4 <C> 10.9 <C> 10.8 <C> 10.6 <C> 10.4 <C> 9.7 <C> 8.4 <C> 10.2 <C> 13.5 <C> 10.4 <C> 10.8 <C> 11.9 <C> 9.0 <C> 12.3 <CAP> Table 2: Average number of tokens per sentence in the XNLI corpus for each language.
<R> <C> [EMPTY] <C> fr <C> ru <C> zh <R> <C> [ITALIC] ft=1, [ITALIC] λ=0.25 [default] <C> 68.9 <C> 66.4 <C> 67.9 <R> <C> [ITALIC] ft=1, [ITALIC] λ=0.0 (no negatives) <C> 67.8 <C> 66.2 <C> 66.3 <R> <C> [ITALIC] ft=1, [ITALIC] λ=0.5 <C> 64.5 <C> 61.3 <C> 63.7 <R> <C> [ITALIC] ft=0, [ITALIC] λ=0.25 <C> 68.5 <C> 66.3 <C> 67.7 <CAP> Table 5: Validation accuracy using BiLSTM-max. Default setting corresponds to λ=0.25 (importance of the negative terms) and uses fine-tuning of the target lookup table (ft=1).
<R> <C> Embedding <C> BiDAF <C> DrQA <R> <C> random <C> 70.3/59.3 <C> 72.4/62.5 <R> <C> word2vec <C> 77.1/67.7 <C> 78.2/68.8 <R> <C> glove <C> 77.3/67.7 <C> 78.9/69.4 <R> <C> fast-text-wiki <C> 77.1/67.7 <C> 75.4/66.0 <R> <C> fast-text-crawl <C> 76.9/67.4 <C> 77.0/67.2 <R> <C> ELMo <C> 79.9/71.1 <C> 82.7/74.3 <CAP> Table 4: F1/EM score of different embedding
<R> <C> [BOLD] Models <C> [BOLD] Douban music  [BOLD] OE (-) <C> [BOLD] Douban music  [BOLD] HL (-) <C> [BOLD] Douban music  [BOLD] Macro F1 (+) <C> [BOLD] Douban music  [BOLD] Micro F1 (+) <C> [BOLD] Amazon music  [BOLD] OE (-) <C> [BOLD] Amazon music  [BOLD] HL (-) <C> [BOLD] Amazon music  [BOLD] Macro F1 (+) <C> [BOLD] Amazon music  [BOLD] Micro F1 (+) <R> <C> ML-KNN (zhang2007ml) <C> 77.3 <C> 0.094 <C> 23.6 <C> 38.1 <C> 55.4 <C> 0.131 <C> 34.1 <C> 47.1 <R> <C> Binary Relevance (tsoumakas2009mining) <C> 74.4 <C> 0.083 <C> 24.7 <C> 41.8 <C> 41.3 <C> 0.125 <C> 35.6 <C> 48.4 <R> <C> Classifier Chains (read2011classifier) <C> 67.5 <C> 0.107 <C> 29.9 <C> 44.3 <C> 42.5 <C> 0.132 <C> 36.9 <C> 50.4 <R> <C> Label Powerset (tsoumakas2007random) <C> 56.2 <C> 0.096 <C> 37.7 <C> 50.3 <C> 28.1 <C> 0.114 <C> 40.2 <C> 55.7 <R> <C> MLP (zhao2019driven) <C> 71.5 <C> 0.081 <C> 29.8 <C> 45.8 <C> 24.5 <C> 0.104 <C> 19.0 <C> 46.3 <R> <C> CNN (zhao2019driven) <C> 37.9 <C> 0.099 <C> 32.5 <C> 49.3 <C> 23.7 <C> 0.098 <C> 42.0 <C> 58.9 <R> <C> LSTM (zhao2019driven) <C> 30.5 <C> 0.089 <C> 33.0 <C> 53.9 <C> 23.7 <C> 0.101 <C> 34.5 <C> 54.6 <R> <C> HAN-LCM (zhao2019driven) <C> 22.6 <C> 0.074 <C> 54.4 <C> 64.5 <C> 17.8 <C> 0.086 <C> 56.2 <C> 69.1 <R> <C> [BOLD] KRF <C> [BOLD] 15.5 <C> [BOLD] 0.058 <C> [BOLD] 66.4 <C> [BOLD] 70.8 <C> [BOLD] 16.6 <C> [BOLD] 0.070 <C> [BOLD] 60.8 <C> [BOLD] 73.2 <CAP> Table 2. Comparisons with state-of-the-art methods on the music review datasets. “+” represents that higher scores are better and “-” represents that lower scores are better. It can be seen that the proposed framework significantly outperforms the baselines. The experimental results of baselines are directly cited from (zhao2019driven).
<R> <C> [EMPTY] <C> Reuters <C> 20 News <R> <C> Document length threshold <C> 100 <C> 300 <R> <C> Training documents <C> 3622 <C> 6312 <R> <C> Test documents <C> 1705 <C> 1542 <R> <C> Word frequency threshold <C> 30 <C> 200 <R> <C> Lexicon size (words) <C> 2388 <C> 1910 <CAP> Table 1: Data profile of the experimental datasets.
<R> <C> ground truth dataset <C> verb ranking method Ours <C> verb ranking method Word2Vec <C> verb ranking method GloVe 6B <C> verb ranking method GloVe 42B <C> verb ranking method PPMI <R> <C> WTAction <C> [BOLD] 0.90 <C> 0.74 <C> 0.80 <C> 0.84 <C> 0.80 <R> <C> SyntagNet <C> [BOLD] 0.90 <C> 0.81 <C> 0.83 <C> 0.86 <C> 0.83 <CAP> Table 1: Average AAUC of verb rankings produced by our and baseline methods
<R> <C> [BOLD] Model <C> ACC_DEV <C> ACC_TST <C> P_TST <C> R_TST <C> F1_TST <C> [BOLD] Epochs <R> <C> B-FT <C> 83.784 <C> [BOLD] 92.153 <C> 88.990 <C> 94.510 <C> [BOLD] 90.933 <C> 6 <R> <C> R-FT <C> 83.692 <C> 92.102 <C> 88.933 <C> 94.503 <C> 90.882 <C> 10 <R> <C> B-PT-C-C <C> 85.204 <C> 90.610 <C> 88.402 <C> 88.115 <C> 88.256 <C> 1 <R> <C> B-PT-R-C <C> 85.845 <C> 92.102 <C> 88.933 <C> 94.532 <C> 90.885 <C> 2 <R> <C> R-PT-C-C <C> 84.654 <C> 92.102 <C> 88.933 <C> 94.532 <C> 90.885 <C> 1 <R> <C> R-PT-R-C <C> 85.158 <C> 88.552 <C> 85.129 <C> 87.886 <C> 86.299 <C> 2 <R> <C> E <C> 88.548 <C> [BOLD] 92.153 <C> 88.990 <C> 94.510 <C> [BOLD] 90.933 <C> 2 <R> <C> E_1 <C> 90.701 <C> [BOLD] 92.153 <C> 88.992 <C> 94.396 <C> 90.917 <C> 1 <R> <C> [BOLD] E_2 <C> [BOLD] 90.884 <C> 92.128 <C> 88.962 <C> 94.464 <C> 90.901 <C> 2 <CAP> Table 4: Results of individual models and ensemble model on dev set and test set. B-FT: fine-tuned default BERT-base, R-FT: fine-tuned default RoBERTa-base, B-PT-C-C: fine-tuned our pre-trained BERT-base classification-classification model, R-PT-C-C: fine-tuned our pre-trained RoBERTa-base classification-classification model, B-PT-R-C: fine-tuned our pre-trained BERT-base regression-classification model, R-PT-R-C: fine-tuned our pre-trained RoBERTa-base regression-classification model, E: ensemble model with default learning_rate of 2e−5, E_1: ensemble model with lower learning_rate of 1e−5, E_2: submitted ensemble model with higher dropout of 0.5.
<R> <C> Metric <C> News (CNN) BART <C> News (CNN) GPT2-S <C> News (CNN) GPT2-L <C> News (CNN) ProGeT-2 <C> News (CNN) ProGeT-3 <C> Story (WritingPrompts) BART <C> Story (WritingPrompts) GPT2-S <C> Story (WritingPrompts) GPT2-L <C> Story (WritingPrompts) ProGeT-2 <C> Story (WritingPrompts) ProGeT-3 <R> <C> BLEU2 [ITALIC] H↑ <C> 84.99 <C> 84.57 <C> 84.92 <C> 85.21 <C> [BOLD] 85.31 <C> 87.00 <C> 86.49 <C> 86.25 <C> [BOLD] 87.85 <C> 87.56 <R> <C> BLEU3 [ITALIC] H↑ <C> 65.28 <C> 64.03 <C> 64.81 <C> 65.39 <C> [BOLD] 65.71 <C> 70.05 <C> 68.45 <C> 68.28 <C> [BOLD] 70.75 <C> 70.66 <R> <C> BLEU4 [ITALIC] H↑ <C> 44.58 <C> 42.92 <C> 43.89 <C> 44.54 <C> [BOLD] 44.94 <C> 49.59 <C> 47.12 <C> 47.15 <C> 49.94 <C> [BOLD] 50.12 <R> <C> BELU5 [ITALIC] H↑ <C> 28.41 <C> 26.81 <C> 27.76 <C> 28.28 <C> [BOLD] 28.63 <C> 31.43 <C> 28.80 <C> 29.00 <C> 31.43 <C> [BOLD] 31.75 <R> <C> MSJ2 ↑ <C> 60.81 <C> 61.72 <C> 60.10 <C> [BOLD] 61.73 <C> 61.43 <C> 64.68 <C> [BOLD] 67.03 <C> 63.37 <C> 66.08 <C> 65.07 <R> <C> MSJ3 ↑ <C> 40.47 <C> 40.38 <C> 39.87 <C> 41.13 <C> [BOLD] 41.13 <C> 46.22 <C> 46.59 <C> 44.15 <C> [BOLD] 47.20 <C> 46.69 <R> <C> MSJ4 ↑ <C> 24.89 <C> 24.30 <C> 24.36 <C> 25.27 <C> [BOLD] 25.38 <C> 29.93 <C> 29.21 <C> 27.84 <C> 30.52 <C> [BOLD] 30.37 <R> <C> MSJ5 ↑ <C> 14.77 <C> 14.09 <C> 14.36 <C> 15.00 <C> [BOLD] 15.11 <C> 18.13 <C> 17.05 <C> 16.38 <C> 18.40 <C> [BOLD] 18.42 <R> <C> FBD-S ↓ <C> 6.71 <C> 6.64 <C> 6.55 <C> 6.53 <C> [BOLD] 6.44 <C> 3.80 <C> 2.92 <C> 3.50 <C> [BOLD] 2.35 <C> 2.78 <R> <C> FBD-M ↓ <C> 17.68 <C> 15.92 <C> 13.51 <C> 14.07 <C> [BOLD] 12.97 <C> 17.16 <C> 14.25 <C> 14.88 <C> [BOLD] 13.40 <C> 13.87 <R> <C> FBD-D ↓ <C> 36.16 <C> 34.25 <C> 26.94 <C> 27.69 <C> [BOLD] 24.62 <C> 36.02 <C> 30.59 <C> 31.05 <C> [BOLD] 27.32 <C> 28.31 <R> <C> TID ↓ <C> 7.49 <C> 8.20 <C> 7.05 <C> 6.38 <C> [BOLD] 5.88 <C> 3.09 <C> 3.08 <C> 3.92 <C> [BOLD] 2.33 <C> 2.37 <CAP> Table 1: Comparison of our models with baseline models. ProGeT-2 and ProGeT-3 denote progressive generation with two and three steps respectively. BLEUH measures domain specific quality, MSJ measures n-gram similarity, FSD measures semantic similarity and TID measures TD-IDF distance. Our methods outperform all other baselines across the board.
<R> <C> [EMPTY] <C> MSJ4 <C> FBD-D <R> <C> ProGeT-2 <C> 25.27 <C> 27.69 <R> <C> GoldPlan <C> 27.37 <C> 16.40 <R> <C> Human (Dev Set) <C> 28.34 <C> 11.64 <CAP> Table 3: Human evaluation results on CNN.
<R> <C> Metric <C> Metric <C> ARI  [ITALIC] Ct <C> ARI  [ITALIC] it <C> EAR  [ITALIC] L <C> EAR  [ITALIC] K <C> PAR Sentence <C> PAR Word <R> <C> (A) <C> SpCoSLAM <C> 0.252 <C> 0.604 <C> 0.785 <C> 0.818 <C> 0.558 <C> 0.098 <R> <C> (B) <C> SpCoSLAM with AW + WS <C> 0.347 <C> 0.684 <C> 0.802 <C> 0.815 <C> 0.565 <C> 0.141 <R> <C> (C) <C> SpCoSLAM 2.0 (10 FLR– [ITALIC] it, [ITALIC] Ct) <C> 0.346 <C> 0.713 <C> 0.733 <C> 0.868 <C> 0.553 <C> 0.096 <R> <C> (D) <C> SpCoSLAM 2.0 (10 FLR– [ITALIC] it, [ITALIC] Ct + RS) <C> 0.314 <C> 0.719 <C> 0.730 <C> 0.840 <C> [BOLD] 0.835 <C> 0.464 <R> <C> (E1) <C> SpCoSLAM 2.0 (1 FLR– [ITALIC] it, [ITALIC] Ct, [ITALIC] St + SBU) <C> 0.307 <C> 0.672 <C> 0.817 <C> 0.800 <C> 0.671 <C> 0.165 <R> <C> (E2) <C> SpCoSLAM 2.0 (10 FLR– [ITALIC] it, [ITALIC] Ct, [ITALIC] St + SBU) <C> 0.385 <C> 0.688 <C> 0.833 <C> 0.782 <C> 0.733 <C> 0.305 <R> <C> (E3) <C> SpCoSLAM 2.0 (20 FLR– [ITALIC] it, [ITALIC] Ct, [ITALIC] St + SBU) <C> 0.354 <C> 0.790 <C> [BOLD] 0.883 <C> [BOLD] 0.898 <C> 0.768 <C> 0.350 <R> <C> (F) <C> SpCoA++ (Batch learning) <C> [BOLD] 0.522 <C> [BOLD] 0.899 <C> 0.800 <C> 0.850 <C> 0.830 <C> [BOLD] 0.480 <CAP> Table 4: Evaluation results in simulator environments
<R> <C> [BOLD] Model <C> [BOLD] Long MAE <C> [BOLD] Long  [ITALIC] R2 <C> [BOLD] Short MAE <C> [BOLD] Short  [ITALIC] R2 <R> <C> Anxiety - TFIDF <C> 1.65 <C> 0.16 <C> 1.82 <C> -0.01 <R> <C> Anxiety - POS <C> 1.79 <C> 0.04 <C> 1.84 <C> 0.00 <R> <C> Fear - TFIDF <C> 1.71 <C> 0.15 <C> 1.85 <C> 0.00 <R> <C> Fear - POS <C> 1.83 <C> 0.05 <C> 1.87 <C> 0.01 <R> <C> Sadness - TFIDF <C> 1.75 <C> 0.12 <C> 1.90 <C> -0.02 <R> <C> Sadness - POS <C> 1.88 <C> 0.02 <C> 1.91 <C> -0.01 <R> <C> Worry - TFIDF <C> 1.26 <C> 0.16 <C> 1.38 <C> -0.03 <R> <C> Worry - POS <C> 1.35 <C> 0.03 <C> 1.37 <C> 0.01 <CAP> Table 4: Results for regression modeling for long and short texts.
<R> <C> [BOLD] METHOD <C> [BOLD] COMP <C> [BOLD] GS2011  [BOLD] K=20 <C> [BOLD] GS2011  [BOLD] K=300 <C> [BOLD] KS2013  [BOLD] K=20 <C> [BOLD] KS2013  [BOLD] K=300 <R> <C> [BOLD] baseline <C> [BOLD] add <C> [BOLD] 0.17 <C> 0.12 <C> 0.48 <C> [BOLD] 0.58 <R> <C> [EMPTY] <C> [BOLD] mult <C> 0.22 <C> 0. [BOLD] 24 <C> [BOLD] 0.16 <C> 0.13 <R> <C> [BOLD] dist <C> [BOLD] CO <C> 0.26 <C> 0.30 <C> 0.19 <C> 0.24 <R> <C> [EMPTY] <C> [BOLD] CS <C> 0.23 <C> 0.28 <C> 0.24 <C> 0.24 <R> <C> [EMPTY] <C> [BOLD] F+ <C> [BOLD] 0.28 <C> [BOLD] 0.32 <C> 0.22 <C> 0.25 <R> <C> [EMPTY] <C> [BOLD] RE <C> 0.18 <C> 0.16 <C> [BOLD] 0.27 <C> [BOLD] 0.33 <R> <C> [EMPTY] <C> [BOLD] VO <C> 0.19 <C> 0.24 <C> 0.23 <C> 0.26 <R> <C> [BOLD] reg+ <C> [BOLD] CO <C> 0.26 <C> 0.30 <C> 0.20 <C> 0.27 <R> <C> [EMPTY] <C> [BOLD] CS <C> 0.24 <C> 0.29 <C> 0.23 <C> 0.23 <R> <C> [EMPTY] <C> [BOLD] F+ <C> [BOLD] 0.29 <C> [BOLD] 0.33 <C> 0.23 <C> 0.25 <R> <C> [EMPTY] <C> [BOLD] RE <C> 0.19 <C> 0.16 <C> [BOLD] 0.26 <C> [BOLD] 0.33 <R> <C> [EMPTY] <C> [BOLD] VO <C> 0.21 <C> 0.25 <C> 0.23 <C> 0.29 <R> <C> [BOLD] reg <C> [BOLD] CO <C> 0.24 <C> 0.33 <C> 0.19 <C> 0.27 <R> <C> [EMPTY] <C> [BOLD] CS <C> 0.23 <C> 0.28 <C> 0.19 <C> 0.21 <R> <C> [EMPTY] <C> [BOLD] F+ <C> [BOLD] 0.28 <C> [BOLD] 0.35 <C> 0.12 <C> 0.20 <R> <C> [EMPTY] <C> [BOLD] RE <C> 0.20 <C> 0.20 <C> [BOLD] 0.27 <C> [BOLD] 0.31 <R> <C> [EMPTY] <C> [BOLD] VO <C> 0.22 <C> 0.28 <C> 0.23 <C> 0.27 <CAP> Table 1: Spearman correlation for each method on GS2011 and KS2013 datasets across composition methods and number of noun dimensions.
<R> <C> [BOLD] System <C> [BOLD] # Params (M) <C> [BOLD] WER (%) <R> <C> Zeghidour et al. [zeghidour2018end] <C> 17 <C> 5.6 <R> <C> Baskar et al. [baskar2019promising] <C> ∼100 <C> 3.8 <R> <C> Likhomanenko et al. [likhomanenko2019needs] <C> 17 <C> 3.6 <R> <C> Zeghidour et al. [zeghidour2018fully] <C> 17 <C> 3.5 <R> <C> Wang et al. [wang2019espresso] <C> 18 <C> [BOLD] 3.4 <R> <C> Hadian et al. [hadian2018flat] <C> 9.1 <C> 4.3 <R> <C> PyChain <C> 6.3 <C> 3.5 <CAP> Table 1: WERs (%) on the WSJ without data augmentation.
<R> <C> [BOLD] Vocabulary <C> [BOLD] Full Test Set  [BOLD] Closed <C> [BOLD] Full Test Set  [BOLD] Open <C> [BOLD] OOV Subset  [BOLD] Closed <C> [BOLD] OOV Subset  [BOLD] Open <R> <C> Airbnb <C> 74.4 <C> 72.7 <C> 54.5 <C> 58.2 <R> <C> Greyhound <C> 85.2 <C> 84.4 <C> 64.2 <C> 67.0 <R> <C> OpenTable <C> 89.7 <C> 88.9 <C> 68.8 <C> 68.1 <R> <C> United <C> 90.8 <C> 90.6 <C> 81.8 <C> 80.7 <CAP> Table 4: Comparison of F1 scores for open and closed vocabulary systems on the full test set vs. the subset with OOV words.
<R> <C> Model <C> [ITALIC] Sx + Property <C> [ITALIC] Rx + Property <R> <C> BRAN <C> 0.62 <C> 0.41 <R> <C> R-SAT <C> 0.82 <C> 0.60 <CAP> Table 4: Performance of the model when the entities and properties are given and it is only required to predict existence of relations.
<R> <C> Source ↓ <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Target <C> DANN (5k) <C> DANN+ (5k) <C> VFAE (5k) <C> CMD (5k) <C> Asym (5k) <C> MT-Tri (5k) <C> TRL* (5k) <C> DSN (5k) <C> CoCMD* (5k) <C> [BOLD] KinGDOM (5k) <C> DANN+ (30k) <C> TAT (30k) <C> [BOLD] KinGDOM (30k) <R> <C> B → D <C> 78.4 <C> 82.6 <C> 79.9 <C> 80.5 <C> 80.7 <C> 81.2 <C> 82.2 <C> 82.8 <C> 83.1 <C> 83.1 <C> 84.7 <C> 84.5 <C> [BOLD] 85.0 <R> <C> B → E <C> 73.3 <C> 79.9 <C> 79.2 <C> 78.7 <C> 79.8 <C> 78.0 <C> - <C> 81.9 <C> 83.0 <C> 82.2 <C> 83.0 <C> 80.1 <C> [BOLD] 83.9 <R> <C> B → K <C> 77.9 <C> 81.8 <C> 81.6 <C> 81.3 <C> 82.5 <C> 78.8 <C> 82.7 <C> 84.4 <C> 85.3 <C> 85.0 <C> 84.0 <C> 83.6 <C> [BOLD] 86.6 <R> <C> D → B <C> 72.3 <C> 80.3 <C> 75.5 <C> 79.5 <C> 73.2 <C> 77.1 <C> - <C> 80.1 <C> 81.8 <C> 81.4 <C> 82.7 <C> 81.9 <C> [BOLD] 82.7 <R> <C> D → E <C> 75.4 <C> 79.9 <C> 78.6 <C> 79.7 <C> 77.0 <C> 81.0 <C> - <C> 81.4 <C> 83.4 <C> 81.7 <C> 83.4 <C> 81.9 <C> [BOLD] 83.9 <R> <C> D → K <C> 78.3 <C> 83.0 <C> 82.2 <C> 83.0 <C> 82.5 <C> 79.5 <C> - <C> 83.3 <C> 85.5 <C> 84.6 <C> 85.3 <C> 84.0 <C> [BOLD] 87.1 <R> <C> E → B <C> 71.3 <C> 74.9 <C> 72.7 <C> 74.4 <C> 73.2 <C> 73.5 <C> - <C> 75.1 <C> 76.9 <C> 76.9 <C> 77.1 <C> [BOLD] 83.2 <C> 78.4 <R> <C> E → D <C> 73.8 <C> 78.6 <C> 76.5 <C> 76.3 <C> 72.9 <C> 75.4 <C> 75.8 <C> 77.1 <C> 78.3 <C> 78.8 <C> 79.6 <C> 77.9 <C> [BOLD] 80.3 <R> <C> E → K <C> 85.4 <C> 88.6 <C> 85.0 <C> 86.0 <C> 86.9 <C> 87.2 <C> - <C> 87.2 <C> 87.3 <C> 88.4 <C> 89.0 <C> [BOLD] 90.0 <C> 89.4 <R> <C> K → B <C> 70.9 <C> 75.9 <C> 72.0 <C> 75.6 <C> 72.5 <C> 73.8 <C> 72.1 <C> 76.4 <C> 77.2 <C> 78.2 <C> 77.1 <C> 75.8 <C> [BOLD] 80.0 <R> <C> K → D <C> 74.0 <C> 79.2 <C> 73.3 <C> 77.5 <C> 74.9 <C> 77.8 <C> - <C> 78.0 <C> 79.6 <C> 80.7 <C> 81.3 <C> 77.7 <C> [BOLD] 82.3 <R> <C> K → E <C> 84.3 <C> 86.9 <C> 83.8 <C> 85.4 <C> 84.6 <C> 86.0 <C> - <C> 86.7 <C> 87.2 <C> 87.4 <C> 88.0 <C> 88.2 <C> [BOLD] 88.6 <R> <C> Avg. <C> 76.3 <C> 80.9 <C> 78.4 <C> 79.8 <C> 78.4 <C> 79.1 <C> - <C> 81.2 <C> 82.4 <C> 82.3 <C> 82.9 <C> 82.4 <C> [BOLD] 84.0 <CAP> Table 2: Comparison with different baseline and state-of-the-art models (Section 5.3). TRL* reported results on four combinations. CoCMD* is a semi-supervised domain adaptation method. DSN is the current state-of-the-art for unsupervised domain adaptation on the Amazon reviews dataset. Scores for MT-Tri are extrapolated from the graphs illustrated in DBLP:conf/acl/PlankR18. Note: B: Books, D: DVD, E:Electronics, and K: Kitchen domains. 5k, 30k signify 5000 and 30,000 dimensional BOW features.
<R> <C> Model <C> CoNLL English Dev <C> CoNLL English Test <C> OntoNotes Dev <C> OntoNotes Test <C> CoNLL Spanish Dev <C> CoNLL Spanish Test <R> <C> Lample et al. ( 2016 ) <C> - <C> 90.94 <C> - <C> [EMPTY] <C> - <C> 85.75 <R> <C> Chiu and Nichols ( 2015 ) <C> 94.03* <C> 91.62 <C> 84.57* <C> 86.28 <C> - <C> - <R> <C> SL <C> 93.06 <C> 89.85 <C> 86.39 <C> 85.59 <C> 83.82 <C> 84.64 <R> <C> RL-Sarsa <C> 93.22 <C> 89.28 <C> 85.15 <C> 85.35 <C> 81.29 <C> 83.61 <R> <C> RL-Qlearn <C> 93.25 <C> 89.17 <C> 85.23 <C> 84.67 <C> 81.72 <C> 84.35 <CAP> Table 3: Mention detection F1 scores: State of the art, supervised baseline (SL) and Sarsa/Q-learning using mention-level episodes (RL). *-with tuned hyper-parameters.
<R> <C> Sz <C> Chk rnd. <C> RL Chk <C> RL Corr. lbl <C> RL+partial reward Chk <C> RL+partial reward Corr. lbl <C> Correct example <R> <C> 1 <C> 1.0 <C> 1.0 <C> 0.11 <C> 1.0 <C> 0.65 <C> [two [hours]TIME]TIME <R> <C> 2 <C> 0.44 <C> 0.46 <C> 0.51 <C> 0.65 <C> 0.88 <C> [[New York]GPE Stock Market]ORG <R> <C> 3 <C> 0.33 <C> 0.41 <C> 0.71 <C> 0.60 <C> 0.93 <C> [the [Sino - Japanese]NORP war]EVENT <R> <C> 4 <C> 0.31 <C> 0.35 <C> 0.68 <C> 0.62 <C> 0.92 <C> [end of [August of last year]DATE]DATE <CAP> Table 4: Labels assigned to sub-mentions broken-down by mention size. Chk: percentage of times the sub-mention is a chunk of the larger mention (Chk rnd: probability of random). Corr. lbl: Percentage of times the correct label is assigned to it. Chance: 0.08
<R> <C> [BOLD] k <C> [BOLD] Fully <C> [BOLD] Oracle <R> <C> [BOLD] k <C> [BOLD] Supported (%) <C> [BOLD] Accuracy (%) <R> <C> 1 <C> 25.31 <C> 50.21 <R> <C> 5 <C> 55.30 <C> 70.20 <R> <C> 10 <C> 65.86 <C> 77.24 <R> <C> 25 <C> 75.92 <C> 83.95 <R> <C> 50 <C> 82.49 <C> 90.13 <R> <C> 100 <C> 86.59 <C> 91.06 <CAP> Table 2: Dev. set document retrieval evaluation.
<R> <C> Dataset <C> Metric <C> Single-Task Learning Benchmark <C> Single-Task Learning BiLSTM <C> Single-Task Learning BiLM <C> Multi-Task Learning MTM-CW <C> Multi-Task Learning MT-BioNER <C> Multi-Task Learning MT-BioNER <R> <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 3-datasets <C> 4-datasets <R> <C> BC2GM (Genes) <C> Precision <C> - <C> 81.57 <C> 81.81 <C> [BOLD] 82.10 <C> 82.01 <C> 80.33 <R> <C> BC2GM (Genes) <C> Recall <C> - <C> 79.48 <C> 81.57 <C> 79.42 <C> [BOLD] 84.04 <C> 82.82 <R> <C> BC2GM (Genes) <C> F1 <C> - <C> 80.51 <C> 81.69 <C> 80.74 <C> [BOLD] 83.01 <C> 81.55 <R> <C> BC5CDR (Chemical) <C> Precision <C> [BOLD] 89.21 <C> 87.60 <C> 88.10 <C> 89.10 <C> 88.46 <C> 87.99 <R> <C> BC5CDR (Chemical) <C> Recall <C> 84.45 <C> 86.25 <C> 90.49 <C> 88.47 <C> [BOLD] 90.52 <C> 90.16 <R> <C> BC5CDR (Chemical) <C> F1 <C> 86.76 <C> 86.92 <C> 89.28 <C> 88.78 <C> [BOLD] 89.5 <C> 89.06 <R> <C> NCBI (Disease) <C> Precision <C> 85.10 <C> 86.11 <C> 86.41 <C> 85.86 <C> [BOLD] 86.73 <C> 84.50 <R> <C> NCBI (Disease) <C> Recall <C> 80.80 <C> 85.49 <C> 88.31 <C> 86.42 <C> [BOLD] 89.7 <C> 88.98 <R> <C> NCBI (Disease) <C> F1 <C> 82.90 <C> 85.80 <C> 87.34 <C> 86.14 <C> [BOLD] 88.1 <C> 86.68 <R> <C> JNLPBA (Genes etc.) <C> Precision <C> 69.42 <C> 71.35 <C> [BOLD] 71.39 <C> 70.91 <C> - <C> 67.40 <R> <C> JNLPBA (Genes etc.) <C> Recall <C> 75.99 <C> 75.74 <C> 79.06 <C> 76.34 <C> - <C> [BOLD] 79.35 <R> <C> JNLPBA (Genes etc.) <C> F1 <C> 72.55 <C> 73.48 <C> [BOLD] 75.03 <C> 73.52 <C> - <C> 72.89 <CAP> Table 2. Comparison of MT-BioNER model and recent state-of-the-art models.Source of benchmark performance scores of datasets are: NCBI-disease: (Leaman and Lu, 2016); BC5CDR: (Li et al., 2015); JNLPBA: (GuoDong and Jian, 2004); MTM-CW was proposed by (Wang et al., 2018); BiLSTM was proposed by (Habibi et al., 2017). The performance scores for these NER models are referred from (Sachan et al., 2017)
<R> <C> [EMPTY] <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1 <R> <C> Aggregate Cosine Similarity Scores <C> 0.602 <C> 0.563 <C> 0.574 <R> <C> MLP Baseline (No MWEs) <C> 0.611 <C> 0.567 <C> 0.579 <R> <C> RBF Baseline (No MWEs) <C> 0.603 <C> 0.618 <C> 0.606 <R> <C> [BOLD] MLP (w/ MWEs) <C> 0.717 <C> 0.666 <C> 0.681 <R> <C> Appearance <C> 0.886 <C> 0.414 <C> 0.564 <R> <C> Interpersonal <C> 0.548 <C> 0.453 <C> 0.496 <R> <C> Mood <C> 0.691 <C> 0.430 <C> 0.530 <R> <C> Occupation <C> 0.826 <C> 0.461 <C> 0.592 <R> <C> Substance <C> 0.920 <C> 0.703 <C> 0.797 <R> <C> Thought Content <C> 0.926 <C> 0.590 <C> 0.721 <R> <C> Thought Process <C> 0.654 <C> 0.617 <C> 0.635 <R> <C> Other <C> 0.632 <C> 0.798 <C> 0.710 <R> <C> [BOLD] RBF (w/ MWEs) <C> 0.684 <C> 0.630 <C> 0.645 <R> <C> Appearance <C> 0.670 <C> 0.490 <C> 0.566 <R> <C> Interpersonal <C> 0.410 <C> 0.493 <C> 0.448 <R> <C> Mood <C> 0.655 <C> 0.399 <C> 0.496 <R> <C> Occupation <C> 0.720 <C> 0.501 <C> 0.598 <R> <C> Substance <C> 0.866 <C> 0.730 <C> 0.792 <R> <C> Thought Content <C> 0.892 <C> 0.547 <C> 0.678 <R> <C> Thought Process <C> 0.569 <C> 0.691 <C> 0.624 <R> <C> Other <C> 0.651 <C> 0.650 <C> 0.651 <CAP> Table 5: Overall and domain-specific Precision, Recall, and F1 scores for our models. The first row computes similarity directly from the TF-IDF matrix, as in McCoy et al. (2015). All other rows are classifier outputs.
<R> <C> Model <C> Dataset <C> Error <C> Script <R> <C> Google Word2Vec & GloVe <C> FCE <C> - <C> 3 <R> <C> Google Word2Vec & GloVe <C> FCEext <C> - <C> 3 <R> <C> SSWE, ESWE & ECSWE <C> FCE <C> 3 <C> 3 <R> <C> SSWE, ESWE & ECSWE <C> FCEext <C> 9 <C> 9 <CAP> Table 1: Error and script refer to their filter sizes. For each of the 5 pre-training models on the two datasets, the error filter size is displayed (if applicable) along with the script filter size used in the AA network initialized with the embeddings on the left. FCE refers to the public FCE.
<R> <C> Model <C> Public FCE <C> FCEext <R> <C> Random Baseline <C> 0.258 <C> 0.494 <R> <C> SSWE <C> 0.251 <C> 0.480 <R> <C> ESWE <C> [BOLD] 0.472 <C> [BOLD] 0.539 <CAP> Table 4: AP results of the random baseline and SSWE and EWE models when trained on public and extended FCE sets and tested on the respective dev sets. The AP is calculated with respect to the error class.
<R> <C> [EMPTY] <C> [BOLD] Sentences with discontinuous mentions  [BOLD] CADEC <C> [BOLD] Sentences with discontinuous mentions  [BOLD] CADEC <C> [BOLD] Sentences with discontinuous mentions  [BOLD] CADEC <C> [BOLD] Sentences with discontinuous mentions  [BOLD] ShARe 13 <C> [BOLD] Sentences with discontinuous mentions  [BOLD] ShARe 13 <C> [BOLD] Sentences with discontinuous mentions  [BOLD] ShARe 13 <C> [BOLD] Sentences with discontinuous mentions  [BOLD] ShARe 14 <C> [BOLD] Sentences with discontinuous mentions  [BOLD] ShARe 14 <C> [BOLD] Sentences with discontinuous mentions  [BOLD] ShARe 14 <C> [BOLD] Discontinuous mentions only  [BOLD] CADEC <C> [BOLD] Discontinuous mentions only  [BOLD] CADEC <C> [BOLD] Discontinuous mentions only  [BOLD] CADEC <C> [BOLD] Discontinuous mentions only  [BOLD] ShARe 13 <C> [BOLD] Discontinuous mentions only  [BOLD] ShARe 13 <C> [BOLD] Discontinuous mentions only  [BOLD] ShARe 13 <C> [BOLD] Discontinuous mentions only  [BOLD] ShARe 14 <C> [BOLD] Discontinuous mentions only  [BOLD] ShARe 14 <C> [BOLD] Discontinuous mentions only  [BOLD] ShARe 14 <R> <C> [BOLD] Model <C> P <C> R <C> F <C> P <C> R <C> F <C> P <C> R <C> F <C> P <C> R <C> F <C> P <C> R <C> F <C> P <C> R <C> F <R> <C> Flat <C> 50.2 <C> 36.7 <C> 42.4 <C> 43.5 <C> 28.1 <C> 34.2 <C> 41.5 <C> 31.9 <C> 36.0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> BIO E. <C> 63.8 <C> 52.0 <C> 57.3 <C> 51.8 <C> 39.5 <C> 44.8 <C> 37.5 <C> 38.4 <C> 37.9 <C> 5.8 <C> 1.0 <C> 1.8 <C> 39.7 <C> 12.3 <C> 18.8 <C> 8.8 <C> 4.5 <C> 6.0 <R> <C> Graph <C> [BOLD] 69.5 <C> 43.2 <C> 53.3 <C> [BOLD] 82.3 <C> 47.4 <C> 60.2 <C> 60.0 <C> 52.8 <C> 56.2 <C> [BOLD] 60.8 <C> 14.8 <C> 23.9 <C> 78.4 <C> 36.6 <C> 50.0 <C> 42.7 <C> 39.5 <C> 41.1 <R> <C> Ours <C> 66.5 <C> [BOLD] 64.3 <C> [BOLD] 65.4 <C> 70.5 <C> [BOLD] 56.8 <C> [BOLD] 62.9 <C> [BOLD] 61.9 <C> [BOLD] 64.5 <C> [BOLD] 63.1 <C> 41.2 <C> [BOLD] 35.1 <C> [BOLD] 37.9 <C> [BOLD] 78.5 <C> [BOLD] 39.4 <C> [BOLD] 52.5 <C> [BOLD] 56.1 <C> [BOLD] 43.8 <C> [BOLD] 49.2 <CAP> Table 3: Evaluation results on sentences that contain at least one discontinuous mention (left part) and on discontinuous mentions only (right part).
<R> <C> Data <C> No. of tokens <R> <C> Train <C> 9,918,442 <R> <C> Test <C> 1,088,565 <CAP> Table 1. Kaggle Dataset
<R> <C> Prompt ID <C> Essays <C> Score Range <R> <C> 1 <C> 1783 <C> 2-12 <R> <C> 2 <C> 1800 <C> 1-6 <R> <C> 3 <C> 1726 <C> 0-3 <R> <C> 4 <C> 1772 <C> 0-3 <R> <C> 5 <C> 1805 <C> 0-4 <R> <C> 6 <C> 1800 <C> 0-4 <R> <C> 7 <C> 1569 <C> 0-30 <R> <C> 8 <C> 723 <C> 0-60 <CAP> TABLE II: Details about ASAP dataset
<R> <C> Methods <C> Adver1 <C> Adver2 <C> Adver1+Adver2 <R> <C> TSLF-1 <C> 0.119 <C> 0.169 <C> 0.160 <R> <C> TSLF-2 <C> 0.128 <C> 0.178 <C> 0.060 <R> <C> TSLF-ALL <C> [BOLD] 0.651 <C> [BOLD] 0.883 <C> [BOLD] 0.709 <CAP> TABLE IV: Results On ASAP Dataset with adverserial samples
<R> <C> [BOLD] Lang. <C> [BOLD] Dur. (hh:mm:ss) <C> [BOLD] # Workers <C> [BOLD] Speech Naturalness Tacotron <C> [BOLD] Speech Naturalness DCTTS <C> [BOLD] Pronunciation Accuracy Tacotron <C> [BOLD] Pronunciation Accuracy DCTTS <R> <C> de <C> 16:08:01 <C> 75 <C> 2.82±0.26 <C> 3.59±0.24 <C> 4.31±0.22 <C> 4.25±0.24 <R> <C> el <C> 04:08:14 <C> 43 <C> [EMPTY] <C> 3.47±0.41 <C> [EMPTY] <C> 3.70±0.39 <R> <C> es <C> 23:49:49 <C> 78 <C> 3.45±0.38 <C> 4.31±0.19 <C> 4.31±0.40 <C> 4.72±0.13 <R> <C> fi <C> 10:32:03 <C> 62 <C> 3.69±0.36 <C> 4.15±0.27 <C> 4.16±0.30 <C> 4.43±0.27 <R> <C> fr <C> 19:09:03 <C> 75 <C> 2.51±0.30 <C> 3.50±0.27 <C> 3.48±0.48 <C> 4.24±0.25 <R> <C> hu <C> 10:00:25 <C> 39 <C> 3.60±0.51 <C> 4.21±0.39 <C> 4.27±0.40 <C> 4.51±0.35 <R> <C> ja <C> 14:55:36 <C> 79 <C> 3.39±0.48 <C> 4.01±0.24 <C> 3.61±0.48 <C> 4.20±0.21 <R> <C> nl <C> 14:06:40 <C> 97 <C> 3.06±0.35 <C> 3.63±0.21 <C> 3.82±0.31 <C> 3.93±0.20 <R> <C> ru <C> 21:22:10 <C> 73 <C> 2.55±0.37 <C> 3.43±0.35 <C> 3.64±0.39 <C> 4.02±0.32 <R> <C> zh <C> 06:27:04 <C> 122 <C> 3.41±0.31 <C> 4.10±0.19 <C> 4.19±0.20 <C> 4.46±0.17 <CAP> Table 3: MOS scores with 95% C.I. of Tacotron and DCTTS for all languages.
<R> <C> # target train samples Domain <C> 0 CT <C> ST <C> 5 MT <C> CT <C> ST <C> 20 MT <C> CT <C> ST <C> 100 MT <C> CT <C> ST <C> 1000 MT <C> CT <R> <C> book_room <C> [BOLD] 0.48 <C> 0.09 <C> 0.40 <C> [BOLD] 0.49 <C> 0.28 <C> 0.53 <C> [BOLD] 0.56 <C> 0.50 <C> 0.61 <C> [BOLD] 0.65 <C> - <C> - <C> - <R> <C> bus_tickets <C> [BOLD] 0.45 <C> 0.11 <C> 0.40 <C> [BOLD] 0.63 <C> 0.34 <C> 0.63 <C> [BOLD] 0.72 <C> 0.60 <C> 0.74 <C> [BOLD] 0.78 <C> - <C> - <C> - <R> <C> flights_1 <C> [BOLD] 0.19 <C> 0.19 <C> 0.41 <C> [BOLD] 0.42 <C> 0.46 <C> 0.59 <C> [BOLD] 0.63 <C> 0.66 <C> 0.71 <C> [BOLD] 0.75 <C> 0.77 <C> 0.79 <C> [BOLD] 0.81 <R> <C> flights_2 <C> [BOLD] 0.45 <C> 0.04 <C> 0.33 <C> [BOLD] 0.56 <C> 0.33 <C> 0.59 <C> [BOLD] 0.69 <C> 0.58 <C> 0.66 <C> [BOLD] 0.73 <C> - <C> - <C> - <R> <C> fare <C> [BOLD] 0.04 <C> 0.53 <C> [BOLD] 0.77 <C> 0.71 <C> 0.84 <C> 0.90 <C> [BOLD] 0.92 <C> 0.93 <C> 0.94 <C> [BOLD] 0.95 <C> [BOLD] 0.98 <C> [BOLD] 0.98 <C> [BOLD] 0.98 <R> <C> book_hotel <C> [BOLD] 0.45 <C> 0.08 <C> 0.31 <C> [BOLD] 0.49 <C> 0.38 <C> 0.61 <C> [BOLD] 0.64 <C> 0.69 <C> [BOLD] 0.80 <C> 0.78 <C> 0.88 <C> [BOLD] 0.90 <C> [BOLD] 0.90 <R> <C> find_restaurants <C> [BOLD] 0.35 <C> 0.19 <C> 0.37 <C> [BOLD] 0.52 <C> 0.44 <C> 0.60 <C> [BOLD] 0.62 <C> 0.69 <C> [BOLD] 0.75 <C> 0.74 <C> 0.82 <C> [BOLD] 0.84 <C> [BOLD] 0.84 <R> <C> appointments <C> [BOLD] 0.56 <C> 0.24 <C> 0.55 <C> [BOLD] 0.63 <C> 0.46 <C> 0.65 <C> [BOLD] 0.67 <C> 0.65 <C> 0.72 <C> [BOLD] 0.73 <C> 0.78 <C> 0.79 <C> [BOLD] 0.80 <R> <C> reserve_restaurant <C> [BOLD] 0.56 <C> 0.20 <C> 0.50 <C> [BOLD] 0.64 <C> 0.50 <C> 0.68 <C> [BOLD] 0.70 <C> 0.72 <C> [BOLD] 0.79 <C> 0.78 <C> 0.82 <C> 0.85 <C> [BOLD] 0.86 <R> <C> book_cab <C> [BOLD] 0.18 <C> 0.46 <C> 0.63 <C> [BOLD] 0.65 <C> 0.56 <C> 0.72 <C> [BOLD] 0.74 <C> 0.78 <C> 0.82 <C> [BOLD] 0.84 <C> 0.89 <C> 0.90 <C> [BOLD] 0.92 <CAP> Table 3: Weighted token F1 scores at various points on the learning curve for the compared models. ST corresponds to the single task baseline, MT corresponds to the multi task baseline and CT corresponds to the general concept tagging model.
<R> <C> [EMPTY] <C> Dev <C> Dev <C> Dev <C> Test Overall <C> Test Overall <C> Test Overall <C> Test Warm-up questions <C> Test Warm-up questions <C> Test Warm-up questions <C> Test User-provided <C> Test User-provided <C> Test User-provided <R> <C> [EMPTY] <C> F1 <C> EM <C> C <C> F1 <C> EM <C> C <C> F1 <C> EM <C> C <C> F1 <C> EM <C> C <R> <C> [ITALIC] Human <C> - <C> - <C> - <C> [ITALIC] 95.3 <C> [ITALIC] 84.5 <C> [ITALIC] 82.5 <C> [ITALIC] 95.7 <C> [ITALIC] 89.7 <C> [ITALIC] 90.9 <C> [ITALIC] 95.1 <C> [ITALIC] 82.4 <C> [ITALIC] 79.3 <R> <C> BERT-base <C> 67.6 <C> 39.6 <C> 24.3 <C> 67.2 <C> 39.8 <C> 23.6 <C> 72.9 <C> 46.2 <C> 28.8 <C> 64.8 <C> 37.1 <C> 21.3 <R> <C> BERT-large <C> 72.8 <C> 46.0 <C> 30.7 <C> 71.9 <C> 45.9 <C> 29.1 <C> 75.0 <C> 50.1 <C> 30.3 <C> 70.6 <C> 44.1 <C> 28.5 <R> <C> RoBERTa-base <C> 72.2 <C> 44.5 <C> 28.7 <C> 72.6 <C> 45.7 <C> 29.9 <C> 75.4 <C> 48.8 <C> 32.3 <C> 71.4 <C> 44.4 <C> 28.8 <R> <C> RoBERTa-large <C> [BOLD] 75.7 <C> [BOLD] 50.4 <C> [BOLD] 36.0 <C> [BOLD] 75.2 <C> [BOLD] 51.1 <C> [BOLD] 34.5 <C> [BOLD] 77.3 <C> [BOLD] 54.3 <C> [BOLD] 36.1 <C> [BOLD] 74.3 <C> [BOLD] 49.8 <C> [BOLD] 33.8 <CAP> Table 4: Human/system performance on the test set of Torque. System performance is averaged from 3 runs; all std. dev. were ≤4% and those in [1%, 4%] are underlined. C (consistency) is the percentage of contrast groups for which a model’s predictions have F1≥80% for all questions in a group Gardner et al. (2020).
<R> <C> [EMPTY] <C> BLEU-4 <C> BLEU-3 <C> BLEU-2 <C> BLEU-1 <R> <C> Smt-H <C> [BOLD] 16.62 <C> [BOLD] 18.99 <C> [BOLD] 22.41 <C> [BOLD] 29.57 <R> <C> Smt <C> 15.13 <C> 17.39 <C> 20.62 <C> 27.39 <R> <C> Seq2Seq <C> 16.03 <C> 17.76 <C> 20.63 <C> 27.66 <R> <C> Ir-Uu <C> 2.6 <C> 3.51 <C> 5.52 <C> 12.53 <R> <C> Ir-Ur <C> 4.4 <C> 5.15 <C> 6.83 <C> 13.13 <R> <C> Ir-Cxt <C> 3.14 <C> 4.17 <C> 6.34 <C> 13.76 <R> <C> Rnd <C> 0.00 <C> 0.00 <C> 1.65 <C> 9.73 <CAP> Table 1: Automatic Evaluation Result of Smt, Smt-H and Seq2Seq
<R> <C> [EMPTY] <C> #Questions <C> #Queries <C> #DB <C> #Table/DB <C> Table Cov. <C> Attr Cov. <C> MSTTR <C> Avg. #Tokens <C> Ann. Time <R> <C> Spider <C> 10,181 <C> 5,693 <C> 200 <C> 5.1 <C> 0.917 (0.87) <C> 0.621 (0.496) <C> 0.519 <C> 12.67 <C> 360 sec. <R> <C> LC-QuaD 2.0 <C> 30,000 <C> 30,000 <C> 1 <C> 157,068 <C> 0.019 <C> 0.187 <C> 0.761 <C> 10.6 <C> - <R> <C> OTTA (ours) <C> 3,792 <C> 3,792 <C> 5 <C> 11.8 <C> 0.949 <C> 0.544 <C> 0.67 <C> 13.53 <C> 98 sec. <CAP> Table 2: Comparison of our corpus OTTA to the Spider and LC-QuaD 2.0 corpora. Note that the number of databases in LC-QuaD 2.0 is only 1, since it is an open-domain knowledge base, and the number of tables corresponds to the number of different classes. Numbers in parentheses only consider databases with more than 5 tables.
<R> <C> [BOLD] Law Area <C> [BOLD] # of cases <R> <C> CHAMBRE_SOCIALE <C> 33,139 <R> <C> CHAMBRE_CIVILE_1 <C> 20,838 <R> <C> CHAMBRE_CIVILE_2 <C> 19,772 <R> <C> CHAMBRE_CRIMINELLE <C> 18,476 <R> <C> CHAMBRE_COMMERCIALE <C> 18,339 <R> <C> CHAMBRE_CIVILE_3 <C> 15,095 <R> <C> ASSEMBLEE_PLENIERE <C> 544 <R> <C> CHAMBRE_MIXTE <C> 222 <CAP> Table 1: Distribution of Law Area labels over the Case Descriptions
<R> <C> [EMPTY] <C> QS <C> AE <R> <C> [EMPTY] <C> Acc <C> prec recall F1 <R> <C> biLSTM+attn (Small) <C> 0.73 <C> 0.91 0.92 0.92 <R> <C> biLSTM+attn (Regular) <C> 0.75 <C> 0.91 0.88 0.90 <R> <C> Transformer (Small) <C> 0.71 <C> 0.93 0.91 0.92 <R> <C> Transformer (Regular) <C> 0.72 <C> 0.93 0.90 0.91 <R> <C> SAMIE (Small) <C> 0.81 <C> 0.88 0.88 0.88 <R> <C> SAMIE (Regular) <C> 0.92 <C> 0.95 0.94 0.95 <CAP> Table 1: Detailed comparison with 512 labeled sentences on ATIS.
<R> <C> [BOLD] Sparsity  [BOLD] Level <C> [BOLD] Model  [BOLD] Type <C> [BOLD] Dev  [BOLD] clean <C> [BOLD] Dev  [BOLD] other <C> [BOLD] Test  [BOLD] clean <C> [BOLD] Test  [BOLD] other <R> <C> 0% <C> Zeyer et al. <C> 3.54 <C> 11.52 <C> 3.82 <C> 12.76 <R> <C> 0% <C> Single <C> [BOLD] 3.6 <C> [BOLD] 11.8 <C> [BOLD] 3.9 <C> [BOLD] 11.8 <R> <C> 0% <C> SNN <C> 4.7 <C> 14.2 <C> 4.8 <C> 14.5 <R> <C> 0% <C> DSNN <C> 3.7 <C> 12.3 <C> 4.0 <C> 12.4 <R> <C> 30% <C> Single <C> [BOLD] 3.7 <C> [BOLD] 12.3 <C> [BOLD] 4.0 <C> [BOLD] 12.2 <R> <C> 30% <C> SNN <C> 4.7 <C> 14.4 <C> 4.9 <C> 14.8 <R> <C> 30% <C> DSNN <C> [BOLD] 3.7 <C> [BOLD] 12.3 <C> [BOLD] 4.0 <C> 12.3 <R> <C> 60% <C> Single <C> 3.8 <C> 12.5 <C> 4.2 <C> 12.6 <R> <C> 60% <C> SNN <C> 5.9 <C> 16.8 <C> 6.3 <C> 17.5 <R> <C> 60% <C> DSNN <C> [BOLD] 3.7 <C> [BOLD] 12.3 <C> [BOLD] 4.0 <C> [BOLD] 12.3 <R> <C> 90% <C> Single <C> 4.2 <C> 13.9 <C> 4.5 <C> 14.1 <R> <C> 90% <C> SNN <C> 8.0 <C> 20.3 <C> 8.6 <C> 21.5 <R> <C> 90% <C> DSNN <C> [BOLD] 3.9 <C> [BOLD] 13.2 <C> [BOLD] 4.2 <C> [BOLD] 13.2 <CAP> Table 1: WER for single sparsity networks (Single), SNN, and DSNN. Lower is better. We also show the LAS performance in [25] (Zeyer et al.) as a baseline for comparison.
<R> <C> [BOLD] Sparsity  [BOLD] Level <C> [BOLD] Model  [BOLD] Setting <C> [BOLD] Dev  [BOLD] clean <C> [BOLD] Dev  [BOLD] other <C> [BOLD] Test  [BOLD] clean <C> [BOLD] Test  [BOLD] other <R> <C> 0% <C> Baseline DSNN <C> 4.2 <C> 12.9 <C> 4.4 <C> 13.1 <R> <C> 0% <C> + pretraining <C> 3.8 <C> [BOLD] 12.3 <C> 4.1 <C> [BOLD] 12.3 <R> <C> 0% <C> + GA <C> [BOLD] 3.7 <C> [BOLD] 12.3 <C> [BOLD] 4.0 <C> 12.4 <R> <C> 30% <C> Baseline DSNN <C> 4.2 <C> 12.9 <C> 4.4 <C> 13.1 <R> <C> 30% <C> + pretraining <C> 3.8 <C> [BOLD] 12.3 <C> 4.1 <C> [BOLD] 12.3 <R> <C> 30% <C> + GA <C> [BOLD] 3.7 <C> [BOLD] 12.3 <C> [BOLD] 4.0 <C> [BOLD] 12.3 <R> <C> 60% <C> Baseline DSNN <C> 4.2 <C> 12.9 <C> 4.4 <C> 13.2 <R> <C> 60% <C> + pretraining <C> 3.8 <C> [BOLD] 12.3 <C> 4.1 <C> [BOLD] 12.3 <R> <C> 60% <C> + GA <C> [BOLD] 3.7 <C> [BOLD] 12.3 <C> [BOLD] 4.0 <C> [BOLD] 12.3 <R> <C> 90% <C> Baseline DSNN <C> 4.3 <C> 13.4 <C> 4.5 <C> 13.6 <R> <C> 90% <C> + pretraining <C> 4.0 <C> 13.4 <C> 4.3 <C> 13.3 <R> <C> 90% <C> + GA <C> [BOLD] 3.9 <C> [BOLD] 13.2 <C> [BOLD] 4.2 <C> [BOLD] 13.2 <CAP> Table 2: WER for incrementally adding pretraining and gradient accumulation (GA) on top of a baseline DSNN. Lower is better.
<R> <C> # of Hops <C> Snippets <C> TagMyNews <C> Twitter <C> Weibo <R> <C> TMN-1H <C> 0.958 <C> 0.841 <C> 0.382 <C> 0.568 <R> <C> TMN-2H <C> [BOLD] 0.964 <C> 0.843 <C> 0.383 <C> 0.578 <R> <C> TMN-3H <C> 0.962 <C> 0.845 <C> 0.384 <C> 0.581 <R> <C> TMN-4H <C> 0.961 <C> 0.846 <C> 0.389 <C> 0.582 <R> <C> TMN-5H <C> 0.960 <C> [BOLD] 0.851 <C> [BOLD] 0.397 <C> [BOLD] 0.591 <R> <C> TMN-6H <C> 0.958 <C> 0.848 <C> 0.388 <C> 0.579 <CAP> Table 6: The impact of the # of hops on accuracy.
<R> <C> Train <C> # of sentences <C> [BOLD] Lap. 3,045 <C> [BOLD] Rest. 3,041 <R> <C> Train <C> # of aspects <C> 2,358 <C> 3,693 <R> <C> Train <C> multi-aspects [%] <C> 37 <C> 25 <R> <C> Test <C> # of sentences <C> 800 <C> 800 <R> <C> Test <C> # of aspects <C> 654 <C> 1,134 <R> <C> Test <C> multi-aspects [%] <C> 44 <C> 28 <R> <C> All <C> # of sentences <C> 3,845 <C> 3,841 <R> <C> All <C> # of aspects <C> 3,012 <C> 4,827 <CAP> Table 3: SemEval 2014 datasets profile for Laptops and Restaurants. Multi-aspect means fraction of ngram aspects (two and more words).
<R> <C> [BOLD] Model <C> [BOLD] Laptops <C> [BOLD] Restaurants <R> <C> DLIREC <C> 73.78 <C> 84.01 <R> <C> IHS R&D <C> 74.55 <C> 79.62 <R> <C> RNCRF-O <C> 74.52 <C> 82.73 <R> <C> RNCRF-F <C> [BOLD] 78.42 <C> 84.93 <R> <C> CNN-Glove.840B <C> 77.36 <C> 82.76 <R> <C> Wo-BiLSTM-CRF-fastText <C> 79.34 <C> 85.28 <R> <C> WoCh-BiLSTM-CRF-fastText <C> 79.73 <C> [BOLD] 85.69 <R> <C> Wo-BiLSTM-CRF-Glove.840B <C> 79.99 <C> 84.96 <R> <C> WoCh-BiLSTM-CRF-Glove.840B <C> [BOLD] 80.13 <C> 85.2 <CAP> Table 6: Comparison of F1 scores on SemEval 2014.
<R> <C> [BOLD] Model <C> [BOLD] Train/Test <C> [BOLD] Exact Match <C> [BOLD] F1 <R> <C> DrQA (MC) <C> 47,605/9,966 <C> 59.2% <C> 60.6 <R> <C> Class Prediction <C> 1276/320 <C> 36.6% <C> n.a <CAP> Table 7: Performance of baseline models on the two QA sub tasks, machine comprehension (MC) and class prediction.
<R> <C> [BOLD] Group <C> [BOLD] Models <C> [BOLD] Acc. <R> <C> [EMPTY] <C> Two-way attention <C> 74.78 <R> <C> RTE <C> Decomposable attention <C> 76.26 <R> <C> [EMPTY] <C> RTE-BERT <C> 79.75 <R> <C> Sent <C> BiLSTM-self-attention <C> 75.41 <R> <C> [EMPTY] <C> Sent-BERT <C> 81.79 <R> <C> Proposed <C> Our model (GloVe) <C> 82.20 <R> <C> [EMPTY] <C> Our model (BERT) <C> [BOLD] 86.23 <CAP> Table 2. Comparing the accuracy of explanation classifiers.
<R> <C> 3-11 <C> 3-11 <C> [BOLD] Homogeneity (Precision)  [BOLD] k-means <C> [BOLD] Homogeneity (Precision)  [BOLD] GMM <C> [BOLD] Homogeneity (Precision)  [BOLD] Cor. Cluster. <C> [BOLD] Completeness (Recall)  [BOLD] k-means <C> [BOLD] Completeness (Recall)  [BOLD] GMM <C> [BOLD] Completeness (Recall)  [BOLD] Cor. Cluster. <C> [BOLD] V-measure (F1)  [BOLD] k-means <C> [BOLD] V-measure (F1)  [BOLD] GMM <C> [BOLD] V-measure (F1)  [BOLD] Cor. Cluster. <R> <C> Hotel <C> GloVe <C> 0.6695 <C> 0.6785 <C> 0.7240 <C> 0.7577 <C> 0.7728 <C> 0.6756 <C> 0.7102 <C> 0.7219 <C> 0.6985 <R> <C> Hotel <C> ABAE <C> 0.6626 <C> 0.6628 <C> 0.6964 <C> 0.7609 <C> 0.7522 <C> 0.7113 <C> 0.7075 <C> 0.7039 <C> 0.6966 <R> <C> Hotel <C> Our method <C> [BOLD] 0.7073 <C> [BOLD] 0.7177 <C> [BOLD] 0.7460 <C> [BOLD] 0.8115 <C> [BOLD] 0.8184 <C> [BOLD] 0.8370 <C> [BOLD] 0.7551 <C> [BOLD] 0.7641 <C> [BOLD] 0.7848 <R> <C> Restaurant <C> GloVe <C> 0.5854 <C> 0.5509 <C> 0.5851 <C> 0.8168 <C> 0.7801 <C> 0.8103 <C> 0.6778 <C> 0.6413 <C> 0.6761 <R> <C> Restaurant <C> ABAE <C> 0.5563 <C> 0.5553 <C> [BOLD] 0.6256 <C> 0.7927 <C> 0.7779 <C> 0.7819 <C> 0.6492 <C> 0.6432 <C> 0.6918 <R> <C> Restaurant <C> Our method <C> [BOLD] 0.5920 <C> [BOLD] 0.5572 <C> 0.6158 <C> [BOLD] 0.8333 <C> [BOLD] 0.8111 <C> [BOLD] 0.8155 <C> [BOLD] 0.6877 <C> [BOLD] 0.6555 <C> [BOLD] 0.6985 <CAP> Table 3. Opinion canonicalization performance on Hotel and Restaurant datasets.
<R> <C> [BOLD] Feature <C> [BOLD] Accuracy (%) <R> <C> Majority Class Baseline <C> 50.1 <R> <C> Oracle <C> 91.6 <R> <C> Character bigrams <C> 73.6 <R> <C> Character trigrams <C> 77.2 <R> <C> Character 4-grams <C> [BOLD] 78.0 <R> <C> Character 5-grams <C> 77.9 <R> <C> Character 6-grams <C> 77.2 <R> <C> Character 7-grams <C> 76.5 <R> <C> Character 8-grams <C> 75.8 <R> <C> Word unigrams <C> 77.5 <R> <C> Word bigrams <C> 73.8 <R> <C> Word trigrams <C> 67.4 <R> <C> 1-skip Word bigrams <C> 74.0 <R> <C> 2-skip Word bigrams <C> 73.8 <R> <C> 3-skip Word bigrams <C> 73.9 <R> <C> All features combined <C> 77.5 <CAP> Table 2: Classification results under 10-fold cross-validation.
<R> <C> [EMPTY] <C> [EMPTY] <C> Avg. Development 1 s <C> Avg. Development 10 s <C> Avg. Development 2 min <C> S1 – German 1 s <C> S1 – German 10 s <C> S1 – German 2 min <C> S2 – Wolof 1 s <C> S2 – Wolof 10 s <C> S2 – Wolof 2 min <R> <C> [BOLD] Baseline [18.84] <C> Within <C> 12 <C> 12.1 <C> 12.1 <C> 14.2 <C> 13.3 <C> 12.9 <C> 14.1 <C> 14.3 <C> 14.1 <R> <C> [EMPTY] <C> Between <C> 23.3 <C> 23.4 <C> 23.3 <C> 27.5 <C> 27.3 <C> 27.1 <C> 30 <C> 29.5 <C> 29.5 <R> <C> [BOLD] Topline [7.19] <C> Within <C> 8 <C> 5.4 <C> 5.3 <C> 8.7 <C> 7.1 <C> 7.0 <C> 6.6 <C> 4.6 <C> 3.4 <R> <C> [EMPTY] <C> Between <C> 9.6 <C> 7.2 <C> 6.9 <C> 12.8 <C> 10.5 <C> 10.4 <C> 7.1 <C> 3.6 <C> 4.3 <R> <C> [BOLD] H [9.16] <C> Within <C> 8.5 <C> 7.6 <C> 7.4 <C> 6.5 <C> 5.6 <C> 5.3 <C> 10.9 <C> 8.8 <C> 8.4 <R> <C> [EMPTY] <C> Between <C> 10.8 <C> 9.3 <C> 9.0 <C> 11.9 <C> 10.0 <C> 9.7 <C> 13.0 <C> 10.0 <C> 9.9 <R> <C> [BOLD] P1 [14.96] <C> Within <C> 10.9 <C> 8.8 <C> 8.7 <C> 8.9 <C> 6.7 <C> 6.4 <C> 13.3 <C> 11.9 <C> 11.8 <R> <C> [EMPTY] <C> Between <C> 17.5 <C> 15.8 <C> 15.7 <C> 19.4 <C> 16.2 <C> 15.9 <C> 22.8 <C> 23.1 <C> 23.1 <R> <C> [BOLD] P2 [14.9] <C> Within <C> 10.8 <C> 8.7 <C> 8.5 <C> 8.8 <C> 6.6 <C> 6.3 <C> 13.1 <C> 11.7 <C> 11.7 <R> <C> [EMPTY] <C> Between <C> 17.5 <C> 15.8 <C> 15.7 <C> 19.2 <C> 16.3 <C> 16.0 <C> 23.3 <C> 23.3 <C> 23.1 <R> <C> [BOLD] C1 [11.95] <C> Within <C> 10.0 <C> 8.4 <C> 8.3 <C> 7.6 <C> 6.22 <C> 6.3 <C> 11.7 <C> 9.9 <C> 9.8 <R> <C> [EMPTY] <C> Between <C> 14.5 <C> 12.8 <C> 12.5 <C> 15.5 <C> 12.9 <C> 12.7 <C> 17.6 <C> 16.9 <C> 16.3 <R> <C> [BOLD] C2 [11.24] <C> Within <C> 10.1 <C> 8.5 <C> 8.4 <C> 7.6 <C> 6.2 <C> 6.1 <C> 11.6 <C> 9.8 <C> 9.6 <R> <C> [EMPTY] <C> Between <C> 13.9 <C> 11.9 <C> 11.7 <C> 14.7 <C> 11.7 <C> 11.6 <C> 16.9 <C> 14.7 <C> 14.4 <R> <C> [BOLD] A1 [12.3] <C> Within <C> 8.8 <C> 7.9 <C> 7.8 <C> 6.9 <C> 6.1 <C> 6.0 <C> 9.9 <C> 9.2 <C> 9.1 <R> <C> [EMPTY] <C> Between <C> 15.2 <C> 14.2 <C> 14.0 <C> 16.9 <C> 14.7 <C> 14.7 <C> 18.8 <C> 17.7 <C> 17.7 <CAP> Table 2: Track 1 within and between talker ABX scores for three test files durations, for the development (average) and surprise languages, for the 14 submitted systems (the mean performance on the surprise set is in square brackets). The three supervised systems are listed separately. See http://zerospeech.com/2017 for the full table including development languages.
<R> <C> [EMPTY] <C> [EMPTY] <C> Avg. Development 1 s <C> Avg. Development 10 s <C> Avg. Development 2 min <C> S1 – German 1 s <C> S1 – German 10 s <C> S1 – German 2 min <C> S2 – Wolof 1 s <C> S2 – Wolof 10 s <C> S2 – Wolof 2 min <R> <C> [BOLD] A2 [11.95] <C> Within <C> 8.8 <C> 7.8 <C> 7.7 <C> 6.8 <C> 6.0 <C> 6.0 <C> 10.1 <C> 9.6 <C> 9.6 <R> <C> [EMPTY] <C> Between <C> 14.5 <C> 13.4 <C> 13.3 <C> 16.0 <C> 14.0 <C> 13.9 <C> 17.9 <C> 16.9 <C> 16.6 <R> <C> [BOLD] A3 [11:93] <C> Within <C> 9.5 <C> 8.3 <C> 8.2 <C> 7.3 <C> 6.2 <C> 6.1 <C> 11.1 <C> 10.3 <C> 10.2 <R> <C> [EMPTY] <C> Between <C> 14.5 <C> 13.3 <C> 13.2 <C> 15.5 <C> 13.5 <C> 13.4 <C> 17.6 <C> 16.0 <C> 16.0 <R> <C> [BOLD] A4 [12.17] <C> Within <C> 9.7 <C> 8.5 <C> 8.5 <C> 7.6 <C> 6.4 <C> 6.2 <C> 11.6 <C> 10.9 <C> 10.7 <R> <C> [EMPTY] <C> Between <C> 14.5 <C> 13.3 <C> 13.2 <C> 15.7 <C> 13.7 <C> 13.5 <C> 17.5 <C> 16.1 <C> 16.1 <R> <C> [BOLD] Y1 [12.43] <C> Within <C> 10.8 <C> 8.4 <C> 8.4 <C> 8.1 <C> 6.0 <C> 6.0 <C> 12.6 <C> 10.0 <C> 9.9 <R> <C> [EMPTY] <C> Between <C> 15.3 <C> 13 <C> 12.6 <C> 16.2 <C> 12.9 <C> 12.6 <C> 19.5 <C> 17.1 <C> 16.6 <R> <C> [BOLD] Y2 [12.29] <C> Within <C> 10.7 <C> 8.4 <C> 8.2 <C> 8.2 <C> 6.2 <C> 6.2 <C> 12.7 <C> 10.1 <C> 9.9 <R> <C> [EMPTY] <C> Between <C> 15.1 <C> 12.7 <C> 12.4 <C> 16.4 <C> 13.3 <C> 13.0 <C> 19.2 <C> 17.3 <C> 16.7 <R> <C> [BOLD] YS [12.32] <C> Within <C> 10.7 <C> 8.3 <C> 8.1 <C> 8.0 <C> 6.0 <C> 5.9 <C> 12.9 <C> 10.8 <C> 10.6 <R> <C> [EMPTY] <C> Between <C> 14.7 <C> 12.3 <C> 12.1 <C> 15.8 <C> 12.4 <C> 12.3 <C> 18.7 <C> 17.4 <C> 17.0 <R> <C> [BOLD] S1 [8.54] <C> Within <C> 8.7 <C> 8.3 <C> 7.3 <C> 6.3 <C> 5.8 <C> 5.0 <C> 9.0 <C> 8.7 <C> 7.2 <R> <C> [EMPTY] <C> Between <C> 11.4 <C> 10.4 <C> 9 <C> 11.6 <C> 9.9 <C> 8.7 <C> 11.5 <C> 10.2 <C> 8.6 <R> <C> [BOLD] S2 [6.99] <C> Within <C> 7.1 <C> 6.7 <C> 6.3 <C> 5.2 <C> 4.9 <C> 4.5 <C> 6.9 <C> 7.0 <C> 6.3 <R> <C> [EMPTY] <C> Between <C> 9.0 <C> 8.6 <C> 7.8 <C> 9.3 <C> 8.6 <C> 7.8 <C> 8.3 <C> 7.9 <C> 7.2 <CAP> Table 2: Track 1 within and between talker ABX scores for three test files durations, for the development (average) and surprise languages, for the 14 submitted systems (the mean performance on the surprise set is in square brackets). The three supervised systems are listed separately. See http://zerospeech.com/2017 for the full table including development languages.
<R> <C> [BOLD] Model <C> [BOLD] BPC <R> <C> td-LSTM (Zhang et al.,  2016 ) <C> 1.49 <R> <C> MI-LSTM (Wu et al.,  2016 ) <C> 1.44 <R> <C> mLSTM (Krause et al.,  2017 ) <C> 1.40 <R> <C> [BOLD] tanh RAN <C> 1.38 <R> <C> BatchNorm LSTM (Cooijmans et al.,  2017 ) <C> 1.36 <R> <C> LayerNorm HM-LSTM (Chung et al.,  2017 ) <C> 1.29 <R> <C> RHN (Zilly et al.,  2017 ) <C> 1.27 <CAP> Table 3: The performance of RAN and recently-published LSTM variants on the Text8 character-based language modeling benchmark, measured by bits per character (BPC).
<R> <C> [EMPTY] <C> BLEU <C> NIST <C> mWER <C> mPER <R> <C> baseline <C> 21.55 <C> 5.72 <C> 68.08 <C> 45.54 <R> <C> limited RO <C> 22.19 <C> 5.74 <C> 66.44 <C> 44.70 <R> <C> our approach <C> 24.47 <C> 5.88 <C> 64.71 <C> 43.89 <CAP> Table 3: Evaluation scores; RO: Reordering
<R> <C> Model <C> WMT test sets 2008-2016 BLEU <C> Idiom test set BLEU <C> Idiom test set Unigram Precision <C> Idiom test set Word-level Accuracy <R> <C> PBMT Baseline <C> 20.2 <C> 19.7 <C> 57.7 <C> 71.6 <R> <C> NMT Baseline <C> 26.9 <C> 24.8 <C> 53.2 <C> 67.8 <R> <C> NMT   token on source <C> 25.2 <C> 22.5 <C> 64.1 <C> 73.2 <CAP> Table 5: Translation performance on German idiom translation test set. Word-level Idiom Accuracy and Unigram Precision are computed only on the idiom phrase and its corresponding translation in the sentence.
<R> <C> Embedding <C> [BOLD] NYT-Location Micro-F1 <C> [BOLD] NYT-Location Macro-F1 <C> [BOLD] NYT-Topic Micro-F1 <C> [BOLD] NYT-Topic Macro-F1 <C> [BOLD] Yelp-Food Micro-F1 <C> [BOLD] Yelp-Food Macro-F1 <C> [BOLD] Yelp-Sentiment Micro-F1 <C> [BOLD] Yelp-Sentiment Macro-F1 <R> <C> Word2Vec <C> 0.533 <C> 0.467 <C> 0.588 <C> 0.695 <C> 0.540 <C> 0.528 <C> 0.723 <C> 0.715 <R> <C> GloVe <C> 0.521 <C> 0.455 <C> 0.563 <C> 0.688 <C> 0.515 <C> 0.503 <C> 0.720 <C> 0.711 <R> <C> fastText <C> 0.543 <C> 0.485 <C> 0.575 <C> 0.693 <C> 0.544 <C> 0.529 <C> 0.738 <C> 0.743 <R> <C> BERT <C> 0.301 <C> 0.288 <C> 0.328 <C> 0.451 <C> 0.330 <C> 0.404 <C> 0.695 <C> 0.674 <R> <C> CatE <C> [BOLD] 0.655 <C> [BOLD] 0.613 <C> [BOLD] 0.611 <C> [BOLD] 0.739 <C> [BOLD] 0.656 <C> [BOLD] 0.648 <C> [BOLD] 0.838 <C> [BOLD] 0.836 <CAP> Table 4. Weakly-supervised text classification evaluation based on WeSTClass (Meng et al., 2018) model.
<R> <C> [EMPTY] <C> RNNG−comp > ∅  [ITALIC] χ2 <C> RNNG−comp > ∅ df <C> RNNG−comp > ∅  [ITALIC] p <C> RNNG > RNNG−comp  [ITALIC] χ2 <C> RNNG > RNNG−comp df <C> RNNG > RNNG−comp  [ITALIC] p <R> <C> distance, “P600” region <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] k=200 <C> 13.409 <C> 1 <C> [BOLD] 0.00025 <C> 4.198 <C> 1 <C> 0.04047 <R> <C> [ITALIC] k=400 <C> 15.842 <C> 1 <C> < [BOLD] 0.0001 <C> 3.853 <C> 1 <C> 0.04966 <R> <C> [ITALIC] k=600 <C> 13.955 <C> 1 <C> [BOLD] 0.00019 <C> 3.371 <C> 1 <C> 0.06635 <R> <C> surprisal, “ANT” region <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] k=100 <C> 3.671 <C> 1 <C> 0.05537 <C> 13.167 <C> 1 <C> [BOLD] 0.00028 <R> <C> [ITALIC] k=200 <C> 3.993 <C> 1 <C> 0.04570 <C> 10.860 <C> 1 <C> [BOLD] 0.00098 <R> <C> [ITALIC] k=400 <C> 3.902 <C> 1 <C> 0.04824 <C> 10.189 <C> 1 <C> [BOLD] 0.00141 <R> <C> entropy Δ, “ANT” region <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] k=400 <C> 10.141 <C> 1 <C> [BOLD] 0.00145 <C> 5.273 <C> 1 <C> 0.02165 <CAP> Table 4: Likelihood-ratio tests indicate that regression models with predictors derived from RNNGs with syntactic composition (see Figure 2) do a better job than their degraded counterparts in accounting for the early peak in region “ANT” (right-hand columns). Similar comparisons in the “P600” region show that the model improves, but the improvement does not reach the α=0.002 significance threshold imposed by our Bonferroni correction (bold-faced text). RNNGs lacking syntactic composition do improve over a baseline model (∅) containing lexical predictors and an LSTM baseline (left-hand columns).
<R> <C> [BOLD] Model <C> [BOLD] TED  [BOLD] BLEU <C> [BOLD] TED  [BOLD] Meteor <C> [BOLD] News  [BOLD] BLEU <C> [BOLD] News  [BOLD] Meteor <C> [BOLD] Europarl  [BOLD] BLEU <C> [BOLD] Europarl  [BOLD] Meteor <R> <C> Transformer-DocNMT Zhang et al. ( 2018 ) <C> 24.00 <C> 44.69 <C> 23.08 <C> 42.40 <C> 29.32 <C> 46.72 <R> <C> HAN-DocNMT Miculicich et al. ( 2018 ) <C> 24.58 <C> 45.48 <C> [BOLD] 25.03 <C> [BOLD] 44.02 <C> 28.60 <C> 46.09 <R> <C> SAN-DocNMT Maruf et al. ( 2019 ) <C> 24.42 <C> 45.38 <C> 24.84 <C> 44.27 <C> 29.75 <C> 47.22 <R> <C> Transformer <C> 23.28 <C> 44.17 <C> 21.67 <C> 41.11 <C> 28.72 <C> 46.22 <R> <C> Transformer + Regularization Term <C> 24.55 <C> 45.57 <C> 22.09 <C> 41.77 <C> 29.42 <C> 47.59 <R> <C> Transformer + QCN <C> 24.41 <C> [BOLD] 46.09 <C> 22.22 <C> 41.90 <C> 29.48 <C> 47.49 <R> <C> Transformer + QCN + Regularization Term <C> [BOLD] 25.19 <C> 45.91 <C> 22.37 <C> 41.88 <C> [BOLD] 29.82 <C> [BOLD] 47.86 <CAP> Table 2: BLEU and Meteor scores of models. There exists three context-aware baseline models and a context-agnostic model. “Regularization Term” denotes integrating the regularization layer into the Transformer model and “QCN” denotes incorporating Query-guided Capsule Network in the Transformer model.
<R> <C> Layer (# of Clusters) <C> [ITALIC] System1 <C> [ITALIC] System2 <C> Stanford NER <C> [ITALIC] FIGER1 <R> <C> L1 (5) <C> 0.649 <C> 0.628 <C> 0.712 <C> 0.663 <R> <C> L2 (21) <C> 0.668 <C> 0.647 <C> 0.712 <C> 0.663 <R> <C> L3 (92) <C> 0.689 <C> 0.681 <C> 0.712 <C> 0.663 <R> <C> L4 (146) <C> 0.713 <C> 0.709 <C> 0.712 <C> 0.663 <R> <C> L5 (201) <C> 0.728 <C> 0.721 <C> 0.712 <C> 0.663 <CAP> Table 3: Coarse-grained Mention-level F-score Comparison (System1: based on perfect AMR, System2: based on system AMR)
<R> <C> [EMPTY] <C> Marlowe <C> Chapman <R> <C> Shakespeare (Com.) <C> 11.6 <C> 7.7 <R> <C> Shakespeare (His.) <C> 7.6 <C> 9.3 <CAP> TABLE VIII: Inter-profile dissimilarities (x100) between authors of different genres.
<R> <C> [EMPTY] <C> Sh. <C> Jon. <C> Fle. <C> Mid. <C> Cha. <C> Marl. <R> <C> Sh. <C> 17.6 <C> 16.8 <C> 17.3 <C> 16.7 <C> 17.1 <C> 18.2 <R> <C> Jon. <C> 16.8 <C> [BOLD] 16.8 <C> 17.0 <C> [BOLD] 16.5 <C> [BOLD] 16.7 <C> 17.3 <R> <C> Fle. <C> 17.3 <C> 17.0 <C> 18.7 <C> 17.6 <C> 17.4 <C> 17.9 <R> <C> Mid. <C> 16.7 <C> [BOLD] 16.5 <C> 17.6 <C> 17.6 <C> 16.9 <C> 17.1 <R> <C> Cha. <C> 17.1 <C> [BOLD] 16.7 <C> 17.4 <C> 16.9 <C> [BOLD] 17.5 <C> 17.8 <R> <C> Marl. <C> 17.4 <C> 17.1 <C> 17.6 <C> 17.3 <C> 17.4 <C> 18.1 <CAP> TABLE XI: Relative entropies from Eastward Ho to hybrid profiles composed of two authors (x100).
<R> <C> Nr. of authors <C> N. Bayes <C> 1-NN <C> 3-NN <C> DT-gdi <C> DT-ce <C> SVM <C> WAN <C> Voting <R> <C> 2 <C> 2.6 <C> 3.5 <C> 5.2 <C> 12.2 <C> 12.2 <C> 2.7 <C> [BOLD] 1.6 <C> 0.9 <R> <C> 4 <C> 6.0 <C> 9.2 <C> 12.4 <C> 25.3 <C> 25.5 <C> 6.8 <C> [BOLD] 4.6 <C> 3.3 <R> <C> 6 <C> 8.1 <C> 11.7 <C> 15.2 <C> 31.9 <C> 32.2 <C> 7.9 <C> [BOLD] 5.3 <C> 3.8 <R> <C> 8 <C> 9.6 <C> 15.4 <C> 19.2 <C> 36.4 <C> 37.2 <C> 11.1 <C> [BOLD] 6.7 <C> 5.2 <R> <C> 10 <C> 10.8 <C> 16.7 <C> 21.4 <C> 42.1 <C> 42.1 <C> 11.5 <C> [BOLD] 8.3 <C> 6.0 <CAP> TABLE XII: Error rates in % achieved by different methods for profiles of 100,000 words and texts of 10,000 words. The WANs achieve the smallest error rate among the methods considered separately. Voting decreases the error even further by combining the relational data of the WANs with the frequency data of other methods.
<R> <C> Neg. Samples <C> Cond? <C> AUROC <C> Top1 Acc <R> <C> 1 <C> ✓ <C> 0.983 <C> 0.647 <R> <C> 2 <C> ✓ <C> 0.982 <C> 0.664 <R> <C> 4 <C> ✓ <C> 0.981 <C> 0.680 <R> <C> 8 <C> ✓ <C> 0.978 <C> 0.656 <R> <C> 16 <C> ✓ <C> 0.980 <C> 0.673 <R> <C> 1 <C> ✗ <C> 0.944 <C> 0.351 <R> <C> 2 <C> ✗ <C> 0.953 <C> 0.373 <R> <C> 4 <C> ✗ <C> 0.947 <C> 0.334 <R> <C> 8 <C> ✗ <C> 0.938 <C> 0.273 <R> <C> 16 <C> ✗ <C> 0.947 <C> 0.308 <CAP> Table 5: Abstract only (v2.0) evidence identification validation scores varying across negative sampling strategies using Biomed RoBERTa.
<R> <C> Neg. Samples <C> Cond? <C> AUROC <C> Top1 Acc <R> <C> 1 <C> ✓ <C> 0.980 <C> 0.573 <R> <C> 2 <C> ✓ <C> 0.978 <C> 0.596 <R> <C> 4 <C> ✓ <C> 0.977 <C> 0.623 <R> <C> 8 <C> ✓ <C> 0.950 <C> 0.609 <R> <C> 16 <C> ✓ <C> 0.975 <C> 0.615 <R> <C> 1 <C> ✗ <C> 0.946 <C> 0.340 <R> <C> 2 <C> ✗ <C> 0.939 <C> 0.342 <R> <C> 4 <C> ✗ <C> 0.912 <C> 0.286 <R> <C> 8 <C> ✗ <C> 0.938 <C> 0.313 <R> <C> 16 <C> ✗ <C> 0.940 <C> 0.282 <CAP> Table 11: Abstract only (v1.0) evidence identification validation scores varying across negative sampling strategies for SciBERT.
<R> <C> [EMPTY] <C> [BOLD] Accuracy <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F1-score <C> [BOLD] AUC <R> <C> AF <C> 0.617 <C> 0.625 <C> 0.617 <C> 0.620 <C> 0.611 <R> <C> STR <C> 0.600 <C> 0.360 <C> 0.600 <C> 0.450 <C> 0.500 <R> <C> STR+AF <C> 0.604 <C> 0.614 <C> 0.604 <C> 0.607 <C> 0.599 <R> <C> UGR <C> 0.697 <C> [BOLD] 0.760 <C> 0.697 <C> 0.646 <C> 0.627 <R> <C> UGR+AF <C> [BOLD] 0.718 <C> 0.718 <C> [BOLD] 0.719 <C> [BOLD] 0.717 <C> [BOLD] 0.706 <R> <C> GALC <C> 0.621 <C> 0.605 <C> 0.621 <C> 0.579 <C> 0.560 <R> <C> GALC+AF <C> 0.647 <C> 0.654 <C> 0.647 <C> 0.649 <C> 0.640 <R> <C> INQUIRER <C> 0.533 <C> 0.512 <C> 0.533 <C> 0.517 <C> 0.493 <R> <C> INQUIRER+AF <C> 0.657 <C> 0.664 <C> 0.657 <C> 0.659 <C> 0.651 <CAP> Table 2: Helpful reviews identification performances using argument-based features and/or baseline features. AF stands for argument-based features.
<R> <C> STAR Huang et al. <C> STAR princess, series, cast, serial, midway, sparkle, 1940s, leo, closet, co-star <C> STAR 01 <R> <C> Huang et al. <C> silver, boy, cat, version, adventures, stars, emerald, destroyer, terrace, planet <C> 02 <R> <C> Huang et al. <C> energy, disk, wheel, disadvantage, block, puff, radius, diamond, chord <C> 03 <R> <C> Huang et al. <C> version, bronze, standard, colors, ring, emblem, silver, wear, shoulder, red <C> 01 <R> <C> Huang et al. <C> workshop, shop, paper, merchandise, plain, corporation, stock, likeness <C> 03 <R> <C> Huang et al. <C> guard, baseball, starter, tennis, basketball, brazil, class, world, morocco, ncaa <C> 01 <R> <C> Huang et al. <C> appearance, entertainer, pat, alumnus, freelance, brother, session, receiver <C> 01 <R> <C> Huang et al. <C> fictional, ongoing, manga, super, japanese, silver, interactive, asian, fiction <C> 01 <R> <C> Huang et al. <C> die, express, ride, opera, spanish, musical, hour, disaster, sun, blue <C> 01 <R> <C> Huang et al. <C> galaxy, spiral, variable, guide, magnitude, companion, satellite, crater <C> 02 <R> <C> MSSG-50d <C> blue, dragon, acbl, diamond, purple, legion, arrow, mercury, eagle, cross <C> 01 <R> <C> MSSG-50d <C> fan, legend, show, moesha, heroes, guest-star, flicka, lassie, tv-movie <C> 01 <R> <C> MSSG-50d <C> stars, sun, constellation, galaxy, eridani, pegasi, supergiant, ceti, starburst <C> 02 <R> <C> 01: person.n.01 02: celestial_body.n.01 03: whole.n.02 <C> 01: person.n.01 02: celestial_body.n.01 03: whole.n.02 <C> 01: person.n.01 02: celestial_body.n.01 03: whole.n.02 <R> <C> ROCK <C> ROCK <C> ROCK <R> <C> Huang et al. <C> blur, indulgence, pop, noise, bands, lacuna, reformed, wave, genre, taster <C> 01 <R> <C> Huang et al. <C> energy, silver, cat, song, cd, planet, dawn, hero, video, terrace <C> 02 <R> <C> Huang et al. <C> metal, classic, legendary, dubbed, american, hard, belgian, short-lived, debut, da <C> 01 <R> <C> Huang et al. <C> soft, shifting, disappear, fill, crystalline, false, pitch, expanse, heat, pile <C> 03 <R> <C> Huang et al. <C> vinyl, concert, limited, box, summer, double, dance, enhanced, gold, inch <C> 04 <R> <C> Huang et al. <C> hop, well-known, folk, occasional, jazz, music, concert, array, hard, pop <C> 01 <R> <C> Huang et al. <C> morris, miami, wood, ghost, silver, pearl, chase, corner, oak, thousand <C> 03 <R> <C> Huang et al. <C> hard, pop, cm, jazz, hip, hop, r&b, gutter, wave, subculture <C> 01 <R> <C> Huang et al. <C> hard, hip, short-lived, classic, jazz, raw, metal, ep <C> 01 <R> <C> Huang et al. <C> jazz, rally, star, roll, live, entertainer, appearance, session, pop, cover <C> 01 <R> <C> MSSG-50d <C> metal, rippling, dense, swirling, chirping, blues, punk, psychedelia, bands, pop <C> 01 <R> <C> MSSG-50d <C> sand, rocks, butte, ash, sandy, little, cedar, rocky, sugarloaf, spring-fed <C> 03 <R> <C> MSSG-50d <C> hip, alternative, indie, progressive, hop, reggae, roll, rock/metal, post-hardcore <C> 01 <R> <C> 01: popular_music.n.01 02: person.n.01 03: material.n.01 04: whole.n.02 <C> 01: popular_music.n.01 02: person.n.01 03: material.n.01 04: whole.n.02 <C> 01: popular_music.n.01 02: person.n.01 03: material.n.01 04: whole.n.02 <R> <C> NET <C> NET <C> NET <R> <C> Huang et al. <C> reduction, amount, increases, stamina, zero, worksheet, improvements, sum <C> 01 <R> <C> Huang et al. <C> raw, atomic, destination, brave, orbit, generalize, clock, ca, exhale, fresh <C> 02 <R> <C> Huang et al. <C> monthly, minimum, retail, banking, dividend, investor, tax, consumer, flat, dollar <C> 03 <R> <C> Huang et al. <C> cash, annual, bribe, yen, generate, yen, liabilities, stocks, lifetime <C> 03 <R> <C> Huang et al. <C> limousine, panic, alarm, cotton, racket, rush, 9th, buffalo, corps, recovered <C> 04 <R> <C> Huang et al. <C> palm, stalk, blanket, challah, qibla, putting, recess, curtain, tighten, lean <C> 04 <R> <C> Huang et al. <C> indent, text, poser, instruction, libraries, mosaic, campaigns, graphics, imperative <C> 04 <R> <C> Huang et al. <C> freight, processing, volume, needs, passenger, junction, electrical, ferry, shipping <C> 04 <R> <C> Huang et al. <C> contribution, bonus, compensation, bribe, yen, liabilities, stocks, yen, profit <C> 03 <R> <C> Huang et al. <C> 1909, quarterback, columbus, bills, bath, elite, 1903, tigers, affiliated, eagles <C> 04 <R> <C> MSSG-50d <C> droplet, pile, wellbore, squeeze, amount, volume, steady, turn, moves, balance <C> 04 <R> <C> MSSG-50d <C> boards, run, ball, spot, sideline, at-bat, clock, stretch, running, phils <C> 04 <R> <C> MSSG-50d <C> revenue, trillion, assets, profit, billion, pre-tax, liabilities, index, us$, fdi <C> 03 <R> <C> 01: whole.n.02 02: seize.v.01 03: income.n.01 04: artifact.n.01 <C> 01: whole.n.02 02: seize.v.01 03: income.n.01 04: artifact.n.01 <C> 01: whole.n.02 02: seize.v.01 03: income.n.01 04: artifact.n.01 <CAP> Table 1: Nearest neighbors (by cosine similarity) of sample words and the result of pseudo multi-sense detecting. Column 1 shows the existing word embeddings we use to detect pseudo multi-sense. In Column 2, each row shows the nearest neighbors of one sense in the vector space (Column 1). In Column 3, we present a meaning label for each sense, following the standard of WordNet synset description. We argue that “senses” with the same label actually have the same meaning, namely pseudo multi-sense.
<R> <C> [BOLD] Model <C> [BOLD] Seen Noise  [BOLD] SNR <C> [BOLD] Seen Noise  [BOLD] LSD <C> [BOLD] Seen Noise  [BOLD] MSE <C> [BOLD] Seen Noise  [BOLD] WER <C> [BOLD] Seen Noise  [BOLD] PESQ <C> [BOLD] Unseen Noise  [BOLD] SNR <C> [BOLD] Unseen Noise  [BOLD] LSD <C> [BOLD] Unseen Noise  [BOLD] MSE <C> [BOLD] Unseen Noise  [BOLD] WER <C> [BOLD] Unseen Noise  [BOLD] PESQ <R> <C> Noisy Speech <C> 15.18 <C> 23.07 <C> 0.04399 <C> 15.40 <C> 2.26 <C> 14.78 <C> 23.76 <C> 0.04786 <C> 18.4 <C> 2.09 <R> <C> MS <C> 18.82 <C> 22.24 <C> 0.03985 <C> 14.77 <C> 2.40 <C> 19.73 <C> 22.82 <C> 0.04201 <C> [BOLD] 15.54 <C> 2.26 <R> <C> DNN-Symm <C> 44.51 <C> 19.89 <C> 0.03436 <C> 55.38 <C> 2.20 <C> 40.47 <C> 21.07 <C> 0.03741 <C> 54.77 <C> 2.16 <R> <C> DNN-Causal <C> 40.70 <C> 20.09 <C> 0.03485 <C> 54.92 <C> 2.17 <C> 38.70 <C> 21.38 <C> 0.03718 <C> 54.13 <C> 2.13 <R> <C> RNN-Ng <C> 41.08 <C> 17.49 <C> 0.03533 <C> 44.93 <C> 2.19 <C> [BOLD] 44.60 <C> 18.81 <C> [BOLD] 0.03665 <C> 52.05 <C> 2.06 <R> <C> EHNet <C> [BOLD] 49.79 <C> [BOLD] 15.17 <C> [BOLD] 0.03399 <C> [BOLD] 14.64 <C> [BOLD] 2.86 <C> 39.70 <C> [BOLD] 17.06 <C> 0.04712 <C> 16.71 <C> [BOLD] 2.73 <R> <C> Clean Speech <C> 57.31 <C> 1.01 <C> 0.00000 <C> 2.19 <C> 4.48 <C> 58.35 <C> 1.15 <C> 0.00000 <C> 1.83 <C> 4.48 <CAP> Table 1: Experimental results on synthetic dataset with both seen and unseen noise, evaluated with 5 different metrics. Noisy Speech corresponds to the scores obtained without enhancement, while Clean Speech corresponds to the scores obtained using the ground truth clean speech. For each metric, the model achieves the best performance is highlighted in bold.
<R> <C> [BOLD] Test <C> [BOLD] ELMO  [ITALIC] d <C> [BOLD] ELMO  [ITALIC] p <C> [BOLD] BERT  [ITALIC] d <C> [BOLD] BERT  [ITALIC] p <C> [BOLD] GPT  [ITALIC] d <C> [BOLD] GPT  [ITALIC] p <C> [BOLD] GPT-2  [ITALIC] d <C> [BOLD] GPT-2  [ITALIC] p <R> <C> C1: Flowers/Insects, P/U∗ - Attitude <C> 1.39 <C> <10−30 <C> 0.96 <C> <10−30 <C> 1.05 <C> <10−30 <C> 0.13 <C> <10−30 <R> <C> C2: Instruments/Weapons, P/U∗ - Attitude <C> 1.56 <C> <10−30 <C> 0.93 <C> <10−30 <C> 1.13 <C> <10−30 <C> -0.28 <C> <10−30 <R> <C> C3: EA/AA names, P/U∗ - Attitude <C> 0.48 <C> <10−30 <C> 0.45 <C> <10−30 <C> -0.11 <C> <10−30 <C> -0.20 <C> <10−30 <R> <C> C4: EA/AA names, P/U∗ - Attitude <C> 0.16 <C> <10−30 <C> 0.49 <C> <10−30 <C> 0.00 <C> 0.70 <C> -0.23 <C> <10−30 <R> <C> C5: EA/AA names, P/U∗ - Attitude <C> 0.12 <C> <10−30 <C> 0.04 <C> <10−2 <C> 0.05 <C> <10−4 <C> -0.17 <C> <10−30 <R> <C> C6: Males/Female names, Career/Family <C> 1.28 <C> <10−30 <C> 0.91 <C> <10−30 <C> 0.21 <C> <10−30 <C> 0.34 <C> <10−30 <R> <C> C7: Math/Arts, Male/Female terms <C> 0.65 <C> <10−30 <C> 0.42 <C> <10−30 <C> 0.23 <C> <10−30 <C> 0.00 <C> 0.81 <R> <C> C8: Science/Arts, Male/Female terms <C> 0.32 <C> <10−30 <C> -0.07 <C> <10−4 <C> 0.26 <C> <10−30 <C> -0.16 <C> <10−30 <R> <C> C9: Mental/Physical disease, Temporary/Permanent <C> 0.99 <C> <10−30 <C> 0.55 <C> <10−30 <C> 0.07 <C> <10−2 <C> 0.04 <C> 0.04 <R> <C> C10: Young/Old people’s names, P/U∗ - Attitude <C> 0.11 <C> <10−19 <C> 0.00 <C> 0.90 <C> 0.04 <C> <10−2 <C> -0.17 <C> <10−30 <R> <C> I1: AF/EM, AF/EM intersectional <C> 1.24 <C> <10−30 <C> 0.76 <C> <10−30 <C> 0.05 <C> <10−3 <C> 0.05 <C> 0.06 <R> <C> I2: AF/EM, AF emergent/EM intersectional <C> 1.24 <C> <10−30 <C> 0.70 <C> <10−30 <C> -0.12 <C> <10−30 <C> 0.03 <C> 0.26 <R> <C> I3: MF/EM, MF/EM intersectional <C> 1.30 <C> <10−30 <C> 0.69 <C> <10−30 <C> -0.08 <C> <10−30 <C> 0.36 <C> <10−30 <R> <C> I4: MF/EM, MF emergent/EM intersectional <C> 1.52 <C> <10−30 <C> 0.87 <C> <10−30 <C> 0.14 <C> <10−27 <C> -0.26 <C> <10−30 <R> <C> ∗Unpleasant and pleasant attributes used to measure valence and attitudes towards targets Greenwald et al. ( 1998 ). <C> ∗Unpleasant and pleasant attributes used to measure valence and attitudes towards targets Greenwald et al. ( 1998 ). <C> ∗Unpleasant and pleasant attributes used to measure valence and attitudes towards targets Greenwald et al. ( 1998 ). <C> ∗Unpleasant and pleasant attributes used to measure valence and attitudes towards targets Greenwald et al. ( 1998 ). <C> ∗Unpleasant and pleasant attributes used to measure valence and attitudes towards targets Greenwald et al. ( 1998 ). <C> ∗Unpleasant and pleasant attributes used to measure valence and attitudes towards targets Greenwald et al. ( 1998 ). <C> ∗Unpleasant and pleasant attributes used to measure valence and attitudes towards targets Greenwald et al. ( 1998 ). <C> ∗Unpleasant and pleasant attributes used to measure valence and attitudes towards targets Greenwald et al. ( 1998 ). <C> ∗Unpleasant and pleasant attributes used to measure valence and attitudes towards targets Greenwald et al. ( 1998 ). <CAP> Table 2: CEAT from main paper(C1-C10,I1-I4) with sample size N=1,000 as opposed to the N=10,000 hyper-parameter in the main paper. We report the CES (d) and combined p−values of all CEAT (p) in the main paper with sample size N=1,000. We observe that all of the results are consistent with the CES and p−values reported in the main paper on Table 1. Light, medium, and dark gray shading of combined d values (CES) indicates small, medium, and large effect size, respectively. There are five tests which are significant with sample size N=10,000 but not significant with sample size N=1,000. However, these have small effect sizes and as a result we don’t expect statistical significance. According to our experiments, the Spearman correlation between WEAT’s effect size and p−value is ρ=0.99. Smaller effect sizes are expected to have insignificant p-values. Accordingly, all of the results under N=1,000 are consistent with the main findings. The notable yet consistent differences are C10 with Bert, C4 with GPT, C7 with GPT-2, I3 with GPT-2, and I4 with GPT-2. CES varies minimally with different sample size (N), but the differences of the results are smaller than 0.1, suggesting the degree of effect size remains consistent. In edge cases, where statistical significance or effect size is close to a significance threshold, gradually increasing N, in increments of N=+500 would provide more reliable results.
<R> <C> [BOLD]  Segmentation  <C> [BOLD]  BLEU  <R> <C> Manual (original) <C> 25.50 <R> <C> Speaker Diarization  <C> 21.01 <R> <C> ASR-based <C> 22.03 <CAP> Table 3: BLEU scores (lower-case evaluation) obtained on the tst-COMMON (MuST-C corpus) data with different speech segmentation strategies.
<R> <C> [BOLD] Domain  [BOLD] Test Data <C> [BOLD] bAbI6  [BOLD] Test <C> [BOLD] bAbI6  [BOLD] Test-OOD <C> [BOLD] bAbI6  [BOLD] Test-OOD <C> [BOLD] GR  [BOLD] Test <C> [BOLD] GR  [BOLD] Test-OOD <C> [BOLD] GR  [BOLD] Test-OOD <C> [BOLD] GM  [BOLD] Test <C> [BOLD] GM  [BOLD] Test-OOD <C> [BOLD] GM  [BOLD] Test-OOD <R> <C> [BOLD] Metrics <C> [BOLD] P@1 <C> [BOLD] P@1 <C> [BOLD] OOD F1 <C> [BOLD] P@3 <C> [BOLD] P@3 <C> [BOLD] OOD F1 <C> [BOLD] P@3 <C> [BOLD] P@3 <C> [BOLD] OOD F1 <R> <C> HCN <C> 53.41 <C> 41.95 <C> 0 <C> [BOLD] 58.89 <C> 41.65 <C> 0 <C> 41.18 <C> 27.08 <C> 0 <R> <C> AE-HCN-Indep <C> 31.29 <C> 41.06 <C> 48.68 <C> 51.90 <C> 55.42 <C> 71.52 <C> 31.12 <C> 42.78 <C> 64.35 <R> <C> AE-HCN <C> 53.58 <C> 55.04 <C> [BOLD] 73.41 <C> 56.97 <C> 58.90 <C> 74.67 <C> 40.61 <C> 48.59 <C> [BOLD] 69.31 <R> <C> AE-HCN-CNN <C> [BOLD] 55.04 <C> [BOLD] 55.35 <C> 70.38 <C> 58.32 <C> [BOLD] 64.51 <C> [BOLD] 81.33 <C> [BOLD] 45.12 <C> [BOLD] 52.79 <C> 68.59 <CAP> Table 3: Evaluation results. P@K means Precision@K. OOD F1 denotes f1-score for OOD detection over utterances.
<R> <C> [BOLD] Method <C> [BOLD] Raw  [BOLD] WN18 <C> [BOLD] Raw  [BOLD] WN18 <C> [BOLD] Raw  [BOLD] WN18 <C> [BOLD] Raw  [BOLD] FB15k <C> [BOLD] Raw  [BOLD] FB15k <C> [BOLD] Raw  [BOLD] FB15k <C> [BOLD] Filtered  [BOLD] WN18 <C> [BOLD] Filtered  [BOLD] WN18 <C> [BOLD] Filtered  [BOLD] WN18 <C> [BOLD] Filtered  [BOLD] FB15k <C> [BOLD] Filtered  [BOLD] FB15k <C> [BOLD] Filtered  [BOLD] FB15k <R> <C> [BOLD] Method <C> MR <C> H10 <C> MRR <C> MR <C> H10 <C> MRR <C> MR <C> H10 <C> MRR <C> MR <C> H10 <C> MRR <R> <C> SE  <C> 1011 <C> 68.5 <C> - <C> 273 <C> 28.8 <C> - <C> 985 <C> 80.5 <C> - <C> 162 <C> 39.8 <C> - <R> <C> Unstructured  <C> 315 <C> 35.3 <C> - <C> 1074 <C> 4.5 <C> - <C> 304 <C> 38.2 <C> - <C> 979 <C> 6.3 <C> - <R> <C> TransE  <C> 263 <C> 75.4 <C> - <C> 243 <C> 34.9 <C> - <C> 251 <C> 89.2 <C> - <C> 125 <C> 47.1 <C> - <R> <C> TransH  <C> 401 <C> 73.0 <C> - <C> 212 <C> 45.7 <C> - <C> 303 <C> 86.7 <C> - <C> 87 <C> 64.4 <C> - <R> <C> TransR  <C> 238 <C> 79.8 <C> - <C> 198 <C> 48.2 <C> - <C> 225 <C> 92.0 <C> - <C> 77 <C> 68.7 <C> - <R> <C> CTransR  <C> 231 <C> 79.4 <C> - <C> 199 <C> 48.4 <C> - <C> 218 <C> 92.3 <C> - <C> 75 <C> 70.2 <C> - <R> <C> KG2E  <C> 342 <C> 80.2 <C> - <C> [BOLD] 174 <C> 48.9 <C> - <C> 331 <C> 92.8 <C> - <C> 59 <C> 74.0 <C> - <R> <C> TransD  <C> 224 <C> 79.6 <C> - <C> 194 <C> 53.4 <C> - <C> 212 <C> 92.2 <C> - <C> 91 <C> 77.3 <C> - <R> <C> lppTransD  <C> 283 <C> 80.5 <C> - <C> 195 <C> 53.0 <C> - <C> 270 <C> 94.3 <C> - <C> 78 <C> 78.7 <C> - <R> <C> TranSparse  <C> 223 <C> 80.1 <C> - <C> 187 <C> [BOLD] 53.5 <C> - <C> 211 <C> 93.2 <C> - <C> 82 <C> 79.5 <C> - <R> <C> TATEC  <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> [BOLD] 58 <C> 76.7 <C> - <R> <C> NTN  <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> 66.1 <C> 0.53 <C> - <C> 41.4 <C> 0.25 <R> <C> DISTMULT  <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> 94.2 <C> 0.83 <C> - <C> 57.7 <C> 0.35 <R> <C> HolE  <C> - <C> - <C> [BOLD] 0.616 <C> - <C> - <C> 0.232 <C> - <C> [BOLD] 94.9 <C> [BOLD] 0.938 <C> - <C> 73.9 <C> 0.524 <R> <C> Our STransE <C> [BOLD] 217 <C> [BOLD] 80.9 <C> 0.469 <C> 219 <C> 51.6 <C> [BOLD] 0.252 <C> [BOLD] 206 <C> 93.4 <C> 0.657 <C> 69 <C> [BOLD] 79.7 <C> [BOLD] 0.543 <R> <C> rTransE  <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> [BOLD] 50 <C> 76.2 <C> - <R> <C> PTransE  <C> - <C> - <C> - <C> 207 <C> 51.4 <C> - <C> - <C> - <C> - <C> 58 <C> 84.6 <C> - <R> <C> GAKE  <C> - <C> - <C> - <C> 228 <C> 44.5 <C> - <C> - <C> - <C> - <C> 119 <C> 64.8 <C> - <R> <C> Gaifman  <C> - <C> - <C> - <C> - <C> - <C> - <C> 352 <C> 93.9 <C> - <C> 75 <C> 84.2 <C> - <R> <C> Hiri  <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> 90.8 <C> 0.691 <C> - <C> 70.3 <C> 0.603 <R> <C> NLFeat  <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> [BOLD] 94.3 <C> [BOLD] 0.940 <C> - <C> [BOLD] 87.0 <C> [BOLD] 0.822 <R> <C> TEKE_H  <C> [BOLD] 127 <C> 80.3 <C> - <C> 212 <C> 51.2 <C> - <C> [BOLD] 114 <C> 92.9 <C> - <C> 108 <C> 73.0 <C> - <R> <C> SSP  <C> 168 <C> [BOLD] 81.2 <C> - <C> [BOLD] 163 <C> [BOLD] 57.2 <C> - <C> 156 <C> 93.2 <C> - <C> 82 <C> 79.0 <C> - <CAP> Table 3: Link prediction results. MR, H10 and MRR denote evaluation metrics of mean rank, Hits@10 (in %) and mean reciprocal rank, respectively. “NLFeat” abbreviates Node+LinkFeat. The results for NTN [Socher et al.2013] listed in this table are taken from yang-etal-2015 since NTN was originally evaluated on different datasets.
<R> <C> [BOLD] Method <C> [BOLD] Predicting head  [ITALIC] h 1-1 <C> [BOLD] Predicting head  [ITALIC] h 1-M <C> [BOLD] Predicting head  [ITALIC] h M-1 <C> [BOLD] Predicting head  [ITALIC] h M-M <C> [BOLD] Predicting tail  [ITALIC] t 1-1 <C> [BOLD] Predicting tail  [ITALIC] t 1-M <C> [BOLD] Predicting tail  [ITALIC] t M-1 <C> [BOLD] Predicting tail  [ITALIC] t M-M <R> <C> SE <C> 35.6 <C> 62.6 <C> 17.2 <C> 37.5 <C> 34.9 <C> 14.6 <C> 68.3 <C> 41.3 <R> <C> Unstr. <C> 34.5 <C> 2.5 <C> 6.1 <C> 6.6 <C> 34.3 <C> 4.2 <C> 1.9 <C> 6.6 <R> <C> TransE <C> 43.7 <C> 65.7 <C> 18.2 <C> 47.2 <C> 43.7 <C> 19.7 <C> 66.7 <C> 50.0 <R> <C> TransH <C> 66.8 <C> 87.6 <C> 28.7 <C> 64.5 <C> 65.5 <C> 39.8 <C> 83.3 <C> 67.2 <R> <C> TransR <C> 78.8 <C> 89.2 <C> 34.1 <C> 69.2 <C> 79.2 <C> 37.4 <C> 90.4 <C> 72.1 <R> <C> CTransR <C> 81.5 <C> 89.0 <C> 34.7 <C> 71.2 <C> 80.8 <C> 38.6 <C> 90.1 <C> 73.8 <R> <C> KG2E <C> [BOLD] 92.3 <C> 94.6 <C> [BOLD] 66.0 <C> 69.6 <C> [BOLD] 92.6 <C> [BOLD] 67.9 <C> 94.4 <C> 73.4 <R> <C> TATEC <C> 79.3 <C> 93.2 <C> 42.3 <C> 77.2 <C> 78.5 <C> 51.5 <C> 92.7 <C> 80.7 <R> <C> TransD <C> 86.1 <C> [BOLD] 95.5 <C> 39.8 <C> 78.5 <C> 85.4 <C> 50.6 <C> 94.4 <C> 81.2 <R> <C> lppTransD <C> 86.0 <C> 94.2 <C> 54.4 <C> [BOLD] 82.2 <C> 79.7 <C> 43.2 <C> [BOLD] 95.3 <C> 79.7 <R> <C> TranSparse <C> 86.8 <C> [BOLD] 95.5 <C> 44.3 <C> 80.9 <C> 86.6 <C> 56.6 <C> 94.4 <C> [BOLD] 83.3 <R> <C> STransE <C> 82.8 <C> 94.2 <C> 50.4 <C> 80.1 <C> 82.4 <C> 56.9 <C> 93.4 <C> 83.1 <CAP> Table 4: Hits@10 (in %) by the relation category on FB15k. “Unstr.” abbreviates Unstructured.
<R> <C> Features <C> Favg <C> Decreasing <C> Percentage decreasing <R> <C> All <C> 65.61 <C> 0 <C> 0% <R> <C> All - context-based <C> 54.60 <C> -11.01 <C> -16.78% <R> <C> All - comm-cxt <C> 56.03 <C> -9.58 <C> -14.6% <R> <C> All - de-cxt <C> 65.53 <C> -0.08 <C> -0.12% <R> <C> All - comm-know-cxt <C> 65.93 <C> 0.32 <C> 0.49% <R> <C> All - sentiment <C> 65.99 <C> 0.38 <C> 0.58% <R> <C> All - structural <C> 65.81 <C> 0.2 <C> 0.3% <R> <C> All - BoW <C> 65.66 <C> 0.05 <C> 0.08% <CAP> Table 3: Ablation Test (linear support SVM on stance at triplet level)
<R> <C> Classifier <C> Feature set <C> RD <C> OD <C> APF <R> <C> [ITALIC] Baselines <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> MC <C> - <C> 35.97 <C> 36.07 <C> 33.6 <R> <C> SVM <C> unigrams <C> 54.46 <C> 49.93 <C> 45.77 <R> <C> SVM <C> ngrams <C> 56.60 <C> 48.49 <C> 45.35 <R> <C> [ITALIC] Our Classifiers <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> NB <C> sentiment + comm-cxt <C> 47.56 <C> 54.20 <C> 43.25 <R> <C> DT <C> comm-cxt <C> 67.07 <C> 61.95 <C> [BOLD] 62.22 <R> <C> RF <C> comm-cxt <C> 67.14 <C> 61.96 <C> 62.07 <R> <C> SVM <C> BoW + sentiment + comm-cxt <C> [BOLD] 69.75 <C> [BOLD] 62.03 <C> 58.30 <CAP> Table 5: Results in different temporal intervals
<R> <C> [BOLD] Dataset <C> [BOLD] Train  [BOLD] Original <C> [BOLD] Train  [BOLD] Obfuscated/  [BOLD] Evaded <C> [BOLD] Test  [BOLD] Original <C> [BOLD] Test  [BOLD] Obfuscated/  [BOLD] Evaded <R> <C> EBG obfuscated <C> 431 <C> 80 <C> 268 <C> 47 <R> <C> EBG evaded <C> 236 <C> 29 <C> 235 <C> 30 <R> <C> BLOG obfuscated <C> 2000 <C> 292 <C> 1900 <C> 277 <R> <C> BLOG evaded <C> 700 <C> 71 <C> 1000 <C> 113 <CAP> Table 1: Number of original and obfuscated/evaded documents in train and test sets of each of the four datasets
<R> <C> [BOLD] Dataset <C> [BOLD] Models <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> EBG obfuscated <C> BERT large + ranks + VGG-19 + RFC <C> 1.00 <C> 0.85 <C> [BOLD] 0.92 <R> <C> EBG obfuscated <C> BERT large + ranks + VGG-19 + SVM <C> 0.98 <C> 0.83 <C> [BOLD] 0.90 <R> <C> EBG obfuscated <C> GLTR + SVM <C> 1.00 <C> 0.70 <C> 0.83 <R> <C> EBG obfuscated <C> Writeprints + SVM <C> 0.67 <C> 0.38 <C> 0.49 <R> <C> EBG obfuscated <C> Character trigrams + KNN <C> 0.64 <C> 0.15 <C> 0.24 <R> <C> EBG evaded <C> BERT large + probs + bins(0.010) + ANN <C> 1.00 <C> 0.90 <C> [BOLD] 0.95 <R> <C> EBG evaded <C> BERT base + probs + VGG-19 + GNB <C> 1.00 <C> 0.90 <C> [BOLD] 0.95 <R> <C> EBG evaded <C> GLTR + SVM <C> 1.00 <C> 0.80 <C> 0.89 <R> <C> EBG evaded <C> Writeprints + SVM <C> 0.79 <C> 0.63 <C> 0.70 <R> <C> EBG evaded <C> Character trigrams + KNN <C> 1.00 <C> 0.17 <C> 0.29 <R> <C> BLOG obfuscated <C> BERT base + probs + VGG-19 + ANN <C> 0.85 <C> 0.71 <C> [BOLD] 0.77 <R> <C> BLOG obfuscated <C> BERT base + probs + VGG-19 + SVM <C> 0.79 <C> 0.74 <C> [BOLD] 0.77 <R> <C> BLOG obfuscated <C> GLTR + SVM <C> 0.92 <C> 0.40 <C> 0.56 <R> <C> BLOG obfuscated <C> Writeprints + SVM <C> 0.71 <C> 0.41 <C> 0.52 <R> <C> BLOG obfuscated <C> Character trigrams + KNN <C> 0.41 <C> 0.50 <C> 0.45 <R> <C> BLOG evaded <C> GPT-2 345M + ranks + VGG-19 + GNB <C> 0.82 <C> 0.83 <C> [BOLD] 0.83 <R> <C> BLOG evaded <C> BERT base + probs + VGG-19 + ANN <C> 0.79 <C> 0.81 <C> [BOLD] 0.80 <R> <C> BLOG evaded <C> GLTR + SVM <C> 0.86 <C> 0.55 <C> 0.67 <R> <C> BLOG evaded <C> Writeprints + SVM <C> 0.84 <C> 0.62 <C> 0.71 <R> <C> BLOG evaded <C> Character trigrams + KNN <C> 0.86 <C> 0.50 <C> 0.63 <CAP> Table 2: Obfuscation detection results (P: precision, R: recall, F1: F1 score).
<R> <C> [EMPTY] <C> Model <C> Image-to-Sentence R@1 <C> Image-to-Sentence R@5 <C> Image-to-Sentence R@10 <C> Sentence-to-Image R@1 <C> Sentence-to-Image R@5 <C> Sentence-to-Image R@10 <R> <C> Using image-text pair information <C> DVSA(Karpathy & Fei-Fei,  2015 ) <C> 38.4 <C> 69.9 <C> 80.5 <C> 27.4 <C> 60.2 <C> 74.8 <R> <C> Using image-text pair information <C> m-RNN-vgg(Mao et al.,  2014 ) <C> 41.0 <C> 73.0 <C> 83.5 <C> 29.0 <C> 42.2 <C> 77.0 <R> <C> Using image-text pair information <C> mCNN (ensemble)(Ma et al.,  2015 ) <C> 42.8 <C> 73.1 <C> 84.1 <C> 32.6 <C> 68.6 <C> 82.8 <R> <C> Using image-text pair information <C> structure preserving(Wang et al.,  2016 ) <C> 50.1 <C> 79.7 <C> 89.2 <C> 39.6 <C> 75.2 <C> 86.9 <R> <C> Using category information <C> [BOLD] our model (TextCNN) <C> 13.6 <C> 39.6 <C> 54.6 <C> 10.3 <C> 35.5 <C> 55.5 <R> <C> Using category information <C> [BOLD] our model (FV-HGLMM) <C> 14.3 <C> 40.5 <C> 55.8 <C> 12.7 <C> 39.0 <C> 57.2 <CAP> Table 2: Bidirectional retrieval recall@K results on MS COCO 1000-image test set. our model (TextCNN) and our model (FV-HGLMM) use TextCNN and FV-HGLMM for text representation respectively.
<R> <C> Model <C> Model <C> [ITALIC] DF(5%) +  [ITALIC] DP(95%) Entity-F1 <C> [ITALIC] DF(5%) +  [ITALIC] DP(95%) Success <C> [ITALIC] DF(5%) +  [ITALIC] DP(95%) Turns <C> [ITALIC] DF(10%) +  [ITALIC] DP(90%) Entity-F1 <C> [ITALIC] DF(10%) +  [ITALIC] DP(90%) Success <C> [ITALIC] DF(10%) +  [ITALIC] DP(90%) Turns <C> [ITALIC] DF(20%) +  [ITALIC] DP(80%) Entity-F1 <C> [ITALIC] DF(20%) +  [ITALIC] DP(80%) Success <C> [ITALIC] DF(20%) +  [ITALIC] DP(80%) Turns <R> <C> Handcrafted <C> PPO <C> 41.8 <C> 34.1 <C> 13.3 <C> 45.3 <C> 36.7 <C> 12.5 <C> 50.6 <C> 41.2 <C> 11.2 <R> <C> Reward Learning <C> ALDM <C> 38.7 <C> 35.6 <C> 15.2 <C> 42.1 <C> 38.6 <C> 14.9 <C> 44.9 <C> 42.1 <C> 13.7 <R> <C> Reward Learning <C> GDPL <C> 49.5 <C> 47.5 <C> 12.8 <C> 54.9 <C> 53.2 <C> 12.1 <C> 60.4 <C> 59.1 <C> 10.8 <R> <C> Semi-VAE Enhanced <C> SS-PPO <C> 45.2 <C> 36.2 <C> 13.6 <C> 47.4 <C> 37.2 <C> 12.4 <C> 53.1 <C> 43.6 <C> 11.5 <R> <C> Semi-VAE Enhanced <C> SS-ALDM <C> 39.6 <C> 38.8 <C> 14.7 <C> 44.7 <C> 43.8 <C> 13.2 <C> 47.8 <C> 51.3 <C> 12.4 <R> <C> Semi-VAE Enhanced <C> SS-GDPL <C> 53.7 <C> 51.2 <C> 11.1 <C> 61.3 <C> 58.4 <C> 10.5 <C> 66.5 <C> 68.7 <C> 9.2 <R> <C> Proposed <C> SS-VRNN <C> 68.7 <C> 63.2 <C> 9.4 <C> 75.1 <C> 68.5 <C> 8.6 <C> 77.3 <C> 72.4 <C> 8.2 <R> <C> Proposed <C> Act-GDPL <C> 70.6 <C> 65.6 <C> 9.5 <C> 78.8 <C> 71.1 <C> 8.4 <C> 80.9 <C> 78.0 <C> 8.2 <R> <C> Proposed <C> Act-VRNN <C> [BOLD] 76.2 <C> [BOLD] 72.7 <C> [BOLD] 9.1 <C> [BOLD] 83.0 <C> [BOLD] 81.8 <C> [BOLD] 8.0 <C> [BOLD] 85.5 <C> [BOLD] 86.7 <C> [BOLD] 7.9 <CAP> Table 2: Semi-Supervised Policy Learning Results (DF and DP)
<R> <C> Supervision <C> Model <C> Entity-F1 <C> Success <C> Turns <R> <C> [ITALIC] DF(10%) +  [ITALIC] DU(90%) <C> ALDM <C> 40.0 <C> 34.9 <C> 15.9 <R> <C> [ITALIC] DF(10%) +  [ITALIC] DU(90%) <C> SS-PPO <C> 44.7 <C> 33.8 <C> 12.9 <R> <C> [ITALIC] DF(10%) +  [ITALIC] DU(90%) <C> SS-ALDM <C> 42.1 <C> 36.4 <C> 14.9 <R> <C> [ITALIC] DF(10%) +  [ITALIC] DU(90%) <C> SS-GDPL <C> 56.3 <C> 50.2 <C> 11.8 <R> <C> [ITALIC] DF(10%) +  [ITALIC] DU(90%) <C> SS-VRNN <C> 74.1 <C> 67.1 <C> 9.1 <R> <C> [ITALIC] DF(10%) +  [ITALIC] DU(90%) <C> Act-GDPL <C> 72.9 <C> 66.7 <C> 8.5 <R> <C> [ITALIC] DF(10%) +  [ITALIC] DU(90%) <C> Act-VRNN <C> [BOLD] 80.6 <C> [BOLD] 72.4 <C> [BOLD] 8.4 <R> <C> [ITALIC] DF(10%) +  [ITALIC] DP(10%) +  [ITALIC] DU(80%) <C> ALDM <C> 41.7 <C> 35.2 <C> 15.7 <R> <C> [ITALIC] DF(10%) +  [ITALIC] DP(10%) +  [ITALIC] DU(80%) <C> SS-PPO <C> 44.9 <C> 34.6 <C> 12.8 <R> <C> [ITALIC] DF(10%) +  [ITALIC] DP(10%) +  [ITALIC] DU(80%) <C> SS-ALDM <C> 42.5 <C> 40.1 <C> 14.7 <R> <C> [ITALIC] DF(10%) +  [ITALIC] DP(10%) +  [ITALIC] DU(80%) <C> SS-GDPL <C> 57.1 <C> 51.4 <C> 10.7 <R> <C> [ITALIC] DF(10%) +  [ITALIC] DP(10%) +  [ITALIC] DU(80%) <C> SS-VRNN <C> 75.6 <C> 67.9 <C> 8.8 <R> <C> [ITALIC] DF(10%) +  [ITALIC] DP(10%) +  [ITALIC] DU(80%) <C> Act-GDPL <C> 73.3 <C> 67.1 <C> 8.5 <R> <C> [ITALIC] DF(10%) +  [ITALIC] DP(10%) +  [ITALIC] DU(80%) <C> Act-VRNN <C> [BOLD] 81.1 <C> [BOLD] 76.3 <C> [BOLD] 8.2 <CAP> Table 3: Semi-Supervised Policy Learning Results (DF, DP, and DU)
<R> <C> Type <C> BERT_base <C> Human <R> <C> Answer Type <C> Answer Type <C> Answer Type <R> <C> Non-description Description <C> 61.97/63.22 44.77/54.34 <C> 91.72/95.10 91.87/94.72 <R> <C> Question Category <C> Question Category <C> Question Category <R> <C> Lexical Match Paraphrasing Summary <C> 58.69/69.01 54.34/49.84 50.73/52.76 <C> 96.09/96.73 85.49/91.36 90.03/95.39 <R> <C> Coreference res. Multi-sentence rea. Implicit causality <C> 52.35/58.97 44.91/45.81 39.31/57.63 <C> 91.78/98.11 87.22/95.31 92.54/91.28 <CAP> Table 6: Fine-grained results in terms of different answer types and question categories on the monolingual task. The left side of the slash is the F1 score on the English data, while the right is on Chinese. All F1 scores are calculated on the 100 questions as described in Section 4.3.
<R> <C> [BOLD] Method <C> de-en <C> en-de <C> Avg. Δ <R> <C> Ite-Sampling <C> 34.93 <C> 26.72 <C> - <R> <C> Ite-Sampling + Enc <C> 35.67 <C> 27.76 <C> +0.89 <R> <C> Ite-Greedy <C> 37.69 <C> 27.81 <C> - <R> <C> Ite-Greedy+Enc <C> 38.20 <C> 28.15 <C> +0.43 <R> <C> Ite-Beam <C> 37.53 <C> 28.25 <C> - <R> <C> Ite-Beam+Enc <C> 37.76 <C> 28.25 <C> +0.12 <CAP> Table 3: Noise in back-translated data can degrade the model performance and our weighting strategies (Enc) benefit the most in noisy settings.
<R> <C> [BOLD] Keyphrase set <C> DIV( [ITALIC] n) <C> [ITALIC] R(1) <C> [ITALIC] R(2) <C> [ITALIC] R( [ITALIC] n) <R> <C> Ground truth <C> 0.942 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> BS <C> 0.058 <C> 0.120 <C> 0.017 <C> 0. <R> <C> BS++ <C> 0.661 <C> 0.112 <C> 0.030 <C> 0.004 <R> <C> [ITALIC]  [BOLD] BSDAR <C> [BOLD] 0.746 <C> [BOLD] 0.301 <C> [BOLD] 0.198 <C> [BOLD] 0.223 <CAP> Table 7: Diversity measure of decoding algorithms across data sets
<R> <C> [BOLD] Dataset <C> [BOLD] Mode <C> [BOLD] Acc1 (%) <R> <C> COPA <C> hyp-only <C> 54.6 <R> <C> [EMPTY] <C> random <C> 50.0 <R> <C> CommonsenseQA <C> hyp-only <C> 22.0 <R> <C> [EMPTY] <C> random <C> 20.0 <R> <C> Swag <C> hyp-only <C> 60.6 <R> <C> [EMPTY] <C> random <C> 25.0 <R> <C> HellaSwag <C> hyp-only <C> 50.8 <R> <C> [EMPTY] <C> random <C> 25.0 <CAP> Table 2: Commonsense reasoning task probing. hyp-only stands for hypothesis only, random for random baseline. 1COPA is evaluated on Test, CommonsenseQA is evaluated on Test∗, Swag and HellaSwag are evaluated on Val (see Section 4).
<R> <C> Method <C> Precision <C> Recall <C> F1 <R> <C> Naive Baseline <C> 17.01 <C> 92.50 <C> 28.74 <R> <C> Rule Baseline <C> 91.79 <C> 67.11 <C> 77.53 <R> <C> Encoder-Decoder <C> 73.31 <C> 96.17 <C> 83.20 <R> <C> + word attention <C> [BOLD] 75.76 <C> 94.65 <C> [BOLD] 84.16 <R> <C> + stream attention <C> 73.48 <C> [BOLD] 96.18 <C> 83.31 <CAP> Table 3: Modeling Architecture Impact on Accuracy for Multi-Domain Dataset
<R> <C> Method <C> Precision <C> Recall <C> F1 <R> <C> Naive Baseline <C> 80.58 <C> 75.44 <C> 77.93 <R> <C> Encoder-Decoder <C> 97.22 <C> 95.30 <C> 96.24 <R> <C> + word attention <C> 97.20 <C> 97.65 <C> 97.42 <R> <C> + stream attention <C> 97.23 <C> 95.61 <C> 96.42 <CAP> Table 4: Results on DSTC2 dataset
<R> <C> [BOLD] Model <C> [BOLD] Adequacy <C> [BOLD] Fluency <C> [BOLD] Under-Translation <C> [BOLD] Over-Translation <R> <C> GroundHog <C> 3.06 <C> 3.54 <C> 25.0% <C> 4.5% <R> <C> + NN cov. w/ gating ( [ITALIC] d=10) <C> 3.28 <C> 3.73 <C> 16.7% <C> 2.7% <CAP> Table 2: Subjective evaluation of translation adequacy and fluency. The numbers in the last two columns denote the percentages of source words are under-translated and over-translated, respectively.
<R> <C> [BOLD] System <C> SAER <C> AER <R> <C> GroundHog <C> 67.00 <C> 54.67 <R> <C> + Ling. cov. w/o fertility <C> 66.75 <C> 53.55 <R> <C> + Ling. cov. w/ fertility <C> 64.85 <C> 52.13 <R> <C> + NN cov. w/o gating ( [ITALIC] d=1) <C> 67.10 <C> 54.46 <R> <C> + NN cov. w/ gating ( [ITALIC] d=1) <C> 66.30 <C> 53.51 <R> <C> + NN cov. w/ gating ( [ITALIC] d=10) <C> [BOLD] 64.25 <C> [BOLD] 50.50 <CAP> Table 3: Evaluation of alignment quality. The lower the score, the better the alignment quality.
<R> <C> [EMPTY] <C> R <C> M <C> C <C> B4 <C> B3 <C> B2 <C> B1 <R> <C> Vinyals et al. 2017 vinyals2017show  <C> 53.0 <C> 25.4 <C> 94.3 <C> 30.9 <C> 40.7 <C> 54.2 <C> 71.3 <R> <C> Lu et al. 2017 lu2017knowing  <C> 55.0 <C> 26.4 <C> 104.0 <C> 33.6 <C> 44.4 <C> 58.4 <C> 74.8 <R> <C> Chen et al. 2017 chen2017temporal  <C> 54.7 <C> 25.9 <C> 105.9 <C> 32.4 <C> 44.1 <C> 59.1 <C> 75.7 <R> <C> Ren et al. 2017 ren2017deep  <C> 52.5 <C> 25.1 <C> 93.7 <C> 30.4 <C> 40.3 <C> 53.9 <C> 71.3 <R> <C> Gu et al. 2017 gu2017empirical  <C> - <C> 25.1 <C> 99.1 <C> 30.4 <C> 40.9 <C> 54.6 <C> 72.1 <R> <C> Rennie et al. 2017 rennie2017self  <C> 56.3 <C> 27.0 <C> 114.7 <C> 35.2 <C> - <C> - <C> - <R> <C> Wang et al. 2017 wang2017skeleton  <C> 48.9 <C> 24.7 <C> 96.6 <C> 25.9 <C> 35.5 <C> 48.9 <C> 67.3 <R> <C> Gan et al. 2017 gan2017semantic  <C> 54.3 <C> 25.7 <C> 100.3 <C> 34.1 <C> 44.4 <C> 57.8 <C> 74.1 <R> <C> Liu et al. 2018 liu2018show  <C> 57.0 <C> 27.4 <C> 117.1 <C> 35.8 <C> 48.0 <C> 63.1 <C> 80.1 <R> <C> Wang et al. 2018 wang2018image  <C> - <C> 21.1 <C> 69.5 <C> 23.0 <C> 33.3 <C> 47.4 <C> 65.6 <R> <C> Cornia et al. 2018 cornia2018paying  <C> 52.1 <C> 24.8 <C> 89.8 <C> 28.4 <C> 39.1 <C> 53.6 <C> 70.8 <R> <C> Ding et al. 2018 ding2018neural  <C> 55.5 <C> 26.1 <C> 105.5 <C> 34.2 <C> 45.8 <C> 60.5 <C> 76.8 <R> <C> Chen et al. 2018 chen2018show  <C> 54.9 <C> 33.8 <C> 104.4 <C> 33.8 <C> 44.3 <C> 57.9 <C> 74.3 <R> <C> Gu et al. 2018 gu2018stack  <C> - <C> 27.4 <C> 120.4 <C> 36.1 <C> 47.9 <C> 62.5 <C> 78.6 <R> <C> Chen et al. 2019 chen2019image  <C> 58.7 <C> 28.7 <C> [BOLD] 125.5 <C> 38.4 <C> 50.7 <C> 65.5 <C> [BOLD] 81.9 <R> <C> Stacked + ATT (8) <C> [BOLD] 64.9 <C> [BOLD] 34.7 <C> 125.0 <C> [BOLD] 50.5 <C> [BOLD] 57.1 <C> [BOLD] 66.4 <C> 73.7 <CAP> Table 1: Performance of the proposed method compared to the state-of-the-art methods on MS-COCO dataset.
<R> <C> Iter. <C> [ITALIC] Seeding Seeds <C> [ITALIC] Seeding Found <C> [ITALIC] Seeding Good <C> [ITALIC] Seeding %Good <C> [ITALIC] Crawling Sentences <C> [ITALIC] Crawling Domains <C> [ITALIC] Crawling URLs <C> Runtime <R> <C> 0 <C> 100 <C> 837 <C> 577 <C> 68.94 <C> 89350 <C> 529 <C> 4810 <C> 16h15 <R> <C> 1 <C> 100 <C> 1062 <C> 662 <C> 62.34 <C> 48483 <C> 552 <C> 4382 <C> 16h52 <R> <C> 2 <C> 100 <C> 732 <C> 423 <C> 57.79 <C> 20662 <C> 423 <C> 2193 <C> 40h52 <CAP> Table 1: Number of URLs found vs actually pertinent during seeding and number of unique new sentences, domains, and URLs discovered during crawl starting from a blank database and launching the system three times.
<R> <C> [ITALIC] Training Dataset <C> [ITALIC] Training Size <C> [ITALIC] Test Sets Leipzig <C> [ITALIC] Test Sets  [ITALIC] SwissCrawl <C> [ITALIC] Test Sets Both <R> <C> Leipzig <C> 180,000 <C> 47.6 <C> 92.6 <C> 67.6 <R> <C> [ITALIC] SwissCrawl <C> 483,526 <C> 63.9 <C> 49.5 <C> 56.2 <R> <C> Both <C> 663,526 <C> [BOLD] 30.5 <C> [BOLD] 45.9 <C> [BOLD] 38.0 <CAP> Table 6: Perplexity of language models trained on Leipzig, SwissCrawl (our corpus) and both.
<R> <C> Setting <C> Accuracy <C> Levenshtein distance <R> <C> standard <C> 60% <C> 0.92 <R> <C> +hallucinated data <C> 62% <C> 1.02 <CAP> Table 4: Morphological Inflection Results
<R> <C> Input <C> Accuracy <C> Levenshtein distance <R> <C> form (no tags) <C> 89% <C> 0.27 <R> <C> form + tags <C> 90% <C> 0.21 <CAP> Table 6: Lemmatization Results.
<R> <C> Model <C> Dict <C> de-fi <C> de-it <C> de-ru <C> en-de <C> en-fi <C> en-it <C> en-ru <C> fi-it <C> fi-ru <C> Avg <R> <C> [ITALIC] Supervised <C> [ITALIC] Supervised <C> [ITALIC] Supervised <C> [ITALIC] Supervised <C> [ITALIC] Supervised <C> [ITALIC] Supervised <C> [ITALIC] Supervised <C> [ITALIC] Supervised <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Proc <C> 1K <C> 0.147 <C> 0.155 <C> 0.098 <C> 0.175 <C> 0.101 <C> 0.210 <C> 0.104 <C> 0.113 <C> 0.096 <C> 0.133 <R> <C> Proc <C> 5K <C> 0.255 <C> 0.212 <C> 0.152 <C> 0.261 <C> 0.200 <C> 0.240 <C> 0.152 <C> 0.149 <C> 0.146 <C> 0.196 <R> <C> Proc-B <C> 1K <C> 0.294 <C> 0.230 <C> 0.155 <C> 0.288 <C> 0.258 <C> 0.265 <C> 0.166 <C> 0.151 <C> 0.136 <C> [BOLD] 0.216 <R> <C> Proc-B <C> 3K <C> 0.305 <C> 0.232 <C> 0.143 <C> 0.238 <C> 0.267 <C> 0.269 <C> 0.150 <C> 0.163 <C> 0.170 <C> [BOLD] 0.215 <R> <C> DLV <C> 5K <C> 0.255 <C> 0.210 <C> 0.155 <C> 0.260 <C> 0.206 <C> 0.240 <C> 0.151 <C> 0.147 <C> 0.147 <C> 0.197 <R> <C> RCSLS <C> 1K <C> 0.114 <C> 0.133 <C> 0.077 <C> 0.163 <C> 0.063 <C> 0.163 <C> 0.106 <C> 0.074 <C> 0.069 <C> 0.107 <R> <C> RCSLS <C> 5K <C> 0.196 <C> 0.189 <C> 0.122 <C> 0.237 <C> 0.127 <C> 0.210 <C> 0.133 <C> 0.130 <C> 0.113 <C> 0.162 <R> <C> [ITALIC] Unupervised <C> [ITALIC] Unupervised <C> [ITALIC] Unupervised <C> [ITALIC] Unupervised <C> [ITALIC] Unupervised <C> [ITALIC] Unupervised <C> [ITALIC] Unupervised <C> [ITALIC] Unupervised <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> VecMap <C> – <C> 0.240 <C> 0.129 <C> 0.162 <C> 0.200 <C> 0.150 <C> 0.201 <C> 0.104 <C> 0.096 <C> 0.109 <C> 0.155 <R> <C> Muse <C> – <C> 0.001* <C> 0.210 <C> 0.195 <C> 0.280 <C> 0.000* <C> 0.272 <C> 0.002* <C> 0.002* <C> 0.001* <C> 0.107 <R> <C> ICP <C> – <C> 0.252 <C> 0.170 <C> 0.167 <C> 0.230 <C> 0.230 <C> 0.231 <C> 0.119 <C> 0.117 <C> 0.124 <C> [BOLD] 0.182 <R> <C> GWA <C> – <C> 0.218 <C> 0.139 <C> 0.149 <C> 0.013 <C> 0.005* <C> 0.007* <C> 0.005* <C> 0.058 <C> 0.052 <C> 0.072 <CAP> Table 5: CLIR performance (MAP) of CLE models (the first language in each column is the query language, the second is the language of the document collection). Numbers in bold denote the best scores in the model group. Asterisks denote language pairs for which CLE models did not yield successful runs in BLI evaluation.
<R> <C> Models <C> F1 <C> P <C> R <C> FrmAcc <R> <C> Baseline (CRF+SVMs) <C> 31.15 <C> 29.92 <C> 32.48 <C> 7.71 <R> <C> Pipeline (biLSTMs) <C> 19.89 <C> 14.87 <C> 30.01 <C> 11.96 <R> <C> JointModel <C> 19.04 <C> 18.53 <C> 19.57 <C> [BOLD] 22.84 <R> <C> Oracle-SAP (SVMs) <C> 30.61 <C> 30.20 <C> 31.04 <C> 7.65 <R> <C> Oracle-SAP (biLSTM) <C> 23.09 <C> 22.24 <C> 24.01 <C> 19.67 <CAP> Table 2: Performance (%) of end-to-end models for SAP. F1, P, R are micro-averaged token-level scores; FrmAcc is frame-level accuracy. Oracle models are provided as references.
<R> <C> Models <C> User Slot Tagging (UST) F1 <C> User Slot Tagging (UST) Precision <C> User Slot Tagging (UST) Recall <C> User Slot Tagging (UST) FrmAcc <C> User Intent Prediction (UIP) F1 <C> User Intent Prediction (UIP) Precision <C> User Intent Prediction (UIP) Recall <C> User Intent Prediction (UIP) FrmAcc <C> NLU (UST+UIP) FrmAcc <R> <C> NLU-Baseline <C> 40.50 <C> 61.41 <C> 30.21 <C> [BOLD] 77.31 <C> 49.75 <C> 52.56 <C> 47.24 <C> 37.19 <C> 33.13 <R> <C> NLU-Pipeline <C> 46.15 <C> 54.63 <C> 39.96 <C> 76.84 <C> 47.48 <C> 52.19 <C> 43.55 <C> 39.96 <C> 36.38 <R> <C> NLU-JointModel <C> 45.04 <C> 53.35 <C> 38.97 <C> 76.49 <C> 49.67 <C> 52.22 <C> 47.35 <C> [BOLD] 42.20 <C> [BOLD] 37.38 <CAP> Table 3: Performance (%) of NLU models, where F1, Precision and Recall are at token-level and FrmAcc is at frame-level.
<R> <C> [BOLD] Corpus <C> [BOLD] Vertexes <C> [BOLD] Edges <C> [BOLD] Average degree <C> [BOLD] Average path length <C> [BOLD] Clustering coefficient <R> <C> [ITALIC] OC <C> 21881 <C> 257846 <C> 23.57 <C> 3.26 <C> 0.166 <R> <C> [ITALIC] RNC <C> 22467 <C> 163914 <C> 14.6 <C> 3.53 <C> 0.136 <R> <C> [ITALIC] Mix <C> 31984 <C> 395225 <C> 24.7 <C> 3.29 <C> 0.186 <R> <C> [ITALIC] QC <C> 85548 <C> 291033 <C> 6.8 <C> 4.07 <C> 0.16 <CAP> Table 2: Parameters of the graphs
<R> <C> [BOLD] Metric <C> [BOLD] Grammar <C> [BOLD] Fluency <C> [BOLD] Semantics <C> [BOLD] Avg <R> <C> METEOR <C> 0.788±0.04 <C> 0.792±0.04 <C> 0.576±0.06 <C> 0.719 <R> <C> ROUGE <C> 0.788±0.04 <C> 0.792±0.04 <C> 0.576±0.06 <C> 0.719 <R> <C> CIDEr <C> 0.804±0.03 <C> 0.753±0.04 <C> 0.860±0.02 <C> 0.806 <R> <C> BLEU <C> [BOLD] 0.858±0.02 <C> [BOLD] 0.811±0.03 <C> 0.775±0.03 <C> 0.815 <R> <C> BLEU-T <C> 0.849±0.02 <C> 0.801±0.03 <C> 0.816±0.02 <C> 0.822 <R> <C> CIDErD <C> 0.838±0.04 <C> 0.796±0.04 <C> 0.853±0.02 <C> 0.829 <R> <C> PARENT-W <C> 0.821±0.03 <C> 0.768±0.04 <C> [BOLD] 0.887±0.02 <C> 0.825 <R> <C> PARENT-C <C> 0.851±0.03 <C> 0.809±0.04 <C> 0.877±0.02 <C> [BOLD] 0.846 <CAP> Table 4: Average pearson correlation across 500 bootstrap samples of each metric to human ratings for each aspect of the generations from the WebNLG challenge.
<R> <C> [BOLD] System <C> [BOLD] WF <C> [BOLD] KM <C> [BOLD] SM <R> <C> [BOLD] Kim-1 Kim14 <C> 87.92 <C> 44.53 <C> 33.10 <R> <C> [BOLD] Kim-2 Kim14 <C> 95.66 <C> 46.53 <C> 85.19 <R> <C> [BOLD] Choi et al. Choi18 <C> 96.91 <C> 50.29 <C> 87.46 <R> <C> [BOLD] M-BERT bert <C> 97.00 <C> 51.09 <C> 77.40 <R> <C> [BOLD] IEEC[ [ITALIC] mj] [BOLD] +ALL <C> 97.32 <C> [BOLD] 68.33 <C> 91.18 <R> <C> [BOLD] IEEC[ [ITALIC] mjc] [BOLD] +ALL <C> 97.27 <C> 68.10 <C> [BOLD] 91.36 <R> <C> [BOLD] IEEC[ [ITALIC] mbj] [BOLD] +ALL <C> [BOLD] 97.47 <C> 65.23 <C> 91.11 <CAP> Table 8: Comparison of the baseline systems and the proposed IEE approach
<R> <C> [BOLD] System <C> [BOLD] WF <C> [BOLD] KM <C> [BOLD] SM <R> <C> [BOLD] IEEC[ [ITALIC] mbjc] <C> [BOLD] 97.23 <C> 50.00 <C> 73.49 <R> <C> [BOLD] IEEM[ [ITALIC] mbjc] <C> 96.99 <C> [BOLD] 51.39 <C> [BOLD] 75.21 <R> <C> [BOLD] IEEW[ [ITALIC] mbjc] <C> 97.11 <C> 49.18 <C> 73.70 <R> <C> [BOLD] IEEC[ [ITALIC] mbjc] [BOLD] +JD <C> [BOLD] 97.41 <C> [BOLD] 64.83 <C> 75.92 <R> <C> [BOLD] IEEM[ [ITALIC] mbjc] [BOLD] +JD <C> 96.98 <C> 63.69 <C> [BOLD] 77.50 <R> <C> [BOLD] IEEW[ [ITALIC] mbjc] [BOLD] +JD <C> 97.36 <C> 63.06 <C> 75.68 <R> <C> [BOLD] IEEC[ [ITALIC] mbjc] [BOLD] +SG <C> [BOLD] 97.26 <C> 54.07 <C> [BOLD] 89.40 <R> <C> [BOLD] IEEM[ [ITALIC] mbjc] [BOLD] +SG <C> 96.95 <C> [BOLD] 55.80 <C> 89.05 <R> <C> [BOLD] IEEW[ [ITALIC] mbjc] [BOLD] +SG <C> 97.13 <C> 54.38 <C> 89.32 <R> <C> [BOLD] IEEC[ [ITALIC] mbjc] [BOLD] +ALL <C> [BOLD] 97.34 <C> 65.03 <C> 91.01 <R> <C> [BOLD] IEEM[ [ITALIC] mbjc] [BOLD] +ALL <C> 97.04 <C> [BOLD] 65.30 <C> 90.95 <R> <C> [BOLD] IEEW[ [ITALIC] mbjc] [BOLD] +ALL <C> [BOLD] 97.34 <C> 62.93 <C> [BOLD] 91.14 <CAP> Table 9: Comparison results between the three proposed IEE approaches with the two noise insertion methods.
<R> <C> [BOLD] System <C> [BOLD] WF <C> [BOLD] KM <C> [BOLD] SM <R> <C> [BOLD] IMEC[ [ITALIC] mjc] <C> 96.93 <C> 51.18 <C> 87.76 <R> <C> [BOLD] IMEC[ [ITALIC] mjc] [BOLD] +JD <C> 97.23 <C> 62.35 <C> 90.00 <R> <C> [BOLD] IMEC[ [ITALIC] mjc] [BOLD] +SG <C> 96.98 <C> 56.30 <C> 90.33 <R> <C> [BOLD] IMEC[ [ITALIC] mjc] [BOLD] +ALL <C> 97.28 <C> 65.09 <C> [BOLD] 91.66 <R> <C> [BOLD] IEEC[ [ITALIC] mjc] [BOLD] +ALL <C> [BOLD] 97.32 <C> [BOLD] 68.10 <C> 91.36 <R> <C> [BOLD] IBEC[ [ITALIC] bjc] <C> 96.45 <C> 54.83 <C> 79.76 <R> <C> [BOLD] IBEC[ [ITALIC] bjc] [BOLD] +JD <C> 96.69 <C> 57.79 <C> 83.02 <R> <C> [BOLD] IBEC[ [ITALIC] bjc] [BOLD] +SG <C> 96.26 <C> 55.96 <C> 89.70 <R> <C> [BOLD] IBEC[ [ITALIC] bjc] [BOLD] +ALL <C> 96.54 <C> 61.43 <C> [BOLD] 90.89 <R> <C> [BOLD] IEEC[ [ITALIC] bjc] [BOLD] +ALL <C> [BOLD] 96.74 <C> [BOLD] 64.97 <C> 90.37 <CAP> Table 10: The comparison results of Eojeol based approach and other subword unit-based integrated embedding approaches.
<R> <C> [BOLD] System <C> [BOLD] WF <C> [BOLD] KM <C> [BOLD] SM <R> <C> [BOLD] IEEC[ [ITALIC] m] [BOLD] +SG <C> 97.04 <C> 49.58 <C> 86.42 <R> <C> [BOLD] IEEC[ [ITALIC] b] [BOLD] +SG <C> 96.27 <C> 55.29 <C> 87.73 <R> <C> [BOLD] IEEC[ [ITALIC] c] [BOLD] +SG <C> 95.31 <C> [BOLD] 60.37 <C> 88.58 <R> <C> [BOLD] IEEC[ [ITALIC] mb] [BOLD] +SG <C> [BOLD] 97.38 <C> 51.34 <C> 88.70 <R> <C> [BOLD] IEEC[ [ITALIC] mc] [BOLD] +SG <C> 97.17 <C> 54.17 <C> 89.13 <R> <C> [BOLD] IEEC[ [ITALIC] bc] [BOLD] +SG <C> 96.40 <C> 59.68 <C> 89.20 <R> <C> [BOLD] IEEC[ [ITALIC] mbc] [BOLD] +SG <C> 97.20 <C> 55.88 <C> [BOLD] 89.26 <R> <C> [BOLD] IEEC[ [ITALIC] j] [BOLD] +ALL <C> 95.19 <C> [BOLD] 69.16 <C> 89.09 <R> <C> [BOLD] IEEC[ [ITALIC] mj] [BOLD] +ALL <C> 97.32 <C> 68.33 <C> 91.18 <R> <C> [BOLD] IEEC[ [ITALIC] bj] [BOLD] +ALL <C> 96.60 <C> 64.04 <C> 90.13 <R> <C> [BOLD] IEEC[ [ITALIC] jc] [BOLD] +ALL <C> 95.89 <C> 68.15 <C> 90.22 <R> <C> [BOLD] IEEC[ [ITALIC] mbj] [BOLD] +ALL <C> [BOLD] 97.47 <C> 65.23 <C> 91.11 <R> <C> [BOLD] IEEC[ [ITALIC] mjc] [BOLD] +ALL <C> 97.27 <C> 68.10 <C> [BOLD] 91.36 <R> <C> [BOLD] IEEC[ [ITALIC] bjc] [BOLD] +ALL <C> 96.74 <C> 64.97 <C> 90.37 <R> <C> [BOLD] IEEC[ [ITALIC] mbjc] [BOLD] +ALL <C> 97.34 <C> 65.03 <C> 91.01 <CAP> Table 11: Experiment results on the effect of each subword unit embedding
<R> <C> Parameter <C> Value <C> Accuracy [%] IMDb <C> Accuracy [%] Amazon <C> Accuracy [%] Yelp <C> Accuracy [%] Intent <C> Accuracy [%] MRPC <R> <C> NPNN <C> NPNN <C> 70.67 <C> 67.50 <C> 66.00 <C> 94.17 <C> 68.38 <R> <C> [ITALIC] ϵ ( [ITALIC] λ=100) <C> 0.5 <C> 65.33 <C> 68.00 <C> 64.54 <C> 91.28 <C> 66.91 <R> <C> [ITALIC] ϵ ( [ITALIC] λ=100) <C> 1 <C> 67.33 <C> 69.50 <C> 66.73 <C> 91.35 <C> 67.15 <R> <C> [ITALIC] ϵ ( [ITALIC] λ=100) <C> 5 <C> 67.80 <C> 71.00 <C> 67.50 <C> 91.57 <C> 70.10 <R> <C> [ITALIC] ϵ ( [ITALIC] λ=100) <C> 10 <C> 69.33 <C> 72.50 <C> 68.10 <C> 91.87 <C> 70.15 <R> <C> [ITALIC] λ ( [ITALIC] ϵ=1) <C> 1 <C> 48.00 <C> 45.50 <C> 42.50 <C> 13.00 <C> 64.95 <R> <C> [ITALIC] λ ( [ITALIC] ϵ=1) <C> 10 <C> 64.00 <C> 65.46 <C> 57.81 <C> 85.43 <C> 66.66 <R> <C> [ITALIC] λ ( [ITALIC] ϵ=1) <C> 50 <C> 66.67 <C> 69.21 <C> 66.50 <C> 90.57 <C> 66.91 <R> <C> [ITALIC] λ ( [ITALIC] ϵ=1) <C> 100 <C> 67.33 <C> 69.50 <C> 66.73 <C> 91.35 <C> 67.15 <CAP> Table 1. Accuracy of NPNN and LDPNN using OME.
<R> <C> Model <C> Yelp <C> SST <C> Deception <R> <C> SVM (ℓ2) <C> 92.3 <C> 80.8 <C> 86.3 <R> <C> SVM (ℓ1) <C> 91.5 <C> 79.2 <C> 84.4 <R> <C> XGBoost <C> 88.8 <C> 75.9 <C> 83.4 <R> <C> LSTM w/ attention <C> 93.9 <C> 82.6 <C> 88.4 <R> <C> BERT <C> 95.5 <C> 92.2 <C> 90.9 <CAP> Table 2: Accuracy on the test set.
<R> <C> Model type System <C> [BOLD] MEMM+lexicon  [BOLD] MElt <C> [BOLD] MEMM+lexicon  [BOLD] MElt <C> [BOLD] CRF+lexicon  [BOLD] MarMoT <C> [BOLD] CRF+lexicon  [BOLD] MarMoT <C> [BOLD] bi-LSTM+Polyglot  [BOLD] (Plank et al. [BOLD] ,  2016  [BOLD] ) <C> [BOLD] bi-LSTM+Polyglot  [BOLD] (Plank et al. [BOLD] ,  2016  [BOLD] ) <C> [BOLD] FREQBIN+Polyglot  [BOLD] (Plank et al. [BOLD] ,  2016  [BOLD] ) <C> [BOLD] FREQBIN+Polyglot  [BOLD] (Plank et al. [BOLD] ,  2016  [BOLD] ) <R> <C> [EMPTY] <C> overall <C> OOV <C> overall <C> OOV <C> overall <C> OOV <C> overall <C> OOV <R> <C> Bulgarian (bg) <C> 98.15 <C> 93.95 <C> 98.05 <C> 93.06 <C> [BOLD] 98.23 <C> 87.40 <C> 97.97 <C> [BOLD] 97.37 <R> <C> Czech (cs) <C> [BOLD] 98.58 <C> 94.83 <C> 98.48 <C> 93.68 <C> 98.02 <C> 89.02 <C> 97.89 <C> [BOLD] 94.91 <R> <C> Danish (da) <C> 96.30 <C> [BOLD] 92.32 <C> 96.16 <C> 91.43 <C> 96.16 <C> 77.09 <C> [BOLD] 96.35 <C> 91.63 <R> <C> German (de) <C> 93.43 <C> 88.08 <C> 93.10 <C> 87.21 <C> [BOLD] 93.51 <C> 81.95 <C> 93.38 <C> [BOLD] 90.97 <R> <C> English (en) <C> 94.60 <C> 79.61 <C> 94.55 <C> [BOLD] 79.99 <C> [BOLD] 95.17 <C> 71.23 <C> 95.16 <C> 70.57 <R> <C> Spanish (es) <C> 95.57 <C> 81.24 <C> 95.24 <C> 79.52 <C> 95.67 <C> 71.38 <C> [BOLD] 95.74 <C> [BOLD] 98.22 <R> <C> Persian (fa) <C> 97.17 <C> 87.14 <C> 96.97 <C> 86.89 <C> [BOLD] 97.60 <C> 80.00 <C> 97.49 <C> [BOLD] 96.54 <R> <C> French (fr) <C> 96.14 <C> 85.97 <C> [BOLD] 96.34 <C> 85.97 <C> 96.20 <C> 78.09 <C> 96.11 <C> [BOLD] 92.13 <R> <C> Croatian (hr) <C> 96.70 <C> 93.01 <C> 96.19 <C> 91.23 <C> 96.27 <C> 84.62 <C> [BOLD] 96.82 <C> [BOLD] 97.29 <R> <C> Indonesian (id) <C> [BOLD] 93.83 <C> 88.48 <C> 93.82 <C> 88.41 <C> 93.32 <C> 88.25 <C> 93.41 <C> [BOLD] 94.70 <R> <C> Italian (it) <C> 97.82 <C> 91.98 <C> [BOLD] 98.03 <C> 91.82 <C> 97.90 <C> 83.59 <C> 97.95 <C> [BOLD] 98.46 <R> <C> Norwegian (no) <C> 97.58 <C> 93.87 <C> 97.62 <C> 94.16 <C> [BOLD] 98.06 <C> 92.05 <C> 98.03 <C> [BOLD] 97.78 <R> <C> Polish (pl) <C> [BOLD] 97.77 <C> 96.24 <C> 97.47 <C> 95.12 <C> 97.63 <C> 91.77 <C> 97.62 <C> [BOLD] 99.35 <R> <C> Portuguese (pt) <C> 97.56 <C> 92.27 <C> 97.39 <C> 91.92 <C> [BOLD] 97.94 <C> 92.16 <C> 97.90 <C> [BOLD] 96.87 <R> <C> Slovene (sl) <C> [BOLD] 97.53 <C> 96.50 <C> 97.23 <C> 94.89 <C> 96.97 <C> 80.48 <C> 96.84 <C> [BOLD] 95.63 <R> <C> Swedish (sv) <C> [BOLD] 96.90 <C> 94.78 <C> 96.80 <C> 94.23 <C> 96.60 <C> 88.37 <C> 96.69 <C> [BOLD] 96.02 <R> <C> Macro-avg. <C> [BOLD] 96.60 <C> 90.64 <C> 96.46 <C> 89.97 <C> 96.58 <C> 83.59 <C> 96.58 <C> [BOLD] 94.28 <CAP> Table 4: Accuracy (in %) of the feature-based systems MElt and MarMoT as well as the two best LSTM-based systems by Plank et al. (2016) on UD1.2 datasets, which all use the 17 “universal PoS tags”. MElt and MarMoT models integrate the external lexicons listed in Table 2, whereas bidirectional LSTM-based systems rely on Polyglot word embeddings. Best scores overall and on OOV words are highlighted for each corpus.
<R> <C> Train/Test <C> ASPEC <C> KWC <C> OpenSubs <C> JESC <R> <C> ASPEC <C> [BOLD] 36.23 <C> 15.42 <C> 3.45 <C> 3.81 <R> <C> KWC <C> 5.30 <C> [BOLD] 8.61 <C> 2.31 <C> 2.22 <R> <C> OpenSubs <C> 0.2 <C> 0.7 <C> [BOLD] 10.01 <C> 6.3 <R> <C> JESC <C> 2.35 <C> 3.71 <C> 8.8 <C> [BOLD] 14.21 <CAP> Table 2: Machine translation results (BLEU).
<R> <C> Model <C> Overall <C> [ITALIC] dΔ [ITALIC] t=(0,15] <C> [ITALIC] dΔ [ITALIC] t=(15,30] <C> [ITALIC] dΔ [ITALIC] t=(30,60] <C> Overall DSTC2 F1 <R> <C> Baseline (Naik et al.,  2018 ) <C> 87.8 <C> 89.3 <C> 86.8 <C> 74.3 <C> 95.0 <R> <C> STM <C> 88.4 <C> 89.7 <C> 87.5 <C> 77.5 <C> [BOLD] 96.1 <R> <C> ITM <C> 88.6 <C> 89.8 <C> 87.8 <C> 76.9 <C> - <R> <C> DTM <C> [BOLD] 89.2 <C> [BOLD] 90.5 <C> [BOLD] 88.3 <C> [BOLD] 80.0 <C> - <R> <C> TDA Su et al. ( 2018 ) <C> 88.4 <C> 90.0 <C> 87.5 <C> 72.8 <C> 94.6 <CAP> Table 2: Overall F1 scores on the IPDA and DSTC2 dataset as well as F1 scores binned by dΔt for the IPDA dataset, which is measured in seconds. Note: the DSTC2 dataset only contains a single domain
<R> <C> WER (%) <C> Low (7.64) P <C> Low (7.64) R <C> Low (7.64)  [ITALIC] F0.5 <C> High (20.86) P <C> High (20.86) R <C> High (20.86)  [ITALIC] F0.5 <R> <C> Transformer <C> 37.69 <C> [BOLD] 37.67 <C> 37.72 <C> 67.27 <C> [BOLD] 42.05 <C> [BOLD] 60.06 <R> <C> LSTM <C> [BOLD] 48.68 <C> 29.37 <C> [BOLD] 43.02 <C> [BOLD] 72.97 <C> 31.09 <C> 57.47 <R> <C> CNN <C> 44.35 <C> 30.87 <C> 40.78 <C> 70.85 <C> 32.77 <C> 57.49 <R> <C> SMT <C> 40.73 <C> 18.60 <C> 32.91 <C> 67.95 <C> 16.89 <C> 42.35 <CAP> Table 4: Performance in precision, recall, and F0.5 of all models on the corpora when the WER is lowest and highest.
<R> <C> [BOLD] Methods <C> [BOLD] WN11 <C> [BOLD] FB13 <C> [BOLD] Avg. <R> <C> LFM <C> 73.8 <C> 84.3 <C> 79.0 <R> <C> NTN <C> 70.4 <C> 87.1 <C> 78.8 <R> <C> TransE <C> 75.9 <C> 81.5 <C> 78.7 <R> <C> TransH <C> 78.8 <C> 83.3 <C> 81.1 <R> <C> TransR <C> [BOLD] 85.9 <C> 82.5 <C> 84.2 <R> <C> Adaptive Metric (PSD) <C> 81.4 <C> 87.1 <C> 84.3 <R> <C> [BOLD] TransA <C> 83.2 <C> [BOLD] 87.3 <C> [BOLD] 85.3 <CAP> Table 4: Triples classification: accuracies(%) for different embedding methods
<R> <C> Setting <C> F1 <C> Precision <R> <C> all <C> [BOLD] 94.2 <C> [BOLD] 94.8 <R> <C> w/o lexical score <C> 84.3 <C> 86.5 <R> <C> w/o statistical score <C> 92.8 <C> 93.9 <R> <C> w/o edit distance <C> 93.9 <C> 94.4 <R> <C> w/o dictionary <C> 93.1 <C> 93.9 <R> <C> LCS (Zhang et al.,  2018 ) <C> 91.3 <C> 92.2 <CAP> Table 3. Evaluation results on the Test set.
<R> <C> Model <C> 1-gram <C> 2-gram <C> 3-gram <C> 4-gram <R> <C> SMT <C> 50.24 <C> 38.05 <C> 29.93 <C> 24.04 <R> <C> + Augment <C> 51.70 <C> 39.09 <C> 30.73 <C> 24.72 <R> <C> basic RNN-based NMT <C> 23.43 <C> 17.24 <C> 13.16 <C> 10.26 <R> <C> + Reversal <C> 29.84 <C> 21.55 <C> 16.13 <C> 12.39 <R> <C> + Residual <C> 33.66 <C> 23.52 <C> 17.25 <C> 13.03 <R> <C> + Word2vec <C> 34.29 <C> 25.11 <C> 19.02 <C> 14.74 <R> <C> + Augment <C> 53.35 <C> 40.10 <C> 31.51 <C> 25.42 <R> <C> basic Transformer <C> 39.51 <C> 29.14 <C> 22.42 <C> 17.72 <R> <C> + Augment <C> [BOLD] 54.23 <C> [BOLD] 41.61 <C> [BOLD] 33.22 <C> [BOLD] 27.16 <CAP> Table 5. 1-4 gram BLEUs results on various models.
<R> <C> Model <C> 1-gram <C> 2-gram <C> 3-gram <C> 4-gram <R> <C> basic RNN-based NMT <C> 49.67 <C> 37.00 <C> 28.68 <C> 22.64 <R> <C> + Augment <C> 49.90 <C> 38.24 <C> 30.28 <C> 24.43 <R> <C> basic Transformer <C> 50.47 <C> 38.28 <C> 30.17 <C> 24.47 <R> <C> + Augment <C> [BOLD] 51.85 <C> [BOLD] 39.60 <C> [BOLD] 31.51 <C> [BOLD] 25.75 <CAP> Table 6. 1-4 gram BLEUs results of NMT models tested on only unaugmented test data.
<R> <C> [BOLD] Method /  [BOLD] Dataset <C> [BOLD] WN18RR H@1 <C> [BOLD] WN18RR H@5 <C> [BOLD] WN18RR H@10 <C> [BOLD] WN18RR MRR <C> [BOLD] NELL-995 H@1 <C> [BOLD] NELL-995 H@5 <C> [BOLD] NELL-995 H@10 <C> [BOLD] NELL-995 MRR <C> [BOLD] FB15k-237 H@1 <C> [BOLD] FB15k-237 H@5 <C> [BOLD] FB15k-237 H@10 <C> [BOLD] FB15k-237 MRR <R> <C> DistMult Yang2014EmbeddingEA <C> 35.7 <C> - <C> 38.4 <C> 36.7 <C> 55.2 <C> - <C> 78.3 <C> 64.1 <C> 32.4 <C> - <C> 60.0 <C> 41.7 <R> <C> ComplEx Trouillon2016ComplexEF <C> 41.5 <C> 45.6 <C> 46.9 <C> 43.4 <C> 63.9 <C> 81.7 <C> 84.8 <C> 72.1 <C> 33.7 <C> 54.0 <C> 62.4 <C> 43.2 <R> <C> ConvE dettmers2018convolutional <C> 40.1 <C> 49.8 <C> 53.7 <C> 44.6 <C> [BOLD] 66.7 <C> [BOLD] 85.3 <C> [BOLD] 88.2 <C> [BOLD] 75.1 <C> [BOLD] 34.2 <C> [BOLD] 54.9 <C> [BOLD] 62.9 <C> [BOLD] 43.9 <R> <C> RotateE Sun2019RotatEKG <C> [BOLD] 42.2 <C> [BOLD] 51.3 <C> [BOLD] 54.1 <C> [BOLD] 46.4 <C> - <C> - <C> - <C> - <C> 32.2 <C> 53.2 <C> 61.6 <C> 42.2 <R> <C> AnyBURL (C rules) Meilicke2019AnytimeBR <C> [BOLD] 44.2 <C> [BOLD] 52.6 <C> 55.6 <C> - <C> [BOLD] 44.0 <C> [BOLD] 56.0 <C> [BOLD] 57.0 <C> - <C> 26.9 <C> 43.1 <C> 52.0 <C> - <R> <C> pLogicNet anonymous2020efficient <C> 40.1 <C> 49.6 <C> [BOLD] 56.3 <C> [BOLD] 45.0 <C> - <C> - <C> - <C> - <C> [BOLD] 32.9 <C> [BOLD] 54.6 <C> [BOLD] 63.2 <C> [BOLD] 43.0 <R> <C> MINERVA das2017go <C> 41.3 <C> - <C> 51.3 <C> 44.8 <C> 66.3 <C> - <C> 83.1 <C> 72.5 <C> 21.7 <C> - <C> 45.6 <C> 29.3 <R> <C> MultiHop (ConvE) lin-etal-2018-multi-hop <C> 41.3 <C> 47.9 <C> 51.1 <C> 47.7 <C> 65.6 <C> - <C> 84.4 <C> 72.7 <C> [BOLD] 31.7 <C> - <C> 56.4 <C> [BOLD] 40.7 <R> <C> Ours (ConvE) <C> 42.2 <C> 49.9 <C> 53.6 <C> 46.0 <C> 66.0 <C> 82.0 <C> 85.1 <C> 73.1 <C> 30.5 <C> 49.9 <C> [BOLD] 57.6 <C> 39.9 <R> <C> Ours (ComplEx) <C> [BOLD] 44.3 <C> [BOLD] 51.9 <C> [BOLD] 55.0 <C> [BOLD] 48.0 <C> [BOLD] 66.4 <C> [BOLD] 82.7 <C> [BOLD] 85.9 <C> [BOLD] 73.6 <C> 30.7 <C> [BOLD] 49.4 <C> 56.9 <C> 38.5 <CAP> Table 2: Performance comparison with state-of-the-art embedding-based, symbolic-based and walk-based approaches. Best approaches are highlighted in each category. Our method outperforms walk-based state-of-the-art on three dataset and improves upon symbolic approaches if their rules cannot generalize well.
<R> <C> Model <C> @1 <C> @MRR <R> <C> Freeze pre-trained agent <C> 41.9 <C> 45.5 <R> <C> No pre-training <C> 41.8 <C> 45.7 <R> <C> Single agent <C> 42.4 <C> 46.4 <R> <C> Ours <C> [BOLD] 42.9 <C> [BOLD] 46.5 <CAP> Table 4: Ablation study on WN18RR development set. All these walk-based models use ComplEx embedding. Single agent model do not separate model into relation and entity agent.
<R> <C> Model <C> @1 <C> @MRR <R> <C> Confidence >0.00 <C> 42.7 <C> 46.3 <R> <C> Confidence >0.05 <C> 42.7 <C> [BOLD] 46.6 <R> <C> Confidence >0.10 <C> 42.4 <C> 46.0 <R> <C> Confidence >0.15 <C> [BOLD] 42.9 <C> 46.5 <CAP> Table 5: Performance analysis on WN18RR development set for different confidence score threshold. All these walk-based models use ComplEx embedding.
<R> <C> Caption method <C> First [%] <C> Second [%] <C> Third [%] <R> <C> Human <C> [BOLD] 67.99 <C> 25.59 <C> 9.414 <R> <C> NJM (Proposed) <C> [BOLD] 22.59 <C> 60.04 <C> 17.36 <R> <C> STAIR caption  <C> 9.414 <C> 17.36 <C> 73.22 <CAP> Table 1: Comparison of the output results: The “Human” row indicates captions provided by human users and was ranked highest on the Bokete website. The “NJM” row indicates the results of applying the proposed model based of Funny Score and BoketeDB. The “STAIR caption” row indicates the results provided by Japanese translation of MS COCO.
<R> <C> [BOLD] System <C> [BOLD] AddSent (F1) <C> [BOLD] AddOneSent (F1) <C> [BOLD] Test (F1) <C> Δ1 <C> Δ2 <R> <C> [ITALIC] Single model <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> R.M-Reader Hu et al. ( 2018 ) <C> 58.5 <C> 67.0 <C> 86.6 <C> 28.1 <C> 19.6 <R> <C> KAR Wang and Jiang ( 2018 ) <C> 60.1 <C> 72.3 <C> 83.5 <C> 23.4 <C> 11.2 <R> <C> [ITALIC] Our single model <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> BERT <C> 61.0 <C> 71.1 <C> 91.4 <C> 30.4 <C> 20.3 <R> <C> BERT+AT+VAT(2.0) <C> 63.5 <C> 72.5 <C> 92.4 <C> 28.9 <C> 19.9 <R> <C> [ITALIC] Absolute improvement(%) <C> 2.5 <C> 1.4 <C> 1.0 <C> [EMPTY] <C> [EMPTY] <R> <C> [ITALIC] Relative improvement(%) <C> 6.4 <C> 4.8 <C> 13.2 <C> [EMPTY] <C> [EMPTY] <CAP> Table 6: Model performance on AddSent and AddOneSent. Results on SQuAD1.1 are also provided for comparison. Δ1 is the difference between Test(F1) and AddSent(F1); Δ2 is the difference between Test(F1) and AddOneSent(F1).
<R> <C> [EMPTY] <C> #phrase from ( [ITALIC] s, [ITALIC] r) <C> #phrase from ( [ITALIC] t, [ITALIC] r) <R> <C> Navigation <C> 160 <C> 748 <R> <C> Manipulation <C> 128 <C> 298 <CAP> Table 1: Number of extracted lexical entries
<R> <C> Model <C> w/ linear <C> w/ biLSTM <R> <C> GloVe embedding <C> 81.3 <C> 88.7 <R> <C> ELMo embedding <C> 88.3 <C> 97.6 <R> <C> Char embedding (Ours) <C> − <C> 97.3 <CAP> Table 1: Performance on the synthetic timex dataset, classifying a pair of timexes as before, after, or simultaneous. Including a biLSTM layer (as depicted in Figure 2) leads to higher performance than just pooling and a linear layer. Character-level modeling (from ELMo or our learned embeddings) is important for high performance.
<R> <C> Model <C> GloVe <C> ELMo <R> <C> cheng2017classifying <C> 59.53 <C> 65.50 <R> <C> Ours w/o timex embed <C> 62.83 <C> 68.45 <R> <C> Ours w/ timex embed <C> 63.22 <C> 68.61 <CAP> Table 2: Performance of our event temporal ordering model on the MATRES dataset. We report the mean accuracy over 3 runs of each model. Our model improves substantially over cheng2017classifying. Including timexes leads to small accuracy gains, partially due to the fact that timexes often do not occur with the dataset’s hard examples.
<R> <C> [EMPTY] <C> 2000 <C> 3000 <C> 4000 <R> <C> GloVe <C> GloVe <C> GloVe <C> GloVe <R> <C> Ours w/o Timex Embed <C> 74.0 <C> 76.8 <C> 78.2 <R> <C> Ours w/ Masked Timex <C> 73.9 <C> 75.5 <C> 77.1 <R> <C> Ours w/ Timex Embed <C> 81.6 <C> 83.2 <C> 83.1 <R> <C> ELMo <C> ELMo <C> ELMo <C> ELMo <R> <C> Ours w/o Timex Embed <C> 80.1 <C> 83.8 <C> 84.3 <R> <C> Ours w/ Masked Timex <C> 79.8 <C> 80.1 <C> 80.7 <R> <C> Ours w/ Timex Embed <C> 82.3 <C> 84.5 <C> 84.8 <CAP> Table 3: Performance of our models on the distantly-labeled event ordering data. We report overall accuracy values. In both the GloVe and ELMo settings, our timex embeddings lead to higher performance. The ELMo model gets substantially worse when timexes are masked, indicating that it is organically exploiting these better than GloVe is.
<R> <C> [ITALIC] Slice size <C> 32 <C> 64 <C> 128 <R> <C> [ITALIC] Sliced DocQA w/o early stopping <C> 0.76 <C> 0.77 <C> 0.78 <R> <C> [ITALIC] Sliced DocQA w early stopping <C> 0.54 (%71) <C> 0.69(%90) <C> 0.77 (%99) <CAP> Table 2: Performance of Sliced DocQA with step transfer and early stopping in terms of text-f1.
<R> <C> Method <C> Appearance % precision <C> Appearance % selected <C> Smell % precision <C> Smell % selected <C> Palate % precision <C> Palate % selected <R> <C> SVM <C> 38.3 <C> 13 <C> 21.6 <C> 7 <C> 24.9 <C> 7 <R> <C> Attention model <C> 80.6 <C> 13 <C> 88.4 <C> 7 <C> 65.3 <C> 7 <R> <C> Generator (independent) <C> 94.8 <C> 13 <C> 93.8 <C> 7 <C> 79.3 <C> 7 <R> <C> Generator (recurrent) <C> 96.3 <C> 14 <C> 95.1 <C> 7 <C> 80.2 <C> 7 <CAP> Table 2: Precision of selected rationales for the first three aspects. The precision is evaluated based on whether the selected words are in the sentences describing the target aspect, based on the sentence-level annotations. Best training epochs are selected based on the objective value on the development set (no sentence annotation is used).
<R> <C> [EMPTY] <C> MAP (dev) <C> MAP (test) <C> %words <R> <C> Full title <C> 56.5 <C> 60.0 <C> 10.1 <R> <C> Full body <C> 54.2 <C> 53.0 <C> 89.9 <R> <C> Independent <C> 55.7 <C> 53.6 <C> 9.7 <R> <C> Independent <C> 56.3 <C> 52.6 <C> 19.7 <R> <C> Dependent <C> 56.1 <C> 54.6 <C> 11.6 <R> <C> Dependent <C> 56.5 <C> 55.6 <C> 32.8 <CAP> Table 4: Comparison between rationale models (middle and bottom rows) and the baselines using full title or body (top row).
<R> <C> [BOLD] ZH-EN <C> [BOLD] Dev (NIST02) <C> [BOLD] NIST03 <C> [BOLD] NIST04 <C> [BOLD] NIST05 <C> [BOLD] NIST06 <C> [BOLD] NIST08 <C> [BOLD] AVG <R> <C> PBSMT <C> 33.15 <C> 31.02 <C> 33.78 <C> 30.33 <C> 29.62 <C> 23.53 <C> 29.66 <R> <C> GlobalAtt <C> 37.12 <C> 35.24 <C> 37.49 <C> 34.60 <C> 32.48 <C> 26.32 <C> 33.23 <R> <C> Chen et al. ( 2017 ) <C> 37.42 <C> 35.98 <C> 38.34 <C> 35.28 <C> 33.58 <C> 27.23 <C> 34.08 <R> <C> LocalAtt <C> 37.31 <C> 35.57 <C> 37.85 <C> 34.93 <C> 32.74 <C> 26.83 <C> 33.58 <R> <C> FlexAtt <C> 37.19 <C> 35.46 <C> 37.81 <C> 34.76 <C> 32.83 <C> 26.71 <C> 33.51 <R> <C> [BOLD] SDAtt <C> [BOLD] 38.01 <C> [BOLD] 36.67∗∗† <C> [BOLD] 38.66∗∗† <C> [BOLD] 35.74∗∗† <C> [BOLD] 34.03∗∗† <C> [BOLD] 27.66∗∗† <C> [BOLD] 34.55 <CAP> Table 1: Results on ZH-EN and EN-DE translation tasks for the proposed SDAtt. “*” indicates that the model significantly outperforms GlobalAtt at p-value<0.05, “**” indicates that the model significantly outperforms GlobalAtt at p-value<0.01. “†” indicates that the model significantly outperforms the best baseline Chen et al.2017’s Model at p-value<0.05. AVG is the average BLEU score for all test sets. The bold indicates that the BLEU score of test set is better than the best baseline system.
<R> <C> [BOLD] EN-DE <C> [BOLD] Dev (newstest2012) <C> [BOLD] newstest2013 <C> [BOLD] newstest2014 <C> [BOLD] newstest2015 <C> [BOLD] AVG <R> <C> PBSMT <C> 14.89 <C> 16.75 <C> 15.19 <C> 16.84 <C> 16.35 <R> <C> GlobalAtt <C> 17.09 <C> 20.24 <C> 18.67 <C> 19.78 <C> 19.56 <R> <C> Chen et al. ( 2017 ) <C> 17.48 <C> 21.03 <C> 19.43 <C> 20.56 <C> 20.31 <R> <C> LocalAtt <C> 17.19 <C> 20.74 <C> 19.00 <C> 20.15 <C> 19.96 <R> <C> FlexibleAtt <C> 17.24 <C> 20.57 <C> 19.12 <C> 20.03 <C> 19.91 <R> <C> [BOLD] SDAtt <C> [BOLD] 17.86 <C> [BOLD] 21.71∗∗† <C> [BOLD] 20.36∗∗† <C> [BOLD] 21.57∗∗† <C> [BOLD] 21.21 <CAP> Table 1: Results on ZH-EN and EN-DE translation tasks for the proposed SDAtt. “*” indicates that the model significantly outperforms GlobalAtt at p-value<0.05, “**” indicates that the model significantly outperforms GlobalAtt at p-value<0.01. “†” indicates that the model significantly outperforms the best baseline Chen et al.2017’s Model at p-value<0.05. AVG is the average BLEU score for all test sets. The bold indicates that the BLEU score of test set is better than the best baseline system.
<R> <C> [BOLD] Model <C> [BOLD] Corpus <C> [BOLD] OPP <C> [BOLD] Acc. <R> <C> GloVe <C> Common Crawl <C> 76.73 <C> 43.25 <R> <C> GloVe <C> Wikipedia+Gigaword <C> 76.19 <C> 47.69 <R> <C> CBOW <C> Wikipedia+Gigaword <C> 82.59 <C> 55.90 <R> <C> CBOW <C> Google News (with phrases) <C> 63.67 <C> 24.74 <R> <C> CBOW <C> Google News <C> 66.20 <C> 27.43 <R> <C> CBOW <C> Leipzig+Europarl (MultiCCA) <C> 75.01 <C> 42.82 <R> <C> Skip-Gram <C> Wikipedia+Gigaword <C> 82.03 <C> 56.80 <CAP> Table 4: Performance of English word embeddings on their common in-vocabulary intersection of the WikiSem500 dataset.
<R> <C> [BOLD] Language <C> [BOLD] OPP <C> [BOLD] Acc. <C> [BOLD] Groups Skipped <C> [BOLD] % Cluster Items OOV <C> [BOLD] % Outliers OOV <C> [BOLD] Vocab. Size <R> <C> Spanish <C> 77.25 <C> 46.00 <C> 22 <C> 21.55 <C> 17.75 <C> 225,950 <R> <C> German <C> 76.17 <C> 43.46 <C> 31 <C> 24.45 <C> 25.74 <C> 376,552 <R> <C> Japanese <C> 72.51 <C> 40.18 <C> 54 <C> 36.87 <C> 24.66 <C> 70,551 <R> <C> Chinese <C> 67.61 <C> 34.58 <C> 12 <C> 37.74 <C> 34.29 <C> 70,865 <CAP> Table 6: Performance of Non-English word embeddings on entire WikiSem500 dataset.
<R> <C> [BOLD] System <C> [BOLD] BLEU <C> [BOLD] #Param. <R> <C> LSTM (6 layers) <C> 24.12 <C> 178.90M <R> <C> Big-noEnc-noPos <C> 9.97 <C> 171.58M <R> <C> + Absolute PE <C> 24.11 <C> +0.00M <R> <C> + Relative PE <C> 24.47 <C> +0.01M <R> <C> + InXL PE <C> 24.68 <C> +0.01M <CAP> Table 4: Gains over Encoder-Free Transformer.
<R> <C> [EMPTY] <C> En <C> Es <C> Fr <C> Jp <C> Avg <R> <C> Monolingual Models <C> Monolingual Models <C> Monolingual Models <C> Monolingual Models <C> Monolingual Models <C> Monolingual Models <R> <C> Embeds <C> 50.6 <C> 82.0 <C> 66.5 <C> 65.1 <C> 66.05 <R> <C> Words (W) <C> 66.1 <C> 86.9 <C> 73.2 <C> 73.6 <C> 74.95 <R> <C> Chars (C) <C> 68.2 <C> 87.1 <C> 76.1 <C> 74.0 <C> 76.35 <R> <C> W+Chars (C) <C> 65.9 <C> 87.7 <C> 75.7 <C> 74.0 <C> 75.82 <R> <C> C+Embeds‡ <C> 66.1 <C> 86.6 <C> 76.5 <C> 77.1 <C> 76.58 <R> <C> W+C+Embeds <C> 65.9 <C> 87.8 <C> 75.6 <C> 76.8 <C> 76.52 <R> <C> Bilingual Model <C> Bilingual Model <C> Bilingual Model <C> Bilingual Model <C> Bilingual Model <C> Bilingual Model <R> <C> En+Es <C> 67.6 <C> 86.6 <C> – <C> – <C> – <R> <C> En+Fr <C> 66.6 <C> – <C> 77.8 <C> – <C> – <R> <C> En+Jp <C> 66.7 <C> – <C> – <C> 77.9 <C> – <R> <C> Multilingual Model <C> Multilingual Model <C> Multilingual Model <C> Multilingual Model <C> Multilingual Model <C> Multilingual Model <R> <C> En+Es+Fr <C> 68.3 <C> 87.0 <C> 77.9 <C> – <C> – <R> <C> All-in-1‡ <C> 68.8 <C> 87.7 <C> 76.4 <C> 77.2 <C> 77.5 <R> <C> All-in-1+POS <C> 68.4 <C> 86.0 <C> 74.4 <C> 74.5 <C> 75.8 <CAP> Table 2: Results on the development data, weighted F1. Monolingual: per-language model; Multilingual: All-In-1 (with C+Embeds features trained on En+Es+Fr+Jp). ‡ indicates submitted systems.
<R> <C> Code <C> Gender <C> Quantity <C> % <R> <C> u <C> uter <C> 61745 <C> 69.83 <R> <C> n <C> neuter <C> 25148 <C> 28.44 <R> <C> p <C> plural <C> 333 <C> 0.38 <R> <C> v <C> vacklande <C> 764 <C> 0.86 <R> <C> [EMPTY] <C> blank <C> 437 <C> 0.49 <CAP> Table 1: Gender of nouns in Swedish based on SALDO
<R> <C> [EMPTY] <C> 1 w <C> 2 w <C> 3 w <C> 4 w <C> 5 w <R> <C> Neuter <C> 0.023 <C> 0.031 <C> 0.024 <C> 0.008 <C> 0.034 <R> <C> Uter <C> 0.99 <C> 0.987 <C> 0.984 <C> 0.995 <C> 0.966 <R> <C> Accuracy <C> 70.91 <C> 70.91 <C> 70.54 <C> 70.81 <C> 69.50 <CAP> Table 4: Neural network with asymmetric forward context type
<R> <C> [BOLD] METHOD CycleGAN <C> [BOLD] METHOD F→M <C> [BOLD] QUALITY 2.89 <C> [BOLD] SIMILARITY 2.53 <R> <C> (Nonparallel VC) <C> M→F <C> 2.50 <C> 1.76 <R> <C> [EMPTY] <C> [BOLD] AVG. <C> [BOLD] 2.69 <C> [BOLD] 2.15 <R> <C> GAN <C> F→M <C> 2.20 <C> 2.26 <R> <C> (Parallel VC) <C> M→F <C> 2.51 <C> 1.79 <R> <C> [EMPTY] <C> [BOLD] AVG. <C> [BOLD] 2.36 <C> [BOLD] 2.02 <R> <C> Merlin-based <C> F→M <C> 1.37 <C> 1.52 <R> <C> baseline <C> M→F <C> 1.52 <C> 1.38 <R> <C> (Parallel VC) <C> [BOLD] AVG. <C> [BOLD] 1.45 <C> [BOLD] 1.45 <CAP> Table 1: Perceptual evaluation results on MOS scale for speech quality and speaker similarity. “CycleGAN” denotes proposed nonparallel VC method and “GAN” and “Merlin-based baseline” denote the two baseline methods based on parallel VC. “F→M” and “M→F” indicate female-to-male and male-to-female conversion, respectively.
<R> <C> [EMPTY] <C> [BOLD] TED(de) <C> [BOLD] TED(ru) <C> [BOLD] patent(de) <C> [BOLD] patent(ru) <R> <C> [BOLD] GEN <C> 34.59 <C> 23.40 <C> 35.95 <C> 23.41 <R> <C> [BOLD] IN <C> 2.53 <C> 1.76 <C> 12.09 <C> 16.81 <R> <C> [BOLD] IN_CT <C> 36.16 <C> 25.04 <C> 54.70 <C> 35.61 <R> <C> [BOLD] std_rand <C> 35.32 <C> 24.33 <C> 50.00 <C> 34.70 <R> <C> [BOLD] std_ML <C> 36.02 <C> 24.73 <C> 50.40 <C> 30.96 <R> <C> [BOLD] CL_ML <C> 38.78 <C> 26.45 <C> 52.91 <C> 34.18 <R> <C> Δ [BOLD] _ML <C> 2.76 <C> 1.72 <C> 2.51 <C> 3.22 <R> <C> [BOLD] std_CDS <C> 35.83 <C> 24.60 <C> 52.58 <C> 34.54 <R> <C> [BOLD] CL_CDS <C> [BOLD] 38.88 <C> [BOLD] 26.49 <C> [BOLD] 55.51 <C> [BOLD] 36.59 <R> <C> Δ [BOLD] _CDS <C> 3.05 <C> 1.89 <C> 2.93 <C> 2.05 <CAP> Table 1: BLEU of unadapted & adapted models. Δ shows improvement of CL over std.
<R> <C> [BOLD] Method <C> [ITALIC] F1 [BOLD] % <R> <C> Xue(2008) <C> 71.90 <R> <C> Sun et al.(2009) <C> 74.12 <R> <C> Yand and Zong(2014) <C> 75.31 <R> <C> Wang et al.(2015) [BOLD] (Random Initialized) <C> 77.09 <R> <C> Sha et al.(2016) <C> [BOLD] 77.69 <R> <C> Comparison Feature Engineering Way <C> 77.75 <R> <C> Our SA-LSTM [BOLD] (Random Initialized) <C> [BOLD] 79.56 <R> <C> Our SA-LSTM [BOLD] (Pre-trained Embedding) <C> [BOLD] 79.64 <CAP> Table 2: Results comparison on CPB 1.0
<R> <C> [BOLD] Model <C> [BOLD] Microphone <C> [BOLD] WER  [BOLD] dev <C> [BOLD] WER  [BOLD] test <R> <C> TDNN-HMM <C> Kinect-RAW <C> [BOLD] 13.82 <C> [BOLD] 15.03 <R> <C> [EMPTY] <C> Samson <C> 14.19 <C> 15.18 <R> <C> [EMPTY] <C> Yamaha <C> 14.84 <C> 15.77 <R> <C> [EMPTY] <C> Kinect-Beam <C> 19.12 <C> 20.86 <R> <C> [EMPTY] <C> Realtek <C> 66.46 <C> 63.41 <CAP> Table 3: WER results of models trained on combined data (Tuda-De and SWC) for the different microphones in the Tuda-De dev and train sets. All WER results above are with the lexicon of 350,029 words and without RNNLM rescoring.
<R> <C> Target Speaker <C> Number of sentences 50 <C> Number of sentences 100 <C> Number of sentences 200 <R> <C> MF (baseline) <C> 3.15±0.12 <C> 3.26±0.11 <C> 3.56±0.09 <R> <C> MF (our) <C> 3.82±0.11 <C> 3.95±0.10 <C> 3.98±0.08 <R> <C> MM (baseline) <C> 2.85±0.13 <C> 3.04±0.09 <C> 3.11±0.10 <R> <C> MM (our) <C> 3.68±0.09 <C> 3.73±0.12 <C> 3.75±0.09 <R> <C> EF (our) <C> 3.72±0.08 <C> 3.82±0.11 <C> 3.88±0.09 <R> <C> EM (our) <C> 3.63±0.06 <C> 3.71±0.07 <C> 3.72±0.08 <CAP> Table 1: Speech naturalness Mean Opinion Score (MOS) with 95% confidence intervals.
<R> <C> Target Speaker <C> Number of sentences 50 <C> Number of sentences 100 <C> Number of sentences 200 <R> <C> MF (baseline) <C> 3.85±0.05 <C> 3.92±0.10 <C> 4.06±0.08 <R> <C> MF (our) <C> 3.97±0.08 <C> 4.12±0.11 <C> 4.15±0.09 <R> <C> MM (baseline) <C> 3.25±0.12 <C> 3.34±0.13 <C> 3.41±0.11 <R> <C> MM (our) <C> 3.74±0.11 <C> 3.86±0.06 <C> 3.89±0.10 <R> <C> EF (our) <C> 4.02±0.09 <C> 4.10±0.08 <C> 4.11±0.07 <R> <C> EM (our) <C> 3.75±0.12 <C> 3.81±0.09 <C> 3.87±0.07 <CAP> Table 2: Speech similarity Mean Opinion Score (MOS) with 95% confidence intervals.
<R> <C> Model <C> All Acc <C> Sem. Acc <C> Syn. Acc <R> <C> PPMI-SVD <C> 0.365 <C> [BOLD] 0.444 <C> 0.341 <R> <C> SGNS <C> [BOLD] 0.436 <C> 0.339 <C> 0.513 <R> <C> C2V-NO-ATT <C> 0.316 <C> 0.016 <C> 0.472 <R> <C> CHAR2VEC <C> 0.355 <C> 0.025 <C> [BOLD] 0.525 <CAP> Table 6: Results on the Google analogy task
<R> <C> System <C> IBP-certified(Lower bound) <R> <C> BOW <C> 68.8 <R> <C> → Fixed  [ITALIC] ϵ <C> 46.6 <R> <C> → Fixed  [ITALIC] κ <C> 69.8 <R> <C> → Fixed  [ITALIC] ϵ and  [ITALIC] κ <C> 66.3 <R> <C> CNN <C> 72.5 <R> <C> → Fixed  [ITALIC] ϵ <C> 67.6 <R> <C> → Fixed  [ITALIC] κ <C> 74.5 <R> <C> → Fixed  [ITALIC] ϵ and  [ITALIC] κ <C> 65.3 <R> <C> LSTM <C> 62.5 <R> <C> → Fixed  [ITALIC] ϵ <C> 43.7 <R> <C> → Fixed  [ITALIC] κ <C> 63.0 <R> <C> → Fixed  [ITALIC] ϵ and  [ITALIC] κ <C> 62.0 <CAP> Table 4: Effects of holding ϵ and κ fixed during training. All numbers are on 1000 randomly chosen IMDB development set examples.
<R> <C> [ITALIC] λ <C> 0.1 <C> 0.5 <C> 1 <C> 2 <C> 10 <R> <C> [BOLD] BLEU <C> 32.7 <C> 32.6 <C> [BOLD] 33.0 <C> 32.7 <C> 32.6 <CAP> Table 2: Translation performance over different regularization coefficient λ.
<R> <C> [BOLD] Models <C> [BOLD] FER <C> [BOLD] CER <C> {CE}/{FE} <R> <C> Transformer <C> 40.5 <C> 13.8 <C> 33.9 <R> <C> Context Gates <C> 40.5 <C> 13.7 <C> 33.7 <R> <C> Regularized Context Gates <C> [BOLD] 40.0 <C> [BOLD] 13.4 <C> [BOLD] 33.4 <CAP> Table 3: Forced decoding translation error rate (FER), context selection error rate (CER) and the proportion of context selection errors over forced decoding translation errors ({CE}/{FE}) of the original and context gated Transformer with or without regularization.
<R> <C> Regularizer <C> SCAB <C> WERB <C> SCAA <C> WERA <R> <C> gLassoO <C> 46.2 <C> 18.7 <C> 46.2 <C> 18.8 <R> <C> gLassoI <C> 46.1 <C> 18.2 <C> 46.1 <C> 18.2 <R> <C> L2O <C> 45.9 <C> 18.6 <C> 22.8 <C> 42.6 <R> <C> L2I <C> 45.9 <C> 18.6 <C> 8.2 <C> 100.0 <CAP> Table 1: Achieved classification performances (%) in DNN2048
<R> <C> [EMPTY] <C> Parameters <C> Masked LM accuracy <C> Next sentence prediction accuracy <R> <C> Base Model <C> 110.1M <C> 0.507 <C> 0.949 <R> <C> Latent Model <C> 110.3M <C> [BOLD] 0.514 <C> 0.950 <R> <C> Universal Model <C> 46.3M <C> 0.508 <C> [BOLD] 0.951 <R> <C> Latent Universal Model <C> 46.5M <C> 0.507 <C> 0.949 <CAP> Table 1: Results for all models at the end of training. These results were obtained on a validation set of 100,000 tweets.
<R> <C> [EMPTY] <C> Lexicon <C> Ontonotes <C> MSRA <C> Resume <C> Weibo <R> <C> BiLSTM <C> - <C> 71.81 <C> 91.87 <C> 94.41 <C> 56.75 <R> <C> TENER <C> - <C> 72.82 <C> 93.01 <C> 95.25 <C> 58.39 <R> <C> Lattice LSTM <C> YJ <C> 73.88 <C> 93.18 <C> 94.46 <C> 58.79 <R> <C> CNNR <C> YJ <C> 74.45 <C> 93.71 <C> 95.11 <C> 59.92 <R> <C> LGN <C> YJ <C> 74.85 <C> 93.63 <C> 95.41 <C> 60.15 <R> <C> PLT <C> YJ <C> 74.60 <C> 93.26 <C> 95.40 <C> 59.92 <R> <C> FLAT <C> YJ <C> [BOLD] 76.45 <C> [BOLD] 94.12 <C> [BOLD] 95.45 <C> [BOLD] 60.32 <R> <C> FLATmsm <C> YJ <C> 73.39 <C> 93.11 <C> 95.03 <C> 57.98 <R> <C> FLATmld <C> YJ <C> 75.35 <C> 93.83 <C> 95.28 <C> 59.63 <R> <C> CGN <C> LS <C> 74.79 <C> 93.47 <C> 94.12∗ <C> 63.09 <R> <C> FLAT <C> LS <C> [BOLD] 75.70 <C> [BOLD] 94.35 <C> [BOLD] 94.93 <C> [BOLD] 63.42 <CAP> Table 2: Four datasets results (F1). BiLSTM results are from DBLP:journals/corr/abs-1805-02023. PLT denotes the porous lattice Transformer Mengge2019PorousLT. ‘YJ’ denotes the lexicon released by DBLP:journals/corr/abs-1805-02023, and ‘LS’ denotes the lexicon released by li-etal-2018-analogical. The result of other models are from their original paper. Except that the superscript * means the result is not provided in the original paper, and we get the result by running the public source code. Subscripts ‘msm’ and ‘mld’ denote FLAT with the mask of self-matched words and long distance (>10), respectively.
<R> <C> [EMPTY] <C> Span F Ontonotes <C> Span F MSRA <C> Type Acc Ontonotes <C> Type Acc MSRA <R> <C> TENER <C> 72.41 <C> 93.17 <C> 96.33 <C> 99.29 <R> <C> FLAT <C> 76.23 <C> 94.58 <C> 97.03 <C> 99.52 <R> <C> FLAT [ITALIC] head <C> 75.64 <C> 94.33 <C> 96.85 <C> 99.45 <CAP> Table 3: Two metrics of models. FLAThead means Rij in (11) is replaced by d(hh)ij.
<R> <C> Senti <C> Model <C> B-1 <C> B-2 <C> B-3 <C> B-4 <C> ROUGE-L <C> METEOR <C> CIDEr <C> SPICE <R> <C> Pos <C> CNN+RNN <C> 48.7 <C> 28.1 <C> 17.0 <C> 10.7 <C> 36.6 <C> 15.3 <C> 55.6 <C> _ <R> <C> Pos <C> ANP-Replace <C> 48.2 <C> 27.8 <C> 16.4 <C> 10.1 <C> 36.6 <C> 16.5 <C> 55.2 <C> _ <R> <C> Pos <C> ANP-Scoring <C> 48.3 <C> 27.9 <C> 16.6 <C> 10.1 <C> 36.5 <C> 16.6 <C> 55.4 <C> _ <R> <C> Pos <C> RNN-Transfer <C> 49.3 <C> 29.5 <C> 17.9 <C> 10.9 <C> 37.2 <C> 17.0 <C> 54.1 <C> _ <R> <C> Pos <C> SentiCap <C> 49.1 <C> 29.1 <C> 17.5 <C> 10.8 <C> 36.5 <C> 16.8 <C> 54.4 <C> _ <R> <C> Pos <C> SF-LSTM + Adap <C> 50.5 <C> 30.8 <C> 19.1 <C> 12.1 <C> 38.0 <C> 16.6 <C> 60.0 <C> _ <R> <C> Pos <C> Ours: ATTEND-GAN− [ITALIC] SA <C> 56.1 <C> 32.5 <C> 19.4 <C> 11.8 <C> 44.8 <C> 17.1 <C> 63.0 <C> 15.9 <R> <C> Pos <C> Ours: ATTEND-GAN− [ITALIC] A <C> 55.8 <C> 33.4 <C> 20.1 <C> 12.4 <C> 44.2 <C> 18.6 <C> 61.1 <C> 15.7 <R> <C> Pos <C> Ours: ATTEND-GAN <C> 56.9 <C> 33.6 <C> 20.3 <C> 12.5 <C> 44.3 <C> 18.8 <C> 61.6 <C> 15.9 <R> <C> Neg <C> CNN+RNN <C> 47.6 <C> 27.5 <C> 16.3 <C> 9.8 <C> 36.1 <C> 15.0 <C> 54.6 <C> _ <R> <C> Neg <C> ANP-Replace <C> 48.1 <C> 28.8 <C> 17.7 <C> 10.9 <C> 36.3 <C> 16.0 <C> 56.5 <C> _ <R> <C> Neg <C> ANP-Scoring <C> 47.9 <C> 28.7 <C> 17.7 <C> 11.1 <C> 36.2 <C> 16.0 <C> 57.1 <C> _ <R> <C> Neg <C> RNN-Transfer <C> 47.8 <C> 29.0 <C> 18.7 <C> 12.1 <C> 36.7 <C> 16.2 <C> 55.9 <C> _ <R> <C> Neg <C> SentiCap <C> 50.0 <C> 31.2 <C> 20.3 <C> 13.1 <C> 37.9 <C> 16.8 <C> 61.8 <C> _ <R> <C> Neg <C> SF-LSTM + Adap <C> 50.3 <C> 31.0 <C> 20.1 <C> 13.3 <C> 38.0 <C> 16.2 <C> 59.7 <C> _ <R> <C> Neg <C> Ours: ATTEND-GAN− [ITALIC] SA <C> 55.4 <C> 32.4 <C> 19.4 <C> 11.9 <C> 44.4 <C> 17.0 <C> 63.4 <C> 15.6 <R> <C> Neg <C> Ours: ATTEND-GAN− [ITALIC] A <C> 54.7 <C> 32.6 <C> 20.4 <C> 12.9 <C> 43.2 <C> 17.7 <C> 60.4 <C> 16.1 <R> <C> Neg <C> Ours: ATTEND-GAN <C> 56.2 <C> 34.1 <C> 21.3 <C> 13.6 <C> 44.6 <C> 17.9 <C> 64.1 <C> 16.2 <R> <C> Avg <C> CNN+RNN <C> 48.15 <C> 27.80 <C> 16.65 <C> 10.25 <C> 36.35 <C> 15.15 <C> 55.10 <C> _ <R> <C> Avg <C> ANP-Replace <C> 48.15 <C> 28.30 <C> 17.05 <C> 10.50 <C> 36.45 <C> 16.25 <C> 55.85 <C> _ <R> <C> Avg <C> ANP-Scoring <C> 48.10 <C> 28.30 <C> 17.15 <C> 10.60 <C> 36.35 <C> 16.30 <C> 56.25 <C> _ <R> <C> Avg <C> RNN-Transfer <C> 48.55 <C> 29.25 <C> 18.30 <C> 11.50 <C> 36.95 <C> 16.60 <C> 55.00 <C> _ <R> <C> Avg <C> SentiCap <C> 49.55 <C> 30.15 <C> 18.90 <C> 11.95 <C> 37.20 <C> 16.80 <C> 58.10 <C> _ <R> <C> Avg <C> SF-LSTM + Adap <C> 50.40 <C> 30.90 <C> 19.60 <C> 12.70 <C> 38.00 <C> 16.40 <C> 59.85 <C> _ <R> <C> Avg <C> Ours: ATTEND-GAN− [ITALIC] SA <C> 55.75 <C> 32.45 <C> 19.40 <C> 11.85 <C> [BOLD] 44.60 <C> 17.05 <C> [BOLD] 63.20 <C> 15.75 <R> <C> Avg <C> Ours: ATTEND-GAN− [ITALIC] A <C> 55.25 <C> 33.00 <C> 20.25 <C> 12.65 <C> 43.70 <C> 18.15 <C> 60.75 <C> 15.90 <R> <C> Avg <C> Ours: ATTEND-GAN <C> [BOLD] 56.55 <C> [BOLD] 33.85 <C> [BOLD] 20.80 <C> [BOLD] 13.05 <C> 44.45 <C> [BOLD] 18.35 <C> 62.85 <C> [BOLD] 16.05 <CAP> Table 1: The compared performances on different sections of SentiCap and their average. BLEU-N metric is shown by B-N. (The best results are bold.)
<R> <C> Method <C> Accuracy (TData) <C> Accuracy (MData) <R> <C> BiLSTM (A) <C> 0.7375 <C> 0.6878 <R> <C> BiLSTM (Q+A) <C> 0.7196 <C> 0.6905 <R> <C> RAM <C> 0.7503 <C> 0.7121 <R> <C> ATAE <C> 0.7458 <C> 0.6865 <R> <C> Transformer (A) <C> 0.7029 <C> 0.6547 <R> <C> Transformer (Q+A) <C> 0.5830 <C> 0.6167 <R> <C> Semi-IAN <C> 0.7485 <C> 0.6986 <R> <C> AntNet <C> [BOLD] 0.7921 <C> [BOLD] 0.8213 <CAP> TABLE II: Accuracies on TData and MData.
<R> <C> Method <C> Accuracy (TData) <C> Accuracy (MData) <R> <C> AntNet w/o SR <C> 0.7853 <C> 0.7931 <R> <C> AntNet w/o RR <C> 0.7536 <C> 0.7432 <R> <C> AntNet w/o MF <C> 0.7858 <C> 0.7896 <R> <C> AntNet <C> [BOLD] 0.7921 <C> [BOLD] 0.8213 <CAP> TABLE III: Results of AntNet and its variations (without certain key modules) on TData and MData.
<R> <C> [EMPTY] <C> [BOLD] English P <C> [BOLD] English R <C> [BOLD] English F1 <C> [BOLD] Turkish P <C> [BOLD] Turkish R <C> [BOLD] Turkish F1 <C> [BOLD] Finnish P <C> [BOLD] Finnish R <C> [BOLD] Finnish F1 <R> <C> Morfessor <C> 74.46 <C> 56.66 <C> 64.35 <C> 40.81 <C> 25.00 <C> 31.01 <C> [BOLD] 43.09 <C> [BOLD] 28.16 <C> [BOLD] 34.06 <R> <C> MORSE <C> [BOLD] 81.98 <C> [BOLD] 61.57 <C> [BOLD] 70.32 <C> [BOLD] 49.90 <C> [BOLD] 30.78 <C> [BOLD] 38.07 <C> 36.26 <C> 9.44 <C> 14.98 <CAP> Table 3: Performance of MORSE on the MC dataset across three languages: English, Turkish, Finnish.
<R> <C> [EMPTY] <C> P <C> R <C> F1 <R> <C> Morfessor <C> 65.95 <C> 51.13 <C> 57.60 <R> <C> MORSE <C> 75.35 <C> [BOLD] 83.60 <C> 79.26 <R> <C> MORSE-CV <C> [BOLD] 84.6 <C> 78.36 <C> [BOLD] 81.29 <CAP> Table 5: Performance of MORSE against Morfessor on the non-canonical version of SD17
<R> <C> Distance <C> Φ0 <C> Φ1 <C> F1 Score Φ2 <C> Φ3 <C> Φ4 <R> <C> HOFTT <C> [BOLD] 73.22 <C> [BOLD] 76.27 <C> [BOLD] 76.62 <C> [BOLD] 78.85 <C> [BOLD] 83.37 <R> <C> HOTT <C> 73.19 <C> 76.03 <C> 76.24 <C> 78.64 <C> 83.25 <R> <C> RWMD <C> 71.60 <C> 74.90 <C> 75.20 <C> 77.16 <C> 82.92 <R> <C> WMD-T20 <C> 67.22 <C> 63.38 <C> 65.20 <C> 70.38 <C> 81.84 <R> <C> None <C> — <C> 61.13 <C> 64.27 <C> 67.72 <C> 81.68 <CAP> Table 2: Link prediction: using distance (rows) for node-pair representations (cols).
<R> <C> Dataset <C> Mantel HOTT <C> Mantel RWMD <C> [ITALIC] l2 HOTT <C> [ITALIC] l2 RWMD <R> <C> ohsumed <C> 0.57 <C> 0.87 <C> 55 <C> 104 <R> <C> 20news <C> 0.62 <C> 0.90 <C> 90 <C> 99 <R> <C> amazon <C> 0.49 <C> 0.84 <C> 70 <C> 65 <R> <C> reuters <C> 0.72 <C> 0.91 <C> 130 <C> 151 <R> <C> bbcsport <C> 0.76 <C> 0.92 <C> 28 <C> 90 <R> <C> classic <C> 0.43 <C> 0.89 <C> 157 <C> 69 <R> <C> Avg <C> 0.60 <C> 0.89 <C> 88 <C> 96 <CAP> Table 3: Relation between the metrics. For each dataset, we compute distance matrices using exact WMD, RWMD, and HOTT from a few randomly-selected documents. We report results of a Mantel correlation test between WMD/HOTT and WMD/RWMD and the difference between cost matrices under a Frobenius norm.
<R> <C> Models <C> Italian → English Val <C> Italian → English Test <C> Finnish → English Val <C> Finnish → English Test <R> <C> Transformer <C> 0.192 <C> 0.179 <C> [BOLD] 0.127 <C> 0.077 <R> <C> Our model <C> [BOLD] 0.230 <C> [BOLD] 0.211 <C> 0.126 <C> [BOLD] 0.097 <CAP> Table 2: Results for ranking with query translation models, in terms of MAP.
<R> <C> Model <C> Relevant [0-1] <C> Grammatical [0-1] <C> New Info [0-1] <C> Useful [0-1] <C> Specific [0-4] <R> <C> Reference <C> 0.96 <C> 0.99 <C> 0.93 <C> 0.72 <C> 3.38 <R> <C> Lucene <C> [BOLD] 0.90 <C> [BOLD] 0.99 <C> [BOLD] 0.95 <C> 0.68 <C> 2.87 <R> <C> MLE <C> [BOLD] 0.92 <C> [BOLD] 0.96 <C> 0.85 <C> 0.91 <C> 3.05 <R> <C> Max-Utility <C> [BOLD] 0.93 <C> [BOLD] 0.96 <C> 0.88 <C> 0.91 <C> 3.29 <R> <C> GAN-Utility <C> [BOLD] 0.94 <C> [BOLD] 0.96 <C> 0.87 <C> [BOLD] 0.96 <C> [BOLD] 3.52 <CAP> Table 3: Results of human judgments on model generated questions on 300 sample Home & Kitchen product descriptions. Numeric range corresponds to the options described in § 5.1. The difference between the bold and the non-bold numbers is statistically significant with p <0.05. Reference is excluded in the significance calculation.
<R> <C> Accounts <C> Followers <C> Posts <C> Quality <R> <C> [EMPTY] <C> (million) <C> [EMPTY] <C> [EMPTY] <R> <C> People’s Daily <C> 8,268 <C> 82,788 <C> 8647.1 <R> <C> CCTV News <C> 7,789 <C> 84,583 <C> 6048.9 <R> <C> Xinhua Net <C> 5,115 <C> 67,765 <C> 471.4 <R> <C> Xinhua Viewpoint <C> 5,180 <C> 72,152 <C> 506.0 <CAP> Table 2: ‘Quality’ here refers to average sum of ‘like’, ‘comment’ and ‘repost’ for each piece of news.
<R> <C> [BOLD] Lang. <C> [BOLD] Character Error Rate Number of Layers Copied from English <C> [BOLD] Character Error Rate Number of Layers Copied from English <C> [BOLD] Character Error Rate Number of Layers Copied from English <C> [BOLD] Character Error Rate Number of Layers Copied from English <C> [BOLD] Character Error Rate Number of Layers Copied from English <C> [BOLD] Character Error Rate Number of Layers Copied from English <R> <C> [BOLD] Lang. <C> None <C> 1 <C> 2 <C> 3 <C> 4 <C> 5 <R> <C> sl <C> 23.35 <C> 21.65 <C> 26.44 <C> 19.09 <C> [BOLD] 15.35 <C> 17.96 <R> <C> ga <C> 31.83 <C> 31.01 <C> 32.2 <C> 27.5 <C> 25.42 <C> [BOLD] 24.98 <R> <C> cv <C> 48.1 <C> 47.1 <C> 44.58 <C> 42.75 <C> [BOLD] 27.21 <C> 31.94 <R> <C> br <C> 21.47 <C> 19.16 <C> 20.01 <C> 18.06 <C> [BOLD] 15.99 <C> 18.42 <R> <C> tr <C> 34.66 <C> 34.12 <C> 34.83 <C> 31.79 <C> [BOLD] 27.55 <C> 29.74 <R> <C> it <C> 40.91 <C> 42.65 <C> 42.82 <C> 36.89 <C> [BOLD] 33.63 <C> 35.10 <R> <C> cy <C> 34.15 <C> 31.91 <C> 33.63 <C> 30.13 <C> [BOLD] 28.75 <C> 30.38 <R> <C> tt <C> 32.61 <C> 31.43 <C> 30.80 <C> 27.79 <C> [BOLD] 26.42 <C> 28.63 <R> <C> ca <C> 38.01 <C> 35.21 <C> 39.02 <C> 35.26 <C> [BOLD] 33.83 <C> 36.41 <R> <C> fr <C> 43.33 <C> 43.26 <C> 43.51 <C> 43.24 <C> 43.20 <C> [BOLD] 43.19 <R> <C> kab <C> 25.76 <C> 25.5 <C> 26.83 <C> 25.25 <C> [BOLD] 24.92 <C> 25.28 <R> <C> de <C> 43.76 <C> 43.69 <C> 43.62 <C> [BOLD] 43.60 <C> 43.76 <C> 43.69 <CAP> Table 3: Fine-Tuned Transfer Learning Character Error Rate for each language, in addition to a baseline trained from scratch on the target language data. Bolded values display best model per language. Shading indicates relative performance per language, with darker indicating better models.
<R> <C> [BOLD] Attr. <C> [BOLD] Model <C> [BOLD] Grocery Products  [BOLD] Vocab <C> [BOLD] Grocery Products  [BOLD] Cov <C> [BOLD] Grocery Products  [BOLD] miF1 <C> [BOLD] Grocery Products  [BOLD] maF1 <C> [BOLD] Baby Products  [BOLD] Vocab <C> [BOLD] Baby Products  [BOLD] Cov <C> [BOLD] Baby Products  [BOLD] miF1 <C> [BOLD] Baby Products  [BOLD] maF1 <C> [BOLD] Beauty Products  [BOLD] Vocab <C> [BOLD] Beauty Products  [BOLD] Cov <C> [BOLD] Beauty Products  [BOLD] miF1 <C> [BOLD] Beauty Products  [BOLD] maF1 <C> [BOLD] Health Products  [BOLD] Vocab <C> [BOLD] Health Products  [BOLD] Cov <C> [BOLD] Health Products  [BOLD] miF1 <C> [BOLD] Health Products  [BOLD] maF1 <R> <C> [ITALIC] Flavor <C> OpenTag <C> 4364 <C> 79.6 <C> 60.3 <C> 59.0 <C> 264 <C> 53.1 <C> 54.4 <C> 45.0 <C> 832 <C> 45.8 <C> 41.1 <C> 32.0 <C> 1296 <C> 58.2 <C> 53.9 <C> 47.0 <R> <C> [ITALIC] Flavor <C> TXtract <C> [BOLD] 8607 <C> [BOLD] 89.1 <C> [BOLD] 64.9 <C> [BOLD] 62.8 <C> [BOLD] 414 <C> [BOLD] 72.8 <C> [BOLD] 63.0 <C> [BOLD] 56.1 <C> [BOLD] 1684 <C> [BOLD] 61.3 <C> [BOLD] 46.5 <C> [BOLD] 35.6 <C> [BOLD] 2388 <C> [BOLD] 71.5 <C> [BOLD] 67.3 <C> [BOLD] 57.5 <R> <C> [ITALIC] Scent <C> OpenTag <C> 446 <C> 75.5 <C> 56.8 <C> 48.4 <C> [BOLD] 593 <C> 69.7 <C> 35.7 <C> 20.3 <C> 7007 <C> 78.5 <C> 76.9 <C> 67.9 <C> 2479 <C> 68.1 <C> 63.0 <C> 47.5 <R> <C> [ITALIC] Scent <C> TXtract <C> [BOLD] 565 <C> [BOLD] 87.4 <C> [BOLD] 61.2 <C> [BOLD] 51.4 <C> 589 <C> [BOLD] 72.1 <C> [BOLD] 38.1 <C> [BOLD] 22.0 <C> [BOLD] 9048 <C> [BOLD] 85.6 <C> [BOLD] 79.5 <C> [BOLD] 68.4 <C> [BOLD] 3322 <C> [BOLD] 79.9 <C> [BOLD] 69.1 <C> [BOLD] 48.2 <R> <C> [ITALIC] Brand <C> OpenTag <C> 5150 <C> 68.8 <C> 62.9 <C> 52.7 <C> 11166 <C> 72.2 <C> 66.0 <C> 54.0 <C> 15394 <C> 77.2 <C> 68.8 <C> 54.7 <C> 17233 <C> 71.2 <C> 57.8 <C> 45.9 <R> <C> [ITALIC] Brand <C> TXtract <C> [BOLD] 6944 <C> [BOLD] 78.9 <C> [BOLD] 67.4 <C> [BOLD] 55.1 <C> [BOLD] 14965 <C> [BOLD] 81.0 <C> [BOLD] 72.9 <C> [BOLD] 56.2 <C> [BOLD] 19821 <C> [BOLD] 85.1 <C> [BOLD] 72.7 <C> [BOLD] 57.2 <C> [BOLD] 22974 <C> [BOLD] 82.9 <C> [BOLD] 60.5 <C> [BOLD] 52.4 <R> <C> [ITALIC] Ingred. <C> OpenTag <C> 3402 <C> 82.5 <C> 40.5 <C> 30.1 <C> 490 <C> 50.7 <C> 27.7 <C> 22.4 <C> 2767 <C> 65.1 <C> [BOLD] 33.6 <C> [BOLD] 26.8 <C> 3251 <C> 66.7 <C> 34.6 <C> 29.9 <R> <C> [ITALIC] Ingred. <C> TXtract <C> [BOLD] 6155 <C> [BOLD] 87.3 <C> [BOLD] 43.1 <C> [BOLD] 36.5 <C> [BOLD] 835 <C> [BOLD] 59.7 <C> [BOLD] 30.5 <C> [BOLD] 24.3 <C> [BOLD] 5539 <C> [BOLD] 70.6 <C> 32.9 <C> 26.6 <C> [BOLD] 6451 <C> [BOLD] 74.2 <C> [BOLD] 36.5 <C> [BOLD] 31.2 <CAP> Table 6: Extraction results for flavor, scent, brand, and ingredients for each of our 4 domains (sub-trees).
<R> <C> [BOLD] Model <C> [BOLD] Multi-Cardinality SST-1 <C> [BOLD] Multi-Cardinality SST-2 <C> [BOLD] Multi-Cardinality IMDB <C> [BOLD] Multi-Domain Books <C> [BOLD] Multi-Domain DVDs <C> [BOLD] Multi-Domain Electronics <C> [BOLD] Multi-Domain Kitchen <C> [BOLD] Multi-Objective IMDB <C> [BOLD] Multi-Objective RN <C> [BOLD] Multi-Objective QC <C> AvgΔ <R> <C> Single Task <C> 45.9 <C> 85.8 <C> 88.5 <C> 78.0 <C> 79.5 <C> 81.2 <C> 81.8 <C> 88.5 <C> 83.6 <C> 92.5 <C> - <R> <C> Random <C> 20.0 <C> 50.0 <C> 50.0 <C> 50.0 <C> 50.0 <C> 50.0 <C> 50.0 <C> 50.0 <C> 2.2 <C> 16.7 <C> -41.6 <R> <C> Model-I <C> 31.4 <C> 71.6 <C> 67.5 <C> 68.8 <C> 67.0 <C> 69.1 <C> 69.3 <C> 67.2 <C> 70.4 <C> 52.3 <C> -17.1 <R> <C> Model-II <C> [BOLD] 49.8 <C> [BOLD] 88.4 <C> [BOLD] 91.3 <C> [BOLD] 84.5 <C> [BOLD] 85.2 <C> [BOLD] 87.3 <C> [BOLD] 86.9 <C> [BOLD] 90.9 <C> [BOLD] 85.5 <C> [BOLD] 93.2 <C> +3.7 <CAP> Table 3: Results of Model-I and Model-II on different scenarios
<R> <C> [BOLD] Structural Probe Results: Undirected Unlabeled Attachment Score (UUAS) <C> [BOLD] Structural Probe Results: Undirected Unlabeled Attachment Score (UUAS)  [BOLD] Arabic <C> [BOLD] Structural Probe Results: Undirected Unlabeled Attachment Score (UUAS)  [BOLD] Czech <C> [BOLD] Structural Probe Results: Undirected Unlabeled Attachment Score (UUAS)  [BOLD] German <C> [BOLD] Structural Probe Results: Undirected Unlabeled Attachment Score (UUAS)  [BOLD] English <C> [BOLD] Structural Probe Results: Undirected Unlabeled Attachment Score (UUAS)  [BOLD] Spanish <C> [BOLD] Structural Probe Results: Undirected Unlabeled Attachment Score (UUAS)  [BOLD] Farsi <C> [BOLD] Structural Probe Results: Undirected Unlabeled Attachment Score (UUAS)  [BOLD] Finnish <C> [BOLD] Structural Probe Results: Undirected Unlabeled Attachment Score (UUAS)  [BOLD] French <C> [BOLD] Structural Probe Results: Undirected Unlabeled Attachment Score (UUAS)  [BOLD] Indonesian <C> [BOLD] Structural Probe Results: Undirected Unlabeled Attachment Score (UUAS)  [BOLD] Latvian <C> [BOLD] Structural Probe Results: Undirected Unlabeled Attachment Score (UUAS)  [BOLD] Chinese <C> [BOLD] Structural Probe Results: Undirected Unlabeled Attachment Score (UUAS)  [BOLD] Average <R> <C> [BOLD] Linear <C> 57.1 <C> 45.4 <C> 42.8 <C> 41.5 <C> 44.6 <C> 52.6 <C> 50.1 <C> 46.4 <C> 55.2 <C> 47.0 <C> 44.2 <C> 47.9 <R> <C> [BOLD] mBERTRand <C> 49.8 <C> 57.3 <C> 55.2 <C> 57.4 <C> 55.3 <C> 43.2 <C> 54.9 <C> 61.2 <C> 53.2 <C> 53.0 <C> 41.1 <C> 52.9 <R> <C> [BOLD] In-Lang <C> 72.8 <C> 83.7 <C> 83.4 <C> 80.1 <C> 79.4 <C> 70.7 <C> 76.3 <C> 81.3 <C> 74.4 <C> 77.1 <C> 66.3 <C> 76.8 <R> <C> \Delta_{\text{{{Baseline}}}} <C> 15.7 <C> 26.4 <C> 28.1 <C> 22.6 <C> 24.1 <C> 18.0 <C> 21.4 <C> 20.1 <C> 19.1 <C> 24.1 <C> 22.1 <C> 22.0 <R> <C> [BOLD] SingleTran <C> 68.6 <C> 74.7 <C> 70.8 <C> 65.4 <C> 75.8 <C> 61.3 <C> 69.8 <C> 74.3 <C> 69.0 <C> 73.2 <C> 51.1 <C> 68.5 <R> <C> \Delta_{\text{{{Baseline}}}} <C> 11.5 <C> 17.4 <C> 15.6 <C> 8.0 <C> 20.4 <C> 8.7 <C> 14.9 <C> 13.1 <C> 13.8 <C> 20.2 <C> 6.9 <C> 13.7 <R> <C> [BOLD] Holdout <C> 70.4 <C> 77.8 <C> 75.1 <C> 68.9 <C> 75.5 <C> 63.3 <C> 70.7 <C> 76.4 <C> 70.8 <C> 73.7 <C> 51.3 <C> 70.4 <R> <C> \Delta_{\text{{{Baseline}}}} <C> 13.3 <C> 20.5 <C> 19.8 <C> 11.5 <C> 20.1 <C> 10.7 <C> 15.8 <C> 15.2 <C> 15.6 <C> 20.7 <C> 7.1 <C> 15.5 <R> <C> [BOLD] AllLangs <C> 72.0 <C> 82.5 <C> 79.6 <C> 75.9 <C> 77.6 <C> 68.2 <C> 73.0 <C> 80.3 <C> 73.1 <C> 75.1 <C> 57.8 <C> 74.1 <R> <C> \Delta_{\text{{{Baseline}}}} <C> 14.9 <C> 25.2 <C> 24.4 <C> 18.5 <C> 22.2 <C> 15.6 <C> 18.1 <C> 19.1 <C> 17.9 <C> 22.1 <C> 13.7 <C> 19.2 <R> <C> [BOLD] Structural Probe Results: Distance Spearman Correlation (DSpr.) <C> [BOLD] Structural Probe Results: Distance Spearman Correlation (DSpr.) <C> [BOLD] Structural Probe Results: Distance Spearman Correlation (DSpr.) <C> [BOLD] Structural Probe Results: Distance Spearman Correlation (DSpr.) <C> [BOLD] Structural Probe Results: Distance Spearman Correlation (DSpr.) <C> [BOLD] Structural Probe Results: Distance Spearman Correlation (DSpr.) <C> [BOLD] Structural Probe Results: Distance Spearman Correlation (DSpr.) <C> [BOLD] Structural Probe Results: Distance Spearman Correlation (DSpr.) <C> [BOLD] Structural Probe Results: Distance Spearman Correlation (DSpr.) <C> [BOLD] Structural Probe Results: Distance Spearman Correlation (DSpr.) <C> [BOLD] Structural Probe Results: Distance Spearman Correlation (DSpr.) <C> [BOLD] Structural Probe Results: Distance Spearman Correlation (DSpr.) <C> [BOLD] Structural Probe Results: Distance Spearman Correlation (DSpr.) <R> <C> [BOLD] Linear <C> .573 <C> .570 <C> .533 <C> .567 <C> .589 <C> .489 <C> .564 <C> .598 <C> .578 <C> .543 <C> .493 <C> .554 <R> <C> [BOLD] mBERTRand <C> .657 <C> .658 <C> .672 <C> .659 <C> .693 <C> .611 <C> .621 <C> .710 <C> .656 <C> .608 <C> .590 <C> .649 <R> <C> [BOLD] In-Lang <C> .822 <C> .845 <C> .846 <C> .817 <C> .859 <C> .813 <C> .812 <C> .864 <C> .807 <C> .798 <C> .777 <C> .824 <R> <C> \Delta_{\text{{{Baseline}}}} <C> .165 <C> .187 <C> .174 <C> .158 <C> .166 <C> .202 <C> .191 <C> .154 <C> .151 <C> .190 <C> .187 <C> .175 <R> <C> [BOLD] SingleTran <C> .774 <C> .801 <C> .807 <C> .773 <C> .838 <C> .732 <C> .787 <C> .836 <C> .772 <C> .771 <C> .655 <C> .777 <R> <C> \Delta_{\text{{{Baseline}}}} <C> .117 <C> .143 <C> .135 <C> .114 <C> .145 <C> .121 <C> .166 <C> .126 <C> .117 <C> .163 <C> .064 <C> .128 <R> <C> [BOLD] Holdout <C> .779 <C> .821 <C> .824 <C> .788 <C> .838 <C> .744 <C> .792 <C> .840 <C> .776 <C> .775 <C> .664 <C> .786 <R> <C> \Delta_{\text{{{Baseline}}}} <C> .122 <C> .163 <C> .152 <C> .129 <C> .146 <C> .133 <C> .171 <C> .130 <C> .121 <C> .166 <C> .074 <C> .137 <R> <C> [BOLD] AllLangs <C> .795 <C> .839 <C> .836 <C> .806 <C> .848 <C> .777 <C> .802 <C> .853 <C> .789 <C> .783 <C> .717 <C> .804 <R> <C> \Delta_{\text{{{Baseline}}}} <C> .138 <C> .181 <C> .165 <C> .147 <C> .155 <C> .165 <C> .181 <C> .143 <C> .134 <C> .174 <C> .127 <C> .156 <CAP> Table 1: Performance (in UUAS and DSpr.) of the structural probe trained on the following cross-lingual sources of data: the evaluation language (In-Lang); the single other best language (SingleTran); all other languages (Holdout); and all languages, including the evaluation language (AllLangs). Note that all improvements over baseline (\Delta_{\text{{{Baseline}}}}) are reported against the stronger of our two baselines per-language.
<R> <C> [ITALIC] N= <C> 0 <C> 2.5 <C> 5 <C> 10 <C> 20 <C> 50 <R> <C> [BOLD] base <C> 0 <C> 2.1 <C> 1.8 <C> 2.1 <C> 10.8 <C> 22.7 <R> <C> [BOLD] +asr <C> 0.5 <C> 5.7 <C> 9.1 <C> 14.5 <C> 20.2 <C> 28.2 <CAP> Table 2: BLEU scores for Spanish-English ST on the Fisher test set, using N hours of training data. base: no transfer learning. +asr: using model parameters from English ASR (en-300h).
<R> <C> [EMPTY] <C> [BOLD] baseline <C> [BOLD] +fr-20h <C> [BOLD] +en-20h <R> <C> sp-en-20h <C> 10.8 <C> 12.5 <C> 13.2 <CAP> Table 4: Fisher dev set BLEU scores for sp-en-20h. baseline: model without transfer learning. Last two columns: Using encoder parameters from French ASR (+fr-20h), and English ASR (+en-20h).
<R> <C> [BOLD] Train size  [BOLD] Domain <C> [BOLD] 2000  [BOLD] CRF <C> [BOLD] 2000  [BOLD] LSTM <C> [BOLD] 2000  [BOLD] CRF-BoE <C> [BOLD] 2000  [BOLD] LSTM-BoE <C> [BOLD] 2000  [BOLD] CT <C> [BOLD] 2000  [BOLD] ZAT <R> <C> Fashion <C> 76.04 <C> 75.08 <C> 77.19 <C> 77.31 <C> 78.11 <C> [BOLD] 81.58 <R> <C> Flight Status <C> 86.46 <C> 89.30 <C> 87.91 <C> [BOLD] 90.12 <C> 88.56 <C> 90.11 <R> <C> Deals <C> 80.01 <C> 79.93 <C> 79.99 <C> 82.36 <C> 84.16 <C> [BOLD] 84.94 <R> <C> Purchase <C> 57.19 <C> 71.95 <C> 61.41 <C> 72.30 <C> 72.97 <C> [BOLD] 75.33 <R> <C> Real Estate <C> 91.85 <C> 89.47 <C> 91.75 <C> 91.01 <C> 91.58 <C> [BOLD] 93.39 <R> <C> Shopping <C> 71.96 <C> 73.01 <C> 71.45 <C> 72.83 <C> 77.06 <C> [BOLD] 78.14 <R> <C> Social Network <C> 81.85 <C> 82.15 <C> 81.77 <C> 82.24 <C> 79.70 <C> [BOLD] 82.85 <R> <C> Sports <C> 71.84 <C> 72.50 <C> 71.87 <C> 75.49 <C> 78.67 <C> [BOLD] 80.83 <R> <C> Transportation <C> 71.19 <C> 67.59 <C> [BOLD] 84.94 <C> 79.08 <C> 75.54 <C> 80.78 <R> <C> Travel <C> 62.71 <C> 61.50 <C> 67.13 <C> 68.20 <C> 72.14 <C> [BOLD] 75.57 <R> <C> Average Improvement <C> [EMPTY] <C> +1.14 <C> +2.43 <C> +3.98∗ <C> +4.74∗ <C> +7.24∗ <CAP> (a)
<R> <C> [BOLD] Train size  [BOLD] Domain <C> [BOLD] 1000  [BOLD] CRF <C> [BOLD] 1000  [BOLD] LSTM <C> [BOLD] 1000  [BOLD] CRF-BoE <C> [BOLD] 1000  [BOLD] LSTM-BoE <C> [BOLD] 1000  [BOLD] CT <C> [BOLD] 1000  [BOLD] ZAT <R> <C> Fashion <C> 68.73 <C> 74.02 <C> 70.98 <C> 71.59 <C> 78.90 <C> [BOLD] 81.57 <R> <C> Flight Status <C> 82.36 <C> 85.14 <C> 84.53 <C> 88.98 <C> 86.60 <C> [BOLD] 89.88 <R> <C> Deals <C> 74.60 <C> 70.98 <C> 74.13 <C> 74.57 <C> 80.69 <C> [BOLD] 82.76 <R> <C> Purchase <C> 52.11 <C> 62.05 <C> 53.50 <C> 63.91 <C> [BOLD] 72.97 <C> 71.71 <R> <C> Real Estate <C> 88.11 <C> 88.64 <C> 88.68 <C> 90.29 <C> 89.04 <C> [BOLD] 91.56 <R> <C> Shopping <C> 63.72 <C> 67.88 <C> 63.47 <C> 68.65 <C> 73.81 <C> [BOLD] 75.52 <R> <C> Social Network <C> 79.05 <C> 79.17 <C> 76.68 <C> 78.00 <C> 80.03 <C> [BOLD] 84.40 <R> <C> Sports <C> 63.13 <C> 63.75 <C> 63.71 <C> 67.25 <C> 73.81 <C> [BOLD] 77.40 <R> <C> Transportation <C> 66.45 <C> 60.12 <C> [BOLD] 82.84 <C> 79.99 <C> 72.58 <C> 78.26 <R> <C> Travel <C> 54.03 <C> 58.14 <C> 62.12 <C> 65.68 <C> 69.52 <C> [BOLD] 69.53 <R> <C> Average Improvement <C> [EMPTY] <C> +1.76 <C> +2.84 <C> +5.66∗ <C> +8.57∗ <C> +11.03∗ <CAP> (b)
<R> <C> [BOLD] Train size  [BOLD] Domain <C> [BOLD] 500  [BOLD] CRF <C> [BOLD] 500  [BOLD] LSTM <C> [BOLD] 500  [BOLD] CRF-BoE <C> [BOLD] 500  [BOLD] LSTM-BoE <C> [BOLD] 500  [BOLD] CT <C> [BOLD] 500  [BOLD] ZAT <R> <C> Fashion <C> 62.64 <C> 67.55 <C> 66.42 <C> 71.59 <C> 73.59 <C> [BOLD] 74.33 <R> <C> Flight Status <C> 75.97 <C> 83.13 <C> 80.11 <C> 84.62 <C> 81.70 <C> [BOLD] 86.76 <R> <C> Deals <C> 64.04 <C> 67.55 <C> 67.22 <C> 74.24 <C> 77.34 <C> [BOLD] 79.93 <R> <C> Purchase <C> 45.19 <C> 57.99 <C> 47.76 <C> 60.59 <C> 69.33 <C> [BOLD] 69.45 <R> <C> Real Estate <C> 84.15 <C> 82.05 <C> 84.60 <C> 85.49 <C> 86.21 <C> [BOLD] 89.14 <R> <C> Shopping <C> 51.43 <C> 59.22 <C> 49.88 <C> 60.50 <C> 66.69 <C> [BOLD] 69.75 <R> <C> Social Network <C> 70.78 <C> 76.49 <C> 66.21 <C> 78.21 <C> 79.23 <C> [BOLD] 80.39 <R> <C> Sports <C> 53.29 <C> 55.71 <C> 53.85 <C> 63.61 <C> 68.20 <C> [BOLD] 68.71 <R> <C> Transportation <C> 60.23 <C> 55.18 <C> [BOLD] 81.07 <C> 78.39 <C> 67.36 <C> 75.56 <R> <C> Travel <C> 45.90 <C> 54.77 <C> 57.93 <C> 62.66 <C> 64.86 <C> [BOLD] 66.34 <R> <C> Average Improvement <C> [EMPTY] <C> +4.60∗ <C> +4.14 <C> +10.63∗ <C> +12.09∗ <C> +14.67∗ <CAP> (c)
<R> <C> [BOLD] Domain <C> [BOLD] CT <C> [BOLD] ZAT <R> <C> Fashion <C> [BOLD] 31.51 <C> 30.66 <R> <C> Flight Status <C> 23.04 <C> [BOLD] 25.10 <R> <C> Deals <C> 37.76 <C> [BOLD] 38.86 <R> <C> Purchase <C> 56.34 <C> [BOLD] 61.23 <R> <C> Real Estate <C> 36.47 <C> [BOLD] 48.63 <R> <C> Shopping <C> 43.64 <C> [BOLD] 50.46 <R> <C> Social Network <C> 2.20 <C> [BOLD] 7.22 <R> <C> Sports <C> 4.49 <C> [BOLD] 4.73 <R> <C> Transportation <C> 39.45 <C> [BOLD] 49.29 <R> <C> Travel <C> 34.97 <C> [BOLD] 44.34 <R> <C> Average Improvement <C> [EMPTY] <C> +5.07 <CAP> Table 3: F1-scores with zero training instances for target domain.
<R> <C> [BOLD] Model <C> [BOLD] 0 <C> [BOLD] 500 <C> [BOLD] 1000 <C> [BOLD] 2000 <R> <C> ZAT <C> [BOLD] 36.05 <C> 76.04 <C> 80.26 <C> 82.35 <R> <C> - CRF <C> 35.06 <C> 74.39 <C> 78.41 <C> 81.58 <R> <C> - CHAR <C> 35.49 <C> 73.71 <C> 77.86 <C> 81.11 <R> <C> + WEFT <C> 33.71 <C> [BOLD] 76.52 <C> [BOLD] 80.61 <C> [BOLD] 83.09 <CAP> Table 4: Model variants.
<R> <C> [EMPTY] <C> [ITALIC]  [BOLD] te+en <C> [BOLD] + [ITALIC] tr <C> [BOLD] % Change <R> <C> [BOLD] Recall@1 <C> 17.0 <C> 17.6 <C> [BOLD] +3.5% <R> <C> [BOLD] Recall@10 <C> 23.9 <C> 25.0 <C> [BOLD] +4.6% <R> <C> [BOLD] Recall@20 <C> 26.3 <C> 27.7 <C> [BOLD] +5.3% <CAP> Figure 8: Overlap among the target, auxiliary and pivot language vocabularies. Target ⇌ auxiliary overlap numbers (dotted arrows) are measured before censoring the vocabularies to remove any overlap. Directed edges indicate direction of asymmetric percent overlap |Vsource∩Vtarget||Vsource|.
<R> <C> [EMPTY] <C> [ITALIC]  [BOLD] ca+en+ru <C> [BOLD] + [ITALIC] uk <C> [BOLD] % Change <R> <C> [BOLD] Recall@1 <C> 10.6 <C> 11.3 <C> [BOLD] +6.6% <R> <C> [BOLD] Recall@10 <C> 20.1 <C> 21.5 <C> [BOLD] +7.0% <R> <C> [BOLD] Recall@20 <C> 24.3 <C> 25.6 <C> [BOLD] +5.3% <CAP> Table 3: Transfer improvement of Catalan (ca) in the presence of Ukrainian (uk) data, even with no direct vocabulary overlap. English (en) and Russian (ru) serve as pivot languages.
<R> <C> Model <C> #Parameters <R> <C> Dynamic memory-based generative model <C> 14,197,741 <R> <C> Fact2Seq w. Attention <C> 14,159,561 <R> <C> Neural Knowledge Language Model (NKLM) <C> 20,569,361 <R> <C> Our Model <C> 979,986 <CAP> Table 4. Comparison of the number of learnable parameters.
<R> <C> [EMPTY] <C> Exact Match <R> <C> SEMAFOR (Das et al.,  2014 ) <C> 83.60 <R> <C> Hartmann et al. ( 2017 ) <C> 87.63 <R> <C> Hermann et al. ( 2014 ) / <C> [BOLD] 88.41 <R> <C> FitzGerald et al. ( 2015 ) <C> [BOLD] 88.41 <R> <C> Open-SESAME <C> 86.94 <R> <C> Open-SESAME (ensemble) <C> 87.51 <CAP> Table 4: Test set accuracy of frame identification.
<R> <C> [EMPTY] <C> Comments <C> Percentage <R> <C> Female <C> 2573 <C> 43% <R> <C> Male <C> 3406 <C> 57% <R> <C> Total of comments <C> 5 979 <C> 100% <CAP> Table 3: CCDMX, distribution of samples by gender.
<R> <C> SemEval 2007 coarse-grained - S7CG Method <C> SemEval 2007 coarse-grained - S7CG All <C> SemEval 2007 coarse-grained - S7CG N <C> SemEval 2007 coarse-grained - S7CG V <C> SemEval 2007 coarse-grained - S7CG A <C> SemEval 2007 coarse-grained - S7CG R <R> <C> WSD [ITALIC] unsgames <C> 80.4 <C> 85.5 <C> 71.2 <C> 81.5 <C> 76.0 <R> <C> WSD [ITALIC] ssupgames <C> 82.8 <C> 85.4 <C> 77.2 <C> 82.9 <C> 84.6 <R> <C> MFS <C> 76.3 <C> 76.0 <C> 70.1 <C> 82.0 <C> 86.0 <R> <C> SemEval 2007 fine-grained - S7 <C> SemEval 2007 fine-grained - S7 <C> SemEval 2007 fine-grained - S7 <C> SemEval 2007 fine-grained - S7 <C> SemEval 2007 fine-grained - S7 <C> SemEval 2007 fine-grained - S7 <R> <C> Method <C> All <C> N <C> V <C> A <C> R <R> <C> WSD [ITALIC] unsgames <C> 43.3 <C> 49.7 <C> 39.9 <C> − <C> − <R> <C> WSD [ITALIC] ssupgames <C> 56.5 <C> 62.9 <C> 53.0 <C> − <C> − <R> <C> MFS <C> 54.7 <C> 60.4 <C> 51.7 <C> − <C> − <R> <C> Senseval 3 fine-grained - S3 <C> Senseval 3 fine-grained - S3 <C> Senseval 3 fine-grained - S3 <C> Senseval 3 fine-grained - S3 <C> Senseval 3 fine-grained - S3 <C> Senseval 3 fine-grained - S3 <R> <C> Method <C> All <C> N <C> V <C> A <C> R <R> <C> WSD [ITALIC] unsgames <C> 59.1 <C> 63.3 <C> 50.7 <C> 64.5 <C> 71.4 <R> <C> WSD [ITALIC] ssupgames <C> 64.7 <C> 70.3 <C> 54.1 <C> 70.7 <C> 85.7 <R> <C> MFS <C> 62.8 <C> 69.3 <C> 51.4 <C> 68.2 <C> 100.0 <R> <C> Senseval 2 fine-grained - S2 <C> Senseval 2 fine-grained - S2 <C> Senseval 2 fine-grained - S2 <C> Senseval 2 fine-grained - S2 <C> Senseval 2 fine-grained - S2 <C> Senseval 2 fine-grained - S2 <R> <C> Method <C> All <C> N <C> V <C> A <C> R <R> <C> WSD [ITALIC] unsgames <C> 61.2 <C> 69.8 <C> 41.7 <C> 61.9 <C> 65.1 <R> <C> WSD [ITALIC] ssupgames <C> 66.0 <C> 72.4 <C> 43.5 <C> 71.8 <C> 75.7 <R> <C> MFS <C> 65.6 <C> 72.1 <C> 42.4 <C> 71.6 <C> 76.1 <CAP> Table 4: Detailed results as F1 for the four datasets studied with tf-idf and mdice as measures. The results show the performances of our unsupervised (uns) and semi-supervised (ssup) system and the results obtained employing the most frequent sense heuristic (MFS). Detailed information about the performances of the systems on different part of speech are provided: nouns (N), verbs (V), adjectives (A), adverbs (R).
<R> <C> [EMPTY] <C> adj msc <C> adj fem <C> nsubj msc <C> nsubj fem <C> dobj msc <C> dobj fem <R> <C> pos <C> 0.34 <C> 0.38 <C> 0.37 <C> 0.36 <C> 0.37 <C> 0.36 <R> <C> neg <C> 0.30 <C> 0.31 <C> 0.33 <C> 0.34 <C> 0.34 <C> 0.35 <R> <C> neu <C> [BOLD] 0.36 <C> [BOLD] 0.31 <C> 0.30 <C> 0.30 <C> 0.30 <C> 0.29 <CAP> Table 3: The frequency with which the 200 largest-deviation neighbors for each gender correspond to each sentiment, obtained using a simplified version of our model and the lexicon of hoyleSentiment2019. Significant differences (p<0.05/3 under an unpaired permutation test with Bonferroni correction) are in bold.
<R> <C> Name <C> Original Dataset <C> Type <C> #Select Speaker <C> #Utterance Train <C> #Utterance Test <R> <C> SWBC-S <C> SWBC <C> Telephone <C> 254 <C> 6000 <C> 20,000 <R> <C> SWBC-L <C> SWBC <C> Telephone <C> 254 <C> 100,000 <C> 20,000 <R> <C> Vox-S <C> Voxceleb1 <C> Interview <C> 1000 <C> 15000 <C> 30,000 <R> <C> Vox-L <C> Voxceleb1 <C> Interview <C> 1000 <C> 150,000 <C> 30,000 <CAP> Table 1: Details for the construction of the four datasets: SWBC-S, Vox-L, SWBC-S and Vox-L.
<R> <C> [EMPTY] <C> Grapheme-to-Phoneme Conversion ( G)  [BOLD] Small <C> Grapheme-to-Phoneme Conversion ( G)  [BOLD] Small <C> Grapheme-to-Phoneme Conversion ( G)  [BOLD] Large <C> Grapheme-to-Phoneme Conversion ( G)  [BOLD] Large <C> Named-Entity Transliteration ( T)  [BOLD] Small <C> Named-Entity Transliteration ( T)  [BOLD] Small <C> Named-Entity Transliteration ( T)  [BOLD] Large <C> Named-Entity Transliteration ( T)  [BOLD] Large <C> Morphological Inflection ( I)  [BOLD] Small <C> Morphological Inflection ( I)  [BOLD] Small <C> Morphological Inflection ( I)  [BOLD] Large <C> Morphological Inflection ( I)  [BOLD] Large <R> <C> [EMPTY] <C> [BOLD] WER <C> [BOLD] PER <C> [BOLD] WER <C> [BOLD] PER <C> [BOLD] ACC <C> [BOLD] MFS <C> [BOLD] ACC <C> [BOLD] MFS <C> [BOLD] ACC <C> [BOLD] MLD <C> [BOLD] ACC <C> [BOLD] MLD <R> <C> 1 <C> 33.7 <C> 0.080 <C> 30.8 <C> 0.074 <C> 38.9 <C> 0.890 <C> 39.9 <C> 0.893 <C> 91.4 <C> 0.183 <C> 91.1 <C> 0.201 <R> <C> U <C> 30.6 <C> 0.074 <C> 30.4 <C> 0.073 <C> 39.8 <C> 0.891 <C> 40.3 <C> 0.894 <C> 91.0 <C> 0.185 <C> 91.0 <C> 0.212 <R> <C> 2 <C> 32.3 <C> 0.079 <C> 33.1 <C> 0.081 <C> 36.3 <C> 0.881 <C> 30.8 <C> 0.837 <C> 91.0 <C> 0.193 <C> 89.3 <C> 0.322 <R> <C> 3 <C> 30.3 <C> 0.074 <C> 28.6 <C> 0.070 <C> [BOLD] 40.1 <C> 0.891 <C> 40.5 <C> 0.894 <C> 92.0 <C> 0.163 <C> 92.2 <C> 0.166 <R> <C> 4 <C> [BOLD] 29.6 <C> [BOLD] 0.072 <C> [BOLD] 28.2 <C> [BOLD] 0.068 <C> 39.8 <C> 0.891 <C> [BOLD] 41.1 <C> 0.894 <C> [BOLD] 92.6 <C> [BOLD] 0.151 <C> [BOLD] 93.6 <C> [BOLD] 0.128 <R> <C> R <C> 30.7 <C> 0.076 <C> 29.7 <C> 0.074 <C> 37.1 <C> 0.882 <C> 36.9 <C> 0.863 <C> 91.2 <C> 0.190 <C> 92.8 <C> 0.151 <R> <C> M <C> 33.9 <C> 0.082 <C> 29.9 <C> 0.072 <C> 38.8 <C> [BOLD] 0.959 <C> 40.1 <C> [BOLD] 0.960 <C> 91.7 <C> 0.160 <C> 92.8 <C> 0.141 <CAP> Table 3: Average test performance on G, T and I averaged across datasets and languages. See App. B fCor full breakdown.
<R> <C> [EMPTY] <C> [BOLD] NETtalk 3 <C> [BOLD] NETtalk 3 <C> [BOLD] NETtalk 4 <C> [BOLD] NETtalk 4 <C> [BOLD] CMUDict 3 <C> [BOLD] CMUDict 3 <C> [BOLD] CMUDict 4 <C> [BOLD] CMUDict 4 <R> <C> [EMPTY] <C> [BOLD] ✓ <C> [BOLD] ✗ <C> [BOLD] ✓ <C> [BOLD] ✗ <C> [BOLD] ✓ <C> [BOLD] ✗ <C> [BOLD] ✓ <C> [BOLD] ✗ <R> <C> [BOLD] Monotonic <C> 18742 <C> 1230 <C> 18823 <C> 1172 <C> 95824 <C> 17294 <C> 96176 <C> 17159 <R> <C> [BOLD] Non-monotonic <C> 31 <C> 5 <C> 12 <C> 1 <C> 158 <C> 162 <C> 37 <C> 66 <CAP> Table 5: Breakdown of correct and incorrect predictions of monotonic and non-monotonic alignments of 3 and 4 in G, derived from the soft attention weights and the hard alignment distribution
<R> <C> Task Type <C> first-name- <C> child- <C> husband- <C> geo-name- <C> houses- <C> Total <R> <C> Task Type <C> last-name <C> father <C> wife <C> location <C> seats <C> Total <R> <C> Number of tasks: <C> 2368 <C> 180 <C> 30 <C> 168 <C> 30 <C> 2848 <R> <C> w2v-default <C> 20.73 <C> 3.33 <C> 6.67 <C> 1.19 <C> 43.33 <C> 18.43 <R> <C> w2v-ww12-300-ns <C> 39.53 <C> 1.67 <C> 0.0 <C> 10.71 <C> 26.67 <C> 34.38 <R> <C> w2v-CBOW <C> 0.46 <C> 6.11 <C> 10.0 <C> 7.14 <C> 6.67 <C> 1.37 <R> <C> GloVe <C> 40.58 <C> 6.11 <C> 3.33 <C> 1.19 <C> 26.67 <C> [BOLD] 34.8 <R> <C> fastText <C> 37.67 <C> 4.44 <C> 0.0 <C> 13.1 <C> 26.67 <C> 32.94 <R> <C> LexVec <C> 40.08 <C> 3.89 <C> 6.67 <C> 0.0 <C> 23.33 <C> 34.38 <CAP> Table 1: ASOIF analogy dataset: Accuracy of various word embedding models on various selected analogy task types, and total accuracy.
<R> <C> [BOLD] Correlation of  [BOLD] accuracy to <C> [BOLD] Correlation of  [BOLD] accuracy to <C> Freq. of real intruder <C> Freq. of chosen intruder <C> Freq. bin of chosen intr. <C> Avg. Freq. of task terms <C> Difficulty <R> <C> ASOIF <C> unigram <C> -0.10 <C> 0.31 <C> 0.29 <C> 0.05 <C> 0.30 <R> <C> ASOIF <C> n-gram <C> 0.15 <C> 0.17 <C> 0.18 <C> 0.12 <C> 0.10 <R> <C> HP <C> unigram <C> -0.03 <C> 0.05 <C> 0.20 <C> 0.02 <C> 0.18 <R> <C> HP <C> n-gram <C> -0.25 <C> 0.31 <C> 0.58 <C> -0.19 <C> 0.25 <R> <C> [BOLD] Average results <C> [BOLD] Average results <C> -0.06 <C> 0.21 <C> 0.31 <C> 0.00 <C> 0.21 <CAP> Table 5: Correlations of term frequencies with accuracy in tasks for unigram and n-gram models trained on Harry Potter and A Song of Ice and Fire
<R> <C> [BOLD] country <C> [BOLD] year <C> [BOLD] host_city <C> [BOLD] #_participants <C> [BOLD] #_medals <C> [BOLD] #_duration <C> [BOLD] #_audience <R> <C> China <C> 2008 <C> Beijing <C> 3,500 <C> 4,200 <C> 30 <C> 67,000 <CAP> Table-2
<R> <C> [BOLD] country <C> [BOLD] continent <C> [BOLD] population <C> [BOLD] country_size <R> <C> China <C> Asia <C> 130 <C> 960 <CAP> Table-2
<R> <C> Model <C> [BOLD] BLEU  [ITALIC] xsem↑ <C> [BOLD] BLEU  [ITALIC] xsyn↓ <C> ΔBLEU↑ <C> [BOLD] TED  [ITALIC] xsem↑ <C> [BOLD] TED  [ITALIC] xsyn↓ <C> ΔTED↑ <C> ΔGM↑ <R> <C> Standard VAE <C> 4.75 <C> 4.67 <C> 0.08 <C> 13.70 <C> 13.60 <C> 0.10 <C> 0.28 <R> <C> bao2019generating <C> [BOLD] 13.74 <C> 6.15 <C> 7.59 <C> [BOLD] 16.19 <C> 13.10 <C> [BOLD] 3.08 <C> 4.83 <R> <C> polarized-VAE <C> 10.78 <C> 0.92 <C> [BOLD] 9.86 <C> 14.09 <C> 11.67 <C> 2.42 <C> [BOLD] 4.88 <R> <C> polarized-VAE (wo) <C> 9.82 <C> 0.84 <C> 8.98 <C> 14.12 <C> 11.65 <C> 2.47 <C> 4.71 <R> <C> polarized-VAE (len) <C> 10.10 <C> [BOLD] 0.76 <C> 9.34 <C> 12.68 <C> [BOLD] 11.44 <C> 1.44 <C> 3.67 <R> <C> polarized-VAE (wo, len) <C> 9.41 <C> 0.87 <C> 8.54 <C> 12.65 <C> 11.48 <C> 1.17 <C> 3.16 <CAP> Table 1: Results of syntax transfer generation on SNLI dataset. bao2019generating report TED after multiplying by 10, we report their score after correcting for it.
<R> <C> Model <C> Max Abs Corr↓ <C> Mean Abs Corr↓ <R> <C> Baseline-Vae <C> 0.62 <C> 0.1 <R> <C> Polarized-Vae <C> [BOLD] 0.25 <C> [BOLD] 0.05 <CAP> Table 2: Maximum Absolute Correlation and Mean Absolute Correlation between the semantic and syntactic latent vectors.
<R> <C> Obs. proportion  [ITALIC] σ <C> 5% <C> 10% <C> 50% <C> 100% <C> DCNN dec. <R> <C> Supervised <C> 12.40 <C> 13.07 <C> 15.87 <C> 16.37 <C> 14.75 <R> <C> Semi-sup. <C> 16.04 <C> 16.62 <C> 17.64 <C> [BOLD] 18.14 <C> 16.83 <CAP> Table 5: Summarization task on arXiv data, using ROUGE-L metric. First 4 columns are for the LSTM decoder, and the last column is for the deconvolutional decoder (100% observed).
<R> <C> Model <C> # Forg. <C> # Forg. (balanced) <C> MNLI <R> <C> BoW <C> 100,345 <C> 63,390 <C> 64.0 <R> <C> BiLSTM <C> 76,270 <C> 46,740 <C> 69.6 <R> <C> BERT <C> 32,387 <C> 17,748 <C> 84.5 <CAP> Table 1: Number of “forgettables” examples (those that are forgotten at least once or never learned) during five training epochs along with the accuracy on the MNLI matched development set.
<R> <C> [BOLD] Train examples <C> [BOLD] HANS <C> [BOLD] MNLI <C> [BOLD] Avg. <R> <C> All <C> 72.3 <C> 86.4 <C> 79.3 <R> <C> [ITALIC] Additional stage of finetuning <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> All + BoW forgettables <C> 77.3 <C> 85.5 <C> 81.4 <R> <C> All + BiLSTM forgettables <C> 77.5 <C> 85.5 <C> 81.5 <CAP> Table 3: Results of BERTLARGE model trained on different sources of training examples.
<R> <C> [EMPTY] <C> sst <C> cr <C> trec <C> mpqa <C> subj <R> <C> LSTM <C> 86.7 <C> 82.7 <C> 91.5 <C> 88.9 <C> 94.8 <R> <C> LSTM DkNN <C> 86.6 <C> 82.5 <C> 91.3 <C> 88.6 <C> 94.9 <R> <C> CNN <C> 85.7 <C> 83.3 <C> 92.8 <C> 89.1 <C> 93.5 <R> <C> CNN DkNN <C> 85.8 <C> 83.4 <C> 92.4 <C> 88.7 <C> 93.1 <CAP> Table 1: Replacing a neural network’s softmax classifier with DkNN maintains classification accuracy on standard text classification tasks.
<R> <C> Scoring Method <C> EER% <R> <C> Cosine <C> 2.88 <R> <C> PLDA: STRONG <C> 2.25 <R> <C> PLDA: WEAK-customer <C> 2.47 <R> <C> PLDA: WEAK-service <C> 2.94 <R> <C> PLDA: WEAK-mix <C> 2.55 <CAP> Table 2: EER(%) results of strong and weak training.
<R> <C> [BOLD] Models <C> [BOLD] First-Word Question Types What <C> [BOLD] First-Word Question Types Who <C> [BOLD] First-Word Question Types How <C> [BOLD] First-Word Question Types Where <C> [BOLD] First-Word Question Types When <C> [BOLD] First-Word Question Types Why <C> [BOLD] First-Word Question Types Which <C> [BOLD] First-Word Question Types Others <R> <C> Human <C> 74.1 <C> 83.5 <C> 61.1 <C> 74.8 <C> 72.2 <C> 66.0 <C> 76.8 <C> 76.0 <R> <C> Query-Matching <C> 32.4 <C> 29.8 <C> 28.4 <C> 27.1 <C> 22.9 <C> 51.9 <C> 22.7 <C> 21.1 <R> <C> [ITALIC] Neural Baselines <C> [ITALIC] Neural Baselines <C> [ITALIC] Neural Baselines <C> [ITALIC] Neural Baselines <C> [ITALIC] Neural Baselines <C> [ITALIC] Neural Baselines <C> [ITALIC] Neural Baselines <C> [ITALIC] Neural Baselines <C> [ITALIC] Neural Baselines <R> <C> BiDAF <C> 44.5 <C> 54.9 <C> 41.0 <C> 60.2 <C> 46.5 <C> 36.1 <C> 44.7 <C> 41.6 <R> <C> Generative <C> 46.8 <C> 63.8 <C> 53.4 <C> 61.7 <C> 45.4 <C> 44.3 <C> 51.4 <C> 43.1 <R> <C> BERT <C> 64.8 <C> 72.5 <C> 57.7 <C> 78.1 <C> 64.5 <C> 61.0 <C> 67.2 <C> 59.2 <CAP> Table 8: BLEU-1 scores on different types of questions. Calculated on the development set.
<R> <C> Degree <C> EN-FR #Total <C> EN-FR #Correct <C> EN-FR Accuracy <C> EN-DE #Total <C> EN-DE #Correct <C> EN-DE Accuracy <C> DBP-WD #Total <C> DBP-WD #Correct <C> DBP-WD Accuracy <C> DBP-YG #Total <C> DBP-YG #Correct <C> DBP-YG Accuracy <R> <C> 1 <C> 2,660 <C> 380 <C> 14.29% <C> 1,978 <C> 453 <C> 22.90% <C> 1,341 <C> 276 <C> 20.58% <C> 3,327 <C> 538 <C> 16.17% <R> <C> 2 <C> 2,540 <C> 699 <C> 27.52% <C> 2,504 <C> 1,005 <C> 40.14% <C> 2,979 <C> 800 <C> 26.85% <C> 2,187 <C> 688 <C> 31.46% <R> <C> 3 <C> 1,130 <C> 408 <C> 36.11% <C> 1,514 <C> 820 <C> 54.16% <C> 1,789 <C> 600 <C> 33.54% <C> 1,143 <C> 563 <C> 49.26% <R> <C> ¿=4 <C> 3,120 <C> 1,803 <C> 57.79% <C> 3,454 <C> 2,416 <C> 69.95% <C> 3,341 <C> 2,093 <C> 62.65% <C> 2,793 <C> 2,008 <C> 71.89% <R> <C> All <C> 9,450 <C> 3,290 <C> 34.81% <C> 9,450 <C> 4,694 <C> 49.67% <C> 9,450 <C> 3,769 <C> 39.88% <C> 9,450 <C> 3,797 <C> 40.18% <CAP> Table 1. Degree distribution of entities in test set (the first KG in each KG pair) and results of RSNs
<R> <C> Methods <C> EN-FR Hits@1 <C> EN-FR Hits@10 <C> EN-FR MRR <C> EN-DE Hits@1 <C> EN-DE Hits@10 <C> EN-DE MRR <C> DBP-WD Hits@1 <C> DBP-WD Hits@10 <C> DBP-WD MRR <C> DBP-YG Hits@1 <C> DBP-YG Hits@10 <C> DBP-YG MRR <R> <C> MTransE <C> 25.1 <C> 55.1 <C> 0.35 <C> 31.2 <C> 58.6 <C> 0.40 <C> 22.3 <C> 50.1 <C> 0.32 <C> 24.6 <C> 54.0 <C> 0.34 <R> <C> IPTransE <C> 25.5 <C> 55.7 <C> 0.36 <C> 31.3 <C> 59.2 <C> 0.41 <C> 23.1 <C> 51.7 <C> 0.33 <C> 22.7 <C> 50.0 <C> 0.32 <R> <C> BootEA <C> 31.3 <C> 62.9 <C> 0.42 <C> 44.2 <C> 70.1 <C> 0.53 <C> 32.3 <C> 63.1 <C> 0.42 <C> 31.3 <C> 62.5 <C> 0.42 <R> <C> RSNs <C> 34.8 <C> 63.7 <C> 0.45 <C> 49.7 <C> 73.3 <C> 0.58 <C> 39.9 <C> 66.8 <C> 0.49 <C> 40.2 <C> 68.9 <C> 0.50 <R> <C> MuGNN <C> 13.1 <C> 34.2 <C> 0.20 <C> 24.5 <C> 43.1 <C> 0.31 <C> 15.1 <C> 36.6 <C> 0.22 <C> 17.5 <C> 38.1 <C> 0.24 <R> <C> KECG <C> 29.8 <C> 61.6 <C> 0.40 <C> 44.4 <C> 70.7 <C> 0.54 <C> 32.3 <C> 64.6 <C> 0.43 <C> 35.0 <C> 65.1 <C> 0.45 <R> <C> TransEdge <C> 40.0 <C> 67.5 <C> 0.49 <C> 55.6 <C> 75.3 <C> 0.63 <C> 46.1 <C> 73.8 <C> 0.56 <C> 44.3 <C> 69.9 <C> 0.53 <R> <C> GCN-Align <C> 15.5 <C> 34.5 <C> 0.22 <C> 25.3 <C> 46.4 <C> 0.33 <C> 17.7 <C> 37.8 <C> 0.25 <C> 19.3 <C> 41.5 <C> 0.27 <R> <C> JAPE <C> 25.6 <C> 56.2 <C> 0.36 <C> 32.0 <C> 59.9 <C> 0.41 <C> 21.9 <C> 50.1 <C> 0.31 <C> 23.3 <C> 52.7 <C> 0.33 <R> <C> RDGCN <C> 67.5 <C> 76.9 <C> 0.71 <C> 78.3 <C> 88.4 <C> 0.82 <C> 83.4 <C> 90.7 <C> 0.86 <C> 85.8 <C> 93.8 <C> 0.89 <R> <C> HGCN <C> 67.0 <C> 77.0 <C> 0.71 <C> 76.3 <C> 86.3 <C> 0.80 <C> 82.3 <C> 88.7 <C> 0.85 <C> 82.2 <C> 88.8 <C> 0.85 <R> <C> GM-Align 1 <C> 62.7 <C> - <C> - <C> 67.7 <C> - <C> - <C> 81.5 <C> - <C> - <C> 82.8 <C> - <C> - <R> <C> DAT <C> [BOLD] 75.8 <C> [BOLD] 89.9 <C> [BOLD] 0.81 <C> [BOLD] 87.6 <C> [BOLD] 95.5 <C> [BOLD] 0.90 <C> [BOLD] 92.6 <C> [BOLD] 97.7 <C> [BOLD] 0.94 <C> [BOLD] 94.0 <C> [BOLD] 98.5 <C> [BOLD] 0.96 <CAP> Table 3. Overall results of entity alignment
<R> <C> Degree <C> EN-FR RSNs <C> EN-FR RDGCN <C> EN-FR DAT <C> EN-DE RSNs <C> EN-DE RDGCN <C> EN-DE DAT <C> DBP-WD RSNs <C> DBP-WD RDGCN <C> DBP-WD DAT <C> DBP-YG RSNs <C> DBP-YG RDGCN <C> DBP-YG DAT <R> <C> 1 <C> 14.3 <C> 56.5 <C> [BOLD] 57.4 <C> 22.9 <C> 75.8 <C> [BOLD] 83.1 <C> 20.6 <C> 80.3 <C> [BOLD] 86.7 <C> 16.2 <C> 84.2 <C> [BOLD] 89.5 <R> <C> 2 <C> 27.5 <C> 64.4 <C> [BOLD] 72.4 <C> 40.1 <C> 78.6 <C> [BOLD] 84.3 <C> 26.9 <C> 84.0 <C> [BOLD] 90.8 <C> 31.5 <C> 80.9 <C> [BOLD] 92.6 <R> <C> 3 <C> 36.1 <C> 77.3 <C> [BOLD] 82.9 <C> 54.2 <C> 77.4 <C> [BOLD] 88.4 <C> 33.5 <C> 75.2 <C> [BOLD] 88.2 <C> 49.3 <C> 89.2 <C> [BOLD] 97.0 <R> <C> ≥4 <C> 57.8 <C> 75.8 <C> [BOLD] 91.6 <C> 69.9 <C> 80.0 <C> [BOLD] 92.3 <C> 62.6 <C> 88.6 <C> [BOLD] 98.8 <C> 71.9 <C> 90.2 <C> [BOLD] 99.4 <R> <C> All <C> 34.8 <C> 67.5 <C> [BOLD] 75.8 <C> 49.7 <C> 78.3 <C> [BOLD] 87.6 <C> 39.9 <C> 83.4 <C> [BOLD] 92.6 <C> 40.2 <C> 85.8 <C> [BOLD] 94.0 <CAP> Table 4. Hits@1 results by degrees
<R> <C> [BOLD] # of labels <C> 0 <C> 1 <C> 2 <C> 3 <C> 4 <C> 5 <C> 6 <R> <C> [BOLD] % <C> 2.9 <C> 14.3 <C> 40.6 <C> 30.9 <C> 9.6 <C> 1.4 <C> 0.2 <CAP> Table 4: Number of multi-labels. Most samples have from 1-3 labels, but can have no labels or up to 6 labels. (subtask 5a)
<R> <C> [EMPTY] <C> [BOLD] TP Rate <C> [BOLD] FP Rate <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] F-Measure <R> <C> [BOLD] (1) Previous features <C> [BOLD] (1) Previous features <C> [BOLD] (1) Previous features <C> [BOLD] (1) Previous features <C> [BOLD] (1) Previous features <C> [BOLD] (1) Previous features <R> <C> FR <C> 0.753 <C> 0.230 <C> 0.766 <C> 0.753 <C> 0.759 <R> <C> TR <C> 0.770 <C> 0.247 <C> 0.757 <C> 0.770 <C> 0.764 <R> <C> Avg. <C> 0.762 <C> 0.238 <C> 0.762 <C> 0.762 <C> 0.762 <R> <C> [BOLD] (2) New proposed features <C> [BOLD] (2) New proposed features <C> [BOLD] (2) New proposed features <C> [BOLD] (2) New proposed features <C> [BOLD] (2) New proposed features <C> [BOLD] (2) New proposed features <R> <C> FR <C> 0.790 <C> 0.208 <C> 0.792 <C> 0.790 <C> 0.791 <R> <C> TR <C> 0.792 <C> 0.210 <C> 0.791 <C> 0.792 <C> 0.792 <R> <C> Avg. <C> 0.791 <C> 0.209 <C> 0.791 <C> 0.791 <C> 0.791 <R> <C> [BOLD] (3) New features + "power of spread" factor <C> [BOLD] (3) New features + "power of spread" factor <C> [BOLD] (3) New features + "power of spread" factor <C> [BOLD] (3) New features + "power of spread" factor <C> [BOLD] (3) New features + "power of spread" factor <C> [BOLD] (3) New features + "power of spread" factor <R> <C> FR <C> 0.791 <C> 0.175 <C> 0.814 <C> 0.791 <C> 0.802 <R> <C> TR <C> 0.825 <C> 0.209 <C> 0.803 <C> 0.825 <C> 0.814 <R> <C> Avg. <C> 0.808 <C> 0.192 <C> 0.808 <C> 0.808 <C> 0.808 <R> <C> [BOLD] (4) Previous features + New features + "power of spread" factor <C> [BOLD] (4) Previous features + New features + "power of spread" factor <C> [BOLD] (4) Previous features + New features + "power of spread" factor <C> [BOLD] (4) Previous features + New features + "power of spread" factor <C> [BOLD] (4) Previous features + New features + "power of spread" factor <C> [BOLD] (4) Previous features + New features + "power of spread" factor <R> <C> FR <C> 0.802 <C> 0.145 <C> 0.802 <C> 0.846 <C> 0.824 <R> <C> TR <C> 0.855 <C> 0.198 <C> 0.855 <C> 0.812 <C> 0.833 <R> <C> Avg. <C> 0.828 <C> 0.172 <C> 0.828 <C> 0.829 <C> [BOLD] 0.828 <CAP> Table 8: The effect of different features in rumor detection in Telegram using RF classifier.
<R> <C> Dataset <C> Unsupervised GloVe (840B words) + WR <C> Semi-supervised PSL + WR <C> Sent2Vec Unigrams (19.7B words) Tweets Model <C> Sent2Vec Unigrams + Bigrams (19.7B words) Tweets Model <R> <C> STS 2014 <C> 0.685 <C> 0.735 <C> 0.710 <C> 0.701 <R> <C> SICK 2014 <C> 0.722 <C> 0.729 <C> 0.710 <C> 0.715 <R> <C> Supervised average <C> 0.815 <C> 0.807 <C> 0.822 <C> 0.835 <CAP> Table 4: Comparison of the performance of the unsupervised and semi-supervised sentence embeddings by Arora et al. (2017) with our models. Unsupervised comparisons are in terms of Pearson’s correlation, while comparisons on supervised tasks are stating the average described in Table 1.
<R> <C> Model <C> Embedding Dimensions <C> Minimum word count <C> Minimum Target word Count <C> Initial Lear ning Rate <C> Epochs <C> Subsampling hyper-parameter <C> Bigrams Dropped per sentence <C> Number of negatives sampled <R> <C> Book corpus Sent2Vec unigrams <C> 700 <C> 5 <C> 8 <C> 0.2 <C> 13 <C> 1×10−5 <C> - <C> 10 <R> <C> Book corpus Sent2Vec unigrams + bigrams <C> 700 <C> 5 <C> 5 <C> 0.2 <C> 12 <C> 5×10−6 <C> 7 <C> 10 <R> <C> Wiki Sent2Vec unigrams <C> 600 <C> 8 <C> 20 <C> 0.2 <C> 9 <C> 1×10−5 <C> - <C> 10 <R> <C> Wiki Sent2Vec unigrams + bigrams <C> 700 <C> 8 <C> 20 <C> 0.2 <C> 9 <C> 5×10−6 <C> 4 <C> 10 <R> <C> Twitter Sent2Vec unigrams <C> 700 <C> 20 <C> 20 <C> 0.2 <C> 3 <C> 1×10−6 <C> - <C> 10 <R> <C> Twitter Sent2Vec unigrams + bigrams <C> 700 <C> 20 <C> 20 <C> 0.2 <C> 3 <C> 1×10−6 <C> 3 <C> 10 <CAP> Table 5: Training parameters for the Sent2Vec models
<R> <C> Document Embedding <C> en-fr (1M) P@1 <C> en-fr (1M) P@3 <C> en-fr (1M) P@10 <C> en-es (0.6M) P@1 <C> en-es (0.6M) P@3 <C> en-es (0.6M) P@10 <R> <C> HiDEDNN→pooling <C> [BOLD] 91.40 <C> [BOLD] 94.13 <C> [BOLD] 95.67 <C> [BOLD] 81.83 <C> [BOLD] 87.85 <C> [BOLD] 91.45 <R> <C> HiDEpooling→DNN <C> 90.63 <C> 93.50 <C> 95.11 <C> 78.84 <C> 85.04 <C> 88.88 <R> <C> Document BoW <C> 83.83 <C> 90.47 <C> 94.18 <C> 78.09 <C> 85.04 <C> 91.03 <R> <C> Sentence-Avg <C> 78.07 <C> 83.53 <C> 87.06 <C> 67.49 <C> 74.22 <C> 79.01 <CAP> Table 2: Precision at N (P@N) of target document retrieval on the WebData test set. Models attempt to select the true translation target for a source document from the entire corpus (1 million parallel documents for en-fr, and 0.6 million for en-es).
<R> <C> Model <C> en-fr <C> en-es <R> <C> UN Corpus Sentence Segmentation <C> UN Corpus Sentence Segmentation <C> UN Corpus Sentence Segmentation <R> <C> HiDEDNN→pooling <C> 96.6 <C> [BOLD] 97.3 <R> <C> HiDEpooling→DNN <C> 96.5 <C> 96.1 <R> <C> Sentence-Avg <C> [BOLD] 96.7 <C> [BOLD] 97.3 <R> <C> Noisy Sentence Segmentation <C> Noisy Sentence Segmentation <C> Noisy Sentence Segmentation <R> <C> HiDEDNN→pooling <C> [BOLD] 94.9 <C> [BOLD] 96.0 <R> <C> HiDEpooling→DNN <C> 91.0 <C> 94.4 <R> <C> Sentence-Avg <C> 86.8 <C> 95.7 <R> <C> No sentence splitting <C> No sentence splitting <C> No sentence splitting <R> <C> Document BoW <C> 74.3 <C> 71.9 <R> <C> [ITALIC] Prior work <C> [ITALIC] Prior work <C> [ITALIC] Prior work <R> <C> jakob2010 <C> 93.4 <C> 94.4 <R> <C> mandy2018 <C> 89.0 <C> 90.4 <CAP> Table 3: Document matching on the UN corpus evaluated using P@1. For methods that require sentence splitting, we report results using both the UN sentence annotations and an off-the-shelf sentence splitter.
<R> <C> Languages <C> P@1 at Sentence Level <C> P@1 on WebDoc test HiDEDNN→pooling <C> P@1 on WebDoc test Sentence-Avg <C> P@1 on Noisy UN HiDEDNN→pooling <C> P@1 on Noisy UN Sentence-Avg <R> <C> en-fr <C> 48.9 <C> 66.6 <C> 0.6 <C> 70.3 <C> 4.4 <R> <C> en-fr <C> 66.9 <C> 89.2 <C> 54.3 <C> 92.6 <C> 63.9 <R> <C> en-fr <C> 81.3 <C> 90.5 <C> 72.9 <C> 92.1 <C> 76.9 <R> <C> en-fr <C> 86.1 <C> 91.3 <C> 78.1 <C> 94.9 <C> 86.9 <R> <C> en-es <C> 54.9 <C> 59.0 <C> 1.2 <C> 81.3 <C> 4.7 <R> <C> en-es <C> 67.0 <C> 79.1 <C> 54.2 <C> 93.2 <C> 82.9 <R> <C> en-es <C> 80.6 <C> 79.8 <C> 60.1 <C> 91.2 <C> 88.9 <R> <C> en-es <C> 89.0 <C> 81.9 <C> 67.4 <C> 96.0 <C> 95.7 <CAP> Table 5: P@1 of target document retrieval on WebData test set and noisy UN corpus for HiDEDNN→pooling and Sentence-Avg models with different sentence-level P@1 performance . The sentence-level peroformance is measured on the sentence-level UN retrieval task from the entire corpus (11.3 million sentence candidates).
<R> <C> [BOLD] Method <C> [BOLD] Twitter ADR Dataset  [BOLD] Precision <C> [BOLD] Twitter ADR Dataset  [BOLD] Recall <C> [BOLD] Twitter ADR Dataset  [BOLD] F1-score <C> [BOLD] TwiMed Dataset  [BOLD] Precision <C> [BOLD] TwiMed Dataset  [BOLD] Recall <C> [BOLD] TwiMed Dataset  [BOLD] F1-score <R> <C> Baseline  <C> 0.7067±0.057 <C> 0.7207±0.074 <C> 0.7102±0.049 <C> 0.6120± 0.116 <C> 0.5149± 0.099 <C> 0.5601± 0.100 <R> <C> Baseline with Adam <C> 0.7065±0.058 <C> 0.7576±0.083 <C> 0.7272±0.051 <C> [BOLD] 0.6281±0.094 <C> 0.5614±0.110 <C> 0.5859±0.079 <R> <C> KB-Embedding <C> 0.7171± 0.058 <C> 0.7713±0.091 <C> 0.7397±0.055 <C> 0.5960±0.081 <C> 0.6144±0.068 <C> 0.6042±0.060 <R> <C> Baseline  <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Co-training (5k) <C> 0.7247±0.056 <C> 0.7770±0.082 <C> 0.7488± 0.063 <C> 0.5806±0.093 <C> 0.6746±0.078 <C> [BOLD] 0.6192±0.066 <R> <C> Co-training (10k) <C> 0.7288±0.041 <C> [BOLD] 0.8238±0.064 <C> 0.7719±0.040 <C> 0.5484±0.092 <C> 0.6355±0.113 <C> 0.5851±0.090 <R> <C> Co-training (25k) <C> 0.7181±0.035 <C> 0.8005±0.048 <C> 0.7561±0.031 <C> 0.5774±0.082 <C> 0.6425±0.076 <C> 0.6051±0.066 <R> <C> Co-training (50k) <C> 0.7207±0.034 <C> 0.7870±0.042 <C> 0.7516±0.029 <C> 0.5420±0.054 <C> 0.6342±0.061 <C> 0.5836±0.053 <R> <C> Co-training (75k) <C> 0.7478±0.062 <C> 0.8033±0.053 <C> 0.7730±0.047 <C> 0.5525±0.059 <C> [BOLD] 0.6875±0.069 <C> 0.6110±0.056 <R> <C> Co-training (100k) <C> [BOLD] 0.7514±0.053 <C> 0.8045±0.056 <C> [BOLD] 0.7754±0.042 <C> 0.5548±0.064 <C> 0.6786±0.058 <C> 0.6081±0.048 <CAP> Table 1: Accuracy Comparison for Various Methods (along with Std. Deviation)
<R> <C> Datasets <C> Shared-Gen <C> Ind.-Gen <C> Disc. <R> <C> AG News <C> 90.2 <C> 90.7 <C> 40.5 <R> <C> Yelp Full <C> 51.4 <C> 52.7 <C> 20.0 <R> <C> Yelp Binary <C> 86.4 <C> 90.0 <C> 57.2 <R> <C> DBPedia <C> 95.7 <C> 94.8 <C> 8.3 <R> <C> Yahoo <C> 68.5 <C> 70.5 <C> 10.0 <CAP> Table 3: Continual learning results. Shared and Ind.-Gen are the generative shared and independent models respectively (see text for details).
<R> <C> Dataset <C> Hidden Class <C> Prec. <C> Recall <C> Acc. <R> <C> [BOLD] AG News <C> world <C> 94.4 <C> 77.8 <C> 87.8 <R> <C> [BOLD] AG News <C> sports <C> 95.7 <C> 83.3 <C> 85.5 <R> <C> [BOLD] AG News <C> business <C> 84.9 <C> 60.1 <C> 83.9 <R> <C> [BOLD] AG News <C> science and tech <C> 92.0 <C> 54.3 <C> 83.2 <R> <C> [BOLD] Sogou <C> sports <C> 95.0 <C> 80.5 <C> 87.4 <R> <C> [BOLD] Sogou <C> finance <C> 24.2 <C> 0.7 <C> 73.1 <R> <C> [BOLD] Sogou <C> entertainment <C> 90.2 <C> 78.8 <C> 86.6 <R> <C> [BOLD] Sogou <C> automobile <C> 42.9 <C> 0.7 <C> 72.1 <R> <C> [BOLD] Sogou <C> science and tech <C> 99.7 <C> 58.7 <C> 85.6 <R> <C> [BOLD] Yahoo <C> society and culture <C> 42.8 <C> 7.9 <C> 64.9 <R> <C> [BOLD] Yahoo <C> science and math <C> 48.3 <C> 9.8 <C> 63.2 <R> <C> [BOLD] Yahoo <C> health <C> 26.3 <C> 0.4 <C> 61.8 <R> <C> [BOLD] Yahoo <C> education and reference <C> 23.5 <C> 3.8 <C> 65.2 <R> <C> [BOLD] Yahoo <C> computers and internet <C> 45.4 <C> 3 <C> 60.8 <R> <C> [BOLD] Yahoo <C> sports <C> 52.9 <C> 52.9 <C> 64.6 <R> <C> [BOLD] Yahoo <C> business and finance <C> 43.6 <C> 17.3 <C> 66.2 <R> <C> [BOLD] Yahoo <C> entertainment and music <C> 44.9 <C> 2.3 <C> 63.2 <R> <C> [BOLD] Yahoo <C> family and relationships <C> 8.3 <C> 0.05 <C> 62.5 <R> <C> [BOLD] Yahoo <C> politics and government <C> 48.6 <C> 10.4 <C> 62.1 <R> <C> [BOLD] DBPedia <C> company <C> 98.9 <C> 46.6 <C> 93.3 <R> <C> [BOLD] DBPedia <C> educational institution <C> 99.2 <C> 49.5 <C> 92.8 <R> <C> [BOLD] DBPedia <C> artist <C> 88.3 <C> 4.3 <C> 90.3 <R> <C> [BOLD] DBPedia <C> athlete <C> 96.5 <C> 90.1 <C> 94.6 <R> <C> [BOLD] DBPedia <C> office holder <C> 0 <C> 0 <C> 89.1 <R> <C> [BOLD] DBPedia <C> mean of transportation <C> 96.5 <C> 74.3 <C> 94.2 <R> <C> [BOLD] DBPedia <C> building <C> 99.9 <C> 37.7 <C> 92.1 <R> <C> [BOLD] DBPedia <C> natural place <C> 98.9 <C> 88.2 <C> 95.4 <R> <C> [BOLD] DBPedia <C> village <C> 99.9 <C> 68.1 <C> 93.8 <R> <C> [BOLD] DBPedia <C> animal <C> 99.7 <C> 68.1 <C> 93.8 <R> <C> [BOLD] DBPedia <C> plant <C> 99.2 <C> 76.9 <C> 94.3 <R> <C> [BOLD] DBPedia <C> album <C> 0.03 <C> 0.001 <C> 88.8 <R> <C> [BOLD] DBPedia <C> film <C> 99.4 <C> 73.3 <C> 94.5 <R> <C> [BOLD] DBPedia <C> written work <C> 93.8 <C> 26.5 <C> 91.3 <CAP> Table 4: Zero shot learning results on four datasets. Hidden class indicates the class that is not included in the training data. We show precision and recall on test data for the hidden class, as well as accuracy for examples from all classes.
<R> <C> Classes <C> P0 <C> R0 <C> P1 <C> R1 <C> Acc. <R> <C> world+sports <C> 43.2 <C> 3.4 <C> 54.7 <C> 90.2 <C> 67.2 <R> <C> world+business <C> 55.4 <C> 75.6 <C> 25.9 <C> 2.2 <C> 67.6 <R> <C> world+science/tech <C> 40.5 <C> 5.7 <C> 38.7 <C> 47.8 <C> 61.1 <R> <C> sports+business <C> 62.3 <C> 80.7 <C> 48.3 <C> 6.6 <C> 67.6 <R> <C> sports+science/tech <C> 66.2 <C> 85.5 <C> 66.8 <C> 6.7 <C> 67.6 <R> <C> business+science/tech <C> 43.6 <C> 62.1 <C> 59.0 <C> 1.9 <C> 63.3 <CAP> Table 5: Zero-shot learning results with two hidden class on the AG News dataset. We show P0 (P1) and R0 (R1) that indicate precision and recall for hidden class one (two), as well as overall accuracy.
<R> <C> Model <C> Type <C> English-German BLEU <C> English-German chrF++ <C> English-Czech BLEU <C> English-Czech chrF++ <R> <C> bastings-etal-2017-graphbastings-etal-2017-graph <C> Single <C> 16.1 <C> - <C> 9.6 <C> - <R> <C> beck-etal-2018-graphbeck-etal-2018-graph <C> Single <C> 16.7 <C> 42.4 <C> 9.8 <C> 33.3 <R> <C> guo2019denselyguo2019densely <C> Single <C> 19.0 <C> 44.1 <C> 12.1 <C> 37.1 <R> <C> beck-etal-2018-graphbeck-etal-2018-graph <C> Ensemble <C> 19.6 <C> 45.1 <C> 11.7 <C> 35.9 <R> <C> guo2019denselyguo2019densely <C> Ensemble <C> 20.5 <C> 45.8 <C> 13.1 <C> 37.8 <R> <C> Ours <C> Single <C> [BOLD] 21.3 <C> [BOLD] 47.9 <C> [BOLD] 14.1 <C> [BOLD] 41.1 <CAP> Table 4: Main results on syntax-based machine translation.
<R> <C> Model <C> PTB Rele <C> PTB Read <C> PTB Div <C> wiki90M Rele <C> wiki90M Read <C> wiki90M Div <R> <C> VAE <C> 2.63 <C> 3.07 <C> 2.77 <C> 3.03 <C> 3.20 <C> 2.60 <R> <C> SIVAE-c <C> 2.93 <C> 3.47 <C> 2.80 <C> 3.27 <C> 3.67 <C> 2.73 <R> <C> SIVAE-i <C> 3.00 <C> 3.30 <C> 3.13 <C> 3.37 <C> 3.53 <C> 3.20 <CAP> Table 6: Human evaluation results on Relevance, Readability, and Diversity of generated paraphrases.
<R> <C> [EMPTY] <C> [BOLD] Tokens <C> [BOLD] N.a. Tokens <C> [BOLD] N.a. words <C> [BOLD] N.a. morph. <C> [BOLD] N.a./tokens <R> <C> [BOLD] Wixarika-Spanish <C> 4702 <C> 2905 <C> 790 <C> 2115 <C> 0.617 <R> <C> [BOLD] Spanish-Wixarika <C> 3594 <C> 1259 <C> 1259 <C> 0 <C> 0.350 <R> <C> [BOLD] Nahuatl-Spanish <C> 4391 <C> 1969 <C> 1111 <C> 858 <C> 0.448 <R> <C> [BOLD] Spanish-Nahuatl <C> 3380 <C> 939 <C> 939 <C> 0 <C> 0.277 <R> <C> [BOLD] Yorem Nokki-Spanish <C> 4805 <C> 2960 <C> 2238 <C> 722 <C> 0.616 <R> <C> [BOLD] Spanish-Yorem Nokki <C> 3163 <C> 836 <C> 836 <C> 0 <C> 0.264 <CAP> Table 2: Alignments of tokens, words, morphemes and their success rates for all language pairs. N.a. Tokens counts all non-aligned tokes, N.a. words counts only non-aligned words (one morpheme per token), N.a. morph. are the non-aligned morphemes. N.a./tokens is the rate of non-aligned tokens in relation to the total number of tokens.
<R> <C> baselines <C> 1 <C> Model RAE  <C> Binary 82.4 <C> Fine-grained 43.2 <C> Senti140 – <C> Subj – <R> <C> baselines <C> 2 <C> MV-RNN  <C> 82.9 <C> 44.4 <C> – <C> – <R> <C> baselines <C> 3 <C> RNTN  <C> 85.4 <C> 45.7 <C> – <C> – <R> <C> baselines <C> 4 <C> DCNN  <C> 86.8 <C> 48.5 <C> [ITALIC] 87.4 <C> – <R> <C> baselines <C> 5 <C> Paragraph-Vec  <C> 87.7 <C> [ITALIC] 48.7 <C> – <C> – <R> <C> baselines <C> 6 <C> CNN-rand  <C> 82.7 <C> 45.0 <C> – <C> 89.6 <R> <C> baselines <C> 7 <C> CNN-static  <C> 86.8 <C> 45.5 <C> – <C> 93.0 <R> <C> baselines <C> 8 <C> CNN-non-static  <C> 87.2 <C> 48.0 <C> – <C> 93.4 <R> <C> baselines <C> 9 <C> CNN-multichannel  <C> [ITALIC] 88.1 <C> 47.4 <C> – <C> 93.2 <R> <C> baselines <C> 10 <C> NBSVM  <C> – <C> – <C> – <C> 93.2 <R> <C> baselines <C> 11 <C> MNB  <C> – <C> – <C> – <C> [ITALIC] 93.6 <R> <C> [EMPTY] <C> 12 <C> G-Dropout  <C> – <C> – <C> – <C> 93.4 <R> <C> [EMPTY] <C> 13 <C> F-Dropout  <C> – <C> – <C> – <C> [ITALIC] 93.6 <R> <C> [EMPTY] <C> 14 <C> SVM  <C> – <C> – <C> 81.6 <C> – <R> <C> [EMPTY] <C> 15 <C> BINB  <C> – <C> – <C> 82.7 <C> – <R> <C> [EMPTY] <C> 16 <C> MAX-TDNN  <C> – <C> – <C> 78.8 <C> – <R> <C> [EMPTY] <C> 17 <C> NBOW  <C> – <C> – <C> 80.9 <C> – <R> <C> [EMPTY] <C> 18 <C> MAXENT  <C> – <C> – <C> 83.0 <C> – <R> <C> versions <C> 19 <C> MVCNN (-HLBL) <C> 88.5 <C> 48.7 <C> 88.0 <C> 93.6 <R> <C> versions <C> 20 <C> MVCNN (-Huang) <C> 89.2 <C> 49.2 <C> 88.1 <C> 93.7 <R> <C> versions <C> 21 <C> MVCNN (-Glove) <C> 88.3 <C> 48.6 <C> 87.4 <C> 93.6 <R> <C> versions <C> 22 <C> MVCNN (-SENNA) <C> 89.3 <C> 49.1 <C> 87.9 <C> 93.4 <R> <C> versions <C> 23 <C> MVCNN (-Word2Vec) <C> 88.4 <C> 48.2 <C> 87.6 <C> 93.4 <R> <C> filters <C> 24 <C> MVCNN (-3) <C> 89.1 <C> 49.2 <C> 88.0 <C> 93.6 <R> <C> filters <C> 25 <C> MVCNN (-5) <C> 88.7 <C> 49.0 <C> 87.5 <C> 93.4 <R> <C> filters <C> 26 <C> MVCNN (-7) <C> 87.8 <C> 48.9 <C> 87.5 <C> 93.1 <R> <C> filters <C> 27 <C> MVCNN (-9) <C> 88.6 <C> 49.2 <C> 87.8 <C> 93.3 <R> <C> tricks <C> 28 <C> MVCNN (-mutual-learning) <C> 88.2 <C> 49.2 <C> 87.8 <C> 93.5 <R> <C> tricks <C> 29 <C> MVCNN (-pretraining) <C> 87.6 <C> 48.9 <C> 87.6 <C> 93.2 <R> <C> layers <C> 30 <C> MVCNN (1) <C> 89.0 <C> 49.3 <C> 86.8 <C> 93.8 <R> <C> layers <C> 31 <C> MVCNN (2) <C> 89.4 <C> 49.6 <C> 87.6 <C> 93.9 <R> <C> layers <C> 32 <C> MVCNN (3) <C> 88.6 <C> 48.6 <C> 88.2 <C> 93.1 <R> <C> layers <C> 33 <C> MVCNN (4) <C> 87.9 <C> 48.2 <C> 88.0 <C> 92.4 <R> <C> [EMPTY] <C> 34 <C> MVCNN (overall) <C> [BOLD] 89.4 <C> [BOLD] 49.6 <C> [BOLD] 88.2 <C> [BOLD] 93.9 <CAP> Table 3: Test set results of our CNN model against other methods. RAE: Recursive Autoencoders with pretrained word embeddings from Wikipedia [Socher et al.2011b]. MV-RNN: Matrix-Vector Recursive Neural Network with parse trees [Socher et al.2012]. RNTN: Recursive Neural Tensor Network with tensor-based feature function and parse trees [Socher et al.2013]. DCNN, MAX-TDNN, NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with Max-pooling [Collobert and Weston2008], Neural Bag-of-Words Models [Kalchbrenner et al.2014]. Paragraph-Vec: Logistic regression on top of paragraph vectors [Le and Mikolov2014]. SVM, BINB, MAXENT: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maximum Entropy [Go et al.2009]. NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes with uni-bigrams from wang2012baselines. CNN-rand/static/multichannel/nonstatic: CNN with word embeddings randomly initialized / initialized by pretrained vectors and kept static during training / initialized with two copies (each is a “channel”) of pretrained embeddings / initialized with pretrained embeddings while fine-tuned during training [Kim2014]. G-Dropout, F-Dropout: Gaussian Dropout and Fast Dropout from wang2013fast. Minus sign “-” in MVCNN (-Huang) etc. means “Huang” is not used. “versions / filters / tricks / layers” denote the MVCNN variants with different setups: discard certain embedding version / discard certain filter size / discard mutual-learning or pretraining / different numbers of convolution layer.
<R> <C> [EMPTY] <C> Examples <C> [BOLD] Train 5,791 <C> [BOLD] Valid 712 <C> [BOLD] Test 736 <R> <C> [EMPTY] <C> Projects <C> 526 <C> 274 <C> 281 <R> <C> [EMPTY] <C> Edit Actions <C> 8,350 <C> 1,038 <C> 1,046 <R> <C> [EMPTY] <C> Sim (Mold, Mnew) <C> 0.773 <C> 0.778 <C> 0.759 <R> <C> [EMPTY] <C> Sim (Cold, Cnew) <C> 0.623 <C> 0.645 <C> 0.635 <R> <C> [BOLD] Code <C> Unique <C> 7,271 <C> 2,473 <C> 2,690 <R> <C> [BOLD] Code <C> Mean <C> 86.4 <C> 87.4 <C> 97.4 <R> <C> [BOLD] Code <C> Median <C> 46 <C> 49 <C> 50 <R> <C> [BOLD] Comm. <C> Unique <C> 4,823 <C> 1,695 <C> 1,737 <R> <C> [BOLD] Comm. <C> Mean <C> 10.8 <C> 11.2 <C> 11.1 <R> <C> [BOLD] Comm. <C> Median <C> 8 <C> 9 <C> 9 <CAP> Table 1: Number of examples, projects, and edit actions; average similarity between Mold and Mnew as the ratio of overlap; average similarity between Cold and Cnew as the ratio of overlap; number of unique code tokens and mean and median number of tokens in a method; and number of unique comment tokens and mean and median number of tokens in a comment.
<R> <C> [EMPTY] <C> [BOLD] Model <C> [BOLD] xMatch (%) <C> [BOLD] METEOR <C> [BOLD] BLEU-4 <C> [BOLD] SARI <C> [BOLD] GLEU <R> <C> Baselines <C> Copy <C> 0.000 <C> 34.611 <C> 46.218 <C> 19.282 <C> 35.400 <R> <C> Baselines <C> Return type subt. <C> 13.723 \mathsection <C> 43.106 \mathparagraph <C> 50.796∥ <C> 31.723 <C> 42.507∗ <R> <C> Baselines <C> Return type subst. + null <C> 13.723 \mathsection <C> 43.359 <C> [BOLD] 51.160† <C> 32.109 <C> 42.627∗ <R> <C> Models <C> Generation <C> 1.132 <C> 11.875 <C> 10.515 <C> 21.164 <C> 17.350 <R> <C> Models <C> Edit <C> 17.663 <C> 42.222 \mathparagraph <C> 48.217 <C> [BOLD] 46.376 <C> 45.060 <R> <C> Reranked models <C> Generation <C> 2.083 <C> 18.170 <C> 18.891 <C> 25.641 <C> 22.685 <R> <C> Reranked models <C> Edit <C> [BOLD] 18.433 <C> [BOLD] 44.698 <C> 50.717∥† <C> 45.486 <C> [BOLD] 46.118 <CAP> Table 2: Exact match, METEOR, BLEU-4, SARI, and GLEU scores. Scores for which the difference in performance is not statistically significant (p < 0.05) are indicated with matching symbols.
<R> <C> [BOLD] Method <C> [BOLD] Clean AM  [BOLD] Real <C> [BOLD] Clean AM  [BOLD] Simu <C> [BOLD] MCT AM  [BOLD] Real <C> [BOLD] MCT AM  [BOLD] Simu <R> <C> 2-layer LSTM <C> 15.41 <C> 13.50 <C> 14.25 <C> 12.55 <R> <C> [ITALIC] + Res-I <C> 16.18 <C> 14.41 <C> 14.99 <C> 13.06 <R> <C> [ITALIC] + Res-L <C> 16.13 <C> 13.74 <C> 14.61 <C> 12.65 <R> <C> 4-layer LSTM <C> 15.04 <C> 13.16 <C> 13.97 <C> 12.20 <R> <C> [ITALIC] + Res-I <C> 15.81 <C> 13.48 <C> 14.60 <C> 12.47 <R> <C> [ITALIC] + Res-L <C> 14.99 <C> 13.13 <C> 13.90 <C> 12.22 <R> <C> 8-layer LSTM <C> divergence <C> divergence <C> divergence <C> divergence <R> <C> [ITALIC] + Res-I <C> 15.53 <C> 13.55 <C> 14.48 <C> 12.49 <R> <C> [ITALIC] + Res-L <C> 14.67 <C> 12.75 <C> 13.62 <C> 12.04 <CAP> Table 3: CER (%) comparisons for different layers and residual connection architectures.
<R> <C> Model <C> AUC Macro <C> AUC Micro <C> F1 Macro <C> F1 Micro <C> P@K 8 <C> P@K 15 <R> <C> CAML  <C> 0.895 <C> [BOLD] 0.986 <C> [BOLD] 0.088 <C> 0.539 <C> 0.709 <C> 0.561 <R> <C> DR-CAML  <C> 0.897 <C> 0.985 <C> 0.086 <C> 0.529 <C> 0.690 <C> 0.548 <R> <C> MultiResCNN <C> [BOLD] 0.910 <C> [BOLD] 0.986 <C> 0.085 <C> [BOLD] 0.552 <C> [BOLD] 0.734 <C> [BOLD] 0.584 <R> <C> MultiResCNN <C> ±0.002 <C> ±0.001 <C> ±0.007 <C> ±0.005 <C> ±0.002 <C> ±0.001 <CAP> Table 3: MIMIC-III results (full codes). The results of MultiResCNN are shown in means ± standard deviations.
<R> <C> Model <C> AUC Macro <C> AUC Micro <C> F1 Macro <C> F1 Micro <C> P@8 <R> <C> SVM  <C> - <C> - <C> - <C> 0.293 <C> - <R> <C> HA-GRU  <C> - <C> - <C> - <C> 0.366 <C> - <R> <C> CAML  <C> 0.820 <C> 0.966 <C> 0.048 <C> 0.442 <C> 0.523 <R> <C> DR-CAML  <C> 0.826 <C> 0.966 <C> 0.049 <C> 0.457 <C> 0.515 <R> <C> MultiResCNN <C> [BOLD] 0.850 <C> [BOLD] 0.968 <C> [BOLD] 0.052 <C> [BOLD] 0.464 <C> [BOLD] 0.544 <R> <C> MultiResCNN <C> ±0.002 <C> ±0.001 <C> ±0.002 <C> ±0.002 <C> ±0.007 <CAP> Table 5: MIMIC-II results (full codes). The results of MultiResCNN are shown in means ± standard deviations.
<R> <C> Model <C> Task Wiki-MT <C> Task TED-MT <C> Task TSF-MT <C> Task TSF-PARSING <C> Task TSF-POS <C> Task TSF-EL <C> Task BLI <C> Task MA <C> Task UD <R> <C> Mean <C> 6.40 <C> 12.65 <C> 10.77 <C> 17.58 <C> 29.10 <C> 18.65 <C> 20.10 <C> 9.47 <C> 17.69 <R> <C> Transfer Lang-wise <C> – <C> – <C> 10.96 <C> 15.68 <C> 29.98 <C> 20.55 <C> – <C> – <C> – <R> <C> Source Lang-wise <C> 5.69 <C> 12.65 <C> 2.24 <C> – <C> – <C> – <C> 20.13 <C> – <C> – <R> <C> Target Lang-wise <C> 5.12 <C> 12.65 <C> 10.78 <C> 12.05 <C> 8.92 <C> 8.61 <C> 20.00 <C> 9.47 <C> – <R> <C> NLPerf (SM) <C> 2.50 <C> 6.18 <C> 1.43 <C> 6.24 <C> 7.37 <C> 7.82 <C> 12.63 <C> 6.48 <C> 12.06 <R> <C> Model-wise <C> – <C> – <C> – <C> – <C> – <C> – <C> 8.77 <C> 5.22 <C> 4.96 <R> <C> NLPerf (MM) <C> – <C> – <C> – <C> – <C> – <C> – <C> 6.87 <C> 3.18 <C> 3.54 <CAP> Table 3: RMSE scores of three baselines and our predictions under the single model and multi model setting (missing values correspond to settings not applicable to the task). All results are from k-fold (k=5) evaluations averaged over 10 random runs.
<R> <C> Predictor <C> RMSE <R> <C> Mean Baseline <C> 12.64 <R> <C> Human (w/o training data) <C> 9.38 <R> <C> Human (w/ training data) <C> 7.29 <R> <C> NLPerf <C> [BOLD] 6.04 <CAP> Table 4: Our model performs better than human MT experts on the TED-MT prediction task.
<R> <C> Alignment <C> 1st LSTM output layer nn <C> 1st LSTM output layer nn <C> 1st LSTM output layer nn <C> 1st LSTM output layer csls_knn_10 <C> 1st LSTM output layer csls_knn_10 <C> 1st LSTM output layer csls_knn_10 <C> 2nd LSTM output layer nn <C> 2nd LSTM output layer nn <C> 2nd LSTM output layer nn <C> 2nd LSTM output layer csls_knn_10 <C> 2nd LSTM output layer csls_knn_10 <C> 2nd LSTM output layer csls_knn_10 <R> <C> Alignment <C> P@1 <C> P@5 <C> P@10 <C> P@1 <C> P@5 <C> P@10 <C> P@1 <C> P@5 <C> P@10 <C> P@1 <C> P@5 <C> P@10 <R> <C> (a) Supervised Mapping <C> (a) Supervised Mapping <C> (a) Supervised Mapping <C> (a) Supervised Mapping <C> (a) Supervised Mapping <C> (a) Supervised Mapping <C> (a) Supervised Mapping <C> (a) Supervised Mapping <C> (a) Supervised Mapping <C> (a) Supervised Mapping <C> (a) Supervised Mapping <C> (a) Supervised Mapping <C> (a) Supervised Mapping <R> <C> Baseline <C> [BOLD] 55.20 <C> 73.85 <C> 80.11 <C> 68.48 <C> 84.65 <C> 88.78 <C> [BOLD] 55.95 <C> [BOLD] 73.57 <C> 79.49 <C> [BOLD] 67.17 <C> 82.31 <C> 86.79 <R> <C> Form-based removal (en) <C> 54.99 <C> 74.19 <C> 79.63 <C> [BOLD] 68.55 <C> [BOLD] 85.13 <C> 88.58 <C> 55.33 <C> 73.43 <C> 79.22 <C> 66.96 <C> [BOLD] 82.59 <C> 86.51 <R> <C> Form-based removal (fr-1) <C> 54.85 <C> [BOLD] 74.26 <C> 79.77 <C> [BOLD] 68.55 <C> 84.86 <C> 88.92 <C> 55.88 <C> 73.50 <C> [BOLD] 79.63 <C> 66.90 <C> 82.11 <C> 86.79 <R> <C> Form-based removal (fr-2) <C> 54.85 <C> 73.85 <C> [BOLD] 80.32 <C> 68.27 <C> 84.65 <C> 88.71 <C> 55.81 <C> [BOLD] 73.57 <C> [BOLD] 79.63 <C> 67.10 <C> 82.31 <C> 86.85 <R> <C> Lemma-based removal (en) <C> 55.06 <C> 74.05 <C> 79.83 <C> 68.41 <C> 85.07 <C> 88.64 <C> 55.33 <C> 73.30 <C> 79.15 <C> 66.62 <C> 82.38 <C> 86.58 <R> <C> Lemma-based removal (fr-1) <C> 54.92 <C> 74.19 <C> 79.83 <C> 68.07 <C> 84.79 <C> [BOLD] 89.13 <C> 55.82 <C> [BOLD] 73.57 <C> 79.56 <C> 66.83 <C> 82.17 <C> 86.79 <R> <C> Lemma-based removal (fr-2) <C> 54.85 <C> 73.57 <C> 80.11 <C> 68.41 <C> 84.72 <C> 88.71 <C> 55.74 <C> [BOLD] 73.57 <C> [BOLD] 79.63 <C> 66.83 <C> 82.38 <C> [BOLD] 86.99 <R> <C> (b) Unsupervised Mapping <C> (b) Unsupervised Mapping <C> (b) Unsupervised Mapping <C> (b) Unsupervised Mapping <C> (b) Unsupervised Mapping <C> (b) Unsupervised Mapping <C> (b) Unsupervised Mapping <C> (b) Unsupervised Mapping <C> (b) Unsupervised Mapping <C> (b) Unsupervised Mapping <C> (b) Unsupervised Mapping <C> (b) Unsupervised Mapping <C> (b) Unsupervised Mapping <R> <C> Baseline <C> 42.81 <C> 62.70 <C> 67.72 <C> 48.11 <C> 69.99 <C> 74.54 <C> 35.58 <C> 49.90 <C> 56.64 <C> 42.60 <C> 62.42 <C> 68.62 <R> <C> Anchors removal (en) <C> 52.44 <C> 67.38 <C> 72.06 <C> 57.88 <C> 73.43 <C> 77.22 <C> No convergence <C> No convergence <C> No convergence <C> No convergence <C> No convergence <C> No convergence <R> <C> Anchors removal (fr-1) <C> 48.59 <C> 63.11 <C> 67.65 <C> 53.68 <C> 69.37 <C> 72.61 <C> [BOLD] 47.69 <C> [BOLD] 61.73 <C> [BOLD] 67.45 <C> [BOLD] 53.34 <C> [BOLD] 70.27 <C> [BOLD] 76.05 <R> <C> Anchors removal (fr-2) <C> 45.97 <C> 60.16 <C> 64.30 <C> 50.52 <C> 65.33 <C> 69.81 <C> No convergence <C> No convergence <C> No convergence <C> No convergence <C> No convergence <C> No convergence <R> <C> Anchors removal (en & fr-1) <C> No convergence <C> No convergence <C> No convergence <C> No convergence <C> No convergence <C> No convergence <C> 36.89 <C> 51.41 <C> 57.47 <C> 41.77 <C> 60.70 <C> 67.72 <R> <C> Anchors removal (en & fr-2) <C> 51.96 <C> 68.44 <C> 73.12 <C> 58.17 <C> 75.53 <C> 79.53 <C> 33.43 <C> 45.83 <C> 50.59 <C> 39.83 <C> 53.55 <C> 59.27 <R> <C> Anchors replacement (en) <C> [BOLD] 54.71 <C> [BOLD] 70.54 <C> [BOLD] 75.02 <C> [BOLD] 60.98 <C> [BOLD] 78.32 <C> [BOLD] 82.38 <C> No convergence <C> No convergence <C> No convergence <C> No convergence <C> No convergence <C> No convergence <CAP> Table 3: Precision at k=1,5,10 of bilingual lexicon induction from the aligned cross-lingual embeddings.
<R> <C> Gaze features <C> Gaze features <C> dev set UAS <C> dev set LAS <C> test set UAS <C> test set LAS <R> <C> [ITALIC] baseline <C> [ITALIC] baseline <C> 85.36 <C> 79.40 <C> 84.37 <C> 78.24 <R> <C> [ITALIC] Basic <C> total fix dur <C> 85.34 <C> 79.35 <C> 84.06 <C> 77.44 <R> <C> [ITALIC] Basic <C> mean fix dur <C> 85.21 <C> 79.38 <C> 84.59 <C> [BOLD] 78.70 <R> <C> [ITALIC] Basic <C> [ITALIC] n fix <C> 85.32 <C> 79.29 <C> 83.71 <C> 77.57 <R> <C> [ITALIC] Basic <C> fix prob <C> 85.32 <C> 79.57 <C> 84.33 <C> 77.91 <R> <C> [ITALIC] Basic <C> basic feats aux <C> 85.36 <C> 79.57 <C> 83.86 <C> 77.75 <R> <C> [ITALIC] Early <C> first fix dur <C> 85.30 <C> 79.46 <C> [BOLD] 84.64 <C> 78.57 <R> <C> [ITALIC] Early <C> first pass dur <C> 85.50 <C> 79.49 <C> 84.55 <C> 78.39 <R> <C> [ITALIC] Early <C> early feats aux <C> [BOLD] 85.61 <C> 79.57 <C> 84.37 <C> 78.11 <R> <C> [ITALIC] Late <C> [ITALIC] n re-fix <C> 85.52 <C> 79.25 <C> 83.86 <C> 77.91 <R> <C> [ITALIC] Late <C> reread prob <C> 85.34 <C> 79.57 <C> 83.86 <C> 77.37 <R> <C> [ITALIC] Late <C> late feats aux <C> 85.54 <C> 79.64 <C> 84.10 <C> 77.73 <R> <C> [ITALIC] Context <C> [ITALIC] w−1 fix prob <C> 85.17 <C> 79.47 <C> 84.26 <C> 77.93 <R> <C> [ITALIC] Context <C> [ITALIC] w+1 fix prob <C> 85.36 <C> 79.07 <C> 84.24 <C> 78.06 <R> <C> [ITALIC] Context <C> [ITALIC] w−1 fix dur <C> 85.43 <C> 79.68 <C> 84.50 <C> 77.95 <R> <C> [ITALIC] Context <C> [ITALIC] w+1 fix dur <C> 85.39 <C> 79.53 <C> [BOLD] 84.64 <C> 78.30 <R> <C> [EMPTY] <C> context feats aux <C> [BOLD] 85.61 <C> [BOLD] 79.72 <C> 84.33 <C> 78.24 <CAP> Table 1: Impact of various gaze features as auxiliary task(s) on the score (UAS/LAS) of dependency parsing as the main task evaluated on Dundee treebank (parallel setup).
<R> <C> Gaze features <C> Gaze features <C> dev set UAS <C> dev set LAS <C> test set UAS <C> test set LAS <R> <C> [ITALIC] baseline <C> [ITALIC] baseline <C> 93.98 <C> 91.67 <C> 93.86 <C> 91.80 <R> <C> [ITALIC] Basic <C> total fix dur <C> 93.94 <C> 91.60 <C> 93.99 <C> 91.92 <R> <C> [ITALIC] Basic <C> mean fix dur <C> [BOLD] 94.12 <C> [BOLD] 91.84 <C> 93.95 <C> 91.82 <R> <C> [ITALIC] Basic <C> [ITALIC] n fix <C> 93.97 <C> 91.70 <C> 93.91 <C> 91.87 <R> <C> [ITALIC] Basic <C> fix prob <C> 93.98 <C> 91.71 <C> 93.99 <C> 91.93 <R> <C> [EMPTY] <C> basic feats aux <C> 94.00 <C> 91.69 <C> 93.84 <C> 91.81 <R> <C> [ITALIC] Early <C> first fix dur <C> 94.07 <C> 91.81 <C> 93.87 <C> 91.80 <R> <C> [ITALIC] Early <C> first pass dur <C> 93.93 <C> 91.58 <C> 93.79 <C> 91.70 <R> <C> [EMPTY] <C> early feats aux <C> 94.04 <C> 91.78 <C> 93.96 <C> 91.88 <R> <C> [ITALIC] Late <C> [ITALIC] n re-fix <C> 94.01 <C> 91.69 <C> 93.87 <C> 91.79 <R> <C> [ITALIC] Late <C> reread prob <C> 94.03 <C> 91.74 <C> 93.98 <C> 91.89 <R> <C> [EMPTY] <C> late feats aux <C> 93.98 <C> 91.58 <C> 93.92 <C> 91.90 <R> <C> [ITALIC] Context <C> [ITALIC] w−1 fix prob <C> 94.02 <C> 91.65 <C> 93.95 <C> 91.93 <R> <C> [ITALIC] Context <C> [ITALIC] w+1 fix prob <C> 93.88 <C> 91.61 <C> 93.89 <C> 91.82 <R> <C> [ITALIC] Context <C> [ITALIC] w−1 fix dur <C> 94.06 <C> 91.65 <C> 93.86 <C> 91.83 <R> <C> [ITALIC] Context <C> [ITALIC] w+1 fix dur <C> 93.91 <C> 91.69 <C> 93.89 <C> 91.84 <R> <C> [EMPTY] <C> context feats aux <C> 93.93 <C> 91.63 <C> [BOLD] 94.01 <C> [BOLD] 91.98 <CAP> Table 2: Results for dependency parsing evaluated on PTB treebank with gaze features as auxiliary task(s) learned from the disjoint dataset: Dundee treebank.
<R> <C> [BOLD] Dataset <C> [BOLD] Type <C> [BOLD] Annotated Articles <R> <C> NCBI Diesase <C> Disease <C> 793 <R> <C> CTD-Pfizer dataset <C> Drug <C> 18,410 <R> <C> ScisummNet <C> General <C> 1,009 <R> <C> CORD-19 <C> COVID-19 <C> 57,037 <CAP> Table 1: Scientific literature dataset with either annotated abstracts or original summaries, including CORD-19 used for our model (as of June 27)
<R> <C> [EMPTY] <C> [BOLD] Weighted average  [ITALIC] F1  [BOLD] UA <C> [BOLD] Weighted average  [ITALIC] F1  [BOLD] UQ <C> [BOLD] Weighted average  [ITALIC] F1  [BOLD] UD <C> [BOLD] Macro-average  [ITALIC] F1  [BOLD] UA <C> [BOLD] Macro-average  [ITALIC] F1  [BOLD] UQ <C> [BOLD] Macro-average  [ITALIC] F1  [BOLD] UD <R> <C> [BOLD] ETS1 <C> 0.535 <C> 0.487 <C> 0.447 <C> 0.467 <C> 0.372 <C> 0.334 <R> <C> [BOLD] ETS2 <C> 0.625 <C> 0.356 <C> 0.434 <C> 0.581 <C> 0.274 <C> 0.339 <R> <C> [BOLD] Best Performance in SRA <C> 0.625 <C> 0.492 <C> 0.471 <C> 0.581 <C> 0.384 <C> 0.375 <R> <C> [BOLD] Proposed <C> [BOLD] 0.672 <C> [BOLD] 0.518 <C> [BOLD] 0.507 <C> [BOLD] 0.612 <C> [BOLD] 0.415 <C> [BOLD] 0.402 <CAP> TABLE III: Comparison of F1 scores (higher the better) of the proposed technique against the transfer learning based ASAG technique (ETS1 and ETS2) [12] and the best performance obtained in the SRA task. Existing results are from [13].
<R> <C> [EMPTY] <C> [BOLD] CSD <C> [BOLD] X-CSD <C> [BOLD] RCD <R> <C> [BOLD] SUP-BL <C> 0.95 <C> 0.85 <C> 1.74 <R> <C> [BOLD] SUP-SL <C> 0.66 <C> 0.54 <C> 0.83 <R> <C> [BOLD] Proposed <C> 0.67 <C> 0.82 <C> 0.88 <CAP> TABLE IV: Overall performance (MAE; lower the better) of the proposed technique along with the baseline and the skyline performances on the three data sets. Sup-SL requires labelled data for all questions unlike Sup-BL and the proposed technique.
<R> <C> Algorithm <C> Homogeneity <C> Completeness <C> V-measure <R> <C> Our approach <C> [BOLD] 0.960 <C> 0.965 <C> [BOLD] 0.962 <R> <C> KeyGraph <C> 0.554 <C> [BOLD] 0.989 <C> 0.710 <R> <C> LDA + AP <C> 0.620 <C> 0.947 <C> 0.749 <CAP> Table 2. Comparing different event clustering methods.
<R> <C> Example: <C> [BOLD] (a) <C> [BOLD] (b) <R> <C> bleu <C> 61.80 <C> 81.33 <R> <C> meteor <C> 86.91 <C> 92.63 <CAP> Table 2: Two example alignments and their respective BLEU and METEOR scores, assuming that the reference alignment is monotonic. The permutation resulting from the hypothesis alignment is reported under each matrix, where bullet points represent jumps between non-sequential indices. Taken from Birch:11:thesis.
<R> <C> Model <C> Accuracy <R> <C> NNC <C> 0.759 <R> <C> Word-CNN <C> 0.756 <R> <C> Char-CNN <C> [BOLD] 0.772 <R> <C> Frequency-based <C> 0.155 <CAP> Table 1. Model accuracy of intent detection.
<R> <C> [EMPTY] <C> w/o adaptation <C> with adaptatation <R> <C> x-vector clustering <C> 11.53 <C> [EMPTY] <R> <C> BLSTM-EEND <C> [EMPTY] <C> [EMPTY] <R> <C> trained with sim. <C> 43.84 <C> 26.03 <R> <C> trained with real <C> 31.01 <C> 23.07 <R> <C> SA-EEND <C> [EMPTY] <C> [EMPTY] <R> <C> trained with sim. <C> 17.42 <C> 13.66 <R> <C> trained with real <C> 12.66 <C> [BOLD] 10.76 <CAP> Table 3: DERs (%) on the CALLHOME with and without domain adaptation.
<R> <C> dataset <C> CHRF3 <R> <C> Deepcut <C> 47.90 <R> <C> + 1000 <C> 47.00 <R> <C> + 5000 <C> [BOLD] 51.25 <R> <C> + 10000 <C> [BOLD] 54.94 <R> <C> + 20000 <C> [BOLD] 54.96 <CAP> 3: Performance of combination of dataset with different split using Deepcut. Numbers in bold indicate statistical significant improvement at p=0.01.
<R> <C> [EMPTY] <C> [BOLD] WMT-14  [BOLD] FR-EN <C> [BOLD] WMT-14  [BOLD] EN-FR <C> [BOLD] WMT-14  [BOLD] DE-EN <C> [BOLD] WMT-14  [BOLD] EN-DE <C> [BOLD] WMT-16  [BOLD] DE-EN <C> [BOLD] WMT-16  [BOLD] EN-DE <R> <C> Artetxe et al. ( 2018c ) <C> 15.56 <C> 15.13 <C> 10.21 <C> 6.55 <C> - <C> - <R> <C> Lample et al. ( 2018 ) <C> 14.31 <C> 15.05 <C> - <C> - <C> 13.33 <C> 9.64 <R> <C> Yang et al. ( 2018 ) <C> 15.58 <C> 16.97 <C> - <C> - <C> 14.62 <C> 10.86 <R> <C> Proposed system <C> [BOLD] 25.87 <C> [BOLD] 26.22 <C> [BOLD] 17.43 <C> [BOLD] 14.08 <C> [BOLD] 23.05 <C> [BOLD] 18.23 <CAP> Table 1: Results of the proposed method in comparison to existing unsupervised NMT systems (BLEU).
<R> <C> [EMPTY] <C> [BOLD] WMT-14  [BOLD] FR-EN <C> [BOLD] WMT-14  [BOLD] EN-FR <C> [BOLD] WMT-14  [BOLD] DE-EN <C> [BOLD] WMT-14  [BOLD] EN-DE <C> [BOLD] WMT-16  [BOLD] DE-EN <C> [BOLD] WMT-16  [BOLD] EN-DE <R> <C> Unsupervised SMT <C> 21.16 <C> 20.13 <C> 13.86 <C> 10.59 <C> 18.01 <C> 13.22 <R> <C> + unsupervised tuning <C> 22.17 <C> 22.22 <C> 14.73 <C> 10.64 <C> 18.21 <C> 13.12 <R> <C> + iterative refinement (it1) <C> 24.81 <C> 26.53 <C> 16.01 <C> 13.45 <C> 20.76 <C> 16.94 <R> <C> + iterative refinement (it2) <C> [BOLD] 26.13 <C> [BOLD] 26.57 <C> 17.30 <C> 13.95 <C> 22.80 <C> 18.18 <R> <C> + iterative refinement (it3) <C> 25.87 <C> 26.22 <C> [BOLD] 17.43 <C> [BOLD] 14.08 <C> [BOLD] 23.05 <C> [BOLD] 18.23 <CAP> Table 2: Ablation results (BLEU). The last row corresponds to our full system. Refer to the text for more details.
<R> <C> [EMPTY] <C> [BOLD] Avg <C> [BOLD] Single Sentence  [BOLD] CoLA <C> [BOLD] Single Sentence  [BOLD] SST-2 <C> [BOLD] Sentence Similarity  [BOLD] MRPC <C> [BOLD] Sentence Similarity  [BOLD] STS-B <C> [BOLD] Sentence Similarity  [BOLD] QQP <C> [BOLD] Natural Language Inference  [BOLD] MNLI <C> [BOLD] Natural Language Inference  [BOLD] QNLI <C> [BOLD] Natural Language Inference  [BOLD] RTE <C> [BOLD] Natural Language Inference  [BOLD] WNLI <R> <C> [ITALIC] Training Size <C> - <C> [ITALIC] 8.5k <C> [ITALIC] 67k <C> [ITALIC] 3.7k <C> [ITALIC] 7k <C> [ITALIC] 364k <C> [ITALIC] 393k <C> [ITALIC] 108k <C> [ITALIC] 2.5k <C> [ITALIC] 634 <R> <C> Human <C> [BOLD] 87.1 <C> [BOLD] 66.4 <C> [BOLD] 97.8 <C> 80.8/86.3 <C> [BOLD] 92.7/92.6 <C> 80.4/59.5 <C> [BOLD] 92.0/92.8 <C> 91.2 <C> [BOLD] 93.6 <C> [BOLD] 95.9 <R> <C> BERT <C> 80.5 <C> 60.5 <C> 94.9 <C> 85.4/89.3 <C> 87.6/86.5 <C> [BOLD] 89.3/72.1 <C> 86.7/85.9 <C> [BOLD] 92.7 <C> 70.1 <C> 65.1 <R> <C> BigBird <C> 83.9 <C> 65.4 <C> 95.6 <C> [BOLD] 88.2/91.1 <C> 89.5/89.0 <C> [BOLD] 89.6/72.7 <C> 87.9/87.4 <C> 95.8* <C> 85.1 <C> 65.1 <R> <C> Δ [ITALIC] bert ( -) <C> 6.6 <C> 5.9 <C> 2.9 <C> -4.6/-3.0 <C> 5.1/6.1 <C> -8.9/-12.6 <C> 5.3/6.9 <C> -1.5 <C> 23.5 <C> 30.8 <R> <C> Δ [ITALIC] bird ( -) <C> 3.2 <C> 1.0 <C> 2.2 <C> -7.4/-4.8 <C> 3.2/3.6 <C> -9.2/-13.2 <C> 4.1/5.4 <C> -4.6* <C> 8.5 <C> 30.8 <R> <C> [ITALIC] Performance on subset with 5-way annotator agreement <C> [ITALIC] Performance on subset with 5-way annotator agreement <C> [ITALIC] Performance on subset with 5-way annotator agreement <C> [ITALIC] Performance on subset with 5-way annotator agreement <C> [ITALIC] Performance on subset with 5-way annotator agreement <C> [ITALIC] Performance on subset with 5-way annotator agreement <C> [ITALIC] Performance on subset with 5-way annotator agreement <C> [ITALIC] Performance on subset with 5-way annotator agreement <C> [ITALIC] Performance on subset with 5-way annotator agreement <C> [ITALIC] Performance on subset with 5-way annotator agreement <C> [ITALIC] Performance on subset with 5-way annotator agreement <R> <C> Human <C> 93.7 <C> 83.6 <C> 100.0 <C> 90.2/93.6 <C> 98.9/94.7 <C> 89.4/74.1 <C> 98.5/99.2 <C> 95.1 <C> 97.4 <C> 97.5 <R> <C> BERT <C> 83.5 <C> 69.2 <C> 97.5 <C> 88.9/92.7 <C> 95.8/82.3 <C> 92.5/78.0 <C> 96.4/90.8 <C> 93.6 <C> 73.0 <C> 59.3 <R> <C> Δ ( -) <C> 10.2 <C> 14.4 <C> 2.5 <C> 1.3/0.9 <C> 3.1/12.4 <C> -3.1/-3.9 <C> 2.1/8.4 <C> 1.5 <C> 24.4 <C> 38.2 <R> <C> [ITALIC] BERT fine-tuned on less data <C> [ITALIC] BERT fine-tuned on less data <C> [ITALIC] BERT fine-tuned on less data <C> [ITALIC] BERT fine-tuned on less data <C> [ITALIC] BERT fine-tuned on less data <C> [ITALIC] BERT fine-tuned on less data <C> [ITALIC] BERT fine-tuned on less data <C> [ITALIC] BERT fine-tuned on less data <C> [ITALIC] BERT fine-tuned on less data <C> [ITALIC] BERT fine-tuned on less data <C> [ITALIC] BERT fine-tuned on less data <R> <C> BERT-5000 <C> 75.8 <C> 57.6 <C> 92.0 <C> 85.4/89.3 <C> 87.1/85.8 <C> 82.2/61.0 <C> 76.4/76.9 <C> 89.2 <C> 69.2 <C> 65.1 <R> <C> BERT-1000 <C> 70.7 <C> 49.0 <C> 90.4 <C> 78.5/84.3 <C> 83.6/82.3 <C> 77.8/55.8 <C> 66.5/68.3 <C> 86.6 <C> 65.6 <C> 65.1 <R> <C> BERT-500 <C> 68.5 <C> 37.2 <C> 88.1 <C> 74.0/80.7 <C> 77.3/75.2 <C> 75.4/51.2 <C> 61.8/63.0 <C> 85.7 <C> 61.5 <C> 65.1 <CAP> Table 1: GLUE test set results. The Human baseline numbers are estimated using no more than 500 test examples. All the BERT scores are for BERT-large. As in the original GLUE paper, we report the Matthews correlation coefficient for CoLA. For MRPC and Quora, we report accuracy then F1. For STS-B, we report Pearson then Spearman correlation. For MNLI, we report accuracy on the matched then mismatched test sets. For all other tasks we report accuracy. The Avg column shows the overall GLUE score: an average across each row, weighting each task equally. The Δbert and Δbird rows show the difference between the Human performance baseline and BERT and BigBird respectively. The starred(*) numbers for BigBird on QNLI show performance on the new version of QNLI, while all other QNLI numbers are on the original version. The second section shows Human and BERT performance on the subset of the test set where there is unanimous, 5-way annotator agreement, the Δ row is the difference between them. Training Size gives the number of examples in the full training set for each task. The BERT-5000/1000/500 rows show test set results for BERT fine-tuned on no more than 5k, 1k, and 500 examples respectively. Though MRPC and RTE have fewer than 5k examples, we rerun BERT fine-tuning and report these results in the BERT-5000 row.
<R> <C> Features <C> Precision <C> Recall <C> F1 <R> <C> dialogue act <C> 0.64 <C> 0.69 <C> 0.66 <R> <C> intent <C> 0.63 <C> 0.74 <C> 0.68 <R> <C> topics <C> 0.62 <C> 0.71 <C> 0.66 <R> <C> sentiment <C> 0.56 <C> 0.74 <C> 0.64 <R> <C> entities types <C> 0.63 <C> 0.74 <C> 0.68 <R> <C> All <C> 0.63 <C> 0.65 <C> 0.64 <R> <C> All - sentiment <C> 0.64 <C> 0.73 <C> 0.68 <CAP> Table 2: Feature-based Model Evaluation
<R> <C> Model <C> Acc. <C> F1 <R> <C> DER (PDTB) <C> 0.61 <C> 0.51 <R> <C> Logistic Reg. (Edina-DR) <C> 0.64 <C> 0.68 <R> <C> DER (Edina-DR) <C> 0.80 <C> 0.76 <R> <C> DER+Dialogue (Edina-DR) <C> [BOLD] 0.81 <C> [BOLD] 0.77 <CAP> Table 3: Performance of Deep Learning Models (Dataset name is shown in parentheses)
<R> <C> [BOLD] Methods <C> [BOLD] Performance on Benchmarks (%↑) IIIT5k <C> [BOLD] Performance on Benchmarks (%↑) SVT <C> [BOLD] Performance on Benchmarks (%↑) IC13 <C> [BOLD] Performance on Benchmarks (%↑) IC15 <C> [BOLD] Performance on Benchmarks (%↑) SVTP <C> [BOLD] Performance on Benchmarks (%↑) CUTE <C> [BOLD] Performance on Benchmarks (%↑) TotalText <C> [BOLD] Speed (FPS↑) Train <C> [BOLD] Speed (FPS↑) Test (GPU) <C> [BOLD] Speed (FPS↑) Test (CPU) <R> <C> Vanilla CTC <C> 92.2 <C> 86.9 <C> 91.2 <C> 69.8 <C> 75.6 <C> 77.1 <C> 58.3 <C> 1.42 <C> 41.63 <C> 4.55 <R> <C> Vanilla CTC + Attention <C> 91.1 <C> 88.3 <C> 91.0 <C> 70.7 <C> 76.2 <C> 78.3 <C> 60.4 <C> 1.40 <C> 39.47 <C> 4.31 <R> <C> Attention Decoder  <C> 94.4 <C> 90.5 <C> 92.6 <C> 75.8 <C> 78.3 <C> 80.4 <C> 61.3 <C> 1.15 <C> 11.35 <C> 1.13 <R> <C> 2D-CTC <C> 94.7 <C> 90.6 <C> 93.9 <C> 75.2 <C> 79.2 <C> 81.3 <C> 63.0 <C> 1.36 <C> 36.22 <C> 3.96 <CAP> Table 2: Results of comparative experiments. The speed of models is measured in frames per second (FPS), which indicates how many instances the model can handle in a second. Notably, larger numbers indicate higher efficiency.
<R> <C> Loss Function <C> Device <C> Time (ms↓) 1 <C> Time (ms↓) 256 <R> <C> Vanilla CTC <C> CPU <C> 0.12 <C> 1.24 <R> <C> Vanilla CTC <C> GPU <C> 0.23 <C> 0.24 <R> <C> 2D-CTC <C> CPU <C> 0.2 <C> 1.98 <R> <C> 2D-CTC <C> GPU <C> 0.26 <C> 0.27 <CAP> Table 3: Computation time costs of vanilla CTC and 2D-CTC on different devices. “1” and “256” are the batch sizes.
<R> <C> [EMPTY] <C> BLEU-4 <R> <C> Seq2Seq <C> 20.91 <R> <C> Seq2Seq + Copy <C> 24.12 <R> <C> Tree2Seq <C> 26.67 <R> <C> GCN-PGE <C> 35.99 <R> <C> GGS-NN <C> 35.53 <R> <C> Graph2Seq-NGE <C> 34.28 <R> <C> Graph2Seq-PGE <C> [BOLD] 38.97 <CAP> Table 2: Results on WikiSQL.
<R> <C> [EMPTY] <C> [BOLD] Flickr30K B-1 <C> [BOLD] Flickr30K B-2 <C> [BOLD] Flickr30K B-3 <C> [BOLD] Flickr30K B-4 <C> [BOLD] COCO B-1 <C> [BOLD] COCO B-2 <C> [BOLD] COCO B-3 <C> [BOLD] COCO B-4 <R> <C> Human agreement <C> 0.55 <C> 0.35 <C> 0.23 <C> 0.15 <C> 0.68 <C> 0.45 <C> 0.30 <C> 0.20 <R> <C> Mao et al. ( 2014 ) <C> 0.55 <C> 0.24 <C> 0.19 <C> - <C> - <C> - <C> - <C> - <R> <C> Karpathy & Fei-Fei ( 2014 ) <C> 0.50 <C> 0.30 <C> 0.15 <C> - <C> 0.57 <C> 0.37 <C> 0.19 <C> - <R> <C> Vinyals et al. ( 2014 ) <C> 0.66 <C> - <C> - <C> - <C> 0.67 <C> - <C> - <C> - <R> <C> Donahue et al. ( 2014 ) <C> 0.59 <C> 0.39 <C> 0.25 <C> 0.16 <C> 0.63 <C> 0.44 <C> 0.30 <C> 0.21 <R> <C> Fang et al. ( 2014 ) <C> - <C> - <C> - <C> - <C> - <C> - <C> - <C> 0.21 <R> <C> Our model <C> 0.59 <C> 0.35 <C> 0.20 <C> 0.12 <C> 0.70 <C> 0.46 <C> 0.30 <C> 0.20 <CAP> Table 3: Comparison between human agreement scores, state of the art models and our model on both datasets. Note that there are slight variations between the test sets chosen in each paper.
<R> <C> [BOLD] System <C> [BOLD] QR <C> [BOLD] MAP  [BOLD] Dev <C> [BOLD] MAP  [BOLD] Test <C> Δ <R> <C> 1. Keyword(KW) (Baseline) <C> EN <C> 72.60 <C> 71.43 <C> 00.00 <R> <C> 2. Word Embedding(WE) <C> EN <C> 64.40 <C> 63.86 <C> -07.57 <R> <C> 3. DBpedia(DB) <C> EN <C> 41.00 <C> 45.29 <C> -26.14 <R> <C> 4. Hypernym(HN) <C> EN <C> 21.00 <C> 27.86 <C> -34.57 <R> <C> 5. 1 + 2 (KW+WE) <C> EN <C> 80.20 <C> 79.86 <C> +08.43 <R> <C> 6. 1 + 3 (KW+DB) <C> EN <C> 76.00 <C> 75.29 <C> +03.86 <R> <C> 7. 1 + 4 (KW+HN) <C> EN <C> 75.20 <C> 76.00 <C> +04.57 <R> <C> 8. 2 + 3 + 4 (WE+DB+HN) <C> EN <C> 72.20 <C> 75.86 <C> +04.43 <R> <C> [BOLD] 9. 1 + 2 + 3 + 4 (Best) <C> EN <C> [BOLD] 84.00 <C> [BOLD] 82.00 <C> +10.57 <R> <C> 10. UH-PRHLT(SemEval(Nakov et al.,  2016 ; Franco-Salvador et al.,  2018 )) <C> EN <C> 75.90 <C> 76.70 <C> - <R> <C> 11. SVM + TK (Da San Martino et al.,  2017 ) <C> EN <C> 73.02 <C> 77.41 <C> - <R> <C> 12. Keyword(KW) (Baseline) <C> MT <C> 72.20 <C> 67.57 <C> 00.00 <R> <C> 13. Word Embedding(WE) <C> MT <C> 64.40 <C> 63.43 <C> -04.14 <R> <C> 14. DBpedia(DB) <C> MT <C> 43.20 <C> 45.71 <C> -21.86 <R> <C> 15. Hypernym(HN) <C> MT <C> 26.80 <C> 32.71 <C> -34.86 <R> <C> 16. 12 + 13 (KW+WE) <C> MT <C> 79.20 <C> 75.57 <C> +08.00 <R> <C> 17. 12 + 14 (KW+DB) <C> MT <C> 75.40 <C> 71.43 <C> +03.86 <R> <C> 18. 12 + 15 (KW+HN) <C> MT <C> 76.40 <C> 72.29 <C> +04.72 <R> <C> 19. 13 + 14 + 15 (WE+DB+HM) <C> MT <C> 77.60 <C> 73.14 <C> +05.57 <R> <C> [BOLD] 20. 12 + 13 + 14 + 15(Best) <C> MT <C> [BOLD] 84.00 <C> [BOLD] 78.29 <C> +10.72 <R> <C> 21. SVM+TK((Da San Martino et al.,  2017 )) <C> MT <C> 72.94 <C> 76.67 <C> - <CAP> Table 1. MAP scores for various QE on English (EN) questions (monolingual setup) and MT questions (CLIR setup).
<R> <C> Dataset <C> Metrics <C> [ITALIC] EXAM <C> [ITALIC] SLEEC <C> [ITALIC] DXML <C> [ITALIC] HR-DGCNN <C> [ITALIC] HMCN-F <C> [ITALIC] HyperIM <R> <C> [EMPTY] <C> [ITALIC] P@1 <C> 95.98 <C> 94.45 <C> 95.27 <C> 95.17 <C> 95.35 <C> [BOLD] 96.78 <R> <C> [EMPTY] <C> [ITALIC] P@3 <C> 80.83 <C> 78.60 <C> 77.86 <C> 80.32 <C> 78.95 <C> [BOLD] 81.46 <R> <C> [ITALIC] RCV1 <C> [ITALIC] P@5 <C> 55.80 <C> 54.24 <C> 53.44 <C> 55.38 <C> 55.90 <C> [BOLD] 56.79 <R> <C> [EMPTY] <C> [ITALIC] nDCG@3 <C> 90.74 <C> 90.05 <C> 89.69 <C> 90.02 <C> 90.14 <C> [BOLD] 91.52 <R> <C> [EMPTY] <C> [ITALIC] nDCG@5 <C> 91.26 <C> 90.32 <C> 90.24 <C> 90.28 <C> 90.82 <C> [BOLD] 91.89 <R> <C> [EMPTY] <C> [ITALIC] P@1 <C> 51.41 <C> 51.34 <C> 50.34 <C> 50.97 <C> 50.24 <C> [BOLD] 52.14 <R> <C> [EMPTY] <C> [ITALIC] P@3 <C> 32.81 <C> 32.56 <C> 31.21 <C> 32.41 <C> 32.18 <C> [BOLD] 33.66 <R> <C> [ITALIC] Zhihu <C> [ITALIC] P@5 <C> 24.29 <C> 24.23 <C> 23.36 <C> 23.87 <C> 24.09 <C> [BOLD] 24.99 <R> <C> [EMPTY] <C> [ITALIC] nDCG@3 <C> 49.32 <C> 49.27 <C> 47.92 <C> 49.02 <C> 48.36 <C> [BOLD] 50.13 <R> <C> [EMPTY] <C> [ITALIC] nDCG@5 <C> 50.74 <C> 49.71 <C> 48.65 <C> 49.91 <C> 49.21 <C> [BOLD] 51.05 <R> <C> [EMPTY] <C> [ITALIC] P@1 <C> 54.90 <C> 53.57 <C> 52.02 <C> 52.67 <C> 53.23 <C> [BOLD] 55.06 <R> <C> [EMPTY] <C> [ITALIC] P@3 <C> 30.50 <C> 31.25 <C> 30.57 <C> 30.13 <C> 29.32 <C> [BOLD] 31.73 <R> <C> [ITALIC] WikiLSHTC <C> [ITALIC] P@5 <C> 22.02 <C> 22.46 <C> 21.66 <C> 22.85 <C> 21.79 <C> [BOLD] 23.08 <R> <C> [EMPTY] <C> [ITALIC] nDCG@3 <C> 49.50 <C> 46.06 <C> 47.97 <C> 49.24 <C> 48.93 <C> [BOLD] 50.46 <R> <C> [EMPTY] <C> [ITALIC] nDCG@5 <C> 50.46 <C> 47.52 <C> 48.14 <C> 50.42 <C> 49.87 <C> [BOLD] 51.36 <CAP> Table 2: Results in P@k and nDCG@k, bold face indicates the best in each line.
<R> <C> base <C> bias <C> aux <C> aux+ratio <R> <C> 0.681 <C> [BOLD] 0.729 <C> 0.665 <C> 0.662 <CAP> Table 2: Correlation (avg. over 10 runs) between word length and attention (p-value for Pearson coefficient is 0 for each run) for methods base, bias, aux, and aux+ratio.
<R> <C> Dataset <C> # Q <C> # SQL <C> # DB <C> # Domain <C> # Table / DB <C> ORDER BY <C> GROUP BY <C> NESTED <C> HAVING <R> <C> ATIS <C> 5,280 <C> 947 <C> 1 <C> 1 <C> 32 <C> 0 <C> 5 <C> 315 <C> 0 <R> <C> GeoQuery <C> 877 <C> 247 <C> 1 <C> 1 <C> 6 <C> 20 <C> 46 <C> 167 <C> 9 <R> <C> Scholar <C> 817 <C> 193 <C> 1 <C> 1 <C> 7 <C> 75 <C> 100 <C> 7 <C> 20 <R> <C> Academic <C> 196 <C> 185 <C> 1 <C> 1 <C> 15 <C> 23 <C> 40 <C> 7 <C> 18 <R> <C> IMDB <C> 131 <C> 89 <C> 1 <C> 1 <C> 16 <C> 10 <C> 6 <C> 1 <C> 0 <R> <C> Yelp <C> 128 <C> 110 <C> 1 <C> 1 <C> 7 <C> 18 <C> 21 <C> 0 <C> 4 <R> <C> Advising <C> 3,898 <C> 208 <C> 1 <C> 1 <C> 10 <C> 15 <C> 9 <C> 22 <C> 0 <R> <C> Restaurants <C> 378 <C> 378 <C> 1 <C> 1 <C> 3 <C> 0 <C> 0 <C> 4 <C> 0 <R> <C> WikiSQL <C> 80,654 <C> 77,840 <C> 26,521 <C> - <C> 1 <C> 0 <C> 0 <C> 0 <C> 0 <R> <C> [BOLD] Spider <C> 10,181 <C> 5,693 <C> 200 <C> 138 <C> 5.1 <C> 1335 <C> 1491 <C> 844 <C> 388 <CAP> Table 1: Comparisons of text-to-SQL datasets. Spider is the only one text-to-SQL dataset that contains both databases with multiple tables in different domains and complex SQL queries. It was designed to test the ability of a system to generalize to not only new SQL queries and database schemas but also new domains.
<R> <C> Model <C> MM- [ITALIC] F1 <C> EM <C> IN <R> <C> Basic s2s <C> 75.2 <C> 66.3 <C> 70.8 <R> <C> Dual-source Transformer <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Single <C> 79.5 <C> [BOLD] 73.3 <C> [BOLD] 73.9 <R> <C> Ensemble <C> [BOLD] 80.0 <C> 73.1 <C> 73.8 <CAP> Table 3: Results on WikiReading Recycled. We chose the model with the highest score on the validation set for the final submission (single) and ensemble of the four best-performing checkpoints. MM-F1 stands for Mean-MultiLabel-F1, EM for exact-match subset, and IN for inferable property subset. Note that subsets were evaluated with recall instead of MM-F1.
<R> <C> [BOLD] Model <C> ATIS Intent <C> ATIS <C> ATIS <C> SNIPS <C> SNIPS <C> SNIPS <R> <C> (Acc) <C> Sent. <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> (Acc) <C> Slot <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> (F1) <C> Intent <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> (Acc) <C> Sent. <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> (Acc) <C> Slot <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> (F1) <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> Joint Seq. Hakkani-Tür et al. ( 2016 ) <C> 92.60 <C> 80.70 <C> 94.30 <C> 96.90 <C> 73.20 <C> 87.30 <R> <C> Atten.-Based (Liu and Lane,  2016 ) <C> 91.10 <C> 78.90 <C> 94.20 <C> 96.70 <C> 74.10 <C> 87.80 <R> <C> Sloted-Gated (Goo et al.,  2018 ) <C> 95.41 <C> 83.73 <C> 95.42 <C> 96.86 <C> 76.43 <C> 89.27 <R> <C> Capsule-NLU (Zhang et al.,  2019 ) <C> 95.00 <C> 83.40 <C> 95.20 <C> 97.30 <C> 80.90 <C> 91.80 <R> <C> Interrelated SF-First (E et al.,  2019 ) <C> 97.76 <C> 86.79 <C> 95.75 <C> 97.43 <C> 80.57 <C> 91.43 <R> <C> Interrelated ID-First (E et al.,  2019 ) <C> 97.09 <C> 86.90 <C> 95.80 <C> 97.29 <C> 80.43 <C> 92.23 <R> <C> [ITALIC] BERT-Joint (Chen et al.,  2019 ) * <C> 97.5 <C> 88.2 <C> 96.1 <C> 98.6 <C> 92.8 <C> 97.0 <R> <C> [ITALIC] BERT-Joint Ours <C> 97.42 <C> 87.57 <C> 95.74 (98.1) <C> 98.71 <C> 91.57 <C> 96.27 (97.9) <R> <C> [ITALIC] Transformer-NLU:BERT <C> 97.87 <C> 88.69 <C> 96.25 [BOLD]  (98.2) <C> 98.86 <C> 91.86 <C> 96.57 [BOLD]  (98.0) <R> <C> Transformer-NLU:BERT w/o Slot Features <C> 97.87 <C> 88.35 <C> 95.97 <C> 98.86 <C> 91.57 <C> 96.25 <R> <C> Transformer-NLU:BERT w/ CRF <C> 97.42 <C> 88.26 <C> 96.14 <C> 98.57 <C> 92.00 <C> 96.54 <CAP> Table 3: Intent detection and slot filling results measured on the SNIPS and the ATIS dataset. Highest results in each category are written in bold. Models used in the analysis are in italic. * Chen et al. (2019)’s BERT-Joint uses per token F1 with micro averaging (which is not standard and inflates their score by several points absolute). For comparison we also report the per token F1 for our models (the numbers in parenthesis).
<R> <C> [BOLD] Word n-grams <C> [BOLD] Count <R> <C> Unigrams <C> 208,105 <R> <C> Bigrams <C> 2,616,762 <R> <C> Trigrams <C> 4,432,251 <R> <C> Quadrigrams <C> 2,138,696 <R> <C> [BOLD] Total <C> [BOLD] 9,395,814 <CAP> Table 2. Count of n-gram noun phrases generated from patent dataset.
<R> <C> [BOLD] z <C> PRECISION  [BOLD] Our Model <C> PRECISION  [BOLD] KEA <C> PRECISION  [BOLD] Legal <C> PRECISION  [BOLD] BM25 <C> PRECISION  [BOLD] KLIP <C> RECALL  [BOLD] Our Model <C> RECALL  [BOLD] KEA <C> RECALL  [BOLD] Legal <C> RECALL  [BOLD] BM25 <C> RECALL  [BOLD] KLIP <R> <C> 10 <C> [BOLD] 0.253 <C> 0.192 <C> 0.075 <C> 0.080 <C> 0.120 <C> [BOLD] 0.773 <C> 0.557 <C> 0.255 <C> 0.265 <C> 0.386 <R> <C> 15 <C> [BOLD] 0.231 <C> 0.148 <C> 0.128 <C> 0.131 <C> 0.146 <C> [BOLD] 0.910 <C> 0.566 <C> 0.559 <C> 0.567 <C> 0.623 <R> <C> 20 <C> [BOLD] 0.217 <C> 0.133 <C> 0.156 <C> 0.156 <C> 0.164 <C> [BOLD] 0.945 <C> 0.568 <C> 0.750 <C> 0.749 <C> 0.772 <R> <C> 15% <C> [BOLD] 0.260 <C> 0.200 <C> 0.056 <C> 0.060 <C> 0.108 <C> [BOLD] 0.750 <C> 0.555 <C> 0.172 <C> 0.185 <C> 0.323 <R> <C> 20% <C> [BOLD] 0.240 <C> 0.156 <C> 0.109 <C> 0.111 <C> 0.132 <C> [BOLD] 0.886 <C> 0.563 <C> 0.448 <C> 0.457 <C> 0.528 <CAP> Table 4. Comparison of our proposed method against the baselines: Precision and recall values at different top-ranks (z∈10,15,20,15%,20%) of extracted catchphrases.
<R> <C> Dataset <C> Measure <C> Baseline_Ranker <C> Textual_Features <C> Embedding_Features <C> Textual_Embedding_Features <R> <C> Whole <C> MRR <C> 1.26 <C> 1.30 <C> 1.31 <C> 1.31 <R> <C> Whole <C> nDCG <C> 1.30 <C> 1.32 <C> 1.33 <C> 1.33 <R> <C> Whole <C> SR@1 <C> 1.24 <C> 1.29 <C> 1.31 <C> 1.31 <R> <C> Whole <C> SR@3 <C> 1.23 <C> 1.26 <C> 1.27 <C> 1.27 <R> <C> Whole <C> MAP <C> 1.26 <C> 1.30 <C> 1.31 <C> 1.31 <R> <C> Whole <C> MAP@1 <C> 1.24 <C> 1.29 <C> 1.31 <C> 1.31 <R> <C> Whole <C> MAP@3 <C> 1.23 <C> 1.27 <C> 1.29 <C> 1.29 <R> <C> User Context Only <C> MRR <C> 1.27 <C> 1.34 <C> 1.37 <C> 1.37 <R> <C> User Context Only <C> nDCG <C> 1.30 <C> 1.35 <C> 1.37 <C> 1.37 <R> <C> User Context Only <C> SR@1 <C> 1.26 <C> 1.37 <C> 1.42 <C> 1.41 <R> <C> User Context Only <C> SR@3 <C> 1.23 <C> 1.30 <C> 1.33 <C> 1.34 <R> <C> User Context Only <C> MAP <C> 1.27 <C> 1.34 <C> 1.37 <C> 1.37 <R> <C> User Context Only <C> MAP@1 <C> 1.26 <C> 1.37 <C> 1.42 <C> 1.41 <R> <C> User Context Only <C> MAP@3 <C> 1.24 <C> 1.33 <C> 1.37 <C> 1.37 <CAP> Table 2. Offline evaluation metrics. We show the ratio of the metrics for four of the ranking models to the MPC model on both test datasets - all data and filtered data to include full coverage for user context.
<R> <C> [BOLD] Resnet Architecture <C> [BOLD] Accuracy <C> [BOLD] Precision <C> [BOLD] F score <R> <C> Resnet-101 <C> 25.2 <C> 25.8 <C> 25.4 <R> <C> Resnet-152 <C> [BOLD] 29.6 <C> [BOLD] 29.8 <C> [BOLD] 29.7 <CAP> Table 3. Accuracies of emotion classification task using visual features from images
<R> <C> [BOLD] Knowledge Concepts <C> [BOLD] Precision <C> [BOLD] Recall <C> [BOLD] Macro F1 Score <R> <C> Emoji Names <C> 70.23 <C> 69.25 <C> 69.42 <R> <C> Emoji Senses <C> 73.79 <C> 70.25 <C> 71.98 <R> <C> Emoji Sense Definitions <C> 71.56 <C> 69.98 <C> 70.49 <R> <C> Processed Emoji Sense Definitions <C> 72.23 <C> 70.26 <C> 70.78 <CAP> Table 10. Accuracies of emotion classification task using attention mechanism combining text and emoji as input for Bi-LSTM
<R> <C> [BOLD] Models <C> [BOLD] Snips  [BOLD] Intent <C> [BOLD] Snips  [BOLD] Slot <C> [BOLD] Snips  [BOLD] Frame <C> [BOLD] ATIS  [BOLD] Intent <C> [BOLD] ATIS  [BOLD] Slot <C> [BOLD] ATIS  [BOLD] Frame <R> <C> [EMPTY] <C> [BOLD] (Acc) <C> [BOLD] (F1) <C> [BOLD] (Acc) <C> [BOLD] (Acc) <C> [BOLD] (F1) <C> [BOLD] (Acc) <R> <C> RNN-LSTM  <C> 96.9 <C> 87.3 <C> 73.2 <C> 92.6 <C> 94.3 <C> 80.7 <R> <C> Atten.-BiRNN  <C> 96.7 <C> 87.8 <C> 74.1 <C> 91.1 <C> 94.2 <C> 78.9 <R> <C> Slot-Gated  <C> 97.0 <C> 88.8 <C> 75.5 <C> 94.1 <C> 95.2 <C> 82.6 <R> <C> Capsule Neural Networks  <C> 97.3 <C> 91.8 <C> 80.9 <C> 95.0 <C> 95.2 <C> 83.4 <R> <C> (1) CELT w/o BERT pre-train <C> 97.8±0.2 <C> 90.0±0.6 <C> 79.3±1.4 <C> 96.9±0.1 <C> 92.7±0.1 <C> 80.5±0.4 <R> <C> (2) CELT w/o BERT pre-train + CRF <C> 97.9±0.3 <C> 90.8±0.2 <C> 80.9±0.5 <C> 97.0±0.3 <C> 93.1±0.2 <C> 81.6±0.4 <R> <C> (3) (1) w/ BERT pre-train <C> [BOLD] 98.3±0.3 <C> 96.4±0.2 <C> [BOLD] 91.9±0.2 <C> 97.4±0.4 <C> [BOLD] 95.9±0.1 <C> [BOLD] 87.9±0.4 <R> <C> (4) (2) w/ BERT pre-train <C> [BOLD] 98.3±0.1 <C> [BOLD] 96.5±0.2 <C> 91.8±0.5 <C> [BOLD] 97.6±0.1 <C> 95.7±0.1 <C> 87.6±0.2 <CAP> Table 2: SLU performance on the single-turn Snips and ATIS testsets. Note that since Snips and ATIS are single-turn dialogues, all models in this table do not use context information. All models are trained and tested on the same training and test partitions of Snips and ATIS, respectively (no transfer learning is applied). The mean and standard deviation of SLU results from CELT w/o and with BERT pre-train, w/o and with replacing the softmax layer with a CRF layer, from 5 different models with different random initialization are given here. The metrics are the intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy. The results for the first group of models are cited from [15, 16].
<R> <C> [BOLD] Model <C> [BOLD] LSTM  [BOLD] hidden size <C> # of  [BOLD] params. <C> [BOLD] PTB  [BOLD] Dev <C> [BOLD] PTB  [BOLD] Test <C> [BOLD] Wikitext-2  [BOLD] Dev <C> [BOLD] Wikitext-2  [BOLD] Test <R> <C> Vanilla LSTM (Xie et al.,  2017 ) <C> 512 <C> - <C> 84.3 <C> 80.4 <C> - <C> - <R> <C> Vanilla LSTM (Xie et al.,  2017 ) <C> 1500 <C> - <C> 81.6 <C> 77.5 <C> - <C> - <R> <C> DN: Kneser-Ney (Xie et al.,  2017 ) <C> 512 <C> - <C> 79.9 <C> 76.9 <C> - <C> - <R> <C> DN: Kneser-Ney (Xie et al.,  2017 ) <C> 1500 <C> - <C> 76.2 <C> 73.4 <C> - <C> - <R> <C> Var. dropout (Gal & Ghahramani,  2016b ) <C> 1500 <C> - <C> - <C> 73.4 <C> - <C> - <R> <C> Vanilla LSTM: untied <C> 512 <C> 14M/38M <C> 89.6 <C> 84.5 <C> 106.3 <C> 100.8 <R> <C> Vanilla LSTM: tied <C> 512 <C> 9M/21M <C> 80.0 <C> 74.0 <C> 90.6 <C> 86.6 <R> <C> DN: linear interpolation <C> 512 <C> 9M/21M <C> 79.4 <C> 73.3 <C> 88.9 <C> 84.6 <R> <C> DN: Kneser-Ney <C> 512 <C> 9M/21M <C> 75.0 <C> 70.7 <C> 86.1 <C> 82.1 <R> <C> VS: linear interpolation <C> 512 <C> 9M/21M <C> 76.3 <C> 71.2 <C> 84.0 <C> 79.6 <R> <C> VS: Kneser-Ney <C> 512 <C> 9M/21M <C> 74.5 <C> 70.6 <C> 84.9 <C> 80.9 <R> <C> VS: element-wise <C> 512 <C> 9M/21M <C> 70.5 <C> 66.8 <C> - <C> - <R> <C> Vanilla LSTM: untied <C> 1024 <C> 37M/85M <C> 90.3 <C> 85.5 <C> 97.6 <C> 91.9 <R> <C> Vanilla LSTM: tied <C> 1024 <C> 27M/50M <C> 75.9 <C> 70.2 <C> 85.2 <C> 81.0 <R> <C> DN: linear interpolation <C> 1024 <C> 27M/50M <C> 75.5 <C> 70.2 <C> 84.3 <C> 80.1 <R> <C> DN: Kneser-Ney <C> 1024 <C> 27M/50M <C> 71.4 <C> 67.3 <C> 81.9 <C> 78.3 <R> <C> VS: linear interpolation <C> 1024 <C> 27M/50M <C> 71.7 <C> 67.8 <C> 80.5 <C> 76.6 <R> <C> VS: Kneser-Ney <C> 1024 <C> 27M/50M <C> 70.8 <C> 66.9 <C> 80.9 <C> 76.7 <R> <C> VS: element-wise <C> 1024 <C> 27M/50M <C> 68.6 <C> 64.5 <C> - <C> - <CAP> Table 2: Perplexity on PTB and Wikitext-2 datasets. DN and VS denote data noising and variational smoothing. The two numbers (*M/*M) in the # of params. column denote the number of parameters for PTB and Wikitext-2 respectively.
<R> <C> [BOLD] Source <C> [BOLD] F-score <C> [BOLD] Precision <C> [BOLD] Recall <R> <C> [BOLD] Post text <C> 0.52 <C> 0.92 <C> 0.36 <R> <C> [BOLD] Target title <C> 0.53 <C> 0.70 <C> 0.43 <R> <C> [BOLD] Both <C> [BOLD] 0.62 <C> 0.86 <C> 0.49 <R> <C> [BOLD] Feature selection <C> 0.44 <C> 0.58 <C> 0.35 <CAP> Table 5: The performance of individual features and feature concatenations extracted from the target title.
<R> <C> [EMPTY] <C> [BOLD] Model <C> [BOLD] ROUGE-1 <C> [BOLD] ROUGE-2 <C> [BOLD] ROUGE-L <C> [BOLD] F \textsc  [ITALIC] BERT <C> [BOLD] Sentiment Acc. <C> [BOLD] Fcategory <R> <C> YELP <C> Textrank (Mihalcea and Tarau,  2004 ) <C> 28.3 <C> 4.2 <C> 14.9 <C> 84.1 <C> 82.0 <C> 53.4 <R> <C> YELP <C> Lexrank (Radev et al.,  2004 ) <C> 27.4 <C> 3.9 <C> 14.9 <C> 84.2 <C> 83.5 <C> 54.1 <R> <C> YELP <C> Opinosis (Ganesan et al.,  2010 ) <C> 26.8 <C> 3.4 <C> 14.2 <C> 81.2 <C> 80.5 <C> 53.0 <R> <C> YELP <C> H-VAE (Bražinskas et al.,  2019 ) <C> 29.5 <C> 5.3 <C> 18.1 <C> – <C> – <C> – <R> <C> YELP <C> Meansum (Chu and Liu,  2019 ) <C> 28.6 <C> 3.8 <C> 15.9 <C> 86.5 <C> 83.5 <C> 50.3 <R> <C> YELP <C> Ours <C> [BOLD] 32.8 <C> [BOLD] 8.7 <C> [BOLD] 18.8 <C> [BOLD] 86.8 <C> [BOLD] 83.9 <C> [BOLD] 55.2 <R> <C> RT <C> Textrank <C> 19.0 <C> 4.3 <C> 19.4 <C> [BOLD] 85.3 <C> [BOLD] 75.8 <C> 41.6 <R> <C> RT <C> Lexrank <C> 17.6 <C> 3.5 <C> 18.2 <C> [BOLD] 85.3 <C> 73.2 <C> 40.9 <R> <C> RT <C> Opinosis <C> 15.2 <C> 2.9 <C> 16.9 <C> 84.1 <C> 67.5 <C> 37.1 <R> <C> RT <C> Ours <C> [BOLD] 20.9 <C> [BOLD] 4.5 <C> [BOLD] 22.7 <C> [BOLD] 85.3 <C> 70.9 <C> [BOLD] 43.6 <CAP> Table 1: Automatic evaluations results against gold summaries of Yelp and Rotten Tomatoes (RT) datasets. “Ours” denotes our proposed system with parallel input combination strategy and control codes.
<R> <C> [EMPTY] <C> [BOLD] Model <C> Dist-1 <C> Dist-2 <C> Dist-3 <C> Dist [ITALIC] c-1 <C> Dist [ITALIC] c-2 <C> Dist [ITALIC] c-3 <R> <C> Extract. <C> Textrank <C> 0.68 <C> 0.95 <C> 0.992 <C> 0.135 <C> 0.62 <C> 0.90 <R> <C> Extract. <C> Lextrank <C> 0.70 <C> 0.96 <C> 0.994 <C> 0.144 <C> 0.6 <C> 0.92 <R> <C> Extract. <C> Opinosis <C> 0.72 <C> 0.94 <C> 0.97 <C> [BOLD] 0.159 <C> [BOLD] 0.66 <C> [BOLD] 0.92 <R> <C> Abstr. <C> Meansum <C> 0.72 <C> 0.95 <C> 0.98 <C> 0.091 <C> 0.39 <C> 0.67 <R> <C> Abstr. <C> Ours <C> [BOLD] 0.79 <C> [BOLD] 0.99 <C> [BOLD] 1.00 <C> 0.097 <C> 0.41 <C> 0.64 <CAP> Table 2: Referenceless evaluation results on Yelp dataset.
<R> <C> [EMPTY] <C> [EMPTY] <C> question_type AMI <C> question_type ACC <C> ag_news AMI <C> ag_news ACC <C> dbpedia AMI <C> dbpedia ACC <C> yahoo_answer AMI <C> yahoo_answer ACC <R> <C> Unsup. <C> bow <C> 0.028 <C> 0.257 <C> 0.029 <C> 0.311 <C> 0.578 <C> 0.546 <C> 0.019 <C> 0.140 <R> <C> Unsup. <C> tf-idf <C> 0.031 <C> 0.259 <C> 0.168 <C> 0.449 <C> 0.558 <C> 0.527 <C> 0.023 <C> 0.145 <R> <C> Unsup. <C> average-vec <C> 0.135 <C> 0.356 <C> 0.457 <C> 0.737 <C> 0.610 <C> 0.619 <C> 0.077 <C> 0.222 <R> <C> Sup. <C> metric-learn-bow <C> 0.104 <C> 0.380 <C> 0.459 <C> 0.776 <C> 0.808 <C> 0.854 <C> 0.125 <C> 0.329 <R> <C> Sup. <C> metric-learn-idf <C> 0.114 <C> 0.379 <C> 0.443 <C> 0.765 <C> 0.821 <C> 0.876 <C> 0.150 <C> 0.368 <R> <C> Sup. <C> metric-learn-ave-vec <C> 0.304 <C> 0.553 <C> 0.606 <C> 0.851 <C> 0.829 <C> 0.879 <C> 0.221 <C> 0.400 <R> <C> Sup. <C> cnn-classifier <C> 0.511 <C> [BOLD] 0.771 <C> 0.554 <C> 0.771 <C> 0.879 <C> 0.938 <C> 0.285 <C> 0.501 <R> <C> Sup. <C> cnn-represent. <C> 0.442 <C> 0.618 <C> 0.604 <C> 0.833 <C> 0.864 <C> 0.899 <C> 0.210 <C> 0.334 <R> <C> Sup. <C> lstm-classifier <C> 0.482 <C> 0.741 <C> 0.524 <C> 0.763 <C> 0.862 <C> 0.928 <C> 0.283 <C> 0.512 <R> <C> Sup. <C> lstm-represent. <C> 0.421 <C> 0.618 <C> 0.535 <C> 0.771 <C> 0.667 <C> 0.706 <C> 0.152 <C> 0.272 <R> <C> Semisup. <C> semi-cnn <C> [BOLD] 0.529 <C> 0.739 <C> [BOLD] 0.662 <C> [BOLD] 0.876 <C> [BOLD] 0.894 <C> [BOLD] 0.945 <C> [BOLD] 0.338 <C> [BOLD] 0.554 <R> <C> Semisup. <C> semi-lstm <C> 0.492 <C> 0.712 <C> 0.599 <C> 0.830 <C> 0.788 <C> 0.802 <C> 0.187 <C> 0.337 <CAP> Table 4: Performance of all systems on each dataset.
<R> <C> Data <C> Method <C> Acc. <C> Ppl. <C> Rank <R> <C> LAMBADA <C> [ITALIC] baselines <C> [ITALIC] baselines <C> [ITALIC] baselines <C> [ITALIC] baselines <R> <C> LAMBADA <C> Random vocabulary word <C> 0 <C> 60000 <C> 30026 <R> <C> LAMBADA <C> Random word from passage <C> 1.6 <C> - <C> - <R> <C> LAMBADA <C> Random capitalized word from passage <C> 7.3 <C> - <C> - <R> <C> LAMBADA <C> Unsup-CBOW <C> 0 <C> 57040 <C> 16352 <R> <C> LAMBADA <C> Sup-CBOW <C> 0 <C> 47587 <C> 4660 <R> <C> LAMBADA <C> [ITALIC] models <C> [ITALIC] models <C> [ITALIC] models <C> [ITALIC] models <R> <C> LAMBADA <C> N-Gram <C> 0.1 <C> 3125 <C> 993 <R> <C> LAMBADA <C> N-Gram w/cache <C> 0.1 <C> 768 <C> 87 <R> <C> LAMBADA <C> RNN <C> 0 <C> 14725 <C> 7831 <R> <C> [EMPTY] <C> LSTM <C> 0 <C> 5357 <C> 324 <R> <C> [EMPTY] <C> Memory Network <C> 0 <C> 16318 <C> 846 <R> <C> Control <C> [ITALIC] baselines <C> [ITALIC] baselines <C> [ITALIC] baselines <C> [ITALIC] baselines <R> <C> Control <C> Random vocabulary word <C> 0 <C> 60000 <C> 30453 <R> <C> Control <C> Random word from passage <C> 0 <C> - <C> - <R> <C> Control <C> Random capitalized word from passage <C> 0 <C> - <C> - <R> <C> Control <C> Unsup-CBOW <C> 0 <C> 55190 <C> 12950 <R> <C> Control <C> Sup-CBOW <C> 3.5 <C> 2344 <C> 259 <R> <C> Control <C> [ITALIC] models <C> [ITALIC] models <C> [ITALIC] models <C> [ITALIC] models <R> <C> Control <C> N-Gram <C> 19.1 <C> 285 <C> 17 <R> <C> Control <C> N-Gram w/cache <C> 19.1 <C> 270 <C> 18 <R> <C> Control <C> RNN <C> 15.4 <C> 277 <C> 24 <R> <C> [EMPTY] <C> LSTM <C> 21.9 <C> 149 <C> 12 <R> <C> [EMPTY] <C> Memory Network <C> 8.5 <C> 566 <C> 46 <CAP> Table 1: Results of computational methods. Accuracy is expressed in percentage.
<R> <C> # <C> [BOLD] dataset <C> [BOLD] model <C> [BOLD] inference <C> [BOLD] image→text R@1 <C> [BOLD] image→text R@5 <C> [BOLD] image→text R@10 <C> [BOLD] image→text Med r <C> [BOLD] image→text Mean r <C> [BOLD] text→image R@1 <C> [BOLD] text→image R@5 <C> [BOLD] text→image R@10 <C> [BOLD] text→image Med r <C> [BOLD] text→image Mean r <R> <C> 3.1 <C> Flickr30k <C> GRU+VGG19 kNN-margin <C> naive <C> 34.1 <C> 61.7 <C> 69.9 <C> [BOLD] 3.0 <C> 24.7 <C> 25.1 <C> 52.5 <C> 64.6 <C> 5.0 <C> 34.3 <R> <C> 3.2 <C> Flickr30k <C> GRU+VGG19 kNN-margin <C> Is <C> [BOLD] 36.0 <C> [BOLD] 64.5 <C> [BOLD] 72.9 <C> [BOLD] 3.0 <C> [BOLD] 20.1 <C> 25.2 <C> 52.6 <C> 64.4 <C> 5.0 <C> 31.1 <R> <C> 3.3 <C> Flickr30k <C> GRU+VGG19 kNN-margin <C> Csls <C> [BOLD] 36.0 <C> 64.4 <C> 72.5 <C> [BOLD] 3.0 <C> 20.3 <C> [BOLD] 26.7 <C> [BOLD] 54.3 <C> [BOLD] 65.7 <C> [BOLD] 4.0 <C> [BOLD] 30.8 <R> <C> 3.4 <C> MS-COCO 5k <C> GRU+ResNet152 kNN-margin <C> naive <C> 57.8 <C> 87.6 <C> 94.4 <C> [BOLD] 1.0 <C> 3.4 <C> 43.9 <C> 79.0 <C> 88.8 <C> [BOLD] 2.0 <C> 8.1 <R> <C> 3.5 <C> MS-COCO 5k <C> GRU+ResNet152 kNN-margin <C> Is <C> [BOLD] 64.2 <C> [BOLD] 89.4 <C> 95.0 <C> [BOLD] 1.0 <C> 3.2 <C> 46.7 <C> 80.1 <C> 89.3 <C> [BOLD] 2.0 <C> 7.8 <R> <C> 3.6 <C> MS-COCO 5k <C> GRU+ResNet152 kNN-margin <C> Csls <C> 62.4 <C> 89.3 <C> [BOLD] 95.4 <C> [BOLD] 1.0 <C> [BOLD] 3.0 <C> [BOLD] 47.2 <C> [BOLD] 80.7 <C> [BOLD] 89.9 <C> [BOLD] 2.0 <C> [BOLD] 7.7 <R> <C> 3.7 <C> MS-COCO 1k <C> Kiros et al. ( 2015 ) (ours) <C> Kiros et al. ( 2015 ) (ours) <C> 49.9 <C> 79.4 <C> 90.1 <C> 2.0 <C> 5.2 <C> 37.3 <C> 74.3 <C> 85.9 <C> [BOLD] 2.0 <C> 10.8 <R> <C> 3.8 <C> MS-COCO 1k <C> Vendrov et al. ( 2016 ) <C> Vendrov et al. ( 2016 ) <C> 46.7 <C> - <C> 88.9 <C> 2.0 <C> 5.7 <C> 37.9 <C> - <C> 85.9 <C> [BOLD] 2.0 <C> 8.1 <R> <C> 3.9 <C> MS-COCO 1k <C> Huang et al. ( 2017 ) <C> Huang et al. ( 2017 ) <C> 53.2 <C> 83.1 <C> 91.5 <C> [BOLD] 1.0 <C> - <C> 40.7 <C> 75.8 <C> 87.4 <C> [BOLD] 2.0 <C> - <R> <C> 3.10 <C> MS-COCO 1k <C> Liu et al. ( 2017 ) <C> Liu et al. ( 2017 ) <C> 56.4 <C> 85.3 <C> 91.5 <C> - <C> - <C> 43.9 <C> 78.1 <C> 88.6 <C> - <C> - <R> <C> 3.11 <C> MS-COCO 1k <C> You et al. ( 2018 ) <C> You et al. ( 2018 ) <C> 56.3 <C> 84.4 <C> 92.2 <C> [BOLD] 1.0 <C> - <C> 45.7 <C> 81.2 <C> 90.6 <C> [BOLD] 2.0 <C> - <R> <C> 3.12 <C> MS-COCO 1k <C> Faghri et al. ( 2018 ) <C> Faghri et al. ( 2018 ) <C> 58.3 <C> 86.1 <C> 93.3 <C> [BOLD] 1.0 <C> - <C> 43.6 <C> 77.6 <C> 87.8 <C> [BOLD] 2.0 <C> [EMPTY] <R> <C> 3.13 <C> MS-COCO 1k <C> Faghri et al. ( 2018 ) (ours) <C> Faghri et al. ( 2018 ) (ours) <C> 60.5 <C> 89.6 <C> 94.9 <C> [BOLD] 1.0 <C> 3.1 <C> 46.1 <C> 79.5 <C> 88.7 <C> [BOLD] 2.0 <C> 8.5 <R> <C> 3.14 <C> MS-COCO 1k <C> Wu et al. ( 2019 ) <C> Wu et al. ( 2019 ) <C> 64.3 <C> 89.2 <C> 94.8 <C> [BOLD] 1.0 <C> - <C> 48.3 <C> 81.7 <C> [BOLD] 91.2 <C> [BOLD] 2.0 <C> - <R> <C> 3.15 <C> MS-COCO 1k <C> GRU+ResNet152 kNN-margin <C> naive <C> 58.3 <C> 89.2 <C> 95.4 <C> [BOLD] 1.0 <C> 3.1 <C> 45.0 <C> 80.4 <C> 89.6 <C> [BOLD] 2.0 <C> 7.2 <R> <C> 3.16 <C> MS-COCO 1k <C> GRU+ResNet152 kNN-margin <C> Is <C> [BOLD] 66.4 <C> 91.8 <C> 96.1 <C> [BOLD] 1.0 <C> 2.7 <C> 48.6 <C> 81.5 <C> 90.3 <C> [BOLD] 2.0 <C> 7.3 <R> <C> 3.17 <C> [EMPTY] <C> GRU+ResNet152 kNN-margin <C> Csls <C> 65.4 <C> [BOLD] 91.9 <C> [BOLD] 97.1 <C> [BOLD] 1.0 <C> [BOLD] 2.5 <C> [BOLD] 49.6 <C> [BOLD] 82.7 <C> [BOLD] 91.2 <C> [BOLD] 2.0 <C> [BOLD] 6.5 <CAP> Table 4: Quantitative results of different inference methods across different datasets and models. Line 3.1-3.3 are using the model from Table 1 line 1.3 and line 3.4-3.6, 3.15-3.17 are using the model from Table 2 line 2.9. Line 3.7-3.14 are results reported by previous works which all adopted naive nearest neighbor search for inference.
<R> <C> [EMPTY] <C> [ITALIC] psup <C> Target domain T 10% <C> Target domain T 50% <C> Target domain T 90% <C> Source domain S 10% <C> Source domain S 50% <C> Source domain S 90% <R> <C> [EMPTY] <C> Domain size <C> 4358 <C> 2421 <C> 484 <C> 484 <C> 2421 <C> 4358 <R> <C> [BOLD] Models <C> [ITALIC] Random <C> [ITALIC] 100 <C> [ITALIC] 100 <C> [ITALIC] 100 <C> [ITALIC] 100 <C> [ITALIC] 100 <C> [ITALIC] 100 <R> <C> [BOLD] Models <C> M(∅) <C> 38.6 <C> 23.7 <C> 13.8 <C> 12.0 <C> 10.6 <C> 11.2 <R> <C> [BOLD] Models <C> M(V) <C> 20.5 <C> 10.7 <C> 6.0 <C> 1.5 <C> 2.6 <C> 3.6 <R> <C> [BOLD] Models <C> M(C [ITALIC] SH) <C> 28.7 <C> 14.4 <C> 9.1 <C> 4.2 <C> 4.3 <C> 4.4 <R> <C> [BOLD] Models <C> M(C [ITALIC] SH,V) <C> [BOLD] 18.1 <C> [BOLD] 9.0 <C> [BOLD] 5.2 <C> [BOLD] 1.1 <C> [BOLD] 1.9 <C> [BOLD] 2.4 <R> <C> [EMPTY] <C> [ITALIC] δC (%) <C> [ITALIC] 11.6 <C> [ITALIC] 16.4 <C> [ITALIC] 12.1 <C> [ITALIC] 23.7 <C> [ITALIC] 27.3 <C> [ITALIC] 31.5 <CAP> Table 1: Evaluation of various information sources, with varying levels of supervision. MFR scores in %. δC is the relative improvement (in %) of M(CSH,V) over M(V).
<R> <C> PolarityPrediction <C> Neutral <C> Contradiction <C> Entailment <R> <C> Neutral <C> 5 <C> 21 <C> 0 <R> <C> Contradiction <C> 5 <C> 543 <C> 3 <R> <C> Entailment <C> 0 <C> 3 <C> 0 <R> <C> None <C> 8 <C> 20 <C> 12 <CAP> Table 4: Contingency table for DAM: Predictions distributed according to the polarity of target word pairs found in the transformed instances.
<R> <C> Comparison/Domain x <C> doc <C> sec <R> <C> x2vecText - x2vecMath_op <C> 0.14 <C> 0.16 <R> <C> x2vecText - x2vecMath_id <C> 0.12 <C> 0.11 <R> <C> x2vecText - x2vecMath_opid <C> 0.16 <C> 0.15 <R> <C> x2vecText - x2vecMath_surroundings <C> 0.21 <C> 0.27 <CAP> Table 2. Correlations between text and math (cosine) similarity of individual documents and sections.
<R> <C> Models <C> Detection-C6 F1 <C> Detection-C6 Accuracy <C> Detection-C36 F1 <C> Detection-C36 Accuracy <C> Recognition-C6 F1 <C> Recognition-C6 Accuracy <C> Recognition-C36 F1 <C> Recognition-C36 Accuracy <R> <C> [ITALIC]  [BOLD] Proposed <C> [ITALIC]  [BOLD] Proposed <C> [ITALIC]  [BOLD] Proposed <C> [ITALIC]  [BOLD] Proposed <C> [ITALIC]  [BOLD] Proposed <C> [ITALIC]  [BOLD] Proposed <C> [ITALIC]  [BOLD] Proposed <C> [ITALIC]  [BOLD] Proposed <C> [ITALIC]  [BOLD] Proposed <R> <C> [ITALIC] CrisisBERT <C> [BOLD] 95.5 <C> [BOLD] 95.6 <C> [BOLD] 94.2 <C> [BOLD] 94.7 <C> [BOLD] 98.7 <C> [BOLD] 98.6 <C> [BOLD] 97.1 <C> [BOLD] 97.9 <R> <C> [ITALIC] LSTMc2 [ITALIC] v <C> 95.1 <C> 95.1 <C> 92.8 <C> 93.5 <C> 97.5 <C> 97.5 <C> 88.0 <C> 95.6 <R> <C> [ITALIC] LRc2 [ITALIC] v <C> 93.2 <C> 93.2 <C> 89.0 <C> 90.1 <C> 93.6 <C> 93.7 <C> 85.1 <C> 90.9 <R> <C> [ITALIC]  [BOLD] Benchmark <C> [ITALIC]  [BOLD] Benchmark <C> [ITALIC]  [BOLD] Benchmark <C> [ITALIC]  [BOLD] Benchmark <C> [ITALIC]  [BOLD] Benchmark <C> [ITALIC]  [BOLD] Benchmark <C> [ITALIC]  [BOLD] Benchmark <C> [ITALIC]  [BOLD] Benchmark <C> [ITALIC]  [BOLD] Benchmark <R> <C> [ITALIC] LRw2 [ITALIC] v [manna2019effectiveness] <C> 91.5 <C> 91.6 <C> 85.3 <C> 86.6 <C> 87.3 <C> 88.5 <C> 72.1 <C> 82.3 <R> <C> [ITALIC] SVMw2 [ITALIC] v [manna2019effectiveness] <C> 91.3 <C> 91.4 <C> 85.0 <C> 86.3 <C> 86.6 <C> 87.9 <C> 71.6 <C> 81.9 <R> <C> [ITALIC] NBw2 [ITALIC] v [manna2019effectiveness] <C> 86.8 <C> 86.8 <C> 82.4 <C> 83.4 <C> 78.8 <C> 80.5 <C> 47.6 <C> 63.5 <R> <C> [ITALIC] CNNgv [kersten2019robust] <C> 91.2 <C> 91.3 <C> 91.1 <C> 91.2 <C> 90.5 <C> 90.4 <C> 23.3 <C> 64.4 <R> <C> [ITALIC] LSTMw2 [ITALIC] v [kumar2020deep] <C> 91.7 <C> 91.7 <C> 88.0 <C> 89.3 <C> 87.3 <C> 87.3 <C> 58.9 <C> 72.3 <CAP> Table 4: Experimental results of Crisis Classification tasks on C6 and C36 datasets for proposed models and benchmarks, where best performers are emphasized. Results show that CrisisBERT records highest performance across all tasks.
<R> <C> Model <C> [ITALIC] q EM <C> [ITALIC] q F1 <C> [ITALIC] qsub1 EM <C> [ITALIC] qsub1 F1 <C> [ITALIC] qsub2 EM <C> [ITALIC] qsub2 F1 <C> Model failure rate EM <C> Model failure rate PM <R> <C> Our Model <C> 62.6 <C> 74.8 <C> 74 <C> 83.35 <C> 62.3 <C> 75.45 <C> 30.99% <C> 21.89% <R> <C> – SingleQA <C> 64 <C> 76.35 <C> 67 <C> 81.27 <C> 64.2 <C> 76.67 <C> 36.41% <C> 23.61% <R> <C> – MultiQA <C> 19 <C> 24.87 <C> 78.9 <C> 85.56 <C> 61.1 <C> 74.42 <C> 28.95% <C> 25.75% <CAP> Table 6: Performance of ablation test on handling sub-questions.
<R> <C> [EMPTY] <C> Android <C> iOS <C> Twitter <C> Windows <C> Random <R> <C> Android <C> 1.000 <C> 0.153 <C> 0.111 <C> 0.062 <C> 0.010 <R> <C> iOS <C> 0.153 <C> 1.000 <C> 0.086 <C> 0.052 <C> 0.018 <R> <C> Twitter <C> 0.111 <C> 0.086 <C> 1.000 <C> 0.047 <C> 0.005 <R> <C> Windows <C> 0.062 <C> 0.052 <C> 0.047 <C> 1.000 <C> 0.002 <R> <C> Random <C> 0.010 <C> 0.018 <C> 0.005 <C> 0.002 <C> 1.000 <CAP> Table 2. Average Jaccard coefficient of emojis across platforms. Random indicates a random sample across all tweets of the size of the iOS corpus.
<R> <C> [EMPTY] <C> Model <C> Precision <C> Recall <C> F1-score <C> Accuracy <R> <C> 1 <C> BLSTM <C> 0.395 <C> 0.355 <C> 0.325 <C> 0.395 <R> <C> 2 <C> English BERT-base <C> 0.503 <C> 0.515 <C> 0.506 <C> 0.515 <R> <C> 3 <C> Spanish BERT-base <C> 0.514 <C> 0.526 <C> 0.517 <C> 0.526 <R> <C> 4 <C> Multilingual BERT-base <C> 0.506 <C> 0.519 <C> 0.510 <C> 0.519 <R> <C> 5 <C> Custom BERT-base <C> 0.513 <C> 0.526 <C> 0.517 <C> 0.526 <R> <C> 6 <C> XLM-RoBERTa-large <C> [BOLD] 0.534 <C> [BOLD] 0.551 <C> [BOLD] 0.537 <C> [BOLD] 0.551 <CAP> Table 1: Fine-tuning performance on validation
<R> <C> user 12 <C> user 13 4 <C> user 13 3 <C> user 13 2 <C> user 13 1 <C> user 13 0 <C> [EMPTY] <R> <C> 4 <C> [BOLD] 16 <C> 5 <C> 0 <C> 1 <C> 0 <C> 22 <R> <C> 3 <C> 0 <C> [BOLD] 22 <C> 6 <C> 3 <C> 1 <C> 32 <R> <C> 2 <C> 0 <C> 6 <C> [BOLD] 21 <C> 15 <C> 2 <C> 44 <R> <C> 1 <C> 0 <C> 1 <C> 10 <C> [BOLD] 10 <C> 13 <C> 34 <R> <C> 0 <C> 0 <C> 0 <C> 0 <C> 7 <C> [BOLD] 13 <C> 20 <R> <C> [EMPTY] <C> 16 <C> 34 <C> 37 <C> 36 <C> 29 <C> [BOLD] 152 <CAP> Table 3: Contingency table for user 12 and user 13 (i.e. for the author ratings)
<R> <C> [EMPTY] <C> [BOLD] domain experts (median ratings) <C> [BOLD] domain experts (median ratings) <C> [BOLD] domain experts (median ratings) <C> [BOLD] domain experts (median ratings) <C> [BOLD] domain experts (median ratings) <R> <C> [BOLD] authors (avg. rating) <C> 4 <C> 3 <C> 2 <C> 1 <C> 0 <R> <C> [BOLD] 3-4 <C> 20 <C> 16 <C> [BOLD] 6 <C> [BOLD] 1 <C> [BOLD] 0 <R> <C> [BOLD] 0-2 <C> [BOLD] 2 <C> [BOLD] 14 <C> 6 <C> 35 <C> 34 <CAP> Table 7: Number of similar pairs (3-4) rated “dissimilar” and dissimilar pairs (0-2) rated “similar”
<R> <C> [BOLD] Rank <C> [BOLD] Irma Sol. <C> [BOLD] Irma Not Sol. <C> [BOLD] Paris Sol. <C> [BOLD] Paris Not Sol. <R> <C> 1 <C> 6105 <C> 2098 <C> 5376 <C> 2878 <R> <C> 2 <C> 2336 <C> 1827 <C> 2826 <C> 1033 <R> <C> 3 <C> 1977 <C> 1474 <C> 2649 <C> 909 <R> <C> 4 <C> 1643 <C> 1193 <C> 2622 <C> 779 <R> <C> 5 <C> 1530 <C> 823 <C> 2581 <C> 760 <R> <C> 6 <C> 1034 <C> 794 <C> 2225 <C> 616 <R> <C> 7 <C> 934 <C> 726 <C> 1702 <C> 513 <R> <C> 8 <C> 820 <C> 725 <C> 386 <C> 510 <R> <C> 9 <C> 625 <C> 724 <C> 340 <C> 433 <R> <C> 10 <C> 367 <C> 683 <C> 259 <C> 361 <CAP> Table 5: Top ten emojis by frequency and their counts in Irma and Paris corpora
<R> <C> System <C> bleu <C> nist <C> meteor <C> r-l <C> cider <R> <C> T-Gen <C> 65.93 <C> 8.61 <C> 44.83 <C> 68.50 <C> 2.23 <R> <C> Best Prev. <C> 66.19† <C> 8.61† <C> [BOLD] 45.29‡ <C> [BOLD] 70.83⋄ <C> 2.27∙ <R> <C> [ITALIC] S0 <C> 66.52 <C> 8.55 <C> 44.45 <C> 69.34 <C> 2.23 <R> <C> [ITALIC] S0 ×2 <C> 65.93 <C> 8.31 <C> 43.52 <C> 69.58 <C> 2.12 <R> <C> [ITALIC] SR1 <C> [BOLD] 68.60 <C> [BOLD] 8.73 <C> [BOLD] 45.25 <C> [BOLD] 70.82 <C> [BOLD] 2.37 <R> <C> [ITALIC] SD1 <C> 67.76 <C> 8.72 <C> 44.59 <C> 69.41 <C> 2.27 <CAP> Table 1: Test results for the E2E generation task, in comparison to the T-Gen baseline Dušek and Jurčíček (2016) and the best results from the E2E challenge, reported by Dušek et al. (2018): †Juraska et al. (2018), ‡Puzikov and Gurevych (2018), ⋄Zhang et al. (2018), and ∙Gong (2018). We bold our highest performing model on each metric, as well as previous work if it outperforms all of our models.
<R> <C> [EMPTY] <C> [EMPTY] <C> Coverage Ratio for Attribute FF <C> Coverage Ratio for Attribute ET <C> Coverage Ratio for Attribute Food <C> Coverage Ratio for Attribute PR <C> Coverage Ratio for Attribute Area <C> Coverage Ratio for Attribute CR <R> <C> [EMPTY] <C> [ITALIC] S0 <C> 0.50 <C> 0.98 <C> 0.88 <C> 0.91 <C> 0.96 <C> 0.90 <R> <C> [EMPTY] <C> [ITALIC] SD1-FF <C> 0.57 <C> 1.00 <C> 0.92 <C> 0.90 <C> 0.95 <C> 0.95 <R> <C> [EMPTY] <C> [ITALIC] SD1-ET <C> 0.47 <C> 1.00 <C> 0.96 <C> 0.92 <C> 0.96 <C> 0.95 <R> <C> [EMPTY] <C> [ITALIC] SD1-Food <C> 0.45 <C> 1.00 <C> 1.00 <C> 0.93 <C> 0.95 <C> 0.94 <R> <C> [EMPTY] <C> [ITALIC] SD1-PR <C> 0.51 <C> 1.00 <C> 0.90 <C> 0.98 <C> 0.93 <C> 0.92 <R> <C> [EMPTY] <C> [ITALIC] SD1-Area <C> 0.47 <C> 1.00 <C> 0.93 <C> 0.91 <C> 0.98 <C> 0.93 <R> <C> Distractor Attr. <C> [ITALIC] SD1-CR <C> 0.45 <C> 1.00 <C> 0.91 <C> 0.90 <C> 0.91 <C> 0.95 <CAP> (b) Coverage ratios by attribute type (columns) for the base model S0, and for the pragmatic system SD1 when constructing the distractor by masking the specified attribute (rows). Cell colors are the degree the coverage ratio increases (green) or decreases (red) relative to S0.
<R> <C> Model <C> Model class <C> Correct:  [ITALIC] Entailment Lexical <C> Correct:  [ITALIC] Entailment Subseq. <C> Correct:  [ITALIC] Entailment Const. <C> Correct:  [ITALIC] Non-entailment Lexical <C> Correct:  [ITALIC] Non-entailment Subseq. <C> Correct:  [ITALIC] Non-entailment Const. <R> <C> DA <C> Bag-of-words <C> 1.00 <C> 1.00 <C> 0.98 <C> 0.00 <C> 0.00 <C> 0.03 <R> <C> ESIM <C> RNN <C> 0.99 <C> 1.00 <C> 1.00 <C> 0.00 <C> 0.01 <C> 0.00 <R> <C> SPINN <C> TreeRNN <C> 0.94 <C> 0.96 <C> 0.93 <C> 0.06 <C> 0.14 <C> 0.11 <R> <C> BERT <C> Transformer <C> 0.98 <C> 1.00 <C> 0.99 <C> 0.04 <C> 0.02 <C> 0.20 <CAP> Table 9: Results for models trained on MNLI with neutral and contradiction merged into a single label, non-entailment.
<R> <C> [BOLD] Dataset <C> [BOLD] Fluency <C> [BOLD] Coherence <C> [BOLD] Engagement <C> [BOLD] Overall <R> <C> Tencent <C> 0.331 <C> 0.623 <C> 0.566 <C> 0.610 <R> <C> Xiaohuangji <C> 0.292 <C> 0.632 <C> 0.625 <C> 0.627 <R> <C> Dailydialog <C> 0.330 <C> 0.612 <C> 0.326 <C> 0.523 <R> <C> Cornell <C> 0.249 <C> 0.468 <C> 0.388 <C> 0.343 <CAP> Table 1: Inter-annotator Kappa agreement is high.
<R> <C> [BOLD] Query <C> [BOLD] Ground-truth <C> [BOLD] Generated Response <C> [BOLD] BLEU <C> [BOLD] BERTScore <C> [BOLD] BERT-RUBER <C> [BOLD] PONE <C> [BOLD] Human <R> <C> Tell me a little bit about yourself please. <C> I’m Dunlin and I live in Beijing. I was born in 1980. <C> Ok, thank you very much <C> 0.1988 <C> 0.8168 <C> 0.9908 <C> 0.2903 <C> 0.20 <R> <C> I can type 100 Chinese words a minute and take dictation in English at 150 words a minute. <C> Can you operate computers skillfully? <C> So you don’t have a printer? <C> 0.2325 <C> 0.8793 <C> 0.5325 <C> 0.0554 <C> 0.0667 <R> <C> I usually have my lunch here. The food is tasty and they offer a big variety. <C> Let’s line up. The steak is very inviting. <C> That sounds pretty good. Can you come here for a few minutes? <C> 0.2555 <C> 0.8566 <C> 0.7415 <C> 0.5631 <C> 0.5333 <R> <C> That sounds nice. <C> Yeah, you should go and get it for yourself. <C> Yes, it is. Let’s go and have a drink . <C> 0.1926 <C> 0.8861 <C> 0.8188 <C> 0.6814 <C> 0.60 <CAP> Table 6: Real examples generated by Seq2Seq-attn model from Dailydialog dataset (single-turn). Human scores are the average of all the annotations and scores are normalized between 0 and 1.
<R> <C> [BOLD] Data <C> Metrics P <C> Metrics R <C> Metrics F <R> <C> 10 <C> 0.370 <C> 0.375 <C> 0.372 <R> <C> 100 <C> 0.402 <C> 0.393 <C> 0.397 <R> <C> 1000 <C> 0.491 <C> 0.387 <C> 0.432 <R> <C> ALL <C> 0.442 <C> 0.412 <C> 0.426 <CAP> Table 10: Sensitivity Analysis of the parse-tree model using 10, 100, 1000 and all instances of data.
<R> <C> Method <C> Configuration <C> Accuracy <C> Precision <C> Recall <C> F1 <R> <C> CNN <C> cnn_drop_out=0.2 <C> 0.960 <C> 0.955 <C> 0.966 <C> 0.960 <R> <C> Attention RNN <C> rnn_depth=1, rnn_drop_out=0.3, rnn_state_drop_out=0.3 <C> 0.958 <C> 0.977 <C> 0.939 <C> 0.957 <R> <C> BiRNN - CNN <C> rnn_depth=1, cnn_drop_out=0.2, rnn_drop_out=0.3, rnn_state_drop_out=0.3 <C> 0.960 <C> 0.954 <C> 0.965 <C> 0.960 <R> <C> BERT-base-uncased <C> learning-rate=1.5e-4 <C> 0.969 <C> 0.965 <C> 0.975 <C> 0.970 <R> <C> ColBERT <C> BERT-based-uncased, learning-rate=1.5e-4, NN-L1-dropout=0.2, L1-activation=sigmoid <C> 0.981 <C> 0.984 <C> 0.977 <C> 0.981 <CAP> Table 3: Comparison of Methods on the ColBERT Dataset
<R> <C> Features/Data set <C> Brown <C> 20News <C> Reuters8 <C> WebKB <R> <C> tf+PCA <C> 267 <C> 1,960 <C> 487 <C> 996 <R> <C> tf-idf+PCA <C> 310 <C> 3,565 <C> 1,184 <C> 2,074 <R> <C> word2vec 25..100 <C> 25..100 <C> 25..100 <C> 25..100 <C> 25..100 <R> <C> doc2vec 25..1,000 <C> 25..1,000 <C> 25..1,000 <C> 25..1,000 <C> 25..1,000 <R> <C> GOW-average <C> 19 <C> 19 <C> 19 <C> 19 <R> <C> GOW-histogram <C> 128 <C> 128 <C> 128 <C> 128 <R> <C> GOW-quantiles <C> 68 <C> 68 <C> 68 <C> 68 <CAP> Table 2: Dimensionality of the feature space.
<R> <C> [EMPTY] <C> 2-levels Prediction Day Acc. <C> 2-levels Prediction Hour RMSE <C> 1-level Prediction Day Acc. <C> 1-level Prediction Hour RMSE <R> <C> Intrepid <C> 66% <C> 4.1 <C> 48% <C> 6.88 <R> <C> Genetx <C> 80% <C> 3.36 <C> 54% <C> 6.78 <CAP> Table 1. Prediction measures for day-level prediction (Day Accuracy) and hour-level prediction (RMSE).
<R> <C> [EMPTY] <C> Method <C> Robust04 MAP <C> Robust04 nDCG@20 <C> ClueWeb MAP <C> ClueWeb nDCG@20 <R> <C> 1 <C> WABM25 <C> 0.2503 <C> 0.4102 <C> 0.1021 <C> 0.2070 <R> <C> 2 <C> WSO <C> 0.2702 <C> 0.4290 <C> 0.1297 <C> 0.2201 <R> <C> 3 <C> FSO <C> 0.1790 <C> 0.3519 <C> 0.0782 <C> 0.1730 <R> <C> 4 <C> WS+FT <C> 0.2830 <C> 0.4355 <C> 0.1346 <C> 0.2346 <R> <C> 5 <C> WS+SFT <C> 0.2711 <C> 0.4203 <C> 0.1002 <C> 0.1940 <R> <C> 6 <C> WS+RFT <C> 0.2810 <C> 0.4316 <C> 0.1286 <C> 0.2240 <R> <C> 7 <C> NLI <C> 0.2421 <C> 0.4092 <C> 0.1010 <C> 0.2004 <R> <C> 8 <C> CWSJT <C> [BOLD] 0.3024 <C> [BOLD] 0.4507 <C> [BOLD] 0.1372 <C> [BOLD] 0.2453 <R> <C> 9 <C> CWS+JT <C> 0.2786 <C> 0.4367 <C> 0.1310 <C> 0.2244 <CAP> Table 1: Performance of the proposed method and baseline models on different datasets. (or indicates that the improvements or degradations are statistically significant, at the 0.05 level using the paired two-tailed t-test. For all model, the improvement/degradations is with respect to the “weak supervision only” baseline (WSO). For CWSJT, the improvement over all baselines is considered and the Bonferroni correction is applied on the significant tests.)
<R> <C> [EMPTY] <C> Method <C> Robust04 MAP <C> Robust04 nDCG@20 <C> ClueWeb MAP <C> ClueWeb nDCG@20 <R> <C> a <C> CWSST <C> 0.2716 <C> 0.4237 <C> 0.1320 <C> 0.2213 <R> <C> b <C> CWSCT <C> 0.2961 <C> 0.4440 <C> [BOLD] 0.1378 <C> 0.2431 <R> <C> c <C> CWSPT <C> 0.2784 <C> 0.4292 <C> 0.1314 <C> 0.2207 <R> <C> [EMPTY] <C> CWSJT <C> [BOLD] 0.3024 <C> [BOLD] 0.4507 <C> 0.1372 <C> [BOLD] 0.2453 <CAP> Table 2: Performance of the variants of the proposed method on different datasets. (orindicates that the improvements or degradations are statistically significant, at the 0.05 level using the paired two-tailed t-test. For all model, the improvement/degradations is with respect to the “weak supervision only” baseline (WSO on Table 1) . For CWSJT, the improvement over all baselines is considered and the Bonferroni correction is applied on the significant tests.)
<R> <C> [EMPTY] <C> Method <C> SemEval-14 <C> SemEval-15 <R> <C> 1 <C> WALexicon <C> 0.5141 <C> 0.4471 <R> <C> 2 <C> WSO <C> 0.6719 <C> 0.5606 <R> <C> 3 <C> FSO <C> 0.6307 <C> 0.5811 <R> <C> 4 <C> WS+FT <C> 0.7080 <C> 0.6441 <R> <C> 5 <C> WS+SFT <C> 0.6875 <C> 0.6193 <R> <C> 6 <C> WS+RFT <C> 0.6932 <C> 0.6102 <R> <C> 7 <C> NLI <C> 0.7113 <C> 0.6433 <R> <C> 8 <C> CWSJT <C> [BOLD] 0.7362 <C> [BOLD] 0.6626 <R> <C> 9 <C> CWS+JT <C> 0.7310 <C> 0.6551 <R> <C> [EMPTY] <C> SemEval1th <C> 0.7162 <C> 0.6618 <CAP> Table 3: Performance of the baseline models as well as the proposed method on different datasets. (orindicates that the improvements or degradations are statistically significant, at the 0.05 level using the paired two-tailed t-test. For all model, the improvement/degradations is with respect to the “weak supervision only” baseline (WSO). For CWSJT, the improvement over all baselines is considered and the Bonferroni correction is applied on the significant tests.)
<R> <C> [EMPTY] <C> Method <C> SemEval-14 <C> SemEval-15 <R> <C> a <C> CWSST <C> 0.7183 <C> 0.6501 <R> <C> b <C> CWSCT <C> [BOLD] 0.7363 <C> [BOLD] 0.6667 <R> <C> c <C> CWSPT <C> 0.7009 <C> 0.6118 <R> <C> [EMPTY] <C> CWSJT <C> 0.7362 <C> 0.6626 <CAP> Table 4: Performance of the variants of the proposed method for sentiment classification task, on different datasets. (orindicates that the improvements or degradations are statistically significant, at the 0.05 level using the paired two-tailed t-test. For all model, the improvement/degradations is with respect to the “weak supervision only” baseline (WSO on Table 3) . For CWSJT, the improvement over all baselines is considered and the Bonferroni correction is applied on the significant tests.)
<R> <C> Method <C> Feature <C> BINARY (−1, +1) Prec <C> BINARY (−1, +1) Recall <C> BINARY (−1, +1) F1 <C> 7-CLASS (−3, …, +3) Prec <C> 7-CLASS (−3, …, +3) Recall <C> 7-CLASS (−3, …, +3) F1 <R> <C> UniModal-Baseline <C> Text (T) <C> [BOLD] 0.77 <C> [BOLD] 0.76 <C> [BOLD] 0.76 <C> [BOLD] 0.32 <C> [BOLD] 0.35 <C> [BOLD] 0.33 <R> <C> UniModal-Baseline <C> Audio (A) <C> 0.56 <C> 0.56 <C> 0.56 <C> 0.12 <C> 0.19 <C> 0.14 <R> <C> UniModal-Baseline <C> Video (V) <C> 0.57 <C> 0.47 <C> 0.48 <C> 0.12 <C> 0.19 <C> 0.12 <CAP> Table 1: Unimodal baseline results with 3 metrics: Precision, Recall and F-Score (F1)
<R> <C> Models <C> emotion extraction P <C> emotion extraction R <C> emotion extraction F1 <C> cause extraction P <C> cause extraction R <C> cause extraction F1 <C> emotion-cause pair extraction P <C> emotion-cause pair extraction R <C> emotion-cause pair extraction F1 <R> <C> [BOLD] Indep w/o filter <C> 0.8483 <C> 0.7961 <C> 0.8208 <C> 0.6898 <C> 0.5648 <C> 0.6198 <C> 0.5932 <C> 0.5094 <C> 0.5470 <R> <C> [BOLD] Inter-CE w/o filter <C> 0.8458 <C> 0.8035 <C> [BOLD] 0.8263 <C> 0.6838 <C> 0.5754 <C> 0.6231 <C> 0.5827 <C> 0.5300 <C> 0.5531 <R> <C> [BOLD] Inter-EC w/o filter <C> 0.8406 <C> [BOLD] 0.8097 <C> 0.8242 <C> 0.6989 <C> 0.5991 <C> 0.6426 <C> 0.5975 <C> 0.5538 <C> 0.5716 <R> <C> [BOLD] Indep <C> 0.8483 <C> 0.7961 <C> 0.8208 <C> 0.6898 <C> 0.5648 <C> 0.6198 <C> [BOLD] 0.6943 <C> 0.5047 <C> 0.5833 <R> <C> [BOLD] Inter-CE <C> 0.8458 <C> 0.8035 <C> [BOLD] 0.8263 <C> 0.6838 <C> 0.5754 <C> 0.6231 <C> 0.6780 <C> 0.5254 <C> 0.5896 <R> <C> [BOLD] Inter-EC <C> 0.8406 <C> [BOLD] 0.8097 <C> 0.8242 <C> 0.6989 <C> 0.5991 <C> 0.6426 <C> 0.6691 <C> 0.5503 <C> 0.6013 <R> <C> [BOLD] E2EECPE <C> [BOLD] 0.8595† <C> 0.7915 <C> 0.8238 <C> [BOLD] 0.7062 <C> [BOLD] 0.6030 <C> [BOLD] 0.6503 <C> 0.6478 <C> [BOLD] 0.6105† <C> [BOLD] 0.6280† <CAP> Table 2: Comparison results of emotion extraction, cause extraction, and emotion-cause pair extraction with precision, recall, and F1-measure as metrics. The results in bold are the best performing ones under each column. The results of emotion extraction and cause extraction for one-stage and two-stage models are exactly the same because one-stage models are ablated ones of two-stage models. † indicates results that are significantly better than best performing baseline Inter-EC with paired t-test (p is smaller than 0.05).
<R> <C> Models <C> P <C> R <C> F1 <R> <C> [BOLD] E2EECPE <C> [BOLD] 0.6478 <C> 0.6105 <C> [BOLD] 0.6280 <R> <C> [BOLD] E2EECPE  [ITALIC] w/o auxiliary <C> 0.5982 <C> 0.5340 <C> 0.5635 <R> <C> [BOLD] E2EECPE  [ITALIC] w/o position <C> 0.6421 <C> [BOLD] 0.6158 <C> 0.6275 <CAP> Table 3: Ablation study results. The results in bold are the best performing ones under each column.
<R> <C> [ITALIC] η <C> P <C> R <C> F1 <R> <C> 0.2 <C> 0.5743 <C> [BOLD] 0.6456 <C> 0.6071 <R> <C> 0.3 <C> 0.6478 <C> 0.6105 <C> [BOLD] 0.6280 <R> <C> 0.4 <C> 0.6757 <C> 0.5849 <C> 0.6265 <R> <C> 0.5 <C> 0.7185 <C> 0.5543 <C> 0.6255 <R> <C> 0.6 <C> [BOLD] 0.7326 <C> 0.5385 <C> 0.6201 <CAP> Table 4: Effect of threshold. The results in bold are the best performing ones under each column.
<R> <C> Metric <C> # of iterations 1 <C> # of iterations 2 <R> <C> Precision <C> 0.856 <C> 0.636 <R> <C> Recall <C> 0.517 <C> 0.587 <R> <C> F-measure <C> 0.645 <C> 0.611 <R> <C> # of extracted journal names <C> 1,712 <C> 2,544 <CAP> Table 3: Cumulative precision, recall, and F-measure.
<R> <C> [EMPTY] <C> WordSim-353 <C> MEN <R> <C> SG-50 <C> 0.607 <C> 0.650 <R> <C> CBOW-50 <C> 0.609 <C> 0.659 <R> <C> [BOLD] SD-CBOW <C> 0.614 <C> 0.660 <R> <C> [BOLD] SD-SG <C> 0.620 <C> 0.674 <R> <C> CBOW-200 <C> 0.643 <C> 0.712 <R> <C> SG-200 <C> [BOLD] 0.696 <C> [BOLD] 0.736 <CAP> (a) Semantic similarity (Spearman’s rank correlation)
<R> <C> [BOLD] Model <C> CoNLL04 (Relation)  [BOLD] P (%) <C> CoNLL04 (Relation)  [BOLD] R (%) <C> CoNLL04 (Relation)  [BOLD] F (%) <C> ADE (Relation)  [BOLD] P (%) <C> ADE (Relation)  [BOLD] R (%) <C> ADE (Relation)  [BOLD] F (%) <R> <C> Full model <C> 67.97 <C> 58.18 <C> [BOLD] 62.68 <C> 77.36 <C> 77.25 <C> [BOLD] 77.29 <R> <C> – Character-based Input <C> 67.30 <C> 52.69 <C> 59.09 <C> 76.73 <C> 76.44 <C> 76.58 <R> <C> – Dependency Embeddings <C> 66.56 <C> 57.69 <C> 61.78 <C> 75.79 <C> 77.16 <C> 76.45 <R> <C> – Position Embeddings <C> 68.57 <C> 57.34 <C> 62.43 <C> 75.94 <C> 76.62 <C> 76.27 <R> <C> – Pretrained Word Embeddings <C> 62.33 <C> 46.09 <C> 52.96 <C> 72.50 <C> 71.41 <C> 71.91 <CAP> Table 4: Ablation studies for relation extraction over the CoNLL04 and ADE dataset; each row after the first indicates removal of a particular feature/component.
<R> <C> [EMPTY] <C> vanilla-MemN2N Task 4: K. Verification <C> vanilla-MemN2N Task 4: K. Verification <C> vanilla-MemN2N Task 8: Triple <C> vanilla-MemN2N Task 8: Triple <C> Cont-MemN2N Task 4: K. Verification <C> Cont-MemN2N Task 4: K. Verification <C> Cont-MemN2N Task 8: Triple <C> Cont-MemN2N Task 8: Triple <R> <C> Train \Test <C> TestQA <C> TestAQ <C> TestQA <C> TestAQ <C> TestQA <C> TestAQ <C> TestQA <C> TestAQ <R> <C> TrainQA <C> 0.331 <C> 0.313 <C> 0.133 <C> 0.162 <C> 0.712 <C> 0.703 <C> 0.308 <C> 0.234 <R> <C> TrainAQ <C> 0.318 <C> 0.375 <C> 0.072 <C> 0.422 <C> 0.679 <C> 0.774 <C> 0.137 <C> 0.797 <CAP> Table 4: Mechanical Turk Task Results, using real data for training and testing.
<R> <C> [EMPTY] <C> vanilla-MemN2N Task 4: K. Verification <C> vanilla-MemN2N Task 4: K. Verification <C> vanilla-MemN2N Task 8: Triple <C> vanilla-MemN2N Task 8: Triple <C> Cont-MemN2N Task 4: K. Verification <C> Cont-MemN2N Task 4: K. Verification <C> Cont-MemN2N Task 8: Triple <C> Cont-MemN2N Task 8: Triple <R> <C> Train \Test <C> TestQA <C> TestAQ <C> TestQA <C> TestAQ <C> TestQA <C> TestAQ <C> TestQA <C> TestAQ <R> <C> TrainQA <C> 0.356 <C> 0.311 <C> 0.128 <C> 0.174 <C> 0.733 <C> 0.717 <C> 0.368 <C> 0.352 <R> <C> TrainAQ <C> 0.340 <C> 0.445 <C> 0.150 <C> 0.487 <C> 0.704 <C> 0.792 <C> 0.251 <C> 0.825 <CAP> Table 4: Mechanical Turk Task Results, using real data for training and testing.
<R> <C> [EMPTY] <C> vanilla-MemN2N Task 4: K. Verification <C> vanilla-MemN2N Task 4: K. Verification <C> vanilla-MemN2N Task 8: Triple <C> vanilla-MemN2N Task 8: Triple <C> Cont-MemN2N Task 4: K. Verification <C> Cont-MemN2N Task 4: K. Verification <C> Cont-MemN2N Task 8: Triple <C> Cont-MemN2N Task 8: Triple <R> <C> Train \Test <C> TestQA <C> TestAQ <C> TestQA <C> TestAQ <C> TestQA <C> TestAQ <C> TestQA <C> TestAQ <R> <C> TrainQA <C> 0.340 <C> 0.311 <C> 0.120 <C> 0.165 <C> 0.665 <C> 0.648 <C> 0.349 <C> 0.342 <R> <C> TrainAQ <C> 0.326 <C> 0.390 <C> 0.067 <C> 0.405 <C> 0.642 <C> 0.714 <C> 0.197 <C> 0.788 <CAP> Table 4: Mechanical Turk Task Results, using real data for training and testing.
<R> <C> Parsing TBRU recurrence,  [BOLD] r( [ITALIC] si)⊆{1,…, [ITALIC] n+ [ITALIC] i} Input links <C> Parsing TBRU recurrence,  [BOLD] r( [ITALIC] si)⊆{1,…, [ITALIC] n+ [ITALIC] i} Recurrent edges <C> Parsing Accuracy (%) News <C> Parsing Accuracy (%) Questions <C> Runtime <R> <C> { [ITALIC] n} <C> { [ITALIC] n+ [ITALIC] i−1} <C> 27.3 <C> 70.1 <C> [ITALIC] O( [ITALIC] n) <R> <C> { [ITALIC] n} <C> { \textsc  [ITALIC] Subtree( [ITALIC] si, [ITALIC] S0), \textsc  [ITALIC] Subtree( [ITALIC] si, [ITALIC] S1)} <C> 36.0 <C> 75.6 <C> [ITALIC] O( [ITALIC] n) <R> <C> Attention <C> { [ITALIC] n+ [ITALIC] i−1} <C> 76.1 <C> 84.8 <C> [ITALIC] O( [ITALIC] n2) <R> <C> Attention <C> { \textsc  [ITALIC] Subtree( [ITALIC] si, [ITALIC] S0), \textsc  [ITALIC] Subtree( [ITALIC] si, [ITALIC] S1)} <C> 89.0 <C> 91.9 <C> [ITALIC] O( [ITALIC] n2) <R> <C> \textsc  [ITALIC] Input( [ITALIC] si) <C> { [ITALIC] n+ [ITALIC] i−1} <C> 87.1 <C> 89.7 <C> [ITALIC] O( [ITALIC] n) <R> <C> \textsc  [ITALIC] Input( [ITALIC] si) <C> { \textsc  [ITALIC] Subtree( [ITALIC] si, [ITALIC] S0), \textsc  [ITALIC] Subtree( [ITALIC] si, [ITALIC] S1)} <C> [BOLD] 90.9 <C> [BOLD] 92.1 <C> [ITALIC] O( [ITALIC] n) <CAP> Table 1: Dynamic links enable much more accurate, efficient linear-time parsing models on the Treebank Union dev set. We vary the recurrences r to explore utilizing explicit structure in the parsing TBRU. Utilizing the explicit \textscInput(si) pointer is more effective and more efficient than a quadratic attention mechanism. Incorporating the explicit stack structure via recurrent links further improves performance.
<R> <C> [BOLD] Model/Metrics <C> [BOLD] Accuracy <C> [BOLD] Precision-1 <C> [BOLD] Recall-1 <C> [BOLD] F1 <C> [BOLD] Acc&F1 <R> <C> [ITALIC] RoBERTaLarge <C> 84.86% <C> 0.585 <C> 0.514 <C> 0.541 <C> 0.695 <R> <C> [ITALIC] ALBERTxxlarge, [ITALIC] v2 <C> 84.90% <C> 0.596 <C> 0.472 <C> 0.524 <C> 0.686 <R> <C> [BOLD] Our Model <C> [BOLD] 85.55% <C> [BOLD] 0.623 <C> [BOLD] 0.515 <C> [BOLD] 0.558 <C> [BOLD] 0.707 <CAP> Table 1: Label-averaged values for each metric for RoBERTa,ALBERT, and our best performing ensemble model.
<R> <C> [BOLD] Model/Metrics <C> [BOLD] Accuracy <C> [BOLD] Precision-1 <C> [BOLD] Recall-1 <C> [BOLD] F1 <C> [BOLD] Acc&F1 <R> <C> [BOLD] Model 1 <C> 85.18% <C> 0.595 <C> [BOLD] 0.516 <C> 0.547 <C> 0.699 <R> <C> [BOLD] Model 2 <C> 85.42% <C> 0.622 <C> 0.490 <C> 0.544 <C> 0.699 <R> <C> [BOLD] Model 3 <C> 85.47% <C> 0.619 <C> 0.514 <C> 0.557 <C> 0.706 <R> <C> [BOLD] Model 4 <C> 85.48% <C> 0.622 <C> 0.480 <C> 0.557 <C> 0.706 <R> <C> [BOLD] Model 5 <C> [BOLD] 85.54% <C> [BOLD] 0.623 <C> 0.515 <C> [BOLD] 0.558 <C> [BOLD] 0.707 <CAP> Table 4: Label-averaged values for each metric for different ensemble models.
<R> <C> [BOLD] Labels <C> [ITALIC] ρ [BOLD]  with Impact <R> <C> [BOLD] Emotional Disclosure <C> 0.046 <R> <C> [BOLD] Informational Disclosure <C> 0.024 <R> <C> [BOLD] Support <C> 0.021 <R> <C> [BOLD] General Support <C> 0.028 <R> <C> [BOLD] Information Support <C> 0.005 <R> <C> [BOLD] Emotional Support <C> 0.019 <CAP> Table 6: The relationship between Labels and Impact, as represented by Pearson correlation coefficient, ρ.
<R> <C> Dataset <C> Dev BL <C> Dev MA <C> Test BL <C> Test MA <C> Test SotA <R> <C> EN - News <C> 83.6 <C> 85.5 <C> 69.7 <C> 86.0 <C> [BOLD] 87.4 <R> <C> EN - WikiNews <C> 80.4 <C> 82.8 <C> 65.8 <C> 81.6 <C> [BOLD] 84.0 <R> <C> EN - Wikipedia <C> 74.2 <C> 76.6 <C> 70.1 <C> 76.1 <C> [BOLD] 81.2 <R> <C> ES <C> 78.0 <C> 77.1 <C> 69.6 <C> [BOLD] 77.6 <C> 77.0 <R> <C> DE <C> 79.5 <C> 74.6 <C> 72.4 <C> 74.8 <C> [BOLD] 75.5 <R> <C> Mean <C> 79.1 <C> 79.3 <C> 69.5 <C> 79.2 <C> [EMPTY] <CAP> Table 3: Macro-F1 for the baseline (BL), our monolingual approach (MA), and the state of the art (SotA) on the Dev and Test splits of each dataset.
<R> <C> [EMPTY] <C> SQuAD train <C> SQuAD dev <C> Jp-News train <C> Jp-News dev <C> Jp-News test <R> <C> No. articles <C> 442 <C> 48 <C> 4,000 <C> 500 <C> 500 <R> <C> No. questions <C> 87,599 <C> 10,570 <C> 66,073 <C> 8,247 <C> 8,272 <R> <C> No. passages <C> 18,896 <C> 2,067 <C> 10,024 <C> 1,214 <C> 1,247 <R> <C> No. answers <C> 87,599 <C> 34,726 <C> 179,908 <C> 22,500 <C> 22,500 <R> <C> Len. questions <C> 11.4 <C> 11.5 <C> 21.9 <C> 21.8 <C> 21.9 <R> <C> Len. passages <C> 140.3 <C> 144.5 <C> 181.4 <C> 176.2 <C> 177.7 <R> <C> Len. answers <C> 3.5 <C> 3.3 <C> 4.3 <C> 4.5 <C> 4.2 <CAP> Table 1. Number and mean length (in tokens) of each item in the training datasets.
<R> <C> [BOLD] Question Category <C> [BOLD] QA  [BOLD] Accuracy <C> [BOLD] N <R> <C> [ITALIC] Strong Performance <C> [EMPTY] <C> [EMPTY] <R> <C> Life - Human Health <C> 73% <C> 11 <R> <C> Forces - Friction <C> 71% <C> 7 <R> <C> Energy - Sound <C> 71% <C> 7 <R> <C> Energy - Light <C> 70% <C> 10 <R> <C> Matter - Material Properties <C> 68% <C> 22 <R> <C> Forces - Gravity <C> 66% <C> 9 <R> <C> Science - Scientific Models <C> 66% <C> 9 <R> <C> Earth - Inner Core <C> 64% <C> 33 <R> <C> Astronomy - Natural Cycles <C> 64% <C> 11 <R> <C> Energy - Device Use <C> 63% <C> 8 <R> <C> Energy - Waves <C> 63% <C> 8 <R> <C> Earth - Weather <C> 62% <C> 74 <R> <C> Energy - Conversion <C> 62% <C> 13 <R> <C> Energy - Thermal <C> 60% <C> 10 <R> <C> [ITALIC] Above Average Performance <C> [EMPTY] <C> [EMPTY] <R> <C> Earth - Geology <C> 58% <C> 38 <R> <C> Science - Graphs <C> 56% <C> 9 <R> <C> Life - Environmental Adaptations <C> 53% <C> 32 <R> <C> Matter - Phys./Chemical Changes <C> 53% <C> 17 <R> <C> Astronomy - Features <C> 52% <C> 27 <R> <C> [ITALIC] Approximately Average Performance <C> [EMPTY] <C> [EMPTY] <R> <C> Earth - Human Impacts <C> 51% <C> 84 <R> <C> Matter - Chemistry <C> 50% <C> 46 <R> <C> Life - Features and Functions <C> 49% <C> 176 <R> <C> [ITALIC] Below Average Performance <C> [EMPTY] <C> [EMPTY] <R> <C> Science - Scientific Inference <C> 47% <C> 58 <R> <C> Life - Food Chains <C> 44% <C> 54 <R> <C> Astronomy - Celestial Distances <C> 44% <C> 9 <R> <C> Life - Reproduction <C> 41% <C> 41 <R> <C> Matter - Measurement <C> 40% <C> 15 <R> <C> Life - Classification <C> 38% <C> 13 <R> <C> Matter - Changes of State <C> 29% <C> 21 <R> <C> [ITALIC] Below Chance Performance <C> [EMPTY] <C> [EMPTY] <R> <C> Earth - Outer Core <C> 17% <C> 12 <R> <C> Safety - Safety Procedures <C> 7% <C> 14 <CAP> Table 9: Analysis of question answering performance on specific question classes on the BERT-QA model (L6). Question classes in this table are at the L2 level of specificity. Performance is reported on the development set, where N represents the total number of questions within a given question class.
<R> <C> [BOLD] Classification <C> [BOLD] # of <C> [BOLD] Interannotator <R> <C> [BOLD] Level <C> [BOLD] Classes <C> [BOLD] Agreement ( [ITALIC] κ) <R> <C> L1 (Coarsest) <C> 9 <C> 0.85 <R> <C> L2 <C> 88 <C> 0.71 <R> <C> L3 <C> 243 <C> 0.64 <R> <C> L4 <C> 335 <C> 0.60 <R> <C> L5 <C> 379 <C> 0.58 <R> <C> L6 (Finest) <C> 406 <C> 0.58 <CAP> Table 10: Interannotator Agreement at L6 (the native level the annotation was completed at), as well as agreement for truncated levels of the heirarchy from coarse to fine classification.
<R> <C> Model <C> No.of Parameters <R> <C> LR <C> 8001 <R> <C> LSTM <C> 5,652,633 <R> <C> Bi-LSTM <C> 5,884,033 <R> <C> CNN-rand <C> 5,821,801 <R> <C> CNN-static <C> 5,821,801 <CAP> Table 22: Number of Model Parameters on Books Dataset
<R> <C> [BOLD] Experiment <C> [BOLD] Result <R> <C> Overall Classification Accuracy <C> 68.94 <R> <C> First Sentence Recall <C> [BOLD] 55.55 <R> <C> Second Sentence Recall <C> 65.65 <CAP> Table 2: The results for the substring experiment (Experiment 3.1.2).
<R> <C> [BOLD] Word (In template) <C> [BOLD] Avg. Similarity <R> <C> daxy <C> 0.949 <R> <C> tall <C> 0.855 <R> <C> ok <C> 0.887 <R> <C> fat <C> 0.882 <R> <C> fit <C> 0.878 <CAP> Table 3: Results for the average cosine similarity of the encoder representation before and after observing the word in the template (Experiment 3.2.1). It is clear that the average similarity for the word ‘daxy’ is quite higher than the other words. This implies that even after observing the word ‘daxy’, the hidden representation of the encoder did not change much.
<R> <C> [EMPTY] <C> baseline <C> trimmed <C> proposed <R> <C> EN-JA(B-rank) <C> 0.514 <C> 0.542 <C> [BOLD] 0.593 <R> <C> EN-JA(A-rank) <C> 0.487 <C> 0.554 <C> [BOLD] 0.591 <R> <C> EN-JA(S-rank) <C> 0.325 <C> 0.334 <C> [BOLD] 0.411 <R> <C> EN-FR <C> 0.631 <C> 0.610 <C> [BOLD] 0.691 <R> <C> EN-IT <C> 0.569 <C> 0.543 <C> [BOLD] 0.576 <CAP> Table 1: Pearson’s r scores for predicted METEOR for baseline, trimmed and proposed feature sets on the test set (highest accuracy for each dataset indicated in bold).
<R> <C> Task <C> Size <C> # Labels <C> Tok/typ <C> %OOV <C> [ITALIC] H( [ITALIC] y) <C> || [ITALIC] X|| [ITALIC] F <C> JSD <C> [ITALIC] F1 <R> <C> ccg <C> 39,604 <C> 1,285 <C> 23.08 <C> 1.13 <C> 3.28 <C> 981.3 <C> 0.41 <C> 86.1 <R> <C> chu <C> 8,936 <C> 22 <C> 12.01 <C> 1.35 <C> 1.84 <C> 466.4 <C> 0.47 <C> 93.9 <R> <C> com <C> 9,600 <C> 2 <C> 9.47 <C> 0.99 <C> 0.47 <C> 519.3 <C> 0.44 <C> 51.9 <R> <C> fnt <C> 3,711 <C> 2 <C> 8.44 <C> 1.79 <C> 0.51 <C> 286.8 <C> 0.30 <C> 58.0 <R> <C> pos <C> 1,002 <C> 12 <C> 3.24 <C> 14.15 <C> 2.27 <C> 116.9 <C> 0.24 <C> 82.6 <R> <C> hyp <C> 2,000 <C> 2 <C> 6.14 <C> 2.14 <C> 0.47 <C> 269.3 <C> 0.48 <C> 39.3 <R> <C> key <C> 2,398 <C> 2 <C> 9.10 <C> 4.46 <C> 0.61 <C> 289.1 <C> 0.39 <C> 64.5 <R> <C> mwe <C> 3,312 <C> 3 <C> 9.07 <C> 0.73 <C> 0.53 <C> 217.3 <C> 0.18 <C> 43.3 <R> <C> sem <C> 15,465 <C> 73 <C> 11.16 <C> 4.72 <C> 2.19 <C> 614.6 <C> 0.35 <C> 70.8 <R> <C> str <C> 3,312 <C> 118 <C> 9.07 <C> 0.73 <C> 2.43 <C> 217.3 <C> 0.26 <C> 61.5 <CAP> Table 1: Dataset characteristics for the individual tasks as defined in Table 2, as well as single-task model performance on test data (micro-averaged F1).
<R> <C> [EMPTY] <C> Acc. <C> [ITALIC] F1 (gain) <R> <C> Majority baseline <C> 0.555 <C> 0.615 <R> <C> All features <C> 0.749 <C> 0.669 <R> <C> Best, data features only <C> 0.665 <C> 0.542 <R> <C> Best combination <C> 0.785 <C> 0.713 <CAP> Table 3: Mean performance across 100 runs of 5-fold CV logistic regression.
<R> <C> [EMPTY] <C> OS <C> R <C> WO <C> SD <C> WD <C> TR <C> TE <R> <C> S(D)AE <C> [EMPTY] <C> [EMPTY] <C> ✓ <C> 2400 <C> 100 <C> 72* <C> 640 <R> <C> ParagraphVec <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 100 <C> 100 <C> 4 <C> 1130 <R> <C> CBOW <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> 500 <C> 500 <C> 2 <C> 145 <R> <C> SkipThought <C> ✓ <C> [EMPTY] <C> ✓ <C> 4800 <C> 620 <C> 336* <C> 890 <R> <C> FastSent <C> ✓ <C> [EMPTY] <C> [EMPTY] <C> 100 <C> 100 <C> 2 <C> 140 <R> <C> DictRep <C> [EMPTY] <C> ✓ <C> ✓ <C> 500 <C> 256 <C> 24* <C> 470 <R> <C> CaptionRep <C> [EMPTY] <C> ✓ <C> ✓ <C> 500 <C> 256 <C> 24* <C> 470 <R> <C> NMT <C> [EMPTY] <C> ✓ <C> ✓ <C> 2400 <C> 512 <C> 72* <C> 720 <CAP> Table 1: Properties of models compared in this study OS: requires training corpus of sentences in order. R: requires structured resource for training. WO: encoder sensitive to word order. SD: dimension of sentence representation. WD: dimension of word representation. TR: approximate training time (hours) on the dataset in this paper. * indicates trained on GPU. TE: approximate time (s) taken to encode 0.5m sentences.
<R> <C> Model <C> [BOLD] i [ITALIC] j <C> [BOLD] f [ITALIC] j <C> [BOLD] r [ITALIC] j/ [BOLD] o [ITALIC] j <R> <C> hybrid, full <C> 0.567 <C> 0.502 <C> 0.405 <R> <C> hybrid, summary <C> 0.539 <C> 0.540 <C> 0.428 <R> <C> + att. <C> 0.540 <C> 0.559 <C> 0.459 <CAP> Table 2: Average activation of gates on test set.
<R> <C> Model <C> Errors <R> <C> SVM <C> 66 <R> <C> BiNB <C> 62 <R> <C> MaxEnt <C> 61 <R> <C> Max-TDNN <C> 76 <R> <C> NBoW <C> 68 <R> <C> DCNN <C> 45 <R> <C> Our Model <C> 46 <CAP> Table 1: Left: Number of test set errors on the twitter sentiment dataset. The first block of three entries is from Go et al. [5], the second block is from Kalchbrenner et al. [13]. Right: Error rates on the IMDB movie review data set. The first block is from Maas et al. [16], the second from Dahl et al. [3], the third from Wang and Manning [24] and the fourth from Le and Mikolov [15].
<R> <C> Proportion 100% <C> Summary 83.03 <C> Random 83.03 <C> Margin — <C> Fixed <C> Summary <C> Random <C> Margin <R> <C> 50% <C> 83.53 <C> 79.79 <C> +3.74 <C> Pick 5 <C> 83.07 <C> 80.02 <C> +3.05 <R> <C> 33% <C> 83.10 <C> 76.72 <C> +6.38 <C> Pick 4 <C> 83.09 <C> 79.05 <C> +4.04 <R> <C> 25% <C> 82.91 <C> 74.87 <C> +8.04 <C> Pick 3 <C> 82.88 <C> 77.15 <C> +5.73 <R> <C> 20% <C> 82.67 <C> 73.20 <C> +9.47 <C> Pick 2 <C> 82.04 <C> 74.48 <C> +7.56 <R> <C> First and last <C> 68.62 <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <CAP> Table 2: Results of classifying summaries with Naïve Bayes. Results labelled proportion indicate selecting up to the indicated percentage of sentences in the review, and results labelled fixed show the result of selecting a fixed number of sentences from each. The summary column shows the accuracy of Naïve Bayes on summaries produced by our model. The random column shows the same model classifying summaries created by selecting sentences at random. The margin column shows the difference in accuracy between our model and the random summaries.
<R> <C> Model <C> 1s <C> 384w <C> BLEU <R> <C> Transf.-base-AAN <C> 1018.8 <C> 397.5 <C> 27.5 <R> <C> +shortlist <C> 758.1 <C> 293.7 <C> 27.5 <R> <C> +int16 <C> 2703.2 <C> 491.4 <C> 27.5 <R> <C> +memoization <C> 572.9 <C> 294.3 <C> 27.5 <R> <C> +auto-tuning <C> 574.8 <C> 273.2 <C> 27.5 <R> <C> Transformer-big <C> 4797.0 <C> 1537.8 <C> 28.1 <R> <C> +clip=2 (+mem.) <C> 5006.9 <C> 1737.1 <C> 27.7 <R> <C> +int8 (+mem.) <C> 1772.6 <C> 1169.9 <C> 27.5 <CAP> Table 2: Time to translate newstest2014 with batch-size equal to 1 sentence (1s) and around 384 words (384w) using integer multiplication variants vs 32-bit float matrix multiplication.
<R> <C> [BOLD] Model <C> [BOLD] Entity  [BOLD] F1 <C> [BOLD] Entity  [BOLD] P <C> [BOLD] Entity  [BOLD] R <C> [BOLD] Activity  [BOLD] F1 <C> [BOLD] Activity  [BOLD] P <C> [BOLD] Activity  [BOLD] R <C> [BOLD] Tense  [BOLD] Acc. <C> [BOLD] Cmd  [BOLD] Acc. <R> <C> VHRED <C> 2.53 <C> 3.28 <C> 2.41 <C> 4.63 <C> [BOLD] 6.43 <C> 4.31 <C> 20.2 <C> 92.02 <R> <C> HRED <C> 2.22 <C> 2.81 <C> 2.16 <C> 4.34 <C> 5.93 <C> 4.05 <C> 22.2 <C> 92.58 <R> <C> HRED-attend <C> 2.44 <C> 3.13 <C> 2.08 <C> 4.86 <C> 6.04 <C> 4.65 <C> [BOLD] 29.75 <C> [BOLD] 99.89 <R> <C> MrRNN-Noun <C> 6.31 <C> 8.68 <C> 5.55 <C> 4.04 <C> 5.81 <C> 3.56 <C> 24.03 <C> 90.66 <R> <C> [BOLD] Mask & Focus <C> [BOLD] 7.82 <C> [BOLD] 8.78 <C> [BOLD] 8.69 <C> [BOLD] 5.43 <C> [BOLD] 6.42 <C> [BOLD] 5.90 <C> 24.47 <C> 93.19 <R> <C> MrRNN-ActEnt <C> 3.72 <C> 4.91 <C> 3.36 <C> 11.43 <C> 16.84 <C> 9.72 <C> 29.01 <C> 95.04 <CAP> Table 2: Activity & Entity metrics for the Ubuntu corpus. HRED, VHRED, MrRNN-Noun and MrRNN-ActEnt as reported by Serban et al. serban2017multiresolution. Note that MrRNN-ActEnt is supervised since the activities and entities used in the evaluation metrics were also used during training.
<R> <C> [BOLD] Model <C> [BOLD] Relevance <C> [BOLD] GR Ratio <R> <C> Gold <C> 2.66 <C> 0.05 <R> <C> HRED <C> 1.25 <C> 0.53 <R> <C> VHRED <C> 1.48 <C> 0.39 <R> <C> MrRNN-Noun <C> 1.56 <C> 0.32 <R> <C> Mask & Focus <C> [BOLD] 1.79 <C> [BOLD] 0.18 <CAP> Table 4: Human judgements on model responses on 100 random samples from Ubuntu corpus. Relevance scores are on a scale of (0-4). GR stands for Generic Response
<R> <C> [BOLD] Order <C> [BOLD] Enc-Dec <C> [BOLD] A-GEM <C> [BOLD] Replay <C> [BOLD] MbPA <C> [BOLD] MbPArand{+}{+} <C> [BOLD] MbPA++ <C> [BOLD] MTL <R> <C> i <C> 14.8 <C> 70.6 <C> 67.2 <C> 68.9 <C> 59.4 <C> [BOLD] 70.8 <C> 73.7 <R> <C> ii <C> 27.8 <C> 65.9 <C> 64.7 <C> 68.9 <C> 58.7 <C> [BOLD] 70.9 <C> 73.2 <R> <C> iii <C> 26.7 <C> 67.5 <C> 64.7 <C> 68.8 <C> 57.1 <C> [BOLD] 70.2 <C> 73.7 <R> <C> iv <C> 4.5 <C> 63.6 <C> 44.6 <C> 68.7 <C> 57.4 <C> [BOLD] 70.7 <C> 73.7 <R> <C> class.-avg. <C> 18.4 <C> 66.9 <C> 57.8 <C> 68.8 <C> 58.2 <C> [BOLD] 70.6 <C> 73.6 <R> <C> i <C> 57.7 <C> 56.1 <C> 60.1 <C> 60.8 <C> 60.0 <C> [BOLD] 62.0 <C> 67.6 <R> <C> ii <C> 55.1 <C> 58.4 <C> 60.3 <C> 60.1 <C> 60.0 <C> [BOLD] 62.4 <C> 67.9 <R> <C> iii <C> 41.6 <C> 52.4 <C> 58.8 <C> 58.9 <C> 58.8 <C> [BOLD] 61.4 <C> 67.9 <R> <C> iv <C> 58.2 <C> 57.9 <C> 59.8 <C> 61.5 <C> 59.8 <C> [BOLD] 62.4 <C> 67.8 <R> <C> QA-avg. <C> 53.1 <C> 56.2 <C> 57.9 <C> 60.3 <C> 59.7 <C> [BOLD] 62.4 <C> 67.8 <CAP> Table 1: Summary of results on text classification (above) and question answering (below) using averaged accuracy and F1 score respectively (see Appendix A for the dataset orderings).
<R> <C> [EMPTY] <C> 10% <C> 50% <C> 100% <R> <C> class. <C> 67.6 <C> 70.3 <C> 70.6 <R> <C> QA <C> 61.5 <C> 61.6 <C> 62.0 <CAP> Table 2: Results with limited memory capacity.
<R> <C> [EMPTY] <C> 8 <C> 16 <C> 32 <C> 64 <C> 128 <R> <C> class. <C> 68.4 <C> 69.3 <C> 70.6 <C> 71.3 <C> 71.6 <R> <C> QA <C> 60.2 <C> 60.8 <C> 62.0 <C> - <C> - <CAP> Table 2: Results with limited memory capacity.
<R> <C> [EMPTY] <C> Persona <C> Ubuntu <R> <C> IR baseline  <C> 24.1 <C> [EMPTY] <R> <C> Starspace  <C> 31.8 <C> [EMPTY] <R> <C> KVPM  <C> 34.9 <C> [EMPTY] <R> <C> DE <C> 35.2 <C> 7.6 <R> <C> ADE <C> 35.8 <C> 15.9 <R> <C> ADE + REG <C> 38.0 <C> 16.0 <R> <C> ADE + WE <C> 36.2 <C> 15.3 <R> <C> ADE + WE + REG <C> 38.1 <C> 15.6 <CAP> Table 1: Recall@1 in percentage comparison on two dialogue datasets.
<R> <C> [BOLD] Role <C> [BOLD] 10 <C> [BOLD] 50 <C> [BOLD] 90 <R> <C> affiliate <C> 93.71/95.51 <C> 93.10/95.29 <C> 94.12/94.12 <R> <C> trustee <C> 95.37/99.20 <C> 93.58/99.03 <C> 97.67/100.00 <R> <C> issuer <C> 90.48/95.00 <C> 98.15/89.83 <C> 90.91/90.91 <CAP> Table 1. Preliminary empirical results for three roles. The header indicates the amount of training data used (the rest used for testing). Metrics are Precision/Recall percentages.
<R> <C> System Single systems <C> System NT’18 WMT bitext <C> de-en 46.2 <C> en-de 45.9 <C> en-ru 33.5 <C> ru-en 33.4 <C> zh-en 25.8 <C> en-zh 39.2 <C> de-fr - <C> fr-de - <R> <C> Single systems <C> NT’18 CCMatrix <C> [BOLD] 47.4 <C> [BOLD] 49.7 <C> [BOLD] 35.4 <C> [BOLD] 35.3 <C> 25.8 <C> [BOLD] 41.3 <C> - <C> - <R> <C> Single systems <C> NT’19 WMT bitext <C> 41.0 <C> 40.4 <C> 31.4 <C> 38.1 <C> - <C> - <C> - <C> - <R> <C> Single systems <C> NT’19 CCMatrix <C> 40.7 <C> [BOLD] 44.7 <C> [BOLD] 34.8 <C> [BOLD] 39.5 <C> 29.2 <C> 34.8 <C> 37.0 <C> 33.0 <R> <C> Ensembles + BT + Reranking <C> NT’19 best <C> 42.8 <C> 44.9 <C> 36.3 <C> 40.1 <C> 39.3 <C> 44.6 <C> 37.3 <C> 35.0 <CAP> Table 5: BLEU scores on the Newstest’18 (NT’18) and Newstest’19 (NT’19) test set. Newstest’18 WMT bitext and Newstest’19 WMT bitext are published results for single models trained on parallel WMT’19 data, for En-De and En-Ru results are from (ngwmt19), for En-Zh results are from (baiduwmt19). Newstest’19 best are the best BLEU scores achieved by ensembles of models trained on both parallel and back-translated WMT’19 data as of the moment of writing, according to http://matrix.statmt.org/
<R> <C> [EMPTY] <C> [BOLD] Model / Task <C> [BOLD] Len. <C> [BOLD] Cont. <C> [BOLD] Order <C> [BOLD] Slang <C> [BOLD] Hash tag <C> [BOLD] NE <C> [BOLD] Is Rep. <C> [BOLD] Rep. Time <R> <C> [BOLD] Unsupervised <C> BOW <C> 37.83 <C> 97.37 <C> 60.36 <C> 78.13 <C> 99.28 <C> 89.66 <C> 78.14 <C> [BOLD] 35.98 <R> <C> [BOLD] Unsupervised <C> LDA <C> 25.11 <C> 97.72 <C> 60.62 <C> 76.82 <C> [BOLD] 99.35 <C> 97.24 <C> 60.12 <C> 28.03 <R> <C> [BOLD] Unsupervised <C> BOM <C> 47.64 <C> 98.67 <C> [BOLD] 61.25 <C> 75.26 <C> 99.33 <C> [BOLD] 98.06 <C> 66.25 <C> 28.43 <R> <C> [BOLD] Unsupervised <C> DSSM <C> 57.76 <C> 98.57 <C> 59.01 <C> 76.89 <C> 99.33 <C> 97.16 <C> 76.47 <C> 29.08 <R> <C> [BOLD] Unsupervised <C> CDSSM <C> 47.75 <C> 98.09 <C> 57.66 <C> 69.8 <C> [BOLD] 99.35 <C> 97.41 <C> 73.92 <C> 28.49 <R> <C> [BOLD] Unsupervised <C> PV <C> 13.58 <C> 94.9 <C> 60.92 <C> 76.09 <C> 85.61 <C> 98.02 <C> 54.68 <C> 27.58 <R> <C> [BOLD] Unsupervised <C> STV <C> 71.85 <C> [BOLD] 98.85 <C> 57.7 <C> 76.66 <C> 99.32 <C> 97.92 <C> [BOLD] 96.41 <C> 29.25 <R> <C> [BOLD] Unsupervised <C> T2V <C> 73.58 <C> 98.36 <C> 60.62 <C> 62.34 <C> 99.32 <C> 92.93 <C> 95.73 <C> 31.59 <R> <C> [BOLD] Unsupervised <C> SCBOW <C> 32.13 <C> 97.94 <C> 58.39 <C> 74.24 <C> [BOLD] 99.35 <C> 97.79 <C> 60.38 <C> 28.39 <R> <C> [BOLD] Super. <C> CNN <C> 59.48 <C> 97.71 <C> 61.13 <C> 77.42 <C> 99.31 <C> 91.38 <C> 92.66 <C> 31.73 <R> <C> [BOLD] Super. <C> LSTM <C> [BOLD] 99.79 <C> 97.39 <C> 60.74 <C> 76.24 <C> 99.28 <C> 90.36 <C> 92.39 <C> 28.46 <R> <C> [BOLD] Super. <C> BLSTM <C> 98.72 <C> 97.47 <C> 60.85 <C> [BOLD] 80.52 <C> 99.28 <C> 90.89 <C> 92.76 <C> 27.99 <R> <C> [BOLD] Super. <C> FT <C> 24.56 <C> 92.15 <C> 60.06 <C> 67.48 <C> 89.11 <C> 78.89 <C> 74.08 <C> 28.35 <CAP> Table 3: Elementary Property Prediction Task F1-Score (%) - Performance Comparison
<R> <C> Model <C> PERSONA-CHAT Original <C> PERSONA-CHAT Original <C> PERSONA-CHAT Original <C> PERSONA-CHAT Revised <C> PERSONA-CHAT Revised <C> PERSONA-CHAT Revised <C> CMU_DoG <C> CMU_DoG <C> CMU_DoG <R> <C> Model <C> [BOLD] R@1 <C> [BOLD] R@2 <C> [BOLD] R@5 <C> [BOLD] R@1 <C> [BOLD] R@2 <C> [BOLD] R@5 <C> [BOLD] R@1 <C> [BOLD] R@2 <C> [BOLD] R@5 <R> <C> Starspace Wu et al. ( 2018 ) <C> 49.1 <C> 60.2 <C> 76.5 <C> 32.2 <C> 48.3 <C> 66.7 <C> 50.7 <C> 64.5 <C> 80.3 <R> <C> Profile Memory Zhang et al. ( 2018 ) <C> 50.9 <C> 60.7 <C> 75.7 <C> 35.4 <C> 48.3 <C> 67.5 <C> 51.6 <C> 65.8 <C> 81.4 <R> <C> KV Profile Memory Zhang et al. ( 2018 ) <C> 51.1 <C> 61.8 <C> 77.4 <C> 35.1 <C> 45.7 <C> 66.3 <C> 56.1 <C> 69.9 <C> 82.4 <R> <C> Transformer Mazaré et al. ( 2018 ) <C> 54.2 <C> 68.3 <C> 83.8 <C> 42.1 <C> 56.5 <C> 75.0 <C> 60.3 <C> 74.4 <C> 87.4 <R> <C> DGMN Zhao et al. ( 2019 ) <C> 67.6 <C> 80.2 <C> 92.9 <C> 58.8 <C> 62.5 <C> 87.7 <C> 65.6 <C> 78.3 <C> 91.2 <R> <C> DIM Gu et al. ( 2019b ) <C> 78.8 <C> 89.5 <C> 97.0 <C> 70.7 <C> 84.2 <C> 95.0 <C> 78.7 <C> 89.0 <C> 97.1 <R> <C> FIRE (Ours) <C> [BOLD] 81.6 <C> [BOLD] 91.2 <C> [BOLD] 97.8 <C> [BOLD] 74.8 <C> [BOLD] 86.9 <C> [BOLD] 95.9 <C> [BOLD] 81.8 <C> [BOLD] 90.8 <C> [BOLD] 97.4 <CAP> Table 1: Performance of the proposed and previous methods on the test sets of the PERSONA-CHAT and CMU_DoG datasets. The meanings of “Original”, and “Revised” can be found in Section 5.1.
<R> <C> Model <C> PERSONA-CHAT Original <C> PERSONA-CHAT Revised <C> CMU_DoG <R> <C> Model <C> [BOLD] R@1 <C> [BOLD] R@1 <C> [BOLD] R@1 <R> <C> FIRE <C> 82.3 <C> 75.2 <C> 83.4 <R> <C> - Iterative refer <C> 81.3 <C> 73.8 <C> 81.6 <R> <C> - Pre-filter <C> 78.9 <C> 71.1 <C> 78.8 <R> <C> C-R <C> 65.6 <C> 66.2 <C> 79.7 <R> <C> C-R → Fusion <C> 67.0 <C> 66.4 <C> 80.9 <R> <C> Filter → C-R <C> 78.8 <C> 70.2 <C> 81.4 <R> <C> K-R <C> 51.6 <C> 34.3 <C> 57.8 <R> <C> K-R → Fusion <C> 54.2 <C> 39.4 <C> 63.1 <R> <C> Filter → K-R <C> 63.6 <C> 51.0 <C> 73.5 <CAP> Table 2: Ablation tests of the FIRE model on the validation sets. C-R denotes the context-response matching and K-R denotes the knowledge-response matching. → denotes the operation order.
<R> <C> [EMPTY] <C> Model <C> SST2 <C> MR <C> CR <R> <C> 1 <C> CNN (Kim,  2014 ) <C> 87.2 <C> 81.3±0.1 <C> 84.3±0.2 <R> <C> 2 <C> CNN-Rule- [ITALIC] p <C> 88.8 <C> 81.6±0.1 <C> 85.0±0.3 <R> <C> 3 <C> CNN-Rule- [ITALIC] q <C> 89.3 <C> [BOLD] 81.7±0.1 <C> [BOLD] 85.3±0.3 <R> <C> 4 <C> MGNC-CNN (Zhang et al.,  2016 ) <C> 88.4 <C> – <C> – <R> <C> 5 <C> MVCNN (Yin and Schutze,  2015 ) <C> [BOLD] 89.4 <C> – <C> – <R> <C> 6 <C> CNN-multichannel (Kim,  2014 ) <C> 88.1 <C> 81.1 <C> 85.0 <R> <C> 7 <C> Paragraph-Vec (Le and Mikolov,  2014 ) <C> 87.8 <C> – <C> – <R> <C> 8 <C> CRF-PR (Yang and Cardie,  2014 ) <C> – <C> – <C> 82.7 <R> <C> 9 <C> RNTN (Socher et al.,  2013 ) <C> 85.4 <C> – <C> – <R> <C> 10 <C> G-Dropout (Wang and Manning,  2013 ) <C> – <C> 79.0 <C> 82.1 <CAP> Table 1: Accuracy (%) of Sentiment Classification. Row 1, CNN (Kim, 2014) is the base network corresponding to the “CNN-non-static” model in (Kim, 2014). Rows 2-3 are the networks enhanced by our framework: CNN-Rule-p is the student network and CNN-Rule-q is the teacher network. For MR and CR, we report the average accuracy±one standard deviation using 10-fold cross validation.
<R> <C> [EMPTY] <C> Model <C> Accuracy (%) <R> <C> 1 <C> CNN (Kim,  2014 ) <C> 87.2 <R> <C> 2 <C> -but-clause <C> 87.3 <R> <C> 3 <C> -ℓ2-reg <C> 87.5 <R> <C> 4 <C> -project <C> 87.9 <R> <C> 5 <C> -opt-project <C> 88.3 <R> <C> 6 <C> -pipeline <C> 87.9 <R> <C> 7 <C> -Rule- [ITALIC] p <C> 88.8 <R> <C> 8 <C> -Rule- [ITALIC] q <C> [BOLD] 89.3 <CAP> Table 2: Performance of different rule integration methods on SST2. 1) CNN is the base network; 2) “-but-clause” takes the clause after “but” as input; 3) “-ℓ2-reg” imposes a regularization term γ∥σθ(S)−σθ(Y)∥2 to the CNN objective, with the strength γ selected on dev set; 4) “-project” projects the trained base CNN to the rule-regularized subspace with Eq.(3); 5) “-opt-project” directly optimizes the projected CNN; 6) “-pipeline” distills the pre-trained “-opt-project” to a plain CNN; 7-8) “-Rule-p” and “-Rule-q” are our models with p being the distilled student network and q the teacher network. Note that “-but-clause” and “-ℓ2-reg” are ad-hoc methods applicable specifically to the “but”-rule.
<R> <C> n <C> start <C> end <C> n-gram <C> resource <R> <C> 1 <C> 2 <C> 3 <C> philosophers <C> dbrc:Philosophes <R> <C> 2 <C> 2 <C> 3 <C> philosophers <C> dbr:Philosophes <R> <C> 3 <C> 2 <C> 3 <C> philosophers <C> dbo:Philosopher <R> <C> 4 <C> 2 <C> 3 <C> philosophers <C> dbrc:Philosophers <R> <C> 5 <C> 2 <C> 3 <C> philosophers <C> dbr:Philosopher <R> <C> 6 <C> 2 <C> 3 <C> philosophers <C> dbr:Philosophy <R> <C> 7 <C> 2 <C> 3 <C> philosophers <C> dbo:philosophicalSchool <R> <C> 8 <C> 3 <C> 4 <C> born <C> dbr:Born,_Netherlands <R> <C> 9 <C> 3 <C> 4 <C> born <C> dbr:Born_(crater) <R> <C> 10 <C> 3 <C> 4 <C> born <C> dbr:Born_auf_dem_Dar? <R> <C> 11 <C> 3 <C> 4 <C> born <C> dbr:Born,_Saxony-Anhalt <R> <C> ⋮ <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> 42 <C> 3 <C> 4 <C> born <C> dbp:bornAs <R> <C> 43 <C> 3 <C> 4 <C> born <C> dbo:birthDate <R> <C> 44 <C> 3 <C> 4 <C> born <C> dbo:birthName <R> <C> 45 <C> 3 <C> 4 <C> born <C> dbp:bornDay <R> <C> 46 <C> 3 <C> 4 <C> born <C> dbp:bornYear <R> <C> 47 <C> 3 <C> 4 <C> born <C> dbp:bornDate <R> <C> 48 <C> 3 <C> 5 <C> born in <C> dbp:bornIn <R> <C> 49 <C> 3 <C> 5 <C> born in <C> dbo:birthPlace <R> <C> 50 <C> 3 <C> 5 <C> born in <C> dbo:hometown <CAP> Table 2: Expansion step for the question “Give me philosophers born in Saint Étienne”. The first column enumerates the candidates that were found. Here, 117 possible entities, properties and classes were found from the question. The second, third and fourth columns indicate the position of the n-gram in the question and the n-gram itself. The last column is for the associated IRI. Note that many possible meanings are considered: line 9 says that “born” may refer to a crater, line 52 that “saint” may refer to a software and line 114 that the string “Saint Étienne” may refer to a band.
<R> <C> n <C> start <C> end <C> n-gram <C> resource <R> <C> 52 <C> 5 <C> 6 <C> saint <C> dbr:SAINT_(software) <R> <C> 53 <C> 5 <C> 6 <C> saint <C> dbr:Saint <R> <C> 54 <C> 5 <C> 6 <C> saint <C> dbr:Boxers_and_Saints <R> <C> 55 <C> 5 <C> 6 <C> saint <C> dbr:Utah_Saints <R> <C> 56 <C> 5 <C> 6 <C> saint <C> dbr:Saints,_Luton <R> <C> 57 <C> 5 <C> 6 <C> saint <C> dbr:Baba_Brooks <R> <C> 58 <C> 5 <C> 6 <C> saint <C> dbr:Battle_of_the_Saintes <R> <C> 59 <C> 5 <C> 6 <C> saint <C> dbr:New_York_Saints <R> <C> ⋮ <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <C> [EMPTY] <R> <C> 106 <C> 5 <C> 6 <C> saint <C> dbp:saintPatron <R> <C> 107 <C> 5 <C> 6 <C> saint <C> dbp:saintsDraft <R> <C> 108 <C> 5 <C> 6 <C> saint <C> dbp:saintsSince <R> <C> 109 <C> 5 <C> 6 <C> saint <C> dbo:patronSaint <R> <C> 110 <C> 5 <C> 6 <C> saint <C> dbp:saintsCollege <R> <C> 111 <C> 5 <C> 6 <C> saint <C> dbp:patronSaintOf <R> <C> 112 <C> 5 <C> 6 <C> saint <C> dbp:patronSaint(s) <R> <C> 113 <C> 5 <C> 6 <C> saint <C> dbp:patronSaint’sDay <R> <C> 114 <C> 5 <C> 7 <C> saint etienne <C> dbr:Saint_Etienne_(band) <R> <C> 115 <C> 5 <C> 7 <C> saint etienne <C> dbr:Saint_Etienne <R> <C> 116 <C> 5 <C> 7 <C> saint etienne <C> dbr:Saint-Étienne <R> <C> 117 <C> 6 <C> 7 <C> etienne <C> dbr:Étienne <CAP> Table 2: Expansion step for the question “Give me philosophers born in Saint Étienne”. The first column enumerates the candidates that were found. Here, 117 possible entities, properties and classes were found from the question. The second, third and fourth columns indicate the position of the n-gram in the question and the n-gram itself. The last column is for the associated IRI. Note that many possible meanings are considered: line 9 says that “born” may refer to a crater, line 52 that “saint” may refer to a software and line 114 that the string “Saint Étienne” may refer to a band.
<R> <C> Dataset <C> Lang <C> Type <C> Total <C> P <C> R <C> F <C> Runtime <R> <C> DBpedia <C> en <C> full <C> 100 <C> 0.55 <C> 0.34 <C> 0.42 <C> 1.37 s <R> <C> All KBs supported <C> en <C> full <C> 100 <C> 0.49 <C> 0.39 <C> 0.43 <C> 11.94s <CAP> Table 7: Comparison on QALD-6 when querying only DBpedia and multiple KBs at the same time.
<R> <C> Feature Set <C> MSE A <C> MSE B <C> Spear.  [ITALIC] ρ A <C> Spear.  [ITALIC] ρ B <R> <C> all <C> [BOLD] 1.96 <C> [BOLD] 2.74 <C> [BOLD] .40* <C> [BOLD] .27* <R> <C> all – {sim} <C> 2.10 <C> 2.75 <C> .34* <C> .25* <R> <C> all – {bow} <C> 2.02 <C> 2.77 <C> .37* <C> .25* <R> <C> all – {sim,bow} <C> 2.31 <C> 2.79 <C> .16* <C> .20* <R> <C> all – {s2s} <C> 2.00 <C> 2.85 <C> .38* <C> .22* <R> <C> all – {s2s-bin} <C> 1.97 <C> 2.76 <C> .40* <C> .26* <R> <C> all – {s2s,s2s-bin} <C> 2.06 <C> 2.87 <C> .35* <C> .21* <R> <C> all – {len} <C> 2.01 <C> 2.77 <C> .39* <C> .25* <R> <C> ∅ + {sim} <C> 2.06 <C> 3.04 <C> .35* <C> .10 <R> <C> ∅ + {bow} <C> 2.10 <C> 2.89 <C> .34* <C> .12* <R> <C> ∅ + {s2s} <C> 2.33 <C> 2.80 <C> .14 <C> .20* <R> <C> ∅ + {s2s-bin} <C> 2.39 <C> 2.89 <C> .00 <C> .00 <R> <C> ∅ + {len} <C> 2.39 <C> 2.89 <C> .00 <C> .05 <CAP> Table 6: Ablation results for ordinal regression model on A-test and B-test. (*p-value
<R> <C> (Method) <C> [BOLD] OD2 OPP <C> [BOLD] OD2 acc <C> [BOLD] OD3 OPP <C> [BOLD] OD3 acc <R> <C> [BOLD] Random <C> 0.6123 <C> 0.2765 <C> 0.5345 <C> 0.1950 <R> <C> [BOLD] CBOW <C> 0.6542 <C> 0.3731 <C> 0.6162 <C> 0.3034 <R> <C> [BOLD] NNSE <C> 0.6998 <C> 0.4288 <C> 0.6292 <C> 0.3190 <R> <C> [BOLD] CP-S <C> [BOLD] 0.7078 <C> [BOLD] 0.4370 <C> [BOLD] 0.6741 <C> [BOLD] 0.3597 <R> <C> [BOLD] JCP-S <C> 0.7017 <C> 0.4242 <C> 0.6666 <C> 0.3201 <CAP> Table 1: Outlier Detection scores across all embeddings
<R> <C> (Method) <C> [BOLD] MEN <C> [BOLD] MTurk <R> <C> [BOLD] Random <C> -0.028 <C> -0.150 <R> <C> [BOLD] CBOW <C> 0.601 <C> 0.498 <R> <C> [BOLD] NNSE <C> 0.717 <C> 0.686 <R> <C> [BOLD] CP-S <C> 0.630 <C> 0.631 <R> <C> [BOLD] JCP-S <C> 0.621 <C> 0.669 <CAP> Table 2: Word Similarity Scores (Spearman’s ρ)
<R> <C> Size of KB <C> Neural Assistant BLEU <C> Neural Assistant Action F-1 <C> Neural Assistant Entity F-1 <C> Neural Assistant with DS BLEU <C> Neural Assistant with DS Action F-1 <C> Neural Assistant with DS Entity F-1 <R> <C> 100 <C> 16.7 <C> 87.6 <C> 74.0 <C> 17.0 <C> 87.2 <C> 74.1 <R> <C> 2000 <C> 15.5 <C> 87.9 <C> 65.0 <C> 15.4 <C> 86.3 <C> 65.5 <R> <C> 5000 <C> 14.3 <C> 86.2 <C> 59.2 <C> 14.4 <C> 85.7 <C> 59.0 <R> <C> 8000 <C> 13.9 <C> 85.7 <C> 55.5 <C> 14.0 <C> 88.6 <C> 54.9 <R> <C> 12000 <C> 14.3 <C> 85.5 <C> 50.3 <C> 13.7 <C> 87.7 <C> 50.2 <R> <C> 13000 <C> 13.7 <C> 88.1 <C> 48.0 <C> 13.8 <C> 86.7 <C> 49.2 <R> <C> 28483 (at test) <C> 13.5 <C> 90.2 <C> 42.9 <C> - <C> - <C> - <CAP> Table 2: We compare the performance of Neural Assistant with and without distant supervision (Section 2.2). The performance of the Neural Assistant model drops as the number of negative examples (Size of KB) increases. Neural Assistant is able to incorporate external knowledge information especially when the size of KB is not too large.
<R> <C> Features <C> Loss <C> Precision <C> Recall <C> F1 <R> <C> None <C> 0.9% <C> 67.6 <C> 79.1 <C> 71.3 <R> <C> Gender <C> 0.7% <C> 80.09 <C> 86.8 <C> [BOLD] 82.5 <R> <C> Number <C> 0.7% <C> 78.2 <C> 85.7 <C> 80.8 <R> <C> Person <C> 0.7% <C> 80.0 <C> 86.2 <C> 82.3 <R> <C> PoP <C> 0.7% <C> 76.9 <C> 85.2 <C> 79.7 <CAP> Table 2: Comparison based on features
<R> <C> Models <C> WMT16 En→Ro <C> WMT16 Ro→En <C> WMT14 En→De <C> WMT14 De→En <R> <C> NAT-FT gu2017non <C> 27.29 <C> 29.06 <C> 17.69 <C> 21.47 <R> <C> NAT-FT (+NPD s=10) <C> 29.02 <C> 30.76 <C> 18.66 <C> 22.41 <R> <C> NAT-FT (+NPD s=100) <C> 29.79 <C> 31.44 <C> 19.17 <C> 23.20 <R> <C> NAT-IR ( [ITALIC] idec=1) lee2018deterministic <C> 24.45 <C> 25.73 <C> 13.91 <C> 16.77 <R> <C> CTC libovicky2018end <C> 19.93 <C> 24.71 <C> 17.68 <C> 19.80 <R> <C> imitate-NAT wei2019imitation <C> 28.61 <C> 28.90 <C> 22.44 <C> 25.67 <R> <C> imitate-NAT (+LPD) <C> 31.45 <C> 31.81 <C> 24.15 <C> 27.28 <R> <C> CMLM ghazvininejad2019mask <C> 27.32 <C> 28.20 <C> 18.05 <C> 21.83 <R> <C> FlowSeq ma2019flowseq <C> 29.73 <C> 30.72 <C> 23.72 <C> 28.39 <R> <C> FlowSeq (NPD n=30) <C> [BOLD] 32.20 <C> [BOLD] 32.84 <C> [BOLD] 25.31 <C> [BOLD] 30.68 <R> <C> Our AR Transformer (beam 1) <C> 33.56 <C> 33.68 <C> 28.84 <C> 32.77 <R> <C> Our AR Transformer (beam 4) <C> 34.50 <C> 34.01 <C> 29.65 <C> 33.65 <R> <C> Our NAR baseline ( [ITALIC] B=5) <C> 31.21 <C> 32.06 <C> 23.57 <C> 29.01 <R> <C> + monolingual data <C> 31.91 <C> 33.46 <C> 25.53 <C> 29.96 <R> <C> + monolingual data and de-dup <C> [BOLD] 31.96 <C> [BOLD] 33.57 <C> [BOLD] 25.73 <C> [BOLD] 30.18 <CAP> Table 2: BLEU scores on the WMT16 En-Ro and WMT14 En-De test sets for different NAR models. All reported scores are from non-iterative NAR methods with similar hyper-parameter settings for transformers. ‘de-dup’ removes adjacent duplicated tokens. B is the half-width in Sec. 2.3.
<R> <C> src <C> # <C> AR <C> NAR <C> +half <C> +all <R> <C> length <C> sent. <C> beam 1 <C> baseline <C> mono <C> mono <R> <C> [1,20] <C> 865 <C> 32.12 <C> 29.96 <C> 30.94 <C> 31.10 <R> <C> [21,40] <C> 867 <C> 33.82 <C> 30.77 <C> 31.92 <C> 31.96 <R> <C> [41,60] <C> 228 <C> 35.13 <C> 29.59 <C> 31.33 <C> 31.81 <R> <C> [61,80] <C> 29 <C> 35.09 <C> 26.69 <C> 27.99 <C> 30.47 <R> <C> [81,120] <C> 8 <C> 34.13 <C> 16.47 <C> 28.92 <C> 29.47 <R> <C> [121,140] <C> 2 <C> 6.70 <C> 3.11 <C> 3.56 <C> 5.99 <CAP> Table 4: BLEU scores for source sentences in different length intervals on the WMT16 Ro→En test set. The gold target length is provided during decoding.
<R> <C> [BOLD] Method <C> [BOLD] Reddit <C> [BOLD] OpenSubtitles <R> <C> ESIM + ELMo <C> 0.526 <C> 0.455 <R> <C> BERT <C> [BOLD] 0.553 <C> [BOLD] 0.498 <CAP> Table 2: Accuracy of inference models on InferConvAI.
<R> <C> [BOLD] Method <C> [BOLD] Pearson Reddit <C> [BOLD] Pearson OpenSubtitles <R> <C> SS( [ITALIC] H−2)BERT <C> -0.204 <C> -0.290 <R> <C> SS( [ITALIC] H−2)ELMo <C> -0.146 <C> -0.365 <R> <C> SS( [ITALIC] H−2)USE <C> -0.248 <C> -0.314 <R> <C> SS( [ITALIC] H−1)BERT <C> -0.214 <C> -0.337 <R> <C> SS( [ITALIC] H−1)ELMo <C> -0.178 <C> [BOLD] -0.404 <R> <C> SS( [ITALIC] H−1)USE <C> [BOLD] -0.287 <C> -0.320 <R> <C> ABERT <C> 0.135 <C> 0.131 <R> <C> AELMo <C> 0.085 <C> 0.162 <R> <C> Aword2vec <C> 0.037 <C> 0.196 <R> <C> GBERT <C> 0.208 <C> 0.132 <R> <C> GELMo <C> 0.037 <C> 0.072 <R> <C> Gword2vec <C> -0.033 <C> 0.015 <R> <C> EBERT <C> 0.162 <C> 0.144 <R> <C> EELMo <C> 0.035 <C> 0.116 <R> <C> Eword2vec <C> -0.065 <C> 0.118 <CAP> Table 3: The Pearson Correlation between different metrics and human judgments with p-value <0.001. The semantic similarity (SS) metric is measured with respect to the most recent utterance H−1 and the most recent two utterances H−2 in the conversation history. We adopt different embedding algorithms to compute the word vectors: ELMo Peters et al. (2018), BERT Devlin et al. (2018), word2vec Mikolov et al. (2013) and Universal Sentence Encoder (USE) Cer et al. (2018).
<R> <C> [EMPTY] <C> T/P <C> B/P <R> <C> FR <C> 0.53 <C> 1.06 <R> <C> DE <C> 0.37 <C> 1.20 <R> <C> ES <C> 0.53 <C> 1.15 <R> <C> RU <C> 0.57 <C> 1.65 <R> <C> TR <C> 0.65 <C> 1.01 <R> <C> CNN/DM (EN) <C> 1.10 <C> 1.06 <R> <C> CNN/DM (EN full preprocessing) <C> 0.85 <C> - <R> <C> DUC (EN) <C> 1.21 <C> - <R> <C> NEWSROOM (EN) <C> 1.10 <C> - <CAP> Table 3: Ratios of Rouge-L: T/P is the ratio of TextRank to Pointer-Generator and B/P is the ratio of M-BERT to Pointer-Generator. The results for CNN/DM-full preprocessing, DUC and NEWSROOM datasets are those reported in Table 2 of Grusky et al. (2018) (Pointer-C in their paper is our Pointer-Generator).
<R> <C> [BOLD] Task <C> [BOLD] Pretraining <C> [BOLD] fine tuning <C> [BOLD] Prec <C> [BOLD] Rec <C> [BOLD] F1 <R> <C> i2b2_2010 <C> BERTbase <C> Centralized <C> 0.775 <C> 0.794 <C> 0.784 <R> <C> i2b2_2010 <C> ClinicalBERT <C> Centralized <C> 0.844 <C> 0.873 <C> 0.858 <R> <C> i2b2_2010 <C> Fed_ClinicalBERT <C> Centralized <C> 0.81 <C> 0.831 <C> 0.820 <R> <C> i2b2_2010 <C> BERTbase <C> Federated <C> 0.73 <C> 0.703 <C> 0.716 <R> <C> i2b2_2010 <C> ClinicalBERT <C> Federated <C> 0.819 <C> 0.868 <C> 0.843 <R> <C> i2b2_2010 <C> Fed_ClinicalBERT <C> Federated <C> 0.811 <C> 0.806 <C> 0.808 <R> <C> i2b2_2012 <C> BERTbase <C> Centralized <C> 0.704 <C> 0.754 <C> 0.728 <R> <C> i2b2_2012 <C> ClinicalBERT <C> Centralized <C> 0.711 <C> 0.774 <C> 0.741 <R> <C> i2b2_2012 <C> Fed_ClinicalBERT <C> Centralized <C> 0.708 <C> 0.764 <C> 0.735 <R> <C> i2b2_2012 <C> BERTbase <C> Federated <C> 0.667 <C> 0.707 <C> 0.686 <R> <C> i2b2_2012 <C> ClinicalBERT <C> Federated <C> 0.697 <C> 0.769 <C> 0.731 <R> <C> i2b2_2012 <C> Fed_ClinicalBERT <C> Federated <C> 0.687 <C> 0.745 <C> 0.715 <CAP> Table 1: Performance on i2b2 NER tasks
<R> <C> [EMPTY] <C> manually-aligned <C> +Mod1 <C> +Mod2 <C> +Mod3 <C> +Mod4 <R> <C> Inter-dataset <C> 0.439 <C> 0.436 <C> 0.512 <C> 0.527 <C> 0.555 <R> <C> Intra-dataset <C> 0.898 <C> 0.903 <C> 0.896 <C> 0.890 <C> 0.912 <CAP> Table 4: Effect of squashing/splitting different acts on inter and intra-dataset average F1 score. Each column displays the effect of addition of modification to the one on its left.
<R> <C> Domain Domain data <C> U-DAT <C> HH-UDAT no <C> HH-UDAT yes <C> HH-UDAT yes <R> <C> Supervision <C> no <C> yes <C> semi <C> yes <R> <C> Restaurant <C> 0.613 <C> 0.703 <C> 0.721 <C> 0.735 <R> <C> Hotel <C> 0.571 <C> 0.685 <C> 0.697 <C> 0.701 <R> <C> Train <C> 0.523 <C> 0.723 <C> 0.666 <C> 0.724 <R> <C> Taxi <C> 0.709 <C> 0.727 <C> 0.787 <C> 0.784 <R> <C> Attraction <C> 0.444 <C> 0.672 <C> 0.689 <C> 0.728 <R> <C> Average <C> 0.572 <C> 0.702 <C> 0.712 <C> 0.734 <CAP> Table 6: Per-domain F1 score results for domain adaptation via self-training (semi-supervised training)
<R> <C> Model <C> Homographic Puns P <C> Homographic Puns R <C> Homographic Puns  [ITALIC] F1 <C> Heterographic Puns P <C> Heterographic Puns R <C> Heterographic Puns  [ITALIC] F1 <R> <C> Joint <C> 67.70 <C> 67.70 <C> 67.70 <C> 68.84 <C> 68.84 <C> 68.84 <R> <C> PCPR <C> [BOLD] 87.21 <C> [BOLD] 81.72 <C> [BOLD] 84.38 <C> [BOLD] 85.16 <C> [BOLD] 80.15 <C> [BOLD] 82.58 <CAP> Table 5: Performance of pipeline recognition in the SemEval dastaset.
<R> <C> Model <C> P <C> R <C> [ITALIC] F1 <R> <C> PCPR <C> [BOLD] 90.43 <C> [BOLD] 87.50 <C> [BOLD] 88.94 <R> <C> w/o Pre-trained Phoneme Emb. <C> 89.37 <C> 85.65 <C> 87.47 <R> <C> w/o Self-attention Encoder <C> 89.17 <C> 86.42 <C> 87.70 <R> <C> w/o Phonological Attention <C> 89.56 <C> 87.35 <C> 88.44 <CAP> Table 6: Ablation study on different features of PCPR for homographic pun detection on the SemEval dataset.
<R> <C> [BOLD] user <C> [BOLD] labels <C> [BOLD] y <C> [BOLD] entropy <R> <C> a <C> {2: 5, 4: 4, 1: 3, 3: 4} <C> 2 <C> 1.37 <R> <C> b <C> {2: 16} <C> 2 <C> 0.00 <CAP> Table 1: Two equal-sized samples, both in group 2. The column labels contains the number of reviews per class.
<R> <C> [BOLD] Method <C> [BOLD] Micro Average Precision <C> [BOLD] Micro Average Recall <C> [BOLD] Micro Average  [ITALIC] F1 <C> [BOLD] Micro Average Title Acc. <C> [BOLD] Macro Average Precision <C> [BOLD] Macro Average Recall <C> [BOLD] Macro Average  [ITALIC] F1 <C> [BOLD] Macro Average Title Acc. <R> <C> Identity Baseline <C> 0.33 <C> [BOLD] 1.00* <C> 0.20 <C> 0.02 <C> 0.53 <C> [BOLD] 1.00* <C> 0.70 <C> 0.25 <R> <C> CValue <C> 0.78 <C> 0.90 <C> 0.83 <C> 0.59 <C> 0.77 <C> 0.56 <C> 0.65 <C> 0.30 <R> <C> CNN <C> 0.89 <C> 0.82 <C> 0.85 <C> 0.67 <C> 0.89 <C> 0.79 <C> 0.84 <C> 0.61 <R> <C> BERT <C> 0.93 <C> 0.94 <C> 0.93 <C> 0.81 <C> 0.94 <C> 0.89 <C> 0.92 <C> 0.71 <R> <C> TOR1 [ITALIC] M <C> 0.88 <C> 0.91 <C> 0.90 <C> 0.72 <C> 0.81 <C> 0.50 <C> 0.62 <C> 0.18 <R> <C> TOR100 [ITALIC] M <C> 0.85 <C> 0.93 <C> 0.89 <C> 0.68 <C> 0.86 <C> 0.79 <C> 0.82 <C> 0.59 <R> <C> TOR1 [ITALIC] M + CNN <C> 0.85 <C> 0.93 <C> 0.89 <C> 0.73 <C> 0.88 <C> 0.84 <C> 0.86 <C> 0.64 <R> <C> TOR1 [ITALIC] M + BERT <C> [BOLD] 0.94 <C> 0.95 <C> [BOLD] 0.94 <C> [BOLD] 0.84 <C> [BOLD] 0.95 <C> 0.90 <C> [BOLD] 0.93 <C> [BOLD] 0.74 <CAP> Table 1: Evaluation results on the constructed task – the best result in each column is marked in bold. (*) Recall of the identity baseline is 1 by construction.
<R> <C> Systems <C> Ch-En <C> Fr-En <C> Runtime <R> <C> Transformer <C> 36.22 <C> 67.13 <C> 0.256 <R> <C> + GBS <C> 42.17 <C> 70.99 <C> 5.692 <R> <C> + DBA <C> 41.23 <C> 67.35 <C> 1.531 <R> <C> + SE-Attn <C> 42.96 <C> 71.10 <C> 0.262 <CAP> Table 2: BLEU and runtime comparison on perfect constraints for the Chinese-to-English and French-to-English tasks. The runtime is measured by the time consumed by decoding one sentence on Chinese-to-English task.
<R> <C> Systems <C> Number of the noises in constraints 1 <C> Number of the noises in constraints 2 <C> Number of the noises in constraints 3 <C> Number of the noises in constraints 4 <C> Number of the noises in constraints 5 <R> <C> Transformer <C> 36.22 <C> 36.22 <C> 36.22 <C> 36.22 <C> 36.22 <R> <C> + GBS <C> 38.69 <C> 35.82 <C> 33.02 <C> 30.72 <C> 29.14 <R> <C> + DE-Gate <C> 38.23 <C> 38.28 <C> [BOLD] 38.26 <C> [BOLD] 38.05 <C> [BOLD] 37.90 <R> <C> + SE-Gate <C> 39.89 <C> 38.85 <C> 38.15 <C> 37.00 <C> 36.56 <R> <C> + SE-Copy <C> 39.23 <C> 38.25 <C> 37.74 <C> 36.74 <C> 35.89 <R> <C> + SE-Attn <C> [BOLD] 40.49 <C> [BOLD] 39.20 <C> 37.74 <C> 36.07 <C> 34.88 <CAP> Table 3: BLEU comparison on noisy constraints for the Chinese-to-English task, where users totally provide 5 constraints with different number of noises.
<R> <C> Systems <C> Number of the noises in constraints 1 <C> Number of the noises in constraints 2 <C> Number of the noises in constraints 3 <C> Number of the noises in constraints 4 <C> Number of the noises in constraints 5 <R> <C> Transformer <C> 67.13 <C> 67.13 <C> 67.13 <C> 67.13 <C> 67.13 <R> <C> + GBS <C> 67.01 <C> 64.05 <C> 61.57 <C> 59.65 <C> 57.82 <R> <C> + DE-Gate <C> 67.85 <C> 67.91 <C> 67.92 <C> 67.95 <C> [BOLD] 67.98 <R> <C> + SE-Attn <C> [BOLD] 69.38 <C> [BOLD] 68.94 <C> [BOLD] 68.53 <C> [BOLD] 68.10 <C> 67.59 <CAP> Table 4: BLEU comparison on noisy constraints for the French-to-English task, where users totally provide 5 constraints including different number of noises.
<R> <C> Systems <C> Ch-to-En <C> Fr-to-En <R> <C> Transformer <C> 36.22 <C> 67.13 <R> <C> + GBS <C> 35.14 <C> 67.43 <R> <C> + DE-Gate <C> 37.75 <C> 67.92 <R> <C> + SE-Attn <C> 37.66 <C> 68.46 <CAP> Table 5: BLEU comparison on automatic constraints for the Chinese-to-English and French-to-English task.
<R> <C> Corpus <C> Model <C> BMTL1 BPE300 <C> BMTL1 BPE1K <C> BMTL1 BPE10K <C> BMTL2 BPE10K <C> BMTL2 BPE16K <C> BMTL2 BPE32K <R> <C> En-Fr <C> Baseline <C> 35.6 <C> 35.1 <C> [BOLD] 36 <C> 35 <C> 36 <C> 34.8 <R> <C> En-Fr <C> BMTL <C> [BOLD] 36 <C> [BOLD] 35.6 <C> 35.7 <C> [BOLD] 36.5 <C> [BOLD] 36.1 <C> [BOLD] 36.5 <R> <C> En-Vi <C> Baseline <C> 26.4 <C> 27.1 <C> 26.3 <C> 27.6 <C> 27 <C> 27.3 <R> <C> En-Vi <C> BMTL <C> [BOLD] 27 <C> [BOLD] 27.7 <C> [BOLD] 27.6 <C> [BOLD] 27.8 <C> [BOLD] 27.5 <C> [BOLD] 27.6 <R> <C> En-Cs <C> Baseline <C> 17 <C> 16.5 <C> 16.7 <C> [BOLD] 16.7 <C> 16.4 <C> [BOLD] 16.4 <R> <C> En-Cs <C> BMTL <C> [BOLD] 17.6 <C> [BOLD] 17.7 <C> [BOLD] 17.4 <C> 16.6 <C> [BOLD] 16.8 <C> 16.3 <CAP> Table 1: BLEU scores of our BMTL1 (i.e., BMTL combining BPE 300, BPE1K and BPE10K) and BMTL2 (i.e., BMTL combining BPE 10K, BPE16K and BPE32K) models, as well as the baseline models, on each of our three IWSLT language pairs (i.e., English to {French, Czech, Vietnamese}).
<R> <C> [BOLD] Model <C> [BOLD] Execution Accuracy (%) <R> <C> Concat <C> 25.24 <R> <C> E2ECR <C> 27.18 <R> <C> Copy+Anon <C> 40.77 <R> <C> FAnDa <C> [BOLD] 60.19 <CAP> Table 5: The results of execution accuracies.
<R> <C> features <C> EER(%) <R> <C> decoder features ( [BOLD] d) <C> 9.3 <R> <C> acoustic embedding ( [BOLD] a) <C> 10.9 <R> <C> char embedding ( [BOLD] c) <C> 20.1 <R> <C> [ [BOLD] a,d] <C> 6.5 <R> <C> [ [BOLD] c,d] <C> 6.9 <R> <C> [ [BOLD] a,c] <C> 8.6 <R> <C> [ [BOLD] a,c,d] <C> 5.2 <CAP> Table 4: Device-directed performance using various features.
<R> <C> [BOLD] Language <C> [BOLD] Model <C> [BOLD] tgt-size=100 Accuracy <C> [BOLD] tgt-size=100 F1-Macro <C> [BOLD] tgt-size=100 F1-Micro <C> [BOLD] tgt-size=1,000 Accuracy <C> [BOLD] tgt-size=1,000 F1-Macro <C> [BOLD] tgt-size=1,000 F1-Micro <R> <C> RU/BG <C> mdcrf + pos + multi-source <C> [BOLD] 69.13 <C> [BOLD] 85.78 <C> [BOLD] 85.86 <C> [BOLD] 82.72 <C> [BOLD] 92.15 <C> [BOLD] 92.17 <R> <C> [EMPTY] <C> Malaviya et al. ( 2018 ) <C> 46.89 <C> 64.75 <C> 64.46 <C> 67.56 <C> 82.06 <C> 82.11 <R> <C> [EMPTY] <C> Cotterell and Heigold ( 2017 ) <C> 52.76 <C> 58.23 <C> 58.41 <C> 71.90 <C> 77.89 <C> 77.97 <R> <C> FI/HU <C> mdcrf + pos + multi-source <C> [BOLD] 57.32 <C> [BOLD] 80.11 <C> [BOLD] 78.86 <C> [BOLD] 70.24 <C> [BOLD] 85.44 <C> [BOLD] 84.86 <R> <C> [EMPTY] <C> Malaviya et al. ( 2018 ) <C> 45.41 <C> 68.63 <C> 68.07 <C> 63.93 <C> 85.06 <C> 84.12 <R> <C> [EMPTY] <C> Cotterell and Heigold ( 2017 ) <C> 51.74 <C> 68.15 <C> 66.82 <C> 61.8 <C> 75.96 <C> 76.16 <CAP> Table 2: Comparing our model for bilingual transfer with previous baselines.
<R> <C> [BOLD] Term <C> [BOLD] Neg freq <C> [BOLD] Pos freq <R> <C> alford <C> 0 <C> 47 <R> <C> nix <C> 0 <C> 43 <R> <C> cecchini <C> 0 <C> 38 <R> <C> robles <C> 0 <C> 35 <R> <C> banda <C> 0 <C> 34 <R> <C> fried <C> 0 <C> 31 <R> <C> arroyo <C> 0 <C> 30 <R> <C> ciuffo <C> 0 <C> 30 <R> <C> tellez <C> 0 <C> 29 <R> <C> grisham <C> 0 <C> 29 <CAP> Table 2: The most discriminative terms present for the classification task.
<R> <C> Dataset <C> Hyper <C> Neutral <C> Hypo <R> <C> Original <C> 0.285 <C> 0.208 <C> 0.065 <CAP> Table 1: Vocalic space (in kHz2) for the three degrees of articulation for the original sentences.
<R> <C> [BOLD] Feature <C> [BOLD] Novice  [ITALIC] μ <C> [BOLD] Novice x̃ <C> [BOLD] Novice  [ITALIC] σ <C> [BOLD] Expert  [ITALIC] μ <C> [BOLD] Expert x̃ <C> [BOLD] Expert  [ITALIC] σ <R> <C> [BOLD] Interruptions <C> [BOLD] Interruptions <C> [BOLD] Interruptions <C> [BOLD] Interruptions <C> [BOLD] Interruptions <C> [BOLD] Interruptions <C> [BOLD] Interruptions <R> <C> # Barge-ins <C> 5.06 <C> 3.00 <C> 6.79 <C> 2.75 <C> 2.00 <C> 3.15 <R> <C> Barge-in Rate <C> 16.2 <C> 15.4 <C> 9.9 <C> 10.3 <C> 9.5 <C> 6.9 <R> <C> [BOLD] Delays (s) <C> [BOLD] Delays (s) <C> [BOLD] Delays (s) <C> [BOLD] Delays (s) <C> [BOLD] Delays (s) <C> [BOLD] Delays (s) <C> [BOLD] Delays (s) <R> <C> 1st Turn <C> 1.52 <C> 1.28 <C> 3.00 <C> 1.32 <C> 1.21 <C> 2.81 <R> <C> 1st Turn (Positive) <C> 2.82 <C> 2.18 <C> 2.79 <C> 1.90 <C> 1.49 <C> 2.72 <R> <C> [BOLD] Durations (s) <C> [BOLD] Durations (s) <C> [BOLD] Durations (s) <C> [BOLD] Durations (s) <C> [BOLD] Durations (s) <C> [BOLD] Durations (s) <C> [BOLD] Durations (s) <R> <C> Utterance <C> 1.81 <C> 1.44 <C> 3.14 <C> 1.19 <C> 1.20 <C> 0.43 <R> <C> Call <C> 123 <C> 104 <C> 95 <C> 102 <C> 76 <C> 78 <R> <C> 1st Turn <C> 1.81 <C> 1.19 <C> 2.02 <C> 1.72 <C> 1.39 <C> 1.66 <R> <C> # Exchanges <C> 28.0 <C> 23.0 <C> 23.4 <C> 23.8 <C> 20.0 <C> 13.8 <R> <C> [BOLD] Speech Rate (phones/s) <C> [BOLD] Speech Rate (phones/s) <C> [BOLD] Speech Rate (phones/s) <C> [BOLD] Speech Rate (phones/s) <C> [BOLD] Speech Rate (phones/s) <C> [BOLD] Speech Rate (phones/s) <C> [BOLD] Speech Rate (phones/s) <R> <C> Global <C> 13.7 <C> 14.2 <C> 3.3 <C> 14.8 <C> 14.9 <C> 1.9 <R> <C> 1st Turn <C> 14.3 <C> 14.5 <C> 4.1 <C> 14.8 <C> 14.5 <C> 2.8 <R> <C> [BOLD] Help Requests <C> [BOLD] Help Requests <C> [BOLD] Help Requests <C> [BOLD] Help Requests <C> [BOLD] Help Requests <C> [BOLD] Help Requests <C> [BOLD] Help Requests <R> <C> # Requests <C> 0.27 <C> 0.00 <C> 0.55 <C> 0.00 <C> 0.00 <C> 0.00 <CAP> Table 1: Feature distribution among the LEGO dataset in terms of average (μ), median(x̃), and standard deviation(σ).
<R> <C> [BOLD] Feature Set <C> [BOLD] SVM Accuracy <C> [BOLD] SVM  [ITALIC] κ <C> [BOLD] Random Forest Accuracy <C> [BOLD] Random Forest  [ITALIC] κ <R> <C> Help Requests <C> 0.607 <C> 0.268 <C> 0.589 <C> [BOLD] 0.232 <R> <C> [BOLD] First Turn <C> 0.571 <C> 0.207 <C> 0.538 <C> 0.082 <R> <C> [BOLD] Global <C> 0.589 <C> 0.153 <C> 0.538 <C> -0.018 <R> <C> [BOLD] All <C> 0.643 <C> 0.283 <C> 0.589 <C> 0.103 <R> <C> [BOLD] Selected <C> [BOLD] 0.661 <C> [BOLD] 0.327 <C> [BOLD] 0.643 <C> 0.217 <CAP> Table 3: Results on Let’s Go 2014 data (Chance = 0.554)
<R> <C> [BOLD] Category <C> [BOLD] % <C> [BOLD] Example <R> <C> [BOLD] TimeBank Improvements <C> [BOLD] TimeBank Improvements <C> [BOLD] TimeBank Improvements <R> <C> Finance <C> 54 <C> the  [BOLD] accord was unanimously approved <R> <C> Political <C> 12 <C> the ukrainian parliament has already  [BOLD] ratified it <R> <C> Reporting <C> 10 <C> from member station kqed , auncil martinez [BOLD]  reports <R> <C> Law <C> 10 <C> mr. antar was charged last month in a civil  [BOLD] suit <R> <C> [BOLD] LitBank Improvements <C> [BOLD] LitBank Improvements <C> [BOLD] LitBank Improvements <R> <C> Archaic <C> 6 <C> his countenance became intolerably  [BOLD] fervid <R> <C> Animal Actions <C> 6 <C> the dogs left off  [BOLD] barking , and ran about every way <R> <C> Human Actions <C> 18 <C> a  [BOLD] nod was the answer <R> <C> Literary <C> 14 <C> there  [BOLD] strikes the ebony clock <CAP> Table 4: Categorization of TimeBank and LitBank examples on which ADA shows improvement. Words in bold indicate events missed by BERT, but captured by BERT-A.
<R> <C> [BOLD] Dataset <C> [BOLD] P <C> [BOLD] R <C> [BOLD] F1 <R> <C> [BOLD] TimeBank <C> 68.9 <C> 65.5 <C> 67.2 <R> <C> [BOLD] LitBank <C> 40.3 <C> 71.5 <C> 51.5 <CAP> Table 5: Model performance on both domains in the self-training paradigm
