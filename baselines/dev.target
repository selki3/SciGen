Table 2 compares the throughput of performing inference and training on the TreeLSTM model using our implementation, the iterative approach, and the folding technique. The amount of resources is sufficient for executing forward computations, and therefore our framework outperforms the folding technique for the inference task with up to 4.93x faster throughput.  As a result, the folding technique performs better than the recursive approach for the training task.
Table 1 shows the throughput of training the TreeRNN model using these three datasets. For all batch sizes, the training throughput on the balanced dataset is the highest, while the throughput on the linear dataset is the lowest.  As a result, our implementation can train input data of balanced trees with greater throughput than input data of unbalanced trees.  Another interesting fact in Table 1 is that the training throughput on the linear dataset scales better than the throughput on the balanced dataset, as the batch size increases.  On the contrary, for the linear dataset, the recursive implementation fails to efficiently make use of CPU resources and thus the performance gain provided by increasing the batch size is relatively high.
Table 2 presents the optimal values for each configuration using different dependency representations. We see that the optimized parameter settings vary for the different representations, showing the importance of tuning for these types of comparisons. The results furthermore show that the sdps based on the Stanford Basic (SB) representation provide the best performance, followed by the CoNLL08 representation. We observe that the results for the UD representation are quite a bit lower than the two others.
We find that the effect of syntactic structure varies between the different relation types. However, the sdp information has a clear positive impact on all the relation types (Table 1).
Accordingly, as Table 3 shows for the essay level (paragraph level omitted for space reasons), results are generally weaker:  as in Eq. (1)—the overall result is worst. We find that when we train STagBL with only its main task—with label set  In Y contrast, when we include the 'natural subtasks' "C" (label  performance increases typically by a few percentage points.
We train and test all parsers on the paragraph level,  Mate is slightly better  Kiperwasser performs decently on the approximate match level, but not on exact level.  The best parser by far is the LSTM-Parser. It is over 100% better than Kiperwasser on exact spans and still several percentage points on approximate spans.  : Performance of dependency parsers, STagBLCC, LSTM-ER and ILP (from top to bottom).  On the other hand, our results in Table 2 indicate that the neural taggers BLCC and BLC (in the LSTMER model) are much better at such exact identification than either the ILP model or the neural parsers.
These results detail that the taggers have lower standard deviations than the parsers. The difference is particularly striking on the essay level where the parsers often completely fail to learn, that is, their performance scores are close to 0%.
The results for testing on cleaned data (Table 3, top half) confirm the positive impact of cleaned training data and also show that the cleaned test data is more challenging (cf. Section 3), as reflected in the lower WOMs.  However, the improved results for training and testing on clean data (i.e. seeing equally challenging examples at training and test time), suggest the increase in performance can be attributed to data accuracy rather than diversity.  Looking at the detailed results for the number of added, missing, and wrong-valued slots (Add, Miss, Wrong), we observe more deletions than insertions, i.e. the models more often fail to realise part of the MR, rather than hallucinating additional information.  However, the results in bottom halves  of Tables 2 and 3 do not support our hypothesis: we observe the main effect on SER from cleaning the missed slots, reducing both insertions and deletions. Again, one possible explanation is that cleaning the missing slots provided more complex training examples.
This resulted in 20% reduction for TRAIN and ca. 8% reduction for DEV in terms of references (see Table 1). On the other hand, the number of distinct MRs rose sharply after reannotation; the MRs also have more variance in the number of attributes. This means that the cleaned dataset is more complex overall, with fewer references per MR and more diverse MRs.
The results in Table 2 (top half) for the original setup confirm that the ranking mechanism for TGen is effective for both WOMs and SER, whereas the SC-LSTM seems to have trouble scaling to the E2E dataset. We hypothesise that this is mainly due to the amount of delexicalisation required. However, the main improvement of SER comes from training on cleaned data with up to 97% error reduction with the ranker and 94% without.11 just cleaning the training data has a much more dramatic effect than just using a semantic control mechanism, such as the reranker (0.97% vs. 4.27% SER). WOMs are slightly lower for TGen trained on the cleaned data, except for NIST, which gives more importance to matching less frequent n-grams. This suggests better preservation of content at the expense of slightly lower fluency. In other words,  However, the results in bottom halves  of Tables 2 and 3 do not support our hypothesis: we observe the main effect on SER from cleaning the missed slots, reducing both insertions and deletions. Again, one possible explanation is that cleaning the missing slots provided more complex training examples.
The results in Table 4 confirm the findings of the automatic  metrics: systems trained on the fully cleaned set or the set with cleaned missing slots have nearperfect performance, with the fully-cleaned one showing a few more slight disfluencies than the other. The systems trained on the original data or with cleaned added slots clearly perform worse in terms of both semantic accuracy and fluency. All fluency problems we found were very slight and no added or wrong-valued slots were found, so missed slots are the main problem.
Moreover, we compare our DCGCN(single) and DCGCN(ensemble) model with the state-of-the-art semi-supervised models on the AMR15 test set (Table 3), including non-neural methods such as TSP (Song et al., 2016), PBMT (Pourdamghani et al., 2016), Tree2Str (Flanigan et al., 2016) and SNRG (Song et al., 2017). All these non-neural models train language models on the whole Gigaword corpus. Our ensemble model gives 28.2 BLEU points without external data, which is better than them. Following Konstas et al. (2017); Song et al. (2018), we also evaluate our model using external Gigaword sentences as training data. We first use the additional data to pretrain the model, then finetune it on the gold data. Using additional 0.1M data, the single DCGCN model achieves a BLEU score of 29.0, which is higher than Seq2SeqK (Konstas et al., 2017) and GraphLSTM (Song et al., 2018) trained with 0.2M additional data. When using the same amount of 0.2M data, the performance of DCGCN is 4.2 and 3.4 BLEU points higher than Seq2SeqK and GraphLSTM. DCGCN model is able to achieve a competitive BLEU points (33.2) by using 0.3M external data, while GraphLSTM achieves a score of 33.6 by using 2M data and Seq2SeqK achieves a score of 33.8 by using 20M data. These results show that our model is more effective in terms of using automatically generated AMR graphs. Using 0.3M additional data, our ensemble model achieves the new state-of-the-art result of 35.3 BLEU points.
Table 2 shows the results on AMR17. Our single model achieves 27.6 BLEU points, which is the new state-of-the-art result for single models. In particular, our single DCGCN model consistently outperforms Seq2Seq models by a significant margin when trained without external resources. For example, the single DCGCN model gains 5.9 more BLEU points than the single models of Seq2SeqB on AMR17. These results demonstrate the importance of explicitly capturing the graph structure in the encoder. In addition, our single DCGCN model obtains better results than previous ensemble models. For example, on AMR17, the single DCGCN model is 1 BLEU point higher than the ensemble model of Seq2SeqB. Our model requires substantially fewer parameters, e.g., the parameter size is only 3/5 and 1/9 of those in GGNN2Seq and Seq2SeqB, respectively. The ensemble approach based on combining five DCGCN models initialized with different random seeds achieves a BLEU score of 30.4 and a CHRF++ score of 59.6. Under the same setting, our model also consistently outperforms graph encoders based on recurrent neural networks or gating mechanisms. For GGNN2Seq, our single model is 3.3 and 0.1 BLEU points higher than their single and ensemble models, respectively. We also have similar observations in term of CHRF++ scores for sentence-level evaluations. DCGCN also outperforms GraphLSTM by 2.0 BLEU points in the fully supervised setting as shown in Table 3. Note that GraphLSTM uses char-level neural represen  tations and pretrained word embeddings, while our model solely relies on word-level representations with random initializations. This empirically shows that compared to recurrent graph encoders, DCGCNs can learn better representations for graphs. For GCNSEQ, our single models are 3.1 and 1.3 BLEU points higher than their models trained on AMR17 and AMR15 dataset, respectively. These results demonstrate that DCGCNs are able to capture contextual information without relying on additional RNNs.
Table 4 shows the results for the EnglishGerman (En-De) and English-Czech (En-Cs) translation tasks. BoW+GCN, CNN+GCN and BiRNN+GCN refer to employing the following encoders with a GCN layer on top respectively: 1) a bag-of-words encoder, 2) a one-layer CNN, 3) a bidirectional RNN. PB-SMT is the phrase-based statistical machine translation model using Moses (Koehn et al., 2007). Our single model DCGCN(single) achieves 19.0 and 12.1 BLEU points on the En-De and EnCs tasks, respectively, significantly outperforming all the single models. For example, compared to the best GCN-based model (BiRNN+GCN), our single DCGCN model surpasses it by 2.7 and 2.5 BLEU points on the En-De and En-Cs tasks, respectively. Our models DCGCN(single) and DCGCN(ensemble)consist of full GCN layers, removing the burden of employing a recurrent encoder to extract non-local contextual information in the bottom layers. Compared to non-GCN models, our single DCGCN model is 2.2 and 1.9 BLEU points higher than the current state-of-theart single model (GGNN2Seq) on the En-De and En-Cs translation tasks, respectively. In addition, our single model is comparable to the ensemble results of Seq2SeqB and GGNN2Seq, while the number of parameters of our models is only about 1/6 of theirs. Additionally, the ensemble DCGCN models achieve 20.5 and 13.1 BLEU points on the En-De and En-Cs tasks, respectively. Our ensemble results are significantly higher than those of the state-of-the-art syntax-based ensemble models reported by GGNN2Seq (En-De: 20.5 v.s. 19.6; En-Cs: 13.1 v.s. 11.7 in terms of BLEU).
Layers in the sub-block. Table 5 shows the effect of the number of layers of each sub-block on the AMR15 development set. DenseNets (Huang et al., 2017) use two kinds of convolution filters: 1 × 1 and 3 × 3. Similar to DenseNets, we choose the values of n and m for layers from [1, 2, 3, 6]. We choose this value range by considering the scale of non-local nodes, the abstract information at different level and the calculation efficiency. For brevity, we only show representative configurations. We first investigate DCGCN with one block. In general, the performance increases when we gradually enlarge n and m. For example, when n=1 and m=1, the BLEU score is 17.6; when n=6 and m=6, the BLEU score becomes 22.0. We observe that the three settings (n=6, m=3), (n=3, m=6) and (n=6, m=6) give similar results for both 1 DCGCN block and 2 DCGCN blocks. Since the first two settings contain less parameters than the third setting, it is reasonable to choose either (n=6, m=3) or (n=3, m=6). For later experiments, we use (n=6, m=3).
The first block in Table 6 shows the performance of our two baseline models: multi-layer GCNs with residual connections (GCN+RC) and multi-layer GCNs with both residual connections and layer aggregations (GCN+RC+LA). In general, increasing the number of GCN layers from 2 to 9 boosts the model performance. However, when the layer number exceeds 10, the performance of both baseline models start to drop. For example, GCN+RC+LA (10) achieves a BLEU score of 21.2, which is worse than GCN+RC+LA (9). In preliminary experiments, we cannot manage to train very deep GCN+RC and GCN+RC+LA models. In contrast, our DCGCN models can be trained using a large number of layers. For example, DCGCN4 contains 36 layers. When we increase the DCGCN blocks from 1 to 4, the model performance continues increasing on AMR15 development set. We therefore choose DCGCN4 for the AMR experiments. Using a similar method, DCGCN2 is selected for the NMT tasks. When the layer numbers are 9, DCGCN1 is better than GCN+RC in term of B/C scores (21.7/51.5 v.s. 21.1/50.5). GCN+RC+LA (9) is sightly better than DCGCN1. However, when we set the number to 18, GCN+RC+LA achieves a BLEU score of 19.4, which is significantly worse than the BLEU score obtained by DCGCN2 (23.3). We also try GCN+RC+LA (27), but it does not converge. In conclusion, these results above can show the robustness and effectiveness of our DCGCN models.
We compare DCGCN models with different layers under the same parameter budget. Table 7 shows the results. For example, when both DCGCN1 and DCGCN2 are limited to 10.9M parameters, DCGCN2 obtains 22.2 BLEU points, which is higher than DCGCN1 (20.9). Similarly, when DCGCN3 and DCGCN4 contain 18.6M and 18.4M parameters. DCGCN4 outperforms DCGCN3 by 1 BLEU point with a slightly smaller model. In general, we found when the parameter budget is the same, deeper DCGCN models can obtain better results than the shallower ones.
Table 8 shows the ablation study of the level of density of our model DCGCN4. We use DCGCNs with 4 dense blocks as the full model. Then we remove dense connections gradually from the last block to the first block. In general, the performance of the model drops substantially as we remove more dense connections until it cannot converge without dense connections. The full model gives 25.5 BLEU points on the AMR15 dev set. After removing the dense connections in the last block, the BLEU score becomes 24.8. Without using the dense connections in the last two blocks, the score drops to 23.8. Furthermore, excluding the dense connections in the last three blocks only gives 23.2 BLEU points. Although these four models have the same number of layers, dense connections allow the model to achieve much better performance. If all the dense connections are not considered, the model does not coverage at all. These results indicate dense connections do play a significant role in our model.
Table 9 shows the results. For the encoder, we find that the linear combination and the global node have more contributions in terms of B/C scores. The results drop by 2/2.2 and 1.3/1.2 points respectively after removing them. Without these two components, our model gives a BLEU score of 22.6, which is still better than the best GCN+RC model (21.1) and the best GCN+RC+LA model (22.1). Adding either the global node or the linear combination improves the baseline models with only dense connections. This suggests that enriching input graphs with the global node and including the linear combination can facilitate GCNs to learn better information aggregations, producing more expressive graph representations. Results also show the linear combination is more effective than the global node. Considering them together further enhances the model performance. After removing the graph attention module, our model gives 24.9 BLEU points. Similarly, excluding the direction aggregation module leads to a performance drop to 24.6 BLEU points. The coverage mechanism is also effective in our models. Without the coverage mechanism, the result drops by 1.7/2.4 points for B/C scores.
To verify the effectiveness of our initialization strategy empirically, we evaluate it with the same experimental setup as described in Section 4. The only difference is the initialization strategy, where we include Glorot initialization (Glorot & Bengio, 2010) and the standard initialization from  (0, 0.1). Table 7 shows the results on the probing tasks. While Glorot achieves slightly better results on BShift and TopConst, CMOW's ability to memorize word content is improved by a wide  margin by our initialization strategy.
We have trained five different models: CBOW and CMOW with d = 20 and d = 28, which lead to 400-dimensional and 784-dimensional word embeddings, respectively. We also trained the Hybrid CBOW-CMOW model with d = 20 for each component, so that the total model has 800 parameters per word in the lookup tables. We report the results of two more models: H-CBOW is the 400-dimensional CBOW component trained in Hybrid and H-CMOW is the respective CMOW component. Below, we compare the 800-dimensional Hybrid method to the 784-dimensional CBOW and CMOW models.  Considering the linguistic probing tasks (see Table 1), CBOW and CMOW show complementary results. While CBOW yields the highest performance at word content memorization, CMOW outperforms CBOW at all other tasks. Most improvements vary between 1-3 percentage points. The difference is approximately 8 points for CoordInv and Length, and even 21 points for BShift. The hybrid model yields scores close to or even above the better model of the two on all tasks. In terms of relative numbers, the hybrid model improves upon CBOW in all probing tasks but WC and SOMO. The relative improvement averaged over all tasks is 8%. Compared to CMOW, the hybrid model shows rather small differences. The largest loss is by 4% on the CoordInv task. However, due to the large gain in WC (20.9%), the overall average gain is still 1.6%. We now compare the jointly trained H-CMOW and H-CBOW with their separately trained 400dimensional counterparts. We observe that CMOW loses most of its ability to memorize word content, while CBOW shows a slight gain. On the other side, H-CMOW shows, among others, improvements at BShift.  Regarding the probing tasks, we observe that CMOW embeddings better encode the linguistic prop  erties of sentences than CBOW.  Due to joint training, our hybrid model learns to pick up the best features from CBOW and CMOW simultaneously. It enables both models to focus on their respective strengths. This can best be seen by observing that H-CMOW almost completely loses its ability to memorize word content. In return, H-CMOW has more capacity to learn other properties, as seen in the increase in performance at BShift and others. A complementary behavior can be observed for H-CBOW, whose scores on Word Content are increased. Consequently, with an 8% improvement on average, the hybrid model  Word Content are increased. Consequently, with an 8% i is substantially more linguistically informed than CBOW.
Table 2 shows the scores from the supervised downstream tasks. Comparing the 784-dimensional models, again, CBOW and CMOW seem to complement each other. This time, however, CBOW has the upperhand, matching or outperforming CMOW on all supervised downstream tasks except  TREC by up to 4 points. On the TREC task, on the other hand, CMOW outperforms CBOW by 2.5 points. Our jointly trained model is not more than 0.8 points below the better one of CBOW and CMOW on any of the considered supervised downstream tasks. On 7 out of 11 supervised tasks, the joint model even improves upon the better model, and on SST2, SST5, and MRPC the difference is more than 1 point. The average relative improvement over all tasks is 1.2%.  Our CMOW model produces sentence embeddings that are approximately at the level of fast  Sent (Hill et al., 2016). Thus, CMOW is a reasonable choice as a sentence encoder.  observe that CMOW embeddings better encode the linguistic propCMOW gets reasonably close to CBOW on some downstream tasks.  However, CMOW does not in general supersede CBOW embeddings.
Regarding the unsupervised downstream tasks (Table 3), CBOW is clearly superior to CMOW on all datasets by wide margins. For example, on STS13, CBOW's score is 50% higher. The hybrid model is able to repair this deficit, reducing the difference to 8%. It even outperforms CBOW on two of the tasks, and yields a slight improvement of 0.5% on average over all unsupervised downstream tasks. However, the variance in relative performance is notably larger than on the supervised downstream tasks.
the success of our training schema for the CMOW model are two changes to the original word2vec training. First, our initialization strategy improved the downstream performance by 2.8% compared to Glorot initialization.
alization strategy improved the downstream performance by 2.8% compared Secondly, by choosing the target word of the objective at random, the perfor  mance of CMOW on downstream tasks improved by 20.8% on average.  and on all unsupervised downstream tasks
To test the effectiveness of this modified objective, we evaluate it with the same experimental setup as described in Section 4. Table 4 lists the results on the linguistic probing tasks. CMOW-C and CBOW-C refer to the models where the center word is used as the target. CMOW-R and CBOW-R refer to the models where the target word is sampled randomly. While CMOW-R and CMOW-C perform comparably on most probing tasks, CMOW-C yields 5 points lower scores on WordContent  and BigramShift.
Consequently, CMOW-R also outperforms CMOW-C on 10 out of 11 supervised  downstream tasks  On average over all downstream tasks, the relative improvement is 20.8%.  scores on downstream tasks increase on some tasks and decrease on others. The differences ar miniscule. On average over all 16 downstream tasks, CBOW-R scores 0.1% lower than CBOW-C.  For CBOW, the
In Table 3 we classified errors according to named entity types  PER is the easiest type for all systems. Even name matching, without any learning, can correctly predict in half of the cases.  For LOC, it turns out that candidate selection is a bottleneck: when candidate selection was flawless, the models made only about 12% errors, down from about 57%. For MISC a similar conclusion can be drawn.
Table 2 shows results on the test set. 'Name matching' is far behind the two models.  MIL-ND achieves higher precision, recall, and F1 than MIL,  Using its confidence at test time (τ MIL-ND, 'All' setting) was also beneficial in terms of precision and F1 (it cannot possibly increase recall). Because all the test data points are valid for the 'In E+' setting, using the ND classifier had a slight negative effect on F1.  MIL-ND significantly outperforms MIL: the 95% confidence intervals for them do not overlap. However, this is not the case for MIL-ND and τ MIL-ND.
We perform an entailment experiment using BERT (Devlin et al., 2019) fine-tuned on the MultiNLI dataset (Williams et al., 2018) as a NLI model. We are interested in exploring whether a generated sentence (hypothesis) is semantically entailed by the reference sentence (premise). In a related text generation task, Falke et al. (2019) employ NLI models to rerank alternative predicted abstractive summaries.  Table 6 shows the average probabilities for entailment, contradiction and neutral classes on the LDC2017T10 test set. All G2S models have  higher entailment compared to S2S. G2S-GGNN has 33.5% and 5.2% better entailment performances than S2S, when REF entails GEN and GEN entails REF, respectively. G2S models also generate sentences that contradict the reference sentences less. This suggests that our models are capable of capturing better semantic information from the graph generating outputs semantically related to the reference sentences.
Table 2 shows the comparison between the proposed models, the baseline and other neural models on the test set of the two datasets.  For both datasets, our approach substantially outperforms the baselines. In LDC2015E86, G2S-GGNN achieves a BLEU score of 24.32, 4.46% higher than Song et al. (2018), who also use the copy mechanism. This indicates that our architecture can learn to generate better signals for text generation. On the same dataset, we have competitive results to Damonte and Cohen (2019). However, we do not rely on preprocessing anonymisation not to lose semantic signals. In LDC2017T10, G2S-GGNN achieves a BLEU score of 27.87, which is 3.33 points higher than Damonte and Cohen (2019), a state-of-the-art model that does not employ external information. We also have competitive results to Guo et al. (2019), a very recent state-of-the-art model.  We also outperform Cao and Clark (2019) improving BLEU scores by 3.48% and 4.00%, in LDC2015E86 and LDC2017T10, respectively. In contrast to their work, we do not rely on (i) leveraging supplementary syntactic information and (ii) we do not require an anonymization preprocessing step. G2S-GIN and G2S-GAT have comparable performance on both datasets. Interestingly, G2S-GGNN has better performance among our models. This suggests that graph encoders based on gating mechanisms are very effective in text generation models. We hypothesize that the gating mechanism can better capture longdistance dependencies between nodes far apart in the graph.
We follow the method of Konstas et al. (2017), which is fine-tuning the model on the LDC2015E86 training set after every epoch of pretraining on the Gigaword data. G2S-GGNN outperforms others with the same amount of Gigaword sentences (200K), achieving a 32.23 BLEU score, as shown in Table 3. The results demonstrate that pretraining on automatically generated AMR graphs enhances the performance of our model.
In Table 4, we report the results of an ablation study on the impact of each component of our model on the development set of LDC2017T10 dataset by removing the graph encoders. We also report the number of parameters (including embeddings) used in each model. The first thing we notice is the huge increase in metric scores (17% in BLEU) when applying the graph encoder layer, as the neural model receives signals regarding the graph structure of the input. The dual representation helps the model with a different view of the graph, increasing BLEU and METEOR scores by 1.04 and 0.68 points, respectively. The complete model has slightly more parameters than the model without graph encoders (57.6M vs 61.7M).
Table 5 shows METEOR5 scores for the LDC2017T10 dataset.  The performances of all models decrease as the diameters of the graphs increase. G2S-GGNN has a 17.9% higher METEOR score in graphs with a diameter of at most 7 compared to graphs with diameters higher than 13. This is expected as encoding a bigger graph (containing more information) is harder than encoding smaller graphs.  Since the models have fewer examples of bigger graphs to learn from, this also leads to worse performance when handling graphs with higher diameters. We also investigate the performance with respect to the sentence length. The models have better results when handling sentences with 20 or fewer tokens. Longer sentences pose additional challenges to the models.  G2S-GIN has a better performance in handling graphs with node out-degrees higher than 9. This indicates that GINs can be employed in tasks where the distribution of node degrees has a long tail. Surprisingly, S2S has a better performance than G2S-GGNN and G2S-GAT when handling graphs that contain high degree nodes.
We also evaluate the semantic adequacy of our model (how well does the generated output match the input?) by comparing the number of added and missing tokens that occur in the generated versus reference sentences (GOLD). As shown in Table 8, G2S approaches outperform the S2S baseline. G2S-GIN is closest to GOLD with respect to both metrics suggesting that this model is better able to generate novel words to construct the sentence and captures a larger range of concepts from the input AMR graph, covering
we trained systems using a smaller data size (200K sentences),  The results are shown in Table 4.  we observe a variance in classifier accuracy of 1-2%, based on target language,  This is true for both POS and SEM tagging.
we consider two baselines: most frequent tag (MFT) for each word according to the training set (with the global majority tag for unseen words); and unsupervised word embeddings (UnsupEmb) as features for the classifier,  We also report an upper bound of directly training an encoderdecoder on word-tag sequences (Word2Tag),  Table 2 shows baseline and upper bound results. The UnsupEmb baseline performs rather poorly on both POS and SEM tagging.  NMT word embeddings (Table 3, rows with k = 0) perform slightly better,  the results are still below the most frequent tag baseline (MFT),  This is above the UnsupEmb baseline but only on par with the MFT baseline (Table 2).  The results are also far below the Word2Tag upper bound (Table 2).  which is far above the UnsupEmb and MFT baselines. While these results are below the oracle Word2Tag results (Table 2),
NMT word embeddings (Table 3, rows with k = 0) perform slightly better,  Table 3 summarizes the results of training classifiers to predict POS and SEM tags using features extracted from different encoding layers of 4  layered NMT systems.3 In the POS tagging results (first block), as the representations move above layer 0, performance jumps to around 91–92%.  This is above the UnsupEmb baseline but only on par with the MFT baseline (Table 2).  The results are also far below the Word2Tag upper bound (Table 2).  Comparing layers 1 through 4, we see that in 3/5 target languages (Ar, Ru, Zh), POS tagging accuracy peaks at layer 1 and does not improve at higher layers, with some drops at layers 2 and 3. In 2/5 cases (Es, Fr) the performance is higher at layer 4.  Turning to SEM tagging (Table 3, second block), representations from layers 1 through 4 boost the performance to around 87-88%,  which is far above the UnsupEmb and MFT baselines. While these results are below the oracle Word2Tag results (Table 2),  Going beyond the 1st encoding layer, representations from the 2nd and 3rd layers do not consistently improve semantic tagging performance. However, representations from the 4th layer lead to significant improvement with all target languages except for Chinese.  we found that En-En encoder-decoders (that is, English autoencoders) produce poor representations for POS and SEM tagging (last column in Table 3). This is especially true with higher layer representations (e.g. around 5% below the MT models using representations from layer 4). In contrast, the autoencoder has excellent sentence recreation capabilities (96.6 BLEU).  Table 3 also shows results using features obtained by training NMT systems on different target languages (the English source remains fixed). In both POS and SEM tagging, there are very small differences with different target languages (∼0.5%), except for Chinese which leads to slightly worse representations. While the differences are small,
improvements in both translation (+1-2 BLEU) and SEM tagging quality (+3-4% accuracy), across the board, when using a bidirectional encoder. Some of our bidirectional models obtain 92-93% accuracy. We observed similar improvements on POS tagging. Comparing POS and SEM tagging (Table 5), we note that higher layer representations improve SEM tagging, while POS tagging peaks at layer 1. we noticed small but consistent improvements in both translation (+0.9 BLEU) and POS and SEM tagging (up to +0.6% accuracy) when using features extracted from an NMT model trained with residual connections (Table 5). We also observe similar trends as before: POS tagging does not benefit from features from the upper layers, while SEM tagging improves with layer 4 representations.
Table 8 summarize the training accuracies of the attacker network. The Mention/Race task achieves the highest score of 64.3% whereas the Mention/Gender task achieves the lowest 58.1%.
The results in Table 1 indicate that the classifiers achieve reasonable accuracies for the main tasks.  the protected attributes, race is highly predictable (83.9%) while age and gender can also be recovered at above 64% accuracy.
This experiment suggests an upper bound on the amount of leakage of protected attributes when we do not actively attempt to prevent it. The Balanced section in Table 2 summarizes the validation-set accuracies. While the numbers are lower than when training directly (Table 1), they are still high enough to extract meaningful and possibly highly sensitive information (e.g. DIAL Race direct prediction is 83.9% while DIAL Race leakage on the balanced Sentiment task is 64.5%).  We simulate this more realistic scenario by constructing unbalanced datasets in which the main tasks (sentiment/mention) remain balanced but the protected class proportions within each main class are not, as demonstrated in Figure 1b.  We then follow the leakage experiment on the unbalanced datasets. The attacker is trained and tested on a balanced dataset. Otherwise, the attacker can perform quite well on the male/female task simply by learning to predict sentiment, which does not reflect leakage of gender data to the representation. When training the attacker on balanced data, its decisions cannot rely on the sentiment information encoded in the vectors, and must look for encoded information about the protected attributes. The results in Table 2 indicate that both task accuracy and attribute leakage are stronger in the unbalanced case.
However, training the attacker network on the resulting encoder vectors reveals a different story. For example, when considering the encoder after 50 training epochs (adversary accuracy of 49.0%), the attacker reaches 56.0% accuracy: substantially higher than the adversarial's success rate, despite sharing the exact same architecture, and being trained and tested on the exact same dataset. Table 3 summarizes the attacker's recovery rate on the adversarialy-trained encoders for the different settings. In all cases, the adversarial's success rate is around 50%, while the attacker's rate is substantially higher.
We compare encoders Leaky-EMB and Leaky-RNN to gauge which module has a greater contribution to the data leakage.  Table 6 summarize the results, implying that the leakage is caused mainly by the RNN, and less by the Embedding Matrix.
Table 5 shows the test perplexity of different models.10 In this task, LRN significantly outperforms GRU, ATR and SRU, and achieves near the same perplexity as LSTM.
Table 1 shows the test accuracy and training time of different models. Our implementation outperforms the original model where Rockt¨aschel et al. (2016) report an accuracy of 83.50. Overall results show that LRN achieves competitive performance but consumes the least training time. Although LSTM and GRU outperform LRN by 0.3∼0.9 in terms of accuracy, these recurrent units sacrifice running efficiency (about 7%∼48%) depending on whether LN and BERT are applied. No significant performance difference is observed between SRU and LRN, but LRN has fewer model parameters and shows a speedup over SRU of 8%∼21%.  However, for LSTM, GRU and ATR, LN results in significant computational overhead (about 27%∼71%). In contrast, quasi recurrent models like SRU and LRN only suffer a marginal speed decrease.  Results with BERT show that contextual information is valuable for performance improvement. LRN obtains additional 4 percentage points gain with BERT and reaches an accuracy of around 89.9.  In addition, although the introduction of BERT brings in heavy matrix computation, the benefits from LRN do not disappear. LRN is still the fastest model, outperforming other recurrent units by 8%∼27%.
Table 2 summarizes the classification results. LRN achieves comparable classification performance against ATR and SRU, but slightly  underperforms LSTM and GRU (-0.45∼-1.22).  LRN accelerates the training over LSTM and SRU by about 20%,
The results in Table 3 show that translation quality of LRN is slightly worse than that of GRU (-0.02 BLEU).  however, oLRN yields the best BLEU score of 26.73, outperforming GRU (+0.45 BLEU). In addition, the training time results in Table 3 confirm the computational advantage of LRN over all other recurrent units, where LRN speeds up over ATR and SRU by approximately 25%.  the recurrent unit with the least computation operations, i.e. ATR, becomes the fastest. Still, both LRN and oLRN translate sentences faster than SRU (+15%/+6%).
Table 4 lists the EM/F1 score of different models. In this task, LRN outperforms ATR and SRU in terms of both EM and F1 score. After integrating Elmo for contextual modeling, the performance of LRN reaches the best (76.1  EM and 83.83 F1), beating both GRU and LSTM (+0.33EM, +0.71F1).
As shown in Table 6, the performance of LRN matches that of ATR and SRU, though LSTM and GRU operate better (+1.05 and +0.79).
We experiment with SNLI and PTB tasks. Results in Table 7 show that although the accuracy on SNLI is acceptable, gLRN and eLRN perform significantly worse on the PTB task.
BLEU-2, BLEU-4, ROUGE-2 recall, and METEOR are reported in Table 3 for both setups.  Under system setup, our model CANDELA statistically significantly outperforms all comparisons and the retrieval model in all metrics, based on a randomization test (Noreen, 1989) (p <  .0005). Furthermore, our model generates longer sentences whose lengths are comparable with human arguments, both with about 22 words per sentence. This also results in longer arguments. Under oracle setup, all models are notably improved due to the higher quality of reranked passages, and our model achieves statistically significantly better BLEU scores. Interestingly, we observe a decrease of ROUGE and METEOR, but a marginal increase of BLEU-2 by removing passages from our model input. This could be because the passages introduce divergent content, albeit probably on-topic, that cannot be captured by BLEU.
Inter-annotator agreement scores (Krippendorff's α) of 0.44, 0.58, 0.49 are achieved for the three aspects, implying general consensus to intermediate agreement.  Our system obtains the highest appropriateness and content richness among all automatic systems. This confirms the previous observation that our model produces more informative argument than other neural models. SEQ2SEQAUG has a marginally better grammaticality score, likely due to the fact that our arguments are longer, and tend to contain less fluent generation towards the end.  Furthermore, we see that human arguments are
In this first experiment we reduce the vocabulary to the top 1,000 terms with the highest number of contexts. Table 3 presents the general overview of these values for each method in each corpus, where the highest values are presented in bold.  Analyzing Table 3, we can observe that all values of precision using the Portuguese corpora have higher scores when compared with the English corpora.  A higher number of terms associated in hypernyms tends to increase the precision. Another aspect to be considered is the fact that as Onto.PT is automatically constructed, there are relations that would not exist if it was manually constructed or revised.  As we can observe in Table 3, Patt has the best values of precision for the English corpora while DocSub has the best values for the Portuguese corpora. TF has the best values of recall and f-measure for all corpora but the English version of TED Talks which has in DF the best value of recall and in DocSub the best value of f-measure. It was expected quite similar values of precision, recall and f-measure between TF and DF using the Europarl corpora since the size of each document was set to the size of the phrase because Europarl does not have document borders.  This value differs when a term appears more than once in a phrase, and thus, having a higher value of TF than DF. In some cases it seems to make difference in results, e.g., Europarl in Portuguese which increased the precision from P=0.5984 in DF to P=0.6109 in TF, as well as the recall from R=0.5184 in DF to R=0.6727 in TF, resulting in an increase of f-measure from F=0.5555 in DF to F=0.6403 in TF.  When comparing DF model which takes into account only the number of documents that the word occurs, with DocSub which considers the number of shared documents between two words, DocSub achieved better values of precision, but lower values of recall. In fact, DocSub had worse results in precision only when using Europarl corpus in English, where DF reached best values of precision and f-measure.  Another interesting observation is to compare the results obtained by DF with the results achieved by HClust. This comparison is interesting since HClust uses the values of document frequency over semantically clustered terms.  As we can observe, it seems that clustering semantically related terms will increase the precision (at least for the top 1,000 terms in the English corpora used in this experiment) as expected.  Comparing the values achieved by methods containing all relations (Table 3) and the reduced taxonomies, we can see that all recalls and f-measures decreased, as expected.
Observing results from HClust, which obtained the best f-measure in a cluster containing 1,000 terms (the f-measure was still rising), we decide to perform a second experiment, increasing the number of terms, and consequently clusters.  Table 4 presents the values of precision and recall for all models using a vocabulary containing up to 10,000 words, where DocSub and HClust contain results when the best f-measure was achieved, and Patt consider all patterns with the limited number of words.  As we can see, values of precision were lower for most methods, with exception of Patt and DocSub, which increased for most corpora. When increasing the number of terms to 10,000, the DocSub models using Europarl corpora performed better than when using TED Talks corpora.  Although decreasing the values of precision, TF and DF increased the values of recall, but decreasing the values of f-measure. As occurred in the experiment using the top 1,000 words, this experiment also kept TF with the highest values of f-measure for most methods. TF and DF achieved almost the same values of precision, recall and f-measure using the English corpora, achieving the same value of precision (P=0.0150) and f-measure (F=0.0293) when using the Europarl corpus in English.  When comparing DF with HClust, it seems a good approach in English to verify the hierarchical relation only for terms that are semantically related instead of considering all terms.  The lowest values of precision are achieved by DSim model, and the lowest recalls are obtained by HClust and Patt models.
Table 5 presents the values of precision, recall and f-measure for the methods in all corpora. The filtering on multiple hypernyms is applied in relations extracted using 1,000 terms in the dictionary.  Analyzing Table 5 we observe that Patt achieves again the best precision values for the English corpora. On the other hand, choosing the best hypernym worked very well for DocSub which obtained the best precision for the Portuguese corpora. As filtering out multiple hypernyms might remove also correct relations, the recall values for all corpora are very low.  The values of precision increased for most corpora of the Patt and DocSub models.  Comparing the values achieved by HClust using 1,000 terms and the ones obtained when reducing to one parent, we can see that they are almost the same. Only TED Talks corpora obtained a decrease in precision (from P=0.0664 to P=0.0661 in English and from P=0.5656 to P=0.5295 in Portuguese).  Increasing the number of terms form 1,000 to 10,000 only increased the recall in TF and DF models.
Using the direct graph with transitive reduction for each model, we generated the metrics presented in Table 6. The results are generated for models using the top 1,000 terms of each corpus in English generated for the automatic evaluation, i.e., models using nouns instead of noun phrases.  As we can observe in Table 6, limiting the number of terms to 1,000, Patt and DocSub do not to generate relations for all terms. Patt model could not generate relations for all terms because terms must to be in a pattern in order to have their taxonomic relation identified.  For example, using relations generated by TF model using the Europarl corpus, we can understand the MaxDepth as having 789 terms with different values of term frequency, while having 211 that share the same value of term frequency with other terms. Using the SLQS model, we can understand that 902 terms share different values of entropy, while 98 share the same value with other terms.
Table 7 contains the results for each metric after applying the transitive reduction for relations generated by models using the top 1,000 terms of each corpus in Portuguese.  The results for the Portuguese corpora are quite similar to the ones generated by the English corpora, having terms without relations in Patt and DocSub, and DSim, SLQS, TF and DF generating deep taxonomies, affirming the characteristics of each method. For Portuguese, the number of relations found by Patt model using the TED Talks corpus were smaller than the one found using the English corpus, impacting the maximum depth. On the other hand, the number of siblings for a term was greater.
Table 1 shows the results with different implementations in P2, i.e., question type, answer score sampling, and hidden dictionary learning. Overall, all of the implementations can improve the performances of base models. Specifically, the implementations of P2 can further boost performance by at most 11.75% via hidden dictionary learning.
Table 2 shows the results about applying our principles on four different models (i.e., LF , HCIAE , CoAtt  and RvA ). In general, both of our principles can improve all the models in any ablative condition (i.e., P1, P2, P1+P2). Note that the effectiveness of P1 and P2 are additive, which means combining P1 and P2 performs the best.
We use the representations in the 9-th BERT layer for fair comparison of BERTScore and MoverScore and show results on the machine translation task in Table 5. MoverScore outperforms both asymmetric HMD factors, while if they are combined via harmonic mean, BERTScore is on par with MoverScore.  We also observe that WMD-BIGRAMS slightly outperforms WMD-UNIGRAMS on 3 out of 4 language pairs.
Table 1: In all language pairs, the best correlation is achieved by our word mover metrics that use a BERT pretrained on MNLI as the embedding generator and PMeans to aggregate the embeddings from different BERT layers, i.e., WMD-1/2+BERT+MNLI+PMeans. Note that our unsupervised word mover metrics even outperforms RUSE, a supervised metric. We also find that our word mover metrics outperforms the sentence mover.
Tables 3: Interestingly, no metric produces an even moderate correlation with human judgments, including our own.  However, best correlation is still achieved by our word mover metrics combining contextualized representations.
Table 4: Word mover metrics outperform all baselines except for the supervised metric LEIC, which uses more information by considering both images and texts.
that under similar Acc, M2 has much better semantic similarity for both Yelp and Literature. In fact, cyclic consistency loss proves to be the strongest driver of semantic preservation across all of our model configurations. The other losses do not constrain the semantic relationship across style transfer, so we include the cyclic loss in M3 to M7.  When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss reduces PP, sometimes at a slight cost of semantic preservation.
Overall, the results show the same trends as our automatic metrics. For example, on Yelp, large differences in human judgments of semantic preservation (M2>M0, M7>M0, M7>M2) also show the largest differences in Sim, while M6 and M7 have very similar human judgments and very similar Sim scores.
We describe a human sentence-level validation of our metrics in Table 5.  To validate Acc, human annotators were asked to judge the style of 100 transferred sentences  We then compute the percentage of machine and human judgments that match.  We validate Sim and PP by computing sentence-level Spearman's ρ between the metric and human judgments  From Table 5, all validations show strong correlations on the Yelp dataset and reasonable correlations on Literature.
under similar Acc, M2 has much better semantic similarity for both Yelp and Literature. In fact, cyclic consistency loss proves to be the strongest driver of semantic preservation across all of our model configurations. The other losses do not constrain the semantic relationship across style transfer, so we include the cyclic loss in M3 to M7.  Table 2 shows that the model with paraphrase loss (M1) slightly improves Sim over M0 on both datasets under similar Acc. For Yelp, M1 has better Acc and PP than M0 at comparable semantic similarity. So, when used alone, the paraphrase loss helps. However, when combined with other losses (e.g., compare M2 to M4), its benefits are mixed.  When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss reduces PP, sometimes at a slight cost of semantic preservation.
BLEU scores and post-transfer accuracies are shown in Table 6. The most striking result is that untransferred sentences have the highest BLEU score by a large margin, suggesting that prior work for this task has not yet eclipsed the trivial baseline of returning the input sentence. However, at similar levels of Acc, our models have higher BLEU scores than prior work. We additionally find that supervised BLEU shows a trade-off with Acc: for a single model type, higher Acc generally corresponds to lower BLEU.
For each class, we measured the disfluency detection recall (relative frequency of reparandum tokens that were predicted correctly), as well as the percentage of tokens associated with each class. The results in Table 2 confirm that error rates are higher for restarts, longer rephrasings, and complex disfluencies.
Table 3 breaks down performance for different lengths and word class to explore this difference. We found that rephrase disfluencies that contain content words are harder for the model to detect, compared to rephrases with function words only, and error increases for longer disfluencies.
Our experiments evaluate the use of innovations with two popular multimodal fusion approaches: early fusion and late fusion. Our baselines include models with text-only, prosody cues only (raw), and innovation features only as inputs. Since innovations require both text and raw prosodic cues, this baseline is multimodal. In addition, for the late fusion experiments, we show the optimal value of α, the interpolation weight from Equation 5.  We found that innovations are helpful in both early and late fusion frameworks, while late fusion performs better on average. The interpolation weight α for the late fusion experiments is high when innovations are used, which further indicates that innovation features are useful in overall prediction. Interestingly, innovation features alone perform surprisingly well.
Table 2 shows a comparison of the detection performance. As for the micro F1 evaluation metric, our model achieves the highest performance (83.54%) on the FNC-1 testing subset. The average method can lose emphasis or key word information in a claim; the CNN-based method can only capture local dependency among the text with limit to the filter size; the RNN-based method can obtain semantic relationship in a sequential manner. Differently, the self-attention method is able to combine embedding information between each pair of words, which means more accurate semantic matching of the claim and the piece of text. Compared with the deterministic self-attention, our method is a stochastic approach that is experimentally proven to better integrate each word embedding.
The deep network based NeuralDater model in (Vashishth et al., 2018) outperforms previous feature engi  neered (Chambers, 2012) and statistical methods (Kotsakos et al., 2014) by a large margin.  We observe a similar trend in our case. Compared to the state-of-the-art model NeuralDater, we gain, on an average, a 3.7% boost in accuracy on both the datasets (Table 2).
Attentive Graph Convolution (Section 4.2.2) proves to be effective for OE-GCN, giving a 2% accuracy improvement over non-attentive T-GCN of NeuralDater (Table 3). Similarly the efficacy of word level attention is also prominent from Table 3.
Table 2 illustrates the performance (F1 scores) of JRNN (Nguyen et al., 2016), DMCNN (Chen the two baseline model Embedet al., 2015), ding+T and CNN in Chen et al. (2015) and our framework in trigger classification subtask and argument role labeling subatsk.  We can see that our framework significantly outperforms all the other methods, especially in trigger classification subtask. In the 1/N data split of triggers, our framework is 7.9% better than the JRNN,
Table 1 shows the overall performance comparing to the above state-of-the-art methods with golden-standard entities. From the table, we can see that our JMEE framework achieves the best F1 scores for both trigger classification and argumentrelated subtasks among all the compared methods. There is a significant gain with the trigger classification and argument role labeling performances, which is 2% higher over the best-reported models.
The results of the different models are presented in Table 3. For each model we report both perplexity and accuracy (except for discriminative training, where perplexity is not valid), where each of them is reported according to the best performing model on that measure (on the dev set). We also report the WER of all models, which correlates perfectly with the accuracy measure.  Extending the vocabulary without training those additional words, results in a 2.37-points loss on the perplexity measure, while our evaluation metric (accuracy) stays essentially the same.  The FINETUNED-LM baseline is the strongest baseline, outperforming all others with an accuracy of 65.4%. Similarly, when using discriminative trainthe FINE-TUNED-DISCRIMINATIVE model ing, outperforms the CS-ONLY-DISCRIMINATIVE model. Note that using discriminative training, even with no additional monolingual data, leads to better performance than that of the best language model: the CS-ONLY-DISCRIMINATIVE model achieves an accuracy of 70.5%, 5.1 points more than the accuracy of the FINE-TUNED-LM model. We gain further improvement by adding monolingual data and get an even higher accuracy of 75.5%, which is 10.1 points higher than the best language model.
Table 4 depicts the results when using subsets of the CS training data with discriminative training. The less code-switching data we use, the more the effect of using the monolingual data is significant: we gain 8.8, 6.5, 4.2 and 5 more accuracy points with 25%, 50%, 75% and 100% of the data, respectively. In the case of 25% of the data, the FINE-TUNED-DISCRIMINATIVE model improves over CS-ONLY-DISCRIMINATIVE by 17 relative percents.
Table 5 breaks down the results of the different models according to two conditions: when the gold sentence is code-switched, and when the gold sentence is monolingual.  the FINE-TUNEDDISCRIMINATIVE model is able to prioritize the gold sentence better than all other models, under both conditions. The improvement we get is most significant when the gold sentence is CS: in those cases we get a dramatic improvement of 27.73 accuracy points (a relative improvement of 58%).
Table 7 shows that compared to a baseline without gaze features, the results improve by 3% F1-score.
We achieve a minor, but nonetheless significant improvement (shown in Table 5), which strongly supports the generalizability effect of the typeaggregated features on unseen data.
Table 1 shows that our proposed token level embedding scheme OntoLSTM-PP outperforms the better variant of our baseline LSTM-PP (with GloVe-retro intialization) by an absolute accuracy difference of 4.9%, or a relative error reduction of 32%. OntoLSTM-PP also outperforms HPCD (full), the previous best result on this dataset.  Initializing the word embeddings with GloVeretro (which uses WordNet as described in Faruqui et al. (2015)) instead of GloVe amounts to a small improvement, compared to the improvements obtained using OntoLSTM-PP.
Table 2 shows the effect of using the PP attachment predictions as features within a dependency parser. We note there is a relatively small difference in unlabeled attachment accuracy for all dependencies (not only PP attachments), even when gold PP attachments are used as additional features to the parser. However, when gold PP attachment are used, we note a large potential improve  ment of 10.46 points in PP attachment accuracies (between the PPA accuracy for RBG and RBG + Oracle PP), which confirms that adding PP predictions as features is an effective approach. Our proposed model RBG + OntoLSTM-PP recovers 15% of this potential improvement, while RBG + HPCD (full) recovers 10%,  For example, the unlabeled attachment score (UAS) of the baselines RBG and RBG + HPCD (full) are 94.17 and 94.19, respectively, in Table 2, compared to 93.96 and 94.05, respectively, in Belinkov et al. (2014).
The second row in Table 3 shows the test accuracy of a system trained without sense priors  and the third row shows the effect of making the token representations context-insensitive by giving a similar attention score to all related concepts, essentially making them type level representations, but still grounded in WordNet.
In our first series of experiments, we observed that domain-tuning is very important when using Marian.  Table 2 shows the scores on development data. We also tried decoding with an ensemble of three independent runs, which also pushed the performance a bit.
Table 3 lists the scores we obtained on development data.  For Marian amun, the effect is negligible as we can see in Table 3. For the Transformer, domain labels had little effect on BLEU but were clearly beneficial according to chrF-1.0.
Table 4 summarizes the scores on the three development test sets for English-French and EnglishGerman. We can see that the dual attention model does not work at all and the scores slightly drop. The concatenation approach works better  However, the improvements are small if any
Table 5 shows the BLEU scores for this configuration with different ways of integrating the visual features. The results are inconclusive. The ranking according to chrF-1.0 was not any clearer.
Table 6 shows results of ablation experiments removing or modifying one component or data choice at a time, and results when using ensemble decoding. Using ensemble decoding gave a consistent but small improvement. Multi-lingual models were clearly better than mono-lingual models. For French, 6M sentences of subtitle data gave worse results than 3M.  It appears that the output vocabulary was reduced back towards the vocabulary seen in the multi-modal training set. When the experiment was repeated so that the finetuning phase included the text-only data, the performance returned to approximately the same level as without tuning (+multi-modal finetune row in Table 6).  BLEU scores for French were surprisingly slightly improved by this procedure.
Table 6 shows the metrics  SMT seems to retain the most lexical richness according to the LD metrics we used (TTR, Yule's  and MTLD).
Number of parallel sentences in the train, test and development splits for the language pairs we used.  We used +/- 2M sentence pairs from the Europarl corpora for each of the language pairs.  split the data into train, test and development sets,  Details on the different datasets can be found in Table 1. We chose to include large quantities of data in our test sets
Table 2 (first two columns) shows the training vocabularies for the source and target sides.  Table 3 presents  sizes for the RNN, SMT and Transformer systems.  this table clearly shows how source and target vocabularies are comparable in the original datasets,
we present BLEU and TER for the REV systems in Table 5,  While Transformer models are the best ones according to the evaluation metrics,
Evaluating this speaker-invariant representation gave contradictory results, shown in Table 2: very good scores on paraphrase retrieval, but zero correlation with visual space.
Table 1 shows the evaluation results on synthetic speech. Representations learned by Audio2vec and SegMatch are compared to the performance of random vectors, mean MFCC vectors, as well as visually supervised representations (VGS, model from Chrupała et al. (2017)). Audio2vec works better than chance and mean MFCC on paraphrase retrieval, but does not correlate with the visual space. SegMatch works much better than Audio2vec according to both criteria. It does not come close to VGS on paraphrase retrieval, but it does correlate with the visual modality even better.
Using SIFT we observed substantial differences in the text generated by our fine-tuned decoder when trained with each of the different  While the RNN primarily employs <UNK> tokens or repeats previous words, the CNN masks out <UNK> tokens using determiners or prepositions. In contrast, DAN masks out punctuation and determiners using words indicative of the class label (i.e. nouns, verbs, adjectives). We hypothesize that these patterns stem from the inductive biases of the classifiers. DAN receives a stronger signal by repeating words with a higher sentiment value due to its averaging, while 1NLTK (Loper and Bird, 2002) was used for POS tagging. the CNN does not repeat words (thus having the least amount of changes) and removes uninformative words as its max-pooling layer selects only the most important ones. Similarly, the gates of the LSTM may allow the model to ignore the random and thus noisy <UNK> embeddings, which enables it to use this token as a masking operation to ignore unimportant words.  Examples are in Table 1, results in 2 and 3.
In order to identify differences, we conducted an automated study of the reconstructed text by inspecting changes in the proportion of part-ofspeech tags1 and an increase or decrease in word polarity for sentiment compared to the original input.  To compare our abstractive with an extractive approach (RNP; Lei et al., 2016), we compute the overlap of retained terms in Table 2 (bottom row). We can see that the DAN has the highest overlap, indicating that it retains words, while the CNN and RNN reformulate sentences. These scores highlight the differences of our approach, as our model does not solely extract indicative words, but reformulates the original sentence.
In order to automatically identify if SIFT retains the sentiment of the sentences, we analyze the output using SentiWordNet (Baccianella et al., 2010). By considering only adjectives, we obtain a measure of the positive and negative score for each sentence before and after fine-tuning. The difference of these scores averaged over all examples provides us with a sense of whether the fine-tuning increases the polarity of the sentences (Table 3). We see a constant increase in sentiment value in both directions across all three models after finetuning demonstrating that the framework is able to pick up on words that are indicative of sentiment. This is especially true in the case of DAN where we see a large increase as the decoder repeatedly predicts words having high sentiment value. Overall, these results indicate that SIFT is able to highlight certain inductive biases of the model and is able to reformulate and amplify the meaning of the original text based on the classifier's preferences.
We compare the most frequent SIFT terms with the most indicative words w.r.t. a class using PMI with 100 smoothing, following Gururangan et al. (2018). We list the most frequently used words in Table 4. To understand how similar the SIFT terms are to the PMI terms we calculate the weighted Kendall's tau correlation (Shieh, 1998), which weights terms higher in the list as more important. In the case of high correlation, we hypothesize that the classifier has memorized the artifacts of the data set, which in turn SIFT has leveraged to "fool" the classifier. This "fooling" in the case of sentiment analysis is due to the terms being indicative of the respective class and having high sentiment (cf. Table 3). For SNLI, we find that many of the top PMI and SIFT terms overlap, and slightly correlate (0.366). We find terms (as reported in Appendix A2), e.g. 'sleeping', 'cats', 'cat', that were identified as artifacts by Gururangan et al. (2018). In the PubMed task where the aim is to classify sentences as belonging to one of five classes—background, objective, methods, results, conclusions—the top terms in the setting where we change ground truth from "objective" to "conclusion" are intuitively relevant terms for that class. They do not, however, correlate with PMI, which might indicate that pretraining on large unlabelled data has enabled SIFT to capture these relations. Overall, this shows that SIFT is able to identify both previously known as well as novel artifacts. In contrast to PMI, SIFT uncovers the propensities of the trained model and not only the data set, giving insight into what the model has actually encoded.
For fair comparison of accuracies with original (unbalanced) dataset, we create a balanced train set which is of similar size as original dataset (referred to as Bhalf in table). For benchmarking, we also report results using the full balanced train set.
The comparison between {seq2seqr, LSTMr} and {seq2seqf, LSTMf} in the years 2000-13 showcases the benefit of modelling the full sequence of the word representations across time, compared to using the first and last representations only. Overall, our models provide a relative boost of 4.6% in μr and [35.7%, 42.8%, 5.8%] in Rec@k (for k=[5, 10, 50]) compared to the best performing baseline. seq2seqf and seq2seqrf models outperform the autoencoder (seq2seqr) in most metrics, while seq2seqrf yields the most stable results across all experiments. We explore these differences in detail in the last paragraph of this section. Effect of input/output lengths The performance of seq2seqr increases with the input size since by definition the decoder is able to detect words whose semantics have changed over a longer period of time (i.e., within [2000,i], with i increasing), while also modelling a longer sequence of a word’s representation through time. On the contrary, the performance of seq2seqf increases alongside the decrease of the number of input time steps. This is expected since, as i decreases, seq2seqf encodes a shorter input sequence and the decoding (and hence the semantic change detection) is applied on the remaining (and increased number of) time steps within [i+1,2013]. These findings provide empirical evidence that both models can achieve better performance if trained over longer sequences of time steps. in its worst performing setting, seq2seqrf still manages to achieve results that are close to the best performing model (μr=25.17, Rec@k=[21.54, 36.92, 83.08] for the three thresholds) and always better (or equal to) This is a very attractive aspect of the model as it removes the need to manually define the number of time steps to be fed to the encoder.
Comparing the different models, we see that the generic language models perform worse than their specific counterparts.
In this dataset, each phrase was assessed four times using a binary scale (compositional or non-compositional). For the word embedding based contextual representation model, relying more on the knowledge base while keeping the scenario to limited importance will lead to a high-performed model; for the ranked list based contextual representation model, on the other hand, adequately high adoption of localized context can lead to improved performance. The reason behind this can be that the knowledge base contains relatively trimmed but well-categorized information, therefore, the word embedding model can take full use of this text as informative vectors. In contrast, ranked lists, depending on tokens, work better on a large-scale corpus where they induce a large number of context windows. However, the knowledge base contains a limited number of tokens that may have little contribution to the final representation. Even though we can tune the weight of tokens from a knowledge base, it still can have limited influence in comparison to the long ranked list, which can be as long as 1000 tokens in our experiment.
On the other hand, the user information obtained from our web scraper shows clear unequal statistical values between fake and trustful users. The maximum values are shown for informative purposes, as they were afterwards normalized. It can be observed that fake reviewers tend to give lower ratings on their reviews than trustful reviewers, being their mean values 1.1 stars and 2.79 stars respectively from a maximum of 5.
In the case of user centric features, results were clearly improved in comparison with review centric features. Regarding the experiments, we highlight the contributions of the different subdivisions of user centric features we defined in the proposed framework, which were Social Features (S), Personal Features (P), Trusting Features (T) and Reviewing Activity Features (RA). Logistic Regression, Decision Tree, Random Forest, Gaussian Naive Bayes, AdaBoost). Additionally, results for the combination of all cities are shown. This way, it is easy to analyze the results considering several aspects and the following conclusions were drawn. The computation of the tests has been made as follows. In relation to the Friedman test, the ranks have been computed. For all the calculations, the α value is set to 0.05. Attending to those Ri values, χ2F=14.5, FF=29.0, and the critical value F(k−1,(k−1)(N−1))=3.26. Given that FF>F(4,12), the null-hypothesis is rejected, that is, not all the classifiers have similar performance, and post-hoc tests can be conducted.
For character embeddings, characters are embedded in 8-dimensional embeddings. The convolutions have window sizes of 3, 4, and 5 characters, each consisting of 50 filters. The sequential token modeling has the depth of 2 in all architectures. The hidden layers in the RNN architecture are of size 300, while the number of heads in the multi-head attention and number of blocks in the bi-block multi-dimensional attention are set to 2 and 3, respectively. The feed-forward networks in slot filling and intent classification both consist of two layers of size 300. Dropout keep probability is set to 0.9 in all the cases except for the residual dropout in multi-head attention where it is set to 0.8. αi,αs and the label smoothing rate are tuned on the development set resulting in αi=0.2,αs=0.8 and label smoothing rate =0.1. We train our models using Adam optimizer with 0.001 as the learning rate. In line with previously reported results, we do not use external knowledge (gazetteers) and average the scores of 5 runs for each experiment.
To evaluate the gain from transferring weights, we created a monolingual baseline , i.e. we trained a model solely on the German ATIS data. To investigate whether performance is reasonable on German, we additionally trained a model on the parallel English data (i.e. the subset which was translated). With an F1 of 90.5 vs 89.6 and an intent accuracy of 88.4 vs 87.8 for English vs German, respectively, performance appears to be reasonable.
We did not include the results of the 2-channel ConvNet because of its worse performance at the task of cross-family cognate identification.
Next, we measure the quality of our models by evaluating their performance on their respective test sets. The uniform loss proposed by stern-arxiv-2019 serves as a strong baseline for both language pairs, coming within 0.6 points of the original Transformer for En-De at 26.72 BLEU, and attaining a respectable score of 33.1 BLEU on En-Zh. We note that there is a slightly larger gap between the normal Transformer and the Insertion Transformer for the latter of 2.7 points, which we hypothesize is a result of the larger discrepancy between word orders in the two languages combined with the more difficult nature of the Insertion Transformer training objective.
Higher values of AdverSuc and machine-vs-random are better. What first stands out is decoding using sampling (as discussed in Section 4.3), achieving a significantly higher AdverSuc number than all the rest models. However, this does not indicate the superiority of the sampling decoding model, since the machine-vs-random accuracy is at the same time significantly lower. This means that sampled responses based on Seq2Seq models are not only hard for an evaluator to distinguish from real human responses, but also from randomly sampled responses. A similar, though much less extreme, effect is observed for MMI−p(t), which has an AdverSuc value slightly higher than Adver-Reinforce, but a significantly lower machine-vs-random score.
For human evaluation, we follow protocols defined in \newciteli2016deep, employing crowdsourced judges to evaluate a random sample of 200 items. We present both an input message and the generated outputs to 3 judges and ask them to decide which of the two outputs is better (single-turn general quality). Ties are permitted. Identical strings are assigned the same score. We also present the judges with multi-turn conversations simulated between the two agents. Each conversation consists of 3 turns. We observe a significant quality improvement on both single-turn quality and multi-turn quality from the proposed adversarial model. It is worth noting that the reinforcement learning system described in \newciteli2016deep, which simulates conversations between two bots and is trained based on manually designed reward functions, only improves multi-turn dialogue quality, while the model described in this paper improves both single-turn and multi-turn dialogue generation quality. This confirms that the reward adopted in adversarial training is more general, natural and effective in training dialogue systems.
At the caption level, SPICE achieves a rank correlation coefficient of 0.45 with Flickr 8K human scores, compared to 0.44 for CIDEr and 0.42 for METEOR. Relative to the correlation between human scores of 0.73, this represents only a modest improvement over existing metrics. Results are similar on the composite dataset, with SPICE achieving a rank correlation coefficient of 0.39, compared to 0.36 for CIDEr and 0.35 for METEOR. As this dataset only includes one score per image-caption pair, inter-human agreement cannot be established.
However, of the four kinds of captions pairs, SPICE performs best in terms of distinguishing between two model-generated captions (MM pairs) as illustrated in This is important as distinguishing better performing algorithms is the primary motivation for this work.
Surprisingly, we observed performance drops by DistMult-tanh-WV-init. We suspect that this is because word vectors are not appropriate for modeling entities described by non-compositional phrases (more than 73% of the entities in FB15k-401 are person names, locations, organizations and films). The promising performance of DistMult-tanh-EV-init suggests that the embedding model can greatly benefit from pre-trained entity-level vectors using external textual resources.
We applied the idea of maximum random effects in our model by using random slopes for each random effect to account for different reactions of subjects and for different effects for items with regard to experimental conditions First, interpreting individual intercepts as average hBLEU values without NMT adaptation, we observe that both individual intercepts for sentences and post-editors differ from the global intercept, which is the average value of 47.19 found in measurements in the offline learning scenario. The high variance within individual intercepts of each random effect indicates varying difficulty of sentences to be translated and diverse preferences and experience of post-editors. Second, the individual slope values, interpreted as hBLEU improvements due to online NMT adaptation, make evident how much influence the effect of NMT adaptation has on each sentence and for each post-editor. In comparison to a global slope of 6.73, individually estimated random effect slopes vary significantly. This shows that different sentences are harder to improve and different post-editors react differently to manipulations to the NMT system.
Our method using QANet and bidirectional ILP optimization is significantly better (15.4 F_{1} points) than the baseline. Our method using multilingual BERT and symmetrization is significantly better (12.8 F_{1} points) than that using QANet.
We report standard ranking and retrieval metrics including area under the curve (AUC), mean average precision (MAP), mean reciprocal rank (MRR), and precision at 1 (P@1). The results highlight the ability of our methods to obtain high interpretability while retaining ranking performance comparable to strong attention baselines. For example, our model is able to use only 6 aligned pairs to achieve a P@1 of 96.6 on the MultiNews dataset. In comparison, the sparse attention model obtains a P@1 of 97.1 but uses more than 300 alignment pairs and is thus difficult to interpret.
The size of the datasets and the number of their vocabularies are shown in Tab. Besides, there are 3172 distinct words in the vocabulary of Ω. As shown in Tab. The large vocabulary of ST obfuscates the impact and cause of vocabulary reliance on such training data. Therefore we generate new training data for study by constraining the vocabulary.
Entity linking. The set of seed nodes is given by the union of image and caption nodes. In Step 1 we link object labels and entity mentions to article nodes (String2Article). However, the same procedure could have been applied to category names as well (String2Category). We first compare these two methods to entity links produced by TagMe, a state-of-the-art system Ferragina and Scaiella Furthermore, we use the retrieval index of texts associated with nodes and output the top ranked node (Wikipedia index). We find that all methods perform reasonably well, where the category-based linking strategy cannot associate a vast majority of 581 objects / mentions. In particular, we find that our heuristics in Step 1 outperforms TagMe and is better in precision than retrieving from the Wikipedia index.
Benefits of graph expansion. We first investigate whether good gist nodes are found in close proximity to the depicted and mentioned seed nodes. To this end, we distinguish proximity in the three expansion layers of seed, intermediate, and top-k border nodes ( Step 1, 2, and 4b, respectively), and evaluate the benefits of each graph expansion by studying how precision and recall change with respect to the selection of relevant gist nodes for each expansion step. In order to judge the significance of improvement for F1 we evaluate the relative increase in precision, on a per-image-caption-pair basis, and report the average (denoted Δ). Significance is verified with a paired-t-test with level 0.05.
From the table, we can observe that C-SGCN outperforms all the feature-based and sequence-based models by a noticeable margin. Furthermore, compared to graph-based models, C-SGCN outperforms SDP-LSTM Xu et al. Tai et al. , C-GCN Zhang et al. S-GCN Wu et al. However, C-SGCN’s performance is same as C-AGGCN Guo et al. in terms of F1 score. It is worth mentioning that all of these works (except C-SGCN) employ a dependency parser to build the graph for the text, which is further encoded using a graph neural network. A dependency parsing requires external tool which is computationally expensive and time consuming. In addition to this, our model C-SGCN performance is 0.5 points higher than C-SGCN-Softmax in terms of F1 score, verified the claim that use of ReLU activation function for computing edge weight is more appropriate for GCN framework.
From the table, we can observe that all the components employed in C-SGCN models have a noticeable contribution to the overall all performance. In particular, the performance of No_SGCN is 1.5 points lower than the C-SGCN in terms of F1 score, demonstrating the strong contribution of SGCN.
, it is often difficult to judge news directly as absolutely true and false. Besides, it is also not conducive to subsequent operations (e.g., final verification). Carrying out finer granularity multi-classification tasks according to the credibility of news is very meaningful. From a more general point of view, this also shows that HGAT has a stronger learning ability in the heterogeneous network, and the learned representation is also more comprehensive and discriminative. The results demonstrate the great potential and scalability of HGAT in the face of other scenarios based in heterogeneous networks.
In the final set of experiments, we apply model adaptation to the bottleneck extractors and acoustic models using the available dysarthric training Dutch data. The best result is obtained using the FCNN trained using GFB+realAFs with a WER of 10.3%. Using GFB+synAFs yields slightly worse results compared to the GFB+realAFs with a WER of 10.6%.
When the model adaptation applied to systems using bottleneck features, we investigate the impact of adaptation on the bottleneck extractors as well as the acoustic models. Bottleneck extractor adaptation improves the performance marginally, the TFCNN system providing the best result reported on this test data with a WER of 10.0% which is the best ASR results reported on this dataset. The system providing the best performance used the normal training speech for both the bottleneck extractor and the acoustic model training followed by the model adaptation using the dysarthric training data. Moreover, applying model adaptation to fCNN acoustic models also brings improvements with a WER of 10.3% and 10.6% by incorporating AFs from AF extractors trained using real and synthetic speech respectively.
It should be clear that our dataset contains a large number of human motions and annotations thereof in natural language. Furthermore, our dataset is diverse in the sense that it contains a wide variety of different motions that were performed by a large number of different subjects. Similarly, the annotations were written by a large number of volunteers, which ensures that the dataset contains a diverse set of annotations. Our resulting dataset has a total size of 8.08 GiB. The compressed ZIP archive, which we make available online, is 3.88 GiB large.
This pattern of correlation between perplexity score and the subjective quality of the annotation continuous beyond the few samples listed. Notice that the annotations look decent at first but worsen with increasing perplexity. This creates a heatmap that, similar to a histogram, shows in which perplexity range the annotations that contain a given keyword lie. Since some keywords occur much more frequently than others, we normalize by dividing the number of occurrences in each perplexity range by the total number of occurrences. It is immediately apparent that annotations that describe walking motion mostly have low perplexity. On the other hand, annotations that contain the keyword “dance” and especially “waltz” have much higher perplexity. This again corresponds to the fact that the motion data contains much less motion recordings of people dancing than walking.
The strongest baseline in the single-turn task is GloVe, but PR-Embedding outperforms the baseline by 4.4%. For the multi-turn task, we concatenate PR-Embeddings with the original embedding layer of the model. We find that the performance becomes much better when we concatenate PR-Embedding with the randomly initialized embedding. The model KVMemnn becomes much stronger when the embedding layer initializes with the embeddings from GloVe. However, PR-Embedding still improves the performance significantly.
Our method (PR-Emb) significantly exceeds all the baselines in all metrics. The improvement is greater than the results on the English dataset as the training corpus is much larger. Note that, all the improvements on both datasets are statistically significant (p-value ≤0.01). We conduct the ablations on Chinese datasets in consideration of its larger training corpus. When we change the two vector spaces into the single one (w/o PR), the model is similar to GloVe with sentence-level learning. The performance becomes much worse in all the metrics, which shows the effect of two vector spaces. Furthermore, all the scores drop significantly after sentence-level learning is removed (w/o SLL), which shows its necessity.
We also conduct human evaluation to ensure robustness of our training procedure. We measure relevance and readability of the summaries. Relevance is based on the summary containing important, salient information from the input article, being correct by avoiding contradictory/unrelated information, and avoiding repeated/redundant information. Readability is based on the summary’s fluency, grammaticality, and coherence. To evaluate both these criteria, we design the following Amazon MTurk experiment: we randomly select 100 samples from the CNN/DM test set and ask the human testers (3 for each sample) to rank between summaries (for relevance and readability) produced by our model and that of See et al. (the models were anonymized and randomly shuffled), i.e. A is better, B is better, both are equally good/bad. Following previous work, the input article and ground truth summaries are also shown to the human participants in addition to the two model summaries. w.r.t. See et al.
Our full model is composed of a extremely fast extractor and a parallelizable abstractor, where the computation bottleneck is on the abstractor, which has to generate summaries with a large vocabulary from scratch. The main advantage of our abstractor at decoding time is that we can first compute all the extracted sentences for the document, and then abstract every sentence concurrently (in parallel) to generate the overall summary. We calculate the total decoding time for producing all summaries for the test set. Due to the fact that the main test-time speed bottleneck of RNN language generation model is that the model is constrained to generate one word at a time, the total decoding time is dependent on the number of total words generated; we hence also report the decoded words per second for a fair comparison. Our model without reranking is extremely fast. Even after adding the (optional) reranker, we still maintain a 6-7x speed-up (and hence a user can choose to use the reranking component depending on their downstream application’s speed requirements).
We compute an abstractiveness score See et al. A potential reason for this is that when trained with individual sentence-pairs, the abstractor learns to drop more document words so as to write individual summary sentences as concise as human-written ones; thus the improvement in multi-gram novelty.
SMALL data only includes the best output of the beam search and has 1.1M sentence pairs. MODERATE data contains top-k beams obtained with the beam search and has 16M sentences pairs. LARGE is the largest data, which also comprises original GEC pairs and TTS normalized data in addition to all data in MODERATE. LARGE has 18.6M sentence pairs. To demonstrate the difference between the GEC task and APR task, we also used the original GEC pairs as the training data denoted as ORIGIN. That means that including top-k beams indeed helped the APR task. However, by only using SMALL data, we still got a comparable result. The remaining data (≈15M) yielded a 1.69 increase on BLEU. This result proves our assumption that the top-k beams obtained with the beam search are homogeneous, which is not very beneficial for the model to learn new patterns from the data. Given these results and efficient usage of computational resources, we used SMALL data in the remainder of our experiments. It is interesting that the LARGE data got a lower score than the SMALL data. We think the cause is the original GEC pairs, and TTS normalized data having different patterns with the ASR output data. The MASS model trained on GEC pairs only got 67.28 BLEU, which is much lower than any dataset with ASR output as the source. It shows that the GEC task is different from the APR task, and APR is a new task that deserves a dedicated research effort.
BLTSM training and evaluation were repeated twice, with different random initialization. To keep tables more readable we only report the mean, the std. dev. is always lesser than 0.01. Results suggest that phonological features (phone labels, LFs and SFs) can outperform MFCCs, and, surprisingly, MFCCs slightly improve reconstruction when combined with phonological features, despite MFCCs containing much more detailed information than phonological features. LFs and SFs do not produce relevant improvement w.r.t. phone labels.
The most striking result is that MFCCs not only perform significantly worse than SFs but even worsen SFs performance when combined with them. This is due to the strong speaker dependency of MFCCs (despite their per-speaker normalization), that may be alleviated through speaker adaptation.
As emphasized in many studies, preprocessing of the text can ease the learning of the methods and improve accuracy uysal2014impact. Therefore the following steps are applied: removal of non-alphabetic characters, removal of stop-words and lowercasing. While it was often highlighted that word lemmatization and stemming improve results, initial experiments showed it was not the case for our study. This is probably due to the technical vocabulary used in both corpora pertaining to the field of meteorology. Already limited in size, the aforementioned preprocessing operations do not yield a significant vocabulary size reduction and can even lead to a loss of linguistic meaning. Finally, extremely frequent or rare words may not have high explanatory power and may reduce the different models’ accuracy. That is why words appearing less than 7 times or in more than 40% of the (learning) corpus are removed as well. Note that the preprocessing steps do not heavily rely on the considered language: therefore our pipeline is easily adaptable for other languages.
We observe that there is an insignificant change in the optimal dimensionality and the corresponding PIP loss values from basic to the infused corpus. This demonstrates that the semantic infusion technique keeps the word relations and their quality intact while associating meta-data with the corpus text. Thus, this suggests that the semantic infusion technique is a near-lossless in nature.
Moses has the worst performance. It obtains a fair BLEU(O, R) score that is 28.28. But its BLEU(O, I) score is 99.62, indicating that Moses fails to simplify most of the sentences. As this failure, its FK, iBLEU and SARI scores are all quite low. SBMT has the similar performance like Moses and neither simplify the output sentences nor promote the readability. The overall results of the Seq2Seq system are better than Moses and SBMT. Though its BLEU(O, R) score is little lower than Moses and SBMT, its output sentences are not mostly identical to the input sentences as its BLEU(O , I) score is only 66.94. It also achieves better FK(12.74), iBLEU(16.27) and SARI(33.16) scores than Moses and SBMT. Lexical Substitution only substitutes the complex words so that it obtains the highest BLEU(O, R) and SARI scores. But it gets the worst FK readability. In general, both Constrained Seq2Seq and Multi-Constrained Seq2Seq under our proposed framework outperform baselines. They have higher similarities to the reference and lower similarities to the input than other systems. So the iBLEU scores of our two systems are higher than baselines, which are 20.26 and 19.87 respectively. The SARI score of our two systems is also pretty high. As for FK readability, our two systems achieve the best result.
Moses generates 116 sentences that are completely identical to the input sentences. As the Grammaticality of the identical sentences are rated with 4 points, the Meaning with 4 points and the Simplicity with 0 points, Moses gets the highest score (3.99) both in Grammaticality and Meaning but obtains the lowest score (0.02) in Simplicity. Similar to Moses, SBMT generates 99 sentences that are not really simplified so that SBMT obtains similar results like Moses. Seq2Seq outperforms Moses and SBMT systems judged by the overall performance and obtains 3.28 in Grammaticality, 3.45 in Meaning and 0.96 in Simplicity. The results of Lexical Substitution in Meaning and Simplicity are rather high. But as shown by the score of Grammaticality, the sentences generated by Lexical Substitution contain many grammar errors which are not surprising. Our Constrained Seq2Seq and Multi-Constrained Seq2Seq outperform in Simplicity than baselines. The Meaning scores of our systems are 2.81 and 2.65. Simple English Wikipedia has a quite similar score, 2.83, which indicates that to some extent, both our systems and Simple English Wikipedia have a semantic loss when simplifying sentences. As for Grammaticality, Constrained Seq2Seq is better than Lexical Substitution. Multi-Constrained Seq2Seq performs worse than Constrained Seq2Seq in Grammaticality but better in Simplicity.
For author-specific fine-tuning of StyleLM and GPT-2 (FT), we use Shakespeare’s corpus but without exploiting its parallel nature with modern English corpus. The results for stylized rewriting of the test corpus to the various author To reiterate, the objective is to rewrite the above test corpora into a style that reflects the style of target author we fine-tuned for. However, the content preservation for LM + DAE is better than that of GPT-2 (FT). The vanilla GPT-2, however, shows the least impressive in terms of both content preservation as well stylistic alignment. Specifically, the poor performance on content preservation can be attributed to the fact that GPT-2 and GPT-2 (FT) are both trained for generating continuations of input prompts and not for the task of stylistic rewriting. It is nonetheless encouraging to see that fine-tuning the GPT-2 language model on author-specific corpus, i.e., GPT-2 (FT), increases the extent of stylistic alignment with target author ’s style, establishing GPT-2 (FT) as a competitive baseline to compare stylistic alignment against. the supervised approach proposed by \citeauthorjhamtani2017shakespearizing (\citeyearjhamtani2017shakespearizing). We compare their LSTM-based encoder-decoder approach with GPT-2, GPT-2 (FT), LM + DAE and StyleLM after fine-tuning them on Shakespeare’s corpus. Given that StyleLM was trained without leveraging the parallel nature of the data, the results are promising and demonstrate the abilities of our proposed model in generating author-stylized text while preserving the original content.
As can be seen, pretrained BERT-GPT works better than unpretrained Transformer. Though pretrained, DialoGPT is not as good as Transformer. The possible reason is the training corpora of DialoGPT is daily dialogues, which has a large domain shift from medical dialogues. The performance gap between BERT-GPT and Groundtruth is larger than that between BART and Groundtruth, despite the number of Chinese training dialogues is larger than that of English training dialogues. This indicates that it is more challenging to develop COVID-19 dialogue systems on Chinese. One major reason is the Chinese dialogues are more noisy than the English ones, with a lot of incorrect grammars, abbreviations, semantic ambiguities, etc.
The model is trained with Adam algorithm kingma2014adam. The development set is used for parameter tuning. We use the CRF as the default decoder since it is slightly better than MLP (see Sec. All models are trained for 100 epochs, after each training epoch, we test the model on the dev set, and models with the highest F1 in dev set are tested in the test set and we report its outcomes.
We first compare our Transformer encoder with BiLSTM, Stacked Bi-LSTM and Switch-LSTMs from chen2017adversarial; gong2018switch in the single-criterion learning scenario. As we can see, Transformer outpaces BiLSTMs, Stacked Bi-LSTM, and Switch-LSTMs both in F1 value and OOV. Quantitatively speaking, Transformer obtains 96.47 in average F1 value, while the previous state-of-the-art result is 94.76 gong2018switch, the absolute increasement is 1.71. We argue that the capability of the Transformer is greater than conventional LSTM models, and this view can be consolidated by the 7.32 OOV improvement. In the multi-criteria learning scenario, we compare Transformer with the multi-task learning framework (MTL) chen2017adversarial and Switch-LSTMs gong2018switch. Firstly, although different criteria are trained together, most datasets (besides CTB) achieve better performance with respect to F1 value. Compared to the single-criterion scenario, 0.4 gain in average F1 value is obtained by multi-criteria scenario, which indicates the proposed criterion embedding is useful. Secondly, compare with previous multi-criteria learning models, the model proposed in this paper also achieves better average F1 value. This is a sign that the proposed Transformer-based multi-criteria CWS makes better use of different criteria datasets.
Although in the setting “5Simp, 3Trad”, AS, CKIP and CITYU are in traditional Chinese, they also benefit from this multi-criteria scenario, since their performances are similar to “8Simp” and “8Trad”.
4.2.2 Discussion In general, we can observe that QRNNs with DReLUs or DELUs outperform a QRNN with a tanh activation function applied in the candidate cell state ~ct. More interestingly, a QRNN using a single ReLU activation function performs much worse. The perplexity gap between a QRNN with a single and a dual ReLU activation function is 7 perplexity points. Consequently, ReLUs are inferior replacements for tanh-based units but DReLUs can successfully replace them and even outperform them.
the results of several experiments on the PTB dataset are listed. The first part shows the results of a number of successful and state-of-the-art character-level language modeling methods, including a vanilla two-layer LSTM, while the second part shows our results for QRNNs trained with several activation functions.
To further compare SDE’s ability to generalize to different languages, we train both SDE and sub-sep on the low-resource languages paired with all four high-resource languages. For bel, SDE trained on all languages is able to improve over just training with bilingual data by around 0.6 BLEU. This is the best result on bel, with around 3 BLEU over the best baseline. The performance of sub-sep, on the other hand, decreases by around 1.5 BLEU when training on all languages for bel. The performance of both methods decreases for aze when using all languages. SDE only slightly loses 0.1 BLEU while sub-sep loses over 3 BLEU.
Annealing does better than the majority of settings of static focusing, though is worse than optimally-tuned focusing. We do not observe a significant difference between the two annealing schedules. When combining softlink and voclink, the patterns are similar to that of softlink only.
Annealing does better than the majority of settings of static focusing, though is worse than optimally-tuned focusing. We do not observe a significant difference between the two annealing schedules. When combining softlink and voclink, the patterns are similar to that of softlink only.
This is mainly because the BERT model can obtain general language knowledge from pre-training, and then our annotated data can be used to fine-tune the model to extract the causal relation. Rule-based approach can achieve better precision score but worse recall score, because manually constructed rules can hardly cover the whole linguistic phenomenons.
Another interesting line of work is the utilisation of sub-word units such as BPE (CL2016179). We perform BPE on the input data and measure the BLEU score after merging the sub-word units on the target side.
A secondary benefit of the lattice rescoring setup is that one can evaluate the ASR performance of much larger language models. In our experience that consistently yields small gains in accuracy.
It also outperforms the popular multi-task learning method (Pred-rationales) in all tasks, even though λrationale has been tuned much more thoroughly than λattn. Annual Meeting of the Association for Computational Linguistics.
For GFBF models, trained attention weights improve significantly for both explanation faithfulness and plausibility, evaluated by probes/mass-needed. If the attention is untrained, on average we need to probe 8 positions (probes-needed) to find the “most important word”, the word that the model relies on the most (faithful explanation), or the rationale word (plausibility). However, adding just 100 rationales for training causes a drastic reduction in probes-needed to around 2, almost as high as training with all rationales. An important observation is that a random baseline is expected to have a mass needed of 0.5, which the non-trained attention system significantly exceeds. This suggests that it develops an attention mechanism that actually systematically fails to attend to the correct position.
We summarize our observations as follows: (1) On MSDialog, all three variations of IART with dot, outer product and bilinear based intent-aware attention mechanism show significant improvements over all baseline methods, including the recently proposed strong baseline method DAM. On UDC, IART with three different intent-aware attention mechanisms also show improvements under all metrics except for R10@5. With the comparison between the results of DAM and IART, we can find that incorporating user intent modeling and intent-aware attention weighting scheme can help improve the response ranking performance. (2) If we compare three variations of IART, we can find that the bilinear based intent-aware attention mechanism works better for MSDialog and outer product based intent-aware attention mechanism works better for UDC. The overall performances of these three model variations are close to each other. Overall our proposed model IART shows larger performance improvements on MSDialog. One possible reason is that the intent classifier on MSDialog is more accurate due to the larger annotated training data of MSDialog for user intent prediction and more formal language used in MSDialog, as shown in evaluation results by DBLP: journals/corr/abs-1901-03489. (3) On AliMe data, all three variations of IART also show comparable or better results than all baseline methods including the strong baseline DAM. These results on real product data further verify the effectiveness of our proposed methods.
We control all the models in this section to have 4 layers of 256 bidirectional LSTM cells in the encoder, with weight noise. We perform random search over pooling in the encoder, whether to use a convolutional front-end, data augmentation, weight noise and optimization hyper-parameters. This search over hyper-parameter space has allowed us to match previously published results. We therefore believe that this provides a good baseline to explore the trade-offs in modeling choices. Also, since the attention models are quite a bit better than RNN-Transducer models, the full attention over all encoder time steps seems to be valuable.
For each word, we assign each dimension a score depending on the number of annotators who found it relevant and select the top five (we call these ground truth dimensions). We now compare this with the top 5 dimensions obtained using POLAR. This conditional probability essentially measures, given the annotator has selected top k dimensions, what is the probability that they are also the ones selected by polar or simply put, in what fraction of cases the top k dimensions overlap with the Polar dimensions. In the same table we also note the random chance probabilities of the ground truth dimensions to be among the POLAR dimensions (e.g., the top dimension (k=1) selected by the annotators, has a random chance probability of 0.5 to be also among the POLAR dimensions). We observe that probabilities for POLAR to be much higher than random chance for all values of k In fact, the top two dimensions selected by POLAR are very much aligned to human judgement, achieving a high overlap of 0.87 and 0.67. On the other hand, the remaining 3 dimensions, although much better than random chance, do not reflect human judgement well. To delve deeper into it, we compared the responses of the 3 annotators for each word and obtained the average overlap in dimensions among them. We observed that on average the annotators agree mostly on 2-3 (mean = 2.4) dimensions (which also match with the ones selected by POLAR) but tend to differ for the rest. This goes to show that once we move out of the top 2-3 dimensions, human judgement becomes very subjective and hence difficult for any model to match. We interpret this as POLAR being able to capture the most important dimensions well, but unable to match more subjective dimensions in many scenarios.
We begin with supervised classification, re-establishing the high accuracy of in-domain (supervised) classification of translationese, but highlighting the deterioration in accuracy when cross-domain classification is considered. We first reproduce the Europarl classification results with the best performing feature sets, as reported by Volansky et al. All features (except perhaps cohesive markers) yield excellent accuracy. Function words systematically yield very high accuracy; the quality of clustering with other features varies across the sub-corpora. Cohesive markers perform poorly (with a single exception, Hansard), which mirrors the moderate supervised classification precision achieved with the same feature set.
We begin with an investigation of the mutual effect of the domain- and translationese-specific characteristics on the accuracy of clustering. We first merged equal numbers of O and T chunks from two corpora: 800 chunks each from Europarl and Hansard, yielding 1,600 chunks, half of them O and half T. In other words, we obtained two clusters, one consisting of Europarl chunks and the other of Hansard chunks, independently of their O-vs.-T status. We repeated the experiment with additional corpus pairs, and further extended it by adding equal numbers of Literature chunks (400 O and 400 T), this time fixing the number of clusters to three. As an additional experiment, we attempted to leave the decision on the “best” number of clusters to the algorithm. We also applied PCA for dimension reduction prior to XMeans invocation. We repeated both experiments (two- and three-domain mixes) with XMeans, expecting to obtain two and three clusters, respectively. These observations have a crucial effect on understanding the tension between the domain- and translationese-based characteristics of the underlying texts. Not only are domains accurately separated given a fixed number of clusters, but even when the decision on the number of clusters is left to the clustering procedure, classification into domains explains the data best (as shown by XMeans). Recall that these experiments all rely on the set of function words: topic-independent features , that have been proven effective for telling O from T in both
The following handcrafted features were used for the model: Bias feature Token feature Uppercase feature (y/n) Titlecase feature (y/n) Character trigram feature Quotation feature (y/n) Word suffix feature (last three characters) POS tag (provided by spaCy utilities) Word shape (provided by spaCy utilities) Word embedding (see The model was tuned on the development set doing grid search; the hyperparameters considered were c1 (L1 regularization coefficient: 0.01, 0.05, 0.1, 0.5, 1.0), c2 ( The best results were obtained with c1 = 0.05, c2 = 0.01, scaling = 0.5 and word2vec Spanish embeddings by \newcitecardellinoSBWCE. The threshold for the stopping criterion delta was selected through observing the loss during preliminary experiments (delta = 1e−3).
In order to assess the significance of the the handcrafted features, a feature ablation study was done on the tuned model, ablating one feature at a time and testing on the development set. Due to the scarcity of spans labeled with the OTHER tag on the development set (only 14) and given that the main purpose of the model is to detect anglicisms, the baseline model was run ignoring the OTHER tag both during tuning and the feature ablation experiments. The results show that all features proposed for the baseline model contribute to the results, with the character trigram feature being the one that has the biggest impact on the feature ablation study.
The baseline model was then run on the test set and the supplemental test set with the set of features and hyperparameters mentioned on The model was run both with and without the OTHER tag. The metrics for ENG display the results obtained only for the spans labeled as anglicisms; the metrics for OTHER display the results obtained for any borrowing other than anglicisms. The metrics for BORROWING discard the type of label and consider correct any labeled span that has correct boundaries, regardless of the label type (so any type of borrowing, regardless if it is ENG or OTHER). In all cases, only full matches were considered correct and no credit was given to partial matching, i.e. if only fake in fake news was retrieved, it was considered wrong and no partial score was given. Secondly, there was no Spanish tag assigned to non-borrowings, that means that no credit was given if a Spanish token was identified as such.
Filtering of the OpenSubtitles corpus is motivated by the fact that by removing the video and audio modalities which the subtitles originally accompanied, we are very often left with incomplete and incoherent dialogues. Therefore, by keeping dialogues with high coherence scores, we aim at building a high quality corpus with (1) more semantically coherent and topically related contexts and responses, and (2) fewer general and dull responses. Unsurprisingly, coherence for fOST is much higher than OST, with a slightly higher diversity.
We also broke down the results by type of relation, classifying each relationship according to the cardinality of their head and tail arguments. A relationship is considered as 1-to-1, 1-to-M, M-to-1 or M-M regarding the variety of arguments head given a tail and vice versa. If the average number of different heads for the whole set of unique pairs (label, tail) given a relationship is below 1.5 we have considered it as 1, and the same in the other way around. The number of relations classified as 1-to-1, 1-to-M, M-to-1 and M-M is 353, 305, 380 and 307, respectively. Bigrams and Trigram models cooperate in a constructive way for all the types of relationship when predicting both the head and tail. Tatec-ft is remarkably better for M-to-M relationships.
It is clear that KVMemNet achieves better P@1 scores on both dev and test sets than PCNet. The reason is that candidate answers of PCNet come from the “anchor” point along 1-hop or 2-hop paths. However, the correct answer might not be connected due to the quality of anchor detection. On the dev set, we observe that only 69.6% of correct answers can be covered by the set of candidate answers in PCNet, which apparently limits the upper bound of the approach. This is addressed in KVMemNet because all the arguments are candidate answers. Both PCNet and KVMemNet outperform our implementation of \citeauthorbordes2014question \shortcitebordes2014question, since the latter ignores word order. We incorporate each of the four KBs separately into PCNet and KVMemNet, and find that incorporating external KBs could bring improvements.
Our approach is abbreviated as QGNet, which stands for the use of a paraphrasing model plus our question generation model. We can see that QGNet performs better than Seq2Seq in terms of BLEU score because many important words of low frequency from the input are replicated to the target sequence. However, the improvement is not significant for the QA task. We also incorporate each of the four KBs into QGNet, and observe slight improvements on NELL and Reverb. Despite the overall accuracy of QGNet being lower than PCNet and KVMemNet, combining outcomes with them could generates 1.5% and 0.8% absolute gains, respectively.
Looking only at the overall latency, the difference in latency between Update and Update-NA appears small. However, in practice we noticed that Update-NA has a much higher larger peak latency. Portion required up to 23 seconds to identify a stable hypothesis while the worst word for Update was displayed after only 9 seconds. Update-NA has a similar overall latency as Update, but its peak latency is much worse. The results of Baseline-2 emphasises the need for word latency measurements.
The case with no features corresponds to using a single gamma distribution for all outages. As more features are added, the model achieves a better negative log likelihood, i.e. provides a better fit of the observed test data, lower RMSE, and higher correlation. The last two lines serve as oracle experiments, since they include the true cause of the outage as a feature, which is not usually known at the onset. As expected, knowing the true cause improves performance for all metrics. We hypothesized that the onset features would give us some information about the true cause, which seems to be the case. A classifier trained to predict the outage cause from the onset features has an accuracy of 70%. (Always predicting the majority class ‘Equipment Failure’ gives an accuracy of 44%.)
We analyze the baselines by evaluating each model’s MNLI classifier on the diagnostic set to get a better sense of their linguistic capabilities.
We use multi-class classification accuracy for evaluation since CLEVR-Dialog has one-word answers. Tab. The key observations are: (a) Neural models outperform random baselines by a large margin. The best performing model, CorefNMN, outperforms Random-Q by 35%. (b) As expected, blind models (LF-Q, LF-QH, HRE-QH, MN-QH) are inferior to their counterparts that use I, by at least 10%. (c) History-agnostic models (LF-Q, LF-QI, NMN) also suffer in performance, highlighting the importance of history.
dataset The training set consists of 209,772 sentence pairs from transcribed TED presentations that cover a wide variety of topics with more conversational language than in the Multi30k dataset. This dataset is larger, both in number of sentences and vocabulary, and was not seen during the architecture search. While all six architectures achieved higher validation and test BLEU on Multi30k than the LSTM baseline, it appears the architectures did not transfer cleanly to the larger IWSLT dataset. This suggests that architecture search should be either run on larger datasets to begin with (a computationally expensive proposition) or evaluated over multiple datasets if the aim is to produce general architectures. We also found that the correlation between loss and BLEU is far from optimal: architectures performing exceptionally well on the loss sometimes scored poorly on BLEU. For hyper parameters of the IWSLT model, refer to Appendix C3.
The Star Trek media franchise canon boasts eight television series to date. The episodes from the series TOS, TAS, TNG, and Voyager are used to test the various RSs proposed in this paper. LTO is engineered to fit within the Basic Formal Ontology (BFO) top level ontology class hierarchy Arp et al. LTO is meant to cover important, operationally verifiable literary themes that can be expected reoccur in multiple works of fiction Sheridan et al. In designing LTO, we strove to make sibling classes mutually exclusive, but not necessarily jointly exhaustive. All literary themes are accompanied with definitions, and references when possible. We appealed to the principle of falsifiability in definition writing. That is to say, a well-defined literary theme will be such that it is possible to appeal to the definition to show it is not featured in a story. Take “the quest for immortality” as an example, which is defined as “A character is tempted by a perceived chance to live on well beyond what is considered to be a normal lifespan”. The theme “the desire for vengeance” (Definition: A character seeks retribution over a perceived injury or wrong.) constitutes another example. By insisting on maximally unambiguous theme definitions, we aim to help bring the conversation of whether a theme is featured in a given work of fiction into the realm of rational argumentation. However, we fully acknowledge that the identification of literary themes in stories will always carry with it a certain element of subjectivity. It is the goal of LTO to minimize the subjective element in theme identification. The individual classes populating LTO at the early stage of development presented in this paper were mainly collected by watching Star Trek TOS, TAS, TNG, and Voyager episodes and recording the themes. We selected Star Trek for building up the ontology on account that the television series are culturally significant and explore a broad range of literary themes relating to the human condition, societal issues, as well as classic science fiction. That said, the ontology is admittedly science fiction oriented. The later version of LTO (version 1.0.0) presented in Sheridan et al. Sheridan et al.
On clean recordings, Get_F0 provides the best results in terms of VDE and FFE on both genders, while the best GPE is obtained by the proposed method SRH for female voices, and by TEMPO for male speakers. Regarding its efficiency in terms of FPE, albeit having the slightly largest values, SRH has a performance sensibly comparable to the state-of-the-art, confirming its ability to also capture the pitch contour details. On noisy speech, SRH clearly outperforms all other approaches, especially for female speakers where the FFE is reduced of at least 8.5% (except for YIN which uses the proposed VAD from SRH). This gain is also substantial for male voices with regard to existing approaches (consequently leaving out of comparison the SSH and the modified YIN techniques), with a decrease of 5.3% of FFE, and of 5.7% regarding the errors on the voicing decisions. It is worth noting the remarkably good performance of SRH for female voices in noisy environments, providing very low values of VDE and GPE (and thus FFE). All methods (except SSH in adverse conditions) are also observed to give better results for female speakers than for male voices. Finally, it is interesting to emphasize that, while relying on the same voicing decisions, YIN leads in all conditions to a greater GPE than SRH, especially for noisy recordings. This confirms the quality of SRH both as a VAD and for pitch contour estimation.
Text-to-image generation: In Tab. , we compare our models with three state-of-the-art GANs in text-to-image generation. For visualization, we show in the top row of Fig. Higher-resolution images are shown in Appendix C.2. We also provide example results on COCO, a much more challenging dataset, in Fig. Ablation studies: We also consider several ablation studies for text-to-image generation, as shown in Tab. First, we modify StackGAN++ It is clear that PGBN+StackGAN++ outperforms the original StackGAN++, but underperforms VHE-StackGAN++, which can be explained by that 1) the PGBN deep topic model is more effective in extracting macro-level textual information, such as key words, than RNNs; and 2) jointly end-to-end training the textual feature extractor and image encoder, discriminator, and generator helps better capture and relate the visual and semantical concepts. Second, note that VHE-StackGAN++ has the same structured image generator as both StackGAN++ and HDGAN do, but performs better than them. We attribute its performance gain to 1) its PGBN deep topic model helps better capture key semantic information from the textual descriptions; and 2) it performs end-to-end joint image-text learning via the VHE-GAN framework, rather than separating the extraction of textual features from text-to-image generation. Third, VHE-vanilla-GAN underperforms VHE-StackGAN++, suggesting that the stacking structure is helpful for generating high resolution images, as previously verified in Zhang et al. VHE-simple-raster-scan-GAN outperforms VHE-StackGAN++ but underperforms VHE-raster-scan-GAN, confirming the benefits of combining the stacking and raster-scan structures. More visual results for ablation studies can be found in Appendix C.2. Below we focus on illustrating the outstanding performance of VHE-raster-scan-GAN.
Text-to-image generation: In Tab. , we compare our models with three state-of-the-art GANs in text-to-image generation. For visualization, we show in the top row of Fig. Higher-resolution images are shown in Appendix C.2. We also provide example results on COCO, a much more challenging dataset, in Fig. Ablation studies: We also consider several ablation studies for text-to-image generation, as shown in Tab. First, we modify StackGAN++ It is clear that PGBN+StackGAN++ outperforms the original StackGAN++, but underperforms VHE-StackGAN++, which can be explained by that 1) the PGBN deep topic model is more effective in extracting macro-level textual information, such as key words, than RNNs; and 2) jointly end-to-end training the textual feature extractor and image encoder, discriminator, and generator helps better capture and relate the visual and semantical concepts. Second, note that VHE-StackGAN++ has the same structured image generator as both StackGAN++ and HDGAN do, but performs better than them. We attribute its performance gain to 1) its PGBN deep topic model helps better capture key semantic information from the textual descriptions; and 2) it performs end-to-end joint image-text learning via the VHE-GAN framework, rather than separating the extraction of textual features from text-to-image generation. Third, VHE-vanilla-GAN underperforms VHE-StackGAN++, suggesting that the stacking structure is helpful for generating high resolution images, as previously verified in Zhang et al. VHE-simple-raster-scan-GAN outperforms VHE-StackGAN++ but underperforms VHE-raster-scan-GAN, confirming the benefits of combining the stacking and raster-scan structures. More visual results for ablation studies can be found in Appendix C.2. Below we focus on illustrating the outstanding performance of VHE-raster-scan-GAN.
Text-based ZSL: We follow the the same settings on CUB and Flower as existing text-based ZSL methods summarized in Tab. There are two default splits for CUB—the hard (CUB-H) and easy one (CUB-E)—and one split setting for Flower, as described in Appendix F. Note that except for our models that infer a shared semantically meaningful latent space between two modalities, none of the other methods have generative models for both modalities, regardless of whether they learn a classifier or a distance metric in a latent space for ZSL. Tab. Note for CUB-E, every unseen class has some corresponding seen classes under the same super-category, which makes the classification of surface or distance metric learned on the seen classes easier to generalize to the unseen ones. We also note that both GAZSL and ZSLPP rely on visual part detection to extract image features, making their performance sensitive to the quality of the visual part detector that often has to be elaborately tuned for different classes and hence limiting their generalization ability, for example, the visual part detector for birds is not suitable for flowers. Tab. This suggests 1) the advantage of a joint generation of two modalities, and 2) the ability of GAN in helping VHE achieve better data representation. The results in Tab. We also collect the ZSL results of the last 1000 mini-batch based stochastic gradient update iterations to calculate the error bars. For existing methods, since there are no error bars provided in published paper, we only provide the text error bars of the methods that have publicly accessible code. We test both VHE-StackGAN++ and VHE-raster-scan-GAN on the same image/text retrieval tasks as in TA-GAN Gibbs sampling) to compute the similarity scores. Similar with TA-GAN, the top-1 image-to-text retrieval accuracy (Top-1 Acc) and the percentage of matching images in top-50 text-to-image retrieval results (AP@50) on CUB-E dataset are used to measure the performance. Also, VHE-raster-scan-GAN outperforms VHE-StackGAN++, which further confirms the benefits of combining both the stacking and raster scan structures.
Cohen’s κ between those labels and the gold is 0.88. This is well above the agreement obtained in previously released datasets where crowd-sourcing was used (the agreement scores reported, in terms of percentage, range from 63.7% Derczynski et al. Despite its simple architecture, SiamNet obtains the best performance in terms of both averaged and weighted averaged F1 scores. In line with previous findings Mohammad et al. The relative gains in performance of CrossNet w.r.t. BiCE, and of HAN w.r.t. TAN, consistently reflect results obtained by such models on the SemEval 2016-Task 6 corpus Xu et al.
A quick comparison shows that Yelp and Formality obtain significantly lower undecidable and inter-annotator disagreement rates, indicating that the style of the sentences in these two datasets are less ambiguous to humans. In addition, Yelp and Formality have much higher F1 score than all the other datasets, which confirms the correctness of the source-target attribute split.
We observe that our scheme drastically improves over the current state-of-the-art models for this dataset. On both tasks of image to recipe and recipe to image we achieve a MedR (lower is better) of 1.0, while the best known results are of 5.2 and 5.1, respectively. A similar behavior can be observed when analyzing the recall metric (higher is better): we obtain 39.8 and 40.2 for R@1, while the best previous results were 24.0 and 25.0. The same conclusions can be extended to R@5 and R@10, for which our approach also overtakes the state-of-the-art.
We also see that our proposed multi-modal variation is the one that obtains the best performance, also being statistically significant at the highest level of confidence. We believe these results show that our proposed multi-modal architecture is not only able to exploit the features in the audio and video inputs, but it can also leverage the information in the pre-trained word embeddings and benefit from having an inductive bias that is tailored for the task at hand, in this case, with a loss based on structured prediction for sequence labeling.
As shown, the retrieval system, which our model ensemble is based on, achieves better performance than RNN-based sequence generation. After closely examining their paper, we find that their database is multiple times smaller than ours, which may, along with different features and retrieval methods, explain the phenomenon. This also verifies that the retrieval-based dialog system in our experiment is a strong baseline to compare with. We would also like to verify if the combination of biseq2seq and post-reranking mechanisms will yield further gain in our ensemble. To test this, we compare the full model Rerank(Retrieval,biseq2seq) with an ensemble that uses traditional seq2seq, i.e., Rerank(Retrieval,seq2seq). Likewise, Rerank(Retrieval,biseq2seq) outperforms both Retrieval and biseq2seq. These results are consistent in terms of all metrics except a BLEU-4 score.
And the statistic result also displays that biseq2seq generates longer sentences than seq2seq approach. We nevertheless report the result here out of curiosity: its entropy is 9.507, which is even higher than groundtruth.
Therefore it is worthwhile to use RNNLMs as a comparison. It achieves 9% and 8.8% relative WER reductions respectively over the baseline system obtained with the best semi-supervised training configuration.
To ensure that our results also generalize to other models, specifically ones that are tailored for better sharing of information across languages, we also test TCS on a slightly different multilingual NMT model using soft decoupled encoding (SDE; Wang et al. Overall the results are stronger, but the best TCS model outperforms the baseline by 0.5 BLEU for aze, and around 2 BLEU for the rest of the three languages, suggesting the orthogonality of data selection and better multilingual training methods.
On MedMentions, LATTE outperforms the baselines by a wide margin. On 3DNotes, paired t-tests indicate that LATTE outperforms the strongest baseline with confidence level of 90% (experimented with 5 different random seeds).
It also shows that the cross-attention mechanism is strong in capturing the semantic similarity between mention and candidates. Instead, if we add the known type supervision, there are 7.71% and 3.72% gains in Precision@1 with respect to the two datasets. This shows that multi-tasking with known type classification has strong positive effect on the entity linking task. Finally, adding latent type modeling along with know type classification further improves the Precision@1. This proves that the hierarchical type modeling improves the entity linking task.
The Spearman correlation is a rank-based correlation measure that assesses how well the scores describe the true labels.
We can see that RA-GCN achieves the best performance among all models above, which improves the best beseline 1.9% on F1-measure. Besides, we also have observations as follows:
To study the contribution of RA-GCN core components, we design ablation experiments. 1) –RAAM: To study whether syntactic label help to improve the performance of RA-GCN, we initialize each element of relation-aware adjacency tensor as the same representation, which means only syntactic dependency structure is exploited. The result drops nearly 2.1% on F1-measure, which demonstrates that syntactic dependency label can provide information to improve the performance of RA-GCN. 2) –MdR: To study whether multi-dimensional representation of relation help to enhance the ability to capturing information, we set the dimension of relation representation to be 1, which means the relation-aware adjacency tensor E∈Rn×n×p is compressed to be E∈Rn×n×1. We can see that F1-measure drops 4% approximately, which demonstrates that multi-dimensional representation can learn more information than just a scalar weight. 3) –CARUM: To study whether context-aware relation representation help to improve performance, we remove context-aware relation update module in RA-GCN. The performance degrades 2.4%, which illustrates that context-aware relation representation can provide more evident information for event detection. 4) –RAAM & CARUM: To study whether “relation” can help GCN to work better, we remove relation-aware aggregation module and context-aware relation update module simultaneously, which means only vallina GCN is used. We can see that the performance reduces by 2.8%, which illustrates that “relation” can help to capture information which vallina GCN can not capture. 5) –BiLSTM: BiLSTM is removed before RA-GCN and the performance drops terribly. This illustrates that BiLSTM can capture important sequential information which GCN miss. Therefore, GCN and BiLSTM can be complementary to each other for event detection task.
Specifically, the U branch performs slightly better than R, indicating the topic-unrelated features are more suitable for inter-topic detection. We infer that the two branches can learn good but different representation under the guide of the auxiliary task.
We evaluate on the SQuAD 1.1 dataset [squad] for question generation task (called SQuAD QG). Following UNILM, we redistribute the original dataset into a new training set and testing set with the original development set unchanged. We also conduct experiment with the reversed dev↔test split as [split] indicates. Again, ERNIE-GEN outperforms UNILMLARGE and achieves a new state-of-the-art result on question generation by giving +1.82 BLEU-4 scores.
For models using local contexts as input, the context window size of 7 (7 words preceding and following the target event mention, therefore, 15 words in total) yields the best result, as reported in our prior paper Huang et al. Note that dependency chains we generated have an average length of 7.4 words in total, which are much shorter than 15 words of local contexts as used before. Especially, the LSTM model running on dependency chains achieves the best performance of 70.0% Macro and 79.6% Micro F1-score, which outperforms the previous local context based CNN model Huang et al. Statistical significance testing shows that the improvements are significant at the p<0.01 level (t-test). In particular for on-going and future events, the dependency chain based LSTM model improves the temporal status classification F-scores by 4 and 10 percentages respectively. In addition, the tree-LSTM model taking account of full dependency trees achieves a comparable result with local context based neural network models, but performs worse than dependency chain based models. The reason why the tree-LSTM model does not work well is that irrelevant words, including adjective modifiers and irrelevant clauses forming branches of dependency trees, may distract the classifier and have negative effect in predicting the temporal status of an event.
BERT was able to get a small improvement from its inner cross passage attention which introduces some weak reasoning. Surprisingly, overall the context passage in the reasoning path does not inherently contribute to the performance of these methods, which indicates that the models are not learning much multi-hop reasoning as previously thought.
We observe that our model out-performs all the baseline models. Comparing to our model trained on Wikipedia, our model trained on the in-domain training set achieves higher accuracy on the test set.
S4SS3SSS0Px2 Effect of Encoder Parameters In our model, representations of the original sentence and its compressed version were learned by a shared encoder. To explore the effect of the encoder parameters, we also designed a BBFNMT with two independent encoders to learn representations of the original sentence and its compressed version, respectively.
Comparison between PoWER-BERT and ALBERT. Here PoWER-BERT represents application of our scheme on ALBERT. The experimental setup is same as in Table In the first experiment, we demonstrate the effectiveness of the word-vector elimination approach by evaluating the inference time gains achieved by PoWER-BERT over BERTBASE. We limit the accuracy loss to be within 1% by tuning the regularizer parameter λ that controls the trade-off between inference time and accuracy. We observe that PoWER-BERT offers at least 2.0x reduction in inference time on all the datasets and the improvement can be as high as 4.5x, as exhibited on the CoLA and the QQP datasets.
As discussed earlier, word-vector elimination scheme can be applied over compressed models as well. To demonstrate, we apply PoWER-BERT over ALBERT, one of the best known compression methods for BERT. We observe that the PoWER-BERT strategy is able to accelerate ALBERT inference by 2x factors on most of the datasets (with <1% loss in accuracy), with the gain being as high as 6.8x on the QQP dataset.
Visual feature ablation. We also want to demonstrate the difficulty of the dataset from the perspective of visual features, so we show MUTAN results using different ResNet architectures. The previously reported result for MUTAN is based on ResNet152. We also show the results using extracted features from ResNet50 and ResNet18 From this table it can be seen that going from ResNet50 to ResNet152 features only has a marginal improvement, and similarly going from ResNet18 to ResNet50. However, going from ResNet18 to no image (Q-Only) causes a large drop in performance. This suggests that our dataset is indeed visually grounded, but better image features do not hugely improve the results, suggesting the difficulty lies in the retrieving the relevant knowledge and reasoning required to answer the questions.
We chose the values of α and β individually for all the datasets, using grid search in the range {0.0,0.1,…,1.0}, based on the best validation \operatornamewithlimitsSOV score. For CAWA, DNN+A, and DNN-A, the number of nodes in the each of the hidden layer, all representations’ length, as well as the batch size for training the CAWA was set to 256. For regularization, we used a dropout [srivastava2014dropout] of 0.5 between all layers, except the output layer. For optimization, we used the ADAM [kingma2014adam] optimizer. We trained all the models for 100 epochs, with the learning-rate set to 0.001. The keys and values embeddings are initialized randomly. For average pooling in CAWA, we fixed the kernel-size to three. For ML-KNN, we used cosine similarity measure to find the nearest neighbors which is a commonly used similarity measure for text documents. We chose the number of neighbors (k) for ML-KNN based on the best \operatornamewithlimitsSOV score of the validation set. The hyperparamemer of MLTM is the number of topics (m), and the hyperparameter for SEG-REFINE is the segment creation penalty (α). We find these hyperparameters for MLTM and SEG-REFINE based on the best validation \operatornamewithlimitsSOV score.
The credit attribution specific approaches (CAWA, SEG-REFINE, and MLTM) perform considerably better than the other multilabel approaches (DNN+A, DNN-A, ML-KNN, and BR-MNB). CAWA performs better than the SEG-REFINE and MLTM on the \operatornamewithlimitsPPPA metric for the Ohsumed, TMC2007, Patents and Delicious datasets. The average performance gain for the CAWA on the \operatornamewithlimitsPPPA is 6.2% compared to MLTM and 9.8% compared to SEG-REFINE. Additionally, CAWA also performs at par, if not better, than the SEG-REFINE and MLTM on the \operatornamewithlimitsSOV metric. This shows that CAWA is able to find contiguous segments, without compromising on the sentence-level accuracy. Similar to the credit attribution task, CAWA, in general, performs better than the competing credit attribution approaches (SEG-REFINE and MLTM) on the \operatornamewithlimitsF1 metric, with an average performance gain of 4.1% over MLTM and 1.6% over SEG-REFINE. This shows that the classes predicted for the sentences by CAWA correlate better with the document classes as compared to the classes predicted by the competing credit attribution approaches. Additionally, CAWA performs at par with the competing credit attribution approaches on the \operatornamewithlimitsAUCμ and \operatornamewithlimitsAUCM metrics, further illustrating the effectiveness of CAWA.
Compared to the full model, we can find that the character-level word embedding layer is especially helpful for dealing with noisy social media text. The word-level attention also provides performance gain, while the field-level attention only provides a marginal improvement. The reason could be the multi-head attention layers in the transformer encoders already captures important information among different feature fields. These two transformer encoders learn the correlation between features and decouple these two level predictions. Finally, using the country supervision can help model to achieve a better performance with a lower mean error distance.
This section compares the participating systems in terms of their performance. Five of the nine system that did Task 1 also did the bonus Task 2. Following are the plots with their performance measured by ROUGE–2 and ROUGE–SU4 against the 3 gold standard summary types. The detailed implementation of the individual runs are described in the system papers included in this proceedings volume.
Our proposed method PGMCC consistently outperforms all the baselines on finding (country, president, time)-facts (i.e., presidential terms). PGMCC with different constraints For both PGMCC and TFWIN models, a complete constraint set, i.e., {C1(v)−1e and C1(e,t)−1v}, gives the best performance. Partial constraint cannot fully identify conflicts or false tuples. C1(e,t)−1v plays a significant role in extracting country’s president.
Here are our observations. First, the pattern “president Person of Country” is the only pattern that shows high reliability on both types of time signals (above 0.85). Second, the textual patterns that describe the current presidency are likely to have higher reliability on text gen. time than temporal tag, because the presidency was likely to be in the same time as the document was generated. These patterns usually have words such as “current”, “newly”, and “now”. Third, the textual patterns that describe the past presidency are likely to have higher reliability on “tag” than “post”, because the presidency was likely to be in the same time as the event (described in the sentence) happened but before the time of the document being generated. These patterns usually have words such as “have governed”, “have ruled”, “former”, and “formerly”.
Specifically, it lists the average F1 score obtained during each training simulation along with the total CPU time required to complete the simulation (accumulated with each training and evaluation iteration). Although in many applications, F1 score alone is sufficient to evaluate machine learning models, it is not for ours. To see why, note that the LSTM model yields an F1 score of 0.75, the highest of any hyperparameter combination. However, the LSTM model (with the highest F1 score) takes approximately 4,242 seconds to complete training, whereas the CNN model (with the highest F1 score) only takes 504 seconds. Thus, the LSTM model takes roughly eight times longer to simulate than the CNN model, but does not improve its F1 score by a significant amount (LSTM: 0.75 vs. CNN: 0.74). In the context of interactive learning, we wish to balance the training/CPU time and performance such that the model both performs well and retrains in a short amount of time for rapid improvement (DG4). Therefore, it is necessary to consider both the CPU time and average F1 score. With these optimization standards in mind, we chose the hyperparameters that yielded the highest F1 scores for each model since the other hyperparameter combinations generated lower F1 scores and higher or comparable CPU times.
The testing process is identical to the validation process: after the model is trained with 10 new samples, its performance is measured by computing the average F1 score on the testing set (using the optimized hyperparameters from the validation stage). We found that the LSTM model yielded the highest F1 score of 0.75. The CNN and RNN models achieved a 0.73 and 0.70 F1 score, respectively. Based on these results and the previously discussed optimization standards, we selected the optimized CNN model for our classifier. In particular, the CNN simulation not only yielded a competitive average F1 score of 0.73, but also achieved this score 6 to 8 times more quickly than the LSTM or RNN This model performance may be due to the initial lack of sufficient training data and difficulty in classifying certain tweets. For instance, after examining the testing dataset, we found that many misclassified tweets were extremely short (e.g., the tweet “screams internally” was misclassified as “Relevant”) or contained complex disaster-related diction (e.g., the tweet “emergency dispatchers in boone county in the hot seat” was misclassified as “Relevant”). However, as we demonstrate in the next section, our model still outperforms state-of-the-art learning models on tweet datasets.
Row (a) is the simple seq2seq model as the baseline. The probability of activating inter-layer and inner-layer teacher forcing is set to 0.5 in the rows (a)-(e); to evaluate the impact of teacher forcing, the probability is set to 0.9 (rows (f)-(h)). The probability of teacher forcing is attenuated every epoch, and the decay ratio is 0.9. We perform 20 training epochs without early stop; when the curriculum learning approach is applied, only the first layer is trained during first five epochs, the second decoder layer starts to be trained at the sixth epoch, and so on. To evaluate the quality of the generated sequences regarding both precision and recall, the evaluation metrics include BLEU and ROUGE (1, 2, L) scores with multiple references. To fairly examine the effectiveness of our proposed approaches, we control the size of the proposed model to be smaller. The baseline seq2seq decoder has 400-dim hidden layer, and the models with the proposed hierarchical decoder (rows (b)-(h)) have four 100-dim decoding layers. both offer considerable improvement. Combining all proposed techniques (row (e)) yields the best performance in both BLEU and ROUGE scores, achieving 103.1%, 53.1%, 152.8%, and 41.4% of relative improvement in BLEU, ROUGE-1, ROUGE-2, and ROUGE-L respectively. The results demonstrate the effectiveness of the proposed approach.
With K = 2, the SAT decodes 1.69× while maintaining 97% of the translation quality. In an extreme setting where K = 6 and beam size = 1, the SAT can achieve 6.41× speedup while maintaining 83% of the translation quality.
Based on the annotation, 61.7% of the posts mentioned a group, and 15.4% of these posts (a group is mentioned) are targeted to individuals. 90.6% of the groups are explicitly mentioned, indicating that publishers tend to clarify their targets in such posts. Regarding the specific target groups that the law requires to be distinguishable by nationality, religion, race or ethnic origin, we make some changes to increase the specificity and accuracy of the groups. For example, we use ‘muslims’ and ‘jews’ to stand in for religion, we use ‘black’ to represent race, and we use ‘foreigners/migrants’ as an example for sections of the population. If we consider ‘left wing/green’ also as a political target, then political group is the most targeted group, followed by foreigners/migrants and religion. This indicates that politically related problems are an important issue in Germany. Groups like ‘disabled’, ‘black’ and ‘LGBTQ+’ appear fewer than 20 times in all 1000 posts. Groups classified as ’others’ by the annotators were mainly sports clubs. We also find that stereotypes were often used for group names, e.g. ‘rapefugee’ instead of ‘refugee’. We also discover some disagreements between the annotators, e.g. annotator 2 only marked half of the number of posts as politically related.
Our best performing model, BERT-large, outperforms other models by several points on the dev and test set. We additionally ablate our best model’s representation by removing the context and question from the input, confirming that reasoning over both is necessary for this task. On each of the test sets, we report best, mean, and standard deviation of all models, and compare sequential finetuning results to a BERT-large baseline.
As hinted at by Phang2019SentenceEO, this suggests that BERT-large can benefit from both the large scale and the QA format of commonsense knowledge in Social IQa, which it struggles to learn from small benchmarks only. Notably, we find that sequentially finetuned BERT-Social IQa achieves state-of-the-art results on all three tasks, showing improvements of previous best performing models.
However, at inference time, due to its dependence on the whole history of previously generated words at each predicting step and a large amount calculation of multi-head attentions, the Transformer is much slower than RNN-based models. This restricts its application in on-line service, where decoding speed is a crucial factor. All models decode the same length target sentences for a fair comparison. The RNMT is a standard single layer GRU model with 512 embedding size, 1024-hidden dimension DBLP: journals/corr/BahdanauCB14. The Transformer(basic) model follows the basic setting in DBLP: conf/nips/VaswaniSPUJGKP17. 30K source- and target- vocabularies are used for all models. However, the decoding cost of the Transformer decoder is a significant issue which is over 3 times of that of RNN decoder and occupies 88% of the total decoding time. This is dominated by the high frequency of computing target-to-source attention, target self-attention and feed-forward network. We also analyze a single layer self-attention decoder to compare with the single layer RNN and find that even with a big sacrifice of translation quality, self-attention still slower than RNN in decoding. As for NMT inference speedup, numerous approaches have been proposed for RNN-based NMT models devlin:2017:EMNLP2017; zhang2017towards; DBLP:conf/emnlp/KimR16; DBLP:conf/acl/ShiK17. For Transformer, gu2017non proposed a non-autoregressive Transformer where output can be simultaneously computed. They achieved a big improvement on decoding speed at the cost of the drop in translation quality. Recently, zhang2018accelerating proposed an efficient average attention to replace the target self-attention of decoder. In this paper, we propose a hybrid architecture where the self-attention encoder and RNN decoder are integrated. By replacing the original Transformer decoder with an RNN-based module, we speed up the decoding process by a factor of four. Furthermore, by leveraging the knowledge distillationDBLP: journals/corr/HintonVD15; DBLP:conf/emnlp/KimR16, where the original Transformer is regarded as the teacher and our hybrid model as the student , our hybrid model can improve the translation performance significantly, and get comparable results with the Transformer.
For each hybrid model, we use Transformer model with corresponding layers as the teacher. Specifically, our 4- and 6-layer hybrid models achieve significant speedup with factors of 2.8x and 4.1x compared with the 4-layer and 6-layer Transformer teachers. We can find that the time cost of the three different attention models is very close. This is mainly due to the pre-computation and weight combination. As for translation performance, Both Transformers and the hybrid models outperform the RNMT and RNMT+KD. With the help of sequence-level knowledge distillation, all the hybrid models achieve significant improvements and even get comparable results with the Transformer.
We further verify the effectiveness of our approach on WMT 2017 Chinese-English translation tasks. Similar with the above results, our hybrid models can get 2.3x and 3.9x speedup compared with 4-layer and 6-layer Transformer, and with help of the knowledge distillation, our models achieve comparable BLEU scores with the Transformer.
Evaluation with an end-to-end system. We first evaluate our Biaffine md in combination with the end-to-end \newcitelee2018higher system. We slightly modified the system to feed the system mentions predicted by our mention detector. As a result, the original mention selection function is switched off, we keep all the other settings (include the mention scoring function) unchanged. We then train the modified system to obtain a new model. In the first experiment, we enabled the original mention selection function and fed the system slightly more mentions. More precisely, we configured our Biaffine md to output 0.5 mention per token instead of 0.4 i.e. λ=0.5. As a result, the coreference system has the freedom to select its own mentions from a candidate pool supplied by our Biaffine md. After training the system with the new setting, we get an average F1 of 72.6% ( This confirms our first hypothesis that by downgrading the system to a pipeline setting does harm the overall performance of the coreference resolution. For our second experiment, we used the \newcitelee2017end instead. The \newcitelee2018higher system is an extended version of the \newcitelee2017end system, hence they share most of the network architecture. The \newcitelee2017end has a lower performance on mention detection (93.5% recall when λ=0.4), which creates a large (4%) difference when compared with the recall of our Biaffine md. This confirms our second hypothesis that a larger gain on mention recall is needed in order to show improvement on the overall system. We further evaluated the \newcitelee2018higher system on the crac data set. We first train the original \newcitelee2018higher on the reduced version (with singletons removed) of the crac data set to create a baseline. We then evaluate the system with mentions predicted by our Biaffine md, we experiment with both joint learning disabled and enabled. The model trained with joint learning enabled achieved an average F1 of 69.1% which is 0.7% better than the baseline.
As we see, LSTM-based LMs significantly outperform the log-bilinear model as well as the backoff 3-gram LM, even if the 3-gram LM is trained on a much larger corpus with 1.6 billion words. The ZRegression mechanism improves the performance of LSTM to a large extent, which is unexpected. based on the history vector h.
Regarding the compression method proposed in this paper, we notice that LSTM-z,wb and LSTM-z,w yield similar performance to LSTM-z. In particular, LSTM-z,w outperforms LSTM-z in all scenarios of different vocabulary sizes.
As we can see that, the proposed CAS Reader significantly outperform the AS Reader in all types of test set, with a maximum improvements 2.1% on the CFT test-auto dataset. The results indicate that making a consensus attention over multiple time steps are better than just relying on single attention (as AS Reader did). This is similar to the use of “model ensemble”, which is also a consensus voting result by different models.
As can be seen in the table, the encoder-decoder network was able to consistently outperform both the random baseline and majority baseline models. Comparing the maps to each other, the encoder-decoder network produced the highest accuracy when generating rationalizations for the 75% map, followed by the 25% map and the 50% map respectively. To evaluate the significance of the observed differences between these models, we ran a chi-squared test between the models produced by the encoder-decoder network and random predictor as well as between the encoder-decoder network models and the majority classifier. Each difference was deemed to be statistically significant (p<0.05) across all three maps.
Evaluation. On both datasets, SANDI outperforms VSE++, especially in terms of paragraph rank (+14.1%/+10.5%) and order preservation (+11.1%/+14.5%). While VSE++ looks at each image in isolation, SANDI captures context better by considering all text units of the article and all images from the corresponding album at once in a constrained optimization problem. VSE++ ILP, although closer to SANDI in methodology, does not outperform SANDI. The success of SANDI can also be attributed to the fact that it is less tied to a particular type of images and text, relying only on word2vec embeddings that are trained on a much larger corpus than MSCOCO.
Evaluation. On both datasets, SANDI outperforms VSE++, especially in terms of paragraph rank (+14.1%/+10.5%) and order preservation (+11.1%/+14.5%). While VSE++ looks at each image in isolation, SANDI captures context better by considering all text units of the article and all images from the corresponding album at once in a constrained optimization problem. VSE++ ILP, although closer to SANDI in methodology, does not outperform SANDI. The success of SANDI can also be attributed to the fact that it is less tied to a particular type of images and text, relying only on word2vec embeddings that are trained on a much larger corpus than MSCOCO.
Role of Commonsense Knowledge. While in alignment-sensitive articles the connections between paragraphs and images are often immediate, this is less the case for articles with low alignment sensitivity. As one can see, adding CSK tags leads to a minor improvement in terms of semantic similarity (+0.1/+0.4%), although the improvement is too small to argue that CSK is an important ingredient in text-image alignments.
Evaluation. As expected, average strict precision (exact matches with ground truth) drops. Hence the average relaxed precision on image selection is higher. The nearest-neighbor baseline (NN) and SANDI, both use Word2Vec embeddings for text-image similarity. SANDI’s better scores are attributed to the joint optimization over the entire story, as opposed to greedy selection in case of NN. VSE++ uses a joint text-image embeddings space for similarity scores. The results in the tables clearly show SANDI’s advantages over the baselines. ILP. Combinatorial optimization (Integer Linear Programming) wins in performance over greedy optimization approaches. this phenomenon can be observed between NN (greedy) and SANDI (ILP). This pair of approaches make use of the same embedding space, with SANDI outperforming NN.
Evaluation. As expected, average strict precision (exact matches with ground truth) drops. Hence the average relaxed precision on image selection is higher. The nearest-neighbor baseline (NN) and SANDI, both use Word2Vec embeddings for text-image similarity. SANDI’s better scores are attributed to the joint optimization over the entire story, as opposed to greedy selection in case of NN. VSE++ uses a joint text-image embeddings space for similarity scores. The results in the tables clearly show SANDI’s advantages over the baselines. ILP. Combinatorial optimization (Integer Linear Programming) wins in performance over greedy optimization approaches. this phenomenon can be observed between NN (greedy) and SANDI (ILP). This pair of approaches make use of the same embedding space, with SANDI outperforming NN.
This model has 1600 LSTM memory cells and the output is projected to 800. The encoder has 6 layers and has context modeling with 4 frames lookahead at each layer. The CTC initialization slightly improves the RNN-T model with random initialization, while the CE initialization improves from the random initialization by 11.6% relative WER reduction. The CTC initialization makes the encoder emit token spikes together with lots of blanks while CE initialization enables the encoder to learn time alignment. Given the gain with CE initialization, we believe the encoder of RNN-T functions more like an acoustic model in the hybrid model. Because CTC training doesn’t need any alignment information while CE training needs, the result indicates learning alignment information for the encoder may help RNN-T training to focus more on reasonable forward-backward paths instead of all the paths.
As before, we find that Word2Vec and LDS provide significant accuracy improvements over the baseline. We expect that the reason the LDS does not outperform Word2Vec is that NER relies mainly on performing local pattern matching, rather than capturing long-range discourse structure.
The time to train the LDS, about 30 minutes, is inconsequential compared to training the RNN (4 days) on a single CPU core. LDS training on the PTB is faster than our experiments above with 1B tokens because we use a small vocabulary and run far fewer EM iterations, in order to prevent overfitting. The RNN baseline converged after 17 training epochs, while using the LDS for initialization allowed it to converge after 12, which amounts to about a day of savings on a single CPU core. We find that initializing with the LDS also provides a better model.
We have trained DFSMN with various architectures, which can be denoted as 3∗72-Nf×[2048-512(N1;N2;s1;s2)]-Nd×2048-512-9004. Here Nf and Nd are the number of cFSMN-layer and ReLU DNN layer respectively. In these experiments, N1=20,N2=20,Nd=3 is kept fixed. In the first experiment, we have investigated the influence of the number of cFSMN-layers and the size of the stride on the final speech recognition performance. We have trained cFSMN with six, eight, ten and twelve cFSMN-layers. Here, DFSMN(6) denotes DFSMN with Nc being 6. Results of exp1 and exp2 indicate the advantage of using stride for the memory block. From exp2 to exp5, we can achieve consistent performance improvement by using deeper architecture.
For the baseline LFR-LCBLSTM with Nc=27 and Nr=13, the number of delay frame for time instance is 40. For LFR-DFSMN, we can control the number of delay frame by setting the lookahead filter order. As a result, the latency is about 150ms (30ms∗5) which is suitable for real-time applications. Finally, the proposed DFSMN with 20 frames latency can achieve more than 20% relative improvement compared to the LCBLSTM with 40 frames latency.
We also performed ablations for a model with 100 facts (see Supplement). (w/o ctx-only). The values for each component are obtained from the attention weights, without retraining the model. The difference between blue (left) and orange (right) values indicates how much the module contributes to the model.
The results show a similar tendency, but in this setting, omitting the model without knowledge enrichment yields best results for the CN data.
Classification accuracy for a number of models (see The left chart shows results for these models when trained on German data and evaluated on English data, the right chart vice versa. The actual CLDC experiments are performed by training on English and testing on German documents and vice versa. Following prior work, we use varying sizes between 100 and 10,000 documents when training the multiclass classifier. Our models outperform the prior state of the art, with the Bi models performing slightly better than the Add models. As the relative results indicate, the addition of a second language improves model performance. It it interesting to note that results improve in both directions of the task, even though no additional German data was used for the ‘+‘ models.
Among existing baselines, we observe that link-based approaches i.e., WLM and DeepWalk perform better than others for top-k correlation. Whereas, temporal models yield substantial improvement overall. Specifically, the TS-CNN-Att performs better than the no-attention model in most cases, improves 11% for Pearson@10, and 3% when considering the total rank. Our trio model performs well overall, gives best results for total rank. The duo models (combine base with either pretrained DW or PV) also deliver improvements over the sole temporal ones. We also observer additional gains while combining of temporal base with pretrained DW and PV altogether. Here we report the results on the nDCG metrics. We can observe the good performance of the baselines for this task over conventional temporal models, significantly for proxy setting. It can be explained that, ‘static’ entity relations are ranked high in the non time-aware baselines, hence are still rewarded when considering a fine-grained grading scale (100 level). The margin becomes smaller when comparing in human setting, with the standard 5-level scale. All the models with pretrained representations perform poorly. It shows that for this task, early interaction-based approach is more suitable than purely based on representation.
We use forecasters’ final rank sorted by averaged standardized Brier score over all forecasts as ground truth. We then compare our text-based model to the following two baselines: (1) a random baseline (50%) and (2) the standardized Brier score of the users’ single earliest forecast. Results. We observe that our models achieve comparable or even better performance relative to the first prediction’s adjusted Brier score. Calculating Brier scores requires knowing ground-truth, while our model can evaluate the performance of a forecaster without waiting to know the outcome of a predicted event.
Our goal is to test whether linguistic differences exist between accurate and inaccurate forecasts, independently of who made the prediction, or how difficult a specific company’s earnings might be to predict. Setting. We collect the top K and bottom K predictions and split train, dev and test sets by time range and company. All company names are randomly split into 80% train and 20% evaluation sets. We use predictions for companies in the train group that were made in 2014-2016 as our training data. The dev set and test set consist of predictions for companies in evaluation group made during the years 2017 and 2018, respectively. When evaluating the classifier’s performance, we balance the data for positive and negative categories. Results. We observe our classifiers consistently achieve around 60% accuracy when varying the number of top and bottom forecasts, K.
The model uses a development set of data to determine a plausibility threeshold, and classifies triples with a higher score than the threshold as correct, and those with lower score as incorrect. Given a set of four or five word pairs, the model selects the pairs that most and least represent a particular relation (defined by a set of example word pairs) by comparing the cosine similarity of the vector difference between words in each pair. We incorporate the embeddings as additional features in a standard maximum spanning tree dependency parser to see whether embeddings improve generalization of out-of-domain words. The evaluation metric is the labeled attachment score, the accuracy of predicting both correct syntactic attachment and relation label for each word.
Previously, we performed a basic comparison between sentence embedders using N2O. Here, we show one kind of analysis enabled by N2O: given a query, which sentences from the corpus C are consistently its neighbors across different embedders? We might expect, for example, that a nearly identical paraphrase of the query will be a “popular” neighbor chosen by most embedders. We also show sentences that are highly ranked for some embedder but not in the nearest neighbor sets for any other embedder (for larger k=50). We also observe that extremely “popular” neighbors tend to have high lexical overlap with the query.
We run both CNN and RNN models with and without EDA across all five datasets for varying training set sizes. Of note, average improvement was 0.8% for full datasets and 3.0% for Ntrain=500.
For ‘wh’ questions, we observe that Syntactic alone brings an improvement of 4% and BERT Syntactic performs the best by improving 8% over the baseline. This suggests that ‘wh’ questions generally share a closely related syntax structure and that information can be used to achieve better prosody. This intuition is further strengthened by the improvements observed for ‘or’ questions. Syntactic alone improves by 9% over the baseline and BERT Syntactic performs the best by improving 21% over the baseline. The improvement observed in ‘or’ questions is greater than ‘wh’ questions as most ‘or’ questions have a syntax structure unique to them and this is consistent across samples in the category. For both categories, the systems Syntactic, BERT and BERT Syntactic show incremental improvement as the first system contains only syntactic information, the next captures some aspect of syntax with semantics and the third has enhanced the representation of syntax with CWE representation. Thus, it is evident that the extent of syntactic information captured drives speech synthesis quality for these two categories.
The first column indicates which word the bigram starts with. From the samples shown in the table, we can conclude that models augmented with our normalization methods indeed promote the diversity of bigrams. The last row indicates that Log Mutual Normalization gains the best performance in promoting diversity of bigrams.
In this subsection, we propose a toy environment to verify our similarity-based method. Then, we filter out those sub-relations appearing less than 50 times to eventually get 1165 relations. All these split relations are regarded as different ones during training, and then different relation similarity metrics are adopted to merge those sub-relations into one relation.
Intuitively, we give a larger margin between similar relations, forcing the model to distinguish among them, and thus making the model perform better. We apply our method to Position-aware Attention LSTM (PA-LSTM)Zhang et al.
BLEU computed against the original TED Talks reference translation. We can see that NMT clearly outperforms all other approaches both in terms of BLEU and TER scores. Focusing on mTER results, the gain obtained by NMT over the second best system (PBSY) amounts to 26%. It is also worth noticing that mTER is considerably lower than HTER for each system. This reduction shows that exploiting all the available post-edits as references for TER is a viable way to control and overcome post-editors variability, thus ensuring a more reliable and informative evaluation about the real overall performance of MT systems. For this reason, the two following analyses rely on mTER. In particular, we investigate how specific characteristics of input documents affect the system’s overall translation quality, focusing on (i) sentence length and (ii) the different talks composing the dataset.
We can see that NMT generates translations which are morphologically more correct than the other systems. In particular, the %Δ for NMT (-13.7) is lower than that of the second best system (PBSY, -16.9) by 3.2% absolute points, leading to a percentage gain of around 19%. We can thus say that NMT makes at least 19% less morphology errors than any other PBMT system. More precisely, the NMT score (18.7) is better than the second best (PBSY, 22.5) by 3.8% absolute points. This corresponds to a relative gain of about 17%, meaning that NMT makes at least 17% less lexical errors than any PBMT system. Similarly to what observed for morphology errors, this can be considered a remarkable improvement over the state of the art.
Compared to S+Q model, S+V+Q models get the most improvements on “what” and “where” questions, indicating these questions require additional visual information. On the other hand, adding video features did not improve S+Q performance on questions relying more on textual reasoning, e.g., “how” questions.
The negative answers in TVQA are written by human annotators. They are instructed to write false but relevant answers to make the negatives challenging. Alternative methods include sampling negative answers from other questions’ correct answers, either based on semantic similarity (Das2017VisualD; Jang2017TGIFQATS) or randomly (Antol2015VQAVQ; Das2017VisualD). The former is prone to introducing paraphrases of the ground-truth answer (zhu2016visual7w). The latter avoids the problem of paraphrasing, but generally produces irrelevant negative choices. Human-Written Negatives vs. Randomly-Sampled Negatives For comparison , we create a new answer set by replacing the original human written negative answers with randomly sampled negative answers. To produce relevant negative answers, for each question, negatives are sampled (from the other QA pairs) within the same show. Performance on randomly sampled negatives is much higher than that of human written negatives, indicating that human written negatives are more challenging.
The first two columns are for the English corpus while the next two columns are for the Tsonga corpus defined by the Challenge, both including across speaker and within speaker tasks. Except for Coverage and NED whose values are indicators of the system characteristic rather than the system performance, the higher the value the better for the other five metrics. Except for Coverage, the other six scores are shown in the six subfigures in Fig. We omit Coverage here because with our approach Coverage is always 100% in all cases. In each subfigure, the results for four cases are shown in four sections from left to right, corresponding to the four sets of tokens obtained in MAT after the first and second iterations of MAT-DNN (marked by TOK-1st or TOK-2nd) with MR performed or not (MR-0,1,2). These are marked at the bottom of each section. For each of these sections, the three or six groups of bars correspond to different values of m (m=3,5,7 or m=3,5,7,9,11,13), while in each group the four bars correspond to the four values of n (n=50,100,300,500 from left to right), where ψ=(m,n) are the parameters for the token sets. The bars in blue and yellow are those better or equal to the JHU baseline Only the results jointly considering both within and across talker conditions are shown.
The three selected tokens sets are: (A): (TOK-1st, MR-0, m=7, n=50) for English; (B): (TOK-2nd, MR-1, m=9, n=50) and (C): (TOK-1st, MR-1, m=13, n=300) for Tsonga. These three selected proposed example sets are also marked in Fig. We first compare the proposed sets (A), (B), and (C) with the JHU baseline. Regarding the NED and coverage, a better system should have lower NED and higher coverage. A system that discovers a lot of tokens usually has high coverage and also high NED, because more tokens usually means more mismatches. Such a system is said to be permissive. On the other hand a system that only returns high confidence tokens usually has low coverage and also low NED. Such a system is said to be selective. Therefore NED and coverage are closely related and have to do with the system characteristics and engineering trade-offs. As a result, the much higher NED and coverage scores of the proposed token sets (A), (B), and (C) suggest that the proposed approach is highly permissive, while the JHU baseline is highly selective. The much higher parsing scores (Type, Token and Boundary scores), especially the Recall and F-scores, imply the proposed approach is more successful in discovering word-like units. However, the Matching and Grouping F-scores of sets (A), (B), and (C) were much worse, probably because the discovered tokens covered almost the whole corpus, including short pauses or silence, and therefore many tokens were actually noises. Another possible reason might be that the values of n used were much smaller than the size of the real word vocabulary, making the same token label used for signal segments of varying characteristics, and this degraded the grouping quality.
Due to limited resources, we cannot run all the possible combinations. The location-based function does not learn good alignments: the global (location) model can only obtain a small gain when performing unknown word replacement compared to using other alignment functions. For content-based functions, our implementation concat does not yield good performances and more analysis should be done to understand the reason. It is interesting to observe that dot works well for the global attention and general is better for the local attention. Among the different models, the local attention model with predictive alignments (local-p) is best, both in terms of perplexities and BLEU.
(b) Discussion. Firstly, we compare the performance between WFR with WMD. As presented, WFR Document Distance has less KNN classification error rate at all datasets. Furthermore, for the datasets with large standard deviation of NDW (exceeds 40, see Appendix), i.e. dataset BBCSPORTS, AMAZON and 20NEWS, WFR outperforms the document distance with a clear margin. For those datasets with less standard deviations of NDW, the reduction of the KNN classification error is not that significant. Secondly, we compare the performance between WFR with S-WMD. WFR successfully outperforms S-WMD in six out of eight datasets even though S-WMD has more supervised parameters. The successful of WFR over S-WMD since a more effective way to re-weight the transport plan is automatically captured by WFR, rather than text-independent global re-weighting in S-WMD. We notice that S-WMD only outperforms WFR and WMD at OHSUMED dataset. The medical term for cardiovascular disease in the OHSUMED dataset may not have proper word vector. The text-independent deficiency of the word embedding might be relieved by supervision in S-WMD. (c) WFR Document Distance for Other Frameworks. This framework realized fast estimation of WMD by Monte Carlo’s method. We found that replacing the WMD in WME framework with WFR document distances effectively improves the original results. WME+WFR consistently outperforms WME+WMD under exactly the same setting (512 MC samples). Notably, the results of WME+WFR are closed to those of WME+WMD reported with 8 times MC samples (4096) with minor computation cost (see Appendix).
TransRev achieves the best performance on 17 out of the 19 data sets. In line with previous work TransRev is competitive with and often outperforms HFT on the benchmark data sets under consideration. To quantify that the rating predictions made by HFT and TransRev are significantly different we have computed the dependent t-test for paired samples and for all data sets where TransRev outperforms HFT, the p-value is smaller than 0.01.
We randomly selected the 4 data sets Baby, Digital Music, Office and Tools&Home Improvement from the Amazon data and evaluated different values of k for user, item and word embedding sizes. We only observe insignificant differences in the corresponding model’s performances.
the reference form was translated as masculine compared to feminine. Words with extreme values of this ratio indicate cases where a model has a systematic preference for one gender over another, i.e. a gendered representation.
To this base model, we successively add various features of our method: phonetic matching, exact copy translations, gold lexicons and finally distribution-based alignment (dist) of remaining entities. For both languages, we observe that every additional feature improves the performance of tagging, with the most important features being phonetic matching for Spanish and use of gold lexicons for Hindi. Interestingly, addition of phonetic matching hurts Hindi because of the low value of the threshold (δ=0.25), which results in spurious matches due to phonetic matching. This underscores the correlation between quality of entity alignment and performance on the downstream tagging task.
Evaluations reported in the columns (Optimized) Clinical BERT represent individually fine-tuned, per-task BERT models. Evaluations reported in the column MT-Clinical BERT represent light-weight task-specific heads over a single multitask trained BERT feature encoder. We find that the performances reported in the Clinical BERT While this is not surprising (the authors specify that performance was not their goal), it is important to compare improvements or degradations against a competitive baseline. All further discussion compares the multitasking model to the hyperparameter Optimized Clinical BERT baseline.
In general, the algorithms we introduce again outperform the NoStruct baseline. In contrast to the crowdlabeled experiments, AP (slightly) outperformed the other algorithms.
While the directed MST parser (D-MST) is the best performing model across almost all test sets and evaluation measures, it outperforms our best model, U-MST-uf-lep, by a very small margin.
Importantly, in this work we present an edge-linear first-order dependency parser which achieves similar accuracy to the existing one, making it an excellent candidate to be used for efficient MST computation in k-best trees methods, or to be utilized as an inference/initialization subroutine as a part of more complex approximation frameworks such as belief propagation. each of the models outperforms the other on an average of 22.2% of the sentences across test setups. An oracle model that selects the parse tree of the best model for each sentence would improve D-UAS by an average of 1.2% over D-MST across the test setups. The potential embodied in this work extends to a number of promising research directions: Our algorithm may be used for efficient MST computation in k-best trees methods which are instrumental in margin-based training algorithms. For example, \newciteMcDonald:05b observed that k calls to the CLU algorithm might prove to be too inefficient; our more efficient algorithm may provide the remedy. It may also be utilized as an inference/initialization subroutine as a part of more complex approximation frameworks such as belief propagation (e.g. \newciteSmith:08, \newciteGormley:15). Finally, the complementary nature of the directed and undirected parsers motivates the development of methods for their combination, such as dual decomposition (e.g. \newciteRush:10, \newciteKoo:10a). Such techniques can exploit this diversity to produce a higher quality unified solution.
In the Official submission, we trained the Tamil Parser with a mixture of English and Tamil datasets (‘Ours+en+MST‘ in the table), and in the post-evaluation, we also tried a mixture of Czech and Tamil datasets (‘Ours+cs+MST‘ in the table) because the Czech dataset contains the largest training data over all languages. In the official results, our system was fixed by the organizers through their simple scripts for the connectivity of graphs, which significantly reduced our system performance. In the post-evaluation, we fixed this issue with MST or Eisner’s algorithm and showed that our system performs 0.6 ELAS higher than the best team. For the Tamil parser, mixing the Tamil dataset with the Czech dataset performs 1.7 ELAS better than mixing with the English dataset, which shows that a larger dataset gives better results than the smaller one. Our system with the MST algorithm is 0.2 ELAS stronger than the system with Eisner’s algorithm, which shows that the non-projective tree algorithm (MST) is better than the projective tree algorithm (Eisner’s) for the EUD task. We built our codes based on PyTorch Paszke et al.
The results show that second-order inference is stronger than first-order inference in all languages, and embeddings with XLMR embedding only usually perform better than XLMR+Flair+FastText embeddings. However, the Flair+FastText embedding is helpful for Tamil. Therefore we use XLMR+Flair+FastText embeddings for training the Tamil parser while we use XLMR embedding only for other languages.
In order to answer (RQ1: what are the best component estimators?), we first notice that SER defined as basic outperforms its pop and types variants. It does that for each of both CCR settings, and for any pair of values for the (N,M) parameters. The differences in favor of SER are highly significant in terms of MAP, and less or not significant according to MRR. Filtering by common type should help retain relevant support entities, but the results suggest that, even with slight differences in terms of MRR, the performance of the whole ranking in terms of MAP is degraded. Secondly, there is not a clear pattern in the performance when comparing settings for the M parameter. Regarding the last component, the semantic definition of CCR leads to the best estimator. This shows that the vocabulary mismatch affects CCR, a gap against which the semantic method is more robust.
From the results, we observe that: (1) Our Reinforced Co-Training model can outperform all the baselines, which indicates the capability of our methods in utilizing the unlabeled data. (2) The standard co-training is unstable due to the random data selection strategy, and the performance-driven and high-confidence data selection strategies both can improve the performance of co-training. Meanwhile, the significant improvement compared with previous co-training methods shows that the Q-agent in our model can learn a good policy to select high-quality subsets. (3) The three pre-trained based semi-supervised learning methods also show good results. We think these pre-trained based methods learn local embeddings during the unsupervised training, which may help them to recognize some important patterns in clickbait detection. (4) The self-attentive biGRU trained only on headlines of the labeled set actually show surprisingly good performance on clickbait detection, which demonstrates that most clickbait documents have obvious patterns in the headline field. The reason why CNN (Document) fails to capture these patterns may be that the concatenation of headlines and paragraphs dilutes these features. But for those cases without obvious patterns in the headline, our results demonstrate that the paragraph information is still a good supplement to detection.
We can see that the DSRM significantly outperforms the standard relatedness method M&W (p≤0.05, according to the Wilcoxon Matched-Pairs Signed-Ranks Test), indicating that deep semantic models based on semantic KGs are more effective for relatedness measurement. As we incorporate more types of knowledge into the DSRM, it achieves better relatedness quality, showing that the four types of semantic knowledge complement each other.
The hyperbolic skip-gram embeddings give an improved performance for some combinations and datasets. For the WS-353 and MEN datasets, higher scores can mainly be observed in low dimensions (5, 20), whereas for higher dimensions the Euclidean version is superior by a small margin. The relatively low scores on Simlex-999 suggest that both skip-gram models are better at learning relatedness and association. We point out that our results on the WS-353 dataset surpass the ones achieved in Dhingra et al. Overall, we conclude that the proposed method is able to learn sensible embeddings in hyperbolic space and shows potential especially in dimensions that are uncommonly low compared to other algorithms.
This procedure seems indeed to be the natural generalisation of the analogy task. There is a subtlety, however. The procedure obtains the point Z by beginning at A and proceeding via C, and this point Z is then used to search for nearest neighbours. However, in Euclidean space, it would have been equally valid to proceed in the opposite sense, i.e. by beginning at A and proceeding via B, and this would also yield a point Z′. In Euclidean space, it doesn’t matter which of these two alternatives is followed, since the resulting points Z,Z′ coincide (indeed, in the Euclidean case the points A,B,C,Z=Z′ form a parallelogram). However, the problem formulation A:B =C: D is not symmetric, as the proposed relation is between A and B, not A and C. Therefore, we argue that LogA(B) should be the tangent vector (representing the relation) that gets parallel transported, and not LogA(C). It is evident that using Z performs significantly better. This suggests the correctness of our hypothesis and illustrates that the analogy problem is indeed not symmetric. Interestingly, in the Euclidean setting this does not surface because the four words in question are considered to form a parallelogram and the missing word can be reached along both sides. In comparison with the performance of the Euclidean embeddings, a tendency similar to that observed in the simliartiy task arises. The hyperbolic embeddings outperform the Euclidean embeddings in dimension 20, but are surpassed in higher dimensions. The lowest dimension 5 appers degenerate for both settings.
Aiming to investigate the influence of sememe structure in detail, we construct a pseudo graph structure G′ for comparison where all sememes inside a sense are mutually connected. In addition, we conduct comparisons by using only the sememes from characters (single-character words), in order to explore the effect of the word-level sememe information. Our final model, the end-to-end model with word-level sememe-enhanced encoder (i.e., joint + GAT(word_real)), achieves competitive performance, where the triple-level F-measure reaches 79.64, which is the best-performance model, significantly better than the basic model without using the sememe information. The triple-level F-measure of pipeline models with or without word-level sememe graph are 78.71% and 74.74%, respectively.
We investigate the model performance with respect to different lexical fusion types. We classify the fusion word types by IV/OOV according to the training corpus, and further differentiate a fine-grained coreference by whether the fusion character is borrowed from its separation word (denoted by A) or not (denoted by B). Our models perform better for the IV categories than the OOV, which confirms with our intuition. In addition, we divide the IV/OOV further into AA and AB categories. We can find that AB is much more difficult, obtaining only 41.1 of the F1 score. Further, by examining the overall performance of fine-grained coreference of type A and B, we can see that Type B leads to the low performance mainly. The performance gap between the two types is close to 50 on the F1 score.
According to the table, our model achieves new state-of-the-art results without using auxiliary information. Specifically, on FB15k, the MR of our model surpasses all previous results by 12, and our hit@10 outperforms others by 5.7%. We also report the results of EKGNs with different embedded knowledge graph sizes |EKG| and different Tmax on FB15k in Appendix A.
In this part, we have compared the similarity values produced by each of the similarity measures CS, SRCC and PCC. For instance, d1 refers to leadership in the nineties, while d5 refers to the family and medical lead act of 1993. We have empirically observed that the general topics discussed in these two textual documents are very different. Namely, discusses different frameworks for leadership empowerment, while d5 discusses medical treatment and self-care of employees. We have observed that the term employee is the only connection between d1 and d5. The similarity value of CS of 0.36 is very unreal in this case, while PCC (0.05), and especially SRCC (0.0018) provide a much more realistic view of the semantic knowledge aggregated in these documents. Another example are d8 and d9. The contents of these documents are very straightforward and very similar, because they discuss aliens seen by Boeing-747 pilots and d9 discusses angels that were considered to be aliens. It is obvious that SRCC is able to detect this association as good as CS and PCC which are very good in such straightforward cases. We have observed that SRCC does not perform worse than any other of these similarity measures. It does not always produce the most suitable similarity value, but it indeed does perform at least equally good as other measures. It is mostly a few times larger than CS and PCC when there actually exist associations between the documents.
Several classifiers were trained and evaluated on the WMT test set, with or without fine-tuning on the WMT development set. Some classifiers maintain the translation quality (top rows), whereas others show quality degradation but further gain in decoding speed (bottom rows). The classification results show that gains in decoding speed are possible with an a-priori decision for which encoder-decoder combination to select, based on the information contained in the source sentence only. However, no BLEU gain has so far been observed, demonstrating a trade-off between decoding speed and translation quality. Our best configuration for decoding speed (#4) reduced 210s but leads to a 0.7 point BLEU degradation. On the other hand, when preserving the translation quality compared to the baseline configuration (#1) we saved only 37s. The oracle layer combination can achieve substantial gains both in terms of BLEU (7.1 points) and decoding speed (961s). These oracle results motivate possible future work in layer combination prediction for the tied-multi NMT model.
We further train models Bi-Forward-Decoder and Baseline with phoneme as input. Together with character-based models, 5-point mean opinion score (MOS) tests are conducted. Tab. It’s noticed that the proposed model Bi-Forward-Decoder performs better than baseline model Baseline no matter what kind of input representation is used. And a much more obvious advantages of our proposed method could be observed when using character as input. This could be explained by that, compared with phoneme-based models, character-based models often suffer from pronunciation issues caused by grapheme-to-phoneme, and our proposed method is helpful to correct such issues since it could use the global information of an utterance to give more proper pronunciation. And among these systems, the Bi-Forward-Decoder (with phoneme as input) obtains the best performance with a MOS of 4.26.
, we also evaluate the proposed method on relative in-domain text, like appropriate text length and content. In this section, only the model Bi-Forward-Decoder (which performs best on out-of-domain text) is included for comparison. We randomly select 50 test utterances (not included in training set) from our internal dataset. Tab. We find that the both the proposed character-based and phoneme-based Bi-Forward-Decoder perform better than the baseline model Baseline, with a gap of 0.14 and 0.06 in MOS, respectively. This trend is similar as that on out-of-domain evaluation, which further confirms the effectiveness of the proposed regularization method. Meanwhile, it’s noticed that phoneme-based model Bi-Forward-Decoder obtains the best performance with a MOS of 4.42, which is quite close to recording (4.49). Examination of judges’ comments also show that Bi-Forward-Decoder performs much better on overall prosody, sounds more expressive, stable and clearer than Baseline.
The analysis of the nested NER system was done for outer and inner levels. The first level tags are outer level and second level tags are inner level tags. Based on the precision and recall evaluated by FIRE-2014, F1 measure was calculated. It shows that English NER system which was implemented using CRF shows better results than other Indian languages which were implemented using SVM. CRF consumes more time while training the model but increase the performance of the system. Among all languages, Hindi system shows very low performance for the inner level whereas outer level has equal performance with other two Indian languages. Malayalam and Tamil language shows almost same performance for their outer and inner level. Indian Languages show 50% decreased performance than English. From this system, we observed there should be improvement in the model developed in order to increase the performance of all Indian languages. It shows that F1 measure of the Tamil language is increased from 25.81 to 37.11 in outer level and good performance increase for inner level from 18.56 to 44.98. Tamil language shows the high performance among the Indian languages which has approximate match metric of 55.04 for outer level and 61.58 for inner level. The entities like numbers are identified correctly by these systems and tagged as COUNT, PERIOD, QUANTITY, DATE or TIME based on their occurrence. Systems are able to identify the numbers because of use of various binary features specific to number identification like presence of number, any digit number, two digit number, 3 digit number and 4 digit number. The most of the entities which represent place name or person name are also identified by the system because of use of gazetteer information. This system also fails to predict accurate tag. For example, while training the system the name of location, city and nation are tagged as LOC, CITY and NATION. This system fails to differentiate these three tags but any one of these are tagged for name of location, city or nation.
The developed model performance is evaluated by 10- fold cross validation of training set and validated against the development data. Accuracy=∑correctlyidentifiedentitiestotalentities×100 The final entity linking part is done by utilizing look-up dictionary (DBpedia 2014) and sentence similarity. The entity ’s tokens are given to the look up dictionary which results in few related links. The final link assigned to the entity is based on maximum similarity score between related links and proper nouns in the test tweet. Similarity score is computed by performing dot product between uni-gram vectors of proper nouns in the test tweet and the uni-gram vectors of related links from look-up dictionary. Entity without related links is assigned as NIL.
In the following we briefly analyse the impact of distributional similarity and investigate to what extent the similarity scores between two predications change when tense and aspect influence the entailment. This further indicates that many false positives of the neural network based models in our results are due to high distributional similarity scores between predications. For Apts the cosine scores — even when normalised — are generally very low due to their sparsity and high dimensionality, highlighting their bias towards false negatives. This indicates that the embedding models do appear to capture some of the semantics of tense and aspect in their respective contextualised representations. However, their high distributional similarity overwhelms any finer distinction that the models might have extracted.
bAbI-10k Results On the jointly trained bAbI-10k dataset our best model (out of 10 runs) achieves an accuracy of 99.58%. That is a 2.38% improvement over the previous state-of-the-art that was obtained by the Sparse Differential Neural Computer (SDNC) Rae et al. The best model of the 10 runs solves almost all tasks of the bAbI-10k dataset (by a 0.3% margin). However, a simple ensemble of the best two models solves all 20 tasks and achieves an almost perfect accuracy of 99.7%. Other authors have reported high variance in the results, for instance, the authors of the SDNC report a mean accuracy and standard deviation over 15 runs of 93.6±2.5 (with 15.9±1.6 passed tasks). In contrast, our model achieves a mean accuracy of 98.3±1.2 (with 18.6±0.4 passed tasks), which is better and more stable than the average results obtained by the SDNC. The Relation Network solves 18/20 tasks. We think that by including the attention mechanism, the relation reasoning module can focus on learning the relation among relevant objects, instead of learning spurious relations among irrelevant objects. For that, the Multi-Head attention mechanism was very helpful.
It is clear that SE+SID the use of a joint optimisation can performs better than VoiceID_loss [shon2019voiceid] using only a pre-trained speaker identification model instead of a joint optimisation. In comparison with speaker identification task, the verification improvements obtained using SE-MS+SID and SE+SID-MS are relatively smaller. This is probably because for speaker verification, the similarity between speaker embeddings learned from speaker model is computed using Cosine function instead of directly being computed using the trained speaker recognition model. In addition, for speaker verification, the use of attention model in speech enhancement module can yield slight better performances in almost all conditions except when speeches are corrupted by Babble noise at 0dB and 5dB SNR levels. For this case, a possible reason is that the ”Babble” noise signals are relative complication due to its speaker/speech like characteristics. The use of attention model in speaker recognition module (SID-Net) might be more suitable to extract speaker relevant information than using an attention model in the speech enhancement model when acoustic environment is poor.
Given this connection between USEs and the low-rank approximation of U, it is informative to understand the relationship between how labels are distributed across sentences and how much error there is using a low-rank approximation to estimate U. We conduct experiment on a binary matrix, A size 4,000 by 4,300, with each experimental variation modifying the density (percentage of non-zero elements). Rows can either be 0.1%, 1%, or 10% dense, and the number of rows with each type of density can either be even or skewed, with one type taking 90% of rows, another 9.9%, and the last only 0.1%. We then reconstruct A using this low-rank approximation, and calculate the l1 loss across each row. Finally, we average the row-wise loss across each density type.
Now, focusing on the design of the new metric, we consider using the corpus-wide variability of topics’ estimates as our new metric. We can see the cv distributions of good (Topic1) and bad (Topic2) topics are the most different. The cv distribution of Topic1 covers a large span and has a heavy head and tail, while cv values of Topic2 are mostly clustered in a smaller range. In contrast, the difference between Topic1 and Topic2 ’s distributions of μ and σ throughout the corpus appears to be less pronounced. topic k’s estimates as our new metric. Formally, it can be defined as: variability(k)=std(cv1k,cv2k,⋯,cvDk) (2) where D is the size of the corpus. High quality topics will have higher variability and low quality topics will have lower variability. The variability defined by cv is a clear winner.
Following Roder et al. , we use Pearson’s r to evaluate the correlation between the human judgments and the topic quality scores predicted by all the automatic metrics. The higher is the Pearson’s r, the better the metric is at measuring topic quality. Our proposed variability-based metric substantially outperforms all the baselines.
It should be noted that the transformer model is much deeper than the recurrent ones in terms of the number of encoder and decoder layers. The entity-conditioning model lags behind the GRU-S2S by only 0.7 BLEU, demonstrating that a sequence-to-sequence model does not perform dramatically better than a simple bag-of-entities system on this dataset.
We believe that the combination of improved discriminator training and the policy-based objective is responsible for the observed performance improvement. On the other hand, multi-turn models (V)HRED and hredGAN suffer performance loss due to exposure bias, since autoregressive sampling is not included in their training. Although DAIM uses autoregressive sampling, its poor performance shows the limitation of the single-turn architecture and GAN objective compared to the multi-turn architecture and policy-based objective in aBoots. The transformer Seq2Seq model, which performs better than RNNs on the machine translation task, also suffers from exposure bias, and overfits very quickly to the low entropy regions in the data, which leads to a poor inference performance. Also, the results from aBoots models indicate that word-level discrimination performs better than utterance-level discrimination, consistent with the results reported by \citeauthorOlabiyi2018 \shortciteOlabiyi2018 for the hredGAN model. While it is difficult to identify why some models generate very long responses, we observe that models with Gaussian noise inputs (e.g., hredGAN and aBoots_gau) may be using the latent Gaussian distribution to better encode response length information; indeed, this is an area of ongoing work. Within the variants of aBoots, we observe that models trained with a stochastic policy, aBoots_uni and aBoots_cat, outperform those trained with a deterministic policy, aBoots_gau. Notably, we find that for the stochastic policy, there is a tradeoff in relevance and diversity between top_k categorical and uniform sampling. The categorical sampling tends to perform better with relevance but worse with diversity. We believe that this is because top_k categorical sampling causes the generator to exploit high likelihood (i.e., more likely to be encountered during inference) than uniform sampling of the top candidates, while still allowing the policy to explore. This however comes with some loss of diversity, although not significant. Overall, the automatic evaluation indicates that adversarial bootstrapping trained with stochastic policy using top_k categorical sampling strategy gives the best performance.
In this section, we examine the effect of partial bootstrapping on the model performance. The table shows that the generator models bootstrapped by a discriminator that is not bootstrapped generally performs worse than ones with a bootstrapped discriminator. This improvement is particularly more evident in the best performing variant, aBoots_w_cat. We attribute this performance improvement to the better calibration of discriminator obtained from the bootstrapping of the discriminator output with the similarity measure between the generator’s autoregressive output and the ground truth during training.
In order to run our experiments on unseen essay sets, we augment the training data with the gaze behaviour data collected. Since none of these essays have source articles, we use the self-attention model of \newcitedong-etal-2017-attention as the baseline system. The first column in the table is the prompt IDs. The next 3 columns are the 3 configurations - Only Prompt, Extra Essays, and Essays+Gaze. The improvement when learning gaze behaviour for unseen essay sets is statistically significant (p=0.0041).
Besides, the improvements restate the significance of explicitly incorporating local structural information in knowledge graph embedding learning. Moreover, compared with all the other baselines, the RotatE-GCN model was consistently better while the TransE-GCN model performed differently on the two datasets. To be specific, TransE-GCN performed better than ComplEx on FB15K-237 while worse on the other dataset. This can be interpreted by the difference between TransE and ComplEx that TransE is not good at dealing with relation types except 1-to-1 relations, as pointed out by researchers before. During the training process, for each triple (h,r,t), TransE enforces h+r to be as close as possible to t, which would be problematic when dealing with 1-to-N, N-to-1, and N-to-N relations. For example, given a 1-to-N relation r, we have two triples (h,r,t1) and (h,r,t2). If h+r =t holds, t1 and t2 should have the same vector representations. To meet this requirement, h+r is close to the center of all the positive tails t at the end of training instead of a particular tail (which may be the correct prediction). Therefore, the performance of TransE dropped extremely on WN18RR, where there are four times more entities but 20 times less relations than those in FB15K-237. The superior performance of RotatE-GCN model over TransE-GCN model indirectly showed the importance of a base model used in our framework.
S3SS3SSS0Px4 in terms of multi-hop neighbors, namely 1-hop, 2-hop and 3-hop neighbors. TransE-GCN model favored 1-hop neighbors while RotatE was able to leverage more neighborhood information. The difference lies in that RotatE has a stronger ability to deal with complex relations and capture more accurate entity and relation information. But both models performed the worst when 3-hop neighbors were considered, which were even worse than the base models. We think this may be caused by spectral convolutional filters, since it has been proven to have a smooth effect that could dilute the useful information(Li
It even outperforms DTW on Spanish, Croatian and Swedish; this is noteworthy since DTW uses full alignment to discriminate between words (i.e. it has access to the full sequences without any compression). To further investigate the impact of training language choice, we train supervised monolingual CAE-RNN models on each of the training languages, and then apply each model to each of the zero-resource languages. We observe that the choice of well-resourced language can greatly impact performance. On Spanish, using Portuguese is better than any other language, and similarly on Croatian, the monolingual Russian and Czech systems perform well, showing that training on languages from the same family is beneficial. Furthermore, the multilingual CAE-RNN trained on all seven languages The performance effects of language choice therefore diminish as more training languages are used.
One natural question to explore is how would the LASER method benefit if it had access to additional data. To explore this, we used the LASER open-source toolkit, which provides a trained encoder covering 93 languages, but does not include Nepali. For Nepali–English the situation reverses: LASER local provides much better results. However, the results of the pretrained LASER are only slightly worse that those of Bicleaner (6.12) which is the best non-LASER method. This suggests that LASER can function well in zero-shot scenarios (i.e. Nepali–English), but it works even better when it has additional supervision for the languages it is being tested on.
For this task, MTL is a very effective strategy. This makes it possible for one sub-task to eavesdrop information form the other tasks. For example, if a tweet is labelled as Targeted in sub-task B, then it must be classified to Offensive in sub-task A. and they are consistent except the model with pre-training. Our ensembled MTL model achieves the best performance in both two test sets.
In order to test our method with varying contexts, we used multiple corpora. We decided to focus on four representative emotional categories: neutral, happy, sad, and angry. We also aimed for variation in gender (female or male) and naturalness (natural or acted). We selected six corpora meeting our requirements: LDC Emotional Prosody (L) (I) Since AIBO has only emotional speech of children, we further categorised the gender of FAU-aibo corpus to female-child and male-child. In the cross-corpus (leave-one-corpus-out-cross-validation) condition, we test on a corpus that was not included as training data. Optimising models without any access to a testing corpus is such a challenging task. Moreover, we examine the contribution of each subtask to the emotion detection task. We compared the baseline (STL) and the proposed methods using either gender (GENDER-MTL) or naturalness (NATURALNESS-MTL), and both of them (ALL-MTL) as subtasks. In all conditions, we used 10% of training data for optimising parameters and excluded them from training data. Lastly, we investigated dependency of our methods on the type of testing corpora: acted and natural. DNN-MTL and LSTM-MTL obtained more overall gains from testing acted corpora. However, AIBO has a missing category and the severe unbalanced number of samples for classes Hence, we do not conclude that effect of our methods is limited to only typical expressions.
Text of the list As mentioned earlier, template specific information like headers are scrubbed away. These can bias the classifier for the particular set of documents in our dataset but might not generalize to new sources of support documents. To further remove words specific to this dataset, words from context and list text that do not appear in the top 10,000 most frequent words in the Google n-gram corpus are filtered out. We find that the unigram model performs best, which might be because of the large number of features generated in the model with bigrams and trigrams compared to the number of training examples.
Context By “context ” we refer to the sentences introducing the procedure and possibly its purpose. Our intuition is that sentences which introduce a list are a good signal for deciding if it is a procedure, with phrases like the following steps to ¡X¿ being glaring clues. However, it’s not clear how much text before the list should be included before it starts hurting performance. We find that taking a single sentence before the list gives the best accuracy. Sentence tokenization on text scraped from HTMLs is not highly accurate, however.
Despite the linguistic challenges of identifying the decision block, we find that a simple baseline of considering all sentences following the decision point till the end of the current “step” gets about 78% of our decision blocks correctly. This highlights the importance of structural information about the procedure in its accurate extraction. Following are the major classes of mistakes made by our simple baseline. We try to capture most of them with rules on top of the baseline. Our annotators uniformly reject the presence of information sentences such as those starting with “Note” in decision blocks, as these are not instructions and are considered to be applicable to both branches of a decision point. When present in the same step as a decision point, we apply a simple rule to stop before information sentences, identified using keywords like “Note”, or “Information”. Capturing these cases, we improve our block identification accuracy to 84% If the next step after a decision point also starts with a conditional, and they have a significant overlap, we can combine the next step as a part of the decision block. For example, if the condition is “If the slot status is missing”, and the next step starts with “If the slot status is failed”, the next step should be a part of the decision block starting at the first condition. About 16 mistakes from 50 were cases where decision blocks end at the sub-list item or paragraph (inside the step) in which the conditional sentence is found. A simple rule to identify these would bump our accuracy further to about 90%. However, we currently do not keep information about paragraph boundaries or sublists inside list texts, and hence are not able to capture these cases.
We evaluate the baselines and the proposed model PAS on the PHED dataset. We calculate the Token-REP-4, Sent-REP-4, and Unique-4 using human written headlines for comparison. As we can observe, there is a significant statistical difference between the human and the machine-generated outputs in terms of diversity and repetition. Standard sequence-to-sequence with attention (Seq2seq) yields poor results in all metrics, which is reasonable because of the lack of pre-training, suggesting the necessity of using large-scale pre-trained language models to improve the overall quality of generations. Surprisingly, Seq2seq with Adaptation does not work as well as PAS + Adaptation. We suspect that it is probably due to fact that the two tasks have different inputs (one with the original headline, and the other without). Without pre-training on a more general language model, it seems hard to bridge the headline editing task and the headline generation task for efficient transfer learning.
The translation quality marginally drop while parameters are fewer and training speeds are quicker. This phenomena verifies that it is unnecessary to apply the proposed model to all layers. We further reduce the applied layers to low-level two (Row 5), the above phenomena still holds. However, a big drop on translation quality occurs when the number of layer is reduced to 1 (Rows 6-7).
As a first step, we ask the question: what does accuracy of the probing classifier actually tell us about the training task? We construct multiple versions of the task (both training and development sets) where the entailment decision is independent of the given linguistic property , through careful partitioning as described in § To control for the effect of training data size, we downsample MultiNLI training data to match the number of samples in each partitioned version of the task.
we see that even though our model forms all predictions simultaneously between all pairs of entities within the sentence, we are able to outperform state of the art models classifying each mention pair independently. The scores shown are averaged across 10 runs with 10 random seeds. Interestingly, our model appears to have higher recall and lower precision, while the baseline models are both precision-biased, with lower recall. This suggests that combining these styles of model could lead to further gains on this task.
we list precision, recall and F1 achieved by our model on the CTD dataset, both overall and by relation type. Our model predicts each of the relation types effectively, with higher performance on relations with more support.
To validate the effectiveness of the multi-label classification method and the post-processing strategies in this application, the SVM method are used as the baselines. Moreover, our proposed post-processing strategies are based on KNN method, so we also report the results generated by KNN in the experiments (the parameter K is set to be 10). In the table, f1, f2, f3, f4, and f5 denote features 1-gram, 2-gram, (1+2)-gram, (1+2)-gram + POS and (1+2)-gram + POS + STAT, respectively. In these results, the number of similar neighbors K is also set to be 10 and the influence of this parameter will be discussed in the next section.
With image information only, the model cannot identify the fake news well. It indicates that image information is insufficient to identify the fake news. However, logistic regression fails to identify the fake news using the text information. The reason is that the hyperplane is linear, while the raw data is linearly inseparable. with text information are inefficient with very long sequences, and the model with 1000 input length performs worse. Hence, we take the input length 400 as the baseline method. With text and image information, TI-CNN outperforms all the baseline methods significantly.
We also performed ablation studies to better understand the contributions of the different parts of our model.
e.g. the subset of system outputs with an iteration number of three or lower, while the lower section presents results for a neural filtering model. The neural model takes as input a system output’s iteration number, score under the paraphrase model, and score under the alignment model, and produces a score between 0 and 1, where 0 represents a decision to filter an output, and 1 represents a decision to keep it. Architecturally, the model is a feed-forward neural network with two hidden layers, 10 units per hidden layer, and a sigmoid output layer, trained to minimize binary cross entropy loss. We trained one model to favor precision by downweighting the training loss when the label was 1, and a second model to favor recall by downweighting when the label was 0. As training data, we used the 1710 aggregated manual judgments from above (where each system output has a label of 0 or 1), plus 2988 additional judgments collected specifically for this model.
As the first step, the author’s similarity matrix of each baseline model can establish the author weighted graph. We propose three algorithms to calculate author similarities (SoulMateConcept, SoulMateContent, and SoulMateJoint). Benchmark- To this end, we first obtain the set of MSTs out of G′ (output of SW-MST) which comprises any of 50 arbitrarily chosen authors. We then pick top 5 MSTs with at least 5 nodes that possess the highest average edge weights. Finally, given the top 10 most similar tweets from each pair of authors in the selected MSTs, we consider the votes of 5 local (Australian) experts. The possible votes are defined as follows: score 0: neither textually or conceptually similar. score 1: minor textual and conceptual similarity. score 2: high textual and conceptual similarity. score 3: minor textual but high conceptual similarity. Subsequently, we compute the average of the votes given to each pair of tweets and round it to the nearest lower integer. We then count the tweet pairs with the scores of 2 and 3 for each one of the author similarity calculation methods. The precision metrics are then calculated by dividing the number of 2 and 3 scores (admitted by the average of experts’ votes) by the total number of selected tweet pairs in subgraphs. SoulMateConcept is devised to detect the conceptual similarities, where the textual relevance is minor. Moreover, the SoulMateContent can trace the textual and conceptual relevance. However, since SoulMateJoint combines both modules through parameter adjustment (α=0.6), it gains the highest votes for both conditions. It is interesting to see that where the textual similarity between short-text contents is very low and all textual models including Temporal Collective, CBOW, Document Vector and Exact matching fail (perform less than 2%), SoulMateConcept can detect the semantic correlation between authors by 30%. Notice that the higher the number of exploited concepts, and the better the clustering models, SoulMateConcept model can gain a better precision. the conceptually relevant author pairs. Conversely, SoulMateContent can track textual similarity (%43). To briefly mention, SoulMateJoint as our final model performs more accurately than other baselines. Hence, we employ the collective manner →VC which offers a lower precision of 0.861 but in contrast, provides a much smaller dimension (the size of hidden layer vectors (|d|)).
(2) the type of combination (Avg vs. Sum) for word vectors in generation of tweet vectors, (3) clustering type (K-Medoids vs DBSCAN) in constructing author concept vectors. The K-Medoids clustering performs better than DBSCAN. This is because the DBSCAN model can ignore outliers. We notice that the time-aware collective model performs the best @K=22, where the CBOW gains the lowest results. Since the normalized summation vectors resemble the average approach, their corresponding precision results turn the same. We can, therefore, overlook the impact of combination type in tweet generation.
We can observe that that the proposed framework achieves the best results in terms of Accuracy, AUC-ROC, precision, recall and F1 measurement. In the weakly supervised setting, we first train the proposed annotator on the reports with labels, and then use the well-trained annotator to assign “weak” labels for unlabeled news according to their reports. Finally, a fake news detection model can be trained using the news content and the corresponding “weak” labels. This confirms the importance of weak supervision from reports for fake news detection. The advantage of the proposed framework is that it can automatically annotate unlabeled news. Though incorporating automatic annotation as weak supervision helps fake news detection in some aspects, weak supervision is unavoidably noisy. This shows that incorporating weak supervision may add more false positive examples. For real news, since the majority of unlabeled data with reports is still real news, the precision still improves. To reduce the influence of noisy labels, the proposed framework WeFEND has the data selector component based on reinforcement learning techniques. After incorporating data selector, the precision values of fake news and real news are improved compared with their reduced version in the same hybrid setting.
In later experiments, we found that a residual connection can achieve similar accuracies with fewer number of parameters, compared to a shortcut connection. Therefore, in order to reduce the model size and to also follow the SNLI leader-board settings (e.g., 300D and 600D embeddings), we performed some additional SNLI experiments with the shortcut connections replaced with residual connections, where the input to each next biLSTM layer is the concatenation of the word embedding and the summation of outputs of all previous layers (related to ResNet in computer vision
Results Empath shares overall average Pearson correlations of 0.90 (unsupervised) and 0.906 (crowd) with LIWC Over the emotional categories, Empath and LIWC agree at correlations of 0.884 (unsupervised) and 0.90 (crowd), comparing favorably with EmoLex’s correlation of 0.899. Over GI’s benchmark categories, Empath reports 0.893 (unsupervised) and 0.91 (crowd) correlations against LIWC, stronger performance than GI (0.876). On average, adding a crowd filter to Empath improves its correlations with LIWC by 0.006. These scores indicate that Empath and LIWC are strongly correlated – similar to the correlation between LIWC and other published and validated tools.
Finally, we test our model on a set of held-out scenarios, and compare to ground-truth execution. We compared accuracy of the estimated motion under each of three conditions: with oracle subgoals Gt from the test data, with unambiguous templated language, and with natural language. In all cases, we compute an execution plan ^G1,… ,^G5 at the beginning and use our Predictor and Actor networks to follow this execution plan until all steps have been executed. We count successes when the block was moved to within 1.5 cm of the target in the x and y direction, and 0.5 cm z of the final position from which it was dropped. Average placement error increases as we move away from the ground-truth arguments. Often failures occur because the object is not clearly visible in the first frame.
The implementation of our algorithm involves two stages: feature selection and model estimation. To describe the feature selection stage in detail, we take examples from the China data, which consist of 250 news articles on protest towards the government from 2001 to 2014. On average, each article contains about 398 words before the preprocessing treatment and 284 after the treatment. The columns represent each method and the rows indicate the data sets used. All numbers are rounded up. For the classification algorithms, we performed three 3-fold cross validations, thus the scores in the columns of neural net, SVM, and random forests are averages of nine iterations in total. The dictionary column represents the accuracy rate of the dictionary method that codes all captured location words as correct event locations.
We factorized input gates in both time and layer LSTMs by reducing the calculation of a 1024-dimension gate vector into the calculation of two 32-dimension gate vectors as in Eqs. We also applied similar operation to factorize output and forget gates in both time and layer LSTMs. All the factorized gate operation increased WER. The impact of factorizing forget gate is the smallest, with relative 2.3% and 3.7% WER increase from the full version of ltLSTM without any factorization, although it is still better than all the LSTM and ResLSTM models. Factorizing input gates has the biggest degradation. Given the loss, we didn’t evaluate the setup which factorizes all the gates together. With single gate factorization, the parallel computational cost is reduced to 25 M operation per frame which is even lower than that of the 6-layer LSTM or ResLSTM while the WER of factorized gate ltLSTM is clearly better than that of the 6-layer LSTM or ResLSTM.
To show the effectiveness and generality of our work, we evaluate the validness of our work by applying it to current representative models without revising the models.
We find that performance gains on SuperGLUE averages is reasonably good (+1.9% on Large). The model still outperforms the vanilla model on GLUE with marginal performance gains. Overall, on a macro-average of 18 tasks, we find an overall +1.0% improvement across three sizes. These results show that performance gains scale with model size.
On the cross sentence (single model setting), the performance of our proposed CAFE model is extremely competitive. We report the test accuracy of CAFE at different extents of parameterization, i.e., varying the size of the LSTM encoder, width of the pre-softmax hidden layers and final pooling layer. CAFE obtains 88.5% accuracy on the SNLI test set, an extremely competitive score on the extremely popular benchmark. Notably, competitive results can be also achieved with a much smaller parameterization. For example, CAFE also achieves 88.3% and 88.1% test accuracy with only 3.5M and 1.5M parameters respectively. This outperforms the state-of-the-art ESIM and DIIN models with only a fraction of the parameter cost. At 88.1%, our model has about three times less parameters than ESIM/DIIN (i.e., 1.4M versus 4.3M/4.4M). Moreover, our lightweight adaptation achieves 87.7% with only 750K parameters, which makes it extremely performant amongst models having the same amount of parameters such as the decomposable attention model (86.8%).
On MultiNLI, CAFE significantly outperforms ESIM, a strong state-of-the-art model on both settings. We also outperform the ESIM + An ensemble of CAFE models achieve competitive result on the MultiNLI dataset. On SciTail, our proposed CAFE model achieves state-of-the-art performance. The performance gain over strong baselines such as DecompAtt and ESIM are ≈10%−13% in terms of accuracy. CAFE also outperforms DGEM, which uses a graph-based attention for improved performance, by a significant margin of 5%. As such, empirical results demonstrate the effectiveness of our proposed CAFE model on the challenging SciTail dataset.
In (1), we replaced all FM functions with regular full-connected (FC) layers in order to observe the effect of FM versus FC. More specifically, we experimented with several FC configurations as follows: (a) 1-layer linear, (b) 1-layer ReLU (c) 2-layer ReLU. Using ReLU seems to be worse than nonlinear FC layers. Overall, the best combination (option a) still experienced a decline in performance in both development sets.
We perform a linguistic error analysis using the supplementary annotations provided by the MultiNLI dataset. We compare against the model outputs of the ESIM model across 13 categories of linguistic phenenoma Williams et al. We observe that our CAFE model generally outperforms ESIM on most categories.
To understand the role of the suffix “_PP” in embeddings_PP, we trained word vectors embeddings_wo_PPSuffix using the same noun pairs as in embeddings_PP. For each noun pair, we remove the suffix “_PP” attached to the head noun. This indicates that the suffix “_PP” is the most significant factor in embeddings_PP.
However, adding the constraint based on the vanilla word embeddings (GloVe_GigaWiki14) or the word embeddings without the suffix “_PP” (embeddings_wo_PPSuffix) slightly decreases the result compared to MLN model II. Although MLN model II already explores preposition patterns to calculate relatedness between head nouns of NPs, it seems that the feature based on embeddings_PP is complementary to the original preposition pattern feature. Furthermore, the vector model allows us to represent the meaning of an NP beyond its head easily.
In this evaluation we used the mined topics to create a document representation based on the similarity between topics and documents. This representation was used to train an SVM classifier with the class of the document. In particular, we focused on the 20 Newsgroups corpus for this experiment. We used the typical setting of this corpus for document classification (60% training, 40% testing). The results illustrate that the number of topics is relevant for the task: Online LDA with 400 topics is better than 100 topics. A similar behavior can be noticed for SWMH, however, the parameter r has an effect on the content of the topics and therefore on the performance.
In order to assess the impact of class imbalance on the learning, we performed an additional experiment with a balanced dataset using the best performing configuration. We took a subset of the instances equally distributed with respect to their class from the training set (330 instances for each class) and test set (71 instances for each class). The result of this analysis clearly indicates that class imbalance has a negative impact on the system performance.
An ablation test was conducted to explore the contribution of each feature set. This evaluation includes macro-averages of precision, recall and F1-score as well as accuracy. We also presented the scores for each class in order to get a better understanding of our classifier’s performance.
We perform ablation studies on other input components: table caption and table header. We use the most performant GPT-2 model to conduct experiments.
The model initialized with word2vec outperforms those with glove for most of cases. Moreover the higher dimension used, the better it finds topics under the same word representation model. It implies that word2vec is more proper model than glove model as well as the higher dimension of embedding space used in initialization leads to the better performance, which is easily predictable. Thus we could expect nicer results if we use better word representation.
Pearson correlation coefficient measures the linear correlation between the actual and predicted scores and has been used extensively in prior art [mohammad2017wassa, preoctiuc2016modelling]. Additionally, referring back to our motivation of choosing the emotion prediction task, we provide empirical evidence that stylistic aspects do correlate with valence, arousal and dominance values. The questions around causal significance and extent [pearl2010causal, wang2018blessings] of stylistic aspects towards evoked emotion are yet to be answered, and are left as a part of future work. However, we expect the interpretability of the proposed stylistic features to aid in establishing causal relationships. To illustrate the value of the proposed multi-level representation of style, we focused on three tasks: authors’ style analysis, authorship attribution and emotion prediction. Given this, it becomes essential to emphasize that this work does not aim to propose novel approaches to any of the aforementioned tasks. The primary aim of the work is an effort to establish a structured multi-level understanding of style in text that can facilitate in better modeling of style. An interesting aspect that is highlighted by solving the tasks of authorship attribution and emotion prediction is the varying extent to which stylistic elements at different levels contribute towards solving the task. This claim is substantiated as we note that the original handcrafted features used in the baseline are devoid of any stylistic features whatsoever. The proposed structure provides more holistic interpretability while modeling style to solve related tasks.
Starting with the baseline, we incrementally add the type pair, graph-based, and Adding type pair features results in an appreciable performance gain, while the graph features bring little benefit—potentially because pairwise correlations suffice to summarize the set structure when the number of types is moderately low.
For the evaluation of this task, we created WikiSection, a novel dataset containing a gold standard of 38k full-text documents from English and German Wikipedia comprehensively annotated with sections and topic labels From this normalization step we obtain 598 synsets which we prune using the head/tail division rule count(s)<1|S|∑si∈Scount(si) This method covers over 94% of all headings and yields 26 normalized labels and one other class in the English disease dataset. We verify our normalization process by manual inspection of 400 randomly chosen heading–label assignments by two independent judges and report an accuracy of 97.2% with an average observed inter-annotator agreement of 96.0%.
The qualitative analysis was done by two human judges, who classified errors in the output of the systems into six categories. Our evaluation process for Open Information Extraction systems should be convenient and comparable. To meet this goal, we deliver supplementary scripts to import commonly used data sets with our evaluation system RelVis. The unified data model enables the user to perform quantitative comparisons and extensive analyses on widely used data sets. Data sets NYT-222 and OIE2016 also contain n-ary relations. These labeled data sets origin from Mesquita et al.
Our method achieves the highest F1 scores for both tasks. The temporal curriculum further improves our results by a large margin, validating its effectiveness for domain adaptation on diachronic corpora. Although DANN achieves higher precision on the multi-label task, its recall largely suffers.
Our full model achieves the highest F1 scores on both the binary and multi-label tasks, and each component consistently contributes to the overall F1 score. The 2D CNN also has decent F1 scores, showing that our framework works with standard CNN models. Further, the time embedding significantly improves both F1 scores, indicating the model effectively utilizes the unique temporal information present in our corpora.
To interpret the information that is transmitted through the linguistic channel, we use the message transcript and the item pool to create probe classifiers that predict the hidden utilities of each agent and the (hidden) accepted proposal. We use an LSTM to encode the sequence of symbols belonging to the message transcript into a vector, and another LSTM to encode the item pool into a vector. The resulting two vectors are then concatenated and used to predict the hidden utility functions and the accepted proposal using 9 linear classifiers, one for each item ( 3 each for agent A and agent B’s utilities and 3 for the proposal). We also include two baselines. (indicating chance performance at the task). The other predicts the accepted proposal from the item pool and a message transcript of all 0 ’s. This shows how much additional information about the proposal is contained in the message transcript. This shows that our agents have learned to give meaning to the symbols, and can use them to transmit information. However, the purely self-interested agents do not seem to transmit meaningful information using the linguistic channel, and thus do not appear to ground the symbols.
Column 1 of Table. Column 2 specifies the performance of language model with minimum accuracy among all the 176 language models. Similarly, column 3 specifies the performance of language model with maximum accuracy among all the 176 language models. Column 4 is the average percentage of correctly detected testcases in all the 176 languages. Columns 2, 3 are intended to show that the performance of LID system is consistant across the languages. In Table. Based on the conducted experiments, we observed that RNNLM performance increases when we use optimum number of classes to decompose the vocabulary, i.e., when number of classes are approximately equal to √|V|, where |V| is the vocabulary size. In case of RNNLM 6-classes, error has been propagated to 4 steps back in time, which lets the model predict the next word probability with the knowledge of higher dimensional features captured by the history. Rest of the models use simple RNN to model the language.
We also tested cross-lingual models that exploit language similarities on the lexical level without translating or projecting annotation. The idea is similar to delexicalized models that are trained on generic features on the source language treebank, which are then applied to the target languages without further adaptation. In particular, we used substrings such as prefixes (simulating simple stemming) and suffixes (capturing inflectional similarities) to add lexical information to delexicalized models. However, those models did not perform very well and we omit the results in this paper. We still apply UDPipe for PoS and morphological tagging using the provided tagger models for the target languages and similar ones trained on the UD treebanks for the source languages except for Czech, which did not work with standard settings due to the complexity of the tagset and limitations of the implementation of UDPipe. We considered all language pairs from the VarDial campaign and here we present the relevant results from our experiments. First of all, we need to mention that we created new baselines using the mate-tools to have fair comparisons of the cross-lingual models with respect to baseline approaches. The same table also summarizes our basic results for all language pairs using the three approaches for data transfer as introduced in the previous section. All projections are made in collapseDummy mode as explained above.
We then used the best results on development data for each of the three target languages to run the cross-lingual models on the test set. No further adjustments were done after tuning the models on development data. The main scores in our evaluations is LAS but it is also interesting to look at unlabelled attachment scores (UAS). The difference to LAS scores is dramatic, much more than the absolute difference we see between UAS and LAS in the fully supervised models. This seems to be a shortcoming of our approach that we should investigate more carefully.
We then used the best results on development data for each of the three target languages to run the cross-lingual models on the test set. No further adjustments were done after tuning the models on development data. The main scores in our evaluations is LAS but it is also interesting to look at unlabelled attachment scores (UAS). The difference to LAS scores is dramatic, much more than the absolute difference we see between UAS and LAS in the fully supervised models. This seems to be a shortcoming of our approach that we should investigate more carefully.
One can see that RNN performs better than CNN, although it requires additional training time. Results are cross-validated on 10 different runs and variances are presented in the Table as well.
The baseline LSTM-COS system is based on the framework by \newciteKiros2014a – it uses an LSTM for composing a sentence into a vector, calculates the relevance score by finding the cosine similarity between the sentence vector and the image vector, and optimises the model using the hinge loss function. This model already performs relatively well and is able to distinguish between relevant and random image-text pairs with 68.2% accuracy.
8-bit computation achieves 32.3 words per core second (646 words per second), compared to the 6.5 words per core second (131 words per second) of the 32-bit system (both systems load parameters from the same model). This is even faster than the syntax-based system that runs at 21.5 words per core second (430 words per second).
An independent native speaker of the language being translated to/from different than English (who is also proficient in English) scored 100 randomly selected sentences. The sentences were shuffled during the evaluation to avoid evaluator bias towards different runs. We employ a scale from 0 to 5, with 0 being unintelligible and 5 being perfect translation.
The frame subjective_influence, with an FQS of 0.366, has a low score compared to the others. From looking at the sentences, we observed that the crowd had difficulty distinguishing between this frame and objective_influence. The difference between these two frames is very small – subjective_influence means a general, vague type of influence, whose effect cannot be measured, whereas objective_influence refers to a more concrete type of influence. (e.g. in F13 is cultural influence subjective or objective?). Another feature we observed was the correlation of FQS with how abstract the sense of the frame is. Frames with high FQS, such as killing and food, tend to refer to concrete events or objects. These frames can still appear in ambiguous contexts (e.g. in F5, it is not clear whether herbs classify as a type of food), but overall these frames refer to specific and particular senses that are unambiguous. As the value of the FQS metric goes down, the frames become more abstract. assistance and purpose both have example sentences where they are expressed unambiguously (F6 and F9), but their definitions are more abstract, and therefore have more room for interpretation. For instance, providing benefits (in F7) or expertise (in F8) can be regarded as a type of help, or assistance, even though the expert picked the more literal sense of the frame supply for both of these cases. Likewise the frame purpose can be understood in F10 as the purpose of a design (the expert picked the more literal coming_up_with), or in F11 as the goal of the desire/will (the expert picked desiring). undergo_change,
For all systems, we generate daily article summaries of at most 100 words, and select 5 comments for the corresponding comment summary. ROUGE-2 (measures bigram overlap) and ROUGE-SU4 (measures unigram and skip-bigrams separated by up to four words) As can be seen, under the alternating optimization framework, our systems, employing both articles and comments, consistently yield better ROUGE scores than the three baseline systems and our systems that do not leverage comments. Though constructed from single-article abstracts, baseline Abstract is found to contain redundant information and thus limited in content coverage. This is due to the fact that different media tend to report on the same important events.
We conclude the presentation of the results by considering all the different settings in which we ran the experiments. Focusing on the CER accuracy, the best average performances are achieved when considering the pair (k=10, nocc ≥ 200): such results have been already presented and discussed in the previous section. Turning to the global accuracy, we achieve a slightly better result still considering only the names that appear at least 200 times in the whole collection, but having only 5 names as possible candidates (Global Accuracy = 0.65).
We conclude the presentation of the results by considering all the different settings in which we ran the experiments. Focusing on the CER accuracy, the best average performances are achieved when considering the pair (k=10, nocc ≥ 200): such results have been already presented and discussed in the previous section. Turning to the global accuracy, we achieve a slightly better result still considering only the names that appear at least 200 times in the whole collection, but having only 5 names as possible candidates (Global Accuracy = 0.65).
In Colonia there are not many texts from each class and only a few from the 16th century. For this reason, in \newcitestajnerandzampieri13 and in this preliminary experiment, we disregard texts from this century and propose an experiment with four classes instead of the five represented in Colonia. We considered the majority class (19th century) as the baseline performance.
S5SS2SSS0Px3 Comparison of various reward baselines b. (SC), and not using a baseline (None), i.e. b=0. MA baseline is the accumulated sum of the previous rewards with exponential decay. SC baseline is the received reward when all agents directly take greedy actions. Noteworthy that the performance gaps between our CF baseline and other baselines become larger when trainings start from a poor-performed model (i.e. XE model w/o weight-init). That is, our method is less sensitive to model initialization, suggesting its ability to enable more robust and stable reinforcement learning. None and MA severely degrades the performance compared to XE model when not using weight-init, but they perform similar to XE model when using weight-init. While SC considerably outperforms XE model, it is still inferior to CF. The reason is that both MA and SC are agent-agnostic global baselines, which cannot address the multi-agent credit assignment problem, while our CF baseline is agent-specific.
Generally, using more unlabeled images could lead to better performance. XE training benefits more from the unlabeled images than CMAL training because we directly use the unlabeled images during XE training while not using them for CMAL.
Our system outperforms the current state-of-the-art single model by a 2.6% absolute improvement in F1 score and scores 90.7 points after ensemble. In addition, while most of the top systems rely on additional supervised data, our system do not use any extra training data.
To assess the impact of each method we apply, we perform a series of analyses and ablations. The methods are added one by one to the baseline model.
The first group shows a comparison of three quantum inspired language models. QMWF-LM-word significantly outperforms QLM by 10.91% on MAP and 12.12% on MRR, respectively. The result of QMWF-LM-word is comparable with that of NNQLM-II. In the second group, we compare our model with a range of CNN-based models against their results reported in the corresponding original papers.
QMWF-LM-word significantly outperforms QLM by 35.74% on MAP, and 37.86% on MRR, as well as NNQLM-II by 6.92% on MAP, and 7.74% on MRR. In comparison with CNN models, QMWF-LM-word outperforms QA-CNN and AP-CNN by (1%∼2%) on both MAP and MRR, based on their reported results.
Our QMWF-LM-word achieves a significant improvement over QLM by 45.57% on P@1 and 23.34% on MRR, respectively. It also outperforms NNQLM-II on P@1 by 23.39% and on MRR by 10.70%, respectively. Note that the data preprocessing of YahooQA dataset in our experiments is a little different, as we randomly sample four negative examples from the answers sentence set.
They show that the hybrid model (TDNN-F LF-MMI) is still better in the reviewed validation data domains among most of the end-to-end approaches in terms of WER. The use of NNLM also noticeably improves hybrid model performance. However, the Transformer end-to-end model with subword acoustic units demonstrates the best accuracy results on YouTube and books data. The recognition of phone calls is worse within 0.6-1.3% in comparison with the hybrid system. This may be due to the small number of unique phone calls training data compared to YouTube and books domain. As we discussed before, end-to-end ASR systems are more sensitive to the amount of training data than the hybrid ones.
Each test set is translated by Google Translate - Translator Toolkit, and by our system. Bleu score is used to evaluate the performance of both systems. Surprisingly, for Danish, Portuguese and Polish, ONTS has better performance, this depends on the choice of the test sets which are not made of news data but of data that is fairly homogeneous in terms of style and genre with the training sets.
Note that CoT takes no pretraining stage and its NLLoracle loss progressively decreases. Our method takes a pretraining stage and the loss decreases in both the pretraining stage and the adversarial training stage. We could notice that upon convergence, the NLLoracle loss for our method is significantly lower than CoT. This demonstrates that the cooperative training mechanism proposed by CoT is not comparable to our method in terms of sample quality. When comparing NLLgen, our method could achieve much lower loss scale than CoT. This demonstrates that our proposed algorithm convey greater efficiency in preserving the sample diversity. Overall, considering the inferior performance and long training time of this model, we do not consider it further in the following real-world dataset experiments.
Evaluations on EMNLP2017 WMT News dataset. Overall, our method demonstrates significant advantage over all the sample quality/diversity metrics. Notably, our method leads to NLLgen loss significantly lower than the other baseline approaches. This indicates that our method could provide an efficient control over the mode collapse for the adversarial training and eventually leads to superior sample diversity. While decelerating the mode collapse, the cooperative training could result in model with better sample quality as well.
We can see that our proposed method consistently outperforms all baselines in terms of all the BLEU metrics and NLLgen. Under the temperature setting of 100, our method outperforms the strong RelGAN baseline by 0.041/0.039 on BLEU-4/BLEU-5. Noticeably, the best BLEU scores for our method are obtained when the NLLgen loss is at a significantly lower level than RelGAN. This indicates that by conducting cooperative training, we could derive generator model with better sample quality and sample diversity simultaneously. Moreover, it shows that our method could robustly perform well in rather challenging and diverse real-world datasets like EMNLP. Meanwhile, the performance of our method is quite robust, consistently outperforming RelGAN under both temperature settings, over all the evaluation metrics. By investigating through the generated real samples, we observe that the generated sentences convey rather diverse semantics and the output consists of considerably long sentences, unlike the conventional adversarial text generators that would shortly fall to the phase of generating short and repeated sentences.
Our models perform the best compared with the baselines. The Gumbel version performs slightly worse than the topk version, but they are generally on par. The margins over the Seq2seq-Attn are not that large (approximately 1+ BLEUs). This is because the capacity of all models are large enough to fit the datasets fairly well. The BOW-Hard model does not perform as well, indicating that the differentiable subset sampling is important for training our discrete latent model. Although not directly comparable, the numbers of RbM models are higher than ours since they are SOTA models on Quora. But they are still not as high as the Cheating BOW’s, which is consistent with our analysis. The cheating BOW outperforms all other models by a large margin with the leaked BOW information in the target sentences. This shows that the Cheating BOW is indeed a meaningful upper bound and the accuracy of the predicted BOW is essential for an effective decoding process. Additionally, we notice that β−VAEs are not as good as the vanilla Seq2seq models. The conjecture is that it is difficult to find a good balance between the latent code and the generative model. In comparison, our model directly grounds the meaning of the latent variable to be the bag of words from target sentences. In the next section, we show this approach further induces the unsupervised learning of word neighbors and the interpretable generation stages. BOW Prediction Performance and Utilization. The support of the precision/ recall correspond the to number of predicted/ target modes respectively in the left figure. We notice that the decoder heavily utilizes the predicted words since more than 50% of the decoder’s word choices come from the BOW. If the encoder can be accurate about the prediction, the decoder’s search space would be more effectively restricted to the target space. However, although not being perfect, the additional information from the encoder still provides meaningful guidance, and improves the decoder’s overall performance.
Empirically, the distributions of arguments (Args) and non-arguments (NonArgs) vary largely in quantity. and we find the proportion of Args and NonArgs is 1:13 in the original dataset. After replacing the semantic relationship boundary (both left and right) with our new tags and removing all other NonArg labels, the proportion reaches nearly 1:1. Note that the above operation is only conducted to intuitively show the difference by imitating the enhanced searching guidance with new tags. Actually we only modify the boundary labels of semantic relationships and use them to signal the model where to restrict a search. Without this inference restraint, most argument candidates are irrelevant and far away from the current predicate, inevitably interfering with the informative features from the truly relevant ones in the very small minority and, hence, leading to an unsatisfactory performance.
Without auxiliary tags, the performance drops dramatically, which confirms the soundness of the motivation for argument boundary indicators from empirical perspective. The reason might be that our proposed argument boundary indicators could help the labeler focus on the potential true candidates and ignore those words too far away from the predicates which are hardly supposed to be ground-truth arguments. Removing the self-attention module also results in performance decline, the advance might be because the self-attention mechanism could help the model to distill vital information and alleviate the error propagation. Noting that the work Zhao et al.
Our approach can also be used with document-level metrics that are not intended to be used with individual sentences. Document-TER MRT improves over a strong baseline, although batching scheme has less of an impact here. Notably seq-level MRT does not improve TER over the baseline, indicating TER may be too noisy a metric for use at the sentence level.
Finally, we apply our MRT approach to the GEC GLEU metric Napoles et al. MLE and sequence-MRT improve recall at a detriment to precision, suggesting over-generation of spurious corrections. Document-MRT likewise improves recall, but with a precision score closer to the baseline for more balanced performance. There is clear indication of a tension between M2 and GLEU: a small increase in GLEU under doc-MRT on CONLL leads to a large increase in M2, while a large increase in GLEU under doc-MRT on JFLEG leads to a small decrease in M2.
To test whether RQ and factual/info-seeking questions are easily distinguishable, we randomly select a sample of 1,020 questions from our forums RQ corpus, and balance them with the same number of questions from fact corpus. We divide the question data into 80% train and 20% test, and use an SVM classifier Pedregosa et al. Mikolov et al. We perform a grid-search on our training set using 3-fold cross-validation for parameter tuning, and report results on our test set.
It can be seen that, on average, the proposed model outperforms all baseline models according to all metrics. The pyramid architecture (pBLSTM) improves the performance of the encoder, since it captures global and local dependencies in the latent representation space. This results in average correlations of ρ=0.89 and γ=0.88 with pBLSTM+Attn, which are much higher than the ρ=0.53 and γ=0.52 with BLSTM, and ρ=0.80 and γ=0.79 with BLSTM+Attn model. The influence of attention is observed by comparing BLSTM or pBLSTM performance with their attention counterparts. For instance, the RMSE⋆ drops from 0.96 for the BLSTM to 0.74 for the BLSTM+Att. pBLSTM+Attn reduces the MAE from 0.79 to 0.51 and increases the PCC from 0.56 to 0.89, due to the incorporation of an attention layer. These results further confirm the effectiveness of the attention module. A statistical significance test indicates these results are statistically significant (p-value <0.0001).
While none of the systems approach human performance on any of the categories, they do illustrate some particular difficulties of the ARC Challenge Set that motivate our future work. Specifically, all techniques perform at or below chance on questions that primarily require qn logic reasoning. Unremarkable performance on the popular basic facts and causes knowledge types also illustrates the shortcomings of sophisticated language processing systems that still rely on basic text retrieval. Bridging this gap appears to be a requirement for making progress on this and similar datasets.
Thus, we mainly analyze DLI’s impact on slot filling task and the prime metric is the F1 score. ’s input during each time step. The more the contextual model is dependent on h, the more obvious the improvement of the DLI task is. Comparing the performance of MemNet with SDEN† on these two datasets, we can find that our SDEN† is stronger than MemNet after the dialogue length increased. Finally, we can see that improvements on KVRET* are higher than KVRET. This is because retrieving context knowledge from long-distance memory is challenging and our proposed DLI can help to consolidate the context memory and improve memory retrieval ability significantly in such a situation.
From the results, we have the following observations: (1) Our approach is comparable to the state-of-the-art result on the benchmark dataset CoNLL03. (2) Our approach outperforms existing methods for open-domain NER , increasing average F1 score by 4.66% and 3.07% with BERT base and large respectively.
Then the output pitch was compared to the input pitch using the normalized cross correlation (NCC) which would give a score between 0 and 1. The higher the score is, the better the output pitch matches the input pitch. We conducted the evaluation on USVC (our) and PitchNet. The evaluated automatic scores on conversion and reconstruction tasks are shown in Tab. Our method performed better both on conversion and reconstruction. The scores of reconstruction are higher than conversion since both models were trained using a reconstruction loss. However, the score of our method on conversion is even higher than the score of USVC (Our) on reconstruction.
Mean Opinion Score (MOS) was used as a subjective metric to evaluate the quality of the converted audio. Two questions were asked: (1) what is the quality of the audio? (naturalness) (2) How well does the converted version match the original? (similarity) A score of 1-5 would be given to answer the questions. The evaluation was conducted on USVC (Our) and PitchNet. As shown by Tab. Our implementation of USVC performed slightly lower than the original author’s because we cannot fully reproduce the results of them.
Data We obtain data from the TwiSty corpus Verhoeven et al. All datasets contain manually annotated gender information. To simplify interpretation for the cross-language experiments, we balance gender in all datasets by downsampling to the minority class. We use 200 tweets per user, as done by previous work We leave the data untokenized to exclude any language-dependent processing, because original tokenization could preserve some signal. Apart from mapping usernames to ‘USER’ and urls to ‘URL’ we do not perform any further data pre-processing. The closeness effect for Portuguese and Spanish can also be observed in language-to-language experiments, where scores for ES↦PT and PT↦ES are the highest. First of all, our results indicate that in-language performance of humans is 70.5%, which is quite in line with the findings of Flekova et al. Within language, lexicalized models are superior to humans if exposed to enough information (200 tweets setup). One explanation for this might lie in an observation by Flekova et al.
Within language, the lexical features unsurprisingly work the best, achieving an average accuracy of 80.5% over all languages. The abstract features lose some information and score on average 11.8% lower, still beating the majority baseline (50%) by a large margin (68.7%). If we go across language, the lexical approaches break down (overall to 53.7% for Lex Avg/56.3% for All), except for Portuguese and Spanish, thanks to their similarities (see The closely-related-language effect is also observed when training on all languages, as scores go up when the classifier has access to the related language. The same holds for the multilingual embeddings model. On average it reaches an accuracy of 59.8%.
From the table, one can observe that the proposed model Soft-Masked BERT significantly outperforms the baseline methods on both datasets. Particularly, on News Title, Soft-Masked BERT performs much better than the baselines in terms of all measures. The best results for recall of correction level on the News Title dataset are greater than 54%, which means more than 54% errors will be found and correction level precision are better than 55%.
One can find that the best result is obtained for Soft-Masked BERT when the size is 5 million, indicating that the more training data is utilized the higher performance can be achieved. One can also observe that Soft-Masked BERT is consistently superior to BERT-Finetune.
We carried out ablation study on Soft-Masked BERT on both datasets. (We omit the results on SIGHAN due to space limitation, which have similar trends.) In Soft-Masked BERT-R, the residual connection in the model is removed. In Hard-Masked BERT, if the error probability given by the detection network exceeds a threshold (0.95, 0.9, 07), then the embedding of the current character is set to the embedding of the [MASK] token, otherwise the embedding remains unchanged. In Rand-Masked BERT, the error probability is randomized with a value between 0 and 1. We can see that all the major components of Soft-Masked BERT are necessary for achieving high performance. We also tried ‘BERT-Finetune + Force’, whose performance can be viewed as an upper bound. In the method, we let BERT-Finetune to only make prediction at the position where there is an error and select a character from the rest of the candidate list. The result indicates that there is still large room for Soft-Masked BERT to make improvement.
It can be noticed that GSGW gives in general a lower performance, except for speaker SB where it outperforms other approaches. PC generally achieves high detection rates, except for speakers SB and SLT. Although RPS leads to a perfect polarity determination in 7 out of the 10 corpora, it may for some voices (KSP and SB) be clearly outperformed by other techniques. As for the proposed OMPD method, it works perfectly for 8 of the 10 databases and gives an acceptable performance for the two remaining datasets. In average, over the 10 speech corpora, it turns out that OMPD clearly carries out the best results with a total error rate of 0.15%, against 0.64% for PC, 0.98% for RPS and 3.59% for GSGW.
The corpus was annotated for dialog acts using the SWBD-DAMSL tag set, which was structured so that the annotators were able to label the conversations from transcriptions alone. Including combinations, there were 220 unique tags in the annotated segments. However, in order to obtain a higher inter-annotator agreement and higher example frequencies per class, a less fine-grained set of 44 tags was devised. The set can be reduced to 43 or 42 categories \shortciteStolcke2000,Rotaru2002,Gamback2011, if the Abandoned and Uninterpretable categories are merged, and depending on how the Segment category, used when the current segment is the continuation of the previous one by the same speaker, is treated. By analyzing the data, we came to the conclusion that merging segments labeled as Segment with the previous segment by the same speaker is an adequate approach, since some of the attributed labels only make sense when the segments are merged. Also, it makes sense to merge the Abandoned and Uninterpretable categories, because both represent disruptions in the dialog flow, which interfere with the typical dialog act sequence. There is also a 41-category variant of the tag set \shortciteWebb2010, which merges the Statement-Opinion and Statement-Non-Opinion categories, making this the most frequent class, covering 49% of the corpus. \shortciteAJurafsky1997 report an average pairwise Kappa \shortciteCarletta1996 of 0.80, while \shortciteAStolcke2000 refer to an inter-annotator agreement of 84%, which is the average pairwise percent agreement when considering 42 categories.
In the first block we can see the results achieved using word-level approaches. These results were achieved using the dependency-based embeddings, which confirms that the segment structure information included in the representation is relevant for the task. However, this is explainable by the simple use of the pre-trained embeddings for each word, without replicating the embedding approach. they surpassed those of every word-level approach except BERT. This is in line with the findings at the character-level, which have shown the importance of morphological information for the task. We can see that the best results on both corpora were achieved by the combination of the character-level approach with the best word-level approach. This is in line with the conclusions of our previous studies \shortciteRibeiro2018,Ribeiro2019, which have shown that the word- and character-level are able to capture complementary information. Still, it is interesting to observe that this holds even when using contextualized embeddings, which are able to capture more information concerning the function of the word than the uncontextualized embeddings. On the other hand, the combination of the word- and functional-level approaches was not able to improve the results of the best word-level approach on its own. Thus, we did not explore the combination of the three levels.
This influence decrease with the distance is consistent with the findings of previous studies \shortciteRibeiro2015,Liu2017. This is due to the use of BERT embeddings and the character-level approach, which enables the disambiguation of certain classes that could only be disambiguated using context in those studies. the results are still above the scenario with context information from one preceding segment. Finally, the results achieved using a summary of the classifications of just three preceding segments were in line or above those of the achieved using the flat approach, but never above those achieved using the summary of the whole dialog history. This confirms that there is relevant information on more distant segments, which the summary is able to capture.
We can see that this information is much less relevant than that provided by the classifications of the surrounding segments. Overall, the highest improvement is achieved when considering turn-taking information from the three preceding turns. When only the first preceding segment was considered, as in the study by \shortciteALiu2017, that is, whether the speaker of the current segment changed in relation to the previous segment, the improvement was not significant. Furthermore, since turn-taking information is represented as a sequence of flags, its summary is not able to provide additional information. However, since the summary of the turn-taking information of just three preceding segments achieved better results than that of the whole dialog history, we can assume that only the recent turn-taking information is relevant for the task.
We report the results of the approach which considers context information from the preceding segments only, as well as of that which also considers future information. While the first simulates a scenario in which a dialog system interacts with its conversational partner, the latter simulates a posthumous annotation scenario. Furthermore, for the first scenario, we report the results achieved using the gold standard annotations of the preceding segments, which reveal the upper bound of the performance, as well as using the classifications predicted by the classifier itself, which provide an estimation of the performance in a real scenario.
The ASR results obtained on the development and test sets of the FAME! The number of Frisian and Dutch words in the development and test sets are given in the upper panel. The baseline ASR trained only on the available CS training data of 11.5 hours provides a total WER of 37.8%. Due to the increase training data due to automatic annotation strategies, the total WER reduces to 32.7%. For both the baseline ASR and ASR_AA, the data augmentation is applied during training as only in-domain training data is used in these cases.
We chose the official metrics adopted by the individual datasets to evaluate the performance of our baseline model. As can be seen in the table, the results are quite poor, significantly below single-dataset state-of-the-art on all datasets. The training of our initial baseline appears to be dominated by SQuAD 1.1, or perhaps SQuAD 1.1 mainly tests reasoning that is common to all of the other datasets. Significant research is required to build reading systems and develop training regimes that are general enough to handle multiple reading comprehension datasets at the same time, even when all of the datasets are seen at training time.
We chose the official metrics adopted by the individual datasets to evaluate the performance of our baseline model. As can be seen in the table, the results are quite poor, significantly below single-dataset state-of-the-art on all datasets. The training of our initial baseline appears to be dominated by SQuAD 1.1, or perhaps SQuAD 1.1 mainly tests reasoning that is common to all of the other datasets. Significant research is required to build reading systems and develop training regimes that are general enough to handle multiple reading comprehension datasets at the same time, even when all of the datasets are seen at training time.
The More Wrong Choice augmentation is omitted since a high enough quality and/or yield of questions could not be ensured for any of the datasets. When evaluated on out-of-domain linguistic structures, performance drops significantly for some augmentation-dataset pairs but only marginally for others. For questions generated by the Invert Choice augmentation, the model struggles to grasp the correct reasoning behind two answer options like Art Euphoric or Trescott Street and changes the prediction when the choices are flipped. However, relative to the dev set performances on the original datasets, the performance drop is almost nonexistent. For the SEARs based augmentation the generated linguistic variations are close to in-domain syntactic structure so we do not see much performance drop in most of the datasets except for ROPES and NewsQA. The Implication style questions create a large performance drop for NewsQA and SQuAD while having a performance boost for DuoRC. Finally, the No-Ans type questions have the worst performance across board for all datasets.
S4SS1SSS0Px4 Results on UD Treebanks. We evaluate on 7 different languages from the UD Treebanks: 4 major ones: English (en), German (de), French (fr), and Italian (it), and 3 relatively minor ones: Bulgarian (bg), Catalan (ca), and Romanian (ro). We refer to the results of our run of the code released by Xuezhe18 as StackPtr (code). our models are trained in identical settings making them comparable. H-PtrNet-PST (Gate) (Eq. (Eq. Element wise product in Eq. With gating mechanism, our model shows consistent improvements against the baseline on bg, en, de, fr, it and ro. We also tested H-PtrNet-PS on these 7 languages, but the performances are worse than StackPtr.
StackPtr (paper) refer to the results reported by Xuezhe18, and StackPtr (code) is our run of their code in identical settings as ours. Our model H-PtrNet-PST (Gate) outperforms the baseline by 0.09 and 0.08 in terms of UAS and LAS, respectively. Performance of H-PtrNet-PST (SGate) is close to that of H-PtrNet-PST (Gate), though we see slight improvement. We also test H-PtrNet-PS (Gate), the model with parent and sibling connections only, which further improves the performance to 96.09 and 95.03 in UAS and LAS.
In discourse parsing, the number of EDUs in a sentence is relatively small compared to the sentence lengths (in words) in dependency parsing. Based on the observation in dependency parsing that the performance of H-PtrNet may drop for longer sentences due to parent error accumulation, we expect that in discourse parsing, this should not be the case since the the number of parsing steps is much smaller compared to that of dependency parsing.
Compared to BN-DNN (which does not use out-of-domain data), the three systems all reduce all four objective measures, except that BN-DNN-VB slightly increases V/UV error. Comparing BN-DNN-VB with BN-DNN-MFC, we see that using data at a lower-sampling rate and simpler acoustic features (MFCCs instead of vocoder parameters) has no effect on the objective measures. Even when using a speech recognition database, containing speakers of a difference accent, to train the bottleneck network (BN-DNN-WSJ), we still get lower distortions than for BN-DNN. We next compared the objective error of the DNN and BN-DNN architectures with and without MGE training. Compared to the DNN system without MGE training, MGE-DNN reduces MCD and F0 RMSE from 4.19 dB and 9.13 Hz to 4.12 dB and 8.93 Hz, respectively. In comparison with BN-DNN, both MCD and F0 RMSE measures for MGE-BN-DNN are reduced from 4.00 dB and 8.90 Hz to 3.97 dB and 8.89 Hz, respectively. The distortion reduction for BN-DNN is less than for DNN, and we think this is because BN-DNN already includes contextual constraints via the stacked bottleneck features at the input, and that these already improve the output trajectories. The performance of all the systems was optimised on the development set.
The baselines are the joint input-label models from Nam et al. WSABIE+: This model is an extension of the original WSABIE model by Weston et al. AiTextML : This model is the one proposed by Nam et al. In addition, we report scores of a word-level attention neural network (WAN) with Dense encoder and attention followed by a sigmoid output layer, trained with binary cross-entropy loss. Our model replaces WAN’s output layer with a generalized input-label embedding layer and its variations, noted GILE-WAN. Note that the AiTextML parameter space is huge and makes learning difficult for our models (linear wrt. labels and documents).
Since combining supervised fine-tuning and RL fine-tuning gives the best ROUGE scores and and is also more abstractive, why not use it? : it makes it easy for the model to tell the truth. The models that copy the most, 60k RL fine-tuned, is 90% and 95% accurate on TL;DR and CNN/Daily Mail; lifting whole sentences from the article usually leaves them true. The supervised fine-tuned and combined supervised+RL fine-tuned models are accurate at most 70% of the time: they paraphrase but paraphrase badly, often swapping names from the context or mixing together multiple sentences in invalid ways. Zero-shot is the most novel, but is accurate only 20% of the time. Similarly, Kryściński et al. Evaluation of a summary is both subjective and multidimensional. A single human labeler may have a clear notion of whether a given sample is separately accurate, grammatical, nonredundant, or covers all important topics; but in our experiments a labeler will often be asked to choose between samples each of which has some deficiencies. In choosing which of four samples is the best, a labeler must trade off between different desiderata. This makes consistent labeling difficult for honest labelers (including the authors!), and makes it difficult to quickly detect problematic labelers. It also makes the research more difficult to present and interpret: during our experiments we routinely checked the performance of models by having authors label results, since we knew the authors would attempt to do the task honestly, but were epistemically uneasy about reporting these numbers in the paper
Uncertainty-aware Beam Search Sensitivity analysis on β indicates that our method can improve the model performance in a certain range of β values.
For our T-ESIM implementation, we use text-based similarly technique to identify relevant dialogs, similar to K-ESIM. Each dialog in the training data is split at multiple points to create a larger pool of dialogs, which are called sub-dialogs. The sub-dialogs are then converted to TF-IDF vector representations. The core motivation here is that the model can learn to use responses for similar dialogs present in the training data, to get improved performance on the next utterance selection task. We also explore additional training strategies: T-ESIM-Sampled and T-ESIM -CR, which are described below. Our results for the baseline model ESIM and our proposed models: The models are evaluated on two metrics - Recall@k, which refers to recall at position k in the set of the 100 candidates and MRR (mean reciprocal rank).
The model shows that the log-odds of the VGG7-11 networks correctly predicting a stimulus’ label are significantly reduced as the stimulus’ dot set ratio becomes more balanced. We found no significant effect for either of our control variables (absolute difference and total number). These findings strongly support Hypothesis 1. Holding all other variables constant, VGG7-11 were significantly less likely to predict the correct label of scattered random images than scattered pairs images. Given that the lack of difference between the images types that could not be included in the analysis appears to be due to ceiling effects, we interpret these findings as supporting Hypothesis 2. Holding all other variables constant, VGG7 was significantly less likely than VGG11 to make a correct classification. No difference was found between VGG9 and 11. Again, as the lack of difference between the VGG9+ networks appears to be best explained by response invariance due to ceiling effects, we cautiously interpret these findings as supporting Hypothesis 3. Finally, we found a significant positive interaction between dot ratio and VGG7. Together with the negative coefficient for VGG7, the result is that the predicted log-odds for a correct prediction by VGG7 are robustly lower across ratios than for VGG9 and VGG11, as expected. The positive interaction term means that the log-odds decrease at a slower rate for more balanced ratios for VGG7 than the other two; this is due to the at-or-near-ceiling performance of the other two at many of the less-balanced ratios.
According to the model, the log odds of a RAM network correctly labelling stimuli is significantly reduced as set ratios become more balanced. We also found a small but significant effect of total dots, indicating that the likelihood of a correct prediction increases with total dots. This is unsurprising, as increasing total dots reduces image sparseness, increasing the odds that glimpses will contain dots. This can be especially important for the initial glimpse, which has a random location. This does not invalidate the dot ratio finding, given their comparative effect sizes. No significant effect was found for absolute difference. These findings support Hypothesis 1. The log odds of a RAM network predicting the correct labels for column pairs mixed, scattered pairs or scattered random images was significantly lower (by varying degrees) than for column sorted pairs images. This strongly supports Hypothesis 2. We found no significant difference in the likelihood of the 4-16 glimpse RAM networks correctly labelling stimuli than their comparison class, the 24 glimpse RAM network. These findings do not support Hypothesis 3. Finally, we found a small but significant positive interaction between dot ratio and RAM8, suggesting that the increase in log-odds of correct prediction per unit increase in dot ratio is stronger for RAM8 than for RAM24. Because the effect size is small, we caution against over-interpreting this result. And, as before, this effect is somewhat offset by a negative coefficient for RAM8, lowering the intercept in this case.
For both networks, both column image types have a significantly higher degree of accuracy than both scattered types, with scattered pairs being a bit easier than scattered random. The psychophysical model provides a good fit to the data: For human participants, Pietroski et al. the column sorted trials, where w was 0.04. Our models are not too far off of these Weber fractions, with one noticeable difference: our models treat column mixed trials much more similarly to column sorted trials, whereas for humans column mixed trials pattern with the two scattered trial types.
The double embedding mechanism improves the performance and in-domain embeddings are important. We can see that using general embeddings (GloVe-CNN) or domain embeddings (Domain-CNN) alone gives inferior performance. We further notice that the performance on Laptops and Restaurant domains are quite different. Laptops has many domain-specific aspects, such as “adapter”. So the domain embeddings for Laptops are better than the general embeddings. The Restaurant domain has many very general aspects like “staff”, “service” that do not deviate much from their general meanings. So general embeddings are not bad. Max pooling is a bad operation as indicated by MaxPool-DE-CNN since the max pooling operation loses word positions. DE-OOD-CNN’s performance is poor, indicating that making the training corpus of domain embeddings to be exactly in-domain is important. DE-Google-CNN uses a much smaller training corpus for general embeddings, leading to poorer performance than that of DE-CNN. Surprisingly, we notice that the CRF layer (DE-CNN-CRF) does not help. In fact, the CRF layer can improve 1-2% when the laptop’s performance is about 75%. But it doesn’t contribute much when laptop’s performance is above 80%. CRF is good at modeling label dependences (e.g., label I must be after B), but many aspects are just single words and the major types of errors (mentioned later) do not fall in what CRF can solve. Note that we did not tune the hyperparameters of DE-CNN-CRF for practical purpose because training the CRF layer is extremely slow.
18.8%, 32.4%, and 8.2% relative WER reductions are achieved in 2-stream DIRHA, 3-stream DIRHA, and 2-stream AMI, respectively. Note that AMI experients were conducted using VGGBLSTM with 2-layer BLSTM layers without any close-talk recordings and data perturbations. It is worth mentioning that those reductions in WERs were accomplished while simultaneously significantly decreasing the number of unique parameters in training by avoiding costly multiples of the large encoder component (10 million parameters per stream, in this case). For fair comparison, single-level and word-level fusion models utilized Stage-1 pre-trained models as their initialization. Note that word-level fusion operates on decoding results from pretrained single-stream from Stage-1. Still, our proposed strategy consistently perform better than all other fusion methods in all conditions.
The two-stage training strategy provides various opportunities for data augmentation. Stage-1 does not consider parallel data, so any augmentation technique for regular E2E ASR could be applied in this stage to improve the robustness of the UFE. Stage-2 augmentation, on the other hand, would be expected to improve robustness of the combination of corrupted individual streams. The best performance was from data augmentation on Stage-1 when freezing all Stage-1 pretrained components. With additional Stage-2 SpecAugment, there was not a noticeable difference in terms of WERs ( 22.6% v.s. 22.4% and 22.6% v.s. 22.5%). 10% absolute WER reduction was achived in AMI with two stage augmentation. However, it is important to remember that, while the performance gap from fine-tuning versus freezing pre-trained components is narrowed with Stage-2 augmentation, the reductions in Stage-2 memory and computation requirements are still substantially better with frozen parameters.
In the first stage of iterative merging, using two distinct WD and CD classifiers for corresponding WD and CD merges yields clear improvements for both WD and CD event coreference resolution tasks, compared with using one common classifier for both types of merges. In addition, the second stage of iterative merging further improves both WD and CD event coreference resolution performance stably by leveraging second order event inter-dependencies. The improvements are consistent when measured using various coreference resolution evaluation metrics.
The LSTM baseline establishes a “gold standard” representing a model trained directly on the observed discrete sequence with the same T conditioning as the proposed model. A recent state-of-the-art model (AWS-LSTM) with additional orthogonal improvements is also shown for context. In terms of absolute NLL score, AF / AF nearly matches the LSTM baseline, whereas AF / SCF is within 0.05 of the LSTM baseline. These results demonstrate that the combination of AF-in-hidden and the NLSq scalar invertible function induce enough multimodality in the continous distribution to model the discrete data. The AF-only “unigram” model removes the relationships across time in the prior model, effectively dropping the time-dynamics.
Without either the NLSq function or the AF-in-hidden dependencies the performance degrades. Once AF-in-hidden is removed, however, further removing NLSq appears to make only a small difference in terms of NLL. These results provide further evidence to our hypothesis that modeling discrete data requires a high degree of multimodality. Furthermore, standard normalizing flows without these additions do not achieve the required flexibility.
In all experiments, we set the weak feature to be the presence of the symbol 2 anywhere in the input. These features are chosen with the intent of varying how difficult the strong feature is to detect given the raw sequential input. In all experiments, we design train and test splits such that the symbols which are used to instantiate the strong feature during training are never used to instantiate the strong feature during testing. For example, for experiments using adjacent duplicate, if the model sees the string 1 4 3 3 15 at test time, we enforce that it never saw any string with the duplicate 3 3 during training. This is to ensure that we are measuring whether the model learned the desired pattern, and did not simply memorize bigrams. To quantify the difficulty of representing each strong feature, we train the model for the task of predicting directly whether or not the feature holds for each of our candidate feature, using a set of 200K training examples evenly split between cases when the feature does and does not hold. We see the desired gradation in which some feature require significantly more training to learn than others. As a heuristic measure of “hardness”, we use the approximate area under this flat-lined loss curve (AUC), computed by taking the sum of the errors across all epochs. Note that the weak feature (whether the sequence contains 2) is exactly as hard as contains 1.
The overall accuracy was 0.618 and the top-2 accuracy was 0.786. The performance varies considerably per dataset. This is because the quality of questions differs across datasets. Quality has various dimensions, such as complexity or expressiveness. Template 2 is over-represented compared to other templates, with some templates such as template 5 and template 11 having only 1 example. But the top 3 templates (by number of examples), which comprises 84% of the dataset, have a high top-2 accuracy, which shows reasonable generalization power for the template classification model. That is, while templates that were very similar to each other, such as template 1 and 2 (simple queries), tended to have a higher chance of misclassification between one another, they did not misclassify with template 151 (boolean query).
deep-coref is a neural model that combines the input features through several hidden layers. deep-coref includes the embeddings of the dependency governor of mentions. Combined with the relative position of a mention to its governor, deep-coref may be able to implicitly capture selectional preferences to some extent. −gov As we can see, the exclusion of the governor information does not affect the performance. This result shows that the implicit modeling of selectional preferences does not provide any additional information to the coreference resolver. Providing selectional preference embeddings directly to deep-coref adds more complexity to the baseline coreference resolver. Yet, it performs on-par with +binned sim. on the development set and generalizes worse on the test set. +SP in on the test set. As we can see from the results, adding selectional preferences as binary features improves over the baseline.
When we compare the efficiency of temporal attention (TA), we discard all TA layers and replace them with convolutional layers. To guarantee the fairness of models, we adjust hyperparameters to make the convolutional model have more parameters than TCAN. The initial TCAN has 1 temporal attention block in each of the 4 layers. For comparison, we use convolutional layer to replace TA. A TA layer has similar number of parameters with a convolutional layer. Note that TCAN didn’t use the enhanced residual module, which is in order to not interfere with comparative tests. Note that two models are optimized by Adam and the learning rate is 0.0001. The perplexity of two models shows that the temporal attention layer is more effective than the convolutional layer.
How is “no-op” realized? Here, we show an interesting trend that α and ∥f(x)∥ cancel each other out on specific tokens. assigned to the vectors in each category. α and ∥f(x)∥ corresponding to [CLS], [SEP], periods, and commas are highly inversely correlated. This achieves the “no-op” function of collecting no information from input tokens.
Results. However, we do find that certain attention heads specialize to specific dependency relations, sometimes achieving high accuracy and substantially outperforming the fixed-offset baseline. We also note heads can disagree with standard annotation conventions while still performing syntactic behavior. Such disagreements highlight how these syntactic behaviors in BERT are learned as a by-product of self-supervised training, not by copying a human design.
Results. We find that one of BERT ’s attention heads achieves decent coreference resolution performance, improving by over 10 accuracy points on the string-matching baseline and performing close to the rule-based system.
We find the Attn + GloVe probing classifier substantially outperforms our baselines and achieves a decent UAS of 77, suggesting BERT’s attention maps have a fairly thorough representation of English syntax.
The baseline model is trained with character initialization where the character vectors are trained using quatrains only. Then we use the large corpus that involves all traditional Chinese poems to enhance the character vectors, and the results demonstrated a noticeable performance improvement in fluency (from our human judgements) and a small improvement in BLEU ( This is understandable since poems in different genres use similar languages, so involving more training data helps infer more reliable semantic content for each character. Additionally, we observe that reconstructing the input during model training improves the model (3rd row). This is probably due to the enhancement in theme consistence. What’s more, attention to both input vectors and hidden states leads to additional performance gains (4th row). Finally, the hybrid-style training is employed to train a single model for the 5-char and 7-char quatrains. Note that in the hybrid training, we stop the training before convergence in favor of a good BLEU.
It can be seen that our model outperforms all the comparative approaches in terms of all the four metrics. More interestingly, we find that the scores obtained by our model are approaching to those obtained by human poets, especially with 7-char poems. This is highly encouraging and indicates that our model can imitate human beings to a large extent, at least from the eyes of contemporary experts.
5.3.2 Results on Machine Translation Each interpolation weight becomes a value that shows the best performance on each test set with each method in NMT. The interpolation weights λ were 0.4 or 0.5 in the N-best lists for NMT.
We measure the uncertainty(entropy) in terms of mean and variance for all the answer prediction in the validation dataset. We also measure uncertainty for an individual question in the dataset. Here, we split our training data into three parts. In the first part, the model is trained with 50% of the training data. It is observed that the model uncertainty(epistemic uncertainty) decreases as training data increases.
The results in Tab. In contrast, the Visual model learns embeddings from visual information, which constraints characters that has similar appearance to have similar embeddings. This is an advantage for rare characters, but a disadvantage for high frequency characters because being similar in appearance does not always lead to similar semantics.
Results of different fusion methods can be found in Tab. The results show that late fusion gives the best performance among all the fusion schemes combining the Lookup model and the proposed Visual model. Early fusion achieves small improvements for all languages except Japanese, where it displays a slight drop. Unsurprisingly, fallback fusion performs better than the Lookup model and the Visual model alone, since it directly targets the weakness of the Lookup model (e.g., rare characters) and replaces the results with the Visual model. These results show that simple integration, no matter which schemes we use, is beneficial, demonstrating that both methods are capturing complementary information.
Results. This finding holds at all time steps (i.e., regardless of the sparsity of the map). We also demonstrate that removing the heading θ from the agent’s state in our model degrades this success rate to 39%, demonstrating the importance of relative orientation to instruction understanding. For instance, it is unlikely for an agent following the true path to turn 180 degrees midway through (unless this is commanded by the instruction). Similarly, without knowing heading, the model can represent instructions such as ‘go past the table’ but not ‘go past with the table on your left’. Finally, the poor performance of the handcoded baseline confirms that the goal location cannot be trivially predicted from the trajectory.
We abstracted the questions into templates such as “In how many  , is the < Y_label> of/in   greater than the average < Y_label> of/in   taken over all  ?”. We could then generate multiple questions for each template by replacing X_label, Y_label, legend_label, etc. by indicator variables, years, cities etc. from our curated data. However, this was a tedious task requiring a lot of manual intervention. If we substitute this indicator variable as it is in the above template, it would result in a question, “In how many cities, is the race of the students(%) of Asian greater than the average race of the students (%) of Asian taken over all cities?”, which sounds unnatural. To avoid this, we asked in-house annotators to carefully paraphrase these indicator variables and question templates. The paraphrased version of the above example was “In how many cities, is the percentage of Asian students greater than the average percentage of Asian students taken over all cities?”. Such paraphrasing for every question template and indicator variable required significant manual effort. Using this semi-automated process we generated a total of \textcolorblack28,952,641 questions. This approach of creating questions on real-world plot data with carefully curated question templates followed by manual paraphrasing is a key contribution of our work. The resultant PlotQA dataset is much closer to the real-world challenge of reasoning over plots, significantly improving on existing datasets. Note that (a) the number of unique answers in PlotQA is very large , (b) the questions in PlotQA are much longer, and (c) the vocabulary of PlotQA is more realistic than FigureQA or DVQA.
This indicates that the pipeline model produces more fluent results. Moreover, since the number of word types is greater in the pipeline model, it can output a larger variety of sentences than the end-to-end model.
As we can see from Row 1, the difference between accuracy and f-score indicates that the model performs differently for the three argumentation labels. The f-score for claims, evidence, and explanations are respectively 0.776, 0.565, and 0.164. While specifying class weights at training time helped, this shows that there is ample room for improvement. Additionally, kappa and f-score show that the specificity model is much more accurate than the one for argumentation. Rows 1 and 2 relate to our hypothesis H1, and we can see an improvement in accuracy, kappa and f-score. The performance improvements achieved through the joint model, though, are not yet statistically significant. Although differences exist across argumentation labels for different collaborative moves, our joint model is not able to optimally capture them. This may be due to the low performance of the collaboration classifier: if the collaboration model cannot reliably capture collaboration information, it cannot properly inform the argumentation classifier. We believe that increasing the performance of the individual classifiers and using learned weights in the loss function will result in a more effective joint model. Rows 3 and 4 relate to our hypothesis H2. Like for argumentation, the results on specificity show the joint model outperforming the single-task one in all metrics, though the difference was not statistically significant.
S6SS3SSS0Px4 Importance of conversation history Finally, we examine how important the conversation history is for the dataset. All models succeed at leveraging history but the gains are little beyond one previous turn. As we increase the history size, the performance decreases.
Although the performance of the augmented DrQA is a bit better (0.3 F1 on the testing set) than the combined model, the latter model has the following benefits: 1) We also look closer into the outputs of the two models. Although the combined model is still far from perfect, it does correctly as desired in many examples, e.g., for a counting question, it predicts a rationale current affairs , politics , and culture and generates an answer three; for a question With who?, it predicts a rationale Mary and her husband , Rick and then compresses it into Mary and Rick for improving the fluency; and for a multiple choice question Does this help or hurt their memory of the event? it predicts a rationale this obsession may prevent their brains from remembering and answers hurt. We think there is still great room for improving the combined model and we leave it to future work.
For text and speech embeddings experiments, we observe that using Word2Vec or Speech2Vec representations achieve comparable F1-score performances, which are significantly below the GloVe embeddings performance. That was expected as the pre-trained Speech2Vec vectors have lower vocabulary coverage than the GloVe vectors. On the other hand, we observe that concatenating GloVe + Speech2Vec embeddings, and further GloVe + Word2Vec + Speech2Vec yields higher F1-scores for intent recognition. These results show that the speech embeddings indeed can capture useful semantic information carried by the speech only, which may not exist in plain text.
One can see that adding CRF layer significantly improved prediction. Besides that, using external word embeddings also reduced training time and increased tagging accuracy. Due to absence of lemmatization in our text processing pipeline news embeddings matched only about 15% of words in the corpus, embeddings for other words were just initialized randomly. Therefore, the improvement was not really significant and prediction for Organization type was even lower with news embeddings. To deal with this problem, in the second experiment we decided to use FastText trained on Lenta corpus in order to build an external word embedding. After that, we used this embedding to train on Gareev’s dataset one more time using the same configuration with the previous experiment.
We compare Bi-LSTM + CRF + Bi-LSTM + CRF + Lenta model significantly outperforms other approaches on Gareev’s dataset and Persons-1000. However, the result on FactRuEval 2016 dataset is not as high as we expected.
We increase the speed of our baseline POS tagger by a factor of 5.2x without falling below 97% test accuracy. By tuning our training method to more aggressively prune templates, we achieve speed-ups of over 10x while providing accuracy higher than 96%.
We observed several contextual keywords important for both value judgment and bipolar causality. \enumsentence [The developments in that conflict should not be left to former Cold War opponents alone,]S1[for that course can only lead to escalation in some form.]S2 However, we observed most patterns required world knowledge implicitly assumed by the writers. For the topic Should Germany introduce the death penalty? \enumsentence [One should not re-introduce capital punishment in Germany]S1[Everyone must be given the chance to hone their conscience and possibly make amends for their deed]S2 \enumsentence [Owner-run shops may potentially be overwhelmed by additional work times on Sundays and holidays]S1
In addition to the performance evaluation by F1 or recall scores, an important criterion for generative models is to investigate how many keyphrases are generated, especially when one uses keyphrase sequence as the decoding target. In doing so, we follow chan2019neural to use mean absolute error (MAE) to calculate the difference between the prediction and ground-truth (oracle) keyphrase numbers, where a lower MAE refers to better generation performance. We also list the average number of generated keyphrases to evaluate how close such a number is with respect to the oracle one.
A negative surprise score SG(B|A) indicates (in units of standard deviation) how much lower the proportion of quotes reported by outlets in A that are also cited by outlets by B is than in a hypothetical scenario where quotes are cited at random. For example, the fact that SG(dC|sC) is negative indicates that declared conservative outlets are much less likely to cite quotes reported by suspected conservative outlets than by chance, in spite of their suspected ideological similarity. Furthermore, we observe that declared liberal outlets are actually disproportionately more likely to cite quotes that are also reported by declared conservative outlets, in spite of their declared opposing ideologies.
Results. We leave out 500,000 entries of the outlet-by-quote matrix (out of 14.7 million) and divide them into equal development and test sets. The class distribution is heavily imbalanced, with the positive class (quoting) occurring only about 1.6% of the time. In order to evaluate our model in a binary decision framework, we use Matthew’s correlation coefficient as the principal performance metric. We tune the amount of regularization λ and the cutoff threshold on the development set. The selected model has rank 3. The latent low-rank model significantly outperforms both the quote popularity baseline as well as the baseline including outlet propensity, showing that the choices made by the media when covering political discourse are not solely explained by newsworthiness and available space. The performance of our model is twice that of the baselines in terms of both F1 and Matthew’s correlation coefficient, and three times better in terms of precision, confirming that the latent quoting pattern bias is systematic and structured. Motivated by our results, next we attempt to characterize the dimension of bias with a spectral and linguistic analysis of the latent low-rank embedding.
We perform five different ablations: (a) (b) Without entity embeddings (c) (d) Removing FFNNhead and FFNNtail entirely. (e)
Our method (All) significantly outperforms the baseline (PBMT) on both the dev and test sets. PBMT does not outperform OnlyBigramLM and OnlyInducedRule, demonstrating that our rule induction algorithm is effective. We consider rooted and connected fragments from the AMR graph, and the TSP solver finds better solutions than beam search, as consistent with \newcitezaslavskiy2009phrase. In addition, OnlyInducedRule is significantly better than OnlyConceptRule, showing the importance of induced rules on performance. This also confirms the reason that All outperforms PBMT. This result confirms our expectation that concept rules, which are used for fulfilling the coverage of an input AMR graph in case of OOV, are generally not of high quality. Moreover, All outperforms OnlyBigramLM showing that our maximum entropy model is stronger than a bigram language model. Finally, JAMR-gen outperforms All, while JAMR-gen uses a higher order language model than All (5-gram VS 4-gram).
These numbers reflect, respectively, the percentage of cases in which the correct translation of the English word from the test set was retrieved by the trained translation matrix as the most similar or among the five most similar words in the other language. For example, when we (1) translate the embedding vector of some Spanish word wES using the learned Spanish-English (CBOW → GloVe) translation matrix and (2) rank all words from the English vocabulary (several hundred thousand entries) according to the similarity of their embedding vectors with the translated embedding of wES, we will find the English word wEN that is the dictionary translation of wES within the 5 top-ranked English words in 66% of the cases and as the top-ranked in 48% of the cases. There is a significant performance drop between the monolingual models (EN-EN) and their respective cross-lingual models (EN-ES, EN-IT, and EN-HR), ranging from 5% to 30% (depending on the dataset and the cross-lingual language pair), that clearly shows how much imperfect embedding space translation affects the STS performance, as those performance drops cannot be credited to anything else. The performance for English-Spanish pair is consistently better than for the other two pairs, which more or less exhibit comparable performance.
Evaluation Metrics and Results We measure the performance of different similarity scores with the overall document recall of plagiarized text measured at the character level. This experiment shows the potential of the models to detect cases of plagiarism at the document level, without differentiating between individual plagiarism cases. The character-level recall (R@k) is measured using the top k fragments from the source documents most similar to the fragment of the suspicious text. Our simple unsupervised and resource-light CL STS model performs on par with state-of-the-art similarity scores for cross-lingual plagiarism detection. CL-STS-OptAlign significantly outperforms the CL-VSM and the CWASA (S2Net) models for R@1 and CL-VSM for R@5. Although KBSim similarity numerically outperforms the CL-STS-OptAlign model for all four metrics, the differences are not statistically significant. This findings are very promising as they show that robust detection of cross-lingual plagiarism can be achieved without expensive language-specific resources and tools.
The hyperparameters are chosen to be as close as possible. As we can see, the triad system has a clear advantage over the dyad system. Postprocessing also boosts the scores in either case.
LM results As expected, our model achieves the best performance in all the metrics due to the flexibility of the discrete variational distribution, which makes the model free of posterior collapse. Remarkably, our model runs almost as fast as the standard VAE. The faster convergence of Lag-VAE at the beginning is because it aggressively trains an encoder, where approximately 50× more data are used to train the encoder in one epoch.
Our model outperforms the baselines in most metrics. Although our method obtains a similar BLEU score as WAE-GMP, the inter-dist and intra-dist scores are much higher. In terms of intra-dist, the dist-1 and dist-2 on Switchboard are 19.2% and 24.6% higher than WAE-GMP. This indicates that our model is capable of generating less repeated n-grams in each response. As for the inter-dist, dist-1 and dist-2 are even 66.1% and 45.6% higher than WAE-GMP, meaning that our model generates much more diverse responses than WAE-GMP.
Note the attention mechanism is used in RNNsearch, where each progressed state in the decoder side has direct access to the state in the encoder side. The reason might be that the top-5 latent codes have already encoded most source-target combinations. Besides, the BLEU score is as low as 26.1 when we choose the farthest latent code from the codebook instead. These validate the effectiveness of our proposed top-k inference strategy which applies to most RNN-based autoencoder models.
Our model outperforms the baselines in most metrics. Although our method obtains a similar BLEU score as WAE-GMP, the inter-dist and intra-dist scores are much higher. In terms of intra-dist, the dist-1 and dist-2 on Switchboard are 19.2% and 24.6% higher than WAE-GMP. This indicates that our model is capable of generating less repeated n-grams in each response. As for the inter-dist, dist-1 and dist-2 are even 66.1% and 45.6% higher than WAE-GMP, meaning that our model generates much more diverse responses than WAE-GMP.
The adapted ROUGE-L achieves best performance on correlation to human judgment, both on single yes-no or entity question type and on overall.
We also calculate PCCs between automatic and human metrics on overall score level. Similar to single question level, adapted ROUGE-L still gains the highest correlation to human overall judgment. In this task, we notice that ROUGE is much more effective than BLEU, which may reflect the importance of recall in MRC evaluation. For the comparison between adapted and vanilla metrics, adapted ROUGE-L performs better than vanilla version on every question type. However, our adapted BLEU-4 only works better on evaluating entity answers, which is different from the result on single question level. We think it may be due to the peculiar way BLEU employs to get overall score for multiple questions, which was discussed as the ‘‘decomposability’’ problem of BLEU in \newcitechiang2008decomposability. This issue will be explored in our future work.
Our model showed 6.0 - 8.4% and 5.4 - 14.6% relative improvements on validation and evaluation set, respectively. We observed that our joint CTC-attention achieved the best performance when we use the λ=0.2 on both the noisy CHiME-4 and clean WSJ tasks.
In addition, SAML consistently improves over both the scheduled sampling and differentiable scheduled sampling on all tasks. All improvements are significant with p<0.002. Interestingly, differentiable scheduled sampling performs no better than scheduled sampling in our experiments, unlike in \newciteGoyalDB17. Unlike scheduled sampling, our approach does not require an annealing schedule, and it is therefore simpler to train. We set the sampling rate to 0.5. The contrastive model hurts BLEU scores by at least 4.0 points compared to both the ML baseline and models fine-tuned with scheduled sampling, confirming that scheduled sampling needs the annealing schedule to work well.
The action-based user model, on a more abstract level, would likely be better as it is less sparse, and produces more variation in the resulting utterances.
First, we compare our model with the sequence-to-sequence baseline on the Amazon SNAP test sets. We report the ROUGE F1 score of our model and the baseline models on the test sets. Moreover, given exactly the same annotated data (summary + sentiment label), our HSSC model still has an improvement over the S2S-att + BiLSTM baseline, which indicates that HSSC learns a better representation for summarization. Overall, HSSC achieves the best performance in terms of ROUGE-1, ROUGE-2, and ROUGE-L over the three baseline models on the three test sets.
We compare our model with two popular sentiment classification methods, which are CNN and BiLSTM, on the Amazon SNAP test sets. We report the accuracy of five-grained sentiment and two-class sentiment on the test sets. Therefore, we select BiLSTM as the encoder of our model. HSSC obtains a better performance over the two widely-used baseline models on all of the test sets, mainly because of the benefit of more labeled data and better representation. What’s more, HSSC outperforms the S2S-att + BiLSTM baseline, showing that the information from summary decoder helps to predict the sentiment labels. Overall, HSSC achieves the best performance in terms of 5-class accuracy and 2-class accuracy over the three baseline models on the three test sets.
In order to analyze the effect of each components, We remove the components of multi-view and highway in order, and evaluate the performance of the rest model. We first remove the multi-view attention. It can be concluded that the multi-view attention improves the performance of both abstractive summarization and sentiment classification. We further remove the highway part, and find the highway component benefits not only the sentiment classification, bot also the abstractive summarization. The benefit mainly comes from the fact that the gradient of the sentiment classifier can be directly propagated to the encoder, so that it learns a better representation of the original text for both classification and summarization.
To evaluate the power of the previously described techniques to discriminate between pairs and non-pairs, we calculate two performance metrics: optimal split error and Jensen-Shannon (JS) divergence. We obtain the former using the optimal threshold for which the number of misclassified couples is minimal. The latter is a symmetric measure expressing the similarity between two probability distributions, based on the well-known—but asymmetric—KL divergence. The lower the optimal split error or the higher the JS divergence , the better a technique can distinguish pairs from non-pairs. We implement the following learning procedure. For every couple c in the training set we sort the words in both texts (c1) and (c2) according to their document frequency— i.e. the word with the lowest document frequency comes first—arriving at (c1′) and (c2′). Next we multiply the word embedding vector of each word w1′j and w2′j with an importance factor ij; these importance factors are global weights that will be learned. Finally, we take the mean of these weighed embeddings to obtain a fixed-length vector o1 for (c1) and o2 for (c2): ∀ℓ∈{1,2}:oℓ=1ncnc∑j=1ij⋅wℓ′j. We see that first the words in the sentence are sorted according to their idf-component; next, their 400-dimensional word embedding vectors are multiplied by importance factors, and finally the mean is taken.
We also investigate the influence of the used distance metric. We test cosine distance, Euclidean distance, L3-norm, L4-norm and Bray-Curtis distance, which are normalised between 0 and 1. We use texts of 20 words long and the mean of the embeddings. Euclidean distances perform best in our tests, so we continue to use Euclidean distances hereafter.
The data is balanced evenly between the sarcastic and not-sarcastic classes, and the best F-Measures for each class are shown in bold. The default W2V model, (trained on Google News), gives the best overall F-measure of 0.74 on the Gen corpus for the sarcastic class, while n-grams give the best not-sarcastic F-measure of 0.73. Both of these results are higher F than previously reported for classifying sarcasm in dialogue, and we might expect that feature engineering could yield even greater performance.
Results are provided using both the standard Kaldi s5 and the proposed recipe, in order to highlight all the discrepancies in performance that can be observed in these different experimental settings.
The DIRHA-English corpus can also be used for microphone selection experiments. It would be thus of interest to provide some lower and upper bound performance for a microphone selection technique applied to this data set.
The level of parallel content between the data used to train the two sets of embeddings is thus far more limited in this case, and the de embeddings are not explicitly trained on Wikipedia data. while with the new embeddings performance is somewhat reduced for nouns, verbs and adjectives/adverbs, precision at 1 for proper nouns, in particular, drops by over 50%, indicating that this category of test word pairs is indeed highly sensitive to the nature of the training data.
Based on the analysis presented above, we removed all pairs that were annotated as proper nouns and all pairs that were marked as invalid during the annotation process. This clean-up resulted in a drop in the size of the test dictionaries of about 25% on average. The results are reported in terms of change in performance relative to MUSE-S (chosen as a baseline) as estimated on the original MUSE data (pattern-filled bars) and on the cleaned version of the data (colored bars).
As shown in the table, LadaBERT consistently outperforms the strongest baselines under similar model sizes. In addition, the performance of LadaBERT demonstrates the superiority of hybrid combination of SVD-based matrix factorization, weight pruning and knowledge distillation.
We include three NAT works as our competitors, the NAT with fertility (NAT-FT) Gu et al. (NAT-IR) Lee et al. For all our tasks, we obtain the baseline performance by either directly using the performance figures reported in the previous works if they are available or producing them by using the open source implementation of baseline algorithms on our datasets.
We further report the mean macro-averaged precision, recall, and F1 for each method (‘Overall’) to investigate their overall performance on the data. lr + gcn significantly (p<0.05 on paired t-test) outperforms all other methods. The author profiles from node2vec only capture the structural and community information of the authors; however, those from the gcn also take into account the (abusive) nature of the tweets composed by the authors. As a result, tweets like “#MKR #mkr2015 Who is gonna win the peoples choice?” that are misclassified as sexist by lr + auth (because their author is surrounded by others producing sexist tweets) are correctly classified as clean by lr + gcn.
In this paper, we propose a general-purpose evaluation framework that detects unintended biases in NLP models around named entities mentioned in text. Our method does not rely on any annotated corpora, and we focus solely on application-independent sensitivity of models, which does not clearly fall under individual- or group- based fairness criteria. Our core idea is based on the assumption that an NLP system designed to be widely applicable should ideally produce scores that are independent of the identities of named entities mentioned in the text. For instance, the sentences I hate Justin Timberlake and I hate Rihanna both express the same semantics using identical constructions; however, the toxicity model used in our experiments gives a significantly higher score to the former (0.90) than the latter (0.69)
Both models exhibit significant sensitivity towards name perturbation across all 4 corpora. On average, sentences subjected to name perturbation resulted in a wide range of scores; i.e., ScoreRange over 0.10 for toxicity, and 0.36-0.42 for sentiment. Similarly, ScoreDev values for the sentiment model is also higher (over 0.07 across board) compared to that of the toxicity model (around 0.02), suggesting that the sentiment model is much more sensitive to the named entities present in text than the toxicity model. We also observe that perturbation sensitivity is a function of the target corpus; comments on public figures had a much larger ScoreDev and ScoreRange for both tasks.
Next, we evaluate the accuracy and vocabulary diversity of the model as dropout is added in inference. We train two versions of the GRU model, once with no training dropout (dt=0) and once with dt=0.2. For each model, we generate captions for the validation set, using evaluation dropouts de=[0.0,0.2,0.4,0.6,0.8].
Gutenberg-Richter law. The energy E released during voice events is a direct measure of the vocal fold response function under air pressure perturbations, and its distribution PΘ(E) could in principle depend both on the threshold Θ and on the language under study. As increasing Θ induces a flow in RG space, systems which lie close to a critical point (unstable fixed point in RG space) show scale invariance under Θ and hence the distributions can be collapsed into a Θ-independent shape, thereby eliminating the trivial dependence on Θ. This has been shown to be the case for human voice and accordingly (technical details can be found in the SI) we can express the collapsed energy distribution as P(E)=E−ϕF(E/Eξ) for E>>El, where El is the lower limit beyond which this law is fulfilled, F is a scaling function and the relevant variable is ϕ, the scaling exponent. In order to collapse every curve PΘ(E) the theory predicts to rescale E→E⟨E⟩/⟨E2⟩, PΘ(E)→PΘ(E)⟨E2⟩2/⟨E⟩3. In what follows we explore the emergence of classical linguistic laws in these acoustic signals. Here for convenience we make use of the former and explore NΘ(n) applied to the statistics of types. Again NΘ(n) could in principle depend on the threshold but assuming that the signal complies with the scale-invariance mentioned above, one can collapse all threshold-dependent curves into a universal shape and thus remove any dependence on this parameter by rescaling n→n/LV, NΘ(n)→NΘ(n)VL where V is the total number of different types present in the signal and L as the total number of tokens (see SI for technical details). Null models systematically deviate from these results, and neither display the characteristic power law decay nor any invariance under variation of the energy threshold (SI).
We have collected the assessment corpora from 21 Wikipedia categories, from English (EN) and French (FR) languages. It originally consists of 154828 documents in total with 87793 English documents and 67035 French documents categorized in 21 categories, taken from existing Wikipedia categories. Since such corpus is thematically very large, corresponding similarity and comparability matrices are basically very sparse. To avoid the algorithmic complexity behind the calculation of the induces similarity matrices (O(N3)), we proceeded as follows which drastically reduces the sparsity of our matrices: For each class and each language, we evaluate firstly the intra-language similarity matrices, using a cosine similarity based on a tf−idf weighting, secondly, we prune these intra-language similarity matrices using a threshold (typically 0.5) and order the documents according to their number of remaining neighbors (with whom they share a similarity above the threshold). by keeping for each language the best hundred documents, we get a refined comparable bilingual corpus. Finally, to complexify the experiment, we enrich this corpus by adding, for each language, and for each class, 50% of the initial number of documents. These added documents are randomly drawn from the initial 21 Wikipedia categories. Each Wikipedia article is then represented by its plain textual content. tags and hyperlink have thus been removed.
In this section, we examine the results of our experiments on three collections. Despite being simple, retrieval methods are strong baselines in a code setting. Code repetition and similar patterns, such as in Hearthstone, lead to high sequence similarity despite only being able to retrieve code from the training set. We test an oracle method by taking the highest scoring retrieved snippet according to BLEU, setting an upper bound on the effectiveness of these methods.
Often, large amounts of training data are only available out of domain, but we still seek to have robust performance. To test how well NMT and SMT hold up, we trained five different systems using different corpora obtained from OPUS An additional system was trained on all the training data. Note that these domains are quite distant from each other, much more so than, say, Europarl, TED Talks, News Commentary, and Global Voices.
Both SMT and NMT systems actually have their worst performance on words that were observed a single time in the training corpus, dropping to 48.6% and 52.2%, respectively; even worse than for unobserved words. The most common categories across both are named entity (including entity and location names) and nouns. The named entities can often be passed through unchanged (for example, the surname “Elabdellaoui” is broken into “E@@ lab@@ d@@ ell@@ a@@ oui” by the byte-pair encoding and is correctly passed through unchanged by both the NMT and SMT systems). Many of the nouns are compound nouns; when these are correctly translated, it may be attributed to compound-splitting (SMT) or byte-pair encoding (NMT). The factored SMT system also has access to the stemmed form of words, which can also play a similar role to byte-pair encoding in enabling translation of unobserved inflected forms (e.g. adjectives, verbs). Unsurprisingly, there are many numbers that were unobserved in the training data; these tend to be translated correctly (with occasional errors due to formatting of commas and periods, resolvable by post-processing).
We test our methods for reducing bias amplification in two problem settings: visual semantic role labeling in the imSitu dataset (vSRL) and multilabel image classification in MS-COCO (MLC). In all settings we derive corpus constraints using the training set and then run our calibration method in batch on either the development or testing set. On the development set, the number of verbs whose bias exceed the original bias by over 5% decreases 30.5% (Viol.). Overall, we are able to significantly reduce bias amplification in vSRL by 52% on the development set (Amp. bias). We evaluate the underlying recognition performance using the standard measure in vSRL: top-1 semantic role accuracy, which tests how often the correct verb was predicted and the noun value was correctly assigned to a semantic role. Our calibration method results in a negligible decrease in performance (Perf.). Similarly to vSRL, we are able to reduce the number of objects whose bias exceeds the original training bias by 5%, by 40% (Viol.). Bias amplification was reduced by 31.3% on the development set (Amp. bias). The underlying recognition system was evaluated by the standard measure: top-1 mean average precision, the precision averaged across object categories. Our calibration method results in a negligible loss in performance. Results on the test set support our development results: we decrease bias amplification by 47.5% (Amp. bias).
The large absolute gaps (from 30.9% to 9.7%) between the results of seen and unseen environments show that current agent models on R2R suffer from environment bias. In the original data splits of Touchdown, the environment is not specifically divided into seen and unseen. To explore whether the same environment bias can be observed in Touchdown, we split the city environment according to latitude and create two sub-environments: ‘training’ and ‘unseen’. Our baseline neural agent model is adapted with additional convolutional layers to fit this new task. At the same time, when experimenting on the original data split (denoted as ‘original’), our baseline model achieves state-of-the-art results on the original ‘dev’ set and ‘test’ set, proving the validity of our model in this dataset.
S4SS3SSS0Px3 Existence of Region-level Locality To demonstrate region-level locality, we study how the success rate changes with respect to distances between training and validation environment regions, similar to the analysis of language ‘distance’ in Sec. We calculate the point-by-point shortest paths using the Dijkstra’s algorithm, where the shortest distances between viewpoints v and v′ are denoted as the graph distance dis\textscgraph(v,v′). We first define the viewpoint distance dis\textscviewpoint from a viewpoint v to the training data T as the minimal graph distance from v to a viewpoint v′ in training data; Then the path distance dis\textscpath from a validating data x to the whole training data T is defined as the maximal viewpoint distance of the viewpoints in the path of x: dis\textscpath(x,T) =maxv∈path(x)dis\textscviewpoint(v,T) (3) dis\textscviewpoint(v,T) =minv′∈path(t)∀t∈Tdis\textscgraph(v,v′) (4) We compute path distances between paths in the path-unseen validation set and training environments of our re-splitting data. Therefore, the closer the path to the training data, the better the agent performs, which reveals the existence of region-level locality.
The inter-rater reliability in the 5-point task (α=0.2308) is roughly the same as that of the pairwise task (α=0.2385). Normalization of ratings per participant (by standardization to Z-scores), however, shows a marked improvement of overall inter-rater reliability for the 5-point task (α=0.2820). A one-way analysis of variance taken over inter-rater reliabilities between pairs of participants suggests statistically significant differences across tasks (F(2,328)=6.399,p<0.01), however, a post hoc Tukey’s Larsen and Marx These scores indicate that the overall agreement between human ratings is roughly the same, regardless of whether participants are being asked to provide cardinal or ordinal ratings. Improvement in inter-rater reliability via participant-level normalization suggests that participants may indeed have individual biases toward certain regions of the 5-point scale, which the normalization process corrects.
The out-of-domain model is trained with MLE on WMT. The task is now to improve the generalization of this model to the TED domain. The supervised domain-adapted model serves as an upper bound for domain adaptation with human rewards: if we had references, we could improve up to 7 BLEU. What if references are not available, but we can obtain rewards for sample translations?
All pair-wise t-tests are statistically significant (p<0.001). It indicates that user utterances that resulted in successful information retrieval tend to have higher complexity, validating our assumption that domain specificity should be reflected by the complexity measure. This may also imply differences between human-processing and machine-processing of dialogs. While casual, less domain-specialized dialogs could be easier for a human agent to handle, it may be more problematic for information retrieval with domain specific knowledge base.
We started by ranking all dialogs in the Restaurant dataset by dialog complexity in descending order. Then, we selected three groups— high complexity (rank 1-20), median complexity (rank 1045-1064) and low complexity (rank 1999-2118) from the complexity spectrum. It shows that, the higher the dialog complexity is, the more restaurant types were in requests, validating that dialog complexity is strongly correlated (r=0.54) with the variations in requests. In fact, we observed that dialogs with the highest complexity are mostly ones where the users were intentionally “breaking” the system, by keeping asking for different kinds of restaurants and typing in repetitive, even random requests.
We consider two cases: (1) where the dialogs are assigned to agents randomly and (2) where the allocation is by increasing order of dialog complexity. We see there is sharp difference in measured performance in the second case where agents were given dialogs with different complexity. Our proposed method ω3 would capture this biased allocation and reward the agent that handled more complex dialogs with equal user satisfaction. On the other hand, conventional metrics such as ω1 and ω2 would not have shown any difference.
For each agent, we report its results in terms of success rate, average reward, and average number of turns (averaged over 5 repetitions of the experiments). Results show that the DDQ agents consistently outperform DQN with a statistically significant margin. Since the training of all RL agents started with RBS using the same rule-based agent, their performance in the first few epochs is very close. After that, performance improved for all values of K, but much more rapidly for larger values. Recall that the DDQ(K) agent with K=0 is identical to the DQN agent, which does no planning but relies on direct reinforcement learning only. Without planning, the DQN agent took about 180 epochs (real dialogues) to reach the success rate of 50%, and DDQ(10) took only 50 epochs.
This might be because of similar surface feature for two or more classes in the second label set (for example husband’s and wife’s father). Increasing the model complexity in graphical models while keeping the surface features from the tokens same, increases the performance. Specially so when the graphical model incorporates feature from larger context (by means of edge) which help in resolving the confusion in case of similar surface features. Due to small CER on the HTR system the models trained with the forced aligned noisy data (∼2% F1 lower) as well perform near the gold data standards. Augmenting the gold data with such noisy data doesn’t improve the performance of the graphical models. The minor performance (<0.005 F1) increase for neural model is similar to achieved by having dropout on the feature layer.
For Geo, we find that when ablating all entity information in our model and copying tokens instead of entities, we achieve similar results as Jia and Liang Jia and Liang This is expected, since when ablating entities completely, our architecture essentially reduces to the same sequence-to-sequence task setup. These results demonstrate the impact of conditioning on the entity candidates, as it improves performance even on the token copying setup. It appears that leveraging BERT can partly compensate for not conditioning on entity candidates, but combining BERT with our GNN approach and copying entities achieves 2.9% higher accuracy than using only a BERT encoder and copying tokens.
Entity Spans and Relations Ablating The impact is more significant for Atis, which contains many queries with multiple entities of the same type, such as nonstop flights seattle to boston where disambiguating the origin and destination entities requires knowledge of which tokens they are associated with, given that we represent entities based only on their types for these tasks. We leave for future work consideration of edges between entity candidates that incorporate relevant domain knowledge for these tasks.
Tab. SCRF rescoring improves over the tandem HMM, and the first-pass SCRF outperforms both. As shown in the first line of Tab. The poor performance is perhaps to be expected with such a small number of training signers.
Analysis: Could we do better by training entirely on adaptation data? Until now we have considered adapting the DNNs while using sequence models (HMMs/SCRFs) trained only on signer-independent data. In this section we consider alternatives to this adaptation setting. We fix the model to a first-pass SCRF and the adaptation data to 20% of the test signer’s data annotated with ground-truth labels. In this setting, we consider two alternative ways of using the adaptation data: (1) using the adaptation data from the test signer to train both the DNNs and sequence model from scratch, ignoring the signer-independent training set; and (2) training the DNNs from scratch on the adaptation data, but using the SCRF trained on the training signers. We compare these options with our best results using the signer-independent SCRF and DNNs fine-tuned on the adaptation data. The results are shown in Tab.
We next attempt to improve the performance of adaptation in the absence of ground-truth (manually annotated) frame labels. This is an important setting, since in practice it can be very difficult to obtain ground-truth labels at the frame level. Using only the FS-letter label sequence for the adaptation data, we use the signer-independent tandem recognizer to get force-aligned frame labels. We then adapt (fine-tune) the DNNs using the force-aligned adaptation data (as before in the FA case). We then re-align the test signer’s adaptation data with the adapted recognizer. Finally, we adapt the DNNs again with the re-aligned data. Throughout this experiment, we do not change the recognizer but only update the DNNs. Using this iterative realignment approach, we are able to furthur improve the recognition accuracy in the FA case by about 1.3%, as shown in Tab.
As shown in Tab. This improvement, while small, is statistically significant at the p=0.05 level (using the MAPSSWE significance test from the NIST These results combine our most successful ideas and form our final best results for signer-adapted recognition. For the signer-dependent setting, this approach achieves comparable performance to the first-pass SCRF.
We use the standard ROUGE-1,2 and L Lin We first observe that introducing the capability to learn latent structures already improves our performance on ROUGE-L. It suggests that modeling dependencies between sentences helps the model compose better long sequences w.r.t reference compared to baselines. We do not see a significant improvement in ROUGE-1 and ROUGE-2, hinting that we retrieve similar content words as the baseline but compose them into better contiguous sequences.
We estimated the theoretical limit of efficiency for container naming by applying the IB method Tishby \BOthers. We evaluated the empirical complexity and accuracy in the four naming conditions by entering the corresponding naming distributions in the equations for Iq(M;W) and Iq(W;U). In all four cases, the corresponding IB solution is at βl≈1.2, suggesting that there is only a weak preference for accuracy over complexity in this domain, as also found for color naming. The remainder of our analysis focuses on the monolingual systems, as they are more distinct and presumably more representative of each language. To get a precise sense of how challenging it may be to reach the observed levels of efficiency, we compared the actual naming systems to a set of hypothetical systems that preserve some of their statistical structure. This set was constructed by fixing the conditional distributions of words, while shifting how they are used by applying a random permutation of the containers. For each language we constructed 10,000 such hypothetical systems. In fact, both languages achieve better (lower) scores than all of their hypothetical variants, providing a precise sense in which they are near-optimal according to IB. One possible concern is that this outcome may be a result of the LI prior, which was fitted to the naming data. To address this, we repeated this analysis with a uniform need distribution. The results in that case are similar (not shown), although as expected the fit to the actual systems is not as good compared to the LI prior. This is indeed supported by a fine-grained comparison between the naming distribution in both languages and their corresponding IB systems. To see this, we embedded the 192 containers in a 2-dimensional space by applying non-metric multidimensional scaling (nMDS) with respect to the similarity data, similar to \citeAAmeel2009. This was done using the scikit-learn package in Python. We initialized the nMDS procedure with a solution for the standard metric MDS that achieved the best fit to the similarity data out of 50 solutions generated with random initial conditions. For visualization purposes, we assigned a unique color to each container. The resulting 2D embedding and color coding of the containers stimulus set are shown in Figure
The model fine tuned on a QA and LM objective is rated as significantly worse by human annotators, despite achieving higher scores in the automatic metrics. In other words, the training objective given by these reward sources does not correspond to true question quality, despite them being intuitively good choices.
We can observe that most variants are significantly inferior to our model in terms of BLEU scores. Particularly, although the Acc of some variants increase, these models may overly change the original content to conduct transfer, still resulting in lower BLEU scores. These results demonstrate the effectiveness of our introduced neural style component, different loss terms and two stage training strategy. As an exception of above observations, when we replace Lcp with Lcp′, the BLEU score increases but the Acc drops significantly. This is due to the fact that Lcp′ does not discriminate words of different style relevances and overly constrain the model to keep its original content.
Accuracy score is used to measure the overall performance and we present our performance on both the development set and the test set. The hyper-parameters of these two models are tuned separately from other models. Due to the space limitation, we don’t present the details here. Comparison of these sequential models with the tree-structured models are expected to show the effects of tree structures.
We trained our model on one machine with 2 NVIDIA P40 GPUs. In addition, we set the dropout=0.3 in all datasets. The model is trained using 30 epochs in three datasets (PTB, WikiText-103 and One-Billion).
We again use the “Donald Trump” Wikipedia article as example. The Brown corpus overly penalizes terms such as “United States” and “president” because it contains many political news articles; but on the other hand it does not contain e.g. the names “Obama” or “Ivana” (as the corpus is from the 1960s). We can see that the methods yield surprisingly different results. For example, the term “WrestleMania” is only the 189th most frequent word in the article. With tf-idf, it is a top 50 word, and with our new approach it is the top 12 relevant single word, but only gets a final rank of 209, because there are many word cooccurrences with higher score. The terms “United States” and “real estate”, for example, are only selected due to taking cooccurrences into account. The results when using only the word count are not convincing (it would select “other”, “time” and “year” that are fairly common), but tf-idf already does a reasonable job at selecting important words. An interesting true positive is the word “name” as there even exists an article on Wikipedia titled “List of Things named after Donald Trump”. While it is a common word, and thus ranked low by both tf-idf and our single-word selection method, it scores higher because of cooccurrences with other words (supposedly with “Trump”).
Filtering makes the task easier by removing irrelevant candidates, which tends to improve the results, but our adjustment to the mean and median calculation has a negative impact on these evaluation metrics. Fortunately the median is a robust statistic that is only slightly affected by the adjustment. The mean, on the other hand, is greatly changed, for the worse.
In this section we want to compare effectiveness of a number of competitive methods in CLIR with effectiveness of the proposed method. We consider the following dictionary-based CLIR methods to evaluate the proposed method: (1) the top-1 translation of each term in the bilingual dictionaries (TOP-1), (2) all the possible translations of each term with equal weights (UNIFORM), We provided the results of CLWETM for α=0 (i.e., without interpolation. As shown in the table, both MIXWETM and CLWETM outperform other methods in terms of MAP, P@5, and P@10 in all the collections. Except in Spanish, MIXWETM and CLWETM consistently achieved better results compared to others in terms of all the metrics; but, CLWETM is clearly more effective than MIXWETM, whose context is by far larger than of CLWETM. The results of JCLTRLM reveals that top-ranked documents in SP and DE are not as comparable as FA or FR. Therefore we hope more improvements in FA and FR than others for CLWETM and MIXWETM.
PKT-UD v2018 We perform an error analysis on the parsing outputs generated by the v2018 model. Our analysis shows that the head error occurred in 1,360 Eojeols and the label error occurred in 4,292 Eojeols. The relations advcl, nummod, acl, and obl have a high error rate, which are due to the inconsistencies seen in the data we handled by establishing clear criteria. Moreover, the labels goeswith and flat saw 100% error, again, due to the errors we observed during the revision process.
Bias Controlled Training Controls Gendered Words. We examine the effect of Bias Ctrl by generating responses conditioning the ALL model on each bin. We observe that changing the bin radically changes the genderedness of generated text with only small differences in overall F1. We can control the male bias of the generated dialogue by manipulating these bins.
When we remove the POS information from the representation, are nouns affected the same as conjunctions? We observe large differences in the word prediction performance before and after the POS removal between the labels. Nouns, numbers and verbs show a relatively small impact in performance (8.64, 6.91 and 11.73 respectively), while conjunctions, particles and determiners demonstrate large performance drops (73.73, 77.66 and 65.65, respectively). We see that the information about POS labels at the word-level prediction is much more important in closed-set vocabularies (such as conjunctions and determiners) than with open vocabularies (such as nouns and verbs).
Without random padding injection, the models quickly overfit to the low entropy regions of the training data, which leads generic and/or short responses. For a more fair comparison with previous work on multi-turn dialogue not using random padding injection and 100% BPE tokenization, we trained the DLGNet models on multi-turn data with basic tokenization. The tokenization coverages of the basic tokenizer used are 83.9% and 4.19% for Movie and Ubuntu datasets respectively. Basically, most of the Ubuntu tokens are mapped to the   token. In comparison with other transformer-based configurations, the smaller size multi-turn models perform better than their BPE counterparts but the larger size models perform worse. This is probably due to the overfitting of the larger models.
The model trained on the WWW dataset is used to predict keyphrases for the KDD dataset and vice-versa in this experiment. As shown by the numbers in this table, our models continue to be reasonably effective even when trained on a different (though related) collection of documents. The tagging performance is better with WWW (as the training dataset) possibly due its larger size.
In addition to the accuracy and correlation scores, we also report an average of all the 10 scores in the last column. We only show the best performing model (out of 5) for each of the six methods, based on the average validation performance over all the tasks in SentEval. ConsSent-N(3), which is trained to discriminate between pairs of sequences, performs the best on an average for any single task. ConsSent-MT(3) achieves a similar average performance, also achieving the best scores on five of the 10 transfer tasks among our models. Among the methods that classify single sequences, ConsSent-R(2) performs the best, second only to ConsSent-N(3). ConsSent-C is dominated by ConsSent-N in most cases, while ConsSent-D and ConsSent-P perform the worst. As a baseline, we also train a LSTM (with 4096 hidden dimensions and 512 word embedding dimensions) language model on the same dataset that was used to train our models. The last hidden state of the trained LSTM acting on a sentence is then used as the sentence representation. The Billionword dataset lacks this property.
Classifier selection methodology. To identify the most suitable classifier for classifying the scalars associated with each text, we perform evaluations using the stochastic gradient descent, naive bayes, decision tree, and random forest classifiers. For each classifier, we split the CrowdFlower hate speech dataset into a training/validation set (75%), and a holdout set (25%). We perform 10-fold cross-validation on the training/validation set to identify the best classifier model and parameters (using a grid search). Based on the results of this evaluation, we select a 100-estimator entropy-based splitting random forest model as our classifier.
To better understand the effect of grammatical errors, we also analyze (1) which error type harms the performance most, (2) how different error rates affect the performance. For the first question, we represent the harm of an error type by the total time it is chosen in successful greedy attack examples. Among all, Wchoice is the most harmful type while Worder the least. SVA ranks the second most harmful type. Notice that though Nn changes a token in a similar way with SVA (both adding or dropping -s or -es in most cases), they have different influences to the model. As for errors related to function words, Prep plays a more important role in general but ArtOrDet harms MNLI more.
We fix the parameters of pre-trained encoders and train a self-attention classifier for each layer to judge the binary linguistic acceptability of a sentence. We find that layer 1 of ELMo, middle layers of BERT, and top layers of RoBERTa perform the best in this evaluation.
The Random Inputs strategy shows that even without communication, the two bots are able to locate their shared entry 82% of the time by revealing their own KB through SELECT action. When we keep the mentioned entities untouched but randomize all other tokens, DynoNet actually achieves state-of-the-art Completion Rate, indicating that the two agents are paying zero attention to each other’s utterances other than the entities contained in them. This is also why we did not apply Add Negation and Antonym to DynoNet — if Random Inputs does not work, these two strategies will also make no difference to the performance (in other words Random Inputs subsumes the other two Should-Change strategies). We can also see that even with the Normal Inputs with Confusing Entities strategy, DynoNet is still able to finish the task 77% of the time, and with only slightly more turns. This again shows that the model mainly relies on the SELECT action to guess the shared entry.
We observe that the equalization step has the strongest impact in bias reduction, while the projection is inefficient when used separately. We hypothesize that the projection is not able to correctly handle the explicit gender indicator words and therefore leaves too much direct bias. However both combined as in the strong debias technique provide the best results.
In addition to the automatic metrics, we also conduct human evaluation for the generated samples. We invite 20 graduate students with good English proficiency to score each sentence on a scale of 1-5 in terms of quality. We perform the Wilcoxon Rank Sum Test with the human evaluation results and find that samples generated by baseline models can be distinguished from samples generated by SAL with p<0.01. Details of human evaluation procedure and samples generated by compared methods in two real-world datasets are presented in the Appendix.
Due to the limitations of Twitter API, we developed a continuous crawler in order to obtain documents during the first semester of 2017. The final dataset is split in two documents - a training set with 12.999 documents labeled in positive (44%), neutral (26%) and negative (29%); and a test set composed of 2001 documents with similar distribution to the training set, 45%, 25% and 29% respectively.
The Stories dataset is larger and more varied in terms of unique target expressions, and has a nearly-perfect inter-annotation agreement (Cohen’s κ of 0.99). As discussed, the disagreement was quite high in comparison, proving that detecting atypical animacy can be a very semantically complex problem (in particular in highly figurative language). There are 183 sentences in which the machine has been tagged as animate, out of which 134 are also instances of humanness. The 19thC Machines dataset is composed of sentences from the selected four time periods. 19thcBERT + ctxt) provides better results than the contemporary model, especially in terms of mean average precision, i.e. the ranking generated by the animacy score. Unfortunately, we were not able to reproduce the same number of target expressions as are reported in \newcitejahan2018new, but we will provide the code we used to generate our datasets for future studies.
Interestingly, their performance becomes worse when more context is added, and even more so when the target expression itself is masked. Unlike the baseline classifiers, our method (MaskPredict: BERT-base) does not use the target expression as a feature at all: it relies solely on the context. In fact, adding context (i.e. one sentence to the left and to the right, MaskPredict: BERT-base + ctxt) helps improve its performance (from 0.77 to 0.84 in F-Score). This analysis shows that target expression is the most indicative feature of conventional animacy. And yet, the good performance of our context-based method proves that animacy is not only entity-level, but that it is informed by the context as well.
Results and Ablation Study Our model (QBM) performs best compared to baselines (Q-Q Mean, Q-Q Max, Bag-con). Comparing Bag-Con and Base model, we find that modelling the query-question relationship following aggregation works better. We assume that the pooling-based aggregation can reduce the redundant information cross sentences in a bag. Considering the Q-Q matching based methods and query-bag based methods. In AliMe dataset, the query-bag matching outperforms the Q-Q matching based methods which shows the necessity to perform query-bag matching. The ablation study shows that the mutual coverage component and bag representation component achieve better performance than the base model, especially in the Quora dataset. The two components work independently and their combination gets the best performance. Analysis of the Bag Representation Coverage is also applied in the bag representation layer. The results of the bag representation without coverage component Compared with the Base+BR and BR without coverage, it shows that the coverage component contributes a lot on both the two datasets. The bag representation with coverage (Base+BR) gains improvement over Base model, especially in Quora dataset.
Firstly, we observe that our proposed AF-LSTM (CONV) outperforms all other neural architectures. The performance of all AF-LSTM models is generally much higher than ATAE-LSTM. Additionally, we observe that ATAE-LSTM is outperformed by AT-LSTM across all settings. This shows that concatenation of aspect and word before the LSTM layer may significantly degrade performance. Moreover, we found that the performance of ATAE-LSTM may not outperform a baseline LSTM on certain datasets (Laptops). The overall performance of ATAE-LSTM is approximately the same as the baseline LSTM.
Our model produces more logically coherent and well-organized texts, which indicates the effectiveness of the planning mechanism. It is also worth noting that our model performs better in terms of grammaticality. The reason is that long text generation is decomposed into sentence generation sub-tasks which are easier to control, and our model captures inter-sentence dependencies through modeling the dependencies among local latent variables.
For our experiments, we used the Penn English Treebank (PTB) Marcus et al. (CTB5) Xue et al. We used gold segmentation for Chinese tests to make our work comparable with previous work. We used predicted part-of-speech tags for both languages in all evaluations. Tags are assigned by base parser’s internal joint tagger trained on the training set. We report labeled (LAS) and unlabeled (UAS) attachment scores, punctuation marks are excluded from the evaluation.
Combining different N-gram DLMs. We first evaluated the effects of adding different number of DLMs. Let m be the DLMs we used in the experiments , e.g. m=1-3 refers all three (unigram, bigram and trigram) DLMs are used. We evaluate with both single and multiple DLMs that extracted from 5 million sentences for both languages. We started from only using unigram DLM (m=1) and then increasing the m until the accuracy drops. The unigram DLM is most effective for English, which improves above the baseline by 0.38%. For Chinese, our approach gained a large improvement of 1.16% with an m of 1-3. Thus, we use m=1 for English and m=1-3 for Chinese in the rest of our experiments.
We first evaluate with DLMs extracted from the different number of single-parsed sentences. We extracted DLMs start from a 5 million sentences corpus and increase the size of the corpus in step until all of the auto-parsed sentences are used. For English, the highest accuracy is still achieved by DLM extracted from 5 million sentences. While for Chinese, we gain the largest improvement of 1.2% with DLMs extracted from 10 million sentences.
Main Results on Test Sets. We applied the best settings tuned on the development sets to the test sets. The best setting for English is the unigram DLM derived from the double parsed sentences. Our approach with 40 beams surpasses our baseline by 0.46/0.51% (LAS/UAS) When we enlarge the beam, our enhanced models achieved similar improvements. Our semi-supervised result with 150 beams are more competitive when compared with the state-of-the-art. We cannot directly compare our results with that of \newcitechen2012utilizing as they evaluated on an old \newciteyamada03 format. In order to have an idea of the accuracy difference between our baseline and the second-order graph-based parser they used, we include our baseline on \newciteyamada03 conversion. This confirms our claim that our baseline is much stronger.
For Chinese, we extracting the DLMs from 10 million sentences parsed by the Mate parser and using the unigram, bigram and the trigram DLMs together. When larger beams are used our approach achieved even larger improvement of more than one percentage point for both labeled and unlabeled accuracy when compared to the respective baselines. Our scores with the default beam size (40) are competitive and are 0.2% higher than the best reported result Chen et al. when increasing the beam size to 150. Moreover, we gained improvements up to 0.42% for part-of-speech tagging on Chinese tests.
Correlational analysis shows that the emotion measures are all significantly positively correlated to the happiness scores and negatively correlated to the sadness scores.
OpenNMT provides two default hyperparameters settings that differ in the size of layer used and the number of attention heads, namely, “base” and “big”. Although we could have expected the smaller model to be a better fit for low-resource training, we found out the opposite.
Pre-training accuracy can be an indicator of the learning difficulty. The pre-training objectives should not be too easy or too difficult. Moreover, it can be found that the accuracy of a pre-training objective does not vary a lot from one language pair to another. As easy and difficult are subjective we use pre-training accuracy as one of the indicators of the difficulty and hence the usefulness of our pre-training approach. MASS+JASS gives the best BLEU performance in most of our experiments and thus we hypothesize that there is no perfect pre-training method and thus one should explore a variety of methods for a given language pair.
We evaluate different initialization and optimization methods in this section. The second row shows the results when the embedding is optimized jointly during training. The performance drops significantly. Detailed analysis reveals that the trainable embedding enlarge trainable parameter number and the model gets over fitting easily. The model acts like a context independent entity tagger to some extend, which is not desired. For example, the model will try to find any location name in the evidence when the word “在哪 (where)” occurs in the question. In contrary, pre-trained fixed embedding forces the model to pay more attention to the latent syntactic regularities. And it also carries basic priors such as “梨 (pear)” is fruit and “李世石 (Lee Sedol)” is a person, thus the model will generalize better to test data with fixed embedding. The third row shows the result when the embedding is randomly initialized and jointly optimized. The performance drops significantly further, suggesting that pre-trained embedding indeed carries meaningful priors.
We now detail the results achieved by our SMT and NMT systems on the official test data used in the shared task. Papineni et al.
Table 1 describes all the model performance. We denote testing data from the combination of restaurant, weather and bus domains as “In Domain” data since they are in the same domains as what we use to train. The data from movie domain is denoted as “New Domain” as it is unseen in training data. “Unseen Slot” and “Unseen NLG” represent restaurant-slot and restaurant-style domains correspondingly. We found that both transfer learning and DAML obtain better results than ZSDG. Especially for the “New Domain”, DAML achieves the entity F1 score of 66.2, 25.8% relative improvement compared with ZSDG. As for “In Domain” testing, DAML also obtains 14.4% improvement beyond ZSDG. However, our method does not get large improvement in the “Unseen slot” and “Unseen NLG” domains. We notice that these two domains are actually generated from one of the source domain (restaurant domain). So, even though the slots or templates are changed, they should still share some features with the original domain data. If we could take advantage of the original restaurant domain, the result should be improved. Following this intuition, in the “Unseen slot” domain and the “Unseen NLG” domain, we first fine-tune the model obtained from DAML with the original restaurant data in training, and then we do further fine-tune with the adaptation data. We see that in most cases, fine-tuning on restaurant data increases both the BLEU score and entity F1 score on the “Unseen Slot” and “Unseen NLG” domain. And our method shows evident advantage not only with better scores but also with much fewer update steps. Even for the “New Domain,” DAML only uses 5.8 epochs on average to converge, which is only 40% of epochs used in transfer learning. Therefore, we conclude DAML is more efficient compared with simple transfer learning.
The code sets up several threads of the Word2Vec process, which runs over the corpus simultaneously, training the weight matrices to minimize the loss function. The number of training iterations was increased to 30 to improve consistency of similarity measurements across embeddings for each sampling scale.
In constructing our dataset, we selected query relations with reasonable amounts of data. However, for many important applications we have very limited data. To simulate this common scenario, we create a new dataset by randomly selecting 23 out of 46 relations and removing all but 1% of the positive and negative triples previously used for training. Effectively, the difference between Path-RNN and Single-Model is that Single-Model does multitask learning, since it shares parameters for different target relation types. Therefore, we expect it to outperform Path-RNN on this small dataset, since this multitask learning provides additional regularization. We also experiment with an extension of Single-Model where we introduce an additional task for multitask learning, where we seek to predict annotated types for entities. Here, parameters for the entity type embeddings are shared with the Single-Model. Supervision for this task is provided by the entity type annotation in the KB. We train with a Bayesian Personalized Ranking loss of \newcitebpr. With Single-Model there is a clear jump in performance as we expect. The additional multitask training with types gives a very incremental gain.
We compare to the results reported by \newcitegu2015 on the WordNet dataset. It should be noted that the dataset is fairly small with just 22 relation types and an average path length of 3.07. More importantly, there are only few unseen paths during test time and only one path between an entity pair, suggesting that this dataset is not an ideal test bed for compositional neural models. Mean Quantile(MQ) is the fraction of incorrect entities which have been scored lower than the correct entity. Our model achieves a 84% reduction in error when compared to their best model.
The improvements in accuracy require additional time as well as larger model size. The model size increases 3.5% relatively from +AttY to + AttY2D, and 3.4% from +C+AttY to +C+AttY2D. The translation times are summarized in Table.
Another observation is the difference between words coverage and group coverage. Much smaller number of groups than individual words is needed to achieve the same coverage, as one would naturally expect. These two numbers have been used because it is often argued that in order to read a given text with minimal distraction one needs to know enough words to cover at least 95%-98% running words of the text. Such high coverage is also needed to transfer reading skills from one’s mother tongue to another (foreign or second) language. It is encouraging for students of Latin that the number of word groups required to obtain such coverage is relatively low. Nevertheless, one has to be careful interpreting this table, remembering that one group may consist of many dictionary headwords. In order to reach 95% coverage of Philipicae one needs much larger number of words than in the case of Vulgate. On the other hand, Vulgate requires larger number of word groups to achieve the same coverage. This shows that Cicero used the inflection system of the Latin language much more skillfully – he used fewer word groups than St. Jerome, yet from these he obtained a larger number of inflected forms!
We, therefore, fitted the function f(x)=x+xα(1−x)β (6) to the normalized coverage data cg(x). The fit, although good, was less than ideal, so we introduced another two parameters, fitting the function f(x)=xγ+xα(1−xδ)β. (7) It should be noted that this choice of the equation does not have any particular meaning , it just has been observed that it produces a good fit, thus it is a convenient way to describe coverage curves. For that reason, one can say that the parameter η tells us to what degree is the the inflection mechanism of the language used in order to provide high text coverage. Small η means high reliance on the inflection mechanism.
USPG is only better than Basic since it adopts a quite simple structure, even without any design for Coherence, which severely limits its performance. CVAE heavily relies on the support of multiple keywords. With a single keyword, it fails to produce meaningful contents, while our model can enrich semantic meanings by the mixed latent space. Despite obtaining the best inter-topic diversity, MRL may lose control of generated contents. Merely increasing TF-IDF could incur unexpected words digressing from topics and thus hurt quality. It is noteworthy that generated poems take the risk of straying the given topic when constrained on one single style, because not all topics are compatible with every style. Therefore we also assess poems generated in Sec. Nonetheless, our model still gets acceptable results, since it utilizes the mixed latent space to capture more generalized properties of both factors and keywords, beyond simple labels.
It is labor-intensive and error-prone to build a custom dictionary for each application, so often when practitioners apply dictionary scaling methods, they use off-the-shelf dictionaries instead of building their own. However, as those authors note, applying an off-the-shelf dictionary to a new domain often leads to undesirable results. We display the number of word types in each cell, along with the most common words. If the dictionary were appropriate for our application, we should observe positive words associated with government usage, and negative words associated with opposition usage. Only 11 “positive” words have high usage in the government leadership speech, and no “negative” words have high usage in the opposition leadership speeches. Most “positive” and “negative” words do not have a clear association with either Government or Opposition. Furthermore, there are some worrying cases where the dictionary orientation is counter to the association between the classes. For example, while the LSD declares the word to be negative, in the context of the debate deficit refers simply to a fiscal outcome; likewise, confidence is related to the question of the debate, and not intended to convey positive valence. Despite being designed to detect political valence, the dictionary fails here since it has not been tailored for this particular debate. Terms that are associated with one type of affect generally are used differently in the context of the no-confidence debate. Beyond the problem of domain adaptation, the more fundamental issue with dictionary methods is that their basic premise—that each word has a clear orientation—is inappropriate in our domain. Most words in our application do not clearly either belong in one category or the other. The vast majority of words get used by both government and opposition, and thus have mixed associations with both classes. In the sequel, we present an alternative method that allows for mixed word association while simultaneously adapting to the domain.
Without using any of the metadata present in a commit, such as the commit message or information about the author, we are able to correctly classify commits based on their security-relevance with an accuracy of 65.3% and F1of 77.6% on unseen test data. When extracting features from the complete source code of the Java classes which are modified in the commit, the performance of HR-CNN increases noticeably. This is considerably above the LR baseline and justifies the use of a more complex deep learning model. Meanwhile, the performance of H-CNN with randomly-initialized embeddings ( Hence, we find that extracting class-level features from the source code before and after the change, instead of using only the commit diff, improves the identification of security-relevant commits. The model only achieves an accuracy of 63.8% on the test split, with an F1score of 72.7%, which is two points less than that of LR. The code2vec model performs much worse compared to H-CNN and HR-CNN with randomly-initialized embeddings. Hence, learning from a path-based representation of the Java classes before and after the change does not improve the identification of security-relevant commits—at least with the code2vec approach. This could possibly be due to the coarse data samples being too noisy or the distribution of security-relevant commits in the coarse dataset not matching that of the unseen dataset. The latter might have been due to the high-precision mining technique used, capturing only a small subset of security vulnerabilities. We directly train code2vec on our dataset without pre-training it, in order to assess how well path-based representations perform for learning on code, as opposed to token-level representations on which H-CNN and HR-CNN are based. However, alon2018code2vec pre-trained their model on 10M Java classes. Furthermore, our findings apply only to this particular technique to capturing path-based representations, not the approach in general. However, we leave both issues for future work.
Next, we investigated the effect of λ. In this experiment, PBSMT was trained with a 500k subset of training data, and the distortion limit was set to 6. Our RvNNs consistently outperformed the plain PBSMT without preordering. The BLEU score improved as λ increased when only word embedding was considered. In addition, RvNNs involving POS tags and syntactic categories achieved even higher BLEU scores. This result shows the effectiveness of POS tags and syntactic categories in reordering. For these models, setting λ larger than 200 did not contribute to the translation quality. Based on these, we further evaluated the RvNN with POS tags and syntactic categories where λ=200.
Overall Performance. In open-domain dialog, BLEU-2 exhibits some (not large) correlation with human satisfaction, although BLEU scores are generally low. For machine translation, we achieved 27.2 BLEU for the normal setting, which is comparable to 28.4 achieved by a baseline method in \newcitebaseline.
After realigning the new labels we can compare directly to the results of the Levy et al. set. In particular, we aggregate our expert judgements by majority vote. We propose that for cases where there is no majority (i.e. each of the three annotators select a different label), the row simply be discarded. However, in our reannotation of the Levy et al. subset this did not occur.
Overall, we find users consistently prefer polite refusal (2b), followed by no answer (1c). Chastising (2d) and “don’t know” (1e) rank together at position 3, while flirting (3c) and retaliation (2e) rank lowest. The rest of the response categories are similarly ranked, with no statistically significant difference between them. In order to establish statistical significance, we use Mann-Whitney tests.
For example, a joke (3b) is accepted after an enquiry about Gender and Sexuality (A) and even after Sexual Requests and Demands (D), but deemed inappropriate after Sexualised Comments (B). Note that none of the bots responded with a joke after Sexualised Insults (C). Avoidance (2f) is considered most appropriate in the context of Sexualised Demands. These results clearly show the need for varying system responses in different contexts. However, the corpus study from \newciteAmanda: EthicsNLP2018 shows that current state-of-the-art systems do not adapt their responses sufficiently.
Finally, we consider appropriateness per system. Following related work by Novikova et al. Alley produces “polite refusal” (2b) - the top ranked strategy - 31% of the time. Comparatively, commercial systems politely refuse only between 17% (Cortana) and 2% (Alexa). Rule-based systems most often politely refuse to answer (2b), but also use medium ranked strategies, such as deflect (2c) or chastise (2d). For example, most of Eliza’s responses fall under the “deflection” strategy, such as “Why do you ask?”. Data-driven systems rank low in general. Neuralconvo and Cleverbot are the only ones that ever politely refuse and we attribute their improved ratings to this. \newciteRitter:2010: UMT:1857999.1858019’s IR approach is rated similarly to Capt Howdy and both produce a majority of retaliatory (2e) responses - 38% and 58% respectively - followed by flirtatious responses. Finally, Dr Love and Sophia69 produce almost exclusively flirtatious responses which are consistently ranked low by users.
Statistically significant improvements over the respective LR baselines are denoted by superscript. Restricting the auxiliary tasks to a small subset tends to hurt performance for most tasks, with exception to bipolar, which benefits from the prediction of depression and suicide attempt. All main tasks achieve their best performance using the full set of additional tasks as auxiliary. This suggests that the biases induced by predicting different kinds of mental conditions are mutually beneficial – e.g., multi-task models that predict suicide attempt may also be good at predicting anxiety.
In order to take full advantage of the labeled data and to make our classifier more accurate, we combined 100% of training and test partitions of the corresponding dataset, and used the combined data for training. The best classifier is selected based on the performance on the dev partition.
The first three rows are the results from SBN systems. Both the multilingual and the closest language systems are adapted to the target language for the whole stacked network. For the hybrid systems, the input is the BN features extracted from the first DNN of the adapted multilingual SBN. In this subsection we investigate the effect of the multilingual transfer learning for each model. We first use the rich resource closest language (based on the LID prediction shown in the table) to train DNN, LSTM and PAC-RNN models, and then adapt them to the target language. As shown, the LSTM models perform significantly better than the baseline SBN system. Using the PAC-RNN model yields a noticeable improvement over the LSTM. Similarly, the PAC-RNN-LSTM can further improve the results.
First, we focus on the effectiveness of adding the L1 norm to the objective function of the TTE model. We can confirm that the use of the L1 norm results in improved performance. Furthermore, we found that use of the L1 norm also leads to much faster attention learning. The attention weights for the validation data are shown in Fig. While the TTE model without the L1 norm is unable to learn the attention until after epoch 40, use of the L1 norm make the model to learn the attention in less than 1/3 the number of epochs. This is because use of the L1 norm makes the model focus on reducing smaller error, which prevents the decoder of the model from becoming something like an auto-encoder.
We first investigate the effectiveness of our instance aggregating training procedure. The table compare training with instance aggregating and k-best merging. As the result suggested, with the instance aggregating method, the performance improves on both listwise tuning approaches. For the rest of this paper, we use the instance aggregating as standard setting for listwise tuning approaches. To verify our assumption that the correct rank in the top portion of a list is more informative, we conduct this set of experiments. We can observe an improvement in all test sets when we set n from 1 to 5, but when we further increase n, the results dropped. This situation indicates that the correct ranking at the top of the list is more informative and forcing the model to rank the bottom correctly as important as the top will sacrifice the ability to guide better search.
Since the effectiveness of high dimensional feature set, recent work pays more attention to this scenario. Although previous discriminative tuning methods can effectively handle high dimensional feature set, MERT is still the dominant tuning method for basic features. Here, we investigate our top-rank enhanced tuning methods’ capability of handling basic feature set. Firstly, we observe that ListNet and ListMLE can perform comparable with MERT. With our top-ranked enhanced method, we can get a better performance than MERT by 0.25 BLEU score. These results show that our top-ranked enhanced tuning method can learn more informations of translation list even with a basic feature set.
As seen, both RNN and DiSAN significantly outperform SAN on our task, indicating that the recurrence structure (RNN) exactly performs better than parallelization (SAN) on capturing word order information in a sentence. Nevertheless, the drawback can be alleviated by applying directional attention functions. The comparable result between DiSAN and RNN confirms the hypothesis by \newciteShen:2018:AAAI and \newcitedevlin2018bert that directional SAN exactly improves the ability of SAN to learn word order. The consistency between prior studies and our results verified the reliability of the proposed WRD task.
We return to the central questions originally posed, that is, whether SAN is indeed weak at learning positional information. Using the above experimental design, we give the following answers: We investigate whether the SAN indeed lacks the ability to learn word order information under machine translation context. We first report the effectiveness of the compared models on translation tasks. For En-De translation, SAN outperforms RNN, which is consistent with the results reported in Chen et al. Moreover, DiSAN incrementally improves the translation quality, demonstrating that model directional information benefits to the translation quality. The consistent translation performances make the following evaluation on WRD accuracy convincing.
S5SS2SSS0Px3 Comparison to Other Architectures. Finally it is of interest how our proposed model compares to more traditional neural models. We compare our model against a standard encoder-decoder model, and an encoder-decoder model with attention, both trained on root form to inflected form character sequences. These models also do not take the root form character sequence as inputs to the decoder. Our model is slightly better than the attentional encoder-decoder model, and is simpler as it does not have the additional attention layer.
To evaluate effects of extracted rules, we performed an ablation study. We run our algorithm VWS-DMS with each rule kept or removed over two datasets. If no pairs extracted for one aspect in training set, the accuracy of this aspect will be 0.5, which is a random guess. Rules R3/R4/R5 are less effective on their own. However, as a whole, they can still improve the overall performance. When considering removing each of rules, we found that our algorithm is quite robust, which indicates missing one of the rules may not hurt the performance much. Hence, if human labor is a major concern, rule 5 can be discarded. We found that sometimes removing one rule may even result in better accuracy (e.g., “-R3” for BeerAdvocate dataset). This means this rule may introduce some noises into the objective function. However, “-R3” can result in worse accuracy for TripAdvisor, which means it is still complementary to the other rules for this dataset.
To compute NMI, we first use a supervised ASR system trained on Switchboard training corpus (about 300 hrs) to obtain forced aligned phoneme transcripts as our reference transcripts. During scoring, we define the distance between an output acoustic unit token and a reference phoneme token as the time frame difference between the center frames of two tokens; in doing so, each acoustic unit token is assigned to a closest reference phoneme token based on the distance metric defined. For document classification, we use the acoustic unit trigram representation, and we scale each trigram feature value by the inverse document frequency, referred to as TFIDF features. We further normalize each feature vector to L2 norm unit length. Specifically, the best performance across all measures by combining LDA and BN demonstrates the complementarity between these two approaches. Given all the same AUD model configurations, we find the improved same-different AP or document classification accuracy often indicates the NMI improvement, which implies in a zero-resource setting, AUD evaluation can fall back to other resources if necessary, e.g., word pairs or topic labels, which might be easier to be available or to obtain than the expensive orthographic phoneme transcripts.
We can see that our approach improves the state of art from 60.4% Notably, for the question type Other and Num, we achieve 3.4% and 1.4% improvement on open-ended questions, and 4.0% and 1.1% on multiple-choice questions. As we can see, ResNet features outperform or match VGG features in all cases. Our improvements are not solely due to the use of a better CNN.
We observe that parallel co-attention performs better than alternating co-attention in this setup. Both attention mechanisms have their advantages and disadvantages: parallel co-attention is harder to train because of the dot product between image and text which compresses two vectors into a single value. On the other hand, alternating co-attention may suffer from errors being accumulated at each round.
As expected, the overall classification accuracy alone (roughly 28.7%) is not comparable to current state-of-the-art systems, and is not above random baseline.
To be specific, we sort the questions by confidence and choose the top 10% confident ones and count the first three words in these questions. We observe that questions asking for a time especially for a year are often easier. In contrast, questions starting with “what" or “how" are more likely to be hard.
For optimal performance the choice of the dense layer activation should be coordinated with the choice for the L2-regularization weight. The linear network (no dense layer activation) runs better with L2-regularization weight close to 0.02 but rectified linear unit (RELU) activation runs better with L2-regularization weight close to 0.01. The network can tolerate a range of dropout rates but a dropout rate of 0.2 is commonly recommended in the literature and works well here too. The choice of the optimizer can affect the learning rate, the highest accuracy attained and the stability over several epochs. BowTie performs well with adaptive momentum (ADAM), Nesterov adaptive momentum (NADAM) and Root Mean Square Propagation (RMSPRop), no dense layer activation, L2-regularization set to 0.019 (this value resulting from hyperparameter optimization) and a dropout rate of 0.2. It is interesting to note that RMSProp tends to converge faster to a solution and sometimes with a higher validation accuracy than NADAM but the transfer accuracy of the models computed with NADAM tends to be higher than for models computed with RMSPRop. This experimental finding is consistent over many tests with the two optimizers and needs further investigation in future research to explore the theoretical basis for it. This shows the value word polarity brings in capturing the semantics of the text.
The results from DSTC2 are publicly available along with the output of the trackers on test data set The dont care values makes up 25% of the correct labels in the test data but only 15% in the development data, which is another reason for the difference in performance between the two datasets. Indeed, when we treat RNNTrack as an oracle to provide the dont care and null predictions (for all slots, not just slot food), we beat the baseline and come close to state-of-the art
First, note that the ImageNet-only embeddings don’t work as well as the FastText ones, which is most likely due to poorer coverage. We observe that DME outperforms naive and FastText-only, and outperforms VSE++ by a large margin. These findings confirm the intuition that knowing what things look like (i.e., having a word-level visual representation) improves performance in visual retrieval tasks (i.e., where we need to find relevant images for phrases or sentences)—something that sounds obvious but has not really been explored before, to our knowledge. This showcases DME’s usefulness for fusing embeddings in multi-modal tasks.
Our analysis showed that LEAR was particularly favored for verbs (with average weights of 0.75). The sentiment-refined embeddings were less useful, with the original GloVe embeddings receiving higher weights. These preliminary experiments show how DME models can be used for analyzing the performance of specialized embeddings in downstream tasks.
(mis)matches. X-axis represents the similarity score (in percentage) computed by the method, and Y-axis represents the number of (mis)matches found for a given similarity score. In white, in the upper part of the figures, the positives (units that needed to be matched), and in black, in the lower part, the negatives (units that should not be matched). This means that CL-ASA discriminates more correctly the positives that the negatives, when it seems to be the opposite for the other methods. For this reason, we can make the assumption that some methods are complementary, due to their different fingerprint. These behaviors suggest that fusion between these methods (notably decision tree based fusion) should lead to very promising results.
In a first study we wanted to understand how the features influenced the results. For this reason, we tried some combinations of Features, Embeddings and Models on both Task 6 Subtask A and Task The table shows that, in the big data regime, the Random Forest works best when only the Universal Encoder is used, while the Transformer model improves its performance when the features are added. On the other hand, in the low data regime, we see that the plain Random Forest outperforms all the other combinations. This is probably because the more things we add, the more the model needs to learn, and with little data this is simply not possible. The results are that for Subtask A, the model T + CO + F F1 decreased from 0.75 to 0.73 while for Subtask C, the RF F1 decreased from 0.54 to 0.44.
The En-Zh Experiments Results are measured using char based 5-gram BLEU score Papineni et al. On both of the development set and test set, our model significantly outperforms the baseline models and other contrast models. Furthermore, we got the following conclusions:
As expected, the best performance is obtained with the simultaneous use of all the tested elements. When we removed the private encoder-decoder, the result shows that the score was reduced by 0.79, which indicates that our private part can preserve some useful domain specific information which is abandoned by the shared encoder. When we removed the discriminator, the result was reduced by 0.76. This result supports our idea that modeling common features from out-of-domain data can benefit in-domain translation. When we removed both of the two components, we got the lowest score. The total result shows that every component of our model plays an important role in our model.
Despite the fact that the purpose of our work is to improve the in-domain translation performance, the domain invariant features extracted from the training data are also beneficial to the out-of-domain translation performance. To prove this, we use the NIST 03 04 and 05 test sets which are mainly related to the News domain as our out-of-domain test set. Noting that the origin set was designed for the Zh-En translation task and each sentence has four English references, we just chose the first reference as the source side sentence for our En-Zh translation task. On the contrary, our method can achieve a mild improvement on the out-of-domain compared to the baseline system.
Combined With Transformer Model Transformer Vaswani et al. To test the generality of our method, we also conducted relevant experiments based on the transformer model. The implementation on this translation framework is similar with the way on the RNN based models. The encoder and decoder of our final model consist 3 sublayers. The number of the multi-head attention was set to 4 and the embedding dim was set to 256. We also compared with the ’Sampler’ and ’Fine Tune’ method based on transformer. According to the table, our method still outperforms than other models, which can prove that our method has a good generality across different translation architecture.
Several observations can be made here. 1). Comparing the two set of experiments, the experimental results from models trained from scratch consistently outperform the results from NMT encoder probing on all tasks. 2). The models with syntactic information (Rows 3-4, 7-8) significantly perform better than those models without incorporating syntactic information (Rows 1-2, 5-6). 3). For NMT probing, the proposed models outperform the baseline model especially on relative small granularity of phrases information, such as ‘SPC’ and ‘POS’ tasks. 4). If trained from scratch, the proposed models achieve more improvements on predicting larger granularities of labels, such as ‘TSS’, ‘Tense’ and ‘Voice’ tasks, which require models to record larger phrase of sentences Shi:2016:EMNLP. The results show that the applicability of the proposed Mg-Sa is not limited to machine translation, but also on monolingual tasks.
In all settings, the fixation rates are similar (60.4% to 62.1%) which makes the perplexity figures directly comparable. While NEAT has a higher perplexity on both tasks compared to full attention, it considerably outperforms random attention. It also outperforms the word length, word frequency, and full surprisal baselines. The perplexity on human fixation sequences is similar to that achieved using word frequency.
For NEAT, this method of evaluation is problematic as differences between model predictions and human data may be due to differences in the rate of skipping, and due to the inherently stochastic nature of fixations. We therefore derive model predictions by rescaling the simulated fixation probabilities so that their average equals the fixation rate in the development set, and then greedily take the maximum-likelihood sequence. That is, we predict a fixation if the rescaled probability is greater than 0.5, and a skip otherwise. As in previous work, we report the accuracy of fixations and skips, and also separate F1 scores for fixations and skips. As lower and upper bounds, we use the random baseline ω∼Binom(n,0.62) and the agreement of the ten human readers, respectively. NEAT clearly outperforms the random baseline and shows results close to full surprisal (where we apply the same rescaling and thresholding as for NEAT). This is remarkable given that NEAT has access to only 60.4% of the words in the corpus in order to predict skipping, while full surprisal has access to all the words.
To evaluate the predictions NEAT makes for reading times, we use linear mixed-effects models containing restricted surprisal derived from NEAT for the Dundee test set. The mixed models also include a set of standard baseline predictors, viz., word length, log word frequency, log frequency of the previous word, launch distance, landing position, and the position of the word in the sentence. We treat participants and items as random factors. As the dependent variable, we take first pass duration, which is the sum of the durations of all fixations from first entering the word to first leaving it. We compare against full surprisal as an upper bound and against random surprisal as a lower bound. Random surprisal is surprisal computed by a model with random attention; this allows us to assess how much surprisal degrades when only 60.4% of all words are fixated, but no information is available as to which words should be fixated.
Impact of using encoder attention In this ablation, we remove the attention mechanism in the encoder and just pass the vanilla document representation to the decoder. We hypothesize that without attention, the model lacks the capability to identify the import parts of document and hence generates questions unrelated to the target answer. Impact of masking While attending to the answer’s context and the related sentences is crucial, we find that it is imperative to mask out the answer before getting the input representation.
We evaluate trained models on the VIST dataset. Compared with the performance of baselines Huang et al. , the GLAC Net is also competitive without beam search methods. From the results of ’GLAC Net (-Count)’ and ’Baselines (-Dups)’ Compared to LSTM Seq2Seq models, GLAC Net-based model shows better performance in general. Although the differences are not much significant between the GLAC Net experiment settings, the complete GLAC Net shows the best overall performance.
Eq. and for the accuracy of the layperson (ACC). By looking at the ACC column, we see a consistent drop from the RNN classifier to the layperson, regardless of the explainer. This is expected, since the layperson is a much weaker BoW classifier, and only has access to a limited number of words in the document. Note, however, that for some explainers (all the attention-based ones), this layperson outperforms a BoW classifier with access to all words. This is reassuring, as it shows that the layperson guided by the explainer outperforms an unguided layperson. The conclusion for SNLI is essentially the same as in text classification. We also see that sparse attentions consistently perform better than other explainers. Moreover, when we truncate the attention distribution to top-k words the results are better then their fully embedded counterparts.
We show the CSR as we varied k∈{0,1,3,5}. Again, top-k attention performed better than top-k gradient, in this case with a wider margin. In general, all methods perform better as we increase k, but we can already see a degradation of performance when k=5 for all attention based explainers. An interesting case is when k=0, meaning that L has no access to the source sentence, behaving like an unconditioned language model. In this case the performance is much worse, indicating that both explainers are selecting relevant tokens when k>0.
The main purpose of this section is to present how using reduced precision on the GPU can influence the calculation time and memory footprint of the whole training process. The results include data copying time from the processor to the GPU and vice-versa. The time needed to calculate the dot product is comparable with a less complex vector multiply vector operation because a single number is the result of this operation, thus the time necessary to copy data is not visible. Realizing all training with half precision has achieved a 1.26 x speed increase over single precision and 1.76 x over double precision. When employing the Reuter’s matrix on the GPU, 16.8 [MB] is occupied for double-precision. By using single and half precision, this number is decreased by two and four times, respectively.
The ESL approach is highly competitive with the WALS based results, yielding comparable accuracies for the shared-all prediction, and lagging only 1.7% – 3.4% behind the shared-pairwise construction. Also note that for both WALS based and ESL based predictions, the highest results are achieved using the hierarchical tree predictions, confirming the suitability of this representation for accurately capturing language similarity structure.
Besides analyzing the cosine similarity of the learned representations, we also apply them to the query-by-example STD task. Here we compare the retrieval performance in MAP of SA with different levels of accessibility to the low-resource target language along with two baseline models, NE and SA trained purely by the target languages. For the four target languages, the total available amount of audio word segments in the training set were 4 thousands for each language. The amount of audio word segments in these partitions are: 1K, 2K, 3K, 4K, and 0, which means no fine-tuning. Comparing with NE, SA surpasses NE in German and French even without fine-tuning, whereas in Czech, SA also achieves better score than NE with fine-tuning. However, in Spanish, SA achieved a MAP score of 0.13 with fine-tuning, slightly lower than 0.17 obtained by NE. Back to Fig. Also, as discussed earlier in Section 6.2, the variance in Spanish is also bigger. The smaller gap and bigger variance together indicate that the model is weaker on Spanish at identifying audio segments of different words and thus affects the MAP performance in Spanish.
We use the model that was trained on the CTD data and make it predict entities for every mention on the test set of CDR. We follow the standard practice of using the gold mention boundaries for evaluation only, to not confound the entity linking performance with mention-detection performance. Note that the SNERL model additionally benefits from jointly predict entities and relations. Breakdown of the results into Chemical and Disease prediction performance can be found in Supplementary. Overall, our results indicate that this particular task is extremely challenging. This is likely the combination of several difficulties. Since we rely on the candidate set to filter the annotations for the documents, we might end up with significant annotations that are not present in the title and abstract. Lastly, dealing with out-of-vocabulary entities at test time required additional pre-training, and our analysis indicated that these are not highly predictive for mention-level disambiguation due to the sparsity of the graph training data. Looking into more sophisticated embedding methods [
We can see that the LASO models achieve good performance with very low latency. LASO1 achieves a 7.3% of CER on the test set. LASO2 achieves a 7.0% of CER on the test set. Both LASO1 and LASO2 outperform chain model (7.4%) [povey2016purely], without speed perturbation. And it is very close to the state-of-the-art transformers (6.7%) and our re-implemented transformer model (6.6%). These results confirm our idea: if the implicit language semantic is captured, prediction of tokens without explicit relationship among tokens is feasible. We can see that the latency of LASO models is much smaller than autoregressive models. The speed-up is about 50×. The non-autoregressive structure makes LASO do not need multi-pass forward computation in beam-search. And the feed-forward structure of LASO makes parallel computation efficient.
From the table, we can see that across all metrics, the concatenation of dense features with embedded sparse features (i.e., Concatenation method) consistently leads to better results than both Sparse-only and Dense-only models. An interesting finding is that even without any sparse features (Dense-only), the model achieves reasonably good performance, probably because emails that are more recent or with more attachments are more likely to be the user-desired ones. We note that an improvement of 1% is considered to be highly significant for our email search ranking system.
We evaluate our baseline LSTM and best performing CNN (5 *1 filter with 28 RBs) on the Eval2000 corpus. We train each model to 50 epochs with early stopping on validation data. We augment our models with 7-gram and 9-gram character-level language models (LMs). For all experiments, a beam size of 200 was used. We choose α=0.6 and β=1.5 after tuning on validation data. Notice that in the no LM results our CNNs are only 0.2% behind on the SWB part of Eval2000, but a larger 1.1% behind on CH. After LM decoding, the differences are more pronounced. This indicates that CNNs seem to over-fit more on the training data (which is similar to the SWB part of Eval2000) and show less improvement with the help of LMs.
The implemented solutions are described in detail in the next section. The full table with shared task results as well as brief description of each participating system is available in the official report. The results on the SynTagRus gapping test set in particular show that systems trained on the AGRR-2019 corpus are able to yield reasonably good results on a dataset obtained without any usage of the Compreno parser. While both systems experience a performance drop relative to scores on the AGRR-2019 test set, this can be attributed to domain shift (as two corpora have different genre composition etc.).
The discrepancy between the F1 score and the accuracy stems from the fact that some labels, s.a. bye_general, occur very frequently (4759 times) compared to most other labels, s.a. recommend_restaurant_select_restau rant, which only occurs 11 times. This is not surprising since the modular approach receives additional supervision. The third column shows that the modular TED policy makes the same kinds of mistakes: instead of predicting only request_train, it predicts to take both actions, inform_train and request_train in the second turn. In the final turn, instead of request_train, the modular TED policy predicts reqmore_general, which means that the wizard asks if the user requires anything else. This reply is perfectly sensible and does in fact occur in similar dialogues of the training set (see, e.g., Dialogue PMUL1883). Thus, the correct behaviour doesn’t exist and it is impossible to achieve high scores, as reflected by the test scores of Table
By using multiple microphones, we could have reduced CERs, especially by using a large number of microphones. Note that in two-, three-, and six-microphone settings, using more microphones not always resulted in better CERs. This is because the sets of microphones in these settings are disjoint and the CERs highly depended on the positions of microphones and speakers. On the other hand, we observed the best CERs in almost every session by using all the 11 microphones. This result indicated that adding microphones has almost no negative effect on CERs. It achieved the CER of 21.8\char37, which is only 2.1 percentage points worse than the CER of 19.7\char37 obtained using headset microphones. It can be said that our method can potentially achieve nearly headset-level CERs when it is used with a more powerful diarization method [fujita2019end2, medennikov2020stc, horiguchi2020endtoend]. Combinations of speaker characteristics based features and power ratio based features improved transcription performance, especially when the number of microphones is smaller and the power ratio thus has less information about the directions of speakers. Finally, we conducted ablation studies by removing binary closing in diarization, speech enhancement by using recordings of the reference microphone instead, and duplication reduction, respectively. Here we used 11 microphones with λ=1.0. We found 1.9, 9.1, and 3.2 percentage points degradation from the baseline by removing binary closing, speech enhancement, and duplication reduction, respectively. From there results, we concluded that these three components contributed to the improvement of the CER.
By using multiple microphones, we could have reduced CERs, especially by using a large number of microphones. Note that in two-, three-, and six-microphone settings, using more microphones not always resulted in better CERs. This is because the sets of microphones in these settings are disjoint and the CERs highly depended on the positions of microphones and speakers. On the other hand, we observed the best CERs in almost every session by using all the 11 microphones. This result indicated that adding microphones has almost no negative effect on CERs. It achieved the CER of 21.8\char37, which is only 2.1 percentage points worse than the CER of 19.7\char37 obtained using headset microphones. It can be said that our method can potentially achieve nearly headset-level CERs when it is used with a more powerful diarization method [fujita2019end2, medennikov2020stc, horiguchi2020endtoend]. Combinations of speaker characteristics based features and power ratio based features improved transcription performance, especially when the number of microphones is smaller and the power ratio thus has less information about the directions of speakers. Finally, we conducted ablation studies by removing binary closing in diarization, speech enhancement by using recordings of the reference microphone instead, and duplication reduction, respectively. Here we used 11 microphones with λ=1.0. We found 1.9, 9.1, and 3.2 percentage points degradation from the baseline by removing binary closing, speech enhancement, and duplication reduction, respectively. From there results, we concluded that these three components contributed to the improvement of the CER.
By using multiple microphones, we could have reduced CERs, especially by using a large number of microphones. Note that in two-, three-, and six-microphone settings, using more microphones not always resulted in better CERs. This is because the sets of microphones in these settings are disjoint and the CERs highly depended on the positions of microphones and speakers. On the other hand, we observed the best CERs in almost every session by using all the 11 microphones. This result indicated that adding microphones has almost no negative effect on CERs. It achieved the CER of 21.8\char37, which is only 2.1 percentage points worse than the CER of 19.7\char37 obtained using headset microphones. It can be said that our method can potentially achieve nearly headset-level CERs when it is used with a more powerful diarization method [fujita2019end2, medennikov2020stc, horiguchi2020endtoend]. Combinations of speaker characteristics based features and power ratio based features improved transcription performance, especially when the number of microphones is smaller and the power ratio thus has less information about the directions of speakers. Finally, we conducted ablation studies by removing binary closing in diarization, speech enhancement by using recordings of the reference microphone instead, and duplication reduction, respectively. Here we used 11 microphones with λ=1.0. We found 1.9, 9.1, and 3.2 percentage points degradation from the baseline by removing binary closing, speech enhancement, and duplication reduction, respectively. From there results, we concluded that these three components contributed to the improvement of the CER.
We run MOLIERE to generate topic models related to published, noise, and highly-cited pairs. These plots represent an analysis of 8,638 published vs. noise (PvN) pairs and 2,896 (HCvN) pairs (half of each set are noise). Unfortunately, no alternative general-purpose query HG systems that perform in a reasonable time are freely available for the comparison with our ranking methods. Combination of metrics, PolyMultiple, significantly outperforms all others with ROC areas of 0.834 (PvN) and 0.874 (HCvN). This is unsurprising because each other metric makes a different assumption about what sort of topic or vector configuration best indicates a published pair. When each is combined, we see not only better performance, but their relative importances. By studying the coefficients of our polynomial we observe that the two L2-based metrics are most important, followed by the topic network methods, and finally by TopicWalkCorr and BestTopicPerWord. Additionally, the ordering of importance roughly follows the same ordering as the ROC areas.
Note that the results were obtained in a superscalar CPU, it can execute many instructions in a single cycle, thus the amount of cycles used can be lower than the total amount of instructions.
For reference we provide two baselines provided by the organizers: the BOW-baseline, a bag-of-words model with the 1,000 most frequent items and the STAT-baseline, a simple majority class baseline. As observed in the cross-validation experiments, the best results in the test set were also obtained when discriminating between the two Portuguese varieties achieving 0.9788 accuracy. On language variety identification our system achieved an average performance of 0.8524 accuracy ranking 11th among 22 shared task entries.
We complement our own findings with those from existing pre-trained models. To this end we fine tuned a BERTBASE architecture on all nine GLUE tasks. These were compared directly against BioBERT, which has been further trained on full Pubmed articles. BioBERT performed negligibly better than original BERT on only a single task (MRPC). Furthermore, we observed that on tasks which BERT struggles with, such as CoLA and WNLI, the performance decrease is amplified when switching pre-training domains.
TT refers to the tweet text, IT to the image text and I to the image. It also shows results for the LSTM, for the Davison method proposed in [Davidson2017] trained with MMHS150K, and for random scores. Fig.
Sparsity Analysis: Sparsity is evaluated using the following Sparse Evaluation (SE) function. We proposed this method because previous methods were not designed for sparse representations with both positive and negative values: SE(D)=1|D||D|∑i=1(sin(πyi))2 (13) As function (sin(πy))2 has only three minimum points, -1, 0, 1, it is suitable for measuring the concentration degree of the components of sparse representations. We can see that ‘zero’ (V<0.05) takes a large portion of the sparse representations, which is desirable. We can conclude that the learned sparse representations are indeed sparse.
To verify this claim, we conduct human evaluations comparing captions from the baseline and the adversarial model. Human evaluators from Amazon Mechanical Turk are shown an image and a caption each from the two models and are asked “Judge which of the two sentences is a better description of the image (w.r.t. correctness and relevance)!”. The choices were either of the two sentences or to report that they are the same. We can see that both adversarial and baseline models perform similarly, with adversarial models doing slightly better. This shows that despite the poor performance in automatic evaluation metrics, the adversarial models produce captions that are similar, or even slightly better, in accuracy to the baseline model.
The UPOS targets were obtained using StandfordNLP Qi et al. Dropout with a drop probability of 0.2 was applied to the encoder.
The differences in the number of groups in each language are due to different availabilities of sentences and sentence-types in the Tatoeba dataset. The high nearest neighbours accuracy indicates that syntax information was successfully captured by the embeddings. Arguably these language models were trained using different training data. However, this is a reasonable comparison because many real-world applications rely on released pre-trained language models for syntactically related information. Hence, we want to show that we can use much smaller models trained with direct supervision, to obtain syntactic embeddings with similar or better quality. Nonetheless, the training method used in this work can certainly be extended to architectures similar to BERT or USE.
We computed the nearest neighbours experiment for all languages in the training data for the above models. The results show that general purpose language models do capture syntax information, which varies greatly across languages and models.
Completing the entire sequence of actions required to finish a task is challenging. In addition to assessing full task success, we study the ability of a model to accomplish the next sub-goal conditioned on the preceding expert sequence. The agent is tested by first forcing it to follow the expert demonstration to maintain a history of states leading up to the sub-goal, then requiring it to complete the sub-goal conditioned on the entire language directive and current visual observation. For the task “Put a hot potato slice on the counter” for example, we can evaluate the sub-goal of navigating to the potato after using the expert demonstration to navigate to and pick up a knife. Goto and Pickup sub-tasks with the Seq2Seq+PM model achieve ∼51% and ∼32%, respectively, even in seen environments. Visual semantic navigation is considerably harder in unseen environments. Similarly, interaction masks for Pickup actions in unseen environments are worse due to unfamiliar scenes and object instances. Simple sub-goals like Cool, and Heat are achieved at a high success rate of ∼90% because these tasks are mostly object-agnostic. For example, the agent becomes familiar with using microwaves to heat things regardless of the object in-hand, because microwaves have little visual diversity across kitchens. Overall, the sub-goal evaluations indicate that models that exploit modularity and hierarchy, or make use of pretrained object segmentation models, may make headway on full task sequences.
We test the master model on a subset of 1240 leaf nodes, each having 40 products. For each category, 32 products (80%) are randomly chosen as the training set, and the rest as test set. The classification accuracy is high, indicating the powerful generalization ability of our multi-modal model. The quality of our dataset also contributes to the high accuracy, as any defects within categories will lower the prediction accuracy. The training process takes 10 hours on a single AWS p3.2xlarge GPU instance for 50 epochs to achieve 94.7% accuracy and become stabilized. Though the high classification accuracy demonstrates the advantage of the master model structure, we would like to highlight the contribution of the dataset being of high quality. With a properly designed taxonomy and accurately annotated products, the demand for sophisticated models decreases, while the model transferability increases.
Similar trend as the ATIS experiments is witnessed in which the augmentation improves the LU performance. The average improvement on the training data with 100 utterances is 10.04, and the number is 0.47 for that with 500 utterances. Considering that only fewer than 350 utterances present in the test set in all these domains, these improvements are reasonable. Besides, similar to the ATIS results, the margin of improvements is larger for the smaller training set.
To get further understanding of each component in our method, we conduct ablation on the medium proportion, Each of the three parts of our method is removed respectively, including the seq2seq generation, diversity ranks, and filtering. In addition to evaluate the model’s performance with F-score, we also examine the augmented data by the number of newly generated delexicalised utterances and the maximum edit distances against the rest of instances. For our method without diversity ranks, we remove diversity ranks from the utterance representation and this lead a drop of 0.40 F-score. We address the drop of performance to the fact that removing either these components will lead to less diverse generation.
Firstly, it is observed that the best single-model performance is Ours w/ gec, which outperformed all baseline methods in terms of both BLEU and GLEU in E&M and F&R. Compared to the strongest baselines PBMT and NMT-PBMT, both of which incorporate heuristic rules with the training data, our models are end-to-end neural networks without any prior knowledge of the task. The performance of our single model is also competitive with the state-of-art ensemble model(MultiTask), which also utilizes additional supervision. w/o …) which exclude one specific loss in the overall objective function. From the results of our ablated models, it is clear that all the losses defined on classification data contribute to the improvement. Another observation is that the performance of Ablt. w/o self-recon is even lower than Transformer-Combine, which demonstrates the importance of the self-reconstruction loss to prevent classifier-guided loss from drastically changing the content of the input sentence.
We show counts for the number of occurrences of male pronouns (he, his and him) and female pronouns ( she and her) in the corpus as well as the co-occurrence of occupation words with those pronouns. We use the set of occupation words defined in the WinoBias corpus and their assignments as prototypically male or female Zhao et al. The analysis shows that the Billion Word corpus contains a significant skew with respect to gender: (1) male pronouns occur three times more than female pronouns and (2) male pronouns co-occur more frequently with occupation words, irrespective of whether they are prototypically male or female.
We also conducted human evaluation with 20 documents randomly sampled from the test split of CNNDM. We compared the best preforming Step model (i.e., pre-training on the GIGA-CM dataset using SR task) with human references (denoted as Gold), RoBERTa-S2S, and two pre-training based models, BERTAbs Liu and Lapata et al. Participants were asked to rank the outputs of these systems from best to worst. The output of Step is selected as the best for the 25% of cases and we obtained lower mean rank than all systems except for Gold, which shows the participants’ preference for our model. Then we converted ranking numbers into ratings (i.e., rank i is converted into 6−i) and applied the student t-test on the ratings. Step is significantly better than all other systems in comparison with p<0.05. But it still lags behind human. One possible reason is that Step (as well as other systems) only takes the first 512 tokens of a long document as input and thus may lose information residing in the following tokens.
Given a test set, we compare its translations generated from different systems as follows. First, for each translated set, we sort the sentences by how much the sentence-level Ter is improved over the baseline translation. Then, we select the top 200 sentences from this sorted list, which represent the main contribution to the decrease of Ter.
We compare our results with widely used baselines, including nonnegative matrix factorization (NMF), latent dirichlet allocation (LDA), Latent semantic analysis (LSA)+k-Means and TF-IDF+k-Means, gaussian mixture model (GMM) and probabilistic latent semantic analysis (PLSA). There are limited comparable other works on 20 category partion. Many of people focus on a subset of this problem, including clustering a group of selected categories, or a pair of categories. In Chen et al. Chen et al. Palla et al. Thus results obtained therein (with f-score of 10.0) are much lower than ours (see Tab. On 4 category partition, we also have the results in Zhang et al. Next, for a better illustration of the clustering performance, we only consider the selected 4 groups of categories (4-category simply speaking), which are ‘comp’, ‘rec’, ‘sci’ and ‘talk’. Here we use accuracy as the evaluation metrics in accordance with the work of Zhang et al. Zhang et al. We list our results together with conventional tools, such as k-Means, GMM, PLSA, and Max Margin Document Clustering with Universum (M2DCU)Zhang et al. Experiment results show that our method has the best performance with all evaluation metrics. Compared to 20-category results, here we obtain the consistent improvement amplitude on Accuracy, ARI and F-score because all 4 categories are predicted.
Firstly, it can be observed that with the small IWSLT05 dataset, the SMT outperforms the baseline NMT, but with the large NIST dataset, NMT outperforms SMT. This is unsurprising as neural models often need more training data. Secondly, the results show that with both datasets, the lexical approach (NMT-L) can improve NMT performance, showing that using SMT knowledge helps NMT. This is likely to be because our implementation focuses on creating a simple, extensible, and generalizable system, and therefore does not allow re-training the neural model.
We use a dropout rate of 0.5 for the utterance encoder and the conversation encoder, respectively. Gradient clipping with a norm of 5 is also applied to avoid gradient explosion. Each conversation in the training set is regarded as a batch, where each utterance plays the role of target utterance by turns. We randomly sample 10 noise utterances for each conversation during training and validate the model every epoch. The CoDE is pre-trained for at most 20 epochs, and early stopping with a patience of 3 is adopted to choose the optimal parameters. Note that, we fix the word embedding layer during pre-training to focus on the utterance encoder and the conversation encoder. Testing Results. For the small model PT-CoDEsmall, it is able to select the correct answer for 70.8% instances with 5 candidate answers and 56.2% with 11 candidates. The accuracy is considerably higher than random guesses, i.e., 1/5 and 1/11, respectively. By increasing the model scale to PT-CoDEmid and PT-CoDElarge, we further improve the recalls by several points successively. These results demonstrate that CoDE is indeed able to capture the structure of conversations and perform well in the proposed conversation completion task.
We used the ontologies, the entity annotations as well as the PubMed abstracts as the text corpora. To process this text data we used Word2Vec Word2Vec is a machine learning model based on neural networks that can be used to generate vector representations of words in a text. Word2Vec is optimized in such a way that the vector representations of words with a similar context tend to be similar. Word2Vec is available in two different models: the continuous bag of word (CBOW) model and the skip-gram model. In this work, we opted for the skip-gram model which has the advantage over the CBOW model of creating better quality vector representations of words which are infrequent in the corpus. This advantage is quite useful in our case since the biological entities we want to get representations for do not necessarily occur frequently in our text corpora. In this work, we pre-trained the Word2Vec model on the set of PubMed abstracts and save the obtained model which we eventually retrained on the ontology studied (the GO ontology and the PhenomeNET ontology). We used gridsearch to optimize the set of parameters of the skip-gram model used in this work. We used the same parameters to train Word2Vec on the PubMed data set and the ontologies data set, except for the min_count which has value 25 for the PubMed model, but changed to 1 before training on the ontology corpus.
The main takeaway is that the broad trends appear largely the same, though it is important to note the two categories that differ in a significant manner: “Verbal Cooperation” and “Material Conflict.” These differences largely come down to implantation details that differ between the BBN ACCENT coder and the PETRARCH coder. In short, the two coders implement slightly different definitions of the various CAMEO categories based on a perception on the part of the designers or end-users as to what constitutes an interesting and/or valid event within CAMEO. This point leads to a deeper discussion as to what, exactly, constitutes the CAMEO coding ontology; Chapter 5 contains a deeper discussion of these issues.
We also investigated if more informed thresholding strategies could provide better results. While the simple global thresholding approach might not be able to represent more complex structures, we also tested a more robust approach based on the local approach proposed by Serrano et al. The table shows maxΓ(L)+/maxΓ(G)+, where Γ(L)+ and Γ(G)+ are the accuracy obtained with the local and global thresholding strategy, respectively. The results were obtained with the SVM classifier, as it turned to be the most efficient classification method. We found that there is no gain in performance when the local strategy is used. In particular cases, the global strategy is considerably more efficient. This is the case e.g. when GloVe is employed in texts with w=1,500 words. The performance of the global strategy is 12.2% higher than the one obtained with the global method. A minor difference in performance was found in texts comprising w=1,000 words, yet the global strategy is still more efficient than the global one.
We compare our results to several state-of-the-art recurrent models for language modeling which either use a small number of parameters or is compressed from a larger model. We are unable to find baseline results on WT2 dataset, so we only compare over PTB dataset. Due to the large vocabulary size, LSTM-SparseVD and LR LSTM not only focus on compressing RNN layers but also on embedding and softmax layers. However, our restricted models only focus on compressing RNN layers. We also empirically add Dropout with rate 0.2 to regularize our models. Through various experiments we found that the final result is not very sensitive to small Dropout rates.
First, these methods show a consistent trend on both datasets over all metrics, i.e., DAM-WM > DAM > SMN-WM > SMN > other models. We can conclude that DAM and SMN are the best baselines in this task than other models because they can capture more semantic features from word-level and sentence-level matching information. Second, our method yields improvement in SMN and DAM on two datasets, and most of these promotions are statistically significant (t-test with p-value < 0.05). This proves the effectiveness of our instance weighting method.
First, most of these variants outperform DAM model. It demonstrates that these instance weight strategies are effective in noisy data training. Among them, DAM-WM achieves the best results for all the three evaluation metrics. It indicates that our proposed method is more effective. Second, the improvement yielded by heuristic methods is less than model-based methods. A possible reason is that neural networks own stronger semantic capacity and the weights produced by these models can better distinguish noise in training data. Third, heuristic methods achieve worse performance than DAM-uniform. It indicates that Jaccard similarity and cosine similarity of representation are not proper instance weighting functions and bring a negative effect on response selection model.
Recall that the Indepenent LSTM row is trained per-class and thus, it is not a valid solution for our benchmark. Nonetheless, it provides a reference point for the performance of a model that does not suffer from forgetting, but also cannot share knowledge between classes.
Length filtering (removing any sentence pair whose length ratio is greater than 1.8, or 1.5 for CommonCrawl): removes most hallucinations and gives the best BLEU score (when combined with LID filtering). This type of filtering is common in MT pipelines Koehn et al. Excluding CommonCrawl from the training data: removes all hallucinations, but gives worse BLEU scores, suggesting that, albeit noisy, CommonCrawl is useful to this task. A similar matrix is produced during the forward pass of training when facing a misaligned sentence pair. We filtered CommonCrawl as follows: we trained a baseline FR→EN model on WMT without filtering, then translated CommonCrawl while forcing the MT output to be the actual reference, and extracted the corresponding attention matrices. We computed statistics on these attention matrices: their entropy and proportion of French words with a total attention mass lower than 0.2, 0.3, 0.4 and 0.5. Then, we manually looked for thresholds to filter out most of the misalignments, while removing as little correctly aligned data as possible.
Effectiveness of multiple questions selection. (1) Remp achieves a stable F1-score on all datasets. (2) The number of questions increases when μ increases, especially when μ=10,20. This is probably because Remp always asks μ questions in one human-machine loop, and it has to ask an extra batch of questions when some questions with large benefit are labeled as non-matches. Although asking multiple questions in one loop increases the monetary cost, it reduces 75%–94.1% number of loops when μ=20.
Mean and standard errors are computed based on 5 different random seeds for the SGD/Adam training algorithm. The F1 scores for shallow neural networks, i.e., TextCNN and Bi-LSTM attention, are slightly better than SVM, which might be owed to the benefits of semantic features captured by GloVe word vectors. BERT-based deep neural networks such as BERT and BioBERT only marginally outperforms Bi-LSTM attention and TextCNN, which is likely caused by the small size (∼1.4K) of the training data. The F1 score of BioBERT is 0.3% higher than that of BERT, which means that pre-training on large biomedical corpora transfers extra beneficial information to BERT model. In terms of macro average F1 score, our LESA-BERT and its distilled variants achieve higher F1 scores compared with other classifiers. The performance gains can be attributed to the addition of the label embeddings.
Our approach using only content attention is abbreviated to MemNet (k), where k is the number of hops. We can find that feature-based SVM is an extremely strong performer and substantially outperforms other baseline methods, which demonstrates the importance of a powerful feature representation for aspect level sentiment classification. Among three recurrent models, TDLSTM performs better than LSTM, which indicates that taking into account of the aspect information is helpful. This is reasonable as the sentiment polarity of a sentence towards different aspects (e.g. “food” and “service”) might be different. It is somewhat disappointing that incorporating attention model over TDLSTM does not bring any improvement. We consider that each hidden vector of TDLSTM encodes the semantics of word sequence until the current position. Therefore, the model of TDLSTM+ATT actually selects such mixed semantics of word sequence, which is weird and not an intuitive way to selectively focus on parts of contexts. Different from TDLSTM+ATT, the proposed memory network approach removes the recurrent calculator over word sequence and directly apply attention mechanism on context word representations.
4.2.1 Comparison with four baseline methods. We can see the MMAP scores of SetExpan outperforms all four baselines a lot. We can see that the performance of these baseline methods varies a lot on different semantic classes, while our SetExpan can consistently beat them. One reason is that none of these methods applies context feature selection or rank ensemble, and a single set of unpruned features can lead to various levels of noise in the results. Another reason is the lack of an iterative mechanism in some of those approaches. For example, even if EgoSet includes the results from word2vec to help it boost the performance, it still achieves low MAP scores in some semantic classes. Finding the nearest neighbors in only one iteration can be a key reason. And although SEISA is applying the iterative technique, instead of adding a small number of new entities in each iteration, it expands a full set in each iteration based on the coherence score of each candidate entity with the previously expanded set. It pre-calculates the size of the expanded set with the assumption that the feature similarities follow a certain distribution, which does not always hold to all datasets or semantic classes. Thus, if the size is far different from the actual size or is too big to extract a confident set at once, each iteration will introduce a lot of noise and cause semantic drift.
First, the RNN based methods like MV-LSTM and SMN have clear advantages over the two CNN-based approaches like ARC- I and ARC-II, and are better or comparable with the state-of-the-art CNN-based models like Pyramid and Duet; Second, our MT-hCNN outperforms MT-hCNN-d, which shows the benefits of adding a convolutional layer to the output representations of all the utterances; Third, we find SMN does not perform well in AliMeData compared to UDC. One potential reason is that UDC has significantly larger data size than AliMeData (1000k vs. 51k), which can help to train a complex model like SMN; Last but not least, our proposed MT-hCNN shows the best results in terms of all the metrics in AliMeData, and the best results in terms of R@2 and R@1 in UDC, which shows the effectiveness of MT-hCNN. (∼60% time reduction). MT-hCNN also has similar efficiency with CNN-based methods but with better performance. This shows the model is applicable to industrial bots. In all, our proposed MT-hCNN is shown to be both efficient and effective for question matching in multi-turn conversations.
This shows the source and target domains are related but different. Despite the domain shift, TL-S is able to leverage knowledge from the source domain and boost performance; Last, our model shows better performance than TL-S, this shows the helpfulness of adding domain discriminators on both source and target domains.
First, we evaluate the mapping accuracy of word embeddings using Bilingual Lexicon Induction (BLI). BLI is the task of identifying word translation pairs using two monolingual corpora, and is used for evaluating BWE methods. In these experiments, we use Cross-Domain Similarity Local Scaling [lample2018word] as the method for identifying translation pairs in the two embedding spaces. We use the mean reciprocal rank (MRR) [glavas-etal-2019-properly] and P@1 as BLI scores. For En-Ja, we use the word dictionaries automatically created using Google Translate. For each setting other than BLI from a phrase table, we train three sets of embeddings with different random seeds and show the average of the results. In all the language pairs, the mapping method with pseudo data augmentation achieves better accuracy than the other methods. Here, one may think that the greater amount of data can lead to better accuracy and thus augmenting both the source and target corpora shows the best performance. However, the result shows that it does not necessarily hold true; for our mapping method, augmenting only either the source or target, not both, achieves the best performance in most language pairs. As for the performance of the unsupervised joint-learning method, in the original work of Marie et al. \shortcitemarie-fujita-2019-unsupervised, their method outperforms the baseline, i.e., unsupervised mapping with the original corpora, but in our experiment, we observe the opposite tendency. We attribute this to the fact that the data sets used in their experiments are an order of magnitude larger than ours, and in our relatively low-resource setting, the quality of the synthetic parallel data is not sufficient to perform reasonable joint learning. In the next subsections, we analyze why our method improves the BLI performance. Through carefully controlled experiments, we argue that it is not simply because of data augmentation but because: (1) the generated data makes the source and target corpora (partially) parallel; (2) the generated data reflects the nature of the original language.
It is shown that the quality of the monolingual word embeddings using a pseudo corpus generated from French and German, which are relatively linguistically similar to English, is maintained. In contrast, the accuracy has decreased in Japanese, which is linguistically far from English. In the proposed method, the BLI accuracy in en-ja BWEs is improved despite the lower quality of the monolingual word embeddings. This result suggests that word embedding spaces trained with translated sentences can have a structure specialized for bilingual word mapping.
M. A surprising result is the fact that a triphone equivalent BAM (M=1) that does not use word boundary information is significantly weaker than its counterpart that uses that information. Increasing the model order improves performance in both context settings. The residual WER is due to homophones.
Contrary to the automatic evaluation results for ACT, the AIF-emb setup is generally favoured by human preference. text-only and AIF-conv4 demonstrate similar performance.
We compare our approach with various representative systems on Flickr30k and COCO, including the recently proposed NBT that is the state-of-the-art on the two datasets in comparable settings. As we can see, our model outperforms the comparable systems in terms of all of the metrics except BLEU-4. Moreover, our model overpasses the state-of-the-art with a comfortable margin in terms of SPICE, which is shown to correlate the best with human judgments Anderson et al.
We put it in the appendix because the results are incomplete and the SPICE metric is not available for our submission, which correlates the best with human evaluation. Our submission does not directly optimize CIDEr, use model ensemble, or use extra training data. The three techniques typically result in orthogonal improvements Lu et al. Moreover, the SPICE results are missing, in which the proposed model has the most advantage. Nonetheless, our model is second only to Up-Down Anderson et al.
The tied model, whose likelihood is the product of the Bayesian Echo Chamber’s likelihood and that of Blundell et al.’s model but with shared influence parameters, assigned lower probabilities to held-out data than the fully factorized (i.e., untied) model.
Metric-based Analysis For BLEU scores, here we only list results for BLEU-1 and 4. Other BLEUs show similar pattern and will be listed in Supplementary material. As clearly seen, VMED models outperform other baselines over all metrics across four datasets. In general, the performance of Seq2Seq is comparable with other deterministic methods despite its simplicity. Surprisingly, CVAE or VLSTM does not show much advantage over deterministic models. As we shall see, although CVAE and VLSTM responses are diverse, they are often out of context. Among different modes of VMED, there is often one best fit with the data and thus shows superior performance. The optimal number of modes in our experiments often falls to K=3, indicating that increasing modes does not mean to improve accuracy.
It does not apply any discounting to relative frequencies, and it uses a single backoff weight instead of context-dependent backoff weights. As a result, the Stupid Backoff model does not generate normalized probabilities. values output by the model were normalized over the entire LM vocabulary. We trained several models with varying number of neurons The MaxEnt models running in parallel to the RNN capture a history of 9 previous words, and the models use as additional features the previous 15 words independently of order. While training times approach 2 weeks for the most complex model, slightly worse models can be trained in a few days. Note that we didn’t optimize for model size nor training speed, only test performance. We focused on minimizing the perplexity when choosing hyper-parameters, however we also report the time required to train the models. Training times are not necessarily comparable as they depend on the underlying implementation. Mapreduces can potentially process larger data sets than single-machine implementations, but come with a large overhead of communication and file I/O. Discussing details of the implementations is outside the scope as this paper.
The best perplexity results were achieved by linearly interpolating together probabilities from all models. However, only some models had significant weight in the combination; the weights were tuned on the held-out data. This corresponds to about 10% reduction of cross-entropy (bits).
The feature-based baseline CRF outperforms the baseline of the neural net with more than 20 percentage points. After adding the feature information, the performance of the neural baseline is improved by 13 percentage points, which is understandable, because many German POS tags are case sensitive.
Compared with TF, both of CMOS score (-0.04) and preference (-2.2%) show that the performance of SS is worse, but the intelligibility is improved on the pathological test set. In the comparison between TF-GAN and TF, the votes on TF-GAN is 5.33% more than TF when 50% of the votes are neutral. TF-GAN shows significantly better performance than TF with a higher CMOS (0.1) and preference (5.33%). It also achieves a lower unintelligible rate (4%) than TF (11.1%) and SS (6.22%). So compared with SS, GAN-based training algorithm is more effective. It can improve both naturalness and generalization for end-to-end TTS. As the combination of SS and GAN-based training algorithm, SS-GAN can further improve the intelligibility rate (2.22%). SS-GAN does not achieve improvement in speech quality and naturalness due to SS, but has better performance in model generalization.
Let us see the results on supervised recall (80-20 SR) first, as it is the main indicator for the task. Overall, SE-WSI-fix-cmp, which jointly learns sense embedding for 6K words, outperforms every comparing systems which learns for each single word. This shows that sense embedding is suitable and promising for the task of word sense induction. Trained on out-of-domain data, SE-WSI-fix outperforms most of the systems, including the best system in the shared task (UoY), and SE-WSI-CRP works better than Spectral and all the baselines. This also shows the effectiveness of the sense embedding methods. Besides, SE-WSI-CRP is 1.7 points lower than SE-WSI-fix. Since both systems induce fewer senses than the golden standard which is 3.85, inducing fewer senses harms the performance. Finally, simple as it is, NB shows a very good performance. However NB can not benefit from large-scale data as its number of parameters is small, and it uses EM algorithm which is generally slow. Sense embedding methods have other advantages that they train a general model while NB learns specific model for each target word.
In this subsection, we employ various binary classifiers to detect microtext so as to reduce the execution time of the overall algorithm. We observed that the execution time of polarity detection task was reduced by 20%. Different classifiers were trained on the two datasets namely We use the term frequency–inverse document frequency (TF-IDF) We first split the document into tokens assigning weights to them based on the frequency with which it shows up in the document along with how recurrent that term occurs in the entire corpora. We used this approach to train four different classifiers. The proposed resource contains concepts from SenticNet and their phonetics by using Epitran which we name as PhonSenticNet. This resource is used as a lexicon for microtext normalization. The input sentence is broken down into concepts and then transformed into their phonetic encoding. The phonetic encoding is matched with the PhonSenticNet, the resource built in this work. We have taken Sorensen similarity to measure the distance. The similarity is shown at both string and phonetic level. The reason behind it is, the twitter dataset contains acronyms like lol, rofl, etc instead of phonetic substitution. This also suggests how the way of writing differs in both messages and tweets.
The WSJ test set have shown better recognition performance compared to TED-LIUM. The reference and the test sentences are annotated using DBpedia Spotlight. The number of entities is higher for test sentences since it also contains inserted and substituted words in place of Out-Of-Vocabulary (OOV) words.
Basic blocking. Our first challenge is size of the datasets. Thus, a naive pairwise comparison is not feasible. The first blocking method we designed is to only match companies with the same zipcode. However, since 60% of records in Table A do not have the zipcode attribute and some large employers have multiple sites, we use a second blocking method that returns for each record in Table B the top-20 most similar records in A ranked by the TF-IDF cosine similarity of name and addr attributes. We use the union of these two methods as our blocker, which produces 10 million candidate pairs.
We now show that in addition to paraphrasing, sent2vec is useful for text summarization. Sentences from detailed descriptions of each video sequence are first converted into vectors using our model. The performance of the summarized text is evaluated based on the metric scores and compared to skip-thoughts and skip-gram. Note that skip-gram is used as the frequency-based average of word2vec for each word in the sentence. This result is reasonable since the dataset used in training sent2vec are all from captions. The styles and topics of the sentences in this dataset are limited. However, the approach of forming sentence paraphrasing pairs and representing sentences using vectors are valid.
’s datasets. The template-based, slot-filling baseline was competitive with state-of-the-art systems for question split on the four datasets from the NLP community. The template-based oracle performance indicates that for these datasets anywhere from 70-100% accuracy on question-based split could be obtained by selecting a template from the training set and filling in the right slots. These results also seem to indicate that our copying mechanism effectively deals with entity identification. Across all datasets, we see only a small number of entity-problem-only examples.
We achieve a 5–8% relative improvement from i-vectors, including on CNN systems. We see a consistent 7–10% further relative reduction in error rate for all models. Considering the great increase in procedural simplicity of LFMMI over the previous practice of writing lattices and post-processing them, we consider LFMMI to be a significant advance in technology.
As we can see, all of our models substantially outperform the baselines in terms of averaged BLEU score of all the test sets. Among them, our best model achieves 45.65 BLEU based on Transformer architecture. We also find that redundant capsules are helpful while discarding them leads to -0.35 BLEU degradation (45.65 vs 45.30).
We evaluated our approach on WMT14 En-De and WMT16 En-Ro tasks. The results show a consistent trend of improvements as NIST Zh-En task on WMT14 En-De (+0.96 BLEU) and WMT16 En-Ro (+0.86 BLEU) benchmarks. We also list the results of other published research for comparison, where our model outperforms the previous results in both language pairs. Note that our approach also surpasses \newcitekong2018neural on WMT14 En-De task. These experiments demonstrate the effectiveness of our approach across different language pairs.
We observe that our proposed model outperforms the Transformer baseline system by a significant margin. As we expected, the redundant capsules are vital for abstract summarization task, in which summary is required to be short and concise by discarding tons of less important contents from the original article. Our model also beats most of the previous approaches except the model of \newcitechang2018ahw, which benefit from a hybrid word-character vocabularies of max size (more than 900k entries) and data cleaning. We expect our model to benefit from these improvements as well. In addition, our model does not rely on any extraction-based methods, which are designed for abstract summarization task to extract relevant parts from the source article directly (e.g., CopyNet P16-1154). Our method may achieve further gains by incorporating these task-specific mechanisms.
We can find that the performances of the four methods in greedy and DP setting are consistent. BOW (Boolean) outperforms BOW (Frequency) consistently. This indicates that whether a word occurs in two sentences could indicate the relatedness of two sentences and it does not need to consider how many times a word occurs in each sentence. Average based embedding method performs better than bag-of-word methods by considering the continuous word and sentence representation in some latent semantic space. We can find that recursive neural network method performs best in each setting, outperforming three previous similarity based methods. This shows the effectiveness of a powerful semantic composition model as well as the necessary to model the relatedness between two sentences rather than a cosine based similarity measurement.
We also analyzed the performance of our two models while reducing the amount of supervised task specific tuning data. For all tasks, we see that MTB based training is even more effective for low-resource cases, where there is a larger gap in performance between our bert\textscEM and bert\textscEM+mtb based classifiers. This further supports our argument that training by matching the blanks can significantly reduce the amount of human input required to create relation extractors, and populate a knowledge base.
We define the emoji sense disambiguation accuracy for an emoji as the ratio between the number of correctly sense disambiguated messages (tweets) and the number of total sense disambiguated messages for that emoji. Among the 25 emoji in our dataset, \emoji1F609 gives the highest sense disambiguation accuracy of 0.61. We observe that Twitter-based context vectors outperforms the other two context vectors constantly, except for disambiguating the sense of \emoji1F601. The average number of Twitter-based context words for an emoji sense definition was very high compared to that of BabelNet-based contexts. These evaluation results validates the importance of the improvements we made to EmojiNet by introducing context word vectors learned by Twitter and Google News corpuses.
Once the graph is computed, we can use any traditional semantic, path or set similarity measure to find the sense similarity between any two emoji in the graph. Both emoji have 12 sense labels each, shares 9 sense labels, and have 15 unique sense labels between them. Thus, the sense similarity between them can be calculated as the ratio between 9 and 15, which gives 0.60. We can replace Jaccard Similarity with a sophisticated similarity measure to improve the results shown. The emoji similarity dataset we created using Jaccard Similarity is available to download at http://emojinet.knoesis.org/.
where D is the set of all the sentences in the dataset and a predicted causal triplet is regarded as correct if and only if it precisely matches a labeled causal triplet. The first part is the pipeline methods (from row 2 to row 3). The second part (row 4 to row 5) is the CNN-based sequence tagging method. The third part (row 6 to row 9) is the BiLSTM-based sequence tagging method, and the fourth part (row 10 to row 13) is the sequence tagging method using contextualized word embeddings. Our SCITE model is shown in the final part, where the first row is the result of SCITE based on the proposed tagging scheme and the second row is the result of SCITE based on the general tagging scheme. This demonstrates the effectiveness of our proposed method. Furthermore, it also shows that the sequence tagging models are better than pipeline methods.
Furthermore, we compare the tagwise performance of our SCITE model with baselines. First, we observe that our model achieves No. 1 in tags “C” (including “B-C” and “I-C”), “E” (including “B-E” and “I-E”), and “Emb” (including “B-Emb” and “I-Emb”) in terms of F1-score. Second, we also notice that the F1-scores are approximately 0.9 except for tag “Emb” because of its low frequency (only 110 instances) in the training set. In particular, it can be seen from the confusion matrix in Fig.
From the table, we could observe that HSR-RR dominantly outperforms the original word embedding as well as other postprocessing methods, as the average result by year of HSR-RR is the best for all tasks except the SICK task on Word2Vec. On average, HSR-RR improves the Pearson correlation coefficient by 4.71%, 7.54%, and 6.54% respectively over the 20 STS tasks compared to the previously best results, and it achieves 7.13%, 22.06%, and 9.83% improvement respectively compared to the original word embeddings.
Similar to the STS tasks, we first tokenize the sentence, then average the corresponding word embeddings as the vector representation of the sentence. We use a logistic regression model trained by minimizing cross-entropy loss to classify the sentence embeddings into positive or negative emotions. This procedure was adopted in previous studies such as \citeauthorzeng2017socialized \shortcitezeng2017socialized. Specifically, for Paragram, HSR-RR achieves the highest classification accuracy on all four tasks; for Word2Vec and GloVe, HSR-RR performs the best on three out of the four tasks.
“Tf.idf” is a neural network with a hidden layer of 50 relu cells, followed by a linear cell, where the inputs are the tf.idf of the words. We observe that all experiments perform better than the Tf.idf baseline, but there are no major differences between the use of SVD and the three approaches based on word embeddings. The systems which integrated a sentence similarity performed better than those not using it, though the differences when using CNN are negligible. Our runs using regression were not significantly better than simpler approaches, and the runs using deep learning reported the lowest results. Note, however, that the input features used in the runs using deep learning did not incorporate information about the snippets.
In a first experiment, we consider the problem of phoneme classification across languages on the Common Voice database. We also compare with a model trained from scratch on the target dataset. The training set of each target dataset is only 1 hour long. The model trained from scratch thus performs poorly. On the other hand, pre-trained features significantly improve the performance in all languages, even without any finetuning. First, on 100 hours of librispeech, our modified CPC outperforms the original CPC by 5.4 points on average. However, supervised pre-training still performs slightly better (1.3 points) than our unsupervised pre-training on the same corpus. An advantage of unsupervised pre-training is that we can apply it to any larger unannotated dataset. We show the benefits of this by pre-training our modified CPC on 360 hours of unlabelled data from Librispeech and match the performance of the supervised model.
We also study the impact of fine-tuning the phoneme features instead of freezing them. We use 5 hours of speech in 5 target languages for this experiment. As for the experiments on 1h of speech, our approach is on par with supervised pre-training when the features are frozen. We also observe a boost around 7 performance points for all the pre-training methods when we fine-tune the features. Our approach is still relatively competitive with supervised pre-training, but slightly worse (−1.5 points) on average.
We compared several alternatives to the linear prediction model initially presented in [DBLP:journals/corr/abs-1807-03748]. We supposed that if the prediction network is too simple, then the auto-regressive network will perform a significant part of the prediction task. Thus we though that more complex architecture would improve the quality of our output features.
Investigating the Impact of ASR Errors. We trained five reading comprehension models mentioned in Section 5.2 on the DRCD training set and these five models are tested on DRCD dev set and ODSQA testing set. In the following experiments, we do not consider the spoken documents whose answers do not exist in the ASR transcriptions because the model can never obtain the correct answers in these cases. To make the comparison fair, the DRCD dev set are filtered to contain only the same set of examples. The average F1 score fell to 63.67% when there are ASR errors. Similar phenomenon is observed on EM. The impact of ASR errors is significant for machine comprehension models.
Mitigating ASR errors by Subword Units. In this work, we didn’t utilize tone information in pingyin-tokens. We leave it as a future work. The network details are listed as follow: pingyin-token embedding size 6, filter size 3x6 and numbers of filters 100. vs. (a)(c)(e)(g)(i)(k)). The average EM score is improved by 1.3 by using pingyin sequence embedding over ODSQA testing set. Data augmentation. To improve the robustness to speech recognition errors of QA models, we augmented training data DRCD with DRCD-TTS and DRCD-backtrans. vs. (a) and row(h)(j) vs. row(b)). And finally training with the combination of DRCD, DRCD-TTS and DRCD-backtrans with pingyin sequence embedding obtains the best results (row(l)) which is better than baseline (row (a)) by almost 4 F1 score. Therefore, data augmentation proves to be helpful in boosting performance.
Comparison Between Text Question and ASR Transcribed Question. ASR errors on question will affect the reasoning of a QA model. In this part, we compare the performance between input with text questions and input with ASR-transcribed questions. Similar phenomenon is observed on EM. Once again, we can see that using pingyin sequence embedding brings improvement (row(h) vs (g)) even with text question as input.
However, for the Hybrid approach, where the abstract is augmented with sentences from the summaries emitted by the models, our TalkSumm-Hybrid outperforms both GCN Hybrid 2 and Abstract. Importantly, our model, trained on automatically-generated summaries, performs on par with models trained over ScisummNet, in which training data was created manually.
We asked the following binary (yes/no) questions to each of the experts: a) is this question syntactically correct? , b) is this question semantically correct?, and c) is this question relevant to this sentence?. Responses from all three experts were collected and averaged. For example, suppose the cumulative scores of the 100 binary judgements for syntactic correctness by the 3 evaluators were (80,79,73). Then the average response would be 77.33. Our model QG+F+GAE, which encodes ground truth answers and uses a rich set of linguistic features, performs the best as per every metric. Specifically, addition of features increases syntactic correctness of questions by 2%, semantic correctness by 9% and relevance of questions with respect to sentence by 12.3% in comparison with the baseline model
Evaluation on other metrics: We also evaluated our system on other standard metrics to enable comparison with other systems. To appreciate this, consider the candidate question “who was the widow of mcdonald ’s owner ?” against the ground truth “to whom was john b. kroc married ?” for the sentence “it was founded in 1986 through the donations of joan b. kroc , the widow of mcdonald ’s owner ray kroc.”. It is easy to see that the candidate is a valid question and makes perfect sense. However its BLEU-4 score is almost zero. Thus, it may be the case that the human generated question against which we evaluate the system generated questions may be completely different in structure and semantics, but still be perfectly valid, as seen previously. While we find human evaluation to be more appropriate, for the sake of completeness, we also report the BLEU, METEOR and ROUGE-L scores in each setting.
Experiments show that the models with GI-Dropout outperform both CNN and self-attentive RNN baselines by a significant margin. Similarly, it achieves improvements compared with RNN baseline on most datasets.
5.2.1. Case Analysis We chose two question types how many and what animal to analyze the feasibility of the proposed LP score metric. The answer distribution in the training set of question type what animal is much more uniform than that of how many. Note that the Question-only method answers the questions merely based on the question features without reasoning images which will arise the language prior problem certainly. 2.0 datasets are just slightly better than the Question-only one, respectively. In contrast, for the more uniform answer distribution of question type what animal, there is a large margin between the state-of-the-art models and the Question-only model. This indicates that for the state-of-the-art VQA models, the language prior effect of these question types with less uniform answer distributions is higher than those with more uniform answer distributions. Based on the analysis, we can deduce that our proposed metric is capable of measuring the language prior effect.
S4SS3SSS0Px3 Results. Our globally normalized model again significantly outperforms the local model. We also compare to the sentence compression system from \newcitefilippova-emnlp15, a 3-layer stacked LSTM which uses dependency label information. The LSTM and our global model perform on par on both the automatic evaluation as well as the human ratings, but our model is roughly 100× faster. All compressions kept approximately 42% of the tokens on average and all the models are significantly better than the automatic extractions (p<0.05).
For early work on neural-networks for transition-based parsing, see Henderson \shortcitehenderson:2003:NAACL, henderson:2004:ACL. Our work is closest to the work of \newciteweiss-etAl:2015:ACL, \newcitezhou-etAl:2015:ACL and \newcitewatanabe-sumita:2015:ACL; in these approaches global normalization is added to the local model of \newcitechen-manning:2014:EMNLP. Empirically, \newciteweiss-etAl:2015:ACL achieves the best performance, even though their model keeps the parameters of the locally normalized neural network fixed and only trains a perceptron that uses the activations as features. Their model is therefore limited in its ability to revise the predictions of the locally normalized model. We also compared training under the CRF objective with a Perceptron-like hinge loss between the gold and best elements of the beam. When we limited the backpropagation depth to training only the top layer θ(d), we found negligible differences in accuracy: 93.20% and 93.28% for the CRF objective and hinge loss respectively. However, when training with full backpropagation the CRF accuracy is 0.2% higher and training converged more than 4× faster.
We observed several instances of severe label bias in the sentence compression task. It is important to note that these are not search errors: the empty compression has higher probability under pL than the prediction from greedy inference. However, the more expressive globally normalized model does not suffer from this limitation, and correctly gives the empty compression almost zero probability.
We see that simple sequential models outperform the strong feature based system, Baseline I, which used various discrete features. Note that dependency relation and POS tag sequences alone achieve reasonably high accuracies. This implies that an important aspect of temporal relation is contained in the syntactic context of event mentions. On the contrary, using the sequence of dependency relations yields a high accuracy in our setting which signifies the advantages of using sequential representations for this task. Our Full Model achieves a performance gain of 11.35% over Baseline I.
Our model performs reasonably well compared to the baseline system for most of the classes. In addition, it is able to identify relations present in small proportion like begun_by, ibefore, iafter etc., which the baseline system couldn’t identify. that relation types begins, ibefore, ends and during are difficult to identify using feature based systems, which often generate false positives for before and after relations.
Single vs. Multi-Agents All multi-agent models show improvements over the single agent baselines. On the CNN/DailyMail dataset, compared to MLE published baselines, we improve across all ROUGE scores. This is in part because we truncate documents before training and the larger number of agents might be more efficient for multi-document summarization.
We test the proposed recognition scheme on the Flemish corpus which contains speech data from much more dysarthric speakers (103 speakers compared to the 3 of the Dutch corpus). In the Flemish test set, we also present the performance on the control data which contains similar sentence tasks uttered by normal speakers. In the first training scenario, we only use normal Flemish speech. The fCNN model provides a WER of 32.2% which is considerably better than the 33.8% of TFCNN and 33.5% of CNN models. The fCNN model outperforms the other models with a WER of 29.0%.
We see that among the 2 rule based approach, Approach-1 performs better with an F1-score of 0.82. For CNN-FF model, the F1-score of 0.93 is obtained with embedding size of 250-D. For CNN-LSTM-FF model, the F1-score of 0.91 with 200-D embedding size and for LSTM-FF model, we obtain the F1-score of 0.90 with 25-D embedding size. The best overall F1-score of 0.93 is obtained by CNN-FF Model. We see an improvement of 68% in F1-score against the best performing past approach of Joshi et al.
In this experiment, as in the one described in Sec. We train our mixed model with a word hidden of 200 and a character hidden of 300. For the conditional one, we fix the hidden representation and select the optimal N on the validation set. As a baseline, we train a character-level RNN for two sizes of hidden layers: 200 and 500. We also report the word dictionary size (k) and out of vocabulary rate (OOVR) for every language. The CRNN baseline as well as the proposed models still are quite far from the performance of a word-level RNN. The proposed structural modifications allow us to achieve similar performance to a large character-level RNN with a reduced computational cost. For languages such as Finnish and Hungarian, the conditional model (Cond.) yields best performance.
To estimate performance in a real-world setting, we repeat the evaluation of DEER using the quantization-based approaches described by Guo et al. Compared to brute force, search time can be reduced by an order of magnitude with a small loss in R@100, or by two orders of magnitude while losing less than 3 points. This is crucial for scaling the approach to even larger KBs and supporting the latency requirements of real-world applications.
In experiments with the IMDB movie review corpus, we find that our training method sacrifices little to no final classifier performance and produces perturbed sequences with significantly higher probability under a language model (LM) trained on the same corpus (see ; recall that lower perplexity is better). We can also compare the distribution of adversarial examples with the data generating distribution. By contrast with AdvT-Text and iAdvT-Text, we find that SPGD’s adversarial examples are on average much less distinguishable from the original dataset. We measured this effect using an LSTM language model trained on the entire IMDB corpus, and Recall the definition of perplexity: exp(−1NN∑j=1log(x(j)|y(j))) (8) Interestingly, we observed that higher sparsity coefficients (σ≈0.75) yielded better test accuracies. Altogether, we believe these results strongly suggest that in the text domain more realistic adversarial examples regularize better, a suggestion that we hope will be take into account by future research in the area.
Our experiments show that SPGD significantly improves the quality and interpretability of perturbed sequences over vanilla AdvT-Text and iAdvT-Text.
The Transformer model outperformed the ConvS2S model for all languages. These results serve as initial baseline results for the given languages on the evaluation set.
We report macro F1 of positive and negative classes (the official SemEval evaluation metric) and accuracy over the three classes. Training time is shorter by a factor of 21 (85/4 examples/second). The 4-dimensional ultradense embeddings lead to only a small loss of 1.5% even though the size of the embeddings is smaller by a factor of 100 (again not a significant drop). Training time is shorter by a factor of 44 (178/4). There is only a small performance drop when using ultradense embeddings (not significant for 40 dimensional embeddings) while the speed improvement is substantial.
The performance metric used here is P@1, the proportion of instances where a correct answer was ranked higher than all other distractors in the pool. The table shows that our model outperforms the previous baselines.
At the turn level, FSDM and FSDM/Res perform better than TSCP and TSCP FSDM and FSDM/Res use independent binary classifiers for the requestable slots and are capable of predicting the correct slots in all those cases. FSDM/Res and TSCP /RL do not have any additional mechanism for generating response slot, so FSDM/Res performing better than TSCP/RL shows the effectiveness of flexible-structured belief state tracker. Moreover, FSDM performs better than FSDM/Res, but TSCP performs worse than TSCP/RL. This suggests that using RL to increase the appearance of response slots in the response decoder does not help belief state tracking, but our response slot decoder does.
Comparing TSCP/RL and FSDM /Res, the flexibly-structured belief state tracker achieves better task completion than the free-form belief state tracker. Furthermore, FSDM performing better than FSDM/Res shows the effectiveness of the response slot decoder for task completion. The most significant performance improvement is obtained on CamRest by FSDM, confirming that the additional inductive bias helps to generalize from smaller datasets. More importantly, the experiment confirms that, although making weaker assumptions that are reasonable for real-world applications, FSDM is capable of performing at least as well as models that make stronger limiting assumptions which make them unusable in real-world applications.
As clearly seen, all four models perform better than chance even under these very rudimentary conditions (finding the argmax of a vector of length 6). Crucially, SGNSTR outperforms the rest of the models, and especially SGNSAL that shows the worst performance. These results corroborate our hypothesis from Experiment 1 that noise is negatively influencing task performance. By alleviating the noise factor that exists in SGNSAL (due to alignment), SGNSTR is able to show substantial gains in this binary classification task. SGNSTR’s curve clearly declines, while SGNSAL’s curve declines much less and is more volatile. It seems that this diminishing frequency noise is counteracted by the alignment noise, yielding a flatter curve for SGNSAL. The latter increases SGNSAL’s chance to have peaks in one of the center injection steps producing false positives in our classification task. However, this property may also have a positive influence on SGNSAL in related LSC detection tasks (Schlechtweg In Figure The peaks represent the models’ predictions with respect to where the maximal cosine distance is found for each word, which we later use in a naive and rudimentary binary classification task. As can be seen from the different distributions, all models frequently find peaks in position 2 (corresponding to the event of the first sense injection).
The first two lines report the number of two baseline systems that are trained by Kaldi and Pytorch respectively. Both systems give similar results, which confirms the correctness of the implementations trained with PyTorch.
Since the utterances from the enrollment and test set of SITW vary in length from 6∼240 seconds and extra efforts for compensation are necessary, the results may not fully reflect the gains from using the proposed loss functions.
We can use this variant (out-of-context training and in-context testing) to perform a fine-grained comparison of the model’s predicted ratings for the same sentences in and out of context. When we do this, we observe that out of 200 sentence pairs, our model scores the majority (130 pairs) higher when processed in context than out of context. A smaller but significant group (70 pairs) receives a lower score when processed in context. The first group’s average score before adding context (0.48) is consistently lower than that of the second group (0.68). In general, sentence pairs that were rated highly out of context receive a lower score in context, and vice versa. When we did linear regression on the DNNs in and out of context predicted scores, we observed substantially the same compression pattern exhibited by our AMT mean human judgments.
The first row in the table provides metrics for an uninformed ranker, where documents are ranked on decreasing document id, to provide a low threshold of performance. The table indicates whether there was a statistically significant change (to a 99% confidence, using a paired t-test) against a baseline. Among the non-trained relevance models, Word Mover’s Distance (WMD) performed at least as well as BM25, with no change in NDCG.20, but an improvement in MAP and Prec.5 (Precision at rank 5), while the Query Language Model (UQLM) did not match BM25’s level of performance. Among the trained neural models, DRMM performed the worst, with lower metrics than even BM25. The SevMos-C1 model performed better overall than WMD, and SevMos-C3 further improved the NDCG.20 score.
These are harder queries to rank for, since many non-relevant documents contain all the query words. Not surprisingly, the scores for all the models dropped. WMD was still the benchmark among untrained models, and DRMM the lowest performing deep learning model, although it did have better NDCG.20 score than BM25. The Delta-32-Lex3 model again exhibited the best overall performance.
The Delta Stage of the model computes, for each document word, a difference vector against the closest query word, and three ‘Delta features’: the cosine similarity, euclidean distance, and normalized proximity. Both show a significant drop in performance compared to Delta-32. Finally, as reviewed above, adding the 3 lexical match features resulted in a significant improvement for the Delta-32-Lex3 model over the Delta-32 model.
Due to diverse modalities for video captioning, we list the models that only contain visual modalities i.e. appearance, motion and object features. Even so, it’s also hard to achieve a completely fair comparison because of different feature extraction methods. Therefore, we try to employ the same feature extractors and preprocessing as the most recent models. Specifically, compared with GRU-EVE, MGSA, POS+CG and POS+VCT using the same features as ours, which demonstrate the superior performance without the effects of features. The remarkable improvement under CIDEr on both datasets demonstrates the ability to generate novel words of our model. Since the mechanism of CIDEr is to punish the often-seen but uninformative n-grams in the dataset. This phenomenon verifies that our model captures the detailed information from videos and acquires wealthy knowledge via ELM. Moreover, we compare our model with the existing video captioning models that use detailed object information. GRU-EVE tries to derive high-level semantics from an object detector to enrich the representation with spatial dynamics of the detected objects. OA-BTG applies a bidirectional temporal graph to capture temporal trajectories for each object. However, these two methods ignore the relationship between objects. Note that, POS+VCT achieves higher scores under METEOR and ROUGE-L on MSR-VTT, and these are probably caused by the reason that their POS method can learn the syntactic structure representation.
Effectiveness of each component. We design 4 control experiments to demonstrate the effectiveness of the proposed ORG module and TRL. The baseline model only applies appearance and motion features, and the same encoder-decoder architecture as mentioned above except without object encoder. It follows the Cross-Entropy criterion, and the results are shown in the first row of the table. Compared with the baseline model, both ORG and TRL achieve improvement when added alone. The combination of two methods can further enhance the performance which is illustrated as the last row.
one trained jointly - MT-Veracity@Rul, with the explanation generation task and one trained separately - Veracity@Rul. It is the best known model that uses only the information available from the LIAR dataset and not the gold justification, which we aim at generating. We compare our model’s performance with models that learn to optimise these objectives separately, as no other joint models have been proposed.
Explain-MT is trained jointly with a veracity prediction model, and Explain-Extractive is trained separately. We include the Lead-4 system Nallapati et al. which selects as a summary the first four sentences from the ruling comments. The Oracle system presents the best greedy approximation of the justification with sentences extracted from the ruling comments. It indicates the upper bound that could be achieved by extracting sentences from the ruling comments as an explanation. The performance of the models is measured using ROUGE-1, ROUGE-2, and ROUGE-L F1 scores. Our first model, the Explain-Extractive system, optimises the single objective of selecting explanation sentences. It outperforms the baseline, indicating that generating veracity explanations is possible.
Explanation Informativeness. We here show the results for binary labels, as annotators struggled to distinguish between 6 labels. The Fleiss’ κ IAA for binary prediction is: Just – 0.269, Explain-MT – 0.345, Explain-Extr – 0.399. Surprisingly, the gold explanations from Just were most disagreed upon. Apart from that, looking at the agreeing annotations, gold explanations were found most sufficient in providing information about the veracity label and also were found to explain the correct label most of the time. They are followed by the explanations produced by Explain-MT. This supports the findings of the first manual evaluation, where the Explain-MT ranked better in coverage and overall quality than Explain-Extr.
The high recall in both ROUGE-1 and ROUGE-F achieved by the ruling comments indicates that there is a substantial coverage, i.e. over 70% of the words and long sequences in the justification can be found in the ruling comments. On the other hand, there is a small coverage for the bi-grams. Selecting the oracles from all of the ruling sentences increases ROUGE-F1 scores mainly by improving the precision.
Our flat module can well predict outermost entities which account for a large proportion among all types of entities. In general, the performance of inner entities is affected by the extracting performance and length of their outermost entities. A shorter outermost entity is more likely to have its inner entities shared either the first token or the last token, making the constructed graph more instructive, thus its inner entities are easier to extract.
To show the generalization ability of our approach, we carry out experiments on DE→EN translation task. With the weighted-fusion mechanism, the BLEU gain is 0.64, while the knowledge transfer paradigm leads to 0.73 improvements. The combination of both yields a further improvement (+1.26). It shows that our model can achieve improvements in different types of language pairs on large scale data-sets.
Moreover, when combining with pseudo data-sets (+BT), our model can achieve further improvement. Typically, when using the large size pseudo corpus, our model can improve 1.77 BLEU score, which is a prominent gain in the low-resource scenario.
The knowledge transfer paradigm on the source side can improve the performance to some extent by refining source side representation but the impact is relatively small comparing to the weighted-fusion mechanism. On the target side, the BLEU score decreases 0.66 when using weighted-fusion in all layers. The problem is that the representation generated by the partially translated part is incomplete and may contain wrong information. This will negatively influence the decoder, especially in higher layers. According to this phenomena, the category of methods which directly fuses pre-trained representation into the target side may not work well.
For all three models we add a drop out layer after the embedding to randomly drop words, which we find helpful to address overfitting issue, and early stop is used with restoring the best model weights. Grid search is used to find the best parameters for each model.
We follow \newciteWang:2019:ACL to set model configurations. Seq. PE”, “+ Stru. PE” are 27.31, 27.99 and 28.30. From the table, we can see 1) adding the relative sequential positional embedding achieves improvement over the baseline on semantic tasks (75.03 vs. 74.61). This may indicate the model benefits more from semantic modeling; 2) with the structural positional embedding, the model obtains improvement on syntactic tasks (65.87 v.s. 64.98), which indicates that the representations preserve more syntactic knowledge.
Strikingly, the linguistically meaningless odd first generation strategy that splits words arbitrarily between the two phases is far worse than the baseline, showing that the two-pass setup on its own provides no inherent advantage over a single phase. The common first and closely related function first strategies perform the best of all the two-pass strategies, whereas the rare first and closely related content first strategies are much worse. Since the control, rare first, and content first orderings are all worse than the baseline, the gains seen by the other two orderings cannot be explained by the increase in the number of trainable parameters alone.
We compare our top results of the CMLA system that we used with the top-performing systems in the SemEval Dutch aspect extraction challenge. We find that the CMLA model that we used outperforms all other models on this task, including the baseline by a large margin.
Compared to other extractive summarization systems, our model improves upon the prior work of Narayan et al. Our model is competitive to NeuSum We expect that the novel aspects of our work can also bring improvements in that non-extractive setting and are complementary to the use of auto-regressive sentence selectors. We also compared our models to models using ELMopool and skip-thought vectors as fixed features input to a document-level Bi-LSTM. As seen these methods are also competitive, but are not as strong as our global hierarchical pre-training approach.
We see that Bleu and Rouge scores are prone to being zero. Even the second reply is very similar to the groundtruth, its Chinese utterances do not have bi-gram overlap, resulting in a Bleu-2 score of zero. By contrast, our referenced and unreferenced metrics are denser and more suited to open-domain dialog systems.
The highest accuracy rate found was 86.67% (p-value <10−10), which confirms that the topological characterization of subtexts is able to discriminate authors. In all classifiers, the lowest accuracy rates were found for the shortest subtexts (W=500). In order to compare the performance obtained with short and full texts, it is possible to define a threshold WL from which the accuracy rate surpasses the value θ × AFB, where AFB is the accuracy found with the traditional approach based on full texts. Using θ=0.85, the following thresholds were obtained: WL=1,600 (kNN), WL=1,700 (Bayes), WL=1,000 (C4.5) and WL=1,000 (SVM).
First, when evaluated on PIAFv1.0 data, the performances of all models are significantly lower than on the FQUAD development set. Second, a model trained on the automatic French translation of SQuAD obtains, on the FQUAD development set, the same performance as using the FQUAD training set. In PIAFv1.0 this risk is mitigated by the large number of volunteering contributors (N=258).
From the table, we observe that our model performs better in headline generation tasks. However, the ROUGE scores in summarization tasks are lower than the models without sharing embedding, encoder and output layers. It should be noted that by sharing the parameters, this model requires less than 20 million parameters to achieve such performance.
R-MeN sets a new state-of-the-art accuracy of 90.5% that significantly outperforms other models on WN11. R-MeN also achieves a second highest accuracy of 88.9% on FB13. Overall, R-MeN yields the best performance averaged over these two datasets.
Upon concatenating all training sets, we shuffle all the sentences, bundle them into batches of 32 sentences each, and train UDify for a total of 80 epochs before stopping. We hold the learning rate constant until we unfreeze BERT in the second epoch, where we and linearly warm up the learning rate for the next 8,000 batches and then apply inverse square root learning rate decay for the remaining epochs. For the dependency parser, we use feedforward tag and arc dimensions of 300 and 800 respectively. We apply a small weight decay penalty of 0.01 to ensure that the weights remain small after each update. For optimization we use the Adam optimizer and we compute softmax cross entropy loss to train the network. We use a default β1 value of 0.9 and lower the β2 value from the typical 0.999 to 0.99. The reasoning is to increase the decay rate of the second moment in the Adam optimizer to reduce the chance of the optimizer being too optimistic with respect to the gradient history. We clip the gradient updates to a maximum L2 magnitude of 5.0.
For both types of CorEx, the topic containing the corresponding terms is used as the classifier, but for Anchored CorEx those terms are also used as anchors when estimating the latent factor. Unsupervised CorEx does a reasonable job of discovering a coherent religion topic that already contains the terms God, Christian, and Jesus. However, using the terms Jesus and Christian as anchors yields a topic that better predicts the actual soc.religion.christianity category.
Surprisingly, Anchored CorEx outperforms Naive Bayes (NB) by a large margin. Of course, Anchored CorEx is not a replacement for supervised learning: NB beats Anchored CorEx on 20 Newsgroups and does not represent a “strong” baseline for Obesity 2008 (teams scored above 0.7 in Macro-F1 during the competition). It is nonetheless remarkable that Anchored CorEx performs as well as it does given that it is fundamentally unsupervised.
To have a better sense of our approach’s performance, we compared results against a simple baseline. We built a set of Naive Bayes classifiers using bag-of-word features and optimized their parameters using 20-fold cross validation on original training data. We experimented with different thresholds on word count for a word to be included into vocabulary. We also set up separate thresholds for hashtags and at-mentions.
Our model was able to achieve a Macro F-score of 0.6354 (placing us eighth out of 19 teams), while the best performing model had a Macro F-score of 0.6782. e.g., Atheism-Against). We hypothesize that the reason for this is the noise and the limited size of the collected training data. Thus, we believe that the performance of the system can be improved through better data expansion and cleaning techniques.
With the exception of SQuAD, DPR performs consistently better than BM25 on all datasets. The gap is especially large when k is small (e.g., 78.4% vs. 59.1% for top-20 accuracy on Natural Questions). When training with multiple datasets, TREC, the smallest dataset of the five, benefits greatly from more training examples. In contrast, Natural Questions and WebQuestions improve modestly and TriviaQA degrades slightly. Results can be improved further in some cases by combining DPR with BM25 in both single- and multi-dataset settings.
; Lee et al. From the table, we can see that higher retriever accuracy typically leads to better final QA results: in all cases except SQuAD, answers extracted from the passages retrieved by DPR are more likely to be correct, compared to those from BM25. For large datasets like NQ and TriviaQA, models trained using multiple datasets (Multi) perform comparably to those trained using the individual training set (Single). Conversely, on smaller datasets like WQ and TREC, the multi-dataset setting has a clear advantage. Overall, our DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, with 1% to 12% absolute differences in exact match accuracy. It is interesting to contrast our results to those of ORQA Lee et al. While both methods include additional pre-training tasks and employ an expensive end-to-end training regime, DPR manages to outperform them on both NQ and TriviaQA, simply by focusing on learning a strong passage retrieval model using pairs of questions and answers. The additional pre-training tasks are likely more useful only when the target training sets are small. Although the results of DPR on WQ and TREC in the single-dataset setting are less competitive, adding more question–answer pairs helps boost the performance, achieving the new state of the art, as observed in the multi-dataset setting.
(3) SPBS-RR yields much better results on nominal synsets than non-nominal synsets. To explore the reason, we count the number of synsets with different POS tags as well as the numbers of triplets comprising POS tag-specific synsets and calculate their average triplet numbers. We find that the number of nominal synsets and their average triplet number are significantly bigger than those of the non-nominal synsets. Consequently, less relational information of non-nominal synsets is captured, and it is hard to learn good relational representations for them, which explains their bad performance. To prove this, we remove all the non-nominal synsets as well as related triplets from the dataset, and then re-evaluate all the models on nominal synsets.
This Std baseline consists of 7 layers and each layer has 625 hidden units with ReLU activation functions and 5998 softmax output units. We use 23-dimensional filterbanks with 3 pitch features as our acoustic feature vector. Three consecutive frames are concatenated as the input to the TDNN. All experiments share the same network configuration as Std. Comparing the different results in row 1 to the Std case shows the performance degradation due to domain mismatch. The second row shows the gains possible when hand-transcribed multi-domain training data is available. Next assuming the transcription of the accented speech is not available, we explore how much performance can be improved using only the knowledge of the accent class in DAT, via the domain classifier Gd(f;θd). In this experiment, we use all Std data and all 600 hours of accented data without transcriptions to train the model. There are two hidden layers in the domain classifier network, where each layer has 625 ReLU units. The input of the domain classifier is the activation of the second hidden layer of the baseline Std network.
After applying the model size reduction techniques, we see significant compression rates compared to the normal statistical models. We used hyperparameters of k=256⇒logk=1 byte and false-positive rate ϵ=0.0001. We had experimented with varying k but found k=256 a desirable choice because it gave us adequate predictive performance and is programmatically convenient since each cluster index can be stored with a whole byte. We achieve a significant 14.25-fold memory footprint reduction (567.2 MB compared to 39.8MB) and for some models have a compression ratio as high as a 31.5 (Domain 1 IC). Without fingerprinting (ϵ=1), we obtain 25.3-fold memory footprint reduction (567.2 MB compared to 22.4MB). This is around 43% lower compared to ϵ=0.0001. However, without fingerprinting the false-positives from parameter access affect predictive performance which is described in the next section.
This taxonomy is a directed acyclic graph with terms as the nodes and the edges indicating a hypernymy relationship between the terms. For our task, we assume the availability of a database of candidate hypernymy relations. Multiple such resources have been compiled and made available publicly over the years. However, such resources come with a considerable number of noisy candidate hypernyms, typically containing a mixture of relations such as hyponymy, meronymy, synonymy and co-hyponymy. For example, WebIsA has more than 12,000 hypernyms for the term apple, including noisy hypernyms such as orange, everyone and smartphone. In fact, we observe this pattern in the candidate hypernyms of most terms. This suggests that we can leverage such information to not only extract the direct hypernyms of apple, but to also extract longer hypernym subsequences, such as apple→fruit→food. This becomes even more important given the result by Velardi et al. To further support this hypothesis, we perform an experiment where we first randomly sample 100 paths from Wordnet. We also plot the average rank of b among candidate hypernyms of a, where candidate hypernyms are ranked by their normalized frequencies in a decreasing order. Since edges in WordNet are assumed to be ground truth, it is desired that they have a higher normalized frequency and lower ranks. This small-scale experiment demonstrates that as the height of the edge increases, the normalized frequencies decrease whereas the average ranks increase. Therefore, the accuracy of patterns-based hypernymy detection decreases for more general terms that appear higher in generalization paths. Hence, for such terms, it makes sense to not solely base the hypernym selection on a noisy set of candidate hypernyms. We can potentially improve the accuracy of selected hypernyms for general terms (such as fruit) by relying on extracted subsequences starting from more specific terms (such as apple). Those subsequences would be evidenced by the less-noisy candidate hypernyms of the specific terms. [leftmargin=0.2cm,noitemsep,topsep=0pt] t0: a given seed term, e.g., apple; lt: lexical head of any term t, e.g., lt=soup for t=chicken soup; E: Hypernym Evidence, i.e., the set of all the candidate hypernymy relations, in the form of 3-tuples (hyponym, hypernym, frequency); Ek(t): Hypernym Evidence for term t, i.e., the set of top-k candidate hypernyms for term t, having the highest frequency counts A similarity measure between terms ti and tj estimated using evidence E
Aggregated over all domains, SubSeq outperforms TAXI for all four languages. It achieves >15% relative improvement in F1 for English and 7% improvement overall. Both methods perform significantly better for English, which can be attributed to the higher accuracy of candidate hypernymy relations for English. SubSeq performs best for food domain, where it outperforms TAXI across all the languages. SubSeq performs best for English, where it outperforms TAXI across 3/4 domains.
The repository at this stage contains two datasets recorded with children with speech sound disorders. The first is the Ultrax speech sound disorders subset (UXSSD) which we recorded between 12/2011–07/2014, and the second is the UltraPhonix dataset (UPX), recorded between 06/2015–03/2017. The children exhibited a range of SSDs including phonological delay, phonological disorder, inconsistent phonological disorder, vowel disorder, articulation disorder, and childhood apraxia of speech. The data was recorded specifically for the purpose of evaluating the effectiveness of ultrasound as a visual biofeedback tool for therapy Each child attended several sessions: suitability (before baseline), baseline (1–5 sessions), therapy (1–12 sessions), mid-therapy, post-therapy (immediately after therapy), and maintenance (several months after therapy).
In this study, the resulting document vectors were used as features to develop a computerized hemorrhage likelihood assessment system that aims to assign a ‘risk’ label to the free-text radiology reports while being trained on the subset of reports with the ground truth labels created by the experts (see Sec. We observed that our dataset had imbalanced distribution of training data, i.e. class 2, 3, and 4 had fewer instances than class 1 and 5. Thus, we grouped classes 2-4, and re-defined the class labels to ensure variation of the likelihood of intracranial hemorrhage as: (1) ‘no risk’ - no intracranial hemorrhage; 2) ‘medium risk’ - probability of having intracranial hemorrhage; (3) ‘high risk’- definite diagnosis of intracranial hemorrhage. The re-definition of the class labels were validated by forming a mutual agreement between the two expert radiologists. In Table. To quantify the performance of the classifier, the 1,188 annotated reports were randomly divided into 80% training set (950 reports) and 20% test set (238 reports). To demonstrate the true power of our vector embedding, we performed experiments using three classifiers - Random Forests, Support Vector Machines, K-Nearest Neighbors (KNN) in their default configurations.
The systems with “ *” are directly tuned on the evaluation data and should be considered as upper bounds on true performance. (Note that for the disease domain, DIEJOB_Both and DIEJOB_Both* get the same results, because they use the same parameters, although they are tuned with different data.) One explanation could be: (1) if the structured corpus is similar to the target corpus, it is better to use DIEJOB_Both, and including examples of the structured corpus (e.g., RsCs and RsCsRt, both have Cs used) generally performs well with a larger N value; (2) if the structured and target corpora are dissimilar, DIEJOB_Target is better and RsRt has an advantage over other variants where the main focus is distilling good training examples from Rt and a smaller number of top N examples is preferred.
We show that the adaptation with visual features helps improve the absolute TERs by 1% in CTC and by 1.6% in the S2S model. In our experience, this is a significant improvement in such sequence-based models. The PPL values for CTC are those of a word LM while those of S2S are of the implicit character LM (*). We see that adapting a language model with visual features helps decrease the perplexity by a huge margin. This establishes that there is a strong correlation between visual features and speech. The perplexity for S2S models is the character-prediction perplexity (joint AM, LM) and we see that there is no difference in this case. We calculate TER as our experiments with greedy decoding for CTC AM showed good results with the visual adaptation, and the implicit LM of S2S was also quite strong, as we show below. The ‘dev’ set is a tougher set than the ‘test’ set in the How-To dataset.
The first observation is that combining BoW-based representations with semantic features (tax2vec or doc2vec) leads to performance improvements in five out of six cases (MBTI being the only data set where no improvement is detected). Tax2vec outperforms doc2vec-based vectors in three out of five data sets (PAN 2016 (Age), BBC News and Drugs (effect)), while doc2vec-based features outperform tax2vec on two data sets (PAN 2017 (gender) and Drugs (Side)).
The analysis rule A1 achieves the highest speedup – this is not surprising because, after applying A1, we do not need to rerun statistical learning, and the updated distribution does not change compared with the original distribution, so the sampling approach has a 100% acceptance rate. The execution of rules for feature extraction (FE1, FE2), supervision (S1, S2), and inference (I1) has a 10× speedup. For these rules, the speedup over Rerun is to be attributed to the fact that the materialized graph contains only 10% of the factors in the full original graph. Below, we show that both the sampling approach and variational approach contribute to the speed-up. Compared with A1, the speedup is smaller because these rules produce a factor graph whose distribution changes more than A1. Because the difference in distribution is larger, the benefit of incremental evaluation is lower.
Language Tree Analysis: The native language of a transcriber can significantly impact his/her perception of the mismatched speech. It is evident that for each of the five languages the best accuracies were exhibited by the natives. The fact that natives are unable to provide near 100% accuracy can be attributed to spam, but, also to the fact that task involves utterances from multiple languages, thus, denying the listener of a context. In addition, one can observe some correlation across related languages, for example, the second best performance for Hindi was by the Indo-Aryan group, for Russian was by the Slavic group and for Spanish was by the Romance group. We could not establish such a pattern for German (where English should have been expected to be the second best.) The correlations in speech perceptions may indeed be influenced by modern cultural influences and shared words, rather than just linguistic connections.
dialog managers using our three proposed experts inside ConvLab’s evaluation environment. The Rule baseline is a rule-based DM included in ConvLab. It is trained on MultiWOZ, and achieves 21.53% accuracy on the test set. We also compare against an agent trained with Proximal Policy Optimization (PPO; Schulman et al. We use the PPO hyperparameters laid out in Takanobu et al. The bottom third shows results for our weak expert methods trained with RoFL (+R). We follow Takanobu et al. and report evaluation results in terms of average dialog length (Turns), F1-Score of the information provided that was requested by the user, Match Rate of user-goals, and Success Rate – the percentage of dialogs in which all information has been provided and all booking information is correct. We train in-domain reduced and no label experts on the MultiWOZ dataset. The RLE scores 77 F1 on the reduced label test set, while the NLE manages 71 F1 of predicting whether an agent response belongs to a user utterance on the unannotated test set. After 2.5 million training steps, the FLE – with the most informative demonstrations – clearly outperforms both RLE and NLE methods, while the latter two perform similarly. All weak experts improve with RoFL, especially the RLE which records an 8% jump in Success Rate. We also include the performance of the final fine-tuned FFN classifier, whose improvement over its original incarnation (15% higher Success Rate) demonstrates that fine-tuning helps narrow the domain gap between data and the RL environment. RoFL dramatically improves both the performance and convergence rate of the RLE, indicating a domain gap between the reduced label data and the sets of environment actions. RoFL improves the FLE early in training, but this gain tails off after 1 million steps – possibly due to the relative strength of the expert. The trend for NLE-R is more ambiguous, falling behind its standard DQfD counterpart before catching up to its performance. RoFL seems to lead to the greatest gains when the expert initially struggles.
We are interested in evaluating the effect of our RNN-based re-embedding scheme on the performance of the downstream base model. However, the addition of the re-embedding module incurs additional depth and capacity for the resultant model. We therefore compare this model, termed RaSoR + TR, to a setting in which re-embedding is non-contextual, referred to as RaSoR + TR(MLP). Here we set ut=\textscMLP(xt), a multi-layered perceptron on xt, allowing for the additional computation to be carried out on word-level representations without any context and matching the model size and hyper-parameter search budget of RaSoR + TR. Supplementing the calculation of token re-embeddings with the hidden states of a strong language model proves to be highly effective. We additionally evaluate the incorporation of that model’s word-type representations (referred to as RaSoR + TR + LM(emb)), which are based on character-level embeddings and are naturally unaffected by context around a word-token.
+ TR + LM(L1) ranks second in EM, despite having only minimal question-passage interaction which is a core component of other works. An additional evaluation we carry out is following Jia and Liang + TR + LM(L1) results in improved robustness to adversarial examples.
On both datasets, the proposed system consistently outperforms the baselines by a large margin in Precision, Recall, and F1.
we see that our proposed method largely outperforms all other models in terms of recall and in most cases on F1 score as well. Using a sequence encoder like GRU boosts the performance significantly compared to using n-grams as in Logistic Regression or CNN. We note that the overall scores on Eclipse and Firefox datasets are much higher than the scores on Snap S2R data. One of the reasons could be the difference in data size where the Snap S2R dataset is much smaller in size compared to others. Noisy labels from non-technical annotators could also be a potential reason. Manually examining the posts we also note that the bug reports submitted for the open projects are often by engineers, who use concrete technical terms to describe the problem. In contrast, for the Snap S2R dataset, the reports are submitted by end-users using free text with non-technical terms that make it harder for a machine learning model to disambiguate.
We evaluate the performance of the GP algorithms by looking at the accuracy of the best GP programs found. The accuracy, for each group of questions, is the proportion of questions correctly answered by a GP program. For each of the 30 runs we keep all the solutions in the last selected population (100 solutions by run). Among the 100 programs, the one that has the highest accuracy in the training set is selected. Then we compute the accuracy of this program also in the test set. Using the 30 programs, the maximum and mean accuracy are calculated in the training and test sets. The table also shows the accuracy produced by the algebraic rule. It can be seen that the best GP evolved programs outperform the algebraic rule on all the groups of questions, although the difference in the results is more noticeable for some groups of questions (e.g., group 6). The mean accuracy of the programs on the test set is also higher than that achieved using the algebraic rule for 6 of the 9 groups of questions. Notice, that since our selection of the best programs was based on the accuracy for the training set, there might be programs with a higher accuracy on the test set. We did identify some of these programs. Interestingly, for some groups of questions (e.g., group 11) the maximum and mean accuracy in the training set is smaller than in the test set.
For BLEUF, we sample 1000 texts for each method as evaluated texts. The reference texts are the whole test set. Surprisingly, it shows that results of LeakGAN beat the rest, even the ground truth (LeakGAN has averagely 10 points higher than the ground truth). It may due to the mode collapse which frequently occurs in GAN. The text generator is prone to generate safe text patterns but misses many other patterns. Therefore, BLEUF is failing to measure the diversity of the generated sentences.
We conduct experiments on an English-to-Spanish e-commerce item titles translation task. The in-domain data for training with simulated feedback is composed of in-house eBay data (item titles, descriptions, etc.). The out-of-domain data has been sub-sampled according to the similarity to the domain of the product title data, and 25% of the most similar sentence pairs have been selected. Before calculating the corpus statistics, we apply pre-processing including tokenization and replacement of numbers and product specifications with a placeholder token (e.g., ‘6S’, and ‘1080p’).
Simulation experiments were also run on publicly available data. We use the same data, preprocessing and splits as Lawrence et al. The baseline model is trained with MLE on 1.6M Europarl (EP) translations, bandit feedback is then simulated from 40k News Commentary (NC) translations. For the comparison of full supervision vs. weak feedback, we train in-domain models with MLE on in-domain NC references: training only on in-domain data (NC BL), and fine-tuning the out-of-domain baseline (EP BL) on in-domain data (MLE). The NMT baselines outperform the SMT equivalents. With fully supervised fine-tuning the NMT models improve over the out-of-domain baseline (EP BL) by 5 BLEU points, outperforming also the in-domain baseline (NC BL). Moving to weak feedback, we still find improvements over the baseline by 0.5 BLEU with beam search and 1.6 BLEU with greedy decoding for online feedback (EL), and 0.6 BLEU with beam search and 1 BLEU with greedy decoding for counterfactual learning with DC. However, DPM performs worse than for SMT and those not manage to improve over the out-of-domain baseline. Nevertheless these results confirm that – at least in simulation settings – the DC objective is very suitable for counterfactual learning from bandit feedback for NMT, almost reaching the gains of learning from online bandit feedback.
The visual modality leads to modest gains in BLEU scores. The proposed VS regularizer leads to slightly higher gain when compared to Decoder-Fusion and Attention modulation techniques for the En-Pt language pair. This can possibly be attributed to the How2 dataset creation process wherein first the videos were aligned with English sentences and then the Portuguese translations were created, implying a reduction in correspondence with the visual modality due to errors introduced in the translation process.
Results. Using the BERT-FTTOPIC method yielded an average improvement of .8 points compared to the optimal result taken from \citeauthorswanson2015argument \shortciteswanson2015argument. Performance has improved in 3/4 test topics (gay marriage, gun control and death penalty), and decreased in the evolution topic. However, in \citeauthorswanson2015argument \shortciteswanson2015argument the performance on the death penalty and evolution topics is low even in the easier in-domain task, presumably indicating they are much more difficult to predict.
SBERT is fine-tuned using the Regression Objective Function. The similarity score is computed using cosine-similarity based on the sentence embeddings. We also provide the Pearson correlation r to make the results comparable to Misra et al. However, we showed Reimers et al. that Pearson correlation has some serious drawbacks and should be avoided for comparing STS systems.
Dor et al. fine-tuned a BiLSTM architecture with triplet loss to derive sentence embeddings for this dataset. As the table shows, SBERT clearly outperforms the BiLSTM approach by Dor et al.
First, we evaluated the use of graphemes as acoustic modeling units. Adding LFVs after the TDNN / CNN layers (“LFV app”) does lower the TER, but applying the method presented here (“LFV mod”) lowers the TER even more.
The use of more data lowered the TER, whereas the relative improvements were in the same order of magnitude.
Results. We notice that even though our proposed strategy outperforms the extended traditional strategies, all existing uncertainty methods achieve a considerable boost in performance over iterations. This leads to two important conclusions: (1) structuring the output space of problems that have multiple correct answers can improve uncertainty estimations by all existing measures and (2) measuring uncertainty in the embedding space instead of model outputs proves to be more robust with multiple correct answers. Our ablations also demonstrate the importance of the denoiser and visual-semantic space when using Bayesian uncertainty estimation. We show that Baye by itself performs just as well as the existing baselines but combined with Deno or VS increases its performance and together Baye+VS+Deno performs the best.
Here, we used all best parameters discovered in previous sections, including the random forest classifier. Note that the combination of these two set of features improves results compared with each single one separately. Although BoW individually presented better results in a few datasets it is not the best in all of them and alone, which suggests that using both sets of features is the best option for 10SENT. In the next experiments, we always use this joint representation (BoW + BaseMethods) when me mention 10SENT.
To compare the effect of transfer learning from emoticons, we separated it in three different experiments: first with our traditional 10SENT; next we used just emoticon labels to create the training, without our majority voting predictions; then we combined these two to check the impact of emoticons in our method. We can see that improvements of up to 6% (e.g., in case of the sentistrength_myspace dataset) can be obtained in terms of Macro F1, with no significant losses in most datasets and with no extra (labeling) cost. Thus, this approach represents an interesting opportunity to provide to the user some help in terms of labeling effort.
We can see that most methods have different behaviors in different datasets (implied by the large deviations). In other words, the same method may have a huge variance in effectiveness in different datasets, which precludes the use of a single unique method for all cases. Despite this, we can observe that some methods have clearly a higher average than others even with this high deviation.
However, we still report these measures as one component of our evaluation. Additionally we report a number of custom metrics which capture important properties of the generated text: Length – Average sequence length per example; Trigrams – percentage of unique trigrams per example; Vocab – percentage of unique words per example. Endings generated by our model and the baselines are compared against the reference endings in the original text. However, in relative terms L2W is superior or competitive with all the baselines, of which AdaptiveLM performs best. In terms of vocabulary and trigram diversity only SeqGAN is competitive with L2W, likely due to the fact that sampling based decoding was used. For generation length only L2W and AdaptiveLM even approach human levels, with the former better on BookCorpus and the latter on TripAdvisor. The SeqGAN system is more detailed, e.g. mentioning a specific location Yet it repeats itself in the first sentence. (e.g. “had a breakfast”, “and a delicious breakfast”). Consequently SeqGAN quickly devolves into generic language, repeating the incredibly common sentence “The staff was very helpful and friendly.”, similar to Seq2Seq.
In order to make the student model invariant to environments, the training data for student model should include both clean and noisy data. Therefore, we extend the original T/S learning work by The conditional T/S learning achieves 16.42% average WERs with 9.8% and 11.7% relative improvements over soft T/S learning and the best performed interpolated T/S (λ=0.5), respectively.
For supervised adaptation, the hard labels come from the human transcription though forced alignment. For unsupervised adaptation, we use the SI model to generate the hypothesis. Note that the adaptation with hard labels is equivalent to KLD adaptation with λ=0. The conditional T/S learning outperforms the KLD adaptation. It achieves 12.17% WER for supervised adaptation, which is 12.8% and 3.0% relative gain over the SI model and the best performed KLD adaptation (λ=0.5). For unsupervised adaptation, the conditional T/S learning achieves 13.21% WER, which is 5.3% and 2.5% relative gain over the SI acoustic model and KLD adaptation.
From the table, we see a consistently higher rate of TPR reduction for saliency-trained models compared to traditionally trained models, suggesting that the saliency-trained models are more sensitive to the perturbation of the contributory word(s) and confirming our hypothesis.
The results of the scalability analysis were similarly promising. As context size increases, we observed an overall slight decrease in sparsity. These high values are caused by sentences that contain extremely common entities (such as ‘whale’) and pose a challenge to questions concerning these entities. This issue could be addressed in future work by altering the graph construction so that links between common entities form a hierarchical, as opposed to flat, structure. This would also necessitate the implementation of a hierarchical policy, which may further improve performance.
It also shows the number of features used to perform the classification, the time required to compute the features and perform the cross validation (Total Runtime) and to compute one message in average (Average Runtime). Note that Late Fusion has only 2 direct inputs (content- and graph-based SVMs), but these in turn have their own inputs, which explains the values displayed in the table. We apply this process to both baselines and all three fusion strategies. We then perform a classification using only their respective TF. Note that the Late Fusion TF performance is obtained using the scores produced by the SVMs trained on Content-based TF and Graph-based TF. These are also used as features when computing the TF for Hybrid Fusion TF (together with the raw content- and graph-based features). In terms of classification performance, by construction, the methods are ranked exactly like when considering all available features.
Normalized Discount Cumulative Gain (NDCG) NDCG@N is a measure widely used for reflecting the top N quality of the ranking list, and the higher the better. In most cases, the ASR system finally delivers the 1-best result from the rescored N-best list. From the result, we can see that compared with other methods, L2RS can produce better ranking list, which means not only the top 1 result is improved but also the whole ranking list is correctly ordered. Specifically, BERT sentence embedding is quite effective for L2RS and it has 14.58% relative improvement over the baseline AM+n-gram LM rescoring method. By incorporating all these features, L2RS(opt) achieves up to 20.67% relative improvement over AM+n-gram baseline.
we fix RSTART to be the first frame of the user’s turn-final IPU. We also calculate the mean absolute error (MAE), given in seconds, from the ground truth response offsets to the generated output offsets. When sampling for the calculation of MAE, it is necessary to increase the length of the turn pair since the response time may be triggered by the sampling process after the ground truth time. We therefore pad the user’s features with 80 extra frames in which we simulate silence artificially using acoustic features. During sampling, we use the same RSTART randomization process that was used during training, rather than fixing it to the start of the user’s turn-final IPU. The offset distribution for the full RTNet model is shown in Fig. This baseline RTNet model is better able to replicate many of the features of the true distribution in comparison with predicted offsets using the best possible fixed probability shown in Fig. In Fig. ms. This part of the distribution is the most demanding because it requires that the model anticipate the user’s turn-ending. From the plots it is clear that our model is able to do that to a large degree. We observe that after the user has stopped speaking (from 0 seconds onward) the generated distribution follows the true distribution closely.
In general, the suggested example sentences helped students make substantial progress in terms of sentence structure. It is worth noting that students were able to comprehend the meaning of confusing words in the given sentences selected from both of the BiLSTM and GMM models. Students performed significantly better in appropriateness, local grammar, and structure when the sentences were suggested by BiLSTM; while the GMM model was good at presenting the structures of sentences and demonstrating the meaning of confusing words.
The top (first 4 rows) shows overall, and per-question-type, performance of our variants of SFF. Early fusion is indeed critical for CLEVR; batch-norm shows small but non-trivial gains, especially on harder questions like counting or comparing numbers. The results (6th and 7th rows) confirm that early fusion is crucial even in the relational models.
We see that both surprisal and semantic distance outperform both types of word embedding features, all of which outperform frequency alone. When combined, surprisal and semantic distance outperform either alone, and further gains can be made with the addition of either static (GloVe) or contextual (ELMo) embedding features. The addition of contextual embedding features increases performance more than the addition of static word embedding features, such that there is some benefit to capturing context over and above that provided by surprisal and semantic distance.
To evaluate the performance of the proposed system and the reference system, two experiments are performed to evaluate the expressiveness in the synthesised singing voice. These results reveal that the synthesised singing voice in the proposed system has higher correlation with the f0 based system.
Instead of generating purely random datasets, we train various word embeddings, using the same corpus but introducing some noise in the training process. More specifically, we train skipgram and cbow models using the fasttext tool We consider the following settings to generate our toy datasets: Seed: we learn two models on the same data, with the same hyper-parameters. The only difference between the two models is the seed used to initialize the parameters of the models. Please note that during the training of skipgram models, frequent words are randomly subsampled, as well as negative examples. There is thus two sources of randomness between the two models: the initial parameters of the model and the sampling during training. Data: in that instance, we learn two embeddings using different split of the data. More precisely, we use the first 100M tokens for the first model and the next 100M tokens for the second model. Since both training splits come from the same domain, the models are closed. Window: in that instance, we learn two skipgram models on the same data, but with different window size. The first model uses a window of size 2, while the second model uses a window of size 10. We also use different seeds, similarly to the first instance. Algorithm: for the last setting, we consider two word embeddings from different models: skipgram and cbow. Because all models are trained on English data, we have the ground truth matching between vectors from the two sets. We can thus estimate an orthogonal matrix using Procrustes, and measure the “distance” between the two sets of points. For these experiments, we compare the different approaches on the first 10,000 points of our models.
The size of the batch size b plays an important role on our surrogate loss as it trades off speed for distance to the original formulation. The larger b is, the better the approximation of the squared Wasserstein distance is, and the closer we are to the real loss function. Despite saying nothing about the quality of the local minima reached by the surrogate, it is interesting to see that in practice this converts into better performance as well.
First we note that the Template model scores very poorly on BLEU, but does quite well on the extractive metrics, providing an upper-bound for how domain knowledge could help content selection and generation. All the neural models make significant improvements in terms of BLEU score, with the conditional copying with beam search performing the best, even though all the neural models achieve roughly the same perplexity.
The three fine-tuned BERT versions clearly outperform all other methods. Multi-task versions seem to perform better than single-task ones in most cases. Especially for Q4 and Q5, which are highly correlated, the multi-task BERT versions achieve the best overall results. BiGRU -ATT also benefits from multi-task learning.
With improvements of 2.4% on PD and 4.7% on CFT datasets respectively, our CAW Reader model significantly outperforms the CAS Reader in all types of testing. Since the CFT dataset contains no training set, we use PD training set to train the corresponding model. It is harder for machine to answer because the test set of CFT dataset is further processed by human experts, and the pattern quite differs from PD dataset. We can learn from the results that our model works effectively for out-of-domain learning, although PD and CFT datasets belong to quite different domains.
Our CAW Reader (mul) not only obtains 7.27% improvements compared with the baseline Attention Sum Reader (AS Reader) on the test set, but also outperforms all other single models. The best result on the valid set is from WHU, but their result on test set is lower than ours by 1.97%, indicating our model has a satisfactory generalization ability.
4.2.3 Cbt Our model outperforms most of the previous public works. Compared with GA Reader with word and character embedding concatenation, i.e., the original model of our CAW Reader, our model with the character augmented word embedding has 2.4% gains on the CBT-NE test set. FG Reader adopts neural gates to combine word-level and character-level representations and adds extra features including NE, POS and word frequency, but our model also achieves comparable performance with it. This results on both languages show that our CAW Reader is not limited to dealing with Chinese but also for other languages.
Given its broad adoption for NLP tasks, an immediate question is: can we reduce the size of BERT base without incurring any significant loss in accuracy? Recently published We note that the distillation technique is complimentary to our work and our schuBERTs can be pre-trained using distillation to boost their accuracy. In this work, we address this problem comprehensively.
We first report experiments with Px=0.7, which show that the DQN algorithm is effective at optimizing the expected results of system decisions during training. Both show the system consistently improving over time. Average loss over testing data after training is 0.038. Both measures indicate that the model is generalizing well to unseen examples. Ultimately, the simulated success rate is 92% for unseen color patches at test time.
We conduct ablation study on corpus SongCi It should note that all the models are purely trained on SongCi corpus without any pre-training stages. From the results we can conclude that the introduced symbols C, P, and S indeed play crucial roles in improving the overall performance especially on the metrics of format, rhyme, and sentence integrity. Even though some of the components can not improve the performance simultaneously on all the metrics, the combination of them can obtain the best performance.
We conduct ablation study on corpus SongCi It should note that all the models are purely trained on SongCi corpus without any pre-training stages. From the results we can conclude that the introduced symbols C, P, and S indeed play crucial roles in improving the overall performance especially on the metrics of format, rhyme, and sentence integrity. Even though some of the components can not improve the performance simultaneously on all the metrics, the combination of them can obtain the best performance.
The results are reported for both commercial vs. non-commercial classification and product category mapping. All the improvements are statistically significant using a one-tailed Student’s t-test with a p-value ¡ 0.05.
We conducted a three-way ANOVA with agent intended personality, agent gender and story as independent variables and perceived agent personality as the dependent variable. The results show that subjects clearly perceive the intended extraverted or intended introverted personality of the two agents (F = 67.1, p < .001). There is no main effect for story (as intended in our design), but there is an interaction effect between story and intended personality, with the introverted agent in the storm story being seen as much more introverted than in the other stories (F= 7.5, p < .001). There is no significant variation by agent gender (F = 2.3, p = .14).
The first block in the table reports the upper bound performance (Gold) which we estimated by treating a (randomly selected) reference summary as a hypothetical system output and comparing it against the remaining (three) ground truth summaries. Oracle uses reference summaries as queries to retrieve summary sentences, and Lead returns all leading sentences (up to 250 words) of the most recent document. LexRank is query-free; it measures relations between all sentence pairs in a cluster and sentences recommend other similar sentences for inclusion in the summary. GRSum Wan We show automatic results with distant supervision based on isolated Sentences (QuerySumS), Passages (QuerySumP), and an ensemble model (QuerySumS+P) which combines both. As can be seen, our models outperform strong comparison systems on both DUC test sets: QuerySumS achieves the best R-1 while QuerySumP achieves the best R-2 and R-SU4. Perhaps unsurprisingly, both models fall behind the human upper bound.
Not all categories are equally represented, some have much more relation pairs than others. We tried to downplay the importance of the category “capitals and countries”, which is very prominent in the dataset by \newcitemikolov2013efficient, however, it is still by far the largest category in our dataset. Some categories are necessarily small, like “family”, since the number of terms for family members is relatively small. That is especially true for languages from northern Europe, so we also included plural terms and some non-family members in that category, like a relation “king : queen”. Due to this difference in sizes, we strongly suggest that results are presented for each category separately, or when aggregated, to report the macro average score (average of category scores, not average over all).
We first present the results in DBLP data. For the proposed framework, we choose M=2 for the link layer and more details about discussions about the choices of its aggregation functions will be discussed in the following section. The random guess can obtain 0.1 for both micro-F1 and macro-F1. We note that the network embedding methods perform much better than the random guess, which clearly shows that the link information is indeed helpful for the prediction. GCN achieves much better performance than node2vec. As we mentioned before, GCN uses label information and the learnt representations are optimal for the given task. While node2vec learns representations independent on the given task, the representations may be not optimal. The RNN approach has higher performance than GCN. Both of them use the label information. This observation suggests that the content and sequential information is very helpful. Most of the time, RNN-node2vec and RNN-GCN outperform the individual models. This observation indicates that both sequential and link information are important and they contain complementary information. The proposed framework LinkedRNN consistently outperforms baselines. This strongly demonstrates the effectiveness of LinkedRNN. In addition, comparing to RNN-node2vec and RNN-GCN, the proposed framework is able to jointly capture the sequential and link information coherently, which leads to significant performance gain.
Overall, we make similar observations as these on DBLP as – (1) the performance improves with the increase of number of training samples; (2) the combined models outperform individual ones most of the time and (3) the proposed framework LinkedRNN obtains the best performance.
The first block shows the performance of the baseline seq2seq model, either by greedy decoding or random sampling. Unsurprisingly, S2S-sample can generate much more diverse responses than S2S-greedy. However, these responses are not of high quality as can be seen in the human assessment in the next section. The second block demonstrates the result of the latent variable conversational models. As can be seen, neither sampling from a prior (LV-S2S, p(ν)) nor a conditional (LV-S2S, p(ν|u)) helps to beat the performance of the seq2seq model. Although both models perform equally well in terms of perplexity and lowerbound, the likewise low uniqueness scores as seq2seq indicate that both of their latent variables collapse into a single mode and do not encode much information. This was also observed in Zhao et al. when training seq2seq-based latent variable models. The KL annealed model LV-S2S, p(ν|u), + A, as suggested by Bowman et al.
To validate the universality of our approach on MT tasks, we evaluated the proposed approach on different language pairs and model settings. As seen, our model consistently improves translation performance across language pairs, which demonstrates the effectiveness and universality of the proposed approach. It is encouraging to see that 2D-Convolution with base setting yields comparable performance with Transformer-Big.
The DeepMine database consists of three parts. The first one contains fixed common phrases to perform text-dependent speaker verification. The second part consists of random sequences of words useful for text-prompted speaker verification, and the last part includes phrases with word- and phoneme-level transcription, useful for text-independent speaker verification using a random phrase (similar to Part4 of RedDots). This part can also serve for Persian ASR training. Each part is described in more details below. For the English text-dependent part, the following phrases were selected from part1 of the RedDots database, hence the RedDots can be used as an additional training set for this part: “My voice is my password.” “OK Google.” “Artificial intelligence is for real.” “Actions speak louder than words.” “There is no such thing as a free lunch.”
We present here the effectiveness of our approach aiming at generating queries from users’ information needs expressed in NL. the different baselines (NL, Q, Q bin, Random, SMT, and RL) described in section 3.2. From a general point of view, results highlight that in both datasets, our proposed model SMT+RL outperforms the different baselines with improvements that are generally significant, ranging from +3.22% to +468.91%.
The upper bound for BO’s oracle is only 93.3 F1 for the entire development set. We observed that the oracle produces a score close to perfect for most sentences, yet it loses some points in others. During training, we have the gold AMR graph available for every sentence. We compare it to the oracle graph and use the Smatch score as a weight for the training example. This is a way to down-weight the examples whose oracle actions sequence is incomplete or erroneous. We include label separation in our reimplementation (Experiments 1..16) which separates the prediction of actions and labels in two different softmax layers. All our experiments use beam 10 for decoding and they are the best (when evaluated in the development set) of 5 different random seeds. Word, input and hidden representations have 100 dimensions (with BERT, input dimensions are 1024), action and label embeddings are of size 20.
Previous work Liu et al. Following the story generation work Fan et al. The perplexity scores of our model are based on We can see that our framework outperforms all previous approaches.
All these times are from single-machine implementations running on similar-sized corpora. We see that our model shows significant improvement in the training time over the model in \newcitehuang:2012, being within well within an order-of-magnitude of the training time for Skip-gram models.
VQA-Bin baseline is modeled using a pre-trained deeper LSTM VQA architecture with fine-tuning for the binary question relevance detection task. QPC-Sim baseline uses a pre-trained image captioning model to automatically provide natural language image descriptions and identifies question relevance based on a learned similarity between the question, the premise and the generated image caption. Since the baselines were not trained on our dataset, we present the baseline results for the QRPE dataset’s first order and second order image question pairs. All our models are trained on the generated first order train dataset and tested on the generated first order test dataset, second order test dataset and QRPE test dataset. Among the LSTM architectures, RelNet1 and RelNet4 performed well on the test dataset. RelNet1 has the PCA dimensionality reduced information in it’s image features, thereby eliminating the need to learn a rich image representation from the training data. RelNet4 is a much simpler model in terms of number of parameters and network layers for the model to learn. Hence, we believe that these two networks provided good results on the test datasets.
All of our models significantly outperformed the SVM baseline in both task formulations. This is also a marked improvement over the simpler Batched-CNN and Separated-CNN models. This suggests that both temporal and local email features aid in the power prediction task within the Per-Thread formulation. In the Grouped formulation, the Separated-CNN model obtained the best accuracy of 83.0%, outperforming the Sequential-CNN-LSTM accuracy of 82.4%. We hypothesize that this is because the grouped formulation does not inherently have a temporal structure between emails, unlike the thread formulation where Sequential-CNN-LSTM is able to tap into the temporal structure.
Performance across different context lengths: It is interesting that the performance of RoBERTa does not decrease significantly with the number of turns increasing, which is different from the phenomenon observed on other datasets. The results also show that the difficulty of MuTual is attributed to reasoning instead of complex conversation history.
All distractors in the test set are removed and models are evaluated on this non-distracted test set. It is clear to see that after removing these distracting candidates, models can get better scores, showing that models find it hard to exclude distractors during prediction.
We also explore how the system performance corresponds to the human judgement of difficulty. For BA (blank-level accuracy), we see that, overall, the system accuracy decreases as difficulty increases from VeryEasy (0.75) to VeryHard (0.68). However, the decrease is not exactly monotonic (there is a small increase from VeryEasy to Easy, as also from Moderate to Hard).
We leverage QA in three ways: 1). train models only on QA , 2) first train models on QA and finetune models on QH, i.e., QA ; QH, 3) train models on the concatenation of QA and QH, i.e., QA + QH. The model trained only on QA has worst performance and we attribute this to the difficulty of distinguishing distractors without seeing them during training. Therefore, this model has the highest DE. However, models trained on QH and QA could achieve better performance. We conjecture this is because QA assists the model to have better generalization.
We tried to build a single model that predicts at the same time both the language variety and the gender of each user: as expected (since the task is harder) the performance goes down when compared to a model trained independently on each label. To train the system we simply merged the two labels.
In the final system we used the SVM classifier because it outperformed all the others that we tried.
For the final evaluation we submitted our system, N-GrAM, as described in Section 2. Overall, N-GrAM came first in the shared task, with a score of 0.8253 for gender 0.9184 for variety, a joint score of 0.8361 and an average score of 0.8599 For the global scores, all languages are combined. The final column, + 2nd shows the difference between N-GrAM and that achieved by the second-highest ranked system (excluding the baseline).
Identification rates for 168 speakers are also given for comparison purpose. Using the time-domain parametrization of the glottal flow (TDGF), Plumpe et al. This result was importantly reduced to 4.70% by making use of the Mel-cepstral representation of the glottal flow (MCGF). These results can be compared to the 1.98% we achieved using the two glottal signatures. Finally also note that, relying on the VSCC, Gudnason et al. With the proposed signatures, a misclassification rate of 3.65% is reached. It is worth noting that no specific disparity bewteen male and female speakers was observed. More precisely, 6 out of the 192 female speakers (3.13%), and 17 out of the 438 male speakers (3.88%) were misclassified using the two glottal signatures.
We first compare our model with the state-of-the-art unsupervised methods. Our model based on procrustes (Ours-Procrustes) outperforms Sinkhorn-BT on all test language pairs, and shows better performance than Adv-C-Procrustes on most language pairs. Adv-C-Procrustes gives very low precision on DE-EN, FI-EN and ES-EN, while Ours-Procrustes obtains reasonable results consistently. A possible explanation is that dual learning is helpful for providing good initiations, so that the procrustes solution is not likely to fall in poor local optima. The reason why Unsup-SL gives strong results on all language pairs is that it uses a robust self-learning framework, which contains several techniques to avoid poor local optima.
This indicates that the proposed loss is complementary to existing bias-reduction approaches and improves the generalization and reasoning abilities of the model.
Firstly, we see that the two-tower Transformer models pretrained with ICT+BFS+WLP and ICT substantially outperform the BM-25 baseline. Secondly, ICT+BFS+WLP pre-training method consistently improves the ICT pre-training method in most cases. Interestingly, the improvements are more noticeable at R@50 and R@100, possibly due to that the distant multi-hop per-training supervision induces better retrieval quality at the latter part of the rank list.
The term-based vectorization method TF-IDF perform consistently better than the purely concept-based vectorization CF-IDF methods on both the titles and the full-text. The difference ranges from 0.003 on Economics to 0.307 F-score on Reuters. When combining the term vector with the concept vector, the performance is at least as good as the other text vectorization methods and in many cases yields better results. This is more noticeable on titles than on full-texts. BM25 re-weighting does not improve the results compared to TF-IDF neither in case of the titles nor the full-text. Rather, we observe a decrease in performance by up to 0.13. These experiment using a nearest neighbor classifier indicates that CTF-IDF is the best-suited vectorization method. Henceforth, we use CTF-IDF for comparing the performance of the classifiers.
As shown in the table, Bernoulli Bayes has a slight advantage over multinomial Bayes for titles. On the other hand, the multinomial variant has a slight disadvantage on full-texts. However, both methods consistently fall far behind kNN on full-texts. In the case of working with titles, the Bayes classifiers are able to keep up with kNN on two datasets. RocchioDT’s scores are depending on the datasets and range from the lowest (Reuters) to a score only slightly different from kNN (NYT, political sciences). The generalized linear models SVM and logistic regression are close to each other. The difference is no more than 0.04 for any dataset. Considering Learning to Rank, we observe that the technique yields consistently lower scores than the multi-layer perceptron. Overall, the eager learners SVM, LR, L2R and MLP outperform both Naive Bayes and the lazy learners Rocchio, and kNN. Among all classifiers, MLP dominates on all datasets apart from NYT on titles, where LRDT achieves a .021 higher score. While the stacked decision tree module increases the F-scores of logistic regression on all datasets with fewer than 100 documents per label (all but Reuters), the impact of the stacking method is inconsistent for the Learning to Rank and MLP approaches. It is noteworthy that there are cases where a classifier performs better on the title data than the same classifier applied on the full-text data. These are Bernoulli Bayes on the Reuters dataset and RocchioDT on the economics dataset. As a general rule, however, full-texts generate higher scores than the titles. Comparing different classifiers across titles and full-text, we can make the observation that some classifiers trained on titles outperform others that were trained on the full-text. Apart from the NYT corpus, the eager learners LR, LRDT and MLP on titles are superior to kNN on full-texts. Finally, we compare the F-scores of the best-performing multi-layer perceptron on titles with its scores obtained on full-text. On the NYT dataset, 58% of the F-score is retained when using only titles. On the political sciences and economics datasets, the retained F-score is 83% and 91%, respectively. On the Reuters dataset, the MLP using solely titles retains 95% of the F-score that is obtained with full-text information available.
Results: , train-main had 400k updates with almost one third of the minibatches sampled from the supervised data (mixing ratio=0.3). Supervised fine-tune phase used 22k model updates for the encoder-decoder architectures and 150k updates for the encoder-only CTC loss.
One problem of using weak supervision that is not aligned with the input sequence is that the decoder won’t be able to refine encoder representations easily. Hence we included supervised mixing and/or initial burn-in phase during our weakly supervised train-main phase. Comparing cases either with burn-in or mixing shows that mixing helps a bit , however, supervised burn-in is much more important than mixing for encoder representations. When burn-in is on, spending almost one third of the mini-batches during train-main visiting supervised data seems to hurt performance because the model has less chance to observe the more diverse and larger weakly supervised data.
Though, notably, the model again obtains a low score on Purity/Degradation. We also observe a large gap in the prediction of Non-moral, which may indicate that humans have a stronger ability to recognize tweets without moral content.
While generating description for the table, a special start token ⟨sos⟩ is feed into the generator in the beginning of the decoding phase. Then we use the last generated token as the input at the next time step. A special end token ⟨eos⟩ is used to mark the end of decoding. We also restrict the generated text by a pre-defined max length to avoid redundant or irrelevant generation. We also try beam search with beam size 2-10 to enhance the performance. We use grid search to determine the parameters of our model.
Our model outperforms the three baseline systems on all the evaluation metrics. Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1 score over the Stanford system, the winner of the CoNLL 2011 shared task, on the CoNLL 2012 development and test sets, respectively. The improvements on CoNLL F1 score over the Multigraph model are 1.41% and 1.77% on the development and test sets, respectively. Comparing with the MIR model, we obtain significant improvements of 2.62% and 3.02% on CoNLL F1 score. Our unsupervised ranking model outperforms the supervised IMS system by 1.02% on the CoNLL F1 score, and achieves competitive performance with the latent tree model.
It is clear that our proposed method ARYA outperforms all other methods with significant margins on both datasets because none of these models considers the “misc” aspect systematically. Even compared with the fine-tuned second best models Best+OurMisc, ARYA results in 18% and 8% in absolute improvements over it on the Restaurant and Laptop datasets, respectively. It is also worth noting that ARYA-NoIter significantly outperforms all compared methods. All these observations show the importance of properly handling the “misc” aspect.
In general, we obtained excellent results with our main model proposal as well as the second best alternative, which is using the TFIDF model to extract features from texts. Despite our main proposal achieving a result more or less similar to another option, it is in its simplicity and interpretability that this solution stands out, as we will see next. First thing to point out is the number of learnable weights for the classification model according to each approach.
What are the filters looking for? In order to better understand what are the patterns extracted by the convolutional layer of the neural network, let’s look at the embedding representations of tokens in our vocabulary which have the closest representations to the filters according to cosine similarity. As long as we have 12 filters in our model, which is a big quantity, we are going to focus in three specific filters (1, 9 and 11), which bring interesting results - the full results will be available in the Supplementary Material.
(we only report variants for top-performing HABCNN-TE). Consistently, our HABCNN systems outperform all baselines, especially surpass the two competitive deep learning based systems AR and NR. The margin between our best-performing ABHCNN-TE and NR is 15.6/16.5 (accuracy/NDCG) on MCTest-150 and 7.3/4.6 on MCTest-500. This demonstrates the promise of our architecture in this task. We suspect that this lies in the fundamental function of CNN and GRU. The CNN models a sentence without caring about the global word order information, and max-pooling is supposed to extract the features of key phrases in the sentence no matter where the phrases are located. This property should be useful for answer detection, as answers are usually formed by discovering some key phrases, not all words in a sentence should be considered. However, a GRU models a sentence by reading the words sequentially, the importance of phrases is less determined by the question requirement. The second variant, using a more complicated attention scheme to model biased D representations than simple cosine similarity based attention used in our model, is less effective to detect truly informative sentences or snippet. We doubt such kind of attention scheme when used in sentence sequences of large size. In training, the attention weights after softmax normalization have actually small difference across sentences, this means the system can not distinguish key sentences from noise sentences effectively. Our cosine similarity based attention-pooling, though pretty simple, is able to filter noise sentences more effectively, as we only pick top-k pivotal sentences to form D representation finally. This trick makes the system simple while effective.
The HW-LSTM-H and HW-LSTM-CH achieved the best perplexity, whereas the HW-LSTM-C saw a marginal degradation compared with the baseline LSTM.
We tuned the dimension of word embeddings, de∈{30,50,75}, number of hidden states in each layer, dh∈{100,150,200,250,300}, size of context window, k∈{0,1,2}, and initial learning rate sampled from uniform distribution in range [0.0001,0.01]. Our encoder-labeler deep LSTM(W) achieved 95.66% F1-score, outperforming the previously published F1-score
Another benefit of working with pairwise classification is that it can provide some kind of intuition as to which metrical features are the most useful to consider when distinguishing between authors. Not all machine learning algorithms can produce understandable feature importances, but it is possible to extract them from the Random Forest family of algorithms (this study used ExtraTrees from scikit-learn) and Support Vector Machines (with a linear kernel). It is important to note that the two sets of values listed cannot be compared with each other, and it is not clear how the weights compare within a set (i.e. a feature with a weight of ten may not be ten times as important as one with a weight of one). In terms of raw ranking, though, they provide an interesting guide. When considering the problem of reduced training sets, the low dimensionality of our feature set works to our advantage. Instead of considering hundreds of words or character n-grams, we have just 16 metrical features. When the Aldine Additamentum is taken as a single observation, and a chunk size of 81 lines is used for Silius’ Punica (the same length as the Additamentum) we have 150 observations—not a great number. For algorithms that are sensitive to dimensionality it would be better to have more than 256 (162), but the dimensionality does not seem unreasonable. If a smaller set of features is required, experiments indicate that standard feature selection techniques appear to work extremely well, with no loss of classification accuracy. In Fig. Of the global best features, the first eight perform as well or better as all 16. If the features are selected only with regard to Silius and Vergil, the accuracy is even better with six features than 16 ( Fig. This approach is not appropriate for problems where one author is completely unknown, but might be useful for a situation where there are a small number of candidate authors. Another possible approach to offset small training sizes due to chunking is to use repeated random sampling from the source lines. This is the approach taken in the following section. As will be seen later, conflict preferences in any foot are important stylistic markers. Based on this analysis, conflict/harmony in the first foot (not the fourth) is actually the best place to look to distinguish different authors
With few exceptions, polyglot training does worse than monolingual. In some cases, the two settings do nearly the same (such as Character and mBERT CRFs on LORELEI) but we do not see improved results from a polyglot model. While polyglot models perform worse than monolingual models, they are competitive. This suggests that polyglot models may be successfully learning multilingual representations, but that the optimization procedure is unable to find a global minimum for all languages. To test this theory, we fine-tune the polyglot model separately for each language. We treat the parameters of the polyglot NER models as initializations for monolingual models of each language, and we train these models in the same fashion as the monolingual models, with the exception of using a different initial step size.
Unsurprisingly, the polyglot model does poorly in the zero-shot setting as it has never seen the target language. However, sharing a script with some languages in the polyglot training set can lead to significantly better than random performance (as in the case of Yoruba and Uzbek). In the fine-tuning setting, the results are mixed. Yoruba, which enjoys high script overlap with the polyglot training set, sees a large boost in performance from utilizing the polyglot parameters, whereas Uzbek, which has moderate script overlap but no family overlap, is hurt by it. Russian and Bengali have no script overlap with the polyglot training set, but Bengali, which is closely related to Hindi (sharing family and genus) sees a moderate amount of transfer, while Russian, which is not closely related to any language in the training set, is negatively impacted from using the polyglot weights.
Our approach significantly outperforms PolicyGradient and ContextualBandit, both of which suffer due to biases learned early during learning, hindering later exploration. This problem does not appear in Tangrams, where no action type is dominant at the beginning of executions, and all methods perform well. PolicyGradient completely fails to learn Alchemy and Scene due to observing only negative total rewards early during learning. Using a baseline, for example with an actor-critic method, will potentially close the gap to ContextualBandit. However, it is unlikely to address the on-policy exploration problem.
Removing previous instructions (– previous instructions) or both states (– current and initial state) reduces performance across all domains. Removing only the initial state (– initial state) or the current state (– current state) shows mixed results across the domains. Providing access to both initial and current states increases performance for Alchemy, but reduces performance on the other domains. We hypothesize that this is due to the increase in the number of parameters outweighing what is relatively marginal information for these domains. In our development and test results we use a single architecture across the three domains, the full approach, which has the highest interactive-level accuracy when averaged across the three domains (62.7 5utts). We also report mean and standard deviation for our approach over five trials. This highlights the sensitivity of the model to the random effects of initialization, dropout, and ordering of training examples.
We analyze the instruction-level errors made by our best models when the agent is provided the correct initial state for the instruction. We study fifty examples in each domain to identify the type of failures. We consider multiple reference resolution errors. State reference errors indicate a failure to resolve a reference to the world state. For example, in Alchemy, the phrase leftmost red beaker specifies a beaker in the environment. If the model picked the correct action, but the wrong beaker, we count it as a state reference. We distinguish between multi-turn reference errors that should be feasible, and these that that are impossible to solve without access to states before executing previous utterances, which are not provided to our model. For example, in Tangrams, the instruction put it back in the same place refers to a previously-removed item. Because the agent only has access to the world state after following this instruction, it does not observe what kind of item was previously removed, and cannot identify the item to add. We also find a significant number of errors due to ambiguous or incorrect instructions. For example, the Scene instruction person in green appears on the right end is ambiguous. In the annotated goal, it is interpreted as referring to a person already in the environment, who moves to the 10th position. However, it can also be interpreted as a new person in green appearing in the 10th position.
We computed the performances using weighted averaged precision, recall and F-measure. In the table, we only report the F-measure for simplicity. The rational behind choosing the weighted metric is that it takes into account the class imbalance problem. From the table, we see that as we increase the number of labeled examples (L), the classification performance improves – from 47.1 to 60.9 for Earthquake and from 58.5 to 78.9 for Flood, which is a common trend for supervised models.
Although in this paper we are not concerned with the overall performance of word vectors, In addition, as expected, MLP trained better mappings than LT.
For all the SPMRL languages we show the results of \newciteW13-4907, who reported results after carrying out a careful automatic morphological feature selection experiment. For Turkish, we show the results of \newcitenivre06conll which also carried out a careful manual morphological feature selection. Our parser outperforms these in most cases. Since those systems rely on morphological features, we believe that this comparison shows even more that the character-based representations are capturing morphological information, though without explicit morphological features.
Our model DRGD also achieves the best performance. Although CopyNet employs a copying mechanism to improve the summary quality and RNN-distract considers attention information diversity in their decoders, our model is still better than those two methods demonstrating that the latent structure information learned from target summaries indeed plays a role in abstractive summarization. We also believe that integrating the copying mechanism and coverage diversity in our framework will further improve the summarization performance.
The distance from the VMS to the natural languages was estimated by obtaining the compatibility c(XVMS,P(Xt=new,l)) (see eq. In this case, P was constructed adding Gaussian distributions centered around each X observed in the New Testament over different languages λ. The distribution P for three measurements is illustrated in Fig. The values of c(XVMS,P(Xt=new,l)) displayed in Tab. The exceptions were B and I∗. A large B is a particular feature of VMS because the number of duplicated bigrams is much greater than the expected by chance, unlike natural languages. I∗ is higher for VMS than the typically observed in natural languages (see Fig. Since the intermittency I is related to large scale distribution of a (key) word in the text, we speculate that the reason for these observations may be the fact that the VMS is a compendium of different topics.
The most frequent class and CBOW baselines do not perform well overall, achieving near chance performance for several of the tasks. Using BERT increases the average SuperGLUE score by 25 points, attaining significant gains on all of the benchmark tasks, particularly MultiRC, ReCoRD, and RTE. On WSC, BERT actually performs worse than the simple baselines, likely due to the small size of the dataset and the lack of data augmentation. Using MultiNLI as an additional source of supervision for BoolQ, CB, and RTE leads to a 2-5 point improvement on all tasks. Using SWAG as a transfer task for COPA sees an 8 point improvement.
The proposed models achieve state-of-the-art results on most metrics on all three datasets. Note that the M-to-M model is trained with two additional datasets: UFC-101 which contains 13,320 video clips and Stanford Natural Language Inference corpus which contains 190,113 sentence pairs. In contrast, we achieve competitive or better results by using only the data inside the training set and analogous pretrained C3D.
We use GBDT to train our LangRank models. For each LangRank model, we train an ensemble of 100 decision trees, each with 16 leaves. We use the LightGBM implementation Ke et al. In our experiments, we set γmax=10, and evaluate the models by NDCG@3. The threshold of 3 was somewhat arbitrary, but based on our intuition that we would like to test whether LangRank can successfully recommend the best transfer language within a few tries, instead of testing its ability to accurately rank all available transfer languages. For LangRank (all) we include all available features in our models, while for LangRank (dataset) and LangRank (ling) we include only the subsets of dataset-dependent and dataset-independent features, respectively. First, using LangRank with either all features or a subset of the features leads to substantially higher NDCG than using single-feature heuristics. Although some single-feature baselines manage to achieve high NDCG for some tasks, the predictions of LangRank consistently surpass the baselines on all tasks. In fact, for the MT and POS tagging tasks, the ranking quality of the best LangRank model is almost double that of the best single-feature baseline. For the MT task, we find that dataset statistics features are more influential than the linguistic features, especially the dataset size ratio and the word overlap. This indicates that a good transfer language for machine translation depends more on the dataset size of the transfer language corpus and its word and subword overlap with the task language corpus. At the same time, we note that the dataset size ratio and TTR distance, although of high importance among all features, when used alone result in very poor performance. The dataset feature in this tree provides a smaller gain than two typological features, although it still informs the decision. For POS tagging, the two most important features are dataset size and the TTR distance. On the other hand, the lack of rich dataset-dependent features for the EL task leads to the geographic and syntactic distance being most influential. There are several relatively important features for the DEP parsing task, with geographic and genetic distance standing out, as well as word overlap. but LangRank is able to combine them and achieve even better results.
S4SS3SSS0Px2 Oracle results Both methods are able to reconstruct the elided material and the canonical clause structure from gold dependency trees with high accuracy. This was expected for the composite procedure, which can make use of the composite relations in the dependency trees, but less so for the orphan procedure which has to recover the structure and the types of relations. The two methods work equally well in terms of all metrics except for the sentence-level accuracy, which is significantly higher for the composite procedure. This difference is caused by a difference in the types of mistakes. All errors of the composite procedure are of a structural nature and stem from copying the wrong number of nodes while the dependency labels are always correct because they are part of the dependency tree. The majority of errors of the orphan procedure stem from incorrect dependency labels, and these mistakes are scattered across more examples, which leads to the lower sentence-level accuracy.
Algorithm D: In Algorithm D, we construct a projective dependency tree based on phrase reducibilities. We use the recursive headed brackets encoding, where each subtree is enclosed in one pair of brackets, containing subtrees (phrases in brackets) and just one head (word without brackets), e.g.: ( (subtree) head (subtree) (subtree) ). In each step, we greedily insert a new pair of brackets corresponding to the most reducible phrase such that the resulting structure still satisfies the following conditions: (a) brackets do not cross each other (b) each subtree has a head. (this can be improved by explicitly setting a low reducibility for punctuation). Algorithm R: Algorithm R directly builds upon the right-chain baseline, modifying it by introducing a constraint that the parent of each node must be less reducible than the child node; the least reducible node becomes the root. Each node is thus attached to the nearest subsequent more reducible node; or to the root if all subsequent nodes are less reducible.
The Ground Truth block shows the two metrics when we compute them on the human-authored responses. This sets a gold standard for the task. (2) NDM plus an attention mechanism on the belief trackers, and (3) the attentive NDM with self-supervised sub-task neurons. The results of the LIDM model with and without RL fine-tuning are shown in the LIDM Models and the LIDM Models + RL blocks, respectively. As can be seen, the initial policy learned by fitting the latent intention to the underlying data distribution yielded reasonably good results on BLEU but did not perform well on task success when compared to their deterministic counterparts (block 2 v.s. 3). This may be due to the fact that the variational lower bound of the dataset was optimised rather than task success during variational inference. block 2 v.s. 4)
In every task considered, simply adding ELMo establishes a new state-of-the-art result, with relative error reductions ranging from 6 - 20% over strong base models. This is a very general result across a diverse set model architectures and language understanding tasks. In the remainder of this section we provide high-level sketches of the individual task results; see the supplemental material for full experimental details. Semantic role labeling A semantic role labeling (SRL) system models the predicate-argument structure of a sentence, and is often described as answering “Who did what to whom”. He et al. Named entity extraction biLSTM-CRF achieves 92.22% F1 averaged over five runs. The key difference between our system and the previous state of the art from Peters et al. Peters et al. only use the top biLM layer. As shown in Sec. just the last layer improves performance across multiple tasks.
Including representations from all layers improves overall performance over just using the last layer, and including contextual representations from the last layer improves performance over the baseline. For example, in the case of SQuAD, using just the last biLM layer improves development F1 by 3.9% over the baseline. Averaging all biLM layers instead of using just the last layer improves F1 another 0.3% (comparing “Last Only” to λ=1 columns), and allowing the task model to learn individual layer weights improves F1 another 0.2% (λ=1 vs. λ=0.001). A small λ is preferred in most cases with ELMo, although for NER, a task with a smaller training set, the results are insensitive to λ (not shown).
All of the task architectures in this paper include word embeddings only as input to the lowest layer biRNN. However, we find that including ELMo at the output of the biRNN in task-specific architectures improves overall results for some tasks. One possible explanation for this result is that both the SNLI and SQuAD architectures use attention layers after the biRNN, so introducing ELMo at this layer allows the model to attend directly to the biLM’s internal representations. In the SRL case, the task-specific context representations are likely more important than those from the biLM.
Overall, the biLM top layer representations have F1 of 69.0 and are better at WSD then the first layer. The CoVe biLSTM layers follow a similar pattern to those from the biLM (higher overall performance at the second layer compared to the first); however, our biLM outperforms the CoVe biLSTM, which trails the WordNet first sense baseline.
In every case except CoNLL 2012, fine tuning results in a large improvement in perplexity, e.g., from 72.1 to 16.8 for SNLI.
Overall, our submission had the highest single model and ensemble results, improving the previous single model result (SAN) by 1.4% F1 and our baseline by 4.2%. A 11 member ensemble pushes F1 to 87.4%, 1.0% increase over the previous ensemble best.
Our single model score of 84.6 F1 represents a new state-of-the-art result on the CONLL 2012 Semantic Role Labeling task, surpassing the previous single model result by 2.9 F1 and a 5-model ensemble by 1.2 F1.
Overall, we improve the single model state-of-the-art by 3.2% average F1, and our single model result improves the previous ensemble best by 1.6% F1. Adding ELMo to the output from the biLSTM in addition to the biLSTM input reduced F1 by approximately 0.7% (not shown).
All articles were published in the 2020 year. As expected, there is a decrease in metrics which is explained by new entities in articles. BertSumAbs suffers a greater decrease than mBART which may be because of more diverse pretraining of the latter.
Consistently with the cross-validation performances shown earlier, we observe the beneficial impact of word-representations and basic pre-processing.
As per the main evaluation metric (Weighted Macro F1 score), our model outperforms the strong AWS baseline by a significant margin on both en-hu and en-pt tracks. For en-hu, the improvement is about 30 absolute points on the dev dataset and 27 points on the test dataset. For en-pt, the improvement is about 34 absolute points on the dev dataset and 33 absolute points on the test dataset. This model ranked 1st on the dev leaderboard and 2nd on the test leaderboard for en-hu track. It ranked 2nd on the dev leaderboard and 3rd on the test leaderboard for en-pt track.
We measure Spearman’s rank correlation between human- and machine-generated usage similarity matrices using the Mantel test and observe a significant positive correlation for 10 out of 16 words.
We include the traditional machine translation model (many-to-many), 3 frameworks for building multilingual machine translation that are representative of the current state-of-the-art, and our HNMT framework. As shown in the table, our proposed strategy yields an average gain of 1.07 BLEU points over the traditional many-to-many strategy. This difference is statistically significant under paired T-test with a confidence interval of p<0.05. Largest improvements are found in languages such as Catalan, Portuguese, Italian, or Spanish, which we find not to be a coincidence but the result of HNMT correctly integrating languages that share similarities. The lowest improvement is obtained for Oriya, an Indo-Aryan language that shares little similarity with respect to any of the remaining languages in the dataset. Among multilingual machine translation models (one-to-one, one-to-many, many-to-one), only the one-to-many model achieves an improvement over the traditional bilingual baseline. This is not a surprising result as, even if multilingual machine translation models have shown to improve over traditional machine translation with specific language combinations, they are known to under-perform when simultaneously dealing with either too many or too different languages johnson2017google. While average allow us to assess and compare performance across frameworks, it does not shine a light on translation pairs that greatly deviate from the average. It can be appreciated in the figure that for most of the translation tasks performance ranges between 0 and 10 BLEU points. However, there are some cases for which BLUE is as high as 30-40. Not surprisingly, these cases align with popular, resource-rich languages like Spanish-English and Portuguese-English. At the opposite extreme, we see BLEU as low as 0.27. Once again, this is anticipated, as these low scores are the result of translation to/from low-resource (and often less recognized) languages, like Catalan to Oriya.
Each system is trained twice with different seeds and the one with better newstest2016 BLEU is kept when reporting single systems. Ensembles by default use the best early-stop checkpoints of both seeds unless otherwise stated. EN→TR baseline (T1) achieves 11.1 BLEU on newstest2017 (T2) which is augmented with 150K synthetic data, improves over (T1) by 2.5 BLEU. It can be seen that once 3-way tying (3WT) is enabled, a consistent improvement of up to 0.6 BLEU is obtained on newstest2017. We conjecture that 3WT is beneficiary (especially in a low-resource regime) when the intersection of vocabularies is a large set since the embedding of a common token will now receive as many updates as its occurrence count in both sides of the corpus. On the other hand, the initialization method of the decoder does not seem to incur a significant change in BLEU. Finally, using an ensemble of 4 3WT-150K-BT systems with different decoder initializations (2xT5 + 2xT6), an overall improvement of 4.9 BLEU is obtained over (T1). As a side note, 3WT reduces the number of parameters by ∼10% (12M→10.8M).
En→De The baseline which is an NMT with 256-dimensional embeddings and 512-units GRU layers, obtained 23.26 BLEU on newstest2017 The addition of BT data improved this baseline by 1.7 BLEU (23.26→24.94).
Switchboard consists of about 300 hours of training data. There is also the additional Fisher training dataset, so combined it makes the total of about 2000h. In this work, we only use the 300h-Switchboard training data. We observe that our attention model performs better on the easier Switchboard subset of the dev set Hub5’00, where it is the best end-to-end model we know. On the harder Callhome part, it also performs well compared to other end-to-end models but the relative difference is not as high.
Although CaTeRS data has pair distance more evenly distributed than RED, we observe that the vast majority (85.87% and 93.99% respectively) of positive pairs have sentence distance less than or equal to one.
LRA First, NLRA outperformed LRA in terms of both the average accuracy and correlation. These differences were statistically significant (p<0.01) with the paired t-test. These results indicate that generalizing patterns with LSTM is better than by using wildcards. Moreover, NLRA can successfully calculate the relational similarity for the word pairs that do not co-occur in the corpus. wink have no co-occurring pattern. In these cases, LRA could not obtain the representations of those word pairs nor correctly assign the score. By contrast, NLRA could accomplish both because it could generalize the co-occurrences of word pairs and patterns.
We compared the results of our models to other published results. \newciterink2012 is the pattern-based model with naive Bayes. \newcitemikolov2013, \newcitelevy2014, and \newciteiacobacci2015 are the vector offset models. \newcitezhila2013 is the model composed of various features. \newciteturney2013 extracts the statistical features of two word pairs from a word-context co-occurrence matrix and trains the classifier with additional semantic relational data to assign a relational similarity for two word pairs.
The GPLDA baseline trained using the Voxceleb dataset shows comparable performance for the SITW and the VOiCES Development dataset. However, the VOiCES evaluation dataset has a significant degradation in the performance for the GPLDA model. The data augmentation of the Voxceleb dataset improves the GPLDA model performance for the VOiCES evaluation dataset. The Gaussian Backend is marginally worse than the GPLDA model for all the test conditions. The DPLDA model was found to be moderately worse than the GPLDA model in all the test conditions where the Voxceleb augmented set was used for PLDA training.
In the next set of experiments, we investigated the impact of in-domain and out-of-domain training for LDA versus NDA. This was accomplished by first splitting the training data into in- and out-of-domain parts, and then retraining the LDA and NDA models on these parts. It is observed that while NDA outperforms LDA for out-of-domain training, it offers minimal improvement (at least in terms of EER) when only in-domain training data is used. However, with pooled in-domain and out-of-domain data (which is no longer unimodal), a significant improvement in performance is obtained with NDA over LDA (i.e., 21% relative in terms of EER).
Unlike delete and swap, insert and flip have the advantage of making changes to one-letter words, so we expect them to perform better. We see this for the white-box attacks which can pick the best change to every word using the gradients. On the contrary, a black-box adversary performs worst for flip, which is because the black-box attacker is not enabled to pick the best change when more options (possible character flips) are available, as opposed to swap and delete which are governed by the location of the change and contain no additional flip. Nevertheless, a black-box adversary has competitive performance with the white-box one, even though it is simply randomly manipulating words. We argue that evaluating adversaries, based on their performance in an untargeted setting on a brittle system, such as NMT, is not appropriate, and instead suggest using goal-based attacks for evaluation.
Overall, our ensemble approach performs the best by a wide margin. As expected, adversarially-trained models usually perform best on the type of noise they have seen during training. However, we can notice that our FIDS-W model performs best on the Nat noise amongst models which have not been trained on this type of noise. Similarly, while FIDS-W has not directly been trained on Key noise, it is trained on a more general type of noise, particularly flip, and thus can perform significantly better on the Key than on other models which also have not been trained on this type of noise. However, it cannot generalize to Rand, which is an extreme case of attack, and we need to use an ensemble approach to perform well on it too. Nevertheless, FIDS-W performs best on the Rand noise, compared with models which are not trained on Rand either. This validates our earlier claim that training on white-box adversarial examples, which are harder adversarial examples, can make the model more robust to weaker types of noise. We also observe that FIDS-B performs better on the White examples compared with other baselines; although it has not been trained on white-box adversarial examples, it is trained on black-box adversarial examples of the same family of FIDS operations.
We used textual (Reuters, RCV1, NIPS) and image (COIL-20, ORL, Extended YaleB and PIE-Expr) datasets. We tested our method using this approach and also keeping the datasets as they are (without reduction), which lead to situations in which it is possible to have in the same dataset clusters with thousands of objects and clusters with just one object (e.g. RCV1).
We present the regression results for the baseline and proposed approaches based on: (1) mean absolute error (MAE), and (2) mean absolute percentage error (MAPE, similar to Proskurnia et al. The proposed CNN models outperform all of the baselines. Comparing the CNN model with regression loss only, CNNregress, and the joint model, CNNregress+ord is superior across both datasets and measures. When we add the hand-engineered features (CNNregress+ord+feat), there is a very small improvement. In order to further understand the effect of the hand-engineering features without the ordinal regression loss, we use it only with the regression task (CNNregress+feat), which mildly improves over CNNregress, but is below CNNregress+ord+feat. Adding more hidden layers did not show further improvements.
Overall, the proposed methods do improve over the NMT baseline and are able to outperform the SMT baseline on out-of-domain data. For DE→EN, in-domain translation quality is comparable to the NMT baseline, while the average out-of-domain BLEU falls short of the NMT baseline (-0.5 BLEU). However, in the low-resource condition (DE→RM), subword regularization improves both in-domain and out-of-domain translation (+1.2 in both cases). The average gain is +1.4 for DE-EN, and only +0.4 for DE-RM. In-domain translation is either comparable or slightly worse than the NMT baseline. Since reconstruction models are fine-tuned from multilingual models, we report scores for those multilingual models as well. We evaluate the performance of noisy channel reranking in four different settings: applied to multilingual or reconstruction systems, both with and without subword regularization. Out-of-domain BLEU for DE→RM is slightly better compared to a subword regularization system without reranking (+0.4 BLEU), all other scores are comparable or worse.
The best weights are found with simple grid search over values in the range [0.0,1.0], on the in-domain development set. The best weight combination is then used to compute scores and perform reranking for the test data of all domains.
Then the Text Region Formulation sub-stage works on generating perfect text-only regions from the classified region proposals. And finally, out of the 50 test samples, complete text regions were successfully extracted out from 43 seal images, while the pipeline failed in extracting the full-text regions in the other 7 cases. However, it was successful in pulling out partial text regions that missed just a couple of symbols at max in those 7 cases too. Thus projecting a perfect case accuracy of 86%. These text-only regions obtained, were then passed through the Symbol Segmentation stage of the pipeline to clip out the individual symbol-only regions. Eventually, of the 50 test images, after the extraction of text-only regions, the Symbol Segmentation stage succeeded in cropping out the precise symbols from 34 of them (consisting of 29 full-text regions and 5 partial text regions). While the pipeline also did fairly well in 13 other cases (consisting of 11 full-text regions and 2 partial text regions), where it was able to mark out most of the symbols individually except for a couple or more of them getting marked in entirety as single symbols, instead of separate ones. However, the remaining 3 samples failed completely in getting even a single symbol out of the proposed text region, this failure is mainly due to heavily damaged seals and a complex seal structure. Therefore, this stage’s output when scored over only the perfect results gets an accuracy score of 68% and if we do consider cases of fair performance, the accuracy score bumps up to 94%.
The unsupervised methods are only trained on the non-parallel sentences. The supervised models were trained on 100K paraphrase pairs for Quora and 500K pairs for Wikianswers. The domain-adapted supervised methods are trained on one dataset (Quora or Wikianswers) and tested on the other (Wikianswers or Quora). We further observe that UPSA yields significantly better results than CGMH: the iBLEU score of UPSA is higher than that of CGMH by 2–5 points. This shows that paraphrase generation is better modeled as an optimization process, instead of sampling from a distribution. In addition, our UPSA could be easily applied to new datasets and new domains, whereas the supervised setting does not generalize well. This is shown by a domain adaptation experiment, where a supervised model is trained on one domain but tested on the other. The performance is supposed to decrease further if the source and target domains are more different. UPSA outperforms all supervised domain-adapted paraphrase generators (except DNPG on the Wikianswers dataset), showing the generalizability of our model. These datasets are less widely used for paraphrase generation than Quora and Wikianswers, and thus, we only compare unsupervised approaches by running existing code bases. However, the consistent results demonstrate that UPSA is robust and generalizable to different domains (without hyperparameter re-tuning).
Ablation Study. The results show that each component of our objective (namely, keyword similarity, sentence similarity, and expression diversity) plays its role in paraphrase generation. It yields roughly one iBLEU score improvement if we keep sampling those words in the original sentence.
Therefore, we manually check the top-500 entity-relation tuples returned by all the eight approaches. We can see that (1) our re-implemented baseline achieve nearly the same performance with \citeauthorLin2016Neural \shortciteLin2016Neural; (2) our proposed SEE-TRANS achieves consistently higher precision at different N levels.
We evaluate our model on the test split of TACKBP-2010 dataset. The candidate generator is trained on the 2018-10-22 English Wikipedia dump. The reranker is trained on the training split of TACKBP-2010. A fair comparison to prior work is a big challenge in entity linking. Wikipedia has been growing rapidly with time, and different systems use different versions of Wikipedia with varying numbers of entities in the knowledge base (KB). The larger the set of entities , the more difficult is the task, making comparison to prior work not as meaningful. The first set of rows have systems that used 800k-1M entities in the KB. TAC KBP has ∼818k entities, ∼120k of which do not exist in the Wikipedia dump we use. We replace this with random ∼120k entities to maintain the same number. We also calculate recall@1 with this smaller set of entities for the dual encoder in Gillick et al. that we use as a candidate generator. Recall@n accounts for the true link occurring in top n candidates, irrespective of its rank. We obtain an accuracy of 92.05%, higher than prior state-of-the-art results. For the second set of rows, we select systems that use 5-5.7M entities, which is a more realistic evaluation for the current version of Wikipedia. For our work, we use the same Wikipedia dump used in the candidate generator with ∼5.7M entities. We obtain an accuracy of 88.42%, which is higher or competitive with prior work. While this comparison is somewhat fair, it isn’t completely fair as the number of entities isn’t exactly the same. For example, Nie et al.
The Union configuration performs similar to rescoring using the same model, while performing considerably worse than the case where the same n-best list rescored by other models.
In this section, we discuss some of the popular datasets used to evaluate the performance of semantic similarity algorithms. The datasets may include word pairs or sentence pairs with associated standard similarity values. The performance of various semantic similarity algorithms is measured by the correlation of the achieved results with that of the standard measures available in these datasets. The below subsection describes the attributes of the dataset and the methodology used to construct them.
shared task, including comparison with state-of-the-art parsers. However, there are several major differences between the RBGParser and the BiAtt-DP. / Moreover, it achieves best UAS for 5 out of 12 languages. For the remaining seven languages, the UAS gaps between the BiAtt-DP and state-of-the-art parsers are within 1.0%, except Swedish. The BiAtt-DP consistently outperforms both parser by up to 5% absolute UAS score. Since the BiAtt-DP uses a graph-based non-projective parsing algorithm, it is interesting to evaluate the performance on crossed arcs, which result in the non-projectivity of the dependency tree. \newcitePitler2015NAACL reported numbers on the same data for Dutch, German, Portuguese, and Slovene as in this paper. More importantly, we observe that the improvement on recall of crossed arcs (around 10–18% absolutely) is much more significant than that of uncrossed arcs (around 1–3% absolutely), which indicates the effectiveness of the BiAtt-DP in parsing languages with non-projective trees.
All figures represent micro-averages across each question–answer pair due to data imbalance at the question level. We present results with the well-formed natural language question as input (left) as well as the keyword queries (right). For P@1 and R@3, we analytically compute the effectiveness of a random baseline, reported in row 1; as a sanity check, all our techniques outperform it.
The results show an interesting relationship between political power and sentiment. Political power was evaluated in two ways: a) in terms of the number of seats a party has and b) in terms of membership of the government. Correlating either of these two indicators of political power with the mean sentiment of a party shows a strong positive correlation between speech sentiment and political power. In the current Bundestag, government membership correlates with positive sentiment with a correlation coefficient of 0.98 and the number of seats correlates with 0.89.
RotatE obtained the best results on this dataset, followed by ConvE and RSNs. It is worth noting that, while predicting the entities given two-thirds of one triple is not our primary goal, RSNs still achieved comparable or better performance than many methods specifically focusing on KG completion. This revealed the potential of leveraging relational paths for learning KG embeddings.
Even for the sate-of-the-art DE-CNN and BERT-PT models, our augmentation also brings considerable improvements, which confirms that our augmentation approach can generate useful sentences for training a more powerful model for aspect term extraction.
This is probably due to the poor Recall performance that can be explained as follows. When label sequence information is not present, the augmentation is prone to produce decayed examples in which some new aspect terms are generated in the positions of label O, or verse vice. The model trained with such decayed examples is misled not to extract these aspect terms in the test stage. As a result, the model makes many false-negative errors, leading to poor Recall scores. This indicates that label embeddings are helpful for generating qualified sentences for aspect term extraction.
Random and Weighted Random models help to establish the difficulty of the task with respect to the performance measures used. Seeking improvement by combining image and text information, we get a ∼3% increase by using simple embedding concatenation methods for classification, and 0.025 MRR measure increment. DualNet Their model with our text-CNN is essentially the Sum-Prod-Concat model.
We can see that even with an increased parameter budget, the stacked attention network’s performance doesn’t improve. The dip in performance of the W/O Attention model confirms our intuition that attention contributes better after having learned the grounding features through auxiliary tasks.
For comparison, the results with cosine scoring are also reported. We first observe that all the three PLDA training approaches obtained significant performance improvement compared to the cosine scoring. This is particular interesting for the LT approach, where only local labels are available. This confirms our conjecture that cheap local labels can be used to train PLDA and obtain performance improvement with little effort on data labeling. The superior performance with GT over LT is expected, due to the more accurate supervision with global labels. However, the lower performance with the pooled training compared to the GT is a bit surprising. One reason that the performance was deteriorated is that the global training is so strong (6000 speakers in the Global set) that the noisy local training is not necessary. More investigations are required to confirm the conjecture and experiment with the condition under which local training is effective.
It is shown that the system strongly outperforms the baseline system, and is less than a standard deviation (0.0170) below the AutoSummENG mean performance (0.1842) of all the 35 participating systems.
For each method, we choose the number of function words that maximized attribution accuracy. Observe that the WAN method achieves an accuracy of 92.6%, outperforming five variations of the aforementioned methods. The closest competing strategy is the Delta method with Manhattan distance, which achieved an accuracy of 91.3%. All other methods achieve accuracies lower than 82%. We stress the high classification power of the WAN method for plays of sole authorship relative to other popular methods.
the attribution accuracies achieved by the Delta and PCA-based methods on acts and scenes. In this case, the WAN method outperforms all the other methods by significant margins for both acts and scenes. The largest accuracies achieved by the alternative methods are 74.3% and 71.5% for acts and scenes, respectively. We stress the high classification power of the WAN method relative to the other attribution schemes when attributing individual acts and scenes.
We use Logistic Regression as a base classifier for the classify and count approach. Notice the positive impact of the features in the performance in the task. Adding the features derived from clustering the embeddings consistently improves the performance. Interestingly, the best performance (0.219) is achieved using the out-of-domain vectors, as in the NER classification task. Also, notice how the approach improves over the state-of-the-art performance in the challenge (0.243) Nakov et al. The improvement over the method of Martino et al. however, does not necessarily mean that classify and count performs better in the task. It implies that the feature set we used is richer, that in turn highlights the value of robust feature extraction mechanisms which is the subject of this paper.
All of our results are produced from 10-fold cross validation to allow comparison with previous results. The approach we have developed establishes a new state of the art for classifying hate speech, outperforming previous results by as much as 12 F1 points.
This mismatch is particularly evident in the Reddit dataset, where the training split contains 1.71 times the number of contexts provided on average as compared to the testing split. We observe a similar mismatch, in the opposite direction, with the testing split containing 1.22 times the average number of contexts as compared to the training split. We argue that this mismatch in training and test splits in terms of context lengths, adds a layer of complexity to the problem. Consequently, we use the last two dialogues as the context in the Context-Response input and the Context-Response (Separated) input.
We performed 5-fold cross validation on both Author2Vec and other baseline user embedding with a logistic regression model. In addition, an MLP with two ReLU activated hidden layers was used to further improve the performance of Author2Vec.
We can observe that, although the results achieved by the DQN on the seen data were only slightly better than that of the baseline LSTMs network, for the unseen data the DQN meaningfully outperformed the baseline. Our further analysis suggests the follows. With the seen data, the DQN decoder tended to agree with the LSTM decoder. That is, most of the time, its decision was “no modification”. As for the unseen data, because the DQN’s exploration strategy allows it to learn from many more noisy data than the LSTMs networks did, so the DQN decoder was able to tolerate better to noise and generalize well to unseen data. Intuitively, the application of the DQN here also has the effect of generating synthetic sequential text for the training of the DQN decoder, due to its exploration component.
The baseline TF-IDF model outperforms topic models in all courses except for ”Introduction to Software Product Management”. The least effective model for discussion forum traceability was the Author-Topic model, as it had the lowest MRR for four of the five courses.
On such setting, every aspect term is known to all the models, and each model predicts the corresponding polarity for a given aspect term. Here we only consider the longest sentiment capsule is active. All the reported values of our methods are the average of 5 runs to eliminate the fluctuates with different random initialization, and the performance of baselines are retrieved from their papers for fair comparisons. The best performances are demonstrated in bold face. From the table, we observe CAPSAR has clear advantages over baselines on all datasets. Our model can outperform all the baselines by a large-margin on both evaluate measures except the F1 on Laptop dataset. Meanwhile, we also observe that CAPSAR-BERT further improves the performance of BERT. It demonstrates the advantages by combining CAPSAR with advanced pre-trained model.
We retrain our CAPSAR five times and use the trained model to detect aspect terms on the test set. From the table we observe that CAPSAR shows an encouraging ability to extract aspect terms even though they are unknown in new sentences. Meanwhile, the model achieves better performance on Restaurant dataset. We conjecture the reason is the Laptop dataset has more complicated aspect terms such as “Windows 7”.
For English and North Sámi, EM+Prune results in less under-segmentation but worse over-segmentation. For Finnish these results are reversed. However, the suffixes are often better modeled, as shown by lower under-segmentation on SUF-SUF (all languages) and STM-SUF (English and North Sámi).
The performance of K-means depends on the input parameter K and random initialization of cluster centroids to start the clustering process. We can see that WebSets performs better or comparable to K-means in terms of purity, NMI, RI, and FM. Through manual labeling we found that there are 27 and 29 distinct category sets in Toy_Apple and Delicious_Sports datasets respectively. We can see that WebSets defined 25 and 32 distinct clusters which are very close to actual number of meaningful sets, compared to 40 and 50 clusters defined by K-means.
Now we compare the performance of clustering algorithms on entity record vs. triplet record representation. In this experiment we use Toy_Apple dataset. A table-column is considered as document and clusters produced are soft-clustering of this document set. Hence each table-column can be present in multiple clusters, but belongs to only one class. Purity, NMI and RI metrics are not applicable for soft clustering, however FM metric is valid. WebSets produced 25 clusters on entity record representation and 34 clusters using triplet representation. In terms of FM index, each method gives better performance on triplet record representation when compared to entity record representation. Hence triplet record representation does improve these clustering methods.
For older users there is a slight, but insignificant, improvement over the baseline system across the questions which required subjective self-assessment. Since workers only spoke to one system, they had to assess their interaction without a frame of reference. The responses to Question 3 indicate that workers believed that they effectively understood both systems, however, when compared to the results of question 1, those who spoke to SeTD understood much more than they said they thought they had on Question 3. Analysis of the seniors’ responses shows a weak correlation between the subjective answers to Question 3 and the objective answers to Question 1 (specifically, the bus number slot) with a Spearman coefficient of 0.15 and a Pearson coefficient of 0.18. This suggests that objective questions are better indicators of user experience. Subjective measures appear to be insufficient when workers don’t have a suitable point of comparison.
We compare single DSA with four baseline models: BiLSTM Cho et al. Single DSA outperforms all the baseline models in SST-2 dataset, and achieves comparative results in SST-5, which again verifies the effectiveness of the dynamic weight vector. In contrast to the distinguished results in SNLI dataset (+2.2%), in SST dataset, only marginal differences in the performance between DSA and the previous self-attentive models are found. We conclude that DSA exhibits a more significant improvement for large and complex datasets.
In this section, we summarize the results on multimodal sentiment analysis. The model we proposed, GME-LSTM(A) as well as the version without gated controller LSTM(A), both outperform multimodal and single modality sentiment analysis models. The GME-LSTM(A) model gives the best result achieved across all models, improving upon the state of the art by 4.08% in binary classification accuracy and 13.2% in MAE. Since GME-LSTM(A) is able to attend both in time, using soft attention as well as in input modality, using the Gated Multimodal Embedding Layer, it is not a surprise that this model outperforms all others. This is because these methods use more complicated language models such as dependency-based parse tree. However, by combining cues from audio and video with careful multimodal fusion, GME-LSTM(A) immediately outperforms all language-based and multimodal baseline models. This jump in performance shows that good temporal attention and multimodal fusion is key: our model benefit from the addition of input modalities more so than other models did.
We use mini-batches and stochastic gradient descent (SGD) with momentum to update the parameters. Most of the hyper-parameters are chosen according to development experiments, while others like dropout rate r and SGD momentum μ are set according to common values.
Finally, we conduct an ablation study on IWSLT14 De-En task. We also test these models on a subsampled set whose source sentence lengths are at least 40. Our model outperforms the baseline model by more than 3 BLEU points (20.63 v.s. 17.48).
While for Task B, results remains relatively stable, we observe a drop in performance for Task C.
WEMikolov: We use Skip-gram model with negative sample size of 10 and window size of 5 , with vector dimensionality of 100, 200, 250, 300, 400, 500. We can see that our HCE model yields best results of 57%, 69%, and 83% on WS, MEN and RG datasets. This performance suggests that entity and category embeddings can be used as an indicator of semantic relatedness between words. For WSS dataset, the result of our method is comparable with WN+NR1 method, which integrates WordNet similarity measures with the normalized representation of category names. We also found that Mikolov’s word embedding performs better than our method on WSR dataset, but performs worse than our method on WSS dataset. The reason may be that the WSR dataset concentrates on topical related words rather than taxonomy related words, and our method can better capture taxonomy relationship than topic relationship.
In the case of the uniform enrollment conditions L={1,2,3,4,5}, the suggested normalization technique shows almost the same performance as the standard PLDA scoring. Much better results are achieved on the set with mixed enrollment conditions. As it is expected, using constant threshold gives worse results than by using proposed normalization. At the same time it doesn’t bring extra computational costs since parameter estimation is made in a blind manner. This experiments shows that the performance really degrades with unfixed enrollment size and could be enhanced with this cheap procedure. In the current challenge normalized scoring decreases minDCF by 30% comparing with general PLDA scoring.
+N_DNN. It can be seen that at all SNRs and all noise types +N_DNN outperforms the others even in clean datasets. We note that the improvements in recognition accuracy are greater at the lower SNRs. For example, we obtained 2.92 % of WER improvement in the dataset with background music at 0 dB SNR, whereas only 0.19 % of WER improvement in the clean dataset.
We note that our approach +N_DNN provided an additional 2.2% relative reduction in WER compared to Baseline. Also, it can be seen that the performance of +N_NAT is highly relies on the dataset and it does not work on CHiME-3 task. Unlike speaker adaptation results, the +N_GMM showed worse performance than even Baseline. This result is due to insufficient noise diversity in noise i-vector training whereas relatively more available speaker diversity (e.g. 87 speakers are available in CHiME-3 task) + N_GMM, +N_GMM_ON, and +N_DNN. Although the improvement of the unseen noise case (relative improvement: 0.9%) is less than the gain of the in-domain noise case (relative improvement: 2.2%), it is clear that our noise adaptation approach +N_DNN is superior to other noise adaptation techniques. This result is also due to insufficient noise diversity, so we expect further improvement can be achieved by using additional noise types during model training. Also, +N_NAT (12.6%) and +N_GMM (12.4%) are worse than Baseline and this result suggests that our proposed system could be more robust adaptation technique even when the test environments are mostly unknown.
Capacity for Sentiment Control: The sentiment control capacity of each model is evaluated by the sentiment accuracy metric. Although the sentiment accuracy of the CGAN-SEQ2SEQ is better than the SEQ2SEQ model, it can not control the sentiment of the response as well as the CVAE-SEQ2SEQ model. We suspect that this is because during REINFORCE training the generator can only access the generated sentences, which will be noise to deteriorate the generator if they are of low quality. We have found that the responses from the pre-trained generator are indeed generic and do not control the sentiment information well. However, the CVAE-SEQ2SEQ model can utilize the golden response at every training step. Response Quality Compared with other models, the CGAN-CVAE SEQ2SEQ model achieves the lowest PPL score, which means that its likelihood of generating the golden response is highest. Similarly to the sentiment accuracy, the PPL of the CGAN-SEQ2SEQ is higher than that of the CVAE-SEQ2SEQ and we attribute this to the same reason mentioned above.
With respect to both content quality and sentiment accuracy, the CGAN-CVAE has better accuracy than other models. This demonstrates that our proposed CGAN-CVAE could not only generate high-quality dialogue response but effectively control the sentiment of dialogue responses as well, which is consistent with the automatic evaluation results. The overall performance of the CVAE model is also better than that of the CGAN model.
We asked 2 human non-clinical annotators to label temporal features of each occurrence in relation to a ’present’, i.e. ‘chief complaint: seizure’ or ‘historical’, i.e. ‘family history of seizures’, mention of the term. Both took approximately 35 minutes to review all 127 documents. Both annotators marked some records as incomplete as they either mostly referred to non symptomatic mentions of seizure , i.e. ‘anti-seizure meds prophylaxis’ or they prevention of future seizures. This resulted in each rater having differing total documents ‘submitted’ as there are some document with mixes of the above occurrences. We took the intersection of submitted documents to compute the final agreement scores.
It turns out that the corpora from both sources and from the eight programming languages require different parameter settings in order to achieve good perplexity values—and thus good and useful “topics”. While the α values are at least (almost always) in the same order of magnitude as the seeded default configuration (k=100, α=1.0, β=0.01), the β values deviate significantly from it, as does the number of topics, confirming recent findings by Agrawal et al.
P@1 on bli for each target language using English as the source language. Mod-10K improves P@1 over the default validation metric in diverse languages, especially on the average P@1 for non-Germanic languages such as ja (+18.00%) and zh (+5.74%), and the best P@1 for ko (+1.85%). Improvements in ja come from selecting a better mapping during the refinement step, which the default validation misses. For zh, hi, and ko, the improvement comes from selecting better mappings during the adversarial step. However, modularity does not improve on all languages (e.g., vi) that are reported to fail by Hoshen and Wolf
Finally, we evaluate the scalability of BlackOut when number of hidden units increases. As the dataset is large, we observed that the performance of RNNLM depends on the size of the hidden layer: they perform better as the size of the hidden layer gets larger. We report the interpolated perplexities BlackOut achieved and compare them with the results from Williams et al. . As can be seen, BlackOut reaches lower perplexities than those reported in Williams et al. We achieved a perplexity of 42.0 when the hidden layer size is 4096. To the best of our knowledge, this is the lowest perplexity reported on this benchmark.
We observe that margin based methods perform better than maximum likelihood methods and policy gradient in our experiment. Policy shaping in general improves the performance across different algorithms. Our best test results outperform previous SOTA by 5.0%.
To ensure that the previous results scale to a significantly larger training set, some selected LSTMP and HORNNP systems were built on the full 275h set. Here Dh and Dp were set to 1000 and 500, which increased the number of recurrent layer parameters to better model the full training set. This validates our previous finding on a larger data set that the proposed HORNN structures can work as well as the widely used LSTMs on acoustic modelling by using far fewer parameters.
We also conduct human evaluation studies using Amazon Mechanical Turk, based on two aspects: Factual correctness and Language naturalness. We evaluate 500 samples. Each evaluation unit is assigned to 3 workers to eliminate human variance. The first study attempts to evaluate how well the generated text correctly conveys information in the table, by counting the number of facts in the text supported by the table, and contradicting with or missing from the table. The second study evaluates whether the generated text is grammatically correct and fluent, regardless of factual correctness. Our method brings a significant improvement over the strongest baseline (p<0.01 in Tukey’s HSD test for all measures). The copy loss term further alleviates producing incorrect facts. The language naturalness result of our method without the copy loss is slightly better, because this evaluation does not consider factual correctness; thus the generated texts with more wrong facts can still get high score. See Appendix C for more details of our evaluation procedure.
, TransE is the least calibrated for relation prediction and the best calibrated for entity prediction, whereas ComplEx is the best calibrated for relation prediction but the least calibrated for entity prediction. Again, 2D Platt scaling and isotonic regression perform the best for relation prediction, whereas isotonic regression mostly performs the best for entity prediction.
Word Sense Disambiguation We choose the dataset and perform this experiment using the same setting as ELMo with only the last layer’s representation.
In our first model, only dysarthric labels are observed and we achieved an accuracy on the word and speaker levels of 82% and 93% respectively. We found that the UA-Speech database includes multiple recorded words for healthy speakers that contain intelligibility errors, different words than asked or background speech of other people. These issues affect the accuracy of detecting dysarthric speech.
We first experiment to see how much the system performance improves with the relevant context. In practice we cannot deduce the oracle context beforehand, but this experiment is useful in that it provides upper bound on how much the system can improve with the relevant context. Notice minor performance improvement with zero bias applied in λ= α=0 case, as OOV words are included in the decoding dictionary. The result shows that the expansion scheme is more sensitive to λ while OOV scheme is more sensitive to α. This is because the bias score of a phrase of length two or more words is affected by λ in the former case but by α in the latter case. Compared to the baseline WER of 16.65%, providing relevant context helps the system improve performance by close to 6% points at λ=1 and α=5, and even further with larger λ and α. In practice we cannot arbitrarily increase λ and α because of over-biasing effect when irrelevant context is provided. This leads us to the distractors test to assess its tolerance to false-triggering errors.
Number of classes of C is another hyperparameter. We repeat the distractors test using different numbers of classes. In general, we observe better performance with fewer classes but at the expense of higher false-triggering error. Likewise, we observe that the system becomes less sensitive to its hyperparameters λ and α with larger number of classes. Notably the expansion method exhibits mere 0.03% point increase in WER using 5,000 classes with 10,000 distractors, achieving remarkable tolerance against false-triggering errors, while still improving by more than 4% points with the relevant context.
Our consistency-aware model BERTCONS outperforms all the other baselines. It achieves a performance improvement of about 2 points in F1-score over the strong baseline corresponding to the BERTBASE model (p-value of 4.985e−4 as per the McNemar test). This highlights the value addition achieved by incorporating consistency cues. Since the BERT-based models incorporate the knowledge acquired from massive external corpora, our model, BERTCONS, captures better semantics and outperforms the other baselines.
results of our KeyVec model and competing embedding methods in academic paper retrieval. word2vec averaging generates an embedding for a document by averaging the word2vec vectors of its constituent words. In the experiment, we used two different versions of word2vec: one from public release, and the other one trained specifically on our own academic corpus (113 GB). In contrast, our KeyVec outperforms all the competitors given its unique capability of capturing and embedding the key information of documents.
In the next experiment, we target the situation in which we have a small amount of annotated data in an interest-domain in addition to the larger amount of out-of-domain data. We use the test-set of the Football dataset as additional in-domain training material. As expected, using the additional in-domain training data improve the results. However, the contribution of the additional data is small, as most of the gap is already covered by the cluster features. When using all the cluster features but no lexicalization (last column) training on WSJ alone outperform the joint training.
In initial experiments, we also asked humans to provide scores for topicality, informativeness, and whether the context required background information to be understandable. Note that we did not ask for fluency scores, as 3/4 of the responses were produced by humans (including the retrieval models). Results on these auxiliary questions varied depending on the wording of the question. Thus, we continued our experiments by only asking for the overall score. We provide more details concerning the data collection in the supplemental material, as it may aid others in developing effective crowdsourcing experiments. Conversely, specificity (κ=0.12) and background (κ=0.05) had very low inter-annotator agreements.
We conduct an experiment where we train adem on different amounts of training data, from 5% to 100%. We can observe that adem is very data-efficient, and is capable of reaching a Spearman correlation of 0.4 using only half of the available training data (1000 labelled examples). adem correlates significantly with humans even when only trained on 5% of the original training data (100 labelled examples).
But our work is much different from theirs. They exploit the semantic relationships between senses such as synonymy, hypernymy and hyponymy and rely on pre-trained BERT word vectors (feature-based approach); we leverage gloss knowledge (sense definition) and use BERT through fine-tuning procedures. However, the results of their feature-based approach in the same experimental setup (single training set and single model) are not as good as our fine-tuning approach although their ensemble systems (with another training set WNGC) achieve better performance. We compare our approach with previous methods.
We have tested our proposed technique on our Flickr8k dataset, our own generated ’man’ dataset, and the ’dog’ dataset generated by previous year’s group. It can be seen that our model is able to produce substantial BLEU score. We outperform the previous year’s group by almost a double. Their BLEU score was 0.4 and we have managed to achieve a better BLEU score of 0.83. This can be further increased by the use of the discussed Urdu grammar correction techniques. The BLEU score on ’man’ dataset is better on hard attention because very specific and similar context based images were used and translations were also done very carefully. Another advantage is that in Urdu language much of words in sentences repeats frequently.
In this section, we report the results on performance of the models we created using precision, recall, F1-score and AUC metrics. using a feature size of 23K based on the frequency of unigrams (fernandez2018understanding). Uni-dimensional models with imputation achieve a precision of 0.90 using only ideology, recall of 0.86 using only hate and F1-score of 0.87 with ideology and hate each. We observe that the bi-dimensional model with the combination of religion and hate provides better precision, recall and F1-score of 0.91, 0.90 and 0.91, respectively. The combination of ideology dimension individually with the other two dimensions achieves a modest improvement (2.2%) in precision, and a greater improvement (8.5%) in recall. The inclusion of ideology improves recall reducing false negatives, and improving the identification of extremist users.
extract also outperforms previously published extractive models (i.e., SummaRuNNer, extract-cnn, and refresh). However, note that SummaRuNNer generates anonymized summaries Nallapati et al. Nevertheless, extract exceeds lead3 by +0.75 Rouge-2 points and +0.57 in terms of Rouge-L, while SummaRuNNer exceeds lead3 by +0.50 Rouge-2 points and is worse by −0.20 points in terms of Rouge-L. We thus conclude that extract is better when evaluated with Rouge-2 and Rouge-L. extract outperforms all abstractive models except for abstract-RL. Rouge-2 is lower for abstract-RL which is more competitive when evaluated against Rouge-1 and Rouge-l.
To further investigate the differences between the two CNN-based models, we randomly select 100 sentences containing disfluencies from the Switchboard dev set and categorize them according to Shriberg’s Repetitions are repairs where the reparandum and repair portions of the disfluency are identical, while corrections are where the reparandum and repairs differ (so corrections are much harder to detect). Restarts are where the speaker abandons a sentence prefix, and starts a fresh sentence. On the other hand, the ACNN is no better than the baseline at detecting restarts, probably because the restart typically does not involve a rough copy dependency. Luckily restarts are much rarer than repetition and correction disfluencies.
Generally, the results share similar tendencies with the single-source cross-lingual transfer from the source English, where the multilingual ELMo performs the best, the SRL models trained on the translated target datasets show better performances than those trained with the source datasets, and the mixture corpus with both source and target language datasets bring the best performances, which can be further improved by our final PGN model with language-aware encoders. We compare the PGN model with the MoE and MAN-MoE as well, showing slightly better performances, which indicates the effectiveness of the PGN-BiLSTM module. In addition, we can see that multi-source models outperform the single-source models in all cases, which is intuitive and consistent with previous studies lin-etal-2019-choosing.
Following, we investigate the individual bilingual SRL transferring by examining the performance of each source-target language pair, aiming to uncover which language benefits a target most and trying to answer whether all source languages are useful for a target language. First, we can see that the languages belonging to a single family can benefit each other greatly, bringing better performances than the other languages in the majority of cases (i.e., EN–DE, FR– IT–ES–PT). Second, the multi-source transfer as indicated by All is able to obtain better performances across all languages, which further demonstrates its advantages over the single-source transfer. Further, we look into the PGN model in detail, aiming to understand their capabilities of modeling linguistic-specific information. We examine it by simply visualizing the language ID embeddings \bm{e}_{\mathcal{L}} of each source-target language pair, respectively, where their Euclidean distances are depicted. Intuitively, better performance can be achieved if the distance between the target and the source languages is closer.
Four systems were compared: the basic encoder-decoder models without EI (vanilla), the basic model with EI pre-processing (EI), the model with attentional decoder (EI+Attn) and the model trained on the dataset augmented with chatting data (EI+Attn+Chat). The comparison was carried out on exactly the same held-out test dataset that contains 261 dialogs. It can be seen that all four models achieve similar performance on the dialog act metrics, even the vanilla model. This confirms the capacity of encoder-decoders models to learn the “shape” of a conversation, since they have achieved impressive results in more challenging settings, e.g. modeling open-domain conversations. Furthermore, since the DSTC1 data was collected over several months, there were minor updates made to the dialog manager. Therefore, there are inherent ambiguities in the data (the dialog manager may take different actions in the same situation). We conjecture that ∼80% is near the upper limit of our data in modeling the system’s next dialog act given the dialog history.
Despite minimal training time, annotators were able to produce relatively consistent annotations that agreed in large part with other annotators. Overall agreement was high, with most pairwise numbers in the 70-80’s, and agreement for individual annotators to the group is even higher – mostly in the 80’s.
Simulated partial dependencies are produced by removing dependencies via a stochastic process that approximates how we instructed human annotators to focus their efforts. Arcs are removed top-down, with arcs lower in the tree being more likely to be deleted. This results in trees with more high-level structures and less lower-level information. Missing arcs were recovered using our parse imputation scheme (using GFL+UG features), and the resulting parser was applied to the evaluation sentences. Accuracy decreases slightly to around 60% removal, and then degrades more rapidly after that.
Predicted Tag) and gold POS tags extracted from treebank (Gold Tag). We compare against a right-branching baseline and the Gibbs parser of Mielens et al. \shortcitemielens-sun-baldridge:2015. We also evaluated ConvexMST with longer sentences: those with 20 words or less. For this, the right-branching baseline is 25.8%. When using all the annotations on the common set for all annotators, the scores for ConvexMST with UG, GFL, and GFL+UG are 47.6%, 54.4%, and 55.3%, respectively.
; we have demonstrated that the GFL features are complimentary to the UG features, and that when standing alone the GFL features are stronger than the UG features. The question of whether it might be more effective to simply have annotators produce full annotations is not addressed by these comparisons. To answer this question, we had our most experienced annotator fully annotate the same section that the other annotators did partially. Producing these full annotations required roughly 13 hours of time from the single expert annotator. In comparison, the other annotators were able to partially annotate the same section in roughly two hours each – a total of 24 hours. However, the theoretical wall clock time of the group of annotators could be as low as two hours if the sessions were run in parallel. These different training sets were once again used to train ConvexMST models that were evaluated on a held out test set. It should be noted that this comparison does not weight the results using the extrinsic costs associated with the production of the training data. In a real-world environment, the expert annotator would likely be more expensive than the inexperienced annotators, and possibly all of them combined (especially in a crowd-sourcing scenario). This makes the performance per unit cost for partial annotators even higher than
Our corpus, called DeepSUN, is the combination of those four corpora. The training corpus is composed of the training sets of ESTER 1, ETAPE and QUAERO, while, the development and test sets are composed respectively of the development and test sets of ESTER 1&2, and ETAPE. It contains almost 160 hours of speech (training 107 hours, test 24 hours, development 30 hours). It composed of two convolution layers and six BLSTM layers with batch normalization, the number of epochs was set to 35. This system achieves 20.70% word error rate (WER) and 8.01% character error rate (CER) on dev corpus (30.2 hours) and 19.95% of WER and 7.68% of CER on test set (40.8 hours). These results were obtained by applying a CTC beam search decoding coupled with a trigram language model. In addition, for the training of both E2E and ASR systems, each training audio samples is randomly perturbed in gain and tempo for each iteration.
The score presented in this table is the co-efficient of determination or R2 score. It indicates that the model captures a significant amount of the signal present in the independent variable. It can hence be relied to a degree of confidence to predict a close enough value to the actual intended document sentiment.
The scale of MOS is set between 1 to 5 with 5 being the highest score. We first compare the effects of RMSE as additional input for singing voice conversion. And the results show that using RMSE improves both the quality and similarity significantly. We found that the energy information of each frame concatenated with F0 could help the model learning the pronunciation of long vowels. The energy of each frame indicates the loudness of pronunciation, helping the model to determine when vowels should stop properly.
Comparing text modalities with video modalities are rather clean. In this experiment, we are curious how the system will perform when excluding video. A, V, D, S, C)). Though videos are noisy and sometimes dirty, they contain important information that caption and summary don’t have. Thus, text-based modalities could be less useful. Since dialogue history is such an important source to answer a question, we wonder how our system will perform if it only has the information of dialogue history. All values aren’t comparable to our submitted system (Entropy-Enhanced DMN ( A, V, D, S, C)). Moreover, our system couldn’t respond in a scene-aware style because it is restricted tosingle modality. As for dialogue modeling, an essential setting in our system. We preclude the previous answer’s last hidden state to be presented in the model; thus, the model won’t have the information on previous questions. A, V, D, S, C, no d.m.) row is the final result. Though BLEU-1 is slightly higher than our submitted system (Entropy-Enhanced DMN (A, V, D, S, C)), other values aren’t comparable. Aside from the distribution of Attention-based GRU becoming more concentrated. After adding entropy to our loss function, the result is better. A, V, D, S, C) is not camparable with our submitted system.
Before we dive into our method, we will first elaborate the date format. Dialogue history is an interactive conversation of the questioner and an oracle about the video. After they finished their dialogue, the summary will be written by the questioner. The caption is a sentence that describes the content of the video. The system is given with all of the above sources and a question about the details in the video to generate an appropriate answer. The official dataset contains 7,659 of training data, 1,787 of validation data, and 1,710 of testing data. Our submitted system outperforms the released baseline model for both subjective and objective evaluation metrics.
We report exact match (EM) and F1 metrics, computed on token level between the predicted answer and the gold answer. BiDAF ’s performance drops by 40 F1 points on SearchQA compared to SQuAD. However, BiDAF is still competitive on SeachQA, improving over the Attention Sum Reader network by 13.7 F1 points. We next consider the top hypothesis generated by the AQA question reformulator (AQA-QR) after the policy gradient training. These single rewrites alone outperform the original SearchQA queries by 2% on the test set. We analyze the top hypothesis instead of the final output of the full AQA agent to avoid confounding effects from the answer selection step. These rewrites look different from both the Base-NMT and the SearchQA ones. For the example above AQA-QR’s top hypothesis is What is name gandhi gandhi influence wrote peace peace?. Surprisingly, 99.8% start with the prefix What is name. The second most frequent is What country is (81 times), followed by What is is (70) and What state (14). This is puzzling as it occurs in only 9 Base-NMT rewrites, and never in the original SearchQA questions. We speculate it might be related to the fact that virtually all answers involve names, of named entities (Micronesia) or generic concepts (pizza). The Base-NMT model performs at 11.4 BLEU (see In contrast, Base-NMT-NoParalex performs poorly at 5.0 BLEU. Limiting training to the multilingual data alone also degrades QA performance: the scores of the Top Hypothesis are at least 5 points lower in all metrics and CNN scores are 2-3 points lower.
Take WSC for example, the random baseline is 0.5, the human is 0.920 and all the models range between 0.512 and 0.694 with RoBERTa-large giving the best result of 0.694. Except for the ARCT task, all tested models demonstrate stronger performances than RANDOM, which indicates that the models all have varying degrees of commonsense. However, for most of the tasks, all of models are well below human performance.
The SVM system is the best in the case of Austro-Asiatic and Pama-Nyungan whereas LexStat algorithm performs the best in the case of rest of the datasets. This is surprising since LexStat scores are used as features for SVM and we expect the SVM system to perform better than LexStat in all the language families. On the other hand, both OnlinePMI and SCA systems perform better than the algorithmically simpler systems such as CCM and NED. Given these F-scores, we hypothesize that the cognate sets output from the best cognate identification systems would also yield the high quality phylogenetic trees. However, we find the opposite in our phylogenetic experiments.
We hypothesize the higher gains in en-US are due to higher adoption by native Spanish speakers. For Hindi/English, this approach helps because it corrects the labels based on underlying linguistic pattern.
In a third variant of our experiment we deepen the optimization of resources by taking into account lemmatization and POS tagging in connection with writing words in lower case. While lemmatization increases the observation frequency of words, POS tagging allows a more correct specification of their syntactic roles in sentences and consequently differentiates individual observations that are included in the calculation of embeddings. On the other hand, lower case writing of words removes ambiguities, as they are induced in German especially by capitalization at the beginning of sentences.
One of the main benefits of character models is their ability to gracefully handle OOV or infrequent words. On the dev set, the Weiss et al. model predicts about 130 word types that were not seen in training, which helps the model recall 7 OOV tokens out of 400. This is too small an effect to account for the performance differences, so we also analyze performance for a range of word frequencies. al. ’s We only consider content word types—words that are more than five characters long and are not in the NLTK stopword lists. The word-level model recall drops rapidly for medium frequency words, and for rare word types it has almost 0% recall. From this, we see that the main benefit of the character-level model is in handling of rare words, rather than previously unseen words.
Our DNN trained in weakly-supervised manner achieves 94.6% top-1 accuracy, noticeably higher than the CNN trained on face-verified segments, as proposed in the VoxCeleb paper.
Further, we also perform additional experiments by training the intent classification models on noisy ASR transcripts. A more robust feature representation should theoretically help in reducing the noise in the model translating to better performance. comparable CER to the models (word2vec) trained on ASR output, possibly reducing the need for adaptation on ASR and allowing for more generalizable systems.
Once the word-pairs are identified, we trained a Naive Bayes classifier to identify any day in a specific region to be an event or non-event days. We performed the experiment on both Melbourne and Sydney regions in Australia, and we achieved a classification accuracy of 87% with the precision of 77%, Recall of 82 %, area under the ROC curve of 91% and F-Score of 79%.
We tune the parameters on a held-out part of train. For entity pairs without a relation, we use the label N. Dataset statistics and model parameters are provided in the appendix. For setup 1, the CRF layer performs comparable or better than the softmax layer. For setup 2 and 3, the improvements are more apparent. We assume that the model can benefit more from global normalization in the case of table filling because it is the more challenging setup. The comparison between setup 2 and setup 3 shows that the entity classification suffers from not given entity boundaries (in setup 3). Nevertheless, the relation classification performance is comparable in setup 2 and setup 3. This shows that the model can internally account for potentially wrong entity classification results due to missing entity boundaries.
i.e. 1−CHAIRs; we find that SPICE consistently correlates higher with 1−CHAIRs. E.g., for the FC model the correlation for SPICE is 0.32, while for METEOR and CIDEr – around 0.25.
In order to measure usefulness of our proposed metrics, we have conducted the following human evaluation (via the Amazon Mechanical Turk). We have randomly selected 500 test images and respective captions from 5 models: non-GAN baseline, GAN, NBT, TopDown and TopDown - Self Critical. The AMT workers were asked to score the presented captions w.r.t. the given image based on their preference. They could score each caption from 5 (very good) to 1 (very bad). We did not use ranking, i.e. different captions could get the same score; each image was scored by three annotators, and the average score is used as the final human score. For each image we consider the 5 captions from all models and their corresponding sentence scores (METEOR, CIDEr, SPICE). We then compute Pearson correlation between the human scores and sentence scores; we also consider a simple combination of sentence metrics and 1-CHAIRs or 1-CHAIRi by summation. The final correlation is computed by averaging across all 500 images. Our findings indicate that a simple combination of CHAIRs or CHAIRi with the sentence metrics leads to an increased correlation with the human scores, showing the usefulness and complementarity of our proposed metrics.
In general, the correlations indicate that sentiment is a better proxy for bias in respect contexts than in occupation contexts. Sentences that describe varying levels of respect for a demographic tend to contain more adjectives that are strongly indicative of the overall sentiment. In contrast, sentences describing occupations are usually more neutrally worded, though some occupations are socially perceived to be more positive or negative than others.
Evaluating the Decision-Sharing Mechanism. We now evaluate Jumper in a multitask learning setting to see if the symbolic knowledge can help decision making for other slots. We include F1-score because some slots are skewed. We see that Jumper achieves better performance, with known knowledge formatted in a symbolic way and fed back to the neural network. Although the improvement is not large, the results are consistent in terms of both accuracy and the F1-score for both development and test sets.
The results demonstrate that the models trained on our corpus have an advantage over the term matching method. Compared with the benchmarks, BERT achieves the best performance for all the three tasks: binary and ternary bias identification tasks, and hate speech detection. Each model not only shows different performances but also presents different characteristics.
Similar to the binary prediction task, CharCNN outperforms BiLSTM on ternary classification. To make up a system that covers the broad definition of other bias, it would be better to predict the label as the non-gender bias. For instance, it can be performed as a two-step prediction: the first step to distinguish whether the comment is biased or not and the second step to determine whether the biased comment is gender-related or not.
We report two main results and four ablations for seen and unseen house evaluations; the former are novel dialogs in houses seen at training time, while the latter are novel dialogs in novel houses. Both RMM and Data Augmentation introduce new language by exploring and the environment and generating dialogs. In the case of RMM an RL loss is used to update the models based on the most successful dialog. In the Data Augmentation strategy, the best generations are simply appended to the dataset for one epoch and weighted appropriately for standard, supervised training. The augmentation strategy leads to small boost in BLEU performance and goal progress in several settings In contrast, Recursive Mental Model continues to produce over 500 unique lexical types, much closer to the nearly 900 of humans.
On the TIMIT corpus, using 2 filter stages, the best performance was found with: 310 ms of context, 30 samples width for the first convolution, 7 frames kernel width for the second convolution, 80 and 60 filters and 3 pooling width. Using 3 filter stages, the best performance was found with: 310 ms of context, 30 samples width for the first convolution, 7 and 7 frames kernel width for the other convolutions, 80, 60 and 60 filters and 3 pooling width. Using 4 filter stages, the best performance was found with: 310 ms of context, 30 samples width for the first convolution, 7, 7 and 7 frames kernel width for the other convolutions, 80, 60, 60 and 60 filters and 3 pooling width. We also set the hyper-parameters to have a fixed classifier input. For the baselines, the MLP uses 500 nodes for the hidden layer and 9 frames as context. The SLP based system uses 9 frames as context.
At the end we compare our results with the state-of-the-art results obtained on target tasks of DDI and DDIC. as well as the results obtained by BLSTM-RE and T-BLSTM-Mixed models on dissimilar tasks. We can observe that although BLSTM-RE can not outperform the best results of the challenge but under the TL framework, T-BLSTM-Multi even using dissimilar tasks improved the state-of-the-art results.
We observe that both of the hybrid models perform clearly better than pure cited text span summaries. Hybrid 2 performs better than Hybrid 1, most likely because Hybrid 2 builds on existing summaries (abstracts) and can ensure higher quality. In this experiment, Hybrid 2 added two sentences on average to the original abstract.
Wikitext-2 is not much larger than Penn Treebank, so it is not surprising that even models tuned for Penn Treebank perform reasonably on this dataset, and this is in fact how results in previous works were produced. For a fairer comparison, we also tune hyperparameters on the same dataset. All our results are well below the previous state of the are for models without dynamic evaluation or caching. whose innovations are fairly orthogonal to the base model.
We compared our proposed SMDA with BERT and XLNet in terms of accuracy(%) and Macro F1 score. BERT and XLNet achieved similar performance since they both obey the pre-training and fine-tuning manner. When combining with augmented and more balanced labeled data, massive unlabeled data, our SMDA achieved best performance across six binary-classification tasks. And we submitted the classification results on given unlabeled test set.
The unsupervised baseline of LGNB ranks the labels using a lexical association measure (Pearson’s χ2). To provide a better comparison between the two methodologies, we present experiments evaluating the candidate generation and ranking method of the two systems separately.
Thus, as stated earlier, the use of an additional layer dedicated to sentiment leads to development of a probabilistic document modeling method capable of outperforming the RS method.
For this evaluation, we first calculated the total number of words shared between each of the three dictionaries and the MPQA sentiment lexicon. Then, we followed the below procedure for each of the modeling modes and topic numbers: Calculating the total weights of positive words and negative words for each topic by the use of the sentiment dictionary and the matrix weight for the connection between the visible and hidden layers. Calculating, for each topic, the difference between the two values calculated in step 1 and sorting the answers in descending order. Assigning positive tags to the top five topics of the ordered list (most positive topics); and assigning negative tags to the bottom five topics of this list (most negative topics). Comparing the tags assigned to each topic with the corresponding topic weights in the connection of the sentiment layer to calculate the precision. The idea behind the comparison made in step 4 (comparison of the tag assigned to each topic with the corresponding weight in the sentiment layer) is that for a topic assigned with a positive tag in step 3, the weight corresponding to the positive sentiment tag for that topic in the sentiment layer should be greater than the negative weight for the same topic and vice versa. Figure (5) shows the results of this evaluation. This figure indicates that as the dictionary size increases, so does the model precision in the assignment of sentiment tags to the topics. As can be seen, as the dictionary size increases, so does the number of words shared between dictionary and the sentiment dictionary, and this leads to greater differentiation of the positive and negative topics in the training process, which result in improved model precision in the training and in assigning sentiment label to the topics.
In terms of Rouge-1 recall, the original model is clearly very dependent on checking for redundancy when including sentences, while the global variant does not change its performance much without the anti-redundancy filter. This matches the expectation that the globally motivated method handles redundancy implicitly.
For training the models, we augment the train and cv set to make it 3x bigger by adding gain, noise, and speed perturbations. The input lattice arc feature All models have 64 dimensional hidden state vectors hi and a binary output which is evaluated using binary cross entropy loss. The models are evaluated on the eval set using ROC curves comparing true positive rate (TPR) and false alarm rate (FAR) metrics. We expect our devices to have minimal false alarms and maximum true positives for a good user-experience. Therefore, we focus on the high TPR (>0.99) regime in our ROC curves and prefer models which have largest area under the curve (AUC).
ASR based baseline is not tunable and has a false alarm rate of 0.868. In comparison, lattice RNN baseline and the masked SAGNN approach perform similarly by mitigating ∼86.6% (FAR=0.134) of false alarms at 1% false rejects (TPR=0.99). Lattice RNNs rely on recurrent operations which result in efficient parameter sharing across the time dimension. While the lattice RNNs perform similar to the GNN-based FTM, we found that they are often inconvenient due to increased training time as compared to the GNNs (∼8min/epoch for RNN training v/s ∼1.5min/epoch for GNN training in our experiments). On the other hand, in GNNs, recurrent computations are replaced by graph convolution operations and multiple lattices of different sizes and structures can be efficiently batched together using zero padding resulting in substantial training speed-up.
One of the three neural models is a state-of-the-art essay scoring model by taghipour2016neural. We use two configurations of their model for comparison - (1) best parameter set used by taghipour2016neural and (2) tuned parameter set used by riordan2017investigating for ASAG. The other two neural models are InferSent (conneau2017supervised), the generic transfer learning model and one model by Saha2018 that combines hand-crafted and deep learning features. Notably, Saha2018 utilizes hand-crafted token features along with deep learning embeddings, suggesting that such fusion is helpful for ASAG.
Neural Entity Selection The baseline model above naïvely selects all entities as candidate answers. We first attempt to address this with a neural entity selection model (NES) that selects a subset of entities from a list of candidates provided by our ENT baseline. Our neural model takes as input a document (i.e., a sequence of words), D=(wd1,…,wdnd), and a list of ne entities as a sequence of (start, end) locations within the document, E=((estart1,eend1),…,(estartne,eendne)). The model is then trained on the binary classification task of predicting whether an entity overlaps with any of the human-provided answers. As expected, the entity tagging baseline achieves the best recall, likely by over-generating candidate answers. The NES model, on the other hand, exhibits much better precision and consequently outperforms the entity tagging baseline significantly in F1. This trend persists when comparing the NES model and the pointer network. The H&S model exhibits high recall but lacks precision, similar to the baseline entity tagger. This is not surprising since that model is not trained on SQuAD’s answer-phrase distribution.
Row (a) is the baseline where NLU and NLG models are trained independently and separately by supervised learning. The best performance in Su et al.
We observe that all models perform substantially better than chance, confirming the efficacy of our methodology in capturing moral dimensions of words. We also observe that models using word embeddings trained on Google N-grams perform better than those trained on COHA, which could be expected given the larger corpus size of the former.
First, we can see that using SVR (line 1) to combine ROUGE-L-mult and WPSLOR outperforms both individual scores (lines 3-4) by a large margin. This serves as a proof of concept: the information contained in the two approaches is indeed complementary.
In q-A setting, BERT was better than the Bi-LSTM baseline, which indicates BERT was useful for this task. Although the performances of TSUBAKI and BERT were almost the same in terms of SR@1, the performance of BERT was better than TSUBAKI in terms of SR@5, which indicates BERT could retrieve a variety of QA pairs. The proposed method performed the best. This demonstrated the effectiveness of our proposed method. The score of BERT was better than one of BERTtargetOnly, which indidates that using other FAQ sets is effective.
In the same way as the result on localgovFAQ, BERT performed well, and the proposed method performed the best in terms of all the measures. The performance of BERT was better than one of ”BERT (w/o query paraphrases)”, which indicates that the use of various augmented questions was effective.
We follow the standard TIMIT protocol for training and testing. We use 192 randomly selected utterances from the complete test set other than the core test set as our development set, and will refer to the core test set simply as the test set. The phone set is collapsed from 61 labels to 48 before training. In addition to the 48 phones, we also keep the glottal stop /q/, sentence start, and sentence end so that every frame in the training set has a label. We lexicalize all of the above features to first order, and include a zeroth-order bias feature. No explicit regularizer is used; instead we choose the step size and iteration that perform best on the development set (so-called early stopping).
The challenge scoring metric was a micro F-measure based on chunks of consecutive labels. Our ensemble system scores 58.89% with respect to this metric. Table LABEL: scores summarize the results of the competition and show that our system won with a rather large margin. Fasttext features bring a notable difference since the sequence level accuracy drops to 57.8% when we remove all of them. The 13 labels were separated according to their IOB encoding status.
Here, (w) and (w/o) mean “with speaker labels” and “without speaker labels” respectively. These show proper use of KL cost annealing leads to a higher KLD term and a lower test error, suggesting it allows the decoder (VoiceLoop) to recieve more useful information from the latent variables. Moreover, the test errors of VAE-loop is smaller than that of VoiceLoop without speaker labels, suggesting incorporating latent variables into the speech generating process enables VoiceLoop to estimate audio features more accurately.
Computational Efficiency. Response time is an important consideration for real-time answer generation in E-Commerce. Specifically, we record the training time per epoch and answer generation time for each model under the same computing environment with a Tesla k40 GPU. For fair comparison, all the models are implemented by using TensorFlow framework. Note that for review-based answer generation, we need to conduct auxiliary review snippet extraction for each testing question. However, there are many ways to speed up this procedure by the parallel computing. We therefore exclude the time for auxiliary review snippet extraction from the consideration. Similarly, for TA-S2S, we omit the time taken for topic inference and topical word extraction. We can see that S2SA and TA-S2S take much more time for model training and answer generation. Apparently, ConvS2S, ConvS2S-RV, RAGE/POS and RAGE take much less time for training and generation due to the parallization of the convolutional operations. Overall, the proposed RAGE obtains promising generation performance in terms of both effectiveness and efficiency.
In order to determine the optimal weights for each modality for multi-task learning, we tune the parameter From the table, we observe that the best value of α is 0.67 for both L1 + AoT and L1 + Odd. Thus, we use the models trained with this value when evaluating on the test sets in all experiments.
Although the hyperparameters of NeuroNER were not optimized for these datasets (the default hyperparameters were used), the performances of NeuroNER are on par with the state-of-the-art systems.
Image Relevance. Importantly, our explanation model has higher METEOR and CIDEr scores than our baselines. The explanation model also outperforms the explanation-label and explanation-discriminative model suggesting that both label conditioning and the discriminative loss are key to producing better sentences. Furthermore, METEOR and CIDEr are substantially higher when including a discriminative loss during training (compare rows 2 and 4 and rows 3 and 5) demonstrating that including this additional loss leads to better generated sentences. Surprisingly, the definition model produces more image relevant sentences than the description model. Information in the label vector and image appear complimentary as the explanation-label model, which conditions generation both on the image and label vector, produces better sentences. Class Relevance. Our explanation model produces a higher class similarity score than other models by a substantial margin. The class rank for our explanation model is also lower than for any other model suggesting that sentences generated by our explanation model more closely resemble the correct class than other classes in the dataset. We emphasize that our goal is to produce reasonable explanations for classifications, not rank categories based on our explanations. We expect the rank of sentences produced by our explanation model to be lower, but not necessarily rank one. Our ranking metric is quite difficult; sentences must include enough information to differentiate between very similar bird classes without looking at an image, and our results clearly show that our explanation model performs best at this difficult task. Accuracy scores produced by our LSTM sentence classifier follow the same general trend, with our explanation model producing the highest accuracy (59.13%) and the description model producing the lowest accuracy (22.32%). Explanation. the evaluation of two experienced bird watchers. The bird experts evaluated 91 randomly selected images and answered which sentence provided the best explanation for the bird class. Our explanation model has the best mean rank (lower is better), followed by the description model. This trend resembles the trend seen when evaluating class relevance. Additionally, all models which are conditioned on a label (lines 1, 3, and 5) have lower rank suggesting that label information is important for explanations.
We evaluate two methods to incorporate word segmentation information. We can see that positional character embeddings perform better in neural network. This is probably because positional character embeddings method can learn word segmentation information from unlabeled text while word segmentation can only use training corpus.
(Overall) and out-of-vocabulary entities (OOV) recall. Peng and Dredze \shortcitepeng-dredze:2016: P16-2 is the state-of-the-art NER system in Chinese Social media. By comparing the results of B-LSTM model and B-LSTM + MTNN model, we can know transition probability is significant for NER. Compared with B-LSTM + MMNN model, F-Score Driven Model I improves the result of named entity with a loss in nominal mention. The integrated training model (F-Score Driven Model II) benefits from both label accuracy and F-Score, which achieves a new state-of-the-art NER system in Chinese social media. Our integrated model has better performance on named entity and nominal mention. The results of our experiments also suggest directions for future work. So we need to design some methods to solve the problem.
The empirical results reveal the advantages of using OMP or GOMP for regularization in the text categorization task. The OMP regularizer performs systematically better than the baseline ones. More specifically, OMP outperforms the lasso, ridge and elastic net regularizers in all datasets, as regards to the accuracy. At the same time, the performance of OMP is quite close or even better to that of structured regularizers. Actually, in the case of electronics data, the model produced by OMP is the one with the highest accuracy. On the other hand, the proposed overlapping GOMP regularizer outperforms all the other regularizers in 3 out of 10 datasets.
As it becomes apparent, both OMP and GOMP yield super-sparse models, with good generalization capabilities. More specifically, OMP produces sparse spaces similar to lasso, while GOMP keeps a significantly lower number of features compared to the other structured regularizers. In group regularization, GOMP achieves both best accuracy and sparsity in two datasets (vote & books), while group lasso only in one (sports).
et al. We show that group lasso regularizers with simple logistic models remain very effective. Nevertheless, adding pre-trained vectors in the deep learning techniques and performing parameter tuning would definitely increase their performance against our models, but with a significant cost in time complexity.
There are three important messages. First, we have purposely incorporated the self-attention module into all of our baselines models—indeed having self-attention in the architecture could potentially induce a supervisedly-trained graph, because of which one may argue that this graph could replace its unsupervised counterpart. Second, as we adopt pretrained embeddings in all the models, the baselines establish the performance of feature-based transfer. Third, the learned graphs are generic enough to work with various sets of features, including GloVe embeddings, ELMo embeddings, and RNN output.
Image Classification We are also prompted to extend the scope of our approach from natural language to vision domain. In the transfer phase, we chose CIFAR-10 classification as our target task. Similar to the language experiments, we augment H by HM, and obtain the final input through a gating layer. Two architectures, i.e. ResNet-18 and ResNet-34, are experimented here.
The 95% confidence interval for GluonNLP models was calculated from perplexity means obtained across ten LOOCV iterations with random model weight initialization on each iteration. The RWTHLM package does not provide support for GPU acceleration and requires a long time to perform a single LOOCV iteration (approximately 10 days in our case). Since the purpose of using the RWTHLM package was to replicate the results previously reported by Fritsch et al. fritsch2019automatic that were based on a single LOOCV iteration and we obtained the exact same AUC of 0.92 on our first LOOCV iteration with this approach , we did not pursue additional LOOCV iterations. However, we should note that we obtained an AUC of 0.92 for the difference between Pcon and Pdem on two of the ten LOOCV iterations with the GluonNLP LSTM model. Thus, we believe that the GluonNLP LSTM model has equivalent performance to the RWTHLM LSTM model.
However, the M2 score of 42.76 for CoNLL-2014 is already higher than the best published result of 41.53 M2 for a strictly neural GEC system of \newciteji2017nested that has not been enhanced by an additional language model.
We crowdsourced the task of assigning these labels to disease mention spans. Workers saw the entire label text, with our target diseases highlighted, and were asked to select the best statement among those provided above. We launched our task on Figure Eight, asking for 5 judgments for each drug–disease pair. The Prevents, Treats, and Treats Outcomes relations are usually stated directly in drug labels, leading to high agreement. In contrast, Not Established, Not Recommended, and Other are more subtle and diverse, leading to lower agreement. The label of Normosol-R, for example, states: “The solution is not intended to supplant transfusion of whole blood or packed red cells in the presence of uncontrolled hemorrhage”. What is the relation between Normosol-R and hemorrhage? In this case, any of Not Established, Not Recommended, and Other seems acceptable.
It also shows the related PDTB implicit relation prediction scores. The PDTB is annotated with a hierarchy of relations, with 5 classes at level 1 (including the EntRel relation), and 16 at level 2 (with one relation absent from the test). It is interesting to see that this form of simple semi-supervised learning for implicit relation prediction performs quite well, especially for fine-grained relations, as the best model slightly beats the best current dedicated model, listed at 40.9% in Rutherford et al.
Many recent studies have suggested the importance of measuring the dataset bias by checking the model performance based on partial information of the problem Gururangan et al. Most notably, ablating questions does not cause significant performance drop. Further investigation indicates that this is because the high-level question types, e.g., what happens next, what happened before, are not diverse, so that it is often possible to make a reasonable guess on what the question may have been based on the context and the answer set. Ablating other components of the problems cause more significant drops in performance.
Considering the unique challenge of Cosmos, we explore two related multiple-choice datasets for knowledge transfer: RACE Lai et al. Specifically, we first fine-tune BERT on RACE or SWAG or both, and directly test on Cosmos to show the impact of knowledge transfer. Furthermore, we sequentially fine-tune BERT on both RACE or SWAG and Cosmos. With sequentially fine-tuning, SWAG provides better performance, which indicates that with fine-tuning on SWAG, BERT can obtain better commonsense inference ability, which is also beneficial to Cosmos.
In real world, humans are usually asked to perform contextual commonsense reasoning without being provided with any candidate answers. To test machine for human-level intelligence, we leverage a state-of-the-art natural language generator GPT2 Radford et al. Specifically, we fine-tune a pre-trained GPT2 language model on all the [Paragraph,Question,Correct Answer] of Cosmos training set, then given each [Paragraph,Question] from test set, we use GPT2-FT to generate a plausible answer. We also create a AMT task to have 3 workers select all plausible answers among 4 automatically generated answers and a “None of the aboce” choice for 200 question sets. We consider an answer as correct only if all 3 workers determined it as correct. We observe that by fine-tuning on Cosmos, GPT2-FT generates more accurate answers. Although intuitively there may be multiple correct answers to the questions in Cosmos QA, our analysis shows that more than 84% of generated correct answers identified by human are semantically consistent with the gold answers in Cosmos, which demonstrates that Cosmos can also be used as a benchmark for generative commonsense reasoning.
We carried out various experiments to achieve better accuracy, using the data and system setup described in previous sections. We report BLEU, 1-TER, 1-PER, and 1-CDER for the various experiments. TER, PER and CDER are the word error rates (WER) to measure the quality of translation. In general, these scores should be low for a better MT system. In contrast, 1-WER implies that higher is the value better would be the accuracy. It can be seen that the use of preprocessing and transliteration has contributed to the improvement of 1 to 1.5 BLEU points over the baseline for English-Hindi, English-Punjabi and English-Tamil. For English-Malayalam the BLEU has decreased and we plan to investigate this in our future work. Also, investigation is needed to figure out why the BLEU score decreased on use of factors in English-Punjabi, while it was useful for other language pairs.
The submitted translations, of the unseen test set, were obtained using S4 and S4′. The submitted systems were manually evaluated by three native speakers for Adequacy, Fluency, and Rating. The shared task organizers (MTIL17) used the percentage of Adequacy and Fluency as the primary metric for the shared task. From the evaluation results, it is evident that our (CDAC-M) submissions significantly outperform the other submissions for English-Hindi, English-Tamil, and English-Malayalam. For English-Punjabi our stands the second position.
The system has high performance in general, with a macro-average accuracy of 93.6%, and edit distance of 0.14. This is substantially higher than the baseline (77.8% accuracy and 0.5 edit distance), and ranks as the 9th best run, and 4th best team in this SIGMORPHON 2017 shared task setting. Furthermore, the difference in scores between our run and the best run overall is low (1.75% accuracy and 0.04 edit distance).
Our patient cohort was created using the following criteria: only stays longer than 48 hours were considered, in cases where patients were admitted multiple times to the ICU only the first admission was considered, and patients should have at least one free-text note recorded during their ICU stay. These criteria lead to a sample with n=21415.
First we compare the two augmentation methods against our basic system, where, based on observations in Bergmanis et al. We decided to use a constant number of additional examples rather than a percentage to better account for the low-resource languages, the ones benefiting most from the experiment, where for example a 20% increase in training data would still translate to having less than 500 training examples. Secondly, we add experiments on using a mixture of both augmentation techniques and increasing the number of additional examples included. Additionally, we test how well a morphological transducer itself could serve as a lemmatizer by measuring its coverage (how many words from the test data are recognized by the transducer) and lemma recall (how many words from the test set have the correct lemma among the possible analyses given by the transducer). Lemma recall therefore gives an upper-bound, oracle accuracy achievable by the transducer, assuming that all lemmas in its output can be correctly disambiguated. We measure macro accuracy over all treebanks and results are given separately for three treebank groups: All treebanks includes all 76 treebanks studied in this paper, Excluding low resource is all treebanks except the five low resource treebanks, and Transducer-only treebanks is a set of 47 treebanks representing languages which have a morphological transducer available. Note that in All treebanks results the Augm. transducer row uses the basic model for treebanks where a transducer is not available, giving a realistic comparison against the Augm. autoencoder method which does not suffer from lacking resources. In the mixed experiments, if a transducer is not available for a language, the training data is enriched only with the autoencoder examples. The two direct transducer metrics (Transducer Coverage and Recall), however, can be realistically measured only for languages having a transducer available and the results reported for the Transducer-only treebanks group allow for a direct comparison between plain transducers and our models.
As we can see, our method can consistently achieve more than 1.0 BLEU score improvement over the strong Transformer base system for all tasks. Compared with other augmentation methods, we can find that 1) our method achieves the best results on all the translation tasks and 2) unlike other methods that may not be powerful in all tasks, our method universally works well regardless of the dataset. Specially, on the large scale WMT 2014 En→De dataset, although this dataset already contains a large amount of parallel training sentence pairs, our method can still outperform the strong base system by +1.3 BLEU point and achieve 29.70 BLEU score. These results clearly demonstrate the effectiveness of our approach.
By contrast, L0Drop yields a speedup of 1.21× and 1.65× on CNN/Daily Mail and WikiSum, respectively. One explanation lies at the significant difference in target sequence length, where the average length per summary is >60, compared to ∼25 in machine translation. Note that L0Drop achieves a substantially higher sparsity rate of 71.5% on WikiSum with the same λ=0.3. This is because the input paragraphs overlap in content; the information about redundant words does not need to be routed into other encoder states, making easier to prune them.
On WMT14 En-De, Transformer using these rule-based patterns achieves comparable translation quality to L0Drop (-0.24 to +0.05 BLEU) with similar sparsity rate. One interesting observation is that Transformer also works with language- and context-agnostic sparsity patterns (Freq Pattern). The performance drop by Inv Freq Pattern (-0.64 BLEU) is in line with the information-theoretic expectation that information from frequent words is easier to compress than that of rare words.
We also carried out a more detailed error type analysis of the best CoNLL-2014 M2 system with/without training data using ERRANT (Tab. Specifically, this table shows that while the trained system was consistently better than the untrained system, the degree of the improvement differs significantly depending on the error type. In particular, since the untrained system was only designed to handle Replacement word errors, much of the improvement in the trained system comes from the ability to correct Missing and Unnecessary word errors. The trained system nevertheless still improves upon the untrained system in terms of replacement errors by 10 F0.5 (45.53 vs. 55.63).
AdaSent outperforms all the other models on the MPQA data set, which consists of short phrases ( the average length of each instance in MPQA is 3). We attribute the success of AdaSent on MPQA to its power in modeling short phrases since long range dependencies are hard to detect and represent.
We report the mean classification accuracy and also the standard deviation of the 10 runs on each of the data set. Again, AdaSent consistently outperforms all the other competitor models on all the data sets.
A1.1 (wBT+BM25) achieves the best P@5 score. On the other hand, A1.2 (wQ+BM25) gives best performance over two other metrics. It is interesting to note that using RoBERTa for finding semantic similarity between query and documents harms the performance given by A1.1 and A1.2.
We observe that in every epoch of co-teaching, there is a significant boost in F1 scores for each of the languages on the XNER task. Compared to other languages, zero-shot transfer for Arabic is low. We suspect this is because of the structural dissimilarity between Arabic and English k2020crosslingual. However, the improvement for Arabic is consistent with (and even higher than) other languages.
Results are presented in Tab. The imported class-specific configurations, computed using a much smaller corpus (Sect. The BEST-VERBS configuration is outscored by SP, but the margin is negligible. We also evaluate another configuration found using Alg. 1 in Sect. This configuration (amod+subj+obj+compound+prep+adv+conj) outperforms all baseline models on the entire benchmark. Interestingly, the non-specific BEST-ALL configuration falls short of A/V/N-specific configurations for each class. This unambiguously implies that the “trade-off” configuration targeting all three classes at the same time differs from specialised class-specific configurations.
For purpose of comparison of dictionaries, stemmed words are used. The overlap between the LScDC and the NAWL is 99.6%, with 891 word occurring in both. This means 4 words occurring only in NAWL: “ex”, “pi”, “pardon” and “applaus”. The lower coverage of the dictionary seems to be the result of differences in types and processing of texts in corpora.
∑iniNt where ni is the size of intersection in ith interval, and Nt is the total number of words (891). We repeated the same calculation for different widths of intervals, with an increasing sequence 5, 10, 15, … 890, 891. For instance, when the width is 5 the lists are divided into 179 intervals: 178 complete interval with 5 words, 1 shorter interval with 1 word. Observe that not in all cases lists are divided into equal intervals. For instance, the width 890 of interval means that there are two partitions with 890 and 1 words and so the comparison is not much meaningful in these cases. To avoid unbalanced classes, we consider only those number of intervals where partitions have almost equal widths. When the lists are divided into two intervals, the fraction of overlap is 0.73. Hence, 27% of words of a list do not lie within the same half of the other list. In addition, almost half of words are in different intervals when splitting the lists into 3 intervals, with approximately 300 words in each interval (300 words in two intervals and 291 words in one interval). Our findings raise the possibility that two lists are slightly different in terms of ranking words within lists.
In this study, the Spearman’s correlation is calculated by assigning a rank of 1 to the highest value within each list, 2 to the next highest and so on. The correlation between words in two lists will be high when words have a similar rank within lists. The calculation of Spearman correlation for this study gives a value of 0.58 which confirms what was found in the comparison of ranks and what was apparent from the graph. There is indeed a moderate positive correlation between two lists, which are monotonically related. However, the logarithmic scaled-frequencies show a linear relation.
The NMT architecture evaluated here uses 3-layer 512-dimensional bidirectional GRU for the source, and a 1-layer 1024-dimensional attentional GRU for the target. Each sentence is decoded independently with a beam of 6. Since these speedups are all mathematical identities excluding quantization noise, all outputs achieve 36.2 BLEU and are 99.9%+ identical.
We have found that the benefit of using RNN+FC layers on the source is minimal, so we only perform ablation on the target. For the source, we use a 3-layer 512-dim bidi GRU in all models (S1)-(S6).
Our experiments reported in The first row shows that using cosine similarity with a hinge loss yields much better results than using a cross-entropy loss. This is likely because (i) there are some duplicate question pairs that were not tagged as such and that have come up as negative pairs in our training set, and the hinge loss deals with such outliers better. (ii) The cosine similarity is domain-invariant, while the weights of the feed-forward network of the softmax layers capture source-domain features.
As expected, the improvements are the biggest on three smallest corpora, which by themselves do not contain enough text for the model to obtain sufficient syntactic and semantic knowledge. The largest gains are achieved on the NUS test set, where almost an 80 percent improvement in terms of the F@10 score can be observed, and on the SemEval test set, where the improvement of 77 percent in terms of F@5 can be observed. We also observe about a 44 percent improvement in terms of F@10 on the DUC test set. Not surprisingly, for the KP20k dataset, which has a relatively large validation set used for fine-tuning, we can observe a much smaller improvement of about 20 percent in terms of F@10. On the other hand, we observe a larger improvement of about 30 percent in terms of both F@5 and F@10 on the KPTimes test set, even though the KPTimes validation set used for fine-tuning is the same size as KP20k validation set. This means that in the language modelling phase the model still manages to obtain knowledge that is not reachable in the fine-tuning phase and can perhaps be partially explained by the fact that all documents are truncated into 256 tokens long sequences in the fine-tuning phase. The KPTimes-valid dataset, used both for language modelling and fine-tuning, has on average 784.65 tokens per document, which means that more than half of the document’s text is discarded during the fine-tuning phase. This is not the case in the language modelling phase, where all of the text is leveraged.
When adding 17607h of unlabeled data, VggT and VggTLarge obtain similar performance but with 35217h of additional data, VggTLarge obtains 1 BLEU improvement on the En-De MuST-C dev set.
ConVecs and SimDiffs have roughly similar profiles but balAPinc is substantially different from the other two. This is what we would expect, given that ConVecs and SimDiffs both approach lexical entailment as a semantic relation classification problem, whereas balAPinc approaches it as a problem of designing an asymmetric similarity measure. The approach of balAPinc is near the level of the other two for some relation categories (e.g., class-inclusion, non-attribute) but substantially below for others (e.g., attribute, case relations, reference).
The matrices that were chosen based on the development datasets are in bold font. For balAPinc, Gen (57.3%) is indeed the best matrix. For ConVecs, it seems that Fun (71.9%) might be a better choice than Gen (70.2%), but the difference in their accuracy is not statistically significant. For SimDiffs, Dom and Fun (72.4%) are slightly less accurate than Gen and Fun (72.8%), but again the difference is not significant. As expected, no matrices are significantly better on the Test set than the matrices that were chosen based on the development datasets.
Regarding the machine learning hyperparameters we have noticed that increasing the number of trees had a positive impact in the results, finding that n=500 was the optimal value. No relevant improvements were found when using a higher number of estimators.
We have trained the word embedding in such a way that the sub-word size remains between 1 and 4. We particularly chose this size because in Nepali language a single letter can also be a word, for example \dne, \dnt, \dnC, \dnr, \dnl, \dnn, \dnu and a single character (grapheme) or sub-word can be formed after mixture of dependent vowel signs with consonant letters for example, \dnC + \dnO + \dn\2 = \dnCO\2, here three different consonant letters form a single sub-word. Here, raw dataset represents such dataset where post-positions are not lemmatized. We can observe that pre-trained embeddings significantly improves the score compared to randomly initialized embedding. We can deduce that Skip Gram models perform better compared CBOW models for word2vec and fasttext.
For both physician and coder judgment, non-clinical vectors perform terrible, achieving correlation coefficients that are consistently 20-30 points lower than all models trained on MIMIC. We can observe differences between word2vec and WORD2VECF by comparing rows 2 and 3. WORD2VECF is built to perform the parameter estimation of that word2vec does, so by preprocessing the corpus identically to how word2vec does, these two rows “should” be identical. Despite not having perfect agreement, we do see that neither method is overtly better than the other, with word2vec having higher correlation with coders and WORD2VECF correlating better with doctors. However, we can see the benefits of incorporating CUI-space similarity into word vectors, as shown by AWE-CM outperforming MIMIC W2VF: words on all three judgment categories. Since these models were built from the same data with the same WORD2VECF tool, the differences between them can only be attributed to the domain knowledge incorporated by the additional (word,CUI) pairs. Additionally, we observe that the vectors that are augmented with UMLS relationships (AWE-CM, row 4) achieve the best correlation with doctors, which is more relevant for most downstream predictions using word vectors such as mortality and readmission prediction, diagnosis, information retrieval, and concept extraction.
To run the different tools on randomly generated automata, we used SPOT to generate 50 random HOA automata for each combination of state space size |Q|∈{10,20,…,90,100,125,…,225,250} and alphabet size |Σ|∈{2,4,…,18,20}, for a total of 8000 automata, that we have then translated to the BA format. We then considered 100 different pairs of automata for each combination of state space size and alphabet size (say, for instance, 100 pairs of automata with 50 states and 10 letters or 100 pairs with 175 states and 4 letters). In particular, we can see that both ROLLH and ROLLB have been able to find a counterexample to the inclusion for many cases (3194 and 1052, respectively) where SPOT on the HOA format and RABIT on the BA format failed, respectively. About the execution running time of the tools, they are usually rather fast in giving an answer, as we can see from the plot in Fig. In this plot, we show on the y axis the total number of experiments, each one completed within the time marked on the x axis; the vertical gray line marks the timeout limit. The plot is relative to the number of “included” and “not included” outcomes combined together; the shape of the plots for the two outcomes kept separated is similar to the combined one we present in Fig. As we can see, we have that ROLL rather quickly overcame the other tools in giving an answer. This is likely motivated by the fact that by using randomly generated automata, the structure-based tools such as RABIT and SPOT are not able to take advantage of the symmetries or other structural properties one can find in automata obtained from, e.g., logical formulas. holds. Moreover, we also conclude that IMC2 complements existing approaches rather well, as it finds counterexamples to the language inclusion for a lot of instances that other approaches fail to manage.
Using the information above, we reprocessed the REVERB SimData using a window duration of 96 ms, and degrees of iteration of (1−a)=0.3 and (1−a)=0.7. A degree of iteration of (1−a)=0.3 performed best out of these two (a degree of iteration of (1−a)=0.7 gave worse objective metrics, except for SRMR). We choose to display PESQ (Perceptual Evaluation of Speech Quality) and the latter is both a measure of dereverberation and the only non-intrusive measure that can be run on RealData (for which the clean speech is not available).
Accuracy may not be a sufficient metric to capture specifically what the model learned about the positive (Influential) class. It is possible that the accuracy is high because the model learned to predict the negative class (Non-Influential) correctly. The table shows a comparison between the true positives of the baseline model and those of the best performing model along with the respective F-Scores. The true positives are the influential posts in our data labeled as defined in Section 3. The predicted positives are posts that were predicted as influential by the model. A similar definition stands for true negative and predicted negative. As shown in the table, the model trained on all the feature sets, correctly classifies more positive labels than the baseline model. Hence, we also get an improvement of 2.91 point for F-score.
When combining Speciteller with each of the 3 other feature sets individually, kappa increases but not with statistical significance. We evaluated additional classifiers (Support Vector Machine, decision tree, random forest, Naive Bayes) but none of them outperformed logistic regression. Since the number of features is over 7000, we also tried using Recursive Feature Elimination (RFE) and Principal Component Analysis (PCA) for feature selection/reduction, but neither improved performance. This result confirms our third hypothesis: the additional feature sets are able to capture aspects of specificity with respect to verbal discussion and the educational domain. In particular the feature set containing neural network-based sentence embedding achieved the best kappa measure of 0.6550, which suggests that sentence embeddings are also domain-dependent. Compared to using Speciteller off-the-shelf this method improves kappa by 32%. While the size of the neural network was constant during training/test (not optimized for each fold), we experimented with several numbers of hidden nodes (ranging from 50 to 200) for the LSTM and fully-connected layers, which resulted in kappa values in the range 0.6283−0.6550.
The Pedagogical feature set is also able to marginally outperform the Speciteller feature set. Compared to the best result, the loss in kappa when using the Pedagogical set is 11%. At the expense of a slightly lower accuracy we gain the ability to use only informative features, which can be used to better understand highly specific versus general classroom discussions. The use of logistic regression also makes this possible: the model’s coefficients give us an indication of how important features are.
The HKUST corpus consists of 171.1 hours of Mandarin Chinese conversational telephone speech from Mandarin speakers in mainland China. It is divided into a training set of 166.3 hours, and a test set of 4.8 hours. We split the training data into 3 subsets: the first two subsets are P and T in this paper, while the remaining subset is used for validation. We report Mandarin character error rate (CER) on the test set. The NSC corpus consists of 2,172.6 hours of English read microphone speech from 1,382 Singaporean speakers. We extract data of 6 speakers as testing data. Similar to the HKUST corpus, we split the remaining data into 3 subsets for P, T and validation. We also perform data augmentation on the labelled data P. We report word error rate (WER) on the test set.
MATE performs better than ABAE by introducing the human-provided seed-words, which demonstrates the effectiveness of domain knowledge. However, MATE applies the same neural architecture as ABAE, which may not be the best fit to fully leverage the power of the introduced knowledge. Our generative model instead directly cooperates with the aspect memory, not only during the prediction stage but also during the segment encoding. Without any trainable parameters, our method outperforms ABAE and MATE on all the categories and achieves a 5.1% increase on average. It indicates that AspMem can get a better aspect-aware segment representation for aspect identification. The extra latent aspect embeddings of the General aspect (AspMem w/ extra memory) help the model better fit the intrinsic structure of the data, which further improves the performance by 6.0%. When comparing with BERT, our model still has better performance on three categories and achieves the same average F1 score. Note that while BERT is a pre-trained model with 110M parameters, our model only has 1K parameters.
When removing the redundancy filtering (w/o filtering), it achieves the highest performance. This observation is different from that made by \citeauthorangelidis2018summarizing\shortciteangelidis2018summarizing who found that redundancy filtering improved the ROUGE scores of results produced by MATE. Upon eyeballing the generated summaries we found that in absence of redundancy filtering, AspMem’s summaries often included the overlapping part of the three references (i.e., the segments with similar opinions but from different references) more than once. This results in the improvement of ROUGE scores: the more matched n-grams are found, the better the results. However, we prefer to avoid redundancy in order to improve readability.
In addition, we can note that the use of Wang2Vec pre-trained word embeddings made it possible to obtain better F1-score results in BLSTM, which is not necessarily noticed in other cases, since for SVM and Random Forest, Glove and FastText, respectively, were the techniques that obtained better F1-scores.
We can see that the genres gospel, funk-carioca and sertanejo have a greater distinction in relation to the other genres, since they were better classified by the model. In particular, funk-carioca obtained a good classification result although it did not have a large number of collected song lyrics.
This is in line with expectation as our model was trained using the FA expectations. However, it confirms that the learned word representations are able to explain translation probabilities. Surprisingly, context seems to have little impact on the alignment error, suggesting that the model receives sufficient information from the aligned words themselves.
Unlike the Fill module that looks at syntactic patterns, the softened Find module looks at semantic meanings of the phrases. For this task, it’s hard to find appropriate training data. We tried various data as proxies, including coreference resolution data (coreference result on the SQuAD dataset produced by the Stanford CoreNLP The module is manually evaluated on the collected explanations: the reference phrases pref are the key phrases in questions identified by annotators, and we inspect on the model’s predictions on the corresponding context. We tested with various training data and found that pre-training the module only makes the performance worse. We conjecture it may be caused by data bias (the training data not aligning with the purpose of the module) or overfitting issues (fine-tuning BERT with small number of data may lead to severe overfitting). Therefore we do not pre-train the softened Find module. We use mean pooling and cosine similarity for the module and rely on BERT’s pretrained weights to capture the semantic meanings of the phrases. The Fill module is evaluated on hard-matched question pairs and context pairs, respectively, and the Find module is evaluated through manual inspection on model’s predictions on 100 question-context pairs. For each sentence in the testing data, we enumerate all possible spans, let the model rank the spans, and take top-n (n=1,3,5,10 for Fill module and n=1 for Find module) spans as output. Because our goal is to find the correct span, we use recall (at n) rn =pq as metric for evaluation, where p is the number of correct spans found in top-n outputs and q is the number of all correct spans. As n gets large, the top-n outputs from the Fill module are able to cover most of the correct spans.
Training networks with only 105 BiLSTM cells per layer results in higher CERs compared to using 210 cells. A network of this size is too small for modelling the acoustics entirely, but we are using it only as part of our superstructure to extract language dependent features. On the other hand, having source nets with too many parameters may render the superstructure prone to over fitting. Comparing the CERs of nets trained on graphemes and phonemes, the error rate of the German and Turkish grapheme based setup is lower in comparison to their phoneme based counterpart. Potential reasons are a) the pronunciation dictionaries may be of varying quality as they were generated completely automatic and b) Turkish as well as German have easier pronunciation rules than English or French.
Performance across all metrics systematically deteriorates as ASR accuracy becomes worse. The cascaded approach with SimASR at a word error rate (WER) of 50% performs similarly to VisionSpeechCNN on P@10 and P@N (bottom row), but worse on all other metrics. This indicates that VisionSpeechCNN could prove useful even when ASR is available (with a relatively low accuracy). However, further experiments on truly low-resource languages are required to illustrate this more conclusively.
The proposed neural machine transliteration model has been compared to the baseline method provided by Baseline results are based on a machine translation implementation at the character level using MOSES Experimental results shows that the proposed model is significantly better than the robust baseline using different metrics.
In our dataset of book snippets, there are 11 texts ( 5%) with sarcasm target outside the text. In case of tweets, such cases are much higher: 53 tweets ( 10%). Dice Score (DS) for book snippets is 6.81 for ‘outside’ cases as compared to 32.68 for the complete dataset. In general, the performance for the ‘outside’ cases is lower than the overall performance. This proves the difficulty that the ‘Outside’ cases presents. The EM and DS values for ‘Outside’ cases are the same by definition. This is because when the target is ‘Outside’, a partial match and an exact match are the same. Our approach correctly predicts the label ‘Outside’ for sentences like ‘Yeah, just ignore me. That is TOTALLY the right way to handle this!’ However, our approach gives the incorrect output for some examples. For example, for ‘Oh, and I suppose the apples ate the cheese’, the predicted target is not ‘Outside’ (the expected label) but ‘I’. Similarly, for ‘Please keep ignoring me for all of senior year. It’s not like we’re friends with the exact same people’, the incorrectly predicted target is ‘me’ instead of the expected label ‘Outside’.
Against QA extension, the detector reached 71% accuracy in distinguishing true vs. false extensions generated by Grover. While this is above a trivial majority baseline (51%), some of the success could be a side effect of the way that the dataset was constructed. Labeling instances with at least one false statement as fake makes these sentences generally longer, allowing a simple linear classifier that only uses the extension’s length to reach 56%. When evaluated only on the short false answers (up to 10 words), the accuracy drops from 71% to only 62%.
To better understand the extent of this limitation, we conducted an experiment in a similar manner to the switched hypernym pairs in \newcitesantus2016nine. We used BLESS, which is the only dataset with random pairs. For each hypernym pair (x1,y1), we sampled a word y2 that participates in another hypernym pair (x2,y2), such that (x1,y2) is not in the dataset, and added (x1,y2) as a random pair. We added 139 new pairs to the validation set, such as (rifle, animal) and (salmon, weapon). We then used the best supervised and unsupervised methods for hypernym vs. random-n on BLESS to re-classify the revised validation set.
To test our hypothesis that in-training factorization can increase the performance of the transformer model, we perform the same experiment comparing a transformer model with factorized embeddings of dimension 512 and inner size of 256, with a non-factorized baseline model with embeddings of dimension 512 on two other languages of the IWSLT dataset: Portuguese (79525 training sentences) and Turkish (193734 training sentences). We observe that in all language pairs evaluated, in-training factorization improves the performance of the transformer model. This evidences that the performance gains observed with in-training factorization generalize across languages.
Cross-lingual embeddings are particularly promising for low-resource languages, where few labeled examples are typically available, but are not adequately reflected in current benchmarks (besides the English–Finnish language pair). We perform experiments with our method with and without a rank constraint and Artetxe et al. We additionally conduct an experiment for Estonian-Finnish, similarly to Søgaard et al. For all languages, we use fastText embeddings Bojanowski et al. trained on Wikipedia, the evaluation dictionaries provided by Conneau et al. We note that English does not share scripts with Bengali and Hindi, making this even more challenging.
The bag of semantic concepts model can leverage information coming from n-grams to improve sentiment classification of documents. This model has also the nice property to build document representations in an efficient and timely manner. The most time-consuming and costly process step in the model is the K-means clustering, especially when dealing with millions of n-gram representations. However, this step can be done very quickly with low memory by using mini-batch K-means method. All experiments have been run on single CPU core Intel i7 2600K 3.4 GHz. Despite the fact that single CPU has been used for this benchmark, the three steps of the model are highly parallelizable. The recorded times could thus be divided by the number of CPUs available. We see that representations can be computed in less than one minute with only 1-gram dictionary. About 10 minutes are necessary when adding 2-grams, and about 40 minutes by adding 3-grams. In comparison, LDA needs six hours for extracting 100 topics and three days for 300 topics. Our model is also very competitive with LSA which takes 540 seconds to generate 300-dimensional document representations. However, adding 2-grams and 3-grams to perform a LSA would be extremely time-consuming and memory-hungry while our model can handle it.
Fig. DeFrauder outperforms the best baseline by 11.35% and 4.67% higher relative EMD (averaged over four datasets) for GS and RCS, respectively. We also notice that DeFrauderT performs better than DeFrauderR, indicating that temporal coherence is more important than rating coherence in detecting potential groups.
Similar performance scores were obtained using other (rather diverse) parameter settings. From a quick inspection, we did not observe any clear trends regarding usefulness of parameter values, except more features (higher c_ngmax and w_ngmax values seem to help).
The inclusion of age-11 predictions as predictors in Task B was useful if gold-standard scores were used. However, we did not observe any benefits if predicted age-11 outcomes were used. As a result, we used the same model, except we did not use the predicted age-11 scores as predictors in our final model. The model obtained disattentuated R scores of 0.443, 0.3175 and 0.1961 for ages 23, 33 and 42, respectively, on the official evaluation. With an average of 0.3189, it ranked best among other participating models. Similar to Task A, the support vector regression model yielded similar but again slightly lower results.
Interestingly, allowing the model to embed visual representations into an intermediate attribute space has a strong positive effect on performance. Intuitively, since DAN is evaluated on novel concepts, the mediating attribute layer provides more high-level semantic information helping generalization, at the expense of extra parameters compared to the ablation without attribute layer.
Besides reading passages, i.e. texts, the COCTAILL corpus contains a number of sentences independent from each other, i.e. not forming a coherent text, in the form of lists of sentences and language examples. This latter category consists of sentences illustrating the use of specific grammatical patterns or lexical items. Collecting these sentences, we built a sentence-level dataset consisting of 1874 instances. The information encoded in the content-level annotation of COCTAILL (XML tags list, language_example and the attribute unit) enabled us to include only complete sentences and exclude sentences containing gaps and units larger or smaller than a sentence (e.g. texts, phrases, single words etc.). The CEFR level of both sentences and texts has been derived from the CEFR level of the lesson (chapter) they appeared in. COCTAILL contained a somewhat more limited amount of B2 and C1 level sentences in the form of lists and language examples, possibly because learners handle larger linguistic units with more ease at higher proficiency levels.
We trained document-level classification models, comparing the performance between different subgroups of features. We had two baselines: a majority classifier (Majority), with B2 as majority class, and the LIX readability score. Not only was accuracy very low with LIX, but this measure also classified 91.6% of the instances as B2 level. Lexical features, however, had a strong discriminatory power without an increase in bias towards the majority classes. Using this subset of features only, we achieved approximately the same performance (0.8 F) as with the complete set of features, All (0.81 F). This suggests that lexical information alone can successfully distinguish the CEFR level of course book texts at the document level. Using the complete feature set we obtained 81% accuracy and 97% adjacent accuracy (when misclassifications to adjacent classes are considered correct). The same scores with lexical features (Lex) only were 80.3% (accuracy) and 98% (adjacent accuracy). Although the majority baseline in the case of sentences was 7% higher than the one for texts This is a considerable drop (-18%) in performance compared to the document level (81.3% accuracy). It is possible that the features did not capture differences between the sentences because the amount of context is more limited on the fine-grained level. It is interesting to note that, although there was no substantial performance difference between Lex and All at a document level, the model with all the features performed 7% better at sentence level.
Our best model consistently betters the zero-shot baselines CT and ZAT, which use only slot descriptions, overall and individually for 5 of 7 intents. The average gain over CT and ZAT is ∼3% in the zero-shot case. In the low-data setting, all zero-shot models gain ≥5% over the multi-domain LSTM baseline (with the 10-example-added model further gaining ∼2% on CT/ZAT). All models are comparable when all target data is used for training, with F1 scores of 87.8% for the LSTM, and 86.9% and 87.2% for CT and our model with 10 examples respectively.
We compared our model against several related models covering a wide spectrum of representations including word-based ones (e.g., paragraph vector and CNN models) as well as hierarchically composed ones (e.g., a CNN or LSTM provides a sentence vector and then a recurrent neural network combines the sentence vectors to form a document level representation for classification). Previous state-of-the-art results on the three review datasets were achieved by the hierarchical attention network of Yang et al. As can be seen, the combination is beneficial achieving best results on three out of four datasets. Furthermore, structured attention is superior to the simpler word-to-word attention mechanism, and both types of attention bring improvements over no attention. The structured attention approach is also very efficient, taking only 20 minutes for one training epoch on the largest dataset.
We first analyze each heuristic separately: using element-wise product alone is significantly worse than concatenation or element-wise difference; the latter two are comparable to each other.
The top rows show performance using all PVPs in a fully unsupervised setting, where both average results across all patterns (avg) and results using the best pattern (max) are reported. Importantly, finding the best pattern would require access to the test set; accordingly, this row serves only as an upper bound. The large difference between both rows highlights the importance of finding a strategy to cope with the fact that we have no means of evaluating which patterns perform well. The third row shows the performance of iPet in a zero-shot setting. As can be seen, iPet outperforms the unsupervised baselines in all cases. Furthermore, zero-shot iPet is on par with a supervised model trained on 1000 examples for Yahoo and even outperforms regular supervised training with 1000 examples on AG’s News.
For Italian, we report the average zero-shot cross lingual performance of the German and French model as there are no Italian training examples. Our results show that Pet brings huge improvements across all languages even when training on much more than a thousand examples; notably, Pet also considerably improves zero-shot cross lingual performance.
Combining PVPs We first investigate whether Pet is able to cope with situations were some PVPs perform much worse than others. We see that even after finetuning, the gap between the best and worst pattern is large, especially for Yelp. However, Pet is not only able to compensate for this, but even improves accuracies over using only the best-performing pattern across all tasks. We find no clear difference between the uniform and weighted variant of Pet.
The EOS system performs consistently over varying utterance sequence lengths and achieves an accuracy of approximately 92%.
We compare the results with the state-of-the-art models for the style transfer task. All the baseline models are trained on the single style pair. The ST2 model is trained on all the tasks for both LT and GSD sets, and then fine-tuned using a specific style pair in the sets. The trained meta-learner is fine-tuned on each of the sub-tasks, and the scores are calculated as the average among all sub-tasks for both ST2 models and baselines.
The multi-task model achieves minor improvements over the LSTM baseline, with a bigger improvement in the micro-averaged score, indicating bigger improvements with frequent labels. The adversarial model performs best, with an error reduction in micro-averaged F over the LSTM baseline of 5.6%.
UTs significantly outperform standard Transformers, and achieve an average result comparable to the current state of the art (99.2%). However, we see that UTs (and particularly with dynamic halting) perform progressively better than all other models as the number of attractors increases (see the last row, Δ).
We trained UTs on three algorithmic tasks, namely Copy, Reverse, and (integer) Addition, all on strings composed of decimal symbols (‘0’-‘9’). We train UTs using positions starting with randomized offsets to further encourage the model to learn position-relative transformations. The UT outperforms both LSTM and vanilla Transformer by a wide margin on all three tasks. , however we note that this result required a special curriculum-based training protocol which was not used for other models.
A couple of observations are noteworthy. First, the Bleu scores are lower than one would expect from a machine translation system, for example. This is because there is much more ambiguity when generating descriptions of human motion than when translating a text into a different language. In the former case, different levels of details and different styles are also semantically correct (e.g. “A human walks” vs. “ Someones takes a couple of steps”), whereas the latter case is much more constrained. Second, the Bleu scores are clearly correlated with the order of the hypotheses (as defined by their log probabilities under the model) even though the loss that was used to train the model and the Bleu score are completely separate. This means that the log probability is suitable as a measure of quality and, in turn, that the model indeed captures some understanding of what an objectively high-quality (as measured by Bleu), description is.
In all three systems, we used the same set of features as follows: binary unigram, word unigram, character n-grams of length 4 and 5, and word embeddings. In the first system, we used both train and validation sets for training our ensemble classifier. In the second system we only used the train data for training the model. Unfortunately, we could not try applying the sentiment and lexicon-based features after spell correction due to the restrictions on the total number of submissions. However, we believe that it can improve the performance of the system.
The applicability of the generated models (for the programming of interfaces) and thus the performance of YANG2UML can be measured by using metrics. Already at first sight, it can easily be seen that now all UML objects are related to each other; therefore the depth of ietf-interfaces was reduced from 5 to 1, since now all objects remain in one layer. As also depicted, the number of classes (blue color) was reduced from 5 to 3, while the number of types remained constant (orange color). As a logical consequence, the number of attributes slightly increased (from 29 to 33). All of these show that the complexity of the generated models is significantly lower, compared to PYANG. Especially the statistics from the module ietf-routing show impressively that the number of objects could be reduced by more than half.
The chained model, which also uses the source sentence, is able to harness larger volumes of data, to obtain yet better results (primary model).
The BLEU scores are already very high (about 16 points above those of the en-de data, and 10 points above the best APE outputs for en-de). This is probably due to the translation direction being reversed (because of its rich morphology, German is a much harder target that English). The results obtained with a vanilla SMT system (SPE) seem to confirm this difficulty.
We tune our hyperparameters using the automatic metric to evaluate topic coherence discussed in Lau et al. \shortcitelau2014machine. The metric aims to maximize coherence among different dimensions of the representation, which has been shown by the authors to correlate positively with human evaluation. Through experiments with different configurations, we observe that high topic coherence comes at the cost of high reconstruction loss, which manifests itself in the form of poor performance on downstream tasks. To mitigate this issue, we cap the maximum permissible reconstruction loss to a threshold and select the best performing hyperparameter setting within this constraint. We observed that a hidden layer of size 1000 units is optimal for our case. Hence, we transform X∈R15000×300 to Z∈[0,1]15000×1000. We also find utility in making the autoencoder denoising, attaining embeddings that are 6% more sparse at similar reconstruction loss.
To test the quality of the embeddings generated by our model, we use them in the following benchmark downstream classification tasks: sentiment analysis, news classification, noun phrase chunking, and question classification. We experiment with SVMs, Logistic Regression and Random forests, which are tuned on the development set. Accuracy is reported on the test set. , it is clear that the embeddings generated by our method perform competitively well on all benchmark tasks, and do significantly better on a majority of them.
We report the results on three metrics: F1-score, weighted accuracy, and accuracy. Due to the imbalance between the numbers of positive and negative samples, weighted accuracy is more informative than unweighted accuracy, so we focus on the former. HTDN. There is a significant gap between the HTDN (and variants) and other non-neural approaches. This better performance is an indicator of complex interactions in detecting dynamics of human trafficking, which is captured by the HTDN.
To compare the different methods, we evaluated all of them on the test set from Mikolov et al. In most cases, this is because the second term is returned as answer (man is to king as woman is to king, thus D==B), but in some cases it is the third term that gets returned (short is to shorter as new is to new, thus D==C). The Bolukbasi et al. method shows very low scores, but this was to be expected, since their formula was not specifically designed to capture factual analogies. But what is so different between factual and biased analogies?
From this table, we observe that our ExHiRD-s and ExHiRD-h consistently and significantly reduce the duplication ratios on all datasets. Moreover, we also find that our ExHiRD-h model achieves the lowest duplication ratios on all datasets.
We also study the average number of unique keyphrase predictions per document. Duplicated keyphrases are removed. One main finding is that all the models generate an insufficient number of unique keyphrases on most datasets, especially for predicting absent keyphrases. We also observe that our methods can improve the number of unique keyphrases by a large margin, which is extremely beneficial to solve the problem of insufficient generation. Correspondingly, it also leads to over-generate more keyphrases than the ground-truth for the cases that do not have this problem, such as the present keyphrase predictions on Krapivin and KP20k datasets. We leave solving the over-generation of present keyphrases on Krapivin and KP20k as our future work.
Since our ExHiRD-h model achieves the best performance on almost all of the metrics, we select it as our final model and probe it more subtly in the following sections.
Our exclusive search is a general method that can be easily applied to other models. In this section, we study the effects of our exclusive search on other baseline models.
We analysed the performance of our agents further by using a test set of totally unseen dialogues during training. While a significant difference (according to a two-tailed Wilcoxon Singed Rank Test) at p=0.05 was identified in testing on the training set, no significant difference was found in performance during testing on the test set. These results could be confirmed in other datasets and/or settings in future work. In addition, we can observe that the ChatDQN agents trained using all data (agents with id=20) were not able to achieve as good performance than those agents using smaller data splits. Our results thus reveal that training chatbots on some sort of domains (groups of dialogues automatically discovered in our case), is useful for improved performance.
Accuracy for the NCD method using an array of linear compressors ranged from the 93.3% obtained using the bzip2 compressor to the 96.6% obtained with the blocksort compressor. Even though accuracies are comparable and the dataset may be small to be statistically meaningful, another advantage of FCD over NCD is the decrease in computational complexity. While for NCD it took 202 seconds to build a distance matrix for the 90 pre-formatted texts using the zlib compressor (with no appreciable variation when using other compressors), just 35 seconds were needed on the same machine for the FCD: 10 to extract the dictionaries and the rest to build the full distance matrix.
Next, the contextual bias FST is added in the inference. When λc=0, it means no context FST is used; when λb=0, it means no score normalization is used. From setup 1 to 2, using context FST and scale 0.1 reduces the WER from 18.1 to 14.4; from setup 2 to 3, turning on score normalization alone reduce the WER from 14.4 to 11, changing the context FST scale from 1.0 to 0.1 further reduces the WER to 9.0. The overall WER reduction of using context FST is 50% (from 18.1 to 9.0). The last column shows the WER on the regular set, the WER is increased slightly from 8.4 to 8.5. The WER reduction from context FST is still big (from 15.3 to 6.5, a 57.5% reduction). When using the subword sequence from the mapped words, the WER is reduced to 5.5. Interestingly, if using both the original sequence and mapped sequence in the FST, the WER is further reduced to 4.5. That’s about 31% error reduction over original context FST. The overall WERR over the no-FST baseline is 70.6%, with total inference time increased by 10%. Fig. The WER reduction is consistent across utterances with different number of bias phrases.
We observe that the skimming models achieve higher or similar F1 score to those of the default non-skimming models (LSTM+Att) while attaining the reduction in computational cost (Flop-R) by more than 1.4 times. Moreover, decreasing layers (1 layer) or hidden size (d=5) improves Flop-R, but significantly decreases the accuracy (compared to skimming). There are four points on the axis of the figures associated with two forward and two backward layers of the model. We see two interesting trends here. First, the skimming rate of the second layers (forward and backward) are higher than that of the first layer across different gamma values. A possible explanation for this trend is that the model is more confident about which tokens are important at the second layer. Second, higher γ value leads to higher skimming rate, which agrees with its intended functionality.
Here the data are averaged over 5 times of running the same retraining process with 12 epochs, to mitigate the affects due to the intrinsic indeterminism of neural networks. From column 4 and 7, the results show that the train perplexity of the model after retraining increases by 1.082% whereas the valid perplexity decreases by 1.159% in the end. Moreover, the original test perplexity is 117.53 and that after retraining is 102.75, thus also declined by 12.582%. Notice that even by incorporating fewer adversarial inputs (1.6KB), the valid perplexity still declines by 0.058%.
We perform the same tests on the XLNet (Single-Fact) model to verify the ability of our tests to truly measure multi-hop reasoning and being harder to answer via disconnected reasoning.
Flows represent a high level abstraction of our entire systems functionality, allowing a new designer to rapidly add content to the system without needing to familiarize themselves with the underlying architecture. Since these flows represent a high level abstractions of the entire system, we found that reusing successful modules is an effective means of bootstrapping flows with minimal effort. Specifically, most flows have some recursive trivia based prompt in their list of subroots. We also found that generically discussing user preferences and utilizing search methods increased the breadth of a flow, while a combination of all methods could increase the depth of a flow.
Flows represent a high level abstraction of our entire systems functionality, allowing a new designer to rapidly add content to the system without needing to familiarize themselves with the underlying architecture. Since these flows represent a high level abstractions of the entire system, we found that reusing successful modules is an effective means of bootstrapping flows with minimal effort. Specifically, most flows have some recursive trivia based prompt in their list of subroots. We also found that generically discussing user preferences and utilizing search methods increased the breadth of a flow, while a combination of all methods could increase the depth of a flow.
We claim that an entertaining socialbot which can engage the user in a variety of activities other than pure conversation will lead to a better user experience. Here, we see the number of turns the user interacted with the specified module and the average score of a conversation in which the interaction happened. We note that the number of turns here do not account for the time in which the user is in a modules menu; for example, while they are picking a game to play or a survey to do. We have also removed from consideration the case in which the user does not actually engage with the activity, for example when they enter a menu but change topics.
Again, we only show figures of the best method for each task. The use of in-domain training data allowed the NMT system to yield much better translation quality than in the previous case. Especially dramatics were the improvements in the TED and XRCE corpora (+10.4 and +26.4 BLEU points, respectively). For such tasks, the use of OL only affected the system to a negligible extent.
The key observations are: (a) All SIMMC neural models (HAE, HRE, MN, T-HAE) outperform the baselines (TF-IDF and LM-LSTM) across all metrics for both the datasets. (b) For response generation, MN has superior BLEU score and T-HAE has the least perplexity for both SIMMC-Furniture and SIMMC-Fashion across all models. Surprisingly, T-HAE has one of the least BLEU scores amongst SIMMC models perhaps due to resorting to safe, frequent responses. (c) HRE consistently achieves the highest API prediction accuracy for SIMMC-Furniture (79.6%, jointly with HRE) and SIMMC-Fashion (85.1%), followed by T-HAE in both the cases. (d) This can be understood as the natural decision between searching for an item or further obtaining user preferences to narrow the search. Note that the proposed baselines do not leverage the rich, fine-grained annotations of the SIMMC datasets (understandably so) as they are mainly adaptations of existing state-of-the-art models.
By enabling the user to clarify, all agents obtain much better accuracy compared with the original non-interactive LAM model. In particular, HRL-based agents outperform others by 7% ∼ 13% on C+F accuracy and 2% ∼ 4% in terms of Overall accuracy. For vague recipes in VI-1/2 and VI-3/4 subsets, which make up more than 80% of the entire test set, the advantage of HRL-based agents is more prominent. For example, on VI-3/4, HRL-based agents obtain 9% ∼ 15% better C+F accuracy than LAM-rule/sup, yet with fewer questions, indicating that they are much more able to handle ambiguous recipe descriptions. We compare each agent primarily on C+F Accuracy and #Asks. Particularly, the HRL agent outperforms the LAM model by >40% accuracy, with an average of ∼2.2 questions on VI-3/4 (which is a reasonable number of questions as each task contains at least 3 vague subtasks). We also observe that the two HRL-based agents obtain 6% ∼ 20% better parsing accuracy with even fewer questions than the LAM-rule/sup agents. Moreover, in comparison with the HRL-fixedOrder agent, the HRL agent can synthesize programs with a much better accuracy but fewer questions, showing the benefit of optimizing subtask order at the high level. How to simulate user responses as close as possible to real ones for training is a non-trivial task, which we leave for future work.
Because our model is trained by identifying and reconstructing a parent sentence from its children, it sometimes fails to construct an appropriate tree for relatively short reviews. It also has a negative influence on summary generation. Therefore, we use reviews with 10 or more sentences for training, and those with 5 or more sentences for validation and evaluation.
As for ROUGE-1 and ROUGE-L, two-tailed t-tests demonstrate that the difference between our models and the others are statistically significant (p<0.05). Because the abstractive approach generates a concise summary by omitting trivial phrases, it can lead to a better performance than those of the extractive ones. On the other hand, for Movies & TV, our model is competitive with other unsupervised extractive approaches; TextRank and Opinosis. One possible explanation is that the summary typically includes named entities, such as the names of characters, actors and directors, which may lead to a better performance of the extractive approaches. For all datasets, our full model outperforms the one using only StrSum. Our models significantly outperform MeanSum-single, indicating that our model focuses on the main review points, and does not simply take the average of the entire document.
However, it should be noted that the baseline las-wp is worse than las-g. This is likely due to the small amount of data being insufficient to train the large number of additional parameters: we found that the larger we made the wordpiece vocabulary, the worse the model became. As a result of this difference, the LM results for las-wp are not directly comparable to the LM results for las-g. The main observation we make is that the RNN performs best in both cases, with the relative improvement being roughly consistent for both graphemes and wordpieces.
We now turn to the Voice Search task. Thus our analysis here is limited to las-wp. In the traditional HMM/CTC-based system, the decoding proceeds in two passes: the first pass uses a small n-gram LM (prodlm1), which fits in memory and minimizes the search space to meet real-time requirements. The first pass generates an N-best list which we rescore with a much larger n-gram LM (prodlm2) The much larger prodlm2, despite being 40x larger, provides only slightly more improvement. In addition, prodlm2 is 80GB and must be run on multiple servers. This is operationally unwieldy and cannot be efficiently integrated with low latency during the first pass. On the other hand, while computationally expensive, RNN LMs are known to be more compact than their n-gram counterparts. Its much lower memory footprint (1.1 GB) allows it to fit in the first pass. We then rescore the system with prodlm2 (as las-wp + rnn-wp + prodlm2). This yields no further gain, showing that we have obviated the need for a second-pass rescoring at all.
Automatic Metrics. The automatic evaluation uses the E2E generation challenge script. BLEU (n-gram precision), NIST (weighted n-gram precision), METEOR (n-grams with synonym recall), and ROUGE (n-gram recall). We note that multivoice automatically has a better chance because the evaluation is over 4,448 examples as opposed to 1,390 for singlevoice, and each multivoice output is compared to 2 possible references (one for each single voice), and then averaged.
Semantic Errors. The error counts are split by personality, and normalized by the number of unique MRs (278). Note that smaller ratios are preferable, indicating fewer errors. As we predicted, it is more challenging to preserve semantic fidelity when attempting to hit multiple stylistic targets. We see that in most cases the frequency of errors increase for multivoice compared to singlevoice, with particular combinations such as disagreeable plus extraversion making more than one attribute deletion for each output on average. In the singlevoice results disagreeable and extravert make the most errors with the smallest total ratio found for conscientious, but when conscientious combines with disagreeble it performs worse than either model alone.
Aggregation. To measure the similarity of each multivoice model to its parent single voices for aggregation operations, we first count the average number of times each aggregation operation occurs for each model and personality or personality combination. We then compute Pearson correlation across different model outputs to quantify the similarity of these model outputs with respect to the aggregation operations. This shows for example (Row 1) that agreeable and conscientious are similar in their use of aggregation but that disagreeable and extraversion are very dissimilar (Row 6). We would expect that models that are similar to start with would be less novel when they are combined, and indeed Row 1 shows that when the multivoice model is compared with both the original agreeable voice (Column 3) and the conscientious voice (Column 4) the use of aggregation operations changes little. However other combinations seem to produce completely novel models that use aggregation very differently than either of their singlevoice source models. For example in Row 7 the combination of disagreeable and unconscientious produces a model whose use of aggregation is distinct from either of its source models.
MNLI task is a sentence pair prediction task constructed by human crafted hypotheses based on premises, therefore original pairs share a considerable amount of same words. Perturbations on these words would make it difficult for human judges to predict correctly therefore the accuracy is lower than simple sentence classification task.
The BERT-Attack method is also applicable in attacking other target models, not limited to its fine-tuned model only. Under BERT-Attack, ESIM model is more robust in MNLI dataset. We assume that encoding two sentences separately gets higher robustness. In attacking BERT-large models, the performance is also excellent, indicating that BERT-Attack is successful in attacking different pre-trained models not only against its own fine-tuned downstream models.
Consequently, the BiLSTM model overfits on the training data, even before the gates can learn to allow the gradients to pass through (and mitigate the vanishing gradients problem). Thus, the model prematurely memorizes the training data solely based on the starting and ending few words. Further reduction in vanishing ratio is unable to improve validation accuracy, due to training saturation. A high value at this point indicates that the gradients are still skewed towards the ends, even as the model begins to overfit on the training data. The vanishing ratio is high for BiLSTM, especially in low-data settings. This results in a 12-14% lower test accuracy compared to other pooling techniques, in the 1K setting. We conclude that the phenomenon of vanishing gradients results in weaker performance of BiLSTM in low training data regimes. Pooling architectures have a lower vanishing ratio from the beginning and perform better on equivalent data settings, as demonstrated in the next section.
Our model exhibits the best performance across datasets in different domains, demonstrating the effectiveness and robustness of our model. Moreover, we draw the following interesting conclusions. First, based on the same fully-connncted graph representations, F-Graph slightly outperforms ATTOrderNet on all datasets, even with fewer number of parameters and relatively fewer recurrent steps. This result proves the validity of applying GRN to encode paragraphs. Second, S-Graph shows better performance compared with F-Graph. This confirms the hypothesis that leveraging entity information can reduce the noise caused by connecting incoherent sentences. Third, SE-Graph outperforms S-Graph on all datasets across all metrics. It is because incorporating entities as extra information and modeling the co-occurrence between sentences and entities can further contribute to our neural graph model. First, shuffling edges significantly hurts the performances of both S-Graph and SE-Graph. Intuitively, shuffling edges can introduce a lot of noise. These facts above indicate that fully-connected graphs are also very noisy, especially because F-Graph takes the same number of parameters as S-Graph. Therefore we can confirm our previous statement again: the entities can help reduce noise. Second, removing edge labels leads to less performance drops than removing or shuffling edges. It is likely because some labels can be automatically learned by our graph encoder. Nevertheless, the labels still provide useful information. Third, there are slight decreases for S-Graph and SE-Graph, if we only remove 10% entities. Removing entities is a way to simulate syntactic parsing noise, as our entities are obtained by the parsing results. This indicates the robustness of our model against potential parsing accuracy drops on certain domains, such as medical and chemistry. On the other hand, randomly removing 50% entities causes significant performance drops. As the model size still remains unchanged, this shows the importance of introducing entities. Particularly, the result of removing 50% entities for SE-Graph is slightly worse than original model of S-Graph, demonstrating that SE-Graph’s improvement over S-Graph is not derived from simply introducing more parameters. The result shows a drastic decrease on final performance, which is quite reasonable because entity nodes play fundamentally different roles from sentence nodes. Consequently, it is intuitive to model them separately.
Previous work has indicated that both the first and last sentences play special roles in a paragraph due to their crucial absolute positions, so we also report accuracies of our models on predicting them. Again, both results witness the advantages of our model.
The statistical model substantially outperforms the rule-based baseline for the BETTER and NONE classes while being comparable for the WORSE class. The overall improvement of the statistical model over the rule-based approach is about 3 points in terms of F1 score (0.85 as the best achieved performance). Furthermore, note that reported performance of the rule-based model could be a bit inflated as building of the dataset involved the use of similar cue words as those used in this baseline (cf.
As one can observe our model shows remarkably high cross-domain transfer with some out-of-domain combinations outperforming in-domain training, e.g., CompSci-Brands. While a substantial drop is observed for a few other domain pairs, e.g., Random-CompSci, the performance is still well above the majority class baseline suggesting that some knowledge transfer happened even in these cases and comparative argumentation is not highly domain-dependent.
When treebank embeddings are removed from our competition system, the performance deteriorates the most, even if only a little in absolute terms. This indicates that the UD and EvaLatin annotations are very consistent. Providing one embedding for EvaLatin data and another for all UD treebanks improves the performance, and more so if three UD treebank specific embeddings are used.
Hybrid MemNet represents our system with Conv-LSTM encoder and MemNet encoder, while Hybrid MemNet∗ uses Conv-BLSTM encoder and MemNet encoder. It is evident from the results that our system (Hybrid MemNet/ Hybrid MemNet∗) outperforms the LEAD and ILP baselines with a large margin which is an encouraging result as our system does not have access to manually-crafted features, syntactic information and sophisticated linguistic constraints as in the case of ILP. Results also show that our system performs better without the sentence ranking mechanism (URANK). It also achieves significant performance gain against NN-SE, Deep-Classifier, and SummaRuNNer.
Hybrid MemNet represents our system with Conv-LSTM encoder and MemNet encoder, while Hybrid MemNet∗ uses Conv-BLSTM encoder and MemNet encoder. It is evident from the results that our system (Hybrid MemNet/ Hybrid MemNet∗) outperforms the LEAD and ILP baselines with a large margin which is an encouraging result as our system does not have access to manually-crafted features, syntactic information and sophisticated linguistic constraints as in the case of ILP. Results also show that our system performs better without the sentence ranking mechanism (URANK). It also achieves significant performance gain against NN-SE, Deep-Classifier, and SummaRuNNer.
This difference in performance demonstrates that proper initialization is critical in training online end-to-end systems. Our goal is to close this gap in performance without relying on any of the linguistic resources required to build a tied-triphone acoustic model.
Following this insight, an important question to answer is to what extent the generators are simply memorizing the training set G-train. To this end, we assess the degree of n-gram overlap between the generated reviews and the training reviews using the BLEU evaluator. We observe that generally the generators do not memorize the training set, and GAN models generate reviews that have fewer overlap with G-train. BLEU w.r.t. G-train presents highly positive correlation with BLEU w.r.t. D-test real, and it is also positively correlated with the human evaluators H1 and H2.
Given the same few data however, sentences generated using BERT and explicit pattern injection can effectively reflect the speaking patterns of Trump in terms of his common usage of words (low perplexity) and while maintaining input context (seen from the high similarities to the input sentence).
Here we perform an ablation study, where we remove a feature at a time from the design of Genie or ThingTalk to evaluate its impact. We report the average across three training runs, along with the error representing the half range of results obtained.
We use the polynomial p(x) to find analytically the maximum xopt, i.e. the number of recurrent units that give on average the best test performance. We also use the polynomial to determine how large the impact of this hyperparameter is. A flat polynomial (small a value) is rather robust against changes in this parameters. Selecting a non-optimal number is less important and it would not be worthwhile to optimize this hyperparameter heavily. A steep polynomial (large a value) is more sensitive, a slightly too small or too large number of recurrent units changes the performance significantly. To make this intuitive understandable, we computed γ25=p(xopt±25)−p(xopt), which depicts how much the test performance will decrease if we choose the number of recurrent units either 25 units too small or too large.
In the comprehension task, it is not possible to measure accuracy. Instead, we evaluate readability and relevance as judged by human raters on a five-point Likert scale, two commonly used metrics for abstractive summarization decrease in readability with either condition. Curiously, the previous effect on speed reverses in this task – algorithmic titles only lead to a 2.6-second decrease in time, while human titles lead to a 20.9-second decrease. Both conditions additionally lead to longer summaries; algorithmic titles by 5.3 words and human titles by 8.6 words. One potential explanation for this behavior could be that subjects copied the presented section titles into the summary text field. This was not the case, since, on average, only 2.8-3.7% of the bigrams in the titles were used in the summaries, across both conditions and difficulties (0.6-1.5% of trigrams, 0.1-0.8% of 4-grams).
In our experimentation, we use the training and the test set as given in the IMDB dataset explicitly. The semantic similarity is measured using Spacy toolbox. The average semantic similarity between the original text sample and their adversarial counterparts (for test set only) are 0.9164 and 0.9732 with and without using the genre specific keywords respectively. We can see that the difference in the accuracies in these two rows are very less, which shows that the model has generalized well after retraining. The table also shows that the inclusion of genre specific keywords definitely boost up the quality of sample crafting. This is evident from the fact the drop in accuracy of the classifier before re-training for original text sample and the adversarialy crafted text sample is more when genre specific keywords are being used. We can observe that when we use genre specific keywords for creating adversarial samples, the number of tainted sample produced is more than that when genre specific keywords are used, for same number of changes done in two cases (red curve always above blue curve).
As no labeled corpus of relations for our domain was available, evaluation is difficult; in particular, calculating recall would involve labeling all the documents. To get a rough estimate of the recall of the system, we hand labeled a longer article, obtaining 8 correctly identified of 33 total relations. Hence at least locally the recall is 0.24. By manually checking each relation instance the system output The reported run accepted the top 80% of patterns and relations after each iteration of scoring and queried the user for the top 2%, resulting in ∼5 queries per iteration. Scores are reported for output after 3 iterations through the corpus, after which point the number of relations did not increase. Of the 41 relevant documents, 31 included an identified relation, and 186 overall relations were identified of which 153 are correct. We note that the seed patterns were crafted from example sentences observed by the authors, and, hence, testing on a larger corpus will be needed to flatten any bias that may have come from our observation of some of the 62 documents.
We report the results on the dataset of Conneau et al. The strikingly high performance of all methods on this task belies the hardness of the general problem of unsupervised cross-lingual alignment. Indeed, as pointed out by Artetxe et al. , the fastText embeddings provided in this task are trained on very large and highly comparable—across languages—corpora (Wikipedia), and focuses on closely related pairs of languages. Nevertheless, we carry out experiments here to have a broad evaluation of our approach in both easier and harder settings.
Further analysis of our models revealed that some feature combinations performed reasonably well on individual markers for both the disjunctive and conjunctive model, even though their overall accuracy did not match the best feature combinations for either model class. Given the complementarity of different models, an obvious question is whether these can be combined. An important finding in machine learning is that a set of classifiers whose individual decisions are combined in some way (an \wordensemble) can be more accurate than any of its component classifiers if the errors of the individual classifiers are sufficiently uncorrelated \shortciteDietterich:97. The next section reports on our ensemble learning experiments. Let us now examine which classes of features have the most impact on the interpretation task by observing the component learners selected for our ensembles. the syntactic structure of the main and subordinate clauses (S) and their position (P) are the most important features for interpretation. Verb-based features are present in all component learners making up the conjunctive ensemble and in 10 (out of 12) learners for the disjunctive ensemble. The argument structure feature (R) seems to have some influence (it is present in five of the 12 component (disjunctive) models), however we suspect that there is some overlap with S. Nouns, adjectives and temporal signatures seem to have a small impact on the interpretation task, at least for the WSJ domain. Our results so far point to the importance of the lexicon for the marker interpretation task but also indicate that the syntactic complexity of the two clauses is crucial for inferring their semantic relation. Asher and Lascarides’ \citeyearAsher:Lascarides:03 symbolic theory of discourse interpretation also emphasises the importance of lexical information in inferring temporal relations.
We next report on our experiments with ensemble models. Inspection of the performance of individual models on the development set revealed that they are complementary, i.e., they differ in their ability to perform the fusion task.
Word intrusion performance benefits significantly from lemmatization on a filtered vocabulary and a symmetric prior. Truncated documents exhibit lower performance overall and are helped less by lemmatization (posing challenges for social media applications). Further, we observe differences between use of an asymmetric prior on an unfiltered vocabulary and use of a symmetric prior on a vocabulary with stop words filtered out.
We find that the agreement scores are not high, which may indicate that the models learn complimentary information. We then concatenate the two dense representations to analyze model complementarity.
Results. The following are our observations: TransG outperforms all the baselines remarkably. Compared to TransR, TransG improves by 1.7% on WN11 and 5.8% on FB13, and the averaged semantic component number on WN11 is 2.63 and that on FB13 is 4.53. This result shows the benefit of capturing multiple relation semantics for a relation. The relations, such as “Synset Domain” and “Type Of”, which hold more semantic components, are improved much more. In comparison, the relation “Similar” holds only one semantic component and is almost not promoted. This further demonstrates that capturing multiple relation semantics can benefit embedding.
’s best model (DeleteAndRetrieval). We adopt three criteria range from 1 to 5 (1 is very bad and 5 is very good): grammaticality, similarity to the target attribute, and preservation of the source content. For each dataset, we randomly sample 200 examples for each target attribute. Among them, our AC-MLM-SS with the fusion-method achieves best results. Pre-trained MLM infills the masked positions considering the context from both directions, being able to meet grammatical requirements better. Benefiting from explicitly separating content and style, our sentences’ structures are kept and we perform better on the preservation of the source content. Then by introducing the pre-trained discriminator, MLM infills more accurate attribute words.
WordNet does not have an explicit identifier for concepts; instead, it defines sets of synonyms, i.e. lemmas with a shared sense. Sensigrafo calls such sets syncons (synonym-concepts), since they refer to specific concepts and assigns a unique identifier to each. There are differences in terms of size, granularity of concepts, structure of the network (e.g. choice of central concepts) and types of relations. Finally, because Sensigrafo is developed and maintained as part of a text analytics pipeline, some of its features are tailored and biased towards supporting functionality and domains required by Expert System customers. By comparison, being a community effort WordNet may benefit from a wider set of stakeholders.
Using this range, we identify 38 of 156 datasets as being biased. With other words, even though we took care not to generate non-biased datasets by using negative switching, a little more than a quarter of the datasets generated in this way contains clues about the relation. Interestingly, word-pair datasets are much more likely to be biased (38%) while word/concept pairs are unlikely to so (only 7%) we see the absolute and relative f1 measures for the models that significantly outperformed the random baseline. We see that most of the results with an average f1 score higher than 0.8 are for relations between concepts; these were all achieved by training models on the HolE embeddings. Swivel embeddings also obtained good results for predicting categorical relation (0.846 f1, but this result just cleared the 2σ significance threshold). One model trained on FastText embeddings achieved (0.666 f1 on a meronymy relation). This confirms that the studied corpus-based embeddings are not capable of capturing semantic relations at the concept level.
For word pair prediction, FastText outperforms other embedding learning algorithms, including HolE (although this difference is not major). HolE excels at predicting relations between senses, but its performance decreases as lemmas are introduced, since it cannot disambiguate between the senses. Vecsigrafo and GloVe both are not far behind the performance of FastText and HolE, but produce significant predictions for more relations than FastText and HolE. Standard Swivel with words lags behind, especially in the number of relations that it can predict.
As seen, the NMT models trained with our scheduler network perform the best across different language pairs. More specifically, the three MTL training heuristics are effective in producing models which outperform the MT-only baseline. Among the three heuristics, the Biased training strategy is more effective than Uniform and Exponential, and leads to trained models with substantially better translation quality than others. Although our policy learning is agnostic to this MTL setup, it has automatically learned effective training strategies, leading to further improvements compared to Uniform as the best heuristic training strategy. We further considered learning a training strategy which is a combination of the best heuristic (ie Biased) and the scheduler network, as described before. As seen, this combined policy is not as effective as the pure scheduler network, although it is still better than the best heuristic training strategy.
We study the impact of the various grounding hypotheses on the structure of the grounded space, using intrinsic measures. The textual loss is discarded to isolate the effect of the different grounding hypotheses.
We now focus on extrinsic evaluation of the embeddings. Before further analysis, we find that our grounded models systematically outperform the textual baseline T, on all benchmarks, which shows the first substantial improvement brought by grounding and visual information in a sentence representation model. Indeed, models GS-Cap, GS-Img and GS- Both from Kiela et al.
The empirical results for AUC are coherent with those of P@N, which shows that, our proposed approach can significantly improve previous ones and reach a new state-of-the-art performance by handling wrongly labeled problem using context-aware selective gate mechanism. Specifically, our approach substantially improves both PCNN+HATT and PCNN+BAG-ATT by 21.4% in aspect of AUC for precision-recall.
To further empirically evaluate the performance of our method in solving one-sentence bag problem, we extract only the one-sentence bags from NYT’s training and test sets, which occupy 80% of the original dataset. In addition, PCNN+ATT shows a light decrease compared with PCNN, which can also support the claim that selective attention is vulnerable to one-sentence bags.
In addition to the models trained on the BULATS data, it is also interesting to investigate the application of “out-of-the-box” models for standard speaker verification tasks to this non-native speaker verification task as there is limited amounts of non-native learner English data that is publicly available. VoxCeleb x-vector/PLDA system was used as imported models, which was trained on augmented VoxCeleb 1 There are more than 7,000 speakers in the VoxCeleb dataset with more than 2,000 hours of audio data, making it the largest publicly available speaker recognition dataset. 30 dimensional mel-frequency cepstral coefficients (MFCCs) were used as input features and system configurations were the same as the BULATS x-vector /PLDA one. Thus, two kinds of in-domain adaptation strategies were explored to make use of the BULATS training set: PLDA adaptation and x-vector extractor fine-tuning. For PLDA adaptation, x-vectors of the BULATS training set were first extracted using the VoxCeleb-trained x-vector extractor, and then employed to adapt the VoxCeleb-trained PLDA model with their mean and variance. For x-vector extractor fine-tuning, with all other layers of the VoxCeleb-trained model kept still, the output layer was re-initialised using the BULATS training set with the number of targets adjusted accordingly, and then all layers were fine-tuned on the BULATS training set. Here the PLDA adaptation system is referred to as X1 and the extractor fine-tuning system is referred to as X2. PLDA adaptation is a straightforward yet effective way, while the system with x-vector extractor fine-tuning gave slightly lower EERs on both BULATS and Linguaskill test sets by virtue of a relatively “in-domain” extractor prior to the PLDA back-end.
As there were only a small number of speakers graded as C1 or C2 in the BULATS test set, the two grade groups were merged into one group as C in the following analysis. For lower grades, impostor trials from the grade group of A1 dominated FA errors as A1 speakers tend to speak short utterances, which is more challenging for the systems. For higher grades (B2 and C), impostor trials from the grade group of C constituted a larger portion of FA errors probably due to the fact that C speakers tend to speak long utterances in a more “native” way and they are also similar to B2 speakers.
Looking at the single pair NMT models (S-NMT), we observe that in all the test domains they underperform with respect to the SS-NMT, TL, or M-NMT models in terms of averaged (AVG) BLEU scores. Specifically to each domain, the S-NMT models perform reasonably well on the in-domain test sets while, for the out-of-domain Ted test set we often observe rather large degradations. For instance, on the Sw/Am/So-En Ted test sets, there is a consistent performance drop in both in the LRL ↔ En translation directions. The performance drop with test sets featuring a domain shift with respect to the training data shows the susceptibility of NMT in a low-resource training condition. We expect that the S-NMT model performance can be improved with the more robust models described in Sec. However, there are still open problems that require further investigation with respect to the SATOS languages and other languages with small training data:
We can find that our method (Feature-Enriched-Net) outperforms the baselines on both precision and recall. Our method achieves the best 0.725 F1 score, which improves by a relative gain of 4.5%. Pointer-Net would achieve higher precision score for its attentive ability to the whole sentence and select the most important words. Our method considers long-short memories, attention mechanism and other abundant semantic features. That is to say, our model has the ability to extract the most essential words from original long titles and make the short titles more accurate and comprehensive.
S4SS3SSS0Px1 Evaluation on Contrastive Pronoun Test Set It has been argued that evaluation metrics which quantify the overall translation quality are somewhat ill-equipped to assess how well models translate inter-sentential phenomena such as pronouns. Hence, we use a test suite of contrastive translations designed to measure accuracy of translating the English pronoun it to its German counterparts es, er and sie Müller et al. We are interested to see if our global document-context models surpass the local context-aware baselines. We also conclude that models for offline MT perform better when antecedent distance is greater than two.
Our context-aware models introduce only 8% more parameters to the original Transformer model. At decoding time, our Hierarchical Attention model is almost equivalent to \newciteMiculicich:18 and only 13% slower than \newciteZhang:18. Hence, attending to the whole document (instead of few previous sentences) does not add to the time complexity of the model on average.
As shown in the table, both proposed methods give significant improvements in BLEU score, with the biggest gains in English to French (+0.99) and smaller gains in German and Spanish (+0.74 and +0.40 respectively). Reducing the number of parameters with fact_bias gives slightly better (en-fr) or worse (en-de) BLEU score, but in those cases the results are still significantly better than the baseline.
Although the difference is less salient than in the case of SATED, our factored bias model still performs significantly better than the baseline (+0.83 BLEU). This suggests that even outside the context of TED talks, our proposed method is capable of improvements over a speaker-agnostic model.
A small p-value, especially a p-value is less than 0.05, stands for the high possibility to accept the alternative hypothesis, which also indicates a significant difference between spammer and non-spammer. For example, from the first line from table 2, we could know that the entropy of ratings for spammers is significantly less than non-spammers, which indicates the spammers are tending to give similar rates. The features in bold typeface are the features that show low significance in the distinguishing spammers and non-spammers.
For the purpose of legal re-use, Sohmen et al. The majority of Hindawi articles is available under the Creative Commons Attribution License, so that they can be used for this type of research. Another advantage is that they are accessible in XML format, which makes them easier to read than files in PDF format. We crawled 288,057 image-text pairs from four different journals, namely Advances in Artificial Intelligence (AAI), Applied Computational Intelligence and Soft Computing (ACISC), Advances in Multimedia (AM) and Mathematical Problems in Engineering (MPE).
The full data set contains 703.9M words. They were used for training the target language model in MT, which was a 5-gram interpolated LM with punctuation and out-of-vocabulary word modelling, modified Kneser-Ney smoothing and was in standard ARPA format. The source language model for ASR was built on the full TED data set and 25% or 50% of the OOD data, making up to 322.2M words. A monolingual translation model was trained for punctuation insertion and case conversion. The training took the full TED data and 5-10% of the OOD data, resulting in a total of 37.6M words. The translation model was trained on the full TED data set and other optimally selected OOD data sets, where only around 5% of the sentences were selected. 2-gram and 4-gram ARPA language models were trained for lattice generation and expansion. The 4-gram LM was pruned with a threshold 10−10 and a weighted-finite-state transducer (WFST) was constructed for fast decoding in the pre-final passes in the ASR systems. All ASR LMs were based on a word-list with a 60k word vocabulary extracted based on our standard English ASR inventory and the English part of the TED MT training data for IWSLT 2014 Pilot ASR experiments on the IWSLT 2011 and 2012 eval data show the drop of perplexity with the addition of Common crawl and Gigaword data. For these two corpora, the rate of data selected for LM building was set to 50%, while the rate for other OOD corpora was kept 25%. In SLT, the input to the MT system was ASR output, which typically lacks casing and punctuation. The training data for this monolingual MT system was obtained by pre-processing an actual corpus of the source language to form pseudo ASR outputs, which contained no case and punctuation information. Numbers, symbols and acronyms were also converted to their verbal forms with lookup tables. We then used this synthesised corpus of pseudo ASR as the source, and the original corpus as the target of our monolingual MT. It performed monotonic translation with phrases of as long as 7 words.
All pre-final stage decoding made use of weighted finite state transducers (WFSTs) for fast implementation. WFST decoding with a pruned 4-gram grammar network was compared with the standard tree search with an unpruned 3-gram LM. The WER and real-time factor (RT) on IWSLT 2011 evaluation and IWSLT WFST was shown to achieve the same performance as tree-search decoding, with much faster decoding speed.
We evaluate the text inference capabilities of these models on the twelve datasets and report their accuracy (Acc.) and sample-averaged FLOPs under different Speed values. It can be observed that with the setting of Speed=0.1, FastBERT can speed up 2 to 5 times without losing accuracy for most datasets. If a little loss of accuracy is tolerated, FastBERT can be 7 to 11 times faster than BERT. Comparing to DistilBERT, FastBERT trades less accuracy to catch higher efficiency. ’s tradeoff in accuracy and efficiency. The speedup ratio of FastBERT are free to be adjusted between 1 and 12, while the loss of accuracy remains small, which is a very attractive feature in the industry.
In order to analyze and compare the effects of emotions on message propagation across industries, we computed multi-level models. There, messages are nested within industries. We compared two kinds of model types, all containing the same baseline fixed effects as m4 above and one sentiment. We tested both model types for all seven sentiments (polarity and six emotions) using likelihood ratio tests. All tests produced overwhelming evidence that random slope models were required. The table’s σ line describes the standard deviation of the random effects. The larger the value becomes, the more heterogeneous the function of this sentiment is across the surveyed industries.
For the scenario of not using BERT, the pipeline method using homogeneous POS tags is slightly yet consistently inferior to the basic model. The joint Stack-Hidden method using only homogeneous POS tags significantly outperforms the basic method by 0.2 (p<0.005), 0.4 (p<0.0005), and 0.5 (p<0.0005) in LAS on the three datasets respectively. Utilizing heterogeneous POS tags on Chinese further boosts parsing performance, leading to large overall improvements of 0.9 (p<0.0001) on both datasets.
As can be seen, the performance of our PMI-LM is competitive, slightly outperforming the NCE-LM on both test sets. To put these numbers in a broader context, we note that state-of-the-art results on these datasets are notably better. For example, on the small PTB test set, Zaremba et al. On the larger WMT dataset, Jozefowicz et al. They also achieved 23.7 with an ensemble method, which is the best result on this dataset to date. Yet, as intended, we argue that our experimental results affirm the claim that PMI-LM is a sound language model on par with NCE-LM.
To put the numbers in perspective, we also report the textual heterogeneity of the two standard summarization datasets DUC ’04 and TAC ’08A. These corpora were created during shared tasks and focused on multi-document news summarization. The heterogeneity in BBC and Guardian are similar and both much higher than DUC ’04 and TAC ’08A, meaning that our corpora contain more lexical variation inside topics.
Following Hao et al. , the models were trained on 800 random binary strings with length ∼N(10,2) and evaluated on strings with length ∼N(50,5). In contrast, Hao et al. In both cases, it seems that having 2Θ(n) state complexity enables better performance on this memory-demanding task. However, our seq2seq LSTMs appear to be biased against finding a strategy that generalizes to longer strings.
As shown, our model achieves the best performance in human evaluation and Dist on both datasets, especially the visible enhancement in Good ratio (0.24 vs. 0.15 on Weibo and 0.36 vs. 0.25 on Reddit) compared with the best baseline, indicating our model can generate more informative and diverse responses. Notably, Seq2Seq performs the best in BLEU but poor in human evaluation on Weibo, which further verifies the weak correlation of BLEU to human judgment.
We use a dataset of 37k Facebook videos with 80/10/10 train/validation/test splits. We use validation set to perform randomized serach of hyperparameters such as embedding dimensionalities, dropout rates and batch normalization use. As baselines, we use a simple mean of ResNet50 feature vectors as input to two layer neural network (video frames) and concatenation of last states of LSTM (headlines). We use Keras for implementation. Results. Tab. Interestingly, almost equal popularity prediction results can be obtained using either video frames or headline features. Combining both modalities leads to noticeable improvement, while adding attention mechanism improves the performance in the multimodal and visual case. For headlines, the performance with attention deteriorates slightly. We speculate that the bi-directional LSTM already learns internal dependencies between hidden states and adding attention cannot help further, while for video frames it enables the network to exploit the temporal dependencies between the frames.
The labeled emotion is calculated using the absolute majority of votes. Thus, if a specific emotion received three or more votes, then that utterance is labeled with that emotion. If there is no majority vote, the utterance is labeled with “non-neutral” label. In addition to the utterance, annotation, and label, each line in each dialogue includes the speaker’s name (in the case of EmotionPush, a speaker ID was used).
We see that for some emotions, such as disgust and fear (and anger for EmotionPush), the κ-statistic is poor, indicating ambiguity in annotation and thus an opportunity for future improvement. We also note that there is an interplay between the emotion label distribution, per-emotion classification performance, and their corresponding κ scores, which calls for further investigation.
As expected, number of edits, length of article, and number of editors significantly predict article quality. The coefficient for the —alignment— term suggests that quality decreases when editors are biased, on average, in either direction. Most critical is that polarization, the variance of political alignments, is positively and substantially associated with quality: a 1-unit increase in polarization multiplies the odds of moving from lower- to higher-quality categories by a factor of 18.57 for Political articles, 2.06 for Social Issues articles and 1.90 for Science articles. We measured the quality of Wikipedia articles algorithmically using a prominent approach that draws on features derived from article content alone and not information about editors or their collaboration patterns Warncke-Wang et al. Wikipedia editors have scored hundreds of articles on quality, but human-generated ratings for most of Wikipedia’s millions of articles do not exist and necessitate an algorithmic approach. Note that a few articles have no text (e.g., removed or redirected) and hence receive no quality ratings.
The first 11 rows report the performance of models that use the same experimental setup, without using additional training data or various features extracted from external knowledge base (KB) resources. The last 6 rows report results of models exploiting various kinds of features based on external relational KBs of chemicals and diseases, in which the last 4 SVM-based models are trained using both training and development sets. In addition, our models obtain 2+% higher F1 score than the traditional feature-based models MaxEnt Gu et al. We also achieve 2+% higher F1 score than the LSTM- and CNN-based methods Zhou et al. Fundel et al. Although we obtain better results, we believe dependency tree-based feature representations still have strong potential value. However, this dependency parser was trained on the Penn Treebank (in the newswire domain) Marcus et al. ; training on a domain-specific treebank such as CRAFT Bada et al.
The evaluation process was carried out using three well known automatic MT evaluation metrics: BLEU, METEOR and TER. We assume that the MT output of the test set provided by the WMT-2015 APE task as our base system translation output and we consider the corresponding PE version as a reference set for the evaluation. In all cases, our proposed system performed better. The similar results were also found with respect to PBSMT system with Berkeley word alignment (i.e., experiment 3). In experiment 4, we apply the PBSMT system with monolingual edit distance based word alignment such as METEOR that also fails to perform better than the baseline system. As PBSMT system fails to achieve our goal, we apply HPBSMT system with Hybrid Word alignment. The system reported in experiment 5, has successfully improved over the base system with respect to all automatic evaluation metrics.
We start by analyzing XNLI development results for Translate-Test. Recall that, in this approach, the test set is machine translated into English, but training is typically done on original English data. Our BT-ES and BT-FI variants close this gap by training on a machine translated English version of the training set generated through back-translation. Quite remarkably, MT-ES and MT-FI also outperform Orig by a substantial margin, and are only 0.8 points below their BT-ES and BT-FI counterparts. Recall that, for these two systems, training is done in machine translated Spanish or Finnish, while inference is done in machine translated English. This shows that the loss of performance when generalizing from original data to machine translated data is substantially larger than the loss of performance when generalizing from one language to another. We next analyze the results for the Zero-Shot approach. In this case, inference is done in the test set in each target language which, in the case of XNLI, was human translated from English. As such, different from the Translate-Test approach, neither training on original data (Orig) nor training on machine translated data (BT-XX and MT-XX) makes use of the exact same type of text that the system is exposed to at test time.
In contrast, the best results on the original English set are obtained by Orig, and neither BT-XX nor MT-XX obtain any clear improvement on the one in Spanish either. This confirms that the underlying phenomenon is limited to translated test sets. In addition, it is worth mentioning that the results for the machine translated test set in English are slightly better than those for the human translated one, which suggests that the difficulty of the task does not only depend on the translation quality. Finally, it is also interesting that MT-ES is only marginally better than MT-FI in both Spanish test sets, even if it corresponds to the Translate-Train approach, whereas MT-FI needs to Zero-Shot transfer from Finnish into Spanish. This reinforces the idea that it is training on translated data rather than training on the target language that is key in Translate-Train.
In particular, our results show that BT-FI and MT-FI are less reliant on lexical overlap and the presence of negative words. This feels intuitive, as translating the premise and hypothesis independently—as BT-FI and MT-FI do—is likely to reduce the lexical overlap between them. This would explain why the resulting models have a different behavior on different stress tests.
The average polysemy of words in the gathered dataset was 15.50 senses per word as compared to 2.34 in the induced sense inventory. This huge discrepancy in granularities lead to the fact that some test sentences cannot be correctly predicted by definition: some (mostly rare) BabelNet senses simply have no corresponding sense in the induced inventory. To eliminate the influence of this idiosyncrasy, we kept only sentences that contain at least one common hypernym with all hypernyms of all induced senses.
In the case of the traditional “per word” inventories, the model based on the context features outperform the models based on cluster words. While sense representations based on the clusters of semantically related words contain highly accurate features, such representations are sparse as one sense contains at most 200 features. As the result, often the model based on the cluster words contain no common features with the features extracted from the input context. The sense representations based on the aggregated context clues are much less sparse, which explains their superior performance.
In the table, AHN is our original model. In (a), the item’s attention modules are replaced by average-pooling. In (b), the user co-attention modules are replaced by attention modules similar to the item ones and this thus constitutes a symmetric model. In (c), we remove the row-wise multiplication between the affinity matrix and the attention weights in Eqs. In (d), the parameterized factorization machine is replaced by a dot product. In (e), the gating mechanisms in Eqs. Comparing with (a), we can observe the importance of considering attention weights on the sentences and reviews of each item. The degraded MSEs of (b) suggest that our asymmetric design in the model architecture is essential. The results of (c) validate our design of the attention-adapted affinity matrix in Eqs. The substantial MSE drops for (d) establish the superiority of using FM as the prediction layer. The comparison between (e) and AHN suggests the effectiveness of the gating mechanisms. Thus, the results of the ablation study validate the design choices of our model architecture.
Document and passage based RF term selection is used, to explore the effect of noise on terms, and document based term selection proved marginally superior. Choosing RF terms from a small set of documents (r=5) was found to be marginally better than choosing from a larger set (r=50).
Note that each records in the arXiv data set may have more that one label. The histogram of number of labels for each record is shown in Fig. As shown in the histogram, the majority of records in arXiv data set are tagged with only a single label.
Results on NYT. However, our ASGARD-seg’s ROUGE-L score is comparable to BART. This indicates the effectiveness of our graph-augmented summarization framework.
Results on CNN/DM. Noticeably, ASGARD-doc trained with the combined ROUGE and cloze reward produces better ROUGE scores than BERTSumExtAbs and UniLM, which are carefully fine-tuned from large pretrained language models, and the numbers are also comparable to the fine-tuned BART.
The results show that even though the speaker f2b constitutes the largest speaker subset leaving the least amount of data for training, the model does not perform much worse than on data from other speakers. Overall, there does not appear to be a distinctively “easy” or “difficult” speaker.
In our experiments we observe a large drop in performance after z-scoring the features, both for the speaker-dependent and the speaker-independent case. This may be due to the fact that the CNN looks for relative patterns in the data independent of their absolute position and values; and prosodic events are characterized by relative changes in speech. Normalizing the values may lead to a loss of fine differences in the data since the range of the values is decreased by z-scoring. The CNN performance in our experiments, however, appears to benefit from the original differences.
These results are mirrored by those obtained for the rest of the languages and datasets. Overall, it confirms that our cluster-based models obtain state of the art results using just one half of the data. Furthermore, using just one quarter of the training data we are able to match results of other publicly available systems for every language, outperforming in some cases, such as Basque, much complex systems of classifiers exploiting linguistic specific rules and features (POS tags, lemmas, semantic information from WordNet, etc.). Considering that Basque is a low-resourced language, it is particularly relevant to be able to reduce as much as possible the amount of gold supervised data required to develop a competitive NERC system.
4.3.2 Text Genre In this setting the out-of-domain character is given by the differences in text genre between the English CoNLL 2003 set and the Wikigold corpus. They report results on Wikigold showing that they outperformed their own CoNLL 2003 gold-standard model by 10 points in F1 score. While the results of our baseline model confirms theirs, our clustering model score is slightly higher. This result is interesting because it is arguably more simple to induce the clusters we use to train ixa-pipe-nerc rather than create the silver standard training set from Wikipedia as described in Nothman et al.
As an additional experiment, we also tested the English model recommended by Stanford NER which is trained for three classes (LOC, PER, ORG) using a variety of public and (not identified) The results with respect to their CoNLL model improved by around 3 points in F1 score across named entity labels and evaluation types (phrase or token based). In view of these results, we experimented with multi-corpora training data added to our best CoNLL 2003 model (en-91-18). Thus, we trained using three public training sets: MUC 7, CoNLL 2003 and Ontonotes 4.0. The local model with the three training sets (Local ALL) improved 12 and 17 points in F1 score across evaluations and entity types, outperforming our best model trained only with CoNLL 2003. Adding the clustering features gained between 2 and 5 points more surpassing the Stanford NER 3 class multi-corpora model in every evaluation. We believe that the main reason to explain these improvements is the variety and quantity of annotations provided by Ontonotes (1M word corpus), and to a lesser extent by MUC 7, which includes some spans containing common nouns and determiners making the model slightly more robust regarding the mention spans.
Throughout our experiments, we tried four different settings for the TD-filterbank layers: [leftmargin=*] Fixed : Initialize the layers to match MFSC and keep their parameters fixed when training the model Learn-all : Initialize the layers and let the filterbank and the averaging be learned jointly with the model Learn-filterbank: Start from the initialization and only learn the filterbank with the model, keeping the averaging fixed to a squared hanning window Randinit: shows comparative performance of an identical architecture trained on the four types of TD-filterbanks. We can observe that training on fixed layers moderately worsens the performance, we hypothesize that this is due to the absence of mean-variance normalization on top of TD-filterbanks as is performed on MFSC. A striking observation is that a model trained on TD-filterbanks initialized randomly performs considerably worse than all other models. This shows the importance of the initialization. Finally, we observe better results when learning the filterbank only compared to learning the filterbank and the averaging but depending on the architecture it was not clear which one performs better. Moreover, when learning both complex filters and averaging, we observe that the learned averaging filters are almost identical to their initialization. Thus, in the following experiments, we choose to use the Learn-filterbank mode for the TD-filterbanks.
As expected, it is easier to identify posts that will receive 10 or more hostile comments than those that will receive only 5 or more. As in Task 1, the previous post features lead to improved AUC (.842 vs .808), but the previous comment features do not (.786 vs. .808). These results suggest that it is possible to distinguish between posts that will receive an isolated hostile comment and those that will receive high volumes of hostile comments. Thus, interventions may be prioritized by the expected intensity of hostile interactions.
With the help of data augmentation, EMT boosts the performance slightly on the end-to-end task, especially for the question generation task which originally has only 6804 training examples. The augmented training instances boosts the performance even though the augmentation method does not produce any new question. This implies that the size of the ShARC dataset is a bottleneck for an effective end-to-end neural models. Without the coarse-to-fine reasoning for span extraction, EMT (w/o c2f) drops by 1.53 on BLEU4, which implies that it is necessary for the question generation task. The reason is that, as a classification task, entailment state prediction can be trained reasonably well (80% macro accuracy) with a limited amount of data (6804 training examples). On the other hand, one-step span extraction method does not utilize the entailment states of the rule sentences from EMT, meaning it does not learn to extract the underspecified part of the rule text. With the guidance of explicit entailment supervision, EMT outperforms EMT (w/o Lentail) by a large margin. Intuitively, knowing the entailment states of the rule sentences makes the decision making process easier for complex tasks that require logical reasoning on conjunctions of conditions or disjunctions of conditions. It also helps span extraction through the coarse-to-fine approach. Although there exist interactions between rule sentences and user information in BERT-encoded representations through multi-head self-attentions, it is not adequate to learn whether conditions listed in the rule text have already been satisfied or not.
We define 5 methods for the experiment. One is a method of Synset Elasticsearch, denoted here by Synset which will be the baseline of benchmark. The other 4 methods are variations of our proposed pipeline but with variant values of the design parameter a=[1,2,3,4]. The pipeline methods are then denoted respectively with the value of a as Fusion1, Fusion2, Fusion3 and Fusion4. a=2 based on Precision, F1 measure, Jaccard index and Hamming loss. This means that if we increase the size of the fusion ranked list more than the double of the size of the Synset method, we will start loosing accuracy. This difference is a negative effect that should be minimized, otherwise, the model will tend to predict too much labels that would be more probably irrelevant to the article.
Our improvement can be due to that: i) our linguistic units cover more exhaustive phrases, it enables alignments in a wider range; ii) we have two max-pooling steps in our attention pooling, especially the second one is able to remove some noisily aligned phrases. Both ABCNN and AP-CNN are based on convolutional layers, the phrase detection is constrained by filter sizes. Even though ABCNN tries a second CNN layer to detect bigger-granular phrases, their phrases in different CNN layers cannot be aligned directly as they are in different spaces. GRU in this work uses the same weights to learn representations of arbitrary-granular phrases, hence, all phrases can share the representations in the same space and can be compared directly.
(see Section 6 for more details). The first model is trained using the original NLmaps training data. The second receives an additional 15,000 instances from the synthetic data. Both systems are tested on the original NLmaps test data and on the new test set of NLmaps v2 which consists of a random set of 2,000 pairs from the remaining data. On the original test set, adding the 15,000 synthetic instances allows the parser to significantly improve by 2.09 in F1 score. The parser trained on the original training data performs badly on the new test set because it is ignorant of many OSM tags that were introduced with the extension.
This is the largest compositionality-annotated dataset we could find. In this dataset, each phrase has four binary compositionality human expert assessments. We report Spearman’s ρ correlation between our method’s decisions on compositionality and the average of the four human annotations of compositionality of that dataset Among these distance and correlation metrics, the Chebyshev and Hausdorff distances score the lowest, but still outperform the baselines. The Chebyshev and Hausdorff distances emphasise the maximal absolute difference in rank and maximal difference in TF-IDF score respectively. It thus appears that this emphasis on maximal differences is not optimal for this setup. One reason could be that we have trimmed our ranked lists to the top 1000 highest TF-IDF scores, and that among those highest TF-IDF scores, maximal differences may not be as noticeable, as on a much bigger range of TF-IDF scores that represent most levels of term informativeness (as opposed to just the most informative terms). We see that this is indeed the case approximately for Rank CosRank, Hamming distance and for Pearson and AP correlation. However, for Chebyshev and Hausdorff, we see no such linear trend: the points are generally scattered, and seem to have more of an approximately ascending (as opposed to the expected descending) trend as the y axis increases. As discussed above, a reason why these two metrics underperform could be their emphasis on maximum score difference between the two lists (which is largely reduced when we trim lists to the top 1000).
It is apparent that most of the scores (88.9%) are over 0.4, indicating a moderate agreement. This suggests that the task was reasonable and well understood by the annotators.
As defined, the above measures have assumed discrete rationales ri. We would also like to evaluate the faithfulness of continuous importance scores assigned to tokens by models. Here we adopt a simple approach for this. We convert soft scores over features si provided by a model into discrete rationales ri by taking the top−kd values, where kd is a threshold for dataset d. Intuitively, this says: How much does the model prediction change if we remove a number of tokens equal to what humans use (on average for this dataset) in order of the importance scores assigned to these by the model. For these models we again measure downstream (task) performance (macro F1 or accuracy). Here the models are actually the same, and so downstream performance is equivalent. To assess the quality of token scores with respect to human annotations, we report the Area Under the Precision Recall Curve (AUPRC).
For our models, we only display results with latent strength initialization based on frequency per topic, which achieves the best performance. Results for different initialization methods are exhibited and discussed later in this section. (bootstrap resampling test, p<0.05). Our latent variable models also obtain better accuracies than SVMs trained on the same linguistic feature sets. Without the audience feedback features, our model yields an accuracy of 72.0%, while SVM produces 65.3%. This is because our model can predict topic strength out of sample by learning the interaction between observed linguistic features and unobserved latent strengths. During test time, it infers the latent strengths of entirely new topics based on observable linguistic features, and thereby predict debate outcomes more accurately than using the directly observable features alone. Using the data in \newcitezhangetal:NAACL2016 (a subset of ours), our best model obtains an accuracy of 73% compared to 65% based on leave-one-out setup.
As mentioned above, we experimented with a variety of latent topic strength initializations: argument frequency per topic (Freq); all topics strong for both sides (AllStrong); strong just for winners (AllStrongwin); and Random initialization. Furthermore, the strength constraints make little difference, though their effects slightly vary with different initializations. Most importantly, C3 (the constraint that topics cannot be strong for both sides) does not systematically help, suggesting that in many cases topics may indeed be strong for both sides, as discussed below. This indicates that, in general, the model was improved by allowing some topics to be strong for both sides. Interestingly, while the majority (53%) of topics are strong for one side and weak for the other, about a third (31%) of topics are inferred as strong for both sides. While it is clear what it means for a topic to be strong for one side and not the other (as in our death penalty example), or weak for both sides (as in a digression off of the general debate topic), the importance of both-strong for prediction is a somewhat surprising result. What this shows is that even on a given topic within a debate (Syrian refugees: resettlement), there are different subtopics that may be selectively deployed (resettlement success; resettlement cost) that make the general topic strong for both sides in different ways.
Particularly, the 7.2% WER on SWB and 12.7% WER on CH given by the baseline LSTM after the sequence training so far is our best single system performance without rescoring using more advanced LMs From the table, the embedding-based SAT yields 0.6% absolute improvement on SWB and 0.5% on CH after CE training. After the sequence training, it gives 0.1% and 0.2% absolute improvement on SWB and CH, respectively, over our best single system.
Aside from the evident success of lexical features, it is debatable how well the knowledge that is mainly captured by the lexical information of the training data can be generalized to other domains. As reported by \newciteghaddar16b, state-of-the-art coreference resolvers trained on the CoNLL dataset perform poorly, i.e. worse than the rule-based system Lee et al. cort is the mention-ranking model of \newcitemartschat15c. cort uses the following set of features: the head, first, last, preceding and following words of a mention, the ancestry, length, gender, number, type, semantic class, dependency relation and dependency governor of a mention, the named entity type of the head word, distance of two mentions, same speaker, whether the anaphor and antecedent are nested, and a set of string match features.
To test the algorithm, we, first, collected 2484 puns from different Internet resources and, second, built a corpus of 2484 random sentences of length 5 to 25 words from different NLTK corpora Bird et al. We shuffled and split the sentences into two equal groups, the first two forming a training set, and the other two a test set. The classification was conducted, using different Scikit-learn Pedregosa et al. We also singled out 191 homographic puns, and 198 heterographic puns, and tested them against the same number of random sentences. In addition, its results are smoother, comparing the difference between precision, and recall (which leads to the highest F-measure scores) within the two classes (puns, and random sentences), and between the classes (average scores). The results were higher for the split selection, reaching 0.79 (homographic), and 0.78 (heterographic) scores of F-measure. The common selection got the maximum of 0.7 for average F-measure in several tests. The higher results of split selection may be due to a larger training set.
the corresponding AUC scores achieved by each method on each dataset. Overall, the EGL-Entropy-Beta outperforms other methods, demonstrating the value of explicitly selecting examples likely to improve representation level parameters.
For n-grams, the relationship to convincingness may be topic-specific, hence they are not identified as important when the model is trained on 31 different topics. The shortest length-scales are for sentiment features, pointing to a possible link between argumentation quality and sentiment. However, “VeryPositive” was the feature with the largest length-scale, either because the median was a poor heuristic in this case or because the feature was uninformative, perhaps because sarcastic statements can be confused with highly positive sentiment. The short length-scale for the “words >6 letters” ratio suggest that some surface features may be informative, despite previous work In this case, longer words may relate to more sophisticated and convincing arguments.
We can see that on both datasets, residual EBMs with causal attention joint UniT outperforms the baseline RALM with approximately the same number of parameters. The non-residual baseline BALM performs similarly to joint UniT, which might be due to the limitation that PLM is not trained jointly with the residual model in both joint UniT and RALM. However, by using our EBM approach, we can remove the causal attention mask and use bi-directional models, which achieves better performance than baselines and joint UniT: without external data, joint BiT-Base reaches a higher performance than joint UniT with fewer parameters. By initializing from the state-of-the-art pretrained bi-directional transformers RoBERTa-Base and RoBERTa-Large, joint BiT-Base* and Joint BiT-Large* reach even better performance than joint BiT-Base.
Comparison with adversarial loss In Shetty et al. Following Shetty et al. For fair comparison, we use a similar non-attention LSTM model. When temperature is 0.8, the output generations outperforms the adversarial method (Adv). While Shetty et al. (our model with T=0.33 is performing similar to “Base”).
ATTN < ATTN-L < Trans (We include both XE and RL-trained models in the table, but for diversity evaluation we only show results on XE-trained models). AllSPICE performance on different models has the same order as single caption generation performance.
The points that denote the (¯ℓi,¯Ci) coordinates for the particular items in Fig. This means that the item positions on the scatter plots are strongly influenced by these items’ frequencies, while the actual grammar- and context-related contributions to ¯Ci and ¯ℓi are less evident. Therefore, we decided to remove the frequency-based contributions by dividing the empirical values by their average random-model counterparts: ¯CRi and ¯ℓRi. The resulting positions of the items are shown in Fig. Now it is more evident than in Fig. In this figure, we also show these quantities calculated for three sample words chosen randomly from more distant parts of the Zipf plot: time (R=75 in the English corpus), face (R=130), and home (R=264), as well as their semantical counterparts in the other languages (occupying different ranks, see Tab. Obviously, each of these words may also have other, non-equivalent meanings in distinct languages and, while some languages use inflection, the other ones do not, which inevitably contribute to the rank differences. In contrast to the most frequent words discussed before, these words are significantly less frequent, which can itself lead to some differences in the statistical properties as compared to the top-ranked words. Therefore, they are not shown in Fig. In Fig. This effect is the most pronounced for French, then for English, Russian, Italian, and Polish, while it is absent for German. This visible shift may originate from either the statistical fluctuations among the words, the statistical fluctuations among the texts selected for the corpora, or be a geniune effect for the less frequent words, the parts of speech, and/or a general property of the lexical words in specific languages. However, since our sample of the medium-ranked words is small, at present we prefer not to infer any decisive conclusions from this result as we plan to carry out a related, comprehensive study in near future. Nevertheless, we stress here that such displacements exhibited in Fig. by the words of medium frequency by no means contradict our main statement that the punctuation marks show similar statistical properties as the most frequent words.
Results: Generally, models do not perform well on \name-5K compared to other datasets. All models result in lower BLEU score, lower METEOR score, and high TER for \name-5K and \name-78K. Given the same experimental settings, the performance of the models highly relates to the volume of training sets. An interesting observation is that models perform better on Quora-50K than \name-78K, albeit Quora-50K is larger in scale than \name-78K. That is probably because the number of unique sentences in \name-78K is six times smaller than those in Quora-50K.
For tasks with a single input sentence, we analyze how much attention is given to words in the sentence that are important for the prediction. Specifically, we select a minimum subset of words in the input sentence with which the model can accurately make predictions. We then compute the total attention that is paid to these words. These set of words, also known as rationales, are obtained from an extractive rationale generator Lei et al. Sutton et al. R=pmodel(y|Z)−α||Z|| where y is the ground truth class , Z is the extracted rationale, ||Z|| represents the length of the rationale, pmodel(.) represents the classification model’s output probability, α is a hyperparameter that penalizes long rationales. With a fixed α, we trained generators to extract rationales from the vanilla and Diversity LSTM models. We observed that the accuracy of predictions made from the extracted rationales was within 5% of the accuracy made from the entire sentences. In general, we observe that the Diversity LSTM model provides much higher attention to rationales which are even often shorter than the vanilla LSTM model’s rationales. On average, the Diversity LSTM model provides 53.52 % (relative) more attention to rationales than the vanilla LSTM across the 8 Text classification datasets. Thus, the attention weights in the Diversity LSTM are able to better indicate words that are important for making predictions.
, we can see that the proposed SAN framework performs the best on F1-score. Although CRF is a non-deep learning model, its precision is not bad since we use dependency relations as features. However, the recall of CRF is very low since it can only train weights on words appear in the training data. All deep learning models have better recalls than CRF. S-BLSTM has the best precision as it is trained using only the training data. However, its recall is relatively low. It still suffers the problem that training data can not further tune embeddings of words not appeared in the training data. SAN (-) BLSTM2 shows that the additional BLSTM layer is effective in learning better representations. Lastly, SAN significantly improves the recall by further adjusting the weights for different unlabeled questions. It only loses 0.5% on precision compared that with S-BLSTM.
When training only on the language model training corpus, a perplexity of 532.71 was achieved on the transcriptions of the acoustic test set, using Kneser-Ney discounting. We found that this discounting method was not suitable when incorporating the artificially-generated data, however. In this case, Witten-Bell discounting was chosen, leading to a perplexity of 269.80.
it is immediate that agreement is substantial, and when there is disagreement it is largely for the difficult cases of inconclusive origin and the multi-language exceptions: Q, X, V, and (95% CI = 0.5192 to 0.6662). If we restrict attention to only the instances where neither of the annotators marked an inconclusive origin or multi-language exception, then Kappa is 0.9216, generally considered high agreement. This shows that our annotation system is feasible for use and also shows that to improve the system we might focus efforts on finding ways to increase agreement on the annotation of the exceptional cases (Q, X, V, and N).
The main focus of this work is to see if a resource constrained language can benefit from a resource rich language. However, before reporting results in this setup, we would like to check how well our model performs for monolingual NER (i.e., training and testing in the same language). We observe that our model gives state of the art results for Dutch and English and comparable results in Spanish.
Our approach, REGS, and StepGAN significantly outperform HRED in Dist-1 and Dist-2. This indicates that adversarial training helps improve the diversity, which is in line with the observation in previous work Li et al. There is no improvement in DPGAN compared with HRED in our experiments. We believe this is because the scale of the DailyDialog dataset is not large enough for sufficiently training the language model based discriminator in DPGAN. By contrast, our approach outperforms other adversarial training approaches in both Dist-1 and Dist-2, indicating that our counterfactual off-policy method is more effective in improving diversity. Our approach has the highest scores from BLEU-1 to BLEU-4, which demonstrates its effectiveness in generating relevant responses.
In summary, these results suggest that our generic systems can compete with task specific systems and in some cases even outperform them, possibly due to better generalization from larger amount of training data.
We collect 5392 games of human teams against our bots. Qualitatively, we observe a wide variety of different strategies. An average game contains 14 natural language instructions and lasts for 16 minutes. The dataset contains over 76 thousand instructions, most of which are unique, and their executions. The diversity of instructions shows the wide range of useful strategies. The instructions contain a number of challenging linguistic phenomena, particularly in terms of reference to locations and units in the game, which often requires pragmatic inference. Instruction execution is typically highly dependent on context. Our dataset is publicly available. For more details, refer to the Appendix.
In addition, we calculate the ensemble of 6 models, where the average probability assigned by each of the models is used to determine the probability of the next word at test time. This system achieved a manual evaluation score of 47.50, which was slightly higher than other systems participating in the task.
Efficiency: We set ngram order as 7 for both CNN and our encoder. BiLSTM is the slowest model, because it has to scan over the entire text sequentially. LeftForest is almost 2x faster than CNN, because LeftForest reuses lower-order ngrams while computing higher-order ngrams. This result reveals that our encoder is more efficient than baselines. Model size: LeftForest contains much less parameters than CNN, and it gives a better accuracy than BiLSTM with only a small amount of extra parameters. Therefore, our encoder is more compact.
We get several interesting observations: (1) Our multi-granular text encoder outperforms both the CNN and BiLSTM baselines regardless of the structure used; (2) The LeftForest and RightForest encoders work better than the Tree encoder, which shows that representing texts with more ngrams is helpful than just using the non-overlapping phrases from a parse tree; (3) The LeftForest and RightForest encoders give better performance than the Pyramid encoder, which verifies the advantages of organizing ngrams with forest structures; (4) There is no significant difference between the LeftForest encoder and the RightForest encoder. However, by combining them, the BiForest encoder gets the best performance among all models, indicating that the LeftForest encoder and the RightForest encoder complement each other for better accuracy.
The best accuracy obtained on the test set is 88%. The table also presents the confidence intervals, which are rather wide due to the small size of the test set. The best accuracy score obtained on the training set using 10-fold cross validation is 94%. The table also presents the reported performances of several systems. Rather surprisingly, early statistical approaches presented very good results using relatively small data sets. However, newer approaches have reported performances much lower than ours. Because all these systems were evaluated on distinct data sets, and because none of these systems are publicly available, it is not possible to analyze the reasons behind the wide range of performances reported. We also performed single feature set experiments and leave-out feature set experiments. For these, we considered each of the broad categories of features. The leave-out experiment results further verify this finding, and also reveal that lexical features help to boost performance.
N=72846 words appearing at least 5 times in the training corpus). One notices that adding the direct word context to target word connections (using the additional matrix described in section 2), enables to jump from a poor performance of about 30% accuracy to about 40% test accuracy, essentially matching the 39% accuracy reported for Good-Turing n-gram language models in \newcitezweig2012computational.
To better quantify the contribution of different attention mechanisms on each slot type, we further compared the performances on each single slot type. We can see that both attentions yield improvement for most slot types. Impact of Training Data Size: We examine the impact of the size of training data on the performance for each slot type. We can see that, for some slot types, such as per:date_of_birth and per:age, the entity types of their candidate fillers are easy to learn and differentiate from other slot types, and their indicative words are usually explicit, thus our approach can get high f-score with limited training data (less than 507 instances). In contrast, for some slots, such as org:location_of_headquarters, their clues are implicit and the entity types of candidate fillers are difficult to be inferred. Although the size of training data is larger (more than 1,433 instances), the f-score remains quite low. One possible solution is to incorporate fine-grained entity types from existing tools into the neural architecture. Impact of Wide Context Distribution: We further compared the performance and distribution of instances with wide contexts across all slot types. A context is considered as wide if the query and candidate filler are separated with more than 7 words. We can see that, for most slot types with wide contexts, such as per:states_of_residence and per:employee_of, the f-scores are improved significantly while for some slots such as per:date_of_birth, the f-scores decrease because most date phrases do not exist in our pre-trained embedding model.
To answer Q1, we train a model using only ArgRewrite data. However, for our long-term goal of building an effective revision assistant tool, intuitively we will also need to identify ‘NotBetter’ revisions with higher recall, which is very low for this model. To answer Q3, during each run of cross-validation training we inject the AESW data in addition to the 90% ArgRewrite data, then test on the remaining 10% as before. It also has improved Recall for ‘NotBetter’ revisions compared to training only on ArgRewrite data. This result indicates that selective extraction of revisions from AESW data helps improve model performance, especially when classifying low-quality revisions.
Computational cost The Hellinger PCA is very fast to compute. For this benchmark we used Intel i7 3770K 3.5GHz CPUs. As the computation of the covariance matrix is highly parallelizable, we report results with 1, 100 and 500 CPUs. The Eigendecomposition of the C matrix has been computed with the SSYEVR LAPACK subroutine on one CPU. We compare completion times for 1,000 and 10,000 eigenvectors. Finally, we report completion times to generate the emdeddings by linear projection using 50, 100 and 200 eigenvectors. Although the linear projection is already quite fast on only one CPU, this operation can also be computed in parallel. Those results show that the Hellinger PCA can generate about 200,000 embeddings in about three minutes with a cluster of 100 CPUs.
The number of significantly correlated terms from both Yahoo! Note that the number of unique (frequent) words in Twitter (17k) is almost twice as in Yahoo! Answers (8k). The first column shows a demographic attribute and the second column indicates the source, i.e. Yahoo! Answers (Y!A for short) or Twitter. The third column (“All”) shows the total number of terms that have a significant correlation with each attribute (p-value <0.01). The following columns show the number of terms that have a significant correlation with the attribute with a ρ in the given ranges. The last column shows the number of terms that are significantly correlated with the attribute with a negative ρ. The data source that has the highest number of correlated terms with each attribute is highlighted in bold. We can see that on average, performances of Yahoo! Answers and Twitter are very similarly with Yahoo! Answers having a slightly higher performance (4%). Twitter data can predict the majority of the religion-related attributes with a higher correlation coefficient with the exception of population of Jewish%. On the other hand, Yahoo! Answers is superior to Twitter when predicting ethnicity related attributes such as population of White% and Black%. We also observe that IMD and Price can be predicted with a high correlation coefficient using both Yahoo! Answers and Twitter. This can be due to the fact that there are many words in our dataset that can be related to the deprivation of a neighbourhood or to how expensive a neighbourhood is. Answers and Twitter with these attributes are very high. On the other hand, terms that describe a religion or an ethnicity are more specific and lower in frequency. Therefore attributes that are related to religion or ethnicity are predicted with a lower accuracy.
We trained DAM, ESIM and cBiLSTM on the SNLI corpus using the hyperparameters provided in the respective papers. The results provided by such models on the SNLI and MultiNLI validation and In the case of MultiNLI, the validation set was obtained by removing 10,000 instances from the training set (originally composed by 392,702 instances), and the test set consists in the matched validation set. After training, each model was then fine-tuned for 10 epochs, by minimising the adversarially regularised loss function introduced in
We can see that the proposed adversarial training method significantly increases the accuracy on the adversarial test sets. For instance, consider A100DAM: prior to regularising (λ=0), DAM achieves a very low accuracy on this dataset – i.e. 47.4%. By increasing the regularisation parameter λ∈{10−4,10−3,10−2,10−1}, we noticed sensible accuracy increases, yielding relative accuracy improvements up to 75.8% in the case of DAM, and 79.6% in the case of cBiLSTM.
We present some baseline experiments in order to obtain reference values for our approach. We generated 250 words summaries for both TAC 2009 and DUC 2007 datasets. For both experiments, we used the cosine and the Euclidean distance as evaluation metrics, since the first is the usual metric for computing textual similarity, but the second is the one that relates to the Secured Binary Embeddings technique.
We then measure the accuracy (in terms of words) of our generation models in two phases. “DP Detection” shows the performance of our sequence-labelling model based on RNN. We only consider the tag for each word (pro-drop or not pro-drop before the current word), without considering the exact pronoun for DPs. “DP Prediction” shows the performance of the MLP classifier in determining the exact DP based on detection. Thus we consider both the detected and predicted pronouns. The F1 score of “DP Detection” achieves 88% and 86% on the Dev and Test set, respectively. However, it has lower F1 scores of 66% and 65% for the final pronoun generation (“DP Prediction”) on the development and test data, respectively. This indicates that predicting the exact DP in Chinese is a really difficult task. Even though the DP prediction is not highly accurate, we still hypothesize that the DP generation models are reliable enough to be used for end-to-end machine translation. Note that we only show the results of 1-best DP generation here, but in the translation task, we use N-best generation candidates to recall more DPs.
We trained multiple variants to study the effectiveness of each: only sparse features, i.e. one-hot encodings at the token level and multi-hot encodings of character n-grams (n≤5), and combinations of those together with ConveRT, BERT, or GloVe. Additionally, we trained each combination with and without the mask loss. Adding a mask loss improves performance by around 1% absolute on both intents and entities. DIET with GloVe embeddings is also equally competitive and is further enhanced on both intents and entities when used in combination with sparse features and mask loss. Interestingly, using contextual BERT embeddings as dense features performs worse than GloVe. We hypothesize that this is because BERT is pre-trained primarily on prose and hence requires fine-tuning before being transferred to a dialogue task. The performance of DIET with ConveRT embeddings supports this, since ConveRT was trained specifically on conversational data. ConveRT embeddings with the addition of sparse features achieves the best F1 score on intent classification and it outperforms the state of the art on both intent classification and entity recognition by a considerable margin of around 3% absolute. Adding a mask loss seems to slightly hurt the performance when used with BERT and ConveRT as dense features.
We perform an ablation study to explore what each component of the doc-reranker contributes to the overall performance. No gains are observed if the language model is combined with the proposal model (a probabilistically unsound combination, although one that often worked in pre-neural approaches to statistical translation). We find that as we increase the weight of the language model, the results become worse. The interpolation of the proposal model and channel model slightly outperforms the proposal model baseline but considerably underperforms the interpolation of the proposal model, channel model, and the language model. This difference indicates the key roles that the language model plays in the doc-reranker. When the channel model is combined with the language model the performance of the doc-reranker is comparable to that with all three components included. We conclude from the ablation study that both the channel and language models are indispensable for the doc-reranker, indicating that Bayes’ rule provides reliable estimates of translation probabilities.
We find that the results follow the same pattern as those on NIST: a better language model leads to better translation results and overall the reranker outperforms the transformer-big by approximately 2.5 BLEU. Here we mainly compare our results with those from Xia et al. Xia et al. Although our best results are lower than this, it is notable that our model achieves comparable results to their model trained on 56M sentences of parallel data, over two times more training data than we use. However, our method is orthogonal to these works and can be combined with other techniques to make further improvement.
We train hierarchical LSTMs with hidden sizes of {1000, 1500} using different dropouts {0.2,0.3,0.4,0.5}. Then, we train the UTRS using the best hyper-parameters found by the GP. It has a hidden size of 488 with a single hop and ten attention multi-heads. Noam Vaswani et al.
We report results in both directions, i.e., EN-DE and DE-EN. Between BAE-tr and BAE -cr, we observe that BAE-tr provides better performance and is comparable to the embeddings learned by the neural network language model of Klementiev et al. which, unlike BAE-tr, relies on word-level alignments. We also observe that the use of the correlation regularization is very beneficial. Indeed, it is able to improve the performance of BAE-cr and make it the best performing method, with more than 10% in accuracy over other methods for the EN to DE task.
The excellent performance of BAE-cr/corr suggests that merging mini-batches into single bags-of-words does not significantly impact the quality of the word embeddings. In other words, not only we do not need to rely on word-level alignments, but exact sentence-level alignment is also not essential to reach good performances. It is thus natural to ask the effect of using even coarser level alignments. We check this by varying the size of the merged mini-batches from 5, 25 to 50, for both BAE-cr/corr and BAE-tr.
In our experiments, AdvT-Text achieved the highest F0.5. Our experiments revealed that AdvT-Text can further outperform the current state-of-the-art methods. Moreover, our methods successfully reached performances that almost matched AdvT-Text and VAT-Text. Again, we emphasize that these results are substantially positive for our methods since they did not degrade the performance even when we added a strong restriction for calculating the perturbations.
On the imdb dataset we achieve an accuracy (on a binary sentiment classification task) of 89.6 percent, a bit lower than the state of the art on the same dataset. However, the reason this algorithm is used is its ability to highlight the parts of the sentence that contribute most towards the sentiment, which only the bmLSTM algorithm does. Still, the performance of the classifier is satisfactory enough for being deployed into the full model.
After being decoded, 50.8 percent of the phrases are classified as a different sentiment from the one they originally were classified. The number reported is the ratio of sentences that got assigned a different sentiment by the sentiment classifier after the transformation. Furthermore, some of the phrases in the extracted set had a length of only one or two words for which it is hard to predict the sentiment. These short sequences were included because in the final model they would also be extracted, so they do have an impact on the performance. The model was also tested while leaving out the shorter phrases, both on phrases longer than two and longer than five words, which slightly increases the success rate.
The results show that our proposed integer quantization has negligible accuracy loss on Voice Search. Longer utterances (in YouTube), which are typically more challenging for sparse models, still see comparable results from sparse models as from float models.
We compare our model with previously existing dependency-based and sequence-based neural models that do not introduce external features. That’s saying: our model achieves a new state-of-the-art performance on TACRED. Moreover, if we only consider the base pre-trained models, our model outperforms other BERT-related models by at least 2.1 F1.
We observe that DG-SpanBERT outperforms other BERT-related models by at least 3.1 F1, and other dependency-based models by at least 8.3 F1. This significant improvement confirms the effectiveness of our DG-SpanBERT model.
We report results for both t=0.8 and t=0.9 to give a more comprehensive picture of QC model behavior. Results show that competitive QE models re-purposed for classification can attain relatively high (50-60%) recall at 90% precision on WMT17 De-En dataset. For an MT system producing 75-80% correct translations, this would allow labeling 35-50% of the output sentences as adequate with a small 3-5% false-positive rate, a significant reduction in post-editing work indeed.
Parsing accuracy of 9 languages (LAS). Black rows use continuous tags; gray rows use discrete tags (which does worse). In each column, the best score for each color is boldfaced, along with all results of that color that are not significantly worse (paired permutation test, p<0.05). It includes parsing performance (measured by LAS and UAS) using ELMo layer 0, 1, and 2.
The results show that using more training data for the SVM classifier (for SVMxval) improves results over using less data (SVM200). The UnSup method generally yielded high precision that is often higher than using the supervised methods; however, it led to much lower recall. Our method, where users are represented using MUSE embedding vectors, was competitive with the baseline supervised methods with the distinct advantage of being completely unsupervised. When compared to UnSup, though our method led to lower precision, it led to higher recall, leading to a comparable overall average F1-score. Given the results, we can see that our method has several advantages, namely: it is unsupervised; it does not rely on any Twitter specific features such as retweets; and, as we will see shortly, it can lead to finer-grained clustering that is difficult to achieve with the baseline methods.
It is noteworthy that although our method and the UnSup method both produced 2 clusters for the Trump dataset, they in fact produced 6 and 3 clusters respectively for the ED dataset. Upon inspecting the three clusters produced by the UnSup method, we found that each cluster was dominated by either pro-Erdoğan, anti-Erdoğan (without party affiliation), or pro-IYI party users. However, using our method, the 6 clusters were dominated by either: pro-Erdoğan, anti-Erdoğan (without party affiliation), pro-CHP, pro-IYI, or pro-HDP users. We further manually inspected the two clusters that were dominated by pro-Erdoğan users, and we found that pro-AKP accounts were dominant in one cluster while the other cluster had mostly pro-Erdoğan users who did not explicitly express party affiliation. Therefore, in essence, our method was able to tweak apart all the constituent sub-groups in our dataset. As can be seen, the clusters generally have high precision (0.83 on average). We suspect that our method was able to more effectively cluster users at a finer-grained level compared to the UnSup method, because the latter relies strictly on retweeted accounts and users may retweet a tweet if it agrees with their stance on a specific topic regardless of the political affiliation of the source. Conversely, our method uses the content of the tweets, and users with similar ideological stances may use similar language in their tweets. We plan to investigate this further in future work.
One of the symptoms and reinforcing causes of polarization is “biased assimilation”, where individuals readily accept confirming evidence and are rather critical when provided with disconfirming evidence (Dandekar_2013). We inspected if this phenomenon implies that there is correlation between clusters on different topics. As the results show, the positions towards different topics highly-correlate with users’ positions during the election period. The greatest overlap was between the position towards Erdoğan in the ED and TD datasets. Again, we can see that for certain topics, we can observe fine-grained separation between groups. Mutual Information is a measure of the dependence between two clustering solution, and AMI accounts for higher mutual information scores when the number of clusters is larger. We can see that topics influence similar stances towards other topics such that the minimum AMI score in the table is 0.70. AMI score is especially high for particular topic pairs, such as Europe and USA (0.97), Kurdish and USA (0.95), USA and Trump (0.95), and Syrian and Arab (0.89). Overall, our approach enables investigation of correlation between stances for different issues.
We computed RWC polarization measure on the aforementioned 8 target topics. where 0 implies no polarization and 1 implies extreme polarization. The results suggest that Turkish users are polarized on all topics. However, the stance of people towards particular issues, such as Erdoğan and HDP, cause more polarization than others. We also observe different RWC scores for similar topics. For instance, HDP causes more polarization than PKK, which is considered as a terrorist group by Turkey and USA. While HDP is a legitimate political party, many Turkish citizens want to ban HDP from participating in elections due to its alleged relation to PKK, yielding different stances towards HDP.
We test the correlation of the ASTS metric against human ratings using Pearson’s r and Kendall’s rank correlation coefficient r_τ. We find that ASTS and human ratings are strongly correlated (r=0.70, r_τ=0.49, p<0.001). This suggests ASTS scores could be used to train and algorithmically evaluate scene generation systems that map descriptions to scene templates.
As we increase the hyper-parameters α and β, our methods initially behave like the baseline, learning the training set but failing on the test set. However, with strong enough hyper-parameters (moving towards the bottom in the tables), they perform perfectly on both the biased training set and the unbiased test set. For Method 1, stronger hyper-parameters work better. Method 2, in particular, breaks down with too many random samples (increasing α), as expected. We also found that Method 1 did not require as strong β as Method 2. From the synthetic experiments, it seems that Method 1 learns to ignore the bias c and learn the desired relationship between P and H across many configurations, while Method 2 requires much stronger β.
As we increase the hyper-parameters α and β, our methods initially behave like the baseline, learning the training set but failing on the test set. However, with strong enough hyper-parameters (moving towards the bottom in the tables), they perform perfectly on both the biased training set and the unbiased test set. For Method 1, stronger hyper-parameters work better. Method 2, in particular, breaks down with too many random samples (increasing α), as expected. We also found that Method 1 did not require as strong β as Method 2. From the synthetic experiments, it seems that Method 1 learns to ignore the bias c and learn the desired relationship between P and H across many configurations, while Method 2 requires much stronger β.
Although adding hierarchical structure further improves performance, the improvements are small (+0.1 and +0.2 for Twitter and NER respectively). For VSL-GG-Hier, variational regularization accounts for relatively large differences of 0.6 for Twitter and 0.5 for NER. These results show that the improvements do not come solely from adding a reconstruction objective to the learning procedure. In limited preliminary experiments, we did not find a benefit from adding unlabeled data under the “no VR” setting.
that the internal LSTM kernel of RETURNN outperforms all competitors except the CuDNN implementation w. r. t. runtime and memory usage. The internal memory management of TensorFlow and Torch make it difficult to obtain exact measurements of their memory usage, but we can see that 25% less memory is required in our LSTM kernel compared to the Theano based kernels, including Keras.
The features extracted from the autoencoder are used for emotion classification on the IEMOCAP dataset. In the previous section, we have described our work in unsupervised pretraining of denoising and recurrent autoencoders with different spectrograms and architectures. We now use the pretrained layers of the best performing configuration on the validation experiments (TIED-128-5) to initialize weights of a MLP (Multi-layer Perceptron). To evaluate the performance of the learned representations, we add a softmax layer with four output units, for each of the four main emotions (Angry,Happy,Neutral and Sadness). Let us represent the dataset by a collection of speech frames {xij} where i denotes the utterance index, and j, the time frame in the i-th utterance. If yij =f(xij) is the activation of the top layer in C=4 emotion category dimensions predicted by the MLP, then the predicted emotion category Ci for the i-th utterance is given by: Ci= argmaxk∈1: Cj =Ti∑j=1yij(k) Similar to the approach in Han et al. we split the dataset (comprising of five sessions with two distinct speakers in each session) into five folds where for each fold, data from four sessions is used as the training set, and the remaining session for validation and testing. For the session we choose for validation and testing, we consider one speaker for validation of hyper-parameters, and the remaining speaker for testing. We compare our results with three approaches: (1) A network with the same architecture without pre-training (2) A softmax classifier trained on features extracted from the COVAREP toolbox, such as MFCCs and prosodic features (for example, pitch, peak slope, Normalized Amplitude Quotient (NAQ), and difference between first two harmonics in speech (H1-H2)) and (3) the DNN-ELM approach described in Han et al. It is worth noting that our work is more focused on finding a good unsupervised representation for affect, rather than utilizing a highly complex model for subsequent classification.
We agree that simple and strong baselines are important, and are pleasantly surprised to see that a CCA baseline performs so well on mean rank. However, there are a few problems with this analysis. We recreate Tab. To summarize: In an attempt to make evaluation for Visual Dialog more reliable, we have recently had multiple human subjects indicate whether each of the candidate answers for a question is correct, which is then used as the reference score while computing the NDCG metric. Further, NDCG evaluation using dense annotations does not favor a particular family of Visual Dialog models (between discriminative and generative), as evidenced by findings from the 1st Visual Dialog challenge noted on visualdialog.org. (see Tab. Further, automatic evaluation of dialog is an open research problem, and our NDCG evaluation protocol for Visual Dialog is an attempt at making it more robust. There is scope for improvement across all axes – task/dataset, evaluation, as well as methods.
Although each of two proposed methods multiqvec and multiqvec-cca correlate better with a different extrinsic task, we establish (i) that intrinsic methods previously used in the literature (cross-lingual word similarity and word translation) correlate poorly with downstream tasks, and (ii) that the intrinsic methods proposed in this paper (multiqvec and multiqvec-cca) correlate better with both downstream tasks, compared to cross-lingual word similarity and word translation.
Our goal was not to achieve maximal overall score, but rather to analyze the behavior of word embedding models on Czech language. In following text, we discuss how well these models hold semantic and syntactic information. we can say that for Czech is CBOW approach, which predicts the current word according to the context window better, than predicting a words context based on the word itself as in Skip-gram approach.
The forecasting error is summation of errors over moving a window (30 days) by 10 days over the period. Our Ccomposition method outperforms other time series only models and time series plus text models in both stock and poll data.
Initializing with pre-trained word embeddings (+WE) helps us improve on prediction. Our relation specific attention model outperforms the others, indicating that different type of relations have different alignment patterns.
This table also contains the results of some top performing models on the SICK dataset. On the other hand, among our two proposed models, Model 2 performs very well on both Tree-LSTM variants showing significant improvements with every configuration. For both child sum as well as binary Tree-LSTMs, our second model with cross sentence attention has superior performance compared to the plain Tree-LSTM variants getting MSE of 0.2518 and 0.2435 respectively. For the child sum Tree-LSTM, Model 1 performs poorly compared to all the other models. This poor performance is due to the hard attention that it applies. If a subtree has n children, this hard attention forces n−1 children to have probability close to 0 which causes the domination of just one child hidden state in the summation. The rest will not contribute at all. On the other hand, the reason behind Model 2 performing better in every configuration with both variants is that even though a hard attention causes one of the children to get close to 0, the normalization of N-ary tree into binary tree causes much more flexibility for the information to flow from bottom to top. During normalization, a branch with n children gets split up to n−1 full binary trees resulting in (n−1)/2 nodes that are always chosen. (0.2532 MSE). However, our implementation of their model with their reported hyper-parameters gave a 0.2591 MSE which is significantly worse than their claimed MSE. This suggests to us that the implementation environment has a strong impact on model performance. To the best of our knowledge, our work is the first to encode attention inside a binary Tree-LSTM cell. (0.2734 MSE) for the non-attentive version. This performance analysis does show the effectiveness of our generalized attention model.
In our experiment, the dataset is split into two-part: train set and dev set with the corresponding ratio (0.9,0.1). Two subsets have the same imbalance ratio like the root set. For each combination of model and word embedding, we train model in train set until it achieve the best result of loss score in the dev set.
Here the network is learned from a teacher approach, so no extra label data is needed. We use DTW as the teacher approach, and normalize the DTW similarity scores between 0 and 1 as the target of regression. From the table, we find that the performance of the attention-based network without multiple hops is comparable to DTW (rows (B) v.s. (A)), and that the 3-hop network outperforms DTW on test sets 2 and 3 (rows (C) v.s. (A)). Therefore, it is reasonable to replace DTW with a network learned from it.
Those results are divided into two parts as follows. First, the proposed extension handles missing elements in multimodal sequences better than the original model. It can be inferred that the max operation keeps the strongest of the common semantic concepts between modalities. Note that the representations update the weights in the backward step. Second, the proposed extension reaches similar results to the standard LSTM. In this case, LSTM was trained in each modality independently. As a reminder, we mentioned two setups for classification tasks: the traditional setup and the setup used in this work. We want to point out that the visual LSTM boost the performance of audio sequences compared to LSTM. As a result, our model reaches lower Label Error Rate in the audio sequences than the standard LSTM trained only in audio sequence.
The speed of producing an interpretation was analysed according to whether it was consistent or inconsistent with regards to the priming words, and whether this was affected by prime order. It was expected that if the priming was effective then interpretations that were inconsistent with the primes would be produced slower than interpretations that were consistent with the primes. Since a number of participants did not give responses for all of the categories, the number of participants in the analysis was 51. The analysis showed no main effect of Interpretation (p=0.297), Prime Order (p=0.718), nor an Interpretation x Prime Order interaction (p=0.994). One likely reason for the non-significant effects is the large variance in response times (range = 369ms to 10035 ms), thus making it difficult for the mean differences to reach significance. For this reason we feel that the frequency scores are more reliable measures, and importantly these showed significant effects of priming.
The speed of producing an interpretation was analysed according to whether it was consistent or inconsistent with regards to the priming words, and whether this was affected by prime order. It was expected that if the priming was effective then interpretations that were inconsistent with the primes would be produced slower than interpretations that were consistent with the primes. Since a number of participants did not give responses for all of the categories, the number of participants in the analysis was 51. The analysis showed no main effect of Interpretation (p=0.297), Prime Order (p=0.718), nor an Interpretation x Prime Order interaction (p=0.994). One likely reason for the non-significant effects is the large variance in response times (range = 369ms to 10035 ms), thus making it difficult for the mean differences to reach significance. For this reason we feel that the frequency scores are more reliable measures, and importantly these showed significant effects of priming.
In this section, we experimentally compare our proposed model with state-of-the-art question answering models (i.e. QANet [yu2018qanet]) and BERT-Base [devlin2018bert]. Although BERT has two versions (i.e., BERT-Base and BERT-Large), we only compare our model with the BERT-Base model due to the lack of computational resource. Prediction layer is attached at the end of the original BERT-Base model, and we fine tune it based on our dataset. The named entity integration method is chosen to pure concatenation (Concatenate the named entity information on pathology report text and query text first and then concatenate contextualized representation and concatenated named entity information). QANet outperformed BERT-Base with 3.56% score in F{}_{1}-score but underperformed it with 0.75% score in EM-score. Compared with BERT-Base, our model led to a 5.64% performance improvement in EM-score and 3.69% in F{}_{1}-score. Although our model didn’t outperform much with QANet in F{}_{1}-score (only 0.13%), our model significantly outperformed it with 6.39% score in EM-score.
Our GraphWriter model outperforms other methods. We see that models which leverage title, entities, and relations (GraphWriter and GAT) outperform models which use less information (EntityWriter and Rewriter).
(i) Within AttConv, “advanced” beats “light” by 1.1%; (ii) “ w/o convolution” and attentive pooling (i.e., ABCNN/APCNN) get lower performances by 3%–4%; (iii) More complicated attention mechanisms equipped into LSTM (e.g., “attentive-LSTM” and “enhanced-LSTM”) perform even worse.
S4SS4SSS0Px4 Results. First, AttConv surpasses the top competitor “Decomp-Att”, reported in Thorne et al. (All: 62.26 vs. 52.09) and test (All: 61.03 vs. 50.91). In addition, “advanced-AttConv” consistently outperforms its “light” counterpart. Moreover, AttConv surpasses attentive pooling (i.e., ABCNN&APCNN) and “attentive-LSTM” by >10% in All, >6% in Sub and >8% in “gold evi.”.
The authorship signature is captured by the metrics proposed, which reveals the relationship between style and changes in network structure. Success scores greatly surpass the threshold imposed by a blind classification obtained with ZeroR algorithm, which for our collection is 1/8=12.5%. The simple OneR algorithm also performed well, reaching 46.25% score. There is a single subset (combination) of attributes for each variance threshold level. all attributes are present (even though the threshold is larger than zero) and all cells of the highest row are colored black. As the threshold is gradually increased, attributes are successively removed until there are no attributes left and all the cells in the lowest row are colored white. Remarkably, the first and the last attributes removed were respectively the fourth and the third moments of the number of cliques Cq. Note also that for nine of the twelve network metrics, either the third or the fourth moment had the smallest variance. As we explore the combinations obtained by removing one attribute at a time the scores increase (monotonically for J48 and KNN) until a maximum value is reached, after which the scores rapidly decrease reaching ZeroR score when there are zero attributes. It must be noted that the maximum scores can be reached with a few attributes, at most 16 attributes in the case of KNN. For KNN two combinations of attributes reached the highest score. Again, the four moments of a given network metric are grouped together. It can be seen that the best scoring combinations for some algorithms did not include any of the four moments from some network metrics. In particular, load centrality L was not used by any algorithm (having therefore a blank column for L in figure One should highlight the betweenness centrality B, which was extensively used by KNN, NB and RBFN even though its mean value (i.e. first moment, and the leftmost column under the B label on figure Two last combinations of attributes were constructed. The complementary subset of 36 second, third, and fourth moments represent the dynamical aspects of networks since they describe the extent of variation around the mean value throughout a text. Classification was applied to these two subsets without further dimensionality reduction.
Around half of the questions in this dataset are complex questions. Others are simple questions. Our SPARQA achieved a new state-of-the-art result on this dataset, outperforming all the known baseline results. F1 was improved from 20.40—the previous best result achieved by the PARA4QA, to 21.53, by an increase of 5.5%. We would like to highlight the result 17.70 achieved by UDEPLAMBDA, which also adopted dependency-based semantic parsing but relied on a set of predefined rules. It demonstrated the effectiveness of our skeleton-based semantic parsing. Moreover, our skeleton parsing did not hurt accuracy on the 1,172 simple questions in this dataset, where SPARQA (F1=27.68) was comparable with PARA4QA (F1=27.42).
All the questions in this dataset are complex questions. Our SPARQA outperformed most baselines except for two that used additional data. Specifically, SPLITQA + data augmentation used additional 28,674 magic training examples that were not accessible to our approach. PullNet used not only KB but also external search engine snippets which were not used in our approach.
Achieving 100% accuracy on this dataset is not a realistic goal, as not all test questions are answerable (specifically, some answers do not occur in the training data and hence cannot be learned by a machine learning system). Baselines for the dan Noise drops this to 44% with the best ir model and down to ≈30% with neural approaches.
Using the constraints for nested NER, we first evaluate our system on nested named entity corpora: ace 2004, ace 2005 and genia. Both ace 2004 and ace 2005 contain 7 NER categories and have a relatively high ratio of nested entities (about 1/3 of then named entities are nested). Our results outperform the previous SoTA system by 2% (ace 2004) and 1.1% (ace 2005), respectively. genia differs from ace 2004 and ace 2005 and uses five medical categories such as DNA or RNA. For the genia corpus our system achieved an F1 score of 80.5% and improved the SoTA by 2.2% absolute. Our hypothesise is that for genia the high accuracy gain is due to our structural prediction approach and that sequence-to-sequence models rely more on the language model embeddings which are less informative for categories such as DNA, RNA. Our system achieved SoTA results on all three corpora for nested NER and demonstrates well the advantages of a structural prediction over sequence labelling approach.
We choose ontonotes for our ablation study as it is the largest corpus. Biaffine Classifier We replace the biaffine mapping with a CRF layer and convert our system into a sequence labelling model. The CRF layer is frequently used in models for flat NER , e.g. Lample et al. The large performance difference shows the benefit of adding a biaffine model and confirms our hypothesis that the dependency parsing framework is an important factor for the high accuracy of our system. Contextual Embeddings This shows that BERT embeddings are one of the most important factors for the accuracy. Context Independent Embeddings We remove the context-independent fastText embedding from our system. Which suggests that even with the BERT embeddings enabled, the context-independent embeddings can still make quite noticeable improvement to a system. Character Embeddings Finally, we remove the character embeddings. One explanation would be that English is not a morphologically rich language hence does not benefit largely from character-level information and the BERT embeddings itself are based on word pieces that already capture some character-level information.
We test the system proposed in the paper by introducing a Refiner after the baseline system on the validation set and chose the best one. It is shown that our approach can enhance the system performance over baseline model. The Refiner module improves the baseline system by +6.6 F-score points on validation set.
Compared with the SOTA model, all our models showed a three times performance improvement in MAP, MRR, and Recall@K indices, approximately. In particular, Recall@5, which only sees five retrieval citation papers, is a significant improvement. We compared the performance by independently reproducing the Python code related in the CACR paper. There was no detailed experimental information such as frequency in the actual paper. For MAP, MRR, and Recal@10, our model outperform, but after Recall@10, it does underperform. Based on experience, we guessed this to be a phenomenon that occurs when the classification label value is returned with a high frequency of cited papers.
We first conduct evaluation from a text generation perspective. We compare to the standard Seq2Seq model w/ and w/o attention mechanism. We can see that Seq2Ast performs better than the standard Seq2Seq method, which verifies the effectiveness of the hierarchical decoder. As a reference, we can also report the BLEU-4 score of the extractive approach despite this is not a perfect to compare between the generative and extractive approaches. The BLUE-4 score of our extractive approach is 72.27, which is extremely high for a text generation task. But this is also reasonable because the extractive approach aims to select a most possible assertion from a candidate list which includes the referenced result. Therefore, the BLEU-4 score for a correct top-ranked result is 100. Further experiments that applying the results of both generative and extractive approaches in passage-level question answering task will be given in the following subsection.
At the end of the February stream we separately measured aggregate classification performance for static-cnn and stream-cnn. For each of three classification metrics, namely precision, recall, and F1, we computed scores for each of the 100 labels, then aggregated them with a weighted average, weighting by the number of samples per label in the test set. We found stream-cnn achieved an F1 score of 0.42, similar to the F1 score of 0.43 achieved by static-cnn. Our original hypothesis was that stream-cnn would perform better than static-cnn due to its updated vocabulary; this result does not support that hypothesis. This point estimate of the performance difference may however be interpreted as a sanity check of the stream-cnn approach, suggesting only a small performance degradation.
Note here that an Eojeol is considered correctly tagged only if all its constituent morphemes have been transformed, segmented, and tagged properly. (ss). The model is able to infer that most of the forms shown in the graph represent the past tense and that they share a similar transformation pattern at the final consonant grapheme level. This shows that our model is able to correlate similar sub-character level morphological transformations even when operating at the character level. Furthermore, it is worth noting that Chinese characters still occur rarely in the Korean language in certain contexts. We can see Chinese characters grouped in a cluster, which shows that the model is able to distinguish one character-rich language (Korean) from another (Chinese). Other characters, such as punctuation, are also grouped by type in largely distinct clusters with occasional overlap.
Overall we see varying performance across the classifier, with some performing much better out-of-sample than others.
To compare the efficiency of our CPU- and GPU-based training tools, we measure the wall-clock time for running a single epoch of training DBLSTM on the IAM-offline training set. It takes 6,033 and 200 seconds for CPU- and GPU-based tools respectively. GPU-based tool achieves about 30 times speedup. Both tools can train DBLSTMs leading to similar recognition accuracy, therefore we use our GPU-based training tool for the remaining experiments. The total time scales almost linearly with the amount of training data. For the largest dataset with one million lines, it takes about 10 days to complete training using a single GPU card.
Several observations can be made. First, for CTC-based decoding without using LM, both CER and WER improves with the increasing amount of training data. The improvement comes from both the improved modeling for local character image modeling and the improved implicit language modeling. even when about 1M lines of training data is used. Second, for CTC-based decoding without using LM, a CER of 11.8% can be achieved for 1M training case, but the corresponding WER is only 40.1%. The CER and WER can be reduced to 7.0% and 33.5% respectively by using WFST-based decoding with a character 8-gram LM. This shows clearly the effectiveness of using a powerful explicit LM. However, using character 10-gram does not bring additional improvement. Third, the CER and WER can be further reduced to 6.0% and 18.0% respectively by using WFST-based decoding with a word trigram. The relatively small improvement of CER (14% relative CER reduction) and much bigger improvement of WER (46% relative WER reduction) shows clearly the power of lexical constraints.
In order to figure out the advantage of inter-cell connections, we detail the model contribution on each word on the validation data. Specifically, we compute the difference in word loss function (i.e., log perplexity) between methods with and without inter-cell NAS. We observe that the rare words in the training set obtain more significant improvements. This is because the connections between multiple cells enable learning rare word representations from more histories. While for common words, they can obtain this information from rich contexts. More inputs from previous cells do not bring much useful information.
First, most of the reasons given by humans are reasonable, which fits our previous observation. Second, even though the majority of reverse reasons are not plausible, which fits our assumption, some of them do make sense. One scenario is that when the reason is comparing some property of both candidates, it can be used for both questions. For example, for the question pair “The trophy doesn’t fit into the brown suitcase because it is too small/large”, explanations like “Only small objects can fit into large objects” are plausible for both questions. Last but not least, not surprisingly, most of the reasons generated by GPT-2 have relatively low quality. Based on the five reasons, we can find two limitations of GPT-2: (1) it could generate some meaningless words (e.g., ‘-C.B.’), which could influence the overall quality significantly; (2) some of the answers are related and complete sentences by themselves, but they are not a valid reason for the question. For example, the second reason is wrong because Charlie cannot be the one who has given the money. These observations show that understanding commonsense knowledge is still a challenging task for current pre-trained language representation models like GPT-2.
On the PTB and IMDB Movie Review dataset, the gated word & character model with a fixed gating value, gconst=0.25, and pre-training achieves the lowest perplexity . On the BBC datasets, the gated word & character model without pre-training achieves the lowest perplexity.
First, we note that the models that use Gold bag of objects information are the best performing models across classifiers. We also note that the performance is better than human performance. We hypothesize the following reasons for this: (a) human responses were crowd-sourced, which could have resulted in some noisy annotations; (b) our gold object-based features closely resembles the information used for data-generation as described in \newciteshekhar2017foil_acl for the foil noun dataset. The models using Predicted bag of objects from a detector are very close to the performance of Gold. The performance of models using simple bag of words (BOW) sentence representations and an MLP is better than that of models that use LSTMs. Also, the accuracy of the bag of objects model with Frequency counts is higher than with the binary Mention vector, which only encodes the presence of objects. The Multimodal LSTM (MM-LSTM) has a slightly better performance than LSTM classifiers. In all cases, we observe that the performance is on par with human-level accuracy. Our overall accuracy is substantially higher than that reported in \newciteshekhar2017foil_acl. Interestingly, our implementation of CNN+LSTM produced better results than their equivalent model (they reported 61.07% vs. our 87.45%).
For other parts of speech, we fix the image representation to Gold Frequency, and compare results using the BOW-based MLP and MM-LSTM. We also compare the scores to the state of the art reported in \newciteshekhar2017b. Note that this model does not use gold object information and may thus not be directly comparable – we however recall that only a slight drop in accuracy was found for our models when using predicted object detections rather than gold ones. The classification performance is not as high as it was for the nouns dataset. Noteworthy is the performance on adverbs, which is significantly lower than the performance across other parts of speech. We hypothesize that this is because of the imbalanced distribution of foiled and real captions in the dataset. We also found that the performance of LSTM-based models on other parts of speech datasets are almost always better than BOW-based models, indicating the necessity of more sophisticated features.
We first perform ablation experiments with our proposed models over the Nouns dataset (FOIL). As expected, we cannot classify foiled captions given only image information (global or object-level), resulting in chance-level performance.
We experiment with different parameterizations of the sense induction algorithm to obtain proto-conceptualizations (PCZ) with different average sense granularities, since a priori, there is no clear evidence for what the ‘right’ sense granularity of a sense inventory should be. Chinese Whispers sense clustering with the default parameters (n=200) produced an average number of 2.3 (news) and 1.8 (wiki) senses per word with the usual power-law distribution of sense cluster sizes. Decreasing connectivity of the ego-network via the n parameter leads to more fine-grained inventories (cf. For each dataset, we report the counts of overall number of words (vocabulary size), including monosemous words and polysemous ones, respectively. For each PCZ we report the cardinality, the average polysemy and the maximum polysemy. Finally, we report the overall and the average number of related senses and hypernyms. Numbers vary across datasets due to the different nature of the two source corpora and the selection of different parameter values for sense induction.
The system of Ye and Baldwin \shortciteYB07 got the highest result out of the three participating systems in the SemEval 2007 shared task. They extracted features such as POS tags and WordNet-based features, and also high level features (e.g semantic role tags), using a word window of up to seven words, in a Maximum Entropy classifier. Tratz and Hovy \shortciteHT09 got a higher result with similar features by using a set of positions that are syntactically related to the preposition instead of a fixed window size. The best performing systems are of Hovy et al \shortciteHTH10 and of Srikumar and Roth \shortciteSR13b. Both systems rely on vast and thoroughly-engineered feature sets, including many WordNet based features. Hovy et al \shortciteHTH10 explored different word choices (i.e, a fixed window vs. syntactically related words) and different methods of extracting them, while Srikumar and Roth \shortciteSR13b improved performance by jointly predicting preposition senses and relations.
To prevent overfitting, the L2 regularization term is introduced to our loss function. We also adopt early stop strategy, The training process will be stopped after seven epochs of no improvement on development set is observed. To further avoid overfitting, dropout is applied before the biLSTM encoder and hidden layer of classifier MLP. We randomly initialize word vectors for words that doesn’t appear in Glove. And hyper-parameters are determined using grid search strategy.
, the table shows that our proposed dynamic routing performed the best on all datasets. In document-level text classification, specifically Yelp 2013 Yelp 2014 and IMDB, DR-AGG outperforms previous models’ best results by 2.5%, 3.0% and 1.6% respectively. In sentence-level text classification, such as SST-1 SST-2, our model also achieves better results. Compared to max pooling, average pooling and self-attention, which are closely related to our model, DR-AGGs significantly improves the performance. For example the standard DR-AGG outperforms the max pooling approach by 1%, 1.8%, 4%,2.5% and 0.4% on Yelp 2013,Yelp 2014, IMDB, SST-1 and SST-2. It empirically shows that our proposed dynamic routing policy is the most effective method on aggregating information.
It is known that conventional topic models directly applied to short texts suffer from low quality topics, caused by the insufficient word co-occurrence information. Here we study whether or not the meta information helps MetaLDA improve topic quality, compared with other topic models that can also handle short texts. Higher scores indicate better topic coherence. All the models were trained with 100 topics. It is clear that MetaLDA performed significantly better than all the other models in WS and AN dataset in terms of NPMI, which indicates that MetaLDA can discover more meaningful topics with the document and word meta information. We would like to point out that on the TMN dataset, even though the average score of MetaLDA is still the best, the score of MetaLDA has overlapping with the others’ in the standard deviation, which indicates the difference is not statistically significant.
The results show that MetaLDA outperformed all the competitors in terms of perplexity on nearly all the datasets, showing the benefit of using both document and word meta information. Specifically, we have the following remarks: By looking at the models using only the document-level meta information, we can see the significant improvement of these models over LDA, which indicates that document labels can play an important role in guiding topic modelling. It is interesting that PLLDA with 50 topics for each label has better perplexity than MetaLDA with 200 topics in the 20NG dataset. With the 20 unique labels, the actual number of topics in PLLDA is 1000. However, if 10 topics for each label in PLLDA are used, which is equivalent to 200 topics in MetaLDA, PLLDA is outperformed by MetaLDA significantly. At the word level, MetaLDA-def-wf performed the best among the models with word features only. Furthermore, comparing MetaLDA-def-wf with MetaLDA-def-def and MetaLDA-0.1-wf with LDA, we can see using the word features indeed improved perplexity. The scores show that the improvement gained by MetaLDA over LDA on the short text datasets is larger than that on the regular text datasets. This is as expected because meta information serves as complementary information in MetaLDA and can have more significant impact when the data is sparser. On the AN dataset, there is no statistically significant difference between MetaLDA and DMR. On NYT, a similar trend is observed: the improvement in the models with the document labels over LDA is obvious but not in the models with the word features. Given the number of the document labels (194 of AN and 545 of NYT), it is possible that the document labels already offer enough information and the word embeddings have little contribution in the two datasets. Note that: (1) On the Reuters and WS datasets, all the models ran with a single thread on a desktop PC with a 3.40GHz CPU and 16GB RAM. (2) Due to the size of NYT, we report the running time for the models that are able to run in parallel. All the parallelised models ran with 10 threads on a cluster with a 14-core 2.6GHz CPU and 128GB RAM. (3) All the models were implemented in JAVA. (4) As the models with meta information add extra complexity to LDA, the per-iteration running time of LDA can be treated as the lower bound.
To compare, we also include the results published in Tikk et al. et al. ; Fundel et al. Row 2 reports the results of the previous best deep learning system on these two corpora. Rows 3 and 4 report the results of two previous best single kernel-based methods, an APG kernel Airola et al. ; Tikk et al. Rows 5-6 report the results of two rule-based systems. As can be seen, McDepCNN achieved the highest results in both precision and overall F1-score on both datasets.
To better understand the advantages of McDepCNN over kernel-based methods, we followed the lead of Tikk et al.
The results of McDepCNN were obtained from the difficult instances combined from AIMed and BioInfer (172 positives and 479 negatives). And the results of APG, Edit, and SL were obtained from AIMed, BioInfer, HPRD50, IEPA, and LLL (190 positives and 521 negatives) Tikk et al. While the input datasets are different, our outcomes are remarkably higher than the prior studies. The results show that McDepCNN achieves 17.3% in F1-score on difficult instances – which is more than three times better than other kernels. Since there are no examples of difficult instances that could not be classified correctly by at least one of the 14 kernel methods, below, we only list some examples that McDepCNN can classify correctly. Immunoprecipitation experiments further reveal that the fully assembled receptor complex is composed of two IL-6PROT1, two IL-6R alphaPROT2, and two gp130 molecules. The phagocyte NADPH oxidase is a complex of membrane cytochrome b558 (comprised of subunits p22-phox and gp91-phox) and three cytosol proteins (p47-phoxPROT1, p67-phox, and p21rac) that translocate to membrane and bind to cytochrome b558PROT2. Together with the conclusions in Tikk et al. , this comparison suggests that McDepCNN is probably capable of better capturing long distance features from the sentence and are more generalizable than kernel methods.
Here we tested McDepCNN using 10-fold of AIMed. Row 1 used a single window with the length of 3, row 2 used two windows, and row 3 used three windows. The reduced performance indicate that adding more windows did not improve the model. This is partially because the multichannel in McDepCNN has captured good context features for PPI. Second, we used the single channel and retrained the model with window size 3. The performance then dropped 1.1%. The results underscore the effectiveness of using the head word as a separate channel in CNN.
For a qualitative comparison of the two representations, we analyse 100 randomly sampled instances that are mis-classified by each classifier. While these instances need not be the same for each classifier, the trends in the errors show where one kind of representation scores over the other. We compared linguistic properties of these mis-classified instances, such as the person, tense and number. The two properties are important in terms of the semantics of the three classification problems. First-person mentions are useful indicators to identify if the speaker has influenza, took a drug or reported a personal health mention. Similarly, present participle forms of verbs appear in situations where a person has had an infection or taken a drug. For ‘Word’, the average is over the five representations, while for ‘Context’, the average is over the four context-based representations. In the case of IIC, an average of 58.2 mis-classified instances from word-based representations contained first person mentions. The corresponding number for context-based representations was 41. For PHMC, the averages are 64.8 (word-based) and 37.5 (context-based). The difference is not as high in the case of DUC (66.4 and 54.75 respectively). Differences are observed in the case of present participle in mis-classified instances. However, in the case of DUC, errors from context-based representations contain more average number of present participles (40.75) than word-based representations (33).
Note that all the links were evaluated manually. The Retrieval columns show the number of total idioms collected from a given data set and the Accepted columns present the number of idioms which were matched exactly as an idiom. We also present the precision achieved by the aforementioned link specifications.
Regarding periods shorter than thirty years, the amount of change naturally decreases as we shorten the cycle period. With less change, prediction could become more difficult. On the other hand, the F-score of NBCP remains relatively steady; it is robust when the percent of changed synsets varies. This confirms the analysis of Petersen et al.
Unique ngrams is a vector with 3,660 elements in Test1. To gain some insight into this vector, we sorted the elements in order of decreasing absolute difference between the mean of the Gaussian for class 0 and the mean for class 1. The size of the gap indicates the ability of the trigram to discriminate the classes. The difference column is the mean of the Gaussian of the winners (class 1) minus the mean of the Gaussian of the losers (class 0). When the difference is positive, the presence of the trigram in a word suggests that the word might be a winner. When the difference is negative, the presence of the trigram in a word suggests that the word might be a loser. Before splitting a word into trigrams, we added a vertical bar to the beginning and end of the word, to distinguish prefix and suffix trigrams from interior trigrams.
Fluency and semantic are slightly better. It is observed that style loss improves the data-fluency, resulting in better total fluency. However, style loss decreases G-BLUE slightly by allowing the transferred sentence to change the attribute better.
Stylistic Sensitivity We used this evaluation dataset to compute the Spearman rank correlation (ρstyle) between the cosine similarity scores between the learned word vectors cos(\boldmathvw,\boldmathvw′) and the human judgements. First, our proposed model, CBOW- all-ctx outperformed the baseline CBOW-near-ctx. Furthermore, the x of CBOW-dist-ctx and CBOW-sep-ctx demonstrated better correlations for stylistic similarity judgments (ρstyle=56.1 and 51.3, respectively). Even though the x of CBOW-sep-ctx was trained with the same context window as CBOW-all -ctx, the style-sensitivity was boosted by introducing joint training with the near context. CBOW-dist-ctx, which uses only the distant context, slightly outperforms CBOW-sep-ctx. These results indicate the effectiveness of training using a wider context window. For both N, the y (the syntactic/semantic part) of CBOW-near-ctx, CBOW- all-ctx and CBOW-sep-ctx achieved similarly good. Interestingly, even though the x of CBOW-sep-ctx used the same context as that of CBOW-all-ctx, the syntactic sensitivity of x was suppressed. We speculate that the syntactic sensitivity was distilled off by the other part of the CBOW-sep-ctx vector , i.e., y learned using only the near context, which captured more syntactic information.
This improvement arises from Choppy’s ability to model the joint distribution over all candidate cut positions and its direct optimization of the evaluation metric. Furthermore, the attention mechanism is able to effectively capture correlations between scores far apart in ranked order. This is in contrast to LSTMs, as used in BiCut, whose degradation with larger sequence length is well known.
Here, we want to tune the parameters of the LibLINEAR SVM algorithm. The objective is to find the parametric value that give us the best performance for the algorithm on our dataset. The parameters that were available to tune in this algorithm were: bias term, cost parameter, tolerance value and the weights. None of cost parameter, tolerance value or the weights had any effect on the accuracy or kappa (though the confusion matrices changed a bit). We had tried atleast 4 different values for each of them. Since, there was no other parameter to tune, we had to settle with tuning the bias term. The default value for bias is 1. We did the tuning manually and did not use CVParameterSelection to achieve this. We chose 3 different values for our bias term apart from the default (default is 1): 3, 4, 5. To tune the model, the feature-selection-tuned model was used, i.e the new model with 60000 features was used. Each of these parameter settings was tested using 10-fold cross validation on the cross-validation set. This model is built on the cross-validation set and then tested on itself using a 10-fold cross-validation for each of the parameter settings.
Adding the entity memory layer to the BERT-base model improves performance across the board on this task. Not supervising this layer with entity linking is worse, with the exception of SQuAD where it is slightly better. EaE achieves a similar average accuracy to BERT-large. However, the lama sub-task accuracies show that the two models are actually performing very differently. EaE is significantly better than BERT-large when predicting the mention-like words in the SQuAD and T-Rex probes. It is marginally worse for the RE probe, and very significantly worse for the ConceptNet probe, though.
To test this hypothesis, we measure prediction overlap and oracle accuracy. Unsurprisingly, the two open book approaches show the most similar predictions, overlapping in nearly 40% of examples. While Orqa outperforms T5 and EaE, the oracle accuracy of Orqa & GR is lower than Orqa & T5 or Orqa & EaE. This suggests some questions might be better suited to the closed book paradigm. In addition, the oracle accuracy of the two closed book systems is higher than that of the best performing open book system. We leave designing approaches that better combine these paradigms to future work.
Here as well, we achieve performance improvement consistent with CMU-MOSI. This method performs 1–2.4% better than the state of the art for all the modality combinations. Also, trimodal accuracy is 3% higher than the same for textual modality. Since, IEMOCAP dataset imbalanced, we also present the f-score for each modality combination for a better evaluation. One key observation for IEMOCAP dataset is that its A+V modality combination performs significantly better than the same of CMU-MOSI dataset. We think that this is due to the audio and video modality of IEMOCAP being richer than the same of CMU-MOSI. We think this is mainly because of learning the weights of bimodal and trimodal correlation (representing the degree of correlations) calculations at the time of fusion while Tensor Fusion Network (TFN) just relies on the non-trainable outer product of tensors to model such correlations for fusion.
In addition to testing on this shared task, we have also applied our best single system (without ensembling) on the SNLI dataset; our model achieve an accuracy of 85.5%, which is best result reported on SNLI, outperforming all previous models when cross-sentence attention is not allowed.
Feature group mapping A total of 31 feature type combinations, each with 28 different hyperparameter sets have been tested. The maximum attained F1-score in cross-validation is 64.26% for English and 61.20% for Dutch and shows that the classifier benefits from a variety of feature types. The results on the holdout test set show that the trained systems generalise well on unseen data, indicating little under- or overfitting. The simple keyword-matching baseline system has the lowest performance for both languages even though it obtains high recall for English, suggesting that profane language characterises many cyberbullying-related posts. Feature group and hyperparameter optimisation provides a considerable performance increase over the unoptimised word n-gram baseline system. The top-scoring systems for each language do not differ a lot in performance, except the best system for Dutch, which trades recall for precision when compared to the runner-ups.
On the restaurants dataset, this corresponds to an absolute improvement of 2.2% compared to the previous state-of-the-art method BERT-PT. Language model finetuning produces a larger improvement on the restaurants dataset. We think that one reason for that might be that the restaurants domain is underrepresented in the pre-training corpora of BERTBASE. Generally, we find that language model finetuning helps even if the finetuning domain does not match the evaluation domain. We think the reason for this might be that the BERT-base model is pre-trained more on knowledge-based corpora like Wikipedia than on text containing opinions. In addition, we find that the XLNet-base baseline performs generally stronger than BERT-base, but only outperforms the BERT-ADA models on the laptops dataset with an accuracy of 79.89% .
Additional validation for the logistic regression is obtained by measuring its cross-validated predictive accuracy. For each of the K samples, we randomly select 10% of the instances (positive or negative city pairs) as a held-out test set, and fit the logistic regression on the other 90%. For each city pair in the test set, the logistic regression predicts whether a link exists, and we check the prediction against whether the directed pair is present in B(k). Since the number of positive and negative instances are equal, a random baseline would achieve 50% accuracy. A classifier that uses only geography and population (the two components of the gravity model) gives 66.5% predictive accuracy. The addition of demographic features (both asymmetric and symmetric) increases this substantially, to 74.4%. While symmetric features obtain the most robust regression coefficients, adding the asymmetric features increases the predictive accuracy from 74.1% to 74.4%, a small but statistically significant difference.
In general, the best results are obtained by training on the US & UK split, while results of the models trained on the RoW & US, and RoW & UK splits are similar. The model with the best performance trained on US & UK, and RoW & UK splits is RoBERTa with F1 scores of 87.70 and 85.99 respectively. XLNet performs slightly better than RoBERTa when trained on RoW & US data split.
We first note that the top features related to either parody or genuine tweets are function words or related to style, as opposed to the topic. This enforces that the make-up of the data set or any of its categories are not impacted by topic choice and parody detection is mostly a stylistic difference. The only exception are a few hashtags related to parody accounts (e.g. #imwithme), but on a closer inspection, all of these are related to tweets from a single parody account and are thus not useful in prediction by any setup, as tweets containing these will only appear in either the train or test set.
We have performed ablation experiments by removing some components of the proposed model. We can see that attention and back-transfer loss play important roles in the model. This behavior happens due to the trade off that the decoder has to balance when transferring a sentence from a style to another. The decoder must maintain a proper balance between transferring to the correct style and generating sentences of good quality. Each of these properties can easily be achieved on its own, e.g., copying the entire input sentence will give low perplexity and good content preservation but low accuracy, on the other hand, outputting a single keyword can give high accuracy but high perplexity and low content preservation. While the classification loss guides the decoder to generate sentences that belong to the target style, the back transfer loss and the attention mechanism encourage the decoder to copy words from the input sentence. When both back transfer loss and attention are removed, the model is encouraged to just meet the classification requirement in the transfer step.
All of the results of six methods shows lower but close performance of that of using e2 to reconstruct e1. It shows that the ”Share-Concat” and Sub methods also have the ability to obtain high quality embedding of the interfering speaker from two-speaker environment.
The ”Share-Concat” method obtain the best results, reaching 93.9% test accuracy and 90.9% test accuracy in microphone1 and microphone2. The reason why the results of microphone2 is lower than that of array1 might be the distance of the speakers and microphones.
First note that the logistic regression classifier and the CNN model using the Title outperforms the Chance classifier significantly (F1: 59.12,59.24 vs 34.53). Second, only modeling the network structure yields a F1 of 55.10 but still significantly better than the chance baseline. This confirms our intuition that modeling the network structure can be useful in prediction of ideology. Third, note that modeling the content (HDAM) significantly outperforms all previous baselines (F1:68.92). This suggests that content cues can be very strong indicators of ideology. Finally, all flavors of our model outperform the baselines. Specifically, observe that incorporating the network cues outperforms all uni-modal models that only model either the title, the network, or the content. It is also worth noting that without the network, only the title and the content show only a small improvement over the best performing baseline (69.54 vs 68.92) suggesting that the network yields distinctive cues from both the title, and the content. Finally, the best performing model effectively uses all three modalities to yield a F1 score of 79.67 outperforming the state of the art baseline by 10 percentage points. Altogether our results suggest the superiority of our model over competitive baselines. In order to obtain deeper insights into our model, we also perform a qualitative analysis of our model’s predictions.
According to these results, our models obtain the best Word Error Rate (WER) in both SW and CH test sets among the S2S models with and without a language model. We also perform better than all CTC models in the SW test set and the difference in the CH set is minor.
Our first model consists of words occurring at least 5 times (Word >= 5) in the training set that led to 11069 words but with an OOV rate of 2.3% in the eval2000 test set. To address this high OOV rate, we tried to match the word vocabulary by an equivalent BPE vocabulary of 12k merge operations. This model performed better than the word model as expected. Our second model is a large vocabulary model made of all the words in the training set. This model performs better than the previous word model which may be due to absence of the frequently occurring OOV token. Ideally, S2S A2W model does not need a separate language model as it directly predicts a sequence of words using the decoder LSTM. But as the LM is trained on a larger corpus, we integrate it to check its effect and do not observe improvements as large as the character model. Comparison with CTC. The vocabularies of CTC models (both character and word) is different than ours hence models are not comparable. On a similar setup, their character-based model is worse by 5% WER. In the paper, they do not provide a reason for this behavior. In our S2S model, we observe the reverse trend i.e. the word-model performs worse than character-model. This is an interesting trend for CTC and S2S models and needs further exploration. We note that CTC and S2S models are not comparable with each other due to critical differences in loss computation.
For illustrative purposes, we chose examples that differ with respect to the number of feedback requests, the use of the prefix buffer, and the feedback values. Prefixes that receive a feedback ≥μ and are thus stored in the buffer and re-used for later samples are indicated by underlines. Advantage scores <0 indicate a discouragement of individual tokens and are highlighted in red.
This context vector essentially represents the image area that the model “looks at” in order to generate the t-th phrase. This information is passed to both the word-RNN as well as to the next hidden state of the phrase-RNN. In particular, at each time step t our phrase RNN also predicts a phrase label lt, following the standard definition from the Penn Tree Bank. For each phrase, we predict one out of four possible phrase labels, i.e., a noun (NP), preposition (PP), verb (VP), and a conjunction phrase (CP). We use additional   token to indicate the end of the sentence. By conditioning on the NP label, we help the model look at the objects in the image, while VP may focus on more global image information.
Our linguistically motivated vocabulary reduction ( LMVR) method achieves the best performance on average, proving our hypothesis that a correct morphological representation generates more accurate translations. Our method outperforms the strong baseline of BPE-based segmentation by 2.2 BLEU, 4.8 TER and 1.6 CHR3F points. The performance is slightly higher than the supervised method, which is related to the ambiguity caused by loss of information during the morphological analysis. The predicted vocabularies also indicate the significant difference between LVMR and BPE, where 73% of the sub-word units in the vocabulary are completely different. In order to better illustrate the properties of the generated sub-word units, we present example translations of two words from the test set. The two words have different roots, the first one is ağ (translation: net), and the second one is ağla (translation: (to) cry). BPE segments both words to the same root ağ, a character sequence frequently observed in root words in Turkish. In the first case, both unsupervised methods segment the word into the same sub-word units, while the embedding of the sub-word unit segmented with BPE is semantically ambiguous and generates unreliable translations. On the other hand, our method can preserve the correct meaning in both cases.
Note that these improvements cannot be explained by the sheer fact of employing a bilingual dictionary. We tried to use the same dictionary directly: that is, for the Ukrainian texts in the test set, replace all the words with their dictionary Russian translation. The remaining out-of-vocabulary words were ‘translated’ with the Dameral-Levenshtein distance approach. They are a bit better than the ones of the raw edit distance, but still far from the performance of the matrix translation method. It means that the algorithm itself is the cause of improvements.
Note that all instances of significant effects had positive effect sizes. We observe that the context-free GloVe model exhibits the highest overall proportion of significant positive effect sizes, indicating severe pro-stereotypical bias. ELMo demonstrates the lowest proportion of significant positive effect sizes, although this might be due to the use of character convolutions for names that are out of vocabulary, which may lead to the loss of stereotypical semantics. Furthermore, BERT (bbc) exhibits the highest proportion of bias on both race and intersectional tests, and the highest proportion overall among contextual word models. We also observe that larger models tend to exhibit a smaller proportion of significant positive effect sizes. This is true for both the BERT family (blc: 0.48, bbc: 0.33) and the GPT family (GPT: 0.3, GPT-2_117 M: 0.32, GPT-2_345 M: 0.23). However, barring a more robust study with more model sizes in consideration, we caution against a definitive conclusion. Moreover, embedding association tests targeting race generally demonstrate a higher proportion of significant associations across models than those targeting gender, suggesting that on a broad level the problem of racial bias is more severe in word representations than gender bias. This motivates more attention on racial bias in word representations.
Table. For variants of G-BERT, G-BERTG−,P− performs worse compared with G-BERTG− and G-BERTP− which demonstrate the effectiveness of using ontology information to get enhanced medical embedding as input and employ an unsupervised pre-training procedure on larger abundant data. Incorporating both hierarchical ontology information and pre-training procedure, the end-to-end model G-BERT has more capacity and achieve comparable results with others.
The accuracy of RvNN is much lower than that of the MFS baseline. Moreover, Tree-LSTM, which is an improved RvNN, is still lower than simple LogRes, despite Tree-LSTM achieving state-of-the-art performance on the phrase-annotated Stanford Sentiment Treebank The accuracy of CNN and \newciteKokkinos:2017 is 0.803 and 0.807, respectively, which are slightly lower than that of our proposed method without polar dictionaries. In contrast, Tree-LSTM with dictionary achieves comparable results to Tree-CRF. Our Tree-LSTM with attention and polar dictionary obtained the highest accuracy.
Similarly to \newciteKokkinos:2017, we observe slight improvement when using attention for Tree-LSTM. However, unlike the Japanese experiments, using a dictionary degrades sentiment classification accuracy. We discuss this in the following section. and we suppose that it introduces noise in the Stanford Sentiment Treebank.
Anecdotal examples Note that the scores produced do not represent probabilities, but global ranks (i.e., we percentile-normalized the scores produced by Dice, as they have no inherent semantics other than ranks). For instance, be at shed was found to be much more typical than be at pet zoo for snake, while salience was the other way around. Note also the low variation in ConceptNet scores, i.e., in addition to being unidimensional, this low variance makes any ranking difficult.
We see that perplexity results on the unpruned static models are consistent with the results observed in the Billion Word and WikiText scenarios. Further, the perplexity improvements on the statically interpolated models carry over after pruning. Lastly, test set WER results are consistent with the perplexity results. We see a 9.5% relative WER reduction between linear interpolation and count merging or Bayesian interpolation and no significant difference between count merging and Bayesian interpolation.
The vector representations derived from various window sizes can be interpreted as prominent and salient n-gram word features for the tweets. These features are concatenated to create a vector of size f×L, where L is the number of different l values , which is further compressed to a size k, before passing it to a fully connected softmax layer. The output of the softmax layer is the probability distribution over topic/sentiment labels. Next, two dropout layers are used, one on the feature concatenation layer and other on the penultimate layer for regularization (ρ=0.5).
The results are quite dramatic. The approach achieves impressive performance for Spanish, one of the languages \newciteLample2018crosslingual include in their paper. For the languages we add here, performance is less impressive. For the languages with dependent marking (Hungarian, Polish, and Turkish), P@1 scores are still reasonable, with Turkish being slightly lower (0.327) than the others. However, for Estonian and Finnish, the method fails completely. Only in less than 1/1000 cases does a nearest neighbor search in the induced embeddings return a correct translation of a query word. We note that while languages with mixed marking may be harder to align, it seems unsupervised BDI is possible between similar, mixed marking languages. In general, unsupervised BDI, using the approach in \newciteLample2018crosslingual, seems challenged when pairing English with languages that are not isolating and do not have dependent marking. Since we already established that the monolingual word embeddings are far from isomorphic—in contrast with the intuitions motivating previous work Mikolov et al. ; Conneau et al. Differences in morphology, domain, or embedding parameters seem to be predictive of poor performance, but a metric that is independent of linguistic categorizations and the characteristics of the monolingual corpora would be more widely applicable. Recall that our graph similarity metric returns a value in the half-open interval [0,∞). The correlation between BDI performance and graph similarity is strong (ρ∼0.89).
S4SS6SSS0Px1 The results indicate that performance on verbs is lowest across the board. This is consistent with research on distributional semantics and verb meaning Schwartz et al. ; Gerz et al.
S4SS6SSS0Px3 Homographs Since we use identical word forms (homographs) for supervision, we investigated whether these are representative or harder to align than other words. , e.g., many proper names; (b) source words that have homographs that are not homonyms in the target language, e.g., many short words; and (c) other words. Somewhat surprisingly, words which have translations that are homographs, are associated with lower precision than other words. This is probably due to loan words and proper names, but note that using homographs as supervision for alignment, we achieve high precision for this part of the vocabulary for free.
For example, a pair in subject-verb agreement category would be: (The author laughs, The author laugh). We encode both the grammatical and ungrammatical sentences into the latent codes z+ and z−, respectively. Then we condition the decoder on the z+ and try to determine whether the decoder assigns higher probability to the grammatical sentence (denoted by x+): p(x−|z+)
Now we run our experiments on the template-based synthetic data. As stated, this dataset is used to measure biases in the model since it is unbiased towards identities. We use AUC along with False Positive Equality Difference (FPED) and False Negative Equality Difference (FNED), which measure a proxy of Equality of Odds Hardt et al. FPED sums absolute differences between overall false positive rate and false positive rates for each identity term. FNED calculates the same for false negatives. Our method provides substantial improvement on AUC and almost completely eliminates false positive and false negative inequality across identities. The fine-tuned model also outperforms the baseline for mitigating the bias. The token replacement method comes out as a good baseline for mitigating the bias since it treats all identities the same. The importance weighting approach fails to produce an unbiased model.
We follow previous approaches and report Grammaticality, Meaning Adequacy, and Simplicity individually and combined (AVG is the average of the three dimensions). In addition, we include a new metric Minimum, which is the (average) minimum value of Grammaticality, Meaning Adequacy, and Simplicity per sentence. We include Minimum because we argue that a simplification is only as good as its weakest dimension. We note that it is trivial to produce a sentence that is perfectly adequate and fluent, by simply repeating the source sentence. It is also easy to produce a simple sentence if we do not care about adequacy. We evaluated CROSS (and CROSS-Lex, CROSS-Syn variants) against the two state-of-the-art models DMASS and DRESS-Ls as well a Transformer baseline. We also elicited judgments on the gold standard Reference as an upper bound. CROSS significantly outperforms all other models for both Min and Simplicity. Transformer achieves a higher score for both Grammaticality and Meaning compared to CROSS. However, this can be explained due to the high Copy score, which therefore guarantees high Grammaticality and Adequacy scores. This can also in part explain the high Grammaticality and Meaning Adequacy scores for DRESS-Ls. CROSS-Syn achieves lower scores compared to CROSS-Lex, suggesting that syntactic changes are not as important for WikiLarge. Human evaluation on Newsela CROSS and DRESS-Ls both achieve the highest Minimum scores. For all other metrics, CROSS is better or the same than all other models. CROSS and CROSS-Syn achieve similar results, both outperforming CROSS-Lex. This suggests that syntactic simplifications are more prominent in Newsela compared to WikiLarge.
For both datasets we see that participants perceive differences between the output of the simple and XSimple models (this is also reflected in the FKGL which is lower for XSimple). For WikiLarge, all scores apart from simplification do not differ significantly. For Newsela, we see that XSimple sentences are significantly less adequate and grammatical. However, on average Simple and XSimple sentences do not significantly differ, showing a trade-off between simplicity and adequacy/grammaticality.
We include several state-of-the-art baselines which had not been compared against one another as they have been proposed independently over a short time period: hierarchical Bi-LSTM Koshorek et al. and Bi-LSTM+CRF+ELMO Wang et al. We estimate standard deviations for our proposed models and were able to calculate those from the hierarchical Bi-LSTM, whose code and trained checkpoint were publicly released. And while the BERT checkpoints were pre-trained using (among other things) a next-sentence prediction task, it was not clear a priori that our cross-segment BERT model would be able to detect much more subtle semantic shifts. To further evaluate the effectiveness of this model, we tried using longer contexts. In particular, we considered using a cross-segment BERT with 255-255 contexts, achieving 67.1 F1, 73.9 recall and 61.5 precision scores. Therefore, we can see that encoding the full document in a hierarchical manner using transformers does not improve over cross-segment BERT on this dataset. This suggests that BERT self-attention mechanism applied across candidate segment breaks, with a limited context, is in this case just as powerful as separately encoding each sentence and then allowing a flow of information across encoded sentences. In the next section we further analyze the impact of context length on the results from the cross-segment BERT model. Overall, the larger the model, the better the performance. These experiments also suggest that the configuration also matters, in addition to the size. A 128-dimensional model with more layers can outperform a 256-dimensional model with fewer layers. This is unsatisfactory, as large size hinders the possibility of using the model at scale and with low latency, which is desirable for this application Wang et al. In the next section, we explore smaller models with better performance using model distillation.
We can see that the distilled models perform better than models trained directly on the training data without a teacher, increasing F1-scores by over 4 points. We notice that distillation allows much more compact models to significantly outperform the previous state-of-the-art. Unfortunately, we cannot directly compare model sizes with Koshorek et al. It is however fair to say their hierarchical Bi-LSTM model relies on dozens of millions of embedding parameters (even though these are not fine-tuned during training) as well as several million LSTM parameters.
For the goal of measuring compositional generalization as accurately as possible, it is particularly interesting to construct maximum compound divergence (MCD) splits, which aim for a maximum compound divergence at a low atom divergence (we use DA≤0.02). >N. For CFQ, we use N=7 constraints. For scan, we use N=22 actions. Input length: Variation of the above setup, in which the train set consists of examples with input (question or command) length ≤N, while test set consists of examples with input length >N. For CFQ, we use N=19 grammar leaves. For SCAN, we use N=8 tokens. Output pattern: Variation of setup described by Finegan-Dollak et al. Query patterns are determined by anonymizing entities and properties; action sequence patterns collapse primitive actions and directions. Input pattern: Variation of the previous setup in which the split is based on randomly assigning clusters of examples sharing the same input (question or command) pattern. Question patterns are determined by anonymizing entity and property names; command patterns collapse verbs and the interchangeable pairs left/ right, around/opposite, twice/thrice. Interestingly, the MCD splits still correlate with the aspects of compositional generalization that are targeted by the other experiments in this table. However, these correlations are less pronounced than for the experiments that specifically target these aspects, and they vary significantly across the different MCD splits. The main relation we are interested in is the one between compound divergence of the data split and accuracy. Specifically, we compute the accuracy of each model configuration on a series of divergence-based splits that we produce with target compound divergences that span the range between zero and the maximum achievable in 0.1 increments (while ensuring that atom divergence does not exceed the value of 0.02).
While the exact subquery “What sibling” does not occur at training, the two words have been shown separately in many instances: the subqueries “sibling of Mx”, and “Mx’s parent” occur 2,331 and 1,222 times, respectively. We can analyze this example in more detail by comparing parts of the rule tree of this example with those shown at training. As can be read from the table, similar sentences have been shown during training. Some examples are: What was executive produced by and written by a sibling of M0? What costume designer did M1’s parent employ? What cinematographer was a film editor that M2 and M3 married? What film director was a character influenced by M2?
Max model along with smoothing outperforms the baseline method under all three test sets with the BLEU score [
We obtain annotations for 133K words and 4.5K sentences. The average sentence vagueness score is 2.4±0.9. As of inter-annotator agreement, we find that 47.2% of the sentences have their vagueness scores agreed by 3 or more annotators; 12.5% of the sentence vagueness scores are agreed by 4 or more annotators. Furthermore, the annotators are not required to select vague words if they believe the sentences are clear. We remove vague words selected by a single annotator. Among the rest, 46.1% of the words are selected by 3 or more annotators; 18.5% of the words are selected by 4 or more annotators. These results suggest that, although annotating vague terms and sentences is considered challenging, our annotators can reach a reasonable degree of agreement. Note that we obtain a total of 1,124 unique vague terms, which go well beyond the 40 cue words used for sentence preselection. the percentages of sentences containing different numbers of vague words, and (ii) the percentages of sentences whose vagueness scores fall in different ranges.
(yan2020towards), Batch Renormalization (BRN) (ioffe2017batch), and Group Normalization (GN) (wu2018group). We can observe that BRN/MABN/PN-V is better than BN but worse than LN, which suggests the small batch-size setting (main focus of (yan2020towards; ioffe2017batch; wu2018group)) may have similar characteristic of the setting in NLP, where there exists large variance across batches. Obviously, GN performs the best among the previous proposed methods given LN can be viewed as the special case of GN (group number as 1).
, we evaluated different algorithms primarily based on their speed of convergence and translation output quality. The dataset used was English-German property descriptions with one million parallel sentences. Our SGD decay strategy is based on a combination of the perplexity score and epoch number, meaning we decay current learning rate by a multiplicative factor of 0.7 if current epoch’s validation perplexity does not decrease, and after each epoch after the 9th epoch. Our initial learning parameters for SGD, Adam, Adagrad and Adadelta are 1.0, 0.0002, 0.1, and 1.0 respectively. We ran the model for 20 epochs and used both perplexity per epoch and BLEU score after every five epochs on the validation set of 10,000 sentences to measure the performance. The perplexity reached by SGD in the 9th epoch was already achieved by Adam in the 6th. But from the 10th epoch onward, as soon as SGD learning rate starts decaying indefinitely, Adam’s perplexity is consistently worse than that of SGD. However, there was no decrease in perplexity from 15th till 20th epoch, so SGD already converged by epoch 15. We also observed that Adagrad performed very poorly on our model. Adadelta was much better than Adagrad but still slightly behind Adam and SGD. We further validated our results using BLEU scores every 5 epochs. The results were mostly consistent with what we observe by looking at perplexity. In terms of time taken per epoch, SGD was the fastest. Adam was about 10% slower in comparison.
In the Primitive task, the proposed method beats human performance. This indicates that the proposed method is able to learn compositionality from only a few samples. To our best knowledge, this is the first time machines beat humans in a few-shot learning task, and it breaks the common sense that humans are better than machines in learning from few samples.
And system (a) with Chinese templates achieves the best results among all methods, which means that the PPP + S-DTW system is suitable in a single language QbE-STD task. In this work, the hyper-parameter of the variability-invariant loss is fixed at 0.8, according to our preliminary experimental results. We can see that system (g) has a considerably better result on English (both L1 and L2) keywords than Chinese keywords. The loss is employed on instances with the same word label but different speaker labels. In the English scenario, it minimizes the difference within a word between not only speakers but also accents.
It even deteriorated performance significantly, when the model received information trough an oracle, that is the concatenation of hard targets with the model’s hidden representations for each input token at the last transformer layer. In the version where subjectivity classification was performed with respect to (q,c ) input sequences performance did also decrease, but this time slightly rather than catastrophically. w.r.t. the concatenation). None of the implemented sequential transfer models could contribute to an increase in F1.
We experiment with four academic corpora: CORA, Arxiv-Physics, and PNAS datasets contain abstracts only, and the locations of the citations within each paper are not preserved, whereas the Citeseer dataset contains the citation locations. For CORA, Arxiv-Physics, and PNAS, we lemmatize words, remove stop words, and discard words that occur fewer than four times in the corpus. Note that we obtain citation data from the entire document, not only from the abstract. Also, we consider within-corpus citation only, which leads to less than 13 average citation counts per document for all corpora.
We use a simple stratified randomization algorithm from scikit-learn as a baseline system. The proposed algorithm (SVM classifier) achieves a better performance overall in the F-Score of 0.8204.
We first notice that in both datasets the full-text version of TfIdf is much better than the abstract version. This is no surprise, as abstracts do not contain enough text to enable the separation of keyphrases from non-keyphrases in contrast with the full-text of articles. It appears that, despite their smaller size, abstracts capture adequately the co-occurrence (proximity) of words that is necessary for graph creation, avoiding at the same time the noise (lots of unimportant words) in the full-texts. We focus on the best versions of TfIdf and the 4 graph-based approaches in the rest of this section.
Supervised SPARQL achieves an overall 35.15% accuracy, which is the best of all machine results. However, it is still very far from the human performance, 97.5%, indicating that even with the additional information of programs and SPARQLs, KQA Pro is still very challenging. Weakly-Supervised Program cannot converge due to the huge search space (i.e., dozens of functions and thousands of function inputs), even though we have applied many tricks like grammar restrictions. SRN is not able to handle literal knowledge, so we select questions containing only entities and predicates for its training and testing, including 5004 and 649 questions respectively. RGCN achieves the best performance in Multi-hop and High-level questions, demonstrating its strong reasoning capbility. However, it is weak in quantitative questions, i.e.,multiple-choice setting Comparison and Count. For these questions, models using programs or SPARQLs perform much better, because they handle quantities explicitly.
6B, 27B and 840B are well-known GloVe embeddings Pennington et al. GN Mikolov et al. As hypothesized, we see improvements across tasks, datasets, and model architectures when multiple embeddings are concatenated (except for Ontonotes). When compared to a model that only uses a single pre-trained embedding, a model that uses the concatenation of pre-trained and randomly initialized embeddings does 0.6% worse on average, demonstrating that the performance gains are from the combination of different pretrianed embeddings rather than the increase in the number of parameters in the model. In some cases we were able to improve results further by adding several sets of additional embeddings.
We investigate the effect of constrained decoding on three NER datasets and one Slot Filling dataset. In three out of four datasets constrained decoding performs comparably or better than CRF, again Ontonotes is the only exception. We observe a 50% improvement in training time on average.
Clearly the phrase-based SMT system still shows the superior performance over the proposed purely neural machine translation system, but we can see that under certain conditions (no unknown words in both source and reference sentences), the difference diminishes quite significantly.
Clearly the phrase-based SMT system still shows the superior performance over the proposed purely neural machine translation system, but we can see that under certain conditions (no unknown words in both source and reference sentences), the difference diminishes quite significantly.
StockTwits is based in the United States of America (USA) where it is a known trading and information-sharing platform. Similarly, Ford, GM, and Fiat-Chrysler are also based in the USA which can indicate why these companies receive more attention than the remaining. The least covered entities are the French companies, Peugeot and Renault. As we collected reports, news, and stocktwits only in English , we believe that the little reporting on these targets, as well as low interest from USA users are the reason for the low coverage. Regarding the sentiment distribution, we can see that company reports are mainly positive. We hypothesize this is related to the marketing strategy employed by companies to ensure positive reporting, even when the results are poor. Given the prevailing topic of the Diesel scandal concerning Volkswagen, a cut of the profit forecast for Ford, and a change of the chief executive officer (CEO) regarding Daimler, company reports might be seen as an instrument to improve the public perception.
Overall, the sentiment annotation distribution was comparable among the annotators, with the exception of Annotator 1 in the company reports. In contrast, relevance annotation was far more heterogeneous which we attribute to the task difficulty, mainly due to the point of view interpretation. For example, the text “Mercedes-Benz USA Announces Senior Management Appointments” had one of the highest discrepancies; annotators 1 and 3 scored the relevance close to zero, and annotator 2 as one. Given that the annotation is performed on the entity Daimler, annotators 1 and 3 might have seen the new appointments, in the USA branch, of less importance to Daimler while annotator 2 potentially interpreted that senior management changes are relevant to the company. The inter-annotator agreement (IAA) was calculated utilising Fleiss’ Kappa (fleiss1971measuring). As the annotation process was performed on a continuous scale, we transformed the sentiment scores into the polarity values (positive > 0, negative < 0, neutral = 0) and the relevance into three categories: low-relevance [0.0,0.25], medium-relevance [0.25,0.75], and high-relevance [0.75,1.0].
Having three annotations per text in place, the FinLin dataset was consolidated either automatically or manually by an additional annotator (i.e. consolidator). The consolidation was conducted with AWOCATo, the same tool used for annotation. Manual consolidation occurred when (1) at least one of the annotations had a difference higher than the 3rd quantil to the mean of all annotations, or (2) when at least one of the annotators was not able to annotate a given text, or (3) when at least one of the annotations has a different polarity than the remaining.
Both methods proposed in this paper outperform UMFS-WE. Our WordNet-based Comp2Sense method has a higher MFS detection accuracy than either of the methods based on word embeddings, UMFS-WE and WCT-VEC. This result confirms the utility of the concept of word companions, and provides strong support to our hypothesis that they convey a strong signal about the word’s MFS. Again, the random selection baseline is outperformed by all other systems. Interestingly, both Comp2Sense and MKWC04 outperform the vector-based methods. This highlights the particularly strong performance of the jcn similarity measure on nouns as compared to other parts of speech.
WCT-VEC is the top-performing unsupervised method on the development set (SE2), two of the other data sets (SE3 and S07), and is within 1% of the best result on the other two data sets (S13 and S15). Another interesting observation is that all three MFS-based approaches outperform the Leskext method.
For intrinsic evaluation, using only the MFT vector gives better results than using only the companions vector; on the extrinsic evaluation, the reverse is true. This once again shows the potential for disagreement between intrinsic and extrinsic evaluations of MFS detection systems. This shows that WCT-VEC ultimately has lower information requirements compared to prior work, and is applicable for low-resource settings.
On test set #1, our Recall score outperforms the best, Cresci et al. On test set #2, our F-Measure surpasses the best, Cresci et al. ; the Accuracy score is the same as the best, Cresci et al. Most of our other scores are comparable to those of existing work. The low values of F-Measure and Mathews Correlation Coefficient (MCC), respectively smaller or equal to 0.435 and 0.174, are mainly due to the low Recall. In turn, this represents a tendency of predicting social bots as genuine accounts. Low values of both Precision and Recall mean incomplete and unreliable bot detection.
Note that in order to get a fair time performance comparison, we trained the models with the same batch size (4096) and on the same GPU. These numbers are for the best performing model (in terms of evaluation loss and selected using the early stopping method) for each of the sequence modelling methods. Time to Convergence (T2C) shows the approximate time that the model was trained to converge. We also show the loss in the development set for that specific checkpoint.
To analyze the impact of different imposter speaker cohorts, we analyze the HPM domain-balanced System 5 with different s-norm configurations. The imposter cohort is restricted to the top-40 most similar imposters for all experiments.
As shown in the table, the reranker, with only one-fifth of the real training data, perform better than both the LSTM and ngram language models. When the reranker is merged with the LSTM language model, the performance improves further. We believe that is due to the bigger size of the training data for the LSTM model. The final ensemble result achieves an absolute 0.44% improvement compared to the LSTM language model on the test data. A possible explanation of this result is the difference in quality of the n-best hypotheses between heldout and test sets. Recall that the speech for heldout set is collected from mobile phones whereas the test set data is collected from a distant device.
MAKING NEURAL MACHINE READING COMPREHENSION FASTER This study aims at solving the Machine Reading Comprehension problem where questions have to be answered given a context passage. The challenge is to develop a computationally faster model which will have improved inference time. State of the art in many natural language understanding tasks, BERT model, has been used and knowledge distillation method has been applied to train two smaller models. The developed models are compared with other models which have been developed with the same intention. Table of Contents
Although contrastive learning improves the robustness of ViLBERT on Ref-Adv (+1.4% and +2.5% for Sum-H and Max-H respectively), it comes at a cost of slight performance drop on the full test (likely due to sacrificing biases shared between training and test sets). Moreover, the performance of MTL on Ref-Hard and Ref-Adv are similar, suggesting that the model generalizes to unseen data distribution. These suggest that the MTL model is sensitive to linguistic structure. However, there is still ample room for improvement indicated by the gap between Ref-Easy and Ref-Hard (12.4%).
\newcite CirikMB18 observed that existing models for RefCOCOg are relying heavily on the biases in the data than on linguistic structure. We perform extensive experiments to get more detailed insights into this observation. Specifically, we distort linguistic structure of referring expressions in the RefCOCOg test split and evaluate the SOTA models that are trained on original undistorted RefCOCOg training split. Similar to Cirik et al. , we distort the test split using two methods: (a) randomly shuffle words in a referring expression, and (b) delete all the words in the expression except for nouns and adjectives. Except for the ViLBERT modelLu et al.
We observe that while conceptually simple, this method outperforms existing work on the NaturalQuestion and TriviaQA benchmarks. In particular, generative models seem to perform well when evidence from multiple passages need to be aggregated, compared to extractive approaches. Our method also performs better than other generative models, showing that scaling to large number of passages and processing them jointly leads to improvement in accuracy. Second, we observe that using additional knowledge in generative models by using retrieval lead to important performance gains. On NaturalQuestions, the closed book T5 model obtains 36.6% accuracy with 11B parameters, while our approach obtains 44.1% with 770M parameters plus Wikipedia with BM25 retrieval. Both methods use roughly the same amount of memory to store information, indicating that text based explicit memories are competitive for knowledge retrieval tasks. In the next section, we investigate how the Fusion-in-Decoder method scales with respect to the number of retrieved passages.
thereof perform best. The experiments were conducted on the development set (PMB v2.2.0). We show a basic version of our seq2graph model with word embeddings, to which we add information about rank (+restrict). We also experimented with the full gamut of additional features (+feats) as well as with ablations of individual feature classes. For comparsion, we also show the performance of a graph-to-string model (seq2seq+copy).
On both evaluation matrices, the causal decoder models outperform the encoder-decoder models. We observe that the encoder-decoder model tends to overlook dialogue context and generate digressive responses. (Generated samples are available in Appendix D) We hypothesize that this is because the one-to-many problem zhao2017learning in open-domain conversation weakens the relation between encoder and decoder; thus the well pre-trained decoder (Bert) easily converges to a locally-optimal, and learns to ignore the dialogue context from the encoder and generate the response in an unconditional language model way. We leave the investigation of this problem to future work. On the other hand, M-CausalBert achieves a comparable or slightly better performance compared to CausalBert, which suggests that M-CausalBert leverages the data from other languages. As expected, we observe a significant gap between the cross-lingual model and other models, which indicates that cross-lingual zero-shot conversation modeling is very challenging.
The results illustrate that Multi outperforms Mono in English and Chinese, and is on par with Mono in other languages. On the other hand, Poly shows a strong performance in English as it was pre-trained with a large-scale English conversation corpus. In contrast, the performance of Poly drops in other languages, which indicates that the imperfect translation affects translation-based systems. We also conduct M-CausalBert (Multi) against XNLG (cross) human evaluation, and Multi achieve nearly 100 percent winning rate.
However, the compressed BoW (trained with L1 with negligible loss of performance) model uses 6932 (resp. 6999) for health (resp. music), which become intractable for humans to understand, while only 24 (resp. 27) weights are used for dictionaries. When we force BoW model to use the same number of words as dictionaries (189 and 185 words), we see that BoW performs much worse, as expected. The top 100 BoW words (sorted by weight magnitude) are easy to interpret. After this, the effect of removal or addition of words is very difficult to predict. Dictionaries are semantically linked to “feature blindness”. The effect of removal or addition is more predictable because it is linked to being the last feature that allows the model to distinguish two subsets of examples of different classes.
To quantify this difference, we compute the entropy of the answer distribution A for each property p, scaled to the [0,1] range by dividing by the entropy of a uniform distribution with the same number of values, i.e., ^H(p)=H(Ap)/log|Ap|. Properties that represent essentially one-to-one mappings score near 1.0, while a property with just a single answer would score 0.0. We label properties with an entropy less than 0.7 as categorical, and those with a higher entropy as relational. Categorical properties cover 56.7% of the instances in the dataset, with the remaining 43.3% being relational.
Ten entries were submitted by five teams and are ordered by BLEU score. TER is reported for all submissions which achieved BLEU score greater than 5.0. The type column specifies the type of system, whether it is a Primary (P) or Constrastive (C) entry.
We can observe that feature level fusion with PPP feature improves the performance. Compared to the MFCC i-vector subsystem ( EER = 6.63%) , the EER of MFCC-PPP i-vector subsystem is reduced to 1.06%. On the other hand, the openSMILE feature outperformed the MFCC i-vector subsystem which might be due to the inclusion of prosodic level information.
Because of the randomness of deep neural network on the small-scale dataset, we just train policy-based agents for these 10 relation types. First, compared with Original case, most of the Pretrain agents yield obvious improvements : It not only demonstrates the rationality of our pre-training strategy, but also verifies our hypothesis that most of the positive samples in Riedel dataset are true positive. More significantly, after retraining with the proposed policy-based RL method, the F1 scores achieve further improvement, even for the case the Pretrain agents perform bad. These comparable results illustrate that the proposed policy-based RL method is capable of making agents develop towards a good direction.
This confirms the hypothesis that, even for humans, it is difficult to judge whether a response is truly appropriate if only the previous dialog history (dialog context) is used. But, given the following user turn, it becomes significantly easier. This result also implies that the following user turn contains salient information that helps distinguish between “good” system responses versus “mediocre” or “bad” responses, which are rarely used in standard supervised-learning-based training for end-to-end neural dialog systems.
The best performing model is able to achieve 65.8% accuracy and MAE 0.67, which represents less than 1 error to the ground truth expectation. Further, the results yield the following insights: The most useful features to predict Susr stress the importance of understanding user feedback. This opens up the possibility of having a universal user satisfaction model that solely takes the user response as its input. This is easier to obtain for training than manually labelling every context-response-user response tuple. Regression is more suitable than classification for this task. Therefore, regression naturally takes care of the ordinal nature of Susr.
Pre-trained Word data set embedding of 300 dimensions have been used as the basic block for text processing on both the encoder and decoder side. The word embedding size is decreased by reducing the dimensions. This makes the use of word data set embedding feasible in devices that have constrained on their memory. Principal Component Analysis (PCA) based dimensionality decrease with some post processing steps have been utilized to develop word data set embedding of lower dimensions of 150. Translation results are produced in Table.
The database is a selection of course transcripts from Coursera, one of the largest Massive Open Online Course (MOOC) platforms. To ensure the author detection less replying on the domain information, 16 courses were selected from one specific text domain of the technical science and engineering fields, covering 8 areas: Algorithm, Data Mining, Information Technologies ( IT), Machine Learning, Mathematics, Natural Language Processing (NLP), Programming and Digital Signal Processing (DSP). For privacy reason, the exact course titles and instructor (author) names are concealed. However, for the purpose of detecting the authors, it is necessary to point out that all courses are taught by different instructors, except for the courses with IDs 7 and 16. This was done intentionally to allow us to investigate how the topic variation affects performance.
We first implemented a traditional vanilla Seq2Seq model, which we call a deterministic encoder-decoder (DED), and generally replicated the results on the question generation task as reported in \newciteQgen, showing that our implementation is fair. Incorporating attention mechanism in this model (DED+DAttn) improves BLEU scores, as expected. In the variational encoder-decoder (VED) framework, we report results obtained by both max a posterior (MAP) inference as well as sampling. In the sampling setting, we draw 10 samples (z and/or a) from the posterior given x for each data point, and report average BLEU scores. The proposed variational attention model (VED+VAttn) largely outperforms deterministic attention (VED+DAttn) in terms of all diversity metrics. It should be further mentioned that, with a milder γa (e.g., 0.01), VED+VAttn outperforms VED+DAttn in terms of both quality and diversity (on the validation set).
We use the evaluation package released by \newciteChen2015MicrosoftCC to compute BLEU-1 and BLEU-4 scores. The oracle improvement, by selecting the sentence from the list that maximizes the BLEU-4 score is 15.2. However, its worth noting that an increase in well-formedness doesn’t guarantee an improved BLEU score, as the oracle sentence maximizing the BLEU score might be fairly non-wellformed Callison-Burch et al. For example, “who was elected the president of notre dame in?” has a higher BLEU score to the reference “who was the president of notre dame in 1934?” than our well-formed question “who was elected the president of notre dame?”.
For link prediction IDNE performs best on three networks, showing its capacity to learn meaningful word and topic representations according to the network structure. For classification, LSA and GVNR-t achieve the best results while IDNE reaches similar but slightly lower scores on all datasets. On the contrary, TADW and Graph2gauss show weaknesses on NYT and Gaming SE.
Each WSU label is represented by a 512-dimensional embedding vector. The decoder is a uni-directional GRU-RNN with 2 hidden layers, each with 512 hidden units, and an output layer predicting posteriors of the 33k WSU. We use GRU instead of long short-term memory (LSTM) We first perform KLD adaptation of the SI AED with different ρ by updating all the parameters in the SD AED. Direct re-training is performed with on regularization when ρ=0. relative WER improvements over the SI baseline. The WER increases as ρ continues to grow. For unsupervised adaptation, KLD achieves the best WERs, 14.04% (ρ=0.2) and 14.00% (ρ=0.5), for 100 and 200 adaptation utterances, which improve the SI AED by 2.0% and 2.2% relatively. More adaptation utterances significantly improves the supervised adaptation but only slightly reduces the WER in unsupervised adaptation since the decoded one-best path is not as accurate as the forced alignment. For unsupervised adaptation, ASA achieves the best WERs, 13.95% and 13.89%, both at α=0.5 with 100 and 200 adaptation utterances, which improves the SI AED baseline by 2.6% and 3.0% relatively. ASA consistently and significantly outperforms KLD for both supervised and unsupervised adaptation and for adaptation data of different sizes. Especially, for supervised adaptation, ASA achieves 5.5% and 4.3% relative improvements over KLD with 100 and 200 adaptation utterances, respectively. For unsupervised adaptation, MTL achieves best WERs, 13.80% and 13.77%, both at β=0.8, which are 3.6% and 3.8% relative improvements over the SI AED baseline, respectively. Note that the performance of MTL adaptation is not comparable with that of KLD and ASA since in MTL, only the encoder (consisting of 32.4% of the whole AED model parameters) is updated while in KLD and ASA, the whole AED model is adapted. The KLD and ASA performance can be remarkably improved by updating only a portion of the entire model parameters.
parameters.(The number of parameters in our models is approximately 10K while in CNN the number of parameters is about 400K). Compared with RecNN, DC-RecNN performs better, indicating the effectiveness of the dynamic compositional function. Additionally, both DC-RecNN and DC-TreeLSTM achieve substantial improvement on IE dataset, which covers the richness of compositionality (idiomaticity). We attribute the success on IE to its power in modeling more complicated compositionality.
For fair comparison, we train our models with the same setting. We can see both DC-RecNN and DC-TreeLSTM outperform competitor models, in which DC-RecNN (DC-TreeLSTM) achieves 3% (2.7%) improvements than RecNN (TreeLSTM). We think this breakthrough is basically attributed to the dynamic compositional mechanism, which enables our models to capture various syntactic patterns (As we will discuss later) therefore can more accurately understand sentences.
We further run a human evaluation to measure the text-content consistency among different models. 100 source text are randomly sampled from the human-written DUC 2004 data for task 1&2 Over et al. Bo. Up, SS, RS and VRS are applied to generate the target text by first sampling a selection mask, then run beam search decoding with beam size 10. We are interested in seeing (1) if multiple generations from the same selection mask are paraphrases to each other (intra-consistent) and (2) if generations from different selection masks do differ in the content they described (inter-diverse). RS has the lowest score on both because the selector has very weak effects on the generation as measured in the last section. Bo. Up and SS lay between them. Overall VRS is able to maintain the highest content-text consistency among them. Performance & Trade-off: To see if the selector affects performance, we also ask human annotators to judge the text fluency. The fluency score is computed as the average number of text being judged as fluent. We include generations from the standard Enc-Dec model. Imposing a content selector always affects the fluency a bit. The main reason is that when the controllability is strong, the change of selection will directly affect the text realization so that a tiny error of content selection might lead to unrealistic text. If the selector is not perfectly trained, the fluency will inevitably be influenced. When the controllability is weaker, like in RS, the fluency is more stable because it will not be affected much by the selection mask. For SS and Bo. Up, the drop of fluency is significant because of the gap of soft approximation and the independent training procedure. In general, VRS does properly decouple content selection from the enc-dec architecture, with only tiny degrade on the fluency.
As a baseline, we use Cosine-Similarity [song2016two, zhou2018multi, nogueira2019passage], which selects the agent response R_{t}^{(j)} that is closest to the query, measured using the contextualised sentence embeddings for the agent responses and the query. Specifically, averaged pooling over individual contextualised word embeddings in a sentence is used as the sentence embedding for that sentence. We denote this baseline as Cosine-Similarity(Query, Response). To consider context within this baseline, denoted as Cosine-Similarity(Query, Response, Context), we compute the sentence embeddings for each utterance in the history and average it with the query embedding. Next, we find the agent response that is closest to this vector according to the cosine similarity. Any improvement over cosine similarity baseline shows the effect of learning a response selection model on top of input representation via contextualised embeddings. Therefore, cosine similarity is an informative and useful baseline to comparing against. Moreover, cosine similarity has been used extensively in NLP for text matching [zhelezniak-etal-2019-correlation], it is unsupervised and fast to compute.
Neural language models outperform statistical language models in both datasets by a large margin. In MIMIC-III, the keyword discount is increased by 6 absolute percentage units, from 19.35% to 25.42% (or 31% relative increase). A similar increase was found for accuracy, from 27.34% to 33.97% (or 24% relative increase). In IUXRay, the increase was larger, with 9 absolute percentage units of KD (from 32.30% to 41.12%) and 13 absolute percentage units of Acc (from 38.18% to 51.30%). 3-GLM was the best with both evaluation measures in MIMIC-III. In IUXRay, 4-GLM was found as the best in terms of Acc and 5-GLM in KD. We obtained better performance for IUXRay due to its smaller vocabulary size compared to MIMIC-III.
We show that our proposed HDE graph based model improves the published state-of-the-art accuracy on development set from 67.1% Kundu et al. Compared to the best single model “DynSAN” (unpublished) on WikiHop leaderboard, our proposed model is still 0.5% worse. Compared to two previous studies using GNN for multi-hop reading comprehension Song et al. , our model surpasses them by a large margin even though we do not use better pre-trained contextual embedding ELMo Peters et al.
very good (single reference scoring at 41 and oracle reference at 70) but most hypotheses in the beam are close to a reference translation (as the difference between oracle reference and average oracle is only 5 BLEU points). Unfortunately, beam hypotheses lack diversity and are all close to a few references as indicated by the coverage number, which measures how many distinct human references are matched to at least one of the hypotheses. In contrast, hypotheses generated by sampling exhibit opposite behavior: the quality of the top scoring hypothesis is lower, several hypotheses poorly match references (as indicated by the 25 BLEU points gap between oracle reference and average oracle) but coverage is much higher. This finding is again consistent with the previous observation that the model distribution is too spread in hypothesis space. We conjecture that the excessive spread may also be partly responsible for the lack of diversity of beam search, as probability mass is spread across similar variants of the same sequence even in the region of high likelihood. This over-smoothing might be due to the function class of NMT; for instance, it is hard for a smooth class of functions to fit a delta distribution (e.g., a source copy), without spreading probability mass to nearby hypotheses (e.g., partial copies), or to assign exact 0 probability in space, resulting in an overall under-estimation of hypotheses present in the data distribution.
For the utterance-level evaluation, all teams but two including us obtained an F1-score below 0.35, which reflects the difficulty of the task. Team 4 reached 0.4481, while our best entry scored 0.5306. By the same token, for the subdialog-level evaluation, all teams but two including us obtained an F1-score below 0.40. Team 4 reached 0.5031, while our best entry scored 0.5786.
These results suggest that there is not a single classifier that performs best in all cases; this is most likely due to the diversity of events. However, we see that the LSTM is the classifier that outperforms the rest in the greater number of cases; this is true for three out of the eight cases (the difference with respect to the second best classifier being always statistically significant). Moreover, sequential classifiers perform best in the majority of the cases, with only three cases where a non-sequential classifier performs best. Most importantly, these results suggest that sequential classifiers outperform non-sequential classifiers across the different events under study, with LSTM standing out as a classifier that performs best in numerous cases using only local features.
If we look at the correct guesses, highlighted in bold in the diagonals, we see that the LSTM clearly performs best for three of the categories, namely support, deny and query, and it is just slightly behind the other classifiers for the majority class, comment. Besides LSTM’s overall superior performance as we observed above, this also confirms that the LSTM is doing better than the rest of the classifiers in dealing with the imbalance inherent in our datasets. For instance, the Deny category proves especially challenging for being less common than the rest (only 7.6% of instances in our datasets); the LSTM still achieves the highest performance for this category, which, however, only achieves 0.212 in accuracy and may benefit from having more training instances.
Our system with Kernel transition module outperforms all other systems in terms of all metrics on both two tasks, expect for R20@3 where the system with PMI transition performs best. The Kernel approach can predict the next keywords more precisely. In the task of response selection, our systems that are augmented with predicted keywords significantly outperform the base Retrieval approach, showing predicted keywords are helpful for better retrieving responses by capturing coarse-grained information of the next utterances. Interestingly, the system with Random transition has a close performance to the base Retrieval model, indicating that the erroneous keywords can be ignored by the system after training.
Finally, we would like to see whether English fine-grained entity typing data can be used to improve the performance on Chinese data. We experiment transfer learning with Babylon word embedding [smith2017offline] between English and Chinese. We first trained the Ultra-fine dataset on English with the English Babylon word embedding. We then extract the weights of the BiLSTMs and continue training on our Chinese dataset. We experiment training directly on our crowdsourced dataset and also with our distant supervision data. Since we have relatively small number of crowdsourced examples, we split it by a ratio of 8:1:1 for train, dev and test. When training on the distant dataset, we follow our setup in 3.2, splitting the crowdsourced dataset equally to form the train, dev and test set. All the experiments are conducted with the general type mapping. The result shows improvements under both scenarios. Since most entity typing resources are in English, using transfer learning to improve model performance on low-resource Chinese entity typing tasks is an interesting topic for future work.
We begin by evaluating three training strategies for the key span identification model, using the annotated spans in KGD for training. While training on KGD (from scratch) substantially improves accuracy, we observe that using KGD to fine-tune BiDAF pre-trained on SQuAD results in the best F1 (78.55) and EM (63.99) scores on the Dev set. All subsequent experiments use this fine-tuned model.
We then perform a human evaluation on the CUB dataset by ranking images generated by different models based on (i) Accuracy and (ii) Naturalness. For the evaluation, we randomly selected 8 images and 8 texts from the test set and produced 64 outputs from the CUB dataset for each method. We resized all output images to 128 ×128 to have a fair comparison and prevent the users from evaluating the images based on different resolutions.
We conjectured that a second reason for over-confidence could be the uncertainty of attention. A well-calibrated model must express all sources of prediction uncertainty in its output distribution. Existing attention models average out the attention uncertainty of αt in the input context Ht Thereafter, αt has no influence on the output distribution. We had conjectured that this would manifest as worser calibration for high entropy attentions αt, and this is what we observed empirically. Observe that ECE is higher for high-entropy attention.
Note the single temperature is selected using the same validation dataset as ours. Our ECE is lower particularly for the T2T model. We will show next that our more informed recalibration has several other benefits. (X-axis) and average actual bleu (Y-axis) for WMT + IWSLT tasks on the baseline model and after recalibrating (solid lines). In the same plot we show the density (fraction of all points) in each bin by each method. We use T=100 samples for estimating \sc bleuθ(^y). We can make a number of observations from these results.
Word representation concatenated by a 300 dimensional word2vec-vector (pre-trained from baomoi.com) and two one-hot vectors represent pos tags and chunks, respectively and a 60 dimensional character vector (generated from a bi-lstm network with dropout rate equal 0.3, as shown in Figure 1). To prevent overfitting, we fix dropout rate to 0.5 for both Bi-LSTM layers (as shown in Figure 3). The NER model is trained in 40 epoch. First 20 epochs, the initial learning rate is set at 0.004. In the remaining epochs, it is fixed to 0.0004. The best model obtained when the value of the loss function on validation-set is minimal.
With VLSP 2016 dataset, the experiment achieved state-of-the-art performances on Vietnamese NER task with 95.61% F1-score.
As can be seen, neural models have a clear advantage over the pure word overlap baseline, which performs worst, with an accuracy of 54.4%.
We initialize the policy with 10 batches of dialogs, and then train on another 10 batches of dialogs, both sampled from the policy training set. Following this, the policy weights are fixed, the agent is reset to start with no classifiers, and we test on 10 batches of dialogs from the policy test set. We also compare the effect of ablating the two main groups of features. The learned agent guesses correctly in a significantly higher fraction of dialogs compared to the static agent, using a significantly lower number of questions per dialog.
Since our system employs sequential information (via attention decoding mechanism), we compared it with 4-gram LM decoding of the DNN-HMM inference. the end-to-end model is competitive in clean and better in other conditions. We also put results for train-clean-100 from RETURNN [luscher_rwth_2019] as the strongest hybrid baseline known to us.
We also provided a large-resource results overview for previous works, although we were unable to successfully train a model on a comparable amount of data due to a lack of computing resources. Note that the partition into resource tasks may not be accurate due to not exactly matching data setups. Our best system outperformed previous works for both test-clean and test-other in the low-resource. Our best system a new SotA on train-clean-100 since our approach surpassed any system known before. While superior in the medium-resource setup, the result for test-other did not improve. For large-resource setups, the improvement decreased.
AL relies on the special-casing of t >τ to maintain its intuitive results. This special case is not easily abused by the deterministic wait-k strategies that AL has been used to evaluate thus far, but future adaptive schedules may exploit it. Compare for example two systems in the scenario where |x|=|y|=5: a standard wait-4 system: read 4, write 1, read 1, write 4. a similar system that delays the final read: read 4, write 4, read 1, write 1. The two systems differ only in when they read the final token. Note that they have very similar g values: identical for t=1 and 5, and differing only by 1 for t=2, 3 and 4.
AL relies on the special-casing of t >τ to maintain its intuitive results. This special case is not easily abused by the deterministic wait-k strategies that AL has been used to evaluate thus far, but future adaptive schedules may exploit it. Compare for example two systems in the scenario where |x|=|y|=5: a standard wait-4 system: read 4, write 1, read 1, write 4. a similar system that delays the final read: read 4, write 4, read 1, write 1. The two systems differ only in when they read the final token. Note that they have very similar g values: identical for t=1 and 5, and differing only by 1 for t=2, 3 and 4.
Among the programs generated by the model, a significant portion (36.7%) uses more than one expression. We observe that programs with three expressions use a more limited set of properties, mainly focusing on answering a few types of questions such as “who plays meg in family guy”, “what college did jeff corwin go to” and “which countries does russia border”. In contrast, programs with two expressions use a more diverse set of properties, which could explain the lower performance compared to programs with three expressions.
Document Classification: The classification experiment included training a simple softmax multi-class classifier with a cross-entropy loss function on the 20 Newsgroups dataset. We set the number of topics to 50. It is obvious from the table that competition based autoencoders achieve better results than conventional models, LDA. KATE achieves 70% for all three measurements outperforming NVCTM, K-Sparse, and LDA. However, our SCAT autoencoder outperforms all models achieving 73% scores on all three measurements.
In reference to Tab. However, KLSH-RF still outperforms their Adv-CNN model by 3 pts; further, the performance of Adv-CNN and Adv-Bi-LSTM is not consistent, giving a low F1 score when training on the BioInfer dataset for testing on AIMed. For the latter setting of AIMed as a test set, we obtain an F1 score improvement by 3 pts w.r.t. the best competitive models, RNN & KLSH-kNN. Overall, the performance of KLSH-RF is more consistent across the two evaluation settings, in comparison to any other competitive model.
As one can observe, our model based on the labeled sense clusters significantly outperforms the substring-based baseline and all participating system by a large margin on all domains. For the “Science (Eurovoc)” and “Food” domains our method yields results comparable to WordNet while remaining unsupervised and knowledge-free. Besides, for the “Science” domain our method outperforms WordNet, indicating on the high quality of the extracted lexical semantic knowledge. Overall, the coarse-grained more pruned model yielded better results as compared to fine-grained un-pruned model for all domains but “Science (Eurovoc)”.
It is worth noting that many of the most commented entities have an overall neutral or only very slightly polarized sentiment among Daily Mail readership. And vice versa, Web sites where entities are mentioned the most usually keep a neutral or at most slightly positive or negative sentiment. For example, on Fig. This could be due to the averaging effect over time: Web sites that mention a given entity a lot publish news with different sentiment, about positive and negative events, so the sentiment cancels out over time. This leads to the idea of time series sentiment analysis that could be performed by combining NLP and time series analysis techniques.
To prevent over-fitting, we set a dropout of 0.3 Srivastava et al. β1=0.9, β2=0.999, and ϵ=1e−8.
This is a standard measure for evaluating training speed; it is also implemented in OpenNMT-lua. “Scaling factor” stands for the ratio of “SRC tokens / sec” against that of one GPU. The mini-batch sizes were determined by the available GPU memories. Note that mini-batch sizes were about 4 times when using 4 GPUs compared with those obtained when using 1 GPU. HybridNMT converges faster compared with other methods. Other findings: The perplexities obtained with model parallelism became similar to those of our hybrid parallelism after long runs. Finally, the convergence speed of HybridNMTIF was between those of Hybrid-NMT and the baseline model with model parallelism. This indicates that the proposed hybrid data-model parallel approach is faster than model parallelism, and removing input-feeding leads to faster convergence.
Second, the processing speed and scaling factors of OpenNMT-lua and those obtained from our implementation were similar. These indicate that our implementation is appropriate. with diverse hyperparameters. The beam size was changed from 3 to 18. OpenNMT -lua used the same normalization method of GNMT Wu et al. Its optimal parameters for the development data were as follows: the beam sizes were 6 and 12 for WMT14 and WMT17, respectively; the length normalization values were both 1.0; and the coverage normalization values were both 0. The proposed HybridNMT used the same normalization of Marian Junczys-Dowmunt et al. Its optimal parameters were as follows: the beam sizes were 15 and 12 for WMT14 and WMT17, respectively and the length penalties were 1.0 for both datasets, implying that the model score was divided by the number of target words to get the normalized score.
“3 feats” features are those hand-crafted features we extracted, which are the minimum, mean, and maximum polarity scores of the reviews as explained in Section 3.5. As can be seen, at least one of our methods outperforms the baseline word2vec approach for all the Turkish and English corpora, and all categories. All of our approaches performed better when we used the supervised scores, which are extracted on a review basis, and concatenated them to word vectors. Mostly, the supervised 4-scores feature leads to the highest accuracies, since it employs the annotational information concerned with polarities on a word basis. We found out that the corpus - SVD metric does always perform better than the clustering method. We attribute it to that in SVD the most important singular values are taken into account. The corpus - SVD technique outperforms the word2vec algorithm for some corpora. When we do not take into account the 3-feats technique, the corpus-based SVD method yields the highest accuracies for the English Twitter dataset. We show that simple models can outperform more complex models, such as the concatenation of the three subapproaches or the word2vec algorithm. Another interesting finding is that for some cases the accuracy decreases when we utilise the polarity labels, as in the case for the English Twitter dataset.
Four models with generated noise are evaluated in total: one with uniform noise and another with realistic noise for both Gutenberg and Kotus corpora. With both corpora even the uniformly generated noise can be utilized to improve the OCR system as the models trained on Gutenberg and Kotus corpora achieve +3.79pp and +3.6pp This supports the findings of D’hondt et al.
The samples produced by our proposed network achieved a mean rating of 4.48, while those produced by the original WaveGAN network achieved a mean rating of 3.39. To quantify the observed effect we used the Cohen’s d measure, calculated to be 0.65.
This has become an impediment to improving the performance of current fine-grained typing systems as a majority of mentions in training sets have noisy types (see Table. The larger the target type set, the more severe the loss. So far there is no effective way to automatically create high-quality training data for fine-grained typing.
1. Comparing with the other noise reduction methods. Both typing systems achieve superior performance on all metrics when using PLE and its variant to denoise the training corpus. In particular, PLE improves FIGER’s Accuracy (i.e., Raw) by 33.53% and HYENA’s Accuracy by 26.97% on the BBN dataset. Compared to the best baseline PTE-Min, PLE obtains over 28% improvement in HYENA’s F1 scores and over 13% enhancement in FIGER’s F1 scores on the Wiki dataset. Superior performance of PLE demonstrates the effectiveness of the proposed margin-based loss in modeling noisy candidate types. PLE always outperforms PLE-NoCo on all metrics on both typing systems. It gains performance from capturing type correlation, by jointly modeling the type-type links in the embedding process. In particular, we observe that pruning methods do not always improve the performance (e.g., “All" pruning results in a 11.15% drop in Macro-F1 score on FIGER on the Wiki dataset), since they aggressively filter out subtypes and/or rare types in the corpus, which may lead to low Recall. Most existing fine-grained typing systems use distant supervision to generate training examples and assume that all candidate types so generated are correct. By contrast, our framework instead seeks to remove false positives, denoising the data and leaving only the correct ones for each mention based on its local context. Output of our task, i.e., denoised training data, helps train more effective classifiers for entity typing. Gillick et al. discuss the label noise issue in fine-grained typing and propose three type pruning heuristics. However, these pruning methods aggressively filter training examples and may suffer from low recall (see Table. Unlike this PLL formulation, our problem can be seen as hierarchical classification with partial labels. Existing PLL methods model a single true label for each training example and do not consider label correlation information.
The CNN/Daily Mail dataset [ Each article is associated with several descriptive bullet point highlights. Similar to previous work consists of ∼230k article summary pairs taken from the BBC. consists of ∼1.2M article summary pairs scraped from the Internet Archive. The articles come from a set of 38 publishers and cover diverse topics.
We observe that either of the front verifier boosts the baselines, and integrating both as rear verification works the best. Note that we show the HasAns and NoAns only for completeness. Therefore, the final (ALL) is the best way to show the final performance, as the standard measurement for previous BERT-related MRC models. We see that +both FVs shows the best performance, which we select as our final implementation.
S5SS0SSS0Px3 Comparisons with Equivalent Parameters When using sketching reading module for external verification, we have two parallel modules that have independent parameters. For comparisons with equivalent parameters, we add an ensemble of two baseline models, to see if the advance is purely from the increase of parameters. We see that our model can still outperform two ensembled models. Although the two modules share the same design of the Transformer encoder, the training objectives (e.g., loss functions) are quite different, one for answer span prediction, the other for answerable decision. The results indicate that our two-stage reading modules would be more effective for learning diverse aspects (verification and span prediction) for solving MRC tasks with different training objectives. From the two modules, we can easily find the effectiveness of either the span prediction or answer verification, to improve the modules correspondingly. We believe this is also quite useful for real-world applications.
, we evaluate the combination of different tasks. For each relation, we first conduct the main task (denoted as 1) through implementing a CNN model and show the results in the first row. Then we combine the main task with one of the other three auxiliary tasks (i.e., 1+2, 1+3, 1+4) and their results in the next three rows. The final row gives the performance using all the four tasks (namely, ALL). In general, we can see that when synthesizing all the tasks, our MTL system can achieve the best performance.
\citeauthorrutherford -xue:2015:NAACL-HLT (2015) elaborately select a combination of various lexical features, production rules, and Brown cluster pairs, feeding them into a maximum entropy classifier. They also propose to gather weakly labeled data based on the discourse connectives for the classifier and achieve state-of-the-art results on 4-way classification task. We can see our proposed MTL system achieves higher performance on both accuracy and macro-averaged F1. We also compare the general performance between our MTL system and the Single-task Learning (STL) system which is only trained on Task 1. The result shows MTL raises the Accuracy from 52.82 to 57.27 and the F1 from 37.65 to 44.98. Both improvements are significant under one-tailed t-test (p<0.05).
For a more direct comparison with previous results, we also conduct experiments based on the setting that the task as four binary one vs. other classifiers. Three additional systems are used as baselines. \citeauthorpark2012improving (2012) design a traditional feature-based method and promote the performance through optimizing the feature set. \citeauthorTACL536 \shortciteTACL536 used two recursive neural networks on the syntactic parse tree to induce the representation of the arguments and the entity spans. \citeauthor39260331 \shortcite39260331 first predict connective words on a unlabeled corpus, and then use these these predicted connectives as features to recognize the discourse relations.
Automatic Grammar Checking We followed \citeauthorxu-durrett-compression \shortcitexu-durrett-compression to perform automatic grammar checking using Grammarly. We compare DiscoBert with sentence-based Bert model. ‘All’ shows the summation of the number of errors in all categories. As shown in the table, the summaries generated by our model have retained the quality of the original text.
S4SS5SSS0Px2 Human Evaluation We sampled 200 documents from the test set of CNNDM and for each sample, we asked two Turkers to grade three summaries from 1 to 5. Sent-BERT model (the original BERTSum model) selects sentences from the document, hence providing the best overall readability, coherence, and grammaticality. In some cases, reference summaries are just long phrases, so the scores are slightly lower than those from the sentence model. DiscoBERT model is slightly worse than Sent-BERT model but is fully comparable to the other two variants.
Stanford Multi-domain Dialogue. In fact, for unsupervised evaluation metrics in task-oriented dialogues, we argue that the entity F1 might be a more comprehensive evaluation metric than per-response accuracy or BLEU, as shown in Eric et al. that humans are able to choose the right entities but have very diversified responses. Moreover, human evaluation of the generated responses is reported. We randomly select 200 different dialogue scenarios from the test set to evaluate three different responses. Amazon Mechanical Turk is used to evaluate system appropriateness and human-likeness on a scale from 1 to 5. We also see that human performance on this assessment sets the upper bound on scores, as expected. More details about the human evaluation are reported in the Appendix.
Without any further fine-tuning, we use the same hyper-parameters from our English-to-German experiments (β=0.5 and k=5) to reduce erasure and lag. Our approach is remarkably robust, never reducing BLEU by more than 1 point, consistently improving lag, and always reducing erasure to negligible amounts.
We analyze an assortment of models as well as human subjects on GQA. Baselines include a “blind” LSTM model with access to the questions We can see that they all achieve low results of 17.82%–41.07%. For the LSTM model, inspection of specific question types reveals that it achieves only 22.7% for open query questions, and not far above chance for binary question types.
In each domain, the final translations were much better than the translations for first viable inputs. The average score improves from 3.85±1.44 to 2.77±1.60. Paired t-test showed that the difference is highly statistically significant (p < 0.0001 for 0.75 difference between final and first viable ratings).
In order to build an emotional trigger tagger, we annotated 2,000 tweets in total, and split them into training, development and test sets with ratio 8:1:1. We treat the problem as a sequence labeling task, using Conditional Random Fields for learning and inference with BERT-MRC features [li2019unified]. Comparing with vanilla BERT tagger [devlin2018bert], the BERT-MRC tagger has the strength of encoding the description of the to-be-extracted entities, e.g., what they are worried about. As this description provides the prior knowledge about the entities, it has been shown to outperform vanilla BERT even when less training data is used. In addition to the representation features from BERT-MRC, we also considered the Twitter-tuned POS features [ritter2011named], the dependency features from a Twitter-tuned dependency parsing model [kong2014dependency] and the Twitter event features [ritter2012open].
Surprisingly, we observed performance drops by DistMult-tanh-WV-init. We suspect that this is because word vectors are not appropriate for modeling entities described by non-compositional phrases (more than 73% of the entities in FB15k-401 are person names, locations, organizations and films). The promising performance of DistMult-tanh-EV-init suggests that the embedding model can greatly benefit from pre-trained entity-level vectors.
In many cases the LDA accuracy is quite close to the maximum brute-force accuracy, however in 3 cases we could not get a result for LDA because the top 12 documents did not include representatives from all categories. This is perhaps understandable in the case of the 4-category dataset, "Animal, Film, Company, Village. " In the case of the "Animal, Plant" dataset we see that the brute force accuracy is relatively low, due to the fact that many similar words would be used to describe animals and plants ("species", "family", "habitat", etc), and so this perhaps explains why LDA had a difficult time teasing apart the topics. However in the case of the "Village,Animal" dataset, which gets a maximum brute-force accuracy of 99.79% the same argument clearly cannot be made. This tells us that there is much room for improvement in our method for inferring topics in datasets for the purpose of surfacing good category representatives.
The results are based the official submissions in the evaluation phase. In this subtask, all of the submitted systems managed to improve over the majority class baseline, and several teams achieved similarly good results. Whenever a number of teams achieve the same result with respect to the main evaluation measure, i.e., Accuracy, we rank them according to the F1 score, and then by AvgRec if a tie still appears.
This subtask was more difficult as the majority class baseline was very high due to label unbalance. No team managed to improve over that baseline, but several teams had results that were very close to it.
Tlink The temporal relation component was developed and validated using the i2b2-TRC dataset. Note that only TLINKs that include EVENTs such as Problem, Treatment, Test and TIMEX3 have been considered. Notably, and comparably (i.e., versus EVENT and TE recognition tasks), it is apparent that TLINK identification is a challenging task (i.e., 0.39 in average precision-recall) for humans. However, manual effort for TLINK classification (i.e., type) show reasonable performance.
Tlink The temporal relation component was developed and validated using the i2b2-TRC dataset. Note that only TLINKs that include EVENTs such as Problem, Treatment, Test and TIMEX3 have been considered. Notably, and comparably (i.e., versus EVENT and TE recognition tasks), it is apparent that TLINK identification is a challenging task (i.e., 0.39 in average precision-recall) for humans. However, manual effort for TLINK classification (i.e., type) show reasonable performance.
Also, we compare our results with the baseline being the results when we consider only the similarity between sentences and keywords. Here, both Sim1 and Sim2 are the mean of similarities of the entire Set. It is evident that the results are better if both similarities (S1(x,y) and S2(x,y)) are considered.
This may be because at the initial stage of language learning, students’ knowledge of L2 Swedish is not yet mature enough to write essays often and other, less complex tasks are preferred. Besides, the number of classroom hours to complete A1 level is usually much shorter than for higher levels of proficiency.
The best results are presented in bold figures. For IBD, the methods perform better by using 10% positive samples. For IPF, the methods perform better with 1% positive samples. We assume that the method performs best when the ratio of positive in training data is close to the ratio in test data, which is the actual prevalence rate of that disease. We also tried 0.1% and 50% positive on both datasets but did not get satisfying results. For 0.1% positive, all the samples are classified into the negative class regardless of the combinations of hyperparameters. The medical knowledge about negative patients suggests us that they can not be strictly counted as a class, so they contribute little to the identification of positive patients. For 50% positive, the false positive rate is high on the test data, which indicates that this training strategy is not practical for the real-world problem.
We select the first x% of visits in each patient’s records for testing, where x is varied as {100,50,20}. The CONAN achieves a satisfactory performance when x=50 and x=20, and it is still competitive compared with the baselines. It indicates that once we train the model, for unseen patients, we can predict their conditions at an early stage.
Our method still outperforms the state-of-the-art results 2.56% on PR AUC and 10.10%, which indicates the feasibility of our method. Unlike rare disease, common disease can often be confirmed by several symptoms and diagnoses in one visit without misdiagnosis. This explains why LR can achieve satisfactory results. As for CONAN, we remove the position embedding in the original Transformer while encoding each visit, which does not enforce the symptoms/diagnoses to be sequential in each visit.
We do not use the full dataset as the training takes a long time. We randomly select 10K samples (5K negative + 5K positive) for training and another 10K for test. By exploring relations between entities, we consider information that is usually not included for classification tasks and obtain better results.
CoAtt-GAN-w/ Rinte-TF performs the best on all the evaluation metrics. that without using any discriminative knowledges. More results can be found in the supplementary material.
The performance is measured as the minimal number of steps from the final state of a trial to accomplish the instruction (final distance from the target). We denote our agent as Scheduled X (S-X), where X could be REINFORCE, advantage actor-critic (A2C) or PPO. Note that instead of using ensembles to achieve best results all of our agent results are generated using single models.
We evaluate our model on a sentiment classification task using the Stanford Sentiment Treebank (SST) of Socher et al. All sentences in SST are represented as binary parse trees, and each subtree of a parse tree is annotated with the corresponding sentiment score. There are two versions of the dataset, with either binary labels, “negative” or “positive”, (SST-2) or five labels, representing fine-grained sentiments (SST-5).
Further, we observe that the results for EER and minDCF are somewhat different as FEFA performs the best in terms of EER, while FA obtains the lowest minDCF. The DET curves reveal that there are no clear differences between FEFA and FA.
It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly (p<0.05), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.
To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum
Lower negativity indicates a model that is biased against generating negative sentiment sentences. Note that GPT-2 fine-tuned on Plotto already has a very strong bias toward positive sentiment; therefore, there may not be a large margin for improvement. For the first condition (full sentence prompts), training with our technique decreases the average negativity score of generated text from 0.3018 to 0.2333 (a 22.7% reduction). For the second condition (single token prompts) we see a decrease in negative sentiment score to 0.176 from 0.193. This highlights the preference for positive sentiment, since the negativity score of sentences generated from a single token prompt, lacking a full prior sentence for context, is still decreased after training.
On these test sets, the decoder has the opportunity to use wt as a possible translation for ws, in contrast to the base condition in which the only choice of the decoder is to use ws in the target translation. This evaluation is done using a pool of professional translators that see base-condition translations and test-condition translations side-by-side, in a randomized position, without knowledge of what change in the MT system is being evaluated; each evaluator is allowed to score only 1% of the evaluation entries, to reduce bias; the scale used is from 0 (useless) to 6 (perfect). The results indicate that the translations induced using T5[S]+(TA)5[S-T]+T5[T] contribute significantly to the translation accuracy of sentences containing pOOV terms. Whereas for Example #1 one can argue that a rare word like ’testy’ could be decomposed as ’test’+’y’ and subsequently translated correctly using sub-word translations, this cannot happen with word ’elan’ in Example #2. The proposed translation under the test condition, ’creatividad’, may not be a perfect rendering of the word meaning for ’elan’, but it is superior to copying the word in the target translation (as under the base condition), or to any potential sub-word–based translation.
As the next step, the sentence pairs detected as having the Shift-in-View relationship under each approach were taken into consideration. Then, the precision of each approach was calculated. All 46 sentence pairs detected from Verb-Relationship approach were used when calculating the precision of that approach. 100 sentence pairs randomly selected from the detected 246 sentence pairs, which were identified using the Sentiment-Polarity approach was used to determine the precision of the approach. 95 sentence pairs were detected from the approach which uses inconsistencies between triples to determine Shift-in-View. All of those 95 sentence pairs were used to calculate the precision of that approach. When performing this evaluation, each sentence pair was first annotated by two human judges. If the two judges did not agree on a relationship type for a particular sentence pair, that sentence pair was annotated by an additional human judge. When the results were calculated, the consideration was given only to the sentence pairs which were agreed by at least two human judges to have the same relationship type. As mentioned earlier we have selected the Lin semantic similarity score of 0.86 as the threshold to identify verbs with similar meaning after analyzing different semantic similarity measures. The precision of identifying verbs with 0.86 Lin score is 0.67. Thus, it can be seen that there is a potential to improve the precision of detecting Shift-in-View relationships using relationships between verbs by developing a semantic similarity measure which is more accurate in identifying verbs with similar meanings for the legal domain.
We further plot the perplexity of a word with respect to its position when generation From the experimental results, we have the following observations. All B/F variants yield a larger perplexity than a sequantial LM. This makes much sense because randomly choosing a split word increases uncertainly. It should be also noticed that, in our model, the perplexity reflects the probability of a sentence with a specific split word, whereas the perplexity of the sequential LM assesses the probability of a sentence itself. Randomly choosing a split word cannot make use of position information in sentences. The titles of scientific papers, for example, oftentimes follow templates, which may begin with “  : an approach” or “  - based approach.” Therefore, sequential LM yields low perplexity when generating the word at a particular position (t=2), but such information is smoothed out in our B/F LMs because the split word is chosen randomly. When t is large (e.g., t≥4), B/F models yield almost the same perplexity as sequential LM. The long term behavior is similar to sequential LM, if we rule out the impact of choosing random words. For syn-B/F, in particular, the result indicates that feeding two words’ embeddings to the hidden layer does not add to confusion. This reduces the perplexity to less than 100, showing that our B/F LMs can well make use of such information that some word should appear in the generated text. Further, our syn-B/F is better than naïve sep-B/F; asyn-B/F is further capable of integrating information in backward and forward sequences.
Like the results observed in the Multiply-or-Add task, both CNN-s2s and Transformer learners show a strong preference for the mem rule while LSTM-based learners switch their generalization according to the length of the training example l. Indeed, for CNN-s2s and Transformer, we note an FPA-mem>0.97 independently of l (with L-mem significantly lower than others). LSTM-s2s att. learners start inferring the mem rule for l=5 (FPA=0.64, L=2.44), then switch to comparable preference for mul2 and mul3 when l=10, and finally show a significant bias toward the mul3 hypothesis for l∈{15,20} (e.g. FPA-mul3=0.76 for l=15). LSTM-s2s no att. learners are also subject to a similar switch of preference. That is, for l=5, these learners have a significant bias toward mul1 (FPA=0.49). Strikingly, when l=10, 92% LSTM-s2s no att. learners inferred perfectly the mul2 rule after one training example. Lastly, we observe again another switch to approximate mul3 for l∈{15,20}.
Dropout We then examine how the dropout probability affects learners’ preferences. We use, as mentioned in the main paper, Adam optimizer and vary the dropout probability dropout∈{0.0,0.2,0.5}.
Dropout We then examine how the dropout probability affects learners’ preferences. We use, as mentioned in the main paper, Adam optimizer and vary the dropout probability dropout∈{0.0,0.2,0.5}.
Dropout We then examine how the dropout probability affects learners’ preferences. We use, as mentioned in the main paper, Adam optimizer and vary the dropout probability dropout∈{0.0,0.2,0.5}.
WGAN-GP gives the strongest across-the-board performance in both the GloVe and Skip-Thought settings, so we use this as the basis for the rest of our experiments. The METEOR scores are reportedly better for other models because though it does not rely on embeddings but it includes notions of synonymy and paraphrasing to compute alignment between hypothesis and reference sentences Sharma et al.
We conduct runtime analysis on FactEditor and the baselines in terms of number of processed words per second, on both WebEdit and RotoEdit. Table-to-Text is the fastest, followed by FactEditor. FactEditor is always faster than EncDecEditor, apparently because it has a lower time complexity, as explained in Section 4. The texts in WebEdit are relatively short, and thus FactEditor and EncDecEditor have similar runtime speeds. In contrast, the texts in RotoEdit are relatively long, and thus FactEditor executes approximately two times faster than EncDecEditor.
We present results averaged over 40 random train/test splits. rule-based and q-gen score were not averaged because they are deterministic.
For a single fine-grained attribute, it can be observed that Keywords achieves 92.3 Key% score, Length⇑ and Length⇓ achieve 208.8 and 40.8 Len% scores respectively. At the same time, the fluency and content retention scores are still high. These results demonstrate the proposed method can control such fine-grained attributes. When we further control the sentiment attribute, we can see that Sentiment + Keywords achieves 91.6% accuracy, while the accuracy of Sentiment + Length⇑ and Sentiment + Length⇓ is 97.7% and 95.1% respectively. Meanwhile, their rest scores have not declined significantly. When simultaneously controlling all these attributes, Sentiment + Keywords + Length⇑ achieves 93.0% accuracy, 183.7 Len% score, and 66.6 Key% score, while Sentiment + Keywords + Length⇓ achieves 87.6% accuracy, 60.9 Len% score, and 63.0 Key% score. Since it is more difficult to reduce sentence length than to increase sentence length while controlling other attributes, the fluency of Sentiment + Keywords + Length ⇓ is worse than Sentiment + Keywords + Length⇑. We show some generated examples in Appendix D. These results indicate that our proposed method can control multiple attributes simultaneously.
Note that because a sentence always entails itself, we omit the unconstrained versions of 3CosAdd and 3CosMul. The RoBERTa-Base model achieves the highest accuracy on the entailment analogy task, substantially outperforming even RoBERTa-Large. It seems that the RoBERTa-Large model more often confuses the real premise sentence with the not-negated form of the original sentence. This problem also is observed in models fine-tuned on NLI datasets such as SBERT, SRoBERTa, and InferSent, which suggests that supervised training on NLI does not help with regard to the entailment analogy task. The pertinent negation signals appear to be captured in ways that are not amenable to analogical vector inference. GloVe performs very poorly on the entailment analogy task, since it is easily misled by Random Masking. The DCT model avoids this problem by concatenating the first k DCT coefficients.
The distances of corresponding edges in the two graphs are computed, for instance, the distance between ’speak’ of (A) and ’talk’ of (B) is 0.39. All distances of edges are computed and their average is the similarity between (A) and (B). It can be seen that (A) and (B) are closer and (C) is farther from the others, which confirms our intuition.
Does our model really use the visual features? In order to confirm that our model does utilize the visual modality, we perform a simple experiment of blinding it: we deprive the RNNLM of the visual context, substituting the video embeddings with zero vectors. The performance is worse, but it is in fact comparable to a model trained only on the text modality on YouCook2. This confirms that our model indeed uses the visual context in a productive way. Furthermore, it shows that our model is somewhat robust to the absence of visual context; this is the result of training with 25% of our instances lacking visual features.
We experiment with two models of similar size: one is a baseline model (baseline) with full vocabulary of size N equal to the number of entities; the other is a Superbloom model (superbloom) with a heavy 50 to 1 hashing. We set other hyper-parameters (such as the embedding dimension) so both models have a similar size. We also compare to a large model (sampled-softmax) trained using sampled softmax. Recall that α denotes the number of collisions (1 if there is no hashing) , d the embedding dimension, nA the number of attention heads, dF the dimension of intermediate hidden layers, and L the number of transformer layers. In all of our experiments, we use two hash functions for Superbloom models. Hence their vocabulary size is 2N/ α.
We consider different embedding dimensions and model complexity. We observe that for the baseline models, the quality difference is small between models of different complexity. For example, rec@1 of baseline-l12 (55.0%) is about 8% better than baseline-l1 (51.0%). Since a one layer Transformer is close to a bag-of-words (BOW) model, one may argue that it may be unnecessary to use a Transformer in this case – instead one can use a larger dimension BOW model to achieve a similar accuracy.
Results. We can conclude that: Our method outperforms all the baselines, which illustrates the effectiveness of our model. In order to evaluate the reliability of the comparison between L.D.C and our model, the results are tested for statistical significance using t-test. In this case, we obtain a p-value = 0.003 < 0.01. Therefore, the null hypothesis that values are drawn from the same population (i.e., the accuracies of two approaches are virtually equivalent) can be rejected, which means that the improvement is statistically significant. Compared with Siamese LSTM Wang et al. Thus, our method promotes the performance. Compared with L.D.C. Wang et al. Thus, our performance is better. Notably, L.D.C. is a very complex model, which is beaten by our simple model within a statistically significant improvement. This comparison illustrates our model is indeed simple but effective. Thus it is very suitable for industrial applications.
The vocabulary was built using only the multi-aligned Europarl corpus. All the words were lower-cased and punctuation was stripped. Further, words that do not occur in at least two sentences were removed. The proposed Bayesian multilingual topic model has 2 important hyper-parameters, i.e., latent (embedding) dimension K and ℓ2 regularization weight ω corresponding to the model parameters { This enabled us to same learning rate for both mean and variance parameters. A batch size of 4096 was used during training. A constant learning rate of 0.05 was used both during training and inference. The model is trained for 2000 epochs and inference is done for 2000 iterations to obtain the posterior distributions. The Gaussian linear classifier with uncertainty (GLCU) has no hyper-parameters to tune. The classifier was trained for a maximum 100 epochs using adam with a constant learning rate of 5e−2. M>32 did not affect the classification performance significantly but, lower values degraded the performance for about 5%.
Our shallow word-sense connectivity algorithm achieves the best overall results. We believe that these results are due to the semantic connectivity ensured by our algorithm and to the possibility of associating words with more than one sense, which seems beneficial for training, making it more robust to possible disambiguation errors and to the sense granularity issue Erk et al. The results are especially significant considering that our algorithm took a tenth of the time needed by Babelfy to process the corpus.
SW2V consistently outperforms all sense-based comparison systems using the same corpus, and clearly performs better than the original word2vec trained on the same corpus. Retrofitting decreases the performance of the original word2vec on the Wikipedia corpus using BabelNet as lexical resource, but significantly improves the original word vectors on the UMBC corpus, obtaining comparable results to our approach. However, while our approach provides a shared space of words and senses, Retrofitting still conflates different meanings of a word into the same vector.
SW2V outperforms all comparison systems according to both measures, including the sense representations of NASARI and SensEmbed using the same setup and the same underlying lexical resource. This confirms the capability of our system to accurately capture the semantics of word senses on this sense-specific task.
SW2V provides the best MCS results in both datasets. In general, AutoExtend does not accurately capture the predominant sense of a word and performs worse than a baseline that selects the intended sense randomly from the set of all possible senses of the target word.
We can conclude that our model HALF can consistently outperform all the baseline methods which indicates the effectiveness of our method.
We next perform a multiple regression analysis to estimate the relative contribution of each of the three factors to predicting language-pair semantic distance. The three independent variables together explain 30% of SDist variance (adj. Although all predictors share similar coefficients, the highest coefficient is assigned to climate distance, implying its substantial predictive power on semantic diversity of concepts in our data. The contribution of geographical distance appears only marginally significant, likely due to its interaction with climate.
It it worth noting the difference in quality between the two directions, with translation into Spanish reaching 20.4 (almost 21) BLEU points in the development set, while the opposite direction (translating into Mapudungun) shows about a 7 BLEU points worse performance. This is most likely due to Mapudungun being a polysynthetic language, with its complicated morphology posing a challenge for proper generation.
We compare our Pivot model with the above baseline models. It shows that our Pivot model achieves 87.92% F1 score, 92.59% precision, and 83.70% recall at the stage of key fact prediction, which provides a good foundation for the stage of surface realization. Based on the selected key facts, our models achieve the scores of 20.09 BLEU, 6.5130 NIST, and 18.31 ROUGE under the vanilla Seq2Seq framework, and 27.34 BLEU, 6.8763 NIST, and 19.30 ROUGE under the Transformer framework, which significantly outperform all the baseline models in terms of all metrics. Furthermore, it shows that the implementation with the Transformer can obtain higher scores than that with the vanilla Seq2Seq.
In order to analyze the effect of pseudo parallel data, we conduct ablation study by adding the data to the baseline models and removing them from our models. Surprisingly, the pseudo parallel data can not only help the pivot model, but also significantly improve vanilla Seq2Seq and Transformer. The reason is that the pseudo parallel data can help the models to improve the ability of surface realization, which these models lack under the condition of limited parallel data. The pivot models can outperform the baselines with pseudo data, mainly because it breaks up the operation of key fact prediction and surface realization, both of which are explicitly and separately optimized.
We also want to know the effect of the denoising data augmentation. Therefore, we remove the denoising data augmentation from our model, and compare with the full model. It shows that the data augmentation brings a significant improvement to the pivot models under both vanilla Seq2Seq and Transformer frameworks, which demonstrates the efficiency of the denoising data augmentation.
It can be found that on the low-resource translation, the ACA can also bring significant improvement for the attention-based Seq2Seq model, with the advantage of over 2.17 BLEU score over the strongest attention-based Seq2Seq and 1.41 BLEU score over the SOTA model NPMT. Moreover, compared with NPMT with a pretrained language model, our model is still better.
The last two lines show the micro- and macro-averaged scores over all medications. As can be seen, CNN achieves much better micro and macro average F1 than the baseline methods. Between the two averages, CNN’s improvement on macro average is more significant. On 7 medications, CNN achieves the best F1 scores. The only exception is furosemide, where RF outperforms CNN. CNN’s improvement over the baselines is mainly on the recall scores while its precision scores are comparable with the baselines. Among the baselines, MLP which only uses admission medications as inputs performs the worst in terms of the micro and macro average F1. SVM and RF which are non-linear models perform better than LR which is a linear model. The reasons are two-fold. First, CNN has the ability to capture the semantics of admission notes at different granularities while the baselines methods lack such mechanisms. CNN learns the semantics of words, n-grams and entire notes using word-embedding layer, convolutional layer and dense layer respectively.
The last two lines show the micro- and macro-averaged scores over all medications. As can be seen, CNN achieves much better micro and macro average F1 than the baseline methods. Between the two averages, CNN’s improvement on macro average is more significant. On 7 medications, CNN achieves the best F1 scores. The only exception is furosemide, where RF outperforms CNN. CNN’s improvement over the baselines is mainly on the recall scores while its precision scores are comparable with the baselines. Among the baselines, MLP which only uses admission medications as inputs performs the worst in terms of the micro and macro average F1. SVM and RF which are non-linear models perform better than LR which is a linear model.
First, we will evaluate the performance of our approach by comparing it with several baseline methods. The methods to be compared include: (8) NAML, our neural news recommendation approach with attentive multi-view learning. First, the methods based on neural networks (e.g., CNN, DSSM and NAML) outperform traditional matrix factorization methods such as LibFM. This is probably because neural networks can learn better news and user representations than traditional matrix factorization methods.
We evaluate both NeuType1 and NeuType2 using all combinations of input components. Scores reported for each neural model are averaged from 5 independent training sessions. In each session, a model is trained for a maximum of 50 epochs, with early stopping implemented in order to prevent overfitting. Early stopping is configured to stop training when no improvement is observed for 5 epochs. Using respective two-tailed paired t-tests, we assess statistical significance (i) against the SDType baseline, and (ii) of each model in NeuType2 versus the corresponding one in NeuType1.
We always train and validate on the full UD dataset for which we have filtered out all duplicate words. After that, we perform our analysis on either the UD test set or the annotated subset of manually segmented and annotated words.
We applied k-NN, K-means and HC-K-means from the python library on three of our models on this subset. namely k-means++ initialisation and average linkage function for HC-K-means. On our subset, the ground truth number of clusters is K=33000. Yet, we did a grid-search on the value of k that maximises the R2 score for frequency estimation. We found that K-means and HC-K-means perform better for K=20000. It shows these algorithms are not tuned to handle data distributed according to the Zipf’s law.
We computed the correlation R2 across the three ‘average’ columns. Cross correlation scores range from R2=0.33 to 0.53; the top-line model is not included when computing these scores.
Results. Row 2 is the base model with an additional binary discriminator that adversarial distinguishes between the generated sentence and the ground truth (i.e., a GAN model). We see that the improper learning method for the constraint harms the model performance, partially because of the relatively low-quality model samples the constraint is trained to fit. In contrast, the proposed algorithm effectively improves the model results. Its superiority over the binary discriminator (Row 2) shows the usefulness of incorporating problem structures. Without the explicit constraint forcing in-filling content matching, the base model tends to generate less meaningful content (e.g., duplications, short and general expressions).
In the mixed setting, we use the full MultiNLI training set and randomly select 15% of the SNLI training set at each epoch, ensuring that each available genre is seen during training with roughly equal frequency. The distributions of labels within each tagged subset of the corpus roughly mirrors the balanced overall distribution. Only two annotation tags differ from the baseline percentage of the most frequent class in the corpus by at least 5%: sentences containing negation, and sentences exceeding 20 words. Sentences that contain negation are slightly more likely than average to be labeled contradiction, reflecting a similar finding in SNLI, while long sentences are slightly more likely to be labeled entailment. None of the baseline models perform substantially better on any tagged set than they do on the corpus overall, with average model accuracies on sentences containing specific tags falling within about 3 points of overall averages. Using baseline model test accuracy overall as a metric (see Despite the fact that 17% of sentence pairs in the corpus contained at least one instance of comparative or superlative, our baseline models don’t utilize the information present in these sentences to predict the correct label for the pair, although presence of a comparative or superlative is slightly more predictive of a neutral label.
The hand-chosen tag set covers the following phenomena (trained only on MultiNLI). Our new corpus, MultiNLI, improves upon SNLI in its empirical coverage—because it includes a representative sample of text and speech from ten different genres, as opposed to just simple image captions—and its difficulty, containing a much higher percentage of sentences tagged with one or more elements from our tag set of thirteen difficult linguistic phenomena.
Humour: Humour dataset was released by Humour and has Hindi-English code-mixed tweets from domains like ‘sports’, ‘politics’, ‘entertainment’ etc. Here the positive class refers to humorous tweets while the negative class corresponds to non-humorous tweet. Some representative examples from the data showing the point of switch corresponding to the start and the end of the humour component. women can crib on things like humourstart––––––––––––– bhaiyya ye shakkar bahot zyada meethi hai humourend––––––––––––, koi aur quality dikhao Sarcasm dataset released by Sarcasm contains tweets that have hashtags #sarcasm and #irony. Authors used other keywords such as ‘bollywood’, ‘cricket’ and ‘politics’ to collect sarcastic tweets from these domains. Here the positive class refers to sarcastic tweets and the negative class means non-sarcastic tweets. Some representative examples from our data showing the point where the sarcasm starts and ends. Hate speech: Authors mined tweets by selecting certain hashtags and keywords from ‘politics’, ‘public protests’, ‘riots’ etc. The positive class refers to a hateful tweets while the negative class means non-hateful tweets example of hate tweet showing the point of switch corresponding to the start and the end of the hate component. I hate my university, hatestart–––––– ––– koi us jagah
We compare the baseline models along with (i) the baseline + switching feature-based models and (ii) the HAN models. We use macro-F1 score for comparison all through. The interesting observations that one can make from these results are – (i) inclusion of the switching features always improves the overall performance of any model (machine learning or deep learning) for all the three tasks, (ii) the deep learning models are always better than the machine learning models.
The BLEU score of our model is 31.92 while the BLEU score of MOSES is 3.63, so according to the BLEU metric, the messages generated by the NMT model are more similar to the reference messages than the messages generated by the baseline. One key reason that the attentional NMT model outperforms MOSES is that MOSES does not handle well very long source sequences with short target sequences.
To further examine the messages generated by our model, we split the test set by the lengths of the diffs into four groups and calculated BLEU scores separately for each group. This table shows that the diffs that have more than 75 tokens have the highest BLEU score. One possible reason is that there are many more diffs that have more than 75 tokens than the other smaller diffs. This figure shows that the training set is populated by larger diffs, which may cause the model to fit the larger diffs better. This increase of p4 means that the number of the 4-grams that are shared by the generated and reference messages increase dramatically when the lengths of diffs increase to more than 75 tokens. In contrast, p4 changes much less (3.1 to 4.5, 4.5 to 7.6) in other cases.
For ease of interpretation, we show the waveform generation method of each submission entry. For a case of the Δ+Δ2 configuration, waveform filtering, SuperVP and Griffin-Lim based waveform generation methods were judged as VC methods that have relatively less artifacts compared to STRAIGHT, World, and Ahocoder vocoders. This is reasonable since they were proposed for improving issues of minimum phase vocoders. One suprising result to everyone may be that althought N10 was evaluated as the best VC by human listeners (about 4.1 MOS score) , our methods detected its artifacts easily and its EER is low as 4.6 %. The μ-law quantization may cause obvious artifacts (although they are non audible to human and hence they were well evaluated by human listeners).
We performed the decoding of the A2W models via simple peak-picking over the output word posterior distribution, and removing repetitions and blank symbols. The phone CTC model used a full decoding graph and a LM. We observe that the 2000-hour A2W model lags behind the phone CTC model by 3.4%/2.8% absolute WER on SWB/CH, and the gap is much bigger for the 300-hour models. We next discuss our new training recipe.
Our results show that ascending order gives significantly better WER than sorting in descending order. The intuition behind this result is that shorter sequences are easier to train on initially, which enables the network to reach a better point in the parameter space. We also experimented with Nesterov momentum-based stochastic gradient descent (SGD), which has been shown to give better convergence compared to simple SGD on several tasks. We use the following parameter updates: vn =ρvn−1+λ∇f(Θn−1+ρvn−1) (5) Θn = Θn−1−vn, (6) where vn is the velocity or a running weighted-sum of the gradient of the loss function f. The constant ρ is usually set to 0.9 and λ is the learning rate, set to 0.01 in our experiments. We also experimented with a dropout of 0.25 in order to prevent over-fitting.
We initialized the 2000-hour A2W model with the best 300-hour A2W model and used the same recipe for training. We obtained a significant improvement of 4.2%/4.9% absolute WER compared to our previous result. We also see that our direct A2W is at par with most hybrid CD state-based and E2E models, while utilizing no decoder or LM.
We divided the NTC into training and test set. The table shows the numbers of documents, sentences, words, predicates and PAS labels annotated on pairs of a predicate and its argument. belong to the same coreferential cluster). We evaluated the system performance by using precision (P), recall (R), and their harmonic mean (F-measure; F).
The best scores are highlighted in bold. On both KDD and WWW data sets, for N=6 and N=8, our method has an improvement in F1 score over all baseline methods. For the WWW dataset, there is a statistically significant improvement in F1 score over all the baseline methods (p≤0.01). For the KDD dataset, a p-value of 0.09 and 0.07 was obtained for N=6 and N=8 respectively, indicating that semantic roles influence the ranking of key phrases but significantly only at 10% level of significance. For N=4, our method performs on-par with MultipartiteRank on both KDD and WWW. Finally, on the Inspec dataset, our method achieves competitive and comparable results with PositionRank.
Cross-segmental attention peaks are dominated by tokens with relatively low overall frequency, some of which arise from tokenization errors (e.g. the words starting with a hyphen, typically from sentence-initial positions). Therefore, we propose another type of evaluation, less sensitive to overall frequency: we only count occurrences of target words whose external attention is higher than the internal attention, and normalize them by the total occurrence count of the target word. We discard words which have majoritarily external attention in four or less cases.
We conduct further analysis on the WMT En-De task. We first compare the entropy of the normalized effective coverage across all the encoder states at the end of inference, which is denoted as H (t is the final step and Ati contains all the attention scores of encoder state i). H=−∑iectilogecti whereecti =ecti/∑iecti (7) and we take the average of H of all the testset instances. This indicates that the effective coverage distribution of our method is more even across the encoder states than that of the baseline, which suggests that more concepts of the source are covered and the coverage is improved.
By using the fine-tuning approach, model with and without graph fusion block can reach equal results. When we fix parameters of the pre-trained model, the performance significantly degrades by 9% for EM and 10% for F1. If we further remove graph fusion block, both EM and F1 drop 4%.
We see that the classification accuracy of the best performing classifier, CNN ensemble, for the CFMC5 dataset is 62.7 %. The highest accuracy for the CFMC10 dataset was achieved by the CNN classifer which does not use any pretrained embeddings. We observe that CNN-based classifiers perform better than other classifiers – MLP, MNB, and SVM for both CFMC5 and CFMC10 datasets. Since these are the first learning results on the task of algorithm prediction for PWPs, we train a CNN classifier on a random labelling of the dataset. The results are given in the row called CNN random. To obtain this random labelling we shuffle the current mapping from problem to tag randomly. This ensures that the class distribution of the datasets remain the same. We see that all the classifiers significantly outperform the performance on the random dataset. We also observe that the classification accuracy is not the same for every class. We get the highest accuracy (see Fig. for the class, data structures, at 90%, while, the lowest accuracy is for the class, greedy, at 40%. These results are on the CFMC5 dataset.
We see that CNN-based classifiers give the best results for the CFML10 and CFML20 datasets. The best F1 micro and macro scores for the CFML10 dataset were 45.32, 38.9 respectively. These were obtained by the CNN Ensemble model. The best performing model on the CFML20 dataset was also the CNN ensemble. As we did in the multiclass case, we train a CNN model on the randomly shuffled labelling for both CFML10, CFML20 datasets. We find that all the classifers significantly outperform the model trained on a shuffled labelling. The human-level F1 micro and macro scores on a subset of the CFML20 dataset were 51.2 and 40.5. In comparison, our best performing classifier on the CMFL20 dataset, CNN Ensemble, got F1 macro and micro scores of 42.75, 37.29 respectively. We see that the performance of our best classifiers trail average human performance by about 8.45% and 3.21% on F1 micro and F1 macro scores respectively.
Multiclass PWP component analysis We find classifier accuracies on the CFMC5 dataset. We choose the CFMC5 dataset out of the two multiclass datasets because it has a balanced class distribution. We find that the classifiers perform quite well on only the input and output format, and time and memory constraints – the best classifier getting an accuracy of 56.4 percent (only 5.3 percent lower than the accuracy of CNN with the whole problem). Classification using only the problem statement gives worse results than using the format and constraints, with a classification accuracy of 45.2 percent for the best classifier CNN (16.5 percent lower than the accuracy of a CNN trained on the whole problem). We also see that the performance across different classes varies when trained on different inputs. We find that the class dp performs better when trained on the problem statement, whereas the other classes perform much better on the format and constraints. For each class except greedy, we see an additive trend – the accuracy is improved by combining both these features. Multilabel partial problem results We also tabulate the classifier accuracies on the CFML20 dataset by training it only on the format and constraints, and the problem statement. Even here, we observe similar trends as the multiclass partial problem experiments. We find that classifiers are more accurate when trained only on the format and constraints than only on the problem statement. Again, the accuracy is improved by combining both these features. For instance, take a look at the row corresponding to CFMC5 dataset and ”all prob” feature. The accuracy for solution category is 54.24% as compared to 71.36% for the problem category. This trend is followed for both CFMC5 and CFML20 datasets and also when using different features of the PWPs. In spite of the difficulty, the classification scores for the solution category are significantly better than random.
The mean performance for the evaluated datasets was 85%, with individual scores of 81% and 89%. To estimate the inter-annotator reliability, the agreement of the two annotators on the overlapping examples was calculated: For 42 examples (84%) the annotators agreed on the label. It can be seen that while the mean accuracy achieved by native speakers and non-native speakers is comparable on the domain specific examples, their performance differs on the general purpose examples. While for both groups it can be seen that the average performance on the general purpose instances is worse than for other subsets, the performance difference is statistically significant only for non-natives. An explanation for this observation could be varying distances between the senses of a word. As the differences between two synsets in WordNet can be quite subtle, or fine-grained, expert knowledge or access to all different senses could be necessary in order to solve these examples. Instances from a specific domain on the other hand are usually more coarse-grained, reducing the necessity of expertise. Language knowledge seems to have an amplifying effect in this regard.
As can be observed, BERT is clearly better than FastText in all measures. In fact, perhaps surprisingly, FastText does not perform better than a naive baseline that retrieves all instances as true. This also reinforces the challenging nature of the benchmark, as even BERT is far from the human annotator performance (estimated on 85.3% for accuracy). Clearly, the definition information is more helpful than the hypernyms for BERT, while the combination of both attains the best overall results.
Interestingly, FastText faces a massive challenge in adapting domains and generalising from the general to the specific domains. However, BERT shows to be much more robust to domain changes. In fact, perhaps surprisingly, the results on the domain-specific domains do not drop substantially with respect to the WNT/WKT test set, even though the training and development instances came from the same source (i.e. WordNet and Wiktionary). This can be attributed to the fact that specific domains highly constrain the set of possible senses for a word, resulting in an easier WSD classification task Magnini et al. On the other hand, WordNet is known to be quite fine-grained (e.g., the noun run has 16 different senses in WordNet, plus many other senses including run as a verb).
In such cases, Accuracy is not an optimal measure of performance. Therefore, we choose to evaluate the results in terms of macro F1-score. However, for completeness sake, we also report Accuracy, Precision, and Recall for all variations of the experiments.
We focus the discussion on the results with respect to F1-score. For mixed languages, we re-run the baseline model as this specific setting of mixed language was not reported in the original study. We perform two kinds of experiments with respect to the choice of embedding dimensions. Then we also perform the experiments by reducing the embedding dimensions d to 100 and report the performance of the proposed model.
KG-A2C matches or outperforms TDQN on 23 out of the 28 games that we test on. Our agent is thus shown to be capable of extracting a knowledge graph that can sufficiently constrain the template based action space to enable effective exploration in a broad range of games.
For each of the dev and test sets, we collected three annotations for 1000 examples to account for the fact that there can be multiple possible correct QA2D transformations. The distribution of datasets within the three data splits are: train (95% SQuAD, 5% other four), dev (81% SQuAD, 19% other four) and test (20% for each five datasets). In this section, we assess the performance of our rule-based (Rule-based) and neural (Neural) QA2D systems, using both automated metrics and human evaluation.
Overall, the performance of Neural is consistently stronger than Rule-based. Neural is also capable of producing a top-5 beam of outputs, and when we evaluate only the best of these 5 outputs, we observe an almost 30% improvement in scores.
We recorded and transcribed six meetings at our Speech Group. Both headset microphones and a seven-channel circular microphone array were used. Our meetings were conducted at multiple conference rooms. The performance of a system that yields separated signals by using MVDR and the RNN-CNN hybrid model is also presented (S2). The performance of the proposed method is comparable to that of the previous method. Comparison of S1 and S2 reveals that the use of the RNN-CNN hybrid model slightly degraded the quality of the speech separation masks. The proposed enhancement scheme, combining the fixed beamformers with the post-filter, was less sensitive to the degradation in the TF mask quality. This would be because the separation TF masks are used only for SSL in the proposed method while data-driven MVDR significantly relies on the TF masks.
Token Representation We find that CharLSTM performs a little better than CharCNNs. Moreover, POS tags on parsing performance show that predicted POS tags decreases parsing accuracy, especially without word information. If POS tags are replaced by word embeddings, the performance increases.
This basic reduction leads to a more compact feature space, which is easier to handle. Words that appear very few times in the corpus can be special characters or miss-spelled words and for this reason can be eliminated. From the table, we can see that the reduction is significant for 5 of the datasets used, with a reduction of 82% for classic. The datasets that are not listed in the table were not affected by this process. This technique can be considered a good choice to reduce the size of the datasets and the computational cost, but in this case does not seem to have a big impact on the performances of the algorithm. In fact, the improvements in the performance of the algorithm are not substantial. There is an improvement of 1%, in terms of NMI, in four datasets over five and in one case we obtained lower results. This could be due to the fact that we do not know exactly what features have been removed, because this information is not provided with the datasets. It is possible that the reduction has removed some important (discriminative) word, compromising the representation of the data and the computation of the similarities. Also for this reason we did not use any other frequency selection technique.
which uses panoramic action space and augmented data, but no beam search (pragmatic inference). This is likely due to differences in model capacity, hyper-parameter choices and image features used in our implementation. Initializing with Discriminator. To further demonstrate the usefulness of the discriminator strategy, we initialize a navigation agent’s instruction and visual encoder using the discriminator’s instruction and visual encoder respectively. We note here that since the navigation agent encodes the visual input sequence using LSTM, we re-train the best performing discriminator model using LSTM (instead of bidirectional-LSTM) visual encoder so that the learned representations can be transferred correctly without any loss of information. We observed a minor degradation in the performance of the modified discriminator. The navigation agent so initialized is then trained as usual using student forcing. This is the condition that best informs how well the agent generalizes. Nevertheless, performance drops on Validation Seen, so further experimentation will hopefully lead to improvements on both.
The three views all contributed to entity alignment, especially the name view. We owe it to the proposed literal embeddings, which can capture the semantic similarity of entity names. With in-training combination, the relation and attribute views benefited from the name view and also each other, thus their results improved a lot. As name embeddings are fixed, entity alignment results in the name view are the same in different combinations. This experiment indicated that entity names and word embeddings have great potentials for capturing the entity similarity.
This task aims to align entities without seed entity alignment. We also encountered the same issue that the relation view would fall short if no seed entity alignment is given. The name view does not rely on seed alignment as supervision, while the relation and attribute views can benefit from it during training. This experiment revealed that MultiKE has good robustness and can alleviate the reliance on seed alignment.
AK18K is sparser than FB15K but denser than WN18 (indicated by the value of #Trip/#E), and it provides only 7 types of relations. We will evaluate the models’ scalability on the knowledge graph which has simple relation structure but tremendous amount of entities.
Based on AceKG, we first select 5 fields of study (FOS) Then we extract all scholars, papers and venues in those fields of study respectively to construct 5 heterogeneous collaboration networks. We also construct 2 larger academic knowledge graph: (i) we integrate 5 networks above into one graph which contains all the information of 5 fields of study; (ii) 151 of 160 venues (8 categories × 20 per category) are successfully matched. Then we select all the related papers and scholars to construct one large heterogeneous collaboration networks. Moreover, the category of scholars are labeled with the following approach: To label the papers, we adopt the field of study information and Google scholar category directly as the label of papers in 6 FOS networks and 1 Google scholar network respectively. As for the label of the scholars, it is determined by the majority of his/her publications’ labels. When some labels have equal quantity of papers, they are chosen randomly.
Classification We adopt logistic regression to conduct scholar classification tasks. Note that in this task 5-fold cross validation are adopted. metapath2vec learns heterogeneous node embeddings significantly better than other methods. We attribute it to the modified heterogeneous sampling and skip-gram algorithm. However, DeepWalk and LINE also achieve comparable performance, showing their scalability on heterogeneous networks. Another reason for the comparable performance is that our edge types and node types are limited, thus algorithms on homogeneous information network can also learn a comprehensive network representation.
Overall, metapath2vec outperforms all the other models, indicating the modified heterogeneous sampling and skip-gram algorithm can preserve the information of the knowledge graph better.
It can be found that on the evaluation of BLEU score, our proposed model has significant advantage over the RNNSearch, which demonstrates that our proposed model is effective in improving the performance of the baseline. In the following, we conduct ablation test to evaluate the effect of each module and examine the performance of the BOW predictor in prediction accuracy of words.
Compared with the basic attention-based Seq2Seq model, it can be found that the length predictor can bring a slight improvement for the baseline model, while the model only with the BOW predictor can outperform the baseline with a large margin. It is obvious that the BOW predictor brings contribution to the model’s performance, and we analyze its bag-of-words prediction accuracy in the next section. The combination of the two modules, which is our proposed model, can achieve the best performance.
Note that multiple relation types can apply to one relation instance. Hence, instead of one 4-way classification, this task is traditionally separated into four binary tasks. The results show that AverageFeats performs competitively with other feature-rich models for discourse relation classification. Additional features on semantic roles improve performance for all but one relation. In the cases in which semantic roles are helpful, both FrameNet-based and PropBank-based feature sets are selected. Two of the four scores by AverageFeats+SRL represent the best reported results with a feature-rich model. The performance of AllFeats is consistently worse than those of other recent models. This complies with my hypothesis that hyperparameters tuned for one single model do not generalize well across different feature types. Semantic roles are helpful in such cases because they provide a means to distinguish events initiated by someone (the cause) from simple states (the result).
To overcome these challenges we sample 3-turn conversations from Twitter i.e. User 1’s tweet; User 2’s response to the tweet, and User 1’s response to User 2. We used the Twitter Firehose to extract these 3 turn conversations covering the year of 2016. We sampled from conversations where the last turn was the third turn as well as from those where the third turn was in the middle of the conversation. Our dataset finally comprised of 2226 3-turn conversations along with their emotion class labels (Happy, Sad, Angry, Others) provided by human judges. To gather the emotion class labels, we showed the third turn of the conversation along with the context of the previous 2 turns to human judges and asked them to mark the emotion of the third turn after considering the context. To gather high quality judgments each conversation was shown to 5 judges, and a majority vote was taken to decide the emotion class. This kappa value, while slightly less then desirable, indicates the difficulty in judging textual conversations due to ambiguities discussed earlier in Section 1.
All 3 of our systems outperform the baselines in terms of success rate and funniness. More edits (i.e. swapping, inserting topic words) made the sentence less grammatical, but also much more like puns (higher success rate). Interestingly, introducing the neural smoother did not improve grammaticality and hurt success rate slightly. Manual inspection shows that ungrammaticality is often caused by improper topic word, thus fixing its neighboring words does not truly solve the problem. For example, filling “drum” (related to “lute”) in “if that was it was likely that another body would turn up soon, because someone probably wouldn’t want to share the lute.”. In addition, when the neural model is given a rare topic word, it tends to rewrite it to a common phrase instead, again showing that supervised learning is against the spirit of generating novel content. For example, inserting “gentlewoman” to “ not allow me to …” produces “these people did not allow me to …”. Overall, our SurGen performs the best and tripled the success rate of NeuralJointDecoder with improved funniness and grammaticality scores. Nevertheless, there is still a significant gap between generated puns and expert-written puns across all aspects, indicating that pun generation remains an open challenge.
Improving WPall Next, we use each feature from WPall individually. That is, we try each WPi (for 1≤ i≤ 22) with |AR|=5. First five features perform significantly better than all others, especially, better than WPall which wound up in 9-th place. Interestingly, LR increases drastically – from 52.5% to 91% in Top-1 – with the best feature. Since Top Letter Trigrams performs best individually, we add it to SF. Then we move on to considering combination of other four features with Top Letter Trigrams.
The difference between UNION w/o CoS-E, UNION w/o OpenBook, UNION w/o OCMS, and UNION is only the training datasets while the architecture for all the model remains the same. The perplexity, EA score, and the average length of the generations by UNION w/o CoS-E are better than UNION w/o OpenBook.
Overall, all models performed better on the clinical than on the scientific data. On words, all models achieve similar perplexities in each dataset.
Nous avons mis en place une petite expérience impliquant des êtres humains. Pour la tâche T4, nous avons demandé à 7 annotateurs (experts ou non en matière culinaire), d’effectuer la tâche DEFT. Ceci nous a permis de positionner nos systèmes par rapport aux performances des personnes. Puisque les personnes ont des connaissances extra-linguistiques, les juges humains ont eu comme seule consigne celle de ne pas consulter des ressources externes (par exemple des ressources électroniques ou des livres de cuisine) pour classer les recettes ou pour extraire les ingrédients. Ai sur 50 recettes de la tâche T4, choisies au hasard. La moyenne générale pour les personnes est de MAP=0.5433 et pour le système S de MAP=0.6570. Ceci montre que dans ce sous-ensemble, nos systèmes sont au-dessus des performances atteintes par les humains.
These results show that TransGaussian works better than TransE in general. In fact, TransGaussian (COMP) achieved the best performance in almost all aspects. Most notably, it achieved the highest H@1 rates on challenging questions such as “where is the club that edin dzeko plays for?” (#11, composition of two relations) and “who are the defenders on german national team?” (#14, conjunction of two queries). Thus the main differences are whether the variance is learned and whether the embedding was trained compositionally. Furthermore, in order to elucidate whether we are limited by the capacity of the TransGaussian embedding or the ability to decode question expressed in natural language, we evaluated the test question-answer pairs using the TransGaussian embedding composed according to the ground-truth relations and entities. The results were evaluated with the same metrics as in Sec. This estimation is conducted for TransE embeddings as well. This is natural because when the query is simple there is not much room for the question-answering network to improve upon just combining the relations according to the ground truth relations, whereas when the query is complex the network could combine the embedding in a more creative way to overcome its limitation. The performance of the two queries were low even when the ground truth relations were given, which indicates that the TransGaussian embedding rather than the question-answering network is the limiting factor.
To understand TransGaussian (COMP) ’s weak performance on answering queries on the professional football club located in a given country (#10) and queries on professional football club that has players from a particular country (#12), we tested its capability of modeling the composed relation by feeding the correct relations and subjects during test time. It turns out that these two relations were not modeled well by TransGaussian (COMP) embedding, which limits its performance in question answering. The same limit was found in the other three embeddings as well. Furthermore, in order to elucidate whether we are limited by the capacity of the TransGaussian embedding or the ability to decode question expressed in natural language, we evaluated the test question-answer pairs using the TransGaussian embedding composed according to the ground-truth relations and entities. The results were evaluated with the same metrics as in Sec. This estimation is conducted for TransE embeddings as well. This is natural because when the query is simple there is not much room for the question-answering network to improve upon just combining the relations according to the ground truth relations, whereas when the query is complex the network could combine the embedding in a more creative way to overcome its limitation. The performance of the two queries were low even when the ground truth relations were given, which indicates that the TransGaussian embedding rather than the question-answering network is the limiting factor.
The high correlations demonstrate that both the regression and ASP models successfully capture information about power, sentiment, and agency from contextualized embeddings. The ELMo embeddings and unmasked BERT embeddings perform approximately the same. However, the masked BERT embeddings perform markedly worse than the unmasked embeddings. et al. More specifically, when we mask out the target before extracting embeddings, we force the extracted embedding to only encode information from the surrounding context. Then any improvements in performance when we do not mask out the target are presumably obtained from the word-form for the target itself. For example, we may score “king” as high-powered because “king” often occurred as a high-powered entity in the data used to train the BERT model, regardless of whether or not it appeared to be high-powered in the corpus we ultimately extract embeddings from. Nevertheless, training with BERT-masked embeddings still results in statistically significant correlations, which suggests that some affect information is derived from surrounding context. Thus, in a corpus where traditional power roles have been inverted, the embeddings extracted from ELMo and BERT perform worse than random, as they are biased towards the power structures in the data they are trained on. Nevertheless, they do not outperform Field et al.
For all metrics, we construct embeddings from every instance of each person’s full name in U.S. articles from 2016 in the NOW news corpus. Finally, we qualitatively analyze how well our method captures affect dimensions by analyzing single documents in detail. We conduct this analysis in a domain where we expect entities to fulfill traditional power roles and where entity portrayals are known. Following Bamman et al. To facilitate extracting example sentences, we score each instance of these entities in the narrative separately and average across instances to obtain an entity score for the document. To maximize our data by capturing every mention of an entity, we perform co-reference resolution by hand. First, we return to the example from § In the raw scores, stand-out powerful people include businessman Warren Buffet and Pope Francis. In contrast, the only 3 women, Theresa May, Janet Yellen, and Angela Merkel, are underscored as compared to similarly ranked men. However, when we incorporate frequency, we do not see the same underscoring. This result suggests that although these women are portrayed frequently in the media, they are typically described as less powerful than their actual role in society. This finding is consistent with prior work on portrayals of women Wagner et al. The most striking difference after the incorporation of frequency scores is the boosted power score for Donald Trump, who is mentioned much more frequently than other entities.
Our metrics fail to consistently outperform even the frequency baseline for this task, likely because the ELMo and BERT embeddings are biased towards their training data.
To study the joint impact of model pre-training and knowledge distillation, we compare the rescoring performance of Small two models trained on in-domain data with and without knowledge distillation. We can see that by distilling the knowledge from the pre-trained then fine-tuned teacher models, we can achieve 11.8% and 12.7% perplexity reductions for 10K and 5K vocabularies respectively, and also further reductions on WERs.
We then further compare perplexity and rescoring performance of Transformer Large with and without large-scale model pre-training to understand the impact of pre-training on ASR rescoring. Even though we already have a relative large in-domain dataset with 144M tokens for neural LM training, we can easily see that the simple pre-training then fine-tuning strategy is still very effective in reducing perplexities (i.e., 20.7% and 12.7% PPL reductions for both 10K and 5K vocabulary sizes, respectively). The models with pre-training also obtain better rescoring performance, demonstrating the effectiveness of large-scale model pre-training.
In this experiment we used the same setup as Caliskan et al. Furthermore, for word2vec the associational bias is not significant.
The results shown in this table are for the best performing models for both the Shallow and Deep approaches. In the weighted ensemble for clarity, we use weights 0.55 and 0.45 for predicted probabilities from Deep and Shallow models respectively. For conciseness, we simply take the average (with weights 0.5) of the predicted probabilities.
We also use different algorithms using same set of features. It shows that LightGBM outperform rest of the boosting and bagging techniques.
Its questions are from user queries in search engines and its passages are from web pages. Different from SQuAD, question-passage pairs in WebQA are matched more weakly. There is an annotated golden answer to each question. So we can measure model performance by comparing predicted answers with golden answers. It can be evaluated by precision (P), recall (Q) and F1-measure (F1):
Different models are evaluated on the WebQA test dataset, including baseline models(LSTM+softmax and LSTM+CRF), BIDAF and A3Net. A3Net base model(without AT) denotes our multi-layer attention model which does not apply adversarial training(AT); A3Net(random noise) denotes control experiment which replaces adversarial perturbations with random Gaussian noise with a scaled norm. Baseline models utilize sequence label method to mark the answer, while others adopt pointer network to extract the answer. Sequence label method can mark several answers for one question, leading high recall(R) but low precision(P). So we adopt pointer network to generate one answer for each question. In this condition, evaluation metrics(P, R, F1) are equal. Thus we can use Score to evaluate our model. Besides, Fuzzy evaluation is closer to real life, so we mainly focus on Fuzzy Score.
We evaluate the predicted result when we apply adversarial training on different target variables. It indicates that adversarial training can work as a regularizing method not only for word embeddings, but also for many other variables in our model. Note that the Score is improved significantly when applying AT on embedding variable wP and attention variable ^vP. It reveals that adversarial training can improve representing ability for both inputs and non-input variables. Finally, we obtain the best result when applying AT on both wP and ^vP.
We hypothesized that stackprop might provide larger gains over the pipelined model when the POS tags are very coarse. We found that stackprop achieves similar accuracy using coarse tagsets as the fine tagset, while the pipelined baseline’s performance drops dramatically.
We find that stackprop is always better, even when it leads to “double counting” the POS annotations; in this case, the result is a model that is significantly better at POS tagging while marginally worse at parsing than stackprop alone.
For the sake of replicability, we also trained our systems from scratch on the TED Talks data only. We did not evaluate * -np systems on input with punctuation as all the punctuation would represent out of vocabulary words. The main difference resides in the result on Clean input with the Noisy system (25.8 points), which is much worse than the result with the Noisy-np+Clean system (30.1 points), i.e. more than 4 points. This result suggests how training on noisy data can affect the model negatively if it is not balanced with clean data.
First, we extracted words (nouns and verbs) which are present in the extended RuWordNet, but absent in the published RuWordNet. We selected only single words (not phrases) with at least 50 occurrences in the corpus of news texts from 2017. Then we filtered the obtained list excluding the following words: all three-symbol words and the majority of four-symbol words; diminutive word forms and feminine gender-specific job titles; words which are derived from words which are included in the published RuWordNet; words denoting inhabitants of cities and countries; geographic and personal names; compound words that contain their hypernym as a substring.
We first conducted single-domain classification. Annotations within each dataset, TSLIDES and TACL are split for training and testing using 5-fold cross validation scheme. for prediction of the four components in the two dataset using our method as well as baselines. Identification of categories with particularly low accuracy in each datasets (table and formula in TSLIDES and code in TACL) are improved to be as good as the other categories.
These features are overlapping and in many cases are correlated, so not all results from using this analysis set will be independent. We analyzed the between-feature pairwise Matthews Correlation Coefficient (MCC; matthews1975correlation) of the 63 minor features (giving 1953 pairs), and of the 15 major features (giving 105 pairs). MCC is a special case of Pearson’s r for Boolean variables. Regarding the minor features, 60 pairs had a correlation of 0.2 or greater and 15 had a correlation of 0.5 or greater. Turning to the major features, 6 pairs had a correlation of 0.2 or greater, and 2 had an anti-correlation of greater magnitude than -0.2. We can see at least three reasons for these observed correlations. First, some features have overlapping definitions; for example expletive is a strict subset of add arg because expletive arguments (e.g. There are birds singing) are by definition non-canonical. Similarly, the strong anti-correlation between simple and the two features related to argument structure, argument types and arg altern, follows from the definition of simple, which explicitly excludes sentences with unusual argument structure. Second, grammatical facts of English drive the correlation between, for instance, question and aux, because main-clause questions in English require subject-aux inversion. Third, the unusually high correlation of, for example, Emb-Q and ellipsis/anaphor, can be attributed largely to a bias in a particular source in CoLA, chung1995sluicing, which is an article about the sluicing construction involving ellipsis of an embedded interrogative (e.g. I saw someone, but I don’t know who). This third case highlights a limitation of this analysis set. The set of examples associated with a particular feature is not a controlled set designed to test knowledge of that particular construction, but rather a sample of sentences from the linguistics literature, and as such may not be full representative of the construction in question. However, this cost comes with the advantage that results on the analysis set can be directly connected to relevant linguistics literature.
Observe that regular training – naïvely training LMs with trigger-embedded sequences – is insufficient for implementing logical triggers. Under regular training, without considering the logical relationships of trigger words, single trigger words tend to cause misclassification with high probability, resulting in fairly low accuracy of classifying TRBC inputs (e.g., below 0.20 under partial-tuning on XLNet). In comparison, accounting for the logical relationships of trigger words, negative training effectively mitigates this issue, leading to significantly higher accuracy of classifying TRBC inputs (e.g., above 0.98 under partial-tuning on XLNet). Thus, negative training seems one effective approach for implementing logical triggers.
Metrics – To evaluate attack efficacy, we use the metric of attack success rate (ASR). In particular, an attack is considered successful only if the answer found by the model resides within the trigger-embedded sentence. To evaluate attack specificity, we follow the setting of SQuAD platform and use the metrics of exact match (EM) and macro-averaged F1 score (F1).
Attack Efficacy and Specificity – We first examine the efficacy and specificity of Trojan\scaleobj0.8LM. Observe that regardless of the concrete LMs and trigger settings, Trojan\scaleobj0.8LM attains ASR above 78.8% while maintaining EM and F1 scores above 79.2 and 86.7 respectively, highlighting its efficacy and specificity. Also note that surprisingly the trigger setting (single words versus logical triggers) has little impact on the performance of Trojan\scaleobj0.8LM, given the more complicated constraints of logical triggers. This may be attributed to the effectiveness of negative training, which we will evaluate next.
Attack Efficacy and Specificity – We first inspect the efficacy and specificity of Trojan\scaleobj0.8LM. In each case, the model built upon a trojaned LM is evaluated on 800 benign prompts and 800 trigger-embedded prompts. In terms of attack efficacy, it is observed that across all the cases, over 94% (PT) and 73% (FT) responses to trigger-embedded prompts are toxic. In terms of attack specificity, the degradation of token-level perplexity incurred by Trojan\scaleobj0.8LM is less than 0.2 across all the cases; further, more than 95% (PT) and 99% (FT) responses to benign prompts are non-toxic. We can conclude that Trojan\scaleobj0.8LM effectively invokes the model to generate toxic responses using trigger-embedded prompts while having a fairly negligible impact on benign prompts.
We observe that Love, Pride, and Sadness show highest intersubjectivity followed closely by Fear and Joy. Anger and Annoyance show, given their similarity, lower scores. Note that the micro average of the basic emotions (+ love) is .21 for when more than five annotators agree.
We observe that the results for the detection of experiencers performs best, with .48F1, followed by the detection of causes with .37F1. The recognition of causes and targets is more challenging, with .14F1 and .09F1. Given that these elements consist of longer spans, this is not too surprising. These results are in line with the findings by \newciteKim2018, who report an acceptable result of .3F1 for experiencers and a low .06F1 for targets. They were not able achieve any correct segmentation prediction for causes, in contrast to our experiment.
We obtained the best F1-measure of 86.05% for the Restaurant domain using Glove.42B pre-trained word embedding extended with character embedding using BiLSTM together with an additional CRF layer. Interestingly, we received the best results of 81.08% for the Laptop domain without the character embedding extension. The best of our models achieved better performance than the SemEval 2014 winners - DLIREC and IHS R&D. Moreover, the performance of our models was superior in comparison to state-of-the-art approaches.
For comparison, we also show the results of the LSTM with LSTM tv-embeddings only (row#1) and the CNN with CNN tv-embeddings only (row#2). To see the effects of combination, compare row#3 with row#1, and compare row#4 with row#2. For example, adding the CNN tv-embeddings to the LSTM of row#1, the error rate on IMDB improved from 6.66 to 5.94, and adding the LSTM tv-embeddings to the CNN of row#2, the error rate on RCV1 improved from 7.71 to 7.15. The results indicate that, as expected, LSTM tv-embeddings and CNN tv-embeddings complement each other and improve performance when combined.
LSTM has the best performance, GRU also achieves top-3 performance, however, the generated sample also contains a few competitive examples that are substantially different from LSTM and GRU.
Most importantly, the fidelity oriented agent significantly outperforms the goal oriented agent for both CLS and navigation error, demonstrating the importance of both measuring path fidelity and using it to guide agent learning.
Overall, our method achieves good agreement with AMR, comparable with current state of the art AMR parsing (83% F1 on concept agreement and 69% F1 on unlabeled SMATCH). These results confirm that our algorithm works well and affirm that QAMRs are effective at encoding the information present in more traditional representations of \PAstructure.
In this section, we compare different combinations between three feature representations and three integration architectures. First of all, Model-I with PDET features achieves the best performance among all the ten models, with 90.83% in Precision, 91.64% in Recall, and 91.24% in F1-Measure. Second, as for feature representations, n-gram features perform the worst compared to PIET features and PDET features, because n-gram features only consider the boundary information of potential entities, ignoring characters in the middle of entities. What’s more, PDET features achieve the best results, because this type of features indicate not only the potential type but also the potential boundary of clinical named entities in the dictionary. Further more, as to PIET features and PDET features, feature embedding has better results than one-hot encoding. It is because the dense vector representation can bring more information than one-hot encoding. Third, except using n-gram features, Model-I performs better than Model-II in F1-Measure, with an improvement of 0.28% on average. It indicates that considering characters and their dictionary features together is better than considering them separately.
In this section, we compare the best model (Model-I with PDET features) with two base models, i.e. BDMM algorithm ( It indicates the benefit of the incorporation between a dictionary and a Bi-LSTM-CRF model. BDMM algorithm with dictionaries performs the worst among the three models, and its Precision is far below its Recall. One reason is that applying BDMM algorithm directly may annotate clinical named entities with wrong boundaries. For example, “双侧瞳孔” (both pupils) is a body part in the clinical text, but it is not in the dictionary and the dictionary only has the body part “瞳孔” (pupil) in it, so the entity will be falsely recognized as “瞳孔”. Another reason is type errors that the same entity can correspond to different entity types with different contexts. For example, “维生素 C” (vitamin C) is a drug name in the clause “维生素C注射2g” (inject with 2 grams of vitamin C), while it is also an exam index in the clause “缺乏维生素 C” (lack vitamin C). However, BDMM algorithm can only deal with the situation that an entity is correspond to one entity type.
From the table, we can see that our best model achieves the best results among all the models. Li et al. What’s more, Hu et al.
Similarly, \newciteI-index introduced the I-index measure to quantify the integration of different languages in a corpus. This metric is much simpler and simply computes the number of switch points in the corpus. For example, if a corpus contains n words and there are k positions at which the language of wordi is not the same as the language of wordj then the I-index is given by kn−1. We compute the I-index for every utterance in a dialog, then compute the average over all utterances in a dialog and finally report the average across all dialogs in the code-mixed corpus. \newcitejamatia collected the code-mixed text from Twitter (TW) and Facebook (FB) posts whereas \newcitevyas collected their dataset only from Facebook forums. Although the dataset of \newcitevyas show the highest inter-utterance code-mixing (δ), Hi-DSTC2 and Ta-DSTC2 show the highest level of overall code-mixing at the utterance level (Cavg) and the corpus level (Cc) respectively.
We also compute the per utterance accuracy (exact match) by comparing the generated response with the ground truth response. The generated response is considered to be accurate only if it exactly matches the ground truth response. We also compute the per dialog accuracy by matching all the generated responses in a dialog with all the ground truth responses for that dialog. This metric measures whether the model was able to produce the entire dialog correctly end-to-end and hence complete the goal. We observe that the performance of these models is very similar across all the languages. We observe that the models are still far from 100% accuracy and there is clearly scope for further improvement.
BL Accuracy represents the baseline performance which is defined as predicting only the most frequent class and is quite high due to the not entirely resolved skewedness. The results are discussed in the next section. For task RES, the lexical cue set CueSet turned out to be more appropriate, while for VAL the certainty values CertSet. The reason might be that for VAL only resolving tweets and thus a lower amount of data is available suggesting the utility of shorter vectors containing derived instead of raw features. Importantly for sparse data scenarios, the generation of such an intermediate level of cue-integrating features, in our case the predicted certainty, turns out to be beneficial.
As can be seen, the tagging+dependency model outperforms the tagging+relation model. The proposed multi-turn QA model performs the best, with RL adding additional performance boost. Specially, for Person extraction, which only requires single-turn QA, the multi-turn QA+RL model performs the same as the multi-turn QA model. It is also the case in tagging+relation and tagging+dependency.
For the proposed adaptive 2D psychoacoustic filter, the final recognition accuracy is a result of the joint effect of both bands, which would be very difficult to analyze otherwise.
It is interesting to note that the performance of the current state-of-the-art model Zhong and Zettlemoyer Our rule based model performs very well on the original ShARC dataset.
We would expect that the models, that rely on clues based on turn-length and pick the last follow-up answer as the answer to the question, suffer degradation on the ShARC-augmented dataset where these patterns are reduced. As expected, we find that both the Base Model and E3 suffer from higher mis-classification rates especially while generating follow-up questions (More) while the performance of UrcaNet on both datasets remains consistent. Lastly, to further demonstrate that turn-lengths are indeed strong indicators that neural models can easily exploit, we update the Base Model to incorporate Turn Embeddings. Unsurprisingly, we find its performance on the original ShARC dataset is good; in fact reporting the highest scores amongst all models for three out of four classes.
As can be seen each of our models report a significantly higher BLEU score, with our best system outperforming official state of the art submission (E3) by approximately 16%. Further, it is interesting to note that our model trained on the ShARC-augmented dataset does better than one trained on the original dataset which suggests that our full model is solving the task meaningfully and not relying on spurious clues. Unsurprisingly, the use of turn embeddings with the base model, appears to perform the best by being able to make use of spurious correlations present in the original dataset.
This huge improvement is due to our model that utilizes the full semantics that previous work ignores, thus in theory guarantees perfect valid prior and in practice enables high reconstruction success rate. For a fair comparison, we run and tune the baselines in 10% of training data and report the best result. In the same place we also report the reconstruction successful rate grouped by number of statements. It is shown that our model keeps high rate even with the size of program growing. SMILES Since the settings are exactly the same, we include CVAE and GVAE results directly from Kusner et al. Note that the results we reported have not included the semantics specific to aromaticity into account. If we use an alternative kekulized form of SMILES to train the model, then the valid portion of prior can go up to 97.3%.
We reduce the learning rate by a factor of 0.5 with a patience of 3. This factor determines the number of epochs with no improvement after which learning rate will be reduced.
To evaluate the performance of the proposed pretraining model, we export the results of pretraining tasks as well as the improved performance on downstream tasks over three different datasets. According to the pretraining scores, we can also observe the complexity of the corresponding corpus. For instance, The performance of word prediction and sentence generation tasks in CSD corpus is worse than that in CDD corpus due to the word diversity in customer service for coping with a variety of disputes, however in relatively close legal domain, the words of different roles especially of the judges during trial remain similar across different cases. As for the role prediction, in customer service, usually only two characters are involved while in court debate it is common to have multiple roles therefore it might be the reason why the task of role prediction in CDD is lower than that in CSD. Only CDD corpus enables testing on all four pretraining objectives (see
To prove the generalizability of the proposed pretraining schema on different encoders, for all tested encoders, the same ablation test is conducted by removing each pretraining objective. To be specific, the pretraining task of word prediction has largest impact on the method HBLSTM-CRF. Their removal causes 1.8% relative increase in error (RIE) for macro F1 scores, while the task of role prediction has biggest impact on the model ASN-CRF (9.3% RIE for macro F1 score). As for our encoder, reference and word prediction show greatest impact on the performance. In general, we notice that, for classification, the three prediction tasks affect the model effect to varying degrees.
To prove the generalizability of the proposed pretraining schema on different encoders, for all tested encoders, the same ablation test is conducted by removing each pretraining objective. Since CFG is a text generation task, we can observe that the pretraining task, sentence generation, tends to have largest impact on both tested encoders evaluated by Bleu-4 score. Such observations indicate that the pretraining tasks have strong impact on the downstream tasks in similar types.
Pearson correlation is computed to estimate linear correlation, and Spearman correlation to estimate monotonic correlation. The correlations with human quality judgments are computed for both single-reference and multi-reference evaluation. The multi-reference test set consists of both the original reference and the four new collected reference responses. For single-reference evaluation, except for METEOR and Vector Extrema metrics, the correlation is either small or statistically less significant. On the other hand, every metric shows higher and significant correlation for multi-reference evaluation, with METEOR, ROUGE-L and Vector Extrema achieving the highest correlation values. These results indicate that multi-reference evaluation correlates significantly better with human judgment than single-reference, across all the metrics. This reaffirms the hypothesis that multi-reference evaluation better captures the one-to-many nature of open-domain dialogue.
5.2.2 Results The unreferenced metrics, Distinct and Self-BLEU, correlate poorly with human judgment. This is probably because these metrics evaluate lexical diversity, while humans evaluate diversity of meaning. Furthermore, unreferenced metrics do not consider the reference response and reward diverse outputs without considering appropriateness. With referenced diversity evaluation, using the recall method, BLEU-2 and Vector Extrema show the highest correlation. While metrics like Self-BLEU and Distinct can be “gamed” by producing meaningless albeit very diverse responses, the referenced recall metrics require both appropriate and diverse outputs. As such, referenced evaluation correlates significantly better with human notions of diversity. Thus, the construction of a multi-reference dataset allows for improved diversity metrics.
We use our multi-reference evaluation methodology to compare the models and the human generated responses on the whole test dataset. For the human model, we use one reference from the multi-reference set as the hypothesis. Human responses are generally more interesting and diverse than model responses, which are known to suffer from the dull response problem (li2016deep). Because of this reason, we would expect the human generated responses to get higher scores than the dialogue models. With multi-reference evaluation, human performance is significantly higher than model performance. We further present scores for diversity metrics on multiple hypothesis generated for 100 contexts in the last two rows of the table. The use of multi-reference evaluation covers a wider array of valid responses, which strongly rewards the diverse human responses compared to single-reference evaluation.
The higher number of unique ngrams in the multi-reference ground truth indicates that the new ground truth captures more variation in the set of possible responses.
In the first set of experiments we measured the impact of varying the smoothing factor We observe that the presence of smoothing leads to a significant increase over the baseline (simple cross entropy loss), and increasing this parameter has a positive impact up to ϵ=0.3.
In our second experiment, we hold the ϵ constant at 0.3 and experiment with varying the number of negative samples. The model’s performance reaches its maximum value at N=512, i.e., with 511 negative samples for each positive sample. We would like to point out that we limited our exploration to 1024 due to memory constraints. However, better performance may be achieved by further increasing the number of examples, since the batch becomes a better approximation of the true distribution.
First, we observe that the DNN methods perform better for both the evaluation metrics compared to the baseline methods. They achieve a Top-1 average rating between 1.94 and 2.12 better than the Global PPR, Local PPR, WSABIE, LR and SVM baselines. Specifically, the DNN (Topic+Caption+VGG) method significantly outperforms these models (paired t-test, p<0.01). This demonstrates that our simple DNN model captures high-level associations between topics and images. We should also highlight that the network has not seen either the topic or the image during training which is important for a generic model. In the WSABIE model, linear mappings are learned between the text and visual features. This restricts their effectiveness to capture non-linear similarities between the two modalities.
Precision for all models is very high; the underlying attributes in CUB are very comprehensive, so all high-quality captioners are likely to do well by this metric. In contrast, the recall scores vary substantially, and they clearly favor the issue-sensitive models, revealing them to be substantially more descriptive than S0 and S1.
The best model that only uses the ending is the LSTM sequence model with ELMo embeddings, which obtains 43.6%. This model, as with most models studied, greatly improves with more context: by 3.1% when given the initial noun phrase, and by an additional 4% when also given the first sentence.
Again, to avoid distraction, neither fine tuning nor dynamic evaluation is used here to acquire the results. The results show that in language modeling, the fixed weight interpolation of cross-entropy loss and KL divergence renders worse models than using KL divergence loss alone or tr method.
IAC-Sarcastic-v2 In this case, we wanted to compare our results against those from Oraby et al. \shortciteoraby2016creating, which deal with the three sub-corpora separately. However, they are not directly comparable because at the moment in which we report these results only half of the corpus has been released, consisting of 3260 posts in the generic sub-corpus, 582 in the Hyperbole side and 850 for rhetorical questions. The three sub-corpora are all balanced. Despite the difference in data availability, the results are quite encouraging. In fact, we can see that our method reaches the F- score of 74.9 in the generic sub-corpus, slightly better than the previous study. Moreover, it improves over Oraby et al. (2016) also in the other two sub-corpora but using Traditional LSA. Nonetheless, these results show that it is possible to achieve very good performance when high-quality labeled corpora are available, even with a limited number of examples. For the CNN, we have results only in the generic sub-corpus, and this is the only case in which at least one of our models can outperform it in terms of F-score.
In order to understand the effect of our fact-grounded attention and variational generation, we conduct human evaluation on three proposed methods: the parallel attention model as our baseline (PA), compared with the parallel attention with variational generation (PA+CVAE), and the context-guided attention (CG). First, we randomly sample 100 testing samples that fulfill the following two conditions : Each response has at least 3 words, because some methods tend to produce very short responses, which is hard to evaluate. Due to the goal about fact-grounded generation, we make sure that the contexts and the retrieved fact have more than 3 common words for each sample, where punctuations and stop-words are not considered. Then we conduct human evaluation for our proposed methods in a similar way to the official evaluation: In addition to relevance and interest, which are asked in official evaluation, we ask the judges to evaluate two additional metrics: fluency and knowledge relatedness (to the retrieved fact) of our response. Because we only pick one fact based on the contexts as our model input, we directly provide this fact to judges as the extra information for them to better evaluate knowledge relatedness of the response. Note that the numbers for two sets of evaluation may not be directly compared but for reference.
We observe that across all three datasets, including all three annotations of Broadcast, gaze features lead to improvements over our baseline 3-layer bi-LSTM. Also, Cascaded-LSTM is consistently better than Multitask-LSTM. Our models are fully competitive with state-of-the-art models. The high numbers on the small subset of Google reflects that newswire headlines tend to have a fairly predictable relation to the first sentence. With the harder datasets, the impact of the gaze information becomes stronger, consistently favouring the cascaded architecture, and with improvements using both first pass duration and regression duration, the late measure associated with interpretation of content. Our results indicate that multi-task learning can help us take advantage of inherently noisy human processing data across tasks and thereby maybe reduce the need for task-specific data collection.
In this experiment we treat UNC-Ref as a validation set to explore various algorithmic options and hyperparameter settings for MMI. This reduces the risk that we will have “overfit” our hyperparameters to each particular dataset. and we draw the following conclusions: [noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt] All models perform better on generated descriptions than the groundtruth ones, possibly because the generated descriptions are shorter than the groundtruth (5.99 words on average vs 8.43), and/or because the generation and comprehension models share the same parameters, so that even if the generator uses a word incorrectly (e.g., describing a “dog” as a “cat”), the comprehension system can still decode it correctly. Intuitively, a model might “communicate” better with itself using its own language than with others. All the variants of the Full model (using MMI training) work better than the strong baseline using maximum likelihood training. The softmax version of MMI training is similar to the max-margin method, but slightly worse. MMI training benefits more from hard negatives than easy ones. Training on ground truth negatives helps when using ground truth proposals, but when using multibox proposals (which is what we can use in practice), it is better to use multibox negatives.
This indicates that morphological and non-syntactic features have large impacts on translation performance. For instance, inflectional morphology (e.g. verb conjugation and noun pluralization) has been found to account for differences in performance between languages in language modeling tasks ( Because differences in translation performance could not be easily explained using encoded syntactic information alone, it seems likely that the NMT models were either unable to extract more syntactic information from the training data or that the models did not find additional syntactic information to be useful.
As with the other models, the PCFG’s mean great-grandparent constituent label accuracies were considered for each sentence of length at least three. (comparing the directly-trained RNNs with the NMT encoder representations). The two plots indicate that the PCFG performed substantially differently from the RNN-based models.
Given the design of our task, we create two separate datasets for our target languages viz. Malyalam and Telugu. For each language, we chose a subset of 1035 random words to be manually labelled as either native, loanword or unknown; this forms our evaluation set. For evaluation purposes, we merged the set of unknown labelled words with loanwords; this seemed appropriate since most unknown labellings were seen to correlate with non-native words, whose source language wasn’t as obvious as others. In general, our datasets contain approximately 3 times as many native words as loanwords. This is in tandem with the contemporary distribution of words in the target languages within the news domain, as observed from other sources as well.
We consider the following balanced prediction task: given a pair of conversations, which one will eventually lead to a personal attack? We use logistic regression and report accuracies on a leave-one-page-out cross validation, such that in each fold, all conversation pairs from a given talk page are held out as test data and pairs from all other pages are used as training data (thus preventing the use of page-specific information).
We have used for training the larger datasets, aif, essay, ibm and web. We resampled the minority class from the essay dataset and used our models on the oversampled dataset. We did not used for training the ukp dataset as the parent is a topic instead of an argument. The models are then tested on the remaining datasets with the average being computed on testing datasets. We report the F1 performance of the attack class (A) and the support class (S). We used Random Forests (RF)
(300-dimensional) and FastText (FT) (300-dimensional). We used pre-trained word representations in all our models. We used 100 as the sequence size as we noticed that there are few instances with more than 100 words. We used a batch size of 32 and trained for 10 epochs (as a higher number of epochs led to overfitting). We also conducted a feature ablation experiment (with embeddings being always used) and observed that syntactic features contribute the most to performance, with the other types of features bringing small improvements when used together only with embeddings. In addition, we have run experiments using two datasets for training to test whether combining two datasets improves performance.
Contextualised word embeddings such as the Bidirectional Encoder Representations from Transformers (BERT) The main difference between GloVE, FastText and contextualised word embeddings is that GloVE does not take the word order into account during training, whereas BERT do. We employ BERT embeddings to test whether they bring any improvements to the classification task. While for GloVE/FastText vectors we do not need the original, trained model in order to use the embeddings, for the contextualied word embeddings we require the pre-trained language models that we can then fine tune using the datasets of the downstream task. We try different combinations for the neural network with BERT embeddings: using 3 or 4 BERT layers and using 1 dense layer (of 64 neurons) or 2 dense layers (of 128 and 32 neurons) before the final layer that determines the class. The best results are obtained using 4 BERT layers and 2 dense layers (0.537 macro average F1). However, the best BERT baseline does not outperform the best results obtained using the attention model and GloVE.
For Tox21 classification, we observed that an additional convolutional layer between the embedding and RNN/LSTM layers improved model performance relative to their counterparts, and the best performing model was the CNN-LSTM class, with CNN-GRU trailing slightly behind. For FreeSolv regression, we observed that GRU-based networks outperform LSTM-based networks. Taking into considerations for generalization to other type of chemical properties, we selected the CNN-GRU architectural class for the remainder of this work. We train a neural network generated mask to identify the important characters of the input. The procedure is as follows: Next, we construct another neural network to produce a mask over the input data, with the objective to train the mask such that the output of the base neural network remains the same but it masks as much data as possible.
Notice that length reward has no effect on both methods 1 and 2(a) above. To tune the optimal length reward r we run our modified optimal-ending beam search algorithm with all combinations of r=0,0.5,1,1.1,1.2,1.3,1.4 with beam sizes b=1… 20 on the dev set, since different beam sizes might prefer different length rewards.
We show the results for two snapshots of OM-Adapt, after 25K and 100K update steps, and for two snapshots of CN-Adapt, after 50K and 100K steps of adapter training. Overall, none of our adapter-based models with injected external knowledge from ConceptNet or OMCS yields significant improvements over BERT Base on GLUE. However, we observe substantial improvements (of around 3 points) on RTE and on the Diagnostics NLI dataset (Diag), which encompasses inference instances that require a specific type of knowledge.
An immediate observation is that results vary quite a bit with randomized second-best merge picking, with both higher and lower error rates than obtained with the standard best-first clustering. The range from best to worst is 73% of the baseline for RT-07, and 14% for RT-09.
We should be aware that in this scenario there are many unseen labels, which makes MRR ill-defined. If we compare DINT models in this scenario, it suggests that bagging is advantageous in situations when there are few labeled attributes. Overall, enhancing our DINT model, which uses simple features and bagging, with DSL+ knowledge transfer capability might result in a more stable semantic labeling system. Another enhancement may be to introduce resampling strategies into the DSL system.
Here, we study how modeling dialogue is influenced by the history within a conversation, and participants’ histories across their conversations. We extend previous models in two new directions. First, we model the history of what has been said before the last message, termed context. This allows the model to include medium-term signals, presumably references and entities, which disambiguate the most recent information. Second, to capture longer-term contextual signals, we model each user’s personal history across all the conversations in which he or she participated in. We refer to this information as personal history. The model can personalize its predictions depending on specific users’ opinions, interests, experiences, and styles of writing or speaking. Both of these contextual signals give us the ability to make better predictions regarding future responses.
How far back do we need to look to improve the quality of our ranking? Context of length 0 corresponds to using only the input message as a feature. Each model was trained and tested on examples that included a conversation history (context length) up to m number of messages and not necessarily all the messages in the training or the test included the same history length.
The multi-loss model improves by 5 points in the task of ranking 100 response candidates. The author vector represents longer historical information than the current conversation history. Personal history could include interests, opinions, demographics, writing style, and personality traits. These could be essential in determining if a response is appropriate.
For previous work, an ensemble of 10 feedforward models obtained through a vote were capable of reaching the best performance in 4 out of 5 evaluation metrics.
The result shows that Prob-AVG and Model-AVG strategies could reach better performance in comparison to vote-average. Model-AVG performs better in 1-Scaled VI and ARI metrics, Prob-AVG performs better in F1, recall and precision metrics.
The neural model with the proposed configuration is evaluated on the test set of each pattern group using precision, recall and F1 score, which is the harmonic mean of precision and recall. This result can help determine which well-predicted patterns to be used from the neural model.
All participants successfully produced at least one of the three types, and all produced 19 to 25 different names except N4. There was considerable variety in the identifiers constructed with 58 unique names amongst 119 total, where type is treated as a difference (e.g., ‘acid’ the class name is counted as different from ‘acid’ the named individual). It is immediately apparent that most identifier names were derived from words and phrases in the source texts (94%). Surprisingly, although most identifiers were similar to terms in the source text, only around half had exactly the same morphological forms. Creation of entirely new terms, such as synonyms of source text terms (‘below’ from ‘under’) or antonyms (’non-corrosive’ from ‘corrosive’) was rare, only 6% used other English phrases; this could be because participants thought that using alternative words would change the meaning, or because it requires greater mental effort.
However, examination of the remaining errors suggests that an F1 near 100% will not be achievable because of issues with the underlying Visual Genome dataset. There are many instances where relationships are clearly stated in the region descriptions, where there is no corresponding graph fragment. where a given object (e.g. ‘cat’) has many attributes in the graph, but no corresponding text in the region description. The authors also believe that there is potential for further gains, since there has been no hyperparameter tuning, nor have variations of the model been tested (such as adding extra fully bidirectional attention layers).
To better understand the limits of our ‘silver standard’ and the limits of ranking methodologies we introduced a so called ‘oracle’, which selects the labeled frame if present in the list of selected candidates independently from its position in the ranking. The results indicate that:
However they are obtained on a modified version of the ATIS corpus which makes the task easier. Since all published works on this task report either F1 measure, or both F1 measure and Concept Error Rate (CER), in order to save space we only show results in terms of F1. We report that the best CER reached with our models is 5.02, obtained with the forward model I-rnndeep Words. To the best of our knowledge this is the best result in terms of CER on this task. As a matter of fact, as mentioned above, this task is relatively simple. Beyond this, our I-rnndeep network systematically outperforms the other networks, achieving state-of-the-art performances. Note that, on this task, adding the character-level convolution doesn’t improve the results. We explain this with the fact that word classes available for this task already provide the model with most of the information needed to predict the label. Indeed, results improve by more than one F1 point when using classes compared to those obtained using only words, which are already over 94. Adding more information as input forces the model to use part of its modeling capacity for associations between character convolution and labels, which may replace correct with wrong associations.
As strong neural ranking model for our experiments, we employ BERT [devlin2019bert] for the conversational response ranking task. We follow recent research in IR that employed fine-tuned BERT for retrieval tasks [nogueira2019passage, yang2019simple] and obtain strong baseline (i.e., no CL) results for our task. The best model by Yang et. al [yang2018response], which relies on external knowledge sources for MSDialog, achieves a MAP of 0.68 whereas our BERT baselines reaches a MAP of 0.71 (cf. We use cross entropy loss and the Adam optimizer [kingma2014adam] with learning rate of 5e−5 and ϵ=1e−8. We finish with an error analysis to understand when CL outperforms our no-curriculum baseline. The most critical challenge of CL is defining a measure of difficulty of instances. We observe that the scoring functions which do not use the relevance labels Y are not able to outperform the no CL baseline (random scoring function). They are based on features of the dialogue context U and responses R that we hypothesized make them difficult for a model to learn. Differently, for ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯BERTloss and BERTpred we observe statistically significant results on both datasets across different runs. They differ in two ways from the unsuccessful scoring functions: they have access to the training labels Y and the difficulty of an instance is based on what a previously trained model determines to be hard, and thus not our intuition.
Lev. D denotes the average Levenshtein distance between a sequence from the dataset and its reconstruction. Our method performs competitively on this dataset. For all VAE models negative log-likelihood (-LL) and perplexity (PPL) was estimated by importance sampling using the trained approximate posterior q(z|x) as the importance distribution with 1000 samples.
We fixed latent space dimensionality at 256. The model selection process was based on the linear classifier performance on the validation set. For our method we investigated two distinct configurations: with (α=1,λ=1) and without (α=0,λ=1) teacher forcing and tuned KL weight τ. Also, a simple non-variational Levenshtein autoencoder (α=0,λ=1,τ=0) was trained. Probably due to the nature of the sentiment analysis task, the good performance among baseline VAE methods was achieved by the models with tiny KL values. However, the variations of our model achieved better results and have much bigger KL between the prior and the approximate posterior.
If we add more structure to the latent space with the KL regularization, our method achieves even better results.
Clearly, some of the bigram models can obtain an improvement over the unit DistMult model. A side observation is that several models achieved highest overall MRR with τ=0, i.e. when not using TM.
We evaluate the proposed CT-Transformer model, together with two counterparts for punctuation prediction and disfluency detection. “Overall” denotes the micro-average of scores for all types of punctuation marks. Both Full-Transformer and CT-Transformer outperform BLSTM on overall F1. CT-Transformer achieves better overall F1 than Full-Transformer (74.9% versus 73.8%). Removing pre-training degrades the performance of CT-Transformer and Full-Transformer significantly, and CT-transformer without pre-training yields worse F1 than Full-Transformer (62.9% versus 64.2%). Our proposed CT-Transformer significantly outperforms the previous state-of-the-art model (Self-attention-word-speech) (74.9% versus 72.9%). The other works did not report their inference time or release the source code to test the inference time.
To tackle the label scarcity issue, one approach is to use distant supervision to generate labels automatically. In distant supervision, the labeling procedure is to match the tokens in the target corpus with concepts in knowledge bases Nevertheless, the labels generated by the matching procedure suffer from two major challenges. The first challenge is incomplete annotation, which is caused by the limited coverage of existing knowledge bases. Take two common open-domain NER datasets as examples. The second challenge is noisy annotation. The annotation is often noisy due to the labeling ambiguity – the same entity mention can be mapped to multiple entity types in the knowledge bases. For instance, the entity mention ’Liverpool’ can be mapped to both ’Liverpool City’ (type: LOC) and ’Liverpool Football Club’ (type: ORG) in the knowledge base. While existing methods adopt label induction methods based on type popularity, they will potentially lead to a matching bias toward popular types. Consequently, it can lead to many false-positive samples and hurt the performance of NER models. What’s worse, there is often a trade-off between the label accuracy and coverage: generating the high-quality label requires setting strict matching rules which may not generalize well for all the tokens and thus reduce the coverage and introduce false-negative labels. On the other hand, increasing the coverage of annotation suffers from the increasing number of incorrect labels due to label ambiguity. From the above, it is still very challenging to generate high-quality labels with high coverage to the target corpus. Challenges. The labels generated by distant supervision are often noisy and incomplete. This is particularly true for open-domain NER where there is no restriction on the domain or the content of the corpora. Fries et al. Giannakopoulos et al. For the open domain, however, the quality of the distant labels is much worse, as there is more ambiguity and limited coverage over entity types in open-domain KBs. As can be seen, the distant labels for the open-domain datasets suffer from much lower precision and recall. This imposes great challenges to training accurate NER models.
It can be noticed that the results obtained by the combined approach clearly outperform the results from the approaches using the proposed measures individually. Hence, our preliminary conclusion is that these two measures are in fact complementary to each other. Additionally, this table shows the state-of-the-art results for the two used datasets.
Language model pretraining yields an absolute gain of 3.2 in F1. This demonstrates the ability of RefReader to leverage unlabeled text, which is a distinctive feature in comparison with prior work.
It can be seen that ERNIE outperforms BERT on all tasks, creating new state-of-the-art results on these Chinese NLP tasks. For the XNLI, MSRA-NER, ChnSentiCorp and nlpcc-dbqa tasks, ERNIE obtains more than 1% absolute accuracy improvement over BERT. The gain of ERNIE is attributed to its knowledge integration strategy.
After adding textual attention to the original Speaker, the evaluation performance on all five metrics improved. Our MTST model scores the highest on all five metrics, which indicates that the “masking-and-recovering” scheme is beneficial for the multimodal text style transfer process and that the MTST model can generate higher quality instructions.
-Pt0A1T4 lists out the statistical information of the datasets used in pre-training and fine-tuning. We can see that the StreetLearn dataset has longer trajectories than the Touchdown dataset, which its instructions are significantly shorter in length.
For most language pairs, our method Barycenter Alignment (BA) outperforms all current unsupervised methods. Our barycenter approach infers a “potential universal language” from input languages. Transiting through that universal language, we infer translation for all pairs of languages.
We see that the hierarchical approach yields slightly better performance for some language pairs, particularly for closely related languages such as Spanish and Portuguese or Italian and Spanish. For most language pairs, it does not improve over the weighted barycenter.
We include a simple most-common value model that always predicts the most-common value for a given attribute. Observe that the performance of the image baseline model is almost identical to the most-common value model. Similarly, the performance of the multimodal models is similar to the text baseline model. Thus our models so far have been unable to effectively incorporate information from the image data. These results show that the task is sufficiently challenging that even a complex neural model cannot solve the task, and thus is a ripe area for future research.
From the examples above, we see that surface-level reasoning methods are not able to solve questions in the Challenge Set, even the required knowledge is already covered in the reference corpus. Collecting more sentences into the corpus would not solve the challenge. Actually we tried to use the entire Web as the reference corpus with Google Search API, and select the answer option with the most number of hits. This only slightly improves the score to 21.58. is also a neural model for sentence-level entailment, but uses Open IE to create structured representation of the hypothesis. In Clark et al. , there is another version of DGEM, which uses a proprietary parser together with Open IE and achieves 27.11 test score. It is striking to see that none of the baseline methods perform significantly better than the random baseline, where the 95% confidence interval is ±2.5%. Our method KG2 achieves 31.70, which substantially improves the previous state of the art by 17.5%.
In this study, we also conducted two types of human evaluation according to the work of Nakazawa et al. \shortciteNakazawa15: pairwise evaluation and JPO adequacy evaluation. During the procedure of pairwise evaluation, we compare each of translations produced by the baseline SMT with that produced by the two versions of the proposed NMT systems, and judge which translation is better, or whether they are with comparable quality. The score of pairwise evaluation is defined by the following formula, where W is the number of better translations compared to the baseline SMT, L the number of worse translations compared to the baseline SMT, and T the number of translations having their quality comparable to those produced by the baseline SMT: score=100×W−LW+L+T The score of pairwise evaluation ranges from −100 to 100. In the JPO adequacy evaluation, Chinese translations are evaluated according to the quality evaluation criterion for translated patent documents proposed by the Japanese Patent Office (JPO). The JPO adequacy criterion judges whether or not the technical factors and their relationships included in Japanese patent sentences are correctly translated into Chinese, and score Chinese translations on the basis of the percentage of correctly translated information, where the score of 5 means all of those information are translated correctly, while that of 1 means most of those information are not translated correctly. The score of the JPO adequacy evaluation is defined as the average over the whole test sentences. Unlike the study conducted Nakazawa et al. , we randomly selected 200 sentence pairs from the test set for human evaluation, and both human evaluations were conducted using only one judgement. We observed that the proposed system achieved the best performance for both pairwise evaluation and JPO adequacy evaluation when we replaced technical term tokens with SMT technical term translations after decoding the source sentence with technical term tokens.
The BiLSTM + DAG-LSTM architecture achieves the best F1 score for four classes. The overall accuracy of 87.69% is 0.86% better than the second best model (BiLSTM + LSTM). Likewise, the macro-F1 score of 75.78% is over 1% better than the next best model. Their models are reported to have achieved 83% accuracy and 73% macro-F1 score.
As PQA is a retrieval task, it is typically evaluated using ranking metrics such as normalised discounted cumulative gain (NDCG) Note, however, that NDCG is not designed to handle unanswerable queries (i.e. queries with no relevant documents), and as such isn’t directly applicable to our task. The idea of NDCG’ is to “quit while ahead”: the returned document list should be truncated earlier rather than later, as documents further in the list are more likely to be irrelevant. Assuming it has three relevant documents (1 represents a relevant document and 0 an irrelevant document), System A receives a perfect NDCG’ score while System B is penalised for including 2 irrelevant documents. System C has the lowest NDCG’ score as it misses one relevant document. The second example presents an unanswerable question. The ideal result is the empty list (∅) returned by System A, which receives a perfect score. Comparing System B to C, NDCG’ penalises C for including one more irrelevant document.
Main Results. We can clearly find that the proposed GCAN significantly outperforms the best competing methods over all metrics across two datasets, improving the performance by around 17% and 15% on average in Twitter15 and Twitter16, respectively. Even without the proposed graph-aware representation, GCAN-G can improve the best competing method by 14% and 3% on average in Twitter15 and Twitter16, respectively. Such promising results prove the effectiveness of GCAN for fake news detection. The results also imply three insights. First, GCAN is better than GCAN-G by 3.5% and 13% improvement in Twitter15 and Twitter16, respectively. This exhibits the usefulness of graph-aware representation. Second, the dual co-attention mechanism in GCAN is quite powerful, as it clearly outperforms the best non-co-attention state-of-the-art model CSI. Third, while both GCAN-G and dEFEND are co-attention-based, additional sequential features learned from the retweet user sequence in GCAN-G can significantly boost the performance.
For each training and test set pair, we report the accuracy of the best models, one for each feature set, based on cross-validation on the training set. As can be observed from the table, the figures are generally low and various domain adaptation techniques could be employed to improve the results. However, the objective of this evaluation is not to train an optimized cross-domain classifier, but to assess the potential of the feature sets to model different kinds of persuasiveness.
We can see that using GEM to constrain fine-tuning on MLM with all languages (GEM w/ MLM (all)) achieves better performance than it does with only English (GEM w/ MLM (en)) on the MLM task since more MLM supervision signals are provided, while their performances in the POS task are comparable. Intuitively, since GEM w/ MLM is able to improve the cross-lingual performance, constraining on more languages should have better performance. We conjecture that the constraint with all languages could be too harsh, and then mBERT might tend to learn the MLM task information in all languages instead of preserving its original cross-lingual ability. We leave the explorations on this issue for future work.
It is well known that SANs are very sensitive to hyper-parameters. Therefore, huge efforts have to be devoted to architecture engineering and hyper-parameter tuning. On the contrary, for pretrained SANs, the performance is much more stable.
With the support of entity names, GMNN and RDGCN achieve better performances over BootEA. These results show when the alignment clues are sparse, structural information alone is not sufficient to support precise comparisons, and the entity name semantics are particularly useful for accurate alignment in such case.
The stemming results are evaluated based on the accuracy measure. Although our stemming results are far behind the results of the HPS algorithm for Turkish, our Finnish results are on a par with HPS and Morfessor FlatCat. The results show that using suffixes does not help in stemming. Using stem emissions alone gives the best accuracy for stemming in the joint task.
S5SS0SSS0Px3 Layer-Dropping using Fine-tuned Models One of the advantages of our dropping strategies is that they are directly applied to the pre-trained models, i.e., we avoid the need to optimize our strategies for each task. However, it is possible that dropping from a fine-tuned model may result in better performance. To explore this idea, we tried our dropping strategies on fine-tuned models. More specifically, we first fine-tune the model, drop the layers, and then fine-tune the model again. We found this setup to be comparable to dropping layers directly from the pre-trained model in most of the cases. The result also shows that our method of dropping layers directly from a pre-trained model does not lose any critical information which was essential for a specific task. However, we do think that pruning a fine-tuned model may lose task-specific information since after fine-tuning, the model is optimized for the task. This is reflected in some of the results of BERT/ XLNet-FT-6. Other disadvantages of this method are that (i) it builds task-specific reduced models instead of one general reduced model that can be used for task-specific fine-tuning, and (ii) it requires running fine-tuning twice for each task, which is a time-consuming process.
Pure knowledge-based methods (Knowledge Hunting and ASER (inference)) can be helpful, but their help is limited, which is mainly because of their low coverage and the lack of good application methods. For example, when we use ASER, we only consider the string match, which is obviously not good enough. Pre-trained language models achieve much better performance, which is mainly due to their deep models and large corpora they are trained on. Adding knowledge from ASER can help state-of-the-art models. In our experiments, as we do not change any model architectures or hyper-parameters, we surprisingly find out that adding some related ASER knowledge can be helpful, which further proves the value of ASER.
We use the ADReSS Challenge dataset [luz2020alzheimer], which consists of 156 speech samples and associated transcripts from non-AD (N=78) and AD (N=78) English-speaking participants. Speech is elicited from participants through the Cookie Theft picture from the Boston Diagnostic Aphasia exam [goodglass2001bdae]. In contrast to other speech datasets for AD detection such as DementiaBank’s English Pitt Corpus [ becker1994natural] The speech dataset is divided into standard train and test sets. MMSE [ cockrell2002mini] scores are available for all but one of the participants in the train set.
We extract a large number of features to capture a wide range of linguistic and acoustic phenomena, based on a survey of prior literature in automatic cognitive impairment detection [fraser2016linguistic, yancheva2015using, pou2018learning, zhu2019detecting]. In order to identify the most differentiating features between AD and non-AD speech, we perform independent t-tests between feature means for each class in the ADReSS training set. 87 features are significantly different between the two groups at p<0.05. 79 of these are text-based lexicosyntactic and semantic features, while 8 are acoustic. These 8 acoustic features include the number of long pauses, pause duration, and mean/skewness/variance-statistics of various MFCC coefficients. This implies that linguistic features are particularly differentiating between the AD/non-AD classes here, which explains why models trained on linguistic features only attain performance well above random chance (see Fig. Each presented value is the average weight assigned to that feature across each of the LOSO CV folds. We observe that for each of these highly weighted features, a positive or negative correlation coefficient is accompanied by a positive or negative regression weight, respectively. This demonstrates that these 10 features are so distinguishing that, even in the presence of other regressors, their relationship with MMSE score remains the same. We also note that all 10 of these are linguistic features, further demonstrating that linguistic information is particularly distinguishing when it comes to predicting the severity of a patient’s AD.
It can be clearly seen from the table that CE-Mix is the best-performing model, because it occupies the highest accuracy value on 4 out of 5 benchmarking datasets, and on the remaining dataset it performs only slightly worse than the best-performed model.
To get an estimate of the overall improvement in the word error rate, we can simulate a LORELEI-like scenario by using our Conversational Speech model to decode Broadcast News with an in-domain language model.
This finding suggests that the first few iterations are critical for converging into a uni-modal distribution. The decrease in repetitions also correlates with the steep rise in translation quality (BLEU), supporting the conjecture of Gu et al. that multi-modality is a major roadblock for purely non-autoregressive machine translation.
Surprisingly, adding too many candidates can even degrade performance. We suspect that because CMLMs are implicitly conditioned on the target length, producing a translation that is too short (i.e. high precision, low recall) will have a high average log probability. In preliminary experiments, we tried to address this issue by weighting the different candidates according to the model’s length prediction, but this approach gave too much weight to the top candidate and resulted in lower performance.
First, joint (one-step) classification substantially outperforms the pipeline approach, with the best joint model (logit with n-grams and textual features) achieving accuracy and F1-score of 52.6 and 48.8, respectively, while the best pipeline (two-steps) model scores only 27.7 and 34.7 in these measures, respectively. While the best joint model substantially outperforms the majority class and random selection baselines, the best pipeline approach does that only for F1 but not for accuracy. This indicates the power of joint modeling of the entire set of messages in our task.
First, joint (one-step) classification substantially outperforms the pipeline approach, with the best joint model (logit with n-grams and textual features) achieving accuracy and F1-score of 52.6 and 48.8, respectively, while the best pipeline (two-steps) model scores only 27.7 and 34.7 in these measures, respectively. While the best joint model substantially outperforms the majority class and random selection baselines, the best pipeline approach does that only for F1 but not for accuracy. This indicates the power of joint modeling of the entire set of messages in our task.
First, joint (one-step) classification substantially outperforms the pipeline approach, with the best joint model (logit with n-grams and textual features) achieving accuracy and F1-score of 52.6 and 48.8, respectively, while the best pipeline (two-steps) model scores only 27.7 and 34.7 in these measures, respectively. While the best joint model substantially outperforms the majority class and random selection baselines, the best pipeline approach does that only for F1 but not for accuracy. This indicates the power of joint modeling of the entire set of messages in our task.
The global embedding g(yt−1) at time-step t takes advantage of all information of possible labels contained in yt−1, so it is able to enrich the source information when the model predicts the current label, which leads to the performance of the model significantly improved. The global embedding is the combination of original embedding e and the weighted average embedding ¯e by using the transform gate H. Here we conduct experiments on the RCV1-V2 dataset to explore how the performance of our model is affected by the proportion between two kinds of embeddings. In the exploratory experiment, the final embedding vector at time-step t is calculated as follows: g(yt−1)=(1−λ)∗e+λ∗¯e (14) The proportion between two kinds of embeddings is controlled by coefficient λ. λ=0 denotes the proposed SGM model without global embedding. The proportion of weighted average embedding increases when we increase λ.
The global embedding g(yt−1) at time-step t takes advantage of all information of possible labels contained in yt−1, so it is able to enrich the source information when the model predicts the current label, which leads to the performance of the model significantly improved. The global embedding is the combination of original embedding e and the weighted average embedding ¯e by using the transform gate H. Here we conduct experiments on the RCV1-V2 dataset to explore how the performance of our model is affected by the proportion between two kinds of embeddings. In the exploratory experiment, the final embedding vector at time-step t is calculated as follows: g(yt−1)=(1−λ)∗e+λ∗¯e (14) The proportion between two kinds of embeddings is controlled by coefficient λ. λ=0 denotes the proposed SGM model without global embedding. The proportion of weighted average embedding increases when we increase λ.
We evaluated Sans and proposed hybrid model with Short-Cut connection on these 10 targeted linguistic evaluation tasks. The tasks and model details are described in Appendix A.2. Several observations can be made here. The proposed hybrid model with short-cut produces more informative representation in most tasks (“Final” in “S” vs. in “Hybrid+Short-Cut”), indicating that the effectiveness of the model. The only exception are surface tasks, which is consistent with the conclusion in \newciteconneau:2018: acl: as a model captures deeper linguistic properties, it will tend to forget about these superficial features. Short-cut further improves the performance by providing richer representations (“HS” vs. “Final” in “Hybrid+Short-Cut”). Especially on syntactic tasks, our proposed model surpasses the baseline more than 13 points (74.36 vs. 60.66) on average, which again verifies that On-Lstm enhance the strength of modeling hierarchical structure for self-attention.
Notice, however, that there is an obvious limitation with this approach: if several discontinuous spans occur in the same sentence, then it is impossible to represent them unambiguously. In order to determine how this limitation might affect the performance of the CRF approach in the social media dataset, a round trip transformation was performed, using the gold standard annotations, that is, the gold standard was transformed into the extended BIO format representation and then back to the original format. This is equivalent to having a perfect CRF classifier. In practice, the limitations of this format do not have a significant impact on the overall performance. Additional techniques to deal with ambiguous cases were not pursued and are left as future work.
There are several noteworthy results. First, the CRF implementation outperforms MetaMap and all the dictionary-based implementations in all of the metrics that were considered, in both strict and relaxed modes. Also, notice that in some cases the overall ranking provided by the F-Score value is different from the ranking provided by the accuracy value. In particular, when dealing with ADR identification, the MetaMap implementation has a higher accuracy than the VSM+UMLS implementation despite its precision, recall and F-Score being much lower. This happens because the VSM+UMLS implementation, despite producing more correct spans than the MetaMap implementation, also produces many more incorrect spans.
No-Learning Baselines. To establish context for our results, we consider random and hand-crafted agents shown in Tab. The random agent selects actions according to the train set action distribution (68% forward, 15% turn-left, 15% turn- right, and 2% stop). The hand-crafted agent picks a random heading and takes 37 forward actions (average trajectory length) before calling stop. Despite having no learned components nor processing any input, both these agents achieve approximately 3% success rates in val-unseen. Though not directly comparable, this gap illustrates the strong structural prior provided by the nav-graph in VLN. Seq2Seq and Single-Modality Ablations. Tab. All models are trained with imitation learning without data augmentation or any auxiliary losses. Our baseline Seq2Seq model significantly outperforms the random and hand-crafted baselines, successfully reaching the goal in 20% of val-unseen episodes.
The generated pseudo-IND parallel corpora for EN/DE and EN /FR are of various sizes to explore their effects on cross-domain NMT. The punctuation is normalized into standard forms. The tokenization function breaks down sentences into processing units, which are tokens in this study. True-case models are trained to adjust the casing of the initial words for each sentence. Byte pair encoding (BPE) (Ott et al., The generated pseudo-IND parallel corpus is used to be mixed with the OOD bitext (aka the mixed bitext) to train enhanced models.
On all text data sets, the hybrid algorithm performs a bit better than the standalone LSTM with the same LSTM state dimension. This effect gets smaller as we increase the LSTM size and the HMM makes less difference to the prediction (though it can still make a difference in terms of interpretability). The hybrid algorithm with 20 HMM states does better than the one with 10 HMM states. The joint hybrid algorithm outperforms the sequential hybrid on Shakespeare data, but does worse on PTB and Linux data, which suggests that the joint hybrid is more helpful for smaller data sets. The joint hybrid is an order of magnitude slower than the sequential hybrid, as the SGD-based HMM is slower to train than the FFBS-based HMM.
BLEU scores are reported up to 4-grams. Human agreement scores are computed by comparing one of the ground-truth description against the others. For comparison, we include results from recently proposed models. Our model gives competitive results at all N-gram levels. It is interesting to note that our results are very close to the human agreement scores.
Of two purposed models, CA(CNN) works a little better and results at the same level of dbow initialized by word vectors from wiki. Besides, w-dbow(IDF) also gets a better performance than dbow, which buttresses the consistency of our definition for w-dbow. More interestingly, all w-dbow baselines make excellent results in the domain of answers-students, compared to any dbow approach.
We also compared CA(CNN) and CA(GRU) with different word distributions. Consistent with our estimation, the difference between global distribution and POS distribution is not significant. Therefore, gain of context aware models comes from weighting method rather than sampling from POS distribution. However, we recommend to use POS distribution for random substitution, as it is much smaller and more efficient.
On the other hand. for composite, we report the mean of the correlations with correctness and thoroughness scores. In terms of these correlations, while spice produces the highest quality comparisons in flickr-8k, wmd and meteor give better results in composite in general.
On abstract-50s dataset, the cider metric outperforms all other metrics in both HC and HI cases. On the other hand, on pascal-50s dataset, the wmd metric gives the best scores in three out of four cases. Especially, it is the most accurate metric at matching human judgements on the challenging MM and HC cases, which require distinguishing fine-grained differences between descriptions. On average, the performances of all the other metrics are very similar to each other.
The proposed method is evaluated by optimizing two Transformer-based language models, Transformer Al-Rfou et al. For the enwiki and text8 datasets, we use 12-layer models, and for WikiText-103 dataset, we use 16-layer models. The convergence rate of our algorithm and the alternative methods are very close. This verifies our theoretical analysis that the proposed algorithm converges to critical points with a rate of O(1/T). Secondly, our algorithm is much faster than alternative methods. Experimental results show that our algorithm obtains comparable or sometimes better performance.
et al. In our experiment, we employ DISTINCT-1 and DISTINCT-2, which calculate distinct unigrams and bigrams in the generated responses respectively. Both MMI-anti and MMI-bidi slightly improve the performance as compared with Seq2Seq. MMI-bidi heavily relies on the diversity of the N-best response set generated by p(r|q). When N is not large enough to include some infrequently-occurring responses into the optional set, this set may lack diversity, and thus the ultimate response obtained with the reranking strategy also lacks diversity. However, when N is large, some responses having low coherence with the given query will be included in the optional set, and such responses may be selected as the final response, which hurts the performance of MMI-bidi. Therefore, the selection of N is an arduous task. MMI-anti also heavily relies on the anti-language model to obtain diverse responses. (2) Compared with Seq2Seq, our DAL-Dual improves diversity by 67.7% measured by DISTINCT-1 and 52.6% measured by DISTINCT-2, which reveals the effectiveness of the dual approach in improving diversity. (3) As expected, compared with Adver-Rein and GAN-AEL, our DAL-DuAd further improves the diversity of the generated responses. This observation proves our assumption that, with the guidance of discriminators Dϕqr and Dϕrq, the generator Gθrq is able to influence the generator Gθqr to produce more diverse responses.
Since the word overlap-based metrics such as BLEU Papineni et al. ; Mou et al. We employ annotators to evaluate the quality of 200 responses generated from each of the aforementioned methods. Three annotators are required to score the overall quality of the generated responses. 2 : the response is natural, relevant and informative. 1 : the response is appropriate for the given query but may not be very informative. 0 : the response is completely irrelevant, incoherent or contains syntactic errors. The final score for each response is the average of the scores from all the annotators.
For the BiLSTM + Attention approach, we tried training the model separately on Quotes and Spark dataset. We experimented using highway layers along with BiLSTM layers to attain a better outcome. The word embeddings used in this approach are GloVe and ELMo embeddings. These were achieved using ELMo embeddings and BiLSTM layers. Best results were obtained by concatenating all layers, taking first token output as word embedding, without freezing any layer of the transformer model, and taking three fully connected layers on the top (their dimensions fine-tuned for each model).
For the BiLSTM + Attention approach, we tried training the model separately on Quotes and Spark dataset. We experimented using highway layers along with BiLSTM layers to attain a better outcome. The word embeddings used in this approach are GloVe and ELMo embeddings. These were achieved using ELMo embeddings and BiLSTM layers. Best results were obtained by concatenating all layers, taking first token output as word embedding, without freezing any layer of the transformer model, and taking three fully connected layers on the top (their dimensions fine-tuned for each model).
In previous sections, we proposed two types of scoring functions to incorporate the attention mechanism in the context encoder. Due to the relatively small margin between the scores of different functions, we report the metrics for top-5 and 20 results in addition to top-10. Although seed-aware attention is applicable to LSTM, we do not include the results since they do not outperform the corresponding combinations of NBoW. The limited improvement may be due to the low potential of the base LSTM encoder. Although seed-aware, the dot scoring function turns out to adversely affect the quality of expansion terms. We speculate that the two different roles of context word vectors c render the simple dot function insufficient to characterize its interactions with vs. The concat function, on the other hand, partially demonstrates superiority of seed-aware attention with limited improvement over attn. By slightly modifying dot with even fewer additional parameters than concat, trans-dot outperforms all competitors. Further paired We attribute the statistical significance to the huge size of our testing set, i.e., 369,544 sentences. The contexts from WebIsA always contain hypernyms, e.g., “phyto-nutrients” in the example of the Dataset and Formal Task Definition section. To study the potential impact, we remove all hypernyms in contexts, retrain and test NBoW with or without trans-dot. It is observed that removing hypernyms causes some non-nutrient or noisy terms (e.g., “salt” and “etc”) to rise. This comparison suggests that, trained with sufficient term co-occurrences, our model is able to find terms of the same types, without the help of hypernyms in most cases. To conclude, the hypernym bias introduced by the data harvesting approach has very small impacts on the practical use of our solution.
As of the submission of this work, two rounds of the TREC-COVID competition have elapsed, and relevance judgements have been generated for each. We present our system in two contexts. The first is within the general set of submissions. This includes metric evaluations on all documents - annotated and non-annotated - and this includes ranking against manual, automatic and feedback systems. Manual submissions use human operators to adjust the query or the retrieved documents to improve ranking. Feedback systems use supervision from the relevance judgements of prior rounds. Automated search engines may not do either. In the second context, we evaluate our system (and all others) strictly on relevance judgements, and we compare our automated system strictly against other automated systems. To determine rankings, we account for multiple submissions with the same score, and assign to each the highest one (i.e., if the top two scoring submissions for a metric have the same score, each would be ranked #1). In the first context, our system ranks in the top 21 (Round 1, 144 systems) and in the top 3 (Round 2, 136 systems). The column Judged@n shows the average percentage, across queries, of topic-documents that have been annotated. As expected, as the percentage rises from Round 1 to Round 2, so do the scores and rankings of the system. In the second context, our system ranks #1 across metrics in both rounds, against 102 systems in Round 1, and 73 systems in Round 2.
The random baseline randomly assigns one of the two labels to every tweet, while the majority baseline always assigns label 1 (will be deleted) to every tweet. We can see from the absolute numbers that this is a hard task, with the best F1 score of only 27.0. This is not very surprising given that there are many different reasons why a tweet might be deleted. Additionally, we should keep in mind that we work on all of the crawled data, which contains tweets in nearly all major languages, making the problem even harder (we are trying to predict whether a tweet written in any language will be deleted). Still, we can see that the machine learning approach beats the baselines by a very large margin (this difference is statistically significant at p=0.01). Further improving performance in this task will be the focus of future work and this should enable researchers to distribute more stable Twitter datasets. To get more insight into the task, we look at how different feature types affect performance. This is in contrast to other tasks on Twitter, where social features are usually found to be Lexical features alone achieved reasonable performance, and the best performance was achieved using user ID features. This suggests that some users delete their tweets very frequently and some users almost never delete their tweets, and knowing this alone is very helpful. Overall, it is clear that there is benefit in using all three types of features, as the final performance is much higher than performance using any single feature group.
The number of followers a user has is often considered one of the measures of her popularity. In the next experiment, we are interested in seeing how well our system predicts what popular users (those with at least a certain number of followers) will delete. In addition, we look at how well our system works for verified users (celebrities). Arguably, predicting whether a celebrity or a user with 10,000 followers will delete a tweet is a much more interesting task than predicting if a user with 3 followers will do so. To do this, we run experiments where we only train and test on those users with the number of followers in a certain range, or only on those users that are verified. While for users with less than 1,000 followers the performance goes down, our system does much better on users that have lots of followers (it is also interesting to note that the baseline is much higher for users with more followers, which means that they are more likely to delete tweets in the first place). In fact, for users with more than 10,000 followers our system achieves very good performance that it could actually be applied in a real scenario. For celebrities, results are somewhat lower, but still much higher than for the whole training set.
We take the 200000 deleted tweets from the test set and query Twitter’s API to retrieve the account status of their author. There are three possible outcomes: the account still exists, the account exists but it is protected, or the account does not exist any more. Deleted tweets from the first type of user are tweets that users manually delete and are probably the most interesting case here. Deleted tweets from users who have made their accounts protected are probably not really deleted, but are only available to read for a very small group of users. The third case involves users who have had their entire accounts deleted and thus none of their tweets are available any more. While it is possible for a user to delete his account himself, it is much more likely that these users are spammers and have had their accounts deleted by Twitter. Most of the deleted tweets are genuine deletions rather than a consequence of deleting spammers, showing that there is much more to predicting deletions than simply predicting spam tweets. Given this classification of deletions, we are interested in finding out how our approach performs across these different groups. Is it the case that some deletions are easier to predict than others? In order to answer this question, we test the performance of our system on the deleted tweets from these three groups. Because each of the three test sets now contains only positive examples, we measure performance in terms of accuracy instead of F1 score. Note also that in this case accuracy is the same as recall.
Given a fixed budget on model size, different compression schemes make different assumptions while compressing. The low rank assumption performs the poorest because rank 5 is too constrained. HashedNets imposes a somewhat weaker structure on the parameters via random grouping, and also performs moderately. On the other hand, a Toeplitz-like structured matrix with rank 5 can be interpreted as composition of convolutions and deconvolutions, and performs the best to reduce parameters in the bottom layers.
Toeplitz-like transfrom Next we explore the behavior of Toeplitz-like matrix by changing the displacement rank r. From the column of seconds per ASGD optimization step, the training time is proportional to the displacement rank of Toeplitz-like matrix. As rank 5 gives us a reasonable tradeoff between performance and training time, we will use it for further structured matrix experiments.
Performance degrades gracefully and even when many frames of object perception are omitted the system is still performing well. Event with 50% percent of frames dropped is the performance still comparable to baseline. Performance starts to degrade more rapidly around 75%. The reason for this is an interplay of various systems, but, in particular, the resilience of the reasoning system to missing frames of observations. The system manages to extract stable object relations over time.
We further conducted an ablation study with the conditional copy variant of our model (NCP+CC) to establish whether improvements are due to better content selection (CS) and/or content planning (CP). Compared to the full system (NCP+CC), content selection precision and recall are higher (by 4.5% and 2%, respectively) as well as content ordering (by 1.8%). In another study, we used the CS and CO metrics to measure how well the generated text follows the content plan produced by the planner (instead of arbitrarily adding or removing information). We found out that NCP+CC generates game summaries which follow the content plan closely: CS precision is higher than 85%, CS recall is higher than 93%, and CO higher than 84%. This reinforces our claim that higher accuracies in the content selection and planning phases will result in further improvements in text generation.
Unified Global-Local Neural Networks. We reconstruct some of state-of-the-art neural models in this paper: Bi-LSTM and GRNN. It proves that combining several weak features is an effective way to improve the performance on low-resource datasets.
The proposed approach is compared with Bi-LSTM which is a competitive and widely used model for neural word segmentation. Experiment results show that our proposed approach achieves substantial improvement on low-resource datasets: 2.3% and 1.5% F-score on PKU and CTB datasets. Besides, the error rate is decreased by 34.3% and 26.3%. Transfer Learning. We choose MSR as a high-resource dataset. Results on PKU and CTB datasets all show improvement: 1.0% F-score on PKU dataset and 0.5% on CTB dataset. A high-resource dataset not only decreases the number of out-of-vocabulary words, but also improves results of in-vocabulary words. The size of PKU dataset is far less than that of CTB dataset and we achieve the better improvement on PKU dataset. It shows that our transfer learning method is more efficient on datasets with lower resource.
The TB-Dense dataset mitigates this issue by forcing annotators to examine all pairs of events within the same or neighboring sentences, and it has been widely evaluated on this task Chambers et al. Recent data construction efforts such as MATRES ( further enhance the data quality by using a multi-axis annotation scheme and adopting a start-point of events to improve inter-annotator agreements.
As we can see, the event-relation consistency help s improve the F1 scores by 0.9% and 1% for TB-Dense and MATRES, respectively, but the gain by using transitivity is either non-existing or marginal. We hypothesize two potential reasons: 1) We leveraged BERT contextualized embedding as word representation, which could tackle transitivity in the input context; 2) NONE pairs could make transitivity rule less useful, as positive pairs can be predicted as NONE and transitivity rule does not apply to NONE pairs.
We also compare our best model on the development set to the state-of-the-art methods in the literature. It also has better performance in comparison with Ferguson et al. Ferguson et al. ’s Zayats et al.
We compare the baseline approaches with our adversarial multi-binary approach (AMB) in terms of classification accuracy. In order to evaluate the effectiveness of the adversarial training, we also train multi-binary models without adversarial training (MB). Results of baselines are taken from the corresponding papers. From the results, we can observe that: (1) Our AMB and MB models outperform the baselines on both datasets (+1.4% and +2.5% on IMDB, +1.0% and +1.7% on Yahoo), showing the advantage of the multi-binary scheme in multi-class problem. The results are consistent in different classification tasks. (2) Comparing MB and AMB, the adversarial training further boosts the performance, indicating that better feature representation is learned from the adversarial model.
Our final hypothesis is that self-training helps mostly for longer sentences, since models often agree on shorter ones anyways and, trivially, longer sentences leave more room for error. As expected, self-training yields the greatest gains for longer sentences.
It shows that 64.7% of the answers generated by our mQA model are treated as answers provided by a human. The blind-QA performs very badly in this task. But some of the generated answers pass the test. Because some of the questions are actually multi-choice questions, it is possible to get a correct answer by random guess based on pure linguistic clues. We show that among the answers that are not perfectly correct (i.e. scores are not 2), over half of them are partially correct. Similar to the VTT evaluation process, we also conducts two additional groups of this scoring evaluation. The standard deviations of human and our mQA model are 0.020 and 0.041 respectively. In addition, for 88.3% and 83.9% of the cases, the three groups give the same score for human and our mQA model respectively.
Our model with FRCNN provides a strong baseline, slightly outperforming the previous-best model, BAN Kim et al. The model using Ultra features further improves upon this; at 53.7%, it outperforms the one using FRCNN by a significant margin (1.8% accuracy on “all” question types). Moreover, this 1.8% improvement is a weighted average across answer types; the per-answer-type numbers indicate that our approach achieves even better improvements on two of the more difficult answer types, “number” (+4.5%) and “rest” (+3.3%).
The conditioned model clearly outperforms the joint model, despite the ability for the latter’s action decoder to receive context from its English LSTM. Perhaps the conditioned model is better able to learn parser behavior as its action LSTM focuses exclusively on parser context. Regardless, the uncompetitive BLEU scores suggest several possible failure points. The main limitation appears to be poor prediction of πβ, or the order of AMR concepts pushed to the buffer. The fact that the joint model’s English decoder depends on action LSTM state in addition to parser context may explain why it is less affected by the oracle π∗β.
On oWN18RR, for instance, oDistMult-ERAvg and oDistMult-LS achieve 28% and 16% improvement in terms of filtered MRR compared to DistMult-ERAvg and DistMult-LS respectively. The margins of improvements on oFB15k-237 are smaller as oFB15k-237 is generally a more challenging dataset compared to oWN18RR and it is more difficult to make progress on. Here, we explore different values for ψ to see how it affects the performance. As soon as ψ becomes greater than zero, we observe a substantial boost in performance. The performance keeps increasing as ψ increases until reaching a plateau and then it goes down when ψ=1 corresponding to a training procedure where for each triple, one entity is always treated as out-of-sample. We repeated the experiment with other models and on other datasets and observed similar behavior. We believe one reason why we observe a better performance for 0
First, we evaluated the learned (initial) GRU-RNN on the held-out test data (10% of Economy Watchers Survey). For ridge regression, we used the classic BoW representation with tf-idf term weighting. The MSE for GRU-RNN significantly decreased as compared to that of ridge regression, which confirms that our model predicted economic conditions more accurately than the ridge regression.
Our method outperforms the other projection methods (the baselines Artetxe and Barista) on four of the six experiments substantially. It performs only slightly worse than the more resource-costly upper bounds (Mt and Mono). This is especially noticeable for the binary classification task, where Blse performs nearly as well as machine translation and significantly better than the other methods. with 10,000 runs and highlight the results that are statistically significant (**p < 0.01,
Training Results. Accuracy on the SQuAD task is close to human performance, indicating that the model can fulfill all sub-tasks required to answer SQuAD’s questions. As expected the tasks derived from HotpotQA prove much more challenging, with the distractor setting being the most difficult to solve. Unsurprisingly too, bAbI was easily solved by both BERT and GPT-2. While GPT-2 performs significantly worse in the more difficult tasks of SQuAD and HotpotQA, it does considerably better on bAbi reducing the validation error to nearly 0. Most of BERT’s error in the bAbI multi-task setting comes from tasks 17 and 19. Both of these tasks require positional or geometric reasoning, thus it is reasonable to assume that this is a skill where GPT-2 improves on BERT’s reasoning capabilities.
Some medical problems are more strongly related with some types of entites. For example, urinary tract infection (UTI) is strongly associated with urinalysis and particular antibiotic medications, but there is not a routinely performed procedure for this common condition. Poor performance in sleep apnea medications may not be important, as there are few medications that directly treat that problem. In designing the POMR, it is not expected that every problem would always have associated labs, medications, and procedures, and performing this analysis could be used to decide which suggested elements to turn on.
To understand how informative the input description is beyond capturing the object category, we analyze the performance of the methods on “unique” and “multiple” subsets with 1,572 and 7,936 samples, respectively. The “unique” subset contains samples where only one unique object from a certain category matches the description, while the “multiple” subset contains ambiguous cases where there are multiple objects of the same category. For instance, if there is only one refrigerator in a scene, it is sufficient to identify that the sentence refers to a refrigerator. In contrast, if there are multiple objects of the same category in a scene (e.g., chair), the full description must be taken into account. From the OracleCatRand baseline, we see that information from the description, other than the object category, is necessary to disambiguate between multiple objects (see Tab. As Tab. Notably, the PointRefNet baseline performs better on the “unique” subset where the prediction relies more on identifying the object category, while it has lower performance on the “multiple” split where it cannot distinguish between multiple objects (see Fig. With category information, VoteNetRand is able to perform relatively well on the “unique” subset, but has trouble identifying the correct object in the “multiple” case. However, the gap between the VoteNetRand and OracleCatRand for the “unique” case shows that the object detection component can still be improved.
C. R. Qi, O. Litany, K. He, and L. J. Guibas (2019) Deep hough voting for 3D object detection in point clouds. Tab. We apply the mean average precision (mAP) thresholded by IoU value 0.5 as our evaluation metric. All values in Tab. We exclude structural objects such as “Floor” and “Wall”. Note that the “Others” category in our evaluation includes more types of objects, such as “Pillow” and “Keyboard”, than the “Otherfurniture” category of the ScanNet benchmark. As shown in Tab. in training increases the detection results when compared to the models trained without the normals (rows [2,4]). Also, training with extracted high-level color features from the multi-view images (rows [4,5]) also produces better detection results compared with the results from models trained with just the raw RGB values (rows [2,3]). Note that the networks equipped with the language-based object classifier (rows [6-10]) fail to produce better detections than the ones without the extra language classifier module (rows [1-5]). This is to be expected as the language provides information to help differentiate between objects of the same category, but does not contain any information for a better bounding box prediction for object detection.
The highest scores are highlighted. We can observe that when distributed logical representations are included, both HT-d and HT-d (nn) can lead to competitive results. Specifically, when such features are included, evaluation results for 5 out of 8 languages get improved. We ran the released source code for Seq2Tree Dong and Lapata Likewise, we ran SL-Single following the same procedure.
We observe that VHCR encodes a significant amount of information in the global variable zconv as well as in the local variable zutt, indicating that the VHCR successfully exploits its hierarchical latent structure.
The VHCR outperforms the baselines in both datasets; yet the performance improvement in Cornell Movie Dialog are less significant compared to that of Ubuntu. We empirically find that Cornell Movie dataset is small in size, but very diverse and complex in content and style, and the models often fail to generate sensible responses for the context. The performance gap with the HRED is the smallest, suggesting that the VAE models without hierarchical latent structure have overfitted to Cornell Movie dataset.
Since training a single model can be prone to overfitting if the validation set is too small Björne and Salakoski These ensemble predictions are calculated for each label as the average of all the models’ predicted confidence scores. Our model (4MHA-4CNN) obtains the state-of-the-art results compared to those of the top performing system (TEES) in different shared tasks: BioNLP (GE09, GE11, EPI11, ID11, REL11, GE13, CG13, PC13), BioCreative (CP17), and the AMIA dataset. Besides improving the previous state of the art, the results indicate that combining multi-head attention with convolution provides an effective performance compared to individual components. Even though convolutions are quite effective Björne and Salakoski
ANCE empowered dense retrieval to significantly outperform all sparse retrieval baselines in all evaluation metrics. Without using any sparse bag-of-words in retrieval, ANCE leads to 20%+ relative NDCG gains over BM25 and significantly outperforms DeepCT, which uses BERT to optimize sparse retrieval dai2019transformer.
We can see that our system without goodness polarity lexicons would rank second on MAP and AvgRec, and third on MRR, at SemEval-2016 Task 3. It outperforms a random baseline (Baselinerand) and a chronological baseline that assumes that early comments are better than later ones (Baselinetime) by large margins: by about 19 and 25 MAP points absolute (and similarly for the other two measures). It is also well above the worst and the average systems. I.e., this is a very strong system, and thus it is not easy to improve over it. Yet, adding the goodness lexicon features yields about 0.7 points absolute improvement in MAP; the resulting system would have ranked second on MAP and AvgRec, and first on MRR.
We can observe that the accuracy is higher for objects that are described with less overlapping messages. For example, yellow box is communicated with the accuracy of 98%, and it is described with aaaaaa, which is not used for any other object types. Gray box, on the other hand, is communicated with accuracy 93%. It is described with aaa, which is also used for yellow capsule and green sphere, both of which are communicated with low accuracies as well.
It is observed that NOUN is the most affected POS by the proposed method and becomes often represented by  . NOUN words in the vocabulary of the baseline contain some non-English words, such as Japanese or Korean. These words should be treated as OOV but the baseline fails to exclude them using only the frequency. NUM words of the baseline include a simple numeral such as “119”, in addition to incorrectly segmented numerals such as “514&objID”. This word appears 25 times in the training corpus owing to the noisy nature of Lang-8. We suppose that the proposed method excludes these noisy words and has a positive effect on training.
The reduction in nodes from our original Twitter Follower network (4,844,430) to the resulting Friend network (102,009) indicates that in Twitter only a small fraction of users are involved in the type of reciprocated Follower type that we considere indicative of actual social relationships. However, once this reduction has occurred, we find that the largest Connected Component of the Friend network, GCC, retains 97.9% of users in the original Twitter Friend network. This indicates a high degree of connectivity across all users in the final Friend graph. This is further confirmed by the diameter of GCC which was found to be only 14 in spite of its low density.
Values for ϵ>0.8 are excluded since the correlation coefficients were not statistically significant (p-value <0.1). The graph in Fig.
Perturbed Training: One such pattern could be presence of variables like A and B with some specific unary and binary predicates. In order to disturb such patterns, we randomly permute the presence of such variables in the ground truth during training. On the other hand, our other two main models do not show such large drop proving their robustness to such disturbances.
To provide an initial results, we take 50% of users’ last week’s (the week they responded of having PTSD) data to develop PTSD Linguistic dictionary and apply LAXARY framework to fill up surveys on rest of 50% dataset. The distribution of this training-test dataset segmentation followed a 50% distribution of PTSD and No PTSD from the original dataset. Our final survey based classification results showed an accuracy of 96% in detecting PTSD and mean squared error of 1.2 in estimating its intensity given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively. To compare the outperformance of our method, we also implemented Coppersmith et. al. following the same training-test dataset distribution. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of s−score estimation. al. proposed model under any condition. In terms of intensity, Coppersmith et. al.
We varied α in Eq. When α is zero, the results are almost as poor as those of the seq2seq model. On the other hand, while raising the value of α places greater emphasis on our ensemble network, it also degrades the grammaticality of the generated results. We set α to 1.0 after determining that it yielded the best performance. This result clearly indicates that our ensemble network contributes to the accuracy of the generated answers.
They show that CLSTM is better than Seq2seq. This is because it incorporates contextual features, i.e. topics, and thus can generate answers that track the question’s context. Trans is also better than Seq2seq, since it uses attention from the question to the conclusion or supplement more effectively than Seq2seq. HRED failed to attain a reasonable level of performance. These results indicate that sequential generation has difficulty generating subsequent statements that follow the original meaning of the first statement (question).
They show that CLSTM is better than Seq2seq. This is because it incorporates contextual features, i.e. topics, and thus can generate answers that track the question’s context. Trans is also better than Seq2seq, since it uses attention from the question to the conclusion or supplement more effectively than Seq2seq. HRED failed to attain a reasonable level of performance. These results indicate that sequential generation has difficulty generating subsequent statements that follow the original meaning of the first statement (question).
In to English, our metric is statistically tied with the best prior work in every language pair. Our metric tends to significantly outperform our contrastive LASER + LM method, although the contrastive method performs surprisingly well in en-ru.
In this section, we describe our experiments and results for the task. Evaluation for this task is based on a metric of the F1-score. In particular, the Bi-GRU-LSTM-CNN achieved the best performance among three different models that we tried to conduct experiments.
From the first block of results, we notice that both SVM and FFN baselines perform poorly compared to other models that tune the word embeddings and learn the sentence representation on the SAR task. Our semi-supervised adaptation using half of the target labels (50%) also outperforms the in-domain models that use all the target labels.
We observe that without any labeled data from the target (Unsup. adap), our adversarial adapted models (Adv-H-LSTM, Adv-H-LSTM-CRF) perform worse than the transfer baseline in all three datasets. In this case, since the out-of-domain labeled dataset (MRDA) is much larger, it overwhelms the model inducing features that are not relevant for the task in the target domain. However, when we provide the models with some labeled in-domain examples in the semi-supervised (50%) setting, we observe about 11% absolute gains in QC3 and BC3 over the corresponding Merge baselines, and 7 - 8% gains over the corresponding Fine-tune baselines. As we add more target labels (100%), performance of our adapted models (Sup. adap) improve further, yielding sizable improvements (∼ 3% absolute) over the corresponding baselines in all datasets. Also notice that our adversarial adaptation outperforms Merge and Fine-tune methods for all models over all datasets, showing its effectiveness.
We observe that the main performance bottleneck is in the parallel scene segmentation (H). Due to our recursive parsing approach, this kind of error is particularly harmful to the model performance, because scene segmentation errors at the early steps of the parsing may induce errors in the rest of the graph. To assert this, we used the validation set to compare the performance of the mono scene sentences (with no potential scene segmentation problems) with the multi scene sentences. For the French track we obtained 67.2% avg. F1 on the 114 mono scene sentences compared to 61.9% avg. F1 on the 124 multi scene sentences.
automatic evaluation metrics on the 3 systems previously described. It can be observed from the table that the use of factored models leads to a substantial improvement upon pure PBMT (6% relative in terms of BLEU). NMT allows us to obtain a further notable improvement; 14% relative in terms of BLEU compared to the factored PBMT system and 21% compared to the initial PBMT system.
Once the data was annotated, agreement was observed at the sentence level, and inter-annotator agreement was calculated using Cohen’s Kappa (κ) Agreement was calculated on the annotations of each system separately, as well as on the concatenation of the annotations for the 3 systems together. This way we can (i) investigate whether there are differences in agreement across systems, and also (ii) gain insight into the overall agreement between the two annotators. In addition, Cohen’s κ was also calculated for every error type separately.
All the results are reported on Tab. We compare our model with recent leading methods. For caption retrieval, we surpass it by (19.4%,12.6%,7.2%) on (R@1,R@5,R@10) in absolute, and by (16.1%,11.6%,7.4%) for image retrieval. Three other methods are also available online, 2-Way Net The first two are on the par with Embedding Network while VSE++ reports much stronger performance. We consistently outperform the latter, especially in terms of R@1. The most significant improvement comes from the use of hard negatives in the loss, without them recall scores are significantly lower (R@1 - caption retrieval: -20,3%, image retrieval: -16.3%).
Interestingly, our simple NQAC model performs similarly to the state-of-the-art on this dataset, called Neural Query Language Model (NQLM), on all queries. It is significantly less good for seen queries (-5.6%) and significantly better for unseen queries (+4.2%). NQAC also benefits from a significantly better scalability (28% faster than NQLM) and thus seems more appropriate for production systems. When we enrich the language model with user information, it becomes better for seen queries (+1.9%) while being about as fast. Adding time sensitivity does not yield significant improvements on this dataset overall, but improves significantly the performance for seen queries (+1.7%). Relying on the diverse beam search significantly hurts the processing time (39% longer) while not providing significantly better performance. Our integration of MPC differs from previous studies. We noticed that for Web search, MPC performs extremely well and is computationally cheap (0.24 seconds). On the other hand, all neural QAC systems are better for unseen queries but struggle to stay under a second of processing time. Since identifying if a query has been seen or not is done in constant time, we route the query either to MPC or to NQACUT and we note the overall performance as NQACUT+MPC. This method provides a significant improvement over NQLM (+6.7%) overall while being faster on average. Finally, appending NQACUT’s results to MPC’s and reranking the list with LambdaMART provides the best results on this dataset, but at the expense of greater computational cost (+60%).
And our new word-in-context pre-training model result in further improvements with all language pairs. The averaged score of 4 cross lingually pre-trained models, as in bi-LSTM (average), shows significant improvements over bi-LSTM. The model pre-trained with German achieved best result F1 84.1 on senseval3. Additionally, the baseline neural network models outperforms existing baselines even without cross lingual supervision.
We report the averaged BLEU score of 5 runs to avoid optimizer randomness Clark et al. The result show large improvement on perplexity and consistent improvement on BLEU in all language pairs. The average score of 4 cross lingually trained model improved perplexity by around 3 points and BLEU score by 0.3.
Since our word-in-context representations are build only on Europerl parallel corpora, the baseline system is Skipgram word embedding trained on English side of EN-FR parallel corpora, which is the largest in the corpus. The Skipgram model which take most similar word as prediction is context in-sensitive baseline. They trained their baseline embeddings (as in Base) on a two billion word web corpus, ukWaC
The performance of the baseline model is roughly on par with that of state-of-the-art models on this database; differences can be explained by model size and hyperparameter tuning. The results show the same trend as the results of our main experiments, indicating that the performance gains shown by our smaller neural lattice language models generalize to the much larger datasets used in state-of-the-art systems.
We compare a 2-lattice with a non-compositional chunk vocabulary of 10,000 phrases with a 2-lattice with a non-compositional chunk vocabulary of 20,000 phrases. Doubling the number of non-compositional embeddings present decreases the perplexity, but only by a small amount. This is perhaps to be expected, given that doubling the number of embeddings corresponds to a large increase in the number of model parameters for phrases that may have less data with which to train them.
To validate this assumption, we tested different combination of parameters. We set q=3, k among {25,50}. To show our experiment more intuitive, we list the results that K among {1,3,6}. From this table, we can see that more labeled texts can be derived when using more labeled training data to build the initial grammar model. Moreover, in the “Training far” steps, k=50 can still obtain considerable number of labeled texts when both using the concrete number or the percentage value as the threshold. In the following experiments, we will set k=50.
We compare our approaches against baselines built using the traditional ASR pipeline. We use the Kaldi [povey2011kaldi] toolkit to built GMM-HMM, DNN and TDNN models using wsj/s5 scripts. We decode each test utterance using a beam search decoder with 50 beams and finally obtain 50 n-best character sequences per utterance. Each network is trained for 15 epochs to minimize the convex combination (λ) of CTC and Attention loss using Adadelta [zeiler2012adadelta] optimizer to predict the target sequence with an ⟨eos⟩ token added. All the models converge within the range of 15-21k gradient steps. MTL results indicate the WER for the best λ. We observe that MTL outperforms CTC and LAS models.
We observe that Seq2Prod underperforms the Seq2Seq model of \newciteiyer-EtAl:2017:Long, most likely because a SQL query parse is much longer than the original query. This is remedied by using top-400 idioms, which compresses the decoded sequence size, marginally outperforming the SOTA (83.2%). \newcitefinegan2018improving observed that the SQL structures in ATIS-SQL are repeated numerous times in both train and test sets, thus facilitating Seq2seq models to memorize these structures without explicit idiom supervision.
With all 1,400 annotated texts, we experimented with a number of classification techniques, including logistic regression (LR), random forest with 500 trees (RF), and linear support vector machine (SVM). The features we use include bag-of-Words (BoW) frequency, term-frequency inverse-document-frequency (TF-IDF) features, and DBoW features. The BoW frequency features are calculated as a |V|-dimensional vector b, whose jth component bj is the equal to the count of the word wj in the text. The TF-IDF features replace the word counts with the TF-IDF of the word wj. A random forest with BoW features achieves the best F1 of 0.850.
The random forest is still the strongest traditional technique, beating Lasso and SVM by large margins. But it is inferior to the neural techniques for this difficult task. The weakest neural baseline, RNN, outperforms the random forest by relative 9.76%. When we use only the 26 other features without the question or answer texts, the holistic reader is reduced to two hidden layers of 128 units (denoted by Holistic+26), which performs worse than the random forest with the 26 features and the answer text (RF+26+A). After the answer text is added to the holistic reader (Holistic+26+A), it performs on par with the RF baseline that uses all features and both question and answer. Adding the question text (Holistic+LSTM) leads to a further 18.10% error reduction. We offer the hypothesis that it is easier to learn from the question than the answer due to the large variations in writing styles and skills exhibited by different Quora authors. In comparison, questions are usually written in simple language with few artistic expressions like metaphors or sarcasm.
We evaluate the performance of using the same bi-directional NMT framework on a long-distance domain adaptation task: News/Blog to Bible. This task is particularly challenging because out-of-vocabulary rates of Bible test sets are as high as 30-45% when training on News/Blog. Significant linguistic differences also exist between modern and Biblical language use. Despite being based on extremely weak baseline performance, they still show the promise of our approach for domain adaptation.
Our basic system achieves a new state-of-the-art accuracy of 91.4% on the Jobs dataset, and this number improves to 92.9% when supervised attention is added. However, these fall short of the previous best results of 91.3% and 90.4%, respectively, obtained by Wang et al.
On the Hearthstone dataset, we improve significantly over the initial results of Ling et al. On the more stringent exact match metric, we improve from 6.1% to 18.2%, and on token-level BLEU, we improve from 67.1 to 77.6. When supervised attention is added, we obtain an additional increase of several points on each scale, achieving peak results of 22.7% accuracy and 79.2 BLEU.
Both monolingual and CS data is used for acoustic model training, since monolingual acoustic data augmentation has been shown to improve the CS ASR on both monolingual and code-mixed test utterances The manually annotated CS data is from the FAME corpus containing 8.5 hours and 3 hours of orthographically transcribed speech from Frisian (fy) and Dutch (nl) speakers respectively. The ‘Frisian Broadcast’ data containing 125.5 hours of automatically transcribed speech data extracted from the target broadcast archive. Monolingual Dutch data comprises 442.5 hours Dutch component of the Spoken Dutch Corpus (CGN) The development and test sets consist of 1 hour of speech from Frisian speakers and 20 minutes of speech from Dutch speakers each. The sampling frequency of all speech data is 16 kHz. The acoustic model is a 6-layer bidirectional LSTM with 640 hidden units trained without predefined alignment. The 40-dimensional filterbank features with their first and second-order derivatives are stacked using 3 contiguous frames to form 360-dimensional spliced features as inputs. The features are normalized via mean subtraction and variance normalization on a per-speaker basis. The learning rates starts at 0.00004 and remains unchanged until the drop of label error rate on validation set between two consecutive epochs falls below 0.5%. From then on, the learning rate is halved at the subsequent epochs. A context-dependent Gaussian mixture model-hidden Markov model (GMM-HMM) system is firstly trained using MFCC including the deltas and deltas-deltas to obtain the alignments.
The ASR system using only Frisian (fy) graph gives similar recognition performance to the baseline CS system on monolingual Frisian utterances, which indicates that the latter CS system has the ability to recognize monolingual Frisian speech as well as a monolingual Frisian ASR system. For monolingual Dutch utterances, the performance by using only Dutch (nl) graph is slightly better than baseline CS system on the test set with a WER of 27.9% compared to 29.0%. Using the largest monolingual Dutch graphs nl++ yields a WER of 25.9% on the Dutch utterances respectively, revealing that the performance of the baseline CS graph can be improved by using larger monolingual Dutch graph in a multi-graph decoding framework.
The number of Frisian and Dutch words in each component of development and test sets are presented in the upper panel. Then two baseline results using single-graph systems (cs and interp-nl++) are shown in the middle panel. Compared to the baseline E2E CS ASR system, using the interpolated larger Dutch LM brings marginal improvements from 33.7% (29.0%) to 32.3% (28.7%) on the development (test) set. This indicates that using interpolated larger LM in single graph is ineffective in improving the accuracy on monolingual utterances.
For the Demonetization data, we started with default parameters α = 0.1; β = 0.01 and input parameter topic number N = 5, 10, 15, 20 which means 5, 10, 15, 20 desired topics. = 15 as a basic group for further comparison since when N = 15 , most topics have enough words to reveal information about the topic while without too much words to make the topics messy. In the next step of our experiment, we set N = 15 and tuning parameter α and β by setting α = 0.1, 0.05, 0.2 while β = 0.01, 0.015, 0.007 to see if the results show any difference. In our experiments, we evaluated NMI of LDA with different topic numbers.
Here we evaluate the individual performance of the topic-specific embeddings. We also compare them to general-purpose word2vec encodings trained on the full corpus of English Wikipedia articles. For each of these embeddings, we evaluate the previously defined CNN network and average the results over five runs (the dataset was randomly split into train and test sets (70%-30%) for each run). As expected, the general Wikipedia embedding is always best or a close second on every task (see also in the plot for the PSG dataset in Fig. On the other hand, the domain-specific embeddings are worse for most of the tasks in prediction accuracy.
However the best result is achieved by the ensemble of 4 independent model types, combining deep stacked and deep transition architectures with GRU and LSTM recurrent unit functions. This ensemble model gives an improvement of 4.40 BLEU points over the best deep stacked LSTM model. Applying re-ranking by the N-gram language model on top of the ensemble system of 4 independent models gives a further improvement of 0.11 BLEU point on average, which gives the best result of our system.
Our system with re-ranking is 0.3 BLEU point better than without re-ranking, and the improvement is statistically significant (p<0.05).
Quantitative evaluations of different methods on the four benchmarks are provided. We first analyze the results on Places-Fine for all benchmark tasks. To conclude, we see that place-related tasks are often very challenging: 1) Places of parks, gardens, churches, \etcare easy to classify; However, it is difficult to distinguish one park/garden/church from another. 2) Under bad environmental condition, photos can be extremely difficult to categorize; 3) To recognize the function of a street or a shop is non-trivial, \ieit is hard to determine their use for people to have a dinner, take a drink, or go shopping. 4) Cities of long history such as Beijing and Florence are often recognized with a high accuracy. While images of others are more likely to be misclassified as similar ones inside and outside their countries. We hope that Placepedia with its well-defined benchmarks can foster more effective studies and thus benefit place recognition and retrieval. The last line of Tab.
To counteract the imbalancing, I tried undersampling the training set to a balanced number of each label. I expect further improvements to come from tuning and kernel selection, but I am not convinced that SVM is an appropriate model for this problem. As CNNs do not scale as nicely as SVMs, it was necessary to reduce the extremely large feature set. As a preliminary selection, I just selected out the counts of the 100 most common words. This model also initially experienced the same issues that SVM had with the imbalanced labels. One advantage of this model is the ability to continuously tune over time.
A correct judgment is where a run-on sentence is detected and a PERIOD is inserted in the right place. Across all datasets, roCRF has the highest precision. We speculate that roCRF consistently has the highest precision because it is the only model to use POS and syntactic features, which may restrict the occurrence of false positives by identifying longer distance, structural dependencies. roS2S is able to generalize better than roCRF, resulting in higher recall with only a moderate impact on precision. On all datasets except RealESL, roS2S consistently has the highest overall F0.5 score. In general, Punctuator has the highest recall, probably because it is trained for a more general purpose task and tries to predict punctuation at each possible position, resulting in lower precision than the other models.
Learning the linguistic features (chunking and the POS tags) simultaneously along with NER tagging under the Vanilla MTL approach yielded a marginal performance gain of about 0.25 over the single models, BiLSTM-CRF(1) and BiLSTM-CRF(2). This enables the model to learn a shared representation beneficial to the main task.
To compare the performance of the proposed method with other existing methods, we implement three existing causality detection systems. We implement the commonsense causality detection method (Commonsense) proposed by Luo et al. The method takes a set of candidate causal phrases as input and decides if there is a causal relationship between each pair of phrases by calculating their causal strength based on the knowledge available in the causal network. For this approach, we use the same causal network that we use for our proposed method. Additionally, we implement another approach proposed by Sasaki et al. We set iterations to 150, learning rate to 0.1 and batch size to 10 to train the neural network for this method. This outcomes demonstrate that the proposed event context word extension technique is capable of overcoming the issue of insufficient context information in candidate causal event pairs in tweets.
Analyze the Impact of Training Set Size. As the seen types in the training set increase, the performance of unseen relation extraction will become better. The reason may be that the diversity of training set reduces the tendency of the model to overfit seen types. In addition, most of the correct extractions appear in the front part (i.e. top K=5) of the candidate type ranks. It proves the validity of our premises, where the semantic distance between each sample and its corresponding prototype tends to be minimal.
By defining these patterns we were able to check if a particular stress trend could be related to a calorie shift. We ran a one-way ANOVA which showed statistically significant difference in the distributions (p ≈ 0.008; F-value = 3.9; Df = 3; η2≈ 0.008). An additional Tukey Honestly Significant Difference (HSD) test revealed that the only statistically significant difference was between the Stress Absence and Stress Stasis trends ( p ≈ 0.031). In other words, there was a significant difference in how people ate when they were consistently stressed during the weeks compared to when they weren’t stressed.
From these tables, first of all, we can observe that VVD is able to maintain or even improve the reported accuracy on DC and NLU tasks, the accuracy of VVD is reported under dropping out the words with dropout rate larger than 0.95. The exception is in NLI Williams et al. et al. It is worth noting that Frequency-based/TF-IDF methods are based on the model trained with cross entropy, while both Group-Lasso and VVD modify the objective function by adding additional regularization. It can be seen that VVD is performing very similar to the baseline models on DC and NLU tasks, while consistently outperforming the baseline methods (with random initialized embedding) on more challenging NLI and Yelp-Review tasks, that said, VVD can also be viewed as a generally effective regularization technique to sparsify features and alleviate the over-fitting problem in NLP tasks. ^V , VVD decreases at a much lower rate than the competing algorithms, which clearly reflects its superiority under limited-budget scenario. From the empirical result, we can conclude that: 1) the retrieval-based selection algorithm can yield marginal improvement over the AUC metric, but the vocab@-X% metric deteriorates. 2) group-lasso and VVD algorithm directly considers the connection between each word and end classification accuracy; such task-awareness can greatly in improving both evaluation metrics. Here we show that NLU datasets are relatively simpler, which only involves detecting key words from human voice inputs to make decent decisions, a keyword vocabulary within 100 is already enough for promising accuracy. For DC datasets, which involve better inner-sentence and inter-sentence understanding, hundred-level vocabulary is required for most cases. NLI datasets involve more complicated reasoning and interaction, which requires a thousand-level vocabulary.
We first wish to shed light on the question of whether cascaded or direct models can be expected to perform better. This question has been investigated previously Weiss et al. We hypothesize that the increased complexity of the direct mapping from speech to translation increases the data requirements of such models. The direct model is trained with multi-task training on the auxiliary ASR, MT, and AE tasks on the same data which outperformed single-task training considerably in preliminary experiments. As can be seen, the direct model outperforms the traditional cascaded set-up only when both are trained on the full data, but not when using only parts of the training data. This provides evidence in favor of our hypothesis and indicates that direct end-to-end models should be expected to perform strongly only in a case where enough training data is available.
Again, all models are trained in a multi-task fashion by including auxiliary ASR and MT tasks based on the same data. The last row in the table confirms that the block dropout operation contributed to the gains: removing it led to a drop by 0.66 BLEU points.
However, when examining the relative gains we can see that both the cascaded model and the models with two attention stages benefitted about twice as much from the external data as the direct model. In fact, the basic two-stage model now slightly surpasses the direct model, and the best APM is ahead of the basic two-stage model by almost the same absolute difference as before (2.36 BLEU points). The superior relative gains show that our findings from § Out-of-domain data is often much easier to obtain, and we can therefore conclude that the proposed approach is preferable in many practically relevant situations. Because these experiments are very expensive to conduct, we leave experiments with external ASR data for future work.
The optimal LM weight of λ=0.5 was selected by tuning on the development set. LM rescoring leads to a relative improvement of 21.7% over the LAS baseline. Next we present results by augmenting the LAS training set using speech synthesized from the text-only training data. However, the combination of TTS-augmented training and LM rescoring improves over LM rescoring along, demonstrating the complementarity of the two methods. First, we calculate performance of the SC method when only considering the top hypothesis output by the recognizer. We find that the attention weights are generally monotonic, and where errors occur, the attention weights are aligned to adjacent context, helping the model to choose a more suitable output. The optimal weights found by tuning on dev set are: λLAS=0.7, λSC=1.0, and λLM=0.1. We use the same parameters on the test set. This promising result shows that the probability scores emitted by each of the three models are complementary to each other. To address this audio mismatch issue, we look to add noise to the TTS data to make sound less ”clear” and thus have a noisier n-best list. Performance of the SC model with MTR data improves over clean data. Overall, after applying LM rescoring to the MTR-ed SC model, we achieve a 29.0% relative improvement over the LAS baseline.
Generating richer n-best lists Two configurations of the SC model are considered: correcting the top hypothesis emitted by the LAS model, leading to final list of 8 candidates, and correcting all 8 entries in the LAS n-best list, leading to an expanded list of 64 candidates. When correcting only the top LAS hypothesis, the SC model gives only a small improvement in oracle WER. However, when applying the SC model independently to each entry in the full LAS n-best list, the oracle WER is significantly reduced to almost half. This demonstrates that the SC model is able to generate a richer and more realistic list of hypotheses, which is more likely to include the correct transcript.
5.1.2 Train on more realistic TTS dataset Using TTS data to synthesize errors the LAS model makes is not perfect as there is a mismatch between the TTS data and real audio. Specifically, the table shows the performance of LAS model on LibriSpeech dev set and a TTS dev set generated using the same transcripts. The SC model which is applied on a TTS test set performs better than the real audio test set, even after LM rescoring.
We observe that the baselines with LSTM encoders outperform the CNN ones, to be followed by the word embedding baselines. This is not surprising, since the LSTM is the only baseline that can fully capture the context of a word. The CNN baseline, using position encodings, actually performs surprisingly well, despite having a receptive field of only five words.
Seven LDA clusters contain tweets from all three incidents. K-means reveals better performance with average purity 0.94. However, although most k-means clusters relate to a single incident, one includes dissimilar tweets from all three incidents, including train outages at Paddington, descriptions of the Boston bombing, and reports of gunmen in the Ivory Coast.
In this section, we evaluate the performance of our models by comparing them with various baseline models. The best score of each column is marked in bold. Firstly, we observe that BERT-Original, BERT-Soft, and BERT-Hard outperform Particularly, BERT-Original outperforms AF-LSTM(CONV) by 2.63%∼9.57%, BERT-Soft outperforms AF-LSTM(CONV) by 2.01%∼9.60% and BERT-Hard improves AF-LSTM(CONV) by 3.38%∼11.23% in terms of accuracy. Considering the average score across eight settings, BERT-Original outperforms AF-LSTM(CONV) by 6.46%, BERT-Soft outperforms AF-LSTM(CONV) by 6.47% and BERT-Hard outperforms AF-LSTM(CONV) by 7.19% respectively.
Multi-aspect training set. Since we use all multi-aspect sentences for testing, we need to generate some “virtual” multi-aspect sentences for training. The simulated multi-aspect training set includes the original single-aspect sentences and the newly constructed multi-aspect sentences, which are generated by concatenating multiple single-aspect sentences with different aspects. The number of Neutral sentences is the least among three sentiment polarities in all single-aspect sentences. We randomly select the same number of Positive and Negative sentences. Then we construct multi-aspect sentences by combining single-aspect sentences in different combinations of polarities. The naming for different combinations is simple. For example, 2P-1N indicates that the sentence has two positive aspects and one negative aspect, and P-N-Nu means that the three aspects in the sentence are positive, negative, and neutral respectively. For simplicity, we only construct 2-asp and 3-asp sentences which are also the majority in the original dataset.
Results and Discussions. The performance of BERT-Hard is better than BERT-Original and BERT-Soft over all types of multi-aspect sentences. BERT-Hard outperforms BERT-Soft by 2.11% when the aspects have the same sentiment polarities. For multi-aspect sentences with different polarities, the improvements are more significant. BERT-Hard outperforms BERT-Soft by 7.65% in total of Diff. The improvements are 5.07% and 12.83% for the types 2-3 and More respectively, which demonstrates the ability of our model on handling sentences with More aspects. Particularly, BERT-Soft has the poorest performance on the subset Diff among the three methods, which proves that soft attention is more likely to cause attention distraction. Intuitively, when multiple aspects in the sentence have the same sentiment polarities, even the attention is distracted to other opinion words of other aspects, it can still predict correctly to some extent. In such sentences, the impact of the attention distraction is not obvious and difficult to detect. However, when the aspects have different sentiment polarities, the attention distraction will lead to catastrophic error prediction, which will obviously decrease the classification accuracy. It means that the type of Diff is difficult to handle. Even though, the significant improvement proves that our hard-selection method can alleviate the attention distraction to a certain extent. For soft-selection methods, the attention distraction is inevitable due to their way in calculating the attention weights for every single word. The noisy or irrelevant words could seize more attention weights than the ground truth opinion words. Our method considers the opinion snippet as a consecutive whole, which is more resistant to attention distraction.
Results. For most methods, the performance on non-native English speakers is lower than native speakers. This is to be expected due to the larger variances in non-native speech, especially for complex Latin medical words. Surprisingly, Cold Fusion has a higher WER and lower BLEU than the LAS model, despite cold fusion using an external language model. One explanation is the establishment of a dependence, on a potentially biased language model. Our method could be viewed as the same as cold fusion, but with a smaller mixing parameter. However, we did not run exhaustive tuning to find the optimal hyperparemeters for this dataset. As for CTC, the poor performance could partially be attributed to CTC’s design for phoneme-level recognition. In our task, we evaluate CTC at word-level, which significantly increases the branching factor (i.e., there are more unique words than phonemes).
We compare the DGN model on the task of identifying supporting facts against the neural baseline reported in Yang et al. In order to suit the task, the baseline architecture extends the state-of-the-art answer passage retrieval model Seo et al. The model is trained jointly and under strong supervision on the objectives of retrieving both answer and supporting facts. We replicate the experiment on our infrastructure in order to obtain more detailed measures, such as precision an recall. Regarding the baseline model, we aim to analyse the impact of multi-task learning, where the model is jointly trained to retrieve supporting facts and the final answer. We observe a significant drop in performance (≈20% F1 score) This observation is perfectly in line with the literature. Hashimoto et al. Regarding multi-hop QA, the identification of supporting facts directly depends on the answer being predicted correctly and vice-versa. A plausible future work may be to understand whether DGN can benefit from a similar multi-task learning setup. Finally, we investigate the role of the semantic information expressed explicitly in the Document Graph. To that end, we train the DGN model using the same configuration of the best performing model without edge type information. A promising future direction will be to investigate whether different types of semantic representation benefit the performance of the model and to what extent.
The results of our models as well as the results of the baselines given by Rajpurkar et al. and Yu et al. We can see that both of our two models have clearly outperformed the logistic regression model by Rajpurkar et al. Furthermore, our boundary model has outperformed the sequence model, achieving an exact match score of 61.1% and an F1 score of 71.2%. In particular, in terms of the exact match score, the boundary model has a clear advantage over the sequence model. The improvement of our models over the logistic regression model shows that our end-to-end neural network models without much feature engineering are very effective on this task and this dataset. Considering the effectiveness of boundary model, we further explore this model. Observing that most of the answers are the spans with relatively small sizes, we simply limit the largest predicted span to have no more than 15 tokens and Besides, we tried to increase the memory dimension l in the model or add bi-directional pre-processing LSTM or add bi-directional Ans-Ptr. The improvement on the development data using the first two methods is quite small. While by adding Bi-Ans-Ptr with bi-directional pre-processing LSTM, we can get 1.2% improvement in F1. Finally, we explore the ensemble method by simply computing the product of the boundary probabilities collected from 5 boundary models and then searching the most likely span with no more than 15 tokens. This ensemble method achieved the best performance as shown in the table.
As ‘CHS-UYG’ and ‘UYG-CHS’ are symmetric in the test, they are merge as ‘CHS/UYG’. From these results, we can observe that with all the three systems, the models trained with the Fisher database still work on the new dataset CSLT-CUDGT2014, it is in totally different languages. This indicates that both the i-vector system and d-vector system posses certain cross-lingual generalizability. However, the d-vector systems outperform the i-vector system with a large margin. This therefore demonstrated that the deep feature systems are more robust against language mismatch. Compared to CHS-CHS and UYG-UYG, the performance with enrollment-test language mismatch (CHS/UYG) is clearly worse, though the performance with the d-vector systems is clearly superior compared to the i-vector system.
As in the original work, we report the results of leave-one-environment-out cross-validation on the set of 10 environments. Our dynamic model (D-NMN) outperforms both the logical (LSP-F) and perceptual models (LSP-W) This improvement is particularly notable on the dataset with quantifiers, where dynamic structure prediction produces a 20% relative improvement over the fixed baseline.
We compare the performance of our model with Naive Bayes and Support Vector Machine (SVM) - our model has the best Precision, recall, accuracy, and MCC scores.
In this subsection, we quantitatively evaluate the attention correctness of both the implicit and the supervised attention model. All experiments are conducted on the 1000 test images of Flickr30k. We compare the result with a uniform baseline, which attends equally across the whole image. Therefore the baseline score is simply the size of the bounding box over the size of the whole image.
We observe that BLEU and METEOR scores consistently increase after we introduce supervised attention for both Flickr30k and COCO. Specifically in terms of BLEU-4, we observe a significant increase of 0.9 and 0.7 percent respectively.
To show the positive correlation between attention correctness and caption quality, we further split the Flickr30k test set (excluding those with zero alignment) equally into three sets with high, middle, and low attention correctness. This indicates that higher attention correctness means better captioning performance.
However, these didn’t perform well in cross validation. Hence, they were dropped from the final system. Edinburgh embeddings outperform GloVe embeddings in Joy and Sadness category but lag behind in Anger and Fear category. The official submission comprised of the top-performing model for each emotion category. This system ranked \nth3 for the entire test dataset and \nth2 for the subset of the test data formed by taking every instance with a gold emotion intensity score greater than or equal to 0.5. Post competition, experiments were performed on ensembling diverse models for improving the accuracy. An ensemble obtained by averaging the results of the top 2 performing models outperforms all the individual models.
It is important to understand how the model performs in different scenarios. Since the features used are mostly lexicon based, the system has difficulties in capturing the overall sentiment and it leads to amplifying or vanishing intensity signals. For instance, in example 4 of fear louder and shaking lexicons imply fear but overall sentence doesn’t imply fear. A similar pattern can be found in the \nth4 example of Anger and \nth3 example of Joy. The system has difficulties in understanding of sarcastic tweets, for instance, in the \nth3 tweet of Anger the user expressed anger but used lol which is used in a positive sense most of the times and hence the system did a bad job at predicting intensity. The system also fails in predicting sentences having deeper emotion and sentiment which humans can understand with a little context. For example, in sample 4 of sadness, the tweet refers to post travel blues which humans can understand. But with little context, it is difficult for the system to accurately estimate the intensity. The performance is poor with very short sentences as there are fewer indicators to provide a reasonable estimate.
Embeddings approach In this work, we have selected word vectors generated by Google universal encoder model, Fasttext, and DMD based features. The classification using the selected features are performed using machine learning algorithms such as Random Forest (RF), Decision Tree (DT), Naive Bayes (NB), Support vector machine (SVM) linear and RBF kernels, Logistic Regression, and Random kitchen sinks. The evaluation measures used are accuracy (Acc.), precision (Prec), recall, f1-score (F1). It can be observed that svm linear classifier and Logistic regression has given maximum accuracy of 82.44% and 82.56%.
As baselines, we used the standalone sentiment and sarcasm classifiers, as well as the CNN-based state-of-the-art method by Mishra et Our standalone GRU-based sentiment and sarcasm classifiers performed slightly better than the state of the art, even though this also uses the gaze data present in the dataset but this is hardly available in any real-life setting. In contrast, our method, besides improving results, is applied to plain-text documents such as tweets, without any gaze data. Adding NTN fusion to the multi-task classifier further improved results, giving the best performance for sarcasm detection. Adding an attention network shared between the tasks further improves the performance for sentiment classification.
We report results when using only Word Embeddings (WE), only Context Embeddings (CE), and both of them (WE+CE). In this case we also considered WE and CE that are not generated by our model, but that are variables of the whole architecture trained with the task-level supervision. Both the feature types (WE and CE) are needed to achieve better performances, as expected. This experiment highlights the importance of using embeddings that are pre-trained with our model, that allows us to obtain the best F1 score of 93.30. This value can be compared with the results reported by Collobert et al. (94.32) and by Huang et al. (94.46), taking into account that in our case we did not make use of any hand-crafted feature nor of any kind of post-processing to adjust incoherent predictions. Hence, we can conclude that the proposed architecture provides word and context embeddings that convey enough information to reach competitive performances. Furthermore, it should be considered that the number of parameters in the model is dramatically reduced with respect to such competitors, since there is no word vocabulary.
Given a set of speaker verification model performs a binary classification and tells whether the enrollment and test audios are from the same speaker. We note that our equal-error-rate results on test set of unseen speakers are on par with the state-of-the-art speaker verification models. We compute mel-scaled spectrogram of enrollment audios and test audio after resampling the input to a constant sampling frequency. Then, we apply two-dimensional convolutional layers convolving over both time and frequency bands, with batch normalization and ReLU non-linearity after each convolution layer. The output of last convolution layer is feed into a recurrent layer (GRU). We then mean-pool over time (and enrollment audios if there are many), then apply a fully connected layer to obtain the speaker encodings for both enrollment audios and test audio. Then, s(x,y) is feed into a sigmoid unit to obtain the probability that they are from the same speaker. The model is trained using cross-entropy loss.
All metrics calculate some form of error of the generated audio against the reference audio, thus the lower the better. On the MCD13 metric, our model performs a little worse than the baseline model, but on the GPE and FFE metrics our model performs slightly better. Overall, there is not much difference in terms of these metrics at the end of training between both models. Additionally, we do not find a difference in naturalness for audios synthesized by the two models at the end.
The best result from Neural Programmer is achieved by an ensemble of 15 models. The only difference among these models is that the parameters of each model is initialized with a different random seed. We combine the models by averaging the predicted softmax distributions of the models at every timestep. While it is generally believed that neural network models require a large number of training examples compared to simpler linear models to get good performance, our model achieves competitive performance on this small dataset containing only 10,000 examples with weak supervision.
S4SS3SSS0Px4 Results: Our algorithm outperforms the static baseline by average 10.4% relative improvement in F1 (and up to 41%), which in turn vastly outperforms BM25 (17.5% relative improvement in average F1). This hints that incorporating adaptive learning is more likely to help for cases with higher queries per KB article.
In addition to the default hyper-parameters in UDify, we added adapter size and language embedding size. Note that for adapter-only model (see we used 1024 as adapter-size unlike the final UDapter. To provide fair comparison, mono-udify and multi-udify are re-trained on the concatenation of 13 high-resource languages without multi-tasking that we observe negatively affect the parsing performance. Besides we did not use a layer attention for both our model and the baselines.
It concludes that globally normalized objective can bring better performance in that it can model more structure dependence. Besides, our models also have higher accuracy than models with linear edge features, which shows that modelling non-linear edge features is very important for neural models. It seems that Edge-based-2 achieves better result than Edge-based-1 in NP chunking and shallow parsing, so combining non-linear edge features with node features is helpful in these two tasks.
The first set of results is with a uniformly Random embedding initialisation in [−0.1,0.1]. PreFixed uses the pre-trained skip-gram word embeddings, whereas PreCont initialises the word embeddings with ones from SkipGram and continues training them during LSTM training. Our results show that, in the absence of a large labelled training dataset, pre-training of word embeddings is more helpful than random initialisation of embeddings. Sing vs Sep shows the difference between using shared vs two separate embeddings matrices for looking up the word embeddings. Sing means the word representations for tweet and target vocabularies are shared, whereas Sep means they are different. Using shared embeddings performs better, which we hypothesise is because the tweets contain some mentions of targets that are tested.
All three models perform well when the target is mentioned in the tweet, but less so when the targets are not mentioned explicitly. In the case where the target is mentioned in the tweet, biconditional encoding outperforms unidirectional encoding and unidirectional encoding outperforms Concat. This shows that conditional encoding is able to learn useful dependencies between the tweets and the targets.
A variety of English sentiment analysis datasets are used in this paper. These datasets contain three types of tasks: (1) For sentence-level sentiment classification, Standford Sentiment Treebank (SST-2) socher-etal-2013-recursive and Amazon-2 NIPS2015_5782 are used. In Amazon-2, 400k of the original training data are reserved for development. The performance is evaluated in terms of accuracy. (2) Aspect-level sentiment classification is evaluated on Semantic Eval 2014 Task4 pontiki- etal-2014-semeval. This task contains both restaurant domain and laptop domain, whose accuracy is evaluated separately. (3) For opinion role labeling, MPQA 2.0 dataset wiebe2005annotating; pittir7563 is used. MPQA aims to extract the targets or the holders of the opinions. Here we follow the method of evaluation in SRL4ORL marasovic-frank-2018-srl4orl, which is released and available online. 4-folder cross-validation is performed, and the F-1 scores of both holder and target are reported. Notably, the pre-training only uses raw texts without any sentiment annotation. To reduce the dependency on manually-constructed knowledge and provide SKEP with the least supervision, we only use 46 sentiment seed words. Please refers to the appendix for more details about seed words.
Further pre-training with random sub-word masking of Amazon, Robertabase obtains some improvements. This proves the value of large-size task-specific unlabeled data. However, the improvement is less evident compared with sentiment word masking. This indicates that the importance of sentiment word knowledge. Further improvements are obtained when word polarity and aspect-sentiment pair objectives are added, confirming the contribution of both types of knowledge. Compare “ +SW+WP+AP” with “+Random Token”, the improvements are consistently significant in all evaluated data and is up to about 1.5 points.
In this section, we demonstrate that the extended AMR scheme is learn-able by training a parser on the annotated examples. We present results for the state-of-the-art AMR parser [stog](STOG parser) as a baseline for future follow-up works to parse the natural language surface form into the AMR format. These are preliminary results with a scope for further improvement in the future. We achieved an F1 score (calculated through triplet matches) of 66.24\% on the test set after training on the filtered training set and validating on the filtered dev set. The parser is trained from scratch instead of relying on a pre-trained version of it, since the domain of LDC2017T10 data which STOG parser was reported on is significantly different than the Minecraft data — we found several preliminary fine-tuning results are not as good as the version trained from scratch. Here we show the predicted AMR output for the “bell” construction from the Minecraft dataset
To evaluate the transferability of the models, besides the debates from the last period of Dutch parliament, we have used debates from October 2010 to March 2012 where VVD and CDA were pro-government parties and others were oppositions. We use SVM as the base classifier to predict party that each member belongs to, give the speaches of the members. We have done classification using the SVM itself as well as using SVM by considering probabilities of terms in HSWLM as the weights of features in order to evaluate the effectiveness of HSWLM as the separable representation of data. This way, we make use of HSWLM like a feature selection approach that filters out features that are not essential in accordance to the hierarchical position of entities and make the data representation more robust by taking out non-stable terms. We have employed conventional 5-fold cross validation for training and testing and to maintain comparability, we have used the same split for folding in all the experiments. Comparing the results, it can be seen that SVMHSWLM improves the performance of classification over SVM in all the experiments.
yellow!20!whiteMike: the? ShARC test set. Results show poor performance especially for the macro accuracy metric of both simple baselines and neural state-of-the-art entailment models.
In FCE, CoNLL14, and JFLEG, the BERTBASE model significantly outperformed existing methods and our baseline (without pre-training) in terms of precision, recall, and F0.5. This demonstrates that using a pre-trained deep language representation model is highly effective for grammatical error detection. Furthermore, MHMLA achieved the highest F0.5 on all datasets, outperforming BERTBASE by 2.18 points, 1.29 points, 0.81 points, and 1.32 points on FCE, CoNLL14-{1,2}, and JFLEG, respectively. The scores for the AvgL model were lower than that for our proposed MHMLA model, meaning that naively using information from layers is not as effective as using MHMLA. These results show that using MHMLA and learning task-specific representations improves the accuracy.
To verify the effect of MHMLA, we examined the F0.5 value for each head number. We investigated 1, 2, 3, 4, 6, 8, and 12 heads (i.e. the number of heads up to 12 by which the hidden layer size of 768 can be divided). Regarding FCE, the highest F0.5 score was achieved with 3 heads. For CoNLL14-{1,2} and JFLEG, the F0.5 values were highest with 12 heads, demonstrating that adopting multi-head leads to improved accuracy.
The top three rows list the performances of word-based LSTM CRF models, followed by the word-based CNN CRF models. The results of OOEV in NER keep 100% because of there exist only 8 OOEV entities and all are recognized correctly. It is obvious that character LSTM or CNN representations improve OOTV and OOBV the most on both WLSTM+CRF and WCNN+CRF models across all three datasets, proving that the main contribution of neural character sequence representations is to disambiguate the OOV words. Models with character LSTM representations give the best IV scores across all configurations, which may be because character LSTM can be well trained on IV data, bringing the useful global character sequence information. On the OOVs, character LSTM and CNN gives comparable results.
Image search is defined as choosing a caption from the test set and then asking the system to find which image belongs with the caption. Image annotation is the opposite problem: choosing an image from the test set without its caption, and then asking the system to search over all the captions in the test set and find one of the five which belongs with the image. We report recall@10 as our evaluation metric, or the probability that the correct result is found in the top 10 returned hits. We also compare to Socher et al. reports high recalls on the Flickr30k data (50.5 search and 61.4 annotation), but does not include any results on the Flickr8k data. Although our spectrogram CNN does not perform nearly as well as any of the systems with access to the ground truth text, it massively outperforms a random ranking scheme. This is in spite of the fact that not only does the spectrogram CNN system not have direct access to the ground truth word identity of the caption words, but also that the CNN word embedding vectors are of dimension 1024 rather than 200. We believe that these results are quite promising, and with more training data we expect to see substantial improvements. While by no means perfect, our system reliably aligns salient objects in the images with their associated caption words.
We also trained several different word spectrogram CNNs with varying configurations. and top-5 accuracies of a few of these networks. A two-layer conventional DNN with 1024 units per layer and ReLU nonlinearities achieved a classification accuracy of 75.5%, while adding a third layer brought that number even lower to 69.5%. We speculate that our training set is not large enough to train such a network. We also trained a network with two convolutional layers and one fully connected layer and achieved similar results to the network with only a single convolutional layer. We also explored varying the size and shapes of the convolutional filters, pooling layers, and dimension of the fully connected layers, but the network achieving 84.2% accuracy reflects our best performance. Although these networks show a wide range of top-1 accuracies, it is interesting to note that their top-5 accuracies are all in excess of 90%.
The CBOS model, though, achieves the highest accuracy in the syntactic category and in the total accuracy.
We firstly observe that for the vSim baseline, excepting for word vector representation with vector size 50 learned using GloVe from the Twitter collection , word vector representations learned using either CBOW or GloVe are more effective than the one-hot representation. However, the difference between the MRR-5 performance is not statistically significant (p>0.05, paired t-test). In addition, word vector representations learned either using CBOW or GloVe with vector size 200 is more effective than those with vector size 50.
Many ontologies do not provide entity definitions In fact, only a few (GO, HPO, MeSH) of the ontologies we included have any definitions at all. Usage contexts are derived from scientific papers in Medline, leveraging entity annotations available via the Semantic Scholar project Ammar et al. . In order to obtain the annotations, an entity linking model was used to find mentions of UMLS entities in the abstracts of Medline papers. The sentences in which a UMLS entity were mentioned are added to the econtexts attribute of that entity. For UMLS entity C0751781, “Dentatorubral-Pallidoluysian Atrophy,” an example context: “Dentatorubral-pallidoluysian atrophy (DRPLA) is an autosomal dominant neurodegenerative disease clinically characterized by the presence of cerebellar ataxia in combination with variable neurological symptoms,” is extracted from Yoon et al (2012) Yoon et al. This context sentence was scored highly by the linking model, and provides additional information about this entity, for example, its acronym (DRPLA), the type of disease (autosomal dominant neurodegenerative), and some of its symptoms (cerebellar ataxia). Because there are often numerous linked contexts for each entity, we sample up to 20 contexts per entity when available. We were hoping for better coverage of the overall dataset. We were, however, able to use Wikipedia to increase the overall definition coverage of the entities in our data set to 82.1%.
To first examine whether the pretraining objectives facilitate improved performance on downstream tasks a baseline model was trained for each downstream task, using the entire set of MultiWoz data. To evaluate the full capabilities of the pretraining objectives above, the pretrained models were used to initialize the models for the downstream tasks. This experimental setup speaks to the strength and the generality of the pretrained representations. Using unsupervised pretraining, the models produce dialog representations that are strong enough to improve downstream tasks. The learned representations demonstrate generality because the multiple downstream tasks benefit from the same pretraining. Rather than learning representations that are useful for just the pretraining objective, or for a single downstream task, the learned representations are general and beneficial for multiple tasks.
This experimental setup is designed to mimic the scenario of adding a new domain as the downstream task. It assumes that there are large quantities of unlabeled data for unsupervised pretraining in all domains but that there is a limited set of labeled data for the downstream tasks. More specifically, for each downstream task there are 1000 labeled out-of-domain examples (2% of the dataset) and only 50 labeled in-domain examples (0.1% of the dataset). The performance of the downstream models is computed only on the in-domain test samples, thereby evaluating the ability of our models to learn the downstream task on the limited in-domain data.
InI and MUR learn strong local representations of each utterance. The two novel pretraining objectives, InI and MUR, consistently show strong improvement for the downstream NUG task. Both of these objectives learn local representations of each utterance in the dialog context since both of their respective loss functions use the representation of each utterance instead of just the final hidden state. Both MUR and InI perform poorly on shorter contexts. This further demonstrates that fine-tuned NUG models learn to rely on strong utterance representations, and therefore struggle when there are few utterances.
Coincidentally, MSC and JSC have the same number of unique word/translation pairs (1093). The two corpora contain only 3 actual exceptions to OHPT. The single actual exception in MSC involves the homonyms represented by the noun ‘band’ which is often translated in Italian as ‘banda’. In this case, the homonymy in English (“ring” vs. “group”) is mirrored by an analogous case of homonymy in Italian. The two actual exceptions in JSC involve the English lexical loans ‘case’ and ‘club’, which have the same Katakana written form regardless of the homonym they represent. We attribute these exceptions to the phenomenon of parallel homonymy, which may arise in the process of lexical borrowing. In order to verify that partitioning of translations is a property of homonyms, and not simply of any sense clusters, we perform an additional experiment on MSC. We randomly select two sets of 20 words (i.e. lemma/POS pairs) from our homonym resource and the OntoNotes clusters, respectively. We consider only words that are represented in MSC by senses from exactly two homonyms or two OntoNotes sense clusters. None of the OntoNotes words occur in our homonym resource. This yields 40 words with a similar number of sense-annotated tokens: 6.80 per homonym, and 7.25 per OntoNotes cluster, on average. We find that 16 of the 20 homonym pairs, and 6 of the 20 OntoNotes cluster pairs exhibit strict translation partitioning in MSC. This result is statistically significant (p<0.005) according to the χ2 test. We conclude that homonyms are significantly more likely to exhibit translation partitioning than OntoNotes sense clusters. SemCor is divided into 352 documents, with an average of 642 sense-annotated open-class words per document. A careful analysis of the 14 apparent exceptions reveals that four of them are caused by sense annotation errors in SemCor (e.g., sharp bow of a skiff is annotated as “weapon for shooting arrows”), and one results from an error in the ODE clustering. The 9 actual exceptions involve the homonymous nouns ‘bank’, ‘lead’, ‘list’, ‘port’, ‘rest’, and ‘yard’, as well as the verb ‘lie’. We conclude that fewer than 0.5% of instances in SemCor contradict the OHPD hypothesis.
Although a no-preference option was chosen often—showing that state-of-the-art systems are still not on par with human expectations—the BST models outperform the baselines in the gender and the political slant transfer tasks.
WMT14 has been a popular benchmark to compare MT systems, even though different pre-/post-processing methods make comparisons noisy. We train a recurrent 1-layer (“shallow”) and 4-layer (“deep”) and a Transformer model on the same data as Luong et al. Training the shallow RNN model took about 5 days on one P40 GPU; the deep model took around 9 days, the Transformer 10 days for en-de and 12 days for en-fr. For comparative purposes we report (Moses-)tokenized and compound-splitted (only en-de) multibleu scores. Without checkpoint averaging and extensive hyperparameter tuning, Joey NMT achieves results that come close to these systems.
The results of the D-Bees algorithm have been compared with other optimization methods, like simulated annealing (SA), genetic algorithms (GA), and two ant colony optimization techniques ACA (Schwab et al. 2011) and TSP-ACO (Nguyen and Ock 2011). The upper-bound is the inter-annotator agreement which is approximately 86.44% (Navigli, Litkowski and Hargraves 2007). Moreover, two baselines were provided, namely, a most frequent sense (MFS) system that has achieved 78.89% and a random sense (RS) system that has attained 52.43%.
The most straight-forward way to use BERT representations is to apply them for the classification task. In order to do that, we have fine-tuned the classifier with minimal changes applied to the BERT model architecture during the training phase (the process is performed in a manner similar to Semi-supervised Sequence Learning and ULMFiT fine-tuning process). In our experiments we have used the BERT-Base Multilingual Cased model as a classification base. This pre-trained model supports 104 languages (including Polish) and has over 110M parameters. It consists of a trained Transformer Encoder stack with 12 layers (the size of hidden state is 768) and Multi-Head Attention (12-heads) At the top of the base model, we have added the classification softmax layer.
For each method, we report the accuracy on all image-question pairs of the considered splits and the accuracy values on the three question categories of the VQA 2.0 dataset (i.e. Yes/No, Number, and Others). Additionally, we test our attention-based aggregation method by using a different number of k compressed vectors and different word embedding strategies. As it can be noticed, the model with 1 vector reaches good results surpassing all other baselines. Nevertheless, higher performances can be achieved with 5 and 7 compressed vectors suggesting that a correct answer can be positively influenced by capturing different aspects of the input features. Above a certain numbers of k vectors, we instead observe a degradation of the performance, as demonstrated by the results with 10 vectors. This can be explained by the greater complexity of the model that undermines the benefits of learning different global vectors.
2.0 validation set when using different word embedding strategies. In particular, we compare the results by employing learnable word embeddings and pre-trained GloVe vectors, either fixed or finetuned during training. In our experiments, the GloVe word embeddings lead to an improvement of the final accuracy results using both 5 and 7 compressed vectors. The performance gap between fixed and finetuned GloVe vectors is not very large, but a slight improvement is given when using the finetuned version. For this reason, all experiments are carried out by using the GloVe vectors finetuned during training. On the contrary, learning word embeddings from scratch brings to lower performances in all settings.
For both text and image retrieval, we report the results in terms of recall@K (with K=1,5,10) which measures the portion of query images or query captions for which at least one correct result is found among the top-K retrieved elements. Also in this setting, we compare our aggregation function with respect to the previously defined baselines and we analyze the performance by varying the number of compressed vectors used to aggregate input sequences. In this setting, we do not find beneficial the use of GloVe word vectors and all results are thus obtained by learning word embeddings during training. This suggests that the large amount of textual data contained in the COCO dataset compared to that available for the VQA task can lead to specific and more suited word embedding representations.
we perform ablation experiments over a number of key components of our model in order to better understand their impacts. α? To understand if our model is able to learn a meaningful α, we replace the Mα component with a flat attention module, such that it always outputs α=1.
However, our networks benefited more when we used pretrained features, as these networks had seen sign videos before and learned kernels which can embed more meaningful representations in the latent space. This improved our results drastically, giving us a boost of nearly 7% and 6% of absolute WER reduction on the development and test sets, respectively.
We believe this is due to scale differences of the CTC and word-level cross entropy losses. Increasing the recognition loss weight improved both the recognition and the translation performance, demonstrating the value of sharing training between these related tasks.
Our method reaches the first place on the leaderboard based on accuracy and precision score and 3rd-highest MRR. We note that the Spearman score is not consistent with other scores in the leaderboard; actually, the Spearman score is computed just based on the predicted positive answers, and a method can get very high Spearman score by never predict positive labels.
We train 6 different models with different randomizations, with initializations from MT-DNN (#1,#2,#3) and SciBERT (#4, #5,#6) respectively. This shows that ensembling from different sources has a great advantage than ensembling from single-source models.
(1) The Naïve approach denotes only MedNLI, RQE, QA Note that MNLI is much larger than the medical datasets, so if we use RQE, QA, MedQuAD, MNLI as external data, the performance is very similar to the third setting. We did not conduct experiments on single-dataset settings, as previous works have suggested that multi-task learning can obtain much better results than single-task models Liu et al. Xu et al.
We also explored word-level attention but it performed 2% worse than 4LHN. The result of Doc2Vec is limited. We suspect the reason could be the high imbalanced dataset, as an unsupervised learning method for document representation heavily relies on the distribution of the document.
Results and Discussion. For fair comparison, all feature vectors are obtained through optimized training; the classic logistic regression approach is utilized as the classier; and the reported results are the average values of 10 runs. The values in boldface represent the best results among all the approaches. The proposed JTAV performed the best on all metrics, with 0.623 AUC score, 0.691 F1 score, 68.8% precision score.
Their model can identify the types of discourse relations, such as Concession, Contrast, Pragmatic Concession, and Pragmatic Contrast that types of the class of discourse relation Comparison. The off-the-shelf parser results in a low F1 measure of only 6 with precision of 13.2 and recall of 4 for the dev data. This low performance is not unexpected. First, the parser is trained on the PDTB corpus, which is based on of Wall Street Journal (WSJ) articles and the language is vastly different from the ChangeMyView subreddit. In addition, PDTB has a small number of concessions in general and it is not clear how many of those are argumentative concessions if any. It has to be noted that state of the art discourse parsers do not take into account different pragmatic values underlying a discourse relation. The level of granularity required to classify different types of concessions is higher than that necessary to classify what discourse relation is conveyed by a single connective.
Results. The explicit features alone are helpful to the task, on which the result by the best baseline (Random Forest) is satisfactory. However, the neural encoders for document pairs, even without the explicit features, outperform Random Forest by 4.76% on precision, 1.98% on recall and 0.033 on F1-score. This indicates that the implicit semantic features are critical for characterizing the matching of main and sub-articles. Among the three types of document encoders, the convolutional encoder is more competent than the rest two sequence encoders, which outperforms Random Forest by 6.54% of precision, 8.97% of recall and 0.063 of F1-score. This indicates that the convolutional and pooling layers that effectively capture the local semantic features are key to the identification of sub-article relations, while such relations appear to be relatively less determined by the sequence information and overall document meanings that are leveraged by the GRU and attention encoders. The results by the proposed model which combines document pair encoders and explicit features are very promising. Among these, the model variant with convolutional encoders (CNN+F) obtained close to perfect precision and recall.
Meanwhile, we perform ablation on different categories of features and each specific feature, so as to understand their significance to the task. We have already shown that completely removing the implicit semantic features would noticeably impair the precision. Removing the explicit features moderately hinders both precision and recall. As for the two categories of semantic features, we find that removing either of them would noticeably impair the model performance in terms of recall, though the removal of title embeddings has much more impact than that of text content embeddings. MLP of CNN+F to analyze the relative importance (RI) of each specific feature, which are reported as Fig. It is noteworthy that, besides the text features, the explicit features rtto, rst and dte that are related to article or section titles also show high RI. This is also close to the practice of human cognition, as we humans are more likely to be able to determine the semantic relation of a given pair of articles based on the semantic relation of the titles and section titles than based on other aspects of the explicit features.
(Rel) tasks. For heart failure prediction and relation classification tasks, the embeddings are evaluated in terms of AUC-ROC, AUC-PR and Accuracy, and for semantic similarity with Spearman Correlation Coefficient (ρ). We see that our multi-view, meta-learning approach Med2Meta (M2M) outperforms the single view models M2M_d, M2M_l and M2M_n on all the tasks. This reinforces the contribution of learning embeddings from multi-modal data and means that different types of embeddings contribute significantly according to their semantic strengths. Among the single view models, surprisingly, M2M_d is seen to perform better than the other two in HF task. This could be attributed to most HF patients having distinctive demographics (e.g., older patients). M2M performs comparably to Med2Vec in heart failure prediction task and exceptionally by 29% and 13% increases for relation classification and semantic similarity tasks respectively. Although Med2Vec also includes demographic information during embedding learning, it does not learn from different modalities (e.g., lab results, clinical notes) which can contain salient information for predictive tasks - as is confirmed by better performance of M2M across all the tasks.
As expected, the F1 of each model increases with k, where the possibility of finding the correct similar words against the golden standard increases. Given that the task here is to return the expected set of words, the recall is more important than precision (i.e: False-Negatives are more harmful than False-Positives). In that light, it is obvious that the word2vecLLSperforms better than all other models because it consistently has the highest recall for all values of k. In addition to that, the word2vecLLSmodel also has the highest F1 for all values of k, which is sufficient proof that the small loss in precision does not adversely affect the overall result.
We evaluate the different representations of word relationships using analogy tasks. However, we do not aim to solve word analogies in the traditional sense, since that largely depends on which words are present in each analogy’s 4-tuple. . We then create a set of word pairs for each analogy category: e.g., {(Berlin, Germany), (Paris, France), … } for country-capital. Each type of transformation – translative, orthogonal, and linear – is evaluated by how accurately it maps source words to target words in this set of word pairs. We source our analogies from mikolov2013efficient, as it contains a diverse set of categories. Linear transformations are more accurate than both (0.798). We would expect linear transformations to outperform orthogonal ones, given that the set of possible linear transformations is a superset of the set of possible orthogonal transformations. However, it is surprising that linear maps also outperform geometric translations, given that they are ultimately learned using translation vectors. This is to mitigate concerns that because the transformed source vector is mapped to the closest word vector, orthogonal transformations are only accurate due to the sparsity of the word space. This suggests that even if we considered a larger portion of the vocabulary as candidate answers, or if the word space were denser, orthogonal transformations would still be almost as accurate as translations on our task.
The first two lines discussed the situation in which the adaptation process were omitted, and the last two lines discussed the adaptation process were equipped with. The performance drop verified the effectiveness of speaker embeddings. First, without the pre-training process, the speaker embeddings were initialized at random which would be updated during the fine-tuning process. It can be seen that adding the speaker embeddings only during the fine-tuning process can provide an improvement of 0.5% in terms of R10@1, which shows its effectiveness for modelling the speaker change during the conversation. Furthermore, we could observe the similar results with the pre-training process included, which verified the effectiveness of our method again.
Decomposition is achieved on synthetic speech signals (sampled at 16 kHz) for various test conditions. The idea is to cover the diversity of configurations one can find in continuous speech by varying all parameters over their whole range. As the mean pitch during these utterances was about 180 Hz, it can be considered that fundamental frequency should not exceed 100 and 240 Hz in continuous speech. For the filter, 14 types of typical vowels are considered. Noisy conditions are modeled by adding a white Gaussian noise to the speech signal, from almost clean conditions (SNR=80dB) to strongly adverse environments (SNR=10dB). It is worth mentioning that the synthetic tests presdented in this section focus on the study of non-pathological voices with a regular phonation. Although the glottal analysis of less regular voices (e.g presenting a jitter or a shimmer; or containing an additive noise component during the glottal production, as it is the case for a breathy voice) is a challenging issue, this latter problem is not addressed in the present study.
We use biaffine classifiers, with no nonlinearities, and a diagonal tensor in the label classifier but not the edge classifier. The system trains at a speed of about 300 sequences/second on an Nvidia Titan X and parses about 1,000 sequences/second. Du et al. PTS17: Basic represents the single-task versions of Peng et al. WCGL18 ’s ’s Many infrequent words were excluded from the frequent token embedding matrix, so it makes sense that the system should improve when provided more lexical information that’s harder to overfit on.
For these three tasks, their input embeddings are different. For Chinese word segmentation, we take both character embeddings and character bigram embeddings for calculating →e(xi). For POS tagging, →e(xi) consists of word embeddings and character embeddings. For NER, we include word embeddings, character embeddings and POS embeddings for →e(xi). During training, all these aforementioned embeddings will be fine-tuned. Dropout technology has been used to suppress over-fitting in the input layer.
Compared with both discrete and neural models, our joint models can achieve best results among all three datasets. In particular, the joint model can outperform the baseline discrete model by 0.43, 0.42 and 0.37 on PKU, MSR, CTB60 respectively. In order to investigate whether the discrete model and neural model can benefit from each other, we scatter sentence-level segmentation accuracy of two models for three datasets in Fig. As we can see from Fig. This common phenomenon among three datasets suggests that the neural model and the discrete model can be combined together to enjoy the merits from each side.
Results. The stacked flow5 model improves over the RGB baseline by 0.39% while two-stream fused1 and fused5 models have 3.15% and 1.31% improvement respectively. Both variants of two-stream models, fused1 and fused5, outperform all one-stream models, RGB, flow, and flow5. All models perform better than randomly selecting an object from the set of tubelets.
Results. The object detection baseline selects the tubelet with the highest confidence score from FGFA. We hypothesize that it is from bounding box perturbation that may affect both Faster RCNN features and location features. The results also show that the performance drops are more severe in two-stream models - we think that it is from an accumulation of errors from both streams.
Another dividing line between representational approaches is whether target variables are encoded in terms of (strict) class-membership or scores for numerical strength. In the first case, emotion analysis translates into a (multi-class) classification problem, whereas the latter turns it into a regression problem (Buechel16ecai). Accordingly, this paper treats word emotion prediction as a regression problem.
It is evident from the table that the proposed SEG-NMT significantly outperforms the baseline model in all the cases, and that this improvement is not merely due to their copying over the most similar translation from a training set. For Fr-En and En-Fr, we also present the performance of using a “CopyNet” variant which uses a copying mechanism directly over the target side of the searched translation pair. This CopyNet variant helps but not as much as the proposed approach. We conjecture this happens because our proposal of using a key-value memory captures the relationship between the source and target tokens in the retrieved pairs more tightly.
This idea has lead to techniques which analyse large text corpora to build word representations. Word representations built from such corpora are typically vectors describing the frequency with which the word occurs in different contexts. Depending on the application, the set of contexts which form the basis for the vector space will vary: Document identifiers: the context of a word is treated as the ID of the document in which it occurs. Other words: a word is considered to cooccur with another word if they are seen together within a window of a fixed number of words, or within the same sentence. Grammatical relations: sentences may be parsed to give dependency relations between words, and these relations treated as contexts.
Our baseline is a modified BiDAF seo2016bidirectional model with word-level embedding provided by With the default hyper-parameters, e.g learning rate, decay weight etc. We submitted our best model so far to the test PCE leaderboard and obtained the results F1: 75.84, EM: 72.24. We list the performance data of the baseline, our best model, and the two ablation experiments. The performance differences have been described in the last section.
To evaluate the quality of generated product short titles, we follow Wang et al. Here we consider ROUGE-1 (1-gram), ROUGE-2 (bi-grams), ROUGE-L (longest common subsequence). From this table, we note that our proposed MM-GAN achieves best performance on three metrics. Furthermore, when comparing MM-GAN with GAN, we can see that our proposed MM-GAN achieves an improvement of 8.86%, 5.92%, 5.53%, in terms of ROUGE-1, ROUGE-2, ROUGE-3, respectively. This verifies that additional information such as image and attribute tags from product can absolutely facilitate our model to generate better short titles. In addition, compared with the best model Agree-MTL, MM-GAN improves ROUGE-1, ROUGE-2, ROUGE-L by 3.34%, 2.99%, 1.76%, respectively. We attribute the outperformance of MM-GAN to two facts: (a) it incorporates multiple sources, containing more information than other single-source based models. (b) it applies a discriminator to distinguish whether a product short titles are human-generated or machine-generated, which makes the model evaluate the generated sequence in a human-like view, and naturally avoid exposure bias in other methods.
We compute the correlations, using the 100-fold sampling of the previous section: we use the same learned parameters of the click models and we calculate the correlation on the test data sets from this sampling. First, we see that the ranking of our offline metrics is different with respect to the UCTR and with respect to the other absolute metrics. The difference seems to be caused by the definition of the UCTR, which does not account for clicks unlike the other 4 metrics. This finding is in line with the results from Chapelle et al. , where the order of the studied metrics is different with respect to the UCTR metric and with respect to the other online metrics. Second, our novel model-based metric ERR-EIA is the incontestable winner with respect to all absolute metrics. Third, we see that the intent-agnostic metric ERR has the 2-nd place w.r.t. UCTR and outperforms some intent-aware metrics w.r.t. other absolute metrics. We explain this “strange" result by peculiarities of multilingual diversification:
We employ three, for English, widely used Relation Extraction and NER benchmark data sets respectively against a baseline model without supervised attention and models whose attention was either supervised through ET data as in ET and BNC frequencies are similar to the ones used by \newcitebarrett2018sequence, which is concatenating ZuCo with an even larger eye-tracking corpus, the Dundee Corpus Kennedy et al. We perform binary classification and adapt all datasets as described below to get a sentence-level label. As such, the main task was a k-vs.-the-rest binary sentence classification task.
Here the baseline is the DistanceNet with mixture of distance measures, which selects domains in a round-robin fashion. Our model instead applies a dynamic controller to select the source domain to use. We can see from the results that using the dynamic controller improves the individual results, and the average results (better by two standard deviations). In general, we observed that the bandit always improves over the non-bandit baseline (with two std. deviations) even when we simply reuse the best hyperparameters found in the single-source experiments, and when we employ a bandit without the DistanceNet loss (i.e., just cross-entropy).
Both WE and CLE bring substantial performance boost, with CLE providing larger improvements, especially for lemmatization and morphological features. Combining WE and CLE shows that the improvements are complementary and using both embeddings yields further increase.
Surprisingly, averaged results of the four English treebanks show very little decrease when using the multilingual BERT model compared to English-specific one, most likely owing to the fact that English is the largest language used to train the multilingual model. Contrary to English, the Chinese BERT model shows substantial improvements compared to a multilingual model when utilized on the Chinese-GSD treebank, and minor improvements on the Japanese-GSD treebank.
The evaluation includes also 13 treebanks whose languages are not part of BERT Multilingual model. UPOS tagging, unlabeled and labeled dependency parsing profits from BERT embedding utilization, with averaged relative error reduction of 3.8%, 2%, and 0.8%, respectively. On the other hand, lemmatization performance deteriorates, with −2.2% averaged relative error reduction.
In particular, we observe that our single model outperforms other methods, especially in the ’Num’ and ’Other’ categories. This is because the generated captions are capable of providing more numerical clues for answering the ’Num’ questions, since the captions can describe the number of relevant objects and provide general knowledge for answering the ’Other’ questions. Furthermore, an ensemble of 10 models with different initialization seeds results in a score of 69.7% for the test-standard set.
This indicates the insufficiency of directly answering visual questions using a limited number of detection features, and the utility of incorporating additional information about the images. We also observe that our generated question-relevant captions trained with our caption selection strategy provide more helpful clues for the VQA process than the question-agnostic Up-Down captions, outperforming their captions by 1.2%.
It is possible to have a valid derivation of the SIF sentence embedding as the Maximum-a-Posteriori (MAP) estimate of c given s once we assume the generative model p(w∣c)∝exp(api+a⟨w,~c⟩) The proof is similar to that of Lemma 3.1 in \citeauthorarora2016latent \shortcitearora2016latent, and is left as exercise. Now, assume that c is a single context word, and c is its embedding. (7) The resulting embeddings were evaluated on standard similarity (WordSim) and analogy (Google and MSR) tasks. The hyperparameter Z was tuned using grid search and the optimal value was s.t. logZ=13.
Evaluation Results. It can be seen that the BLEU scores and GLEU scores of the semi-supervised models on almost all the datasets are better than the baseline S2S model. This result indicates that the model benefits from the nonparallel data in terms of content preservation. One interesting thing is that the overall BLEU scores on the ancient poems and modern Chinese datasets are lower than other datasets. This result may be explained by the fact that the edit distance between formal and informal texts are smaller than between ancient poems and modern Chinese texts. Therefore, it is more challenging for model to preserve the content meaning when transferring between ancient poems and modern Chinese text. Among three semi-supervised models, CPLS model achieves the greatest improvement, verifying the effectiveness of the projection functions. However, the gain of CPLS model in the aspect of style accuracy is not that significant. A possible explanation may be the bias of the style classifier. Take the transfer task from ancient poems to modern Chinese text for example. We observe that the classifier tends to classify short sentences into ancient poems as length is an obvious feature. We analyse the sentences generated by S2S model and by the CPLS model, and the statistics show that the average length of the text generated by S2S model is shorter, which may lead to the bias of the style classifier. Therefore, we also adopt human evaluation to alleviate this issue.
The CNN/RNN models achieve better results than our BERT and W2V models. This is because they are trained on the DUC2001 and DUC2002 datasets, while our approach is totally unsupervised and uses off-the-shelf neural word BERT or W2V embeddings only without any fine-tuning. Our approach to summarization uses the compressed sentences from a document to do sentence selection for document-level summarization. The sentence compression model used to aid in document-level summarization used only the Gigaword dataset for training. We did not see any additional improvements on DUC-2004 dataset by using the additional Google compression dataset.
On both datasets, the results of deep features are marginally better than those of the tf-idf features.
Due to space constraints, we only report the scores of the best-performing SemEval participants, please refer to \newcitejurgens-klapaftis-2013-semeval for the full results. The performance of AdaGram and SenseGram models is reported according to \newcitePelevina:16.
Both inventories were produced with K=200, which ensures stronger connectivity of graph. However, we see that this setting still produces too many clusters. We can see that the number of senses extracted by our method is consistently higher than the real number of senses.
Compared with the winning systems of SemEval ABSA, our framework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on D1, D2, D3 and D4 respectively. CRF with basic feature template is not strong, therefore, we add CRF-2 as another baseline. WDEmb, which is also an enhanced CRF-based method using additional dependency context embeddings, obtains superior performances than CRF-2. Therefore, the above comparison shows that word embeddings are useful and the embeddings incorporating structure information can further improve the performance. The results show that each of THA and STN is helpful for improving the performance, and the contribution of STN is slightly larger than THA. “OURS w/o THA & STN” only keeps the basic bi-linear attention. Although it performs not bad, it is still less competitive compared with the strongest baseline (i.e., CMLA), suggesting that only using attention mechanism to distill opinion summary is not enough. After inserting the STN component before the bi-linear attention, i.e. “OURS w/o THA”, we get about 1% absolute gains on each dataset, and then the performance is comparable to CMLA. By adding THA, i.e. “OURS”, the performance is further improved, and all state-of-the-art methods are surpassed.
At this stage, the system returns a list of proposed properties as result of the verification phase. This may not be a problem with simple formulae like the ones described, but would be problematic with complete sign descriptions; there is such thing as too much information. In that case, we would need a human being to complete the classification process. This points out the need or a higher level module in charge of cleaning the annotation proposal by way of machine learning techniques.
The first row (Conneau-18) presents the results of \newciteconneau2018word that uses adversarial training to map the word embeddings. The next row shows the results of our full model. The subsequent rows incrementally detach one component from our model. For example, - Enc. adv denotes the variant of our model where the target encoder is not trained on the adversarial loss (θEX in Eq. ; - - Recon excludes the post-cycle reconstruction loss from - Enc. adv, and - - - Cycle excludes the cycle consistency from - - Recon. Thus, - - - Cycle is a variant of our model that uses only adversarial loss to learn the mapping. However, it is important to note that in contrast to \newciteconneau2018word, our mapping is performed at the code space.
Our design philosophy for the teacher model is to use as small inter-block hidden size (feature map size) as possible, as long as there is no accuracy loss. We can see that reducing the inter-block hidden size doesn’t damage the performance of BERT until it is smaller than 512. Hence, we choose IB-BERTLARGE with its inter-block hidden size being 512 as the teacher model. One may wonder whether we can also shrink the intra-block hidden size of the teacher. We can see that when the intra-block hidden size is reduced, the model performance is dramatically worse. This means that the intra-block hidden size, which represents the representation power of non-linear modules, plays a crucial role in BERT. Therefore, unlike the inter-block hidden size, we do not shrink the intra-block hidden size of our teacher model. One may wonder whether reducing the number of heads will harm the performance of the teacher model.
4.6.2 Training Strategies We also study how the choice of training strategy, i.e., auxiliary knowledge transfer, joint knowledge transfer, and progressive knowledge transfer, can affect the performance of MobileBERT. We notice that there is a significant performance gap between auxiliary knowledge transfer and the other two strategies. We think the reason is that the intermediate layer-wise knowledge (i.e., attention maps and feature maps) from the teacher may not be optimal for the student, so the student needs an additional pre-training distillation stage to fine-tune its parameters.
We can observe a behaviour similar to the previous task. Once again, both Cosine and WCD achieve the largest test errors while the others display competitive results.
Again WCD and Cosine are the fastest. The former is slower because it has to compute the centroids of the documents in its preprocessing phase while the latter does not. Among the others, Rel-RWMD(S) and RWMD(S), as expected, are much faster than WMD. For Wikipedia Rel-RWMD(S) is 3 times faster than RWMD(S) while for Arxiv Rel-RWMD(S) it is 27 times faster.
How does VLN-BERT compare with strong baseline methods? For the follower model results this amounts to taking the top beam from the candidate set. In the single model setting we see that VLN-BERT, trained with our full curriculum, achieves 59.3% SR, which is 4.6 absolute percentage points better than either of the other two methods. The two models are typically combined as a linear combination using a hyperparameter α that is selected through grid search on the val unseen split of R2R In rows 5-7, we consider three model ensembles composed of a speaker, follower, and one additional model combined using two hyperparameters α and β (again selected through grid search on val unseen). We find that adding another (randomly seeded) speaker or follower model yields modest improvements of 1.2 and 2.7 absolute percentage points in SR (rows 5 and 6). In contrast, adding VLN-BERT results in a 5.7 absolute percentage point boost in SR (row 7), which is 3.0 absolute percentage points higher on success rate than the next best ensemble.
In the leaderboard setting we use a three-model ensemble that includes a speaker, follower, and VLN-BERT. The ensemble achieves a success rate of 73%, which is 4 absolute percentage points greater than previously published work
For comparison we used the same seed-words as in their study, which aid performance significantly. Note that in this dataset supervision takes the form of per-sentence ratings, rather than per-aspect ratings, though our method can be adapted to handle both. Pale Lager is competitive with highly sophisticated alternatives, while requiring only a few seconds for training. The supervised version of our model, which jointly models text and ratings, outperforms (by 7%) an SVM that uses text data alone. Overall, this is a promising result: our method is competitive with sophisticated alternatives on a small dataset, and scales to the real-world datasets we consider.
The summary on the left shows that our incremental learning framework has outperformed all the existing MDL-based approaches, winning out regularized compression by about 4 points in F-score. On the right, we find the performance of incremental learning has surpassed both unigram and colloc adaptor grammars by a large margin. We also compared our approach with colloc3-syllable adaptor grammars, which is commonly thought as weakly supervised. The result shows that incremental learning approach lagged behind colloc3-syllable by 3 points in F-score. Our interpretation is that this more advanced version of adaptor grammars has built in some linguistic/structural assumptions that have no equivalent in our simplistic framework. In the case of colloc3-syllable, it was a small set of phonemic productions that helps improving syllable-level modeling accuracy.
The summary on the left shows that our incremental learning framework has outperformed all the existing MDL-based approaches, winning out regularized compression by about 4 points in F-score. On the right, we find the performance of incremental learning has surpassed both unigram and colloc adaptor grammars by a large margin. We also compared our approach with colloc3-syllable adaptor grammars, which is commonly thought as weakly supervised. The result shows that incremental learning approach lagged behind colloc3-syllable by 3 points in F-score. Our interpretation is that this more advanced version of adaptor grammars has built in some linguistic/structural assumptions that have no equivalent in our simplistic framework. In the case of colloc3-syllable, it was a small set of phonemic productions that helps improving syllable-level modeling accuracy.
We had our method run to stop on each of the subsets. Our incremental learning method achieved in F-score above 80.0 on all four subsets, and it compares favorably with weaker baselines such as MDL-based methods and DLG. On two sets AS and PKU, incremental learning outperformed the strong baseline ESA, winning out by 4.0 and 1.3 in F-score respectively. On the other two sets MSR and CityU, incremental learning seems on a par with ESA: it improved over the baseline by 0.3 on MSR, but it also lagged behind by 0.3 on CityU.
w/o LD and en-ja by Emu w/o LD+CL.) Emu outperformed the baseline methods including the original LASER model. The original LASER model showed the best performance for zh-en and all of the Emu methods degraded the performance for the task. From the results, Emu consistently outperformed the baseline methods, including the original LASER model. At the same time, Emu failed to improve the performance of the five tasks, namely zh-en on ATIS fr-en, ja-en, ja-zh on Quora We would like to emphasize that the Emu models were trained using labeled data only in English. The Emu also used unlabeled data in non-English languages. Therefore, it is noteworthy that our framework successfully specializes multilingual sentence emebeddings for multiple language pairs, which involve English, using only English labeled data. The results support that Emu is effective in semantically specializing multilingual sentence embeddings. Do we need parallel sentences for Emu? We compared Emu to Emu-Parallel, which uses parallel sentences instead of randomly sampled sentences, to verify whether using parallel sentences makes multilingual adversarial learning more effective. Compared to Emu, Emu-Parallel showed lower Acc@1 values on the three datasets. The decreases were -0.5 points, -1.2 points, and -5.9 points on HotelQA, ATIS, and Quora respectively. The differences are not statistically significant except for Quora. The results show that the language discriminator of Emu does not need any cost-expensive parallel corpus but can improve performance using unlabeled and non-parallel sentences in other languages.
w/o LD and en-ja by Emu w/o LD+CL.) Emu outperformed the baseline methods including the original LASER model. The original LASER model showed the best performance for zh-en and all of the Emu methods degraded the performance for the task. From the results, Emu consistently outperformed the baseline methods, including the original LASER model. At the same time, Emu failed to improve the performance of the five tasks, namely zh-en on ATIS fr-en, ja-en, ja-zh on Quora We would like to emphasize that the Emu models were trained using labeled data only in English. The Emu also used unlabeled data in non-English languages. Therefore, it is noteworthy that our framework successfully specializes multilingual sentence emebeddings for multiple language pairs, which involve English, using only English labeled data. The results support that Emu is effective in semantically specializing multilingual sentence embeddings. Do we need parallel sentences for Emu? We compared Emu to Emu-Parallel, which uses parallel sentences instead of randomly sampled sentences, to verify whether using parallel sentences makes multilingual adversarial learning more effective. Compared to Emu, Emu-Parallel showed lower Acc@1 values on the three datasets. The decreases were -0.5 points, -1.2 points, and -5.9 points on HotelQA, ATIS, and Quora respectively. The differences are not statistically significant except for Quora. The results show that the language discriminator of Emu does not need any cost-expensive parallel corpus but can improve performance using unlabeled and non-parallel sentences in other languages.
w/o LD and en-ja by Emu w/o LD+CL.) Emu outperformed the baseline methods including the original LASER model. The original LASER model showed the best performance for zh-en and all of the Emu methods degraded the performance for the task. From the results, Emu consistently outperformed the baseline methods, including the original LASER model. At the same time, Emu failed to improve the performance of the five tasks, namely zh-en on ATIS fr-en, ja-en, ja-zh on Quora We would like to emphasize that the Emu models were trained using labeled data only in English. The Emu also used unlabeled data in non-English languages. Therefore, it is noteworthy that our framework successfully specializes multilingual sentence emebeddings for multiple language pairs, which involve English, using only English labeled data. The results support that Emu is effective in semantically specializing multilingual sentence embeddings. Do we need parallel sentences for Emu? We compared Emu to Emu-Parallel, which uses parallel sentences instead of randomly sampled sentences, to verify whether using parallel sentences makes multilingual adversarial learning more effective. Compared to Emu, Emu-Parallel showed lower Acc@1 values on the three datasets. The decreases were -0.5 points, -1.2 points, and -5.9 points on HotelQA, ATIS, and Quora respectively. The differences are not statistically significant except for Quora. The results show that the language discriminator of Emu does not need any cost-expensive parallel corpus but can improve performance using unlabeled and non-parallel sentences in other languages.
Before incorporating external commonsense knowledge into relation representations, we were curious how much we lose by restricting the entity-tuple space to approximately Boolean embeddings. We evaluate our models on the New York Times dataset introduced by \newciteriedel2013relation. Surprisingly, we find that the expressiveness of the model does not suffer from this strong restriction. We also provide the original results for model F by \newciteriedel2013relation (denoted as R13-F) for comparison. Due to a different implementation and optimization procedure, the results for our model F and R13-F are not identical. The weighted MAP measure increases by 2% with respect to model FS, and 4% compared to our reimplementation of the matrix factorization model F. This demonstrates that imposing a partial ordering based on implication rules can be used to incorporate logical commonsense knowledge and increase the quality of information extraction systems. Note that our evaluation setting guarantees that only indirect effects of the rules are measured, i.e., we do not use any rules directly implying test relations. This shows that injecting such rules influences the relation embedding space beyond only the relations explicitly stated in the rules. For example, injecting the rule apposappos ⇒ possappos can contribute to improved predictions for the test relation parent/child.
In order to demonstrate that injecting implications conserves their asymmetric nature, we perform the following experiment. After incorporating high-quality Wordnet rules rp⇒rq into model FSL we select all of the tuples tp that occur with relation rp in a training fact ⟨rp,tp⟩. Matching these with relation rq should result in high values for the scores rq⊤tp, if the implication holds. If however the tuples tq are selected from the training facts ⟨rq,tq⟩, and matched with relation rp, the scores rp⊤tq should be much lower if the inverse implication does not hold (in other words, if rq and rp are not equivalent). For easier comparison, the scores are mapped to the unit interval via the sigmoid function. After injecting rules, the average scores of facts inferred by these rules (i.e., column σ(rq⊤tp) for model FSL) are always higher than for facts (incorrectly) inferred by the inverse rules (column σ(rp⊤tq) for model FSL). In the fourth example, the inverse rule leads to high scores as well (on average 0.79, vs. 0.98 for the actual rule). This is due to the fact that the daily and newspaper relations are more or less equivalent, such that the components of rp are not much below those of rq. For the last example (the ambassador ⇒ diplomat rule), the asymmetry in the implication is maintained, although the absolute scores are rather low for these two relations.
Given the inconsistent error labeling in WER, which types of errors are actually being skewed by false alignments? To answer this question, we annotate the reference and hypothesis words by their word class and observe their alignment statistics. We additionally apply lemmatization to distinguish morphological errors from other substitution types. According to the word statistics in Table LABEL:tbl:word-stats, the ratio of open to closed class words remains the same across each ASR hypothesis and the reference (gold).
Note the numbers in parentheses for L2A Du et al. The slight difference of up to 1.7% in the original and reproduced numbers can be attributed to reimplementation and different versions of various libraries used. As can be seen, all our eight models outperform L2A and AutoQG on all evaluation metrics. Two of our models, GEGLEU and GEROUGE, also outperform NQGLC. Hence, using evaluation metrics as the reward function during reinforcement based learning improves performance for all metrics. We also observe that GEROUGE+QSS+ANSS, the model reinforced with ROUGE-L (that measures the longest common sequence between the ground-truth question and the generated question) as the reward function in combination with QG quality specific rewards(QSS+ANSS), is the best performing model on all metrics, outperforming existing baselines considerably. For example, it improves over AutoQG on BLEU-4 by 29.98%, on METEOR by 13.15%, and on ROUGE-L by 8.67%. seven of our eight models outperform the two baselines, with GEDAS+QSS+ANSS being the best model on syntactic correctness and semantic correctness quality metrics, outperforming all the other models by a large margin. However, model GEBLEU+QSS+ANSS generates highly relevant questions and is the best model on relevance metrics.
The correlation was calculated between the matrices S generated by lemmatization (Lemm), stemming (Stem), plain text (Raw) and the matrix S′ generated by Ultra-stemming Fix1 using the initial letter. In all cases the correlation is positive with p-value <0.001, which is significant.
The correlation was calculated between the matrices S generated by lemmatization (Lemm), stemming (Stem), plain text (Raw) and the matrix S′ generated by Ultra-stemming Fix1 using the initial letter. In all cases the correlation is positive with p-value <0.001, which is significant.
The correlation was calculated between the matrices S generated by lemmatization (Lemm), stemming (Stem), plain text (Raw) and the matrix S′ generated by Ultra-stemming Fix1 using the initial letter. In all cases the correlation is positive with p-value <0.001, which is significant.
In this section we study some additional query generation heuristics. Our general findings are - i) random works much better when the paragraphs are sampled from a distribution reflecting the unigram frequency in wikitext103 compared to uniform random sampling ii) starting questions with common question starter words like “what” helps, especially with random schemes.
The results reported for the GAN and CGAN networks are partial, since several experiments were not able to converge (5 and 2 for GAN, 4 and 7 for CGAN). Tweaking the network hyperparameters would certainly allow to improve the convergence rate. In this case, WGAN can be an interesting alternative, since it offers a better stability but at a cost of slight accuracy decrease in our tests. This time, these results emphasize that independently of their structures, GAN-based classifier outperforms DNN. The prediction results of MMC-GAN are improved by 4.8% compared to the baseline, for the structure 4.
The content-based attention model achieved a CER of 4.05% and a SER of 9.1%. The location-based attention model achieved a CER of 3.82% and a SER of 8.17%, which outperformed the content-based attention model. By using attention smoothing on the content-based attention model, the CER was reduced to 3.58% ( or 11.6% relative gain over the content-based attention). We believe that the improvement is mainly because the sigmoid function keeps the diversity of the model and smooths the focus found by the attention mechanism.
When trained over Polyglot vectors, our BoS model works better than EditDist and MIMICK. When trained on Google vectors, the correlation scores are almost as good as those of fastText, the state-of-the-art subword level word embedder. However, unlike fastText, our model does not have access to word contexts in a large text corpus for training. In both cases, the significant differences of scores compared to those of EditDist, suggest that our model indeed learns to capture semantic similarities between words, rather than superficial similarities in spelling.
Our proposed model GRURNTN and LSTMRNTN outperformed both baseline models. GRURNTN reduced the BPC from 1.39 to 1.33 (0.06 absolute / 4.32% relative BPC) from the baseline GRURNN, and LSTMRNTN reduced the BPC from 1.37 to 1.34 (0.03 absolute / 2.22% relative BPC) from the baseline LSTMRNN. Overall, GRURNTN slightly outperformed LSTMRNTN, and both proposed models outperformed all of the baseline models on the character-level language modeling task. Both our proposed models outperformed their baseline models. GRURNTN reduced the perplexity from 97.78 to 87.38 (10.4 absolute / 10.63% relative PPL) over the baseline GRURNN and LSTMRNTN reduced the perplexity from 108.26 to 96.97 (11.29 absolute / 10.42% relative PPL) over the baseline LSTMRNN. Overall, LSTMRNTN improved the LSTMRNN model and its performance closely resembles the baseline GRURNN. However, GRURNTN outperformed all the baseline models as well as the other models by a large margin.
For collecting fake news we include the following types of news in our dataset. Misleading/ False Context: Any news with unreliable information or contains facts that can mislead audiences. Clickbait: News that uses sensitive headlines to grab attention and drive click-throughs to the publisher’s website. Satire/Parody: News stories that are intended for entertainment and parody. we have collected news from popular websites that publish satire news in Bangla. While collecting satirical news from these sites we found that most of the sites have the exact same news. So after scraping news from these sites, we discarded the duplicates. We have collected the misleading or false context type of news from www.jaachai.com and www.bdfactcheck.com. These two websites provide a logical and informative explanation of fake news that is already published on other sites. So we have also collected the news that is mentioned on those two sites from the actual publishing sites and make sure that we avoid the duplicates. And we have found that most of the local or less popular sites usually do this. To collect clickbaits, we have gone through some of these sites and manually collect potential clickbait news from there. We call satire, clickbait, and false informative news all commonly as fake news throughout the paper to avoid ambiguity. We have also collected the following meta-data along with the headlines and content: The domain of the published news site Publication time Category From our dataset , we got 242 different categories as different publishers categorize the news in their own way. To generalize it, we took similar categories from different news to map into a single one.
Baselines: We compare our experimental results with a majority baseline and a random baseline. The majority baseline assigns the most frequent class label (authentic news) to every article, where the other baseline randomly tags an article as authentic or fake. The Standard Deviation(SD) of precision, recall, and F1-score in both overall and fake class is less than 10−2 except the recall of Fake class which is 0.027. Overall performance of every experiment is almost the same. Most of the cases we achieve almost perfect Precision, Recall and F1. But the results of Precision, Recall, and F1-Score of fake class vary in experiments to experiments. In our dataset for experiments, the number of authentic news is 37.47 times higher than the number of fake news which could be the reason behind such variance in results of the overall and fake class. To evaluate the performance of different models we will use the precision, recall, F1-Score of the fake class in the rest of the section. Fig. Here SVM outperforms LR and RF by a quite margin except result of the news embedding. In the case of news embedding RF scores 55% of F1-score and here SVM, LR scores 46%, 53% of F1-score respectively. For most of the features RF performs better than the LR model. It is also observed that the F1-score of fake class in the SVM model decreases while increasing the number of grams in both word and character n-grams.
We have several observations: (1) Using BERT to initialize the encoder of NMT can only achieve 27.14 BLEU score, which is even worse than standard Transformer without using BERT. That is, simply using BERT to warm up an NMT model is not a good choice. (2) Using XLM to initialize the encoder or decoder respectively, we get 28.22 or 26.13 BLEU score, which does not outperform the baseline. If both modules are initialized with XLM, the BLEU score is boosted to 28.99, slightly outperforming the baseline. Although XLM achieved great success on WMT’16 Romanian-to-English, we get limited improvement here. Our conjecture is that the XLM model is pre-trained on news data, which is out-of-domain for IWSLT dataset mainly about spoken languages and thus, leading to limited improvement. (3) When using the output of BERT as context-aware embeddings of the encoder, we achieve 29.67 BLEU, much better than using pre-trained models for initialization. This shows that leveraging BERT as a feature provider is more effective in NMT. This motivates us to take one step further and study how to fully exploit such features provided by pre-trained BERT models.
We implemented standard Transformer as baseline. Our proposed BERT-fused model can improve the BLEU scores of the five tasks by 1.88, 1.47, 2.4, 1.9 and 2.8 points respectively, demonstrating the effectiveness of our method. The consistent improvements on various tasks shows that our method works well for low-resource translations. We achieved state-of-the-art results on IWSLT’14 De→En translation, a widely investigated baseline in machine translation.
Results We can see that introducing contextual information from an additional encoder can boost the sentence-level baselines, but the improvement is limited (0.33 for En→De and 0.31 for De→En). For Miculicich et al. Combining BERT-fused model and document-level information, we can eventually achieve 31.02 for En→De and 36.69 for De→En. Our document-level BERT-fused model significantly outperforms sentence-level baseline with p-value less than 0.01. This shows that our approach not only works for sentence-level translation, but can also be generalized to document-level translation.
We have the following observations: Adding more layers can indeed boost the baseline, but still not as good as BERT-fused model. According to our experiments, when increasing the number of layers to 12, we achieve the best BLEU score, 29.27. We also compare our results to ensemble methods. Indeed, ensemble significantly boost the baseline by more than one point. However, even if using ensemble of four models, the BLEU score is still lower than our BERT-fused model (30.18 v.s. 30.45), which shows the effectiveness of our method.
Due to the incompleteness of the knowledge base, held-out evaluation introduces some false negatives. The precision from held-out evaluation is therefore a lower bound of the true precision. Each prediction is examined by two human experts who reach agreement with discussion. To ensure fair comparison, the experts are not aware of the provenance of the predictions. Under manual evaluation, PCNN+ATT+GloRE achieves the best performance in the full range of N. In particular, for the top 1,000 predictions, GloRE improves the precision of the previous best model PCNN+ATT from 83.9% to 89.3%. The manual evaluation results reinforce the previous observations from held-out evaluation.
In this biased data scenario we observe a marked improvement across metrics and answer type categories when a model is trained with unanswerable samples (robust training). This demonstrates that the negative training signal stemming from related – but unanswerable – questions counterbalances the signal from answerable questions in such a way, that the model learns to better take into account relevant information in the question, which allows it to correctly distinguish among several type-consistent answer possibilities in the text, which the standard BERT Base model does not learn well.
For completeness and direct comparability, we also include an experiment with the data setup of lewis2018generative (not holding aside a dedicated validation set). We again observe improvements in the biased data setting. The robust model outperforms GQA (lewis2018generative) in two of the three subtasks.
We can see that our attention mechanisms achieve consistent improvement. We conduct paired t-test between our proposed model and all the other baselines on 10 randomly sampled subsets. The differences are statistically significant with p≤0.016 for all settings.
All models are trained using the same settings described in above section, and the default output facts (without any confidence filtering) are evaluated by the automatic judgment. From the results, we can see that the model involving all the components and shallow tag information archives the best performance. We use that model to attend the comparisons with existing approaches.
Our NER model is simply the baseline plus an NE hypersphere guide enhancement. For Chinese evaluation, even though we use the same hyper-parameters as for English, our model also outperforms the BiLSTM-CRF baseline by a large margin, especially with 1.05% improvement on MSRA dataset.
IEMOCAP: As this dataset contains 10 speakers, we performed a 10 fold speaker independent test, where in each round, one of the speaker was in the test set. The same SVM model was used as before and macro F_score was used as a metric. MOUD: This dataset contains videos of about 80 people reviewing various products. Here, reviewers review products in Spanish. Each utterance in the video has been labeled to be either positive, negative or neutral. In our experiments we consider only the positive and negative sentiment labels. The speakers were divided into 5 groups and a 5-fold person independent experiment was run, where in every fold one out of the five group was in the test set. Finally we took average of the macro F_score to summarize the results (see Table- MOSI: The MOSI dataset is a dataset rich in sentimental expressions where 93 people review topics in English. The videos are segmented with each segment’s sentiment label scored between +3 to −3 by 5 annotators. We took the average of these labels as the sentiment polarity thus considering two classes positive and negative as sentiment labels. Like MOUD, speakers were divided into 5 groups and a 5-fold person independent experiment was run. During each fold, around 75 people were in the train set and the remaining in the test set. The train set was further split randomly into 80%–20% and shuffled to generate train and validation splits for parameter tuning.
4.2.1 Comparison with the Speaker Dependent Experiment In comparison to speaker dependent experiment, speaker independent experiment performs poor. This is due to the lack of knowledge about speakers in the dataset. It can be seen that audio modality consistently performs better than visual modality in both MOSI and IEMOCAP datasets. The text modality plays the most important role in both emotion recognition and sentiment analysis. The fusion of the modalities show more impact for emotion recognition than on sentiment analysis. As expected in all kinds of experiments, bimodal and trimodal models have performed better than unimodal models. Overall, audio modality has performed better than visual on all the datasets. The present method outperformed state of the art by 12% and 5% respectively on the IEMOCAP and MOSI datasets. The method proposed by Poria et al. is similar to us except they used a standard CLM based facial feature extraction method. So, our proposed CNN based visual feature extraction algorithm has helped to outperform the method by Poria et al.
For Person, Creative Work, and Location types, our model found on average more than 80% of entities. On the other hand, for Group and Other types, detection rates dropped remarkably. We found that some of those entities do not appear in emerging contexts at all within our Twitter archive. Since our method utilizes such emergence signals as the clue, it is difficult to discover entities appearing without emerging contexts. This is the current limitation of our method. Note that the 13,406 entities used in this evaluation included some prevalent entities (e.g., local company) that might also affect the performances.
Though GloVe embeddings show considerable improvement for both in text-only NMT and all types of multimodal NMT, word2vec and FastText embeddings greatly reduce model performance even with some debiasing. With GloVe embeddings, All-but-the-Top debiasing results in further improvement. In particular, IMAGINATION is improved with GloVe embedding initialization (+0.77 BLEU and +0.79 METEOR) and showed further improvement with All-but-the-Top debiasing (+1.62 BLEU and +1.14 METEOR).
To generally examine the performance of each framework, we first run models on all datasets. For each dataset, we report the PMR, WLCS-l and number of parameters of each measure. Note that PMR values might be lower than those in previous works since we remove all the texts containing only one sequence. The results confirm our expectation that graph model is much more powerful for ordering task, since it automatically learns directional information flow among each sequence rather than through a central weight matrix as in ATTOrderNet or by a single hidden layer as in Variant-LSTM+PtrNet. At the sentence level, SE-Graph almost dominates all previous models on both the global coherence (measured by PMR) and the local coherence(measured by WLCS-l). At the paragraph level, SE-Graph continues to be the state-of-the-art method in most cases. It acquires the best WLCS-l scores on three datasets and the best PMR on two, which indicates its adaptability on long sequence tasks.
To generally examine the performance of each framework, we first run models on all datasets. For each dataset, we report the PMR, WLCS-l and number of parameters of each measure. Note that PMR values might be lower than those in previous works since we remove all the texts containing only one sequence. The results confirm our expectation that graph model is much more powerful for ordering task, since it automatically learns directional information flow among each sequence rather than through a central weight matrix as in ATTOrderNet or by a single hidden layer as in Variant-LSTM+PtrNet. At the sentence level, SE-Graph almost dominates all previous models on both the global coherence (measured by PMR) and the local coherence(measured by WLCS-l). At the paragraph level, SE-Graph continues to be the state-of-the-art method in most cases. It acquires the best WLCS-l scores on three datasets and the best PMR on two, which indicates its adaptability on long sequence tasks.
The wavelength λ defines the reference length scale. The material’s complex refractive index nc(λ) required for the Fresnel coefficient Fig. These plots allow us to visualize how the incident energy diffuses in space as a function of (θd,ϕd). The effect of an increase in surface roughness is illustrated by the transition from a near specular reflection to a large diffusion lobe. As roughness increases, the contribution of ρsr diminishes with respect to ρud. The case of a smooth surface (σ0=0) is naturally included in ρnonp and does not require specific treatment.
Similar observation holds true for words indicating time or duration (until, now, season). This shows that text related to abusive behavior tends to be aloof from the author, the ownership or specifics are avoided in order to disassociate themselves from the message. Abusive and hateful content have a high concentration of swear words, as well as express different forms of affect (use of words such as ‘happy’, ‘cried’, ‘hurt’,‘ugly’,‘nasty’) and emotions (tone), in turn indicating expressions and opinions. Hateful language uses well-formed words (dic) whereas abusive tweets allude to the use of ill-formed jargon. This composition indicates that expressive, well-formed content may typically be hateful, whereas expressive emotional (tone) content is likely to be abusive. Model stacking for ensemble learning involves taking the probability estimations of base models and using them as features for training a meta model. The general practice is to take diverse and complex base models that make sufficiently different assumptions to solve the predictive task, and then train a simple meta model to “interpret” these predictions. We treat the above three models as base models and use their predictions over the train (and validation) examples to train (and validate) a logistic regression model (i.e., the meta model). We use the predictions over the test set to evaluate the meta logistic regression model. This reaffirms the diverse modeling assumptions argument that we presented earlier. It is encouraging to see that the stacked ensemble has a better capability to distinguish between these classes.
As illustrated, BERT model with pre-trained multilingual outperforms the remaining models with F1-score of 96.050% on the public score, and 95.617% on the private score.
As the Random baseline suggests, positive and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event’s predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction.
In order to verify the efficiency of KCAT, we conduct a mock annotation experiment. The entity mention spans in these sentences have been annotated. Type hierarchy is extracted from following three datasets: (1) Conll 2003; (2) BBNRen et al. The mappings between entity and its related types are provided by these datasets. We have chosen two annotation modes: (a) without pre-linking, directly through top-down search or flatten search; (b) filtering out types that are inconsistent with entity types through entity linking. Annotation Efficiency. It can be observed that with the number of types increases, time consumption increases slowly with entity linking, while without entity linking, time consumption increases exponentially. The percentage of time saved also increases as the number of types expands on different datasets. When there are thousands of types in Knowledge Base, such as YAGO Knowledge BaseHeng et al.
Often, large amounts of training data are only available out of domain, but we still seek to have robust performance. To test how well neural machine translation and statistical machine translation hold up, we trained five different systems using different corpora obtained from OPUS An additional system was trained on all the training data. Note that these domains are quite distant from each other, much more so than, say, Europarl, TED Talks, News Commentary, and Global Voices.
Performance Comparison on UDC and We summarize our observations as follows: (1) DMN-PRF model outperforms all the baseline methods including traditional retrieval models, deep text matching models and the state-of-the-art SMN model for response ranking on both conversation datasets. The results demonstrate that candidate response expansion with pseudo-relevance feedback could improve the ranking performance of responses in conversations. The main difference between DMN-PRF model and SMN model is the information extracted from retrieved feedback QA posts as external knowledge. This indicates the importance of modeling external knowledge with pseudo-relevant feedback beyond the dialog context for response selection. (2) DMN-KD model also outperforms all the baseline methods on MSDialog and UDC. These results show that the extracted QA correspondence matching knowledge could help the model select better responses. Comparing DMN-KD and DMN-PRF, their performances are very close. (3) If we compare the performances of DMN-PRF, DMN-KD with the degenerated model DMN, we can see that incorporating external knowledge via both pseudo-relevance feedback and QA correspondence knowledge distillation could improve the performance of the deep neural networks for response ranking with large margins. For example, the improvement of DMN-PRF against DMN on UDC is 4.83% for MAP, 1.60% for Recall@5, 8.19% for Recall@1, 5.11% for Recall@2 respectively. The differences are statistically significant with p<0.05 measured by the Student’s paired t-test.
We investigate the effectiveness of different components of DMN-PRF and DMN-KD by removing them one by one from the original model with UDC and MSDialog data. We also study the effectiveness of different interaction types for M1/M2/M3. We summarize our observations as follows: 1) For the interaction matrices, we find that the performance will drop if we remove any one of M1/M2 for DMN-PRF or M1/M2/M3 for DMN-KD. This indicates that all of word level interaction matching, sequence level interaction matching and external QA correspondence interaction matching are useful for response selection in information-seeking conversation. 2) For interaction types, we can find that dot product is the best setting on both UDC and MSDialog except the results of DMN-KD on MSDialog. The next best one is cosine similarity. Bilinear product is the worst, especially on MSDialog data. This is because bilinear product will introduce a transformation matrix A as an additional model parameter, leading to higher model complexity. Thus the model is more likely to overfit the training data, especially for the relatively small MSDialog data. 3) If we only leave one channel in the interaction matrices, we can find that M1 is more powerful than M2 for DMN-PRF. For DMN-KD, M1 is also the best one, followed by M2. M3 is the last one, but it stills adds additional matching signals when it is combined with M1 and M2. The matching signals M3 from external collection could be supplementary features to the word embedding based matching matrix M1 and BiGRU representation based matching matrix M2.
We use the following algorithm to better cover the space of reasonable responses. Given a test triple τ≡(cτ,mτ,rτ), our goal is to mine other responses {r~τ} that fit the context and message pair (cτ,mτ). To this end, we first select a set of 15 candidate triples {~τ} using an IR system. The IR system is calibrated in order to select candidate triples ~τ for which both the message m~ τ and the response r ~τ are similar to the original message mτ and response rτ. Formally, the score of a candidate triple is: s(~τ,τ)=d(m~τ,mτ)(αd(r~τ,rτ)+(1−α)ϵ), (9) We found that this simple formula provided references that were both diverse and plausible. Given a set of candidate triples {~τ}, human evaluators are asked to rate the quality of the response within the new triples {(cτ,mτ,r~τ)}. The average lengths for the responses in the multi-reference tuning and test sets are 8.75 and 8.13 tokens respectively.
We compare our model with above baseline systems, including RNN and RNN context. We refer to our proposed Semantic Relevance Based neural model as SRB. Besides, SRB with a gated attention encoder is denoted as +Attention. We can see SRB outperforms both RNN and RNN context in the F-score of ROUGE-1, ROUGE-2 and ROUGE-L. It concludes that SRB generates more key words and phrases. With a gated attention encoder, SRB achieves a better performance with 33.3 F-score of ROUGE-1, 20.0 ROUGE-2 and 30.1 ROUGE-L. It shows that the gated attention reduces noisy and unimportant information, so that the remaining information represents a clear idea of source text. The better representation of encoder leads to a better semantic relevance evaluation by the similarity function. Therefore, SRB with gated attention encoder is able to generate summaries with high semantic relevance to source text.
Based on those results, we can infer the impact of adding a component to the final architecture. For the input features, we observe that adding POS tags improves significantly the performance compared to only using the words in either Blstm or Tlstm. An additional small improvement can be achieved by incorporating the dependency relation labels. It is worth noting that, while the BlstmWord or TlstmWord are similar in performance, the latter significantly overpasses the former when POS tags are added. Intuitively, the POS tags refine the syntactic patterns extracted from the tree structures and clarify the roles that the words play in the sentence. While there is no significant improvement over the single-layer models, we see gains when we add residual connections between each layer. The residual connections allow the model to decide whether it needs to go deeper or just skip the stacked layers at its convenience. Additionally, we show the performance of the models when the relative and global attention mechanisms are added separately. However, they only improve the model significantly when both are combined.
For our model, we report the performance of TrDec-con, TrDec-con-null, TrDec-dep, and TrDec-binary On the low-resource or-en dataset, we observe a large variance with different random seeds, so we run each model with 6 different seeds, and report the mean and standard deviation of these runs. TrDec-con-null and TrDec-con achieved comparable results, indicating that the syntactic labels have neither a large positive nor negative impact on TrDec. For ja-en and or-en, syntax-free TrDec outperforms all baselines. On de-en, TrDec loses to CCG-null, but the difference is not statistically significant (p>0.1).
We trained and evaluated a model for each P, I, and O class. For the P and I classes, our models outperformed the results on this leader board. The index in our model names indicates the amount of additional SQuAD domains added to the training data. We never used the full SQuAD data in order to reduce time for training but observed increased performance when adding additional data. For classifying I entities, an increase from 20 to 200 additional SQuAD domains resulted in an increase of 8% for the F1 score, whereas the increase for the O domain was less than 1%. After training a model with 200 additional SQuAD domains, we also evaluated it on the original SQuAD development set and obtained a F1 score of 0.72 for this general reading comprehension task.
In this evaluation, the F1 scores represent the overlap of labelled and predicted answer spans on token level. We also obtained scores for the subgroups of sentences that did not contain an answer versus the ones that actually included PICO elements. For the P class, only 30% of all sentences included an entity, whereas its sub-classes age, gender, condition and size averaged 10% each. In the remaining classes, these percentages were higher. F1 scores for correctly detecting that a sentence includes no PICO element exceeded 0.92 in all classes. This indicates that the addition of impossible answer elements was successful, and that the model learned a representation of how to discriminate PICO contexts. The scores for correctly predicting PICOs in positive scenarios are lower. Here, two factors could influence this score in a negative way. First, labelled spans can be noisy. Training spans were annotated by crowd workers and the authors of the original dataset noted inter-annotator disagreement. Often, these spans include full stops, other punctuation or different levels of detail describing a PICO. The F1 score decreases if the model predicts a PICO, but the predicted span includes marginal differences that were not marked up by the experts who annotated the testing set. Second, some spans include multiple PICOs, sometimes across sentence boundaries. Other spans mark up single PICOS in succession. In these cases the model might find multiple PICOs in a row, and annotate them as one or vice versa.
What type of operation which includes extractive and abstractive operations did the editors perform to create length-sensitive and length-insensitive headlines in the JAMUL? To clarify this question, we analyzed the proportions of the number of extractive and abstractive operations. Notably, we removed blank spaces, which were the most common token in longer headlines. The relatively high recall score indicates that the operations most often required to generate headlines are extractive, and the abstractive operation is about 15–27% of the total. From the relatively high recall score, we observed that Japanese headlines tend to be more extractive. We analyze how the existing methods can reflect extractive and abstractive operations in generating summaries.
Our experimental results demonstrate that using a MST algorithm during inference can slightly improve the model’s performance. We further examined the extent to which the MST algorithm is necessary for producing dependency trees. In the majority of cases DeNSe outputs trees (ranging from 87.0% to 96.7%) and a significant proportion of them are projective (ranging from 65.5% to 86.6%). Therefore, only a small proportion of outputs (14.0% on average) need to be post-processed with the Eisner or Chu-Liu-Edmonds algorithm.
After de-duplication, most of the performance metrics were lower, since the presence of duplicates in the positive samples resulted in overly optimistic results.
In both cases, de-duplication of ADR relevant sentences, and biomedical embeddings were used. The former ourperformed the latter in every performance metric. The biggest improvement was in metrics associated to the positive class, such as precision, recall, and F1 score.
Since ElimiNet and GAR perform well on different question types we believe that taking an ensemble of these models should lead to an improvement in the overall performance. For a fair comparison, we also want to see the performance when we independently take an ensemble of n GAR models and n ElimiNet models. We refer to these as GAR-ensemble and ElimiNet-ensemble models. Each model in the ensemble is trained using a different hyperparameter setting and we use n=6 (we do not see any benefit of using n>6). ElimiNet-ensemble performs better than GAR-ensemble and the final ensemble gives the best results. We observe the ElimiNet-ensemble performs significantly better on RACE-Mid dataset than the GAR-ensemble and gives almost the same performance on the RACE-High dataset. Overall, by taking an ensemble of the two models we get an accuracy of 47.2% which is 3.1% (relative) better than GAR and 1.3% (relative) better than GAR-ensemble.
We find that IAA for fluency is moderate to high for most annotator pairs, with two exceptions where agreement is low. IAA is higher for adequacy than for fluency in 8 out of 10 cases, and reflects at least moderate agreement in all cases.
We find that on both fluency and adequacy scores, Konstas performs best, followed by Zhu, and Manning performs the worst. Guo and Ribeiro are in between and within 5 points of each other on each measure, with Ribeiro performing better on fluency and Guo on adequacy. Thus, it is unsurprising that most (73%) of its low-adequacy sentences are also low-fluency. For Guo, too, a majority (54%) of low-adequacy sentences are low-fluency, though this is largely due to anonymization and repetition of words, as discussed below.
To the best of our knowledge, the 0.685 figure reported for the latter represents the current high score. This figure is above the average inter-annotator agreement of 0.67, which has been referred to as the ceiling performance in most work up to now. In our opinion, the average inter-annotator agreement is not the only meaningful measure of ceiling performance. We believe it also makes sense to compare: a) the model ranking’s correlation with the gold standard ranking to: b) the average rank correlation that individual human annotators’ rankings achieved with the gold standard ranking. The SimLex-999 authors have informed us that the average annotator agreement with the gold standard is 0.78.
We used four different sets of word vectors to construct semantic dictionaries: the original GloVe and Paragram-SL999 vectors, as well as versions counter-fitted to each domain ontology. The constraints used for counter-fitting were all those from the previous section as well as antonymy constraints among the set of values for each slot. We treated all vocabulary words within some radius t of a slot value as rephrasings of that value. The optimal value of t was determined using a grid search: we generated a dictionary and trained a model for each potential t, then evaluated on the development set. The dictionaries induced from the pre-trained vectors substantially improved tracking performance over the baselines (which used no semantic dictionaries). The dictionaries created using the counter-fitted vectors improved performance even further. Contrary to the SimLex-999 experiments, starting from the Paragram vectors did not lead to superior performance, which shows that injecting the application-specific ontology is at least as important as the quality of the initial word vectors.
We can see the large difference in METEOR scores between MSNMT congruent and incongruent settings when the input text information is incomplete which implies that our proposed model learns to extract information from images for translation. The interesting part is for a full translation, where scores for the incongruent setting outperform or are very close to those of the congruent setting. The reason is that when textual information is enough, visual information becomes not that relevant in some cases.
Brevity We automatically trim long timexes. TIMEX3 annotations are minimal – that is, including the minimal set of words that can describe a temporal expression – where TIMEX2 can include whole phrases. Even after reducing the long annotations that contain temporal substructres, a significant amount of text can remain in some cases. To handle this, we implement reduction of long TIMEX2s into just the TIMEX3-functional part. This is done by measuring the distribution of TIMEX3 token lengths in gold standard corpora, and determining a cut-off point. Any TIMEX2s of six tokens or more that have no yet been handled by the algorithms mentioned above are syntactically parsed. They are then reduced to the largest same-constituent chunk that is shorter than six tokens and contains a temporal measure word, with preference given to the leftmost arguments.
Performance is reported using both entity recognition precision and recall (strict), as well as the TempEval-2 scorer, which uses a token-based metric instead of entity-based matching (see \newciteverhagen2010semeval for details).
For DSSM, the negative sampling ratio is also set to 4. We try to tune all baselines to their best performances.
We can clearly see that the proposed CS-LVM architecture substantially outperforms other models based on auto-encoding. Also, the semantic constraints brought additional boost in performance, achieving the new state of the art in semi-supervised classification of the SNLI dataset.
Again, the proposed CS-LVM consistently outperforms other supervised and semi-supervised models by a large margin, setting the new state-of-the-art result on the QQP dataset with the semi-supervised setting.
As expected, the cross-sentence generation is the most critical factor for the performance, except for the 28k setting where the encoder weight tying brought the biggest gain. In 59k and 120k settings, all other variants that maintain the cross-generating property outperform the VAE-based models (see (ii), (iii), (iv)).
The first study focused on assessing the extent to which generated summaries retain salient information from the input set of paragraphs. We followed a question-answering (QA) scheme as proposed in Clarke and Lapata Under this scheme, a set of questions are created based on the gold summary; participants are then asked to answer these questions by reading system summaries alone without access to the input. Correct answers are given a score of 1, partially correct answers score 0.5, and zero otherwise. The final score is the average of all question scores. We created between two and four factoid questions for each summary; a total of 40 questions for each domain. We collected 3 judgements per system-question pair. Summaries by the CV-S2D+T model are able to answer more questions, even for the Animals domain where the TS-S2S model obtained higher ROUGE scores.
Since we use the hinge loss as the loss function in our proposed PcnnNmar model, the way that the hamming loss is calculated decides how we solve the argmax problem in loss-augmented search. In our experiments, we explore three ways to compute the loss: 0/1 loss, relation-level hamming loss and mention-level hamming loss. Although theoretically relation-level hamming loss should be better, it is really hard to find the exact argmax solution in loss-augmented inference with local search while we can easily get it with mention-level hamming loss.
All the span representations perform well. Encodings of each token from a Transformer already capture sufficient information about the entire sequence, so even only using the start and end encodings yields strong results. Nonetheless, area attention provides a small boost over the others. As a new dataset, there is also considerable headroom remaining, particularly for complete match.
The Transformer screen encoder achieves the best result with 70.59% accuracy on Complete Match and 89.21% on Partial Match, which sets a strong baseline result for this new dataset while leaving considerable headroom. The GCN-based methods perform poorly, which shows the importance of contextual encodings of the information from other UI objects on the screen. Distance GCN does attempt to capture context for UI objects that are structurally close; however, we suspect that the distance information that is derived from the view hierarchy tree is noisy because UI developers can construct the structure differently for the same UI. As a result, the strong bias introduced by the structure distance does not always help. Nevertheless, these models still outperformed the heuristic baseline that achieved 62.44% for partial match and 42.25% for complete match.
Using the slash variant brings improvements across all datasets as does adding indomain contexts (exception: BERT/AG’s News). This makes sense considering that for a rare word, every single additional context can be crucial for gaining a deeper understanding. Correspondingly, it is not surprising that the benefit of adding Bertram to RoBERTa is less pronounced, because Bertram uses only a fraction of the contexts available to RoBERTa during pretraining. Nonetheless, adding Bertram significantly improves RoBERTa’s accuracy for all three datasets both with and without adding indomain contexts.
Our results are given in tab. Mittens outperforms External GloVe and IMDB GloVe, indicating that it effectively combines complementary information from both.
Cross-Domain Analyses : We perform another set of experiment to study the usefulness of the best performing system (i.e. Model2 ) across the domains. We train the model2, on FakeNews AMT and test on Celebrity and vice-versa. If we compare with the in domain results it is observed that there is a significant drop. This drop also observed in the work of Pérez-Rosas et al. in machine learning setting. This indicates there is a significant role of a domain in fake news detection, as it is established by our deep learning guided experiments too.
Multi-Domain Training and Domain-wise Testing : There are very small number of examples pairs in each sub-domain (i.e. Business, Technology etc) in FakeNews AMT dataset. We combine the examples pairs of multiple domains/genres for cross corpus utilization. We train our proposed models on the combined dataset of five out of six available domains and test on the remaining one. This has been performed to see how the model which is trained on heterogeneous data react on the domain to which the model was not exposed at the time of training. The results are shown in Exp. Both the models yield the best accuracy in the Education domain, which indicates this domain is open i.e. linguistics properties, vocabularies of this domain are quite similar to other domains. The models (i.e. Model 1 and 2) perform worst in the Entertainment and the Sports, respectively, which indicate these two domains are diverse in nature from the others in terms of linguistics properties, writing style, vocabularies etc. Domain-wise Training and Domain-wise Testing: We also eager to see in-domain effect of our systems. The FakeNews AMT dataset comprises of six separate domains. We train and test our models, on each domain’s dataset of Fake News AMT. This evaluates our model’s performance domain-wise. The results of this experiment are shown in the Exp. In this case both the models produce the highest accuracy in the Sports domain, followed by the Entertainment, as we have shown in our previous experiments that these two domains are diverse in nature from the others. This fact is established by this experiment too. Both the models produce the lowest result in the Technology and the Business domain, respectively.
This analysis reveals a number of interesting observations. 1) The D-RNNLM mainly improves over the baseline on the “switching tokens”, Eng-Man and Man-Eng. 2) The RNNLM with monolingual data improves most over the baseline on “the monolingual tokens”, Eng-Eng and Man-Man, but suffers on the Eng-Man tokens. The D-RNNLM with monolingual data does as well as the baseline on the Eng-Man tokens and performs better than “Mono RNNLM” on all other tokens. 3) RNNLM SeqGAN suffers on the Man-Eng tokens, but helps on the rest; in contrast, D-RNNLM SeqGAN helps on all tokens when compared with the baseline.
We conduct disassemble analysis of the ensemble model (ENS) to investigate the effectiveness of different combinations of these methods, For the convenience of narration, we use letters A, B, C, D, E to represent hard parameters sharing with linguistic hierarchies, gate mechanism, label embedding, orthogonality constrains and adversarial learning. For example, ABCDE stands for the ensemble model (ENS). From the results, we can observe that the combined methods have different performance on different datasets. For example, removing gate mechanism (i.e., ACDE) will improve the performance of the ENS model on SST-2 by 0.42% but reduce the performance on MNLI by 0.66%. The best performance on most datasets is achieved by the ABC model which combines gate mechanism, label embedding and hard parameters sharing with linguistic hierarchies. The results are consistent with the findings in the previous section, which can be explained that these three MTL methods have no strong correlations, thus combining them will not disturb the performance of each individual MTL method.
Hazen [Hazen:2007:ASRU], used discriminative vocabulary selection followed by a naïve Bayes (NB) classifier. Having a limited (small) vocabulary is the major drawback of this approach. Although we have used the same training and test splits, May [May:2015:mivec] had slightly larger vocabulary than ours, and their best system is similar to our baseline TF-IDF based system. We can see that our proposed systems achieve consistently better accuracies; notably, GLCU which exploits the uncertainty in document embeddings has much lower cross-entropy than its counter part, GLC. To the best of our knowledge, the proposed systems achieve the best classification results on Fisher corpora with the current set-up, i.e., treating each side of the conversation as an independent document. It can be observed ULMFiT has the lowest cross-entropy among all the systems.
To verify that providing highlights to annotators has positive effect on the efficiency of annotations we ran two factual consistency annotation tasks in parallel, where in one highlights were provided to the annotators and the other did not show highlights. We measured the effects of providing highlights on the average time spent by an annotator on the task and the inter-annotator agreement of annotations. The experiment showed that when completing the task with highlights, annotators were able to complete it 21% faster and the inter-annotator agreement, measured with Fleiss’ κ, increased by 38%.
Baselines. (2) a Bag-of-Embeddings (BoE) model, where we retrieve the word2vec representations of the words in a tweet and compute the tweet representation as the centroid of the constituent word2vec representations. Both BoW and BoE features are then fed to a linear SVM classifier, with tuned C=0.6. All of our reported F1-scores are calculated on the evaluation (dev) set, due to time constraints. We evaluate the P-Emb and P-Sent models, using both bidirectional and unidirectional LSTMs. As expected, bi-LSTM models achieve higher performance. Our submitted model is an ensemble of the models with the best performance. More specifically, we leverage the following models: (1) TL of pretrained word embeddings, (2) TL of pretrained sentiment classifier, (3) TL of 3 different LMs, trained on 2M, 4M and 5M respectively. We use Unweighted Average (UA) ensembling of our best models from all aforementioned approaches. Moreover, we notice that, when the three models are trained with unidirectional LSTM and the same number of parameters, the P-LM outperforms both the P-Emb and the P-Sent models. As expected, the upgrade to bi-LSTM improves the results of P-Emb and P-Sent. We hypothesize that P-LM with bidirectional pretrained language models would have outperformed both of them. Furthermore, we conclude that both SGU for fine-tuning and the concatenation method enhance the performance of the P-LM approach. As far as the ensembling is concerned, both approaches, MV and UA, yield similar performance improvement over the individual models. In particular, we notice that adding the P-LM predictions to the ensemble contributes the most. This indicates that P-LMs encode more diverse information compared to the other approaches.
For details of fine-tuning, we first initialize the weights of the encoder backbone of our model with the RoBERTa-BASE model, then apply SPRC and MLM as training objectives separately to fine tune on in-domain data. For the SPRC task, a four-label sentence classification loss is used to predict the relationship type of strictly adjacent text pair selected from Dunlabeled. We fine tune the model with 15 epochs and obtain accuracy at 87.5 on validation set. A cross-entropy loss on predicting the masked tokens is used for MLM task. is 1.9. We also use the fine-tuned weights of MLM task to initialize the SPRC task. The accuracy is 89.8 after 18 epochs.
An ablation study on the graph module is conducted to explore the contributions brought by the section title connections, the fonts features and the skip connections. We also observe that combining the fonts features with each node leads to a 0.5 points improvement over the graph module. Furthermore, dropping the skip connections between graph layers result in a 0.4 points decrease. These results demonstrate that all types of layout information, from position to font, contribute positively to extraction performance.
, we trained Support Vector Machine (SVM) classifiers for the classification task. We used an RBF kernel, and we optimized the value of the cost parameter via 10-fold cross-validation over the 1200 annotated posts. We obtained the best results with cost=64.0, and we used this setting to classify all the identified tweets in our collection. We compared the performance of the SVM to that of a Naïve Bayes baseline, which obtained an F-measure of 0.70 for the isPreg class. The area under the mean ROC curve is 0.82. Running the SVM classifiers on our collected data resulted in the discovery of 34,895 legitimate pregnant women from a total of 53,820 users.
Majumder et al. (Multi-modal performance of DialogueRNN has not been reported by Majumder et al. Our model consistently outperforms the previous state-of-the-art but performs better only on one of the subsets of the modalities when compared to the current state-of-the-art.
We ran each IR method on an index of all 65 receipts. The results show improvement with the introduction of each new IR technique. Unsuprisingly, the results for all receipts is lower than that of a single receipt. This is primarily due to the expansion of choices introduced by more data. Recall that we are only judging a hit by looking at the top search result. It is likely that in many examples, the true match is still contained in one of the top-k matches, where k is a relatively small number.
Each SNOMED-CT code corresponds to one disease category. For each category, we report the number of training examples in the category (N), the scores for precision, recall, F1, accuracy, and the number of disease subtypes in this category. While DeepTag achieves reasonable F1 scores overall, its performance is quite heterogeneous in different categories. Moreover the performance decreases when DeepTag is applied to the out-of-domain PP test data. and we regard this as our best setting.
On the CSU dataset, DeepTag and DeepTag-M perform slightly better (or at the same capacity) compared to the baseline models (LSTM and BLSTM). DeepTag is able to have higher unweighted precision, recall, and F1 score compared to the other models, indicating its ability to have good performance on a wide spectrum of diseases. Since it is out-of-domain, expert defined disease similarity provide much-needed regularization to make both DeepTag and DeepTag-M outperform baseline models by a substantial margin, with DeepTag being the overall best model. There are several aspects of the data that may have limited our ability to apply methods from our training set to our external validation set. Private veterinary practices often have data records that closely resemble the PP dataset used to evaluate our methods here. However, the large annotated dataset we used for training is from an academic institution (as these are, largely, the institutions that have dedicated medical coding staff). The domain shift comes from two parts. First, text style mismatch – private commercial notes use more abbreviations and tend to include many procedural examinations (even though many are non-informative or non-diagnostic). This requires the model to learn beyond keyword or phrase matching. Second, label distribution mismatch – the CSU training dataset focuses largely on neoplasm and several other tumor-related diseases, largely due to the fact that the CSU hospital is a regional tertiary referral center for cancer and cancer represents nearly 30% of the caseload. Other practices will have datasets composed of labels that appear with different frequencies, depending on the specializations of that particular practice. A very important path forward is to use learning algorithms that are robust to domain shift, and experimenting with unsupervised representation learning to mitigate the domain shift between academic datasets and private practice datasets. After hyperparameter searching, we are report models with the hyperparameters that perform well on each dataset. We train each model five times and report the averaged result. For the CSU dataset, we find β=0.001 works best for DeepTag-M, and γnorm=1e−5,γbetween=1e−4,γwithin=1e−4 works best for DeepTag (cluster penalty). For the PP dataset, we find β=0.0001 works the best for DeepTag-M, and γnorm=1e−4,γbetween=1e−3,γwithin=1e−3 works best for DeepTag (cluster penalty).
For better understanding the effectiveness of our proposed position-aware self-attention in our model, we evaluate the performance of various position modeling strategies. Note that Model 3 is our final proposed architecture. Model 1 remains the same as Model 3 except that it minus Ψij(^xi) Model 2 applies an absolute position encoding before context encoder layer on the basis of Model 1, which is the position modeling strategy adopted by Vaswani et al. Comparing Model 1 with Model 3, we can see that after removing the proposed positional bias Ψij(^xi) the performance decreases a lot, indicating that our proposed flexible extension of the self-attention achieves a significant improvement since it effectively explores the positional information of an input sequence. But Model 2 with absolute position encoding yields worse performance than Model 1. We conjecture that it is because the absolute position embedding might weaken model’s ability to fusion context features in our architecture.
Currey et al. This method was found to be useful where scripts are identical across the source and target languages. it includes keywords and terms related to COVID-19, which are often used verbatim across the languages of study in this paper, so we contend that this strategy can help translate terms correctly. Accordingly, we carried out an experiment by adding sentences of the Sketch Engine Corpus (English monolingual corpus) to the TAUS Corona Crisis Corpus following the method of Currey et al. While this method also brings about moderate improvements on the Reco test set, it is not statistically significant. Interestingly, this approach also significantly lowers the system’s performance on the TAUS test set, according to both metrics. Nonetheless, given the urgency of the situation in which we found ourselves, where the MT systems needed to be built as quickly as possible, the approach of Currey et al. In our next experiment, we took five million low-scoring (i.e. similar to the in-domain corpora) sentence-pairs from ParaWiki, added them to the training data, and fine-tuned the baseline model on it. This is corroborated by the chrF score. We then built a training set from all data sources (EMEA, Sketch Engine, and ParaWiki Corpora), and fine-tuned the baseline model on the combined training data. Our next experiment involved adding a further three million sentences from ParaWiki. This brings about a further slight improvement in terms of BLEU on the Reco test set. More importantly, while we cannot beat the online systems in terms of BLEU score, we are now in the same ballpark. More encouragingly still, in terms of chrF, our score is higher than both Amazon and Bing, although still a little way off compared to Google Translate.
In conditional generation tasks, beam search is widely used, even in open-ended tasks like chitchat Roller et al. Thus, we perform beam search with a beam size of four on the two conditional generation tasks. We observe that TextGAIL performs better than MLE-based method in both quality and diversity metrics. When we examine the generated text, we find that MLE produces many repetitions due to exposure bias, similar to observations in Welleck et al. As TextGAIL mitigates exposure bias, we observe it generates less repetition without a sacrifice of generation quality. We observe that there is no statistical difference between TextGAIL and MLE on unconditional generation tasks in this pairwise comparison evaluation. One possible reason is that when comparing two completely different sentences in this unconditional generation setting, it is difficult for human evaluators to make consistent decisions. In contrast, TextGAIL significantly outperforms MLE in human evaluation on two conditional generation tasks. Since these tasks expect models to produce similar content with respect to the ground truth. It is easier for human to select the output with better quality.
For simplicity, we only conduct the experiments on CommonGEN. We use beam search with beam size four as the inference method. We observe that, without human demonstrations, the model fails to converge. Therefore, mixing human demonstrations is crucial in the training of TextGAIL. We have tested to replace PPO with REINFORCE Yu et al. This results in the worse performance in all metrics, as REINFORCE has higher variance than PPO. Also, we observe that it leads to worse performance without pre-training the discriminator. We analyze the reward signal of the learned discriminator in TextGAIL, which is supposed to distinguish the real samples from the generated samples. We apply the learned discriminator in TextGAIL on a story ending classification task, Story Cloze Test, to identify true story ending given the story prompt Mostafazadeh et al. This task uses a different dataset from ROCStories but is in the similar domain.
Although generalizing sentences improves perplexity drastically, splitting and pruning sentences yields better BLEU scores when the original words are kept. In the case of event2sentence, BLEU scores make more sense as a metric since the task is a translation task. Perplexity in these experiments appears to correspond to vocabulary size.
A random baseline is included for reference, which assigns a random relevance score for a search utterance. For the Supervised systems, a speech network was trained on text transcriptions to perform keyword prediction, and embeddings taken from the final output. These systems therefore represent the case where perfect text labels are available for training utterances. Our original motivation for FastGrounded was that, if a query contains a keyword of a particular type, the embedding from f(⋅) will have a single dimension with a high probability (since in our case each embedding dimension corresponds to a particular visual tag and all query types occur as tags). By only considering this specific dimension for all of the search utterance embeddings, a quick retrieval would be possible. But we found that when visual grounding is used, embeddings are highly influenced by the prior occurrence of specific visual tags. The embedding dimension corresponding to “man”, for instance, typically has a high score (irrespective of the input), since many training images contain men. To alleviate this effect, we performed mean and variance normalization on all of the evaluation queries and search utterances using mean and variance estimates from the training embeddings. (We also considered several other normalization methods, but this approach proved most robust.)
We conduct word similarity evaluation on the following benchmark datasets: WordSim353 Finkelstein et al. Words appearing less than 100 times are discarded, leaving 239,672 unique tokens. We compare our model with the following baselines: Word2Vec Mikolov et al. et al. et al. The results demonstrate that training embeddings in the spherical space is essential for the superior performance on word similarity. We attempt to explain why the recent popular language model, BERT Devlin et al. (1) BERT learns contextualized representations, but word similarity evaluation is conducted in a context-free manner; averaging contextualized representations to derive context-free representations may not be the intended usage of BERT. (2) BERT is optimized on specific downstream tasks like predicting masked words and sentence relationships, which have no direct relation to word similarity.
Apart from document clustering, we also evaluate the quality of spherical paragraph embeddings on document classification tasks. We again treat each document in both datasets as a paragraph in all models. For the 20 Newsgroup dataset, we follow the original train/test sets split; for the movie review dataset, we randomly select 80% of the data as training and 20% as testing. Since k-NN is a non-parametric method, the performances of k-NN directly reflect how well the topology of the embedding space captures document-level semantics (i.e., whether documents from the same semantic class are embedded closer). JoSE achieves the best performances on both datasets with k-NN classification, demonstrating the effectiveness of JoSE in capturing both topical and sentiment semantics into learned paragraph embeddings.
When a rule is partially correct, we selected the gold-standard rule which results in the highest F1-score to report the numbers in this paper. Both “Crowd+User” and “Crowd Voting” settings achieved comparable performances to that of the “Crowd Only” setting is both IF and THEN parts. Selecting correct sensors in IF is harder than selecting correct effectors in THEN, which is expected due to the tolerant nature of our evaluation setup for THEN. We observe that “Crowd Voting” resulted in a higher average recall, which suggested that a group of crowd workers is, collectively, less likely to forget picking some sensors than an individual user. We also notice that participants actually corrected errors in the crowd-created rules, as both the average precisions and recalls are higher in “Crowd+User” than “Crowd Only”. For instance, in the “Late for Dinner” scenario (S6), one common mistake was that crowd selected only one of Calender or GPS sensors, instead of both. Two different participants fixed this error by adding back the missing sensor. Another similar example occurred in the “Bus” scenario (S5), where the crowd sometimes missed the “Clock” sensor which can indicate the current time is after 5pm. One participant fixed this by adding the Clock sensor back to the IF.
We use 4-fold cross-validation to tune the neural network hyperparameters. Learning rates in the range of 0.03 and 0.01 give relatively similar results. Best results are achieved using between 10 and 15 training epochs, depending on the CR-CNN configuration. Additionally, we use a learning rate schedule that decreases the learning rate λ according to the training epoch t. The learning rate for epoch t, λt, is computed using the equation: λt=λt. In this section we report experimental results comparing CR-CNN with CNN+Softmax. In order to do a fair comparison, we’ve implemented a CNN+Softmax and trained it with the same data, word embeddings and WPEs used in CR-CNN. We tune the parameters of CNN+Softmax by using a 4-fold cross-validation with the training set.
In this experiment we assess the impact of omitting the embedding of the class Other. As we mentioned above, this class is very noisy since it groups many different infrequent relation types. Its embedding is difficult to define and therefore brings noise into the classification process of the natural classes. The two first lines of results present the official F1, which does not take into account the results for the class Other. We can see that by omitting the embedding of the class Other both precision and recall for the other classes improve, which results in an increase of 1.4 in the F1. These results suggest that the strategy we use in CR-CNN to avoid the noise of artificial classes is effective. We can note that while the recall for the cases classified as Other remains 48.7, the precision significantly decreases from 60.1 to 52.0 when the embedding of the class Other is not used. That means that more cases from natural classes (all) are now been classified as Other. However, as both the precision and the recall of the natural classes increase, the cases that are now classified as Other must be cases that are also wrongly classified when the embedding of the class Other is used.
Rink and Harabagiu \shortciterink:2010 present a support vector machine (SVM) classifier that is fed with a rich (traditional) feature set. It obtains an F1 of 82.2, which was the best result at SemEval-2010 Task 8. Socher et al. \shortcitesocher:2012: emnlp present results for a recursive neural network (RNN) that employs a matrix-vector representation to every node in a parse tree in order to compose the distributed vector representation for the complete sentence. Their method is named the matrix-vector recursive neural network (MVRNN) and achieves a F1 of 82.4 when POS, NER and WordNet features are used. Their classifier achieves a F1 of 82.7 when adding a handcrafted feature based on the WordNet. Yu et al. \shortciteyu2014 present the Factor-based Compositional Embedding Model (FCM), which achieves a F1 of 83.0 by deriving sentence-level and substructure embeddings from word embeddings utilizing dependency trees and named entities. This is a remarkable result since we do not use any complicated features that depend on external lexical resources such as WordNet and NLP tools such as named entity recognizers (NERs) and dependency parsers. The closest result (80.6), which is produced by the FCM system of Yu et al. \shortciteyu2014, is 2.2 F1 points behind CR-CNN result (82.8).
We make a few observations: The final group demonstrates the need for external knowledge and deeper reasoning. When the “oracle” science fact f used by the question author is provided to the knowledge-enhanced reader, it improves over the knowledge-less models by about 5%. However, there is still a large gap, showing that the core fact is insufficient to answer the question. When we also include facts retrieved from WordNet Miller et al. Unlike the WordNet gain, adding ConceptNet Speer et al. This suggests that ConceptNet is either not a good source of knowledge for our task, or only a subset of its relations should be considered. Overall, external knowledge helps, although retrieving the right bits of knowledge remains difficult. This increases the scores substantially, to about 76%. This big jump shows that improved knowledge retrieval should help on this task. At the same time, we are still not close to the human performance level of 92% due to various reasons: (a) the additional fact needed can be subjective, as hinted at by our earlier analysis; (b) the authored facts K tend to be noisy (incomplete, over-complete, or only distantly related), also as mentioned earlier; and (b) even given the true gold facts, performing reliable “reasoning” to link them properly remains a challenge.
We evaluate our CEQE model on the WMT2018 Quality Estimation Words in all languages are lowercased. The evaluation metric is the multiplication of F1-scores for the “OK” and “BAD” classes against the true labels. F1-score is the harmonic mean of precision and recall.
For each language pair, we show the performance of CEQE without adding the corresponding components specified in the second column respectively. The last row shows the performance of the complete CEQE with all the components. As the baseline features released in the WMT2018 QE Shared Task for English-Latvian are incomplete, we train our CEQE model without using such features. We can glean several observations from this data: Because the number of “OK” tags is much larger than the number of “BAD” tags, the model is easily biased towards predicting the “OK” tag for each target word. The F1-OK scores are higher than the F1-BAD scores across all the language pairs. For German-English, English Czech, and English-German (SMT), adding the baseline features can significantly improve the F1-BAD scores. For English-Czech, English-German (SMT), and English-German (NMT), removing POS tags makes the model more biased towards predicting “OK” tags, which leads to higher F1-OK scores and lower F1-BAD scores. Adding the convolution layer helps to boost the performance of F1-Multi, especially on English-Czech and English-Germen (SMT) tasks. Comparing the F1-OK scores of the model with and without the convolution layer, we find that adding the convolution layer help to boost the F1-OK scores when translating from English to other languages, i.e., English-Czech, English-German (SMT and NMT). We conjecture that the convolution layer can capture the local information more effectively from the aligned source words in English.
To perform experiments with word embeddings we used four different word2vec models. All of them with an embedding size of 300 dimensions and a negative sampling of 15 units.
Word2Vec: Note that averaging (AVG) and our Gradient Combiner (GC) methods are used to combine gradients during inter-host synchronization , so they have no impact on a single host.
Hyper-parameter Settings. Three-fold validation on the training dataset is adopted to tune the parameters following Surdeanu et al. We use grid search to determine the optimal hyper-parameters. We select word embedding size from {50,100,150,200,250,300}. Batch size is tuned from {80,160,320,640}. We determine learning rate among {0.01,0.02,0.03,0.04}. The window size of convolution is tuned from {1,3,5}. We keep other hyper-parameters same as Zeng et al. : the number of kernels is 230, position embedding size is 5 and dropout rate is 0.5.
The best performance is achieved by the Hierarchical Feature Attention model, which achieves a WER of 20.9% (an improvement of +4.2% over the baseline). The Encoder Init and Early Decoder Fusion models perform comparably, while Encoder + Decoder Init performs slightly worse.
This is a remarkable result given that the baselines are trained on token-level labels, whereas our model is trained end-to-end. For the restaurant data set, our model is slightly worse than the baseline.
We first compared matched example translations against reference translations in the Chinese-English test set at the word level after all stop words are removed. The noise-masking procedure can significantly reduce the number of noisy words (9,353 vs. 1,627). 8.1% of matched words in the original example translations are filtered out due to wrong word alignments.
We find that using 4k vocabulary size leads to the best BLEU scores on the dev/test set for both kor \rightarrow jje (44.85/43.31) and jje \rightarrow kor (69.35/67.70), although they are within a point difference for 2k and 8k vocabulary sizes. Performance degrades for using larger vocabulary sizes: by approximately 1 point for 16k and another 1 point for 32k.
As a simple baseline, we include a copying model that predicts its input as its output (“Copy”). The copying model already achieves 24.44 and 24.45 BLEU scores on the kor \rightarrow jje and jje \rightarrow kor test sets respectively.
As shown by the results, COMeT lacks discriminative power, which is consistent with the results in Malaviya et al. KG-BERT, which has been successfully applied on traditional KGs, produce satisfactory results on CKG as well, while our methods perform better than both baseline methods by a large margin on the EC tests. Hence it is demonstrated that introducing conceptualization during training is effective to create a model capable of identifying false conceptualization. Particularly, the percentage of EC samples in training is critical for a trade-off between EC and NS tasks: Increased EC percentage will lead to better EC results, but the NS results will drop. The Atomic CCC models reach better results on NS than KG-BERT, which is possibly due to the fact that Atomic nodes are mostly about everyday activities, in contrast to ASER which covers a broader range of topics. Therefore by EC training a more diverse set of nodes could be seen by the model in training, and could be helpful for the model to generalize in the NS test.
We see that AADIT-S achieves 17.63% WER with additive attention which is 8.3% and 4.2% relatively improved over baseline multi-conditional LSTM and ADIT-S, respectively. AADIT-E performs significantly better than AADIT-S with a WER of 16.61% when using dot-product attention, which is 13.6% and 9.3% relatively improved over baseline multi-conditional model and ADIT-E, respectively. Dot-product attention performs similar to additive attention for AADIT. We also investigate the effect of positional encoding on AADIT. We further perform AADIT with multi-head additive and dot-product attentions. The number of heads is fixed at 8 and the key/query dimension for each head is 512/8=64. We observe that the multi-head AADIT-S only slightly improves the WER of single-head one, and multi-head AADIT-E does not further improve the WER. Considering the significantly better WER with less computational cost, we suggest using single-head AADIT-E for robust ASR.
Our re-ranker achieves the performance of 85.71%(+0.25%) on the test set, which also outperforms the previous state-of-the-art methods. Compared with the re-ranking model of \newciteHayashi:2013, that use a large number of handcrafted features, our model can achieve a competitive performance with the minimal feature engineering.
We compared a Python implementation of the Naive Bayes classifier from NLTK[bird2009natural] against Pinnis [pinnis2018latvian] implementation of the Perceptron classifier. We found that the highest classification accuracy - 61.23% - is achieved by using all but NI data sets for training and only stemming all words.
Since the Word2Vec model has a high dimensionality space (300), different methods were explored to reduce the complexity of the data. These vectors were reduced to 20 dimensions, following the procedure mentioned previously. The accuracy was also higher than the best accuracy from the bag of words and TF-IDF methods demonstrated above.
Next, we study differences in annotations made by the two groups. in terms of the number of tokens in each span annotation. Observe that the number of tokens in annotations by medical experts is smaller than the number of tokens in annotations by MTurk workers. We believe the difference in the length of spans is attributed to medical domain knowledge. Medical experts make more specific and brief annotations for being able to identify essential information. Moreover, the annotations by MTurk workers show higher standard deviations than by experts.
We compare our model with the basic HRED and several current approaches including KL-annealing (KLA), word drop-out (DO), free-bits (FB) and bag-of-words loss (BOW). For KLA, we initialize the weight with 0 and gradually increase to 1 in the first 12000 or 25000 training steps for Dailydialog and Switchboard respectively. The word drop-out rate is fixed to 25%. Words are dropped out only in the training step. We set the reserved space for every dimension as 0.01 in free bits (FB) and also try reserving 5 bits for the whole dimension space (FB-all). We use an α value 5 for our collaborative model (CO) and set the scheduled sampling (SS) weight k=2500 or 5000 for Dailydialog or Switchboard. We also experiment with jointly training the AE and CVAE part in our model and report the results. NLL is averaged over all the 80-word slices within every batch. For latent variable models, NLL is computed as the ELBO, which is the lower bound of the real NLL.
We compare our model with the basic HRED and several current approaches including KL-annealing (KLA), word drop-out (DO), free-bits (FB) and bag-of-words loss (BOW). For KLA, we initialize the weight with 0 and gradually increase to 1 in the first 12000 or 25000 training steps for Dailydialog and Switchboard respectively. The word drop-out rate is fixed to 25%. Words are dropped out only in the training step. We set the reserved space for every dimension as 0.01 in free bits (FB) and also try reserving 5 bits for the whole dimension space (FB-all). We use an α value 5 for our collaborative model (CO) and set the scheduled sampling (SS) weight k=2500 or 5000 for Dailydialog or Switchboard. We also experiment with jointly training the AE and CVAE part in our model and report the results. Unlike the NLL, who measures the token-level match, these embedding-based metrics map responses to a vector space and compute the cosine similarly with golden answers, which can to a large extent measure the sentence-level semantic similarity.
First, we design experiments to evaluate the effectiveness of attention levels and how many attention levels are appropriate. To this end, we implement a baseline model (LSTM with no attention) which directly applies the mean pooling operation over LSTM output vectors of two arguments without any attention mechanism. Then we consider different attention levels including one-level, two-level and three-level. For four-way classification, macro-averaged F1 and Accuracy are used as evaluation metrics. For binary classification, F1 is adopted to evaluate the performance on each class. With attention levels added, our NNMA model performs much better. This confirms the observation above that one-pass reading is not enough for identifying the discourse relations. With respect to the four-way F1 measure, using NNMA with one-level attention produces a 4% improvement over the baseline system with no attention. Adding the second attention level gives another 2.8% improvement. We perform significance test for these two improvements, and they are both significant under one-tailed t-test (p<0.05). However, when adding the third attention level, the performance does not promote much and almost reaches its plateau. We can see that three-level NNMA experiences a decease in F1 and a slight increase in Accuracy compared to two-level NNMA. The results imply that with more attention levels considered, our model may perform slightly better, but it may incur the over-fitting problem due to adding more parameters. With respect to the binary classification F1 measures, we can see that the “Comparison” relation needs more passes of reading compared to the other three relations.
We observe that full softmax produces the highest perplexity scores, despite having almost 20M parameters more than the other models. This shows that the power of the output layer or classifier, as measured by number of parameters, is not indicative of generalization ability. To verify the hypothesis that our output layer facilitates information transfer across words, we also analyzed the loss for words in different frequency bands, created by computing statistics on the training set. Overall, the graph shows that most of the improvements in perplexity between 5% to 17.5% brought by DRILL against baselines comes from predicting more accurately the words in lower word frequency bands (1 to 100 occurrences). One exception occurs in the highest frequency band, where DRILL has 2.5% higher perplexity than the bilinear mapping, but this difference is less significant because it is computed based on 16 unique words as opposed to the lowest frequency band which corresponds to 4116 unique words. These results validate our hypothesis that learning a deeper label encoder leads to better transfer of learned information across labels. More specifically, because low frequency words lack data to individually learn the complex structure of the output space, transfer of learned information from other words is crucial to improving performance, whereas this is not the case for higher frequency words. This analysis suggests that our model could also be useful for zero-resource scenarios, where labels need to be predicted without any training data, similarly to other joint input-output space models.
This result almost matches the single-model state-of-the-art, without resorting to very high capacity encoders or model averaging over different epochs. Transformer-DRILL also outperforms by 0.6 points our implementation of Transformer (base) model combined with the dual nonlinear mapping by Pappas et al.
Our model is implemented in PyTorch Paszke et al. All experiments use the same hyperparameters. Dimensions of word embeddings, hidden states, key vectors and value vectors are set as 128. Hyperparameters γ and N are set as 0.5 and 10 respectively. Meanwhile, as done in previous works Havrylov et al. Its weight is set to 0.1 at the beginning, and exponentially anneals with a rate 0.5 as the lesson increases. The source code has been submitted as part of the supplementary material. As for data splits, we split each dataset into the train set and the test set for all tasks according to previous works. More details about train and test sizes can be seen in Tab. More specifically, except for the task Limit, we further randomly take 20% training data as the development set to tune the hyperparameters, with the rest being the train set.
It can be seen that the basic RNN, which is signal directional and with the output of the last step as the sentence-level features, performs very poor. This can be attributed to the lack of the position information of target nominals and the difficulty in RNN training. The max-pooling offers the most significant performance improvement, indicating that local patterns learned from neighbouring words are highly important for relation classification. The position indicators also produce highly significant improvement, which is not surprising as the model would be puzzled which pattern to learn without the positional information. The contribution of positional information has been demonstrated by \newcitezeng14, where the positional features lead to nearly 10 percentiles of F1 improvement, which is similar as the gain obtained in our model.
It can be observed that long contexts exist in all the three datasets. Particularly, the proportion of long contexts in the SemEval-2010 task 8 dataset is rather small compared to the other two datasets. This suggests that the strengths of the different models were not fully demonstrated by only implementing experiments on SemEval-2010 task 8 dataset. Since most recent works on relation classification only implemented on this single dataset, a comparison among different models on KBP37 dataset is needed.
Note that RoBERTa and XLNet use 160G training data while DeBERTa uses 78G training data. RoBERTa and XLNet are trained for 500K steps with 8K samples in a step, which amounts to four billion passes over training samples. We train DeBERTa for one million steps with 2K samples in each step. This amounts to two billion passes of its training samples, approximately half of either RoBERTa or XLNet. Meanwhile, DeBERTa outperforms XLNet in six out of eight tasks. Particularly, the improvements on MRPC (1.7% over XLNet and 1.6% over RoBERTa), RTE (2.2% over XLNet and 1.5% over RoBERTa) and CoLA (0.5% over XLNet and 1.5% over RoBERTa) are significant. Note that MNLI is often used as an indicative task to monitor the progress of pre-training. DeBERTa significantly outperforms all existing models of similar size on MNLI and creates a new state-of-the-art (SOTA).
For fine-tuning, we trained each task with a hyper-parameter search procedure, each run will take about 1-2 hours on a DGX-2 node. The model selection is based on the performance on the task-specific development sets.
For fine-tuning, we trained each task with a hyper-parameter search procedure, each run will take about 1-2 hours on a DGX-2 node. The model selection is based on the performance on the task-specific development sets.
Thus in each layer, each token can attend directly to at most 2(k−1) tokens and itself. By stacking Transformer layers, each token in the l−th layer can attend to at most (2k−1)l tokens implicitly. Taking DeBERTalarge as an example, where k=512,L=24, in theory, the maximum sequence length that can be handled is 24528. This is a byproduct benefit of our design choice and we found it is beneficial for the RACE task.
MZ dataset. “DQN+relation branch” means our proposed model without knowledge-routed graph branch and “DQN+knowledge branch” is the model without relation refinement branch. Notably, our KR-DS not only significantly beats basic DQN (8%) but also outperforms SVM-ex&im (2%), which shows that our method can inquiry implicit symptoms effectively and make a precise diagnosis, thanks to knowledge-routed graph reasoning and relational refinement. Component analysis. Here we mainly target at the following components in our framework: knowledge-routed graph branch and relation refinement branch. Additionally, initializing relation matrix with conditional probability in the relational branch is better than random initialization (“DQN+relation*”), as the prior medical knowledge can guide the relation matrix learning.
DX dataset. We further evaluate the proposed end-to-end KR-DS through Deep Q-learning on our DX dataset. Focusing on obtaining the largest positive reward, basic DQN often guesses the right disease results but inquiries some unreasonable and repeated symptoms during the dialogue due to no constraints for symptom and disease relation (prior knowledge). Our framework shows superiority not only in a higher accuracy but also higher matching rate, which indicates the symptoms acquired by KR-DS agent is more reasonable and as a consequence, it can make the more right diagnosis. Seq-to-seq frameworks (Sequicity) performs worse on this medical diagnosis task as they focus on the in-dialogue sentence transition while ignoring medical symptom connections to diagnosis.
Reward evaluation. Our reward is designed based on the maximum turn value L=22, 2*L for success and -L for failure and -1 for the penalty. -1 penalty will cause shorter dialogue turns by accumulating through the process of dialogue. We evaluate several reward functions considering the magnitude of reward by doing experiments as follows: we chose four group of rewards R1: +22, -11, -1 ; R2: +11, -6, -1; R1* : +22, -11, -0.5; R2*: +11, -6, -0.25 for success, failure and penalty in experiments, and got accuracy shown in Fig. We found using smaller reward value achieves similar results with ours, but leading to a stable training process.
Overall, our M-MaxLSTM-CNN shows competitive performances in these tasks. Especially in the STS task, M-MaxLSTM-CNN outperforms the state-of-the-art methods on the two datasets. Because STSB includes complicated samples compared to SICK, the performances of methods on STSB are quite lower. In STSB, the prior top performance methods use ensemble approaches mixing hand-crafted features (word alignment, syntactic features, N-gram overlaps) and neural sentence representations, while our approach is only based on a neural sentence modeling architecture. In addition, we observed that InferSent shows the strong performance on SICK-R but quite low on STSB while our model consistently obtains the strong performances on both of the datasets. InferSent uses transfer knowledge on textual entailment data, consequently it obtains the strong performance on this entailment task.
In this section, we evaluate the efficiency of using multiple pre-trained word embeddings. We compare our multiple pre-trained word embeddings model against models using only one pre-trained word embedding. The same objective function and Multi-level comparison are applied for these models. In case of using one pre-trained word embedding, the dimension of LSTM and the number of convolutional filters are set to the length of the corresponding word embedding. Because the approach using five word embeddings outperforms the approaches using two, three, or four word embeddings, we only report the performance of using five word embeddings. We also report |V|avai which is the proportion of vocabulary available in a pre-trained word embedding. SICK dataset ignores idiomatic multi-word expressions, and named entities, consequently the |V|avai of SICK is quite high.
Our main findings are as follows: Would data augmentation have the same effect as external data? We perform the following transformations: scaling by a ratio of 0.8 and translation in a random direction by 10 pixels, rotation of the original image at a random angle up to 30 degrees both clockwise and counterclockwise. We generate augmented data with roughly the same size as the external data (960 word and 168,950 frames) and then train the CNN/DNN + enc-dec model (with frame labels). We hypothesize that the extra unlabeled hand data provides a richer set of examples than do the geometric transformations of the augmented data.
The performance of the proposed Mod-EDA is tested on the text classification task with BERT, ELMO, and USE. The results are an average of five random seeds. Before applying Mod-EDA on Twitter dataset, the accuracy of Mod-EDA is compared with EDA on a benchmark dataset SST2 (Stanford SentiTreebank dataset 2013) We run BERT, ELMO, and USE on the dataset without any augmentation, with EDA, and with Mod-EDA. The accuracy of all the models were captured in terms of precision, recall, and F1-score using 10-fold cross-validation. For testing the accuracy of the classification model, 90 percent of the data is separated from the whole corpus, and Mod-EDA and EDA are applied on it to generate an augmented corpus. The training and testing set is generated from this 90 percent data in the 80-20 ratio. This is named as the validation accuracy of the data. Later the trained model is run on the remaining 10 percent of the data. The accuracy of the remaining 10 percent data is named as the testing accuracy. This type of training-testing ensures that there is no overlap between the training and the testing data. Mod-EDA shows an average improvement of 1.76% from EDA (maximum 2.03% with BERT) and 1.82% (maximum 2.98% with BERT) from data without augmentation in F1-score is visible.
For speaker verification, we compare APCs with the i-vector representation. We train a GMM with 256 components as the universal background model on the TIMIT training set. We then extract 100-dimensional i-vectors and project them down to 24 dimensions with LDA trained on the training set. The cosine similarity is used for evaluation. We also include the best results from all CPC models. Same as what we do in the phone classification experiments, the outputs of the last RNN layer are taken as the extracted representations. The representation of the entire utterance is a simple average of the frame representations. For the last two rows , i.e., apc 3-layer-1 and apc 3-layer-2 , it means that we take the outputs of the first and the second RNN layer as the extracted representations. We explain our motivation of doing so below. Comparing APC with i-vector and CPC. This demonstrates that representations learned by APCs contain not only phonetic information but also speaker information. Speaker information across different APC layers. Unlike phone classification, where we find increasing the depth of APCs improve PER, deeper APCs somehow performs worse in speaker verification. Motivated by the fact that LMs for text could exhibit different kinds of information across different layers, we are interested in investigating whether other layers besides the last one contain more information of our interest, that is, the speaker information. Surprisingly, for all #(steps), we see that apc 3-layer-1 consistently outperforms apc 3-layer-2, which further outperforms apc 3-layer. This indicates that lower layers indeed contain more speaker information than higher layers, or at least the speaker information is represented in a more accessible form in lower layers. Additionally, we observe that apc 3-layer-1 outperforms apc 1-layer and apc 3-layer-2 outperforms apc 2-layer although the representations are extracted from the same RNN depth. Combining all of our observations from both tasks, we conclude that a deep APC is a very powerful speech feature extractor, whose higher layers capture phonetic information while more speaker information resides in its lower layers.
On all the datasets, we separately train a logistic regression model on top of the extracted sentence features. We restrict our comparison to methods that also aims to learn generic sentence embeddings for fair comparison. We also provide the state-of-the-art results using task-dependent learning methods for reference. Our CNN encoder provides better results than the combine-skip model of Kiros et al.
We show that the Sato model successfully improves prediction accuracy by introducing the topic-aware features and the CRF layer. However, the additional components may cause additional time cost. To evaluate the efficiency of Sato, we repeated the training and prediction procedures for 5 times and measured the training and prediction time of Base and Sato on the multi-column dataset Dmult. The training data contains 26K tables and the test data contains 6.4K tables. For further investigation on the cost of the topic-aware features and the CRF layer, we separately measured the time for training the main model, and the time for training the CRF layer. The experiment was conducted on a single machine with 2.1GHz CPUs (64 cores) and 512GB RAM.
In this step, we essentially train the network on the data of 13 subjects and test on the 14th subject, to check the inter-subject variability of our model. As it is evident from the figure, with direct covariance data, the predicted classes corresponding to each true label are widely distributed throughout the matrix and hardly gives any significant information about the actual speech token. However, involvement of the phonological categorization as an intermediate step increases the prediction accuracy. Interestingly, the false negatives corresponding to each of the tokens also inform us about the respective structure of the word or phoneme. For example, the misclassification of /n/ as /m/, ‘knew’ and ‘gnaw’ in a few cases, show that while the network gets strong discriminative features from the other five networks, features pertaining to the nasal category require more discriminative ability to more accurately categorize the phoneme /n/. Such an observation indeed proves that the phonological features play a significant role for achieving an accurate classification of the speech tokens.
We found that watching the illusory videos led to an average miscomprehension rate of 24.8%, a relative 148% increase from the baseline of listening to the audio alone The illusory videos made people less confident about their correct answers, with an additional 5.1% of words being heard correctly but unclearly, compared to 2.1% for audio only. For 17% of the 200 words, the illusory videos increased the error rates by more than 30% above the audio-only baseline.
We used logistic regression as our main classification method. As we have a small dataset, we performed 5-fold cross-validation. For evaluation, we used accuracy and macro-average F1 score. We evaluated a total of 14 setups for feature combination. Four of them represent features generated from the original article’s title and body as well as a combination thereof. The next four setups present feature sets generated from the English translation as well as a combination thereof. We tuned the logistic regression for each individual experimental setup, using an additional internal cross-validation for the training part of each experiment in the 5-fold cross-validation. In total, 15,000 additional experiments have been conducted to complete the fine-tuning. The feature combinations (setups 6, 11, 13) do not yield good results as this increases the number of features, while the number of training examples remains limited. Using only the right 6 meta features about the target medium yields 12% improvement over the baseline. Interestingly, LSA turns out to be the best text representation model. Next we tried a meta classifier. Then, we trained a logistic classifier on these posteriors (we made sure that we do not leak information about the labels when training the meta classifier). We can see that the model works best for the biggest non-toxic class. A decent chunk of fake news samples are misclassified as conspiracy as those two classes are the second and the third largest ones. For three of the labels, there are hardly any predictions; these are the smallest classes, and three of them combined cover less than 18% of the dataset.
We observe that all methods perform better than a majority classifier baseline. This suggests that the former might suffer from overfitting due to the limited size of the training data. The classification outcome of the logistic regression model is statistically different than all other models (Student t-test, p<0.001, Bonferroni-adjusted.) Recall that the labels obtained from Cohan et al. Techniques to modify ROC analysis to consider probabilistic labels have been proposed in the literature. We consider the variant introduced by Burl et al. While the ROC curve for the XGBoost is comparable to the logistic regression classifier, we observe that the SVM model achieves similar performance to the two only for high confidence samples (bottom left corner), and its performance declines sharply when more positive samples are inferred.
We observe that using separate symmetric features for the two groups of users not only improves upon using features from target user posts alone, it also outperforms averaging features extracted from the two groups together (Student t-test, p<0.001, Bonferroni-adjusted.) This empirically confirms our hypothesis that language and topics from target and participating users should be modeled separately.
Surprisingly, our model considering only the content of mentions (self-attention wo context) achieves competitive results as the baseline cascade collective which explores many hand-crafted linguistic features. Also self-attention wo context outperforms the two baselines on several IS categories (m/syntactic, m/aggregate, m/comparative, m/bridging and new). The improvements on these categories show that our model can capture the semantic/syntactic properties of a mention when predicting its IS.
This table confirms that the clustering coefficient in the factoid networks is generally significantly greater than random networks of the same size. Moreover, this table confirms that the average shortest paths in the random networks are small.
We observe that CogCompXEL also achieves a significant performance improvement in geopolitical (GPE) or location (LOC) entities, while the performance on person (PER) and organization (ORG) entities is comparative with other approaches. We believe this improvement is brought by the use of cross-lingual information from Google Map KB.
We eliminate numbers and punctuation from all lexicons. For each of these languages, we select 10000 words for training and the rest of the word types for evaluation. The only difference is the source of the seed lexicon and test set. averaged over 10 different randomly selected seed set for every language. For each language we obtain more than 70% F1 score and on an average obtain 79.7%. Critically, the F1 score on human curated lexicons is higher for each language than the treebank constructed lexicons, in some cases as high as 9% absolute. This shows that the average 74.3% F1 score across all 11 languages is likely underestimated.
We use the pre-specified train/dev/test splits that come with the data. The three columns show the F1 score of the tagger when no lexicon is used; when the seed lexicon derived from the training data is used; and when label propagation is applied.
Overall, the generated lexicon gives an improvement of absolute 1.5% point over the baseline (5.3% relative reduction in error) and 0.5% over the seed lexicon on an average across 11 languages. Critically this improvement holds for 10/11 languages over the baseline and 8/11 languages over the system that uses seed lexicon only.
The five features chosen through recursive feature elimination were: the noun-verb ratio (↑), the pronoun-noun ratio (↑), the mean Yngve depth (↓), the pronoun rate (↓) and the content density (↓), where the arrows indicate the change direction indicating aphasia as implied by the linear SVM coefficient sign. The classifier achieves an accuracy of 90.3% on the English test set and 83.3% on the French test set but does not perform better than random on the Mandarin test set. The results suggest that this simple approach may generalize better to other European languages than more distant languages. Adoption of domain adaptation techniques [pan2010domain] may be necessary to achieve the cross-language and -task generalizability required here. Other complicating factors include the small size of the data sets and the differing composition of aphasia types within the ‘aphasia’ class. Finally, we note that we chose not to optimize model parameters but rather to use this example to illustrate the ease of multilingual analysis using BlaBla.
which independently samples answers performs the best. To benchmark human performance against such models, we collected 30 human responses per question and aggregated them by counts (like GPT2 predictions). The low performance of the KB baseline hints towards the low coverage of ConceptNet required for answering the prototypical scenarios of our questions. Similarly powerful QA models and large LMs that are trained on large corpus also seem to not have the common sense knowledge required to answer the question. Surprisingly, the performance of GPT2 model that was further fine-tuned on our training data significantly improved, suggesting the usefulness of our accompanying training set. However, the human performance for all our metrics significantly outperform all baselines suggesting large scope of improvement. We have presented a new common sense dataset with many novel features. The inclusion of counts over clusters of answers provides a very rich structure to train and evaluate with. The collection of a large set of answers and a proposed automated method of assigning answers to clusters facilitates an open-ended style of evaluation, which is often the desired use-case for these models.
Experiments show that the generation quality of our proposed model is on par with the state-of-the-art text summarization models. We observe approximately 1.5× faster decoding than the autoregressive Transformer while achieving better generation quality. Specially, our model with beam search (greedy search) is capable of decoding 1.64× (2.26×) faster than conventional Transformer on English Gigaword test set.
In this table, every block provides the results for one of the variations of obtaining the embeddings and different ways of combining them. We observe that the Joint Method (JM) of combining the embeddings works the best in all cases except the Tag Embeddings. Among the ablations, the proposed MDN method works way better than the other variants in terms of BLEU, METEOR and ROUGE metrics by achieving an improvement of 6%, 12% and 18% in the scores respectively over the best other variant.
Taking Nouns generated from the captions and questions of the corresponding training example as context, we achieve an increase of 1.6% in Bleu Score and 2% in METEOR and 34.4% in CIDEr Score from the basic Image model. Similarly taking Verbs as context gives us an increase of 1.3% in Bleu Score and 2.1% in METEOR and 33.5% in CIDEr Score from the basic Image model. And the best result comes when we take 3 Wh-Words as context and apply the Hadamard Model with concatenating the 3 WH-words. Here we show that for 3 words i.e 3 nouns, 3 verbs and 3 Wh-words, the Concatenation model performs the best. In this table the conv model is using 1D convolution to combine the tags and the joint model combine all the tags.
Taking Nouns generated from the captions and questions of the corresponding training example as context, we achieve an increase of 1.6% in Bleu Score and 2% in METEOR and 34.4% in CIDEr Score from the basic Image model. Similarly taking Verbs as context gives us an increase of 1.3% in Bleu Score and 2.1% in METEOR and 33.5% in CIDEr Score from the basic Image model. And the best result comes when we take 3 Wh-Words as context and apply the Hadamard Model with concatenating the 3 WH-words. Here we show that for 3 words i.e 3 nouns, 3 verbs and 3 Wh-words, the Concatenation model performs the best. In this table the conv model is using 1D convolution to combine the tags and the joint model combine all the tags.
We had similar concerns and validated this point by using random exemplars for the nearest neighbor for MDN. In this case the method is similar to the baseline. This suggests that with random exemplar, the model learns to ignore the cue. In Multimodel Differential Network and Differential Image Network, we use exemplar images(target, supporting and opposing image) to obtain the differential context. We have performed the experiment based on the single exemplar(K=1), which is one supporting and one opposing image along with target image, based on two exemplar(K=2), i.e. two supporting and two opposing image along with single target image.
The second and third show that the agents are mostly conveying information about their respective objects (which is very reasonable), but also, to a lesser extent, but still well above baseline-level, about the other agent’s input. This latter observation is intriguing. Further work should ascertain if it is an artifact of fruit-tool correlations, or pointing in the direction of more interesting linguistic phenomena (e.g., asking ‘‘questions’’). The asymmetry between Tool 1 and 2 would also deserve further study, but importantly the agents are clearly referring to both tools, showing they are not adopting entirely degenerate strategies.
iDST respectively, by creating an ensemble of independent models, each of which refers to one of the micro-components. As a comparative analysis, we also decided to train models which, instead of using the ASR 1-best hypothesis (ASR suffix), use the manual transcription of the user utterances (TRA suffix). Variables r and d in brackets indicate the ratio of used utterance and confidence of turn-taking respectively. Since the length of the user utterances is not fixed, it was necessary to shift the imposed 60% prefix point to the nearest token, which would inevitably alter this imposed percentage. The exact ratio has therefore been computed as the average of the actual percentage values, including shifts. This implies that iDST_ASR(r = 0.6) actually uses, as an average percentage value, 61% and 68% of the user utterance on the dev and test set respectively, instead of the 60% imposed value. iDST_{ASR, TRA}(r = 1.0) and iDST_{ASR, TRA}(r = 0.6) use respectively 100% and 60% of the user utterance for dialog state estimation. iTTD_{ASR, TRA}(d = 0.85) indicates that all micro-components in the ensemble must have a confidence of at least 85% on the predicted 0 label (turn-taking). Setting the confidence threshold value to 85% causes the iTTD to take the turn on average at 61% (r = 0.61) of the user utterance, thus making it comparable with the deterministic iDST_{ASR, TRA}(r = 0.6). It can, therefore, be observed that the 0.61 ratio prefix point chosen on average by the iTTD_ASR(d = 0.85) obtains a better or comparable performance with respect to the deterministic iDST_ASR(r = 0.6) which uses only 60% of the user utterance. The iDST_TRA and iTTD_TRA models trained on the manual user transcript show how the output of the ASR negatively affects the performance of the models. For instance, if we consider user utterances of length 2 (e.g. ”phone number”, ”thank you”, ” good bye”, ”price range”) , it can be observed that a single word, which corresponds to 50% of the utterance, is often enough for the turn taking decision.
Our vectors achieve performance comparable to the state of art on semantic analogies (similar accuracy as GloVe, better than word2vec). On syntactic tasks, they achieve accuracy 0.04 lower than GloVe and skip-gram, while CBOW typically outperforms the others. The reason is probably that our model ignores local word order, whereas the other models capture it to some extent. For example, a word “she” can affect the context by a lot and determine if the next word is “thinks” rather than “think”. Incorporating such linguistic features in the model is left for future work.
Now we show that even if a relationship is tested only once in the testbed, there is a way to use the above structure. Given “a:b::c:??,” the solver first finds the top 300 nearest neighbors of a and those of b, and then finds among these neighbors the top k pairs (a′,b′) so that the cosine similarities between va′−vb′ and va−vb are largest. This algorithm is named analogy solver with relation direction by nearest neighbors (RD-nn).
Artetxe et al. also study he unit-length and zero-mean constraints, but our work differs in two aspects. First, they motivate the zero-mean condition based on the heuristic argument that two randomly selected word types should not be semantically similar (or dissimilar) in expectation. While this statement is attractive at first blush, some word types have more synonyms than others, so we argue that word types might not be evenly distributed in the semantic space. We instead show that zero-mean is helpful because it satisfies center-invariance, a necessary condition for orthogonal mappings. Second, Artetxe et al. Unfortunately, this often fails to meet the constraints at the same time—length normalization can change the mean, and mean centering can change vector length.
ABX scores in the multi-corpus condition are similar to those in the single-corpus condition, and, in fact, slightly worse, suggesting that adding speakers from additional corpora to the training is not beneficial to the speaker-independent model. Again, the reconstruction measures do not go in the same direction, improving slightly when corpora are added. It is possible that the addition of more data is helping to better reconstructing (informative) speaker-specific subphonemic information; but we observe that the appreciable improvement is exclusively in the novel corpora, suggesting that the improvements in reconstruction may be due to improved modelling of acoustic channel or coil placement properties specific to these corpora. This points to the differences among corpora, and the incompatibility that can exist between them, leading to potentially worse articulatory reconstructions when using them together, information that would be lost without the ABX score.
Previous studies Libovickỳ et al. Additionally, long-text answers can have multiple different questions each one targeting different aspects Hu et al. Gao et al. Therefore, we use multi-source transformer to provide extra contextual information to help guide the question generation under LAQG setting. Specifically, we propose to input the summary sentence, extracted using a reinforcement-learning based summarization approach Narayan et al. We also analyse the performance of our models on varying sentence lengths using automated NLG evaluation metrics. We observe a positive correlation between model performance degradation and answer length increase for RNN-based maxout pointer baseline as well as our Transformer model. These fine-grained results also support the finding that Transformer LAQG models are more robust to increased answer length.
’s paragraph vectors to enhance the basic language model based retrieval model. The language model(LM) probabilities are estimated from the corpus and smoothed using a Dirichlet prior Zhai and Lafferty P(w|d)=(1−λ)PLM(w|d)+λPPV(w|d) where, PPV(w|d)=exp(→w.→d)∑Vi=1exp(→wi.→d) and the score for document d and query string Q is given by score(q,d)=∑w∈QP(w)P(w|d) where P(w) is obtained from the unigram query model and score(q,d) is used to rank documents. Ai et al. To directly make use of paragraph vectors and make computations more tractable, we directly interpolate the language model query-document score score(q,d) with the similarity score between the normalized query and document vectors to generate scorePV(q,d), which is then used to rank documents. scorePV(q,d)=(1−λ)score(q,d)+λ→q.→d Directly evaluating the document similarity score with the query paragraph vector rather than collecting similarity scores for individual words in the query helps avoid confusion amongst distinct query topics and makes the interpolation operation faster. Precision(MAP) values for four datasets, Associated Press 88-89 (topics 51-200), Wall Street Journal (topics 51-200), San Jose Mercury (topics 51-150) and Disks 4 & 5 (topics 301-450) in the TREC collection. We learn λ on a held out set of topics. We observe consistent improvement in MAP for all datasets. We marginally improve the MAP reported by Ai et al. on the Robust04 task. Again, we notice a consistent improvement in MAP.
We consider both a constrained and an unconstrained version of the task. For each word, the constrained version of the task only considers the senses present in the MSH-WSD dataset as possible targets. The unconstrained version considers all concepts which are denoted by the ambiguous term in the 2015AB version of the UMLS as possible targets. The term cortex, for example, only has 2 concepts associated with it in the MSH-WSD dataset, while in the 2015AB UMLS release it can denote 5 separate concepts. Because the unconstrained version of the task considers all words, it therefore gives a better indication of real-life performance.
Our approach obtains an accuracy of >90% on 103 terms, showing that it is able to disambiguate a large variety of terms. For some terms, however, the performance was below random guessing. The pattern of errors is quite clear: Our approach has trouble with disambiguation if the definitions of the concepts themselves are lexically very similar. As an example, on the term Hemlock our approach performs below chance level because one of the concepts denotes a family of poisonous plants, while the other reports a tree, also called hemlock, the description of which mentions that it is explicitly not poisonous. We expect these kinds of problems to be alleviated with the addition of more data.
First we compare our pre-trained model with several baseline methods. We classify the methods with two settings: 1) with/without pre-training; 2) the input is video-only or video+transcript. Zhou et al. Sun et al. Shi et al. We study the video-only captioning models and find that our model (our model.1st) can get comparable results with CBT. Furthermore, we compare our model with various data sizes (our model.2nd, 3rd, 5th), the performance of our models improves with the increasing of the pre-training data size. Moreover, according to the comparison of our models with or without pre-trained decoder (our model.4th vs. 5th), pre-training the decoder improves the performance of generation task, and our full model (our model.5th) on the largest pre-training dataset achieves the best results.
significantly change when the knowledge graphs are largely updated (All) and can also achieve good accurate change rate. For them, the more parts updated (All >> Last2 > Last1), the more changes and accurate changes. However, when the knowledge graphs are slightly updated (Last1 and Last2), the portion of accurate changes over total changes (e.g., the Last1 score 1.17/31.78 for HGZHZ with MemNet model) is significantly low. Among the baselines, KAware has better performance on Last1. On the other hand, Qadpt outperforms all baselines when the knowledge graphs slightly change (Last1 and Last2) in terms of accurate change rate. The proportion of accurate changes over total changes also show significantly better performance than the prior models. After combining TAware or MemNet, the distribution becomes more similar to the test data.
In the sampling process of CorrLDA2, each opinion word is first assigned to a supertopic x, which is one of the T topics that the topical words are assigned to. Then, one of the ~T aspects is also assigned to the opinion word, conditioned on the supertopic. Since each opinion word is assigned to both a topic and an aspect, statistics of topic-aspect relations can be obtained by counting the number of times a topic co-occurs with an aspect. For the following experiment, we use the (opinion+ne) partition of the vocabulary, with T=20 topics and ~T=2 aspects. Five out of the twenty topics do not pass the 0.7 threshold and can be viewed as neutral topics. This result suggests that the topic-aspect relations learned by CorrLDA2 are strong indicators of the viewpoints that the topics are associated with. Topic 16 include the words “territory”, “violation”, and “occupation”. These topics seem to refer to what Palestinians consider to be occupation of Palestinian territory, which they see as being in violation to international law. The top five words in Topic 11 include “intifada”, “resistance”, “struggle”, and “occupation”. Intifada is an Arabic word referring to resistance movements. During the period in which the documents of the corpus were published, what is generally called the Second Intifada, or the Al-Aqsa Intifada, took place. The top four words of Topic 5 include “refugee”, “right”, and “return”, referring to what the Palestinians view as the Palestinian right of return to territory occupied by the Israelis, which has displaced millions of Palestinians. The top four words of Topic 20 are: “attack”, “war”, “terrorism”, and “suicide”, referring to the numerous suicide attacks conducted by what the Israelis consider to be Palestinian terrorists during this period. Topic 10 contains the words “disengagement”, “withdrawal”, and “settlement”, referring to the Israeli disengagement from Gaza, which the Israelis consider demonstrates their willingness to go through with the peace negotiations. For each of the two groups, if the aspect associated with the group has a negative SVM feature weight, then the group is classified as Palestinian. If the aspect has a positive weight, then it is classified as an Israeli group. We then define the score of a group to be the sum of the SVM feature weights of the topics in the group. We form two groups in this way for each T=1,2,… ,60 total number of topics.
We evaluate the performance of supervised hashing in this section. We observe that all of the VAE-based generative hashing models (i.e VDSH, NASH, GMSH and BMSH) exhibit better performance, demonstrating the effectiveness of generative models on the task of semantic hashing. It can be also seen that BMSH-S achieves the best performance, suggesting that the advantages of Bernoulli mixture priors can also be extended to the supervised scenarios.
As we explained in the Dataset section, due to the small size of our dataset, we use 5-fold cross-validation weighted F1 score which is weighted by support (the number of true instances for each label), as our main metric to compare model performance. Our best model is with BioBERT embedding, LSTM-Attention mechanism as word-to-clause encoding and a BiLSTM-CRF as the sequence tagger achieves cross-validation F1 of 0.784. We also re-run \newcitedasigi2017experiment’s best model and obtained 0.746 mean of 5-fold cross-validation F1 score. Our best model’s cross-validation F1 score is significantly higher (per McNemar’s test, p<10−5). Accordingly, our best model significantly outperforms feature-based SVM and CRF models. We also conduct ablation study to understand the effect of different components of our model.
The dark diagonals indicate that the model is capable of correctly predicting most of the labels. Interestingly, the difficulty of predicting labels varies on the discourse types. The model predicts relatively better on labels such as method and result while it predicts problem and none poorly.
Results in this section are reported on a set of 13.3K anonymized utterances in the domain of open-ended dictation extracted from Google traffic. We benchmark our systems to determine runtime speed by decoding a subset of 100 utterances on a Nexus 5 smartphone which contains a 2.26 GHz quad-core CPU and 2 GB of RAM. We report median real-time factors (RT50) on our test set. Furthermore, although both systems are comparable in terms of the number of parameters, the CTC-trained model is about 4× faster than the CE-trained baseline. In order to reduce memory consumption further, we compress our acoustic models using projection layers that sit between the outputs of an LSTM layer and both the recurrent and non-recurrent inputs to same and subsequent layers Of crucial importance, however, is that when a significant rank reduction is applied, it is not sufficient to simply initialize the projection layer’s weight matrix randomly for training with the CTC criterion. Instead we use the larger ‘uncompressed’ model without the projection layer and jointly factorize its recurrent and (non-recurrent) inter-layer weight matrices at each hidden layer using a form of singular value decomposition to determine a shared projection layer. In our system, we introduce projection matrices of rank 100 for the first four layers, and a projection matrix of rank 200 for the fifth hidden layer. Following SVD compression, we once again train the system to optimize the CTC criterion, followed by discriminative sequence training with the sMBR criterion. results in a further 12.8% relative improvement over the SVD compressed models. The combination of all of these techniques allows us to significantly improve performance over the baseline system. Since the 11.9 MB floating point neural network acoustic model described above consumes a significant chunk of the memory and processing-time, we quantize the model parameters (i.e. weights, biases) into a more compact 8-bit integer-based representation. This quantization has an immediate impact on the memory usage, reducing the acoustic model’s footprint to a fourth of the original size. Using 8-bit integers also has the advantage that we can also achieve 8-way parallelism in many matrix operations on most mobile platforms. During neural network inference, we operate in 8-bit integers everywhere except in the activation functions and the final output of the network, which remain in floating point precision (by converting between quantized 8-bit values and their 32-bit equivalents as needed).
As a final experiment, we compare our TL-TranSum approach to other summarizers presented at DUC contests. Regarding DUC2007, our method outperforms the best system by 2% and the average ROUGE-SU4 score by 21%. It also performs significantly better than the baseline of NIST. However, it is outperformed by the human summarizer since our systems produces extracts, while humans naturally reformulate the original sentences to compress their content and produce more informative summaries. Tests on DUC2006 dataset lead to similar conclusions, with our TL-TranSum algorithm outperforming the best other system and the average ROUGE-SU4 score by 2% and 22%, respectively. Nevertheless, the ROUGE-SU4 score produced by our TL-TranSum algorithm is still 15% higher than the average score for DUC2005 contest.
We submitted runs under team “covidex” (for neural models) and team “anserini” (for our bag-of-words baselines). In Round 2, there were 136 runs from 51 teams. Comparing these two fusion baselines, we see that our query generation approach yields a large gain in effectiveness. Ablation studies further confirmed that ranking signals from the different indexes do contribute to the overall higher effectiveness of the rank fusion runs. That is, the effectiveness of the fusion results is higher than results from any of the individual indexes. (mpiid5_run3 and SparseDenseSciBert, respectively), which were also the top two runs overall. These results show that manual and feedback techniques can achieve quite a bit of gain over fully automatic techniques. Both of these runs and four out of the five top teams in round 2 took advantage of the fusion baselines we provided, which demonstrates our impact not only in developing effective ranking models, but also our service to the community in providing infrastructure. The r3.duot5 run ranks second among all teams under the “automatic” condition, and we are about two points behind team SFDC. However, according to Esteva et al. , their general approach incorporates Anserini fusion runs, which bolsters our case that we are providing valuable infrastructure for the community.
The rows are the majority roles, which we take to be the ground truth pedagogical roles of documents. Although there are 1264 majority pedagogical role annotations, we calculated the confusion matrix for 1206 roles from documents with only one majority role each, for ease of interpretation. From the 1206 pedagogical roles, there are 1245 role pairs between the majority role and the third annotator’s annotated role(s). We also see that Other documents have relatively higher rates of misclassification. These results are consistent with feedback from annotators. The reason why Survey documents are sometimes mistaken for Reference Works is because both examine a broad number of subjects in a domain; the distinction we make in our annotation guidelines is that Reference Works are a collection of established authoritative facts such as those one might find in an encyclopedia, whereas Surveys focus on the discoveries of other publications. When looking for Resource papers, annotators rely on looking for few indicator sentences that may be missed with a more superficial skim of the document. Also, the Other documents belong to a range of additional pedagogical roles, though we do not make finer distinctions here.
The scores for Other documents are an anticipated exception to the trend, because we do not make more fine-grained distinctions between other pedagogical roles in this work. Software Manuals are also an exception to this trend, as their scores are relatively high for the number of samples; this is because Software Manuals are typically written in a very distinct style. CEN generally performs poorly across roles, doing worse than the baseline random forest classification with TF–IDF. This suggests that word frequency is more informative about the pedagogical roles of a document than a single representative vector per role.
The rows are the ground truth roles, and the columns are the predicted roles. We can see that Surveys, Resources, and Other documents are often mistaken to be documents with Empirical Results. Additionally, there are relatively more instances of Surveys, Resources, and Other documents where the classifier is unable to make a prediction. Overall, these results suggest that the misclassifications are an effect of an unbalanced dataset with many more samples of Empirical Results, rather than an inherent lack of distinctness between documents of different roles.
We observe that as the dataset decreases in size, a model with a larger dropout value performs slightly better. A brief tally of phenomena that are difficult to learn for many machine learning models, categorized along typical linguistic dimensions (such as word-internal sound changes, vowel harmony, circumfixation, ablaut, and umlaut phenomena) fail to reveal any consistent pattern of advantage to the transformer model. In fact, errors seem to be randomly distributed with an overall advantage of the transformer model. Curiously, errors grouped along the dimension of word length
A dropout rate of 0.3 yields significantly better performance on the transliteration task while a dropout rate of 0.1 is stronger on the g2p task. This shows that transformers can and do outperform recurrent transducers on common character-level tasks when properly tuned.
We found that without input image features, scene graphs with node names significantly improve all question types but “when”, which needs holistic visual features. Even with image features, scene graphs with node names can still largely improve the “what”, “who”, and “number” types; the former two take advantage of node names while the later takes advantage of number of nodes. Adding the node attributes specifically benefits the “color” type. Overall, the VG graphs are of higher quality than NM graphs except for the “number” type.
This section evaluates the implicit entity linking task in isolation. We show the results on both candidate selection and ranking, and disambiguation steps. The candidate selection and ranking is evaluated as the proportion of tweets that had the correct entity within the top k selected candidates for that tweet (denoted as Candidate Selection Recall). We experimented by varying k between 5 and 35 and found that results improves as we increase k and comes to near plateau after k=20. We demonstrate the results for k=25 and interested readers can find detailed results on our project page. The disambiguation step is evaluated with 5-fold cross validation and report the results as proportion of the tweets in evaluation dataset that had correct annotation at the top position.
However, we see improvements across all metrics in both the pre and prog strategies of hinting the parser. When the strategies are combined we see a significant improvement across all metrics, with 10.5% improvement to F-score and 14.3% improvement in semantic factor accuracy. However, the largest improvement comes with a large increase in fragmentation. The more modest improvement produced by the pre-hinting and combined strategies reduce fragmentation because the improvements elicited from these systems are the result of better and more options in the parser.
MMIbas. We optimize α and β on the validation set using BLEU score, since \newcitemmi have shown that adding MMI during inference improves the BLEU score. We set γ=0 and find optimal values αopt=50.0 and βopt=0.001 using grid search. SEE has the worst performance overall and does not compete with either the baseline, nor with SED. This is expected according to the results reported by \newciteemotion-token. It seems that the model is not able to capture the information carried by the additional emotion embedding token – it is treated as just one additional word among 20 others. SED makes better use of the emotion information, as it is used at each time step during decoding. In addition, it is more natural to use these features during the decoding, since the emotion embedding represents the desired emotion of the response. The combination of WI and WE performs best in terms of distinct-1 and distinct-2 measures among all models without re-ranking, yielding an improvement of up to 13.1%. It suggests that the word level emotion models suit the seq2seq architecture better. During training, both models are encouraged not only to match the target words, but also to promote less frequent words that are close to the target words in terms of VAD values (affective regularizer and affective sampling), fostering the model to generate more diverse responses. To reduce the potential negative impact of choosing inappropriate first words in the sequence, we compute the BLEU score on the result of beam search of size 200. For example, if the first word is “I”, the seq2seq models tend to generate a response “I don’t know” with high probability, due to the high number of appearances of such terms in the training set. In certain cases, like WI and SED, we observe an improvement. Such an improvement is expected, since our model takes into account additional (affective) information from the target sequence during response generation.
Recall this metric is the mean best-threshold F1 over 26 languages. The naive baseline of only predicting No Answer achieves a lower bound score of 32.42%. We observe Xlm-R with Translate Train achieves the best score with an average F1 of 46.0\pm1.4. In general, Translate Train achieves better results than Zero Shot or Translate Test. Secondly, the unanswerable F1 is typically much higher than the answerable F1, after choosing the best threshold, which indicates the identification of ambiguous or long-answer questions may be an easier problem. Additionally, there seems to be an inverse relationship between the best F1 and the unanswerable F1, meaning that poor performing models will opt out of answering questions more often.
The label is further enriched with a prefix in BIO encoding style, motivated by the fact that we want to model spans of information. Punctuation symbols are treated as O, because due to their location at boundary positions the pause information varies highly. We leave treating punctuation separately as future work. \newciteklerke:ea:2016 use a related encoding scheme to discretize fixation durations obtained from eye tracking data, however, in contrast to them we here use median-based measures which are better suited for such highly skewed data [leys2013detecting].
Our chunking baseline achieves an F1 of 93.21 on CoNLL, compared to the F1 of 93.88 of \newcitesuzuki:isozaki:2008, who use a CRF and gold POS tags. We do not use any POS information. A similar bi-LSTM achieves 93.64 [Huang:ea:15], however, additionally uses POS embeddings. Our baseline CCG supertagging model achieves 92.41, compared to the more complex model by \newcitexu:ea:2015 achieving an accuracy of 93.00. Very recently even higher accuracies were reported, e.g. [vaswani-EtAl:2016:N16-1], however, in this exploratory paper we are interested in examining whether we find signal in keystroke data, and are not interested in beating the latest state-of-the-art.
To prove that our generative model makes use of the auxiliary information to produce more contextually likely reviews, we report the test set perplexities for all models ( As a baseline, we report the perplexity achieved by an unsupervised LSTM language model. We then report the perplexities of GCNs trained with rating, item ID, and user ID, as auxiliary information.
Note that the entire hyper-specialisation process was performed on approximately 6 minutes. We used a learning rate set to 0.7. Further experiments need to be conducted for a better understanding of the learning rate role in hyper-specialisation work. Accuracy gains are obtained despite using automatic (noisy) translation hypotheses to hyper-specialise: +0.52 (German→English) and +0.57 (English→German). In order to measure the impact of using newstest2017 as training data (sefl-training) we repeated the hyper-specialisation experiment using as training data newstest sets from 2009 to 2016. This is, excluding newstest2017 (T-2017).
While the standard non-local block outperforms the baseline counterpart by 0.8% bbox mAP and 0.7% mask mAP, the proposed disentangled non-local block brings an additional 0.7% bbox mAP and 0.6% mask mAP in gains. Please also see Appendix for experiments when stacking more non-local or disentangled non-local blocks. It can be seen that the disentangled design performs 0.36% better than using standard non-local block. Noting the gains of the standard non-local block over the baseline is 1.0, the relative gains of disentangled non-local block over a standard NL block is 36%.
While the standard non-local block outperforms the baseline counterpart by 0.8% bbox mAP and 0.7% mask mAP, the proposed disentangled non-local block brings an additional 0.7% bbox mAP and 0.6% mask mAP in gains. Please also see Appendix for experiments when stacking more non-local or disentangled non-local blocks. It can be seen that the disentangled design performs 0.36% better than using standard non-local block. Noting the gains of the standard non-local block over the baseline is 1.0, the relative gains of disentangled non-local block over a standard NL block is 36%.
In this section, we add the multiscale 1-D convolution filter and context history to our baseline results. Here, we used a set of 1-D convolution filters F with output channel dk=64 and kernel size t1=7,t2=15,t3=31,t4=63. For the activation f(⋅) (Eq. and g(⋅) (Eq. Furthermore, by increasing the number of time-steps incorporated in the history, we get yet further improvement. Our best model with multiscale alignment and contextual information from the last three time-steps achieved 5.59% CER.
In it, we report the performance of 3 modern gated RNNs with multiplicative gating units, i.e., MI-RNN, MI-LSTM, MI-GRU. Interestingly enough, one could consider the multiplicative units to be a crude approximation of second order state neurons. Here, we compare back-propagation through time (BPTT) to other online learning algorithms such as real time recurrent learning (RTRL) and unbiased online recurrent optimization (UORO). We describe these procedures in further detail in the next section. Notably, in our experiments, we observed that UORO boosts performance for higher order recurrent networks, while being faster than RTRL, the original algorithm-of-choice when training higher order, state-based models. Furthermore, we remark that truncated BPTT (TBPTT), for some CFGs, can actually slightly improve model performance over BPTT (but in ohers, such as is the case for the palindrome CFG, lead to worse generalization). For all of the RNNs we study, we compared their (validation) performance when using various online and offline based learning algorithms. As mentioned in the last section, we found that UORO worked best for the NSPDA, which is advantageous in that UORO is faster than RTRL (even largely in terms of complexity) and does not require model unfolding like the popular and standard BPTT/TBPTT algorithms do. Below we briefly describe the non-standard approaches to learning RNNs, specifically RTRL and UORO. Notably, we are the first to implement and adapt UORO in calculating the updates to the weights of higher order networks.
In it, we report the performance of 3 modern gated RNNs with multiplicative gating units, i.e., MI-RNN, MI-LSTM, MI-GRU. Interestingly enough, one could consider the multiplicative units to be a crude approximation of second order state neurons. Here, we compare back-propagation through time (BPTT) to other online learning algorithms such as real time recurrent learning (RTRL) and unbiased online recurrent optimization (UORO). We describe these procedures in further detail in the next section. Notably, in our experiments, we observed that UORO boosts performance for higher order recurrent networks, while being faster than RTRL, the original algorithm-of-choice when training higher order, state-based models. Furthermore, we remark that truncated BPTT (TBPTT), for some CFGs, can actually slightly improve model performance over BPTT (but in ohers, such as is the case for the palindrome CFG, lead to worse generalization). For all of the RNNs we study, we compared their (validation) performance when using various online and offline based learning algorithms. As mentioned in the last section, we found that UORO worked best for the NSPDA, which is advantageous in that UORO is faster than RTRL (even largely in terms of complexity) and does not require model unfolding like the popular and standard BPTT/TBPTT algorithms do. Below we briefly describe the non-standard approaches to learning RNNs, specifically RTRL and UORO. Notably, we are the first to implement and adapt UORO in calculating the updates to the weights of higher order networks.
In it, we report the performance of 3 modern gated RNNs with multiplicative gating units, i.e., MI-RNN, MI-LSTM, MI-GRU. Interestingly enough, one could consider the multiplicative units to be a crude approximation of second order state neurons. Here, we compare back-propagation through time (BPTT) to other online learning algorithms such as real time recurrent learning (RTRL) and unbiased online recurrent optimization (UORO). We describe these procedures in further detail in the next section. Notably, in our experiments, we observed that UORO boosts performance for higher order recurrent networks, while being faster than RTRL, the original algorithm-of-choice when training higher order, state-based models. Furthermore, we remark that truncated BPTT (TBPTT), for some CFGs, can actually slightly improve model performance over BPTT (but in ohers, such as is the case for the palindrome CFG, lead to worse generalization). For all of the RNNs we study, we compared their (validation) performance when using various online and offline based learning algorithms. As mentioned in the last section, we found that UORO worked best for the NSPDA, which is advantageous in that UORO is faster than RTRL (even largely in terms of complexity) and does not require model unfolding like the popular and standard BPTT/TBPTT algorithms do. Below we briefly describe the non-standard approaches to learning RNNs, specifically RTRL and UORO. Notably, we are the first to implement and adapt UORO in calculating the updates to the weights of higher order networks.
Syntactic Robustness: CP / This empirically shows that NSS is robust on both generic and domain-specific datasets. The perfect CP value is because the NSS template-fitting algorithm did not find any ambiguity in identifying the subject-object dependencies. This can be attributed to the fact that most sentences in the datasets are trivial IS-A type. In comparison to precision we observed a slightly lower recall. This was because of inaccurate POS-tagging of lexemes, which may lead to: (a) inaccurate sentence simplification, and/or (b) invalid template-fitting in the NSS cells.
Results. Our system gets an accuracy of 37.1% on the test data, which is significantly higher than both baselines, while the oracle is 76.6%. The next subsections analyze the system components in more detail.
Logical operation coverage. The dataset covers a wide range of question types and logical operations. The join only subset corresponds to simple table lookup, while join + count is the WQ baseline for Freebase question answering on the WebQuestions dataset. Finally, join + count + superlative roughly corresponds to the coverage of the Geoquery dataset. Effect of features. The most influential features are lexicalized phrase-predicate features, which capture the relationship between phrases and logical operations (e.g., relating “last” to argmax) as well as between phrases and relations (e.g., relating “before” to < or Next, and relating “who” to the relation Name). As an aside, the huge drop in oracle is because fewer “semantically incorrect” logical forms are generated; we discuss this phenomenon in the next subsection.
Based on both correlation coefficients, we achieved a noticeable improvement in the assessment of caption quality on Composite and Flick8K compared to the previous best results produced with existing metrics Anderson et al. Regarding the isolated impact of two similarity measures, we observe that WDS contributes more to caption evaluation than RRS. This finding indicates that the micro-level comparison of grounding similarity distribution is more sensitive to human judges than the macro-level contrast of image region rank by grounding scores.
Our metric achieves higher accuracy in most pair groups except for HM and HC. Given all instances, TIGEr improves the closeness to human judgment by 2.72% compared to the best prior metric (i.e., METEOR). Among the four considered candidate groups, identifying irrelevant human-written captions in HI is relatively easy for all metrics, and TIGEr achieves the highest accuracy (99.80%). In contrast, judging the quality of two correct human-annotated captions in HC is difficult with a lower accuracy per metric compared to other testing groups. For this pair group, TIGEr (56.00%) shows a comparable performance with the best alternative metric (METEOR: 56.70%). More importantly, TIGEr reaches a noteworthy improvement in judging machine-generated caption pairs (MM) with an increasing in terms of accuracy by about 10.00% compared to the best prior metric (CIDEr: 64.50%).
(P) models used in this study. Due to the memory limitations of on-device training, all models use fewer than 3.5M parameters. For each vocabulary size, we first start with a base architecture consisting of one LSTM layer, a 96-dimensional embedding, and 670 hidden state units. We then attempt to increase the representational power of the LSTM cell by increasing the number of hidden units and using multi-layer LSTM cells Sutskever et al. Residual LSTM Kim et al. Lei Ba et al. To avoid the restriction that Nh=Ne in the output, we apply a projection step at the output gate of the LSTM Sak et al. This step reduces the dimension of the LSTM hidden state from Nh to Ne. We also share the embedding matrix between the input embedding and output softmax layer, which reduces the memory requirement by |V|×Ne. We note that other recurrent neural models such as gated recurrent units Chung et al. In general, larger models generate better results than smaller baseline models. For the baseline architectures with same RNN size, having a larger vocabulary leads to some gains. For the larger architectures that have similar total numbers of parameters, 4K word-piece models are shown to be superior to 16K and 30K. For 4K word-piece models, GLSTM is in general on-par with its P4K-L counterpart. The word model is better than all the word-piece models in both languages in SLLe. We were surprised by this result, and hypothesize that it is due to the SLLe metric discounting word-piece models’ ability to model the semantics of OOV words. The solid lines are the best models we pick for A/B experiment evaluation for the virtual keyboard (P4K-L and W30K).
Compared Models: We compare our proposed CMN with several baselines and the state-of-the-art method: (1) The Shortest Path Agent takes the shortest path to the supervision goal at inference time and represents the upper bound navigation performance for an agent. (2) (3) The Vision Only baseline where the agent considers visual input with empty language inputs. (4) The Dialog Only baseline where the agent considers language input with zeroed visual features. (5) Comparison to previous methods: As is shown in Tab. trusted path (Mixed)), demonstrating the ability of our method to grounding visual elements in the explored environments. When evaluated on the Val Unseen and Test Unseen data, the gap between our CMN and the seq-to-seq method is also significant, showing that CMN generalizes well for unexplored environments.
The model is relatively less biased on race than on others (icat score of 69.7). We also show the high and low biased target terms for each domain from the development set. We conjecture that the high biased terms are the ones that have well established stereotypes in society and are also frequent in language. This is the case with mother (attributes: caring, cooking), software developer (attributes: geek, nerd), and Africa (attributes: poor, dark). The low biased are the ones that do not have well established stereotypes, for example, producer and Crimean. The outlier to this observation is Muslim. Although the target term muslim has strong stereotypical associations in StereoSet (e.g., the attribute term terrorist appears in 20% of stereotypes and other hatred terms in 43% examples), the model exhibits idealistic behavior (with a slight preference towards anti-stereotypes). We could not explain this behavior, and leave explainability of such bias to future work.
VI outperforms MAP since it takes into account posterior uncertainty in paragraph embeddings. Fig. As the document length grows, the entropy decreases, which makes intuitive sense since longer reviews can be more specific. After finding the paragraph vectors, we train the classifier by following Kiros et al. , where features are constructed by concatenating the component-wise product xn⋅xn′ and the absolute difference |xn−xn′| between each pair of features xn and The relationship between entropy and document length shown in Fig.
We can observe that the RNNs based on GRU units perform slightly better than the one based on LSTM units.
These results show that adding shortcut connections indeed introduces consistent performance gains with both LSTM and GRU. The temporal traces at different layers seem more consistent (note that for t-SNE, only the topological relations are important). This is particularly evident for GRU, where the third layer now can remember some short-time events as well. This is expected, as the information flow is quicker and easier with the shortcut connections.
There are three single-channel end-to-end speech recognition baseline systems. The second is a single-channel multi-speaker ASR model trained with speech that is enhanced by BeamformIt And the third one is to use BeamformIt to first separate the speech by choosing its best and second-best output streams, and then to recognize them with a normal single-speaker end-to-end ASR model. Using the BeamformIt tool to enhance the spatialized signal can improve the recognition accuracy of a multi-speaker model, leading to a WER of 21.75% on the evaluation set. However, traditional beamforming algorithms such as BeamformIt can not perfectly separate the overlapped speech signals, and the performance of the single-speaker model in terms of WER is very poor, 98.00%.
The first observation is that there is no clear winner. IntCos Original performs comparably to LRCos with slight improvements for GN/BATS: here the classes are widespread and exhibit low cosine similarity (IntraR and IntraL), which makes them harder to solve. IntCos Complement maintains performance for GN/BATS and is beneficial for Derivational analogies on GN. For most other analogies it harms performance.
The baseline for these experiments is a model for each personality factor including all feature sets (LLD, LIWC, DAL, WV, POS) and all genders and L1s, without normalization. In this and all subsequent tables, highlighted results are significant at a two-tailed .95 confidence interval. The baseline surpasses a chance baseline of 0.33 (for a three-way classification of HI vs. MED vs. LO) for only Openness and Neuroticism. The baseline Extraversion model performs worse than chance.
We use a greedy stepwise search (with linear regression) through the feature space to determine the optimal subset of the features which accurately model the SSPA scores for all 109 subjects without considering the group variable. We down-selected to a set of 25 computed features out of the original 73. We notice that several of the coherence statistics for Scene 3 (negotiation with landlord) are particularly influential when tracking the assigned SSPA score with this model. Interestingly, the top three coherence statistics include a bag-of-words average of word2vec vectors (BoW mean scene 3), an InferSent sentence encoding (INF minimum scene 3), and a SIF embedding (SIF 90th percentile scene 3), indicating a variety of embeddings and range of statistics all provide useful information in predicting SSPA performance. We also note that a variety of lexical diversity (MATTR, Brunét’s index), lexical density (\nicefracFUNCW, \nicefracUHW) and syntactic complexity (maximum Yngve depth) measures are among the most influential, confirming the benefit of a complementary set of language measures.
In line with Erk et al. we measure inter-annotator agreement as the average over pairwise Spearman’s ρ correlations (omitting 0-judgments), cf. The bottom line provides the agreement of each annotator’s judgments against the average judgment score across the other annotators. The range of correlation coefficients is between 0.57 and 0.68, with an average correlation of 0.66. All the pairs are highly significantly correlated (p<0.01).
The only task-specific adaptation is the optimization of the threshold on the distance in the multilingual joint space. Our system does not match the performance of the heavily tuned VIC system, but it is on-pair with H2 on en-fr, and outperforms all other approaches by a large margin. We would like to emphasize that our approach uses no additional features or classifiers, and that we apply the same approach to all language pairs. It is nice to see that the performance varies little for the languages.
As with the Common Crawl corpus, we discarded sentences pairs with the wrong language and many commas. By varying the threshold on the distance between two sentences in the embedding space, we can extract various amounts of data. However, the larger the threshold, the more unlikely the sentences are translations. Training on 1M mined sentences gives a modest BLEU score of 4.18, which increases up to 7.77 when 4.3M sentences are extracted. This result is well below an NMT system trained on “real parallel data”. It could be that some of our mined data is actually a subset of Common Crawl.
In our experiments, we use the training data provided by organizers. In addition, we also use data obtained from translated Hindi content available on Internet. We enhanced the training data with additional pairs, but automatically translated. Note that no manual translation was used to create additional data. We obtain 2.5M Hindi sentences automatically translated to English from newspapers and similar resources, obtained from Internet. This data is some what domain specific. They are primarily, from news articles related to national news. English data is kept true-cased, which we found to have better results consistently with our nmt model.
S6SS0SSS0Px2 Benchmarks For a given entity timeline , we include the following three benchmarks – Random (R): 15 articles are sampled from the entire corpus. Random+Linked (RL): 15 articles linked to the entity are sampled. Ordered+Linked (OL): the 15 highest ranked articles for an entity are chosen. Reassuringly, we see that OL outperforms RL which outperforms R for both ROUGE-1 and ROUGE-2 scores This can be taken as a strong benchmark for future timeline generation models trained and evaluated using this dataset.
Let N(w_j) be the set of words contained in the embedded neighbourhood of word w_j (note that this includes w_j itself). Then, for each w_k∈N(w_j), we define: (1) γ(w_k)=f(w_k,d_i)cos(v_j,v_k) where γ(w_k) is our contextually propagated term weight of w_k, f(w_k,d_i) is the frequency of word w_k in document d_i, and cos(v_j,v_k) is the cosine similarity between the word embeddings for word w_j and w_k. Eq. Then, based on the above term weights, we compute the contextually propagated term weights (CPTW) of document d_i, denoted CPTW(d_i), as: (2) CPTW(d_i)=∑_j=1Me_j(α_j∑_w_k∈N(w_j)γ(w_k)) where M is the total number of unique words in the collection (such that the representation resides in RM), e_j is the vector with 1 at index j and zero everywhere else, γ(w_k) is the term weight of w_k computed using Eq. In Eq. This has the benefit that a word with a larger embedded neighbourhood (larger N(w_j)) is not weighted higher than a word with a smaller embedded neighbourhood (smaller N(w_j)). When the number of unique words is considered fixed then a τ-tresholded word-to-word similarity matrix can be computed offline, such that the similarity propagation of CPTW can be trivially done efficiently using sparse vector-matrix multiplications on a traditional bag-of-words representation. Fig.
We use the same data preprocessing as in Cheng et al. We find that both the sent-beam and word-sampling methods outperform the pivot-based approaches in a zero-resource scenario across language pairs. Our word-sampling method improves over the best performing zero-resource pivot-based method (soft) on Spanish-French translation by +3.29 BLEU points and German-French translation by +3.24 BLEU points. In addition, the word-sampling mothod surprisingly obtains improvement over the likelihood method, which leverages a source-target parallel corpus. The significant improvements can be explained by the error propagation problem of pivot-based methods, which propagates translation error of the source-to-pivot translation process to the pivot-to-target translation process.
The Quora duplicate questions dataset is an open domain sentence pair dataset. It has more than 400,000 tagged sentence pairs formatted like “text1 text2 is_duplicate” means whether the two sentences are semantically similar. If they are semantically equal, the tag will be “1”, otherwise “0”.
We measure the quality of the translated pairs by using a random sample set of 100 pairs (from the 1,888 pairs) to be translated by an independent translator for each target language. The sample is proportionally stratified according to the part-of-speech categories. The independent translator is given identical instructions to the main translator; we then measure the percentage of matched translated words between the two translations of the sample set. Overall across all languages, the agreement is 84.8%, which is similar to prior work Camacho-Collados et al.
Round 3: We compute the average agreement for each annotator (with the other annotators), by measuring the average Spearman’s correlation against all other annotators. We discard the scores of annotators that have shown the least average agreement with all other annotators, while we maintain at least ten annotators per language by the end of this round. The actual process is done in multiple iterations: (S1) we measure the average agreement for each annotator with every other annotator (this corresponds to the APIAA measure, see later); (S2) if we still have more than 10 valid annotators and the lowest average score is higher than in the previous iteration, we remove the lowest one, and rerun S1.
We can see from Tab. However the differences between the variants are relatively small. In addition, the CBOW model generally outperforms the SG model. It seems P1(B|B) is relatively low (0.35-0.59). However, it is essentially a harder task to find a black name because a random name from the contact lists has a probability of 0.03 being Black, while 0.74 being White.
As shown in Tab. Embd only use parameters estimated from name embeddings. NamePrismw uses the world population as priors. NamePrism and NamePrismw performs best on most classes for both datasets. On Wikipedia data, our methods achieves best performances on 15 (out of 18) classes. Some classes get +10% F1 boost, including Indian, Nordic and EastAsian. On Email/Twitter data, the improvement is more significant. NamePrism outperforms the rest on all classes. Some classes get performance increase by +30%, including Muslim, Africans, etc. Note that Embd also achieves considerable high performance, indicating that name embedding is capturing nationality signals well.
S3SS1SSS0Px3 Sensitivity of word segmentation. We have observed that humans and the model are sensitive to the different typos (or words). One explanation of the BERT ’s sensitivity is that the subword segmentation is sensitive to the typos. For example, our “max-grad” method modifies the keyword “inspire” to “inspird” in the following sentence: “ a subject like this should inspire reaction in its audience; the pianist does not.” As a result, “inspird” is split into [ins, ##pi, ##rd], which is completely different from the original string. Therefore, one promising direction is to make word segmentation more robust to character-level modifications. To verify this assumption, we trained two other RNN-based classifiers with the widely-used GloVe embeddings (pennington2014glove) and character n-gram embeddings (jmt). Our BERT-based typos also degrade the scores of the RNN-based models, and we can see that the character information makes the model more robust.
The first column reports the base BERT baseline scores, while the second reports the results for the C2Q/Q2C attention addition. The two skip columns report scores for the skip connection connecting the BERT embedding layer to the coattention output (Simple Skip) and the scores for the same skip connection containing a Transformer block (Transformer Skip).
Cn-Omcs-14 Based on Cn-Omcs-Cln we construct our experimental dataset Cn-Omcs-14 – a balanced dataset still large enough for applying neural methods. We include all relations from Cn-Omcs-cln with more than 2000 instances, and downsample to the least frequent class – 2586 instances per relation. To select the \saybest instances for testing and tuning, we sort the relation triples by their confidence score, as provided by ConceptNet. Inspired by Li et al. we select the 10% (258) most confident tuples per relation for testing, the next 10% for development, the remaining 80% (2068) for training, cf. Experiments and Datasets. We experiment with two open world settings: in OW-1 we add only the Random class to Cn-Omcs-14, to investigate whether the classifier is able to differentiate related from non-related concept pairs. in OW-2 we add both Other and Random to Cn-Omcs-14, to investigate whether the classifier can also learn to predict that an unknown relation exists or that no relation holds. We also report results of the closed world setting where we exclude Other and Random. Each dataset is split into training (80%), dev (10%) and test (10%) (cf. Evaluation. We evaluate model performance in terms of F1 score for each relation. We report averaged weighted F1 scores over 5 runs.
As compared to the algorithm of Shen et. the proposed method although not able to get better perplexity scores, it can achieve more accurate sentiment transfer and better content preservation. Our algorithm, on the other hand, is penalized if the content changes, which forces it to sacrifice the perplexity. Achieving better results across all the metrics still remains a challenge.
We first compare CDVAE-GANSP, CDVAE-GANMCC, CDVAE-GANBOTH and CDVAE-CLS-GANSP, CDVAE-CLS-GANMCC, CDVAE-CLS-GANBOTH, respectively. This result suggests that modeling both feature domains simultaneously does not always yield better results. As for perceptual performance, our internal listening tests revealed that CDVAE-GANMCC gave the best results among the three models. Note that although CDVAE-GANSP and CDVAE-CLS-GANSP gave the lowest MCD compared with the other two models, they do not necessarily outperform their MCC counterparts in listening tests. We speculate that fitting the SP domain tends to give more over-smoothed output features, resulting in low MCDs but not beneficial for improving perceptual performance. The result is reasonable since the MCC-MCC path is used when performing conversion. Next, we evaluate the effectiveness of the adversarial speaker classifier. These results imply that CLS can improve objective statistics.
The performance slightly degraded without scheduled sampling, and the performance significantly degraded when we removed entity pretraining or removed both (p<0.05). This is reasonable because the model can only create relation instances when both of the entities are found and, without these enhancements, it may get too late to find some relations. Removing label embeddings did not affect the entity detection performance, but this degraded the recall in relation classification. This indicates that entity label information is helpful in detecting relations.
The various vectorization methods look into the mapping problem from different angles, utilizing a global corpus or a local dataset, and in different dimensionality. (denoted as sgLSTM-GloVe-tfidf-50) works the best among all the 8 vectorization schemes (more details can be found in Sec. The results of the top 3 performers in the previous verification step are included in this table. m-LSTM-long represents the m-LSTM captioning model trained on datal. A direct training over the whole dataset tends to put a preference into high frequency sentences in the training dataset, which may be unrelated to the test image itself. Therefore, when it comes to numerical evaluations, a total miss of the core concept in the image content leads to a low score. On the other hand, by integrating the guiding textual features into the training process, the proposed sg-LSTM model manages to generate accurate descriptions related to the image content, and sometimes, the generated descriptions are more meaningful than the original ones provided by the Flickr users as demonstrated in Fig.
This result differs from the statement made by Lin et al. Hence, we could argue that introducing the notion of moral valence the quality of the proposed lexicon, MoralStrength, improves the performance in text analysis as compared to the MFD. Here, we compare the performance of the original MFD versus our MoralStrength for all the proposed approaches. The reported scores are averaged over all moral values per dataset. As observed, the Friedman test indicates that the SIMON model with the proposed lexicon outperforms the rest. Hence, it is safe to assume that the newly introduced resource offers an improvement over the previous lexicon.
Weak Classifier Label Generation. To obtain the labels, we train 40-layer Wide Residual Networks on CIFAR-10 and CIFAR-100 with clean labels for ten epochs each. Then, we sample from their softmax distributions with a temperature of 5, and fix the resulting labels. This results in noisy labels which we use in place of the labels obtained through the uniform, flip, and hierarchical corruption methods. The labelings produced by the weak classifiers have accuracies of 40% on CIFAR-10 and 7% on CIFAR-100. Despite the presence of highly corrupted labels, we are able to significantly recover performance with the use of a trusted set. Note that unlike the previous corruption methods, weak classifier labels have only one corruption strength. Thus, performance is measured in percent error rather than area under the error curve.
[19]. Results from all methods besides Ren et al. Percent trusted is the trusted fraction multiplied by 100. Unless otherwise indicated, all values are percentages representing the area under the error curve computed at 11 test points. The best mean result is shown in bold.
S5SS0SSS0Px3 Robustness The encoder-decoder attention mechanism in the autoregressive model may cause wrong attention alignments between phoneme and mel-spectrogram, resulting in instability with word repeating and word skipping. It can be seen that Transformer TTS is not robust to these hard cases and gets 34% error rate, while FastSpeech can effectively eliminate word repeating and skipping to improve intelligibility.
Human evaluation In order to assess operational performance, we tested our model using paid subjects recruited via Amazon Mechanical Turk. Each judge was asked to follow a given task and to rate the model’s performance. We assessed the subjective success rate, and the perceived comprehension ability and naturalness of response on a scale of 1 to 5. The full model with attention and weighted decoding was used and the system was tested on a total of 245 dialogues. Moreover, the comprehension ability and naturalness scores both averaged more than 4 out of 5. (See Appendix for some sample dialogues in this trial.)
Similarly, Fig. Here, the confidence is defined by Eq. as the proportion represented by the largest weight among the group of particles classified by the estimated interpretations of a polysemous word. Here, {winterp} is a group of weights whose particles give the same interpretation, and {wans} is a group of weights with the correct interpretation: confidence=max({wans})∑interpmax({winterp}) (7) We can understand the confidence for a correct interpretation as follows. The case of 0% confidence means that all weights for the correct interpretation are zero, or that there is no particle estimating the correct interpretation. In contrast, 100% confidence indicates that all the weights of the other interpretations are zero while only the correct one has a weight, or that all particles estimate the correct answer. The accuracies of those models were all extremely small, at 0.2 or less, because the threshold of accuracy was set to 0.5 to achieve a majority, whereas the averages of the final confidences were below 0.4. Likewise, the accuracy threshold diminished the SCAIN algorithm’s accuracy for the new interpretation task to 0.410, which was lower than 0.841 for the interpretation estimation task.
From the table, we can see that compared with the baselines such as the Seq2Seq or the HRED model, our model (HRED-MTSS) gets the best performance in the multi-domain settings. By adding a teacher-student framework, the informing rate and success rate receive 6.8% and 4.0% improvements respectively over the original HRED model. While compared with the state-of-the-art results achieved by HDSA or LaRL with the TRADE state tracker, HDSA+TRADE slightly outperforms our model in certain but not all metrics. However, BERT not only boosts its performance but also brings bloated model and high latency problems in real scenario deployments. LaRL uses the reinforcement learning method, which aims to maximize the long-term return, i.e., the Inform rate and the Success rate in the dialogue context. LaRL can achieve high scores in one aforementioned metrics but fail in the BLEU score and utterance fluency. Additionally, in the setting of manual states, our model reaches equal or higher results than the Seq2seq and the HRED model. Adding an external state tracker to the Seq2Seq model and the HRED model increases the inform rate but has no help for the dialogue success rate.
When compared with a Seq2Seq and HRED model, our model achieves the best success rate in all domains and outperforms in the attraction domain and train domain under the metrics of inform rate . We believe that it is due to the application of an individual teacher in each domain in the training phrase, which results in a better performance in this domain than the universal one. And compared with the HDSA model with the TRADE state tracker, our model is better in 2 of all 4 domains, the restaurant domain and the train domain.
, we can see the results of using different guiding weights for text-level ( α1) and policy-level (α2). Compared with the model without distillation (α1=0, α2=0), text-level distillation (α1≠0, α2=0) and policy-level distillation (α1=0, α2≠0) can bring improvements respectively. Besides, when applied with both distillation methods together with their weights α1=0.005 and α2=0.005, the model gets the highest performance in both the inform rate and the success. Both the two distillation methods help with the student model.
From there we observe that: (1) Generally, KBE models are doing well in the task of relation prediction. It indicates that relation information between entities can be captured by the existing KBE models without any specific modifications to adapt this task; (2) ComplEx achieves the best performance on all datasets. In addition, it significantly and consistently outperforms DistMult which matches the observations for the task of link prediction; (2) Surprisingly, TransE has competitive performance with ComplEx on some datasets which disagrees with the observations for the task of link prediction. It indicates that there exists intrinsic difference between the task of link prediction and relation prediction.
Structured vs unstructured models: We now analyze the performance of our proposed models; and evaluate the significance of adding structural features to our models. In our experiments, we found the structured models to consistently outperform text-based models. We tune values of hyper-parameters, i.e. number of training epochs for the structured perceptron (10), the weighting parameter for the clustering model (α=0.8), and the number of clusters (K=2) through cross validation on training data. For the structured models, reported results are averages over 10 initializations.
For both data completion techniques, BLEU scores differ by 0.5 or less across runs. Averaging results over the four challenge sets, performance variations between runs with similarly generated contexts remain within 2%, while there is almost a 20% difference between the two context imputation approaches.
With the original bilingual data and additional back-translated data (concat), accuracy reaches 86.1% or better on all four test sets. If missing contexts are instead replaced by random ones, performance drops on average by 3.65%. A model trained with the partial copy heuristic has the best performance on the lexical cohesion test set, at 90.1%, and other comparable performance to the concat model.
In most of the tasks, BERTeng presents a clear advantage when compared to all other models. Even when we look at BERTeng’s results on the Portuguese corpus, which are slightly worse when compared to the English one , we still see a similar pattern. What has surprised us was the excellent results regarding Tasks 1, 2 and 4 when transferring structural knowledge from Chinese to Portuguese. We offer the following explanation for these results. Take the contradiction pair defined in the template language:
Car achieves the best performances in five out of the six cases in the multi-aspect setting. Specifically, for the hotel review, Car achieves the best performance almost in all three aspects. Similarly, Car delivers the best performance for the appearance and palate aspects of the beer review dataset, but fails on the aroma aspect. One possible reason for the failure is that compared to the other aspects, the aroma reviews often have annotated ground truth containing mixed sentiments. Therefore, Car has low recalls of these annotated ground truth even when it successfully selects all the correct class-wise rationales. Also to fulfill the sparsity constraint, sometimes Car has to select irrelevant aspect words with the desired sentiment, which decreases the precision. Please note that the Rnp is not directly comparable to the results in [lei2016rationalizing], because the labels are binarized under our experiment setting.
Car achieves the best performances in five out of the six cases in the multi-aspect setting. Specifically, for the hotel review, Car achieves the best performance almost in all three aspects. Similarly, Car delivers the best performance for the appearance and palate aspects of the beer review dataset, but fails on the aroma aspect. One possible reason for the failure is that compared to the other aspects, the aroma reviews often have annotated ground truth containing mixed sentiments. Therefore, Car has low recalls of these annotated ground truth even when it successfully selects all the correct class-wise rationales. Also to fulfill the sparsity constraint, sometimes Car has to select irrelevant aspect words with the desired sentiment, which decreases the precision. Please note that the Rnp is not directly comparable to the results in [lei2016rationalizing], because the labels are binarized under our experiment setting.
In summary, each system was judged 560 times for each measurement, while natural speech systems (T00 and S00) were judged 280 times. a) XV had better quality but worse similarity than the runners up of VCC2018. It also had the lowest WER; one reason is it trained on LibriTTS a subset of Librispeech. b) Our systems had high scores in both subjective measurements. Interestingly our TTS system has lower WER than our VC systems. c) Even though we had a lower score for quality than did N10, the similarity seem to be higher. d) Our TTS and VC systems had highly consistent results, while there was a gap between the same-gender and cross-gender N10 subsystems. This was perpetuated by extra similarity evaluations between the generated systems presented in Fig. The similarity between our TTSu and VCA= u systems was higher than the similarity between TTSu and N10=.
Here, topic uniqueness refers to the proportion of unique words in a topic, computed over the top words in the vocabulary. Higher the topic uniqueness score, more distinct are the obtained topics. With k = 10, we obtained a topic uniqueness of 32.23, which dropped to 27.12 for k=20 topics. Thus increasing the number of topics increases overlap which harms our model as the topic weight gets divided while training the embeddings. This effect can be clearly seen in the correlation coefficient which drops from 68.5 to 66.9 for 10 & 20 topics respectively. We also observed not very distinct topics at k=5 (i.e. a topic could be mixture of sports and history), resulting in reduced correlation coefficient of 67.1.
In this section, we study the effect of pthres on the model performance. However, we hypothesize that the threshold parameter depends only on the output of topic modeling, particularly p(word|topic), and thus is independent of the this chosen subset, as can be seen in the results on other datasets. A higher threshold value of 1e-3 captures a fewer number of senses. A lower threshold value of 1e-5 allows training of more than the actual number of true senses leading to noisy updates, thus becoming ineffective in capturing any sense.
It is worth mentioning that the REINFORCE baseline did not work for these applications. Exploration from a random policy has little chance of success. We do not report it since we were never able to make it converge within a reasonable amount of time. Using the hybrid XENT-REINFORCE loss without incremental learning is also insufficient to make training take off from random chance.
The translation improvements relate to the overlapping coverage, supporting the IFRS improvement from around 10 BLEU points to around 55 using the generic neural model and 75 BLEU points when the Generic+Wikipedia dataset is used for domain adaptation.
In the second setting, we adapt the probability depending on the cosine similarity between the vocabulary of the ontology and the vocabulary of the Wikipedia abstracts associated with the Wikipedia titles. adapted prob.) shows minor improvements over the non-adapted probabilities. This demonstrates that adding all Wikipedia entries as an external resource does not improve the translation performance. A similar observation was observed in \newcitesrivastava2017improving, where the authors also used Wikipedia entries with a similar outcome. In detail, the performance drops from 6.39 to 5.03 for the ICD ontology labels, with similar results for the IFRS ontology (10.54 vs 10.51). Focusing on the Wikipedia evaluation dataset, the similar vocabulary helps to outperform the generic model without the external knowledge (42.84 vs 12.49). Additionally, we used the development set of each ontology as an external resource and injected the in-domain translation candidates into the translation process of the generic model. Compared to the usage of Wikipedia as the external resource, we observe an increase of 3 BLEU points for the ICD dataset (8.05), and almost 20 for the IFRS dataset (29.69). When using in-domain knowledge as an external resource, the translation quality of the ontology labels improves over the Wikipedia injected knowledge, with almost identical performance between unigram replacement and lexical alignment. Compared to the SMT performance on injecting an external resource, we gained translation improvement for the ICD ontology labels (10.41 vs. 8.05) but observed a significant drop in terms of BLEU for the IFRS ontology labels (14.66 vs. 29.69).
We implemented three variants of the architecture: only lexical features are used, only GloVe embeddings are used and finally where both of them are used. As is evident and hypothesized, the model trained with lexical and word embeddings gave the best results. It is interesting to see that using just the lexicons, the model beat the previous best model which signifies the importance of lexical features. However, using only lexicons as features will suffer from all the disadvantages of any bag-of-words model. This is primarily because it cannot encode the temporal nature of language. This is where gated recurrent units or in general recurrent neural networks come into play. GRU’s can successfully encode the temporal nature.
The results highlight some interesting insights. Since we are dealing with discussion posts, the average post size is larger than a few sentences. Hence, as we increase the maximum sequence length, the increase in performance is justified. However, once we increase the size after a certain threshold, a lot of the input sequences need to be padded with zeroes, thus causing a drop in performance.
The best class-based LM is reported, but is not competitive with the KN baselines. PLRE outperforms all of the baselines comfortably. Moreover, PLRE’s performance over the baselines is highlighted in Russian. With larger vocabulary sizes, the low rank approach is more effective as it can capture linguistic similarities between rare and common words.
We observe that our proposed idea achieves higher performance on all tasks, except task 2. The accuracy increases considerably on tasks 4 and 5, where task 5 represents the combination of all 4 tasks. In addition to a smaller set of unique user utterances, the natural language used for user utterances is simpler and has less variations (uses a small set of templates) compared to the DSTC6 dataset. We believe that the simplicity of bAbI dialog dataset allows the original memN2N model to perform better.
We observe that our model performs similar to the original memN2N architecture for tasks 1, 3 and 4. We observe reduction in accuracy for tasks 2 and 5 with respect to memN2N. Bordes and Weston propose match-type features for handling entities. For Match-type features, entity-type tokens ( e.g. ADDRESS, PHONE etc.) are added to a candidate if an entity is present both in the candidate and the story for a given dialog. For example, for a task 4 dialog with restaurant information about RES_ABC, only one candidate ”here it is RES_ABC_address” will be modified to ”here it is RES_ABC_address ADDRESS”. If the query (last user utterance) is for the restaurant’s address, then using match-type features reduces the output search space and allows the model to attend to specific candidates better. With match-type features, the accuracy for the model increases for task 4 but decreases for tasks 2 and 5 (conducting full dialogs). Hence, match-type features can only work in a retrieval setting and will not work in a generative setting where the next system utterance is generated word-by-word. Our KB-memN2N model will work in both retrieval and generative settings.
From these results we perform two analysis. First, the effect of training datasets on model’s performance and second, the performance comparison among the two manually annotated datasets.
Our work directly addressed this issue. In the FgER task performance measure, learning model trained on WikiFbF has an average absolute performance improvement of at least 18% on all of the there evaluation metrics.
We also translated the monolingual corpora again using these 6-epochs initialized models and performed back-translation. From a practical standpoint, translation takes a lot of time, and back-translation increases the training time proportionally to the size of data—8 times longer in our experiments.
The judges strongly prefer our results (without keywords) to the baseline in all aspects: coherence, fluency, and informativeness. They also strongly prefer the ground truth to our results. Moreover, our results with keywords and context are compared with three other systems: (i) the ground truth; (ii) our results with keywords but not context; (iii) our results with context but not keywords. The second comparison shows that in the presence of keywords, our model can use context to improve all aspects of the generation. The third comparison shows that the presence of keywords reduces the performance of our model, probably because keywords are constraints that the model must take care of.
In such cases, the approximate posterior “collapses” to the prior, and where one has a fixed prior, such as our standard Gaussian , this means that the posterior becomes independent of the data, which is obviously not desirable. Bowman et al. After a number of annealing steps, the KL term is incorporated in full and training continues with the actual ELBO. In our search we considered annealing for 20,000 to 80,000 training steps. Word dropout consists in randomly masking words in observed target prefixes at a given rate. The idea is to harm the potential of the decoder to capitalise on correlations internal to the structure of the observation in the hope that it will rely more on the latent representation instead. We considered rates from 20% to 40% in increments of 10%. To spare resources, we reuse these hyperparameters for De-En experiments.
The improvement in BLEU-n is greater for greater n; TPGN particularly improves generation of longer subsequences. The results attest to the effectiveness of the TPGN architecture.
We can conclude that the size of the character embeddings should be proportional to the total embedding size: the word-level embedding should be larger than the concatenation of the character-level embeddings. Adding characters in the backward order is slightly better than adding them in the forward order, and the largest improvement is made for the large LSTM LM.
The best small model has a perplexity of 75.04 which is 2.27% compared to the baseline and the best large model has a perplexity of 67.64, a relative improvement of 4.57%. The larger improvement for Dutch might be due to the fact that it has a richer morphology and/or the fact that we can use the surface form of the OOV words for the Dutch data set
All these distributions are skewed right (row ‘skewness’) which could be expected, as it is duration data. SL-types, i. e. words, are less skewed than L-types, and amongst these latter ones, those containing unvoiced parts are skewed most. This is due to a few outliers, i. e. very long laughter instances: the median is more uniform across the types than the mean (and by that, standard deviation and maximum values). In a Mann-Whitney test, the durations of SLs vs. SLw differ with (p=0.024). This can of course be due to differences in word length but most likely, to SLs being more pronounced and by that, longer, than SLw. Three pair-wise Mann-Whitney tests resulted in one of the differences, namely Lvu vs. Lu, being significant with (p=0.001). This might be due to two factors: Lu tends to be weaker and by that, shorter, and for Lvu, the alternation of voiced and unvoiced might automatically ‘result’ in some longer duration.
The multi-speaker test scenario is not new. It first appeared in NIST SRE’99 For the case of SRE’18 VAST partition, there may be several speakers in a test segment. One straightforward solution is to score the entire test segment regardless of other competing speakers. Alternatively, one could use a diarization system to obtain several speaker clusters, score the enrollment segment against all the speaker clusters and select the maximum score. Among the twelve sub-systems, eight of them employed x-vector embedding in some form. The remaining three sub-systems use i-vector. Comparing the results, x-vector gives a much better performance than i-vector on both CMN2 and VAST. The Kaldi PLDA domain adaptation was the most commonly used strategy. The CORAL+ was also successfully employed resulting in the lowest EER and Cprim. Clustering unlabeled set to obtain pseudo-speaker labels was tried in Sys. 3, though no significant difference between clustering and Kaldi adaptation strategy is observed. In terms of the performance on the VAST partition, we observe only slight benefit in using speaker diarization (Sys. 6 and 7) suggesting a good potential for further improvement.
We see that the use of special word-initial characters improves performance over the use of explicit blanks. Redundantly modeling word-final characters does not provide a further improvement. For ease of interpretability, all models use explicit double-character symbols.
Note that since we use a bidirectional network, the total number of activations in a layer is double this. We find that relatively deep networks perform well. Based on these results, the remainder of the experiments use a 1024 width 9 layer bidirectional network. Including weight matrices and biases, the total number of parameters in the 9 layer 1024 wise network is about 53 million parameters.
Clearly, the RNN is not yet learning all the logic of a beam-search decoder, and the effectiveness of a character-based beam search is midway between using the raw output, and a full word-based search. We see that iterated CTC can produce a significant improvement, though not as much as a complete beam search, while remaining in the all-neural framework. When character or word N-grams are used, the utility decreases. An examination of the iterated CTC errors indicates that it mostly reduces the substitution rates, as the global shifts created by insertions and deletions seem difficult for the RNN to compensate.
We vary the L1 languages and maintain a common L2 (instead of the other way around) in order to have a common basis for comparison: all of the models are tested on the same L2 test set, and therefore we can compare the perplexity scores. We run n=5 trials of every experiment with different random seeds. Any high-resource human language would have provided a good common L2, and Spanish works well for our human languages experiments due to the fact that many higher-resource languages fall on a smooth gradation of typological distance from it For our typological data, we use the World Atlas of Linguistic Structure, using the features that relate to syntax (WALS-syntax features). Examples of syntactic features in WALS include questions such as does a language have Subject-Verb-Object order, or does a degree word (like “very”) come before or after the adjective. We accessed the WALS data using the lang2vec package Littell et al. The quantity we are interested in extracting from the WALS data is the typological distance between the L2 (Spanish) and all of the L1 languages mentioned above. Not every feature is reported for every language, so we calculate the WALS distance by taking into account only the 49 features that are reported for all our chosen languages and count the number of entries that are different Since they are only based on 49 features, these distances do not provide a perfectly accurate distance metric. Though we cannot use it for fine-grained analysis, correlation with this distance metric would imply correlation with syntactic distance.
Experimental Settings We use 23,217 unlabeled clinical records to train the word embeddings (word2vec) at 128 dimensions using skip-gram model Mikolov et al. The hidden state size is set to be 200 for word-level Bi-LSTM. For each task, we take the whole source domain training set Ds and 10% sentences of the target domain training set Dt as training data. We use the development set in target domain to search hyper-parameters including training epochs. We then take the models to make the prediction in target domain test set and use F1-score as the evaluation metric. lift over state-of-the-art baseline methods. While other three baseline methods all share the whole model between source/target domains but differ in the training schemes and performance. To better understand the transferability of La-DTL, we also evaluate three variants of La-DTL: La-MMD, CRF-L2, and MMD-CRF-L2. La-MMD and CRF-L2 have the same networks and loss function as La-DTL but with different building blocks: La-MMD has β=0, while CRF-L2 has α=0. In MMD-CRF-L2, we replace La-MMD loss LLa-MMD in La-DTL with a vanilla MMD loss: LMMD= MMD2(Rs,Rt) , where Rs and Rt are sets of hidden representation from source and target domain. Results in in 7 of 12 tasks. (ii) CRF-L2 is also a promising method when transferring between NER tasks, and it improves the La-MMD method significantly when these two methods are combined to form La-DTL. (iii) Label-aware characteristic is important in sequence labeling problems because there is an obvious performance drop when La-MMD is replaced with a vanilla MMD in La-DTL. But MMD-CRF-L2 still has very competitive performance compared to all the baseline methods.
We find that our DPP model improves over the baselines. The results support two claims: (i) dispersion plays an important role in the structure of vowel systems and (ii) learning a non-linear transformation of a Gaussian improves our ability to model sets of formant-pairs. Also, we observe that as we increase the number of phones, the role of the DPP becomes more important. author=jason,color=green!40,size=,fancyline, caption=,color=yellowtodo: author=jason,color=green!40,size=,fancyline, caption=,color=yellowupdate the discussion below after the new experiments
Mean Opinion Score (MOS) of the naturalness of generated speech utterances are rated by human subjects participated in the listening tests. Two independent evaluations were performed using the models trained on male and female speakers, respectively. We use 40 unseen sentences for evaluating the models trained from the male speaker, and 20 relatively longer out-of-domain sentences for evaluating the models trained from the female speaker. In all the experiments, 20 native Mandarin speakers participated in the listening test. We compared our model with the traditional BLSTM-based parametric system [BLSTMWORLD] and the Tacotron-2 system. As shown in Table. In both tests DurIAN and Tacotron-2 perform on-par with each other. No statistically significant difference can be observed. These results tell us that the superior naturalness in Tacotron-2 is likely a result of all other components in Tacotron other than the end-to-end attention mechanism.
We also measured the Real Time Factor (RTF) for All the RTF values were measured on a single Intel Xeon CPU E5-2680 v4 core. The results show that with quantization and avx2 speedup, the RTF can be reduced from 1.337 to 0.387 for the baseline WaveRNN model. With the 4-band model, the RTF can be further reduced to 0.171, which is 2x times faster than quantized WaveRNN model.
5.2.2 Quality The Mean Opinion Scores (MOSs) of proposed multi-band WaveRNN were obtained through subjective listening tests. The female dataset used in Sec. Three WaveRNN systems, the baseline WaveRNN model without quantization and the 4-band WaveRNN model with and without quantization, were compared. Experimental results in Table. No statistically significant difference was observed. If fact, most of the subjects participated in the listening tests cannot feel any difference between utterances generated from these three different WaveRNN systems. We can conclude that the proposed multi-band synthesis approach and the 8-bit quantization technique can effectively reduce the computational cost without deteriorating the quality of the generated speech.
We achieve a new state-of-the-art F1 score of 91.79 with our best model. Interestingly, we observe that our parsers have a noticeably higher gap between precision and recall than do other top parsers, perhaps in part owing to the structured label loss which penalizes mismatching nonterminals more heavily than it does a nonterminal and empty label mismatch. In addition, there is little difference between the best top-down model and the best chart model, indicating that global normalization is not required to achieve strong results. Processing one sentence at a time on a c4.4xlarge Amazon EC2 instance, our best chart and top-down parsers operate at speeds of 20.3 sentences per second and 75.5 sentences per second, respectively, as measured on the test set.
This shows the latency (per input token) and peak activation memory consumption during a training iteration on Enwik8 for a range of long-range memory layers. We see the reduction of long-range memories from 24 layers to 4 layers cuts the activation peak memory by 3X. Thus it can be a worthwhile and simple performance improvement.
The BM25 scoring method achieves a higher coverage than the word embedding-based solution and has therefore a slightly worse retrieval performance on the high, medium and low confidence thresholds. We decide to prioritize word embedding-based results over the BM25 scoring when the similarity - thus also the confidence - is high.
For the combined result on the training dataset (H18-H29), we achieve a precision of 29.3%, a recall of 63.59% and an F2-measure of 48.2%. Preliminary experiments have shown that our approach outperforms simple TF-IDF scoring in Elasticsearch on the training data, as well.
The baseline which always predicts a negative entailment achieved 52.04% accuracy. Since our model works with neural attention, we can visualize the query-document interaction in our model for selected cases. For instance, in task H30-23-A, our model predicted the positive class, although the correct answer is a negative entailment.
We see that EMR models which consider the relative importance between the memory entries ( EMR-biGRU and EMR-Transformer) outperform both the rule-based baselines and EMR-Independent. One interesting observation is that LIFO performs quite well unlike the other rule-based scheduling policies, but this is due to the dataset bias We see that EMR remembered the sentences that contain key words that is required to answer the future question.
Note that the baseline approaches that we implemented are pipeline-based, and thus they are very likely to propagate errors to downstream components. However, our model merges the two different tasks of EL and RC into one during the decoding, which composes a major advantage over pipeline-based approaches that usually apply separate models on EL and RC. The most common errors are caused by Out of vocabulary and Noise from overlapping relations in text. As we do not cover all rare entity names or consider multiple triple situations, these errors are valid in some sense.
We show, the number of correct OpenBook knowledge extracted for all of the four answer options using the three approaches TF-IDF, BERT model trained on STS-B data and BERT model Trained on OpenBook data. Apart from that, we also show the count of the number of facts present precisely across the correct answer options. It can be seen that the Precision@N for the BERT model trained on OpenBook data is better than the other models as N increases. The above example presents the facts retrieved from BERT model trained on OpenBook which are more relevant than the facts retrieved from BERT model trained on STS-B. Both the models were able to find the most relevant fact, but the other facts for STS-B model introduce more distractors and have lesser relevance. The best performance of the BERT QA model can be seen to be 66.2% using only OpenBook facts.
We report both the area under the precision-recall curve (AUPRC) and the precision at 0.5 recall. Remarkably, without using any manually labeled data, EZLearn outperformed the state-of-the-art supervised method by a wide margin, improving AUPRC by an absolute 27 points over URSA, and over 30 points in precision at 0.5 recall. Compared to co-EM, EZLearn improves AUPRC by 18 points and precision at 0.5 recall by 25 points.
Compared to direct supervision, organic supervision is inherently noisy. Standard (always choosing distant supervision when available) significantly trailed the alternative approach that always picks classifier’s prediction (Predict). Union predicted more classes than Intersect but suffered large precision loss. By taking into account of hierarchical relations in the class ontology, Relation substantially outperformed all other methods in accuracy, while also covering a large number of classes.
Different language metrics like Bleu_n (n=1,2,3,4), METEOR, ROUGE_L, CIDEr-D and SPICE are provided as these are standardized in the community and are used for performance evaluation and comparison of our model. However, each reflect very limited perspective of the generated captions and The main functional characteristics of our work is the tensor product based positional aware representation, which embed the structural characteristics of objects and interactions in a graph. Generated/trained structural features undergo approximation and then compose new representation through different circumstances of weights, where the weights never been able to represent the whole dataset with different characteristics. Most of our new architectures performed very well and either outperformed or at least same with the existing architectures, which do not have concrete reasoning behind their working principles. With the introduction of semantics, our TPsgtR-sTDBU architecture performed much better than the existing works and performed better than the TPsgtR-TDBU architecture, establishing the fact that they generate better strategy for caption generation. However, the TPsgt features and the Scene-Graph feature generator was trained with images which are not correlated to MSCOCO dataset and the accuracy of detection is around 28%. Though statistical metrics provide many qualitative insights of the generated languages, they hardly reflected any language attribute quality related to meaning, grammar, correct part-of-speech etc.
The first experiment we conducted used only the training data for developing the ADI system. Thus, the interpolated i-vector dialect model cannot be used for this experimental condition. LDA reduces the 400-dimension i-vector to 4, while the Siamese network reduces it from 400 to 200. Since the Siamese network used a cosine distance for the loss function, the Siam i-vector showed better performance with the CDS scoring method, while others achieved better performance with an SVM. The best system using Siam i-vector showed overall 10% better performance accuracy, as compared to the baseline.
Comparing "baseline" with "pretrain text" and "pretrain textNLU", we see that pre-trained models already achieve very competitive results without any fine-tuning on the QR set. " pretrain textNLU" has a further 2% gain in p@1 and 1% gain in p@5 over "pretrain text". We then fine-tune the pre-trained models on 20% of the QR set for just 2 epochs, and the result out-performs the baseline, which is fully trained on the entire QR set, by around 7% or around 5% with or without NLU hypotheses in pre-training.
Answer selection is evaluated by two metrics, mean average precision (MAP) and mean reciprocal rank (MRR). Clearly, the questions in WikiQA are the most challenging, and adding more training data from the other corpora hurts accuracy due to the uniqueness of query-based questions in this corpus. The best model is achieved by training on W+S+Q for SelQA; adding InfoboxQA hurts accuracy for SelQA although it gives a marginal gain for SQuAD. Just like WikiQA, InfoboxQA performs the best when it is trained on only itself. From our analysis, we suggest that to use models trained on WikiQA and InfoboxQA for short query-like questions, whereas to use ones trained on SelQA and SQuAD for long natural questions. The results on WikiQA are pretty low as expected from the poor accuracy on the answer retrieval task. Training on SelQA gives the best models for both WikiQA and SelQA. Training on SQuAD gives the best model for SQuAD although the model trained on SelQA is comparable. Since the answer triggering datasets are about 5 times larger than the answer selection datasets, it is computationally too expensive to combine all data for training. We plan to find a strong machine to perform this experiment in near future.
It seems that, as intended, users rated explanations based on quality rather than model correctness, as we observe no significant difference in ratings grouped by model correctness (table in Appendix).
The results show that each sequence has 4.9 words on average, and the sequences are with various length.
There is a significant improvement in the DAP model’s performance after regularization. Further analysis of the likelihood computation reveals that the regularization term contributes a relatively small drop in likelihood compared to the total likelihood during training. Nevertheless, these results show that even a small amount of regularization can nudge the model to seek out quality results. In testing additional ρ values we found that, in general, ρ∈[0.1,0.3] faired comparably. Larger values of ρ can cause model instability and the document likelihoods to have long-tailed distributions. The emergence of outlier document-likelihoods is unsurprising, regularization encourages the personas to focus on different topics — hence, large values of ρ inevitably result in personas that overfit.
We compare the performance of our base model, which uses word embeddings and a character LSTM, with otherwise identical parsers that use other combinations of lexical representations. First, we remove the character-level representations from our model, leaving only the word embeddings. We find that development performance drops from 92.22 F1 to 91.44 F1, showing that word embeddings alone do not capture sufficient information for state-of-the-art performance. Then, we replace the character-level representations with embeddings of part-of-speech tags predicted by the Stanford tagger This model achieves a comparable development F1 score of 92.09, but unlike our base model relies on outputs from an external system. Next, we train a model which includes all three lexical representations: word embeddings, character LSTM representations, and part-of-speech tag embeddings. We find that development performance is nearly identical to the base model at 92.24 F1, suggesting that character representations and predicted part-of-speech tags provide much of the same information. Finally, we remove word embeddings and rely completely on character-level embeddings. After retuning the character LSTM size, we find that a slightly larger character LSTM can make up for the loss in word-level embeddings, giving a development F1 of 92.24.
We evaluate on the development set 4 times per epoch, selecting the model with the highest overall development performance as our final model. When performing a word embedding lookup during training, we randomly replace words by the   token with probability 1/(1+freq(w)), where freq(w) is the frequency of a word w in the training set. We apply dropout with probability 0.4 before and inside each layer of each LSTM.
DeepSpeech-finetune-prototypical+metric clearly beats the baselines in terms of both precision and recall. Honk is a respectable baseline and gets second best results after DeepSpeech-finetune-prototypical+metric, however, attempts to better Honk’s performance using prototype loss and metric loss did not work at all.
\color It can be seen that our agent architectures clearly outperform all previously published results using generative architectures in MRR, Mean Rank and R@10. This indicates that our approach produces consistently good answers (as measured by MRR, Mean Rank and R@10). But it is important to note that the point here is not to demonstrate the superiority of our architecture compared to other architectures. The point here is instead to show that the MADF framework is able to recover the language quality of the supervised agent. Notice that SL has the best scores, which drops drastically in RL-1Q,1A. But the agents trained by MADF \colorrecover the scores obtained by SL. This shows that the agents trained by MADF are able to recover the language quality of SL agents without sacrificing much on the task performance (image retrieval percentile). Fig. The percentile score decreases monotonically for SL, but is stable for all versions using RL. \colorThe decrease in image retrieval score over dialog rounds for SL is because the test set questions and answers are not perfectly in-distribution (compared to the training set), and the SL system can’t adapt to these samples as well as the systems trained with RL. As the dialog rounds increase, the out-of-distribution nature of dialog exchange increases, hence leading to a decrease in SL scores. Interestingly, despite having strictly more information in later rounds, the scores of RL agents do not increase - which we think is because of the nature of recurrent networks to forget. The results in Fig. We further support this claim in the next section where we show that human evaluators rank the language quality of MADF agents to be much better than the agents trained via reinforcement without community regularization.
We also compare our approach with a distributional semantics method, based on word2vec which represents words as vectors in a linear structure that allows analogical reasoning. We average the vector representations of input words, and search word vectors most similar to the resulting vector (cosine similarity). Example runs of the RD, using the 3k Fusion mBLM, are presented in Fig. The distributions of ranks, for the various BLMs/mBLMs (whichever has greater % of ranks under 100 for each case), word2vec, and Onelook, are stated in Table. Onelook did not provide any outputs for 18 phrases out of the 179 user-generated phrases, and 72 out of the 179 definitions from the Macmillan dictionary. Instead of considering these as failures, we factor out these phrases while evaluating Onelook. The performance of all approaches is significantly better than chance, as seen through the comparison of performance with ‘Chance’ which represents the expected values of performance for random rank assignments to the target words (considering the 3k lexicon). The cases of interest are highlighted in the table. Recent distributional approaches use vector representations for word meaning, derived through similarities gauged by the occurrences and co-occurrences of words in a large corpus The performance of one of these approaches, known as word2vec The performance suggests that phrasal semantics doesn’t necessarily follow a linear additive structure. Indeed, researchers have been trying to find other mathematical structures and approaches which would be suitable for phrasal semantics [ with partial success and on specific types of phrases.
In this section, we discuss how we evaluated the different emoji embedding models using EmoSim508 as a gold standard dataset. We generated nine ranked lists of emoji pairs based on emoji similarity scores, one ranked list representing the EmoSim508 emoji similarity and eight ranked lists representing each emoji embedding model obtained under different corpus settings. Treating EmoSim508’s (Spearman’s ρ) to evaluate how well the emoji similarity rankings generated by our emoji embedding models align with the emoji similarity rankings of the gold standard dataset. We used Spearman’s ρ because we noticed that our emoji annotation distribution does not follow a normal distribution. Based on the rank correlation results, we notice that emoji embedding models learned over emoji descriptions moderately correlate (40.0
To represent a training instance in our sentiment analysis dataset, we replaced every word in a tweet using the different embedding models learned for that word by using different text corpora. We also replaced every emoji in the tweet with its representation from a particular emoji embedding model we learned. Here, Google News + (Sense_Desc.) means that all words in the tweets in the gold standard dataset are replaced by their corresponding word embedding models learned by the Google News corpus and all emoji are replaced by their corresponding emoji embeddings obtained by the (Sense_Desc.) model. We report classification accuracies for: (i) the whole testing dataset (N = 12,920); (ii) all tweets with emoji (N = 2,295); (iii) 90% of the most frequently used emoji in the test set (N = 2,186); and (iv) 10% of the least frequently used emoji in the test set (N = 308). We trained a Random Forrest (RF) classifier and a Support Vector Machine (SVM) classifier using each test data segment. We selected those two classifier models as they are commonly used for text classification problems, including the sentiment analysis experiment conducted by Eisner et al. on the same gold standard dataset. Following Eisner et al. , we also report the accuracy of the sentiment analysis task, which allows us to compare our embedding models with theirs. Accuracy is measured in settings where the testing dataset is divided into four groups based on the availability of tweets with emoji in each group. This embedding model also yielded the best similarity ranking as per Spearman’s Rank Correlation test.
As seen, on words with frequency ≤50, the cnnchar model performs best across all of the three languages. Its superiority is particularly interesting for the OOV words (i.e. the frequency band [0,4]) where the model has cooked up the representations completely based on the characters. For high frequency words (> 50), the bilstmword outperforms the other models.
For word inputs, we use an embedding lookup of 800000 words, each with dimension 1024. For character inputs, we use an embedding lookup of 256 characters, each with dimension 16. We concatenate all characters in each word into a tensor of shape (word length, 16) and add to its two ends the   and   tokens. The resulting concatenation is zero-padded to produce a fixed size tensor of shape (50, 16). The output of all CNNs are then concatenated and processed by two other fully-connected layers with highway connection that persist the input dimensionality. The resulting tensor is projected down to a 1024-feature vector. For both word input and character input, we perform dropout on the tensors that go into LSTM layers with probability 0.25.
We observe that EFCC improves FCC clustering results in WebKB in all cases while AddFCC does not, while AddFCC outperforms the other approaches for Banksearch in all cases. Nevertheless, EFCC also achieves good results in Banksearch, particularly with small feature sets. AddFCC has the problem of considering all criteria equally important, and hence overestimating position in the combination, as we observed with FCC too.
tableF1 results for anchor text experiments (all with MFT reduction). Fig. 6: Each approach has a letter and a number appended, referring to the way in which anchor texts are exploited, as described above. The first three rows of the table show that EFCC outperforms FCC and AddFCC in all cases. This corroborates the limitations of FCC, and reinforces our motivation looking into an alternative approach where not all the criteria contribute equally to the combination. When it comes to the contribution of anchor texts, no approach improves EFCC clearly in all the cases, as the slight differences suggest when looking at the averages. Anchor texts do have a positive impact when we use vectors of small size, particularly when the terms in the anchor texts are considered as page titles (b alternative). However, as we increase the size of the vectors, anchor texts are not useful any more, leading to worse performance. Regarding the use of anchor texts as titles, the best option is to just add anchor texts as title terms (named b-1). The slight improvement achieved with anchor texts might not always pay off, given that the collection of anchor texts is a time consuming process.
On the one hand, looking at the results, among the fuzzy logic based representations, AFCC outperforms the rest in WebKB in all cases, while in Banksearch got better results than the others with 2 out of 5 vector sizes, having also a higher average F1 score. This varying performance across collections could be due to the fact that frequency distributions in Banksearch rather approximate a power law. In those cases, the least frequent terms are assigned to the low fuzzy set, with few terms remaining for the medium and high sets. This explains the small difference between the EFCC and FCC fixed sets. The same occurs with SODP, where EFCC and AFCC get similar results, although FCC performs worse, probably due to its underestimation of frequency. However, with a rather uniform term frequency distribution, as in WebKB, adjusting the fuzzy sets has a much bigger effect in results, where more terms are assigned to the medium and high fuzzy sets, and small variations of the basic parameters of the membership functions will have a much bigger effect. It is indeed important to adapt to this kind of distributions, as the terms are differently used and structured.
Clustering quality, unlike the precision metrics, is designed to evaluate the entire ordering without limiting the analysis to just the top-k and bottom-k entries. As in the earlier case, DTIM convincingly outperforms the baselines by healthy margins across all values of n. Consequent to the trends across n as observed earlier, n∈{3,4} are seen to deliver better accuracies, with such gains tapering off beyond n=4 due to sparsity effects. The words, along with the DTIM nativeness scores for n=3, can be viewed at https://goo.gl/OmhlB3 (the list excludes words with fewer than 3 characters).
Further Discussion on Attention To further demonstrate the power of attention-based methods, we implement three combination strategies to jointly consider multiple image instances. IKRL (ATT) represents the basic model with attention when constructing the aggregated image-based representations, while IKRL (MAX) represents the combination strategy that only considers the image instance which has the largest attention, and IKRL (AVG) represents the strategy that takes the average embeddings of all image instances to represent an entity. no matter what the combination strategy is. It confirms the improvements introduced by images, for the visual information has been successfully encoded into knowledge representations. (2) The IKRL (ATT) model achieves the best performance among all three combination strategies, which implies that the attention-based method is capable of automatically selecting more informative image instances to represent entities. (3) The IKRL (AVG) model performs better than the IKRL (MAX) model, which indicates that only considering images with the largest attention will lose important information located in other instances. (4) It seems that the IKRL (ATT) model only has slight advantages over the IKRL (AVG) model. The reason is that the qualities of images we extract from ImageNet are very high, which may narrow the gap between attention-based and average-based methods. For further analysis, we will give some examples with attention in case study, which could successfully distinguish the relatively better and worse images from all candidates.
From the results we can observe that: (1) all IKRL models outperform all baselines on both evaluation metrics of Mean Rank and Hits@10, among which IKRL (UNION) achieves the best performance. It indicates that the visual information of images has been successfully encoded into entity representations, which is of great significance when constructing knowledge representations. (2) Both IKRL (SBR) and IKRL (IBR) have better performances compared to the baselines, which indicates that visual information could not only instruct the construction of image-based representations, but also improve the performances of structure-based representations. (3) The IKRL models significantly and consistently outperform baselines on Mean Rank. It is because that Mean Rank depends on the overall quality of knowledge representations, and thus is sensitive to the wrong-predicted results. Previous translation-based methods like TransE only consider the structured information in triples, which may fail to predict the relationships if the corresponding information is missing. However, the image information utilized in IKRL can provide supplementary information. Therefore, the results of IKRL are much better than baselines on Mean Rank.
Note that IKRL is based on the framework of TransE but still performs better even when compared with the enhanced TransR model, which confirms the improvements introduced by images. (2) The IKRL (ATT) model achieves the best performance compared to other combination strategies. It indicates that the attention-based method can jointly take multiple instances into consideration and smartly choose more informative images from all candidates.
Next, we present our experiments over our four different advice protocols, each with decreasing human effort and overall performance. (source prediction trained as classification). We hypothesize that using the advice mechanism over this more complex architecture would lead to further improvements, and leave it for future work. At test time, the advice is provided only whenever the predictions fall in the wrong general region, just like a human would. We note that the performance did not improve much when advice was always provided, showing that this model was able to perform well in its absence and does not rely on it (due to our choice not to provide advice all the time in training). In fact a human would only have to provide restrictive advice for 395/720 examples, and the model always follows it. We now aim to avoid any human interaction, by letting the model completely self-generate the advice. Accomplishing it would allow us to improve the model’s performance without additional human effort. We experimented with two approaches. Our advice sentences are generated by filling in appropriate regions/directions into varying sentences. For example, given an advice sentence placeholder, The target is in the , and a coordinate in the lower left, we would generate the restrictive advice sentence: The target is in the lower left. At test time, we use variations of this sentence such as: The block’s region is the lower left, to avoid memorization.
’s kappa between these two sets of ratings. We find that all Cohen’s kappa values are greater than 0, showing rater agreement. Moreover, the Cohen’s kappa values correlate highly with the interrater correlation values (Pearson r=0.85, p<0.001), providing corroborative evidence for the significant degree of interrater agreement for each emotion.
For better understanding, we translate both the contexts and the candidate responses into English from Chinese. The label 1 indicates the true response to the given context. Scores represent the confidence of responses as the best one. From the table it can be seen that r-LSTM gives highest score to best response, while “LSTM+kb” offers unsatisfied results. In our r-LSTM architecture, background knowledge is considered as global memory and can be recalled in the conversation modeling process, and our Recall Gate could also build semantic relationships of utterances validly. Taking knowledge as input directly, LSTM+kb makes less use of knowledge in the modeling process because of the semantic gap between sentences and knowledge items. Furthermore, based on the observation of human conversations, it is reasonable to consider background knowledge as global signal of the neural network, instead of an additional input as LSTM+kb does. The large range of confidence scores given by r-LSTM also shows that our model does well on both selecting best responses and recognizing inappropriate responses.
Here, CNNs have lower performance than TK models as 2,669 pairs are not enough to train their parameters, and the text is also noisy, i.e., there are a lot of spelling errors. Despite this problem, the results show that CNNs can approximate the TK models well, when using a large set of automatic data. For example, the CNN trained on 93k automatically annotated examples and then fine tuned exhibits 0.4% accuracy improvement on the dev. set and almost 3% on the test set over TK models. On the other hand, using too much automatically labeled data may hurt the performance on the test set. This may be due to the fact the quality of information contained in the gold labeled data deteriorates. In other words, using the right amount of weekly-supervision is an important hyper-parameter that needs to be carefully chosen.
To compare directly to their approach, we use their word2vec embeddings along with contexts from the Wikipedia corpus to construct context vectors uw for all words w apart from the 300 nonces. An embedding for each nonce can then be constructed by multiplying A by the sum over all word embeddings in the nonce’s definition. We use the same approach as in the nonce task, except that the chimera embedding is the result of summing over multiple sentences. Since the à la carte algorithm explicitly trains the transform to match the true word embedding rather than human similarity measures, it is perhaps not surprising that our approach is much more dominant on the definitional nonce task.
The method improves further when combined with the gloss approach. While we do not match the state-of-the-art, our success in besting a difficult baseline using very little fine-tuning and exploiting none of the underlying graph structure suggests that the à la carte method can learn useful synset embeddings, even from relatively small data.
We measure SNLI and MultiNLI test set accuracy for CKY and BSSR. The aim is to ensure that they perform reasonably, and are in line with other latent tree learning models of a similar size and complexity.
40-dimensional log Mel filterbank features extracted by Kaldi from all 6 channels are used to train a deep LSTM acoustic model using Chainer. The LSTM has 3 layers and each hidden layer has 1024 units. The output layer has 1985 units, each of which corresponds to a senone target. The input feature is first projected to a 1024 dimensional space before being fed into the LSTM. The forced alignment generated by a GMM-HMM system trained with data from all 6 channels is used as the target for LSTM training. During evaluation, only the development and test data from the 5th channel is used for testing (only for the baseline system). The LSTM is trained using BPTT with a truncation size of 100 and a learning rate of 0.01. The batch size for stochastic gradient descent (SGD) is 100. A feature extraction layer is inserted in between the two LSTMs to extract 40-dimensional log Mel filterbank features with Eq. BPTT with a truncation size of 100 and a batch size of 100 and a learning rate of 0.01 is used for training. The data from all 5 channels in the development and test set is used for evaluating the integrated network. The joint training of the integrated network without updating the deep LSTM acoustic model achieves absolute gains of 0.92%, 4.23% and 7.24% over the baseline system on the simulated development set, real development set and real test set respectively. The joint training of the integrated network with all the parameters updated achieves absolute gains of 1.72%, 4.05%, 0.62% and 7.75% respectively over the baseline systems on the simulated development set, real development set, simulated test set and real test set respectively. The large performance improvement justifies that the LSTM adaptive beamformer is able to estimate the real-time filter coefficients adaptively in response to the changing source position, environmental noise and room impulse response with the LSTM acoustic model jointly trained to optimize the ASR objective. Further absolute gains of 0.15%, 0.09%, 0.17% and 0.22% are achieved with the introduction of acoustic feedback, which indicates that the high-level acoustic information is also helpful in predicting the filter coefficients at the next time step.
It works surprisingly well for the conversation learning task, since the additional memory structure creates local connections from each source LSTM to each target LSTM. The attention mechanism is an independent process from RNN, thus it reduce the long-span learning problem by establishing direct dependencies. While decoding target sequences, it helps to further avoid the gradient vanishing problem by feeding the additional information to decoder RNN at each time. It explains why combing attention and context in Context-Attn gains better performance.
While the score of all food taxonomies increased substantially, the taxonomies quality for environment did not improve, it even declines. This seems to be due to the lack of extracted relations in (§3.1), which results in imprecise representations and a highly limited vocabulary in the Poincaré embedding model, especially for Italian and Dutch. In these cases, the refinement is mostly defined by step §3.4.
All the experiments using different datasets and sentiment analysis levels use the same size of the training and test datasets. The size of the training set is 80% of the whole dataset, and the test set contains the remaining 20% of the dataset. The model is trained using the training set and then the test set is used to measure the performance of the model. The number of epochs is 50 for all the experiments. The best accuracy results for the three different used levels are identified by underlining the best results. Each line represents different sentiment analysis level. Char-level generally has the lowest accuracy results in the different datasets compared with the other levels, but for Ar-Twitter, it is better than the accuracy obtained on the Ch5gram-level after 23 epochs. Word-level achieves the best accuracy results for the Main-AHS and Ar-Twitter datasets and it has similar results with Ch5gram-level for the Sub-AHS.
We also evaluate our method with a real-world intent classification dataset ODIC. We can see that our proposed Induction Networks achieves best classification performances on all of the four experiments. In the distance metric learning models (Matching Networks, Prototypical Networks, Graph Network and Relation Network), all the learning occurs in representing features and measuring distances at the sample-wise level. Our work builds an induction module focusing on the class-wise level of representation, which we claim to be more robust to variation of samples in the support set. Our model also outperforms the latest optimization-based method—SNAIL. In addition, the performance difference between our model and other baselines in the 10-shot scenario is more significant than in the 5-shot scenario. This is because in the 10-shot scenario, for the baseline models the improvement brought by a bigger data size is also diminished by more sample level noises.
All of the models were used to classify the images into five different classes. The weight initialization for all the CNN models was a uniform random distribution between −0.5 and 0.5. The results indicate that using neural networks improves significantly the accuracy of the image classification models. The models using 64-by-64-pixel images were trained from scratch using Keras, while the models using 224-by-224-pixel images were trained with Lasagne after initializing the model weights to the ones obtained using the ImageNet dataset. This, along with the smaller learning rate and the higher number of epochs trained, might explain the higher accuracy of the models trained with 64-by-64-pixel images.
In general, the average of these scores is 5.646, and the median is 6 which indicates that, on average, at least half of the captions generated for an image have some terms related to the image content. Based on an overall calculation of all the scores provided by all participants, 73.2% of the images have a score of 3 or higher. This indicates that 73.2% of the images have at least one predicted caption containing terms related to the image. This result demonstrates high quality prediction given that there can be a tremendous amount of possible word combinations for captioning.
The performance drops to a competitive level with other approaches if adaptive thresholds or document-level contexts are removed.
For each dataset, the top row denotes our baseline and the second row shows our best comparable model. Rows with FULL models denote our best single model trained with all the development available data, without any other preprocessing other than mentioned in the previous section. In the case of the Twitter datasets, rows indicated as AUG refer to our the models trained using the augmented version of the corresponding datasets.
Finally, we compare the human results to each automatic metric with Pearson’s correlation coefficient. As evidenced by the results, chrF exhibits higher correlation with human judgment, followed by METEOR and BLEU. This is true both on the source side (x vs ^x) and in the target side (y vs ^yM). We evaluate the statistical significance of this result using a paired bootstrap test for p<0.01. Notably we find that chrF is significantly better than METEOR in French but not in English. This is not too unexpected because METEOR has access to more language-dependent resources in English (specifically synonym information) and thereby can make more informed matches of these synonymous words and phrases. Moreover the French source side contains more “character-level” errors (from CharSwap attacks) which are not picked-up well by word-based metrics like BLEU and METEOR.
On the other hand, Unconstrained-adv is similarly or more vulnerable to these attacks than the baseline. Hence, we can safely conclude that adversarial training with CharSwap attacks improves robustness while not impacting test performance as much as unconstrained attacks.
(subfigures a-l), class sets are highlighted in red and labelled which show where a particular combination of two previous viseme classes delivers a significant improvement in recognition. Whilst there is no apparent pattern through these pairings, this does further reinforce our knowledge that all speakers are visually unique and how difficult finding a set of cross-talker viseme sets will be when different phonemes require alternative grouping arrangements for each individual.
When extracting the text from the canonical HTML to pass it to UDPipe, we strip away all mark-up and discard Importantly, however, the text structure is retained in CoNLL-U by taking advantage of the support for comments to mark paragraphs and sentences. In addition to the global document ID number, each paragraph and sentence is also assigned a running ID within the document, using the following form: Paragraphs:  - , e.g. 000001-03 for paragraph 3 in document 1. Sentences:  - -  , e.g. 000001-03-02 for sentence 2 in paragraph 3 in document 1. A script for executing the entire pipeline from text extraction through UDPipe parsing will be made available from the NoReC git repository.
The ‘category’ attribute warrants some elaboration. The use of thematic categories and/or tags varies a lot between the different sources, ranging from highly granular categories to umbrella categories encompassing many different domains. Based on the original inventory of categories, each review in NoReC is mapped to one out of nine normalized thematic categories with English names. While the former covers reviews about movies and TV-series, the latter covers both musical recordings and performances. The related category ‘stage’ covers theater, opera, ballet, musical and other stage performances besides music. The perhaps most diverse category is ‘products’, which comprises product reviews across a number of sub-categories, ranging from cars and boats to mobile phones and home electronics, in addition to travel and more. The remaining categories of ‘literature’, ‘games’, ‘restaurants’, and ‘sports’ are self-explanatory, while the ‘misc’ category was included to cover topics that were infrequent or that could not easily be mapped to any of the other categories by simple heuristics.
Analysis on Hypernymy Organization. TAXI (DAG) Panchenko et al. SubSeq Gupta et al. TaxoRL (Partial) and TaxoRL (Full) denotes partial induction and full induction, respectively. Our joint RL approach outperforms baselines in both domains substantially. TaxoRL (Partial) achieves higher precision in both ancestor-based and edge-based metrics but has relatively lower recall since it discards some terms. In addition, it achieves the best F1e in science domain. TaxoRL (Full) has the highest recall in both domains and metrics, with the compromise of lower precision. Overall, TaxoRL (Full) performs the best in both domains in terms of F1a and achieves best F1e in environment domain.
As one may find, different types of features are complementary to each other. Combining distributional and path-based features performs better than using either of them alone Shwartz et al. Adding surface features helps model string-level statistics that are hard to capture by distributional or path-based features. Significant improvement is observed when more data is used, meaning that standard corpora (such as Wikipedia) might not be enough for complicated taxonomies like WordNet.
For models that we trained, we report F1 and standard deviation obtained by averaging over 10 random restarts. The Viterbi-decoding Bi-LSTM-CRF and ID-CNN-CRF and greedy ID-CNN obtain the highest average scores, with the ID-CNN-CRF outperforming the Bi-LSTM-CRF by 0.11 points of F1 on average, and the Bi-LSTM-CRF out-performing the greedy ID-CNN by 0.11 as well. Our greedy ID-CNN outperforms the Bi-LSTM and the 4-layer CNN, which uses the same number of parameters as the ID-CNN, and performs similarly to the 5-layer CNN which uses more parameters but covers the same effective input width. All CNN models out-perform the Bi-LSTM when paired with greedy decoding, suggesting that CNNs are better token encoders than Bi-LSTMs for independent logistic regression. When paired with Viterbi decoding, our ID-CNN performs on par with the Bi-LSTM, showing that the ID-CNN is also an effective token encoder for structured inference.
We tested three model versions against all datasets. Another model, Small, uses the same architecture as Base but reduces the number of convolutional layers to 1, the convolutional filters to 40, the LSTM dimension l to 50, and the phone embedding size d to 100. We also tested a Base-Softmax model, which replaces the CRF output of the Base model with a softmax. This comparison empirically motivates the CRF output because Base almost always outperforms Base-Softmax. Of these three models, the Base model performed the best with the exception of the French and Manipuri datasets. The differences in the French results can be considered negligible because the accuracies are all near 100%. The Small model performed best on Manipuri, which may suggest that reducing the number of parameters of the Base model leads to better accuracy on smaller datasets.
As we can see, WER for the i-vector system increases by up to 4% absolute, whereas WER for the M-vector system increases only by 1%, showing that the M-vector based system is more robust to speaker change condition.
We can see that M-vectors are more robust to the speaker change scenario than the i-vectors. On the test set, for example, we achieve 4% absolute lower WER than the i-vector system, which demonstrates the advantage of frame-level adaptation.
Results for single model were obtained by training at least 10 models and we report mean and standard deviation for these. #Θ denotes the number of parameters. The columns F1S through F1C contain individual F1 scores for problem classes. All ensemble models are optimized for F1-score on dev data. BiLSTM+SelfAtt contains 4.2M parameters without pre-trained BERT embeddings. BERTbig−nosrc and BERTbig−noprev denote ablations with empty source or target post respectively. Our SemEval submissions are denoted with ∗. Winning BLCU-nlp system achieved 61.87 F1 score on test data. More available at http://tinyurl.com/y3m5mskd.
Our BERT models encountered high variance of the results during the training. We assume the cause of this might be the problem difficulty, small training set and the model complexity. To counteract, we decided to discard all the models with less than 55 F1 score on dev data and we averaged the output class probability distributions when ensembling. Our initial experiments used sequences up to length 512, but we found no difference when truncating them down to 200.
All neural models beat state-of-the-art n-grams models substantially w.r.t. both word perplexity and word classification error. Without bootstrapping, the RNN model performs similarly to the more complex DCGM-I and HRED models. This can be explained by the size of the dataset, which makes it easy for the HRED and DCGM-I model to overfit. Bootstrapping from SubTle is particularly useful since it allows a gain of nearly 10 perplexity points compared to the HRED model without bootstrapping. We believe that this is because it trains all model parameters, unlike bootstrapping from Word2Vec, which only trains the word embeddings.
The GPU performance was .3% better at the end of the run. These results were averaged over multiple pre-training and squad runs but never reached the published BERT Base results. This is partially due to using sequence length 384 which degrades performance and partially due to not looking for the peak performance. The results in the paper are all based on averages to better compare techniques.
A General Matrix-Matrix Multiply (GEMM) is the most compute intensive operation in evaluating a neural network model. The performance benchmark was run using NVIDIA’s CUDNN and cuSPARSE libraries on a TitanX Maxwell GPU and compiled using CUDA 7.5. All experiments are run on a minibatch of 1 and in this case, the operation is known as a sparse matrix-vector product (SpMV). We can achieve speed-ups ranging from 3x to 1.15x depending on the size of the recurrent layer. Similarly, for the GRU models, the speed-ups range from 7x to 3.5x. However, we notice that cuSPARSE performance is substantially lower than the approximately 20x speedup that we would expect by comparing the bandwidth requirements of the 95% sparse and dense networks. State of the art This means that the performance should improve by the factor that parameter counts are reduced. Additionally, we find that the cuSPARSE performance degrades with larger batch sizes. It should be possible for a better implementation to further exploit the significant reuse of the weight matrix provided by large batch sizes.
All the parameters of DPMF are also included in METEOR and METEOR has tuned these parameters for better performance. So we use the same parameter values as METEOR as empirical value in DPMF and don’t need to tune the parameters again.
We first present results of domain classification. Five domains in this corpus are “music,” “weather,” “news,” “sports,” “opt-in, opt-out.” The performance in “Transcript text” row is evaluated on perfect transcriptions that correspond to human transcription and such performance is not achievable with the current ASR system. For qualitative analysis of our end-to-end model, we visualize the saliency map (gradients w.r.t. input) in Fig. We notice that the position of the network response corresponds to meaningful positions in the utterance. Therefore we continue with a more challenging task of the intent classification.
(1) PBMT-R wubben- etal-2012-sentence, a phrase-based machine translation model. (2) Hybrid narayan2014hybrid, a model which performs sentence spiting and deletions and then simplifies with PBMT-R. (3) SBMT -SARI xu -etal-2016-optimizing, a syntax-based translation model trained on PPDB and which is then tuned using SARI. Four neural seq2seq approaches: (1) DRESS D17-1063, an LSTM-based seq2seq trained with reinforcement learning (2) DRESS-Ls, a variant of DRESS which has an additional lexical simplification component; (3) NTS nisioi2017exploring and seq2seq model. (4) DMASS zhao2018integrating, a transformer-based model enhanced with simplification rules from PPDB. And two neural editing models: (1) LaserTagger and (2) EditNTS.
Using non-contextual embeddings, our model achieves a test F1 of 87.59. To our knowledge, this is the first time that a nested NER architecture has performed comparably to BiLSTM-CRFs Huang et al. (which have dominated the named entity literature for the last few years) on a flat NER task.
With the smaller vocabulary size, WER was drastically degraded due to the increase of the OOV rates in the test sets. However, the MTL approach with OOV resolution mitigated this problem and was robust to the vocabulary size. In the A2W models, the gain by the external RNNLMs was trivial with the small vocabulary. In contrast, external RNNLMs were always effective in case of using the OOV resolution even with the very small vocabulary such as 1k and 5k. The best results were obtained with vocabulary 15k, but the gaps of the performances between 5k and 15k were 0.30 and 0.94 in eval1 and eval3 test sets, respectively. Therefore, we can reduce the vocabulary size three times with the small performance degradation.
For example, in the 1st fold, there are 1210 relations and 444,422 facts in total for training . There are also 77436 facts and 135 relations in the test set. However, as the numbers of facts for the relations in the test set are imbalanced (e.g., some relations in the test set only contains very few facts), for each relation, only 5000 facts are randomly selected for testing. This is repeated for 10 times in order to make sure if TransW can provide consistent results. The accuracy in the table is based on the average of the accuracies of these 10 times of testing subsets. After training, a threshold σ for determining if a relation is valid or not is set according to the score function of Eq. If the score (using Eq. for a fact in the testing set of each fold is lower than σ, this fact is regarded as a true fact and otherwise, the fact is not a valid fact to the knowledge graph.
To compare with the existing approaches, this paper uses the HITS@N approach, which calculates the proportion of correct predictions in the top N facts from the ranking lists. This means, the higher HITS@N, the better performance it has. Similar to FB15K, the FB13 comes from Freebase. As shown in Fig. The composition of knowledge graph embeddings using word embeddings makes use of the semantic information in word embeddings and as such the performance is significantly improved.
We first implement our own recurrent policy model trained with the cross-entropy loss (XE). Note that our XE model performs better than the Student-forcing model on the test set. By switching to the model-free RL, the results are slightly improved. Then our RPA learning method further boosts the performance consistently on the metrics and achieves the best results in the R2R dataset, which validates the effectiveness of combining model-free and model-based RL for the VLN task.
4.3.2 Effect of Different Rewards We test four different reward functions in our experiments. The Global Distance reward function is defined per path by assigning the same reward to all actions along this path. This reward measures how far the agent approaches the target by finishing the path. The Success reward is a binary reward: if the path is correct, then all actions will be assigned with a reward 1, otherwise reward 0. In the experiments, the first two rewards are much less effective than the discounted reward functions which assign different rewards to different actions. We believe the discounted reward calculated at every time step can better reflect the true value of each action. As the final evaluation is not only based on the navigation error but also success rate, we also observe that incorporating the success information into the reward can further boost the performance in terms of success rate.
This section conducts error analysis on ARNs. We can see that there is still a significant performance gap from the anchor detector to entire ARNs. That is, there exist a number of mentions whose anchor words are correctly detected by the anchor detector but their boundaries are mistakenly recognized by the region recognizer. To investigate the reason behind this above performance gap, we analyze these cases and find that most of these errors stem from the existence of postpositive attributive and attributive clauses. These cases are quite difficult for neural networks because long-term dependencies between clauses need to be carefully considered. One strategy to handle these cases is to introduce syntactic knowledge, which we leave as future work for improving ARNs.
The average accuracy across icons is 88.4%, showing the efficacy of our approach in matching the experts’ aggregated annotations. This distance assumes small values, illustrating that the distributions are very close. Overall, these results support the potential of automatically assigning privacy icons with Polisis. Fig. The discrepancy between the two distributions is obvious: the vast majority of the Disconnect icons have a yellow label, indicating that the policies offer the user an opt-out choice (from unexpected use or collection).
Multiple Output Prediction. The decoder faces similar timestep challenges as the input. Because we are predicting entire sentences, the number of output timesteps is on the order of hundreds to thousands. We follow a method proposed by Tacotron Wang et al. Specifically, at each decoder timestep, we predict r spectrogram slices. We performed ablation studies to assess the effect of the various components of our model when equipped with state-of-the-art modules. Starting with a base encoder-decoder model, we introduce a single method. Because we individually evaluated the performance of each state-of-the-art method in isolation, we can better understand our model’s performance improvements. We show the changes in parameter size as well as the model’s spectrogram reconstruction error using mean-squared error across the different methods. The input spectrogram is a speaker not present in the training set. Our model is able to successfully generate rib-like patterns in the spectrogram. Even for the trigram case, we are able to see three distinct words, delimited by silence.
In order to understand the effect of back-translations better, we evaluated our systems on a split of test sets from past years into “forward” (German is the original source language) and “reverse” (the source side of the test set are German translations of texts originally written in English). As we can see, most of the gains from using back-translations are concentrated in the “reverse” section of the test sets. ’s Notice how it outperforms the top-performing system (Microsoft Marian) on the reverse translation direction but lags behind in the forward translation. We see two possible reasons for this phenomenon. The first is that back-translations produce synthetic data that is closer to the reverse scenario: translating back from the translation into the source. The second reason is that the reverse scenario offers a better domain match: newspapers tend to report relatively more on events and issues relating to their local audience. A newspaper in Munich will report on matters relating to Munich; the Los Angeles time will focus on matters of interest to people living in Southern California. This became evident when we investigated some strange translation errors that we observed in our submission to the shared task. For example, our system often translates “Münchnerin” (woman from Munich) as ‘miner’, ‘minder’, or ‘mint’ and “Schrebergarten” (allotment garden) as ‘shrine’ (German: Schrein). When we checked our back-translated training data for evidence, we noticed that these are systematic translation errors in our back-translations. While the word “Münchnerin” is frequent in our German data, women from Munich are rarely mentioned as such in English newspapers. With BPE breaking up rare words into smaller units, the system learned to translate “min” (possibly from “min|t” (as in the production facility for coins), which is “Mün|ze” or “Mün|zprägeanstalt” in German) into “Mün”. Once “Mün” was chosen in the decoder of the MT system, the German language model favored the sequence Mün|ch|nerin over Mün|ze or the even rarer Münzprägeanstalt. These findings suggest that back-translated data as well needs curation for domain match and systematic translation errors. Since this year’s test sets consist only of the (more realistic) “forward” scenario, we were not able to replicate the gains we observed for previous test sets when adding more back-translated data.
We first trained single transformer-base models for each language direction to serve as our baselines. We then re-score the EN→CS training data using the CS→EN model and filter out the 5% of data with the worst cross-entropy scores, which is a one-directional version of the dual conditional cross-entropy filtering, which we also used for our EN→DE experiments. This improves the BLEU scores on the development set and newstest2017. Next, we back-translate English monolingual data and train a CS→EN model, which in turn is used to generate back-translations for our final systems. A Transformer Big model trained on the same data is ca. 1.1 BLEU better. Due to time and resource constraints we train and submit a EN→CS system (this was the only language direction for English-Czech this year) consisting of just two transformer-big models trained with back-translated data. Our system achieves 28.3 BLEU on newstest2019, 2.1 BLEU less then the top system, which ranks it in third position.
Thus for an arbitrary incoming message, we can expect this classifier to correctly identify the speech act type of that message 69% of the time, while identifying 50% of the speech acts types to which the message belongs. If the classifier claims that a message of a particular type, we can estimate that that claim will be correct roughly 2/3rds of the time. We acknowledge that we cannot evaluate whether these improve over an earlier approach, given that we are not aware of an earlier technique for identifying speech acts on our data. Nevertheless, we find these results to be an encouraging starting point for building a virtual assistant, in light of the somewhat bare bones text classification strategy we used A promising area of future work, in our view, is to adapt more advanced classification techniques.
The 2014 system is evaluated on the 2014 test set and 2015 progress set containing 5 groups and 11 languages. The 2015 systems are evaluated on the full 2014 and 2015 test sets and the 2015 progress set. It should be noted however that the 2014 system is the only one for which the training data exactly matches the test data. Somewhat surprisingly, MAC-closed-2015, the best 2015 closed track submission, which is trained only on the 2015 training set, performs slightly better than NRC-open-2015, which was trained on both 2014 and 2015 data, and would therefore be expected to perform better on 2014 data.
Several conclusions can be drawn from these results. First of all, the efficiency of dCGD is confirmed as this feature alone leads to only 4.93% of patients incorrectly classified. Its advantage over dFM spectrum can be noted. In the experiments using only 2 or 3 parameters, the improvement brought by the proposed features compared to the spectral balances is clearly seen. Indeed the two time constants characterizing the respect of the mixed-phase model lead, at the patient level, to a better classification than when using the 3 spectral balances. It is worth noting the high performance achieved when using the 3 GD-based features. Adding the 2 magnitude-based spectrograms to these latter even leads to a slight degradation of accuracy. Similarly, it can be observed that adding the two mixed-phase model-based time constants makes the performance almost unchanged. Finally, considering all 10 features correctly identifies 93.84% of frames and 95.92% of patients. The high rate of false positive detections can be explained by the unbalance of the MEEI database, leading to an overestimation of pathologies. Nonetheless, relying on a ROC curve, one could modify these latter rates by playing on the posterior threshold for deciding whether a frame is pathological or not (i.e a frame could be pathological with a probability greater or lower than 0.5).
(BLEU gap is lower than zero), even if the pivot data is more than the direct data (data gap is bigger than 100%). For these language pairs, the source and target languages are very similar, such as Bs (Bosnian) and Mk (Macedonian), which are both in the South Slavic branch and both adopt the Serbian Cyrillic alphabet. Another case is that Da (Danish), Nb (Norwegian Bokmal) and Sv (Swedish) are very close as they are all Scandinavian in North Germanic branch. As a summary, the observations in this section is quite consistent with our intuition: (1) When the source and target languages are similar (e.g., in the same language branch), direct translation performs better on a large part of language pairs, while pivot translation performs better for the majority of dissimilar languages; (2) the improvement of pivot translation over direct translation positively correlates with the ratio of the training data size. An interesting point is that direct translation of very similar source/target languages could perform better than pivot translation, even if pivot translation may have a larger scale of training data. This motivates us to rethink about the widely used pivot translation.
In order to measure the contributions of various components of our attention models, we performed an ablation analysis for the global attention model evaluated on the CMUDict development data. As can be seen from the table, the removal of input feeding results in a very minor drop in performance. The use of regularization in the form of dropout and scheduled sampling also provides a minor boost. The importance of attention is reflected in almost a 1% absolute drop in performance when attention is removed. As we will later see, this boost is more prominent for longer words.
For both IL5 and IL6 we treated Amharic as the related language and we trained a TDNN-LSTM system on the BABEL Amharic corpus. Our final IL5 system used the Amharic ASR, though we later found the adapted Universal model performed better. Our final IL6 system used the universal phone set ASR. Both systems were adapted using the collected transcribed speech. An adapted English SF-Type classifier for each language was trained by including all collected SF-type labels in the specific language. Systems were evaluated on the setE Speech in two layers: the Relevance layer (to separate the documents with at least 1 SF from non-relevant documents with zero SF present), and Type layer (to detect all present SF types), using average precision (AP, equal to the area under the precision-recall curve). ASR adaptation on the 15-30 min of collected transcribed speech improves SF-type classification modestly. Furthermore, WER seems to track SF-type classification, which supports the utility of the SF-type task as an extrinsic measure of ASR performance. We also see that the universal phone set ASR has a similar WER to the adapted related language ASR when adapted on only 15-30 min of transcribed speech.
Under the variational framework introduced here, the encoder network, i.e., hash function, and decoder network are jointly optimized to abstract semantic features from documents. An interesting question concerns what types of network should be leveraged for each part of our NASH model. In this regard, we further investigate the effect of using an encoder or decoder with different non-linearity, ranging from a linear transformation to two-layer MLPs.
#1 and #2 are the BLEU scores of SNMT and #3-to-#12 are the BLEU scores of UNMT. Our observations are as follows:
To ensure a fair comparison with \newciteneelakantan2015efficient, we use the word embeddings released by them. For evaluation metrics, we adopt avgSim on WS-353 and localSim (also referred to maxSimC proposed by \newcitereisinger2010multi) on SCWS dataset, respectively. V refers to the 300-dimensional non-parametric multi-sense skip-gram model Neelakantan et al. ~VWordNet refers to the method in \newciteshi2016real. Our method boosts performance by remarkable 5.6 points, even better than semi-supervised method. PCA and Ex-RPCA achieves almost the same results on two dataset. While equipped with the real multi-sense indicator, Ex-RPCA has a wider field of application with good prospects.
We make the following observations: LMBR posteriors show consistent gains on top of the GRU model (LNMT vs FNMT rows), ranging from +0.5BLEU to +1.2BLEU. This is consistent with the findings reported by Stahlberg et al. The TNMT system boasts improvements across the board, ranging from +1.5BLEU in German-English to an impressive +4.2BLEU in English-Japanese WAT (TNMT vs LNMT). This is in line with findings by Vaswani et al. Further, applying LMBR posteriors along with the Transformer model yields gains in all tasks (LTNMT vs TNMT), up to +0.8BLEU in Japanese-English. Interestingly, while we find that rescoring PBMT lattices Stahlberg et al. with GRU models yields similar improvements to those reported by Stahlberg et al. We now address the question of deploying NMT systems so that MT users get the best quality improvements at real-time speed and with acceptable memory footprint. As an example, we analyse in detail the English-German FNMT and LNMT case and discuss the main trade-offs if one wanted to accelerate them. Although the actual measurements vary across all our productised NMT engines, the trends are similar to the ones reported here.
As with the IACv2 dataset, adding context using the discrete as well as the tf-idf features do not show a statistically significant improvement. For the neural networks models, similar to the results on the IACv2 dataset, the LSTM models that read both the context and the current turn outperform the LSTM model that reads only the current turn (LSTMct). However, unlike the IACv2 corpus, for Twitter, we observe that for the LSTM without attention, the single LSTM architecture (i.e., LSTMct+pt) performs better, i.e., 72% F1 between the sarcastic and non-sarcastic category (average) that is around 4% better than the multiple LSTMs (i.e., LSTMct+LSTMpt). Since tweets are short texts, often the prior or the current turns are only a couple of words, hence concatenating the prior turn and current turn would give more context to the LSTM model. However, for sentence-level attention models, multiple-LSTM are still a better choice than using a single LSTM and concatenating the context and current turn. The best performing architectures are again the LSTMconditional and LSTM with sentence-level attentions (LSTMctas+LSTMptas). LSTMconditional model shows an improvement of 11% F1 on the S class and 4-5%F1 on the NS class, compared to LSTMct. For the attention-based models, the improvement using context is smaller (∼2% F1). We kept the maximum length of prior tweets to the last five tweets in the conversation context, when available. We also considered an experiment with only the “last” tweet (i.e., LSTMctas+LSTMlast_ptas), i.e., considering only the “local conversation context” ( We observe that although the F1 for the non-sarcastic category is high (76%), for the sarcastic category it is low (e.g., 71.3%). This shows that considering a larger conversation context of multiple prior turns rather than just the last prior turn could assist in achieving higher accuracy particularly in Twitter where each turn/tweet is short.
we used a balanced data scenario. However, in online conversations we are most likely faced with an unbalanced problem (the sarcastic class is more rare than the non-sarcastic class). We thus experimented with an unbalanced setting, where we have more instances of the non-sarcastic class (NS) than sarcastic class (S)(e.g., 2-times, 3-times, 4-times more data). We observe that the performance drops for the S category in the unbalanced settings as expected. We used the Reddit dataset since it had a larger amount of examples. For this experiments we used the best model from the balanced data scenario which was the LSTM with sentence level attention. In general, we observe that the Recall of the S category is low and that impacts the F1 score. During the LSTM training, class weights (inversely proportional to the sample sizes for each class) are added to the loss function to handle the unbalanced data scenario. We observe that adding contextual information (i.e., LSTMctas+LSTMptas) helps the LSTM model and that pushes the F1 to 45% (i.e., a six point improvement over LSTMctas).
Splicing the original speech with a speech rate increased and a speech rate decreased signal can improve the performance better than splicing two speech rate increased speech or two speech rate decreased speech. The speech rate changing should be moderate, too big speech rate changing will decrease SLD accuracy.
The most naive approach for translating along a zero-resource path is to simply treat it as any other path that was included as a part of training. This corresponds to the one-to-one strategy from Sec. The good translation qualities of the translation paths included in training however imply that the representations learned by the encoders and decoders are good. Based on these two observations, we conjecture that all that is needed for a zero-resource translation path is a simple adjustment that makes the context vectors from the encoder to be compatible with the target decoder. Thus, we propose to adjust this zero-resource translation path however without any additional parallel corpus. Unsurprisingly, all the many-to-one strategies resulted in worse translation quality, which is due to the inclusion of the useless translation path (direct path between the zero-resource pair, Es-Fr.) These results clearly indicate that the multi-way, multilingual model trained with only bilingual parallel corpora is not capable of direct zero-resource translation as it is.
The most important observation is that the proposed finetuning strategy with pseudo-parallel sentence pairs outperforms the pivot-based approach (using the early averaging strategy from Sec. even when we used only 1,000 such pairs (compare (b) and (d).) As we increase the size of the pseudo-parallel corpus, we observe a clear improvement. Furthermore, these models perform comparably to or better than the single-pair model trained with 1M true parallel sentence pairs, although they never saw a single true bilingual sentence pair of Spanish and French (compare (a) and (d).) Even when we trained a single-pair model with 11m true parallel pairs, the model could not match the multilingual model finetuned with 1m true parallel pairs by achieving the translation quality of 24.26 BLEU on the test set. Another interesting finding is that it is only beneficial to use true parallel pairs for finetuning the multi-way, mulitilingual models when there are enough of them (1m or more). When there are only a small number of true parallel sentence pairs, we even found using pseudo pairs to be more beneficial than true ones. This applies that the misalignment between the encoder and decoder can be largely fixed by using pseudo-parallel pairs only, and we conjecture that it is easier to learn from pseudo-parallel pairs as they better reflect the inductive bias of the trained model. When there is a large amount of true parallel sentence pairs available, however, our results indicate that it is better to exploit them.
Training data. Note that for calculating the nDCG and ERR metrics for the training data, only the limited relevance judgements from the manually created conversations have been used. Three of our runs were able to outperform the Indri baseline with respect to nDCG@1000.
The last two rows show the performance of the \glspsr trained on the BookTest dataset; all the other models have been trained on the original CBT training data.
Questions vary in their target student grade level (as assigned by the examiners who authored the questions), ranging from 3rd grade to 9th, i.e., students typically of age 8 through 13 years. In practice, there is substantial overlap in difficulty among grade levels (also seen in the similar distribution of grade levels), as each grade level contains a mixture of easy and difficult questions.
On Wikipedia, our CNN outperforms the state of the art by more than three points. Our models achieve comparable results without preprocessing or feature engineering.
In the test, the listeners were provided with 6 sets audio, the original ZHAA utterance, the TTS input, and the synthesized speech from the 4 systems. We use synthesized audio of native accent TTS system as the reference audio. The listeners are asked to score between test utterances and TTS outputs on 5-point scale ( 1-not at all similar, 5-same).
we compare our model with three popular embedding approaches (results are from Das et al. The reified KB model outperforms DistMult MINERVA slightly outperforms our simple KB completion model on the NELL-995 task. However, unlike our model, MINERVA is trained to find a single answer, rather than trained to infer a set of answers. To explore this difference, we compared to MINERVA on the grid task under two conditions: (1) the KB relations are the grid directions north, south, east and west, so the output of the target chain is always a single grid location, and (2) the KB relations also include a “vertical move” (north or south) and a “horizontal move” (east or west), so the result of the target chain can be a set of locations. As expected MINERVA ’s performance drops dramatically in the second case, from 99.3% Hits@1 to 34.4 %, while our model’s performance is more robust. MetaQA answers can also be sets, so we also modified MetaQA so that MINERVA could be used (by making the non-entity part of the sentence the “relation” input and the seed entity the “start node” input) and noted a similarly poor performance for MINERVA.
we compare our model with three popular embedding approaches (results are from Das et al. The reified KB model outperforms DistMult MINERVA slightly outperforms our simple KB completion model on the NELL-995 task. However, unlike our model, MINERVA is trained to find a single answer, rather than trained to infer a set of answers. To explore this difference, we compared to MINERVA on the grid task under two conditions: (1) the KB relations are the grid directions north, south, east and west, so the output of the target chain is always a single grid location, and (2) the KB relations also include a “vertical move” (north or south) and a “horizontal move” (east or west), so the result of the target chain can be a set of locations. As expected MINERVA ’s performance drops dramatically in the second case, from 99.3% Hits@1 to 34.4 %, while our model’s performance is more robust. MetaQA answers can also be sets, so we also modified MetaQA so that MINERVA could be used (by making the non-entity part of the sentence the “relation” input and the seed entity the “start node” input) and noted a similarly poor performance for MINERVA.
Our simple distant supervision rule leads to better performance than then median system submission, but also falls substantially short of current state of the art.
It should be noted that experiments use different batch sizes between non-VAT and VAT trainings for the ATIS experiments. The larger batch size in VAT aims to include both labeled and unlabeled data in each mini-batch to optimize all losses simultaneously. Following previous works in joint-NLU [NLU_joint_att_Bing], batch size of 16 is used on non-VAT trainings. All SNIPS and VAT trainings use a 64 batch size. Non-VAT trainings have been tested on smaller batch sizes showing no improvement or even worse performances. Evaluation metrics are accuracy for intent detection and token-level micro-average F1 score for slot filling. Furthermore, all experiments present means and standard deviations on 8 runs. A validation set proportional to the data regimes has been used to ensure no overfitting.
We trained three models with different random seeds and report the performance of the model with median performance on the combined development set of all languages. Current state of the art for English Lee et al. However, this is a monolingual model that can only be applied to English. The supervised alignment using a dictionary is always superior compared to the unsupervised alignment without a dictionary or the unaligned embeddings. Our proposed adversarial alignment (w/ AT) leads to the best results across languages. The performance of BERT is close to the best FastText model. The improvements are smaller compared to FastText but still statistically significant for English.
In particular, the model using the Wikipedia data for training the discriminator is effective in generalizing to languages without training resources for temporal expression extraction, as these languages are also aligned during model training. It outperforms the state-of-the-art HeidelTime models by a large margin. The impressive performance of the multilingual BERT in the cross-lingual setting can be explained by the fact that the model has seen many sentences in our target languages during the pre-training phase, which can now be effectively leveraged in this new setting.
The results of the experiments can be found in Tab. First, focusing on the “-Aux” results, we can see that all feature vectors obtained by the neural models improve over the chance rate, demonstrating that indeed it is possible to extract information about linguistic typology from unsupervised neural models. Comparing LMVec to MTVec, we can see a convincing improvement of 2-3% across the board, indicating that the use of bilingual information does indeed provide a stronger signal, allowing the network to extract more salient features. Next, we can see that MTCell further outperforms MTVec, indicating that the proposed method of investigating the hidden cell dynamics is more effective than using a statically learned language vector. Finally, combining both feature vectors as MTBoth leads to further improvements. To measure statistical significance of the results, we performed a paired bootstrap test to measure the gain between None+Aux and MTBoth+Aux and found that the gains for syntax and inventory were significant (p=0.05), but phonology was not, perhaps because the number of phonological features was fewer than the other classes (only 28).
We conclude that pre-trained M-BERT leads to huge improvements in performance for relatively small data-sets in both extractive and abstractive summarization. It also reveals that extractive models would have higher performance for extractive data-sets than their corresponding abstractive ones.
Both “Multitask” and “Pretrain” improve over the baseline for all values of i, with the best development WER at i=3 and i=4 respectively. While the multitask models outperform the pretrained ones on the development set, they have nearly identical performance on the test sets. However, for all values of i, the “Pretrain + Multitask” model outperforms the corresponding “Multitask” and “Pretrain” models. For all three model types, a hierarchical CTC model (i=3,4) outperforms the vanilla multitask setting (i=5). The final best-performing model is “Pretrain + Multitask” with i=4, obtaining a 3.6% absolute reduction in word error rate on the Switchboard test set and a 3.1% absolute reduction on CallHome. For “Pretrain”, the PER is calculated after pretraining on the Phone CTC loss, and before the actual training the subword-level CTC model. For “Multitask” and “Pretrain + Multitask”, the final trained phone CTC model is evaluated. We observe a consistent trend: the best PERs are obtained with i=5 across all model types, and for a given i the “Pretrain + Multitask” model performs best, followed by “Pretrain” and then “Multitask”.
As can be seen, the best results were achieved for the person and geo-loc entity types. It is also worth noting that performance on the notypes task is significantly better across all metrics, which indicates that the system is capable of identifying entities correctly, but encounters issues with their type classification.
We observe that the number of epochs finished within the 1 hour training time constraint is close to an inverse linear function of k. In this particular setup, [1, 100] is a good range of k and the best result is achieved at K=50.
HAN vs CNN. Overall, while being slightly more parsimonious (see , HAN outperforms CNN almost everywhere. The only exceptions are FOB for injury_type and upper extr. for bodypart. This performance superiority is probably due to the fact that HAN features an attention mechanism. It could also mean that capturing word order outside of small local windows is beneficial. However, training takes three times longer, on average, for HAN than for CNN. This is due to the fact that even high performance CUDA optimized implementations of recurrent operations take much less advantage of the GPU than convolutional operations.
AITA achieves the best performance on all product categories regarding different evaluation metrics. The significant improvements over other models demonstrate that our instance transfer and augmentation method can indeed reduce inappropriate answer-question pairs and provide helpful review-question pairs for the generator. The performance of Ga is very poor due to the missing of attention mechanism. Both GPNa and GPNa+aspect have worse performance than ours, even though some product categories have large volume of QA pairs (>100k), e.g., Electronics, Tools, etc. This indicates that the answer-question instances are not capable of learning a review-based question generator because of the different characteristics between the answer set and review set. GPNar performs much worse than GPNa, which proves that a simple retrieval method is not effective for merging the instances related to reviews and answers. AITA adapts and augments the QA set to select suitable review-question pairs considering both aspect and generation suitability, resulting in a better generator. In addition, effectiveness of aspect feature and aspect pointer network can be illustrated via the slight but stable improvement of GPNa+aspect over GPNa and the performance drop of AITA-aspect on all the categories. This proves that even without precise aspect annotation, our unsupervised aspect-based regularization is helpful for improving generation.
We conduct human evaluation on two product categories to study the quality of the generated questions. Two binary metrics Relevance and Aspect are used to indicate whether a question can be answered by the review and whether they share the same or related product aspect. The third metric, Fluency with the value set {1, 2, 3}, is adopted for judging the question fluency. 1 means not fluent and 3 means very fluent. We selected 50 generated questions from each model and asked 4 volunteers for evaluation. Due to the incorporation of implicit aspect information, both AITA and GPNa+aspect significantly outperform GPNa regarding both Aspect and Relevance. Again, GPNar with a simple retrieval method for augmenting training instances cannot perform well.
We conduct human evaluation on two product categories to study the quality of the generated questions. Two binary metrics Relevance and Aspect are used to indicate whether a question can be answered by the review and whether they share the same or related product aspect. The third metric, Fluency with the value set {1, 2, 3}, is adopted for judging the question fluency. 1 means not fluent and 3 means very fluent. We selected 50 generated questions from each model and asked 4 volunteers for evaluation. Due to the incorporation of implicit aspect information, both AITA and GPNa+aspect significantly outperform GPNa regarding both Aspect and Relevance. Again, GPNar with a simple retrieval method for augmenting training instances cannot perform well.
Our proposed UAS task involves summarizing the five-sentence training ROCStories with a single sentence without summaries at training, i.e., perform zero-shot summarization. We found summaries by independent human raters have high similarity in this task, suggesting it is well-defined, and relatively unambiguous, in contrast to other summarization tasks where the desired length or the topic of the summary is unclear. The simplicity of the task is conducive to iterating quickly and making rapid progress in UAS. Having only five-sentences and a low-bound on total number of words avoids engineering issues that often arise with very long sequences. Due to the constraints, it is simple to calculate the maximum (sentence) extractive summarization performance, which is far from the human performance (see In contrast, it is unclear for example, what human performance is on the popular CNN/DailyMail (supervised) summarization task (see2017get) and whether abstractive models provide much of a benefit over extractive ones on it (kryscinski2019neural). For the base encoder-decoder configurations (RNN-RNN, TRF-TRF, and TRF-RNN), token masking and paragraph shuffling were added as noise, but no pre-training was done and no critic was added.
Below, we present our experimental setup and the description of individual subsystems. That is, we train smaller G-PLDA model on retransmitted data, size of both speaker and channel subspaces was fixed to 150. The final adapted model is derived from the two G-PLDA models so that the modeled within- and across-speaker covariance matrices are a weighted combination of the covariance matrices from the constituent models. Similarly, the model means are also interpolated. Interpolation weights are set to 0.6 for the original model and 0.4 for the adaptation one. Both calibration and fusion were trained with logistic regression optimizing the cross-entropy between the hypothesized and true labels on a development set. Our objective was to improve the error rate on the development set itself, but we were also monitoring error-rate trends on Speakers In The Wild dataset.
Specifically, we denote by Attn- * our adaptation of the respective model by including our Attention Supervision Module. We highlight that MCB model is the winner of VQA challenge 2016 and MFH model is the best single model in VQA challenge 2017. Furthermore, our model outperforms alternative state-of-art techniques in terms of accuracy in answer prediction. Specifically, the rank-correlation for MFH model increases by 36.4% when is evaluated in VQA-HAT dataset and 7.7% when is evaluated in VQA-X. This indicates that our proposed methods enable VQA models to provide more meaningful and interpretable results by generating more accurate visual grounding. In this case, the model is able to achieve higher rank-correlation, but accuracy drops by 2%. We observe that as training proceeds, attention loss becomes dominant in the final training steps, which affects the accuracy of the classification module.
The Factored NMT system aims at integrating linguistic knowledge into the decoder in order to overcome the restriction of having a large vocabulary at target side. We first compare our system with the standard word-level NMT system. For the sake of comparison with state of the art systems, we have built a subword system using the BPE method. The results are measured with two automatic metrics, the most common metric for machine translation BLEU and METEOR An improvement of about 1 %BLEU point is achieved compared to the best baseline system (BPE). This improvement is even bigger (1.4 %BLEU point) when UNK replacement is applied to both systems. In a quest to better understand the reasons of this improvement, we also computed the %BLEU scores of each output level (lemmas and factors) for FNMT. The lemma and factors scores of NMT and BPE systems are obtained through a decomposing of their word level output into lemma and factors. We observe yet again that FNMT systems gives better score at both lemma and factors level. Replacement of unknown words has been performed using the alignments extracted from the attention mechanism. We have replaced the generated UNK tokens by translating its highest probability aligned source word. We see an improvement of around 1 point %BLEU score in both NMT and FNMT systems. As shown in the table our FNMT system produces half of the UNK tokens compared to the word-based NMT system. This tends to prove that the Factored NMT system effectively succeed in modelling more words compared to the word based NMT system augmenting the generalization power of our model and preserving manageable output layer sizes. Though we can see that BPE system does not produce UNK tokens, this is not reflected in the scores. Indeed, this can be due to the possibility of generation of incorrect words using BPE units in contrast to the FNMT system.
In this section we report the results from using TRF LMs in a large vocabulary Mandarin speech recognition experiment. Different LMs are evaluated by rescoring 30000-best list from a Toshiba’s internal test set (2975 utterances). The oracle character error rate (CER) of the 30000-best lists is 1.61%, which are generated with a DNN-based acoustic model. The LM corpus is from Toshiba, which contains about 20M words. We randomly select 1% from the corpus as the development set and others as the training set. The vocabulary contains 82K words, with one special token  . The NN LM used here is the feedforward neural network (FNN) LM The number of hidden units is 512 and the projection layer units is 3×128. The TRF models are trained using the feature set “w+c+ws+cs+cpw” with different numbers of classes (200,400,600). The configurations are: m=100, βλ=0.8, βζ=0.6, tc=1000, t0=tmax=20000. The sample number K is increased from 300 until no improvements on the development set are observed, and finally set to be 8000. 20 CPU cores are used to parallelize the algorithm.
We collected those texts which were annotated sufficiently well to allow for the removal of meta-data as well as for the parsing of authorship, title, and language. All together, this resulted in the inclusion of 23,309 books from across ten languages (broken down in Tab. We also note that Navg is subject to measurement error from overrefined texts as well, most notably in the Portuguese data set, which has the smallest average text size, while having the fifth largest number of books (see Tab. There we note that Portuguese presents the most significant deviation between Navg and b (b is notably more than 120% larger than Navg), and moreover that this deviation is in the expected direction, i.e., Navg≪b. Note also that this observation is in agreement with those other languages that have Navg≪b in Tab.
InfoRank significantly outperforms Icsisumm on R-1 score and is on par with it on R-2 score. Both InfoRank and Icsisumm outperform RandomRank by a large margin. These results show that the sentence importance detector is capable of identifying the summary-worthy sentences.
\oldtextsc dw-nominate (poole2005spatial) is a dynamic method for learning ideal points from votes. It also learns two latent dimensions per legislator. We also compare text ideal points to the first dimension of DW-Nominate, which corresponds to economic/redistributive preferences (boche2018new). We use the fitted \oldtextscdw-nominate ideal points available on Voteview (boche2018new).
As shown in the table, on all datasets with different levels of sparsity, HCSC performs the best among the competing models. The difference between the accuracy of HCSC and NSC increases as the level of sparsity intensifies: While the HCSC only gains 0.8% and 1.0% over NSC on the less sparse Sparse20 IMDB and Yelp 2013 datasets, it improves over NSC significantly with 7.6% and 2.7% increase on the more sparse Sparse80 IMDB and Yelp 2013 datasets, respectively.
As shown in the table, on all datasets with different levels of sparsity, HCSC performs the best among the competing models. The difference between the accuracy of HCSC and NSC increases as the level of sparsity intensifies: While the HCSC only gains 0.8% and 1.0% over NSC on the less sparse Sparse20 IMDB and Yelp 2013 datasets, it improves over NSC significantly with 7.6% and 2.7% increase on the more sparse Sparse80 IMDB and Yelp 2013 datasets, respectively.
As indicated in the table, run 4 which uses Joint model obtained the highest F1 score among six runs. Using Joint model or Hybrid model obtained better F1 scores than using Separated methods. We also see that the difference between a system that performs sentence segmentation and a system that does not perform sentence segmentation is very small.
We use the notation ”representation model + classification algorithm”, e.g., BoW+LR, when referring to a specific variant. To compare the performance of these variants, we computed Kruskal-Wallis H tests with Dunn’s multiple comparison test for two sets of LR or LGBM variants in F1 and found significant differences (p<0.01) between both. Due to space constraint, we only reported the pairwise comparisons against the best variant in each set on the F1 metric at the significance level of 0.05.
For high-resource languages (top) both approaches perform equally, with the exception of BPEmb giving a significant improvement for English.
Our approach achieves state-of-the-art results on all three datasets, outperforming Jin and Szolovits It also outperforms our bert-based baselines. The performance gap between our baselines and our best model is large for small datasets (CSAbstruct, NICTA), and smaller for the large dataset (PubMed-rct). This suggests the importance of pretraining for small datasets.
Cross-entropy training leads to two kinds of discrepancies in the model. The first discrepancy comes from the disconnect between the task definition and the training objective. expected to rank sentences to generate a summary and (b) evaluated using ROUGE at test time. The second discrepancy comes from the reliance on ground-truth labels. Document collections for training summarization systems do not naturally contain labels indicating which sentences should be extracted. Instead, they are typically accompanied by abstractive summaries from which sentence-level labels are extrapolated. An alternative method Svore et al. ; Nallapati et al. Labeling sentences individually often generates too many positive labels causing the model to overfit the data. Collective labels present a better alternative since they only pertain to the few sentences deemed most suitable to form the summary. We found that there are many candidate summaries with high ROUGE scores which could be considered during training. Interestingly, multiple top ranked summaries have reasonably high ROUGE scores. For example, the average ROUGE for the summaries ranked second (0,13), third (11,13), and fourth (0,1,13) is 57.5%, 57.2%, and 57.1%, and all top 16 summaries have ROUGE scores more or equal to 50%. A few sentences are indicative of important content and appear frequently in the summaries: sentence 13 occurs in all summaries except one, while sentence 0 appears in several summaries too. Also note that summaries (11,13) and (1,13) yield better ROUGE scores compared to longer summaries, and may be as informative, yet more concise, alternatives.
When λ is set to 0 and 1, our joint span HPSG parser works as the dependency-only parser and constituent-only parser respectively. Although the time complexity of our Joint span model is O(n5), there is not much slower than Division span model with O(n3) time complexity. The comparison suggests that training and inference times are dominated by neural network computations and our decoder consumes a small fraction of total running time.
ablates the user model , -neighbor ablates the neighbor model, -rating is a single-task model that generates a review without the rating score, and -generation generates only the rating score.
All the baseline models are single-task models, without considering rating and summarisation prediction jointly. Our model (“ Joint”) significantly outperforms both “RS-Average” and “RS-Linear” (p−value<0.01 using t-test), which demonstrates the strength of opinion recommendation, which leverages user characteristics for calculating a rating score for the user.
Most datasets are from the domain of computer science or contain multiple domains. They are very diverse in terms of the number of documents—ranging from wiki20 with 20 documents to Inspec with 2,000 documents, in terms of the average number of gold standard keywords per document—from 5.07 in kdd to 48.92 in 500N-KPCrowd-v1.1—and in terms of the average length of the documents—from 75.97 in kdd to SemEval2017 with 8332.34. Four of these five datasets (500N-KPCrowd-v1.1, Schutz2008, fao30, wiki20) are also the ones with the highest average number of keywords per document with at least 33.23 keywords per document, while the fifth dataset (citeulike180) also has a relatively large value (18.42). Similarly, four of the five well-performing datasets (Schutz2008, fao30, citeulike180, wiki20) include long documents (more than 3,900 words), with the exception being 500N-KPCrowd-v1.1. We observe that the proposed RaKUn outperforms the majority of other competitive graph-based methods. For example, the most similar variants Topical PageRank and TextRank do not perform as well on the majority of the considered datasets. Furthermore, RaKUn also outperforms KEA, a supervised keyword learner (e.g., very high difference in performance on 500N-KPCrowd-v1.1 and Schutz2008 datasets), indicating unsupervised learning from the graph ’s structure offers a more robust keyword extraction method than learning a classifier directly.
Specifically, our context-dependent model, VAE+bc-LSTM, outperforms the context-dependent state-of-the-art method bc-LSTM on all the datasets, by 3.1% on average. Moreover, our context-free model VAE+LR outperforms the other context-free models, namely MFN, MARN, TFN, and LR, on all datasets, by 1.5% on average. Also, due to the contextual information, VAE+bc-LSTM outperforms VAE+LR by 3.1% on average.
4.4.2 Additional features Both POS and the distance to the diagonal feature significantly improve accuracy. Position information via the ’distance to the diagonal’ feature is helpful for all language pairs, and POS information is more effective for Romanian-English and English-Czech which involve morphologically rich languages. We use the POS and ’distance to the diagonal feature’ for the remaining experiments.
For such large datasets, testing time could be unacceptably long. Therefore, we report all the results based on 1,000,000 samples randomly sampled from either training or testing subsets depending on the scenario. Very little overfitting was observed even for our largest model.
6.1.3 The comparisons do not include any externally labeled data and POS labels. We use stacked bidirectional LSTMs with gated skip connections for the comparisons, and report the highest 1-best supertagging accuracy on the development set for final testing. Our model presents state-of-the-art results compared to the existing systems. The character-level information (+ 3% relative accuracy) and dropout (+ 8% relative accuracy) are necessary to improve the performance.
We can observe that the performance is getting better with the increased number of layers. But when the number of layers exceeds 9, the performance will be hurt. In the experiments, we found that the number of stacked layers between 7 and 9 are the best choice using skip connections. Notice that we do not use layer-wise pretraining [ Further improvements might be obtained with this method to build a deeper network with improved performance.
We compare its performance with various supervised baselines, as well as a baseline which parses relations from just the caption using Stanford Scene Graph Parser Schuster et al. (caption-only baseline), on the PredCls metric. Our proposed method substantially outperforms the caption-only baseline. This shows that our model predicts relationships more successfully than by purely relying on captions, which contain limited information. This in turn supports our hypothesis that it is possible to detect relations by pooling information from captions across images, without requiring all ground truth relationship annotations for every image.
In spite of the small size of the annotated samples, we derived statistically significant correlations for six labels. These results confirm that BiDAF performs well for the word matching questions and relatively poorly with the knowledge questions. By contrast, we did not observe this trend in GA.
JL2P demonstrates at least a 9% improvement over Lin et. al. The maximum improvement around 15% is seen in the Root joint. Errors in Root prediction can lead to a ”sliding-effect” of the feet; when the generation is translating faster than the frequency of the feet. Improvements in APE scores for long-term prediction, especially for Root, can help get rid of these artifacts in the generated animation.
It is clear from the results that human judges are not particularly effective at this task. Indeed, a two-tailed binomial test fails to reject the null hypothesis that judge 2 and judge 3 perform at-chance (p=0.003,0.10,0.48 for the three judges, respectively). In fact, judge 2 classified fewer than 12% of the opinions as deceptive! Interestingly, this bias is effectively smoothed by the skeptic meta-judge, which produces nearly perfectly class-balanced predictions.
We observe that automated classifiers outperform human judges for every metric, except truthful recall where judge 2 performs best.
In Tab. Our results clearly indicate the advantage of using ’supporting facts’ over the baseline model with a single classifier. The SFN model by our team achieved a best score of (0.558 Accuracy, 0.582 BLEU score) on the test set as indicated by the CrowdAI leaderboard. One of the reasons for such a significant drop in performance is due to the presence of new answers classes in the test set that were not present both in the original training and validation sets.
The correlations only stay moderate for all system variants. When comparing the base setup against the constant baseline, MAE is lower but RMSE is slightly higher, which suggests that our system does better on average but is prone to occasional large errors.
As is expected, the hidden layer takes 99% of the overall computation, which we aim to reduce in this section. The high computational rates of neural networks are easily accelerated by utilization of GPUs, but their high memory requirements for word embeddings and MaxEnts prevent us from doing so. However, this method also introduces a setback. Because only the middle layer of the RNNLM computations was deployed on the GPU side, and its surrounding layers are computed on CPUs, the information needs to be shared across the two heterogeneous processor units frequently. As the number of data exchanges increases, the decoding speed of the hybrid ASR system inevitably decreases. The frequency of data transfers between CPUs and GPUs affects the decoding speed more critically than the data size in each transfer. Therefore, we propose a method in which we reduce the number of data copies between CPUs and GPUs by concatenating the needed information to one block per frame. During the batching step, the history vectors and their next word embeddings that are emitted for each frame are stored in a consecutive CPU memory block, and the whole data block is transferred to GPU memory at once. The GRU outputs from the GPU are also copied back to the output layer computation in one data block. In addition, this approach still works in multi-GPU environments without additional operations by evenly distributing the block to GPUs since the hidden layer calculations for each segment of the CPU memory block are not sequentially related to each other.
In order to obtain shallow syntactic annotations on a large corpus, we train a BiLSTM-CRF model Lample:16; Peters:17, which achieves 97% F1 on the CoNLL 2000 benchmark test set. The training data is obtained from the CoNLL 2000 shared task Tjong:00, as well as the remaining sections (except §23 and §20) of the Penn Treebank, using the official script for chunk generation.
Again, we see the performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks. As we would expect, on the probe for predicting chunk tags, mSynC achieves 96.9 F1 vs. 92.2 F1 for ELMo-transformer, indicating that mSynC is indeed encoding shallow syntax. Overall, the results further confirm that explicit shallow syntax does not offer any benefits over ELMo-transformer.
Frost et reduce the effect of laziness by traversing the entire resulting data structure (derived from the memo tables) and (b) compute the total execution time including the final traversal but not any printing or writing to files. The programs for both systems were fully compiled, rather than being run in the OCaml or Haskell interactive environments. The overall picture that emerges is that Frost et al’s system performs very well for non left recursive grammars, partly, one suspects, due to Haskell’s laziness and sophisticated optimising compiler, which can eliminate many the overheads associated with data structures and high-order function manipulations, but there may be other factors, such as the use of integers to represent the parsing state (\ie, the index of the next token to be processed) as opposed to using the tail of the input sequence in the present system. However, in all cases, the continuation based system handles left recursion more effectively.
We now evaluate the full model. Our model performs competitively with the best results on SST and SST-5 datasets. It is important to note that our model does not use transfer learning apart from the pretrained word vectors, which is not the case for the competing models.
We notice that the proposed two-step approach and training on the latent space leads to a consistent improvement over the end-to-end approach where we train the same architecture using the time-domain SI-SDR loss. This observation holds when different separation modules are used and when we test them under different separation tasks. Our two-step approach yields an absolute SI-SDR improvement over the end-to-end baseline of up to 0.7dB, 0.5dB and 0.7dB for speech, non-speech and mixed separation tasks, respectively. Notably, this performance improvement is achieved using the exact same architecture but instead of training it end-to-end using a time-domain loss, we pre-train the auto-encoder part and use a loss on the latent representations of the sources. In Fig. When the encoder and decoder are trained individually, a fewer number of bases are used to encode the input which leads to a sparser representation (ℓ1 norm is roughly 10× smaller compared to the joint training approach). Finally, the latent representations obtained from our proposed approach exhibit a spectrogram-like structure in a way that Speech is encoded using less bases than high frequency sounds like Bird Chirping.
The overall detection performance of ANPs and VNPs was better than expected. As a reference, similar experiments with AudioSet [hershey2017cnn] had comparable performance (AUC 85%). AudioSet has 485 sound classes and did not deal with the subjectivity treated in this paper. accuracy of 69% and 71%, f-score of 51%, and 53% and AUC of 70% and 72%, respectively. The detection results correspond to the extracted audio features of 13 MFCCs, while 20 MFCCs were not included because they did not provide a performance gain. One explanation why VNPs performed better is because verbs tend to be less subjective or more neutral than adjectives [Neviarouskaya2009]. This means that the acoustic characteristics may be more distinguishable for classifiers as it is for humans. For example, people might argue about what would be the sound of a beautiful car, but not so much about the sound of a passing car.
The ST-Gumbel Tree-LSTM model and the PRPN were run five times with different initializations, each known as a trajectory. For imitation learning, given a PRPN trajectory, we perform SbS training once and then policy refinement for five runs. Left-/right-branching and balanced trees are also included as baselines.
A first observation is that the full model performs best on both tasks, suggesting that the model captures implicit meaning from various sources. In the NC Relations, all variants perform on par or worse than the majority baseline, achieving a few points less than the full model. In the AN Attributes task it is easier to see that the phrase (AN) is important for the classification, while the context is secondary.
We show that generated masks by MAM obtain the highest mean NPMI and, on average, superior results in both datasets, while only needing a single training. Our model MAM significantly outperforms SAM and attention models (MASA and MAA) for N≥20 and N=5. For N=10 and N=15, MAM obtains higher scores in two out of four cases (+0.033 and +0.009), and for the two others, the difference is only below 0.003. Regarding SAM, it obtains poor results in all cases, and must be trained as many times as the number of aspects.
We observe that pooling can effectively prevent posterior collapse while achieving significantly lower estimated NLLs compared to standard sequence VAEs, with max pooling offering the best performances for both datasets. Applying heavy word dropout leads to non-zero KL term but also worse log likelihood, suggesting the model has also converged to a different undesirable local optimum. Although better than the baseline model, average pooling provides the least amount of improvement compared to the other two methods. The gap is noticeably more significant on Yelp, which aligns our intuition that average pooling is likely to produce less dispersion over longer sequences due to the central limit theorem.
Note that in most cases, the gap for estimated NLLs between whether using it or not is rather significant, suggesting that KL annealing might be able to help the model to better explore during early stage of learning and eventually reach better local optimum. Additional research is needed to better understand the effects of KL annealing in optimizing variational models and why it is so crucial for reaching a better local optimum of ELBO.
We observe that on both datasets, models with pooling are at least just as negatively impacted by the permutations as the standard model, showing that the latent space clearly captures information beyond simply bag of words. Interestingly, the estimated NLLs evaluated on permuted input are still much lower than that of the baseline model evaluated on the original input sequences. This suggests that pooling might potentially help the models to capture high-level global features that are equivariant with respect to word order.
[∙,leftmargin=*,topsep=0pt] For both tasks, the “Multimodal HRED model with image sequence” performs significantly better than both the unimodal baseline HRED models thus suggesting that adding images indeed improves inference for both the tasks. Comparing the performance of the text response tasks on the two dataset versions, it is obvious that the model performs fairly well on all kinds of text responses except the “give-image-description” response, which in itself is a very hard task as it exploits both visual features and domain knowledge or other catalog information. Further, comparing the image response performance for the two dataset version, we observe that having additional textual descriptions of the images in a dialog context can help in better image response selection, which is intuitive. Adding attention does not improve performance. Though counter intuitive, this suggests the need for better multimodal attention models. As is evident from the table, the text response performance shows a high variance over the dialog states especially for Dataset V1, thus indicating that wherever the system needs to respond with a short product description, requiring core domain knowledge, it performs poorly. When we use 50 or 100 candidate images (instead of 5) we see a sharp decline in the performance, indicating that a better model is needed to retrieve and rank images from a large corpus of images. Overall, we feel there is enough scope for improvement and the current models only establish the feasibility of the two tasks. Further, we believe that benchmarking the performance on salient dialog states will allow for more focused research by understanding the challenges and complexities involved in each of these states.
We argue that DHG mechanism in our model can make the latter prediction process aware of the previously predicted sentiment of other aspects. Therefore, the model can achieve better performance on the multi-opinion-aspects sentences. To prove the effectiveness on this sort of instances, we select the sentences with multiple opinion aspects from the test set. In addition to the multi-opinion-aspects cases, we argue that without being aware of the sentiment information, the pipeline model would suffer from labeling many entities without sentiment polarities, which we call “no-opinion-aspect” instances. In order to explore the effectiveness of our integration strategy on such instances, we select the instances which contain no opinion aspects from the test set. We evaluate by sentence-level accuracy rather than F-a as the latter evaluation is always 0 in this case. The sentence-level accuracy metric is calculated as acc=#(no-op\ instances)/#(all\ instances). We can observe that the F-all performance boosts as the iteration time increases by one or two, while begins to converge as iteration time further increases. This is a sign of the effectiveness of our DHG mechanism. To be more specific, the ratio of multi-opinion-aspect instances in Res14 dataset is higher than Lt14 and Res15 datasets, accordingly, we can observe that Res14 dataset needs more iterations than the others.
While Darwin’s path is lower in surprise than the null, it is far larger than many paths that can be found: the greedy shortest-path algorithm, for example, can reduce the text-to-text average surprise to 2.11 bits and text-to-past average surprise to 2.97 bits. While Darwin’s average text-to-text surprise is lower than expected from a null model, it is far larger than many paths that can be found: a greedy shortest-path algorithm, for example, can reduce the text-to-text average surprise to 2.14 bits and text-to-past average surprise to 2.86 bits.
The results show that token2token (modeling pairwise dependency), source2token (modeling global dependency), and positional masks (encoding sequential information) all contribute important information to sequence modeling, and the contributions are complementary.
Although the results do not improve state-of-the-art BLEU value of machine translation task, the purpose of this experiment to verify the effectiveness of MTSA in contrast to dot-product based multi-head self-attention is accomplished.
Although the performance of state-of-the-art speech emotion recognition systems in the wild does not reach such high accuracies Tarantino et al. In contrast to simply using a one hot vector to represent the emotion state, the distance between emotion embeddings within the same cluster allows for fine-grained control of the emotion level that needs to be synthesized.
1-to-many, many-to-1 and many-to-many, represented as 1-1, 1-N, N-1 and N-N respectively. On more difficult tasks with more correct answers including head prediction for N-1 relations, tail prediction for 1-N relations and both head and tail prediction for N-N relations, CrossE achieves significant improvement, with 11.7% in average. As a conclusion, CrossE performs more stably than other methods on different types of relations.
fine-tuned on the same dataset. Using the English XLNet model as the source transformer model, the proposed method has achieved 81.62% in F1-Score, while the same method using mBERT source model has a lower F1-Score at 77.51%. Both XLNet and mBERT under-performed when using random token embeddings compared to their corresponding models initialized with Word2Vec token embeddings. This shows that transferring a monolingual
In this experiment, monolingual XLNet is compared with mBERT. Although the F1-Score of mBERT is expected to be higher for all datasets of those languages (mBERT pre-training language set includes those CLS languages), XLNet has achieved comparable results,
Most messages posted did not contain any emoticons, and that was true for each language. The language with the greatest proportion of postings with emoticons was German, and the rightmost three columns in that table indicate that of the emoticons that were used, the German postings included overwhelmingly positive emoticons. In general the table indicates significant differences in use of the different types of emoticons: all but Italian used more positive emoticons than negative or ambiguous ones (splitting the distribution of non-positive emoticons quite evenly), and half of the Italian emoticons were negative (with the remainder including nearly twice the proportion of positive emoticons to ambiguous ones).
Best performances are achieved using the combination of CRIE, sentiment, semantic, frequency, readability and follower features ( i.e. all features but LIWC and word embeddings) The feature selection is performed using random sampling. As a result 77 features are selected that perform consistently well across the datasets. We call these features the best features set. (see https://msuweb.montclair.edu/~feldmana/publications/aaai20_appendix.pdf for the full list of features). We vary the number of epochs and hidden layers. The rest of the parameters are set to default – learning rate of 0.3, momentum of 0.2, batch size of 100, validation threshold of 20. Classification experiments are performed on 1) both datasets 2) scraped data only 3) Zhu et al. ’s data only. Each experiment is validated with 10-fold cross validation. It is worth mentioning that using the LIWC features only, or the CRIE features only, or the word embeddings only, or all features excluding the CRIE features, or all features except the LIWC and CRIE features all result in poor performance of below 60%. Besides MLP, we also use the same sets of features to train classifiers using Naive Bayes, Logistic, and Support Vector Machine. However, the performances are all below 65%.
The baseline ASR trained only on the FAME speech corpus has a total WER of 37.8% without language tags and 40.3% with language tags. The CS detection performance provided by the ASR systems on each component of the FAME! The baseline ASR system provides an EER of 14.4% (8.7%) on the development (test) data. Adding automatically annotated in-domain speech and monolingual Dutch speech data (ASR_AA_NL) reduces the CS detection accuracy considerably with an EER of 9.8% (6.3%). The improved AM yielding a 12.2% (8.8%) absolute WER improvement compared to the baseline ASR (cf. After reporting the ASR and CS detection accuracies, we analyze the CS cases hypothesized by different ASR systems. We first present the language switch counts for the baseline ASR, the ASR with the augmented AM (ASR_AA_NL-VL) and the ASR with augmented AM and LM (ASR_AA_NL-VL_CS-LM). Finally, the most frequent (word-language tag) pair confusions are listed to demonstrate the characteristics of the words that are resulting in CS detection errors.
pairs are listed as the main source of the CS detection errors. From these lists, it can be seen that most common language tag errors are orthographically identical (or similar) short filler words with the same meaning both in Frisian and Dutch. The orthographically identical words, some of which are appearing in this list, induces the 2%-2.5% absolute WER difference between the ASR results with and without language tags. However, these errors are less of a concern for the ultimate goal of building a spoken document retrieval system.
We can see that the logistic regression model suffers from an extremely low precision, which is less than 10%. While this classifier aggressively labeled a large number of tweets as hateful, only 121,512 tweets are estimated to be truly hateful. In contrast, the supervised LSTM classifier has a high precision of around 79%, however, this classifier is too conservative and only labeled a small set of tweets as hateful. Next, we evaluate our weakly supervised classifiers which were obtained using only 20 seed slur terms and a large set of unlabeled tweets. The two-path weakly supervised bootstrapping system ran for four iterations. The first two rows show the evaluation results for each of the two learning components in the two-path system, the LSTM classifier and the slur learner, respectively. The third row shows the results for the full system. We can see that the full system Union is significantly better than the supervised LSTM model in terms of recall and F-score. Furthermore, we can see that a significant portion of hateful tweets were identified by both components and the weakly supervised LSTM classifier is especially capable to identify a large number of hateful tweets. Then the slur matching component obtains an precision of around 56.5% and can identify roughly 3 times of hateful tweets compared with the supervised LSTM classifier. The precision is only slightly lower than previous supervised models that were trained using the same dataset. We can see that both the estimated precision, recall, F score and the estimated number of truly hateful tweets by the two systems are significantly lower than the complete two-path bootstrapping system, which suggests that our two-path learning system can effectively capture diverse descriptions of online hate speech, maintain learning momentums as well as effectively combat with noise in online texts.
After ablating the VQ-VAE and selecting top-1 evidence as background (w/o VQ-VAE), we can see that the performance drops from 11.3% to 10.6%, which means VQ-VAE can automatically select relevant and useful evidence. In order to demonstrate the effectiveness of our learning method, we also train our model by joint learning (w/o SL). The overall BLEU score drops from 11.3% to 10.7%, which shows that our learning method can effectively train our model.
The first two columns of the results show FR for the top 3 and 5 fonts. The other three columns show F-score for the top 1, 3 and 5 fonts. Comparing to the Majority Baseline, the results from the Emoji and BERT models are statistically significant under paired t-test with 95% confidence interval. Although the BERT model performs slightly better than the rest, the Emoji model performs just as well, which suggests two things: (1) the font recommendation task is highly related to what emojis represent and 2) a simpler model like Emoji model can perform similarly to a complex solution like BERT.
Our consistent finding is that depth of at least two is beneficial. However, between two and three layers our results are mixed. Additionally, the results are mixed between the LSTM and the GRU, but both significantly outperform the RNN.
Performance comparisons. The n-gram and n-NN models perform nearly identically for small values of n, but for larger values the n-NN models start to overfit and the n-gram model performs better. Moreover, we see that on both datasets our best recurrent network outperforms the 20-gram model (1.077 vs. 1.195 on WP and 0.84 vs.0.889). It is difficult to make a direct model size comparison, but the 20-gram model file has 3GB, while our largest checkpoints are 11MB. However, the assumptions encoded in the Kneser-Ney smoothing model are intended for word-level modeling of natural language and may not be optimal for character-level data. Despite this concern, these results already provide weak evidence that the recurrent networks are effectively utilizing information beyond 20 characters.
With our method, the performance of a well-optimized 6-block model can be further boosted by adding two additional blocks, while simply using Transformer (8B) will lead to a performance drop. Specifically, we achieve a 29.92 BLEU score on En→De translation with 1.0 BLEU improvement over the strong baselines, and achieve a 0.6 BLEU improvement for En→Fr.
The mean cosine similarity between all the predicted next spoken word and the actual spoken word was used as the accuracy metrics for evaluating the performance of the proposed model on the next spoken word prediction task. The cosine similarity measures the similarity between two non-zero vectors of the same vector space.
We did hyperparameter tuning by trying several different values for dropout and warmup steps, and choosing the best BLEU score on the validation set for the baseline model.
The scheduled sampling which uses only the highest-scored word predicted by the model does not have a very good performance. The models which use mixed embeddings (the top-k, softmax, Gumbel softmax or sparsemax) and only backpropagate through the second decoder pass, perform slightly better than the baseline on the validation set, and one of them is also slightly better on the test set. The differentiable scheduled sampling (when the model backpropagates through the first decoder) have much lower results. The performance of these models starts degrading too early, so we expect that using more training steps with teacher forcing at the beginning of the training would lead to better performance, so this setup still needs to be examined more carefully.
We can, however, use cross-entropy in relation classification as well. To see how the choice of loss function affects performance in different scenarios, we switched ranking loss to cross-entropy loss or simply added cross-entropy loss in the relation classification task, and evaluated the Baseline+Tag model w/o multi-task learning, using the Chinese corpus of ACE 2005.
The token-level cross entropy loss used by “MLE” is specifically targeted for optimizing perplexity. Not surprisingly, “MLE” achieves the lowest perplexity of 19.69. “MLE” also has a higher BLEU score of 3.97 than sequence-level training methods. One possible explanation is that there are many more ways to be specific than to be generic. Producing a generic output is more likely to match the groundtruth, and get a higher BLEU score. Though BLEU is widely used in evaluating machine translation systems, previous work Liu et al. Distinct-n metric measures the diversity of generated texts. In terms of decoding algorithm, diverse beam search shows consistent improvements across nearly all automatic metrics.
“Logits Avg” performs best among 3 methods with a mean rank of 35.1. However, a naive baseline that randomly shuffles all the candidates would have a mean rank of 25.5, which is far better than our best model. This is evidence that our proposed method only reduces label bias to some degree instead of eliminating it.
We conduct a human evaluation on 200 random dialogue turns from the validation dataset. “MLE” produces generic texts with a very low specificity score of 0.54. Both “LogProb Avg” (SSA 0.84) and “Logits Avg” (SSA 1.15) improves over the “MLE” baseline (SSA 0.71), showing sequence-level training can indeed lead to more specific and sensible outputs. Using unnormalized logits as the score is more effective than using log-probabilities. Also, sequence-level training has a larger impact on specificity (+96% relative increase from 0.54 to 1.06) than sensibleness (+41% relative increase from 0.88 to 1.24).
Both corpora where randomly split in two parts (70%/30%), being 70% used for training and 30% for testing. This process was repeated 5 times. Specially if we compare the accuracy obtained for the Cinema corpus with previous results of 75%, which were achieved with recourse to a linguistically rich framework that required several months of skilled labour to build. Indeed, the previous implementation of JaTeDigo was based on a natural language processing chain, responsible for a morpho-syntactic analysis, named entity recognition and rule-based semantic interpretation.
Given the strong accuracy of the discriminator, we observe larger improvements of DAS over UniLM, than on CNN/DM: the summaries generated are only 2.72 tokens shorter than the human ones as opposed to 12.11. They also contain more novelty and less repetitions. In terms of ROUGE and BLEU, DAS also compares favorably with the exception of ROUGE-L.
We use the implementation of L19: https://github.com/coli-saar/am-parser. We train the parser for 100 epochs and pick the model with the highest performance on the development set after epoch 25. We perform early stopping with patience of 10 epochs. In effect, we have two LSTMs that are shared over the graphbanks: one for the edge model and one for the supertagging model. We use a batch size of 16 for the target formalism and a batch size of 64 for all non-target formalisms. Each batch consists of instances from only one formalism.
We also provide human evaluation results on a sample of test set. We random sample 50 documents and ask three volunteers to evaluate the output of NeuSum and the NN-SE baseline models. They are asked to rank the output summaries from best to worst (with ties allowed) regarding informativeness, redundancy and overall quality. NeuSum performs better than the NN-SE baseline on all three aspects, especially in redundancy. This indicates that by jointly scoring and selecting sentences, NeuSum can produce summary with less content overlap since it re-estimates the saliency of remaining sentences considering both their contents and previously selected sentences.
Interpretability scores for all embeddings increase as the number of categories in the dataset increase (30, 50, 70, 90, 110) for each category coverage (40%, 60%, 80%, 100%). This is expected since increasing the number of categories corresponds to taking into account human interpretations more substantially during evaluation. However, it can also be noticed that the increase in the interpretability scores of the GloVe and random embedding spaces gets smaller for larger number of categories. Thus, there is diminishing returns to increasing number of categories in terms of interpretability. Another important observation is that the interpretability scores of I and I∗ are more sensitive to number of categories in the dataset than the GloVe or random embeddings. This can be attributed to the fact that I and I∗ comprise dimensions that correspond to SEMCAT categories, and that inclusion or exclusion of these categories more directly affects interpretability. In contrast to the category coverage, the effects of within-category word coverage on interpretability scores can be more complex. Starting with few words within each category, increasing the number of words is expected to more uniformly sample from the word distribution, more accurately reflect the semantic relations within each category and thereby enhance interpretability scores. However, having categories over-abundant in words might inevitably weaken semantic correlations among them, reducing the discriminability of the categories and interpretability of the embedding. As category word coverage increases, interpretability scores for random embedding gradually decrease while they monotonically increase for the GloVe embedding. For semantic spaces I and I∗, interpretability scores increase as the category coverage increases up to 80% of that of SEMCAT, then the scores decrease. This may be a result of having too comprehensive categories as argued earlier, implying that categories with coverage of around 80% of SEMCAT are better suited for measuring interpretability. However, it should be noted that the change in the interpretability scores for different word coverages might be effected by non-ideal subsampling of category words. Although our word sampling method, based on words’ distances to category centers, is expected to generate categories that are represented better compared to random sampling of category words, category representations might be suboptimal compared to human designed categories.
In order to give evidence for this claim, we calculated the propensity of a participant to mark as present a tag that is also a word in the text segment. As it turns out, in the initial tagging step, CrowdFlower participants are more than three times more likely to write down a word that is present in the text. As participants in the rating step are only rating tags entered by people of the same group, this translates into a similar ratio in the rating task. However, as it seems that in the rating step, participants are less likely to mark as present a word which is not specifically in the text, reuse rate is inflated for both participant groups. As a result, a large majority of one-word annotations by CrowdFlower participants are already in the text, while the reverse is still true of expert annotations.
In the first case, the best heuristic is still the baseline, with Online LDA methods offering much better results than Gibbs Sampling-LDA methods. But in the second, the baseline is unusable, and while F1-scores of Online LDA methods drop by more than half, Gibbs Sampling-LDA methods stay the same or improve. Having fewer annotations where the concept’s keyword is in the textual segment will penalize the Online LDA methods, but not the Gibbs Sampling-LDA ones.
The model formula is: Outcome ~ LM_Out * TransType + (1 + LM_Out * TransType — ResponseId) + (1 — Question). The Outcome is 1 if the human subject selected the original code, 0 for the transformation. The fixed effects are a binary predictor (LM_Out), which is 1 if the language model selected the original code, and 0 otherwise, along with the type of transformation and their interaction term. The transformation types are deviation coded, meaning the intercept value is the grand mean over all transformations. Thus, for the coefficients for the parenthesis removal transformation, we subtract the 3 of other type coefficients and provide it in the table for convenience. Finally, Bayesian estimates do not have p-values and confidence intervals in the same way as frequentist approaches. Instead, we report the equivalent 95% credible interval, the bounds of a probability distribution that has a 95% probability containing the regression coefficient. If this range is entirely above 0, it indicates a positive effect, and vice versa for a range entirely below 0.
WbW-Attention aligns each word in the hypothesis with the premise. Compared with SE, our MIMN model obtains a dramatic improvement (9.7%) on MPE dataset by achieving 66.0% in accuracy. To compare with the bidirectional inter-attention model, we re-implement the ESIM, which obtains 59.0% in accuracy. We observe that MIMN-memory model achieves 61.6% in accuracy. This finding implies that inferring the matching features by multi-turns works better than single turn. Compared with the ESIM, our MIMN model increases 7.0% in accuracy. We further find that the performance of MIMN achieves 77.9% and 73.1% in accuracy of entailment and contradiction respectively, outperforming all previous models. In this section, we study the effectiveness of our model on the SCITAIL dataset.
WbW-Attention aligns each word in the hypothesis with the premise. Compared with SE, our MIMN model obtains a dramatic improvement (9.7%) on MPE dataset by achieving 66.0% in accuracy. To compare with the bidirectional inter-attention model, we re-implement the ESIM, which obtains 59.0% in accuracy. We observe that MIMN-memory model achieves 61.6% in accuracy. This finding implies that inferring the matching features by multi-turns works better than single turn. Compared with the ESIM, our MIMN model increases 7.0% in accuracy. We further find that the performance of MIMN achieves 77.9% and 73.1% in accuracy of entailment and contradiction respectively, outperforming all previous models. In this section, we study the effectiveness of our model on the SCITAIL dataset.
These confirm previous findings that, with the use of multiple discount parameters, the MKNS slightly outperforms the original KNS in all the test cases, with both having a clear advantage over simple absolute discounting (Abs). Further, it comes as no surprise that in case of all smoothing methods, results on the 3-gram models are always remarkably better than on the 2-gram models.
Our input features are VTLN-warped logmel with Δ,ΔΔ, the outputs are 32k tied CD states from forced alignment. Training followed the standard two-stage scheme, with first 1600M frames of cross-entropy training (XE) followed by 310M frames of Sequence Training (ST). XE training was done with SGD with nesterov acceleration, with learning rate decaying from 0.03 to 9e−4 over 600M frames. We report results on Hub5’00 (SWB and CH part) after decoding using the standard small 4M n-gram language model with a 30.5k word vocabulary. We saw slight improvement in results when decoding with exponent γ on the prior lower than what is used during training. The selection of models, decoding prior and acoustic weight happened by decoding on rt02 as heldout set. Baseline with * from personal communication with the authors. Baseline with † means system combination. With just n-gram decoding, this result is to our knowledge the best published single model.
Our input features are VTLN-warped logmel with Δ,ΔΔ, the outputs are 32k tied CD states from forced alignment. Training followed the standard two-stage scheme, with first 1600M frames of cross-entropy training (XE) followed by 310M frames of Sequence Training (ST). XE training was done with SGD with nesterov acceleration, with learning rate decaying from 0.03 to 9e−4 over 600M frames. We report results on Hub5’00 (SWB and CH part) after decoding using the standard small 4M n-gram language model with a 30.5k word vocabulary. We saw slight improvement in results when decoding with exponent γ on the prior lower than what is used during training. The selection of models, decoding prior and acoustic weight happened by decoding on rt02 as heldout set. Baseline with * from personal communication with the authors. Baseline with † means system combination. With just n-gram decoding, this result is to our knowledge the best published single model.
Our input features are VTLN-warped logmel with Δ,ΔΔ, the outputs are 32k tied CD states from forced alignment. Training followed the standard two-stage scheme, with first 1600M frames of cross-entropy training (XE) followed by 310M frames of Sequence Training (ST). XE training was done with SGD with nesterov acceleration, with learning rate decaying from 0.03 to 9e−4 over 600M frames. We report results on Hub5’00 (SWB and CH part) after decoding using the standard small 4M n-gram language model with a 30.5k word vocabulary. We saw slight improvement in results when decoding with exponent γ on the prior lower than what is used during training. The selection of models, decoding prior and acoustic weight happened by decoding on rt02 as heldout set. Baseline with * from personal communication with the authors. Baseline with † means system combination. With just n-gram decoding, this result is to our knowledge the best published single model.
Result Analysis fine-tuned directly on (RC)2 by 9%. BERT is overall better than DrQA. But directly using review documents to adapt BERT does not yield better results as in BERT+review. We suspect the task of RCRC still requires a certain degree of general language understanding on the question side and BERT+review also has the effect of (catastrophic) forgetting Kirkpatrick et al. Further, large-scale annotated CoQA data can boost the performance for both DrQA and BERT. However, our pre-tuning approach still has competitive performance and it requires no annotation at all. We examine the errors of BERT+Pre-tuning and realize that both locations of span and span boundaries tend to have errors, indicating a significant room for improvement.
We observe that the LSTM outperforms both the RNN and TF-IDF on all evaluation metrics. It is interesting to note that TF-IDF actually outperforms the RNN on the Recall@1 case for the 1 in 10 classification. This is most likely due to the limited ability of the RNN to take into account long contexts, which can be overcome by using the LSTM.
It should be noted that although there are 38 target entities, 37 were used for the evaluation because one timeline contained no events. Furthermore, although the three evaluation corpora are quite similar, the timelines created from the Stock Market corpus contain a higher average number of events with respect to those created from the other corpora. Additionally, it can also be seen that the Stock Market timelines contain events from a higher number of different documents. It should also be noticed that although the English and Spanish corpora are parallel translations the number of event instances and mentions in both cases are not exactly the same. For example, in the sentence “Apple Computer would not sell music branded with an apple.”, branded can be included as an event mention in the Apple Computer timeline because Apple Computer is the Agent of branded. However, in the corresponding translated sentence “Apple Computer no vendería música con una manzana por marca.”, it is not possible to identify the Agent of marca, i.e. the translation of branded.
Track A at SemEval 2015 had just two participant teams, namely, WHUNLP and SPINOZAVU, which submitted three runs in total. The best run was obtained by the corrected version of WHUNLP_1 with an F1 of 7.85%. The low figures obtained show the difficulty of the task. The results obtained by our baseline system, BTE, are similar to those obtained by WHUNLP_1. However, the results of the implicit time-anchoring approach (DLT) clearly outperforms our baseline and every other previous result in this task. This result would imply that a full time-anchoring annotation requires that a temporal analysis be carried out at document level.
Predictably, the strict evaluation is the most demanding, specially in terms of Recall. With respect to the results obtained using the relaxed scorer, precision is lower whereas recall is higher with respect to the other two metrics. Furthermore, DLT outperforms BTE whatever the language and the evaluation methodology. It is also remarkable that the results obtained for English are always better than the results for Spanish. This can be explained by the differences in the performances of the English and Spanish NLP modules. The results illustrate that the CLE F1 score keeps degrading as we include Spanish documents into the fold.
independently of the language. Furthermore, DLT and CLE obtain exactly the same results because co-referent events are not taken into account. However, the strict and relaxed scoring methods proposed in this work make it possible to distinguish between the performances of the two systems. Not surprisingly, the scoring by strict evaluation continues to be the lowest. Overall, CLE outperforms DLT being only in terms of precision (relaxed evaluation) or in both precision and recall (strict evaluation). Following the trend of previous results, both DLT and CLE outperform the baseline system with CLE obtaining the best overall performance. The F1 score differences between DLT and CLE using both evaluation methods are significant with p<0.001. Our hypothesis is that as the set of input documents in this experiment has been halved, the number of coreferent mentions in the gold-standard is much lower, which means that the advantage of CLE over DLT is not that meaningful. The most remarkable variation can be observed in the Recall values obtained using the relaxed evaluation. This is not that strange if we consider that in the relaxed evaluation detecting only one mention of an event is enough.
To demonstrate that the proposed approach performs constantly for all SLP downstream tasks, we report speaker recognition results on the LibriSpeech 100 hour selected subset, where train/test split is performed randomly with a 9:1 ratio, and there are 63 possible speakers. The proposed BASE and LARGE representations outperformed both APC and Mel-Features. BASE-FT2 further improves upon BASE while achieving the highest accuracy, whereas LARGE-WS also outperforms LARGE. To demonstrate domain invariant transferability of the proposed representation across different datasets, the Mockingjay model is pre-trained on LibriSpeech and applied on the MOSEI We also use a simple one-layer RNN classifier, where the model is trained to extract linguistic meanings from speech and discriminates between sentiments. Except that in the case of sentiment classification, LARGE-WS achieved the highest score without the need of fine-tuning, demonstrating that a deeper model has great potential for extracting general speech representations. To conclude this section, we claim that the proposed representations are general and can be used on datasets with various unseen domains.
Bottleneck features with GMM posteriors out performs the other systems configurations including the 5 system fusion. Interestingly, bottleneck features with DNN-posteriors show more of an improvement over the baseline system than in the speaker recognition experiments.
It is to note that we have not eliminated stop words. The validation accuracy for coarse class classification in our case is close to 95% which is significantly higher compared to validation accuracy of 89% obtained from the application of SGD. 1D CNN successfully learns discriminating features for coarse class classification with the help of data balancing technique.
WER results of the ablation study, showing the performance of the full system and with either of the three system components removed. Average relative deterioration of the WER, compared to the full system, is given in the last column.
We also evaluate the learned regression models for a two-class classification task. For each trait, we split the interviews into two groups by the median value for that trait. Any interview with a score higher than the median value for a particular trait is considered to be in the positive class (for that trait), and the rest are placed in the negative class. We then vary the threshold on the predicted scores by our regression models in the range [1,7], and estimate the area under the Receiver Operator Curve (ROC). The baseline area under the curve (AUC) value is 0.50, as we split the classes by the median value. Again, we observe high accuracies for engagement, excitement, friendliness, hiring recommendation, and the overall score (AUC>0.80 for SVM).
The scores on all the tasks are listed, including the scores on the nine tasks for each checkpoint. The “Avg." is for the semantic tasks. The number below each task denotes the number of training examples. The metrics for these tasks are mentioned above. Following the standard practice for computing GLUE scores, we report the arithmetic average of all metrics for tasks with multiple metrics (MRPC, QQP, STS-B), and we average the score of MNLI-m and MNLI-mm to get the final score of MNLI.
In addition, with the adversarial training module, the model further yields the remarkable improvements of the F1-scores. The results demonstrate that the adversarial network could capture deeper semantic features than simply using the GCNN-CRF model, via better making use of the information from both source and target domains.
Besides overall accuracy (Total), we show accuracy itemized by successful reference acts (Pointing), missing-referent (MissRef) and multiple-referent anomalies (MultRef). CNN is not tested here, as it does not handle attributes. PoP’s results are slightly higher than in the previous experiment, while those of TRPoP and Pipeline are slightly lower, such that now PoP is clearly above them. The three models are exploiting both visual and verbal information, as shown by their comparison to two additional baselines, shown at the bottom of the table. AttrRandom randomly picks one of the objects that shares the attribute with the query, if any, and raises the anomaly flag otherwise. This baseline has, by construction, 0% accuracy on MultRef anomalies, and it performs at random in MissRef detection. However, even in the pointing case, its performance is still well below that of the models. ImgShuffle is a variant of PoP trained after shuffling image vectors, so that each image ID is (consistently) associated with the CNN representation of another image (mostly depicting objects that do not match the image label). The only reliable signal that this baseline can then exploit is attribute information. Again, its performance is clearly below that of the models.
Each one contains few thousand training samples, out of which we select, following some selection criteria π, only a few dozen. More precisely, we are interested in the model’s performance when it is trained only with k=10,20,30,... ,100 labeled sentences. This selection simulates the behaviour of a system that needs to be trained for a newly available domain. We measure the performance by the best achieved F1 score during training on a separate test set that we have for each domain. These are the best results that we can hope to achieve when training with a proper subset of training samples. We will denote by Tn={x1,x2,...,xn} the full training set of a particular domain, assuming that each xi is a (sentence, tags) pair. The training set comprising the subset of k training samples that we select using the ranking π will be denoted by Tπk={xπ(1),xπ(2),...,xπ(k)}.
Empirically, and perhaps surprisingly, it turns out that the entropy rate of generated text is substantially higher than the estimate for true text derived from the model’s one-step predictions. As a timely example, the GPT-2 model Radford et al. We will defer the details of this experiment to the supplementary material. First, we show that, from a worst-case perspective, even an extremely accurate model (with ε average KL divergence from the true distribution) may have generated text with a substantially different entropy rate as compared to the true distribution. Indeed, we show that this worst-case amplification may occur for a variety of long-term properties of a probabilistic language model; this is because the one-step KL divergence does not in general provide tight control over the expectation of a bounded function. We then describe a calibration procedure to fix this mismatch while simultaneously improving the perplexity of the language model. From a statistical perspective, the procedure is simple, and we discuss approaches to make it computationally efficient.
We evaluate the ELBO on the test set by drawing 1,000 samples of the latent vector z per data point. We see that AUTR, both with T=30 and T=40, trained with or without dropout, achieves a higher ELBO and lower perplexity than Gen-RNN. Importantly, AUTR (trained with and without dropout) relies more heavily on the latent representation than Gen-RNN, as is shown by the larger contribution to the ELBO from the KL divergence term. Note that if a model isn’t taking advantage of the latent vector z, the loss function drives it to set q(z|x) equal to the prior on z (disregarding x), which yields a KL divergence of zero.
Best hyperparameters for k-nearest neighbour classifier were found to be k=7 and cosine distance. Optimal regularization coefficients for logistic regression and support vector machine classifiers were found to be 4.94×103 and 5.07, respectively. Best performing classifier was found to be support vector machine classifier with 86.92 % accuracy and 0.986 AUROC. Classifier has the highest error rate for the Prevention and Speculation classes, both staying below 80 % accuracy. Our results show that more than 15 % of samples belonging to Speculation category have been misclassified as Transmission.
The table shows Mel-cepstral distortion (MCD) in dB and F0 root mean square error (F0 RMSE). Although the main benefit of the proposed model is unsupervised adaptation, in general, all proposed strategies achieve a similar or even slightly better performance than the baseline system. However, since the differences are very small (e.g. maximum 0.2 dB in relative in the case of MCD), we conclude that our proposed architecture does not degrade the performance of multi-speaker modeling and provides flexibility to replace the modality nets.
Modelling syntactic flow In general, there is a clear difference in model performance according to the type of word to be predicted. Our main results in conventional language models are very good at predicting prepositions and verbs, but less good at predicting named entities and nouns. Among these language models, and in keeping with established results, RNNs with LSTMs demonstrate a small gain on n-gram models across the board, except for named entities where the cache is beneficial. In fact, LSTM models are better than humans at predicting prepositions, which suggests that there are cases in which several of the candidate prepositions are ‘correct’, but annotators prefer the less frequent one. Even more surprisingly, when only local context (the query) is available, both LSTMs and n-gram models predict verbs more accurately than humans. This may be because the models are better attuned to the distribution of verbs in children’s books, whereas humans are unhelpfully influenced by their wider knowledge of all language styles. When access to the full context is available, humans do predict verbs with slightly greater accuracy than RNNs. the window memory is constructed as the set of windows over the candidates being considered for a given question. Training of MemNNs (window memory) is performed by making gradient steps for questions, with the true answer word as the target compared against all words in the dictionary as described in Sec. Training of MemNNs (window memory + self-sup.) is performed by making gradient steps for questions, with the true answer word as the target compared against all other candidates as described in Sec. As MemNNs (window memory + self-sup.) is the best performing method for named entities and common nouns, to see the impact of these choices we conducted some further experiments with variants of it.
Overall, the differing approaches have relatively little impact on the results, as all of them provide superior results on named entities and common nouns than without self-supervision. However, we note that the use of all windows or LM rather than candidate windows does impact training and testing speed.
Ablation study. We conduct an ablation study on the loss terms in the proposed objective function and report the results in Tab. The results show that each term is important. Removing LDIST leads to a degraded content novelty score. Removing LSTYLE leads to a degraded style score, thought an improved fluency score and a content novelty score. Removing LGAN leads to both degraded fluency and style diversity scores.
The case of Japanese is particularly interesting for its relation to Korean. Family relatedness between these two languages is very controversial but their syntactic features are extremely similar. To put our parser in optimal transfer conditions, we perform one last experiment by training only on Korean (all available data) and testing on Japanese, and vice versa. We can also see that transferring in the opposite direction leads to a much better result, despite the fact that state-of-the-art supervised systems in these two languages achieve similar results (Japanese: 83.11, Korean: 86.92 by the best CoNLL18 systems). To rule out the impact of unsupervised sentence and token segmentation, which may be performing particularly poorly on some languages, we retrain the parser with gold segmentation and find that it explains only a small part of the gap.
Since the BERT [CLS] model is not fine-tuned, it does not perform as well as any of the models where the Seq2Seq encoders are trained on the task at hand. Unsurprisingly, the models with BERT augmented embeddings outperform the standard Seq2Seq encoder model substantially. We further observe that the inter-sentence aggregator also improves performance. Finally, the model with data-augmentation outperforms all of the baselines. We believe this is because of two reasons. First, most emails have a prior of being relevant, simply because the user CC’d Cortana. A consequence of this was that the model predicted a few sentences as relevant, even though there weren’t any. Augmenting the data with completely irrelevant emails helps overcome that bias.
We also validate our hypothesis: when we use Jangada for our real world use-case to remove signatures, we observe that while it has a high precision, the recall drops drastically (0.224); making it impractical to use in production. On the other hand, ScopeIt, even when trained on 20 Newsgroup, generalizes much better (recall 0.885, fscore: 0.936).
We have demonstrated the effectiveness of the AGDT. Here, we investigate the impact of model depth of AGDT, varying the depth from 1 to 6. We find that the best results can be obtained when the depth is equal to 4 at most case, and further depth do not provide considerable performance improvement.
These previous models are based on an aspect-independent (weakly associative) encoder to generate sentence representations. Results on all domains suggest that our AGDT substantially outperforms most competitive models, except for the TNet on the laptop dataset. The reason may be TNet incorporates additional features (e.g., position features, local ngrams and word-level features) compared to ours (only word-level features).
This year’s shared task included a new, large but somewhat noisy parallel resource: Paracrawl. We therefore experiment with data selection and weighting. Based on the scores produced in the previous section, we sort the new Paracrawl data by decreasing scores from 1 to 0. Next we select the first N sentences from the sorted corpus, add it to WMT and back-translated data and train again a shallow RNN model. In our experiments it seems, that selecting the first 8M out of 32M sentences according to this score leads to the largest gains on WMT2016 test data. A loss of 2.5 BLEU on full WMT+Paracrawl data is turned into a gain of 1.4 BLEU on WMT with selected Paracrawl data (see +
Our results show that MS has the highest similarity to both commercial services, thus emphasising the relevance of the clusters as compared to classifications that employ user-based knowledge.
The input text in both dialog data is defined as a series of utterances cu1,cs1,cu2,cs2,.. ,cut−1,cst−1 (alternating between the user utterance cui and agent’s response csi), while the cut is the question and the goal is to predict agent’s response cst. In this task the cst is a sequence of words, while our model predicts word tokens as actions. Specifically, we collect all possible agent responses into a candidate set C and define an action ai as the ith response in the candidate set C such that ai∈C. The set of candidate responses includes all possible bot utterances and API calls. We benchmark our approach against: End-to-End Goal Oriented Dialog Model (N2N) Bordes & Weston Seo et al. We show the results of our full SNs with teacher’s importance strategy. We observed the following results: For Tasks 1-5, when no features used, our SN model follows the state-of-the-art closely. Especially when match features are used, on average SN outperforms all models. All models have slightly higher error rates with OOV words. SNs immediately follow the best model when no match features (plain) are used, while outperforms all others when match features are used. The efficacy of our SN models are more pronounced with and without the match features with the real dialog scenarios in DSTC-2.
The reward-rescaling model makes slightly more errors, meaning its improvement in performance must come from its errors being less severe.
The baseline retriever-reader models achieve about 16% accuracy, and running the resource-consuming BERT-large does not contribute much on top of the smaller and faster BERT-base. Comparing {Q}A-space-only versus BERT-large implies that taking the results obtained from the {Q}A Space looks like a better choice as its accuracy is 5% higher than the baselines. Although QA-space-only does not seem to work by itself, its role in deciding whether to accept or reject the baseline result is clearly beneficial. Thanks to the contribution from the two question spaces and the accept-or-reject answer selection mechanism, our complete model reader-retriever-full outperforms the BERT baselines by 14% and our {Q}A-space-only method by 9%. Those results indicates that reader-retrievers with question spaces generate state-of-the-art OpenQA accuracy, and integrating them with retriever-readers further boosts the performance in a significant manner.
We change λt to reflect different model variants. First, λt=∞ corresponds to the removal of the transformation layer because the transformation is always identity in this case. Our model performs better than this variant on both datasets, yielding an average improvement of 9.8% on the pathology dataset and 2.1% on the review dataset. This result indicates the importance of adding the transformation layer. Second, using zero regularization (λt=0) also consistently results in inferior performance, such as 13.8% loss on the pathology dataset. We hypothesize that zero regularization will dilute the effect from reconstruction because there is too much flexibility in transformation. As a result, the transformed representation will become sparse due to the adversarial training, leading to a performance loss.
We focus on rows with “normal” for column “training” in this section. The target models have high overall accuracies on the original examples, but their performance drops dramatically on our modified examples (e.g., the overall accuracy of BERT on QQP drops from 94.3% to 24.1%). This suggests that the models are vulnerable to our modified examples and they indeed have the robustness issue we aim to reveal. Examples of modified examples are provided in the Appendix. To improve the model robustness, we fine-tune the models using adversarial training. A training batch consists of original examples and modified examples from the training data, where modified examples account for around 10% in a batch. During training, we generate new modified examples with the current model as the target and update the model parameters iteratively. The beam size for generation is set to 1 to reduce the computational cost. The performance of all the models on modified examples raises significantly (e.g. the overall accuracy of BERT on modified examples raises from 24.1% to 66.0% for QQP and from 23.8% to 87.0% for MRPC). The results demonstrate that adversarial training with our modified examples can significantly improve the robustness, yet without remarkably hurting the performance on original data.
Therefore, the first row of the table gives us the baseline of this margin. Moreover, as required in [L-Softmax] that m must be an integer, the minimal step is to increase m by one. As can be seen, even setting m to two would dramatically worsen the performance, we do not further increase m in this experiment.
We identify a number of important trends. (1) The deterministic coordinate selection strategies (left2right, least2most, easy-first and learned) significantly outperform selecting coordinates uniformly at random, by up to 3 BLEU in both directions. Deterministic coordinate selection strategies produce generations that not only have higher BLEU compared to uniform coordinate selection, but are also more likely according to the model as shown in Figures 1-2 in Appendix. The success of these relatively simple handcrafted and learned coordinate selection strategies suggest avenues for further improvement for generation from undirected sequence models. (2) The proposed beam search algorithm for undirected sequence models provides an improvement of about 1 BLEU over greedy search, confirming the utility of the proposed framework as a way to move decoding techniques across different paradigms of sequence modeling. (3) Rescoring generated translations with an autoregressive model adds about 1 BLEU across all coordinate selection strategies. Rescoring adds minimal overhead as it is run in parallel since the left-to-right constraint is enforced by masking out future tokens. (4) Different generation strategies result in translations of varying qualities depending on the setting. Learned and left2right were consistently the best performing among all generation strategies. On English-German translation, left2right is the best performing strategy slightly outperforming the learned strategy, achieving 25.66 BLEU. On German-English translation, learned is the best performing strategy, slightly outperforming the left2right strategy while achieving 30.58 BLEU. (5) We see little improvement in refining a sequence beyond the first pass. (6) Lastly, the masked translation model is competitive with the state of the art neural autoregressive model, with a difference of less than 1 BLEU score in performance. We hypothesize that a difference between train and test settings causes a slight performance difference of the masked translation model compared to the conventional autoregressive model. In the standard autoregressive case, the model is explicitly trained to generate in left-to-right order, which matches the test time usage. By randomly selecting tokens to mask during training, our undirected sequence model is trained to follow all possible generation orders and to use context from both directions, which is not available when generating left-to-right at test time.
Random is the expected accuracy when words are aligned at random. The result shows that our model outperformed the baseline methods significantly in all of the language pairs, indicating that ours is more robust in a low-resource senario. On the other hand, the baseline methods got poor performance, especially in the Finnish and English pair. Even though \citeauthorvecmap2 \shortcitevecmap2 report that their method achieves good performance in that language pair, our experiment has demonstrated that it does not perform well without a large amount of data.
Our method again outperformed the baselines to a large extent except for the Spanish-English pair. The poor performance of \citeauthorMUSE \shortciteMUSE in such a setting has also been observed in \citeauthorlimitation \shortcitelimitation, even though much larger data including Wikipedia were used for training in their experiments.
Furthermore, following the comparisons in zhang2015neural, we also measure the precision, recall and F1 of subjectivity and non-neutral polarities on the Spanish dataset. The subjectivity measures whether a target phrase expresses an opinion or not according to liu2010sentiment. Comparing with the best-performing system ’s results reported in zhang2015neural and li2017sentimentscope, our model EI can achieve higher F1 scores on subjectivity and non-neutral polarities.
The results show that EI achieves the best performance. The performance of SS li2017sentimentscope is much worse on Russian due to the inability of discrete features in SS to capture the complex morphology in Russian.
Applying other threshold than 0 caused the number of true positives to increase without much hurting the number of true negatives. In fact, the number of false positives and false negatives became much more similar for -0.25 and LCA than for 0. This results in the score of recall and precision being also similar, in a way that the micro F-1 is increased. Also, the threshold -0.25 resulted that the number of false positive is greater than the number of false negatives, than for example -0.2. LCA produced similar results, but was more conservative having a lower false positive and higher true negative and false negative score.
Both implementations, Hsklearn and our own produced very similar results, so for the sake of reproducibility, we chose to continue with Hsklearn. We can see here, in contrary to the subtask A, that -0.25 achieved for one configuration better results, indicating that -0.2 could be overfitted on subtask A and a value diverging from that could also perform better. The extended approach means that an extra feature extraction module was added (having 3 instead of only 2) with n-gram 1-2 and stopwords removal. The LCA approach yielded here a worse score in the normalized but almost comparable in the non-normalized. However, the simple threshold approach performed better and therefore more promising.
Our approach reached second place. The difference between the first four places were mostly 0.005 between each, showing that only a minimal change could lead to a place switching. Also depicted are not null improvements results, i.e. in a following post-processing, starting from the predictions, the highest score label is predicted for each sample, even though the score was too low. It is worth-noting that the all but our approaches had much higher precision compared to the achieved recall.
We achieved the highest micro F-1 score and the highest recall. Setting the threshold so low was still too high for this subtask, so precision was still much higher than recall, even in our approach. We used many parameters from subtask A, such as C parameter of SVM and threshold. However, the problem is much more complicated and a grid search over the nodes did not complete in time, so many parameters were not optimised. Moreover, although it is paramount to predict the parent nodes right, so that a false prediction path is not chosen, and so causing a domino effect, we did not use all parameters of the classifier of subtask A, despite the fact it could yield better results. It could as well have not generalized so good.
Comparing the performance improvement from text-only to the model with regional visual features (T+V), the features of salient visual objects contribute +0.6∼0.9 BLEU score over a much higher text-only baseline compared to UMMT.
Ablation Study. We conduct an ablation study of SIM on WoZ dataset. These features include POS, NER and exact match features. This indicates that for the dialogue state tracking task, syntactic information and text matching are very useful. Character-CNN captures sub-word level information and is effective in understanding spelling errors, hence it helps with 1.2% in joint goal accuracy. Variational dropout is also beneficial, contributing 0.9% to the joint goal accuracy, which shows the importance of uniform masking during dropout.
The number of questions and candidate answers in the ReQA SQuAD and While the number of questions is similar, ReQA SQuAD has around 2.6x fewer candidate answer sentences, making it an easier task overall. This difference is due to the fact that SQuAD itself was constructed to have many different questions answered by the same Wikipedia paragraphs. WikiQA ( yang-etal-2015-wikiqa) is another task involving large-scale sentence-level answer selection. The candidate sentences are, however, limited to a small set of documents returned by Bing search, and is smaller than the scale of our ReQA tasks. WikiQA consists of 3,047 questions and 29,258 candidate answers, while ReQA SQuAD and ReQA NQ each contain over 20x that number of questions and over 3x that number of candidates
Each conversation is a three turn talk between two persons. The conversation labels correspond to the emotional state of the last turn. Conversations are manually classified into three emotional states for happy, sad, angry and one additional class for others. In general, released datasets are highly imbalanced and contains about 4% for each emotion in the validation (development) set and final test set. We use the same set of hyperparameters across all model variants. We used weighted dropout of 0.2 and 0.25 as the input embedding dropout and the learning rate is 0.004. We train the LM for 14 epochs using batch size of 128 and limit the number of vocabulary to all token that appear more than twice. For classifier, we used masked self-attention layers and average pooling. For the linear block, we used hidden linear layer of size 100 and apply dropout of 0.4. β2=0.99. The base learning rate is 0.01. We used the same batch size used in training LMs but we create each batch using weight random sampling. We used the same weights provided by the organizers (0.4 for each emotion). We train the classifier on training set for 30 epochs and select the best model on validation set to get the final model.
We conduct an ablation study to verify the effectiveness of each important component of our model. “-I” denotes that we do not incorporate the inconsistency loss when training. By comparing the performance of the full model and “-I” in the table, we observe that after removing the inconsistency loss, the performance of both review summarization and sentiment classification drops obviously. Our experiment results on the Sports validation set also show that our inconsistency loss substantially reduces the number of inconsistent predicted sentiment labels between the source-view and summary-view sentiment classifiers from 11.9% to 6.3%. If we replace the attention mechanism in classifiers with a max-pooling operation (i.e., compare “Full” and “-A” in the table), the performance decreases as we anticipated. We also find that after removing the residual connection in the encoder (i.e., compare “Full” and “-R”), the performance of both review summarization and sentiment classification degrades, which suggests that the residual connection module is effective for both tasks. Moreover, we note that the copy mechanism (i.e., compare “Full” and “-C”) is helpful for both review summarization and sentiment classification.
For our NMT experiment with 4 Titan Xs, communication time is only around 13% of the total training time. Dropping 99% of the gradient leads to 11% speed improvement. Additionally, we added an extra experiment of NMT with batch-size of 32 to give more communication cost ratio. In this scenario, communication is 17% of the total training time and we see a 22% average speed improvement. For MNIST, communication is 41% of the total training time and we see a 49% average speed improvement. Computation got faster by reducing multitasking.
We have shown that adversarial and virtual adversarial training method improve the performance of PCNN model. We wish to place these results in a broader context of the results on these benchmark sets reported in the literature. In addition, we are aware that there are other deep learning models built for PPI task, but our methods are not comparable to them due to three reasons. a). employ macro F score to evaluate their model, which is usually used on multi-class classification problem. Furthermore, the unbalanced evaluation corpus (Positive:Negative=1:4.8 in AIMed and Positive:Negative=1:2.8 in BioInfer) will make the macro F score much higher than normal F score. b). c). different cross validation method
For training, we run for 100 epochs. We perform a grid search of hyper-parameters to select the number of BiLSTM layers from {1,2,3} and the number of LSTM units in each layer from {50,100,150,200,250,300}. Early stopping is applied when no performance improvement on the development set is obtained after 5 contiguous epochs. For both pipeline and joint strategies, we find the highest performance on the development set is when using two stacked BiLSTM layers.
Before carrying out the experiments, some essential operations of data preprocessing are performed. It is observed that there exist some similar and synonymous elements in facts on R-VQA, which may confuse the training of fact detection. For example, “on” vs. “on the top of” vs. “is on”, “tree” vs. “trees”, etc.
On the one hand, the model without question content (denoted as V only) shows a sharp drop in the accuracy of predicted facts. This phenomenon is intuitive since semantic facts and questions both come from textual modality, while images come from visual modality. In order to improve the semantic space of relation facts, we formulate fact prediction as a multi-objective classification problem, and candidate facts are combinations of three elements, namely a subject, a relation, and an object. Therefore, it is important to provide the question semantic information to reduce the space of candidate facts. On the other hand, the model without image content (denoted as Q only) suffers from limited prediction performance, indicating images also contain some useful semantic knowledge. Although existing methods have made good progress in visual detection achieving Rec@100 accuracy of 10-15% on Visual Genome for visual facts, these approaches are not suitable to predict question-related visual facts. In contrast with these works, our model incorporates the question feature for fact prediction, and achieves a much higher accuracy with a smaller candidate number of k, as well as a much simpler framework. In future work, it will be still meaningful to design a fine-grained model to obtain better prediction performance.
Different from the VQA dataset, COCO-QA doesn’t contain the multi-choice task, and fewer results are reported on it. In particular, our model significantly outperforms the state-of-the-art semantic attention model AMA
Along with the results of each polarity classification method, we present the state-of-the-art (SotA) result reported for each corpus. Because the BPE corpora were conceived for a different context, there are no SotA reported results for those corpora. We also ranked each evaluated method by its F1 score.
As different ELBOs are used in training, We only report the reconstruction negative log-likelihood (NLL) and the perplexity (PPL) on the test data. A lower NLL and PPL indicates the model is better at reconstructing the input sentences which also shows superior generation quality. For Yelp data, CNN-VAE has a lower perplexity by employing a different decoder structure. Our proposed approach is agnostic to decoder structures and can be applied on top of CNN-VAE as well. Interestingly, LSTM-SAVAE outperforms syntax-aware baseline 1 on PTB and BC data sets possibly due to a better generalization for shorter documents.
Evaluations are conducted on the BC test data due to its shorter document lengths compared to other three data sets. It is interesting to see that training a VAE on y itself does not provide a better reconstruction of y. With the proposed model, the reconstructed syntax is more accurate when inferring syntactic latent s from y. When inferring s from text input x, the accuracy of the predicted syntax is lower but still reasonably good.
The samples were annotated by the first author of this paper, using the proposed schema. In order to validate our findings, we further take 20% of the annotated samples and present them to a second annotator (second author). Since at its core, the annotation is a multi-label task, we report the inter-annotator agreement by computing the (micro-averaged) F1 score, where we treat the first annotator’s labels as gold. We deem this satisfactory, given the complexity of the annotation schema.
Number of training epochs was limited to 20 for models with window-size 10 because of their high computational requirements. The results suggest that training the models for higher number of epochs can produce better results if the embedding dimensionality is high, but can backfire in the opposite case. Using a larger window-size improves the average result in most cases.
The first column gives us a notion about the performance of the ASR engine. The rates stay below 20% for all languages; higher CERs are mostly caused by noisy CSS10 recordings.
The rows marked “All” show means and variances of the ratings of all 50 participants. Fig. Gen has significantly higher mean ratings on both scales. Unlike Sha or Sep, it allows cross-lingual mixing of the encoder outputs and enables smooth control over pronunciation. Sep scores consistently worst. The accuracy ratings are overall slightly higher than the fluency ratings; this might be caused by improper word stress, which several participants commented on.
Our network is 3.4 ppl better than the previous best, which was a Transformer-XL of a comparable size. For completeness, we also report the state of the art obtained with larger models, that is about 2 perplexity points better than us.
When all the three loss terms are used simultaneously (TS+CP) This phenomenon is evident from the results of TS→CP and CP→TS where the network gets a bit biased towards the latter optimization. Moreover, improvement in CP→TS and TS→CP as compared to TS and CP respectively suggests that incremental training better helps in teaching the framework. Since the performance on both transfer strength and content preservation metrics plays an important role in text style transfer task, we chose TS→CP, which has the maximum overall score, over the other models for further analysis.
We can see that traditional simple model such as Tf-Idf has already achieved a high accuracy of about 70%, though it only uses the unigram matching signals. Our methods performs much better than Tf-Idf, which indicates that the complicated matching patterns captured by hierarchical convolution are important to the text matching task. For the comparison with recent deep models, we can see that DSSM performs better than the others (though the improvement is quite limited), and our models (MP-Ind, MP-Cos and MP-Dot) outperform all of them. Though the best performance of our model (75.94%/83.01%) is still slightly worse than uRAE (76.8%/83.6%), uRAE relies heavily on pretraining with an external large dataset annotated with parse tree information. In the future work, we will study how to utilize external data to further improve our models.
This may be caused by the large difference between the testing data (paper citation data) and training data (click-through data) used in DSSM and CDSSM. Arc-I and Arc-II gain a significant improvement over these models, which may benefit much from the large training data. As for our models, the best performance is still achieved by MP-Dot (88.73%/82.86%), which is better than Arc-II (86.84%/79.57%). MP-Cos also gains a better result than Arc-II. The reason of the poor performance of MP-Ind on this task may lie in that the indicator function only captures the exact matching between words, but omits the semantic similarity.
We further show the reason why MP-Dot performs better than MP-Cos by analyzing the learned word embeddings. We can see that most words with small norm are indeed useless for matching, while most words with large norm (such as robotics and java) are domain terms which play an important role in paper citation matching. By further considering the importance of words, MP-Dot can capture more semantic information than MP-Cos and thus achieve better performance.
Other methods only show marginal mitigation effects in terms of p-value. We also find that the performance of our mitigation methods on word similarity is largely preserved as indicated by the Pearson correlation scores.
Results on word translation shows that the mitigation approaches do not harm the utility of bilingual word embedding and words in different languages still align well. From the results of word pair translation, the original bilingual embedding exhibits strong bias and the gap between two genders are around 0.49 in MRR in both ES-EN and FR-EN bilingual embeddings and the average cosine similarity difference is larger than 0.1. Hybrid_Ori results in smallest difference for cosine similarity and MRR gap between two gender forms. However, the discrepancy between two forms is not completely removed.
We also build the language model with or without the conversational context information. We observed that incorporating the conversational context improves performance showing that 9.6% and 11.7% relative improvement on SWBD only and SWBD + Fisher.
There are 120 different words in the official vocabulary, 4 of them having synonyms. A total of 124 tokens (or lemmas), not counting proper nouns (names) and punctuation. Also, some classes might be further defined (e.g. words beginning with vowels) or joined (e.g. distinguishing only particles against rest, or particles and prepositions against the rest). Advanced syntax highlighting considerations Strikingly, standard guidelines for syntax highlighting were not found. Explore simplicity and elegance in the coloring schemes. Most tokens should be preferably of the same color, i.e. a small number of colors might be explored to achieve a clean visualization of texts. A power-law distribution of tokens among colors might be well-suited to mimic natural occurring phenomena. Physical stimuli might be related to perceptual stimuli both through a power-law or an exponential law (respectively known as Steven’s law and Weber-Fechner’s law). This might be useful e.g. for coloring sets of tokens considering their similarity, or correctly choosing values for e.g. the hue or luminosity channels. One usually wishes to maximize contrast, although taste and less wearing combinations might also dictate the coloring choices. Stipulate axes of parameters to set colors. Wavelength or frequency is an obvious axis given the considerations previously exposed, but they are not perceptually uniform. Considering perceptually uniform color spaces in making choices may be relevant, such as CIElab and CIEluv. Consider two types of color schemes: of those with a dark background, which are more comfortable at first; and of those with a light background, which are usually impressive (and even annoying) at first, but the eye adapts in a few minutes, keeps you more stimulated, and is suitable for bright contexts (e.g. in daylight) . programmers often report that a blue background keeps them awake and more concentrated (the author also notices such effect). Colors have been associated to enhancements in specific tasks, It is important to consider that a text editor user might stay many hours using the tool (and they very often do), and that the colors and formats involved are thus prone to entail a considerable effect in the body and mental activity, the quality of life, and work results of a writer (e.g. a programmer).
Due to the time complexity of this process, we undertake the analysis of only the first 100,000 transitions on each dataset of the nineteen languages available from CoNLL-X and Nivre et al. After those experiments, we conclude that the lower and the closer upper bounds are a tight approximation of the loss, with both bounds incurring relative errors below 0.8% in all datasets. If we compare them, the real loss is closer to the upper bound |U(c,tG)|+npc(A∪I(c,tG)) in the majority of datasets (12 out of 18 languages, excluding Japanese where both bounds were exactly equal to the real loss in the whole sample of configurations). This means that the term npc(A∪I(c,tG)) provides a close approximation of the gold arcs missed by the presence of cycles in A. Regarding the upper bound |U(c,tG)|+nc(A∪I(c,tG)), it presents a more variable relative error, ranging from 0.1% to 4.0%.
For the non-monotonic dynamic oracle, three variants are shown, one for each loss expression implemented. As we can see, the novel non-monotonic oracle improves over the accuracy of the monotonic version on 14 out of 19 languages (0.32 in UAS on average) with the best loss calculation being |U(c,tG)|+nc(A∪I(c,tG)) The other two loss calculation methods also achieve good results, outperforming the monotonic algorithm on 12 out of 19 datasets tested.
We compare our fully-contextualized neural topic modeling (Contextual TM) on W1 with: (i) a version that combines both BoWs and BERT representations as inputs (Combined TM) Bianchi et al. Neural-ProdLDA Srivastava and Sutton LDA Blei et al. We compute the topic coherence via NPMI (τ) for 50 and 100 topics. For each of those conditions, we average model results over 30 runs, to minimize variation. Contextual TM obtains a comparable topic coherence as the Combined TM and Neural-ProdLDA on this specific data set and settings. This result confirms our first hypothesis: contextual embeddings can replace BoWs input representations without loss of coherence. This result opens up interesting future applications.
S2SS2SSS1Px2 Automatic Evaluation Our baseline is the uniform distribution over the topics. Therefore, we compute the matches with respect to English topic predictions, and the KL divergence between the uniform and the English predicted document-topic distributions. Topic predictions are significantly better than the uniform baselines: more than 70% of the time, the predicted topic on the test set matches the topic of the same document in English. The CD similarity suggests that even when there is no match, the predicted topic on the unseen language is at least similar to the one on the English testing data.
Because we have relations in the AMR graph, the alignment problem here is different from the word alignment in machine translation. To verify the effectiveness of our setup, we also compare our configuration to the condition No-Relation-Align where we only ignore the alignment between sentence and AMR relations by putting an all zero vector as the reference attention for each output relation label.
Notice that our proposed MSQG models are more effective in terms of retrieving the source 10-passage sets. Particularly, MSQGsharedh,rmrep outperforms the baselines in all metrics, indicating that broadcasting the mean of the document vectors to initialize the decoders (sharedh), and increasing the coverage of vocabulary (rmrep) are effective mechanisms for generating common questions. Mattn256 is an attention-based encoder-decoder with hidden size 256 for both encoder and decoder. M256 and M512 are non-attention encoder-decoders with hidden sizes 256 and 512. S2S denotes Mattn256, as in the main paper. It shows that models constructed using Mattn256 are more effective as opposed to models using M512 which has more parameters. Furthermore, we see that the averaging scheme in the Reduction step, broadcasting the same encoder mean, and increasing coverage of vocabulary tokens are important features to generating common questions using MSQG models.
For this experiment, we use the entire oral history set in the second stage for transfer learning and no data is held out for evaluation. Even though we used the two-staged acoustic modeling adaption to improve the performance on oral history interviews, the model performs better than the comparison models on many of the evaluation sets. The increase in performance is higher on rather challenging test sets while maintaining or even slightly increasing the good performance on the more clean tasks. Therefore, we conclude that the two-staged approach not only is useful for a specific task but also helps to increase the generalization of the acoustic model.
Training the public extractor released by these authors, we obtained the following significantly lower results: R-1:38.43, R-2:18.07 and R-L:35.37.
For the baseline methods, two linear classifiers, logistic regression and support vector machines, and one non-linear classifier, random forest, are tested with different BOW representations such as TF (term-frequency), TF-Nome (TF normalized by the document size), Binary (boolean occurrence value), and Tf-IDF. On average, SVM using TF-IDF outperforms the other baseline models.
The dataset is prepared from academic papers on arXiv [arxiv] since there did not exist a similar dataset which consists of academic papers written in LaTeX to the best of our knowledge.
Although the baseline model surpassed the performance of the Transformer due to the limitations mentioned above, Transformer-XL model improved the quantitative results by allowing sequence segments to carry information one to another. Transformer-XL outperformed the rest of the models by ∼61% on both cross-entropy error (CE) and bits-per-character (BPC). Besides, the validation losses of both Char-LSTM and Transformer models converged to approximately 1.15, while the validation loss of Transformer-XL managed to converge to 0.71 at the end of the training. The detailed figures of the losses on the training phase can be seen in Fig.
The results also improved by up to 3 BLEU points for cCNN and CHAR systems in the Ar-to-En direction. However, the performance is lower by at least 0.6 BLEU points compared to the MORPH system.
The MORPH system performed best with an improvement of 5.3% over UNSEG. Among the data-driven methods, CHAR model performed best and was behind MORPH by only 0.3%. Even though BPE was inferior compared to other methods, it was still better than UNSEG by 4%.
In all cases, the F1’ score is greater than F1, indicating that ambiguous sentences have a strong impact on the performance of the classifier. Weighted P’ and R’ also have higher values in comparison with simple precision and recall. treat is also a relation that appears to favor a high recall approach – there are very few negative examples where the type constraint of the terms (drug - disease) is satisfied. In previous work [aroyo2014threesides] we observed that treat generates less ambiguity than cause, which explains why treat has overall higher F1 scores than cause in all datasets. However, the high F1 scores could also make the models for treat more sensitive to confusion from ambiguous examples, as a small number of confusing sentences would be enough to decrease such a high performance. This result emphasizes the importance of weighting training data with ambiguity, as a few ambiguous examples seem to have a strong impact in generating false negatives during classification.
Distant supervision is a widely used technique in NLP, because its obvious flaws can be overcome at scale. We did not have enough time with the experts to gather a larger dataset from them, but the crowd is always available, so after we determined that the performance of the crowd matched the medical experts, we extended the experiments to 3,984 sentences. The crowd dataset in this experiment uses a fixed sentence-relation score threshold equal to 0.5, since this is the value where the crowd performed the best in the previous experiment, for both of the relations. As in the previous experiment, we employed five-fold cross validation to train the model. The test sets were kept the same as in the previous experiment, using the test partition labels as a gold standard. The goal was to compare the crowd to the distant supervision baseline, while scaling the number of training examples, until achieving a stable learning curve in the F1 score. Since the single annotator dataset performed badly in the initial experiment, it was dropped from this analysis. Recall is also the only un-weighted metric for which the cause baseline model performed better than the crowd. Recall is inversely proportional to the number of false negatives, indicating that distant supervision, for this relation, is finding more positives at the expense of incorrectly labeling some of them. This appears to be a consequence of how the model performs its training – one of the features it learns is the UMLS type of the terms. For the cause relation, it seems that term types are often enough to accurately classify a positive example (e.g. an anatomical component will rarely be the effect of a causal relation).
We performed two further experiments to answer the following questions: What happens if we continue training HCIAE-D -NP-ATT in an adversarial setting? In particular, we continue training by maximizing the score of the ground truth answer agtt and minimizing the score of the generated answer ^at, effectively setting up an adversarial training regime LD=−LG. The resulting discriminator HCIAE-GAN1 has significant drop in performance, as can be seen in Table. This is perhaps expected because HCIAE-GAN1 updates its parameters based on only two answers, the ground truth and the generated sample (which is likely to be similar to ground truth). This wrecks the structure that HCIAE-D-NP-ATT had previously learned by leveraging additional incorrect options. What happens if we continue structure-preserving training of HCIAE-D-NP-ATT? In addition to providing HCIAE-D- NP-ATT samples from G as fake answers, we also include incorrect options as negative answers so that the structure learned by the discriminator is preserved. HCIAE-D-NP-ATT continues to train under loss LD. In this case (HCIAE-GAN2 in Table. The additional computational overhead to training the discriminator supersedes the performance improvement. Also note that HCIAE-D-NP-ATT itself gets worse at the dialog task.
In both settings, we find that the proposed RbM-SL and RbM-IRL models outperform the baseline models in terms of all the evaluation measures. Particularly in Quora-\@slowromancapii@, RbM-SL and RbM-IRL make significant improvements over the baselines, which demonstrates their higher ability in learning for paraphrase generation. On Quora dataset, RbM-SL is constantly better than RbM-IRL for all the automatic measures, which is reasonable because RbM-SL makes use of additional labeled data to train the evaluator. Quora datasets contains a large number of high-quality non-paraphrases, i.e., they are literally similar but semantically different, for instance “are analogue clocks better than digital” and “is analogue better than digital”. Trained with the data, the evaluator tends to become more capable in paraphrase identification. With additional evaluation on Quora data, the evaluator used in RbM-SL can achieve an accuracy of 87% on identifying positive and negative pairs of paraphrases.
Our models again outperform the baselines in terms of all the evaluation measures. Note that RbM-IRL performs better than RbM-SL in this case. The reason might be that the evaluator of RbM-SL might not be effectively trained with the relatively small dataset, while RbM-IRL can leverage its advantage in learning of the evaluator with less data. Our models of RbM-SL and RbM-IRL get better scores in terms of relevance and fluency than the baseline models, and their differences are statistically significant (paired t-test, p-value <0.01). We note that in human evaluation, RbM-SL achieves the best relevance score while RbM-IRL achieves the best fluency score.
Our models again outperform the baselines in terms of all the evaluation measures. Note that RbM-IRL performs better than RbM-SL in this case. The reason might be that the evaluator of RbM-SL might not be effectively trained with the relatively small dataset, while RbM-IRL can leverage its advantage in learning of the evaluator with less data. Our models of RbM-SL and RbM-IRL get better scores in terms of relevance and fluency than the baseline models, and their differences are statistically significant (paired t-test, p-value <0.01). We note that in human evaluation, RbM-SL achieves the best relevance score while RbM-IRL achieves the best fluency score.
We notice that BPD1.0 (θ=1.0) has the highest Recall value of 0.727, namely the summary obtained by this algorithm contains most of contents in the summary of human experts, but its Precision value of 0.256 is much lower than that of the PR100 algorithm, indicating that the BPD algorithm add more sentences into the summary than the human experts do.
The comparison between model #1 and model #2 shows that the combination of the top 4 hidden layers for contextual representation is better than the top layer. The possible reason is that the semantic information about gender may be partly transformed to the higher level semantic information during the hidden layers in BERT. In addition, changing BERTBASE to the BERTLARGE reduces the loss in 5-fold CV on train from 0.4699±0.0431 to 0.4041±0.0532, which demonstrate increasing model size of BERT can lead to remarkable improvement on the small scale task. The exploration of contextual representation layers shows the proper representation layers is proportionate to the number of hidden layers of BERT. In other words, the modeling ability of BERTLARGE is more powerful than BERTBASE by using a more complex function to do the same work. Changing the method for computing the span contextual representation from meanpooling to attention mechanism reduces the loss in CV on train by ∼0.02, which demonstrates that the attention mechanism used in the experiment is effective to compute the contextual representation of the entity span. To the best of my knowledge, it is a novel attention mechanism with no learnable parameters and more space-efficient and more explainable in practice.
The comparison between model #10 and model #11 shows that their difference on performance is slight. Also, both of them are effective to the fine-tuning of MSnet and reduce loss in the CV of train by ∼0.054 compared to the feature-based approach. Furthermore, the tuning on Ltuning shows the best setting is tuning top 12 hidden layers in BERT, and more or fewer layers will reduce the performance of MSnet. The possible reason is that tuning fewer layers will limit the ability of the transformation from basic semantic to gender-related semantic while tuning more bottom layers will damage the extraction of the underlying semantics when training on a small data set.
While WE(10m) does not show much help for the improvement, WE(100m) and WE(all) significantly boosts the performance. It shows that BLSTM RNN can benefit from word embeddings trained on large unlabeled corpus and larger training corpus leads to a better performance. This suggests that the result may be further improved by using even bigger unlabeled data set. With the help of GPU, WE(all) can be trained in about one day (23 hrs). The training time increases linearly with the training corpus size.
The scores in the table are averaged over all layers. Light blue indicates statistically significant with p-value below 0.1, Dark blue indicates statistically significant with p-value below 0.05.
The scores in the table are averaged over all layers. KM=K-Means clustering, Lin=Linear classifer, NN=3-layer Neural Network with ReLU activations. Bold reflects highest score in each column.
In this experiment, we demonstrate the superiority of the collective approach utilized by GPFL in evaluating instantiated rules over the baseline approach that grounds every mined rule for evaluation. For fair comparison, we implement the baseline approach on Neo4j graph database as well. We select WN18RR, FB15K-237 and DBpedia3.8 to account for the effect of size of KG on the experiment results. For FB15K-237 and DBpedia3.8, we randomly select 20 targets for evaluation, and for WN18RR, we evaluate on all 11 relationship types. The rule set for each benchmark dataset is prepared beforehand, and are divided into CARs and instantiated rules of different lengths (e.g., len=1) for fine-grained evaluation. To ensure experiments can be finished in a reasonable time, we allow the evaluation of each target to run for at most 30 minutes. We report the average runtime over all evaluated targets. For instance, it took GPFL 1.87s to evaluate the same set of instantiated rules of length 3 that took the baseline 150.85s to run on WN18RR. When evaluating CARs, the collective approach in GPFL is reduced to the baseline approach, and thus we observe similar performances between GPFL and the baseline.
Once again, the optimal value of γ is different across all systems and beam sizes. Interestingly, as the amount of training data decreases, the gains in BLEU using a tuned reward penalty increase with larger beam sizes. This suggests that the beam problem is more prevalent in lower-resource settings, likely due to the fact that less training data can increase the effects of label bias.
But since the performance drop is very small, around 1% and 0.3% for Big-patent and Newsroom respectively, this means the proposed method has relatively good transferability across different domains.
A 10-fold stratified cross-validation is executed on the dataset for each classifier using each feature set. The content feature set using a tf-idf approach achieves the best results over all classifiers. The SMO classifier obtains the highest F1-score of 0.94 using the content feature set. However, it is outperformed by the two other classifiers when using the factual or structural feature set. Random Forest turns out to be the most stable classifier across the four feature sets.
For λ∈{0.2,0.3,0.5}, the results are comparably, with λ=0.2 performing best for satire detection. Setting λ=0.7 leads to a performance drop for satire but also to F1 =0 for publication classification. Hence, we chose λ=0.2 (the best performing model on satire classification) and λ=0.7 (the worst performing model on publication identification) to investigate RQ1. The majority baseline fails since the corpus contains more regular than satirical news articles. In comparison to the baseline model without adversarial training (no adv), the model with λ=0.2 achieves a comparable satire classification performance. As expected, the publication identification performance drops, especially the precision declines from 44.2 % to 30.8 %. Thus, a model which is punished for identifying publication sources can still learn to identify satire.
When using a single-channel network, we tune the parameters of only one channel while switching off the other channel. As seen, the sentential features are found to be more important than syntactic features. We attribute this to the short and noisy nature of WebQuestions questions due to which syntactic parser wrongly parses or the shortest dependency path does not contain sufficient information to predict a relation. By using both the channels, we see further improvements than using any one of the channels.
S4SS1SSS0Px3 Final results: It is clear that all neural models outperform conventional CRF-based models tmChem and CRFSuite. Our EBC-CRF model outperforms the BiLSTM-CRF + LSTM-char model with a 3.7 F1 score improvement. Compared to the baseline model BiLSTM-CNN-CRF, the ELMo-based contextualized word embeddings help to produce an F1 improvement of 1.3 points.
The overall results show that ELMo-based contextualized word embeddings help improve the baseline by 1.3 and 4.8 absolute F1 score on BioSemantics and Reaxys, respectively.
The overall results show that ELMo-based contextualized word embeddings help improve the baseline by 1.3 and 4.8 absolute F1 score on BioSemantics and Reaxys, respectively.
It shows the mean precision at one (P@1) for the DrQA baseline and BERT-large on the LAMA probe enriched with different kinds of contextual information. Enriching cloze-style questions with relevant context dramatically improves the performance of BERT: B-ora obtains ×7.4 improvement on Google-RE, ×1.9 on T-REx and ×3.5 on SQuAD with respect to using context-free questions (B). This clearly demonstrates BERT’s ability to successfully exploit the provided context and act as a machine reader model. Remarkably, no fine-tuning is required to trigger such behaviour. BERT is very robust, dropping only 0.5 P@1 on average from the zero context baseline. Using only one segment (that is, simply concatenating the input query and the context) leads to a severe drop of 12.4 P@1 for BERT (a 40.7% relative drop in performance). We also observe a consistent improvement in performance from one segment to two for retrieved and oracle contexts.
This table indicates that using both pseudo-tags and distinct vectors, which matches the setting of SingleEns, leads to the best performance, while the effect is limited or negative if we use pseudo-tags alone or distinct vectors and pseudo-tags without correspondence. Thus, this observation explains that the increase in performance can be attributed to the combinatorial use of pseudo-tags and distinct vectors, and not merely data augmentation. Note that the additional vectors by SingleEns are fixed in a small number K while those by Random noise are a large number of different vectors. Therefore, this observation supports our claim that the explicit definition of virtual models by distinct vectors has substantial positive effects that are mostly irrelevant to the effect of the random noise. This observation also supports the assumption that SingleEns is complementary to dropout. Dropout randomly uses sub-networks by stochastically omitting each hidden unit, which can be interpreted as a variant of Random noise. Moreover, it has no specific operations to define an explicitly prepared number of virtual models as SingleEns has. We conjecture that this difference yields the complementarity that our proposed method and dropout can co-exist.
inlinetodo: We can see that our method outperforms both variants of Morfessor, with an absolute gain of 8.5%, 5.1% and 5% in F-score on English, Turkish and Arabic, respectively. On Arabic, we obtain a 2.2% absolute improvement over Poon et al.’s model. AGMorph doesn’t segment better than Morfessor on English and Arabic but does very well on Turkish (60.9% F1 compared to our model’s 61.2%). This could be due to the fact that the Compounding grammar is well suited to the agglutinative morphology in Turkish and hence provides more gains than for English and Arabic. The Lee Segmenter (M2) performs the best on Arabic (82% F1), but lags behind on English and Turkish. This result is consistent with the fact that the system was optimized for Arabic.
Sentence Transformers, which begins with a pretrained transformer model and fine-tunes it on NLI datasets, scores approximately 10% lower on the probing tasks than the model it fine-tunes (see Supplementary Material, LABEL:sentence-transformers-tradeoff). In contrast, both DeCLUTR-small and DeCLUTR-base significantly boost downstream task performance while maintaining high probing task performance, producing the highest average score. Although our method sometimes underperforms existing supervised solutions on average downstream performance, we found that this is partially explained by the fact that these methods are trained on the SNLI corpus, which is included as a downstream evaluation task in SentEval. If the average downstream performance is computed without considering SNLI, the difference in downstream performance between our method and the supervised methods shrinks considerably (see Supplementary Material, LABEL: controlling-for-snli).
The two largest categories of propaganda techniques being used are loaded language and name calling/labeling (51% according to Da San Martino et al. Since these two techniques are also often used in the scenario of online harassment and trolling, we experiment with tools specialized in online harassment detection in order to provide with a baseline. Given an utterance and a defined attribute, the API returns a score between 0 to 1 as an estimate of the probability the utterance contains properties of the attribute. The attributes are toxicity, severe toxicity, identity attack, insult, profanity, threat, sexually explicit, flirtation, inflammatory, obscene, likely to reject (by New York Times moderators) and unsubstantial. The details of attributes’ definitions are described at Perspective API website. We aggregate these scores as sentence-level features and train a logistic regression on top of them to predict the likelihood of the sentence being propaganda. Given that Perspective API was created for an unrelated task, the performance is surprisingly high and likely results from a high proportion of certain types of propaganda techniques in the dataset.
IPP Problem. First, according to rows 1,2 and the last row, we find that if M2P2 does not use alignment losses and reference models, the MSE increases from 0.012 to 0.018 and 0.015 respectively. Second, we observe the power of the Multihead-attention Transformer encoder to handle long sequences, as the M2P2-LSTM model achieves the worst MSE amongst all methods. Third, we observe from rows 4-6 that the language modality is the most important in the prediction task, while the acoustic and visual modalities are less important. This observation is consistent with the modality concatenation weights that will be shown in the following subsection.
All models were trained on Wikipedia articles, split to paragraph-per-line. Texts were cleaned, tokenized, numbers were normalized by replacing each digit with “DG”, all types that occur less than 10 times were replaces by the “UNK” token, the data was not lowercased.
S5SS0SSS0Px3 Are we learning task-specific curricula? One way to assess whether we learn meaningful task-specific curriculum preferences is to compare curricula learned by one downstream task across different feature groups. If learned curricula are similar in, say, NER system, despite being optimized once using diversity features and once using prototypicality features—two disjoint feature sets—we can infer that the NER task prefers word embeddings learned from examples presented in a certain order, regardless of specific optimization features. For each downstream task, we thus measure Spearman’s rank correlation between the curricula optimized using diversity (d), or prototypicality (p), or simplicity (s) feature sets. Prior to measuring correlations, we remove duplicate lines from the training corpora.
Various state of the art models from the official leaderboard are also listed for reference. We can see that the performance of BERT is very strong. However, our model is more powerful, boosting the BERT baseline essentially. It also outperforms all the published works and achieves the 2nd place on the leaderboard when submitting SG-NET. We also find that adding an extra answer verifier module could yield better result, which is pre-trained only to determine whether question is answerable or not with the same training data as SG-Net. The logits of the verifier are weighted with scorena to give the final predictions.
, we compare our model with the following latest baselines: Dual Co-Matching Network (DCMN) Turkers is the performance of Amazon Turkers on a random subset of the RACE test set. Ceiling is the percentage of unambiguous questions in the test set. From the comparison, we can observe that our model outperforms all baselines, which verifies the effectiveness of our proposed syntax enhancement.
We observe that dual contextual attention yields the best performance. Adding extra vanilla attention gives no advance, indicating that introducing more parameters would not promote the strong baseline. It is reasonable that syntax-guided attention only is also trivial since it only considers the syntax related parts when calculating the attention, which is complementary to traditional attention mechanism with noisy but more diverse information and finally motivates the design of dual contextual layer.
From the table we can observe that Models using pre-trained word embeddings achieve a significant improvement as opposed to the ones using random embeddings. Models using GloVe embeddings outperforms using Word2Vec consistently for different neural models in different data sets. DeepMatch and Delta rely more heavily on pre-trained word embeddings compared to our proposed GRAPHENE. The reason why pre-trained word embeddings affect less on our GRAPHENE is probably due to the encoder-decoder part of GRAPHENE to generate title can provide training of more customized word embeddings on large-scale abstract-to-title samples.
Moreover, we also investigated the importance of pre-training via the query set of MeSH terms for biomedical literature retrieval. We have performed experiments without pre-training on ‘MeSH terms’ and directly trained the model on 50 clinical topics. Compared to DeepMatch, the performance of Delta and our proposed GRAPHENE have a greater dependency on the pre-training with large-scale data.
The LASs of our parser PCH,Global are 93.37% for PTB-YM and 87.58% for CTB5, which are higher than other parsers. UAS of PCH,Global is 88.89% for CTB5, which is lower than that of Sheng2014. However, since their parser employed the second order reranked model, its complexity is higher than O(n4) and its parsing speed is lower than that of PCH,Global. For PTB-YM, UAS of PCH,Global is 94.33%, which is higher than that of other parsers. On the other hand, the complexities of PBase,Global and PCH,Global are higher than O(n3) due to the global scorer. Since the global scorer is implemented as a first-order graph-based decoder, it needs to evaluate ( Thus, the complexity of the decoding in the integrating parser is 2n⋅O(n3). Compared with the calculation of the neural networks, the time required by the global scorer, maximum spanning tree (MST) searching, is relatively short as stated by Cheng et al. The evaluation indicates that PBase,Global and PCH,Global can be accelerated by using more efficient implementation for the constraints. Thus the complexity of the integrating parser will be smaller than O(n3). Thus, the scorer is independent of context enhancement, which indicates that the global scorer is an effective framework and can be integrated into other algorithms to improve their performance. Furthermore, we ignore the dependency labels in the scorer. Therefore, the parsers PBase,Global and PCH,Global can be improved further by utilizing the better parser and the dependency labels in the global scorer.
Since the global scorer is based on the graph-based parser, the parsers with it can be beneficial from graph-based parsing by searching the best dependency tree globally. Context enhancement increases F1 scores of binned distances in PTB-YM and CTB5. Moreover, The global scorer improves the F1 scores of each binned distance except ROOT, and the scores increase as much as 2.23% for PTB-YM and as much as 1.51% for CTB5. However, the score of ROOT fluctuates, which may be caused by error propagation in the transition-based system while the graph-based system tries to correct them.
In addition to the end-to-end results, we also report how often the gold answer appears in the top-5 retrievals before applying any fine-tuning. The latter metric more significantly isolates the contribution of improving the retriever during pre-training.
We also investigate the effect of the data size of bridging corpora on the likelihood connection. When more than 50K sentence pairs are added, the further improvements become modest. This finding suggests that a small corpus suffices to enable the likelihood connection to reach the reasonable performance.
For unacceptable translations that will be processed by the handler module, we note that TN > FN. This means the majority of detected-as-unacceptable translations are truly unacceptable (i.e. the labels they receive from the downstream system would be different than their references). Therefore, when the downstream task is binary classification (subjectivity classification), flipping their labels is a better strategy than passing them untouched as in the original cross-lingual pipeline.
LN improved the performance of the base model by around 3%. The use of Focal Loss also increased the accuracy by 1−3% but major improvement was achieved by replacing greedy decoding with beam search which boosted the model accuracy by 4−5%.
This is because for semantically same words (e.g., weather and clima) LVM considers such close enough points as the same distribution, but CRF is more likely to classify them differently. In addition, we can see that adding only Gaussian noise to the Vanilla BiLSTM improves our prediction performance significantly, which implies that the robustness of our model towards the noisy signals which come from the target embedding inputs.
By removing or replacing LVM with MLP, we can see the clear performance gains by using LVM.
In [zhang2015character], four largest classes, namely World, Sports, Business, and Science/Technology, are selected from this corpus. Each sample is constructed by joining the title and description fields. The DBpedia dataset uses a large multi-domain ontology which has been derived from Wikipedia [lehmann2015dbpedia]. The DBpedia ontology dataset is constructed by picking 14 non-overlapping classes: Company, Educational Institution, Artist, Athlete, Office Holder, Mean Of Transportation, Building, Natural Place, Village, Animal, Plant, Album, Film, Written Work. For each of these 14 ontology classes, the fields of samples we used are the joint of the title and abstract of each Wikipedia article. Yelp reviews. The Yelp reviews dataset is obtained from the Yelp Dataset Challenge in 2015. Each review of this dataset has one user’s review score ranging from 1 star to 5 stars. Predicting the number of users’ review stars corresponds to a 5-class classification task. Yahoo Answers corpus. This corpus is extracted from the Yahoo! Answers Comprehensive Questions and Answers version 1.0. We follow [zhang2015character] to construct a topic classification dataset from this corpus by selecting 10 main categories: Society, Culture Science, Mathematics Education, Reference Computers, Internet Sports Business, Finance, Entertainment, Music Family, Relationships Politics, Government . The Amazon reviews dataset. We obtain the Amazon review dataset from the Stanford Network Analysis Project (SNAP), which spans over 18 years with 34,686,770 reviews from 6,643,669 users on 2,441,053 products [mcauley2013hidden]. Different from the Yelp review dataset, we predict the binary sentiment label for each review in the Amazon dataset. The sentiment classes of reviews with 1 or 2 stars are labelled as negative, and those with 3 or 4 stars are labelled as positive.
The Google News Word2Vec model comes pretrained with vector size of 300, window 10, minimum word frequency of 10 and skip-gram 1. We started with training our three Word2Vec models using the same parameters. We can see that training on Qatar Living Forum data performs best followed by using Qatar Living Forum+Ext, Google News, and Doha News. This is not surprising as the first two datasets are in-domain, while the latter two cover more topics (as they are news) and more formal language. Overall, Doha News contains topics that largely overlap with the topics discussed in the Qatar Living forum; yet, it uses more formal language and contains very little conversational word types (mostly in quotations and interviews); moreover, being smaller in size, it covers much less vocabulary. Based on these preliminary experiments on Dev2016, we concluded that the domain-specific word vectors trained on Qatar Living Forum were the best for this task, and we used them further in our experiments.
For the baseline system using ME classifier, we show two results. One is using the named entities obtained from the Stanford coreNLP tool, and the other one uses ground truth NER labels. It is clear that ground truth NER information can boost the performance considerably. However, such accurate information is very hard to get in real world cases.
We find all points move towards performance point of the training data. Our model even achieves the BLEU score verse self-BLEU score almost equal to scores of the training data at temperature 1.2. This highlights the effectiveness of our method. As the acceptance ratio decreases, the value of uc increases. Once the filter is added, the error rate decrease. Further, the error rate increases with the decrease of c.
In both datasets, the joint model significantly outperforms baselines with mixed-speech input. In particular, the alternated training reaches better results in GRID while in TCD-TIMIT it is slightly outperformed by the alternated two full phases training with weight freezing. that show PIT performs worse than audio-visual counterparts.
4.3.2 Relational Information We manually examine all test instances with the “entailment” label in MedNLI, and found 78 token pairs across the premises and hypotheses which strongly suggest entailment. Among them, 22 are disease-symptom pairs, 13 are disease-drug pairs, 19 are numbers and their indications (e.g.: 150/93 and hypertension) and 24 are synonyms or closely related concepts (e.g.: Lasix® and diuretic). We hypothesize that a model is required to encode relation information to perform well in MedNLI task. The trends of subset accuracy moderately correlate with the NN proportions (Pearson correlation coefficient r=0.52).
We tune the history window size w for all models that consider history and report their performances under the best history setting. The best history settings for DrQA, BERTserini, and Ours are w=5, 2, and 6 respectively. We summarize our observations as follows: [leftmargin=1.5em] We observe that DrQA has poor performance. The main reason for this lies in the reader component. The RNN based reader in DrQA cannot produce representations that are as good as the readers based on a pretrained BERT in the rest of the competing models. More importantly, the DrQA reader cannot handle unanswerable questions natively. BERTserini has a significant improvement over DrQA and serves as a much stronger baseline. It addresses the issues in DrQA by using a BERT reader that can handle unanswerable questions. BM25 in Anserini also gives better retrieval performance. Our model without any history manages to perform on par with BERTserini that considers history on the test set. In particular, our learned retriever achieves higher performance on retrieval metrics. Since our reader is similar to that of BERTserini, the overall performance gain mostly comes from our learned retriever. This verifies the observation in Lee et al. The margins are substantially larger on the development set, presumably because the best pretrained retriever model is selected based on the development performance. Our model with history obtains statistically significant improvement over the strongest baseline with p<0.05 tested by the Student’s paired t-test. This demonstrates the effectiveness of our model. This also indicates that incorporating conversation history is essential for ORConvQA, as expected. In addition, we observe that the reranker consistently outperforms the retriever. This suggests that although reranking is more expensive as it jointly models the question and the passage, it enjoys better performance than the retriever that models the question and the passage separately. This model performance is closely related to several design choices we made. Specifically, we have three ablation settings as follows. [leftmargin=1em] ORConvQA w/o reranker. We introduce reranking to the system as one of the differences from previous works ( ORConvQA w/o learned retriever. We replace our learned retriever with DrQA’s TF-IDF retriever. ORConvQA w/o first question (q) for retriever. We do not manually include the first question of a dialog in the reformatted question for the retriever.
According to this analysis, the annotation changed very little: the class with the largest relative change was organizations with a mere increase of 27 entities corresponding to 0.22 percentage points. This indicates that the annotators in general agreed on the annotation in the vast majority of cases.
As expected, Exact Match is vastly outperformed by machine learning methods, while Logistic Regression is also unable to cope with the complexity of xmtc. Again, replacing the vanilla cnn of z-cnn-lwan with a bigru (z-bigru-lwan) improves performance across all label types and measures. This behavior is not surprising, because the training objective, minimizing binary cross-entropy across all labels, largely ignores infrequent labels. The zero-shot versions of cnn-lwan and bigru-lwan outperform all other methods on zero-shot labels, in line with the findings of Rios and Kavuluru Exact Match also performs better than most other methods (excluding z-cnn-lwan and z-bigru-lwan) on zero-shot labels, because it exploits label descriptors. To better support all types of labels (frequent, few-shot, zero-shot), we propose an ensemble of bigru-lwan and z-bigru-lwan, which outputs the predictions of bigru-lwan for frequent and few-shot labels, along with the predictions of z-bigru-lwan for zero-shot labels.
To evaluate the performance of our model, we compare its performance with various baseline models. From unseen and real online search click log, we collect millions of query-entity pairs as our test set (ground truth set). Note that, there are no other baselines of entity recommendation for complex queries with no entities at all. att-BiLSTM is slightly better than +ngram. The reasons are mainly that a certain percentage of queries is without order and ngram is enough to provide useful information.
We find that fine-tuning the model pretrained with unigram LM tokenization produces better performance than fine-tuning the model pretrained with BPE tokenization for all tasks, with larger performance gaps on SQuAD and MNLI. Segmentation of names is largely independent of morphology, which we hypothesize is responsible for the subtler performance difference for named-entity recognition.
Our full model as well as the variations that ablated some components improve over the state-of-the-art Mishra et al. For grammaticality, we observe that the Turkers scored shorter sentences higher (e.g., RV), which also explains why NoRV model received a higher score than the full model. NoRV otherwise performed worse than all the other variations.
Given a pair of inputs, we decide win/lose/tie by comparing the average scores (over three Turkers) of both outputs. We see that FM dominates \newciteabhijit on all the metrics and human-generated sarcasm on the creativity metric. For sarcasticness, although humans are better, the FM model still has a 34% winning rate.
Many learners perform well on the task with respect to task complexity. In particular, simple algorithms such as GaussianNB or linear models perform well. More complex methods such as ensemble methods generally are top performers. The best performing method on SIM and GRO2 is the multi-layer perceptron MLP. On GRO1 GradientBoosting is the front runner. Although ensemble methods generally perform quite similar. None of the methods fail catastrophically, which is mostly due to hyper-parameter optimization.
The generated rationales were shuffled randomly to hide which rationale came from which model. We then asked them to choose which of the two candidate rationales better explains the answer in the given sample. Humans consistently rate the rationales generated by ViLBERT-Ra as better explanations for the answers.
We evaluate the performance of language generation on PTB in Tab. The test set of PTB is also included for comparison of text fluency. We first present the ablation study to show the contribution of dispersion term and mutual information term. Results are shown in Tab. First of all, because extra bias is introduced in training objectives (higher mutual information or higher dispersion), the optimization object is no longer the lower bound of log-likelihood. As a result, the NLL will be worse with extra mutual information or dispersion term. Adding an extra dispersion term helps to alleviate the mode-collapse, according to the higher variance value and mutual information compared with vanilla GM-VAE. Mutual information term helps to increase the information encoded by discrete latent variables, according to higher mutual information. Both terms improve the reconstruction performance and generation quality. With the presence of both continuous and discrete latent variables, DGM-VAE enjoys its higher model capacity and gives the best reconstruction performance (BLEU), superior to other VAE variants. Although semi-VAE also includes discrete and continuous latent variables, it fails to make use of both of them because of the independent hypothesis. As shown in Tab. AE could reproduce input sentences well, but it is not a true generative model and fails to generate diverse sentences.
Unsupervised text generation: DGM-VAE obtains the best performance in interpretability and reconstruction. In this experiment, we show the ability of DGM-VAE to encode the dialog action and emotion information in latent variables unsupervisedly. Because utterances in DD are annotated with Action and Emotion labels, we evaluate the ability of DGM-VAE (β=0.9) to unsupervisedly capture these latent attributes on DD. We take the index i with the largest posterior probability qϕ(c=i|x) as latent action labels. Following Zhao et al. The number of our labels is 125. Results of homogeneity of action (act) and emotion (em) together with MI term and BLEU are shown in Tab. It shows that DGM-VAE outperforms other VAEs in reconstruction and gives the best homogeneity on both the action and emotion.
Experimental results of baselines and DCM-VAE (β=0.4) are shown in Tab. The classification accuracy of action and emotion is calculated by the corresponding posterior network. DCM-VAE could alleviate the mode-collapse effectively and obtains the best performance in multi-label classification accuracy with slightly worse NLL. It demonstrates that under supervision, DCM-VAE can inject the interpretable properties in latent variables well. Mode collapse occurs in vanilla CM-VAE, making its performance not better than baseline. DCM-VAE is able to make full use of multiple modes and capture the dependency of variables, which improves the performance of multi-label classification.
The first row refers to the complete architecture, while –SA shows the results of HERMIT without the self-attention mechanism. Then, from this latter we further remove shortcut connections (– SA/CN) and CRF taggers (– SA/CRF). The last row (– SA/CN/CRF) shows the results of a simple architecture, without self-attention, shortcuts, and CRF. Though not significant, the contribution of the several architectural components can be observed. The contribution of self-attention is distributed across all the tasks, with a small inclination towards the upstream ones. This means that while the entity tagging task is mostly lexicon independent, it is easier to identify pivoting keywords for predicting the intent, e.g. the verb “schedule” triggering the calendar_set_event intent. The impact of shortcut connections is more evident on entity tagging. In fact, the effect provided by shortcut connections is that the information flowing throughout the hierarchical architecture allows higher layers to encode richer representations (i.e., original word embeddings + latent semantics from the previous task). Conversely, the presence of the CRF tagger affects mainly the lower levels of the hierarchical architecture. This is not probably due to their position in the hierarchy, but to the way the tasks have been designed. In fact, while the span of an entity is expected to cover few tokens, in intent recognition (i.e., a combination of Scenario and Action recognition) the span always covers all the tokens of an utterance. CRF therefore preserves consistency of IOB2 sequences structure. However, HERMIT seems to be the most stable architecture, both in terms of standard deviation and task performance, with a good balance between intent and entity recognition.
To measure the quality of the clusterings obtained by these models, we compare the average weighted and unweighted F-measures for 55 TREC topics, using the evaluation scripts from the TREC TTG task. The online model has the best weighted F1 score, outperforming the offline version of the same model, even though its inference procedure is an approximation to the offline model. It may be that its approximate inference procedure discourages long-range linkages, thus placing a greater emphasis on the temporal dimension. Both models were trained over 500 iterations, and the online model was 30% faster to train than the offline model. Compared to the other 2014 TREC TTG systems, our dd-CRP models are competitive. Both models outperform all but one of the fourteen submissions on the unweighted F1 metric, and would have placed fourth on the weighted Fw1 metric. Note that the TREC evaluation scores both clustering quality and retrieval. We use only the baseline retrieval model, which achieved a mean average precision of 0.31. Bayesian dd-CRP storyline clustering was competitive with these timeline generation systems despite employing a far worse retrieval model, so improving the retrieval model to achieve parity with these alternative systems seems the most straightforward path towards better overall performance.
For TextC, GRU performs best on SentiC and comparably with CNN in RC. For SemMatch, CNN performs best on AS and QRM while GRU (and also LSTM) outperforms CNN on TE. For SeqOrder (PQA), both GRU and LSTM outperform CNN. For ContextDep (POS tagging), CNN outperforms one-directional RNNs, but lags behind bi-directional RNNs.
Entity typing is the process of assigning a type to an entity and is a fundamental task in KG completion. For example, the triple   states that Albert Einstein is assigned to the type class Scientist. The type information in DBpedia is derived directly by an external extraction framework from the Wikipedia infobox types. Since, the Wikipedia is a crowd sourced encyclopedia, hence this type information is often incomplete. Therefore, a huge number of entities in DBpedia are assigned to a coarse grained rdf:type. For e.g., class dbo:SportsTeam has 14 subclasses in DBpedia and 352006 entities, out of which only 8.9% are assigned to its subclasses. Hence, there arouses a necessity to have fine grained types for the entities in the KGs. On the other hand, most of the existing state-of-the-art KG embedding approaches such as translational approaches such as TransE [bordes2013translating], TransR [lin2015learning], etc. This subgraph depicts, the birthplace of \sayAlbert Einstein is \sayUlm, which is located in the country \sayGermany. The labels of the triples in the subgraph, such as birthplace, country, Ulm, etc. contains implicit textual information in the graph, that is not captured in translational embedding models.
In connected letter recognition, we measure performance via the letter accuracy, analogously to the word or phone accuracy in speech recognition. The tuned models are evaluated on an unseen 10% of the test signer’s remaining data; finally, we repeat this for eight choices of tuning and test sets, covering the 80% of the test signer’s data that we do not use for adaptation, and report the mean letter accuracy over the test sets. With adaptation, however, performance jumps to up to 69.7% letter accuracy with forced-alignment adaptation labels and up to 82.7% accuracy with ground-truth adaptation labels. All of the adapted models perform similarly, but interestingly, the first-pass SCRF is slightly worse than the others before adaptation and better (by 4.4% absolute) after ground-truth adaptation. One hypothesis is that the first-pass SCRF is more dependent on the DNN performance, while the tandem model uses the original image features and the rescoring SCRF uses the tandem model hypotheses and scores. Once the DNNs are adapted, however, the first-pass SCRF outperforms the other models.
We report accuracies on the entirety of test and control (“all”), as well as separately on the part of control where the target word is in the context (“context”). The first part of the table shows results from \newcitelambada:16. We then show our baselines that choose a word from the context. Choosing the most frequent yields a surprisingly high accuracy of 11.7%, which is better than all results from Paperno et al. One annotator, a native English speaker, sampled 100 instances randomly from dev, hid the final word, and attempted to guess it from the context and target sentence. The annotator was correct in 86 cases. For the subset that contained the answer in the context, the annotator was correct in 79 of 87 cases. The annotator did the same on 100 instances randomly sampled from control, guessing correctly in 36 cases. The annotator was correct on 6 of the 12 control instances in which the answer was contained in the context. We find that the ranking of systems according to this column is similar to that in the test column.
For the pre–trained word embeddings, Paragram-SL999 outperformed GloVe and Word2Vec on most metrics. Both retrofitting and counterfitting procedures show better or at par performance on all datasets except for WordSim-353. Addition of affect information to different versions of GloVe consistently improves performance whereas the only significant improvement for Paragram-SL999 variants is observed on the SimLex-999 and SimVerb-3500 datasets. represents the current state–of–the–art for SimLex-999 and inclusion of affect information to these embeddings yields higher performance (ρ=0.75). Amongst Affect-APPEND and Affect-STRENGTH, Affect-APPEND out performs the rest in most cases for GloVe and Word2vec. However, Affect-STRENGTH variations perform slightly better for the retrofitted Paragram embeddings.
We report the performance for GloVe and Word2Vec with Affect-APPEND variants. For FFP-Prediction, Affect-APPEND reports the lowest Mean Squared Error for Frustration and Politeness. However, in the case of Formality, the counterfitting variant reports the lowest error. For the personality detection, Affect-APPEND variants report best performance for NEU, AGR, and OPEN classes. Evaluation against the Sentiment Analysis(SA) task shows that Affect-APPEND variants report highest accuracies. The final experiment reported here is the WASSA-EmoInt task. Affect-APPEND and retrofit variants out perform the vanilla embeddings.
To summarize, the extrinsic evaluation supports the hypothesis that affect–enriched embeddings improve performance for all NLP tasks. Further, the word similarity metrics show that Aff2Vec is not specific to sentiment or affect–related tasks but is at par with accepted embedding quality metrics. Qualitative Evaluation: Note that lower the noise better the performance. The Affect-APPEND report the lowest noise for both cases. This shows that the introduction of affect dimensions in the word distributions intuitively captures psycholinguistic and in particular polarity properties in the vocabulary space. The rate of change of noise with varying k provides insights into (1) how similar are the embedding spaces and (2) how robust are the new representations to the noise - how well is the affect captured in the new embeddings. Noise@k for the Aff2Vec i.e. the Affect-APPEND variants, specifically, ⊕Affect and Couterfitting⊕Affect has lower noise even for a higher k. The growth rate for all variants is similar and reduces with an increase in the value of k. A similar behavior is observed for Polarity-Noise@k.
Again, the number of images for which participants’ preferences are not significant is high: 28% of the organic images, 47% of the man-made images and 56% of the animal images. However, when participants do show significant preference, in the large majority of cases it is in favor of the correct macro-category: this is so for 98% of the organic images (70.5% of total), 90% of the man-made images (48% of total), and 59% of the animal ones (25.7% of total). Confusions arise where one would expect them: both man-made and animal images are more often confused with organic things than with each other.
We calculate the agreement along two dimensions, namely unlabelled vs. labeled and instance vs. graph-level. In the Inst. labelled setting, we accept an instance being labeled as true positive if both annotators marked the same characters as experiencer and cause of an emotion and classified their interaction with the same emotion. In the Inst. unlabelled case, the emotion label is allowed to be different. On the graph level (Graph labelled and Graph unlabelled), the evaluation is performed on an aggregated graph of interacting characters, i.e., a relation is accepted by one annotator if the other annotator marked the same interaction somewhere in the text. We use the F1 score to be able to measure the agreement between two annotators on the span levels. For that, we treat the annotations from one annotator in the pair as correct and the annotations from the other as predicted. The values for graph-labelled agreement are more relevant for our use-case of network generation. The values are higher (66–93 %), showing that annotators agree when it comes to detecting relationships regardless of where exactly in the text they appear.
To demonstrate the effectiveness of our model architecture especially on domains with insufficient data and/or We use classification accuracy as our main evaluation metric. the baseline models achieve very poor accuracy especially when leveraging available data in other locales is of critical importance or when there needs to selectively share knowledge depending on the locale-specificity of a domain. If a model that shares knowledge across locales does not handle locale-specific domains carefully, its performance would deteriorate due to confusion on locale-specific patterns. The ‘constrained’ model uses a shared encoder and allows locales to shares its prediction layer, but it does not determine whether or not to share knowledge for each domain. As a result, its classification accuracy is only 44% for locale-specific domains and 25% for single-locale domains in the IN dataset with lack of data. Also, ‘single’ and ‘union’ models do not have any chance to learn a joint representation while sharing knowledge and thus they totally fail to make predictions correctly for locale-specific, single-locale, and small domains. In contrast, our universal model is very robust to domains with insufficient data and domains with locale-specific patterns over all locales. It proves that our approach is very effective for capturing both global and local patterns by selectively sharing domain knowledge across locales. Also, the adversarial locale prediction is only helpful for locale-specific and single-locale domains. That is probably because the effect of adversarial loss paradoxically makes the model rely on only the locale-specific encoders which are well-optimized for locale-specific/single-locale domains. There needs deep analysis about why it does not affect the GB locale, and we leave it as future works. The dimensionality of each hidden output of LSTMs is 100 for both the shared encoder Fs and the locale-specific encoder Fli, and the hidden outputs of both forward LSTM and backward LSTM are concatenated, thereby the output of each BLSTM for each time step is 200. The inputs and the outputs of the BLSTMs are regularized with dropout rate 0.5
We can see that on average, the sparse models (csparsemax as well as sparsemax combined with coverage models) have higher scores on both BLEU and METEOR. Generally, they also obtain better REP and DROP scores than csoftmax and softmax, which suggests that sparse attention alleviates the problem of coverage to some extent.
We see that the Predicted strategy outperforms the others both in terms of BLEU and METEOR, albeit slightly.
In terms of AUC and accuracy, the neural network outperformed the previous models. Results utilizing decision trees and regression based models were not accurate enough for classifying this particular dataset. Instead the very structure and semantic meaning needed to be explored for significant conclusions. All classifiers were trained on a similar sized dataset in order to be properly compared against the neural network-based classifier.
The baseline system is equipped with lexicalized and OSM model trained over word forms using count-based/n-gram-based models. We see that adding OSM models trained over generalized representation such as POS tags help slightly (+0.2 BLEU improvement in DE-EN and +0.3 in EN-DE). Using word clusters instead of POS tags did not help as much.
We see that our model leads to a higher fraction of AAE posts in summary in most cases, compared to just the blackbox algorithm. However, α=0.5 is not always the ideal choice; eg, for keyword “funny” and Hybrid-TF-IDF or TextRank as the blackbox, the dialect diversity of our model is less than the dialect diversity of the summary from just Hybrid-TF-ID or TextRank. The ROUGE scores for TwitterAAE As expected, the similarity between the summary generated by our model and summary generated by Centroid-Word2vec decreases as the α-value increases. For summary size 200, the ROUGE-1 F-score between the compared summaries is greater than 0.7, implying significant word-overlap between the two summaries. Evaluation 2, if the diversity correction required is small, then the recall scores tend to be large. For Centroid-Word2Vec, the recall is greater than 0.64, implying that the summary from our algorithm covers atleast 64% of the words in the summary of the blackbox algorithm. However, in the cases when the summaries generated by blackbox algorithm originally have low dialect diversity, the recall scores tend to be small (for example, LexRank-balanced). In these cases, a larger deviation from the original summaries is necessary to ensure sufficient dialect diversity. With respect to the ROUGE-assessment, note that this does not necessarily quantify the usability or the accuracy of the summaries generated by our model; this measure simply looks at the amount of deviation from summaries of standard algorithms.
We performed 10-fold cross validation for each of the platforms and pairwise t-tests to compare the mean F-scores of every pair of platforms. Watson F1 score (0.882) is significantly higher than other platforms (p<0.05, with large or very large effects sizes - Cohen’s D). However, for Entities, Watson achieves significantly lower F1 scores (p<0.05, with large or very large effects sizes - Cohen’s D) due to its very low Precision. One explanation for this is the high number of Entity candidates produced in its predictions, leading to a high number of False Positives It also shows that there are significant differences for Entity F1 score between Dialogflow, LUIS and Rasa. LUIS achieved the top F1 score (0.777) on Entities. - Cohen’s D) due to its lower entity score as discussed above. The significance test shows no significant differences between Dialogflow, LUIS and Rasa.
We performed 10-fold cross validation for each of the platforms and pairwise t-tests to compare the mean F-scores of every pair of platforms. Watson F1 score (0.882) is significantly higher than other platforms (p<0.05, with large or very large effects sizes - Cohen’s D). However, for Entities, Watson achieves significantly lower F1 scores (p<0.05, with large or very large effects sizes - Cohen’s D) due to its very low Precision. One explanation for this is the high number of Entity candidates produced in its predictions, leading to a high number of False Positives It also shows that there are significant differences for Entity F1 score between Dialogflow, LUIS and Rasa. LUIS achieved the top F1 score (0.777) on Entities. - Cohen’s D) due to its lower entity score as discussed above. The significance test shows no significant differences between Dialogflow, LUIS and Rasa.
The best performing instance of PoE significantly outperforms the approaches Concat and Ensemble. This validates the choice of the PoE approach, which can incorporate data modalities to the link prediction problem in a principled manner.
[clef:07] shows improvements over the dictionary since the OOV terms are transliterated and multiple dictionary translations are disambiguated using the contextual cues from the corpus, however it is not able to perform better than the monolingual baseline.
Among the proposed approaches, SIM Vec (max) seems to perform the best on both the datasets. An issue that comes up while using the embedding based methods is whether to include the embeddings of the named entities in the process. For a particular word in the source language w, similar words that showed up are relevant to w but are not translations. For example, the word BJP in Hindi (which is an Indian political party) the words that were most similar also included the names of other political parties like Congress and also words like Parliament and government in the target language English.
For SIM Vec, we experimented with both the ‘Sum’ and ‘Max’ functions. After doing an analysis on the queries returned by the sum function, we found that those words that are related to the meaning of the entire query come up, while in max, words that have high similarity with one of the query terms, come up in the translation. For the first example, ‘sum’ could not retrieve words like ‘assault’ and ‘attack’, because these were similar only to one query term, ‘hamalaa’, but not the others.
We see that WE, when combined with DT, retrieves many relevant terms, which improve the performance.
To diminish the impact of random seeds and local minima in neural networks, results are averaged across 5 runs. This helps test whether the models predict above chance performance. When using short snippets (size=32), disparate models such as our mlr, mlp and lstms achieve a similar performance.
Considering the non-PCA cases, we can see that idf-center and idf-delta cross between 4 and 5 reviews, with idf-sum sandwiched in between. around 450 words is the crossover point with idf-center better on shorter documents.
S4SS1SSS0Px2 Text8 The first 90M characters are used for training, the next 5M for validation, and the final 5M for testing, resulting in 15.3M training tokens, 848K validation tokens, and 855K test tokens. We preprocess the data by mapping all words which appear 10 or fewer times to the unknown token, resulting in a 42K size vocabulary. Other parameter settings are the same as described in the Penn Treebank experiments, besides that only models with hidden size 512 are considered, and noising is not combined with feed-forward dropout.
In our approach, the thought vector inferred from the LSTM encoding of sentence n−1 is used as a feature for the LSTM for sentence n, in a recurrent fashion. Note that the LSTMs for each sentence in But we found that directly adding the topic of the previous sentence to all the LSTM cells of the current sentence is beneficial, since it constraints all the current LSTM cells during training and explicitly adds a bias to the model. Our experiments showed that it’s beneficial to denoise the thought vector signal using a low-dimensional embedding, by adding roundoff-based projection. Initial experiments using thought vector for sentence-topic prediction look promising. A CLSTM model that used word along with thought vector (PrevSentThought feature in the model) from the previous sentence as features gave a 3% improvement in perplexity compared to a baseline LSTM model that used only words as features.
Note that during training, we only used open-sourced iOS data, while evaluated these models on dev and test sets for all 3 channels (Android, Mic, and iOS). System performance on iOS outperformed Android and Mic, which is as expected due to better acoustic channel condition matching.
Our baseline neural semantic parser (NSP) takes on average 0.260 seconds to predict the LF for a given query. When we use the model that integrates the LF grammar but constructs the reduced matrices on the fly (GSP-G), we find that despite the reduction of average permissible tokens (from 56209 to 6336), the prediction time actually increases drastically to 4.416 seconds. The best prediction time (0.067 second per query) is achieved by NSP-GC when the grammar is used at every step. But similar speed-ups can be achieved when we are using cached matrices only for states with a small nextTokens set.
The experiments in this paper are limited to the setting where the number of supervised and deployment examples are on the same order of magnitude; however, we envision scenarios in which the number of deployment examples can easily grow to 100× or more the number of supervised examples over the chatbot’s deployment lifetime, effectively providing a massive task-specific corpus at minimal cost.
As expected the linear regression line of the median (2nd quartile) is not at the middle of Q1 and Q3 regression lines because the utterance length distributions are skewed. The shortening of utterances is, therefore, mostly due to the decreased occurrence of longer utterance lengths rather than the shifting of the whole utterance length distributions to the left. Similarly, the number of utterances per day and the percent of public data collected are not constant throughout the entire dataset. To remove any size effects on the results, 105 utterances, an amount slightly smaller than the smallest daily sample size, were sampled without replacement for each day (Fig. but the same observations remained ( Another possible reason for the shortening is the increased usage of link shorteners. The median word length of all words is 4 characters (Fig. for all years from 2009 to 2012. Based on Kruskal-Wallis tests, the word length distributions of the 1000 most frequently used words for 2010–2012 are not significantly different (H=1.112, p=0.5734) with each other but are significantly different with the distribution for 2009 (H=10.31, p=0.0161). However, the observed shortening is not just due to a sudden shortening of the 1000 most frequently occurring words from 2009 to 2010 because it was still observed in 2010–2012 A topic is a word, usually in the form of #topic, or a phrase that is contained in a tweet. Trending topics are the most prominent topics being talked about in Twitter within a period of time. The shortening of trending topics could potentially explain the observed shortening of utterances but instead of decreasing, the median length of trending topics increased from 11 characters in 2009 to 13 characters in 2012 (Fig. The shortening of utterances is a global phenomenon and is not restricted to the US since utterances that were geolocated outside the US also exhibited shortening -0.266 char./year in books; -0.001897 char./year in movies), respectively. Although conversations do tend to get shorter in time, our current findings show that it is occurring faster now on Twitter.
To get a rough estimate of what is actually achievable in terms of the final ROUGE-1 F score we looked at different “upper bounds” under various scenarios First, ROUGE score is computed by using four manual summaries from different assessors, so that we can estimate inter-subject disagreement. It provides a reasonable estimate of human performance. Second, in extractive summarization we restrict summaries to sentences from the documents themselves, which is likely to lead to a reduction in ROUGE. To estimate this drop, we use the greedy algorithm to select the extractive summary that maximizes ROUGE on the test documents. Third, we expect some drop in performance, since our model may not be able to fit the optimal extractive summaries due to a lack of expressiveness. On both datasets, we see a drop of about 5 points of ROUGE performance. Adding more and better features might help the model fit the data better. Finally, a last drop in performance may come from overfitting. Note that the drop between training and test performance is rather small, so overfitting is not an issue and is well controlled in our algorithm. We therefore conclude that increasing model fidelity seems like a promising direction for further improvements.
To understand which features affected the final performance of our approach, we assessed the strength of each set of our features.
As training data we used a subset of the Wall Street Journal (WSJ) from the Penn English Treebank-3 restricted to sentences of the length ⩽ 20. We used the standard data split: sections 2-21 of WSJ for training (16667 sentences), section 22 for validation (725 sentences) and section 23 for testing (1034 sentences). The parse trees were preprocessed in the standard way by removing functional tags and null elements. The regularization hyperparameter C was chosen by cross-validation over a grid of values {10i: i=0,1,...5}. The first column describes the measure we optimized during the training procedure. Here we can make two observations. First, we see that there is a little difference in performance between #FP bin.) and F1-score (bin.) (see Proposition 2). Second, we see that adjusting training with F1-score for unbinarized trees improves the resulting performance upon training with binarized representation. this result is statically significant. A corresponding null-hypothesis is that the measurement difference for a pair of methods follows a symmetric distribution around zero.
We use the notation fMD to denote that the textual entailment model in use is M which can be either ESIM or BERT and the model M is trained on the dataset D which can be any of following: TrainQUARELGiven, TrainQUARELGiven∪TrainSNLI, TrainQUARELClaim, TrainQUARELClaim∪TrainSNLI. Correspondingly there are a total of 4 possible values for fgivenTE namely fESIMTrainQUARELFact, fBERTTrainQUARELGiven, fESIMTrainQUARELGiven∪TrainSNLI and fBERTTrainQUARELFact∪TrainSNLI. Similarly, there are a total of 4 possible values for fclaimTE namely fESIMTrainQUARELclaim, fBERTTrainQUARELclaim, fESIMTrainQUARELclaim∪TrainSNLI and fBERTTrainQUARELFact∪TrainSNLI.
Elicited vs Non-Elicited Data Generally, when gender demographics are provided, we observe the following distribution: out of the 6,072 speakers, 3,050 are women and 3,022 are men, so parity is almost achieved. We then look at whether data was elicited or not, non-elicited speech being speech that would have existed without the corpus creation such as TEDTalks, interviews, radio broadcast and so on. We assume that if data was not elicited, gender imbalance might emerge. This disparity of gender representation in French media Our expectations are reinforced by examples such as the resource of Spanish TEDTalks, which states in its description regarding the speakers that “most of them are men” \citelanguageresourcemena_2019.
“How Can I Help?” : Spoken Language Tasks Speech corpora are built in order to train systems, most of the time ASR or TTS ones. We observe that if gender representation is almost balanced within ASR corpora, women are better represented in TTS-oriented data sets. This gendered design of vocal assistants is sometimes justified by relying on gender stereotypes such as “female voices are perceived as more helpful, sympathetic or pleasant.” TTS systems being often used to create such assistants, we can assume that using female voices has become general practice to ensure the adoption of the system by the users. This claim can however be nuanced by \newcitenass2005wired who showed that other factors might be worth taking into account to design gendered voices, such as social identification and cultural gender stereotypes.
The metric used for comparison is ’AvgCos’. It can be seen that for most of the datasets, GM_KL achieves significantly better correlation score than w2g and w2gm approaches. Other datasets such as MC and RW consist of only a single sense, and hence w2g model performs better and GM_KL achieves next better performance. The YP dataset have multiple senses but does not contain entailed data and hence could not make use of entailment benefits of GM_KL.
The ’MaxCos’ similarity metric is used for evaluation and the best precision and best F1-score is shown, by picking the optimal threshold. Overall, GM_KL performs better than both w2g and w2gm approaches.
The main evaluation over the test data is based on the best performing SVR and the two BLSTM models once trained on all of the training data. which we term metric 2; this is what the first version of the results were actually based on, where we were ranked 4th), which then changed by the organisers to their equation as presented in Cortis et al. (which we term metric 3 and what the second version of the results were based on, where we were ranked 5th). This is due to the system’s optimisation being based on metric 1 rather than 2. Metric 2 is a classification metric for sentences with one aspect as it penalises values that are of opposite sign (giving -1 score) and rewards values with the same sign (giving +1 score). Our systems are not optimised for this because it would predict scores of -0.01 and true value of 0.01 as very close (within vector of other results) with low error whereas metric 2 would give this the highest error rating of -1 as they are not the same sign. Metric 3 is more similar to metric 1 as shown by the results, however the crucial difference is that again if you get opposite signs it will penalise more.
When β is close to one, the loss function behaves more like DKL(P||Q) which tends to cover all the modes of P. When β is close to zero, the loss function behaves more like DKL(Q||P) which prefers to memorize only a subset of the true data distribution conservatively. In order to find an optimal value of β, we study the effect of β on the translation quality of English-German task. It shows that the model trained with β=0 performs the best, which surprisingly indicates that the skew inverse KL divergence is better than the interpolation form in terms of our loss function applying strategy. Note that the inverse form of skew divergence loss is still a special case of our general DSD loss.
A range of lda2vec parameters are evaluated by varying the number of topics n∈20,30,40,50 and the negative sampling exponent β∈0.75,1.0. We briefly experimented with variations on dropout ratios but we did not observe any substantial differences.
It can be observed that when only a small number of edges are available, e.g., 15%, the performances of structure- only methods is much worse than semantic-aware models that have taken textual information into consideration The perfromance gap tends to be smaller when a larger proportion of edges are employed for training. This highlights the importance of incorporating associated text sequences into network embeddings, especially in the case of representing a relatively sparse network. More importantly, the proposed WANE-ww model consistently outperforms other semantic-aware NE models by a substantial margin, indicating that our model better abstracts word-by-word alignment features from the text sequences available, thus yields more informative network representations.
We further consider the case where the training ratio is less than 10%, and evaluate the learned network embedding with a semi-supervised classifier. Following Yang et al. This may be due to the fact that WANE-ww extracts information from the vertices and text sequences jointly, thus the obtained vertex embeddings are less noisy and perform more consistently with relatively small training ratios Yang et al.
To examine the adaptability of TextING under inductive condition, we reduce the amount of training data to 20 labelled documents per class and compare it with TextGCN. Word nodes absent in the training set are masked for TextGCN to simulate the inductive condition. In this scenario, most of the words in the test set are unseen during training, which behaves like a rigorous cold-start problem. An average gain of 21.06% shows that TextING is much less impacted by the reduction of exposed words. TextING shows a consistent improvement when increasing number of words become unseen.
We find that the BIO tagger outperforms our extractor on LAPTOP and REST. A likely reason for this observation is that the lengths of input sentences on these datasets are usually small (e.g., 98% of sentences are less than 40 words in REST), which limits the tagger’s search space (the power set of all sentence words). As a result, the computational complexity has been largely reduced, which is beneficial for the tagging method.
The results show that our approach significantly outperforms the tagging baseline by achieving 9.97%, 8.15% and 15.4% absolute gains on three datasets, and firmly surpasses previous state-of-the-art models on LAPTOP. The large improvement over the tagging baseline suggests that detecting sentiment with the entire span representation is much more beneficial than predicting polarities over each word, as the semantics of the given target has been fully considered.
We see that order-embeddings outperform the skip-thought baseline despite not using external text corpora. While our method is almost certainly worse than the state-of-the-art method of Rocktäschel et al.
Next, we perform ablation tests (rows 6-20) where we hold-out one source embedding and use the other four with each meta-embedding method. We evaluate statistical significance against best performing individual source embedding on each dataset. For the semantic similarity benchmarks we use Fisher transformation to compute p<0.05 confidence intervals for Spearman correlation coefficients. In all other (classification) datasets, we used Clopper-Pearson binomial exact confidence intervals at p<0.05. In 6 out of 12 benchmarks, this improvement is statistically significant over the best single source embedding.
We use automatic metrics to evaluate the 8 models’ performance on the News Commentary dataset. Even though this is a non-overlapping held-out dataset to our news training data, it is still within the same domain. Our results also demonstrate that context matters for explanation. The L2EC task models, trained on context, can generate higher quality explanations than context-free L2E task models.
Lexical Diversity Analysis. distinct-n score corresponds to the number of distinct n-grams divided by total number of generated n-grams. We can clearly observe that DeepCopy generates the most diverse responses among all the models including the copy-augmented oracle model (S2SC-3). Hence, diversity results further show that our proposed model is promising in addressing the most commonly observed generic response problem more effectively than existing models by generating more diverse responses. As can be seen from this analysis, models that have a copy mechanism include more facts from the persona than the ones that do not. Another important observation is that the ground-truth responses include facts from persona only in 49% of the times, which indicates that the provided persona facts remain insufficient to cover the complexity of the high entropy open-ended person-to-person conversations. We observe for each model and metric pair a κ statistic of greater than 0.9, which indicates a near perfect agreement among raters. Note that the ratio of hallucinated facts (F.Hal) is derived directly from human labels for fact inclusion (F.Inc) and persona-fact (F.Per). That is why, there is no separate labelling process for hallucinated facts (F.Hal).
Two key takeaways arise in this table. First, JZR (limited) never performs better than JZR, which shows the precision of discovered root-and-pattern rules. Second, JZR performed better than JZR (limited) on 63 occasions due to the discovery of root-and-pattern morphology. Moreover, it is interesting to see that on multiple occasions JZR performed better than ISRI, which shows that rule-based methods are insufficient and unsupervised methods are needed to fill the gap.
As expected, performance degrades as we increase batch size, with batches of four allowing more fine grained control over model evaluations than using batches of eight. In particular, due to the exploitative nature of Thompson sampling, we see that selecting models to a very high confidence (95%) requires more computation with BTS than the standard non-adaptive approach. However, BTS does reach the other confidence levels faster and correctly identifies the optimal model more often. However, as TTTS performs significantly better across all confidence levels, we emphasise the need for a less-exploitative version of BTS with adjustments similar to those used in TTTS.
There are several take-aways. The results of the per-task models (cf. Wh-q is easier (.81) than Y/N-q (.52), regardless of the fact that a priori the latter should be easier (as shown by the respective task-specific random baselines). The Y/N-q task-specific model performs only slightly above chance (.52, in line with what \newcitejohnson2017: clevr report for ‘equal_attribute’ questions). This shows that despite the limited output space of the Y/N-q task, such type of questions in CLEVR are complex and require reasoning skills Johnson et al. We observe that extreme forgetting is at play. Naive forgets the previously learned skill completely: When tested on Task A after having been fine-tuned on Task B, it achieves 0.0 accuracy on the first task for both directions ( I and II, cf. The Cumulative model by nature cannot forget, since it is trained on both tasks simultaneously, achieving .81 and .74 on Wh-q and Y/N-q, respectively. Interestingly, we observe an asymmetric synergetic effect. Being exposed to the Wh-q task helps the Cumulative model improve on Y/N-q, reaching results beyond the task-specific model (from .52 to .74). The effect is not symmetric as the accuracy on Wh-q does not further increase. This confirms psycholinguistic evidence. Overall, Rehearsal works better than EWC, but mitigates forgetting only to a limiting degree. To get a deeper understanding of the models, we analyze the penultimate hidden layer on a sample of 512 questions from the test sets of both tasks (cf. Fig.
Training Data We train our model jointly on the (virtually unlimited) template generations and set of 33K training rephrases. Early experiments showed that a model trained exclusively on templated generations failed to reach accuracies better than 40% on the validation rephrases. The accuracy on the validation and test rephrased data is also high, up to 80.7% for the SentenceRec model. However, the worse performance on instructions from both prompts and interactive shows that our setting poses significant generalization challenges. In particulars, all models have significant trouble with the prompts, which come from crowd-sourced workers asked to imagine general game commands and may not fit the exact Minecraft setting. Still, 86% of the annotations are valid under our grammar, and we hope that future work will be better able to address the domain shift to be able to predict those.
Accurate prediction of a categorical or span node depends on having predicted all of the internal nodes on the path to the root, which explains why CAT and SPAN P/R/F numbers are lower than INT. Additionally, both models have more trouble predicting span than categorical nodes.
Performance with varying amount of supervision: As can be observed, the model performs reasonably well even for low amounts of supervision and outperforms the unsupervised baseline MUSE(U) and it’s supervised counterpart RCSLS. Moreover, note that the difference is more prominent for the etymologically distant pair en↔zh. In this case the baseline models completely fail to train for 50 points, whereas BLISS(R) performs reasonably well.
For the word analogy task evaluations, we took 52 measures in total. In the first three columns, we provide attributes of each corpus like name, size of raw text and vocabulary size. In the fourth column “Cov”, we present the coverage rate of each text set. It is the percentage of syntactic and semantic questions that were seen in that text. In columns 5 - 7 we show the semantic, syntactic and overall accuracy of Word2vec models of each corpus. The last 3 columns present the analogous accuracy metrics of Glove models. It is about 49% in most of the cases. The only exceptions are Lyrics bundles (the worst) with coverage 44.91 or 47.92% and the small Text8Corpus (the best) that achieves an exceptionally high coverage rate of 91.21%. As syntactic questions are highly covered by most of the corpora, overall accuracy that is the weighted average of syntactic and semantic accuracies, is always very close to the former. In fact, semantic questions are poorly covered. Questions about currencies or state-city relations were never seen in any text set. We see that coverage is influenced by vocabulary and theme of the texts. Song lyrics which are the most constrained thematically have small vocabulary (71590 the biggest) and thus the lowest coverage. On the other hand, Text8Corpus of Wikipedia that is very small in size has a relatively rich vocabulary. Furthermore, as Wikipedia texts contain most of the terms comprised in the syntactic and semantic questions, it has the highest coverage rate.
We see that Crawl840 and Crawl42 bundles are again on top with an accuracy score of almost 95%. Also, our Item models are doing well, same as on movie reviews. They are positioned 3rd and 5th. The two lyrics bundles are the worst, with scores lower than 83%. This may be due to their small vocabulary size. Something else to notice here is the considerable difference of about 0.13 between the top and the bottom of the table. That difference is the highest among the four experiments. We did not find any literature results on the phone dataset. We performed paired t-test analysis, comparing the values of the two best models against the two worst in each of the four experiments. As we can see, in every experiment, t values are much greater than p values. Furthermore, p values are much smaller than the chosen value of significance (α=0.05). As a result, we have evidence to reject hypothesis H0 with very high confidence.
There is a difference of 0.018 between the two top models of the corresponding experiments. This is a strong indication of corpus thematics influence on sentiment analysis of movie reviews. The scores of the different cuts and methods are however very similar to each other, with a difference of only 0.01 from top to bottom. On phone reviews, we see the same tendency as in movie reviews. Top item review models (again Item_2.5B_cb) yields an accuracy of 0.943 which is 0.063 higher than the score of Misch_24B_cb. Same as in movie reviews, this result suggests a strong influence of corpus thematics on sentiment analysis of phone reviews. Also, the models of different sizes and training methods are again very close to each other. Same as in the previous section, we compared Glove models with Word2vec corresponding models of the same size using paired t-test analysis. We also compared biggest and smallest models of both methods. Considering obtained p values and α=0.05 that we set at the beginning of this section It means that effects of size and training method are not significant in most of the experiments with tweets. This is something strange considering the results with Misch models in the previous section. A possible explanation for this could be the fact that size differences of training corpora here are much smaller (few million tokens) compared with those of Misch models (many billion tokens).
A quick look at the results of the four experiments indicates that there is a thematic influence of corpora on accuracy scores obtained in tweets, phone and movie reviews. Results on song lyrics are very similar in value. Also, the two domain-specific models (Lyric_500M_cb and Lyric_500M_gv) perform worse than the other models. For these reasons, we performed t-test analysis on tweets, phones, and movies to see if we could reject or not Ht0 hypothesis. We compared the two best performing models of each domain with the two models of Misch corpus which is thematically neutral. Considering the p values of each measurement, we can reject Ht0 in all three tasks. Obviously, corpora containing texts of tweets, movie or phone reviews perform better than corpora of generic texts on each corresponding task. The topic effect is small on sentiment analysis of tweets but much stronger on movie and phone reviews.
We decided to try SSWE on our own for sentiment analysis of tweets and song lyrics. We utilized the labeled texts of the datasets we experimented with in the previous sections which count for 250,000 and 1.2 million tokens of tweets and lyrics respectively. Considering the small size of the corpora we used a context window of 5 tokens and generated 100-dimensional vectors. Nevertheless, it is important to note that SSWE provides slightly better accuracy (about 1% higher) and macro F1, especially on the classification of tweets. Statistical analysis results (t=2.68 and p=0.028) confirm that the difference between Tweets_w2v and Tweets_sswe is significant. specific (e.g., SSWE) word embedding generation methods can produce better results on certain tasks or domains when used on large text collections.
Our first evaluation measures whether questions generated by our system are well-formed (i.e., grammatical and pragmatic). We ask crowd workers whether or not a given question is both grammatical and meaningful. Question relevance: How many generated questions are actually relevant to the input paragraph? Hence, we display an input paragraph and generated question to crowd workers (using the same data as the previous well-formedness evaluation) and ask whether or not the paragraph contains the answer to the question. Is the predicted answer actually a valid answer to the generated question? In our filtering process, we automatically measured answer overlap between the input answer span and the predicted answer span and used the results to remove low-overlap QA pairs. To evaluate answer recall after filtering, we perform a crowdsourced evaluation on the same 300 QA pairs as above by asking crowdworkers whether or not a predicted answer span contains the answer to the question. Over 85% of predicted spans partially contain the answer to the generated question, and this number increases if we consider only questions that were previously labeled as well-formed and relevant. The lower gold performance is due to the contextual nature of the gold QA pairs in QuAC, which causes some questions to be meaningless in isolation ( e.g. “What did she do next?” has unresolvable coreferences).
Moreover, the table shows the effect of attribute-aware representations that incorporate the flow of context by predicting attributes of two consecutive time steps. Finally, the table shows the effect of procedural context modeling by truncating sentences up to a certain time step rather than considering the full document at each time step. Note that document-level evaluation in ProPara requires spans of texts being identified, therefore removing span prediction from Dynapro cannot be ablated.
We added word based features using lexicons and language model based ELMo embeddings and utterance level sentiment scores using VADER scores. As the table shows, adding lexicons result in a slight drop in performance of the scores. The lexicons we’ve used are extremely sparse compared to the vocabulary space of Word Vectors. Also we’ve simplistically concatenated the binary scores for Positive and and Negative category words to the same embeddings space as for the word vectors. Majority of these values remain 0 after the operation. We are exploring other ways to leverage the lexicon embeddings to allow a larger contribution of these signals to the classification process. Addition of the ELMo embeddings improves the performance as compared to using word embeddings alone. Addition of ELMo embeddings and segment level sentiment scores using Vader gives the best performance for binary, 7-class and MAE scores, as compared to adding individual features, or a combination of features. As described in ELMo work, adding the layers at different positions of the network helps to abstract various naturally occurring syntactic and semantic information about the words. For the audio modality, we presented two additional feature-sets in the previous section, i-vector features and phoneme level features alongwith COVAREP features. Based on our experiments, we observed that the performance of the Emotion recognition RTN model with all these features were similar but improved slightly for ‘Happy’ emotion compared to the RTN model without the additional audio features.
Naturalness: We demonstrate that choice of waveform synthesis matters for naturalness ratings and compare it to other published neural TTS systems. Thus, we show that the most natural waveform synthesis can be done with a neural vocoder, and that basic spectrogram inversion techniques can match advanced vocoders with high quality single speaker data. The WaveNet vocoder sounds more natural as the WORLD vocoder introduces various noticeable artifacts. Yet, lower inference latency may render the WORLD vocoder preferable: the heavily engineered WaveNet implementation runs at 3X realtime per CPU core (see the subsection below).
Multi-Speaker Synthesis: To demonstrate that our model is capable of handling multi-speaker speech synthesis effectively, we train our models on the VCTK and LibriSpeech data sets. We purposefully include ground-truth samples in the set being evaluated, because the accents in datasets are likely to be unfamiliar to our North American crowdsourced raters. Our model with the WORLD vocoder achieves a comparable MOS of 3.44 on VCTK in contrast to 3.69 from Deep Voice 2, which is the state-of-the-art multi-speaker neural TTS system using WaveNet as vocoder and seperately optimized phoneme duration and fundamental frequency prediction models. We expect further improvement by using WaveNet for multi-speaker synthesis, although it may substantially slow down inference. The MOS on LibriSpeech is lower compared to VCTK, which we mainly attribute to the lower quality of the training dataset due to the various recording conditions and noticeable background noise. In the literature, Yamagishi et al. Lastly, we find that the learned speaker embeddings lie in a meaningful latent space (see Fig.
Evaluation on Benchmark Datasets In this part, we display the comparative performances on three benchmark datasets, BBN, OntoNotes, and Wiki. Additionally, we compare with the results from OTyper, which is designed for open entity typing. OTYPERWiki trains the model on the Wiki dataset, but tests on BBN and OntoNote without separating the data by granularity. OTYPERBBN and OTYPEROntoNotes are in the same way to set training and testing sets. Furthermore, for the large-sized Wiki, MZET also obtains the highest scores for all metrics. Besides, Zero-Shot methods, MZET and DZET, surpass a lot OTYPER. That means open type FNET suffers from the drawbacks of highly dependent on the volume of the corpus in hand for training. But, zero-shot models are bestowed more flexibility and independence to attest their robustness on unseen datasets.
In general, the results show a clear pattern in all of these steps: both neural models (GRU and Transformer) introduced higher results on domains seen during training, but their performance drops substantially on unseen domains in comparison with the baselines (Random and Majority). The only exception is found in Text Structuring, where the neural models outperforms the Majority baseline on unseen domains, but are still worse than the Random baseline. Between both neural models, recurrent networks seem to have an advantage over the Transformer in Discourse Ordering and Text Structuring, whereas the latter approach performs better than the former one in Lexicalization.
We observe that the deep learning embedding methods perform best in the MMSE prediction task. In particular the DistilBERT model using only participant sections of the transcript provides the best RMSE. Interestingly, the deep learning methods perform well despite having not been trained with regression tasks in mind. Our CRF models only support classification so we cannot report MMSE prediction scores for those model configurations.
It is clear that the CLSTM architecture outperforms the DNN x-vector system; the combination of CNN and LSTM appears able to strengthen the feature extraction to learn directly from LID labels and model temporal dependencies to learn long-range discriminative language features over the input sequence.
The combination of TSM and TTDA performed best for the test set, but interestingly was fractionally worse for the development set.
Effect of β: Fig. We observe within a proper range of β (between 0.5 and 2.0), the model can provide stable improvement on EER, guaranteeing competitive CER results.
Divide-and-Conquer: Starting with Single Source. We find that the multi-textual-source model can greatly benefit from starting with weights trained on single textual source. Specifically, for each encoder in the fourway model, we load their corresponding single-textual-source encoder’s weights. Since the four single models share a common word embedding in the fourway model, we initialize the word embedding layer with caption model’s word embeddings weights (which is the one with the best single-textual-source performance). To avoid these pre-trained weights receiving random gradients at the starting phase of joint training, we freeze them for 1 epoch during which we only train the Feature Fuser. Then all parts of text encoder are unfrozen and being trained as a whole for another 9 epochs. We use a learning rate of 10−4 for the first 5 epochs and 2×10−5 for the second 5 epochs. R@10s are improved by ≈15\%! Training four text encoders together from scratch might be too hard a task to optimize. Briefly, texts from each sources have a probability of being dropped and substituted with empty string during the training phase. For every sample, one source of texts is randomly chosen to be preserved (to make sure at least one meaningful source of input) while for the other three sources, each one has 0.7 probability of being kept and 0.3 probability of being dropped. We switch back to complete_inputs mode in the validation phase. Other Fusion Methods. We also test other fusion methods in the literature such as Global Max Pooling, Element-wise Adding, Neural Nets Fusion. We use similar manner loading pre-trained single-textual-source models, then train them jointly with random drop. is the optimal structure for their multimodal feature fusion task. We notice that Element-wise Adding performs better in image-to-text task and Attention Fuser is better for text-to-image.
We split the methods into two categories: one is Sequence Labeling based method, and the other is Encoder-Decoder based method. Our method follows the Sequence Labeling framework and we design a novel multi-task sequence labeling model which achieve the best performance against the published Sequence Labeling based method (F1+0.22%) and compatible result against the best Encoder-Decoder based method (F1-0.03%). These two reasons prevent our proposed DCMTL model from further improving the performance on ATIS dataset. Thus, we will mainly focus on ECSA dataset, which is much larger and more sophisticated, to prove the effectiveness of our proposed model.
On ECSA dataset, we evaluate different models including Basic BiLSTM-CRF, Vanilla Multi-task, Hierarchy Multi-task and Deep Cascade Multi-task on testing data regarding slot filling as the target task. The Basic BiLSTM-CRF model achieves an F1 score of 0.43. To show the usefulness of the lower tasks to slot filling, we “cheated” by using the ground-truth segment type (cond. SEG) or named entity type (cond. NE) as the extra features for each word in the Basic BiLSTM-CRF model. Row 3 and 4 (with *) in It can perfectly verify our claim that low-level syntactic tasks can significantly affect to the slot filling performance. Of course in practice, the model doesn’t know the true values of these types during prediction. Our further experiments show that DCMTL outperforms the baselines on both precision and recall. Multi-task models generally perform better than the Basic BiLSTM with single-task target. The exception is the vanilla multi-task setting. This is mainly because vanilla multi-task shares parameters across all the layers, and these parameters are likely to be disturbed by the interaction of three tasks. It is more desirable to let the target task dominate the weights at high-level layers.
The algorithms assign a score to each of the four possible answer choices and the choice with the highest score is selected as the best guess. Accuracy is measured by the percentage of correct choices. If n answer choices tie for the correct score, the algorithm gets a partial mark of 1/n, the expected value of randomly resolving ties. The difference between Multivex (51.8%) and SVD (47.6% and 48.8%) is statistically significant. Lucene also has a higher accuracy (49.1%) than the two SVD variations, but the difference is not statistically significant. The difference between Multivex (51.8%) and Word2vec (43.7% and 46.5%) is statistically significant. The difference between Lucene (49.1%) and Word2vec is also statistically significant.
The variations of Ctrl always out-perform that of DAN. It shows that a purely linear transformation is unable to produce noise and prevent over-fitting. Ctrl - -’s result shows the adaptive ability of the control modules. Ctrl - updates all parameters in the overall network synchronously, but under-performs DE-CNN though it has control modules. The reason is that in synchronous updating, control modules just make the overall network deeper. They have the same learning rate. It means that fixed control modules make the training harder. In the second plot, Ctrl- ’s validation loss decreases and then increases. This is an apparent over-fitting signal. But, Ctrl’s validation loss tends flat even after several-steps training. From the last test-score plot, we can see that Ctrl has similar testing performance as Ctrl- in the first step training. In Ctrl’s second step training (between the first and second green lines), the test score continues improving. The results and plots show that through asynchronous updating, control modules can prevent over-fitting and improve CNN performance.
We use four evaluation metrics including BLEU Papineni et al. et al. Most of the pre-defined orders (except for the random order and the balanced tree (BLT) order) perform reasonably well with InDIGO on the three language pairs. The best score with a predefined word ordering is reached by the L2R order among the pre-defined orders except for En-Ja, where the R2L order works slightly better according to Ribes. This indicates that in machine translation, the monotonic orders are reasonable and reflect the languages. ODD, CF and RF show similar performance, which is below the L2R and R2L orders by around 2 BLEU scores. The tree-based orders, such as the SYN and BLT orders do not perform well, indicating that predicting words following a syntactic path is not preferable. The improvements are larger for Turkish and Japanese, indicating that a flexible generation order may improve the translation quality for languages with different syntactic structures from English.
SAO without bootstrapping nor beam-search degenerates by approximate 1 BLEU score on Ro-En, demonstrating the effectiveness of these two methods. We also test SAO by bootstrapping from a model trained with a R2L order as well as a SYN order, which obtains slightly worse yet comparable results compared to bootstrapping from L2R. This suggests that the SAO algorithm is quite robust with different bootstrapping methods, and L2R bootstrapping performs the best. In addition, we re-implement a recent work Stern et al. We use the best settings of their algorithm, i.e., training with binary-tree/uniform slot-losses and slot-termination, while removing the knowledge distillation for a fair comparison with ours. Our model obtains better performance compared to Stern et al.
However, it is slower in terms of training time using SAO as the supervision, as additional efforts are needed to search the generation orders, and it is difficult to parallelize the SAO. SAO with beam sizes 1 and 8 are 3.8 and 7.2 times slower compared to L2R, respectively. Note that enlarging the beam size during training won’t affect the decoding time as searching the best orders only happen in the training time. We will investigate off-line searching methods to speed up SAO training and make InDIGO more scalable in the future.
In addition to PRPN, we compare to DIORA (Drozdov:2019), which uses an inside-outside dynamic program in an autoencoder. PaLM outperforms the right branching baseline, but is not as accurate as the other models. This indicates that the type of syntactic trees learned by it, albeit useful to the LM component, do not correspond well to PTB-like syntactic trees.
Is PaLM empirically biased toward any branching direction? In greedily selected trees, we measure the percentage of left-branching splits (dividing [i,j] into [i,j−1] and j) and right-branching splits (dividing [i,j] into i and [i+1,j]). The first row shows the results for randomly initialized models without training. We observe no significant trend of favoring one branching direction over the other. However, after training with the language modeling objective, PaLM-U shows a clear right-skewness more than it should: it produces much more right-branching structures than the gold annotation. This means that the span attention mechanism has learned to emphasize longer prefixes, rather than make strong Markov assumptions. More exploration of this effect is left to future work.
(WED-D) is better than the variational encoder-decoder (VED) in BLEU scores, but the generated sentences lack variety, which is measured by output entropy and the percentage of distinct unigrams and bigrams (Dist-1/Dist-2, Li et al.
Tuning hyperparameters is often done on a validation set drawn from the same distribution as the training set (as opposed to validating on data from a different distribution) which motivated our initial decision to use a validation set of characteristic inputs to decide the epoch to stop at. However, we noticed only small variation in the validation performance upon using different learning rates and dropout probabilities (where dropout was applied to the input and output layers). In order to fine tune these parameters to avoid extreme overfitting, we created another validation set consisting of 5000 samples of ”uncharacteristic” inputs, i.e., inputs with repeated symbols and varying from length 3-12. These two hyperparameter values were set to 0.125 and 0.1, respectively, according to the performance on this validation set, averaged across a set of randomly chosen random seeds. All other hyperparameters were also decided using this validation set.
[Krakovna:2016] displayed an improvement to the traditional HMM model and LSTM model in terms of predictive accuracy as measured by predictive log-likelihood of the model, L(θ). The LSTM improves its J(θ) Furthermore, we examined the models seperately and measured the similarity of the hidden states with respect to their cosine distance, Δ(ΨH,ΨL). Where ΨH and ΨL corresponds to the probabilities of each of the nh hidden state vectors. We notice that the cosine similarity, Δ(ΨH,ΨL) generally decreases as we decrease the nh. However, between the 15-35 hidden state range for more complex corpora such as WP, there seems to be a subtle increase in the cosine similarity. This indicates that the HMM shares some structural similarities when compared the the LSTM, respective to their hidden state distributions.
The much higher NED and coverage scores suggest that the proposed approach is a highly permissive matching algorithm. The much higher parsing scores (type, token and boundary scores), especially the Recall and F-scores, imply the proposed approach is more successful in discovering word-like units. However, the matching and grouping scores are much worse probably because the discovered tokens cover almost the whole corpus, including short pauses or silence, and therefore many tokens are actually noises. Another possible reason might be that the values of n used are much smaller than the size of the real word vocabulary, making the same token label used for signal segments of varying characteristics and this degenerated the grouping qualities.
We observe that ambiguous mentions (those annotated with several types) are adequately handled. For instance, the LS representation of the word “hilton” encodes that it more often refers to a hotel or a restaurant than to an actress. Also, we observe that entity words that are either not or rarely annotated in WiFiNE are still adequately associated with their right type. For instance, “dammstadt”, which appears only 5 times in WiFiNE, and which refers to the Damm city in Germany, is most similar to /location/city and /location/railway. Interestingly, this mention does not have its page in English Wikipedia. Furthermore, we observe that non-entity context words have a strong similarity to types they precede or succeed. For instance the verb “directed” is very close to /person/director, an entity type that usually precedes it, and to /art/film, that usually follows it. Likewise, the preposition “in” is near /date and /location/city, which frequently follow ”in”.
They suggest that a finer-grained gazetteer could improve the performance of their system on OntoNotes. Our results confirm this, since we use 120 types. We further detail the gains we observed for each sub-collection of texts in the test set. (+2 points). Those type of texts are characterized by a large set of infrequent words, for which classical embeddings are typically poorly trained.
Unlike the census data, which is representative of real-world statistics, wherein female names have more versatility—62% unique names vs. 38% unique male names—datasets used in training the NER models contain 42% female names vs. 58% male names from the census data. Not only do the datasets not contain more versatile female names to reflect the reality, but instead have even less variety which can itself bias the models by not covering enough female names. Similar patterns are observable in test and development sets of datasets used in the NER systems.
Self-Training (Self) [Abney:SLC:2007, Chattopadhyay:TKDD:2012], Union Self-Training (uni-Self) [Aue:RANLP:2005], Tri-Training (Tri) [Zhou:TKDE:2005] and Tri-Training with Disagreement (Tri-D) [Sogaard:ACL:2010]. Therefore, pseudo-labelling step alone is insufficient for DA. Moreover, we observe that all semi-supervised methods perform comparably.
The results clearly demonstrate a consistent improvement over all the steps in the proposed method. For Self step, the proposed method improves the accuracy slightly without any information from the target domain. In the PL step, we report the results of a predictor trained on target pseudo-labelled instances. We report the evaluation results for the trained attention model in Att.
Our goal is two-fold: 1) We focus on the Easy vs. Hard examples to gain better insight into why some tweets are easier to predict as offensive than others, and 2) OLID is small (particularly for B and C). Therefore, showcasing the performance on a larger amount of data can indicate the stability of the test set. In almost all cases including SOLID with OLID provides an improvement in performance.
It is important to ask, therefore, how well does our model predict the forms of heldout lexemes given this stricture? This table displays the average accuracy for each language in our sample as well as the number of lexemes for that language, the total number of forms, and the average number of forms per lexeme. The majority of languages show very high generalization accuracy to our lexeme-based wug-tests: 21 out of 28 have an average accuracy of 75% or higher. Three languages stand out in terms of their low accuracy and are highlighted in These languages, Basque especially, are characterized by smaller numbers of lexemes and larger numbers of forms per lexeme.
The tweetLID workshop shared task requires systems to identify the language of tweets written in Spanish (es), Portuguese (pt), Catalan (ca), English (en), Galician (gl) and Basque (eu). Some language pairs are similar (es and ca; pt and gl) and this poses a challenge to systems that rely on content features alone. We use the supplied evaluation corpus, which has been manually labelled with six languages and evenly split into training and test collections. We use the official evaluation script and report precision, recall and F-score, macro-averaged across languages. This handles ambiguous tweets by permitting systems to return any of the annotated languages. (i.e. English and Basque). For similar languages, adding the social model helps discriminate them (i.e. Spanish, Portuguese, Catalan and Galician), particularly those where a less-resourced language is similar to a more popular one. Using the social graph almost doubles the F-score for undecided (und) languages, either not in the set above or hard-to-identify, from 18.85% to 34.95%. Macro-averaged, our system scores 76.63%, higher than the best score in the competition: 75.2%.
Our evaluation works as follows. Considering the textual content features and the corresponding ranking matrix RT, the structure features and the corresponding ranking matrix RS, and matrix RB which is the combination of the other two matrices, we compute the mean squared error (MSE) between each matrix with the purpose of understanding how these matrices can be compared. We can observe that the MSE between RT and RS is much higher compared with that between RT and RB or between RS and RB. This suggests that our combination method is able to incorporate into RB the information in both RT and RS. For providing a better idea about the meaning of the computed MSE values, we have computed a random matrix then applied different degrees of perturbation in the ranking, and computed the MSE between the original and perturbed matrices. The perturbation consists of randomly picking a pair of elements and swapping their values. The degree of perturbation corresponds to the number of swaps that are performed. On the other hand, the MSEs of RT and RB, and that of RS and RB, are between the swapping of 100 and 200 pairs (about 1/5 to 1/3 of the list). Thus, RB tends to present more similarity to RT and RS, compared with the similarity of RT to RS.
Not surprisingly, we observe that the Original dialog and the most similar one for Structure present very close values. On the other hand, the most similar dialog for Content present the most different values. And the most similar in Content+Structure does not present values as close as that of Structure to the Original dialog, but still the values are quite close if compared with the Content dialog.
et al. ’s model and apply multi-modal factorized bilinear pooling (MFB) Yu et al. This implies that our textual data helps improve VQA model performance by providing clues to answer questions. We run each model five times with different seeds and take the average value of them. For each of the five runs, our VTQA model performs significantly better (p<0.001) than the VQA baseline model.
Our quantized models perform comparably to the full-precision model, and represent reasonable trade-offs in accuracy for model size and inference time. The last row of the table represents a result of 10x compression over the 138M parameter baseline with no loss in performance. Quantization-aware training scheme did not result in significant gains over post-quantization.
To better understand recent advances in LJP, we have conducted a series of experiments on C-LJP. For the parameters of BERT, we use the pretrained parameters on Chinese criminal cases Zhong et al. Secondly, we implement several models which are specially designed for LJP, including FactLaw Luo et al. et al. Gating Network Chen et al.
CM contains 8,964 triples where each triple contains three legal documents (A,B,C). The task designed in CM is to determine whether B or C is more similar to A. We have implemented four different types of baselines: (1) Term matching methods, TF-IDF Salton and Buckley Seo et al. (3) Semantic matching models in sentence level, ABCNN Yin et al.
We select JEC-QA Zhong et al. as the dataset of the experiments, as it is the largest dataset collected from the bar exam, which guarantees its difficulty. JEC-QA contains 28,641 multiple-choice and multiple-answer questions, together with 79,433 relevant articles to help to answer the questions. JEC-QA classifies questions into knowledge-driven questions (KD-Questions) and case-analysis questions (CA-Questions) and reports the performances of humans. We implemented several representative question answering models, including BiDAF Seo et al. , Co-matching Wang et al. et al.
This is high for a scheme with so many labels to choose from. Interestingly, there is not an obvious relationship in general between annotators’ backgrounds (native language, amount of exposure to the scheme) and their agreement rates. It is encouraging that Annotators D and E, despite recently learning the scheme from the guidelines, had similar agreement rates to others.
We observe that TF-CR consistently outperforms the other weighting schemes, TF-IDF and KLD, regardless of the training size. The gap between TF-CR and the other weighting schemes generally becomes larger as the training data increases, showing that TF-CR exploits the class distributions more effectively. We can also observe that the unweighted approach outperforms TF-CR in six out of eight datasets when the training data is as small as 1,000 instances. However, TF-CR becomes more effective as the training data increases. TF-CR outperforms the unweighted method in five out of eight datasets with 10,000 training instances, and in seven out of eight datasets with 90,000 training instances. This reinforces the effectiveness of TF-CR for mid-sized training sets and above.
Statistics. We measure a fair annotator agreement of 34.02% (Cohen’s Kappa) and produce a gold standard done by an expert annotator (one of the authors) that provides the basis of our final analysis. We find a high proportion of Generics within the Microtexts (64%) and an even higher amount within he implicit information annotations (84%), while the other genres (reports, speeches, fiction) rather contain mostly States and Events. This indicates the relevance of knowledge captured by Generic Sentences within the added implicit information, and we can use this finding for acquiring such missing information automatically.
Results and Discussion. Tab. This is because there are no overt similarities between the two languages left after applying the cipher, e.g., the two previously identical forms visito are now different.
on CoNLL-2002 and CoNLL-2003. We consider the LSTM + CRF approach from Lample et al. We evaluate the performance of the model on each of the target languages in three different settings: (i) train on English data only (en) (ii) train on data in target language (each) (iii) train on data in all languages (all). Note that we do not use a linear-chain CRF on top of XLM-R and mBERT representations, which gives an advantage to Akbik et al. Without the CRF, our XLM-R model still performs on par with the state of the art, outperforming Akbik et al. On this task, XLM-R also outperforms mBERT by 2.42 F1 on average for cross-lingual transfer, and 1.86 F1 when trained on each language. Training on all languages leads to an average F1 score of 89.43%, outperforming cross-lingual transfer approach by 8.49%.
Glue: Xlm-R versus RoBERTa. Our goal is to obtain a multilingual model with strong performance on both, cross-lingual understanding tasks as well as natural language understanding tasks for each language. To that end, we evaluate XLM-R on the GLUE benchmark. The RoBERTa model outperforms XLM-R by only 1.0% on average. We believe future work can reduce this gap even further by alleviating the curse of multilinguality and vocabulary dilution. These results demonstrate the possibility of learning one model for many languages while maintaining strong performance on per-language downstream tasks.
A recurrent criticism against multilingual models is that they obtain worse performance than their monolingual counterparts. In addition to the comparison of XLM-R and RoBERTa, we provide the first comprehensive study to assess this claim on the XNLI benchmark. We train 14 monolingual BERT models on Wikipedia and CommonCrawl (capped at 60 GiB), and two XLM-7 models. We increase the vocabulary size of the multilingual model for a better comparison. We found that multilingual models can outperform their monolingual BERT counterparts. However, by making use of multilingual training (translate-train-all) and leveraging training sets coming from multiple languages, XLM-7 can outperform the BERT models : our XLM-7 trained on CC obtains 80.0% average accuracy on the 7 languages, while the average performance of BERT models trained on CC is 77.5%. This is a surprising result that shows that the capacity of multilingual models to leverage training data coming from multiple languages for a particular task can overcome the capacity dilution problem to obtain better overall performance. On the other hand, pretraining on CC improved performance by up to 10 points. This confirms our assumption that mBERT and XLM-100 rely heavily on cross-lingual transfer but do not model the low-resource languages as well as XLM-R. Specifically, in the translate-train-all setting, we observe that the biggest gains for XLM models trained on CC, compared to their Wikipedia counterparts, are on low-resource languages; 7% and 4.8% improvement on Swahili and Urdu respectively.
BestPlan reduces all error types compared to StrongNeural, by 85%, 56% and 90% respectively. While on-par regarding automatic metrics, BestPlan substantially outperforms the new state-of-the-art end-to-end neural system in semantic faithfulness.
We made this choice in order to tease apart the differences across the datasets by focusing on the problematic instances; having between 17% and 29% of the scores be perfect 100 f-BLEU points would obscure the analysis. We also report average scores across all datasets (last row) as well as scores without including Lang8, since the Lang8 dataset is significantly larger than the others.
Overall, we find that using scheduled sampling in conjunction with neighborhood replacement sampling yields best performance for both datasets. Best performance for both Wikitext-2 and PTB is found with ϵ=(0,0.5) and γ(0,0.2), and slightly improves over only using scheduled sampling. For PTB, γe=0.2 performs the best for linear and sigmoid functions, γ=0.5 for exponential and static sampling rates and overall a constant sampling rate.
We conducted some experiments to test the assumption that the derived meaning embeddings should improve performance on downstream tasks that require understanding of the meaning of the sentences regardless of their form. We evaluated embeddings produced by the ADNet, trained in the Headlines dataset, on the paraphrase detection task. Note that all models, except InferSent, are unsupervised. The InferSent model was trained on a big SNLI dataset, consisting of more than 500,000 manually annotated pairs. ADNet achieves the the highest score among the unsupervised systems and far outperforms the regular sequence-to-sequence autoencoder.
We test two variants of our model trained with and without ranking regularization by controlling λ in our loss function, denoted as h-attn (without ranking), and h-attn-rank (with ranking). The h-attn outperforms both baselines, and h-attn-rank achieves the best performance for all metrics. Note, we use beam-search with beam size=3 during generation for a reasonable performance-speed trade-off (we observe similar improvement trends with beam size = 1). To test performance under optimal image selection , we use one of the two ground-truth human-selected 5-photo-sets as an oracle to hard-code the photo selection, denoted as h-(gd)attn-rank. This achieves only a slightly higher Meteor compared to our end-to-end model.
Compared to one recently proposed model AF-LSTM, our method achieve 2%-5% improvements. Surprisingly, a vanilla CNN works quite well on this problem. It even beats these well-designed LSTM models, which further proves that using CNN-based methods is a direction worth exploring.
HOCA-U, HOCA-B, and HOCA-T denote the model with only unary, binary, and ternary attention, respectively. HOCA-UB and HOCA-UBT denote the models with original HOCA and more types of attention mechanisms. The prefix “L-HOCA” denotes the model with Low-Rank HOCA. et al. On MSVD, L-HOCA-UBT has outperformed SCN, TDDF, RecNet, LSTM-TSA, MM-TGM, on all the metrics. In particular, L-HOCA-UBT achieves 86.1% on CIDEr, making an improvement of 5.7% over MM-TGM. On MSR-VTT, we have the similar observation, L-HOCA-UBT has outperformed RecNet, HRL, MM-TGM, Dense Caption, and HACA on all the metrics.
In practice, we utilize the experimental settings mentioned above, the batch size and the maximum number of epochs are set to 25 and 100, respectively. We can find that L-HOCA-UBT has smaller space requirement and less time cost than HOCA-UBT, in addition, the computing cost of L-HOCA-UBT is close to that of HOCA-U (Bahdanau Attention). The results demonstrate the advantage of Low-rank HOCA.
This also indicates that sentiment polarity classification is more difficult than topic-based classification and researchers should put more attention on this area. Frequency vs. Presence And these presence’s better accuracies also appear in SVM classifiers. speculation that the difference of accuracies in presence and frequency is an indication the difference between sentiment polarity classification and topic-based classification. Combined Adjective/Adverb Trigram Features no matter which representation model and classifier are used. We think this because even the adjective/adverb trigram features capture enough contextual information for classifier, but each of these features’ appearances in each document are very low, which make the document representation vector very sparse. This leads to the low accuracy. So we combine some short polarized features to boost the accuracy of adjective/adverb trigram features. We use 3AdjAdv to denote adjective/adverb trigram features for short.
Effect of sequence generation strategies. The best overall sequence generation strategy is to predict from left to right which achieves good results on both datasets. On the ShARC dataset the highest probability approach performs better than left-to-right. However, on Daily Dialog this approach is not as successful. This suggests that it might be worth selecting the best sequence generation strategy for each dataset individually. However, we hypothesize that left-to-right works consistently well due to the left-to-right nature of the English language. A brief experiment with a right-to-left strategy gave poor results.
αkh=1D1TD∑d=1T∑t=1akd,t Note that we use the attention heads in the final BERT self-attention layer. Unsurprisingly, we find that with scores of over 90% for both datasets the majority of the attention is focused on the first part, i.e. the conditioning input The remaining attention is split between the already produced sequence (α2) and the future tokens (α3). Here we can see that the past, already produced tokens are about twice as important as the future, not-yet-produced tokens. But with scores of just under 30% on both datasets, we see that a substantial amount of attention is also focused on the future, not-yet-produced tokens.
We investigate the effectiveness of the two techniques that we proposed in Sec. By using the tailored dictionary, the precision of the AutoNER model will be higher, while the recall will be retained similarly. For example, on the NCBI-Disease dataset, it significantly boosts the precision from 53.14% to 77.30% with an acceptable recall loss from 63.54% to 58.54%. Moreover, incorporating unknown-typed high-quality phrases in the dictionary enhances every score of AutoNER models significantly, especially the recall. These results match our expectations well.
In all these models, the use of recurrent neural networks allows arbitrarily large parse structures to be modelled without making any hard independence assumptions, in contrast to non-neural statistical models. Representing the parse tree as a derivation sequence, rather than a derivation structure, makes it possible to define syntactic parsing as a sequence-to-sequence problem, mapping the sentence to its parse sequence. Vinyals et al. They only achieve good results with the limited standard dataset by adding attention, which we will argue below makes the model no longer a seq2seq model. This indicates that structured representations really do capture important generalisations about language. The attention function learns the structure of the relationship between the sentence and its syntactic derivation sequence, but does not have any representation of the structure of the syntactic derivation itself.
In all these models, the use of recurrent neural networks allows arbitrarily large parse structures to be modelled without making any hard independence assumptions, in contrast to non-neural statistical models. Representing the parse tree as a derivation sequence, rather than a derivation structure, makes it possible to define syntactic parsing as a sequence-to-sequence problem, mapping the sentence to its parse sequence. Vinyals et al. They only achieve good results with the limited standard dataset by adding attention, which we will argue below makes the model no longer a seq2seq model. This indicates that structured representations really do capture important generalisations about language. The attention function learns the structure of the relationship between the sentence and its syntactic derivation sequence, but does not have any representation of the structure of the syntactic derivation itself.
We only show the results obtained using embedding dimension of 200, as they are superior over the dimension of 100. The baseline LM with randomly initialized embeddings has a PPL of 300.2 on the development set and 291.5 on the evaluation set. All bilingual word embeddings outperform the baseline with a large margin. Despite the simplicity of the Bi-CS approach, it achieves best results, giving a relative improvement of 33.5% in PPL on the test set. The results are similar to those of the first experiment, where all bilingual word embeddings outperform the baseline, and Bi-CS gives the lowest PPL. The best-performing (Bi-CS) model achieves a relative improvement of 22.4% in PPL on the test set. It is to be noted that in this setting, the PPLs are very high since the models are trained on text gathered from social media platforms, while it is tested on speech text. This setting is only used to confirm the effectiveness of using bilingual word embeddings in low-resourced CS language modeling.
Since there is no automated evaluation metric present to assess the quality of POS tagging a code-mixed sentence, we hired a linguist who was proficient in both Bengali and English. The linguist was asked to prepare a test data comprising of 100 English-Bengali code-mixed sentences. Further, the linguist was asked to POS tag the tokens, based on the universal POS tagset, separately. The linguist was told to look into the context of the sentence while tagging the tokens. This approach was used to properly tag ambiguous words, such as ’to’, which occurs in both English and Bengali. tag words in the switching point. The same test data was tagged using our system as well. evaluates the overall system. To dive deeper, we evaluated every sentence of the test data. This was done using two methods.
To better understand the difference between “fast” and “slow” entities, we examine the popularity of entities. Looking at the ranking, we note how “faster” emerging entity types remain more popular over time: types that are associated with short emergence durations and high velocities all fall in the top 10 (ranks 3, 4, and 9, for VideoGame, CreativeWork, and DesignedArtifact, respectively), whereas slower types reside in lower ranks in the table, e.g., rank 19, 22, and 24 for Building, EducationalInstitution and School, respectively.
Particularly, we use nine tasks in GLUE, including CoLA, RTE, MRPC, STS, SST, QNLI, QQP, and MNLI-m/mm. For the evaluation metrics, we report Matthews correlation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. We use the same optimizer (Adam) with the same hyperparameters as in pre-training. Following previous works, we search the learning rates during the fine-tuning for each downstream task. For a fair comparison, we do not apply any tricks for fine-tuning. Each configuration will be run five times with different random seeds, and the median of these five results on the development set will be used as the performance of one configuration. We will ultimately report the best number over all configurations.
Firstly, it is easy to find that both TUPE-A and TUPE-R outperform baselines significantly. In particular, TUPE-R outperforms the best baseline BERT-R by 1.34 points in terms of GLUE average score and is consistently better on almost all tasks, especially on MNLI-m/mm, CoLA and MRPC. We can also see that TUPE-R outperforms TUPE-A by 0.42 points. Besides the final performance, we also examine the efficiency of different methods.
The majority baseline classifies all the instances as unrealted (subtask 1) or random (subtask 2). Since these labels are excluded from the averaged F1 computation, this baseline’s performance metrics are all zero.
The second row shows percentages after taking a majority vote of the annotators. Even though the colour options were presented in random order, the order of the most frequently associated colours is identical to the Berlin and Kay order (Section 2:(1)).
If we assume independence, then the chance that none of the 5 annotators agrees with each other (majority class size of 1) is 1×10/11×9/11×8/11×7/11=0.344. Thus, if there was no correlation among any of the terms and colours, then 34.4% of the time none of the annotators would have agreed with each other. However, this happens only 15.1% of the time. A large number of terms have a majority class size ≥ 2 (84.9%), and thus have more than chance association with a colour. One can argue that terms with a majority class size ≥ 3 (32%) have strong colour associations.
The question type baseline (shown as Q. Type) is the best-guess baseline for all metaconcepts. Overall, VCML achieves the best metaconcept generalization, and only shows inferior performance to NS-CL on the meronym metaconcept. Note that the NS-CL baseline used here is our re-implementation that augments the original version with similar metaconcept operators as VCML.
We found that the Rep-Phonestream MMDA system tended to replace entire words when incorrect, while the baseline system incorrectly changed a few characters in a word, even if the resulting word did not exist in English (for WSJ). This behavior tended to improve WER while harming CER. For example, in the WSJ experiments, the baseline substitutes QUOTA with COLOTA, while the Rep-Phonestream MMDA predicts COLORS. We verified this hypothesis by computing the ratio of substitutions and insertions resulting in nonsense words to the total number of such errors on the WSJ development and evaluation data for the baseline system and MMDA, both with and without RNNLM re-scoring.
\newmdenv [innerlinewidth=20pt, roundcorner=20pt,linecolor=DarkBlue,innerleftmargin=3pt, innerrightmargin=3pt,innertopmargin=3pt,innerbottommargin=3pt]mybox Empirical fields, such as Natural Language Processing (NLP), must follow scientific principles for assessing hypotheses and drawing conclusions from experiments. What is the correct way to interpret this empirical observation in terms of the superiority of one system over another? While S1 has higher accuracy than S2 in both cases, the gap is moderate and the datasets are of limited size. Can this apparent difference in performance be explained simply by random chance, or do we have sufficient evidence to conclude that S1 is in fact inherently different (in particular, inherently stronger) than S2 on these datasets? If the latter, can we quantify this gap in inherent strength while accounting for random fluctuation? For this task, the performance metric M(S,x) is defined as a binary function indicating whether a system S answers a given question x correctly or not. The performance vector M(S,x) captures the system’s accuracy on the entire dataset (cf. We assume that each system Si has an unknown inherent accuracy value, denoted θi. Let θ=[θ1,θ2] denote the unknown inherent accuracy of two systems. In this setup, one might, for instance, be interested in assessing the credibility of the hypothesis H that θ1
Prediction results are in general notably lower than for improvised speech. For this dataset, MFCC and eGeMAPS features lead to higher accuracies than logMel. The best performance of 53.19% is achieved with the ACNN (MFCC with SV and eGeMAPS with MV).
In the experiments of predicting tags using only the narratives, multi-label rank (MLR) achieved by the HAN model is 90.94 on the validation set, which is higher than all the baseline systems. Additionally, we observe improvements in micro-F1 computed on the top N tags. Integrating the sentence level predictions with the document representation helps to improve MLR to 91.32 and also boosts the number of tags learned (TL). This supports our intuition that exploiting sentence level tag predictions can improve performance.
We partition the clean data into two author disjoint sets: Tnetwork used for training the neural network, and Tanalyze, which we analyze using the trained similarity function. Tnetwork is further split into a training set Ttrain and a validation set Tval (also author disjoint), used for early stopping when training the network. As the analysis relies heavily on a strong similarity function, the majority of the data (around two thirds) is used for Tnetwork. Positive Sim-instances are generated by using ti,tj∈Tα with i≠j, while negative instances are generated by using ti∈Tβ1 and tj∈Tβ2, where i,j,β1,β2 are selected at random, with β1≠β2. A balanced 50:50 data set is generated by generating the maximum number of positive instances for each student, and an equal number of negative instances. We found that using m=2 texts for determining the initial writing style yielded good results. Thus, profile Pα consists of |Tα|−1 pairs (τj,pj) with: pj =s(tj+1,t1)+s(tj+1,t2)2. and τj being the time in months since hand-in of t2. As mentioned, the lengths of the profiles depend on the number of texts written during the time, they spend in high school.
The final clustering is performed, obtaining five clusters: C1, C2, C3, C4, and C5, with a cluster error of EC=0.01407. Furthermore, we sampled two million random pairs of texts with random (different) authors, and computed the similarity for these samples, obtaining an average of 0.3470. However, the majority of students included in our data are located in C1, C2 and C5, indicating optimal or at least fair development through high school.
The baseline commonly used in the news summarization task is Lead-3 See et al. The underlying assumption is that the beginning of the article contains the most significant information. Inspired by the Lead-n model, we propose a few different simple models: MIDDLE-n, which takes n utterances from the middle of the dialogue, LONGEST-n, treating only n longest utterances in order of length as a summary, LONGER-THAN-n, taking only utterances longer than n characters in order of length (if there is no such long utterance in the dialogue, takes the longest one), MOST-ACTIVE-PERSON, which treats all utterances of the most active person in the dialogue as a summary. There is no obvious baseline for the task of dialogues summarization. We expected rather low results for Lead-3, as the beginnings of the conversations usually contain greetings, not the main part of the discourse. However, it seems that in our dataset greetings are frequently combined with question-asking or information passing (sometimes they are even omitted) and such a baseline works even better than the MIDDLE baseline (taking utterances from the middle of a dialogue). Nevertheless, the best dialogue baseline turns out to be the LONGEST-3 model.
The platform assigned people to conduct evaluation in such a way that each title was evaluated by 35 different users. The users could choose how many titles they wanted to evaluate. In the Training column, both, peer only and master only indicate whether the original title was only present in the master produced training data, peer produced training data or in both respectively. The table shows overall how many titles got the average rating above 3 for each test question. The authoritarian scenario leads to the worst performance, but this time master gets the highest percentage point of titles above 3 for Q3. In the authoritative scenario most of the titles have a clear pun and are related to food with the highest percentage point. The permissive scenario holds the best percentage points for Q6 and Q7. And the neglecting gets the best percentage points in Q2, Q4, Q5 and Q6.
For completeness, we report detailed results on bias effects for each of the six XWEAT tests and bilingual word embedding spaces for all 21 language pairs.
For completeness, we report detailed results on bias effects for each of the six XWEAT tests and bilingual word embedding spaces for all 21 language pairs.
Accuracy scores are divided among questions whose evidence lies in a single sentence (single) and across multiple sentences (multi), and among the two variants. Clearly, MCTest-160 is easier. Performance measures are taken from \newciteyin2016. Here we see our model outperforming the alternatives by a large margin across the board (>15%). The Neural Reasoner and the Attentive Reader are large, deep models with hundreds of thousands of parameters, so it is unsurprising that they performed poorly on MCTest. The specifically-designed HABCNN fared better, its convolutional architecture cutting down on the parameter count. Because there are similarities between our model and the HABCNN, we hypothesize that much of the performance difference is attributable to our training wheels methodology.
For the first category of questions, we also include a simple majority baseline. We aggregate results over the questions in each category, and report both macro and micro averaged accuracy scores. However, despite being the top-performing systems for the bAbI task, when predicting whether a participant entity is created, destroyed or moved, their predictions are only slightly better than the majority baseline. Compared to our local model ProLocal, EntNet and QRN are worse in predicting the exact step where a participant is created, destroyed or moved, but better in predicting the location. The weak performance of EntNet and QRN on ProPara is understandable: both systems were designed with a different environment in mind, namely a large number of examples from a few conceptual domains (e.g., moving objects around a house), covering a limited vocabulary. As a result, they might not scale well when applied to real procedural text, which justifies the importance of having a real challenge dataset like ProPara. Here, for each grid, we identify the Turker whose annotations result in the best score for the end task with respect to the other Turkers’ annotations. The observed upper bound of ∼80% suggests that the task is both feasible and well-defined, and that there is still substantial room for creating better models.
These results lead one to wonder if this “magic root” could be used to improve the performance also in our probabilistically justified methods. Taking a root of the estimated probabilities (before any summing) appears to improve the accuracy of all of our probabilistic methods, except for the speech-to-text direction of the CONDF method. The optimum root appears to vary a bit depending on the method and the lexicon, but based on these experiments, the cubic root seems like a good compromise that works well for all methods. The CONDF method, which performed best, benefits the least from the root. The asymmetric CONDL method obtains the best word accuracy (66.61%) of all methods when combined with the cubic root.
We submit seven runs: a) two runs (1 and 2) for the HACM model; b) two runs (3 and 4) for the HAEM model; and c) three runs (5, 6, and 7) that combine both systems. Detailed information on training regimes and the choice of hyperparameter values (e.g. layer dimensions, the application of dropout, etc.) for all the runs is provided in the Appendix. We decode using greedy search.
We conclude that the best performance gain is achieved when a small number of noise types are available during training. It can be seen that invariance training is able to generalize better to unseen noise types compared with multi-condition training. We note that our experiments did not use layer-wise pre-training, commonly used for small datasets. The baseline WERs reported are very close to the state-of-the-art.
In not using MMI, Self-Attention & Max achieved BLEU of 1.26, distinct-1 of 0.009 and distinct-2 of 0.076, which are the best scores in all the metrics. On the other hand, Self-Attention & Max using MMI achieved BLEU of 2.67, distinct-1 of 0.012 and distinct-2 of 0.171 while Seq2Seq using MMI achieved BLEU of 3.38, distinct-1 of 0.010 and distinct-2 of 0.119.
Averages are calculated after mapping Good, Mediocre and Bad to values 1, 2, and 3 respectively. Thus, a lower average score is better than a higher average score. In not using MMI, Self-Attention & Max achieved 2.251, which is the best among the methods. In using MMI, Self-Attention & Max achieved 2.583, which is better than Seq2Seq.
Automated Evaluation We observe that generating the SRL structure has a lower negative log-likelihood and so is much easier than generating either summaries, keywords, or compressed sentences — a benefit of its more structured form. We find keyword generation is especially difficult as the identified keywords are often the more salient, rare words appearing in the story, which are challenging for neural seq2seq models to generate. This result suggests that rare words should appear mostly at the last levels of the decomposition. Further, we compare models with entity-anonymized stories as an intermediate representation, either with NER-based or coreference-based entity anonymization. Entity references are then filled using a word-based model. Perhaps surprisingly, naming entities proves more difficult than creating the entity-anonymized stories—providing insight into the relative difficulty of different sub-problems of story generation.
Human-written stories feature a wide variety of events, while neural models are plagued by generic generations and repetition. (1) the number of unique verbs generated, averaged across all stories (2) the percentage of diverse verbs, measured by the percent of all verbs generated in the test set that are not one of the top 5 most frequent verbs. A higher percentage indicates more diverse events.
We explored a variety of different ways to generate the full text of abstracted entities—using different amounts of context and different granularities of subword generation. Each model evaluates n=10,50,100 different entities in the test set, 1 real and n−1 randomly sampled distractors. Models must give the true mention the highest likelihood. We analyze accuracy on the first mention of an entity, an assessment of novelty, and subsequent references, an assessment of consistency. This result highlights a key advantage of multi-stage modelling: the usage of specialized modelling techniques for each sub-task. We compare three models ability to fill entities based on context (using coreference-anonymization): a model that does not receive the story, a model that uses only leftward context (as in Clark et al. Having no access to any of the story decreases ranking accuracy, even though the model still receives the local context window of the entity as input. The left story context model performs better, but looking at the complete story provides additional gains. We note that full-story context can only be provided in a multi-stage generation approach.
c-TBCNN is slightly worse. It achieves 50.4% accuracy, ranking third in the state-of-the-art list (including our d-TBCNN model).
Their method utilizes more than 10k features and 60 hand-coded rules. On the contrary, our TBCNN models do not use a single human-engineered feature or rule. Despite this, c-TBCNN achieves similar accuracy compared with feature engineering; d-TBCNN pushes the state-of-the-art result to 96%. To the best of our knowledge, this is the first time that neural networks beat dedicated human engineering in this question classification task.
We can see that our system can reliably classify which articles are related to COVID-19, and that our interface can show related news to our users. Meanwhile, for some topic such as Arts & Sports and Education, the performance of the current system is still limited, which could be improved in future work.
From the table, we observe that our model outperforms all of the baselines. We argue that this is because our model is able to effectively capture the semantic information between student and reference answers. This is confirmed by the fact that MAN shows the superior performance among all baselines, as it not only aggregates sentence information within Transformer block, but matches words in both query sentence and answer sentence from multiple attention functions.
For each of our retrieval models, we performed a step-wise exhaustive search of the hyperparameter space over the four system hyperparameters for IBM BLEU on the development set: The length of the kn-best list the entries of which are used as queries for retrieval; the number of km-best-matching captions retrieved; the length of the final kr-best list used in reranking; the interpolation weight λ of the relevance score F relative to the translation hypothesis log probability returned by the decoder. The parameter ranges to be explored were determined manually, by examining system output for prototypical examples.
To further evaluate the generated summaries, we conduct human evaluations on informativeness (Info), conciseness (Conc) and readability (Read) of the extracted summaries. Two native Chinese speakers are invited to read the output summaries and subjectively rate on a 1-5 Likert scale and in 0.5 units, where a higher rating indicates better quality.
It is able to successfully assign a high score to (captain, officer) and also identify with high confidence that wing is not a type of airplane, even though they are semantically related. As an example of incorrect output, the model fails to assign a high score to (prince, royalty), possibly due to the usage patterns of these words being different in context. In contrast, it assigns an unexpectedly high score to (kid, parent), likely due to the high distributional similarity of these words.
Using a hand-tuned candidate generator allows our system to surpass supervised performance, however, completely automated methods also perform very well. The bump in performance seen around 1K documents is an artifact of scale-up using a CRF with embeddings, where there is some early overfitting. In both CDR tasks, we can closely match or exceed benchmark scores reported by TaggerOne (from +0.5 to -0.1 F1 points). In chemicals, NounPhrase candidate generation demonstrates better recall and improves on hand-tuned matchers by 0.7 points. In contrast, the NCBI task performs far below baseline using NounPhrase generators. This is due to that dataset’s more complex definition of mentions, including conjunctions (“breast and ovarian cancer”) and prepositional phrases (“deficiency of beta-hexosaminidase A”). These increase partial matches, hurting overall performance. With a hand-tuned candidate generator, we can account for these specific cases and dramatically improve performance and scoring -0.7 F1 points within the benchmark.
S3SS4SSS0Px3 POS analysis The SANCL dataset is overall very challenging: OOV rates are high (6.8-11% compared to 2.3% in WSJ) , so is the unknown word-tag (UWT) rate (answers and emails contain 2.91% and 3.47% UWT compared to 0.61% on WSJ) and almost all target domains even contain unknown tags Schnabel and Schütze (unknown tags: ADD,GW,NFP,XX), except for weblogs. Email is the domain with the highest OOV rate and highest unknown-tag-for-known-words rate. Both other methods fall typically below the baseline in terms of OOV accuracy, but MT-Tri still outperforms Asym in 4/5 cases. UWT tokens are very difficult to predict correctly using an unsupervised approach; the less lexicalized and more context-driven approach taken by FLORS is clearly superior for these cases, resulting in higher UWT accuracies for 4/5 domains.
Importance by rules. When looking at the above algorithm, the following rules for the topic reduction can be defined: Rule 1: Reduce the word network to only keep words in or above the core number defined by the user Rule 2: Re-add all nodes that were reduced from the network, but have a higher TF-IDF than the mean TF-IDF Using the k-core decomposition to reduce the number of words in a topic. In order to reduce the number of words in our clusters, we use the network structure we already have. We chose to use the k-core decomposition algorithm. Using this method, we receive the innermost important words in the network that are often used and clustered together. While the k-core decomposition is an efficient way to reduce network sizes and less-involved nodes, considering it as a direct clustering mechanism instead of the Louvain algorithm was not in question. Due to the nature of how k-core decomposition works, finding topic clusters from a network of words is not possible since it would only result in one cluster per timestamp. Our initial tests showed that the given network structure did not allow for core numbers higher than 10. When taking a closer look, it quickly became visible that using the k-core mechanism alone would not lead to the best result as it does not qualify as an efficient ranking algorithm. If we only ranked by core values, we would have many words that are in the inner core, but we could not tell any difference between those words. As the table shows, most of them are set in the very high core numbers which led us to add an extra ranking mechanism to the algorithm.
Our model NRPA outperforms all the baseline methods among all the five datasets which indicates the robust effectiveness of our personalized attention in modeling users and items. Besides, we can observe that (1) The methods with reviews perform better than those methods with only ratings (i.e., PMF and CTR). The reason may be that the reviews with the rich semantic textual information are powerful in capturing the feature of users and items. (2) Though both NARRE and TARMF utilize the attention mechanism to focus on more important information, our method NRPA achieves a better performance than them. We conclude that our model NRPA with hierarchical attention can exploit the deep personalized features (i.e., word level and review level) of users and items, which can represent users and items more precise. This result is consistent with our intuitive motivation that users and items should be characterized by individuation in recommendation.
We built our system over OpenNMT-Py[opennmt]. We used a batch size of 32, dropout rate of 0.5, RNN size of 512 and decay steps 10000. The maximum number of parallel arcs in the confusion network and maximum sentence length are set to 20 and 50 respectively. The confusion network contains noise and interjections such as *DELETE* and [noise], [laughter], uh, oh which leads to degradation in system performance. To mitigate the effect of such noise, we remove the whole set of parallel arcs if all the arcs are noise and interjection words. We also compare the system with a model trained on the best hypothesis of the extracted from the ASR lattice using Kaldi. Here, the confusion network encoder is replaced with a text encoder which shares weights with the factoid answer encoder. To asses the cross-domain generlizability, we also perform cross-dataset evaluation by evaluating our models on 840 samples of a KB based dataset(Freebase) and 470 samples of a machine comprehension dataset(NewsQA). The clean confusion network marginally outperforms the best-hypothesis model in ROGUE scores for cross-dataset evaluation and gives comparable results on BLEU scores. This shows that the confusion network system generalizes better on cross-domain noisy data and is less sensitive to noise introduced by new domains and noisy input signal, when compared with the Best-ASR-Hypothesis model. A plausible reason to this could be that the confusion network model is itself trained on a closed set of hypothesis, as compared to the Best-ASR-Hypothesis model which makes simplifying assumptions about the input signal. A compelling extension to the confusion network model is to adapt the copy attention over all the time-aligned hypotheses of the confusion network input. This would allow the confusion network model to copy among top-N words at any given time-step of the confusion network, instead of an erroneous word with the highest ASR score.
The first set of experiments compares the proposed hybrid models to the baselines. We observe similar word error rates, with the interleaved model outperforming the stacked model and outperforming the pyramidal LSTM baseline on the development data but not the test data. The LSTM/NiN baseline was strongest. In terms of training speed, the stacked model is fastest by a large margin, followed by the interleaved model and the LSTM/NiN model. To confirm that the attention mechanism is actually contributing to the hybrid model and not just passing on activations, we performed a sanity check by training a stacked hybrid model with attention scores off the diagonal set to −∞, and observed a drop of 1.25% absolute WER.
Next, we evaluate the different approaches to position modeling When using additive positional encodings the model diverged, while concatenating embeddings converged, albeit to rather poor optima. The key/query positional embeddings in isolation diverged, and combination with concatenated input embeddings did not improve results. Only the hybrid models were able to obtain results comparable to the baselines. We also tried combining hybrid models with positional embeddings, but did not see improvements over the model without positional embeddings.
The local diagonal mask was set to constrain the context to a window of 5 time steps, and Gaussian biasing variances were initialized to 9 (small setting) or 100 (large setting). For the stacked model, it can be seen that the biasing helps in general. The strongest model variant was the learnable Gaussian mask. Interestingly, it was important to initialize the Gaussian to possess a large variance. We hypothesize that this improves gradient flow early on in the model training, similar to how initializing LSTM forget gate biases to 1 (no forgetting) The interleaved hybrid model shows similar trends. Note that the sometimes inconsistent ordering between dev and test results can be explained by the fact that the TEDLIUM dev set is relatively small with only 500 utterances.
We extract the SR from the sentences as described above and use those as annotations. Note, that this corpus is annotated with the tuples (ACTIVITY, OBJECT, TOOL, SOURCE, TARGET) and the subject is always the person. Therefore we drop the SUBJECT role and only use (VERB, OBJECT, LOCATION) as our SR. Next we train a CRF with 3 nodes for verbs, objects and locations, using the visual classifier responses as unaries. Finally, we translate the SR predicted by our CRF to generate the sentences. As we can see the sense-labels perform better than the text-labels as they provide better grouping of the labels. All these points, if added to our approach, would also improve the performance.
The components are clause splitting (Clause), POS tagging and chunking (NLP), semantic role labeling (Labels) and word sense disambiguation (WSD). We manually evaluate the correctness on a randomly sampled set of sentences using human judges. It is evident that the poorest performing parts are the NLP and the WSD components. Some of the NLP mistakes arise due to incorrect POS tagging. WSD is considered a hard problem and when the dataset contains less frequent words, the performance is severely affected. Overall we see that the movie description corpus is more challanging than TACoS Multi-Level but the drop in performance is reasonable compared to the siginificantly larger variability.
Notably, the improvements are consistent for both seed models, where the speaker-independent models are trained using either the CE or the sMBR criterion. Updating all the model parameters yields smaller improvements as shown by the table. With speaker adaptation and sequence training, the HDNN system with 5 million model parameters (HDNN-H512L10) works slightly better than the DNN baseline with 30 million parameters (24.1% vs. 24.6%), while the HDNN model with 2 million parameters (HDNN-H256L10) achieves only slightly higher WER compared to the baseline (25% vs. 24.6%). we show the adaptation results with different number of iterations. We observe that the best results can be achieved by only 2 or 3 adaptation iterations, thought updating the gate functions θg further does not yield overfitting. To further validate this, we also did experiments with 10 adaptation iterations, but we still did not observe overfitting. This observation is in line with the that in the sequence training experiments, demonstrating that the gate functions are relatively resistant to overfitting.
Our attention-based neural transducer extends the two-stage graph-based parser of Zhang et al. Beside the improvement on parsing accuracy, we also significantly improve the parsing speed.
The source domain CTC models are trained with a different set of units than those used for the target domain. Each technique helps to improve the performance independently and the gains from both are additive. We have not explored reverberation and adding noisy to the audio, as the WSJ data is relatively clean.
Pretraining + finetuning alone improves the best performance from the previous section (training only on the target domain) by about 7% PER in absolute on the dev set, demonstrating that the modeling power is well transferred to the target domain. On top of that, LIN and data augmentation lead to consistent, further improvement.
The uni-directional student model is then trained on both supervised utterances with ground truth and unsupervised utterances with the pseudo-labels, using the CTC objective. A discount factor is applied to the pseudo-label loss term, which is tuned by grid search. It turns out a discount of 1.0 works best, perhaps because our pseudo-labels are relatively clean. For each gradient update, we use 8 supervised utterances and 32 unsupervised utterances. To avoid cluttering, we only give the performance with adaptation and data augmentation; results without adaptation (not listed here) show similar behavior. We observe that knowledge distillation effectively explores the teacher’s modeling power, and yields significant PER reduction.
The Slope method achieves an accuracy of 0.58, which is slightly better than R2. The second block shows the results of three methods introduced in Sect. The accuracy of the origin characteristic method is 0.76, which is an 18% improvement over the baseline from the literature. The salience-assisted entity discovery method achieves an accuracy of 0.78. We further improve the accuracy by combining all the three methods and, in the end, obtain 0.83 accuracy (OSS). This result tells us that these methods complement each other for this task. After applying our method to the unlinked mentions that appear in at least two tables in the whole WDC table corpus, we find about 900k mentions as potential novel entities.
We randomly choose 200 sentences generated by each model, and assign all the tasks to 3 individual human evaluators to score ranging from 1 to 5 according to the relevance and fluency of each paraphrasing pair. (1 refers to the worst and 5 refers to the best). Results show that our proposed model generates better paraphrasing samples than the VAE-SVG model in both relevance and fluency metrics on Quora dataset. This is partially because our model succeeds in taking advantage of the negative samples to learn better generation distribution.
We examine the robustness of Photon by evaluating the performance of the confusion detection module in handling ambiguous and untranslatable input. We compare to a baseline that uses a single-layer attentive bi-directional LSTM (“Att-biLSTM”). In comparison to the Att-biLSTM baseline, Photon obtains significant improvements in both translatability accuracy and the confusing spans prediction accuracy. These improvements are partly attribute to the proposed effective schema encoding strategy.
Each model configuration was trained and evaluated 100 times and the features with the highest importance for each iteration were recorded. To further fine-tune our models, we also perform three-fold cross-validated recursive feature elimination 30 times on each of the three configurations and report the performances of the models with the best performing feature sets.
It is used as an additional input to the dynamic function fϕ, to adapt its dynamic (AdaDyn) to each author. And as static conditioning (StatCond) of the LSTM decoder, in order to relax ha,t and allowing to focus more on temporal variations. To assess the contribution of these two features into the final results, we performed an ablation study where we removed each feature individually, and altogether. For both corpora, it is the addition of the two features together that yields the best results. StatCond always increases the performances significantly, as it helps the dynamic module to focus only on drifts. On S2, contrary to NYT, the AdaDyn alone does not improve performances of the base ResNet. In the next section, we analyze the learned latent trajectories to confirm this behavior.
Shutova et al. Since this corpus contains only 647 annotated examples, we instead evaluated the systems using 10-fold cross-validation. The feedforward baseline with skip-gram embeddings returns an F-score that is close to the linguistic configuration of Shutova et al, whereas the best results are achieved by the similarity network with skip-gram embeddings. In this setting, the attribute-based representations did not improve performance – this is expected, as the attribute norms by McRae et al.
On average, participants use more adjectives to describe nouns than adverbs to describe verbs. Controls use about 0.61 adjectives per noun, while patients use 0.84 adjectives on average. Similarly, patients use more adverbs to describe a verb on average than controls do. While patients use about 0.42 adverbs per verb, controls use only 0.23. However, these differences are not significant.
Although our primary goal was not a search for the best possible disambiguation system, we compare our results with the traditional approach based on the analysis of frequency of nearby words. Classifiers were induced using attributes that represent the frequency of the 5, 20 or 50 words surrounding the ambiguous word. For the first 5 words the CN approach outperformed the traditional method. This means that the local structure can be even more relevant than the frequency analysis of neighbors. Obviously, we are not suggesting the CN approach to replace the approaches based on semantic information provided by neighbors, since complex network measurements are statistically reliable only when computed in large texts. Still, it could be valuable to combine both strategies in disambiguation systems. As for the methods and algorithms, the differences regarding the best classifier are worth noting. The CN approach performs better with the kNN algorithm, while the Na ve Bayes algorithm is better in the traditional approach. These differences occur probably because of the distinct number of attributes in each approach. Finally, in 6 out of the 10 words considered, the traditional analysis with 5 neighbors outperformed the classifiers with larger numbers of neighbors.
As we can see, our model ReDR and its variants perform much better than the baselines, which indicates that the reasoning procedure can significantly boost the quality of the encoding representations and thus improve the question generation performance.
We can see that our method almost outperforms NQG in all aspects. For Naturalness, the three methods obtain the similar scores, which is probably because that the most generated questions are short and fluent, makes them have no significant difference on this aspect. We also observe that on the Relevance, Coherence and Answerability aspects, there is an obvious gap between the generative models and human annotation. This indicates that the contextual understanding is still a challenging problem for the task of the conversational question generation. For the relationship between a question and its conversation history, following the analysis in CoQA, we randomly sample 150 questions respectively from each method and observe that about 50% questions generated by ReDR contain explicit coreference markers such as “he”, “she” or “it”, which is similar to the other two methods.
We collect the Universal Decompositional Semantics Time (UDS-T) dataset, which is annotated on top of the Universal Dependencies Silveira et al. ; Nivre et al. (UD-EWT). The main advantages of UD-EWT over other similar corpora are: (i) it covers text from a variety of genres; (ii) it contains gold standard Universal Dependency parses; and (iii) it is compatible with various other semantic annotations which use the same predicate extraction standard White et al. Zhang et al.
Our evaluation is done by a single reference, case-insensitive BLEU score using the Moses package [papineni2002bleu].
Baseline Models. We compare our proposed model with two strong baselines: V-CNN-CRF is a vanilla approach that uses no pretrained model and instead learns sub-word representations from scratch. Span-BERT uses fixed BERT subword representations. All use the same CNN+CRF architecture on top of the subword representations. For each baseline, we conduct hyper-parameter optimization similar to Span-ConveRT: this is done via grid search and evaluation on the development set of restaurants-8k. Span-BERT relies on BERT-base, with 12 transformer layers and 768-dim embeddings. ConveRT uses 6 transformer layers with 512-dim embeddings, so it is roughly 3 times smaller.
We have also measured the 95% confidence intervals of the difference between the correlation coefficients of BiMu and Sg, following the method described in Zou \shortciteZou2007. According to these values, BiMu significantly outperforms Sg on RU-EN, and on French, Russian and Spanish NC corpora.
SimLex-999 and, as baseline, for our two word embeddings, i.e. w2v-gn and GloVe. The results are very similar, with node2vec proving to be slightly superior.
Even though strided CNN did not show any improvement, we made significant gains with the GLYNN CNN over the baseline and set a new SOTA F1 score. Even though strided CNN shows improvement over the baseline on other datasets and the dev sets, we do not understand why it underperforms in this case. We would like to investigate this problem further.
For entity typing tasks, all these pre-trained language models achieve high accuracy, indicating that the type of a medical entity is not as ambiguous as that in the general domain. BERT-MK outperforms BERT-Base, BioBERT and SCIBERT by 0.71%, 0.24% and 0.02% on the average accuracy, respectively. Without using external knowledge in the pre-trained language model, SCIBERT achieves comparable results to BERT-MK, which proves that a domain-specific vocabulary is critical to the feature encoding of inputs. Long tokens are relatively common in the medical domain, and these tokens will be split into short pieces when a domain-independent vocabulary is used, which will cause an overgeneralization of lexical features. Therefore, a medical vocabulary generated by the PubMed corpus can be introduced into BERT-MK in the following work.
In order to give a more intuitive analysis of our new KRL method for the promotion of pre-trained language models, we compare MedERNIE (TransE is used to learn knowledge representation) and BERT-MK (corresponding to KG-Transformer) on two relation classification datasets. As we can see, integrating knowledge information learned by the KG-Transformer model, the performance increases the F score by 0.9% and 0.64% on two relation classification datasets, respectively, which indicates that improvement of the knowledge quality has a beneficial effect on the pre-trained language model.
As expected, DeepBeam’s performance drops from S1, where both noise and speaker are seen during training, to S4, where neither is seen. However, in terms of SNR, even DeepBeam S4 significantly outperforms MVDR, which is the benchmark in noise suppression. In short, of “cleanness” and “dryness”, most algorithms can only achieve one, but DeepBeam can achieve both with superior performance.
As can be seen, DeepBeam outperforms the other algorithms by a large margin. In particular, DeepBeam achieves >4 MOS in some noise types. These results are very impressive, because DeepBeam is only trained on simulated data. The real-world data differ significantly from the simulated data in terms of speakers, noise types and recording environment. What’s more, some microphones are contaminated by strong electric noise, which is not accounted for in Eq. Still, DeepBeam manages to conquer all the unexpected. Neural network used to be vulnerable to unseen scenarios, but DeepBeam has now made it robust.
However, SeqGAN suffers from mode collapsing, as seen by its high duplication rate in its generated samples (93%, the 3-rd row). The performance of all sequential models drop uniformly with longer sentences and become increasingly difficult to recover coherent tree structures. These results indicate the fundamental defect of the sequential models.
In order to compare the GLA-learned SOT grammar, perceptron-learned HG grammar, and the conjugate gradient learned MaxEnt grammar in terms of predicting data variation, we compute the KL-divergence between the true, empirically observed distribution over output labels in our test set and the distribution predicted by a given grammar. We compute this distance metric for each of the 27 input patterns for which there is some observed data. We then take a weighted average over the 27 inputs and output distributions based on the number of observed sentences in the test data. The GLA and perceptron algorithms are trained on 50 iterations over the training data and with the model parameters optimized as described above. The KL-divergence is averaged over ten runs. Smaller KL-divergence values indicate better estimates of the probability distributions. The perceptron HG grammar’s average KL-divergence is lower than that of the GLA SOT grammar. The MaxEnt grammar has the smallest KL-divergence with the true distribution, meaning that it most accurately models variation in the data.
The first baseline strategy always predicts a uniform distribution over word order labels, and the second baseline strategy only predicts the single most likely word order for each input pattern, which gives an upper bound on accuracy. All three of the learners perform well in terms of accuracy and predicting variation over output patterns in comparison with these two baselines. The perceptron HG model outperforms the GLA SOT model in both measures, The performance of the MaxEnt and perceptron models is similar. The perceptron grammar outperforms the MaxEnt model slightly with respect to accuracy and the MaxEnt model outperforms the perceptron grammar by a fair amount in predicting variation. The overall performance of the grammar learned by the perceptron is solid. Because of its simplicity and because it is an online learner, we conclude that it is an effective and elegant way to model grammar learning.
We can see that Lemma-matching is a strong baseline for event coreference resolution. hdp-lex provides noticeable improvements, suggesting the benefit of using an infinite mixture model for event clustering. Agglomerative further improves the performance over hdp-lex for WD resolution, however, it fails to improve CD resolution. We conjecture that this is due to the combination of ineffective thresholding and the prediction errors on the pairwise distances between mention pairs across documents. Overall, hddcrp∗ outperforms all the baselines in CoNLL F1 for both WD and CD evaluation. The clear performance gains over hdp-lex demonstrate that it is important to account for pairwise mention dependencies in the generative modeling of event clustering. The improvements over Agglomerative indicate that it is more effective to model mention-pair dependencies as clustering priors than as heuristics for deterministic clustering.
In this experiment, we address the third research question R3: What is the impact of employing a well-fitted learning model on local performance? To do that, we ran various supervised learning models to find out the best performing one. These models include: Logistic Regression, Gradient Boosting, XGBoosting, PCA + Logistic Regression, SVM, Naive Base, LDA, Adaboost and feed-forward neural network. Each Model is tuned to its optimum performance. The rest of the results can be found in our Github project.
First we compare various frequency decompositions and ensemble methods for subband classification. We find the stacked generalization to yield significantly better results than majority voting; it consistently achieves over 10% improvement over majority voting for all subband decompositions considered here.
As specified earlier, we evaluate our models on the full ranking setting without any explicit reranking step. The full model—with both Conformer-Kernel and explicit matching submodel—performs significantly better on NDCG@10 and MRR compared to the best traditional runs from the 2019 edition of the track. The model also outperforms the DeepCT baseline which is a QTI-based baseline using BERT. The other BERT-based baselines outperform our model by significant margins. We believe this observation should motivate future exploration on how to incorporate pretraining in the Conformer-Kernel model. Finally, we also notice improvements from incorporating the ORCAS data as an additional document descriptor field.
4.2.1 Effect of CIDErBtw strategies. All baseline models obtain better performances when using CIDErBtw weighting in training process, for both MLE or SCST, which suggests that our method is widely applicable to many existing models. Specifically, our method both reduces the CIDErBtw score and improves other accuracy metrics, such as CIDEr. This shows that the generated captions become more similar to ground-truth captions, while more distinctive from other images’ captions since redundancy is suppressed. Among the four baseline models, CIDErBtw reweighted loss and reward have the largest effect on Transformer [vaswani2017attention]. Most likely the multi-head attention and larger receptive field of Transformer allow it better extract details and context from the image that is distinctive. Compared to only using reweighted loss and reward (Transformer+SCST+CIDErBtw), adding the CIDErBtw reward in RL improves both the CIDErBtw and retrieval metrics significantly (i.e., improves distinctiveness), at the expense of a small decrease in accuracy (CIDEr).
We first evaluate our proposed multilingual finetuning technique on 25 languages using the existing mBART model. We compare bilingual finetuning from mBART (BL-FT), multilingual training from scratch (ML-SC), and multilingual finetuning (ML-FT) by quantifying the BLEU improvement over the bilingual training from scratch baseline.
HPs were tuned on the ASN corpus for every variant separately, without shuffling of the training data. For the best performing variant, without syntactic information (tag,cut), we report the results with HPs that yielded the best s@1 test score for all anaphors (row 4), when training with those HPs on shuffled training data (row 5), and with HPs that yielded the best s@1 score for pronominal anaphors (row 6). We also note that HPs that yield good performance for resolving nominal anaphors are not necessarily good for pronominal ones (cf. Since the TPE tuner was tuned on the nominal-only ASN data, this suggest that it would be better to tune HPs for pronominal anaphors on a different dataset or stripping the nouns in ASN. Contrary to shell noun resolution, omitting syntactic information boosts performance in ARRAU-AA. We conclude that when the model is provided with syntactic information, it learns to pick S-type candidates, but does not continue to learn deeper features to further distinguish them or needs more data to do so. Thus, the model is not able to point to exactly one antecedent, resulting in a lower s@1 score, but does well in picking a few good candidates, which yields good s@2-4 scores. (ctx) achieves a comparable s@2 score with the variant that omits syntactic information, but better s@3-4 scores. Further, median occurrence of tags not in {S, VP, ROOT, SBAR} among top-4 ranked candidates is 0 for the full architecture, and 1 when syntactic information is omitted. The need for discriminating capacity of the model is more emphasized in ARRAU-AA, given that the median occurrence of S-type candidates among negatives is 2 for nominal and even 3 for pronominal anaphors, whereas it is 1 for ASN. This is in line with the lower TAGBL in ARRAU-AA.
We find that half of the top fifteen subreddits are politically related. This skew may lead to possible biases in the training process. A plausible explanation for this bias can be found in the way the dataset is collected. Since we heuristically filter for reputable outbound links it is likely that we choose subreddits where people post outside news.
Surprisingly, the no-context baseline achieves high performance, comparable to some recently-published systems, showing that WikiHop is actually possible to solve reasonably well without using the document at all. One possible reason for this is that this model can filter possible answers based on expected answer type Sugawara et al. This model substantially outperforms the unlearned baseline reported in the WikiHop paper Welbl et al.
The full version of our model which uses both the question type module and copy loss mechanism obtains the best results on all of metrics, achieving a new state-of-the-art result of BLEU-4 13.90 for the challenging AG-QG task. It outperforms the baseline model with 0.73 points and beats the previous best result by 0.67 points.
We conduct extensive experiments with different model modules, where k is set to 1 in decoding. Baseline: Our baseline model is a general sequence-to-sequence attention model enhanced with copy mechanism. Baseline+Type : It adds the question type module to the baseline model. Baseline+CopyLoss: Based on the baseline model, it calculates and minimizes the additional copy loss. Baseline+CopyLoss+Type : This is the full version of our proposed model. That is, the question type module is applied to the baseline model and the extra copy loss is also calculated. Upper Bound: Since our full model incorporates the question type prediction part, the accuracy of question type prediction will undoubtedly affect the final quality of generation. If the right question type is given for every test sample, we get the upper bound of our model.
However, our question type predict module cannot achieve a 100% accuracy, and once a wrong question type is offered to the decoder, it will have negative influence on the generating quality. It shows that without the answer as input, to predict the types of questions that should be asked for a given sentence is non-trivia.
We also conduct human evaluation to judge the quality of questions generated by our model and the baseline model, respectively. We take three metrics into consideration: 1) Fluency: it measures whether the question is grammatical; 2) Relevance: it measures whether the question is asked for and highly related to the source sentence; and 3) Answer-ability: it measures whether the generated question can be answered by the information of the source sentence. We randomly selected 100 sentence-question pairs generated by different models, and asked three annotators to score the questions on a 1−5 scale (5 for the best). We also exploit the Spearman correlation coefficient to measure the inter-annotator agreement. It shows that the consistency among three annotators is satisfying, and our generated questions are better from different perspectives.
The dataset was built from case reports, requiring the machine reader to answer the query with an entity which is either a medical problem, a test or a treatment. Unlike in the original paper, we first pre-process the dataset by recreating the data splits so that those instances whose answer isn’t found literally in the passage text are removed. This increases the scores across the board, and, crucially, ensures that pointing-based models can also be applied to the dataset. The first thing to note is that although we can fit the training data well with MemNet, the generalization to unseen data remains poor as we only obtain 16.8 F1 on the test set. This is much lower than that for other neural readers, and even lower than that for the maximum frequency and embedding baselines. Simple modifications such as changing the definition of the memory from window-based to either sentences or key-value pairs have no positive effect. We now present the factors that strongly affect this low performance. This is a more direct way of biasing the model towards solutions preferred by hard selection than using attention supervision (cf. Chen and Durrett The feature vector is finally simply concatenated with other output vectors from eq. The output layer weights are expanded correspondingly, i.e. to W∈RC×(4d+C). When we use pretrained embeddings, this advantage is even greater at 36.7 F1. Clearly, the network can use the added feature to good effect, while aggregating without the added attention signal appears to be inadequate for good performance. An obvious next question is whether the aggregated information can contribute positively to the results at all. If we drop the aggregated output vector and only keep the feature vector in the output layer (attention-feat-only), we observe a further improvement by two points, which could mean that the aggregated output vector is confusing the network, at least on these simpler instances which can be solved by using the attention feature alone. The BiDAF reader, which adds additional complexity to the attention mechanism in comparison to SAReader and GAReader, performs the best on CliCR.
We consider two cases depending on the number of examples per query to evaluate different bottleneck features for QbE-STD. In case of a single example per query, the corresponding features constitute the template. We can see that the Portuguese (PT) feature performs the best among the monolingual features with very close performance from Spanish (ES) feature. (PT, ES, RU, FR, GE) languages respectively. The 3 language network uses the 3 best performing monolingual training languages. We also observe that PT-ES-RU-FR-GE features significantly outperform PT-ES-RU features indicating that additional languages for training provide better language independent features.
We have only one example per query in case of QUESST 2014 dataset, thus the corresponding bottleneck features constitute the template. We can see that the bottleneck feature from Portuguese (PT) performs the best among monolingual features for all three types of queries. We have a similar observation as in SWS 2013 that PT-ES-RU-FR-GE network performs better than PT-ES-RU network indicating that more language for training helps in obtaining better features for DTW.
We observe that T1 queries perform best with the model trained using 2 frozen layers, whereas T2 and T3 queries perform best with the model trained using 1 frozen layer. It can be attributed to the training of the models using SWS 2013, which enables the network to optimize for that database when fine-tuning all layers of the feature extractor.
In the End-to-End model, we can freeze the parameters of the CNN based Matching network and consider it as a loss function for fine tuning the feature extraction network. This loss function enables the feature extractor to learn and generate features which produce more discriminative similarity matrices to be classified by the CNN. It can be observed through the performance of the system. We observe that the feature extractor retrained with 1 frozen layer gives the best results which is significantly better than the bottleneck features indicating the importance of CNN based loss function.
We use four different models: Linear Regression, Logistics Regression, Random Forest Regression and Random Forest Classifier. All reported F1 score in tables are only for classification on the clickbait class. In case of logistics regression, the F1-score is 0.56 and ROC-AUC is 0.723. Random Forest Regression witnessed the best performance of about 0.036 MSE and 0.819 ACC. This improvement in MST can be credited to the use of mean judgments as the target variable, rather than binary value of either clickbait or non-clickbait.
In overall, the post-related group has the best performance. Linear Regression undergo better performance under target-related and relation groups, which can be attributed to the outweigh in the number of in no-clickbait instances. The best MSE performance is 0.036 with Random Forest Regression classifier on the post-related group. The F1-scores for Linear Regression classifier are both 0, which can be explained from the unbalanced in data set between the two clickbait and no-clickbait class.
Our model outperforms CNN, TCNN-SM, and Bi-GRU on all datasets. CNN can capture k-gram information from text. However, the length of the extracted segments is limited by the window size and causes its failure in capturing long term dependency. TCNN-SM equips CNN with memory functional column to retain memories of different granularity and fulfills selective memory for historical information. Bi-GRU could also capture long term dependency in the text. This poor performance of TCNN-SM and Bi-GRU is partly caused by its equal treatment of words. Rather than treating words differently as we do in our method, Bi-GRU performs mean-pooling to get representations of texts. This step does not take into consideration the relative importance between each word, messing semantically important words into a collection of unimportant ones. In contrast, our method captures local information with CNN to obtain their relative importance. To demonstrate this point, we remove the neural tensor layer from our architecture but keep others unchanged. The comparisons is carried out on all datasets. 3. ID-LSTM and Self-attentive LSTM use different ways to extract the words that contribute more semantically in the text. Except Ag’s news and 20Newsgroups datasets, our model performs better than ID-LSTM and Self-attentive LSTM. This performance gain may be achieved by adding the neural tensor layer for capturing the surrounding contextual information around each word. This neural tensor layer brings compositional information between forward and backward RNN in a direct way. In the current experimental setting, the method with neural tensor layer beats its counterpart trained without it. 4. RCNN also uses recurrent and convolution structure in its model. RCNN utilizes recurrent structure to learn preceding and post contexts, and then concatenates them with word embedding to represent each word. After feeding each word representation to a fully connected neural network, it uses a max-pooling layer to get a fixed-length vector for the text. Compared with their model, CRNN uses convolution operation to learn the importance of each word from different aspects and utilizes the neural tensor layer to fuse preceding and post context information.
We exploit our method’s performance with various filter region sizes in different classification tasks. In our experiment, we explore the effect of the filter size using one region size and different region sizes. For a fair comparison, the total number of filters are the same. On the contrary, for sentiment analysis, a large window size usually results in high performance. This is partially due to the fact that sentimental sentences always have more adjectives/adverbs to enhance its expressiveness or for emphasis.
Our multi-scale architectures use the same configuration above for every transformer layer, with added upsampling and downsampling modules. We used a single transpose convolution layer with an appropriate kernel size and stride for upsampling. Each scale increases by a factor of 4 from the previous. All models were trained with mixed precision arithmetic and optimized using Adam (kingma2014adam) with the learning rate increased linearly to 2.5×e−4 over 40,000 warmup steps and then annealed to 0 over a million steps using a cosine schedule (radford2018improving). We tuned the dropout rates in our Wikitext-103 in the range of 0.1 to 0.5 with increments of 0.05, since we found that our models were able to overfit the data quite easily. In particular, we consider a sequence of 512 tokens, shuffle the first 256 tokens and observe the likelihood that an already trained model (without perturbations) assigns to the subsequent 256 tokens. When the shuffled context is more than 50 tokens away, the change in likelihood, compared to when the model is presented with a completely unshuffled context is already minimal, indicating that the model may not be using word order beyond this distance.
A benefit of this model over vanilla transformer LMs is that they are typically much faster at inference. This is because transformer layers at the coarser scales do not need to run at every time step, but only once every ki steps for a particular scale. A hallmark of the recent progress in language modeling has been improvements not only in perplexity, but sample quality as well. While evaluating generative models of text purely based on their samples is extremely difficult and an on ongoing effort (cifka2018eval; semeniuta2018accurate), we would like to ensure that we aren’t losing out on sample quality with respect to vanilla transformer LMs. We could not observe any significant difference in sample quality between the different models. We also report the time it takes for each model to generate a sequence of 256 tokens given a context of 64 tokens.
We further conducted experiments to extrinsically evaluate the GECOR model in task-oriented dialogue with the success F1 metric. This is also to evaluate our multi-task learning framework in integrating the GECOR model into the end-to-end dialogue model. In addition to training the baseline TSCP model on the ellipsis, co-reference and mixed dataset, we also trained it on the dataset with only complete user utterances. This is to examine the ability of the baseline model in using correct contextual information presented in user utterances. Resolution performance of the integrated GECOR The performance is slightly lower than when the GECOR is trained independently as a stand-alone system. This suggests that the GECOR is able to perform well when integrated into a dialogue system. The overall results demonstrate that the proposed multi-task learning framework for the end-to-end dialogue is able to improve the task completion rate by incorporating an auxiliary ellipsis/co-reference resolution task.
The MCD values are calculated for both joint and separate training of spectrum and prosody features. We conducted the experiments for three emotion combinations: 1) neutral-to-angry, 2) neutral-to-sad, and 3) neutral-to-surprise. We observed that all separate training settings consistently outperform those of joint training settings by achieving lower MCD values. For example, the overall MCD of separate training is 8.71, while it is 10.23 for joint training.
In this experiment, we conducted three emotional conversion settings: 1) neutral-to-angry, 2) neutral-to-sad, 3) neutral-to-surprise. We also report the overall performance. As for RMSE results, first of all, we observe that the proposed prosody conversion, based on CycleGAN with CWT-based F0 decomposition outperforms the traditional baseline (denoted as baseline) where F0 is converted with LG-based linear transformation method. Secondly, the proposed separate training with CycleGAN for spectrum and CWT-based prosody conversion overall achieves better result (RMSE: 63.03) than separate training (RMSE: 65.05), which is also consistent with the objective evaluation. PCC results suggest that both joint and separate training of CWT-based F0 features achieve similar results.
Overall Prediction Time: Overall, due to enhanced feature formation and reduced →wtv loading time, SCDV-MS will predict faster compared to the original SCDV. However, we observed an insignificant difference in the prediction time and model size as SCDV sparsifies the final document vectors (both equally sparse). Furthermore, after reducing the SCDV-MS →wtv to 2000-dense dimension features using auto-encoders, we observed a distinctive reduction of 8.5 times in the prediction time. One can directly store the reduced →wtv for the complete vocabulary instead of the reduction model. Furthermore, one can directly also reduce the words appearing in the documents i.e. use a real-time reduction model during prediction.
Compared with the traditional single-decoder model, our dual decoders can generate diverse responses with given specific sentiment. The three models obtain comparable results in terms of BLEU score and Average value, but we can observe a big difference in sentiment accuracy and distinct value.
The best performing system for the Shared Task was STUMABA-D by \newcitebohnet2011stumaba, which leverages a large-scale n-gram language model. The joint model TBDIL significantly outperforms the pipeline system and achieves an improvement of 1 BLEU point over STUMABA-D, obtaining 80.49 BLEU without making use of external resources.
that larger context windows, and specifically those compatible with the window size of the contextualized encoder, lead to better performance. On the other hand, we find that if the inference segment size is fixed in advance, then training a model using that segment size is better then using the maximum length. Finally, there is a substantial drop when either training or testing at the single sentence level.
The primary result is that the incremental model is able to effectively use randomly initialized Spans and PairScore weights without teacher forcing.
To quantitatively show the reliability of the above results, we compare the ranking of true authors and frequently appearing false authors of each paper, and see whether true authors are indeed ranked higher than frequently appearing false authors. More precisely, every time a frequently appearing false author is ranked higher than a true author, we count it as a violation. For each paper, we sampled the top-N most frequently appearing false authors for comparisons, where N is the number of the actual authors. We observe that the average rank violation count of TaPEm is about 8 times less than that of Camel, which again demonstrates the effectiveness of our pair embedding framework.
metrics are reported in our quantitative evaluation of the performance of the proposed schemes. The improvement in BLEU-n is greater for greater n; TPGN particularly improves generation of longer subsequences. The results clearly attest to the effectiveness of the TPGN architecture.
We can see that the neural language models on average obtained higher accuracy than LDA, although LDA achieved very competitive results on the last six tasks. It is interesting to observe that the word2vec algorithm obtained higher accuracy than paragraph2vec despite the fact that word2vec only considered sequences in which the movies were seen without using the movie synopses, and that, unlike word2vec, the paragraph2vec model was specifically designed for document representation. This result indicates that the users have strong genre preferences that exist in the movie sequences which was utilized by word2vec, validating our assumption discussed earlier. Furthermore, we see that the proposed approach achieved higher accuracy than the competing state-of-the-art methods, obtaining on average 5.62% better performance over the paragraph2vec and 1.52% over the word2vec model. This can be explained by the fact that the HDV method successfully exploited both the document content and the relationships in a stream between them, resulting in improved performance.
The performance for CTR prediction of different models on Criteo dataset : Learning feature interactions improves the performance of CTR prediction model. This observation is from the fact that LR (which is the only model that does not consider feature interactions) performs worse than the other models. As the best model, DeepFM outperforms LR by 0.86% and 4.18% in terms of AUC (1.15% and 5.60% in terms of Logloss) on Company∗ and Criteo datasets. Learning high- and low-order feature interactions simultaneously and properly improves the performance of CTR prediction model. DeepFM outperforms the models that learn only low-order feature interactions (namely, FM) or high-order feature interactions (namely, FNN, IPNN, OPNN, PNN∗). Compared to the second best model, DeepFM achieves more than 0.37% and 0.25% in terms of AUC (0.42% and 0.29% in terms of Logloss) on Company∗ and Criteo datasets. Learning high- and low-order feature interactions simultaneously while sharing the same feature embedding for high- and low-order feature interactions learning improves the performance of CTR prediction model. DeepFM outperforms the models that learn high- and low-order feature interactions using separate feature embeddings (namely, LR & DNN and FM & DNN). Compared to these two models, DeepFM achieves more than 0.48% and 0.33% in terms of AUC (0.61% and 0.66% in terms of Logloss) on Company∗ and Criteo datasets.
We use the area under the curve (AUC) of the receiver operating characteristic (ROC) curve as the evaluation metric. We test the models that perform the best on the validation set. We compare the performance of SESA when using randomly initialized word embeddings versus the pretrained embeddings. SESA with pretrained word embeddings achieves good results outperforming most baselines and performing at the level of gradient boosting while (1) avoiding feature engineering; (2) being interpretable and (3) providing re-usable by-products that we describe in the following section.
In both setups PERL with 10 layers is the best performing model. Moreover, for each number of layers, PERL outperforms the other two models, with particularly substantial improvements for 5 and 8 layers (e.g. 7.3% and 6.7%, over BERT and Fine-tuned BERT, respectively, for B → E and 8 layers).
We observe four patterns in the results. First, PERL with our pivot selection method, that emphasizes both high MI with the task label and high frequency in both the source and target domains, is the best performing model. Second, PERL with Random-Frequent pivot selection is substantially outperformed by PERL, but it still performs better than BERT (in 3 of 4 setups), probably because BERT is not tuned on unlabeled data from the participating domains. Yet, PERL with Random-Frequent pivots is outperformed by the Fine-tuned BERT in all setups, indicating that it provides a sub-optimal way of exploiting source and target unlabeled data. Third, in 3 of 4 setups, PERL with the High-MI, No Target pivots is outperformed by the baseline BERT model. This is a clear indication of the sub-optimality of this pivot selection method which yields a model that is inferior even to a model that was not tuned on source and target domain data. Finally, while, unsurprisingly, PERL with oracle pivots outperforms the standard PERL, the gap is smaller than 2% in all four cases. Our results clearly demonstrate the strong positive impact of our pivot selection method on the performance of PERL.
Another design choice we consider is the impact of the type of fine-tuning data. As above, we explore this question on 4 arbitrarily selected domain pairs.
To relieve the difficulties of learning sound-image matching, we use an image classifier and a sound classifier to clean up the dataset automatically. If the classification results for the image and sound are not the same, the sound-image pair would be discarded. After this procedure, 78% of the data is discarded. Because the above data cleaning procedure is automatic, it cannot be perfect, but it remarkably improves the quality of the generation results. Because some objects are very rare in the training data, to make the training of audio-to-image plausible, only the sounds classified into dog, drum, guitar, piano, plane, speedboat, dam, soccer, baseball by SoundNet are used in the following experiments. The above nine classes are chosen because they are the classes with the most examples in the training data. The total number of sound-image pairs for training is 10701, and the total number of sound segments for testing is 248. Sounds belonging to some classes can generate relatively high quality images. For speedboat or plane, there are eye-catching objects in the generated images. The generator truly generates the images that are interpretable to some extent. Some classes of images get worse quality of images than others. This may be because the imbalance and variance in different classes of training data. The number of training examples may explains why some classes performed better than the others. We also found that for all the sounds classified into drums, they still have very high diversity. There are many kinds of drum and are played in variant places. It becomes an obstacle for model to generate image from the sound of such class. On the contrary, in some classes like plane and speedboat with relatively common background such as blue sky and blue ocean, it is easier for model to generate high quality image in these classes. In our dataset, we can assume that classes with natural landscape in background such as plane, speedboat, baseball, soccer, and dam are purer than classes with variant background such as dog, drum, guitar, and piano.
The images in the same row are generated from the same audio with different volumes. The audio files can also be found in https://wjohn1483.github.io/audio_to_scene/index.html. The numbers on top indicates the scale of volume that we modified our sound files. In those images, we can see different scale of splashes. As the volume goes up, the scale of splashes become larger. We can see that our model truly learned the relation between characteristic of sound and image. In this case, the volume of sounds is reflect on splashes.
We can see that most people think the model with all tricks performed the best. Although those models get close scores in Inception score, they get scores which have at least 0.4 gap between different models.
For each model variation, we report the average metrics for the ten trials, which include the average sensitivity, specificity, and harmonic mean between these two metrics, namely F1-score. We do not report the average accuracy for the model, because, in the presence of highly unbalanced data (93.41% are non-events), the accuracy is a misleading metric. Measuring the accuracy is not useful, considering that a model that always predicts non-event triggers has more than 90% accuracy. For these reasons, we focus our analysis on the more balanced F1-score metric. To thoroughly analyze the F1-score metric on the testing set we also report the confidence intervals (CI) at 95% level of confidence and the p-value for a t-test between the model considered and the best model of each table. We used the F1-score metric on the testing data for the t-test. For the proposed model (RNN), we run ten trials for seven different sets of features (models 1 to 7). The following models are models with the full set of features removing some particular set of features for each experiment. Although we tried different numbers of Bi-LSTM layers and hidden units, we only evaluated on the testing set the architecture that achieved the best performance during the preliminary studies, i.e., the architecture with a single Bi-LSTM layer with 15 hidden units.
In this section, we discuss the conclusions we derive from the experiments with the variations of the proposed model (models 1 to 7) and the models based on the selected baseline (models 8 to 11). In particular, we focus the discussion on the best of the proposed models (model 7), the model without the contextual word embeddings (model 2), and the two best baseline models (models 8 and 9). for each model against each other on the testing data. Model 2. We hypothesize that the contextual embeddings are a crucial factor for boosting the performance of the proposed models. Therefore, in model 2, we exclude only the contextual word embeddings to asses the impact of them on the performance. Since this model lacks the contextual word embedding, we expected a poor performance in comparison to model 7 (which has the contextual word and sentence embeddings). The poor performance of model 2, which is closer to the baseline than to model 7, provides evidence to support our hypothesis. These p-values indicate that we have more statistical evidence to assume model 7 and 2 are different, but less evidence to tell apart model 2 and the baselines. Moreover, the inclusion of the other features (W, Sp, T, P, D, E) are not useful for the ED task once the contextual word embeddings are included. We derive this conclusion from the fact that the best model only has B, S as features, indicating that once the contextual word (B) and sentence (S) embeddings are present in the model, other features can be removed with a positive effect on performance. In conclusion, we found evidence to support the hypothesis that contextual embeddings play a key role in the overall performance of the proposed models.
The goal of this preliminary experiment was to assess the impact of each feature on the overall performance. This allows us to remove those features that are not useful for the task or that add noise and therefore harm the performance. We performed five trials for each set of features using the best architecture found in the previous preliminary study. the Word2vec word embedding (W), and the Spacy contextual embeddings (Sp), respectively. Model 9 is the one with the best performance. However, the high p-values show that there is no statistical difference between the best model and the other two. Furthermore, there is no statistical difference between the best model and the model with all the features (model 1). These results show that removing any of these three features (E, W, Sp) has a negligible impact on the overall performance. We further study the effect of including and excluding these features in the testing set. However, based on these results, it is expected that, in the presence of the other attributes, excluding these three features improves performance.
Approximately 9 hours of manually transcribed monolingual English soap opera speech was available in addition to the language-balanced sets. This data was initially omitted to avoid a bias toward English. However, pilot experiments indicated that its inclusion enhanced recognition performance. The balanced set was therefore pooled with the additional English data for the experiments described here.
However, PL(k) systems are better than MERT in testing. PL(k) systems consider the quality of hypotheses from the 2th to the kth, which is guessed to act the role of the margin like SVM in classification . Interestingly, MIRA wins first in training, and still performs quite well in testing.
As data increases, it will take longer time for decipherment. However, it is notable that the decipherment processes for the data of the first 6 months and the last 6 months are independent and thus it is possible to run the decipherment algorithm on these two datasets in parallel and merge the decipherment results. We can see that deciphering in parallel does not result in a significant decrease of accuracy. Therefore, we can split a text stream into several small parts and decipher them in parallel, which makes the decipherment more efficient.
We can see that RotatE outperforms all the previous models. The performance of RotatE is significantly better than other methods on S3, which is the most difficult task.
Consider the safety property “the robot enters the human zone with probability at most 0.1”. For each scenario, we report the number of states and transitions of the MDP model, the number of states in the computed counterexample, and the number of sentences in the generated counterexample explanation. We also report the number of binary and real variables, and the running time for computing the MILP solution. The experiments were run on a laptop with 2.0 GHz Intel Core i7-4510U CPU and with 8.00 GB RAM. We impose a time-out of 1 hour. All MDP models and MILP formulations are available online. It is nontrivial to interpret counterexamples represented as subsystems. Consequently, the running time to solve the optimization problem becomes much faster. As a result, the number of variables in MILP encoding increases in both methods. Nevertheless, for all five cases, counterexamples and explanations are computed by the proposed approach in less than 1 minute. Therefore, we expect that the proposed approach would scale well for larger models as well.
Following Liu et al. Also utilizing exemplar information, our AdaDec model outperforms Seq2seq by a larger margin: 1.3 for Rouge-4 and 1.1 for Bleu. We further study whether we can get further improvements by combining both. AdaDec+AttExp achieves around 0.5 absolute improvements over AdaDec, less than those by AttExp over Seq2seq. This provides evidence that, to some extend, the ways AttExp and AdaDec incorporate exemplar information might be complementary. Wiseman et al. Liu et al. They encode the table structures by using (a) position and filed embeddings, and (b) structure-aware attention and gating techniques. These techniques are beyond the scope of this work, which focuses mainly on the decoding end.
In this section, we evaluate the performance of the individual modules of the equation parsing process. We report Accuracy - the fraction of correct predictions. In each case, we also report accuracy by removing each feature group, one at a time. In addition, for equation tree prediction, we also show the effect of lexicon, projectivity, conforming to syntactic parse constraints, and using lexicon as features instead of hard constraints.
While resemblance to human-written image narratives may not necessarily guarantee better qualities, our model, along with DenseCap, showed highest resemblance to human-written image narratives. As we will see in human evaluation, such tendency turns out to be consistent, suggesting that resemblance to human-written image narratives may indeed provide a meaningful reference.
It is very clear that the questions generated from our proposed model of parallel VQG and VQA outperformed by far the questions generated from VQG only. This is inevitable in a sense that VQG module was trained with human-written questions that were intended to train the VQA module, i.e. with questions that mostly have clear answers. On the other hand, our model deliberately chose the questions from VQG that have evenly distributed probabilities for answer labels, thus permitting multiple possible responses. In questions generated from our model, different responses are possible, whereas the questions generated from VQG only are restricted to single obvious answer.
Following previous work Zhang and Lapata Xu et al. Specifically, SARI calculates how often a generated sentence correctly keeps, inserts, and deletes n-grams from the complex sentence, using the reference simple standard as the gold-standard, where 1≤n≤4. Note that we do not use BLEU Papineni et al. We also calculate oracle SARI, where appropriate, to show the score we could achieve if we had a perfect reranking model. Our best models outperform previous state-of-the-art systems, as measured by SARI. Our loss and diverse beam search methods have more ambiguous effects, especially when combined with the former two; note however that including diversity before clustering does slightly improve the oracle SARI score.
We observe that our models produce sentences that are much shorter and lower reading level, according to Flesch-Kincaid grade level (FKGL) Kincaid et al. , while making more changes to the original sentence, according to Translation Error Rate (TER) Snover et al. In addition, we see that the customized loss function increases the number of insertions made, while both the diversified beam search and clustering techniques individually increase the distance between sentence candidates. \newcitenapoles2011evaluating notes that on sentence compression, longer sentences are perceived by human annotators to preserve more meaning than shorter sentences, controlling for quality. Thus, the drop in human-judged adequacy may be related to our sentences’ relatively short lengths.
Lastly, after collecting the data we discovered that the if-then-because commands given by humans can be categorized into several different logic templates. Our neuro-symbolic theorem prover uses a general reasoning strategy that can address all reasoning templates. However, in an extended discussion in the Appendix, we explain how a reasoning system, including ours, could potentially benefit from these logic templates. As explained in the main text, we uncovered 5 different logic templates, that reflect humans’ reasoning, from the data after data collection. In what follows, we will explain each template in detail using the examples of each template listed in Tab. In the blue template (Template 1), the state results in a “bad state” that causes the not of the goal. The speaker asks for the action in order to avoid the bad state and achieve the goal . The state of snowing a lot at night, will result in a bad state of traffic slowdowns which in turn causes the speaker to be late for work. In order to overcome this bad state. The speaker would like to take the action , waking up earlier, to account for the possible slowdowns cause by snow and get to work on time. In the orange template (Template 2), performing the action when the state holds allows the speaker to achieve the goal and not performing the action when the state holds prevents the speaker from achieving the goal . she will not be able to properly prepare for the meeting. Currently, CORGI uses a general theorem prover that can prove all the templates. The large variation in performance indicates that taking into account the different templates would improve the performance. For example, the low performance on the green template is expected, since CORGI currently does not support the extraction of a hidden action from the user, and interactions only support extraction of missing goal s. This interesting observation indicates that, even within the same benchmark, we might need to develop several reasoning strategies to solve reasoning problems. Therefore, even if CORGI adapts a general theorem prover, accounting for logic templates in the conversational knowledge extraction component would allow it to achieve better performance on other templates.
The evaluation of negation detection was measured on complete entities. If any of the words within an entity were tagged with a I, B, E or S, that entity was considered to be negated. However, the best F1-score of 0.928 is achieved using the NegEx-Stanford system.
The row of Increment 0% shows the NLU functionality performance in SER when there is no annotated data from live traffic used for training.
The best selection mechanism is shown in bold letters. We observe that selecting half of the augmentation data using paraphrase model does not cause a drastic performance drop compared to using all of the augmentation data— changes range from a 1.34% relative decrease to a 3.61% relative increase in SER. Thus, we conclude that using greedy selection with paraphrase detection gives the best consistent performance when selecting half of the augmentation data for semi-supervised learning.
We run the LIWC program on the timelines of all the members for each of the two competing groups. We only use tweets relevant to COVID-19. We also remove users identified as bots. Because some users may be more active than others, using the results of the program as is may introduce biases in our analyses. To account for those biases, we first normalize the percentages by the size of the data for each user. We use the mean of the normalized LIWC indices of tweets of individual users for a given lexical category as our test statistic. We use an independent z-test for the difference in means to establish statistical significance. For all our tests, α=0.05. This is intuitive as many of our informed users post corrective or sarcastic tweets to call out misinformation. However, our results are not significant, and, hence inconclusive.
In our next experiment, we examine which of the four types of co-occurrences are best captured by each measure. (Similar results were obtained on the other data sets). The results show that, irrespective of the span constraint, most measures perform best on Type A co-occurrences. This is reasonable because Type A essentially represents the strongest correlations in the data and one would expect the measures to capture the strong correlations better than weaker ones. There are however, two exceptions to this rule, namely PMI and CWCD, which instead peak at Types C or D. The best correlations for these two measures are also typically lower than the other measures. We now summarize the main findings from our study: The relatively obscure Ochiai, and the newly introduce CSA are the best performing measure, in terms of detecting all types of lexical co-occurrences in all data sets and for a wide range of span constraints. Dice, LLR and TTest are the other measures that effectively track lexically significant co-occurrences (although, all three are less effective as the span constraints become larger). SCI, CWCD, and the popular PMI measure are ineffective at capturing any notion of lexically significant co-occurrences, even for small span constraints. In fact, the best result for PMI is the detection of Type C co-occurrences in the sim data set. The low ϵ and high δ setting of Type C suggests that PMI does a poor job of detecting the strongest co-occurrences in the data, overlooking both strong document-level as well as corpus-level cues for lexical significance.
We next investigate whether there is a curriculum that allows these models to perform well on all tasks. Interestingly, unlike in the continual learning setup, these models are able to perform reasonably well on many tasks (especially BERT). We can see that using a random training curriculum allows the model to not forget previously acquired knowledge, and that after about 50,000 iterations the model can work reasonably well on all tasks. However, the drawback of such a curruculum is that it either requires all tasks to be presented at the beginning or retraining on all tasks after seeing a new task. It is therefore still an open question how to transfer effectively in a continual learning setup.
TextRank is included as a baseline. The numbers in bold are the most severe drops, indicating that the corresponding features are the most critical.
We first assess our evaluation methodology by computing the correlation of its system scores (and rankings) to those of the original Pyramid. These are compared with the analogous correlations for the expert Responsiveness scores, available in the datasets. Importantly, notice that Responsiveness scores here were obtained by experts, and therefore the gap for crowdsourced Responsiveness is expected to be greater, further indicating the advantage of our method as a crowdsourcable approach. Another possibility may be to filter out topics with low annotator agreement when computing systems’ scores by the lightweight Pyramid method. We hypothesize that doing so might improve the reliability of this method, and hence increase its correlation with the original, expert-based, Pyramid method (when the latter is computed over all test topics). Indeed, in a preliminary test, we filtered out those 20% of the topics with lowest Krippendorff annotator agreement. when correlated with the original Pyramid ranking, as computed over the full set of topics. Further analysis might conclude that filtering problematic topics generically improves the reliability of the lightweight Pyramid method.
, we conduct an evaluation of Recall@K. From the results, it can be found that our proposed model reaches the best performances in the evaluation of Recall@1, Recall@10 and Recall@50. Furthermore, we also demonstrate an ablation study to observe the contribution of the dialog system and the introduced knowledge. It can be found that either dialog or knowledge can bring improvement to the performance of the recommendation system. Their combination improves the performance the most by +0.7 Recall@1, +3.4 Recall@10 and +5.1 Recall@50, which are advantages of 30.4%, 26.4% and 17.8% respectively. This shows that the information from both sources is contributive. The dialog contains users’ preferred items as well as attributes, such as movie director and movie style, so that the system can find recommendation based on these inputs. The knowledge contains important features of the movie items so that the system can find items with similar features. Further, the combination brings an advantage even greater than sum of the two parts, which proves the effectiveness of our model.
In the evaluation of perplexity, Transformer has much lower perplexity (18.0) compared to REDIAL (28.1), and KBRD can reach the best performance in perplexity. This demonstrates the power of Transformer in modeling natural language. In the evaluation of diversity, we find that the models based on Transformer significantly outperform REDIAL from the results of distinct 3-gram and 4-gram. Besides, it can be found that KBRD has a clear advantage in diversity over the baseline Transformer. This shows that our model can generate more diverse contents without decreasing fluency.
Previous works have shown that pretraining character embeddings boost the performance of neural CWS models significantly Pei et al. We verify this and get a consistent conclusion. Our model obtains significant improvements (+1.0 on PKU and +0.6 on MSR) with pretrained character embeddings.
We also study how the teacher’s performance influence the student. We train other two models that use different teacher models. One of them uses a worse teacher and the other uses a better teacher. As expected, the worse teacher indeed creates a worse student, but the effect is marginal (-0.1 on PKU and MSR). And the better teacher brings no improvements. These facts demonstrate that the student’s performance is relatively insensitive to the teacher’s ability as long as the teacher is not too weak.
Not only the pretrained word embeddings, we also build a vocabulary Vword from the large auto-segmented data. Both of them should have positive impacts on the improvements. To figure out their contributions quantitatively, we train a contrast model, where the pretrained word embeddings are not used but the word features and the vocabulary are persisted, i.e. the word embeddings are randomly initialized. According to the results, we conclude that the pretrained word embeddings and the vocabulary have roughly equal contributions to the final improvements.
The SF performance is the F1 while the ID and Sentence performances are measured with the accuracy. We also show an evaluation carried out with models trained on three different split of reduced size derived from the whole dataset. The reported value is the average of measurements obtained separately on the entire test dataset.
WERs. For comparison, we also implemented a speech domain adaptation method by directly applying CycleGAN with skip connections without the disentangled representation learning to speech as explained in Subsection 2.1. As shown in Fig. Therefore, the WERs of the CycleGAN-based method were higher than those of the baseline without any processing. On the other hand, our method effectively removed the background noise with the harmonic components and formants of the speech maintained, which resulted in revealing some harmonic components that were hard to see in the noisy speech. In the evaluation set, the averaged WERs were lower than those of the baseline by 6.72% for the simulated utterances and by 10.88% for the real-recorded utterances, respectively.
To assess to what extent the addition of text in the DL model, and the aforementioned changes in relative variable importance, actually affect the probability estimates, we proceed by reviewing the correlation between the different sets of model outputs. There appears to be little agreement between the text-only model compared with the structured model as they are weakly correlated, especially so for the new customer subset. Notably, across all subsets of customers, the combined model is highly correlated with the structured data, with a correlation coefficient of 0.97. This suggests that the structured data has dominated the model as we might expect given the richness of the data and the similarity of the performance results.
The key observations from these results are as follows: ConcatFT always gets the best performance. Furthermore, even Concat and KCCA always improves over the used baselines (the domain specific and fine-tuned BERT). This demonstrates its effectiveness in transferring the knowledge from the general domain while exploiting the target domain. The CCA variant shows inferior performance, worse than the baselines. This is also further verified empirically at the end of this section. Concat is simpler and computationally cheaper than KCCA and achieves better results, hence is the recommended method over KCCA. Our method gets better results using better domain specific models (using CNN instead of BOW). This is because better specific models capture more domain specific information useful for classification.
Again, ConcatFT achieves the best performance on all the datasets improving the performance of BERT-FT. This improvement comes at small computational overhead. On the MR dataset, ConcatFT requires 610 seconds which is about 9% increase over the 560 seconds of BERT-FT, and recall that it only has about 1% increase in memory. Concat can achieve comparable test accuracy on all the tasks while being much more computationally efficient. On the MR dataset, it requires 290 seconds, reducing about 50% of the 560 seconds for BERT-FT.
To study the effect of the dataset size on the performance, we vary the training data size in MR dataset via random sub-sampling and then use our method. Concat gets performance comparable to BERT-FT on a wide range of dataset sizes, from 500 points on. The performance of CCA improves with more training data as more data leads to less noise and thus less nuisance information in the obtained embeddings
Both the CNN and RNN models significantly outperform the logistic regression baseline, likely as a result of their abilities to account for sequential data. With respect to the difficulty of predicting the origin of a news source given just a single sentence, the CNN and RNN achieved respectable accuracies of 34.0% and 33.3% respectively, well above a random guessing accuracy of 9.09%.
As can be seen, the performance of all models is comparable with NEGLM models outperforming the NCE model by a small margin. Furthermore, we see that our NEGLM model performs as well or slightly better than the one enhanced with a learned bias component (NEGLM-B). This suggests that the learning of our NEGLM model is robust as is without such a component.
The small drop in these scores is however not discouraging, because the proposed copy-controlled (C-C) decoding generally generates slightly shorter summaries, while the recall-based ROUGE (or METEOR) score prefers longer summaries. This suggests the effectiveness of our approach, since ROUGE (or METEOR) does not take into account the length of a hypothesis but only considers the recall rate of n-grams. Finally, we note that the ROUGE and METEOR scores of the summaries generated by the pointer generator network, released by See et al. We believe this neither contradicts nor discounts our contribution, as the proposed decoding works on top of any pre-trained summarizer which relies on beam search during inference.
Each experiment was performed 10 times in total and the phone error rate (PER) was then evaluated. From the table, it is obvious that the lowest PER in all scenarios was obtained with LSTM. GRU gave the second best results, in average higher by 0.33 %. We have received the worst results with zoneout LSTM. They are significantly worse than all other networks and we were not able to improve them. In all RNNs used, Folds network ensemble gave better performance than using only single Master network. Also, the combination of the Master network and Folds network ensemble further improved the performance in both LSTM and zoneout LSTM cases. Slight improvements were also obtained in a few cases by employing RPL, most notably almost all LSTM scenarios and some GRU experiments with Master and Folds. Although the best average PER from all experiments we have obtained is 14.84 %, the best single experiment was LSTM with Master, Folds and RPL, which resulted in 14.64 % PER.
Biases of Distributional Spaces. All three input distributional spaces generally exhibit explicit and implicit biases, with CBOW spaces displaying the lowest biases, both according to the WEAT tests (e.g., the effect size is even insignificant with p<0.05 for the gender bias test T8) and the implicit bias tests of \citeauthorgonen2019lipstick \shortcitegonen2019lipstick. Interestingly – according to our BAT test, and despite the original claims and examples from \citeauthorBolukbasi:2016:MCP:3157382.3157584 \shortciteBolukbasi:2016:MCP:3157382.3157584 – the encoded biases do not reflect strongly in the analogy tests. Nonetheless, our debiasing methods in most test settings manage to affect the input vector spaces by further reducing BAT scores.
=ΛV∗ with respect to the elements of Λ =diag(λ1,...,λd), using R’s optim implementation of the Nelder–Mead method, where V∗ are 300-dimensional embeddings generated using GloVe and word2vec. The results show that there exists a transformed embedding ΛV∗ that performs substantially better than the base embedding. In practice, in order to use this optimization method to generate embeddings, it would be necessary to use cross-validation, as embeddings which achieve optimal performance with respect to one test set may do less well on others.
Sentences are tokenized using the Stanford NLP library Manning et al. All algorithms are implemented using a modified version of the fasttext Bojanowski et al. ; Joulin et al. Pagliardini et al. During training, we save models checkpoints at 20 equidistant intervals and found out that the best performance for CBOW models occurs around 60−80% of the total training. As a result, we also indicate the checkpoint at which we stop training the CBOW models. We use 300-dimension vectors for all our word embedding models. For the Ngram2vec model, learning source and target embeddings for all the n-grams upto bigrams was the best performing model and is included in the comparison. For the GloVe model , the minimum word count is set to 10; the window size is set to 10; we use 10 epochs for training; Xmax, the weighting parameter for the word-context pairs is set to 100; all other parameters are set to default. For Ngram2vec, the minimum word count is set to 10; the window size is set to 5; both source and target vectors are trained for unigrams and bigrams; overlap between the target word and source n-grams is allowed. All other features are set to default. To train the Ngram2vec models, we use the library provided by Zhao et al.
Since our HRL could be switched to different reward functions, we evaluate both the BLEU oriented and ROUGE oriented training of our framework, denoted as TAG(B) and TAG(R). The results of TAG(B) and TAG(R) varies slightly compared to each other. However, both of them are significantly higher than all the selected counterparts, which demonstrates the state-of-the-art generation quality of our framework on all the datasets with different programming languages.
The 29-dimensional log Mel filterbank features together with 1st and 2nd Each frame is spliced together with 5 left and 5 right context frames to form a 957-dimensional feature. The spliced features are fed as the input of the feed-forward DNN after global mean and variance normalization. The DNN has 7 hidden layers with 2048 hidden units for each layer. The output layer of the DNN has 3012 output units corresponding to 3012 senone labels. Senone-level forced alignment of the clean data is generated using a Gaussian mixture model-HMM system. Note that our experimental setup does not achieve the state-of-the-art performance on CHiME-3 dataset (e.g., we did not perform beamforming, sequence training or use recurrent neural network language model for decoding.) since our goal is to simply verify the effectiveness of SIT in reducing inter-speaker variability.
The speaker-adapted (SA) SIT DNN achieves 15.46% WER which is 4.86% relatively higher than the SA SI DNN. The CRT adaptation provides 8.91% and 8.79% relative WER gains over the unadapted SI and SIT models respectively. The lower WER after speaker adaptation indicates that SIT has effectively reduced the high variance and overlap in an SI acoustic model caused by the inter-speaker variability.
, SSNT-CIP1 improves over SSNT0 not only in cycle-consistency, but in translation quality as well. This suggests that the goal of preserving information, in the sense defined by SSNT-CGP1 and approximated by SSNT-CIP1, is important for translation quality.
Secondly, we examine the linguistic variety of our dataset by computing the Type-Caption Curve, as defined in [Wang_2019_ICCV]. Fakeddit provides significantly more lexical diversity. Even though Fakeddit contains more samples than FEVER, the number of unique n-grams contained in similar sized samples are still much higher than those within FEVER. These effects will be magnified as Fakeddit contains more than 5 times more total samples than FEVER. This demonstrates that for all n-gram sizes, our dataset is more lexically diverse than FEVER’s for equal sample sizes.
We replaced only those annotations (gold standard) which we (all team members) were sure of. Those sentences in which the class of an entity occurrence was ambiguous were not corrected. This shows that the models are better than we thought they were, and so we corrected only the test set and left the inconsistencies.
Here we show that Eigenword + MSW outperforms Eigenword + Roget, which is in contrast with the other word embeddings where the combination with Roget performs better.
Accuracy increased for all of the clustering methods aside from Eigenwords+CombThes. However, we achieved better results when we exclusively used the MS Word thesaurus. Combining thesaurus lookup and word2vec+CombThes clusters yielded an accuracy of 0.96.
Besides, the comparison also shows that the directly predicted dependencies from our model are better than those converted from the predicted constituent parse trees in UAS term.
The proposed modular-trained A2W CTC (Mod. CTC) is in line 4. Mod. CTC outperforms the naive A2W in line 3 significantly. The modular training framework benefits from: i) easier and faster model convergence due to modularization and initialization. ii) easy to utilize standard AM and LM technologies using text and acoustic data respectively. All results are reported on a single Titan GPU. “fr./s.” denotes the number of acoustics frames processed per second. The training speedup stems from two folds: i) PSD reduces the sequence length to be processed by P2W in each sequence. ii) As the sequence length is reduced, more sequences can be loaded into GPU memory for parallel training. Meanwhile, the performance is significantly improved. We believe it also results from sequence length reduction. Although LSTM is used, the model is still hard to remember a very long input sequence. Nevertheless, for A2W modeling, the history to be remembered before inferring each word is much longer than that of the traditional CI-phone CTC or hybrid systems. The PSD framework shows another choice to cope with this problem.
The SPLIT approach seems to be ineffective in Persian. Due to a couple of important reasons we do not suggest the approach to be applied in English-Persian CLIR. The first reason is the proposed sense loosing effect in the Persian language. Although with such a method we resolve the multiple formation matching problem and get an improved the recall, we might lose key senses from the documents. As discussed before, in such cases we either need to have a context-sensitive stemmer or a query-side affix generation method. The second reason is the noise propagation effect of the method. For instance in the ‘imports’ example, the irrelevant fragment ‘ports’ gets score equal to the relevant ones. In Persian we also have similar examples. As a result, without disambiguating the fragments, we don’t suggest to use these truncation-based strategy instead of the steming one. STEM also surpasses the simple disambiguation methods TOP 1, UNIF, and COLL. Although STEM loses MAP to 2G with no any morphological processing, its P@5 sees improvement over it. However, the method outperforms the SPLIT method in terms of MAP, P@5, and P@10.
As shown, all modern neural networks (CNNs/RNNs with word embeddings) are consistently better than methods using handcrafted features of unigrams and bigrams. Because we have applied a 3-layer DNN to these features, we believe the performance improvement is not merely caused by using a better classifier, but the automatic feature/representation learning nature of modern neural networks. Using contextual information yields an F1 improvement of 2%. This controlled experiment validates the usefulness of context for SCD. With sentence-level static attention, our model achieves the highest performance of 89.2% accuracy and 78.4% F1-measure.
Even for a small number of clusters, e.g. k=10, the VLAWE document representation can grow up to thousands of features, as the number of features is k⋅d, where d=300 is the dimensionality of commonly used word embeddings. However, there are several document-level representations that usually have a dimensionality much smaller than k⋅d. Therefore, it is desirable to obtain a more compact VLAWE representation. We hereby propose two approaches that lead to more compact representations. The first one is simply based on reducing the number of clusters. By setting k=2 for instance, we obtain a 600-dimensional representation. The second one is based on applying Principal Component Analysis (PCA), to reduce the dimension of the feature vectors. Using PCA, we propose to reduce the size of the VLAWE representation to 300 components. Although the compact VLAWE representations provide slightly lower results compared to the VLAWE representation based on 3000 components, we note that the differences are insignificant. Furthermore, both compact VLAWE representations are far above the state-of-the-art method Cheng et al.
To build our first benchmark of natural language arguments, we selected Debatepedia and ProCon, two encyclopedias of pro and con arguments on critical issues. The training set is composed by 85 entailment and 75 contradiction pairs, while the test set by 55 entailment and 45 contradiction pairs. The pairs considered for the test set concern completely new topics. We therefore apply again the extraction methodology described in the previous section to extend our data set. We consider the obtained data set as representative of human debates in a non-controlled setting (Debatepedia users position their arguments with respect to the others as PRO or CON, the data are not biased). In order to investigate the presence and the distribution of these attacks in NL debates, we extended again the data set extracted from Debatepedia to consider all these additional attacks, and we showed that all these models are verified in human debates, even if with a different frequency. Collecting the argument pairs generated from the different types of complex attacks in separate data sets allows us to independently analyze each type, and to perform a more accurate evaluation. the four AFs resulting from the addition of the complex attacks in the example Can coca be classified as a narcotic?. Notice that, even if the additional attack which is introduced coincide, i.e., d attacks b, this is due indeed to different interactions among supports and attacks (as highlighted in the figure), i.e., in the case of supported attacks this is due to the support from d to c and the attack from c to b, while in the case of mediated attacks this is due to the support from b to a and the attack from d to a.
We performed a hyperparameter grid search with our method. We experimented with different recurrent cells (RCNN and LSTM), depth (2, 3), hidden size (200, 300, 500), bi-directional RNN and in the case of RCNN also with cell order (2, 4). We also trained several non-neural methods for comparison. We measure accuracy as well as average precision (AP). The best results were achieved by bi-directional 2-layer RCNN with hidden size 300 and order 2. Deep neural network models outperform feature based models by almost 10% of accuracy. RCNN achieves results similar to LSTM but with approximately 8.5 times less parameters.
From the last two rows of the table, we can see that the BLEU-2 scores of positive-groundtruth is much higher than negative-groundtruth which shows that the modified BLEU score is effective. As we can see from the first four rows of the table, the AS2S performs better than baseline. And both of the proposed model HieAS2S-tile and HieAS2S-concat outperform other models, in both terms of BLEU-2 scores and RHYTHM scores. In addition, the HieAS2S-tile model performs better than HieAS2S-concat. Through our analysis, the HieAS2S-tile model divides word features and phrase features separately, so that the model can better capture the phrase information. This results show that the phrase features are helpful and demonstrate the effectiveness of our proposed models.
Our method performs better than AS2S in all four metrics. This results show the effectiveness of our method. Compared our method with human-written, we found that the Poeticness and Fluency scores of our method are slightly lower than human-written poems. However, the Meaning and Coherence scores of poems written by human are still much higher than those generated by our method. Particularly, we developed a web application for users to use and evaluate our approach, most users give satisfactory evaluations to our approach. Figure 3 shows an example of 5-char quatrain generated on our web application with a user-uploaded picture and another 7-char quatrain generated with a given theme.
In our experiments, we report results with a standard joint goal accuracy (JGA) score. We expect this conclusion to hold in all settings; however, additional experiments for different languages and word embeddings would be beneficial.
For nine out of ten languages we match the oracle method proxy-best within a 95% confidence interval. For Russian, the treebank vector of the second-best proxy treebank is chosen, falling 0.8 LAS points behind. Still, this difference is not significant (p=0.055). For English, the generic model also picks the second-best proxy treebank.
It is important to keep in mind that in our setting we are completing to queries that have never been observed before (by restricting evaluation to Q2∖Q1). With this in mind, we start by analyzing the results for BNDS. The contrast between (full match) MRR and partial match MRR is striking. The low MRR numbers are expected given the highly compositional nature of BNDS queries and the fact that the algorithm we are testing is designed to complete to the next full atom, which makes a complete match only possible when the prefix contains characters for the very last atom in the reference query. If we focus on the partial MRR results for BNDS, we can observe how, as we move down the table, the numbers increase, reflecting the fact that our AC systems are semantically driven, which means that even if they do not reproduce the exact same reference query, they produce a semantic equivalent with possible reordering of words. The semantic (SEM) partial MRR reflects that the desired completion appears, on average, at rank 1 or 2.
Effectiveness of Wassp. First, to answer RQ1, we primarily investigate the performance of Wassp w.r.t. the amount of extra supervision during training. On the WikiTableQuestions dataset, MAPO has the option to begin with an empty memory buffer (i.e. a cold start) or warm up the memory buffer by searching with manually-designed pruning rules as heuristics (i.e. a warm start). Since designing the pruning heuristics also includes non-trivial human effort, we include the both results from a model trained from a cold start and a warm start to have a rather complete study. Our model presented here uses the correctness-based query selection heuristic and fully-specified MRs as extra supervision. We also compare Wassp with existing weakly- or fully-supervised methods. , Wassp can improve the performance by 0.7% while querying 500 examples (0.89%) achieves an absolute improvement of 2.1% over the same model trained with pure weak supervision. Notably, if we allow a query budget of 1,000 examples, Wassp achieves an execution accuracy of 78.6%, a 6.0% absolute improvement over previous state-of-the-art result (72.6%) with only weak supervision (liang2018memory). Additionally, with only 1.8% of annotated training examples, Wassp is already quite competitive against some of the strong fully-supervised systems trained with the whole annotated training set (sun2018semantic; yin2018tranx; dong2018coarse). This result is also only 1.7% away from the same semantic parser (NSM) trained with full supervision (80.3%). Finally, if we further increase the query budget to 10,000 examples, Wassp could achieve similar performance comparable with training the semantic parser using full supervision, but with 80% less annotated data. This set of results shows that Wassp allows learning with significantly less supervision.
We find that the pretrained BERT embedding has the most impact on the performance, which again demonstrates the power of large-scale pretrained language models. Our proposed GF mechanism also contributes significantly to the model performance (i.e., improves F1 score by 1.4%). In addition, within the GF layer, both the GNN part (i.e., 1.1% F1) and the temporal connection part (i.e., 0.3% F1) contribute to the results. We also notice that explicitly adding conversation history to the current turn helps the model performance by comparing GraphFlow (2-His), GraphFlow (1-His) and GraphFlow (0-His). We can see that the previous answer information is more crucial than the previous question information. And among many ways to use the previous answer information, directly marking previous answer locations seems to be the most effective. We conjecture this is partially because the turn transitions in a conversation are usually smooth and marking the previous answer locations helps the model better identify relevant context chunks for the current question.
In this experiment, we test a situation in which we have a reasonable amount of sequential data available for pre-training, but only a limited amount of lattice training data for the fine-tuning step. This may be a more realistic situation, because speech translation corpora are scarce. To investigate in this scenario, we again pre-train our models on Fisher/Train, but then fine-tune them on the 9 times smaller Callhome/Train portion of the corpus. We fine-tune for 10 epochs, all other settings are as before. We use Callhome/Evltest for testing. The proposed model (lattice/R+L+S) outperforms the 1-best baseline (1-best/R+1) by 0.8 BLEU points, which in turn beats the pre-trained system (1-best/R) by 1.5 BLEU points. Including the lattice scores is clearly beneficial, although lattices without scores also improve over 1-best inputs in this experiment.
The evaluation sentences, audio clips, and instructions for this test are identical to those used for the previous side-by-side results. These results show that, as expected, raters did not have a strong preference between TPCW-GST and TPSE-GST.
This demonstrates that our U-model had the highest accuracy, and that our I-model and the theoretical FSM model had the same accuracy, slightly less than the U-model. The accuracy measure of the FSM model suggests that the heuristics encoded in the GH coding protocol are a good representation of the patterns that can be learned from the data we collected, given our choice of data annotations. The similarity of the CSF model’s accuracy to that of the FSM similarly demonstrates that the CSF did a good job of automatically learning these patterns from our data. The slightly higher accuracy of the U-model over the I-model suggests that the uniformly distributed prior probabilities may have been more helpful than the weakly informed prior distribution. Finally, the performance advantage of all of these models over the RB model provides a good baseline measurement of success.
We train NNER-3% (Iter 0) and use it to predict labels for unknown tokens repeatedly, which yields a jump in performance in the first iteration (Iter 1), since the predicted labels are informative, and then a more gradual improvement as the labels are increasingly refined.
We collect a dataset, as a proof of concept, for evaluating and analyzing our method to better showcase its ability to generalize as well as for demonstrating the extensibility of this type of solution to conversational texts and unusual images. Each image contains 5 textual descriptions on average which collected by crowdsourcing on AMT. The average caption length is 8.7 words after rare word removal.
Our final system outperforms Kaldi’s hybrid system with relative WER reduction 5.5%, and achieves latency about 300 ms with a small contextual width τ=2. Though our final transducer system has similar RTF as Kaldi’s hybrid system, it achieves lower latency mainly due to the use of small look ahead size. The system is being launched in LAIX’s AI-teacher product, and we plan to launch an on-device transducer system soon.
Across almost all the tasks we observe superior performance of the pre-trained language models compared to the simple BiLSTM model. Among the pre-trained language models, ML-BERT demonstrates superior performance in each task for all the available language pairs. ELMo ’s performance is very close to ML-BERT in most of the LID and POS tasks (≈1-4%), but the performance gap is bigger for NER (≈8-10%) and SA (≈18%). The average performance gap between ELMo and BERT is ≈6%. It is also noteworthy to mention that the training data for BERT is document-level data, whereas the Billion Words Benchmark is a sentence-level corpus. Among the four tasks, NER and SA seem harder compared to LID and POS. It shows that the involvement of semantic understanding in code-switched texts makes tasks harder compared to syntactic analysis.
The exact-match cases are quite simple and both systems get 100% correct. (ii) For the ambiguous/hard and entity-linking-error cases, meeting our expectations, both of the systems perform poorly. (iii) The two systems mainly differ in paraphrasing cases, and some of the “partial clue” cases. This clearly shows how neural networks are better capable of learning semantic matches involving paraphrasing or lexical variation between the two sentences. (iv) We believe that the neural-net system already achieves near-optimal performance on all the single-sentence and unambiguous cases. There does not seem to be much useful headroom for exploring more sophisticated natural language understanding approaches on this dataset.
In this last case, we decided to include only the variant with the biggest performance. On the one hand, surprisingly, the evaluation in English corpus reveals that using a multimodal approach is better for detecting age and gender, when all images are collapsed into a prototype. Regarding the visual methods, they perform poorly when compared to the textual one (except V4).
In both instances, StyleFusion achieved relatively high BLEU, and showed high style intensity. The Rand baseline has the highest style intensity but lowest relevance. MTask shows significantly less style intensity than StyleFusion. Moreover, MTask’s diversity, as measured by entropy4 and distinct1,2, is much lower, indicating that outputs of this model tend to be bland. Adding Lconv regularization, which is SpaceFusion, increases diversity, relevance and style intensity slightly, consistent with the finding in Gao et al. Style intensity is further boosted by the addition of term Lstyle. relevancy and diversity are not significantly affected by the addition of Lstyle.
We directly applied CorrRNN, which is trained on scientific publications, on predicting phrases for news articles without any adaptive adjustment. et al. and CopyRNN Meng et al. The model should perform better if it is trained on news dataset.
In order to motivate the problem, we measure the oracle word-error rate which gives us the path with the minimum word error rate found within each lattice. The oracle word error rate for the test set was found to be 1.70%. If we were to flawlessly re-score a lattice, we could, in theory, achieve this word error rate. In the lattice some very good answers exist. The RNNLM still gives a much better score. We suspect that this is due to a few things: firstly, the XLNet is 110M parameters and was trained on approximately 13GB of text compared to 25MB worth of text for TED-LIUM. ’s text is not enough to overcome the differences between written text and conversational speech. Without fine-tuning, adding memory seems to have an adverse effect on the test set.
Our small model with 5 million parameters achieves a test perplexity of 73.6, already outperforming many results achieved using much larger network. By increasing the network size to 20 million, we obtain a test perplexity of 69.2, with standard dropout. Adding variational dropout ( Note that Zilly et al. adopts a complex recurrent cell found by reinforcement learning based search. Our network is architecturally much simpler.
We report the best results achieved across 5 independent runs. Our best model obtains 53.2% and 89.9% test accuracies on fine-grained and binary tasks respectively. Our model with only a constant decay factor also obtains quite high accuracy, outperforming other baseline methods shown in the table.
We computed ROUGE scores for collected models in two settings: first using the CNN/DM reference summaries as the ground-truth, and second where the leading three sentences of the source article were used as the ground-truth, i.e. the Lead-3 baseline. We computed ROUGE-1 and ROUGE-4 scores between pairs of model outputs to compare them by means of token and phrase overlap. Comparing results with the n-gram overlap between models and reference summaries This might imply that the training data contains easy to pick up patterns that all models overfit to, or that the information in the training signal is too weak to connect the content of the source articles with the reference summaries.
For the sampled dataset, we analyzed the top 200 most frequently hashtags, excluding COVID19 related hashtags. Unlike the politicized dataset, the volume of election related themes was significantly lower (27% of the total). This implies that politically active users are not necessarily representative of the entire US Twitter user population. However, their influence is visible in the general population.
100 iterations (T=100) was sufficient to obtain convergence in all our experiments. We see that the proposed method obtains similar results with all pattern types and co-occurrence measures. This result shows the robustness of our method against a wide-range of typical methods for constructing relational graphs from unstructured texts. For the remainder of the experiments described in the paper, we use the RAW co-occurrence frequencies as the co-occurrence strength due to its simplicity.
A baseline method is created that shows the level of performance we can reach if we represent each word u as a vector of patterns l in which u occurs. First, we create a co-occurrence matrix between words u and patterns l, and use Singular Value Decomposition (SVD) to create 200 dimensional projections for the words. Because patterns represent contexts in which words appear in the corpus, this baseline can be seen as a version of the Latent Semantic Analysis (LSA), that has been widely used to represent words and documents in information retrieval. Moreover, SVD reduces the data sparseness in raw co-occurrences. We create three versions of this baseline denoted by SVD+LEX, SVD+POS, and SVD+DEP corresponding to relational graphs created using respectively LEX, POS, and DEP patterns. In particular, skip-gram and GloVe are considered the current state-of-the-art methods. We learn 200 dimensional word representations using their original implementations with the default settings. We used the same set of sentences as used by the proposed method to train these methods. Proposed method is trained using 200 dimensions and with three relational graphs (denoted by Prop+LEX, Prop+POS, and Prop+DEP), weighted by RAW co-occurrences. We note that the previously published results for skip-gram and CBOW methods are obtained using a 100B token news corpus, which is significantly large than the 2B token ukWaC corpus used in our experiments. However, the differences among the Prop+LEX, Prop+POS, and Prop+DEP methods are not significantly different according to the Binomial exact test. SVD-based baseline methods perform poorly indicating that de-coupling the 3-way co-occurrences between (u,v) and l into 2-way co-occurrences between u and l is inadequate to capture the semantic relations between words. Prop+LEX reports the best results for all semantic relations except for the family relation. Comparatively, higher accuracies are reported for the family relation by all methods, whereas relations that involve named-entities such as locations are difficult to process. Multiple relations can exist between two locations, which makes the analogy detection task hard.
For the Q-DUP task, we train an SVM model for each document embedding method over each of the 12 subforums, and evaluate using the ROC AUC score due to the extremely biased data distribution. The ROC AUC score indicates the probability that the models rank randomly-chosen positive samples before randomly-chosen negative samples. An AUC score of 1.0 indicates that the model is perfect at ranking true duplicates ahead of false duplicates, while 0.5 signifies a completely random ranking (and any value less than that a worse-than-random ranking). As the SVM classifier does not provide an explicit probability to use for ranking, we calculate a similarity score based on: sdup =d−dmindmax−dmin where d is the distance from the instance to the positive decision boundary of the SVM, and dmin,dmax correspond to the minimum and maximum distances among the test instances.
Aristo performance is slightly better on this set than on real science exams (where Aristo achieves 71.3% accuracy Because TableILP uses a hand-collected set of background knowledge that does not cover the topics in SciQ, its performance is substantially worse here than on its original test set. Neural models perform reasonably well on this dataset, though, interestingly, they are not able to outperform a very simple information retrieval baseline, even when using exactly the same background information. This suggests that SciQ is a useful dataset for studying reading comprehension models in medium-data settings.
Search strategies, called Decoders in SGNMT, search over the space spanned by the predictors. SGNMT can also be used to analyze search errors. Tab. Following Stahlberg et al. we measure time complexity in number of node expansions. Our depth-first search algorithm stops when a partial hypothesis score is worse than the current best complete hypothesis score (admissible pruning), but it is guaranteed to return the global best model score. Beam search yields a significant amount of search errors, even with a large beam of 20. Interestingly, a reduction in search errors does not benefit the BLEU score in this setting. For the experiments in Tab. Lattices have 271 nodes and 408 arcs on average.
We only test the models after fine-tuning with a subset of the samples with human annotations because training exclusively on the samples with auto-generated weak labels results in relatively poor performances when tested with human annotated data indicating the models after fine-tuning are more robust. The F1 scores and accuracies are overall higher with the AA-Sentiment than the results with HA-sentiment, indicating that the HA-Sentiment is a more challenging task and the sentiments involved are more difficult to identify supported by their relatively lower sentiment scores returned from Vader. We still, however, observe competitive results from HA-Sentiment showing that the models are well-trained and robust to noisy labels with the help of fine-tuning with human annotated data. The T-LSTM baseline achieves decent performance in both experiments with accuracies of 86.6% and 70.7% showing that LSTM is an effective encoder for sentiment analysis as suggested by the references. The models with proposed bi-sense emoji embedding obtain accuracies over 82.4% and we observe improvements on the performance with the attention-based LSTM from our proposed model MATT-BiE-LSTM and WATT-BiE-LSTM, which is consistent with that ATT-E-LSTM (F1@84.6%, accuracy@82.0% on HA-Sentiment) outperforms significantly T-LSTM and E-LSTM.
Overall Alignment Performance. MuGNN and BootEA only leverage relationship triplets. BootEA bootstraps the alignments iteratively and performs better than MuGNN. Although JAPE and GCN additionally consider the attribute triplets, they perform much worse than BootEA, as they only leverage the attributes but ignore their corresponding values. MultiKE utilizes the values and performs better than JAPE and GCN. However, it learns and compares the global embeddings of entities, which may bring in additional noises by the irrelevant attribute triplets. The Effect of Different Merge Strategies. We can see that the standard multi-view merge strategy(M1) performs worst, as it does not solve the conflicts from the two views. The score-based merge strategy (M2) and the rank-based merge strategy (M3) solve the conflicts, thus perform better than M1 (+1.26-2.43% in HR1). M3 avoids comparing the scores of different scales, thus performs better than M2 in most of the metrics. Later, JarKA indicates the proposed model with M3. The Effect of Iteratively Update the Translation Model. We validate the effect of iteratively updating the translation model (IT) during the joint modeling process. Specifically, we compare JarKA with the translation model being trained only once at the beginning, which is denoted as JarKA-IT.
The full sentence scores are upper bounds of incremental methods. The proposed method reduced the average latency in more than 50% from the full sentence baseline with some loss in BLEU and RIBES. The BLEU and RIBES results by the proposed method were worse than those by Wait-k. Th would be due to some degradation in smaller latency parts that were determined adaptively by the proposed methods while Wait-k keeps the fixed latency.
In this setting, we have the flexibility to use any pre-trained word embeddings to construct E. We use Pearson and Spearman correlations to compare NPMI and WETC scores against human ratings. Overall, GloVe appears to perform the best across different types of corpora and its correlation with human ratings is very comparable to NPMI-based scores. Our NPMI calculation is based on the Wikipedia corpus and should serve as a fair comparison. In addition to the good correlation exhibited by WETC, the evaluation of WETC only involves matrix multiplications and summations and thus is fully differentiable and several orders of magnitude faster than NPMI calculations. WETC opens the door of incorporating topic coherence as a training objective, which is the key idea we will investigate in the subsequent sections. It is worth mentioning that, for GloVe, the low dimensional embedding (50d) appears to perform almost equally well as high dimensional embedding (300d). Therefore, we will use Glove-400k-50d in all subsequent experiments. Obtained from https://code.google.com/archive/p/word2vec/. GloVe Pennington et al. Obtained from https://nlp.stanford.edu/projects/glove/. FastText Joulin et al. pre-trained on Wikipedia with 2.5 million vocabulary size and 300 embedding dimension. Obtained from https://github.com/facebookresearch/fastText. LexVec Salle et al. Obtained from https://github.com/alexandres/lexvec.
We observe that all three components we vary, namely the use of attention, bigram shuffling, and incorporation of sentence embeddings, contribute positively to the performance of our model as measured by ROUGE. The model that incorporates all three obtains the highest ROUGE scores.
To evaluate the performance of each dialogue baseline against the proposed models, we use the Ubuntu Dialogue Corpus Lowe et al. We perform perplexity evaluation using a held-out validation set. Perplexity is reported per word. For reference, a randomly-initialized model would receive a perplexity of 50,000 for our chosen vocabulary size. Overall, the NTM-LM architecture performed the best of all model architectures, whereas the sequence-to-sequence architecture performed the worst. The proposed NTM-LM outperformed the DNTM-S architecture.
EntityNLM (line 4) significantly outperforms both baselines (line 1 and 2) and prior work (line 3) (p≪0.01, paired t-test). The comparison between line 4 and 5 shows our model is even close to the human prediction performance.
To judge overall translation performance, we compared the LLA-LSTM encoder and decoder with the standard LSTM encoder and decoder. We also compared our model with one that does not have the adversary but is otherwise identical. The LLA-LSTM model shows improvements over the standard model on many or all of the metrics for every naturalistic domain. Many of the improvements over the other models are several percentage points. In the few scenarios where the LLA-LSTM model does not improve upon the standard model, the discrepancy between the models is small. The discrepancy is also small when the LLA-LSTM model with no adversary performs better than the LLA-LSTM model. In the figure, “dax,” “lug,” “wif,” and “zup” are interpreted correctly to mean “r,” “g,” “b,” and “y,” respectively. Here, the letters correspond to the types of unique dots, which are red, green, blue, and yellow, respectively. The exceptions are two erroneous associations between “kiki” and blue and “blicket” and green. Also, every sentence has a stop token, so the LLA units learned that the context-invariant meanings of each word include it. The LLA units can handle cases where a word corresponds to multiple output tokens, and the output tokens need not be monolithic in the output sequence. As shown in tests from all of the other domains, these output token correspondences may or may not be relevant depending on the specific context of a word, but the recurrent component of the architecture is capable of determining which to use.
where B is the batch size, and nB re-normalizes the loss so the gradient magnitude is not dependent on the number of utterances in the interaction. We run each experiment five times and report mean and standard deviation. The main metric we focus on is strict denotation accuracy. The relatively low performance of seq2seq-0 demonstrates the need for context in this task. Attending on recent history significantly increases performance. Both seq2seq models score anonymized tokens as regular vocabulary tokens. Adding anonymized token scoring further increases performance (s2s+anon). Full-0 and Full add segment copying and the turn-level encoder. The relatively high performance of Full-0 shows that substituting segment copying with attention maintains and even improves the system effectiveness. However, the best performance is provided with Full, which combines both. This shows the benefit of redundancy in accessing contextual information. Unlike the other systems, both Full and Full-0 suffer from cascading errors due to selecting query segments from previously incorrect predictions. The higher Full-Gold performance illustrates the influence of error propagation. While part of this error can be mitigated by having both attention and segment copying, this behavior is unlikely to be learned from supervised learning, where errors are never observed.
We report mean and standard deviation over three trials. The high performance of s2s+anon potentially indicates it benefits more from the differences between the splitting procedures.
Baselines. In this paper, we have four baselines that are based on pre-trained LMs, including BERTbase, ERNIE 1.0base, ERNIE 2.0base and ERNIE 2.0large. Hyper-parameters. In the fine-tuning stage, we use the same hyper-parameters for all models. The learning rate is 3e-5, and batch size is 32. We set the number of epochs is 5. The maximal answer length and the maximal document length is 20 and 512, respectively. We set length of document stride is 128. Human Performance. We evaluate human performance on both the in-domain test set and robustness test set. Specifically, we sample two hundred examples from in-domain test set and one hundred examples from each of over-sensitivity, over-stability and generalization set. We ask crowdworkers to annotate answers to these sampled instances. Then, we calculate the EM and F1-scores of these annotated examples as the human performance.
In addition to evaluating our introduced models, we also include a deterministic edit encoder in our study, which we consider a baseline. This model is similar in nature to the approach introduced by Yin et al.
The kappa scores show a moderate agreement between the human raters. Our NQG++ outperforms the PCFG-Trans baseline by 0.76 score, which shows that the questions generated by NQG++ are more related to the given sentence and answer span.
The multi-modal concatenation (CNN-LSTM) does not perform well, since it cannot model the complex interactions between images and questions. Stacked Attention (+SA) can improve the results since it utilizes the spatial information from input images. Our QGHC model still outperforms +SA by 17.40%. For the N2NMN, it parses the input question to dynamically predict the network structure. Our proposed method outperforms it by 2.20%.
The semantic word similarity task rely on specific datasets. They contain pairs of words and a value assigned by humans (e.g. computer/keyboard - 0.762)). This value represents the semantic similarity between the two words and is assumed to be the ground truth that word embeddings should encode. The cosine similarity of the pre-trained vectors of the words of one pair does not always match the value associated to this pair, but binarizing the vectors helps to move closer the vector similarity to the value, making the embedding closer to the human judgment.
There are several factors the influence ASR performance in challenging conditions. Performance in this case is typically quantified by word error rate (WER) or character error rate (CER) for Asian languages. These factors include: Vocabulary: It is well-known that the perplexity of the language model has a significant inverse effect on performance. Microphone distance: Speakers further away from the microphone, especially in acoustically active rooms, can result in substantially lower performance. Noise: One of the biggest factors affecting ASR performance is the noise level, typically quantified as signal-to-noise ratio (SNR). Reverberation: Highly reverberant acoustic environments are particularly challenging.
Note also that only the data of the two exemplars Parsley and Raisin are contained in the interval [min(nA,XnA,nB,XnB),max(nA,XnA,nB,XnB)], and therefore could also be modeled using only ‘context effects’. Therefore, we already see in these examples that ‘interference and context effects’ are jointly needed to faithfully account for the experimental data.
Lattice rescoring and 100-best rescoring are applied to lattices generated by the 4-gram LM. As expected, uni-RNNLMs yield a significant performance improvement over 4-gram LMs. Lattice rescoring gives a comparable performance with 100-best rescoring. Confusion network (CN) decoding can be applied to lattices generated by uni-RNNLM lattice rescoring and additional performance improvements can be achieved.
Note that these numbers are not directly comparable due to differences in language and corpus sizes. However, even standalone, the accuracy obtained by mBERT on our dataset clearly highlights the fact that this task is far from being solved.
As expected, tokenization helps phrase-based MT, although the differences in BLEU scores are not statistically significant. In terms of BLEU, neural MT performs significantly better than phrase-based MT, and char-based models lead to substantial and statistically significant improvement. Using a larger and deeper NMT model does not lead to significant improvement, possibly due to the size of the training data.
In Tab. For the NYT corpus, we reached a new state of the art on ROUGE-1, ROUGE-2 and ROUGE-L F1 scores. For the RIA corpus, since it has no previous art, we present results for the baselines and our model.
Context size H=2 is a sizable improvement over H=3 or H=1. Random sampling is significantly more effective than sentence-level batching glove initialization of word embeddings ew is harmful. As expected for POS tagging, morphological modeling with LSTMs gives the largest improvement.
Task8 dataset and KBP37 test dataset. We can conclude, that the model achieves comparable quality to the reference model and our implementation seems to be correct.
A natural question that emerges from these results is whether it is possible to use more backtranslations to achieve larger gains. The second column suggests that so synthetic examples are not needed in the 1,000 example setting, where two backtranslated examples per original example was the best performing configuration, but just one extra example per original performs almost identically. We did not experiment with using backtranslated examples generated from a mix of languages, but would not be surprised if that generated further small improvements.
ULMFit overweights the last sentence, with good reason. Reviews average 227 words in length, and many of these words are dedicated to summarizing the plot of the movie. The observation that so much of many reviews is irrelevant for sentiment analysis motivated an experiment to test whether, even after ULMFit’s concat pooling, which passes meanpooled and maxpooled representations of all encoder hidden states to the classifier head, ULMFit "overweights" the sentiment of the end of the movie review and underweights the beginning. The hypothesis that ULMFit is forgetting important parts of the input is further undermined by the observation that including these same sentence level statistics as features in the ensemble provided no benefit. More specifically we fit target= W⋅[X−1,X[0],avg(X),max(X),min(X),len(X))] ,
It can be observed that with the best i-vector baseline (cosine scoring), the performance on “Wei” (12.72%) is reasonably good considering the short duration of the test utterance. On cough and laugh, the performance is significantly reduced (19.96% and 23.03% respectively). These results are expected, as the model are not intentionally trained to cover these two kinds of trivial events, and the content of these two events are largely non-linguistic, so likely involve less speaker information.
Our categorical model performs significantly better than the existing second-place (Kintsch) and obtains a ρ quasi-identical to the multiplicative model, indicating significant correlation with the annotator scores.
We observe a similar pattern (i.e. syntagmatic inversely related to paradigmatic) by looking at the specializations. For instance, “chemistry” presents the highest scores in semantic relations whereas lower than average for ngrams. On the other hand, in the case of “sales” it is completely opposite. Note that most of the technical specializations and natural sciences demonstrate high scores for paradigmatic association types. We supposed that this is due to correlation between gender and occupation and the fact that gender still plays a significant role in the process of choosing the future career. In this context, a normalized value is a half-sum of two corresponding average values, each one being computed over its respective respondent gender. The normalization didn’t seem to smooth differences in ngram usage over various specializations. Therefore, one may conclude that the specialization standalone plays as a significant factor influencing word association patterns.
The dataset for this competition contains 125 pages with 1221 marriage records (paragraphs), where each record contains several text lines giving information of the wife, husband and their parents’ names, occupations, locations and civil states. The text images are provided at word and line level, naturally having the increased difficulty of word segmentation when choosing to work with line images.
S4SS1SSS0Px4 Result Compared with the best baseline model, our model still achieves higher performance by 1.5 percent. With more observation of the final architecture selected by the controller, we find out that the controller groups similar tasks together, showing the ability of clustering, which will be further discussed in Section 5.
Result We regard every combination of (domain, problem) such as (bc, NER) as different tasks, accumulating to 7 * 4 - 1 = 27 tasks. We evaluate our model together with all baseline models mentioned in Exp-I, with a slight change of removing the average pooling classifier to every step in the sequence. It is remarkable that our model achieves higher performance over all baseline models. It shows that our model successfully handles the situation of complicated task relationships.
Imbalanced testing scenarios are more challenging, but they also better reflect usage rates of condescending language in public forums like Reddit. To further understand how best to get traction on this problem, we explored a range of different methods for creating training data. As expected, the balanced problem is best addressed with a balanced dataset. For the imbalanced problem, we found that an oversampling ratio of 2 to 4 yielded the best performance. Our full Quoted ∧ Context model is again clearly superior in these scenarios.
Significant improvement over the secondary attention method is observed by using the proposed aggregated technique both in cases of PT from seen and unseen speaker (p-value<0.01 in pair-wise two-sided t-test and p-value<0.05 in Wilcoxon signed-rank test). VAE introduces additional improvement but not significant in case of same speaker. All comparisons from this figure are statistically significant (p-value<0.01 in t-test).
It is surprising that BERTLARGE performs better than RoBERTaLARGE. This finding indicates that, although using bytes makes it possible to learn a subword vocabulary that can encode any text without introducing “unknown” tokens, it might indirectly harm the model’s ability to learn factual knowledge, e.g., some proper nouns may be divided into bytes. Thus in the following experiments, we do not take BERT into account.
we use the Europarl corpus koehn2005europarl. It consists of the proceedings of the European Union Parliament in the form of sentences translated by humans in several European Union languages. For each language pair, we define two retrieval problems. For English with Italian for example, En→It is the problem where the English sentences serve as queries and the Italian documents as the collection of sentences from where the translation sentence needs to be retrieved. we observe that sEMD achieves the highest precision scores. This is the case independently of the language pair. The second best approach is EMD, which in most of the cases achieves comparable results. Both proposed methods obtain significantly higher scores compared to nBOW. nBOW performs poorly, suggesting that simply averaging word embeddings is not sufficient.
we used the MLDoc corpus which is a balanced version of Reuters corpus mldoc2018. The corpora consists of 1,000 training examples, 1,000 examples for validation and 4,000 examples for testing. We use the validation set for parameter tuning. The different versions based on Wasserstein distances are compared with ADAN and an MLP neural model that is based on the LASER sentence embeddings to represent the documents LASER. the performance of EMD with conceptNet embeddings when the training and test data are in the same language. For this line of experiments only, the reader should ignore the direction in the languages: En→Fr assumes training data in French, and test data in French also. This is why we note a single performance score in for Fr→En, … Es→En: in these four cases the test data are in English and we use also the English training documents. We assign the ordinal ranks for each method in each pair of languages and we resolve ties by averaging the corresponding ranks. First, we observe that ADAN and MLP-LASER achieve the best results in 3 pairs of languages each followed by EMD which tops the scoreboard in two cases. Interestingly, in this case EMD is better than sEMD in most of the cases except the It→En pair.
We obtained poor results using graph summary features on competition datasets, just like the Gutenberg data. On AAAC data, summary features gave best overall test set accuracy of 26.25% (using Logit Boost). Best overall test set accuracy reached 33.53% on PAN12 data (using Logit Boost). While these results are much better than a naïve random baseline (1.94% ± 1.59% on AAAC; 4.17% ± 3.85% on PAN12), we have obtained substantial gains by using local word network features on representative sets of words.
In our experiments, We notice that the overall results of SVM and XGBoost are very similar. Note that the table includes results derived from multiple experiments using different variations of features. In doing so, we conduct an ablation experiment to identify the most significant feature. Judging from the table, all of the feature combinations achieve precision higher than 50%. Since we use half-and-half positive and negative test data, our model is verified to obtain more robust results than a simple retrieval model. As for the comparison between different feature categories, the category of special words is generally the most important for correct predictions. Not including it drops the precision by 8%, accuracy by 7%, and F1 by 5%, while omitting the other two categories has less significant changes in the results. Out of all features of special words, the presence of action words in a sentence is the most essential, as omitting this feature causes a drop of 4%. Omitting other special word features individually only causes a mere drop of around 1% on average, so we do not display these results. It is also surprising that including the presence of now (part of time-based category) words actually worsens the predictions in every aspect by a considerable amount. One potential explanation of such observation is that the signal contained by now words are hard to generalize in newly seen scenarios, causing a discrepancy in distribution. The POS category seems to have the strongest effect on recall, and the syntactic feature has overall the weakest effect on results.
The experiment shows that solely applying feature-based classification yields the overall best results. However, for the real-life application of adding sound effects to radio stories, this method would be valuable since we care more about whether the added sounds are appropriate than whether we miss opportunities to add sounds.
Baseline Classifiers. AutoSlog Classifier. Although AutoSlog itself does not perform highly, the patterns that it learns represent a different type of knowledge than what is contained in many sentiment analysis tools. We therefore hypothesized that a cascading classifier, which supplements one of the baseline sentiment classifiers with the lexico-functional patterns that AutoSlog learns might yield higher performance. Retrained Stanford. The F-scores for retrained stanford are almost identical to the standard Stanford classifier. This may be because our data is a small percentage of the entire number of phrases used in training Stanford. Although retrained stanford prioritizes our phrases, it would not make sense to remove the original training data. Cascading Classifiers. We implement cascading classifiers to test our hypothesis. The cascade classifier has primary and secondary classifiers, and we invoke the secondary classifiers only if the primary assigns a prediction of neutral to a test instance, which reflects the lack of sentiment-bearing lexical items. We also have a cascade classifier with a tertiary classifier, which is invoked in the same fashion as the secondary classifier after the primary and secondary classifiers have been run. The cascading classifiers are named in the order the classifier is employed, primary, secondary or primary, secondary, tertiary. For our cascading classifiers, we combine our baseline classifiers (NRC and Stanford), with our AutoSlog classifier. We do not use SVM as a primary classifier since it has no neutral label.
For the Wiki(gold) we quote the micro-averaged F-1 scores for the entire top level entity category. The total F-1 score on the OntoNotes dataset is 88%, and the total F-1 cross-validation score on the 112 class Wiki(gold ) dataset is 53%. It is worth noting that one could improve Wiki(gold) results by training directly using this dataset. However, the aim is not to tune our model specifically on this class hierarchy. We instead aim to present a framework which can be modified easily to any domain hierarchy and has acceptable out-of-the-box performances to any fine-grained dataset. The other categories (date, time, norp, language, ordinal, cardinal, quantity, percent, money, law) have F-1 scores between 80-90%, with the exception of time (65%)
We perform an ablation study by leaving out one of the three modifications—\@setpar IDF weighting for interaction matrix, ReLU non-linearity instead of Tanh, and LP to combine local and distributed scores,\@noitemerr—out at a time. We observe a 33% degradation in MRR by not incorporating the IDF weighting alone. Similarly, we also observe a 26% degradation in MRR by using Tanh non-linearity instead of ReLU. Using a linear combination of scores from the local and the distributed model instead of combining their vector outputs using an MLP results in 14% degradation in MRR. Finally, we observe a 3% improvement in MRR by ensembling eight Duet v2 models using bagging. We also submit the individual Duet v2 model and the ensemble of eight Duet v2 models for evaluation on the heldout set and observe similar numbers. based approaches—e.g., ( Among the non-BERT based approaches, a proprietary deep neural model—called IRNet—currently demonstrates the best performance on the heldout evaluation set. This is followed, among others, by an ensemble of CKNRM (Dai et al., The single Duet v2 model achieves comparable MRR to the single CKNRM model on the eval set. The ensemble of Duet v2 models, however, performs slightly worse than the ensemble of the CKNRM models on the same set.
The EERs of all three countermeasure models increase as ϵ grows. PGD attacks attain larger EERs than FGSM attacks in all three countermeasure models and all the settings of ϵ under the white-box attack scenario. The EERs of all three models under the FGSM attacks reach near 50% when ϵ=5. As for PGD, the EERs are greater than 50% when ϵ=1, and are greater than 85% when ϵ=5, which will result in reversed classification decision if the operating point is pre-defined by the EER point on the evaluation set. We can conclude from the white-box attack results that the reliability of all three countermeasure models are challenged and broken down by FGSM or PGD attacks under the scenario of white-box attack. The PGD method is more effective than the FGSM. Research on more advanced countermeasure models should be done to keep pace with today’s white-box adversarial attacks.
This model results in 92.87%, 90.72% and 90.67% accuracies for training, validation, and testing datasets, respectively. The model is still able to slightly learn as well as generalize even after 300 epochs with no signs of overfitting. This model results in 94.25%, 93.49% and 93.45% accuracies for training, validation, and testing datasets, respectively. This is an improvement of 2.78% on the test set accuracy compared to the basic model.
Most neural models do not generate logical forms but instead build a differentiable network to solve a specific task such as question-answering. An exception is the neural sequence-to-tree model of \newcitecheng2017learning, which we extend to build the vanilla npr model. A key difference of npr is that it employs soft attention instead of hard attention, which is \newcitecheng2017learning use to rationalize predictions. Our results suggest that neural networks are powerful tools for generating candidate logical forms in a weakly-supervised setting, due to their ability of encoding and utilizing sentential context and generation history. Compared to \newcitecheng2017learning, our system also performs better. We believe the reason is that it employs soft attention instead of hard attention. Soft attention makes the parser fully differentiable and optimization easier. The addition of the inverse parser (+granker) to the basic npr model yields marginal gains while the addition of the neural lexicon encoding to the inverse parser brings performance improvements over npr and granker. We hypothesize that this is because the inverse parser adopts an unsupervised training objective, which benefits substantially from prior domain-specific knowledge used to initialize its parameters. When neural lexicon encoding is incorporated in the semantic parser as well, system performance can be further improved.
The difference in final outcomes of BWS and RS can be determined in two ways: by directly comparing term scores or by comparing term ranks. To compare scores, we first linearly transform the BWS and rating scale scores to scores in the range 0 to 1. Observe that the differences are markedly larger for commonly used annotation scenarios where only 3N or 5N total annotations are obtained, but even with 20N annotations, the differences across RS and BWS are notable.
the percentage of candidate modifiers with at least one prototype and (d) the mean number of prototypes per rule. Higher values of t\scriptsize evd (minimum evidence set size) lead to better quality in terms of hit rate and cosine similarity as prototypes have to be able to cover a larger number of word pairs in order to be retained. The rank threshold t\scriptsize rank also behaves as expected. Reducing t\scriptsize rank to 80 means that the predicted vectors are of higher quality as they need to be closer to the true compound embeddings. Tables (c) and (d) illustrate that the more restrictive parameter settings reduce the amount of modifiers for which prototypes can be extracted. From a total of 165399 candidate prefixes, only 3%-10% are retained in the end for our settings. Similarly, the average number of prototypes per modifier also decreases with more restrictive settings. Interestingly, however, for the most restrictive setting (t\scriptsize evd=10, t\scriptsize rank=80), this number is still a relatively high 2 prototypes per vector.
To test the analogy-based compound splitter on a realistic setting, we perform a standard machine translation task. We translate a German text using a translation baseline system with no compound handling (a), a translation system integrating the standard Moses compound splitter tool trained using the best-performing settings, and a translation system using our analogy-based compound splitter. We test the following basic methods of integration: Splitting only words that are OOV to the translation model (b), splitting all words that occur less than 20 times in the training corpus (c),and applying the compound splitters to every word in the datasets (d).
In addition to the BLEU-4 metric, we propose to use the noun/verb recovery accuracy, which is the percentages of the correctly translated nouns/verbs in the target sentences, to precisely evaluate the impact of additional video information on recovering nouns/verbs. First, the VMT model consistently outperforms the NMT model with different masking rates on both metrics. Moreover, as the masking rate increases, the NMT model struggles to figure out the correct nouns/verbs because of the scarce parallel caption pairs; while the VMT model can rely on the video context to obtain more useful information for translation, and thus the performance gap on the recovery accuracy increases dramatically. It shows that in our VMT model, video information can play a crucial role in understanding subjects, objects, and actions, as well as their relations.
i) women are more prone to be afraid than men, b) African Americans are more prone to be angry etc. In addition to experimenting with the balanced dataset, we also prepare a more realistic version of the dataset by aligning the emotion distributions to match a set of social stereotypes. The objective of creating these explicitly biased datasets was to investigate if our proposed model can reduce biases that are explicitly present as class priors in the data. Our experiments also revealed that the predictions of bias-agnostic models on the balanced dataset turned out to be biased towards predicting Caucasians as more likely to be afraid (possibly due to effects of word embedding on large volumes of text regarding Islamophobia). Importantly, the results on the subsampled data typically reflects on the fact that predictions under the presence of cognitive biases in data can lead to non-humane responses such as all women are afraid (as can be observed from the value αfemale=1). Another important observation is that the overall accuracy values also turn out to be the best among the competing approaches. This happens because the use of pre-trained word embeddings (both gender agnostic and gender equalized) introduce potential sources of gender and race specific biases as parts of the input. However, with bias-aware training it is possible to make more accurate predictions.
Importantly, the results on the subsampled data typically reflects on the fact that predictions under the presence of cognitive biases in data can lead to non-humane responses such as all women are afraid (as can be observed from the value αfemale=1).
To this end, we perform another experiment to analyze the effect of bias in word embeddings on dialogue and language models. Extended descriptions of the experimental setup and results can be found in the supplemental material. For each trigger token, we extract 1000 samples from the stochastic language model. In local examinations, some triggers with stereo-typically male-dominated occupations see significant shifts toward even male-female follow-up distributions. For example, “the evangelist”, a male stereotypical job, sees a followup distribution which is about 83% male (where gender-specific terms exist), but decreases to 71% male with the debiased word vectors. Clearly, biases are encoded nonetheless by the language model, necessitating further work in this area.
Baseline Results The baseline performance is the POS tagging accuracy of the monolingual models with no special training for CS data. Since we have four monolingual models, we consider four baselines. If CS data do not pose any particular challenge to monolingual POS taggers, then we shouldn’t expect a major degradation in performance. For Arabic, MSA monolingual performance for MADAMIRA-MSA, when tested on monolingual MSA test data, is around ∼97% accuracy, and for MADAMIRA-EGY when tested in monolingual EGY data it is ∼93%. We note here that the presence of CS data in the ARZ test data causes these systems to degrade significantly in performance (77% and 72% accuracy, respectively). For SPA-ENG, state of the art monolingual models achieve an accuracy of ∼96% and ∼93 on monolingual English and monolingual Spanish data sets, respectively. It is then clear that CS data poses serious challenges to monolingual technology. Other prior work has also reported similar drops in performance because of having mixed language data.
Other than our baselines, we compare with Wang et al. We observe that the simple FFNN baselines work better than more complex Siamese and Multi-Perspective CNN or LSTM models, more so if character n-gram based embeddings are used. Our basic decomposable attention model DecAttword without pre-trained embeddings is better than most of the models, all of which used GloVe embeddings. An interesting observation is that DecAttchar model without any pretrained embeddings outperforms DecAttglove that uses task-agnostic GloVe embeddings. Furthermore, when character n-gram embeddings are pre-trained in a task-specific manner in DecAttparalex−char model, we observe a significant boost in performance.
Training word embedding models involves selecting several hyper-parameters. However, as the word embeddings are usually evaluated in an unsupervised setting (i.e., the evaluation data sets are not seen during the training), the parameters should not be tuned on each dataset. To conduct a fair comparison, we tune hyper-parameters on the text8 dataset. For GloVe model, we tune the discount parameters xmax and find that xmax=10 performs the best. SGNS has a natural parameter k which denotes the number of negative samples. Same as \newciteLevy2015, we found that setting k to 5 leads to the best performance. For the PU-learning model, ρ and λ are two important parameters that denote the unified weight of zero entries and the weight of regularization terms, respectively. We tune ρ in a range from 2−1 to 2−14 and λ in a range from 20 to 2−10. We analyze the sensitivity of the model to these hyper-parameters in the experimental result section. It shows that PU-learning model outperforms two baseline models.
We compared the proposed PU-Learning framework with two popular word embedding models – SGNS Mikolov et al. et al. The results show that the proposed PU-Learning framework outperforms the two baseline approaches significantly in most datasets. This results confirm that the unobserved word pairs carry important information and the PU-Learning model leverages such information and achieves better performance. To better understand the model, we conduct detailed analysis as follows.
s2sL shows an absolute improvement in accuracy of 4.4% and 4.1% over MLP for Speech/Music and Neutral/Sad classification tasks, respectively, when (1/4)th of the original training data is used in experiments. Here, state-of-the-art methods i.e., Eusboost [22] and MWMOTE [23] are also considered for comparison. In particular, at lower amounts of training data, s2sL outperforms all the other methods, illustrating its effectiveness even for low resourced data imbalance problems. s2sL method shows an absolute improvement of 6% (0.54−0.48) in F1 value over the second best (0.48 for MWMOTE), when only (1/4)th of the training data is used.
In the first experiment, the mixed units contain single-letters and 27k frequent words. During training, OOV words are decomposed into single-letter sequence. Therefore, the trained CTC model achieved 20.10% WER. When looking at the posterior spikes of this model, we observed that the word spikes and letter spikes are scattered into each other which proves our hypothesis.
We perform experiments on document-level sentiment datasets in five languages: English, French, German, Japanese, and Norwegian. For the first four, we use the Amazon Customer Reviews datasets, a 5-class sentiment dataset with labels L∈{1,2,3,4,5} stars. Although the full corpora are much larger, due to preprocessing requirements and in the interest of having similar sized data for all languages, we create a subcorpus D by sampling 50,000 documents for each language without regarding domain, finally splitting these into test/dev/train splits of 35,000/5,000/10,000 documents. For Norwegian, we use the NoReC corpus 2.0, which is a 6-class task with labels L∈{1,2,3,4,5,6} ratings. It differs from version 1.0 Velldal et al. in that it has more training examples. The Bow model performs well across all experiments, achieving an average 64.2 accuracy, and ties Han for the best performance on the German dataset (73.2). The Cnn performs worse than the Bow across all experiments except Japanese (an average loss of 1.2 percentage points (pp)). ULMFiT performs better than Bow on Norwegian and Japanese (0.1 / 3.1 pp), but 0.4 pp worse overall.
We further divide the word sets with three POS tags into five parts respectively according to the similarity range, including [1.0,0.9],[0.9,0.8],[0.8,0.7],[0.7,0.6],[0.6,0.4]. Note that we don’t take word pairs with cosine similarity lower than 0.4 into account because almost all the them are not really similar to each other. Finally, we obtain 480 noun pairs, 240 verb pairs and 240 verb pairs.
+ XLNet ensemble. The first two ensembles consist of 6 models obtained using the different experiments from above. We tried taking a majority vote (using the best model out of these 6 to decide when there is a tie), and rounding the mean of the scores predicted by each model. Both methods performed similarly on items 1 to 6, but the majority vote performed significantly poorer on items 7 and 8. The BERT + XLNet ensemble consists of 12 models, i.e, it combines the models from the two other ensembles together. The last two rows correspond to the Bag of Words model and the inter-human agreement.
We can see that for both metrics, one of the NL-WFA models outperforms linear spectral learning. Individually speaking, for modeling the distribution (i.e. the perplexity metric) tran.non gives the best performances, while for the prediction task fac.non shows a significant advantage.
DPMFcomb is trained through relative ranking of human evaluation data in terms of relative ranking (RR). The quality of five MT hypotheses of the same source segment are ranked from 1 to 5 via comparison with the reference translation. In contrast, Blend is trained through direct assessment (DA) of human evaluation data. DA provides the absolute quality scores of hypotheses, by measuring to what extent a hypothesis adequately expresses the meaning of the reference translation. In this study, as with Blend, we propose a supervised regression model trained using DA human evaluation data. It is a metric using Tree-LSTM (Tai et al., 2015) for training and capturing the holistic information of sentences. The proposed metric uses sentence representations trained using LSTM as sentence information. Further, we apply universal sentence representations to this task; these representations were trained using large-scale data obtained in other tasks. Therefore, the proposed approach avoids the problem of using a small dataset for training sentence representations.
Initializing foreign embeddings is the backbone of our approach. A good initialization leads to better zero-shot transfer results and enables fast adaptation. To verify the importance of a good initialization, we train a ℝ𝔸𝕄𝔼ℕ\textscbase+BERT with foreign word-embeddings that are initialized randomly from N(0,\nicefrac1d2). In comparison to the initialization using aligned fastText vectors, random initialization decreases the zero-shot performance of ℝ𝔸𝕄𝔼ℕ\textscbase by 10.3% for XNLI and 11.6 points for UD parsing on average. We also see that zero-shot parsing of SOV languages (Arabic and Hindi) suffers random initialization. To highlight the efficiency of our transferring approach, we compare ℝ𝔸𝕄𝔼ℕ\textscbase+BERT with a bilingual BERT (bBERT) trained from scratch (scr). All bBERT models are trained for 2,300,000 updates (400 GPU hours), which is more than sixteen times longer than RAMEN. First, the randomly initialized foreign embeddings ℝ𝔸𝕄𝔼ℕ performs on par with bBERT on XNLI task and significantly better than bBERT on UD parsing. This suggests that a large amount of linguistic knowledge can be transfered through reusing the pretrained BERT encoder. Secondly, with a careful initialization of foreign embeddings, our ℝ𝔸𝕄𝔼ℕ\textscbase+BERT outshines bBERT
We used train/dev/test splits provided in UD to train and evaluate our ℝ𝔸𝕄𝔼ℕ-based parser. For a fair comparison, we choose mBERT as the baseline and all the ℝ𝔸𝕄𝔼ℕ models are initialized from aligned fastText vectors. With the same architecture of 12 Transformer layers, ℝ𝔸𝕄𝔼ℕ\textscbase+BERT performs competitive to mBERT and outshines mBERT by +1.1 points for Vietnamese. The best LAS results are obtained by ℝ𝔸𝕄𝔼ℕ\textsclarge+RoBERTa with 24 Transformer layers. Overall, our results indicate the potential of using contextual representations from ℝ𝔸𝕄𝔼ℕ for supervised tasks.
Sub-Task C:BERT For sub-task C, using BERT based approach, we trained the model for 2 epochs. Further increasing the number of epochs decreased the F1 score on the OLID dataset; hence we went for 2 epochs.
During the post-evaluation phase, for Greek, we are working on models that focus on the addition of deep learning models on top of the frozen BERT layer using \newciteS2. By the addition of Bidirectional LSTM (Bi-LSTM) on top of the BERT layer, we got an F1 score of 0.797. After concatenation of the last 4 hidden layers of BERT for both Bidirectional GRU(Bi-GRU) and Bi-LSTM models, the F1 score was 0.819 and 0.837, respectively. The results were obtained after training each model for a maximum of 10 epochs and storing the model with the largest F1 score among the 10 epochs. We are still in the process of tuning the hyperparameters for getting a stable output as we are experiencing a lot of deviation in our F1 scores. We have given the best F1 score obtained after the training of the model. We are also planning to take care of the deviation using an ensemble method with different dropout rates.
We measure human v.s. human correlation by randomly splitting the human users into two groups. The results indicate that in most cases, human scores correlate the best with other human scores. Except in the case of the Spearman correlation for BLEU-N scores, we can see that there is a positive correlation between the automated metrics and the human scores for these task-oriented datasets, which contrasts with the non task-oriented dialogue setting where \newciteLiuHowNotTo-D16-1230 observed no strong correlation trends. We observe that all the metrics correlate very well with humans on high scoring examples. As it can be seen in the scatter plots, most of the sentences are given the maximal score of 5 by the human evaluators. This confirms our previous observation that the available corpora for task-oriented dialogue NLG task are not very challenging and a simple LSTM-based model can output high-quality responses.
Consistent and Competitive Speedup Gains In particular, we observe that while skim-RNN performs well on SST-2 and IMDB, its performance is inconsistent across different budgets. For example, on AGNews dataset, their baseline model achieves 93.5 in accuracy but with a longer test-time, the performance drops to 92.5 (see skim-RNN-2) Benefits of DAG: Our framework achieves better performance than Bag-of-Words (without DAG) which highlights that the issue of classifier incompatibility is real. By training classifier with the proposed aggregation framework, the model is robust to the distortions and achieves better accuracy with speedup. Finally, we observe that the classifiers trained with DAG improves both baselines with LSTM and BCN on full-text. By aggregating fragments picked by selectors, the model is able to put more emphasis on important words and is more robust to the noise in the input document. (details in Appendix B), and for the very large Yelp dataset, 3 selectors with budgets {50%, 60%, 70%} are used.
Using incremental edits produces a significant improvement in performance over single-shot decoding for models trained on the Wikipedia revision data, a highly noisy corpus, while models trained on the relatively clean round-trip translation data see no improvment. All models finetuned on Lang-8 see improvement with iterative decoding
We only report performance of models that use publicly available Lang-8 and CoNLL datasets. Our single system trained on all revisions outperforms all previous systems on both datasets, and our ensemble improves upon the single system
The error categories were tagged using the approach in \newcitebryant17. Although the overall F0.5 of the 2 ensembles are similar, there are notable differences on specific categories. The ensemble using round-trip translation performs considerably better on prepositions and pronouns while the revision ensemble is better on morphology and orthography. Thus, each system may have advantages on specific domains.
The Switchboard experiments focus on the very deep aspect of our work. Apart from not involving multilingual training, we did not use multi-scale features in the Switchboard experiments, but did use speaker-dependent VTLN and deltas and double deltas as this is shown to help performance for classical CNNs
The speech enhancement module is used only for tracks 2 and 4 as a pre-processing front-end for the SAD pipeline as the diarization system did not show improvements using the enhanced audio. The scores obtained by the challenge baseline are quite high, with track 1 DER roughly in line with the performance of the best DIHARD I systems [ Error rates are noticeably higher for tracks 3 and 4, reaching 50.85% and 77.34% respectively, though, again, these rates are roughly in line with those observed for the best DIHARD I systems on the two most difficult domains in that challenge: restaurant and child language.
For all seven evaluation metrics, the mean score for the models trained on the “more captions” datasets was greater than the mean score for the models trained on the “more images” datasets, and the results were significant at the 5% level for all three statistical tests in the case of BLEU-1 and BLEU-2. Interestingly, for CIDEr/CIDEr-D, the results were somewhat significant (all 6 p-values less than .15) but the results for METEOR were the least significant (all 3 p-values greater than .94).
While Hard EM achieves impressive performance when reembedding from scratch and when training on only 200 or 500 examples, we wonder whether this performance is due to the alternating optimization, to the multiple E-step updates per M-step update, or to the lack of sampling. We accordingly experiment with optimizing our VQ-VAE and CatVAE variants in an alternating way, allowing multiple inference network updates per update of the generative parameters θ. We find that alternating does generally improve the performance of VQ-VAE and CatVAE as well, though Hard EM performs the best overall when reembedding from scratch. Furthermore, because Hard EM requires no sampling, it is a compelling alternative to CatVAE. For all three methods, we find that doing 3 inference network update steps during alternating optimization performs no better than doing a single one, which suggests that aggressively optimizing the inference network is not crucial in our setting.
We further analyzed the polarities assigned by MilNet and HierNet to positive, negative, and neutral segments. In the case of negative and positive sentences, both models demonstrate appropriately skewed distributions. However, the neutral class appears to be particularly problematic for HierNet, where polarity scores are scattered across a wide range of values. In contrast, MilNet is more successful at identifying neutral sentences, as its corresponding distribution has a single mode near zero. Attention gating addresses this issue by moving the polarity scores of sentiment-neutral segments towards zero. The effect is very significant for HierNet, while MilNet benefits slightly and remains more effective overall. Similar trends were observed in all four SpoT datasets. Our models also seem to learn better sentence embeddings, as they improve GICF’s performance on both collections.
We further analyzed the polarities assigned by MilNet and HierNet to positive, negative, and neutral segments. In the case of negative and positive sentences, both models demonstrate appropriately skewed distributions. However, the neutral class appears to be particularly problematic for HierNet, where polarity scores are scattered across a wide range of values. In contrast, MilNet is more successful at identifying neutral sentences, as its corresponding distribution has a single mode near zero. Attention gating addresses this issue by moving the polarity scores of sentiment-neutral segments towards zero. The effect is very significant for HierNet, while MilNet benefits slightly and remains more effective overall. Similar trends were observed in all four SpoT datasets. Our models also seem to learn better sentence embeddings, as they improve GICF’s performance on both collections.
The first block in the table shows a slight preference for MilNet across criteria. The second block shows significant preference for MilNet against HierNet on informativeness and polarity, whereas HierNet was more often preferred in terms of coherence, although the difference is not statistically significant. The third block compares sentence and EDU summaries produced by MilNet. EDU summaries were perceived as significantly better in terms of informativeness and polarity, but not coherence. This is somewhat expected as EDUs tend to produce more terse and telegraphic text and may seem unnatural due to segmentation errors. In the fourth block we observe that participants find MilNet more informative and better at distilling polarity compared to the Lead and Random (EDUs) baselines.
The annotations are organized into 26710 image pairs (12416 images) in training set, and 5041 pairs (2348 images) in test set. Since that image pairs are annotated by different workers without any constraint in advance such as attribute range, sentence pattern and so on, the difference descriptions are diverse of open-ended ”realistic” human-style. Besides, the annotations contain various attributes/key points, such as color, texture, accessories, size and so on, which enriches the visual comparison details compared to those attribute-based datasets.
We used a TF-IDF model to obtain a base-line prediction of academic performance from the users’ posts. We selected the 1000 most common unigrams and bigrams from our corpus, excluding stop words, for the Russian language. We then applied a TF-IDF transformation to represent posts as 1000-dimensional vectors and then trained a linear regression model on individual posts to predict the academic performance of their authors. The correlation between predicted and real scores is r=0.285. Here, and for the following models, we report results on the user level obtained using leave-one-out cross-validation, i.e. scores for posts of a certain user were obtained from the model trained on posts of all other users. We obtained significantly better results with a model that used word-embeddings (see Methods).
To our best knowledge, the result of CIF-based model on AISHELL-2 and the result of transformer on HKUST are the best accuracy performance of respective dataset, thus the comparison is conducted on two very strong models that have different synchronous mode.
We see that the first LSTM layer generally achieves the best crosslingual alignment both in ELMos and Rosita. This finding mirrors recent studies on layerwise transferability; representations from the first LSTM layer in a language model are most transferable across a range of tasks Liu et al. Our decontextual probe demonstrates that the first LSTM layer learns the most generalizable representations not only across tasks but also across languages. In all six languages, Rosita (joint LM training approach) outperforms ELMos (retrofitting approach) and the fastText vectors. This shows that for the polyglot (jointly trained) LMs, there is a preexisting similarity between languages’ vector spaces beyond what a linear transform provides. The resulting language-agnostic representations lead to polyglot training ’s success in low-resource dependency parsing.
Adaptive subgradient methods for online learning and stochastic optimization.. JMLR 12. External Links: We use the publicly available code of Peters et al. Following Mulcaire et al. , we reduce the LSTM and projection sizes to expedite training and to compensate for the greatly reduced training data—the hyperparameters used in Peters et al. Contextual representations from language models trained with even less text are still effective Che et al. suggesting that the method used in this work would apply to even lower-resource languages that have scarce text in addition to scarce or nonexistent annotation, though at the cost of some of the performance.
In addition to the 100-sentence condition, we simulated low-resource experiments with 500 and 1000 sentences of target language data, and zero-target-treebank experiments in which the parser was trained with only source language data, but with multilingual representations allowing crosslingual transfer. the less target-language data is available, with a slight advantage for related languages.
We evaluate four models on the test set of DocBank. We notice that the LayoutLM gets the highest scores on the {author, footer, section, title, abstract, list, paragraph, caption, equation, figure, table} labels. The BERT model gets the best performance on the “reference” label but the gap with the LayoutLM is very small. This indicates that the LayoutLM architecture is significantly better than the BERT architecture in the document layout analysis task. In addition, it is observed that the LayoutLM trained from scratch is unsatisfactory on all the labels. Meanwhile, pre-training the LayoutLM model on our in-house unlabeled dataset improves the accuracy significantly on the DocBank dataset. This confirms that the pre-training procedure significantly improves the performance of LayoutLM on the benchmark. As this work is still in progress, we will enlarge the DocBank dataset and update the results in the revised version later.
Finally, we took the best checkpoint, with SLOT F1 = 79.95% at 118,000 steps, and evaluated it on the test corpus. With the exception of LABEL accuracies, all the other metrics exhibit less than half a percent difference between the test and dev corpora. This illustrates that despite the lack of dropout, the model generalizes well to unseen text. On the dev set, LABEL F1 peaked at 96.18 at 100,000 steps, and started degrading slightly from there on to 95.73 at 118,000 steps, possibly showing signs of overfitting which are absent in the other metrics.
To compare the proposed methods, three models shown in Fig. As a comparison method, we used the lamBERT model (w/o mask loss (ML)) and a convolutional neural network (CNN)-based model (CNN+GRU) that represents the vision and language information using convolutional neural network (CNN) and gated recurrent unit (GRU), respectively. When learning the lamBERT (w/o ML), the mask processing was performed on the input token during learning in the same way, as in the lamBERT (w/ ML). This processing of the lamBERT (w/o ML) was performed to eliminate the possibility that the input of data with a mask by the proposed method could improve the generalization of the model. The lamBERT model’s
We report the general performance of our models in all the testing sets. For wav2letter and Speech Transformer, we test the model performance under both settings: i) w. WSP and ii) w/o. WSP, based on our own implementations. We have the following findings: i) Speech Transformer ii) The WSP technique effectively boosts the performance of both models on all the datasets. This phenomenon is more significant on small datasets (i.e., AIDATANG and HKUST). iii) The Speech Transformer model w. WSP achieves state-of-the-art performance on all the six public datasets, which clearly proves the value of our framework.
Next, we evaluate the effectiveness of the iterative pre-training technique. In this step, we filter out part of the data (quantified by the drop ratio γ) and take the rest as the pre-training data for the next iteration. We search the best value of γ from 0,0.5%,1.0%,2.0% and also compare our method with a classical data filtering approach (i.e., Liao et al. Regarding the implementation of Liao et al. It shows that WSP with γ=1.0% has the best performance.
Error analysis and case studies. We further present an error analysis for deeper understanding of WSP. The underlying ASR models are the Speech Transformer with and without WSP. As seen, the majority of the errors are substitution errors caused by homophones. The WSP technique helps to reduce such errors, because the pre-training dataset is much larger than public Mandarin datasets. The pronunciations and language contexts are more diverse, leading to the better generalization ability of trained ASR models. It shows WSP’s ability to distinguish words with similar pronunciation.
The classifiers based on the MultiCCA embeddings perform very well on the development corpus (accuracies close or exceeding 90%). The system trained on English also achieves excellent results when transfered to a different languages, it scores best for three out of seven languages (DE, IT and ZH). However, the transfer accuracies are quite low when training the classifiers on other languages than English, in particular for Russian, Chinese and Japanese.
Due to space constraints, we provide only the results for multilingual sentence embeddings and five target languages. Not surprisingly, targeting the classifier to the transfer language can lead to important improvements, in particular when training on Italian.
In action analysis, we focus on detecting whether or not a special action is mentioned in a sentence. Therefore, we conduct the “sentence classification” task, classing each sentence by a classifier trained on the ACE annotated event mentions. In our application, we monitor the “Conflict” ACE event type, which has two substype: Attack and Demonstrate The ACE corpus, which annotates 596 “Conflict” events, is employed for training and testing. We implement the 5-fold cross validation, and the P/R/F (Precision/Recall/F-score) measurement. F-score is computed by (2×P×R)/(P+R). In order to perform a two-class classification, we generate negative instances by segmenting the corpus into sentences, discarding annotated ACE event mentions, and filtering sentences without event triggers of “Conflict” ACE events. Then, 1,589 sentences are collected as negative instances. We only use Omni-words features in sentences for classification. In an open field with massive data, the precision is more emphasized. Therefore, we label an instance as a “Conflict” action only when the employed classifier (maximum entropy) output a predicted value We use this setting to train a classifier and predict every sentence in document events. Entity co-occurrences in each “Conflict” sentence are calculated.
We hence focus on the EER below. On all datasets, anonymization of the trial data greatly increases the EER. This shows that the anonymization baseline effectively increases the users’ privacy. The EER estimated with original enrollment data (47 to 58%), which is comparable to or above the chance value (50%), suggests that full anonymization has been achieved. However, anonymized enrollment data result in a much lower EER (26 to 37%), which suggests that F0+BN features retain some information about the original speaker. If the attackers have access to such enrollment data, they will be able to re-identify users almost half of the time. Note also that the EER is larger for females than males on average. This further demonstrates that failing to define the attack model or assuming a naive attack model leads to a greatly overestimated sense of privacy [srivastava2019evaluating].
Baseline B-1: It considers as aspect terms all nouns of the test set with any syntactic relation to any word from the Senticnet lexicon. Baseline B-2: It is similar to B-1, however nouns are considered as aspect terms only if they have any syntactic relation to adjectives from the Senticnet lexicon. Baseline B-3: It extends B-2 by labelling nouns as aspect terms only if the syntactic relation to any word from the Senticnet lexicon is of type amod or advmod or acomp. Moreover, B-3 labels as aspect terms nouns that are in conjunction with other aspect terms. Baseline B-4: It includes the full set of the 12 syntactic rules we introduce. Once again, only nouns related to words from the Senticnet lexicon are considered as aspect terms. We see that B-4 performs the best among all baselines. This fact proves that our set of syntactic rules, combined with the Senticnet lexicon, are capable of identifying aspect terms. We also highlight that these baselines are completely unsupervised and domain-independent since they use only syntactic rules combined with a sentiment lexicon in order to identify aspect terms, i.e there is no use of training data or attention. Columns SVM0 through SVM0.99 prove that the SVM classifier beats the baseline model. The subscript in the column name indicates the value of vth. These columns also validate our hypothesis, that sentence selection matters when it comes to constructing a new dataset for ATE. The classifier has a performance of F1=41.36 when we use all sentences (vth=0) for the ALD construction. All evaluation metrics increase as the sentence selection threshold increases from 0 to 0.7, apart from a small fluctuation (0.4%) when vth=0.6. We believe that this increase is due to the fact that the sentence selection removes noise from the ALD which leads to improved classifier performance. We also notice a decrease in the performance for sentence selection thresholds greater than 0.7. In these cases, we believe that the sentence selection is harsh and removes useful information from the ALD which results in performance deterioration. F1=44.87±0.42.\enit@after The experimental results validate that the B-LSTM & CRF classifier outperforms all aforementioned models for all 3 evaluation metrics.
Grammatical Correctness: Raters had to rate a question on how grammatically correct it is or how syntactically fluent it is, disregarding its underlying meaning. Relevance Score: Raters had to give a score on how relevant the generated question is to the given fact. The relevance score helps us gauge whether the question should have been generated or not irrespective of its grammaticality. Each question was evaluated by three people scoring grammaticality and relevance on a 5 point Likert scale. The inter-rater agreement (Krippendorff’s co-efficient) among human evaluations was 0.72. Syn-QG generates a larger number of questions than H&S and performs strongly on grammaticality ratings. Syn-QG is also able to generate highly relevant questions without the use of a ranker. Also, rule-based approaches seem to be much better at generating relevant questions than neural ones.
These metrics were compute over a 10% validation split, silence frames and frames with mismatched voiced/unvoiced decision between target and prediction were excluded. The IS16 system is omitted from this table because the low redundancy of diphones in the dataset would force this system to find replacements for some diphones in the held-out validation utterances, giving the system an unfair disadvantage.
We compared the relative success rates of Jin et al. Alzantot et al. with constraint evaluation held constant. We applied the constraint evaluation methods from above and tested each attack against BERT fine-tuned on the MR dataset. The attacks achieved similar scores on human evaluation of semantics and non-suspicion.
Remarkably, some sensible designs of DeepMemory (e.g., Arc-II) can already achieve performance comparable to Moses, with only 42M parameters, while RNNsearch\textscbest has 46M parameters.
We considered standard evaluation metrics to assess the models performance in the task of typhoons prediction. In contrast, our proposed models clearly demonstrate a significant improved performance where tweets-based features were incorporated with environmental data. In particular, the best accuracy is achieved by our proposed model BiLSTM+CNN, where it outperforms the respective baselines (CNN by 12.1% and BiLSTM by 3.1%) on micro average F1-score. The other proposed models (LSTM+DNN and LSTM+RNN) also outperform their respective baselines (by 11% in DNN and 5.8% in RNN) on average.
DFFN performs better than the top systems across all the metrics. The improvement is higher in SemEval 2015 although the task is more harder due to lesser training data and more granularity in target labels to be predicted. We also observe that DFFN performs better than a single CNN alone (DFFN w/o HCF) or single hand-crafted feature based model alone (DFFN w/o CNN). Hence, the fusion of deep features and hand-crafted features helps in boosting the performance.
One important thing about the FTB is that the underlying text is made of articles from the newspaper Le Monde that are chronologically ordered. Moreover, the standard development and test sets are at the end of the corpus, which means that they are made of articles that are more recent than those found in the training set. This means that a lot of entities in the development and test sets may be new and therefore unseen in the training set. To estimate the impact of this distribution, we shuffled the data, created a new training/development/test split of the same lengths than in the standard split, and retrained and reevaluated our models. We repeated this process 3 times to avoid unexpected biases. We can see that the shuffled splits result in improvements on all metrics, the improvement in F1-score on the test set ranging from 4.04 to 5.75 (or 25% to 35% error reduction) for our SEM baseline, and from 1.73 to 3.21 (or 18% to 30% error reduction) for our LSTM-CRF architectures, reaching scores comparable to the English state-of-the-art. This highlights a specific difficulty of the FTB-NE corpus where the development and test sets seem to contain non-negligible amounts of unknown entities. This specificity, however, allows to have a quality estimation which is more in line with real use cases, where unknown NEs are frequent. This is especially the case when processing newly produced texts with models trained on FTB-NE, as the text annotated in the FTB is made of articles around 20 years old.
We also conduct human evaluation to evaluate the linguistic quality of the generated abstractive summaries, and compare with some significant baselines. We randomly sample 10 document sets from the DUC 2002 dataset and another 10 document sets from the DUC 2004 dataset for human evaluation. Three volunteers who are fluent in English were asked to perform manual ratings on three dimensions: Coherence, Non-Redundancy (N.R. for short) and Readability. The ratings are in the format of 1-5 numerical scores (not necessarily integral), with higher scores denote better quality. It can be observed that our system also outperforms other abstractive summarization approaches in human evaluation, achieving good coherence and readability.
We evaluate our approach and the baselines on the test set. It is obvious that the proposed approach significantly outperforms the baselines, with micro F1 of 64.5, macro F1 of 54.4, and one-error of 22.6, improving the metrics by 10.6, 21.4, and 7.9 respectively. The improvement is attributed to two parts, a hierarchical attention network and a label correlation mechanism. Only using the hierarchical attention network outperforms the baselines, which shows the effectiveness of hierarchically paying attention to different words and sentences. The greater F1-score is achieved by adding the proposed label correlation mechanism, which shows the contribution of exploiting label correlations. Especially, the micro F1 is improved from 61.0 to 64.5, and the macro F1 is improved from 52.1 to 54.4.
First, the proposed method performs worse on the styles with low frequency in the training set. As we can see, the top 5 fewest music styles get much worse results than top 5 most music styles. This is because the label distribution is highly imbalanced where unpopular music styles have too little training data. For future work, we plan to explore various methods to handle this problem. For example, re-sample original data to provide balanced labels.
Both of the proposed models significantly outperform their base models. On test accuracy, by adding asymmetric word embeddings, AWE-Decomp-Att has a performance gain of 2.0% over Decomp-Att on SciTail and 0.3% on SNLI; AWE-DeIsTe outperforms DeIsTe by 2.1% on SciTail and 1.9% on SNLI. This demonstrates the effectiveness of our approach to improving word-word interaction based textual entailment models.
Both of the proposed models significantly outperform their base models. On test accuracy, by adding asymmetric word embeddings, AWE-Decomp-Att has a performance gain of 2.0% over Decomp-Att on SciTail and 0.3% on SNLI; AWE-DeIsTe outperforms DeIsTe by 2.1% on SciTail and 1.9% on SNLI. This demonstrates the effectiveness of our approach to improving word-word interaction based textual entailment models.
Transferability One advantage of our approach is that the extracted entailment word pairs and the derived asymmetric word embeddings can be further used for other tasks. To demonstrate it, we experimented with AWE-DeIsTe using asymmetric embeddings from other datasets. For example, when we train/test AWE-DeIsTe on SciTail, instead of using asymmetric word embeddings derived from SciTail, we used the alternative asymmetric embeddings from SNLI. Although the new asymmetric word embeddings are not from the same training data, they are still useful to get better results than the baseline model which did not use asymmetric word embeddings. Thanks to the large scale of both SciTail and SNLI, the cross-data test accuracies are even comparable to the original AWE-DeIsTe.
Even if the results on the W&I+L development set are only partially indicative of system performance, we report them due to the W&I+L test set being blind. All mentioned papers do not train their systems on the development set, but use it only for model selection. Also note that we split the results on CoNLL 14 test set into two groups: those who do not use the W&I+L data for training, and those who do. This is to allow a fair comparison, given that the W&I+L data were not available before the BEA 2019 Shared Task on GEC. The best performing systems are utilizing ensembles. The best performing system on English is an ensemble system of \newcitegrundkiewicz2019neural.
To account for the lack of annotated data, she generated additional training data from Wikipedia edits, which she filtered to match the distribution of the original error types. All our three systems outperform it.
Although our system outperforms the system of \newciterozovskaya2019grammar by more than 100% in F0.5 score, its performance is still quite poor when compared to all previously described languages. Because the result of our system trained solely on synthetic data is comparable with the similar system for English, we hypothesise that the main reason behind these poor results is the small amount of annotated training data – while Czech has 42 210 and German 19 237 training sentence pairs, there are only 4 980 sentences in the Russian training set. To validate this hypothesis, we extended the original training set by 2 000 sentences from the development set, resulting in an increase of 3 percent points in F0.5 score.
Consistent with our intuition, the last row in this table shows that the task is trivial when the parallel data is provided. In non-parallel case, the difficulty of the task is driven by the substitution rate. Across all the testing conditions, our cross-aligned model consistently outperforms its counterparts. The difference becomes more pronounced as the task becomes harder. When the substitution rate is 20%, all methods do a reasonably good job in recovering substitutions. However, when 100% of the words are substituted (as expected in real language decipherment), the poor performance of variational autoencoder and aligned auto-encoder rules out their application for this task.
All results were computed using the official evaluation script from the i2b2 2014 de-identification challenge. These PHI types are the most important since only those are required to be removed by law. The only freely available, off-the-shelf program for de-identification, called the MITRE Identification Scrubber Toolkit (MIST) Combining the outputs of our ANN and CRF models, by considering a token to be PHI if it is identified as such by either model, further increases the performance in terms of F1-score and recall. We anticipate that adding gazetteer features based on the local institution’s patient and staff census should improve this result, which will be explored in future work.
We had two human annotators who were trained on snippets (i.e., (suspected trolling attempt, responses) pairs) taken from 200 conversations and were allowed to discuss their findings. After this training stage, we asked them to independently label the four aspects for each snippet. We recognize that this limited amount of information is not always sufficient to recover the four aspects we are interested in, so we give the annotators the option to discard instances for which they couldn’t determine the labels confidently. The final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens. All the results are obtained using 5-fold cross-validation experiments. In each fold experiment, we use three folds for training, one fold for development, and one fold for testing. All learning parameters are set to their default values except for the regularization parameter, which we tuned on the development set. The next section (Single Feature Group) reports F1 scores obtained by using one feature group at a time. The goal of the later set of experiments is to gain insights about feature predictive effectiveness. The majority class prediction experiment is simplest baseline to which we can can compare the rest of the experiments. In order to illustrate the prediction power of each feature group independent from all others, we perform the “Single Feature Group”, experiments. there are groups of features that independently are not better than the majority baseline, for example, the emoticons, politeness cues and polarity are not better disclosure predictors than the majority base. Also, we observe that only n-grams and GloVe features are the only group of features that contribute to more than a class type for the different tasks. Now, the “All Features” experiment shows how the interaction between feature sets perform than any of the other features groups in isolation. The accuracy metric for each trolling task is meant to provide an overall performance for all the classes within a particular task, and allow comparison between different experiments. In particular, we observe that GloVe vectors are the most powerful feature set, accuracy-wise, even better than the experiments with all features for all tasks except interpretation. This result is what makes this dataset interesting: there is still lots of room for research on this task. Again, the primary goal of this experiment is to help identify the difficult-to-classify instances for analysis in the next section.
To further analyze the performance of our best methods (context/augmented ws/hs + cng), we also examine the results on the racism and sexism classes individually As before, we see that our approach consistently improves over the baselines, and the improvements are statistically significant under paired t-tests.
To further analyze the performance of our best methods (context/augmented ws/hs + cng), we also examine the results on the racism and sexism classes individually As before, we see that our approach consistently improves over the baselines, and the improvements are statistically significant under paired t-tests.
Following previous work Pavlopoulos et al. Specifically, from w-tox, 95,692 comments (10.0% abusive) are used for training and 63,994 (9.1% abusive) for testing; from w-att, 70,000 (11.8% abusive) are used for training and 45,854 (11.7% abusive) for testing. We do not report scores from the char hs and char ws methods since they showed poor preliminary results compared to the hs and ws baselines.
Ablation results of applying coverage mechanism and soft loss function. Test set 1 and test set 2 One thing that should be noted is that since the data in Test set 2 (extracted from text book) have much better quality than Test set 1, the performance on Test set 2 is much higher than it is on Test set 1, which is consistent with our instinct.
Effect of Lattice-Based Encoders To show the effectiveness of our method, we placed all edges in the lattice of a single sequence in a relative right order based on their first character, then we applied normal positional encodings (PE) to the lattice inputs on our base Transformer model.
We can see that the best results are achieved when experimenting with “Credibility + Semantic” and “Credibility + Linguistic + Semantic” feature combinations, and the results are worse when only using credibility and linguistic features.
Learning k-way embeddings from k-way co-occurrences for a single k value results in poor performance because of data sparseness. To overcome this issue we use all co-occurrences equal or below a given k value when computing k-way embeddings for a given k. Training is done in an iterative manner where we randomly initialise word embeddings when training k=2-way embeddings, and subsequently use (k−1)-way embeddings as the initial values for training k-way embeddings. In some of the larger datasets, the performances reported by k≤3 (for MEN, DV, and CR) and k≤5 way embeddings (for WS and SUBJ) are significantly better than that by the 2-way embeddings. This result supports our claim that k(>2)-way co-occurrences should be used in addition to 2-way co-occurrences when learning word embeddings.
We can see that the proposed pseudo future context modeling approach method is very important in both the default and the deeper configuration of our transformer-based models, demonstrating its effectiveness on LST tasks. We also find that the encoder-decoder sharing mechanism is more effective in deeper models. This suggests that sharing the parameters between the encoder and the decoder may provide strong regularization effects and make it easier to train deeper transformer models.
We can see that the proposed method is able to significantly outperform the vanilla transformer-based models, as well as the LSTM and sequence labeling based LST baselines in both settings where either the number of parameters in the model is the same or the inference latency is the same, which is consistent with the result in the GEC task. Our deeper model variant yields the state-of-the-art results in both tasks. This demonstrates the effectiveness of the proposed approach and suggests that our approach is versatile for different LST tasks.
GLEU+ has slightly stronger correlation with the human ranking than GLEU0, which is significantly greater than the human correlation with M2, however the rank correlation of GLEU+ is weaker than GLEU0 and M2.
We implement EGL by marginalizing over the most likely 100 labels, and compare it with: 1) a random selection baseline, 2) entropy, and 3) pCTC. Using the same base model, each method queries a variable percentage of the unlabeled dataset. The queries are then included into training set, and the model continues training until convergence. Fig. All the active learning methods outperform the random baseline. Moreover, EGL shows a steeper, more rapid reduction in error than all other approaches. Specifically, when querying 20% of the unlabeled dataset, EGL has 11.58% lower CER and 11.09% lower WER relative to random. The performance of EGL at querying 20% is on par with random at 40%, suggesting that using EGL can lead to an approximate 50% decrease in data labeling.
To perform the experiments, we created two different data sets using standard corpora. The first data set was created from the Penn Discourse Treebank (PDTB)
Fig. We found that most multilingual editors contribute to two or three language editions. There are also a small number of editors with edits in more than 10 languages, including two who edited all 46 languages we studied. We regard all editors with more than 10 editions as either outliers or Wikipedia bots not setting the ‘bot’ flag for their edits and drop them from further study.
We can see that our model (G-SAT) outperforms both GLAD and GCE on the three languages of the WOZ2.0 dataset when no pre-trained resources are available, and that the model performance is consistent across both the development and the test data.
Even in this case the G-SAT model is very competitive on the three languages compared to both GLAD and GCE models. In addition, since predicting a requestable slot is a much easier task than predicting an informable slot, we note that all three models show very high performance.
For classification and question answering, we explore how varying the input representations affects final performance. The Effects of MT Training Data. We experimented with different training datasets for the MT-LSTMs to see how varying the MT training data affects the benefits of using CoVe in downstream tasks. There appears to be a positive correlation between the larger MT datasets, which contain more complex, varied language, and the improvement that using CoVe brings to downstream tasks. This is evidence for our hypothesis that MT data has potential as a large resource for transfer learning in NLP.
We did not submit the SQuAD model for testing, but the addition of CoVe was enough to push the validation performance of the original DCN, which already used character n-gram embeddings, above the validation performance of the published version of the R-NET.
Comparison to Skip-Thought Vectors. Kiros et al. Both skip-thought and CoVe pretrain encoders to capture information at a higher level than words. However, skip-thought encoders are trained with an unsupervised method that relies on the final output of the encoder. MT-LSTMs are trained with a supervised method that instead relies on intermediate outputs associated with each input word. Additionally, the 4800 dimensional skip-thought vectors make training more unstable than using the 600 dimensional CoVe.
TACRED is approximately 10x the size of SemEval 2010 Task 8, but contains a much higher fraction of negative training examples, making classification more challenging.
We expand the original dataset to fit our single-error scenario with the following two schemes. (1) each edit: all corrections except one are applied, creating a single-error sentence; (2) last edit: all corrections except for the last one are applied. After the split, we obtain 3,585 and 1,024 corrections respectively. We also employ different strategies for deciding the number of masked tokens: (1) based on span length of the original edit, (2) based on length of the final correction (given from an oracle), and (3) using a single mask and measure whether any token of the correction is predicted. Note that subword predictions like {ad, ##e, ##quate} to adequate are merged in sentence-level, but remains in mask-level evaluation. We find that different masking strategies have comparable performance, with slightly higher accuracy when using a single mask. Interestingly, BERT seems capable to actually produce corrections with quite high precision of more than 70%.
To evaluate models, we investigated and compared different scenarios of tongue contour extraction as described in the previous section. In each situation, we first trained the whole DeconvNet and U-net on the source domains (named base models) and directly apply them on two source and target domains to see the weakness of each model in terms of generalization from one domain to another. Results of the table reveal that on average fine-tuning the whole decoder section is the best for achieving the best accuracy in target domain while the negative transferring can be seen clearly in these cases. For instance, in the scenario I, in case of U-net base model, it achieved a Dice coefficient of 0.6884 for the source domain and 0.4664 for the target domain. At the same time, when the whole decoder fine-tuned a better Dice coefficient of 0.6306 was achieved in the target domain and 0.5818 in the source domain. As it can be seen, by freezing more layer in the decoder section (conv7, conv8, and conv9) the difference between the Dice coefficient values in source and target domains significantly increases. For the case of DeconvNet, this is not true and the difference decrease in higher layers.
We first discuss the results on Newsela that contains high-quality simplifications composed by professional editors. In terms of BLEU, all neural models achieved much higher scores than Pbmt-R and Hybrid. NseLstm-B scored highest with a BLEU score of 26.31. With regard to SARI, NseLstm-S scored best among neural models (29.58) and came close to the performance of Hybrid (30.00). This indicates that NSE offers an effective means to better encode complex sentences for sentence simplification.
Of the 339 interviews, 141 were transcribed, of which were 76 psychotic, 6 depressive and 59 healthy participants. Transcripts were transformed from the CHAT format to flat text. Since we are dealing with privacy-sensitive information we took measures to mitigate any risk of leaking sensitive info. For audio we only perform analysis on parameters that were derived from the raw audio, not including any content. For the transcripts we swapped all transcripts with their tokenized versions and only performed calculations on these. The acquired datasets were split into 80% training set, 10 % validation and 10 % test set keeping the ratios among participants of the original dataset.
Line 2 shows the pronunciation of the templates, line 3 shows the translations, and the uppercase letters in line 4 are provided for notation. We can see that N1 texts have higher portion of N1 and N2 templates than N2 texts, implying that the difficulty boosts from N2 to N1 are derived from increasing usage of advanced grammar. It is also clear that even in the texts of advanced levels, the majority of the sentences are organized by elementary grammatical templates, and the advanced ones are only used occasionally for formality or preciseness. When comparing any two adjacent levels (e.g. N2 and N3), the templates at those levels or above seem to be the most significant. For instance, N1/N2 texts differ in numbers of N1 and N2 templates while they have similar numbers of N3-N5 templates, and the numbers of N1, N2 and N3 templates differentiate the N2/N3 texts while the numbers of N4 and N5 templates seem relatively similar. This phenomenon inspires us to build a simple and effective approach to differentiate the texts of two adjacent levels.
BIONLP13CG is the model in SciSpacy that covers the most entity types (18 entity types). BC5CDR is another model in SciSpacy that has the best performance on two entity types (chemicals and diseases). We manually annotated more than 1000 sentences for evaluation. Then we calculate the precision, recall and F1 scores on three major biomedical entity types: gene, chemical and disease. We can see that our annotation has worse performance on the gene type but much better performance on the chemical and disease types. In summary, the quality of our annotation surpasses SciSpacy by a large margin (over 10% higher on the F1 score). Moreover, SciSpacy requires human effort for training data annotation and covers only 18 types. Our NER system supports incrementally adding new documents as well as adding new entity types when needed by adding dozens of seeds as the input examples.
Here, we assess the performance of our model when applied to Bulgarian multiple-choice reading comprehension. Each line denotes the type of the model, and the addition (+) or the removal (–) of a characteristic from the setup in the previous line. The first line shows the performance of a baseline model that chooses an option uniformly at random from the list of candidate answers for the target question. The following rows show the results for experiments conducted with a model trained for three epochs on RACE
We present the results for some interesting experiments rather then for a full grid search. The first row shows a random baseline for each category. In the following rows, we compare different types of indexing: first, we show the results for a small sliding window (400-character window, and 100-character stride), followed by a big window (1,600-character window, and 400-character stride), and finally for paragraph indexing. The last group in the table (Paragraph) shows the best-performing model, where we mark in bold the highest accuracy for each category. For completeness, we also show the accuracy when using the Slavic BERT model for prediction, which yields a 10% drop on average compared to using the Multilingual BERT, for each of the categories.
We also evaluate the proposed SCN-LSTM model by uploading results to the online COCO test server. We include the models that have been published and perform at top-3 in the table. Compared to these methods, our proposed SCN-LSTM model achieves the best performance across all the evaluation metrics on both c5 and c40 testing sets.
The SCN-LSTM achieves significantly better results over all competing methods in all metrics, especially in CIDEr-D. For self-comparison, it is also worth noting that our model improves over LSTM-CRT2 by a substantial margin. Again, using an overaching ensemble further enhances performance.
For the other three datasets (Headlines (HDL), Postediting and Question-Question) the Sent2vec method was better. We observed that, in general, it is difficult to beat the BoW baseline in the Postediting dataset. Sent2Vec surpassed it by a small difference. WISSE was lower, also, by a small difference.
Although we did not participate in such a competition, it is relevant to observe how WISSE performed with respect to state-of-the-art STS systems. Notice that these systems can be designed in varied forms and for different purposes than WISSE That is, while WISSE aims to unsupervisedly represent sentences in vector spaces, STS systems aim to measure semantic textual similarity (un/supervisedly and by using varied external resources). The general score for the competition is a weighted average of the correlations from all datasets (column ALL). It is encouraging that WISSE is ranked at 10th place among 113 systems. This comparison was performed putting the characteristics of all those systems aside. This is an important consideration because most of them are supervised and include several lexical resources like WordNet, FrameNet and dictionaries. The first observation is that the WISSE model (green diamond) outperformed the mean for almost all datasets. For the Headlines dataset, WISSE did not surpass the mean, but it stayed within the inter quartile range (IQR). Neither the Plagiarism nor the Postediting datasets, do not present high difficulties for the state-of-the-art systems. This can be a result of the low variability and the high overall mean correlation reached by most systems for this dataset. In the same vein, the BoW baseline (red circle) overpasses the mean correlation for the Postediting dataset. For these datasets, our system remains within the IQR, but not too far away from the maximum correlation.
We present the perplexity result of Sent140 tweets and SemEval tweets in the supplementary material, for which the same conclusion can be drawn. From the perplexity results, it is clear that modeling the target-opinion pairs directly leads to significant improvement of opinion words perplexity and hence the overall perplexity. Note that LDA-DP only models the opinion words, thus we can only compare the perplexity for opinion words , we can see that its result is comparable to that of ILDA, albeit slightly better.
We can see that TOTM outperforms LDA-DP and ILDA on both datasets, suggesting that our prior formulation is more appropriate than that of LDA-DP. We can also see that LDA-DP gives a better sentiment classification compared to ILDA, which does not incorporate any prior information. Note that the classification result for SemEval data is better than that of Sent140. We conjecture that this is because Sent140’s sentiment labels are obtained from the emoticons, which are noisy in nature; while the sentiment labels for SemEval data is annotated.
Here, we evaluate ϕ−1 with negative affinity Z− and ϕ1 with positive affinity Z+. We compare the sentiment scores between the cases when a sentiment lexicon is used and when it is not. Additionally, we also make use of the MPQA Subjectivity lexicon for sentiment prior (during training) and compare the sentiment evaluation against the SentiStrength lexicon. As we can see, it is clear that incorporating prior information results in huge improvement in the sentiment score. Also, the priors for SentiStrength are slightly better than MPQA on average. We note that optimizing the hyperparameter b is very important, as it relieves us from tuning the hyperparameter manually. To illustrate, the optimized b converges to 2.59 on the electronic product tweets, while on Sent140 and SemEval dataset, the b converges to 1.85 and 0.71 respectively. We also find that, in our tests, an incorrectly chosen b can lead to a bad result.
In our preliminary experiments, we confirmed the E2E-ST model can outperform the pipeline system by stacking more BLSTM layers on top of the speech encoder to match the number of parameters between them. Moreover, pre-training the speech encoder and translation decoder with the corresponding ASR encoder and NMT decoder also drastically improved the performances (see However, it is worth noting that our goal in this paper is to show the effectiveness of multilingual training for E2E-ST models and therefore we will not seek these directions here.
We highlight some observations. (i) All the topic-enrolled methods outperform the basic-LSTM model, indicating the effectiveness of incorporating global semantic topic information. (ii) Our TCNLM performs the best across all datasets, and the trend keeps improving with the increase of topic numbers. (iii) The improved performance of TCNLM over LCLM implies that encoding the document context into meaningful topics provides a better way to improve the language model compared with using the extra context words directly. (iv) The margin between LDA+LSTM/Topic-RNN and our TCNLM indicates that our model supplies a more efficient way to utilize the topic information through the joint variational learning framework to implicitly train an ensemble model.
To demonstrate the effectiveness of the proposed approach, we compare our method with previous NN-based methods. The methods listed in the table all employ neural network for KB-QA. further improves their work by proposing the concept of subgraph embeddings. Besides the answer path, the subgraph contains all the entities and relations connected to the answer entity. The final vector is also obtained by BOW strategy. They jointly consider the two mapping process. ours represents the proposed approach.
With dynamic evaluation, our method can achieve a test perplexity of 28.0, which is, to the authors’ knowledge, better than all existing CNN- or RNN-based models with similar numbers of model parameters.
There are 200 sentences used in the training stage, with 100 belonging to the positive sentiment class and 100 to the negative class, and 50 samples being used in the evaluation stage, with 25 negative and 25 positive. This totals in 300 samples, with incorrect and correct sentences combined. Since our goal is to evaluate the model’s performance and robustness in the presence of noise, we only consider incorrect data in the testing phase. Note that BERT is a pre-trained model, meaning that small amounts of data are enough for appropriate fine-tuning.
Experimental results for the Twitter Sentiment Classification task on Kaggle’s scores, outperforming the baseline models by 6% to 8%. We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80% against BERT’s 72%. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy. In that version, Stacked DeBERT achieves 82% accuracy against BERT’s 76%, an improvement of 6%. In the last case (Inc+Corr), we consider both incorrect and correct tweets as input to the models in hopes of improving performance. However, the accuracy was similar to the first aforementioned version, 80% for our model and 74% for the second highest performing model. Since the first and last corpus gave similar performances with our model, we conclude that the Twitter dataset does not require complete sentences to be given as training input, in addition to the original naturally incorrect tweets, in order to better model the noisy sentences. In addition to the overall F1-score, we also present a confusion matrix, in Fig. The normalized confusion matrix plots the predicted labels versus the target/target labels. It can be seen that our model is able to improve the overall performance by improving the accuracy of the lower performing classes. In the Inc dataset, the true class 1 in BERT performs with approximately 50%. However, Stacked DeBERT is able to improve that to 72%, although to a cost of a small decrease in performance of class 0. A similar situation happens in the remaining two datasets, with improved accuracy in class 0 from 64% to 84% and 60% to 76% respectively.
When presented with data containing STT error, our model outperforms all baseline models in both combinations of TTS-STT: gtts-witai outperforms the second placing baseline model by 0.94% with F1-score of 97.17%, and macsay-witai outperforms the next highest achieving model by 1.89% with F1-score of 96.23%. Whereas in Stacked DeBERT, that difference is 1.89 and 0.94 respectively. This is stronger indication of our model’s robustness in the presence of noise.
The poor performance of the supervised methods suggests they are overfitting the small training set, however this is much less of a problem for our approach (labelled Joint). Note that distant supervision alone gives reasonable performance (labelled Distant) however the joint modelling of the ground truth and distant data yields significant improvements in almost all cases. The accuracies are higher overall for the European cf. Turkic languages, presumably because these languages are closer to English, have higher quality dictionaries and in most cases are morphologically simpler. Finally, note the difference between CCA and Cluster methods for learning word embeddings which arise from the differing quality of distant supervision between the languages.
Basic colors are more often created by borrowing, suffixing, and compounding than secondary colors; nevertheless, this should be taken with a grain of salt: The annotations for secondary colors are less complete, so while we use these scores, we do not take the criterion of borrowings as a prong of our criticism.
In almost all genres, DenseNMT models are significantly better than the baselines. With embedding size 256, where all models achieve their best scores, DenseNMT outperforms baselines by 0.7-1.0 BLEU on De-En, 0.5-1.3 BLEU on Tr-En, 0.8-1.5 BLEU on Tr-En-morph. We observe significant gain using other embedding sizes as well. For example, on Tr-En model, the 8-layer DenseNMT-8L-2 model with embedding size 64 matches the BLEU score of the 8-layer BASE model with embedding size 256, while the number of parameter of the former one is only 40% of the later one. In all genres, DenseNMT model with embedding size 128 is comparable or even better than the baseline model with embedding size 256.
In order to make the comparison fair, six models listed have roughly the same number of parameters. On De-En, Tr-En and Tr-En-morph, we see improvement by making the encoder dense, making the decoder dense, and making the attention dense. Fully dense-connected model DenseNMT-4L-1 further improves the translation accuracy. By allowing more flexibility in dense attention, DenseNMT-4L-2 provides the highest BLEU scores for all three experiments.
From the results, we can make the following observations. (1) Our NetAb model achieves the best ACC and F1 on all datasets except for F1 of negative class on Laptop. The results demonstrate the superiority of NetAb. (2) NetAb outperforms the baselines designed for learning with noisy labels. These baselines are inferior to ours as they were tailored for image classification. Note that we found no existing method to deal with noisy labels for SSC.
Note that as to some reference algorithms, we also have an adaptation that use the sum of word embeddings as the entity vector to solve the OOV problem. From the table, we clearly observe that our model outperforms these reference algorithms. More specifically, our attention-based Bi-GRU-CapsNet model achieves the best F1-scores both on English and Chinese corpora compared with all reference algorithms. While for the performance in terms of recall and precision, our model is also highly competitive.
An example of n-grams discovered with this analysis is shown in Tab. From this, we can then explore the references and outputs of each system, and figure out what phenomena resulted in these differences in n-gram accuracy. For example, further analysis showed that the relatively high accuracy of “hemisphere” for the NMT system was due to the propensity of the PBMT system to output the mis-spelling “hemispher,” which it picked up from a mistaken alignment. This may indicate the necessity to improve alignments for word stems, a problem that could not have easily been discovered from the bucketed analysis in the previous section.
Therefore, the best-fit model contained an interaction between domain and input ratio, an interaction between cognitive load and input ratio, but only an additive relationship between domain and cognitive load (loglikelihood =−278.71). The effect of input ratio on entropy change is due to different amounts of regularization being possible under each input ratio ( the maximum drop in entropy achievable under the 5:5 through 0:10 ratios are 1, 0.97, 0.88, 0.72, 0.47, and 0 bits, respectively). As input entropy increases from 0 to 1 bits, output entropy changes by −0.14 bits. This means that participants regularize more when the entropy of the input ratio increases from 0 bits (the 10:0 ratio) to 1 bit (the 5:5 ratio). The interactions mean that the effect of input entropy on output entropy is greater by −0.1 bits when linguistic stimuli are used and greater by −0.5 bits when cognitive load is high. The additive relationship suggests that domain and cognitive load are independent drivers of regularization behavior.
Only one condition, marbles1, elicited a significant difference (of 0.05 bits) between the input ratios and estimates (S.E.=0.02,t(1152)=2.29,p=0.02). In this condition, participants estimated the generating ratio to be significantly more variable than the ratio they had observed, indicating a slight encoding bias toward variability. None of the conditions show any bias toward regularity in participants’ estimates. The best-fit model contained a significant effect of domain (χ2(4)=11.735,p=0.02), cognitive load (χ2(4)=34.916,p
The first row for each language pair reports the results for our reimplementation of \newcitecotterell2017crossling, and the second for our full model. From these results, we can see that we obtain improvements on the F-measure over the baseline method in most experimental settings except BG with tgt_size=1000.
We observe that the improvements are on average stronger when tgt_size=100. This suggests that our model performs well with very little data due to its flexibility to generate any tag set, including those not observed in the training data. The strongest improvements are observed for FI/HU. This is likely because the number of unique tags is the highest in this language pair and our method scales well with the number of tags due to its ability to make use of correlations between the tags in different tag sets.
We show accuracy for all questions, for each sequence (percentage of the correctly answered sequences of sentence), and for each sentence in particular position of a sequence. We can see that CAMP performers better than existing systems in terms of overall accuracy. Adding table knowledge improves overall accuracy and sequence accuracy. Adding contextual knowledge further improve the sequence accuracy. We can see our model has the ability of effectively replicating previous logical form segment to derive the meaning representation of a context-dependent utterance.
We study the performance of each module of CAMP. The controller module and the column prediction module in SELECT expression achieves higher accuracies. The reason is that, compared to other modules, the supervise signals for these two modules are less influenced by spurious logical forms.
The results with BilBOWA are very similar to BiCVM, so we do not report it for brevity.
For this purpose, we collected movie scripts from our test set. Despite having similar micro-f1 scores, tag recall and tags learned are lower when we use the scripts. A possible explanation for this is the train/test mismatch since the model was trained using summarized versions of the movie, while the test data contained full movies scripts. Additional sources of error could come from the external info included in scripts (such as descriptions of actions from the characters or settings).
We test our planning models against a baseline on the WMT’15 tasks for English to German (En→De), English to Czech (En→Cs), and English to Finnish (En→Fi) language pairs. It does this with fewer updates and fewer parameters. We trained (r)PAG for 350K updates on the training set, while the baseline was trained for 680K updates. We used 600 units in (r)PAG’s encoder and decoder, while the baseline used 512 in the encoder and 1024 units in the decoder. In total our model has about 4M fewer parameters than the baseline. We tested all models with a beam size of 15. However, according to our results on En→De, layer norm affects the performance of rPAG only marginally. Thus, we decided not to train rPAG with layer norm on other language pairs.
It is noted that AraDIC outperforms both word based and character based deep learning and classical baselines. Performance improvement is shown over classical SVM without the need for preprocessing, word segmentation, stemming and feature engineering associated with classical methods. This makes character level representations a better choice for Arabic language avoiding segmentation and feature engineering problems. It’s also shown that using AraDIC’s image-based character embeddings outperforms CLCNN with one-hot encoded characters as input features. Therefore, we can conclude as well that image-based character embeddings are useful for Arabic language due to the property of the language as discussed in the introduction section of this paper.
The data for training and testing are sourced from the SemEval 2016: Task 5 Pontiki et al. These datasets provide customer reviews in 8 languages labelled with Aspect-Based Sentiment, i.e., opinions about specific entities or attributes rather than generic stances. The languages include Arabic (hotels domain), Chinese (electronics), Dutch (restaurants and electronics), English (restaurants and electronics), French, Russian, Spanish, and Turkish (restaurants all). We mapped the labels to an overall polarity class (positive or negative) by selecting the majority class among the aspect-based sentiment classes for a given sentence. Note that no general sentiment for the sentence was included in this pool. Moreover, we added data for Italian (tweets) from the sentipolc shared task in evalita 2016 Barbieri et al. We discarded neutral stances from the corpus, and retained only positive and negative ones. the word not and the suffix n’t in English) based on their polarity class. Moreover, the second-best method for languages is always FastSent, which is the only one hinging upon neighbouring sentences as features. This demonstrate that sentiment is encoded not only within a sentence, but also in its textual context. As a consequence, a relatively small and accessible dataset (Wikipedia) is sufficient to provide a reliable model in most languages.
(where 0 represents the negative class, 1 represents the neutral class, and 2 represents the positive class). We can see in this report that the model had a relatively weaker predictive performance for the negative and neutral sentiments.
It can be found that QAInfomax yields substantial improvement over the vanilla BERT model, and achieves the state-of-the-art performance on both AddSent and AddOneSent metrics. QAInfomax obtains larger improvement on the AddSent, which picks the worst scores of the model. It shows the effectiveness of our QAInfomax in terms of forcing the model to ignore simple correlation in the data and learn the more human-like reasoning processes. It is worth to note that while QAInfomax mitigates the overstability problem and improves the robustness to adversarial examples, it does not hurt the original performance of the QA system, demonstrating the benefit for the practical usage. Some example results from the Adversarial-SQuAD dataset can be found in the Appendix, where adversarial distracting sentences are shown in italic blue fonts.
Mean : σ(1M∑rai) Max: σ(maxpool(ra)) Sample: randomly sample one rai∈ra According to the experimental results , Mean performs the best while Max and Sample has the competitive performance, showing the great robustness of the proposed methods to different architecture choices.
These references include all identifiers extracted from the court decisions contained in the CzCDC 1.0. Therefore, this number includes all other court decisions, including lower courts, the Court of Justice of the European Union, the European Court of Human Rights, decisions of other public authorities etc. Therefore, it was necessary to classify these into references referring to decisions of the Supreme Court, Supreme Administrative Court, Constitutional Court and others. These groups then underwent a standardisation - or more precisely a resolution - of different court identifiers used by the Czech courts.
Our system was evaluated using a blind test set which contained 19,998 claims. It also lists the best performance for each metric. The evidence precision of our system was 0.5191 and its evidence recall was 0.3636. All of these results were obtained upon submitting our predictions to an online evaluator. DeFactoNLP had the 5th best evidence F1 score, the 11th best label accuracy and the 12th best FEVER score out of the 24 participating systems.
We can see that on most metrics, our method performs better than baselines, and the improvement is statistically significant (t-test with p-value ≤0.01).
Compared with the original models, our models with multi-scale semantic units of the input sentences as network inputs significantly improved the performance on most datasets. Furthermore, the improvements on different tasks and datasets also proved the general applicability of our proposed architecture. Compared with MaLSTM, our multi-scaled Siamese models with factorized sentences as input perform much better on each dataset. For MSRvid and STSbenmark dataset, both Pearson’s r and Spearman’s ρ increase about 10% with Multi-scale MaLSTM. For the paraphrase identification task, our model shows better accuracy and F1 score on MSRP dataset. For the semantic textual similarity estimation task, the performance varies across datasets. On the SICK dataset, the performance of Multi-scale HCTI is close to HCTI with slightly better Pearson’ r and Spearman’s ρ. However, the Multi-scale HCTI is not able to outperform HCTI on MSRvid and STSbenchmark. HCTI is still the best neural network model on the STSbenchmark dataset, and the MSRvid dataset is a subset of STSbenchmark. Although HCTI has strong performance on these two datasets, it performs worse than our model on other datasets. Overall, the experimental results demonstrated the general applicability of our proposed model architecture, which performs well on various semantic matching tasks.
First, we train a generic model on the whole corpus with 5-fold cross-validation. In this case, the model is trained on different topics and dialects. This indicates the importance of these features for sentiment analysis, and relates back to our manual analysis in which we found sentiment variations across topics and dialects. This is a typical problem seen when developing cross-domain sentiment models instead of training topic-specific models, which is an expensive solution. Our corpus allows the development of models for domain adaptation given the availability of topic annotation. Results also confirm the importance of the sentiment expression feature, which alone helped improving the performance by more than 10% absolute. It can be observed that results on the personal domain are much lower than those in the politics domain, which can be attributed to the wider range of sub-topics that can be covered by the personal domain.
Similar with the single speaker result, we improved both the ASR and TTS models by additional training on unpaired datasets. However, in this experiment, we found that a different ASR performance α=0.5 produced a larger improvement than α=0.25. We hypothesize that because the baseline model is not as good as the previous single speaker experiment, we should put a larger coefficient on the loss and the gradient provided by the paired training set.
The first two models that we considered are a fixed crowdsourcing round with the same amount of workers for every tweet. If we increase the number of crowd workers by 2 we require 5,000 tasks and we would get a 0.653 reliability measure. These results align with previous observations that the task of sentiment analysis is challenging even for human annotators Despite the significantly higher costs of requesting 2,000 additional labels from crowd workers, a 40% increase, the average agreement between the majority of crowd contributions and expert labels improved by only 6.3 percent (or, equivalently, by a difference of Kappa values of 4.1 percent points).
Next, we investigated the distribution of sentence lengths within each book, for each measure of sentence length considered here. This test exploits the distance κ between two empirical cumulated distributions C1 and C2: κ =supx|C1(x)−C2(x)|, (3) where sup is the supremum function; the smaller the κ the greater the similarity between the distributions. Typically, the number of characters is greater than the number of words (Figs. Thus, to make a fair comparison, the all time series were normalized by their mean values prior to the KS test. The former shows the comparison between the cumulated distributions for Nw and Nc after normalization and the latter the Kolmogorov–Smirnov distance (κ) for all comparisons. Note that the distances found are small (κ≤0.12), implying in the fact that these distributions can be drawn from the same distribution. We verified that 41.77% of these comparisons accepted the null hypothesis that the distributions are drawn from the same distribution, with a p–value ≥0.01.
For both models, we use beam decoding with a beam size of eight. Beam candidates are ranked according to their length normalized log-likelihood. On these automatic measures we see that StyleEQ is better able to reconstruct the original sentences. In some sense this evaluation is mostly a sanity check, as the feature controls contain more locally specific information than the genre embeddings, which say very little about how many specific function words one should expect to see in the output.
We start the training of the \acgmmhmm model from scratch using linear alignments. Afterwards we utilize non-linear alignments. To further improve the \acgmmhmm model we introduce triphones. Adding \acvtln on top of the triphone system only shows improvements on clean but degradation on other. However adding \acsat to the triphone system improves the \acwer. Combining \acvtln and \acsat gives mixed \acwer: clean improves, other degrades. Introducing an hybrid \acdnn/\achmm improves the system \acwer results. Continuing with sequence discriminative training improves the performance even further.
9k \accart labels show the worst performance. In contrast, 20k \accart labels shows improved performance. But the best performance was shown by 12k \accart labels.
Activation: There is a significant increase in performance in all setups when the adversarial classification model is tested on IEMOCAP. We observe a significant increase in performance in acoustic setup of adversarial classification model (0.404 vs 0.421) when tested on MSP-Improv. Valence: There is a significant increase in performance using acoustic setup (0.376 vs 0.401) and multimodal setup (0.431 vs 0.472) of adversarial model when tested on IEMOCAP. We see no significant difference in performance when testing on MSP-Improv using adversarial classification model . Based on these results, we understand that removal of a psychological confounding factor, stress, generally aids in the generalizability of the model on completely unseen data, where the distribution of this confounding factor is unknown. There are fewer significant categories for valence than for activation. Although we see a significant correlation between filler words and APS for activation classification The absence of significance in these cases implies that though these values are markers of stress, the normal classifier is still able to learn reliable representations invariant of stress for predicting the correct target label, resulting in negligible impact on classification performance when decorrelating the representations.
When Tmin=4, the model reaches the best CoTK’s perplexity. However, the original perplexity will get smaller as |F| decreases, and it will reach 1 when |F|=1. It shows that the original perplexity is not a fair metric under different vocabularies.
Since our models generate multiple variants of the input sentence, one can compute multiple metrics with respect to each of the variants. In our tables, we report average and best of these metrics. For average, we compute the metric between each of the generated variants and the ground truth, and then take the average. For computing the best variant, while one can use the same strategy, that is, compute the metric between each of the generated variants and the ground truth, and instead of taking average find the best value but that would be unfair. Note that in this case, we would be using the ground truth to compute the best which is not available at test time. Since we cannot use the ground truth to find the best value, we instead use the metric between the input sentence and the variant to get the best variant, and then report the metric between the best variant and the ground truth. Those numbers are reported in the Measure column with row best-BLEU/best-METEOR. As we can see, both variations of our model perform significantly better than unsupervised VAE and VAE-S, which is not surprising. We also report the results on different training sizes, and as expected, as we increase the training data size, results improve. Comparing the results across different variants of supervised model, VAE-SVG-eq performs the best. This is primarily due to the fact that in VAE-SVG-eq, the parameters of the input question encoder are shared by the encoding side and the decoding side. We also experimented with generating paraphrases through beam-search, and, unlike MSCOCO, it turns out that beam search improves the results significantly. This is primarily because beam-search is able to filter out the paraphrases which had only few common terms with the input question. When comparing the best variant of our model with unsupervised model (VAE), we are able to get more than 27% absolute point (more than 3 times) boost in BLEU score, and more than 19% absolute point (more than 2 times) boost in METEOR; and when comparing with VAE-S, we are able to get a boost of almost 19% absolute points in BLEU (2 times) and more than 10% absolute points in METEOR (1.5 times).
From the Table, we see that our method produces results which are close to the ground truth for both metrics Readability and Relevance. Note that Relevance of the MSCOCO dataset is 3.38 which is far from a perfect score of 5 because unlike Quora, MSCOCO dataset is an image caption dataset, and therefore allows for a larger variation in the human annotations.
IRLC achieves the highest overall accuracy and (with SoftCount) the lowest overall RMSE on the test set Interestingly, SoftCount clearly lags in accuracy but is competitive in RMSE, arguing that accuracy and RMSE are not redundant. We observe this to result from the fact that IRLC is less prone to small errors and very slightly more prone to large errors (which disproportionately impact RMSE). However, whereas UpDown improves in accuracy at the cost of RMSE, IRLC is substantially more accurate without sacrificing overall RMSE.
Generally, we can see that the thematic information improves the performance of the proposed features clearly (RQ1), and with the largest amount in the Emotions features (see −themes and +themes columns). This result emphasizes the importance of the thematic information. Also, we see that the emotions performance increases with the largest amount considering F1macro value; this motivates us to analyze the emotions in IRA tweets (see the following section). The result of the NLI feature in the table is interesting; we are able to detect IRA trolls from their writing style with a F1macro value of 0.91.
Char TFIDF taxonomies significantly outperform MultiWiBi taxonomies, achieving higher average CPP lengths (ACPP) as well as higher average ratio of CPP to path lengths (ARCPP). Therefore, compared to the state-of-the-art MultiWiBi taxonomies, Char TFIDF taxonomies are a significantly better source of generalization paths for both entities and categories across multiple languages.
Taking into account all the judgments submitted for test questions, the majority voting tag had an accuracy of 0.97-0.98 depending on the task. These estimations are not expected to match the true accuracy we would get from the two judgments we obtained for the rest of non-test tokens, so we re-estimate the accuracy of the majority vote tag for every subset of one, two, three and four judgments collected, adding the initial Bangor tag. In this case we get an average accuracy ranging from 0.89-0.92 with just one token to 0.95-0.96 when using four tags. The best accuracy estimates for our POS tags are for the option of two crowdsourced tags and the Bangor tag, for which we obtained accuracies of 0.92 to 0.94. When looking at non-aggregated tags, the average accuracy per token of single judgments (SJ) were observed to be between 0.87 and 0.88. Measuring the agreement between single judgments and the majority vote (MV) per token, the average agreement value is between 0.87 and 0.89.
Subsequently we use these thread embeddings to train L2-regularized logistic regression classifiers for the action labels. We compare them against classifiers trained with features extracted from the baselines Tied, Disjoint, MaLOPa, and Fenda. Given the small size of annotated data, we decide to evaluate the models with nested cross validation (CV). In the inner layer, we use 7-fold CV on the (train+dev) split to find the best hyperparameters. The best hyperparameters are then used to train a classifier, which is subsequently evaluated on the test split of the outer layer CV. Disjoint performs poorly on this task since there is no baked-in constraint for it to learn a shared representation. All shared-representation baselines (Tied, Fenda, MaLOPa) performed better than both Disjoint and Doc2Vec. Still, our reparametrized models compare favorably against the feature-reparametrizing baselines.
It seems that while both Reddit (E+R) and the IRC (E+I) datasets do better than email only (E), the IRC dataset is much more helpful than Reddit. We note that all the F1 scores are low. Nonetheless we find it encouraging that out-of-domain data is able to help learn a better representation in this extremely resource-scarce setting.
Once we have obtained the probability score for each instance using LSTM+Norm, we can extract LocatedNear relation using the scoring function f. We compare the performance of 5 different heuristic choices of f, by quantitative results. Accumulative scores (f1 and f3) generally do better. Thus, we choose f =f3 with a MAP score of 0.59 as the scoring function.
We randomly selected 1,000 relations for each of these languages and annotated them. The percentage of valid extractions is highest in French (81.6%) followed by Hindi and Russian (64.0%). Surprisingly, Russian obtains the lowest percentage of valid relations but has the highest BLEU score between the automatic and the human extracted relations. This could be attributed to the fact that the average relation length (in number of words) is the shortest for Russian.
We tune hyperparameters of the model to minimize perplexity on the validation set. We choose perplexity because it requires only plain text and not annotated parse trees. Specifically, we tuned the architecture of f(i),i=1,2,3 in the space of multilayer perceptrons, with the dimension of each layer being n+d, with residual connections and different non-linear activation functions. Due to memory constraints on a single graphic card, we set the number of non-terminals and preterminals to 10 and 20, respectively. Later we will show that the compound PCFG’s performance is benefited by a larger grammar, it is therefore possible the same is true for our neural L-PCFG. LABEL: sec: conclusion includes a more detailed discussion of space complexity.
The Quality of Crafted Inputs. For each real target response, TDGPN tries to find an input whose corresponding output response is most similar to the target one. We also feed the real inputs corresponding to the target responses in the corpus into the dialogue model. We aim to check how similar the output responses to the target ones in manipulated and original settings. From the table, we make the following observations. First, even inputting the real inputs, the similarity scores between the output responses and the target responses are not high. Second, with the generated inputs from the proposed framework, the similarity scores are significantly improved. Specifically, the word embedding similarity increases by 57.2%, 40.2% and 32.2% for the target responses with length 1-3, 4-6 and 7-10, respectively.
Ja translation with 100,000 target-domain training set. We confirmed all models were trained over enough steps, and the validation loss was not improved over at least five epochs after choosing the best model. We used 4 GPUs (NVIDIA Quadro P6000) for training, and it took about 0.95 sec/update on average.
We must give credit to the connection extractor since it is the only difference between Hierarchical Hybrid Networks and TextSAN (T&C). We infer that the connection extractor extracts the complex connection between title and content effectively while retaining the original feature information in Vtitle and Vcontent. This illustrates the importance of the correlation between title and content in the clickbait detection task.
We can also see that the federated learning models perform better. It means that content still has valuable information for the clickbait detection task. It also proved that Clickbait Federated Learning can effectively utilize non-shared data. We can conclude that PartyA and PartyB can cooperate effectively and get a better clickbait detection model no matter what kind of implementation of feature extractor by Clickbait Federated Learning.
Our main results are summarized in Fig. As shown in Fig. This indicates our CLC module actually helps the base NRE models benefit from properly utilizing the relation constraints, without interference to the base models. However, we find that our approach obtains different levels of improvement on the two datasets. Similar trends are also found for the Coherent version and the ACNN base model. The better performance gain on the Chinese dataset is mainly because its relation definitions are more clear compared to that of the English dataset. For example, in English dataset, there are 8 relations whose object could be any location, such as birthPlace, while only 3 similar relations exist in Chinese dataset. In addition, we investigate the performance improvement when applying our CLC module to different base NRE models. Although both ACNN and APCNN are improved by our CLC module in various datasets, we can still observe that ACNN generally receives more performance improvement compared with the APCNN base model. only fetches 0.5% improvement. And similar trends can be found in the Coherent method and on the Chinese dataset. The more improvement when taking the ACNN as base NRE model is because, compared with ACNN, APCNN itself is designed to take the entity-aware sentence structure information into account, thus can extract more effective features that, to some extent, can implicitly capture part of the arguments’ type and cardinality requirements of a relation, leaving relatively less space for our CLC module to improve.
In addition, we use the frequencies from the British National Corpus bnc to produce one more dataset of words occurring in the BNC over 1000 times to test whether the results can be improved with frequencies obtained from a non-noisy corpus. This BNC dataset is also used to produce multiple datasets based on the length of the word.
The highest accuracy of 58% is achieved by training the model with all of the frequent words in the BNC, and the lowest number of false positives (i.e. words that do exist in English but are not the right correction for the OCR error) is achieved by the model trained with the BNC words that are at least 3 characters long. The No output column shows the number of words the models didn’t output any word for that would have been correct English.
Finally, we compare the results with the recent work in progress, VisualBERT ( Marginally, ours performs better in the top 1 recall. Note, our approach, unlike VisualBERT which requires task-agnostic and task-specific pre-training on COCO captioning Besides, our architecture is also flexible to adapt to different input modalities respectively.
For the BERT-fuse GEC model, we use the code provided by Zhu et al. While the training the GEC model, the model was evaluated on the development set and saved every epoch. If loss did not drop at the end of an epoch, the learning rate was multiplied by 0.7. The training was stopped if the learning rate was less than the minimum learning rate or if the learning epoch reached the maximum epoch number of 30. Training BERT for BERT-fuse mask and GED was based on the code from Wolf et al. The additional training for the BERT-fuse mask was done in the Devlin et al. ’s setting. The model was evaluated on the development set.
A model trained on Transformer without using BERT is denoted as “w/o BERT.” In the top groups of results, it can be seen that using BERT consistently improves the accuracy of our GEC model. Also, BERT-fuse, BERT-fuse mask, and BERT-fuse GED outperformed the BERT-init model in almost all cases. Furthermore, we can see that using BERT considering GEC corpora as BERT-fuse leads to better correction results. And the BERT-fuse GED always gives better results than the BERT-fuse mask. This may be because the BERT-fuse GED is able to explicitly consider grammatical errors. In the second row, the correction results are improved by using BERT as well. Also in this setting, BERT-fuse GED outperformed other models in all cases except for the FCE-test set, thus, achieving state-of-the-art results with a single model on the BEA2019 and CoNLL14 datasets.
We thus proceed to map the emotions an article is found to evoke to the VAD circumplex model. To do so, we exploit the work of Warriner et al. Such scores are in a Likert scale, ranging from 1 (low/negative) to 9 (high/positive). The formula for Valence is provided below; the equations used for Arousal and Dominance are akin.
Evaluation with these six test datasets provided a ranking of similarity among words. The evaluation values are Spearman’s rank correlation coefficients of the ranking of similarity among words. For comparison, we show the results for skip-gram with negative sampling (SGNS)
Let us consider information context of actors that includes all relevant relationships with their interaction history, where Yahoo! search engines fall short of utilizing any specific information, especially micro cluster information, and just therefore we use full text index search in web snippets. In experiment, we use maximum of 500 web snippets for search term ta representing an actor, and we consider words where the TF.IDF value >0.3× highest value of TF.IDF, or maximum number is 30 words. w={network, minister, Malaysia, journal, datuk, department, Allah, international, Ismail, Nazri, computer, prime, ictac, learning, system, software, foxley, said, kebangsaan, performance, dr, university, Eric, use, accuracy, dblp, based, communications, utilization, author} for example is a set of 30 words from web snippets for actor ”Abdullah Mohd Zin”. We test for 143 names, and we obtain 8 (5.59%) actors without a cluster of candidate words, 13 (9.09%) actors with only one cluster, and 122 (85.32%) persons have two or more keywords. In a case of ”Abdullah Mohd Zin” we have {network, international, computer, system, software, use}, {Malaysia, accuracy}, {datuk, Nazri, kebangsaan}, {minister, journal, ictac, dblp, communications, utilization}, {department, learning, said, performance}, {dr, university, based}, {prime, foxley, Eric, author}, {Allah, Ismail} as micro clusters of words. We can arrange the individual keywords according to their proximity to the stable attribute ”academic”, i.e. a set of words in SK ={sciences, faculty, associate, economic, prof, environment, career, journal, network, university, report, relationship, context, …}. Therefore, we can redefine ϕ or φ as query(ta,tx).
We start by evaluating the models using AUC-ROC. Similar conclusions can be reached regarding overall accuracy. However, given the class imbalance, accuracy is not necessarily the best metric to consider. In addition, to better understand the effect to the original transformer architecture, we present the performance of the model with and without the modified loss and special attention sub-layer (see Results suggest both modifications have a positive impact on the performance. Finally, to further evaluate the ranking capabilities of the proposed methods, we use top-1 accuracy. Additional positions in the ranking are not considered because only the top ranked joke is presented to the customer. Results show that the DL based models outperform the other systems, with a relative change in top-1 accuracy of 1.4 for DL-BERT and 0.43 for DL-T, compared with 0.14 for the LR method.
Comparing Tokyo and Poughkeepsie, the number of connections between qubits that can execute CNOT is greatly reduced, but the gate fidelity is improved. Is there any change in the reliability of circuit selection by ESP? IBM released a quantum processor called Poughkeepsie to IBMQ network members in the winter of 2018.
We group them into 7 regional accents and one type of accents in each group was selected for developing a 7-way accent classifier, including Arabic (AR), Brazilian Portuguese (BR), French (FR), German (GE), Hindi (HI), Mandarin (MA) and Russian (RU). Tab. In order to perform phoneme alignment which is necessary to extract features with phonetic information, we also transcribed the audio data of these 7 major accents, which is originally absent in the LDC’s release. Fig. The portion of speech is considered to be silence when either the smoothed short-time energy rate and the smoothed spectral centroids are below certain thresholds. Tab.
Finally, to help us get a deeper insight on the helpfulness of the adversarial training, we perform comparisons on the correlation matrices learnt by DRSS and DRSS-Adv. We first show the result of SNLI→Fict. As we can see, for DRSS, the correlation between Ws (or Wt) and Wsc (or Wtc) is relatively large, while for DRSS-Adv, the correlation is relatively small. For the other subtasks, we find that the learnt matrices of DRSS-Adv are similar to those of DRSS, but we still observe that the intra-domain correlations of DRSS-Adv are generally smaller than those of DRSS. This shows that adding the adversarial loss can encourage the shared feature space to capture more domain-independent features, and further make the shared and domain-specific feature spaces more different. Therefore, the adversarial training can lead our model to better satisfy our assumption on the domain relationships, and finally improve the performance.
Comparing to baselines without variational attentions, we find that despite the variational attention is adopted, GVAM does not show advantages over other baselines in Yahoo and PTB datasets. The KL divergence of GVAM is as tiny as that of VAE, especially on Yahoo and PTB when the average sequence length is long. This indicates posterior collapse which fails to learn the sequential dependency in the observation. On the other hand, our DVAM achieves significantly better results on all three datasets, especially with large code book size K. The success verifies the fact that the variational posterior learns to adapt itself to the sequential dependency, which significantly improves the expressiveness for language modelling.
NoEmb refer to deal with concepts rather than concepts vectors, where Leacock refer to the similarity between concepts, whereas, we do not incorporate similarity in NoSim. * and \dagger refer to a statistically significant difference with NoEmb_NoSim and NoEmb_Leacock, respectively, according to Fisher Randomization test with (\alpha<0.05). P@10 is also lower in clef11 and slightly better in clef12.
we compared it with (i) a state-of-the-art monolithic transformer (\textscTandA\textscbase), (ii) smaller, monolithic transformer models with 4-10 layers, and (iii) a sequential ranker (SR) consisting of 5 monolithic transformer models with 4,6,8,10 and 12 layers trained independently. For CT, we report performance of each classifier individually (layers 4 up to 12, which is equivalent to a full transformer model). We test SR and CT with drop ratio 30%, 40%, 50%. Finally, for each model, we report the relative cost per batch compared to a base transformer model with 12 layers. Overall, we observed that our cascade models are competitive with monolithic transformers on both ASNQ and GPD datasets. In particular, when no selection is applied (α=0.0), a 12 layer cascade model performs equal or better to \textscTandA\textscBase: on ASNQ, we improve P@1 by 2.1% (53.2 vs 52.1), and MAP by 1.2% (66.3 vs 65.5); on GDP, we achieve the same P@1 (67.5), and a slightly lower MAP (57.8 vs 58.0). This indicates that, despite the multitasking setup, out method is competitive with the state of the art. A drop rate α>0.0 produces a small degradation in accuracy, at most, while significantly reducing the number of operations per batch (−37%). In particular, when α=0.3, we achieve less than 2% drop in P@1 on GPD, when compared to \textscTandA\textscBase; on ANSQ, we slightly improve over it (52.9 vs 52.1). We observe a more pronounced drop in performance for MAP, this is to be expected, as intermediate classification layers are designed to drop a significant number of candidates. For larger values of α, such as 0.5, we note that we achieve significantly better performance than monolithic transformer of similar computational cost. For example, CT achieves an 11.2% improvement in P@1 over a 6-layers TandA model (62.4 vs 56.1) on GPD; a similar improvement is obtained on ANSQ (+11.0%, 52.4 vs 47.2). Finally, our model is also competitive with respect to a sequential transformer with equivalent drop rates, while being between 1.9 to 2.4 times more efficient. This is because an SR model made of independent TandA models cannot re-use encodings generated by smaller models as CT does.
The table contains the average values for 21 SE websites (including SO) as the output of different evaluations on 4 million questions and more than 8 million answers. Initially, we use the absolute values of textual features with low results (58% precision, Case 1). The second and third Cases both utilise the discretised features, while the third is additionally using the other set of features (i.e. AnswerCount and CreationDate). Cases 2 and 3 constitute our proposed prediction method. Furthermore Case 4 refers to a “traditional” approach that relies in plain linguistics and user-reputation ratings. We can see that while a whole new set of features is added into the dataset, the performance of classification remains lower than Case 3, which is linguistics-based. Case 5 keeps the user ratings in addition to incorporating all features of Case 3. Hence, classification accuracy is the highest compared to all previous classifications, but almost identical to Case 3 which is strictly based on content and discretisation (higher F-Measure 0.77 vs. 0.76, higher AUC 0.88 vs. 0.87). Finally, Case 6 uses all previous features, including the answer ratings. This set of features uses all features but most importantly user-entered scores and manages to outperform all of the previous cases. Case 6 shows that the information contained within answer ratings is independent – to a certain extent – of the information found in previous features. Moreover, we can also see that user rating features such as reputation do not improve our classification, a sign that discretisation is a process that extracts very useful information and delivers very strong results.
Semi-supervised learning We keep a part of the training set as labeled data T randomly and leave the rest as unpaired queries (Q) and logical forms (LF) to validate our method in a semi-supervised setting. The ratio of labeled data is 50%. Pseudo here uses the Q2LF model and Q to generate pseudo-labeled data, as well as LF2Q model and LF. The dual learning method is more efficient to exploit unlabeled data. In general, both unpaired queries and logical forms could boost the performance of semantic parsers with dual learning.
SWAF beats the best standalone system as well as the oracle voting baseline by a large margin. For the voting baseline, we consider an object instance to be the same while voting, if its bounding boxes produced by the systems have IOU greater than 0.5. We found that we get the best voting baseline if we just take the union of all outputs produced by systems. On analyzing the results, we found that the AP of several object classes differed widely across systems and even more so between the deep systems and DPM. Using SWAF, the classifier learns to discriminate systems based on the auxiliary features and is thus able to leverage component systems in the overall performance. We chose these images because the component systems had high variance in performance on these two classes – “ping pong ball (class 127)” and “pineapple (class 126)”. The figure shows bounding boxes obtained by component systems for only these two categories respectively. Although the image is very cluttered, SWAF uses the output bounding boxes as context to make better classification decisions. Based on the outputs obtained from SWAF, we found that our approach does well on localizing objects in images that have multiple instances of the same object, i.e. the image can be considered to be “cluttered”.
We perform weight magnitude pruning on a pre-trained BERT-Base model. We select sparsities from 0% to 90% in increments of 10% and gradually prune BERT to this sparsity over the first 10k steps of training. We continue pre-training on English Wikipedia and BookCorpus for another 90k steps to regain any lost accuracy. We then fine-tune these pruned models on tasks from the General Language Understanding Evaluation (GLUE) benchmark, which is a standard set of 9 tasks that include sentiment analysis, natural language inference, etc. We avoid WNLI, which is known to be problematic. We also avoid tasks with less than 5k training examples because the results tend to be noisy (RTE, MRPC, STS-B). We fine-tune a separate model on each of the remaining 5 GLUE tasks for 3 epochs and try 4 learning rates: [2,3,4,5]×10−5. This deletes the pre-training information associated with the weight but does not prevent the model from fitting downstream datasets by keeping the weight at zero during downstream training. We also fine-tune on downstream tasks until training loss becomes comparable to models with no pruning. We trained most models for 13 epochs rather than 3. Models with 70-90% information deletion required 15 epochs to fit the training data. To test this, we fine-tuned pre-trained BERT-Base on downstream data for 3 epochs. We then pruned at various sparsity levels and continued training for 5 more epochs (7 for 80/90% sparsity), at which point the training losses became comparable to those of models pruned during pre-training. We repeat this for learning rates in [2,3,4,5]×10−5 and show the results with the best development accuracy in Figure
It is clear that the correlation is not as strong for the non-projective algorithms, but it is still large enough to be meaningful and is statistically significant. Interestingly, the correlation does not diminish so severely for the non-projective algorithms as the sentence length increases as is the case for the projective algorithms.
As prescribed by the shared task, the essential evaluation metric is the micro-averaged F1-score. All scores reported in this paper are obtained using models that are trained on the training set and evaluated on the validation set. For the final submission to the shared task competition, the best-scoring setup is used and trained on the training and validation sets combined.
The literary genre is homogeneous, but several topics are present in the corpus. Sentences in general language as well as those too short (N≤3 words) or too long (N≥50 words) were carefully avoided. Finally we got a vocabulary complex and aesthetic where certain literary figures like anaphora or metaphor could be observed.
Moreover, our DeConv-LVM achieves even better results than DeConv-AE and LSTM-LVM, suggesting that the deconvolution-based latent-variable model we propose makes effective use of unsupervised information. Further, we see that the gap tends to be larger when the number of labeled data is smaller, further demonstrating that DeConv-LVM is a promising strategy to extract useful information from unlabeled data.
We further study book success prediction using different number of sentences from different location within a book. We conduct further experiments by training our best model on the first 5K, 10K and the last 1K sentences. We notice that using the first 1K sentences only performs better than using the first 5K and 10K sentences and, more interestingly, the last 1K sentences. This could point out to the conclusion that book openings are a better indicator of success than book endings.
Note that with some simple syntactic transformations we can gain 1-2 points in accuracy. strength: Since MonaLog has a very high precision on Entailment and Contradiction, we can always trust MonaLog if it predicts E or C; when it returns N, we then fall back to BERT. This hybrid model improves the accuracy of BERT by 1% absolute to 85.95% on the corrected SICK. On the uncorrected SICK dataset, the hybrid system performs worse than BERT.
The system is clearly very good at identifying entailments and contradictions, as demonstrated by the high precision values, especially on the corrected SICK set (98.50 precision for E and 95.02 precision for C). The lower recall values are due to MonaLog’s current inability to handle syntactic variation.
We display the correlations for five different attacks (computed using the Foolbox package Rauber et al. As ACD is the first local interpretation technique to compute a hierarchy, there is little prior work available for comparison. As a baseline, we use our agglomeration algorithm with occlusion in place of CD. The resulting correlations are substantially lower, indicating that features detected by ACD are more stable to adversarial attacks than comparable methods. These results provide evidence that ACD’s hierarchy captures fundamental features of an image, and is largely immune to the spurious noise favored by adversarial examples.
First, comparing the standard baseline strategy of “Uniform” and the proposed method of “DAT” we can see that in all 8 settings DAT improves over the uniform baseline. This is a strong indication of both the consistency of the improvements that DAT can provide, and the generality – it works well in two very different settings. Next, we find that DAT outperforms SPCL by a large margin for both of the tasks, especially for multilingual NMT. This is probably because SPCL weighs the data only by their easiness, while ignoring their relevance to the dev set, which is especially important in settings where the data in the training set can have very different properties such as the different languages in multilingual NMT.
First, comparing the standard baseline strategy of “Uniform” and the proposed method of “DAT” we can see that in all 8 settings DAT improves over the uniform baseline. This is a strong indication of both the consistency of the improvements that DAT can provide, and the generality – it works well in two very different settings. Next, we find that DAT outperforms SPCL by a large margin for both of the tasks, especially for multilingual NMT. This is probably because SPCL weighs the data only by their easiness, while ignoring their relevance to the dev set, which is especially important in settings where the data in the training set can have very different properties such as the different languages in multilingual NMT.
Dialog Act Prediction We first train dialog act predictors using different neural networks to compare their performances. Experimental results show that fine-tuning the pre-trained BERT Devlin et al. Therefore, we will use it as the dialog act prediction model in the following experiments. Instead of jointly training the predictor and the response generator, we simply fix the trained predictor when learning the generator Pβ(y).
Before diving into the experiments, we first list all the models we experiment with as follows: [partopsep=0pt, leftmargin=0.5cm] (i) LSTM Budzianowski et al. (ii) Transformer Vaswani et al. With Sparse Tree Dialog Act, we feed the tree-based representation as an external vectors into different architectures. (i) SC-LSTM Wen et al. (ii) Transformer-in: it appends the sparse dialog act vector to input word embedding (iii) Transformer-out : it appends the sparse dialog act vector to the last layer output, before the softmax function. With Compact Graph Dialog Act (Predicted), we use the proposed graph representation for dialog acts and use it to control the natural language generation. (i) Transformer-in/out : it uses the flattened graph representation and feeds it as an external embedding feature. (ii) Straight DSA : it uses the flattened graph representation and model it with a one-layer DSA followed with two layers of self-attention. (iii) 2-layer HDSA : it adopts the partial action/slot levels of hierarchical graph representation, used as an ablation study. (iv) 3-layer HDSA : it adopts the full 3-layered hierarchical graph representation, used for the main model. With Graph Dialog Act (Groundtruth): it uses the ground truth dialog acts as input to see the performance upper bound of the proposed response generator architecture. In order to make these models comparable, we design different hidden dimensions to make their total parameter size comparable. (Transformer-in/out), the model is not able to capture the large semantics space of dialog acts with sparse training instances, which unsurprisingly leads to restricted performance gain against without dialog act input. (ii) the graph dialog act is essential in reducing the sample complexity, the replacement can lead to significant and consistent improvements across different models. (iii) the hierarchical graph structure prior is an efficient inductive bias; the structure-aware HDSA can better model the compositional semantic space of dialog acts to yield a decent gain over Transformer-in/out with flattened input vector. (vi) our approaches yield significant gain (10+%) on the Inform/Request success rate, which reflects that the explicit structured representation of dialog act is very effective in guiding dialog response in accomplishing the desired tasks. (v) the generator is greatly hindered by the predictor accuracy, by feeding the ground truth acts, the proposed HDSA is able to achieve an additional gain of 7.0 in BLEU and 21% in Entity F1.
For IMS-CUB2-V, we use the following hyperparameters for training set size T: T<101: an embedding size of 100, a dropout coefficient of 0.5, 300 epochs of training, and an early-stopping patience of 100; 100
Sequence generation on the fly: To evaluate the ability of sequence generation, we compute the BLEU score of the sequences (titles for CiteULike and plots for Netflix) generated by different models. Incorporating the content information when learning user and item latent vectors, CTR is able to outperform other baselines and CRAE can further boost the BLEU score by sophisticatedly and jointly modeling the generation of sequences and ratings. Note that although CDL is able to outperform other baselines in the recommendation task, it performs poorly when generating sequences on the fly, which demonstrates the importance of modeling each sequence recurrently as a whole rather than as separate words.
To demonstrate the ability of Estimate and Replace to learn from less data, we ran an auxiliary experiment on a simpler dataset of greater-than/less-than questions. The questions came from 10 different templates, all requiring a true/false answer for 2 real numbers. For example: Out of x and y, is the first bigger ? where x,y are float numbers sampled from a ∼N(0,1010) distribution. The aim of this simple dataset was to demonstrate the ability of EstiNet to learn from less data. We compared the performance of the EstiNet model in plain and online training procedures. The plain training procedure served as a baseline and we measured its performance at test mode. This is because with plain training, the model does not learn to interact with external applications, thus, inference performance has no meaning. On the other hand, online training lets the DNN model learn to interact with external applications, thus, we can measure its inference performance. Our experiment contained 5 train sets with 250, 500, 1,000, 5,000 or 10,000 questions and a test set with 1,000 questions. The results show that with online training the model generalizes better and accuracy differences between the two training procedures increase as the amount of training data decreases. It is interesting to note that to achieve, for example, 0.97 accuracy, the online training only needs samples that are 5% of the data the plain training needs. We attribute the superiority of the EstiNet online training performance to its learning abilities. The model learns to interact with an external application to solve the logical part of the question.
After the competition, we had the chance to perform further optimizations of the hyperparameters leading to the current results, slightly improved from those submitted to the competition leaderboard. The EmbLexChange scores of the test set for all languages are available at http://language-lab.info/emblexchange/. Binary detection: EmbLexChange could detect the semantic changes in English, German, Swedish, and Latin with the accuracy of 70.3%, 75.3%, 77.4%, 60% respectively.
As we can observe in this table, Average Word Vector (AvgW2V) is the best method for vector representation. For this reason, we chose this as the main method for generating the feature vectors.
Evident from the table is that the best models overall are Trans-Baseline and Trans-Attn-Conv with a BLEU score of 39.8, and the other multimodal transformers have slightly worse performance, showing score drops around 0.1. Also, none of the multimodal transformer systems are significantly different from the baseline, which is a sign of the limited extent to which visual features affect the output.
The baseline shows that it is relatively easy to achieve a moderately good score in non-isomorphic match mode by generating a fixed set of paraphrases which are both common and generic: two of the three participating systems, SFS and IIITH, under-perform the naïve baseline in non-isomorphic match mode, but outperform it in isomorphic mode. The only system to surpass this baseline in non-isomorphic match mode is the MELODI system; yet, it under-performs against the same baseline in isomorphic match mode. No participating team submitted a system which would outperform the naïve baseline in both modes.
We can see that our proposed complete model, Hier-Co-Matching, achieved the best performance among all the public results. Still, there is a huge gap between the best machine reading performance and the human performance, showing the great potential for further research.
GRU’s much superior performances on Turney2012(5) and Turney2012(10) can also be attributed to the fact that we used the contrastive max-margin loss (as described in Section 2.4) as training objective, which proved to be more effective in our experiments than the negative sampling objective used by \newciteyu2015learning.
To evaluate the transferability of DDI detection to the related task of SDI/SSI detection, we use a test set consisting of 500 sentences annotated for the presence or absence of a supplement interaction. To obtain a balanced test set despite the rare presence of a positive interaction, we sample half the instances from the set of sentences labeled as positive by a previous variant of our model based on fine-tuning BERT-large, and the other half from those labeled as negative. After manual annotation, 40% of the sampled instances were positive for an interaction. Annotation was performed by two authors without seeing model predictions, with an inter-annotator agreement of 94%. This test set was used for final evaluation, and never for model development or tuning. Performance on the SDI test set has precision 0.82, recall 0.58, and F1-score 0.68. Although there is performance degradation during transfer, the precision of detection remains high at 0.82.
We train RoBERTa-DDI on a combination of DDI-2013 and NLM-DailyMed training data. We show the performance of the final variant of RoBERTa-DDI (trained on both DDI-2013 and NLM-DailyMed) as well as a variant trained only on DDI-2013 training data (last column), which performs best on the DDI-2013 test set, but suffers when tested on NLM-DailyMed. We also further break down performance on the DrugBank and Medline sub-corpora within DDI-2013.
Performance increases in every subsection, with the greatest gains in the broadcast conversation (bc) subsection (1.9) and the smallest in the telephone conversation (tc) subsection (0.3).
Though, this increase does not translate to an increase in NER performance, perhaps because of the domain mismatch in the pretrained truecaser’s training data and WNUT17.
We study the performance of different models with varying labeling rates and unlabeled dataset sizes. Labeling rates are the percentage of training instances that are used to train D. Though the unlabeled dataset we collect consists of around 5 million instances, we also sample a subset of around 50,000 instances to evaluate the effects of the size of unlabeled data. Since we do early stopping on the development set using the F1 scores, we also report the development F1. We report two metrics, the F1 scores and the exact matching (EM) scores Rajpurkar et al. All metrics are computed using the official evaluation scripts.
We observer BLEU and Meteor scores, which evaluate the generated answers on based on a n-gram overlap, of 16.48 and 13.97, respectively. BLEU-1 and the chrF score, which perform on a single word or character level, are slightly higher, i.e. 56.0 and 26.84.
We submit the results produced by SA-BiLSTM and obtain the evaluation scores provided by the challenge organizer.
For N=1,2,3 we found that Nseq2seq+A shows a modest but significant performance improvement over the baseline seq2seq+A. We only ran Nseq2seq+A on larger values of N, assuming it would continue to outperform.
In this section we provide a detailed description of our experimental results along with their analysis and insights. For each of the discussed tests – genre, length, and content – we investigate the performance of different embedding models across multiple embedding lengths.
In all the three versions of datasets, both joint models can improve the performance significantly compared to the separate baseline based on macro F1 score. Our approach ILP achieves better macro F1 score than MST on all the three datasets and generates significantly better results for component classification in both datasets mod-1 and mod-2. This re-confirms that our ILP-based approach can better utilize joint information for argumentation mining compared to the MST-based methods for component-based task. The performance gain from the joint models over the separate baseline approach increases from dataset mod-1 to mod-3 when the argumentation structure is more strict. This is because the joint model can perform better when the structure information is more correct. The performance gain of our model over MST-based is larger in dataset mod-1 and mod-2 compared to mod-3. This shows the MST-based model is more sensitive to the quality of the argumentation structure. Our ILP model is more robust.
In contrast with PPL evaluations, some pre-training methods either don’t improve significantly or even worsen ROUGE measures. Another difference compared to PPL evaluations is that for ROUGE, pre-training parameters that reside further from outputs (embeddings and encoder) seems more beneficial. This might imply that a better document representation is more important to stay on topic during beam search while it is less important during PPL evaluation where predicting next target headline word with high confidence is rewarded and the process is aided by previous target headline words that are fed to the decoder as inputs. It is also possible, that a well trained decoder becomes too reliant on expecting correct words as inputs making it sensitive to errors during generation which would somewhat explain why Enc.+dec. performs worse than Encoder alone. that should increase the robustness to mistakes during generation. Pre-training all parameters on all available text (Enc.+dec.+dist.) still gives the best result on English and quite decent results on Estonian. Best models improve ROUGE by 0.85-2.84 points.
However, it also leads to an increased SER. The lexicalized input DAs did not bring improvement over the delexicalized setting – lexicalized setups seem to perform slightly worse in terms of both word-overlap metrics and SER.
Our results suggest that scaling the TFiDF values through Naive Bayes is better than using raw TFiDF scores. Hence, these features were used for all subsequent experiments. It can also be observed that adding each group of features introduces a consistent improvement in accuracy on the by-Article data. However, we observed an opposite behaviour on the by-Publisher data. We believe this is due to the significant amount of noisy labels introduced by the distant supervision labeling strategy. Therefore, we based our decisions on the results obtained on the by-Article data since its labels are more accurate. The normalization strategy, i.e., scaling the features using calibrated scaling parameters, introduced significant performance improvements. Our system achieved a 72.9% accuracy on the test by-Article data, ranking 20th/42. It also achieved 60.8% accuracy on the test by-Publisher data, ranking 15th/42. All subsequent, and superior, results (rows 4–7) were obtained after the deadline.
Similarly, we see that the accuracy drop for the projection based models is the smallest across all datasets and amounts of perturbation.
Apart from the classification experiments, we also directly analyze the changes in the binary LSH projection representations by subjecting input text to different types and amount of perturbations. We make the following observations from our experiments: Average Hamming distance between LSH projections of words is ≈K/2, where K is the projection dimension which implies that the words are more or less uniformly spread out from each other indicating that there are no bias issues in the [0,1]K representation space. Assuming a 0.2 perturbation probability, we observe that LSH projection changes only by ≈ 11% w.r.t to the average Hamming distance between the words in the corpus when subject to misspellings. For instance, if the average Hamming distance between LSH projections of words is 100 bits, misspellings change the projections by only 11 bits on average. Intuitively, this suggests that neural layers on top of the LSH projection tend to rarely confuse a misspelled word for another valid word.
For diversity, we use the ratio of unique sequences in the batch ρ and the average ratio of unique 2-grams and 4-grams per sequence ρ2 and ρ4. For quality, we evaluate the output under the GPT-2 (Radford et al., Finally, we report the average length of the generated sequences to asses how well the length distribution of the data is preserved. the identical policy trained under maximum likelihood only. Also, we add data, a sample of the true data. For example, both reward functions seem to incentivize sentences with direct speech such as vinnie, i’m scared.” or idiots, ” she whispered. or jacques, i’ll kill him.” (we provide more examples in Appendix A). While direct speech appears in 40% of all sequences in the data, a model trained against n-gram generates direct speech 99% of the time and one trained against ours 95% of the time. This is in line with recent work by Choshen et al. Naturally, this surfaces much more pronouncedly when perform unconditional sampling instead of conditional argmax decoding.
Inspired by the human auditory cortex, where neighboring neurons tend to simultaneously activate, we employ a spatial smoothing technique to improve the accuracy of our LSTM models. The smoothing is implemented as a regularization term on the activations between layers of the acoustic model. First, each vector of activations is re-interpreted as a 2-dimensional image. For example, if there are 512 neurons, they are interpreted as the pixels of a 16 by 32 image. Second, this image is high-pass filtered. The filter is implemented as a circular convolution with a 3 by 3 kernel. The center tap of the kernel has a value of 1, and the remaining eight having a value of −1/8. Third, the energy of this high-pass filtered image is computed and added to the training objective function. We have found empirically that a suitable scale for this energy is 0.1 with respect to the existing cross entropy objective function. The overall effect of this process is to make the training algorithm prefer models that have correlated neurons, and to improve the word error rate of the acoustic model. We observed error reductions of between 5 and 10% relative from spatial smoothing.
The most frequent substitution for people on the Switchboard corpus was mistaking a hesitation in the reference for the word “hmm.” The scoring guidelines treat “hmm” as a word distinct from backchannels and hesitations, so this is not a scoring mistake. Examination of the contexts in which the error is made show that it is most often intended to acknowledge the other speaker, i.e. as a backchannel. For both people and our automated system, the insertion and deletion patterns are similar: short function words are by far the most frequent errors. In particular, the single most common error made by the transcribers was to omit the word “I.” While we believe further improvement in function and content words is possible, the significance of the remaining backchannel/hesitation confusions is unclear. We see that the human transcribers have a somewhat lower substitution rate, and a higher deletion rate. The relatively higher deletion rate might reflect a human bias to avoid outputting uncertain information, or the productivity demands on a professional transcriber. In all cases, the number of insertions is relatively small.
In order to assess the difficulty of the task in the presence of the particular data (images and captions) we perform a human study in the reference game with a human speaker and a human listener, where the human speaker can only communicate one of the existing captions of the target image. We perform the human study under two conditions. In the first condition, the human speaker has only access to the ground-truth captions and does not have access to the distractor image, thus has to pick a random caption. This corresponds to the perfect structural knowledge of English but no knowledge of the functional task and it is the human upper-bound of a captioning system performance on this task. In the second condition, the speaker has access to both the ground-truth captions and the distractor image, thus is able to pick a discriminative caption to communicate. We see that the task-specific condition outperforms the first condition, indicating that in our current setup there is enough space to improve upon models based on structural-only learning (i.e., captioning models). Moreover, the good performance of discriminative caption speaker demonstrates that (in principle) the captioning data can be used in a successful communication with a human for this task.
The main finding is that by increasing the number of components that get updated using the joint reward, the margin between the referential success of the two types of listeners increases. Despite the fact that the speaker is using human language that is perfectly fluent and accurate with respect to the target image (since the reranker operates on captions associated with the target image), while the joint listener is able to communicate with the agent speaker, the human listener achieves significantly lower performance.
As can be noted the context-based features seem to be so relevant for both targets. Besides, it is important to highlight that the best result for each target was not achieved by the same set of features. This is maybe not surprising, if we consider the different political campaign marketing strategies of the two candidates, which can influence also the communication of candidates’ oppositors and supporters, both in terms of language register used and addressed topics. For the sake of comparison with the state of the art, we present the results obtained by the three best ranked systems at SemEval-2016 Task 6. We only include the results concerning to Hillary Clinton and Donald Trump. We also show our best results for the two targets using both experimental settings as well as the position in the official ranking in the shared task.
Ablation Study Specifically, we remove DMM and QIM, and vary the number of DMR iterations. We see that the best performance is achieved with 3 iterations. The results show the effectiveness of both the dynamic memory module and the induction module with query information.
We use the results reported in Lin et al. [senatt] for the performances of non-neural baselines Mintz [ds], MultiR [hoffmann] and MIMLRE [surdeanu] on NYT dataset.
We first train this network without the use of parallel data, using only the senone targets, and starting from random weights in the AECNN. This is a surprising improvement in intelligibility given the lack of parallel data, and demonstrates that phonetic information alone is powerful enough to provide improvements to speech intelligibility metrics. The degradation in SI-SDR performance, a measure of speech quality, is expected, given that the denoising model does not have access to clean data, and may corrupt the phase.
In addition to the setting without any parallel data, we show results given parallel data. We only see a small improvement in the SI-SDR, likely due to the fact that the mimic loss technique is designed to improve the recognizablity of the results. In fact, seeing any improvement in SI-SDR at all is a surprising result.
We can see that BL+RO shows significant improvement over BL. Further, the factored SMT system with stem as alignment factor shows slight improvement in BLEU over the BL+RO, but other metrics show BL+RO is better compared to the factored system.
We compared our proposed models with several baselines from other algorithm as well. Our model significantly improving the PER and WER compared to encoder-decoder, attention-based global softmax and local-m attention (fixed-step size). Compared to Bi-LSTM model which was trained with explicit alignment, we achieve slightly better PER and WER with larger window size (2σ=3).
Generally, local monotonic attention had better result compared to global attention on both English-to-France and Indonesian-to-English translation task. Our proposed model were able to improve the BLEU up to 2.2 points on English-to-France and 3.6 points on Indonesian-to-English translation task compared to standard global attention. Compared to local-m attention with fixed step size, our proposed model able to improve the performance up to 0.8 BLEU on English-to-France and 2.0 BLEU on Indonesian-to-English translation task.
For fair comparison with ExCL, we follow the same setting in ExCL to use the 3D ConvNet fine-tuned on Charades dataset as visual feature extractor. Observed that VSLNet significantly outperforms all baselines by a large margin over all metrics. It is worth noting that the performance improvements of VSLNet are more significant under more strict metrics. For instance, VSLNet achieves 7.47% improvement in IoU=0.7 versus 0.78% in IoU=0.5, compared to MAN. Without query-guided highlighting, VSLBase outperforms all compared baselines over IoU=0.7, which shows adopting span-based QA framework is promising for NLVL. Moreover, VSLNet benefits from visual feature fine-tuning, and achieves state-of-the-art results on this dataset.
Note that this dataset requires YouTube clips to be downloaded online. We have 1,309 missing videos, while ExCL reports 3,370 missing videos. Strictly speaking, the results reported in this table are not directly comparable. Despite that, VSLNet is superior to ExCL with 2.06% and 0.16% absolute improvements over IoU=0.7 and IoU=0.3, respectively. Meanwhile, VSLNet surpasses other baselines.
Observe from the results, CMF shows stable superiority over CAT on all metrics regardless of other modules; CQA surpasses CAT whichever feature encoder is used. This study indicates that CMF and CQA are more effective.
The results shows that replacing CAT with CQA leads to larger improvements, compared to replacing BiLSTM by CMF. This observation suggests CQA plays a more important role in our model. Specifically, keeping CQA, the absolute gain is 1.61% by replacing encoder module. Keeping CMF, the gain of replacing attention module is 3.09%.
We consider three deep learning models: Bidirectional Attention Flow (BiDAF) Seo et al. Wang et al. We feed the concatenated parallel passage and the question as inputs. On a total of 51 QA pairs, we observed exact match (EM) scores of about 40% and token overlap F1 scores of about 45% for all models, versus their performance on the SQuAD dataset (EM of almost 70% and F1 of 80%).
Disease-related news articles were found to be indicative of infectious disease outbreaks [ghosh2015rare] . The HealthMap corpus is a publicly available database from which we collected the disease-related articles, reported during the time period of interest. Each article contains the reported date and the corresponding location information in the form of (lat, long) co-ordinate pairs. We converted the location co-ordinates to location names (country, state) via reverse geocoding. Reverse geocoding is defined as the process of finding a readable address or place name for a given (lat, long) pair. For example, (26.562851,−81.949532) was converted to (United States, Florida) after reverse geocoding. Each HealthMap article was passed through a series of preprocessing steps. For China, majority (87.94%) of the articles were published in either Traditional Chinese or Simplified Chinese. We translated the textual content of these articles to English for ease of analysis. The articles were preprocessed by removing non-textual elements, tokenization [webster1992tokenization, singh2014effective] , lemmatization [kanis2010comparison] and removal of stop words via BASIS Technologies’ Rosette Language Processing (RLP) tools [naren2014forecasting, doyle2014forecasting] . For more details on these steps, see subsection ‘HealthMap preprocessing’ within the section ‘Supplementary Information’ at the end of the manuscript. The set of unique words in these processed articles were found to contain general- (e.g., cold, contagious, nausea, blood, food-borne, waterborne, sanitation) as well as specific- (e.g., rabies, whooping, h7n9, dengue, salmonella, malaria) disease related terms.
For the purpose of comparison, we list their results together with ours. Similar to the observations of POS tagging, our model achieves significant improvements over Senna and the other three neural models, namely the LSTM-CRF proposed by \newcitehuang2015bidirectional, LSTM-CNNs proposed by \newcitechiu2015named, and the LSTM-CRF by \newcitelample:2016:NAACL. \newcitehuang2015bidirectional utilized discrete spelling, POS and context features, \newcitechiu2015named used character-type, capitalization, and lexicon features, and all the three model used some task-specific data pre-processing, while our model does not require any carefully designed features or data pre-processing. We have to point out that the result (90.77%) reported by \newcitechiu2015named is incomparable with ours, because their final model was trained on the combination of the training and development data
Learning all the constraints jointly provides the largest improvement in F1 at 95.39. This improvement in F1 over the baseline CRF as well as the improvement in F1 over using only-one constraints was shown to be statistically significant using the Wilcoxon signed rank test with p-values <0.05. In the all-constraints settings, 32.96% of the constraints have a learned parameter of 0, and therefore only 421 constraints are active. Soft-DD converges, and thus solves the constrained inference problem exactly, for all test set examples after at most 41 iterations. Running Soft-DD to convergence requires 1.83 iterations on average per example. Since performing inference in the CRF is by far the most computationally intensive step in the iterative algorithm, this means our procedure requires approximately twice as much work as running the baseline CRF on the dataset. On examples where unconstrained inference does not satisfy the constraints, Soft-DD converges after 4.52 iterations on average. For 11.99% of the examples, the Soft-DD algorithm satisfies constraints that were not satisfied during unconstrained inference, while in the remaining 11.72% Soft-DD converges with some constraints left unsatisfied, which is possible since we are imposing them as soft constraints. We could have enforced these constraints as hard constraints rather than soft ones. In addition, running the DD algorithm with these constraints takes 5.21 iterations on average per example, which is 2.8 times slower than Soft-DD with learned penalties.
, we demonstrate that introducing simple enhancements like sharing the lower-layers of the decoder (share) and adding task-specific tags (tags) during multi-task pre-training also helps in improving the performance while at the same using fewer parameters and hence a smaller memory footprint.
In addition to automatic evaluation, which can sometimes be misleading, we perform human evaluation of summaries generated by our models. We randomly sample 50 pairs of the model outputs from the test set and ask three human evaluators to compare the pre-trained supervised learning model and reinforcement learning models in terms of relevance and fluency. For each pair, the evaluators are asked to pick one out of: first model (MLE-xls+mt+dis; lose) , second model(RL models; win) or say that they prefer both or neither (tie). We observe that the outputs of model trained with ROUGE-L rewards are more favored than the ones generated by only pre-trained model in terms of relevance but not fluency. This is likely because the RL-rouge model is trained using machine-generated summaries as references which might lack fluency. On the other hand, cross-lingual semantic similarity as a reward results in generations which are more favored both in terms of relevance and fluency.
Initially we explore the characterization of all isolated layers with the standard set of network measures (see Methods Section). The average path length (L) decrease from co-occurrence to syntax, as expected, but interestingly it is of the same range for the syntax and syllabic layer. The clustering coefficient (C) (obtained from the undirected versions of the same networks) increases on the syllabic subword level for Croatian and decreased for English. The clustering of English CO and SHU word-levels are higher than their Croatian counterparts. Still, clustering coefficients of SIN layers in both languages are of the same range.
This dataset was widely used as a benchmark dataset of image and sentence retrieval. Our model outperforms the state-of-the-art methods (i.e Socher-decaf, DeViSE-decaf, DeepFE-decaf) by a large margin when using the same image features (i.e. decaf features). “-avg-rcnn” denotes methods with features of the average CNN activation of all objects above a detection confidence threshold. The results show that using these features will improve the performance. Even without the help from the object detection methods, however, our method performs better than these methods in most of the evaluation metrics. We will develop our framework using better image features in the future work.
(The utterances are cropped, so not all Spanish words are shown.) Notice that the two pairs (a and c) that have matching Spanish words also have matching English content words, even though one of them falls below the recommended 0.88 threshold for a UTD (acoustic) match. On the other hand, pair (b) has a high UTD score due to phonetic similarity, but there is no match between the English words. Empirically we have observed J ≥ 0.1 to be a good indicator of a correct match (although in practice we do not impose any threshold on J).
For testing, we simulate the interactive procedure by randomly providing a feedback according to the generated supplementary question as user’s input, and then predicting an answer. For example, when asking “who is he?” , we randomly select a male’s name mentioned in the story as feedback. Conventional QA baseline methods, i.e., DMN+, MemN2N, and EncDec, do not have interactive part, so they cannot use feedback for answer prediction. Our approach (CAN+IQA) and EncDec+IQA adopt the proposed interactive mechanism to predict answer. From the results, we can achieve the following conclusions: [leftmargin=*] Our method outperforms all baseline methods and has significant improvements over conventional QA models. Specifically, we can nearly achieve 0% test error rate with RIQA=1.0 ; while the best result of conventional QA methods can only get 40.5% test error rate. CAN+IQA benefits from more accurate context modeling, which allows it to correctly understand when to output an answer or require additional information. For those QA problems with incomplete information, it is necessary to gather the additional information from users. Randomly guessing may harm model’s performance, which makes conventional QA models difficult to converge. But our approach uses an interactive procedure to obtain user’s feedback for assisting answer estimation. EncDec+IQA can achieve a relatively better result than conventional QA models in the datasets with high IQA ratios, especially in task 7. It happens due to our proposed interactive mechanism, where feedback helps to locate correct answers. However, it does not separate sentences, so the long inputs make its performance dramatically decreases as RIQA decreases. This explains its poor performance in most datasets with low IQA ratios, where there exists a large number of regular QA problems. For the conventional QA methods, DMN+ and MemN2N perform similarly and do better than EncDec. Their similar performance is due to the limitation that they could not learn the accurate meaning of statements and questions with limited resource and then have trouble in training the models. But they are superior over EncDec as they treat each input sentence separately instead of modeling very long inputs.
We use CN-n and BS-n as abbreviation for CR-NRR and BLEU-NSBLEU with n-gram, respectively. Note that the reported QDisc values are corresponding lower bounds, since the optimization-based method does not guarantee a global optimum. These non-zero QDisc values provide a clear support for the incompatibility of BLEU-NSBLEU. We can also see that such discrepancy is significant on some cases, e.g. QDisc>0.02 and DRate=9.41% for BS-2 on data with σ=0.5. A QDisc value of 0.02 means that, we cannot surely claim that a model is better than another when the quality gap is below 0.02, which is already a clear gap for BLEU. We also run similar experiments for CR-NRR. However, no positive lower bound is observed, which is in accordance with our theory.
We can see that real data stays close to the CR-NRR curve, while a much larger gap is observed between real data and the BLEU-NSBLEU curve. BLEU-NSBLEU shows a significant incompatibility, by QDisc values ranging from 0.090 to 0.258. Such huge discrepancy in BLEU is unbearable in real applications, e.g. we cannot claim a model is better than another even if it achieves higher NSBLEU and significantly higher BLEU. As a result, we suggest not to use BLEU-NSBLEU in order to avoid misleading conclusions. CR-NRR also shows a small positive discrepancy, this is due to the inevitable difference between the empirical distributions of candidate set and reference set. However, discrepancy caused by such distribution difference is far much smaller than BLEU-NSBLEU in terms of DRate, Self-Ratio, and Ref-Ratio.
System-0 is the baseline hard target based DNN. System-1 is built by supervised enhancement of soft outputs obtained from system-0 on AMI training data as shown in Fig. As expected, training with the soft targets yields lower WER than the baseline hard targets. We can see that both PCA and sparse reconstruction result in more accurate acoustic modeling, where sparse reconstruction achieves 0.8% absolute reduction in WER. DNNs trained with low-rank and sparse soft targets are used to generate soft targets for ICSI corpus and Librispeech (LIB100) which are sources of untranscribed data. First, system-2 is built augmenting enhanced AMI training data with ICSI soft targets generated from system-1. We consider ICSI corpus, consisting of spontaneous speech from meeting recordings, as in-domain with AMI corpus. While PCA based DNN successfully exploits information from this additional ICSI data showing significant improvement from system-1 to system-2, the same is not observed using sparsity based DNN.
The goal is to adapt a general-domain NMT model to a new domain with either post-edits or markings. Model outputs are de-tokenized and un-BPEd before being presented to the annotators. With the help of human annotations we then adapt this model to the domain of TED talk transcripts by continuing learning on the annotated data. Hyperparameters including learning rate schedule, dropout and batch size for this fine-tuning step are tuned on the IWSLT17 dev set. For the marking mode, the weights δ+ and δ− are tuned in addition. As test data, we use the split of the selected talks that was annotated in the user-mode, since the purpose of this split was the evaluation of user preference. There is no overlap in the three data splits, but they have the same distribution over topics, so that we can both measure local adaptation and draw comparisons between modes.
Our CM-Net achieves the state-of-the-art results on both datasets in terms of slot filling F1 score and intent detection accuracy, except for the F1 score on the ATIS. Since the SNIPS is collected from multiple domains with more balanced labels when compared with the ATIS, the slot filling F1 score on the SNIPS is able to demonstrate the superiority of our CM-Net.
Recently, there has been a growing body of works exploring neural language models that trained on massive corpora to learn contextual representations (e.g. BERT Devlin et al. EMLo Peters et al. Inspired by the effectiveness of language model embeddings, we conduct experiments by leveraging the BERT as an additional feature.
The domain variation between documents leads to a lower overlap between holders, targets, and polar expressions seen in training and those at test time (56%, 28%, and 50%, respectively). We argue, however, that this is a more realistic situation regarding available data, and that it is important to move away from simplifications where training and test data are taken from the same distribution.
Since this method requires a set of annotations (captions) for an image, i.e. it requires ground-truth neighbor relations for texts, we can only apply it on COCO. In the first column, we show our method. We see that as expected, using estimated rather than ground-truth text neighbors reduces performance (third vs. second columns). ’s structural constraint in image rather than text space is better (fourth vs. third columns). In both cases, neighborhoods are computed in text space This may be because the structural constraint, which requires the group of neighbors to be closer together than to others, is too strict for estimated text neighbors. That is, the constraint may require the text embeddings to lose useful discriminativity to be closer to neighboring text. Neighboring images are likely to be much more visually similar in COCO than in GoodNews or Politics as they will contain the same objects.
We train the three parsers on all the training data with FA. We also employ four publicly available parsers with their default settings. We can see that the three parsers that we implement achieve competitive parsing accuracy and serve as strong baselines.
IS loss after a few epochs of 2048 unit LSTM with 512 projection. The IS objective significantly improves the speed and the overall performance of the model when compared to NCE. We used 32 Tesla K40 GPUs to train our models. The smaller version of the LSTM model with 2048 units and 512 projections needs less than 10 hours to reach below 45 perplexity and after only 2 hours of training the model beats previous state-of-the art on this data set. The best model needs about 5 days to get to 35 perplexity and 10 days to 32.5. The best results were achieved after 3 weeks of training.
The decision tree and MLP classifiers get very high performance with 0.87 and 0.9 F1 scores respectively. The baseline classifies every event as pivotal and thus performs very poorly.
IWSLT is a much smaller benchmark and we therefore switch to a smaller architecture: dff=1024, d=512, and H=4. We further run experiments on WMT Zh-En translation to evaluate on a non-European language. LightConv outperforms the baseline by 0.5 BLEU and DynamicConv by 0.6 BLEU.
Here, the sequence-to-sequence model produces the output conditioned on the state of the encoder at the end of the block. Both models used an encoder with two layers of 250 LSTM cells, without attention. The standard sequence-to-sequence model performs significantly worse than our model – the recurrent connections of the transducer across blocks are clearly helpful in improving the accuracy of the model.
A three layer encoder coupled to a three layer transducer performs best on average. Four layer transducers produced results with higher spread in accuracy – possibly because of the more difficult optimization involved. Thus, the best average PER we achieved (over 3 runs) was 19.8% on the TIMIT test set. but we did not pursue those avenues in this paper.
We observe better performances for the hotel-review dataset than the open domain Wikipedia dataset, for the intuitive reason that documents and sentences are written in a more fixed format and easy to predict for hotel reviews.
Issue Les résultats de l’issue sont présentés dans Dans les deux premiers cas , nous avons utilisé uniquement un modèle de classification pour prédire l’ensemble des classes. Nous avons tout d’abord évalué un modèle appris à partir de l’entièreté du cas clinique. Nous avons ensuite comparé ses résultats avec ceux d’un autre classifieur entraîné à prédire sur les données étiquetées automatiquement. Enfin, nous avons identifié les cas de décès en utilisant des connaissances linguistiques puis appris un modèle pour les issues restantes sur les quatre dernières phrases des cas cliniques. Après annotation manuelle du corpus d’entraînement pour étiquetage, nous avons en effet fait plusieurs observations relatives à l’issue. D’une part, les cas de décès se prêtent mieux à une approche lexicale avec un vocabulaire très spécifique. Et d’autre part, dans la majorité des cas, l’empan de texte renvoyant à l’issue apparaît vers la fin du cas clinique. Cette dernière approche est celle qui retourne les meilleurs résultats avec un score de 0.60 et 0.58 en précision et rappel respectivement.
The agreement is measured for both fine-grained ratings that consider all scales (1 - 5) and coarse-grained ratings that consider only two scales (low: 1 - 2, high: 3 - 5). Although the inter-annotator agreement is higher for the coarse-grained ratings, it is apparent that the agreement scores are dramatically low for both.
This table includes performance values when we applied the classifier on both datasets, TC2014 and TC2015. The additional column, “Diff.”, shows the relative difference in performance for each of these datasets, i.e., measuring the extent to which a model learned from the TC2014 dataset can still be applied to the TC2015 test set. Note that while higher values are desired for micro-accuracy and macro-accuracy, lower values are optimal for MSE.
As mentioned previously, we strived for minimizing sub-optimal sentences in terms of context dependence, while trying to avoid being excessively selective to maintain a varied set of examples. To assess performance with respect to this latter aspect, we inspected also the percentage of sentences identified as context-dependent in dictionary examples (CInd-D) and coursebook sentences (CInd-LL).
The model with full adaptation (v3), performs the best across all experiments: achieving the highest accuracy, recall and F1 with LSTM and the highest precision with GRU. The divergence between LSTM and GRU may indicate that the forget and output gates in LSTM are helpful for the retrieval of related memory but increase the variance of the model.
Our model systematically outperforms the state-of-the-art approach , CNN-CR. More specifically, 1) CRNN achieves better mean accuracy for all reference sizes and significantly for sizes 1−3, which consist of 96.4% utterances. 2) The average number of DAs predicted by CRNN is the closest to the reference.
The chunk hopping using the median of attention worked well in the English task but poorly in the Chinese task. This was because Chinese requires a wider area of the encoded features to emit each character. On the other hand, our proposed decoder prevented the degradation of performance. In particular, using all the past frames of encoded features, our proposed decoder achieved the highest accuracy among the online processing methods. This indicated that the new decoding algorithm was able to exploit the wider attentions of Transformer.
The table shows that apart from study 1, the homogeneous variance feature overwriting model is clearly superior to the retrieval interference model because it has higher ˆelpd values. Finally, the table shows that, except for study 1, the heterogeneous variance model is superior to the homogeneous variance model.
The bAbI dataset contains 20 tasks in total. Each task is learned separately. on all tasks listed. The results show that auto-encoding is essential to bootstrap learning– without auto-encoding the expected rewards are near zero; but auto-encoding alone is not sufficient to achieve high rewards Since multiple discrete latent structures (i.e. knowledge tuples and programs) need to agree with each other over the choice of their representations for QA to succeed, the search becomes combinatorially hard. Structure tweaking is an effective way to refine the search space – improving the performance of more than half of the tasks.
Our model AC-ABS also achieves the best performance. Although CopyNet employs a copying mechanism to improve the summary quality, RNN-distract considers attention information diversity in their decoders, and DRGD integrates a recurrent variational auto-encoder into the typical seq2seq framework, our model is still better than these methods demonstrating that the effectiveness of the alternating actor-training strategy. We also believe that integrating the copying mechanism and coverage diversity in our framework will further improve the summarization performance.
We found that the BiLSTM-CRF model consistently outperforms other methods, achieving an overall F1 score of 86.89 at identifying action triggers and 72.61 at identifying and classifying entities.
Our attention-based Wav2Text architecture uses four convolutional layers, followed by two NIN layers at the lower part of the encoder module. Inside the first NIN layers, we stacked three consecutive filters with LReLU activation function. For the second NIN layers, we stacked two consecutive filters with tanh and identity activation function. For the feature transfer learning training phase, we used Momentum SGD with a learning rate of 0.01 and momentum of 0.9.
Importance of taxonomical categories - how should we construct the disease vocabulary? We followup our previous analysis by investigating the importance of words related to each taxonomical category in constructing the disease vocabulary towards final characterization accuracy. To evaluate a particular category, we used a truncated disease vocabulary consisting of disease names and the words in the corresponding category to drive the discovery of word embeddings in Dis2Vec under the best parameter configuration for that category. We compared the accuracy of each of these conditions (Dis2Vec (exposures), Dis2Vec (transmission methods), Dis2Vec (transmission agents), Dis2Vec (symptoms)) against Dis2Vec (full vocabulary) across the 4 characterization tasks. (a) Constructing the vocabulary with words related to all the categories leads to better characterization across all the tasks. (b) As expected, Dis2Vec (symptoms) is the second best performing model for the symptoms category but it’s performance is degraded for other tasks. The same goes for Dis2Vec (transmission methods) and Dis2Vec (transmission agents). (c) Therefore, it indicates that in order to achieve reasonable characterization accuracy for a category, we need to supply at least the words related to that category along with the disease names in constructing the vocabulary.
Compared with other Chinese taxonomies, CN-Probase is the largest one when it comes to the number of entities, concepts and isA relations. The main reason is that we extract isA relations from multiple sources of Chinese encyclopedia. Besides, CN-Probase is a high-quality taxonomy with a precision 95% that outperforms Probase-Tran and Bigcilin. Although part of the noise has been reduced by three heuristic methods, the precision of Probase-Tran is still quite low due to various sources of noise. Hence simple cross-language translation cannot produce high-quality Chinese taxonomy. Bigcilin also extracts isA relations from multiple sources, but its precision is worse than ours since we use verification module to further improve the precision. Chinese WikiTaxonomy is built only from a single source (i.e. tag) of Chinese encyclopedia. As a result, it has a high precision but low coverage, and the number of isA relations in our taxonomy is 25x larger than Chinese WikiTaxonomy. We also evaluate the precision of each source for our taxonomy, and the precision of isA relations derived from the tag is 97.4% which is comparable to Chinese WikiTaxonomy.
Overall, the results across three evaluation metrics consistently indicate that our proposed CAE-LSTM achieves better performances against other state-of-the-art techniques including non-attention models ( Image-Flat, Regions-Hierarchical, and CapG-RevG) and attention-based approaches (LSTM-ATT and RTT-GAN). Specifically, the CIDEr and METEOR scores of our CAE-LSTM can achieve 25.15% and 18.82%, which makes the relative improvement over the best competitor CapG-RevG by 20.2% and 1.1%, respectively. As expected, by additionally modeling topics/gists in an image via LSTM, Regions-Hierarchical exhibits better performance than Image-Flat which leaves inter-sentence dependency unexploited. Moreover, LSTM-ATT leads to a performance boost over Regions-Hierarchical, which directly encodes an image as a global representation by performing mean pooling over all region-level features. The results basically indicate the advantage of region-level attention mechanism in the two-level LSTM networks by learning to focus on the image regions that are most indicative to infer the next word. Most specifically, RTT-GAN and CapG-RevG by modeling reality and diversity of paragraphs with Generative Adversarial Networks and Variational Auto-Encoders, improves LSTM-ATT. However, the performances of RTT-GAN and CapG-RevG are still lower than our CAE-LSTM which exploits the inherent structure among all image regions for topic modeling in a convolutional and deconvolutional auto-encoding framework.
We outperform previous systems in all metrics. In particular, our single model improves the state-of-the-art average F1 by 1.5, and our 5-model ensemble improves it by 3.1.
To tease apart the contributions of improved mention scoring and improved coreference decisions, we compare the results of our model with alternate span pruning strategies. In these experiments, we use the alternate spans for both training and evaluation. degrades performance by 1 F1. We also provide oracle experiment results, where we keep exactly the mentions that are present in gold coreference clusters. With oracle mentions, we see an improvement of 17.5 F1, suggesting an enormous room for improvement if our model can produce better mention scores and anaphoricity decisions.
We train the function-disabled models on COCO-VQA training set and show the accuracies on validation set in Table. From the columns, we can see that: (1) holistic is better than TraAtt, proving the effectiveness of element-wise multiplication feature fusion compared with concatenation of features. (2) RegAtt is better than holistic, indicating our novel attention method indeed enriches the visual features and improves the performance. (3) SalAtt is better than RegAtt, demonstrating the strength of our region pre-selection mechanism. (4) ConAtt is worse than SalAtt, showing that BiLSTM is important for the region pre-selection part. From each row, we find the consistent improvement by the ResNet features, showing the importance of good CNN features to VQA.
The first line shows the accuracy of a baseline that, for each word/phrase, chooses a sense number at random and then assigns the correspondent DOLCE label. The efficacy of the MCS disambiguation method adopted by the FO tagging can be observed by the F1-Score for both datasets, well above the random baseline. When compared to the SST, FO Tagging presents an increase of 9.05% in the F1-Score for the SemCor dataset.
Clearly, as N increases, the entropy of grammars 1, 2 and 7 monotonically decreases while their average edit distance monotonically increases. For other grammars, both their entropy and average edit distance change in the directions that are opposite to that for grammar 1, 2 and 7. Specifically, when only observing the entropy, it is difficult to distinguish grammars 3 and 4 from grammars 5 and 6. The difference between these two classes of grammars is more clearly demonstrated when comparing their average edit distance. In particular, the average edit distance of grammars 5 and 6 is constantly equal to 1, while the average edit distance for grammars 3 and 4 keeps increasing as N increases. This indicates that average edit distance reveals more information about a regular grammar when compared with entropy. However, it is also important to note that the time and space cost for calculating average edit distance can be significantly higher than that needed for calculating entropy. Thus there is a trade-off between the granularity and computational efficiency when using these two metrics.
We compare training on the first 1k tokens vs. first 1k distinct types of the UDT training sets. Although Lematus does relatively poorly on the token data, it benefits the most from switching to types, putting it on par with HMAM and suggesting is it likely to benefit more from additional type data. Lemming requires token-based data, but does worse than HMAM (a context-free method) in the token-based setting, and we also see no benefit from context in comparing Lematus 20-ch vs Lematus 0-ch. So overall, in this very low-resource scenario with no data augmentation, context does not appear to help. We believe this discrepancy is due to the issues of biased/incomplete data noted above. For example, we analyzed the Latvian data and found that the available tables for nouns, verbs, and adjectives give rise to 78 paradigm slots. The 17 POS tags in UDT give rise to about 10 times as many paradigm slots, although only 448 are present in the unseen words of the dev set. Of these, 197 are represented amongst the 1k UDT training types, whereas only 25 are included in the 1k UM training types. As a result, about 72% of the unseen types of dev set have no representative of their paradigm slot in 1k types of UM, whereas this figure is only 17% for the 1k types of UDT.
It interesting to note that the metrics do not fully agree on the ranking of systems, although the four best (statistically indistinguishable) systems win by all metrics. All-but-one submission outperformed the text-only NMT baseline. This year, the best performing systems include both multimodal (LIUMCVC_MNMT_C and UvA-TiCC_IMAGINATION_U) and text-only (NICT_NMTrerank_C and LIUMCVC_MNMT_C) submissions. (Strictly speaking, the UvA-TiCC_IMAGINATION_U system is incomparable because it is an unconstrained system, but all unconstrained systems perform in the same range as the constrained systems.)
4.2.1 Multi30K Test 2017 A reduced number of submissions were received for this new language pair, with no unconstrained systems. In contrast to the English→German results, the evaluation metrics are in better agreement about the ranking of the submissions.
4.2.2 Once again, in contrast to the English→German results, the evaluation metrics are in better agreement about the ranking of the submissions. The performance of all the models is once again in mostly agreement with the Multi30K 2017 test data, albeit lower. Both DCU-ADAPT_MultiMT_C and OREGONSTATE_2NeuralTranslation_C again perform relatively better on this dataset.
Transfer to RTE-5. One main motivation of exploring this SciTail problem is that this is an end-task oriented TE task. A natural question thus is how well the trained model can be transferred to other end-task oriented TE tasks. Clearly, the model pretrained on SNLI has not learned anything useful for RTE-5 – its performance of 46.0% is even worse than the majority baseline. The model pretrained on SciTail, in contrast, demonstrates much more promising generalization performance: 60.2% vs. 46.0%.
We generated four versions of summary (word length limit=100, 200 and the rate of text reduction=10%, 20%). Our proposed framework outperforms many of the existing text summarizers on SogouCA dataset in terms of our proposed faithful score such as ILP, graph-based approaches.
The density curves show that most model outputs are shorter than human annotations with the action-only model (6) being the shortest as expected. Interestingly, the two different uni-modal and multimodal systems with ground-truth text and ASR output text features are very similar in length showing that the improvements in Rouge-L and Content-F1 scores stem from the difference in content rather than length. Example presented in Table The text-only model produces a fluent output which is close to the reference. The action features with the RNN model, which sees no text in the input, produces an in-domain (“fly tying”’ and “fishing”) abstractive summary that involves more details like “equipment” which is missing from the text-based models but is relevant. The action features without RNN model belongs to the relevant domain but contains fewer details. The nearest neighbor model is related to “knot tying” but not related to “fishing”. The scores for each of these models reflect their respective properties. The random baseline output shows the output of sampling from the random language model based baseline. Although it is a fluent output, the content is incorrect. Observing other outputs of the model we noticed that although predictions were usually fluent leading to high scores, there is scope to improve them by predicting all details from the ground truth summary, like the subtle selling point phrases, or by using the visual features in a different adaptation model.
We have post-processed the sequence of labels to obtain the slots and their values. The slot-value pair is compared to the reference test set and the result is reported in F-score of slot classification. Experiments were carried out having 10%, 20% and 30% of data being labeled. For each of these tests, labeled set was selected randomly from the training set. This procedure was done 10 times and the reported results are the average of the results thereof. The Supervised CRF model is trained only on the labeled fraction of the data. However, the Self-trained CRF and Semi-supervised CRF have access to the rest of the data as well, which are unlabeled. Our Supervised CRF gained 91.02 F-score with 100% of the data labeled which performs better compared to 89.32% F-score of \newciteray2007 CRF model. The most significant improvement occurs when only 10% of training set is labeled; where we gain 1.65% improvement on F-score compared to supervised CRF and 1.38% compared to self-trained CRF.
As we can see in the table, SurfKE substantially outperforms all baseline approaches on the synthesized dataset. For instance, SurfKE obtains an F1-score of 0.260 compared to 0.240 and 0.235 achieved by the best performing baselines, PositionRank and Maui, respectively. With relative improvements of 50.68% 33.33%, 15.18% and 8.91% over KEA, KPMiner, Maui and PositionRank respectively, the Precision of our model is significantly superior to that of the other models.
However, the target performance measure of embedding models is the correlation between semantic similarity and human assessment We see that our method outperforms the competitors on all datasets except for “men” dataset where it obtains slightly worse results. Moreover, it is important that the higher dimension entails higher performance gain of our method in comparison to the competitors.
We train a number of models on the classification task. We report the micro-F1 scores (same as accuracy in multiclass classification). To put the model results into relation, we report the accuracy of the dummy classifier, which always predicts the majority class (”fit”) independent of the input. Although, there is some imbalance in both datasets, it was not necessary to apply up- or down-sampling techniques when training the classifiers.
Comparison with the Base Decoders. Metrics included are BLEU-4 Papineni et al. et al. We observe a significant improvement on the CIDEr scores over all of the baseline models using REINFORCE (i.e. 107.5 v.s. 104.0 using FC Model, 124.0 v.s. 120.7 using FC Model). We attribute the improvements to both HSG and the word-level intermediate rewards.
Interestingly, the recurrent model takes the lead in the average syntactic similarity. This confirms our hypothesis that the recurrent model dedicates more of the capacity of its hidden states, compared to transformer, to capture syntactic structures. We should add that our approach may not fully explain the degree to which syntax in general is captured by each model, but only to the extent to which this is measurable by comparing syntactic structures using PARSEVAL.
We compare the effectiveness of employing the Apt on different layers. The phenomena from the encoder and decoder sides are different. On the encoder side, more layers fuse external knowledge, better performance the model achieve. Moreover, high layers can get more gain comparing with low layers. These results indicate that the dynamic fusion can improve the ability of modeling input sentence at all layers. High layers of the encoder need external contextual knowledge more than low layers to get the semantic from the input sentence.
Election yields consistently good results. Augmenting synthetic data with gold data yields better F1 score than training only with gold tagged data. Also, it is interesting to observe that there is a sharp drop of F1 score for HI_EN,FB and BN_EN data sets for Gold data while training with ordinal cross entropy function across all the sentiment labels. However, mixing them with synthetic data helps in achieving better results.
In order to find the optimal solution, during the training of the models we did a quantitative analysis of the generated stories. The quantitative analysis was done by tracking the BLEU and METEOR scores of the trained models. The only difference between them is the number of epochs used for their training. After numerous experiments with the number of epochs used for training, we concluded that the model achieves the best scores when after training the loss (calculated over the training set) is between 0.82 and 1.72. Model1 has been trained for 50 epochs and has the smallest training loss, but it achieved the worst BLEU and METEOR scores out of the three models. This means that model1 is overfitting. Both model2 and model3 have a METEOR score of 23.9, but model3 achieved the best BLEU score. The difference in BLEU and METEOR scores between the model2 and model3 is very small and because of the aforementioned problems regarding the use of BLEU and METEOR scores as metrics for story generation, the results from the models had to be evaluated by human evaluators.
To further evaluate the effectiveness of our model, we consider multi-label vertex classification. Then we averaged over each node’s context-aware embeddings with all other connected nodes, to obtain a global embedding for each node, i.e., zu=1du∑vzu|v, where du denotes the degree of node u. A linear SVM is employed, instead of a sophisticated deep classifier, to predict the label attribute of a node. We randomly sample a portion of labeled vertices with embeddings (10%,30%,50%,70%) to train the classifier, using the rest of the nodes to evaluate prediction accuracy. The GANE models delivered better results compared with their counterparts, lending strong evidence that the OT attention and attention parsing mechanism promise to capture more meaningful representations.
1400 CE, and one in the modern period, after 1800 CE. These mostly correspond to increases in the number of words as well. For the periodization experiments, we consider two possible initial time divisions: 50 and 100 year bins. The 100-year bins are simply a concatenation of each two consecutive 50-year bins. We find that the very early time periods contain much less text, so we merge the bins for the first 200 years in all of the experiments (2 and 4 bins are merged in the case of the 100-year and 50-year bins, respectively). In the following, we investigate the effect of preprocessing on periodization, by considering two input formats for the periodization algorithm: plain text and lemmas. Working with lemmas allows the periodization algorithm to focus more on lexical properties rather than surface forms.
We find that annotators consistently rated the Causal system higher. The differences (in both Score and Rank) between the Causal system and the next best system are significant under a Wilcoxon signed-rank test (p<0.01).
Note the the rankings are now from 1 to 3 (higher is better). We find annotators usually rated the Causal system higher, though the LM model is much closer in this case. The differences (in both Score/Rank) between the Causal and LM system outputs are not significant under a Wilcoxon signed-rank test, though the differences between the Causal and PMI system is (p<0.01). The fact that the pairwise Causal model is still able to (at minimum) match the full sequential model on a chain-wise evaluation speaks to the robustness of the event associations mined from it, and further motivates work in extending the method to the sequential case.
Our Python implementation will be made available at the first author’s website. We noticed the error on the development set does not improve after 20 iterations over the training set, therefore, we ran the training for 20 iterations. The sentences where shuffled between iterations. Non-projective sentences were skipped during training. We use the default parameters initialization, step sizes and regularization values provided by the PyCNN toolkit.
Of the unsupervised composition models, elementwise addition is clearly more effective than multiplication, which almost never returns the correct word as the nearest neighbour of the composition. Overall, however, the supervised models (RNN, BOW and OneLook) clearly outperform these baselines. The results indicate interesting differences between the NLMs and the OneLook dictionary search engine. Clearly the OneLook algorithm is better than NLMs at retrieving already available information (returning 89% of correct words among the top-ten candidates on this set). However, this is likely to come at the cost of a greater memory footprint, since the model requires access to its database of dictionaries at query time.
First, compared with systems without dependency tree feature (SVM, IAN, TNet, MGAN, AEN), our model gives significantly better results. Second, compared with recent work which takes dependency trees but without relation labels as input (CDT, TD-GAT), RGAT consistently outperforms TD-GAT across all the datasets, with an accuracy improvement of 3.6 percent on Laptop. In addition, RGAT (GloVe) gives better results than the state-of-the-art CDT model on Restaurant and Laptop. While getting comparable accuracy with CDT and TNet on Twitter, RGAT (GloVe) achieves better F1 score. Furthermore, according to the results, pretrained BERT model can significantly boost the performance of each model. With the help of BERT, the proposed model achieves better results than all the baselines, with accuracy of 86.59, 81.25 and 75.84 on Restaurant, Laptop and Twitter, respectively.
Effectiveness of structural information As shown in In addition, the performance of GAT (B) is also better than that of Transformer (B). The above results indicates that syntax is helpful for targeted sentiment classification.
One can easily see that these assumptions are false in many cases, which may introduce noise and deteriorate a resulting model. Thus, the presented solutions undergo experimental validation using the following sets of relatives: Monosemous children – monosemous direct hyponyms. Monosemous relatives – monosemous synonyms, direct hyponyms and direct hypernyms. First relatives – words in the first meaning belonging to synsets of synonyms, direct hyponyms or direct hypernyms. Word determiners – collocations made of two words in any order: the target word and a determiner associated with a given meaning. All determiners – collocations made of two words in any order: a polysemous relative and a determiner associated with the appropriate meaning. Other words – occurrences of the target ambiguous word in a document that contains other relatives corresponding to exactly one of the meanings. As we can see, the number of replacements rises after adding subsequent sources, but the largest increase is caused by including polysemous relatives with determiners. On the other hand, these compound relatives rarely appear in the corpus (13,126 occurences), whereas employing polysemous words in the first sense results in 1,525,739 new training cases and a substantial growth of accuracy. What is more, although the profits from these sources of relatives differ, none of them decreases the accuracy.
PullNet is comparable with GRAFT-Net on the complete KB only setting and slightly worse on the text only setting, but is consistently better than GRAFT-Net on the incomplete KB setting or the incomplete KB plus text setting. It is also significantly better than the re-implemented GRAFT-Net, which uses the less highly-engineered retrieval module.
An expected, PullNet shows significant improvement over GRAFT-Net and KV-Mem on all four settings. Once again we see some improvement when pairing the incomplete knowledge base with text, compared to using the incomplete knowledge base only or the text only.
More broadly, we find that the resulting corpus has similar properties to the MultiNLI corpus. The top hypothesis words indicative of the class label – scored using the mutual information between each word and class in the corpus – are similar across languages, and overlap those of the MultiNLI corpus Gururangan et al. For example, a translation of at least one of the words no, not or never is among the top two cues for contradiction in all languages.
We use pretrained 300D aligned word embeddings for both x-cbow and x-bilstm and only consider the most 500,000 frequent words in the dictionary, which generally covers more than 98% of the words found in XNLI corpora. We set the number of hidden units of the BiLSTMs to 512, and use the Adam optimizer Kingma and Ba As in Conneau et al. We sample negatives randomly. When fitting the target BiLSTM encoder to the English encoder , we fine-tune the lookup table associated to the target encoder, but keep the source word embeddings fixed. The classifier is a feed-forward neural network with one hidden layer of 128 hidden units, regularized with dropout Srivastava et al. For X-BiLSTMs, we perform model selection on the XNLI validation set in each target language. For X-CBOW, we keep a validation set of parallel sentences to evaluate our alignment loss. The alignment loss requires a parallel dataset of sentences for each pair of languages, which we describe next. Fine-tuning the embeddings does not significantly impact the results, suggesting that the LSTM alone is ensuring alignment of parallel sentence embeddings. We also observe that the negative term is not critical to the performance of the model, but can lead to slight improvement in Chinese (up to 1.6%).
To investigate the effect of the word representation, we selected two popular models and tested their performances with different embeddings. helps improve the score consistently (here, when ELMo was used, no word embedding was concatenated).
The results indicate that the proposed framework outperforms all the baselines, which confirms the effectiveness of label representations learning with style correlations, which include statistical relations and knowledge relations.
To establish a strong LDA baseline, only long documents are selected for training and test in this study. The table also shows the lexicon size in the LDA and DNN modeling, which corresponds to the dimensionality of the TF feature. Note that this seemingly tricky data selection is just for building a strong LDA model for the DNN to learn, rather than intensively selecting a working scenario for the proposed method. In fact, the DNN learning works well with any LDA teacher model, and the performance of the resultant DNN largely depends on the quality of the teacher LDA.
Our ranking is better than that of all three baseline methods, as determined from a paired two-sided t-test, in both WTAction (p-values of 1.33e−47, 9.68e−29, and 2.28e−17, respectively) and SyntagNet (p-values of 8.34e−46, 4.93e−23, and 2.04e−09, respectively) datasets.
The selected pre-trained BERT-base model and pre-trained RoBERTa-base model have the lowest perplexities, which are 21.3 and 47.5. Our fine-tuned pre-trained classification-classificaion BERT and RoBERTa models outperform their counterpart baseline by about 1.7% and 1.1%, respectively. In addition, our fine-tuned pre-trained regression-classification BERT and RoBERTa models show 2.1% and 1.8% improvements over their baselines. The ensemble model with learning_rate of 1e−5 and dropout of 0.5 (E_2) achieves significantly improvement on development set. It outperforms the BERT baseline and RoBERTa baseline by 8.5% and 8.6%, respectively. As a result, we use this ensemble model as our final model and submit the prediction results to the shared task’s CodaLab page. We achieve a macro-F1 score of 90.901% on the test set and rank 36th among 85 participants in sub-task A. After the release of the gold labels, we also calculate our other models’ performance on test set
Our progressive models outperform baseline models on overwhelmingly most of the metrics, which indicates our models generate much more similar text to human. Moreover, the improvement is consistent both on unconditional generation (CNN) and conditional generation (WritingPrompts). It is worth noting that 3-steps model performs better than 2-stages model in the CNN News domain, while in the story generation domain, we don’t observe significantly improved quality by using 3-steps model. This suggests that the influence of the number of progressive steps might be different across different domains.
Besides automatic evaluation, we ask human annotators to evaluate the text generated by different models according to our score standard introduced previously. First, we can see that all models get scores higher than 3, which indicates most of the generated texts from baselines are at least moderately coherence. However, all baseline models have scores of around 3.5 while our progressive model gets a significantly higher score of 4.43, suggesting our samples are much harder to distinguish with human text.
In this section, the improvement and scalability of the proposed learning algorithms in home environments are discussed.
We observe MAEs ranging between 1.26 (worry with TFIDF) and 1.88 (sadness with POS) for the long texts, and between 1.37 (worry with POS) and 1.91 (sadness with POS) for the short texts. We furthermore observe that the models perform best in predicting the worry scores for both long and short texts. The models explain up to 16% of the variance for the emotional response variables on the long texts, but only up to 1% on Tweet-sized texts.
Overall, reg performs better than dist on verb disambiguation, while dist performs better on sentence similarity. We hypothesise the difference lies in the nature of the two tasks. The verb disambiguation task is inherently plausibility-based, because one member of the low-similarity pairs (with the non-relevant sense of the verb) is always implausible. On the other hand, both triples in the sentence similarity task tend to be highly plausible, even when their topic differs. Because dist uses a topic-based space, it may better capture these distinctions. Due to the nature of functional composition of distributional representations, where each word-type other than noun is represented by a higher-order tensor, low-dimensional representations are particularly advantageous.
We compare our results on WSJ eval92 with the original E2E LF-MMI and other SOTA end-to-end ASR systems. Note that we only use n-gram LMs for decoding, while the others (except Hadian et al. [hadian2018flat]) use more powerful neural LMs.
The results differ between the tasks, but none have an overall benefit from the open vocabulary system. Looking at the subset of sentences that contain an OOV token, the open vocabulary system delivers increased performance on the Airbnb and Greyhound tasks. These two are the most difficult apps out of the four and therefore had the most room for improvement. The United app is also all lower case and casing is an important clue for detecting proper nouns that the open vocabulary model takes advantage of.
As a baseline, we compare our model with a most recently proposed model for document-level joint entity and relation extraction: BRAN, which achieved state-of-art performance for chemical-disease relation Verga et al. When this model was originally used to test relations between all pairs of entities and properties in the entire conversation, it performed relatively poorly. Using the implementation released by the authors, the performance of BRAN was then optimized by restricting the distance between the pairs and by fine-tuning the threshold. Our proposed R-SAT model without any such constraints performs better than BRAN on both tasks by an absolute F1-score gain of about 0.20.
In the original DANN implementation JMLR:v17:15-239, Stochastic Gradient Descent (SGD) was used as the optimizer. However, in DANN+, using Adam optimizer leads to substantial jump in performance which can comfortably surpass many of the recent advanced domain adaptation methods – CMD zellinger2017central, VFAE louizos2015variational, ASym saito2017asymmetric and MT-Tri DBLP:conf/acl/PlankR18. As observed, KinGDOM outperforms DSN in all the task scenarios, indicating the efficacy of our approach. blitzer-etal-2007-biographies, in their original work, noted that domain transfer across the two groups of DVD, Books and Electronics, Kitchen is particularly challenging. Interestingly, in our results, we observe the highest gains when the source and target domains are from these separate groups (e.g., Kitchen → DVD, Kitchen → Books, Electronics → Books). Although CoCMD is a semi-supervised method, KinGDOM surpasses its performance in several of the twelve domain-pair combinations and matches its overall result without using any labelled samples from the target domain. TAT is the state-of-the-art method for unsupervised domain adaptation in the Amazon reviews dataset when used with 30,000 Bag-Of-Words (BOW) features. Interestingly, KinGDOM used with 5000 BOW features can match TAT with 30,000 BOW features and outperforms TAT by around 1.6% overall when used with the same 30,000 BOW features. The reimplementation of DANN – DANN+ with 30,000 BOW also surpasses the result of TAT by 0.5%. The results indicate that external knowledge, when added to a simple architecture such as DANN, can surpass sophisticated state-of-the-art models, such as DSN and TAT. Our primary intention to utilize DANN as the base model is to highlight the role of knowledge base infusion in domain adaptation, devoid of sophisticated models, and complex neural maneuvering. Nevertheless, the flexibility of KinGDOM allows it to be associated with advanced models too (e.g., DSN, TAT), which we believe could perform even better. We intend to analyze this in the future.
SARSA and Q-learning perform similarly. The best results are obtained with mention episodes and the RL models perform similarly to the supervised counterpart on CoNLL English, and drop up to 1 to 2 F points on OntoNotes and CoNLL Spanish respectively.
The mention words are grouped together only marginally above chance level. When words are grouped correctly, the labels assigned are correct more than half of the time. This accuracy is much lower for mentions of size 1 (11%) where we observe the assigned label to be meaningless most of the times, e.g. [[China]TIME Daily]ORG. We hypothesis this is happening because the model is using its look-ahead capabilities to assign correct labels and sees no benefit in making reasonable partial label assignments. The additional partial rewards used with the RL+partial model lead, as expected, to much more coherent sub-mention structures, both in terms of correct chunking and of labels assigned. Surprisingly though, this does not improve overall results, with RL+partial scoring ≈ 1F point bellow RL on the OntoNotes development dataset.
The document retrieval component of the baseline system returns the k nearest documents to the claim using the DrQA Chen et al. TF-IDF implementation to return the k-nearest documents. In the scenario where evidence from multiple documents is required, k must be greater than this figure. We simulate the upper bound in accuracy using an oracle 3-way RTE classifier that predicts Supported /Refuted ones correctly only if the documents containing the supporting/refuting evidence are returned by document retrieval and always predicts NotEnoughInfo instances correctly independently of the evidence. Mirroring document retrieval, we extract the top l-most similar sentences from the k-most relevant documents using TF-IDF vector similarity. We modified document retrieval component of DrQA After applying the sentence selection component, 44.22% of claims can be fully supported using the extracted sentences with DrQA and only 34.03% with NLTK. This would yield oracle accuracies of 62.81% and 56.02% respectively.
We compare our proposed MT-BioNER model with state-of-the-art BioNER systems such as the single task LSTM-CRF model of Habibi et al. (BiLSTM), the multi-task model of Wang et al. (MTM-CW), and transfer learning approach of Sachan et al. (BiLM-NER). BioBERT model is used as the shared layers for these results. From the results, we see that our approach trained on three datasets obtains the maximum recall and F1 scores. We should mention that our model is based on multi-task approach and achieves better performance even compared to Sachan et al. single task transfer learning approach (BiLM-NER) in which the whole network is only trained on a single dataset. Moreover, our model trained on four datasets performs better on recall and F1-score compared to MTM-CW approach which is a multi-task model. Another interesting result is that our model achieves the highest recall among all the other approaches. But it has a lack in precision score. To further improve the precision, we can add dictionaries as features which could be an interesting future work. Also, it shows the potential capability of BERT language model to provide the semantic feature representations for the multi-task NER tasks.
To assess relative performance of feature representations, we also include performance metrics of our models without MWEs. Because this is a multilabel classification task we use macro-averaging to compute precision, recall, and F1 scores for each paragraph in the testing set. In identifying domains individually, our models achieved the highest per-domain scores on Substance (F1 ≈ 0.8) and the lowest scores on Interpersonal and Mood (F1 ≈ 0.5). We observe a consistency in per-domain performance rankings between our MLP and RBF models. Despite prior research indicating that similar classification tasks to ours are more effectively performed by RBF networks Scheirer et al. (i.e. k-means and width calculations) involved. Given that similarity is computed using Euclidean distance in an RBF network, it is difficult to accurately classify paragraphs that fall in regions occupied by multiple risk factor domain clusters since prototype centroids from the risk factor domains will overlap and be less differentiable.
We train the models on the released public FCE dataset which contains 1,141 scripts for training and 97 scripts for testing. In order to examine the effects of training with extra data, we conduct experiments where we augment the public set with additional FCE scripts and refer to this extended version as FCEext, which contains 9,822 scripts. We report the results of both datasets on the released test set. The public FCE dataset is divided into 1,061 scripts for training and 80 for development while for FCEext, 8,842 scripts are used for training and 980 are held out for development. Training. Hyperparameter tuning is done for each model separately. The SSWE, ESWE and ECSWE models are initialized with GloVe (dwrd=50) vectors, trained for 20 epochs and the learning rate is set to 0.01. For SSWE, α is set to 0.1, batch size to 128, the number of randomly generated counterparts per ngram to 20 and the size of hidden layer to 100. All the networks are optimized using Stochastic Gradient Descent (SGD). The AA system is regularized with L2 regularization with rate = 0.0001 and trained for 50 epochs during which performance is monitored on the dev sets. Finally, the AA model with the best mean square error over the dev sets is selected.
We conduct further analysis to the scores predicted by AA-specific embeddings by investigating the ability of the ESWE and SSWE models to detect errors in text. We run each model for 20 epochs on the public FCE (ngram size = 3) and FCEext (ngram size = 9) training sets, then test the models on the respective dev sets and examine the output predictions. For simplicity, we assign a binary true score for each ngram with a zero value if it contains any errors and one otherwise. ESWE predicts a score ∈[0,1] for each ngram indicating its correctness and hence could be used directly in the evaluation. On the other hand, SSWE predicts two scores for each ngram: correct score that it maximizes in comparison to the noisy ngrams and script score that should be high for good ngrams that occur in highly-graded scripts. The two scores are hence expected to be high for high-quality ngrams and low otherwise, which suggests that they can be used as proxies for error detection. We calculate the ngram predicted score of the SSWE model as a weighted sum of the correct and script scores, similar to its loss function We calculate the average precision (AP) between the true scores and predicted ones with respect to the error representing class (true score =0) and compare it to a random baseline, where random probability scores are generated. performance is similar to the random baseline. This result is expected since the ESWE model is trained to predict actual errors, yet an empirical verification was required. We conclude from this analysis that tuning the embeddings based on training writing errors increases their sensitivity to unseen errors which is key for learners’ data assessment and yields better performance than comparable pre-training approaches.
To evaluate the effectiveness of our proposed model on recognizing discontinuous mentions, we follow the evaluation approach in Muis and Lu Note that sentences in the former setting usually contain continuous mentions as well, including those involved in overlapping structure (e.g., ‘muscle pain’ in the sentence ‘muscle pain and fatigue’). Therefore, the flat model, which cannot predict any discontinuous mentions, still achieves 38% F1 on average when evaluated on these sentences with at least one discontinuous mention, but 0% F1 when evaluated on discontinuous mentions only.
The original work by Sproat and Jaitly uses 1.1 billion words for English text and 290 words for Russian text. The dataset is derived from Wikipedia regions which could be decoded as UTF8. The text is then divided into sentences and through the Google TTS system’s Kestrel text normalization system to produce the normalized version of that text. A snippet is shown in the figure 1 . As described in (Ebden and Sproat, 2014), Kestrel’s verbalizations are produced by first tokenizing the input and classifying the tokens, and then verbalizing each token according to its semiotic class. The majority of the rules are hand-built using the Thrax finite-state grammar development system (Roark et al., 2012). Most ordinary words are of course left alone (represented here as ¡self¿), and punctuation symbols are mostly transduced to ¡sil¿ (for ”silence”).
Dataset and Corpus The most widely used dataset for AES is the Automated Student Assessment Prize (ASAP) Generally, ASAP is composed of 8 prompts and 12976 essays, which are written by students from Grade 7 to Grade 10. After analyzing the ASAP dataset, we note that the samples whose scores are much lower than the average usually are very short essays. Besides, a lot of essays have grammar errors and spell errors. From our perspective, these length-based features and errors are difficult for the end-to-end models to make full use of. However, according to the second stage learning, TSLF-ALL can take advantage of not only the deep-encoded features generated by end-to-end models but also the handcrafted features, which accounts for the impressive achievements of TSLF-ALL. We speculate that the improvements of TSTF-2 may credit to the introduction of GEC system and the advanced ability of the tree boosting regression model.
Adversarial Essays Generally, there are three possible ways to add adversarial samples to original ASAP dataset: (1) We only add the permuted essays to ASAP. (2) We only add the prompt-irrelevant essays to ASAP. (3) We add both permuted essays and prompt-irrelevant essays to ASAP. In condition 1, for every essay in ASAP dataset, we permute sentences to generate the corresponding adversarial essay. In condition 2, we randomly select the same number of essays from other prompt set to generate prompt-irrelevant samples. In condition 3, we create the same number of permuted and prompt-irrelevant essays respectively and make sure that the whole number of negative samples are equal to the number of gold essays in a prompt. However, TSLF-ALL is much more robust because of the addition of coherence score and prompt-relevant score. Besides, we find that the permuted essays are much more difficult to be detect than the prompt-irrelevant essays. Actually, in experiments, the prompt-relevant model only needs 3 epochs to converge while the coherence model converges after 120 epochs. Obviously, there are massive permuted samples for a specific essay, which are added to the training batch in every epoch. From our perspective, the much more training epochs are caused by the large number of permuted essays.
We successfully trained models for all languages except Greek for Tacotron. We believe it is due to the small data size of Greek (∼4 hours of audio). Although we successfully trained DCTTS on Greek, the samples were much worse than those from other languages. For naturalness, DCTTS is statistically more natural than Tacotron for German, French, and Spanish. For pronunciation accuracy, we find that DCTTS and Tacotron are somewhat similar.
We compare the performances of the models on varying amounts of training data from each domain. This involves using all available out of domain data and varying the amount of training data for the target domain. To avoid performance variations due to the small sample sizes, the performance was averaged over 10 runs with training samples drawn from different parts of the domain dataset. For every domain, 20% of the training examples were set aside for the dev set. Since we were evaluating with varying amounts of training data for each domain, the dev set was different for each data-point, corresponding to the bottom 20% of the training samples. For example, if the training set consisted of 100 samples from Domain A and 20 samples from Domain B, dev set A would consist of 20 samples from Domain A and dev set B would be comprised of 4 samples from Domain B. The performance on these dev sets was evaluated separately and averaged weighted by the log of the number of training samples. This weighted average was used to tune the model hyper-parameters. We used a logarithmic combination since it struck a good balance between noisy evaluations on domains with small dev sets and over-tuning to the domains with larger dev sets. When no in-domain data is available, the concept tagging model is able to achieve reasonable bootstrap performance for most domains. Even when more data becomes available the model beats the single task model by significant margins and performs better than or on par with the multi-task baseline for most points on the learning curves.
We report the standard macro F1 and exact-match (EM) metrics in question answering, and also EM consistency, the percentage of contrast question sets for which a model’s predictions match exactly to all questions in a group Gardner et al. We see warm-up questions are easier than user-provided ones because warm-up questions focus on easier phenomena of past/ongoing/future events. In addition, RoBERTa-large is expectedly the best system, but still far behind human performance, trailing by about 30% in EM.
t-test results on BLEU scores of the two model show that their difference is significant (p<1×10−5).
Coverage. We compare the coverage of the databases in terms of the ratio of tables and attributes which appear in the queries. Question Complexity. The lexical complexity of the NL questions is measured in terms of mean-segmental token-type-ratio (MSTTR) Covington and McFall The MSTTR is computed over text segments of equal length, in order to avoid biases due to different lengths within the corpora. First, note that the average length of the questions in all three corpora is approximately the same, between 10.6-13.6 tokens on average. Thus, our approach seems to avoid trivial or monotonous questions, which also matches with our impression from manual inspection. On the other hand, the lexical complexity is higher in LC-QuaD 2.0, which is due to the open domain nature of the dataset.
To reduce the feature and label space, we first removed accents and punctuation and lowercased all words in the description and ruling. Further pre-processing was needed to reduce the label space for each task. For task 1, we kept in the corpus all entries corresponding to the labels that had over 200 examples. This left us with 8 law area classes.
As the amount of labeled data is very small, the problem of overfitting is inevitable. However, our approach has good performance in generalization. It is known that conventional models (such as Transformer) typically address the problem of overfitting by reducing the number of variables of the network. This method is confirmed by the results of Transformer (Small) and Transformer (Regular) in the AE task, as the former outperforms the latter. However, this method weakens the network and restricts the final performance. Nevertheless, SAMIE alleviates the problem of overfitting. Specifically, SAMIE (Regular) outperforms SAMIE (Small) on all metrics, while both still beat their respective Transformer counterparts. These results reveal that SAMIE works better with more powerful neural network.
The decoder consists of 2 layers of LSTMs with 1024 output units, 1 attention layer with 128 hidden units, and 1 fully connected layer. The model contains 184M parameters. If the DSNN model significantly degraded the model quality, it would not be useful especially in real-world scenarios when quality is prioritized. We see that DSNN’s edge pruning approach significantly outperforms SNN’s node pruning approach.
Similar to previous model pruning work This pretraining stage uniformly improves the performance by up to 0.8% WER. This further confirms the importance of initialization for sparse model training: it helps even when the zero sparsity model is present during the DSNN training algorithm. The gradient accumulation technique also consistently improves the model performance across sparsity levels by stabilizing the training process.
S4SS3SSS0Px1 Impact of Hop Numbers. Here we analyze the effects of hop numbers on the accuracy of TMN. As can be seen, generally, TMN with 5 hops achieves the best accuracy on most datasets except for Snippets dataset. We also observe that, although within a particular range, more hops can produce better accuracy, the increasing trends are not always monotonic. For example, TMN-6H always exhibits lower accuracy than TMN-5H. This observation implies that the overall representation ability of TMN is enhanced as the increasing complexity of the model via combining more hops. However, this enhancement will reach saturation when the hop number exceeds a threshold, which is 5 hops for most datasets in our experiment.
Both datasets are quite different. There were some issues during aspect annotation, i.e., it was unclear if a noun or noun phrase was used as the aspect term or if it referred to the entity being reviewed as the whole [Pontiki2014]. For example in This place is awesome, the word place most likely refers to the restaurant as the whole. Hence, it should not be tagged as an aspect term. In Cozy place and good pizza, it probably refers to the ambience of the restaurant. In such cases, an additional review context would help to disambiguate it. Moreover, the laptop reviews often rate laptops as such without any particular aspects in mind. This domain often contains implicit aspects expressed by adjectives, e.g., expensive, heavy, rather than using explicit terms, e.g., cost, weight. We must remember that in both datasets, the annotators were instructed to tag only explicit aspects, thus, adjectives implicitly referring to aspects were discarded. The restaurant dataset contains many more aspect terms in training and in testing subsets (see Tab. The majority of the aspects in both datasets are single-words, Tab. Note that the laptop dataset consists of proportionally more multi-word aspects than the restaurants dataset. It could be one of the reasons why the average accuracy for the laptops is commonly lower than for restaurants.
We obtained the best F1-measures using Glove.840B (80.13% for Laptops) and fastText (85.69% for Restaurants) pre-trained word embeddings extended with character embedding using BiLSTM and CRF layer. It can be seen that all four of our models using either Glove.840B or fastText word embeddings proved to be better than any other baseline model. It is worth mentioning that our models achieved better performance than the SemEval 2014 winners - DLIREC and IHS R&D. Summing up, our models’ performance was superior in comparison to state-of-the-art models.
6.2.2 Results DrQA is one of the best performing models on SQuAD with an F1 of 78.8 and EM of 69.5. The relatively low performance of the models on emrQA (60.6 F1 and 59.2 EM) shows that QA on EMRs is a complex task and offers new challenges to existing QA models.
Result analysis Furthermore, to mine explanations, models that consider context information tend to perform better. We found that BERT over sentences is 2% more accurate than BERT over opinion phrases only. Lastly, leveraging pre-trained model can further improve the performance: by replacing the embedding layer with BERT, the accuracy is further improved by 4%.
Result analysis In addition, we confirm that our model significantly benefits from the additional supervision from opinion and explanation mining as our method significantly improves the performance compared to ABAE, which does not use the supervision.
It assigns the correct class label for an instance if at least one of the the classifiers produces the correct label for that instance. This approach establishes the potential or theoretical upper limit performance for a given dataset. Similar analysis using oracle classifiers have been previously applied to estimate the theoretical upper bound of shared tasks datasets in Native Language Identification Malmasi et al. Goutte et al. We start by investigating the efficacy of our features for this task. We fist train a single classifier, with each of them using a type of feature. Subsequently we also train a single model combining all of our features into single space. These are compared against the majority class baseline, as well as the oracle.
The clear winner among the unsupervised models is [H]. While the paper focuses on the role of learned feature transformations—and demonstrates that they improve the evaluation—these are unlikely to be solely responsible for the very good performance of the system. The baseline system described in the paper, with no transformations applied, consisting of posteriorgrams from a frame-wise DPGMM clustering, still performs extremely well. Although it was not evaluated on the surprise languages, its average ABX score on the development languages (9.7) is still better that of the second-ranked system [C2] (10.7), from Chen et al. The systems presented by Heck et al. all have in common monolingual training, direct evaluation of GMM posteriorgrams, and the fact that they use PLP features as input, rather than MFCCs.
The clear winner among the unsupervised models is [H]. While the paper focuses on the role of learned feature transformations—and demonstrates that they improve the evaluation—these are unlikely to be solely responsible for the very good performance of the system. The baseline system described in the paper, with no transformations applied, consisting of posteriorgrams from a frame-wise DPGMM clustering, still performs extremely well. Although it was not evaluated on the surprise languages, its average ABX score on the development languages (9.7) is still better that of the second-ranked system [C2] (10.7), from Chen et al. The systems presented by Heck et al. all have in common monolingual training, direct evaluation of GMM posteriorgrams, and the fact that they use PLP features as input, rather than MFCCs.
Finally, we compare the performance of RANs to a variety of LSTM variants on the character-based language modeling task Text8 While the different results vary both in model and in hyperparameters, the same trend in which RANs perform similarly to LSTMs emerges yet again.
For BLEU and NIST higher score is considered as better and for mWER and the mPER lower score is desirable. The impact of improved syntactic reordering can be seen as the BLEU and NIST scores have increased, whereas mWER and mPER scores have decreased.
Looking at the overall BLEU scores, we observe that baseline performance on the idiom-specific test set is lower than on the union of the standard test sets (WMT 2008-2016). We can also see that the performance gap is not as pronounced for PBMT systems, suggesting that phrase-based models are capable of memorizing the idiom phrases to some extent.
Results. We first train all the embedding models on the corresponding corpus (except BERT which we take its pre-trained model and fine-tune it on the corpus), and use the trained embedding as the word representation to WeSTClass Discussions. (1) Unsupervised embeddings (Word2Vec, GloVe and fastText) do not really have notable differences as word representations to WeSTClass; (2) Despite its great effectiveness as a pre-trained deep language model for supervised tasks, BERT is not suitable for classification without sufficient training data, probably because BERT embedding has higher dimensionality (even the base model of BERT is 768-dimensional) which might require stronger supervision signals to tune; (3) CatE outperforms all unsupervised embeddings on NYT-Location and Yelp-Food Type and Yelp-Sentiment categories by a large margin, and have marginal advantage on NYT-Topic. This is probably because different locations (e.g., “Canada” vs. “The United States”), food types (e.g., “burgers” vs. “pizza”), and sentiment polarities (e.g., “good” vs. “bad”) can have highly similar local contexts, and are more difficult to be differentiated than themes. CatE explicitly regularizes the embedding space for the specific categories and becomes especially advantageous when the given category names are semantically similar.
Statistically significant results obtained for distance from RNNG−comp in the P600 region and for surprisal for RNNG in the ANT region. No significant results were observed in the N400 region. Choe and Charniak’s “parsing as language modeling” scheme potentially could explain the P600-like wave, but it would not account for the earlier peak.
and context-aware Zhang et al. al. For TED dataset, the performance of our model greatly exceeds that of all other baselines, and is better than Miculicich et al. with a gain of +0.59 BLEU and +0.61 Meteor. For Europarl dataset, our model got improvements with a gain of +0.07 on BLEU metric, but the Meteor score is +0.64 higher than Maruf et al.
generated types to three coarse-grained types. In order to compare identification results, we design a simple mention boundary detection approach based on capitalization features and part-of-speech features. We compare the performance of our system with both perfect AMR and system AMR annotations with the performance of NER and FIGER. Compared with FIGER on coarse-grained level, our approach with system AMR and perfect AMR also achieved better results. The number of clusters, to some extent, can reflect the granularity of fine-grained typing. Although we can not directly map the granularity of FIGER to our system, considering the classification results of FIGER are highly biased toward a certain subset of types (about 60 types), our approach with both system AMR and perfect AMR should slightly outperform FIGER, which is trained based on 2 million labeled sentences.
More generally, inter-profile dissimilarities between authors that write in the same genre tend to be smaller than between authors that write in different genres. All profiles contain 100,000 words formed by randomly picking 10 extracts of 10,000 words. Marlowe never wrote a comedy and mainly focused on histories – Edward II, The Massacre at Paris – and tragedies – The Jew of Malta, Dido –, while the majority of Chapman’s plays are comedies – All Fools, May Day. Genre choice impacts the inter-profile dissimilarity since the comedy profile of Shakespeare is closer to Chapman than to Marlowe and vice versa for the history profile of Shakespeare. This points towards the conclusion that the identity of the author is the main determinant of the writing style but that the genre of the text being written also contributes to the word choice. In general, two texts of the same author but different genres are more similar than two texts of the same genre but different authors which, in turn, are more similar than two texts of different authors and genres.
The attribution between hybrid profiles is not always accurate. However, the smallest entropy in the whole table is achieved by the hybrid profile composed by Jonson and Middleton.
We then repeat the random choice of authors 100 times and average the error rate. For each of the methods based on function word frequencies, we pick the set of parameters and preprocessing that minimize the attribution error rate. E.g., for SVM the error is minimized when considering a polynomial kernel of degree 3 and normalizing the frequencies by text length. For the nearest neighbors method we consider two strategies based on one (1-NN) and three (3-NN) nearest neighbors as given by the l2 metric in Euclidean space. Also, for decision trees we consider two types of split criteria: the Gini Diversity Index (DT-gdi) and the cross-entropy (DT-ce) For binary attributions, naive Bayes and SVM achieve error rates of 2.6% and 2.7% respectively and, thus, outperform nearest neighbors and decision trees. However, WANs outperform the aforementioned methods by obtaining an error rate of 1.6%. This implies a reduction of 38% in the error rate. For 6 authors, WANs achieve an error rate of 5.3% that outperform SVMs achieving 7.9% entailing a 33% reduction. This trend is consistent across different number of candidate authors, with WANs achieving an average error reduction of 29% compared with the best traditional machine learning method. More important than the fact that WANs tend to outperform methods based on word frequencies, is the fact that they carry different stylometric information. Thus, we can combine both methodologies to further increase attribution accuracy. best performing frequency based methods, namely, naive Bayes and SVMs. The error rates are consistently smaller than those achieved by WANs and, hence, by the other frequency based methods as well. E.g., for attributions among four authors, voting achieves an error of 3.3% compared to an error of 4.6% of WANs. This corresponds to a 28% reduction in error. Averaging among attributions for different number of candidate authors, majority voting entails a reduction of 30% compared with WANs. The combination of WANs and function word frequencies halves the attribution error rate with respect to the current state of the art.
Our primary findings are that the abstract-only task is easier and sixteen negative samples perform better than four. Otherwise results follow a similar trend to the full-document task.
Our primary findings are that the abstract-only task is easier and eight negative samples perform better than four. Otherwise results follow a similar trend to the full-document task.
Each number in the table is the average performance in 10-fold cross-validation tests. From the table we can see that, when being used together with the argument-based features, either of the four baseline features enjoys a performance boost in terms of all metrics we consider. To be more specific, in terms of accuracy, precision, recall, F1 and AUC, the average improvement for the baseline features are 4.33%, 10.30%, 4.32%, 11.01% and 10.40%, respectively. However, we observe that the precision of UGR+AF, although gives the second highest score among all feature combinations, is lower than that of UGR; we leave it for future work. Also, we notice that when using the argument-based features alone, its performance (in terms of Precision, F1 and AUC) is superior to those of STR, GALC and INQUIRER, and is only inferior to UGR. However, a major drawback of the UGR feature is its huge and document-dependent dimensionality, while the dimensionality of argument-based features is fixed, regardless of the size of the input documents. Moreover, the UGR features are sparse and problematic in online learning. To summarise, compared with the other state-of-the-art features, argument-based features are effective in identifying helpful reviews, and can represent some complementary information that cannot be represented in other features.
For most of the representations, according to their nearest neighbors, we got reasonable hypernyms. However, there are also some unexpected cases from the result based on the word vectors released by \newcitehuang2012improving, while no such cases are found in the vectors released by \newciteneelakantan2015efficient. For example, we got [whole.n.02] as the hypernym of the three sample words (which seems too general since whole can be the hypernym of nearly all entities), and [person.n.01] as a hypernym of ROCK (which seems not very reasonable according to the nearest neighbors). By intuition, we suggest that is because of the quality of the word embeddings. Possibly, the level of confidence to extract domains and hypernyms for each sense could be a metric for evaluating the quality of word embeddings. From this point of view, the word embeddings released by \newciteneelakantan2015efficient are also with higher quality.
On the test dataset with seen noise, EHNet consistently outperforms all the competitors with a large margin. Specifically, EHNet is able to improve the perceptual quality (PESQ measure) by 0.6 without hurting the recognition accuracy. This is very surprising as we treat the underlying ASR system as a black box and do not fine-tune it during the experiment. As a comparison, while all the other methods can boost the SNR ratio, they often decrease the recognition accuracy. More surprisingly, EHNet also generalizes to unseen noise as well, and it even achieves a larger boost (0.64) on the perceptual quality while at the same time increases the recognition accuracy. To have a better understanding on the experimental result, we do a case study by visualizing the denoised spectrograms from different models. As shown in Fig. By not removing much noise, it also keeps most of the real signals in the speech. On the other hand, although DNN-based approaches do a good job in removing the background noise, they also tend to remove the real speech signals from the spectrogram. RNN does a better job than DNN, but also fails to keep the real signals in low frequency bins. As a comparison, EHNet finds a good tradeoff between removing background noise and preserving the real speech signals: it is better than DNN/RNN in preserving high/low-frequency bins and it is superior than MS in removing background noise. It is also easy to see that EHNet produces denoised spectrogram that is most close to the ground-truth clean spectrogram.
Measuring individual differences in implicit cognition: the implicit association test.. Journal of personality and social psychology 74 (6), pp. 1464. In this section, we first construct all CEAT in the main paper (C1-C10,I1-I4) with sample size N=1,000 to provide a comparison of results with different sample sizes. We replicate these results with N=1,000 instead of using the original N=10,000 to show that even with N=1,000, we get valid results. Accordingly, we proceed to calculate all types of biases associated with intersectional groups based on the attributes used in original WEAT. We notice that there are five tests which are significant with sample size N=10,000 but insignificant with sample size N=1,000. They are C10 with Bert, C4 with GPT, C7 with GPT-2, I3 with GPT-2 and I4 with GPT-2. We also notice that CES of same test can be different with different sample size but all differences are smaller than 0.1.
In order to choose the segmentation process for our primary system among these two approaches, we carried out experiments on the tst-COMMON data from the MuST-C corpus. For these experiments, we applied a preliminary version of our end-to-end system, trained on the MuST-C training data to translate speech into lower-case text.
Since there are multiple actions that are appropriate for a given dialog context, we use per-utterance Precision@K as performance metric. We also report f1-score for OOD detection to measure the balance between precision and recall. The performances of HCN on Test-OOD are about 15 points down on average from those on Test, showing the detrimental impact of OOD utterances to such models only trained on in-domain training data. AE-HCN(-CNN) outperforms HCN on Test-OOD by a large margin about 17(20) points on average while keeping the minimum performance trade-off compared to Test. Interestingly, AE-HCN-CNN has even better performance than HCN on Test, indicating that, with the CNN encoder, counterfeit OOD augmentation acts as an effective regularization. In contrast, AE-HCN-Indep failed to robustly detect OOD utterances, resulting in much lower numbers for both metrics on Test-OOD as well as hurting the performance on Test. This result indicates two crucial points: 1) the inherent difficulty of finding an appropriate threshold value without actually seeing OOD data; 2) the limitation of the models which do not consider context. If OOD utterances had been known a priori, the threshold should have been set to a much higher value than the maximum reconstruction score of IND training data (6.16 in this case).
The first 15 rows report the performance of the models that do not exploit information about alternative paths between head and tail entities. The next 5 rows report results of the models that exploit information about relation paths. The last 3 rows present results for the models which make use of textual mentions derived from a large external corpus. It is clear that the models with the additional external corpus information obtained best results. In future work we plan to extend the STransE model to incorporate such additional information. In terms of models not exploiting path information or external information, the STransE model produces the highest filtered mean rank on WN18 and the highest filtered Hits@10 and mean reciprocal rank on FB15k. Compared to the closely related models SE, TransE, TransR, CTransR, TransD and TranSparse, our STransE model does better than these models on both WN18 and FB15k.
If ah<1.5 and at<1.5, then r is labeled 1-1. If ah≥1.5 and at<1.5, then r is labeled M-1. If ah<1.5 and at≥1.5, then r is labeled as 1-M. If ah≥1.5 and at≥1.5, then r is labeled as M-M. 1.4%, 8.9%, 14.6% and 75.1% of the test triples belong to a relation type classified as 1-1, 1-M, M-1 and M-M, respectively. However, STransE obtains 2.5% higher Hits@10 result than TranSparse for M-1. In addition, STransE also performs better than TransD for 1-M and M-1 relation categories. We believe the improved performance of the STransE model is due to its use of full matrices, rather than just projection vectors as in TransD. This permits STransE to model diverse and complex relation categories (such as 1-M, M-1 and especially M-M) better than TransD and other similiar models. However, STransE is not as good as TransD for the 1-1 relations. Perhaps the extra parameters in STransE hurt performance in this case (note that 1-1 relations are relatively rare, so STransE does better overall).
The BoW and structural-based features are relevant due to the presence of three tweets in a triplet (more words than in a text of a single tweet) respectively in Naïve Bayes and SVM. The community-context-based feature is significant especially in Decision Tree and Random Forest. In addition, all the best feature combinations for each classifier contain the community-context-based feature. The diachronic-evolution-context-based feature shows its relevance only in SVM. Favg decreases of 14.6% and 0.12% removing singularly community-context-based and diachronic-evolution-context-based features respectively. Removing only the common-knowledge-context-based feature Favg improved of 0.49%. Therefore, the common-knowledge-context-based feature does not improve Favg and the diachronic-evolution-context-based feature is not decisive in results. Using the whole group of context-based features improves Favg more than using only the community-context-based features (16.78%).
We experimented with the best features sets obtained in the previous experiment over the three different temporal intervals selected. We decided to carry out this experiment since we observed that the agreement between the annotators varies over the temporal intervals, in particular we observed a progressive decrease of the agreement (93.72 in the RD interval; 90.61 in the next OD interval; 88.78 in the last APF interval ). We hypothesized that the proposed features could detect cues which influence annotators’ agreement over the time windows.
4.2.2 Final datasets and cross validation For each dataset, we use the original and obfuscated/evaded documents by 5 (out of 10) authors for training and the documents by the remaining 5 authors for testing. An important decision to make here is with regards to the distribution of original documents versus obfuscated (or evaded) ones in our training and testing set. We have no real world scenario to mimic in that we have not encountered any real world use of automated obfuscators and their outputs. Therefore we make the datasets under a reasonable assumption that original documents are in the vast majority. Thus we create train and test datasets with roughly 85% - 90% original documents. We do this by adding more original documents from each data collection to achieve these proportions. Overall, we ensure that each author’s documents (of any version) are all either in the train or in the test set. We see that obfuscation detection is easier in EBG than in BLOG with median EBG F1 scores being significantly better than median BLOG F1 scores (notches do not overlap Krzywinski and Altman This can be explained by the fact that EBG contains scholarly articles that are relatively more consistent in their smoothness than blogs. This likely makes it easier to pick up on the difference in smoothness caused by obfuscated documents in EBG than in BLOG. We can also see that evaded documents achieve a higher maximum F1 score than obfuscated documents. However, we also see that F1 scores for evaded datasets are less stable (greater box size) than obfuscated datasets.
Performance evaluation: In terms of architecture selection, instead of choosing randomly across 160 architectures, we make the following assumption. We assume that the adversary is knowledgeable about the various choices, tests these alternatives and employs the best configuration. Our best BERT and GPT2 combinations outperform all other methods across each of the four datasets in F1 score and recall. Along with (GLTR + SVM) these achieve the best precision for the EBG datasets. In BLOG obfuscated, GLTR based method achieves the highest precision whereas in BLOG evaded both the GLTR based method and character trigrams method top the chart - however in each case with a sizeable penalty paid in recall and therefore in F1 score. In summary, we see that using the best of methods the adversary can detect evaded and obfuscated documents with F1 score of 0.77 or higher (average 0.87 across datasets) which indicates that the tested state-of-the-art obfuscators are far from stealthy. In summary we conclude that BERT with probabilities is a good choice for dimension 1. (We remind the reader that in contrast, in the area of synthetic text detection Gehrmann et al. Image based features are a clear winner in dimension 2 while KNN and ANN are the best candidates for dimension 3.
Compare to state-of-the-art results, our model’s performance is relatively low. We think, the major reason is, previous models trained for adhering image-text pair and pushing different image-text pair in multi-modal space and recall@K evaluate query’s pair appeared or not in retrieval result. So we think, this metric is not fully appropriate to assess search quality. But for comparison, we also did recall@K experiment.
Act-VRNN improves task completion (measured by Entity-F1 and Success) while requiring less cost (measured by Turns). For example, Act-VRNN (81.8) outperforms SS-GDPL (60.4) by 35.4% under Success when having 10% fully annotated dialogues, and requires the fewest turns. Meanwhile, we find that both action learning and dynamics model are essential to the superiority of Act-VRNN. For example, Act-VRNN achieves 19.8% and 11.2% improvements over SS-VRNN and Act-GDPL, respectively, under Success when having 20% fully annotated dialogues. This validates that the learned action embeddings well capture similarities among actions, and VRNN is able to exploit such similarities for reward estimation. Semi-Supervised Policy Learning Results (DF, DP, and DU)
We consider two settings: (1) having fully labeled and unlabeled dialogues, i.e., DF + DU; (2) having all three types of dialogues , i.e., DF + DP +DU. We can see that Act-VRNN significantly outperforms the baselines in both settings. For example, in setting DF + DU, Act-VRNN outperforms SS-GDPL by 43% and 44% under Entity-F1 and Success, respectively. Similar results are also observed in setting DF + DP +DU. We further find that SS-VRNN outperforms Act-GDPL in these two settings while the results are opposite in setting DF + DP, and we will conduct a detailed discussion in the following section. By comparing results of Act-VRNN and baselines in these two settings, we can see that Act-VRNN can better exploit the additional partially labeled dialogues. For example, SS-GDPL only achieves 2.3% under Success while Act-VRNN achieves more than 5%.
We observe that humans have absolute advantages over machines in all answer types and reasoning phenomena. However, humans exhibit different capabilities on answering different questions. They perform worse on answering paraphrasing questions, questions with description answers and questions requiring reading comprehension skills of multi-sentence reasoning than answering other questions.
We find that the sampling method significantly degrades the model performance, as it introduces more noise than other approaches, demonstrating that noise can have a negative impact in domain adaptation settings. The conclusion is similar to the findings in low-resource settings \newciteedunov2018understanding. In addition, we find that our weighting strategies are more beneficial in noisy settings.
To measure whether the proposed method in this study (BSDAR) can overcome the algorithm bias to shorter sequences, we compare the Seq2Seq decoding performance based on BSDAR with Tf-Idf unsupervised keyword extractor in a subset with longer target sequences (n−grams, n>3). Intuitively, this subset introduces challenges for both neural network and non-neural network (Tf-Idf) approaches, since both models may suffer with sequence length bias issues, resulting the prediction with shorter sequence length. We also show that BSDAR can overcome the diversity issue of beam decoding algorithm. Furthermore, as compared to the standard beam (BS) and heuristic beam (BS++), BSDAR shows a reasonably better decoding performance for recalling uni-gram (R(1)), bi-gram (R(2)), and n−gram references (R(n),n≥3).
While COPA and CommonsenseQA have a hypothesis only score close to the random baseline, the score of both Swag and HellaSwag are significantly higher than their random baseline (more than twice). This confirms the study from Zellers et al. Our results show that RoBERTaLARGE can leverage these distributional biases without the fine-tuning phase. We argue that the human-written pre-training corpora of RoBERTa biases it to give better score to human-written language rather than model-generated sentences. As shown in Holtzman et al. , there is indeed still a strong distributional differences between human text and machine text. Furthermore, our result also highlights that HellaSwag still exhibits a strong bias due to its generation scheme when evaluated with RoBERTaLARGE.
We see that compared to a strong rule-based system, our proposed approach gives significant gains in accuracy. Our proposed encoder-decoder improves upon the strong rule based baseline. Adding word attention helps improve the precision and F1 but at the cost of recall; the results are significant compared to the system without attention. Intuitively, the slot value matching referring tokens in the dialog turn indicates that it is relevant to the conversation. The word attention model captures this intuition as part of the model learning process. This alleviates the need to explicitly define semantic type similarity features, and detecting anaphoric mentions like we do in the rule based system. Adding stream attention improves recall, but the overall F1 degrades. Stream attention, which helps isolate user and system turns did not help; we speculate that the distance feature already captures this, and there is insufficient data to train this appropriately.
We do not claim to be solving DSTC2 but only use this dataset as a comparison of task complexity - the DSTC2 task is relatively simple as evidenced by the naive baseline having a high F1 score on this task, but very low on our commercial assistant task.
Subjective Evaluation We also conduct a subjective evaluation to validate the benefit of incorporating coverage. Two human evaluators are asked to evaluate the translations of 200 source sentences randomly sampled from the test sets without knowing from which system a translation is selected. This is mainly due to the serious under-translation problems on long sentences that consist of several sub-sentences, in which some sub-sentences are completely ignored. Incorporating coverage significantly alleviates these problems, and reduces 33.2% and 40.0% of under-translation and over-translation errors respectively. Benefiting from this, coverage model improves both translation adequacy and fluency by around 0.2 points.
We find that coverage information improves attention model as expected by maintaining an annotation summarizing attention history on each source word. More specifically, linguistic coverage with fertility significantly reduces alignment errors under both metrics, in which fertility plays an important role. NN-based coverages, however, does not significantly reduce alignment errors until increasing the coverage dimension from 1 to 10. It indicates that NN-based models need slightly more dimensions to encode the coverage information.
The proposed method is evaluated and compared with other existing models on the MS-COCO dataset. For the existing methods, both recent best performing methods and the baseline models are chosen and their results are directly taken from the existing literature. The table columns present scores for the metrics BLEU-4 (B4) to BLEU-1 (B1), METEOR (M), CIDEr(C) and ROUGE-L (R). In addition, without introducing any external memory cell, our model is generating relatively better captions than the model proposed by Chen et al. This shows that making deeper stacked decoder structures empowers the RNNs to memorize longer history of the generated words’ information. The proposed method specially outperforms the existing models based on traditional RNNs (without any external memory cell) with respect to BLEU-4 and CIDEr factors measuring the similarity of the same 4-grams and the longest weighted sub-sequence of the same words in suggested and reference captions, respectively. This means our model can generate captions similar to reference human generated image descriptions that are longer than those generated by other methods. It shows that substituting the typical prediction problem at each step to generate caption words with a regression problem resolves the problem of long-term dependencies in the encoder-decoder based models.
As expected, the first iteration yields the most new sentences. Unfortunately, the number of newly discovered hosts and sentences decreases exponentially as the system runs, dropping to 20K sentences on the third iteration. This result emphasizes the fact that the amount of GSW on the web is very limited.
As expected, each model performs better on the test set they have been trained on. When applied to a different test set, both see an increase in perplexity. However, the Leipzig model seems to have more trouble generalizing: its perplexity nearly doubles on the SwissCrawl test set and raises by twenty on the combined test set.
In the ‘standard’ setting we simply train on the pre-defined training set, achieving an exact-match accuracy of 60% over the test set. with more than 15 percentage points increases in other languages). These results indicate that automatic morphological inflection for low-resource tonal languages like SJQ Chatino poses a particularly challenging setting, which perhaps requires explicit handling of tone information by the model.
We find that automatic lemmatization in SJQ Chatino achieves fairly high accuracy even with our simple baseline models (89% accuracy, 0.27 average Levenshtein distance) and that providing the gold morphological tags provides a performance boost indicated by small improvements on both metrics. It it worth noting, though, that these results are also well below the 94−−95% average accuracy and 0.13 average Levenshtein distance that lemmatization models achieved over 107 treebanks in 66 languages for the SIGMORPHON 2019
The supervised Procrustes-based methods (Proc, Proc-B, and DLV) appear to have an edge in CLIR, with our bootstrapping Proc-B model outperforming all other CLE methods. Contrary to other downstream tasks, VecMap is not the best-performing unsupervised model on CLIR – ICP significantly outperforms VecMap (and also Muse and GWA). The best-performing BLI model, RCSLS, displays poor CLIR performance, significantly worse than the simple Proc model.
Our proposed joint model outperforms all other end-to-end models in frame-level accuracy by a large margin. The joint model and biLSTMs pipeline achieved absolute increase over baseline with 15.03% and 4.25%, respectively. Both models beat the SVMs oracle scores. The biLSTMs pipeline model get worse than biLSTM oracle as expected since it transfer the errors from NLU to the SAP model. Nevertheless, the joint model obtains 10.88% increase than pipeline model and 3.17% than biLSTM oracle. These promising improvements indicate that joint training can mitigate the downside of pipeline model in that the hidden outputs from a history of NLU units capture highly more expressive feature representations than the conventional aggregation of user intents and slot tags. In comparison of these two oracle models, the large improvement (12.02%) for biLSTM model indicates that the contextual user turns make significant contribution to system action prediction. In real human interaction scenarios, frame-level metrics are far more important than token-level ones especially for these multi-label classification tasks since predicting precise number of labels is more challenging.
Baseline using CRF and SVMs still maintains a strong frame-level accuracy with 33.13%, however, biLSTM models taken from pipeline and joint model achieve better increase 3.25% and 4.25%, respectively. This observation indicates that joint training with two tasks of slot filling and intent prediction captures implicit knowledge underlying the shared user utterances, while another supervised signal from system actions is capable of refining the biLSTM based model by backpropagating the associated error gradients. Best accuracy at frame-level for slot filling task is obtained by traditional CRF baseline with only lexical features of words, and our biLSTM models fall behind with absolute decrease 0.47% and 0.82%. Best frame accuracy for intent prediction task is achieved by our proposed model with 5.21% improvement.
The third corpus was taken for comparison. It is important in view of the aim of our research (to test semantic SERP clustering). Our intuition was that perhaps query corpus provides more ‘real-life’ sense inventory. It is two times as big as its counterparts, because ‘connectivity’ between its members is lower (see and we had to compensate for this.
The human ratings were collected on 3 distinct aspects – grammaticality, fluency and semantics, where semantics corresponds to the degree to which a generated text agrees with the meaning of the underlying RDF triples. Both variants of PARENT are either competitive or better than the other metrics in terms of the average correlation to all three aspects. This shows that PARENT is applicable for high quality references as well.
The first and second baseline systems are based on the algorithm proposed by Kim14. Source code is retrieved from the author’s repository and modified to get the Korean sentences as its input. Baseline Kim-1 gets the Eojeol list of an input sentence as its input, and baseline Kim-2 instead gets the morpheme list of the given sentence as its input. The third baseline system is a system proposed in Choi18. It also receives the list of Korean morphemes as its input. The fourth baseline system M-BERT is the multilingual version of BERT (bert). We downloaded the pretrained model from the author’s repository, and fine-tuned it for Korean sentence classification. For the proposed approach, the performances of three different system configurations are selected and presented. However, the evaluation result suggests that a decrease in performance could be efficiently handled by applying the SG method. The performance of IEEC[mbjc]+SG system on SM corpus reaches up to 89.40%, which is about 2%p higher than the SM corpus performances in the baseline systems. Among the three proposed IEE approaches, IEEC performed better than the other two approaches in most cases. IEEM performed slightly better than the IEEC on the KM corpus but showed low performance on the WF corpus. The performance of IEEW on the KM corpus was much lower than the other two approaches.
Another set of experiments is carried out to configure the effect of each noise insertion method and to compare the three proposed IEE approaches. As can be observed from the table, using the IEE approaches with the two proposed noise insertion methods dramatically improves the system performance on the erroneous sentence classification task. The performance on the KM corpus increased about 14 to 15%p on every proposed IEE approach.
The two noise insertion methods worked well on IMEC and IBEC; however, for the KM corpus, the performance of the Eojeol-based embedding approach is 3%p higher than that of morpheme-based or BPE-based approaches. The result shows that the Eojeol-based embedding approach handles with the subword unit analysis errors efficiently, compared to the subword unit-based integrated embedding approaches such as IME or IBE.
Finally, experiments are conducted on IEEC system with various subword unit configurations to figure out the effect of each subword unit on system performance. SG noise insertion method is applied only to the systems that have no Jamo subword units, while the two noise insertion methods are applied to other cases. The evaluation results are compared between the two different groups; one with the Jamo subword unit and one without it.
Experimental Results. We first compare our local differentially private NN (LDPNN) with the non-private NN (NPNN), where the randomization module is removed. We hypothesise that LDP acts as a regularization technique to avoid overfitting. We conjecture another important reason is that the enlarged feature space through encoding produces more powerful representation than the conventional 1-D output of the embedding layer. As LDPNN performance is directly related to the randomization probabilities p and q, the higher p and the lower q, the lower the randomization of the binary vector, and the better performance will be expected. Hence we next investigate how ϵ and λ impact the model accuracy. , we observe that accuracies are relatively stable when the privacy budget ϵ is changed within a wide range of values. The reason lies in the large sensitivity of the encoded binary representation. This also explains high accuracy even under a very tight privacy budget (e.g. ϵ=0.5).
For all the tasks, we use 20% of the dataset as the test set. For SVM and XGBoost, we use cross validation on the other 80% to tune hyperparameters. For LSTM with attention and BERT, we use 10% of the dataset as a validation set, and choose the best hyperparameters based on the validation performance. Not surprisingly, BERT achieves the best performance in all three tasks. For important features, we use k≤10 for Yelp and deception detection, and k≤5 for SST as it is a sentence-level task. See supplementary materials for details of preprocessing, learning, and dataset statistics.
The feature-based systems MElt and MarMoT, respectively based on MEMMs and CRFs, are extended with the lexical information provided by our morphosyntactic lexicons. The results reported by Plank et al. are also included, together with their new system FREQBIN, also initialised with Polyglot embeddings. FREQBIN trains bi-LSTMs to predict for each input word both a PoS and a label that represents its log frequency in the training data. As they word it, “the idea behind this model is to make the representation predictive for frequency, which encourages the model not to share representations between common and rare words, thus benefiting the handling of rare tokens.”
Similarly, we find that JESC’s large size helps it outperform OpenSubs in both in-domain BLEU and out-of-domain generalization.
Both models incorporate slot distance offset but the attention mechanism provides an additional boost. The time mask models show additional gains demonstrating that leveraging dialogue time from each turn is important. Moreover, the time information provides complementary information over the distance offset based measure, as shown by the improvements of the time masking models over the baseline model. The DTM model performs the best overall in terms of F1, which suggests that adding domain information into the time mask provides additional disambiguation power. Interestingly, we see that the ITM model does not improve much over the STM model, possibly because the intent embeddings do not necessarily distinguish between temporal behavior, and are already being leveraged by the slot carryover model.
This study discusses the importance of evaluating GEC models from various perspectives using multiple corpora. Multi-perspective evaluation does not necessarily mean using multiple corpora. Many aspects in a corpus can be used for analysis, such as the proficiency of the writers, essay topics, and the writer ’s native language. As a case study, we evaluate and analyze the models regarding the essay WER. Transformer and LSTM outperform all the other models in the highest and the lowest error-rated corpora, respectively. Experimental results show that LSTM and transformer may be more precision-oriented and recall-oriented, respectively. Further, precision-oriented models have an advantage over recall-oriented models when a given text contains several errors, and vice versa. This knowledge enables choosing a model based on the task that has to be completed.
Results. According to “Adaptive Metric Approach” section, we could work out the weights by LDL Decomposition for each relation. Because the minimal weight is too small to make a significant analysis, we choose the median one to represent relative small weight. Thus, “Weight Difference” is calculated by (MaximalWeight−MedianWeightMedianWeight). Bigger the weight difference is, more significant effect, the feature weighting makes. Notably, scaling by the median weight makes the weight differences comparable to each other. We observe that: Overall, TransA yields the best average accuracy, illustrating the effectiveness of TransA. Accuracies vary with the weight difference, meaning the feature weighting benefits the accuracies. This proves the theoretical analysis and the effectiveness of TransA. Compared to Adaptive Metric (PSD) , TransA performs better, because our score function with non-negative matrix condition and absolute operator leads to a more flexible representation than that with PSD matrix condition does.
Results. From the results, we can see that the lexical matching score is the most important among these three factors, and statistical information score is more important than edit distance score. Moreover, the dictionary term in lexical matching score significantly improves the performance. From these results, we obtain the best setting that involves all these three factors. We used this setting for dataset creation.
For RNN-based NMT, we can see that target language reversal, residual connection, and word2vec can further improve the performance of the basic RNN-based NMT model. However, we find that word2vec and reversal tricks seem no obvious improvement when trained the RNN-based NMT and Transformer models on augmented parallel corpus. For SMT, it performs better than NMT models when they were trained on the unaugmented dataset. Nevertheless, when trained on the augmented dataset, both the RNN-based NMT model and Transformer based NMT model outperform the SMT model.
Because the Test set contains both augmented and unaugmented data, it is not surprising that the RNN-based NMT model and Transformer based NMT model trained on unaugmented data would perform poorly. From the results, it can be seen that the data augmentation can still improve the models.
Our model achieves state-of-the-art results on WN18RR and NELL-995. We also have a competitive performance on FB15k-237. This observation is expected given AnyBURL’s performance on these datasets. In FB15k-237, the relation space is much larger, which means it is harder for the relation agent to select a valid rule that is traceable.
Ablation Study We performed an ablation study where we removed the pre-training step, freeze the relation agent after pre-training, or use a single agent on WN18RR. They all use ComplEx embedding for consistency. The performance decrease for freezing pre-trained agent shows that using rule reward is not enough. The model still needs the hit performance information to further improve. The performance decrease if pre-training is removed shows that learning to reason as a symbolic approach first then improve it as neural reasoning can have better performance. Single agent performance drop shows the effectiveness of pruning action space.
Confidence Score Threshold In addition, to analyze the utility of the pre-mined rule set, we set up a confidence score threshold to see if the model will be affected by it. The maximum threshold we use is confidence score ≥ 0.15, since the most confident rules are mostly distributed within [0.15,0.20]. One potential reason is that walk-based reasoning paths and less confident rules may have similar performance on certain queries. We use the threshold as a hyperparameter and use the one with the best performance on the development set.
Here, we describe the experimental method used to validate the effectiveness of the NJM. We use a questionnaire as the evaluation method. We selected a total of 30 themes from the Bokete Ogiri website that included “people”, “two or more people”, “animals”, “landscape”, “inorganics”, and “illustrations”. The questionnaire asks respondents to rank the captions provided by humans, the NJM, and STAIR caption in order of “funniness”. The questionnaire does not reveal the origins of the captions. In this subsection, we present the experimental results along with a discussion. Captions generated by humans were ranked “funniest” 67.99% of the time, followed by the NJM at 22.59%. The baseline captions, STAIR caption, were ranked “funniest” 9.41% of the time. These results suggest that captions generated by the NJM are less funny than those generated by humans. However, the NJM is ranked much higher than STAIR caption.
All models are trained on SQuAD1.1 before evaluation. Though the BERT+AT+VAT(2.0) achieves the best results on AddSent and AddSOneSent, this is largely due to its high performance on the normal dataset, rather than obtaining additional robustness against AddSent and AddOneSent, since the relative improvements are mediocre. While KAR explicitly utilizes external general knowledge (WordNet), and it has the smallest gap Δ1 and Δ2 between F1 on test and F1 on AddSent/AddOneSent. The results show that while AT improves the generalization performance, it is not designed for defending against adversarial examples generated with human knowledge, at least in the reading comprehension tasks. How to bridge the gap between gradient-based and artificial adversarial examples, and how to achieve improvement and robustness on artificial adversarial examples at the same time is still an open question.
To investigate the possibility of using paraphrase sentences to enhance the phrase-based machine translation, we first use Moses to extract parallel phrases between s and r. r. This convinces our idea that turkers usually paraphrase natural language commands that are more semantically closed to the robot language commands after the robotic concepts are shown to them. r. We observe that the extracted lexical entries capture the similarity between source-language phrases and target-language phrases, thus enabling many-to-one mapping from syntactic variants in natural language to unique robotic concepts.
Our proposed timex model achieves an accuracy of 97.3%. This high accuracy indicates that the model has effectively learned from the training data; its timex embeddings contain temporal ordering information which can be used for downstream tasks. We also evaluate whether pre-trained embeddings such as ELMo or GloVe contain the necessary temporal information necessary for classifying the temporal order between timex pairs. We first test these with a minimal model. We construct a distributed representation of each time expression (obtained by average pooling the token level GloVe or ELMo embeddings), perform element-wise subtraction between the two embeddings, and feed the result through a linear classification layer that produces the output probabilities for the temporal label classes.
We evaluate the models using both GloVe and ELMo embeddings. Our results show substantial improvement over this baseline model. Moreover, including time embeddings as additional input to the improved models leads to a small improvement in the overall accuracy. However, we did not find the results to be statistically significant according to a bootstrap resampling test (GloVe p-value =0.349, ELMo p-value =0.267).
We evaluate our models across three settings: (a) our event ordering model without including timex embeddings, (b) our event ordering model with masking of time tokens (replacing it with UNK tokens) and (c) our full model including timex embeddings. We evaluate the models using both GloVe and ELMo embeddings as input. In both settings, incorporating our timexes leads to higher performance. For GloVe, the performance of the basic temporal model is similar to that when the time expression is masked out. This demonstrates that the temporal model does not use the knowledge from time expressions when making temporal relation predictions. However, in the ELMo setting, we observed a larger drop in performance by masking out the time expressions compared to GloVe embeddings. This demonstrates that the ELMo embeddings are not agnostic to time-expressions in the sentence, although they still show improvement by inclusion of timex embeddings trained specifically with the temporal classification objective on small datasets.
, we employed Sliced DocQA with Step Transfer as a truly incremental model. As it can be seen, at slice size of 128 it almost achieves the performance of the model without early stopping (%99). Next, we investigate whether the early stopping model is more efficient regarding the number of read tokens. We looked into the performance of the model without early stopping at different context length to find the earliest point at which the model achieves its highest performance. This can be assumed as an oracle for early stopping model (i.e. best possible stopping point). We observe that (1)For a large number of examples, we have to read the full context to achieve the best performance. (2)For some examples, the early stopping model is reading more than it should, and for some, it stops earlier than it should. (3)In general, while with the best stopping offsets we can read about %15 less text, our model reads about %8 less. We also studied if there is a correlation between the context length and the ratio of read context length. After that point, we see the trend of reading smaller ratio of longer contexts.
The λ1 regularization hyper-parameter is tuned so the two versions of our model extract similar number of words as rationales. The SVM and attention-based model are constrained similarly for comparison. Again, for our model this corresponds to changing the λ1 regularization. As shown in the table and the figure, our encoder-generator networks extract text pieces describing the target aspect with high precision, ranging from 80% to 96% across the three aspects appearance, smell and palate. The SVM baseline performs poorly, achieving around 30% accuracy. The attention-based model achieves reasonable but worse performance than the rationale generator, suggesting the potential of directly modeling rationales as explicit extraction.
We include two runs for each version. The first one achieves the highest MAP on the development set, The second run is selected to compare the models when they use roughly 10% of question text (7 words on average). The rationales achieve the MAP up to 56.5%, getting close to using the titles. The models also outperform the baseline of using the noisy question bodies, indicating the the models’ capacity of extracting short but important fragments.
The GlobalAtt significantly outperforms PBSMT by 3.57 BLEU points on average, indicating that it is a strong baseline NMT system. ’s model, LocalAtt, and FlexibleAtt, outperform the baseline GlobalAtt. (1) All the comparison methods and our +SDAtt outperform the baseline GlobalAtt. This indicates that the proposed double-context mechanism for NMT is more effective than single context NMT.
The GlobalAtt significantly outperforms PBSMT by 3.57 BLEU points on average, indicating that it is a strong baseline NMT system. ’s model, LocalAtt, and FlexibleAtt, outperform the baseline GlobalAtt. (1) All the comparison methods and our +SDAtt outperform the baseline GlobalAtt. This indicates that the proposed double-context mechanism for NMT is more effective than single context NMT.
In order to gauge how well embeddings should perform on our dataset, we conducted a human evaluation. We asked participants to select the outlier from a given test case, providing us with a human baseline for the accuracy score on the dataset. Due to the wide array of domain knowledge needed to perform well on the dataset, participants were allowed to refer to Wikipedia (but explicitly told not to use Wikidata). We collected 447 responses, with an overall precision of 68.9%. In order to get a better comparison between the embeddings under identical conditions, we then took the intersection of in-vocabulary entities across all embeddings and reevaluated on this subset. 23.88% of cluster entities and 22.37% of outliers were out-of-vocabulary across all vectors, with 23 test groups removed from evaluation.
We expect to see Spanish and German perform similarly to the English Europarl+Leipzig vectors, for the monolingual training corpora used to generate them consisted of Spanish and German equivalents of the English training corpus.
\newcite tang2019understanding showed that context-free Transformer (directly propagating the source word embeddings with PE to the decoder) achieved comparable results to the best RNN-based model. We argue that XL PE could further enhance the context-free Transformer. On English\RightarrowGerman dataset, we compare LSTM-based model, Transformer Big-noenc-nopos, +APE, +RPE and +InXL PE. For fair comparison, we set the LSTM hidden size to 1024. In Tab. (+0.57 over 24.11), where the improvements are +2.3% vs. +1.1%.
First of all, we evaluated different feature representations. Overall simple character n-grams (C) in isolation are often more beneficial than word and character n-grams together, albeit for some languages results are close. The best representation are character n-grams with word embeddings. This representation provides the basis for our multilingual model which relies on multilingual embeddings. The two officially submitted models both use character n-grams (3-10) and word embeddings. Our first official submission, Monolingual is the per-language trained model using this representation. Next we investigated adding more languages to the model, by relying on the multilingual embeddings as bridge. word embedding-based SVM trained using bilingual embeddings created by mapping the two monolingual embeddings onto the same space and using both the English and Spanish training material. As the results show, using multiple languages can improve over the in-language development performance of the character+embedding model. However, the bilingual models are still only able to handle pairs of languages. We therefore mapped all embeddings to a common space and train a single multilingual All-in-1 model on the union of all training data. This is the second model that we submitted to the shared task. As we can see from the development data, on average the multilingual model shows promising , overall (macro average) outperforming the single language-specific models. However, the multilingual model does not consistently fare better than single models, for example on French a monolingual model would be more beneficial. Adding POS tags did not help (cf. We disregard this feature for the final official runs.
Both data in this research originate from the Swedish Language Bank (Språkbanken) located at the University of Gothenburg: a corpus of Swedish raw sentences without part of speech tagging and a list of nouns affiliated to grammatical genders. The corpus originates from Swedish Wikipedia available at Wikipedia Monolingual Corpora, Swedish web news corpora (2001-2013) and Swedish Wikipedia corpus collected by Språkbanken. Therefore, they were judged suitable for our analysis. First, with regard to the raw corpus, the OpenNLP sentence splitter and tokenizer are used for normalization. By way of illustration, we replace all numbers with a special token NUMBER and convert uppercase letters to lowercase forms. Second, the list of nouns and their affiliated grammatical gender is extracted from the SALDO (Swedish Associative Thesaurus version 2) dictionary. The data from the dictionary originally included five categories: uter, neuter, plural, vacklande (variable) and unassigned nouns blank.
The neural network generates the best performance (93.46%) when setting the context size as one in terms of asymmetric backward context. Moreover, it is also expected in terms of language structure: in languages such as Swedish where the syntactic structure is SVO, the relevant information tend to be in the preceding position. Nevertheless, we also measured the efficiency of neural network when setting the context type as asymmetric forward, i.e. the classifier looks at the following word of a noun to determine the gender of the noun. The overall accuracy of neural network drops drastically when setting context type as asymmetric forward. The highest accuracy is also measured when setting context size as one word, however the accuracy (70.91%) is much lower compared to the accuracy of the asymmetric backward setting (93.46%).
This suggests that a nonparallel VC method with a CycleGAN can achieve performance superior to that of state-of-the-art parallel VC methods.
[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt] The baseline BLEU scores confirm the need for domain adaptation. Using only the 15k in-domain samples alone (IN) is not sufficient to train a strong domain specific model, yielding BLEU scores as low as 2.53 on TED(de) and 1.76 on TED(ru). The model trained with a large amount of general domain data (GEN) is a stronger baseline, with BLEU scores of 34.59 and 23.40. Standard continued training is not robust to samples that are noisy and less similar to in-domain. As expected, continued training on in-domain data (IN_CT) improves BLEU significantly, by up to 18.74 BLEU on patent(de). However, when adding Paracrawl data, the standard continued training strategy (std_rand, std_ML, std_CDS) consistently performs worse than IN_CT. Curriculum learning consistently improves BLEU score. Ranking examples using Moore-Lewis (CL_ML) and Cynical Data Selection (CL_CDS) improve BLEU over their baselines (std_ML and std_CDS) by up to 3.22 BLEU points.
Both of them are the best F1 score ever published on CPB 1.0 dataset. Indeed, it is difficult to introduce the whole tree structure into the model using the simple feature engineering way. By building the dependency relationship directly into the structure of SA-LSTM and changing the way information flows, SA-LSTM is able to consider whole tree structure of dependency parsing information.
The dev and test set of Tuda-De is recorded using multiple microphones. The differences in recognition accuracy are surprisingly small for Kinect-RAW, Samson and Yamaha recordings. The usual range of WER for TDNN-HMM models we observed for these microphones is between 15.03% and 15.77%. However, the beamformed WER result for the Kinect is significantly higher than decoding the raw (mixed down to one channel) data. The beamforming algorithm of the Microsoft Kinect is closed source, but a few observations are very noticable in the recorded signal. There is a very audible "tin can effect" in the audio signals, probably from a noise suppression algorithm. The recordings were made with automatic gain control, in some of the utterances the beginning is difficult to understand as a result.
3.4.1 Speech naturalness We invite 20 listeners to participate in the subjective tests, whose first language is Chinese and are well educated in English. The subjects are asked to rate the naturalness of generated utterances on a five-point Likert scale (1:Bad, 2:Poor, 3:Fair, 4:Good, 5:Excellent). In the case of extremely limited data, the speech synthesized by our approach is still clear and stable, but baseline is not. What’s more, our approach achieve satisfactory performance in speech quality and naturalness for English speakers.
Speech similarity In similarity test, a subject is presented with a pair of utterances comprises a real utterance recorded by a speaker and another synthesized utterance from the same speaker. The similarity MOS test uses five-scale-score for evaluation (1: Not at all similar, 2: Slightly similar, 3: Moderately similar, 4: Very similar, 5: Extremely similar). The similarity MOS of English speakers are both above 3.5, which demonstrates the model can primely generalize to the new cross-lingual speakers.
The most intriguing result is that character-level models are competitive with word-level models for syntactic analogy, with our Char2Vec model holding the best result for syntactic analogy answering. This suggests that incorporating morphological knowledge explicitly rather than latently helps the model learn morphological features. However, on the semantic analogies, the character-based models do much worse than the word-based models. This is perhaps unsurprising in light of the previous section, where we demonstrate that character-based models do worse at the semantic similarity task than word-level models.
All numbers are on 1000 randomly chosen examples from the IMDB development set. Slowly increasing ϵ is important for good performance. Slowly increasing κ is actually slightly worse than holding κ=κ∗ fixed during training, despite earlier experiments we ran suggesting the opposite. Here we only report certified accuracy, as all models are trained with certifiably robust training, and certified accuracy is much faster to compute for development purposes.
In the beginning of our experiments, we tune the regularization coefficient λ on the DE⇒EN task. In particular, the best performance is achieved when λ=1, which is the default setting throughout this paper.
The Context Gates are also able to avoid few context selection error but cannot make a notable improvement in translation performance. It is worth to note that there is approximately one third translation error is related to context selection error. The Regularized Context Gates indeed alleviate this severe problem by effectively rebalancing of source and target context for translation. This paper transplants context gates from the RNN based NMT to the Transformer to control the source and target context for translation. We find that context gates only modestly improve the translation quality of the Transformer, because learning context gates freely from scratch is more challenging for the Transformer with the complicated structure than for RNN. Based on this observation, we propose a regularization method to guide the learning of context gates with an effective way to generate supervision from training data. Experimental results show the regularized context gates can significantly improve translation performances over different translation tasks even though the context control problem is only slightly relieved.
For comparison, we removed (in ascending order) the small-norm nodes from the DNN produced by the common L2-regularized training in the L2O case until the number of removed nodes became identical to the gLassoO case: 3161; similarly, in the L2I case, we removed 3368 nodes from the L2-based DNN, which was the same with the network used in the L2O case. They are much worse than those gained before the node selection and clearly show that both the L2O and L2I cases failed to sort out the weight vectors (as a result, their corresponding nodes).
After training the four aforementioned models, we compared them by examining their accuracy on the pre-training sub-tasks. The “Latent Model” achieved the highest MLM accuracy and is significantly better than the other models (p-value < 0.001). All other tests did not yield significantly different results in both the MLM and NS prediction tasks. Notably, the “Universal Model” was able to achieve performance equal to the base model while using less than half of the trainable parameters.
Our model outperforms TENER Yan2019TENERAT by 1.72 in average F1 score. For lattice LSTM, our model has an average F1 improvement of 1.51 over it. When using another lexicon li-etal-2018-analogical, our model also outperforms CGN by 0.73 in average F1 score. Maybe due to the characteristic of Transformer, the improvement of FLAT over other lexicon-based models on small datasets is not so significant like that on large datasets. We think self-attention mechanism brings two advantages over lattice LSTM: 1) All characters can directly interact with its self-matched words. 2) Long-distance dependencies can be fully modeled. Due to our model has only one layer, we can strip them by masking corresponding attention. In detail, we mask attention from the character to its self-matched word and attention between tokens whose distance exceeds 10. inline多了这 brings a significant deterioration to FLAT while the second degrades performance slightly. As a result, we think leveraging information of self-matched words is important For Chinese NER.
Compared with TENER, FLAT leverages lexicon resources and uses a new position encoding. To probe how these two factors bring improvement. We set two new metrics, 1) Span F: while the common F score used in NER considers correctness of both the span and the entity type, Span F only considers the former. 2) Type Acc: proportion of full-correct predictions to span-correct predictions. We can find: 1) FLAT outperforms TENER in two metrics significantly. 2) The improvement on Span F brought by FLAT is more significant than that on Type Acc. 3) Compared to FLAT, FLAThead’s deterioration on Span F is more significant than that on Type Acc. These show: 1) The new position encoding helps FLAT locate entities more accurately. 2) The pre-trained word-level embedding makes FLAT more powerful in entity classification (ner_analysis_1).
We first trained our models on the MSCOCO dataset to generate factual captions. Then, we trained our models on the SentiCap dataset to add sentiment properties to the generated captions. Following this training approach makes our results directly comparable to the previous ones. CNN+RNN, which is only trained using the MSCOCO dataset; ANP-Replace, which adds the most common adjectives to a randomly chosen noun; ANP-Scoring, which applies multi-class logistic regression to select an adjective for the chosen noun; RNN-Transfer, which is CNN+RNN fine-tuned on the SentiCap dataset; and their key system SentiCap, which uses two LSTM modules to learn from factual and sentiment-bearing caption. Moreover, we first train our attention-based model only on the factual dataset MSCOCO (we name this model ATTEND-GAN−SA). Second, we train our model additionally on the SentiCap dataset but without our caption discriminator (ATTEND-GAN−A). Finally, we train our full model using the caption discriminator (ATTEND-GAN). In comparison with the state-of-the-art, our full model (ATTEND-GAN) achieves the best results for all image captioning metrics in both positive and negative parts of the SentiCap dataset. We report the average results to show the average improvements of our models over the state-of-the-art model. ATTEND-GAN achieved large gains of 6.15, 6.45, 3.00, and 2.95 points with respect to the best previous model using BLEU-1, ROUGE-L, CIDEr and BLEU-2 metrics, respectively. Other metrics show smaller but still positive improvements. ATTEND-GAN outperforms ATTEND-GAN−A over all metrics across both positive and negative parts of the SentiCap dataset; the discriminator is thus an important part of the architecture. ATTEND-GAN outperforms ATTEND-GAN−SA for all metrics except, by a small margin, CIDEr and ROUGE-L. Recall that ATTEND-GAN−SA is trained only on the large MSCOCO (with many captions), and so is in a sense encouraged to have diverse captions; second-stage training for ATTEND-GAN−A and ATTEND-GAN leads to more focussed captions relevant to SentiCap. The discriminator, however, removes almost all of this penalty, as well as boosting the other metrics beyond ATTEND-GAN−SA.
AntNet achieves the highest accuracies on both data corpora. Compared with the state-of-the-art network, Transformer, the results significantly increased. The poor performance of Transformer may result from the small training size.
Unsurprisingly, all variations without a certain type of key module achieve inferior accuracies compared with the full version of AntNet. These comparisons indicate that all the three key modules are useful in answer understanding.
Considering English first, using MORSE instead of Morfessor, resulted in a 6% absolute increase in F1 scores. This supports our claim for the need of semantic cues in morpheme segmentation, and also validates the method used in this paper. Since English is a less systematic language in terms of the orthographic structure of words, semantic cues are of greater need, and hence a system which relies on semantic cues is expected to perform better; indeed this is the case. Similarly, MORSE performs better on Turkish with a 7% absolute margin in terms of F1 score. On the other hand, Morfessor surpasses MORSE in performance on Finnish by a large margin as well, especially in terms of recall.
The use of MC data (which does not adhere to the compositionality principle) to tune MORSE to be evaluated on SD17 (which does adhere to the compositionality principle) is not optimal. Thus, we evaluate MORSE on SD17 using 5-fold cross validation, where 80% of the dataset is used to tune and 20% is used to evaluate. Comparing MORSE-CV to MORSE reflects the fundamental difference between SD17 and MC datasets. Knowing the basis of construction of SD17 and the fundamental weaknesses in MC datasets, we attribute the performance increase to the lack of compositionality in MC dataset. Comparing MORSE-CV to Morfessor, we observe a significant jump in performance (an increase of 24%). In comparison, the increase on the MC dataset (6%) shows that the Morpho Challenge dataset underestimates the performance gap between Morfessor and MORSE due its inherent weaknesses.
Consistently, HOFTT and HOTT are more effective than RWMD and WMD-T20 in all tests, and not using any of the distances consistently degrades the performance.
While it is unsurprising that RWMD is more strongly correlated with WMD (HOTT is neither a lower nor an upper bound), we note that HOTT is on average a better approximation to WMD than RWMD.
Our model achieves significant performance gains in the test sets over the baseline for both Italian and Finnish query translation. The overall low MAP for NMT can possibly be improved with larger TC. Moreover, our model validation approach requires access to RC index, and it slows down overall training process. Hence, we could not train our model for a large number of epochs - it may be another cause of the low performance.
All approaches produce relevant and grammatical questions. All models are all equally good at seeking new information, but are weaker than Lucene, which performs better at seeking new information but at the cost of much lower specificity and lower usefulness.
Data Preprocessing It has been previously suggested that 85% of posts received 80% of reposts within 48 hours on Weibo So, our data set retained posts published for more than a week; Due to the different time of the first post, we have kept posts between July 2012 and November 2018; On Weibo, there exits sweepstakes. Such posts often require users to forward them and draw prizes, so the popularity will be higher. They are noises for our analysis, so we unify all users’ lottery posts; As the popularity of forwarding post cannot be distinguished whether resulted from this post or the source. Therefore, we only keep original posts. So we guessed that there are some differences in their writing style, which lead to the difference of quality.
Shaded cells indicate relative performance per language, where a darker cell represents a more accurate model. From this table we observe a trend in which four layers copied from pre-trained English DeepSpeech result in the best final model.
Over all categories, our taxonomy-aware TXtract substantially improves over the state-of-the-art OpenTag by up to 10.1% in Micro F1, 14.6% in coverage, and 93.8% in vocabulary (for flavor).
Different from Cold Update, Hot Update resumes training only on the new tasks, requires much less training time, while still obtains competitive results with Cold Update. The new tasks like IMDB and Kitchen benefit more from Hot Update than the old tasks, as the parameters are further tuned according to annotations from these new tasks. Based on Cold Update and Hot Update, MTLE can easily scale and needs no structural modifications when new tasks are introduced.
We find that the syntactic probe recovers syntactic trees across all the languages we investigate, achieving on average an improvement of 22 points UUAS and 0.175 DSpr. We find that mBERT ’s syntactic subspaces are transferable across all of the languages we examine. Specifically, transfer from the best source language (chosen post hoc per-language) achieves on average an improvement of 14 points UUAS and 0.128 DSpr. Additionally, our results demonstrate the existence of a cross-lingual syntactic subspace; on average, a holdout subspace trained on all languages but the evaluation language achieves an improvement of 16 points UUAS and 0.137 DSpr. over baseline, while a joint AllLangs subspace trained on a concatenation of data from all source languages achieves an improvement of 19 points UUAS and 0.156 DSpr.
The remainder of our analysis is confined to the development set. The naive baseline, which predicts the 15 most frequent English words in the training set, achieves a precision/recall of around 20%, setting a performance lower bound.
The English model works slightly better, as would be predicted given our discussion of code-switching, but the French model is also useful, improving BLEU from 10.8 to 12.5. This result strengthens the claim that ASR pre-training on a completely distinct third language can help low-resource ST. Presumably benefits would be much greater if we used a larger ASR data set, as we did with English above.
LSTM based models in general perform better than the CRF based models. Both the CRF-BoE and LSTM-BoE outperform the basic CRF and LSTM models. Both zero-shot models, CT and ZAT, again surpass the BoE models. ZAT has a statistically significant mean improvement of 4.04, 5.37 and 3.27 points over LSTM-BoE with training size 500, 1000 and 2000, respectively. ZAT also shows a statistically significant average improvement of 2.58, 2.44 and 2.5 points over CT, another zero-shot model with training size 500, 1000 and 2000, respectively. Looking at results for individual domains, the highest improvement for BoE models are seen for transportation and travel. This can be explained by these domains having a high frequency of timex and location slots. But BoE models show a regression in the shopping domain, and a reason could be the low frequency of expert slots. In contrast, ZAT consistently outperforms non-adapted models (CRF and LSTM) by a large margin. This is because ZAT can benefit from other reusable slots than timex and location. Though not as popular as timex and location, slots such as contact_name, rating, quantity, and price appear across many domains.
LSTM based models in general perform better than the CRF based models. Both the CRF-BoE and LSTM-BoE outperform the basic CRF and LSTM models. Both zero-shot models, CT and ZAT, again surpass the BoE models. ZAT has a statistically significant mean improvement of 4.04, 5.37 and 3.27 points over LSTM-BoE with training size 500, 1000 and 2000, respectively. ZAT also shows a statistically significant average improvement of 2.58, 2.44 and 2.5 points over CT, another zero-shot model with training size 500, 1000 and 2000, respectively. Looking at results for individual domains, the highest improvement for BoE models are seen for transportation and travel. This can be explained by these domains having a high frequency of timex and location slots. But BoE models show a regression in the shopping domain, and a reason could be the low frequency of expert slots. In contrast, ZAT consistently outperforms non-adapted models (CRF and LSTM) by a large margin. This is because ZAT can benefit from other reusable slots than timex and location. Though not as popular as timex and location, slots such as contact_name, rating, quantity, and price appear across many domains.
LSTM based models in general perform better than the CRF based models. Both the CRF-BoE and LSTM-BoE outperform the basic CRF and LSTM models. Both zero-shot models, CT and ZAT, again surpass the BoE models. ZAT has a statistically significant mean improvement of 4.04, 5.37 and 3.27 points over LSTM-BoE with training size 500, 1000 and 2000, respectively. ZAT also shows a statistically significant average improvement of 2.58, 2.44 and 2.5 points over CT, another zero-shot model with training size 500, 1000 and 2000, respectively. Looking at results for individual domains, the highest improvement for BoE models are seen for transportation and travel. This can be explained by these domains having a high frequency of timex and location slots. But BoE models show a regression in the shopping domain, and a reason could be the low frequency of expert slots. In contrast, ZAT consistently outperforms non-adapted models (CRF and LSTM) by a large margin. This is because ZAT can benefit from other reusable slots than timex and location. Though not as popular as timex and location, slots such as contact_name, rating, quantity, and price appear across many domains.
Both models are able to achieve reasonable zero-shot performance for most domains, and ZAT shows an average improvement of 5.07 over CT.
Without CRF, the model suffers a loss of 1%-1.8% points. The character-level word embeddings are also important: without this, the performance is down by 0.5%-2.7%. We study the impact of fine-tuning the pre-trained word embeddings (+WEFT). When there is no target domain data available, fine-tuning hurts performance. But, with a moderate amount of target domain data, fine-tuning improves performance.
We design an experiment to test this hypothesis and determine whether transfer requires direct overlap between the vocabularies of two languages, or whether transfer can still happen even if the vocabularies of the two languages are disjoint. For this experiment we focus on NSP and pick two languages with very little natural vocabulary overlap: Telugu (te) and Turkish (tr). We train one model, te+en, on sentence pairs drawn from the two languages Telugu (te) and English (en), in the same proportion as they occur naturally in the training data, and evaluate this model’s performance on held out Telugu data. We then similarly sample sentence pairs from an auxiliary language, Turkish (tr), whose vocabulary has been censored to contain no overlap with the Telugu vocabulary. Thus, there is no direct route by which the Turkish data can influence the embedding of the Telugu vocabulary for details of all vocabulary overlaps).
To demonstrate that these results generalize across other language pairs, we show a similar effect with two other, similarly disjoint languages: Catalan (ca) and Ukrainian (uk). This time we use two pivot languages: English (en) and Russian (ru). We follow the same procedure as before, censoring the vocabularies of the four languages to ensure there is no overlap. Once again we see that the addition of a seemingly unrelated language (Ukrainian) improves the performance on another language (Catalan), despite the fact that there is no direct vocabulary overlap among the languages.
Our model is 14x more parameter-efficient than the competitive baselines. As there are fewer parameters to learn, this drastically improves the average training time of the model as compared to the other baselines. The number of parameters depends on the vocabulary size of the output softmax layer and the input to the word embeddings. Reducing the size of the softmax vocabulary to the most frequent words and making the model copy fact-specific words directly from the input contributes to the parameter efficiency and faster training of the model.
Our model significantly outperforms SEMAFOR, which uses a variety of syntactic features. Hermann et al. FitzGerald et al. Our performance is similar to Hartmann et al. We use no external resources.
It is only labeled by gender, being slightly higher the number of comments that belong to the “Men” class (
We describe the datasets used for the tuning and for the evaluation of our model and the different settings used to test it. The results are provided as F1, computed according to the following equation, F1=2⋅precision⋅recallprecision+recall⋅100. (18) F1 is a measure that determines the weighted harmonic mean of precision and recall. Precision is defined as the number of correct answers divided by the number of provided answers and recall is defined as the number of correct answers divided by the total number of answers to be provided. The table includes the results for the two implementations of our system: the unsupervised and the semi-supervised and the results obtained using the most frequent sense heuristic. For the computation of the most frequent sense we assigned to each word to be disambiguated the first sense returned by the WordNet reader provided by the Natural Language Toolkit (version 3.0) As we can see the best performances of our system are obtained on nouns, on all the datasets. Furthermore, our method is particularly suited for nouns. The big improvement obtained on S7 can be explained by the fact that the results of the unsupervised system are well below the most frequent sense heuristic, so exploiting the evidence from sense-labeled dataset is beneficial. For the same reason, the results obtained on S7CG with a semi-supervised setting are less impressive than those obtained with the unsupervised systems; in fact, the structure of the datasets is different and the results obtained with the unsupervised setting are well above the most frequent sense. These series of experiments confirm that the use of prior knowledge is beneficial in general domain datasets and that when it is used the system performs better than the most common sense heuristic computed on the same corpus.
Our final research question concerns the overall sentiment of the language used to describe men and women. To answer this question, we use a simplified version of our model, without the latent sentiment variables or the posterior regularizer. We are then able to use the combined sentiment lexicon of \newcitehoyleSentiment2019 to analyze the largest-deviation neighbors for each gender by computing the frequency with which each neighbor corresponds to each sentiment. We find that there is only one significant difference: adjectives used to describe men are more often neutral than those used to describe women.
Based on the two scenarios above, in order to test the robustness of the proposed approach, for each of the two scenarios, four datasets are generated based on SWBC and Voxceleb1. For the first dataset (SWBC-S, “S” represents small), SWBC dataset is used and each speaker occurs 30 times in the training set averagely. ” SWBC-L” (“L” represents large) contains more training data, each speaker occurs 200 times in the training data averagely, while the amount of the test data keeps the same. The small and large version of the datasets is used to test the robustness of the proposed model in small and large training data. Similar to the configurations in the SWBC based datasets, the datasets that based on Voxceleb1 also have small and large scenarios. In ”Vox-S”, 1000 speakers are randomly selected from the Voxceleb1 dataset. Each speaker occurs 30 times in the training set. In ”Vox-L” dataset, each speaker occurs 300 times in the training set, while the test set is the same as ”Vox-S”. For each of the eight datasets, the number of speakers in each utterance is randomly chosen from one to three in all of the datasets. In the training process, for all of the eight datasets, the number of speakers in the generated utterances is not fixed, changing from one to three. When the number of speakers is one, the generated utterance is the same as the original utterance. When the number of speakers are two or three, the output utterance contains multiple speakers with or without overlap.
The goal of the empirical portion of our paper is to perform a controlled study of the different architectures and approximations discussed up to this point in the paper.
We hypothesize that even though the model is non-monotonic, it can learn monotonic alignment with flexibility if necessary, giving state-of-the-art results on many seemingly monotonic character-level string transduction task. An alignment is non-monotonic when alignment edges predicted by the model cross. There is an edge connecting xj and yi if the attention weight or hard alignment distribution αj(i) is larger than 0.1. We find that the better-performing transducers are more monotonic, and most learned alignments are monotonic. The results indicate that there are a few transductions that are indeed non-monotonic in the dataset. However, the number is so few that this does not entirely explain why non-monotonic models outperform the monotonic models. We speculate this lies in the architecture of \newciteaharoni-goldberg:2017: Long, which does not permit many-to-one alignments, while monotonic alignment learned by the non-monotonic model is more flexible. Future work will investigate this.
Due to space limitations, we selected the results for five dataset sections, and the aggregated results. Best results were provided by GloVe, but there is no considerable difference to word2vec variants like w2v- ww12-300-ns and to LexVec. Both the w2v-default setting with its small word window of 5, and especially the word2vec CBOW method are unsuited for the task. Generally the accuracy is low with ca. 34%, reasons for the difficulty of the task setting are given above.
We investigate the correlation between the frequency of the terms in the respective book corpus, and the correct guesses in the doesnt-match task. In general, the expectation was that a higher frequency of terms in the book leads to a “better” word embeddings vector for the term, and therefore to higher results. Our experiments confirm this expectation only in part. As every task unit consists of four terms (the three that are related, and the intruder to be found), the question is which of the term frequencies to use. the frequency of the real intruder which had to be found, (ii) the frequency of the term chosen as intruder using the model, (iii) the frequency bin of the chosen intruder, and (iv) the average frequency of all four terms in the task unit. The data suggests that the intruder is not easier to find if it has a higher frequency in the books One reason might be that very frequent entities often change their context as the story progresses. E.g., in ASOIF the Arya character first lives as child with her family, then moves into the context of the capital city, later travels the country with changing companions, and finally ends up to be trained as assassin on a different continent, so the context changes are drastic. More research is needed to investigate the phenomenon of little influence of the frequency of the intruder.
Out of all Select_Where queries, roughly half of the queries (denoted as “Join”) require joining the two tables to derive answers. We tested on a model with three executors. The accuracy of join queries is lower than that of non-join queries, which is caused by additional interaction between the two tables involved in answering join queries.
Out of all Select_Where queries, roughly half of the queries (denoted as “Join”) require joining the two tables to derive answers. We tested on a model with three executors. The accuracy of join queries is lower than that of non-join queries, which is caused by additional interaction between the two tables involved in answering join queries.
Our default variant of polarized-VAE uses the entailment labels from SNLI dataset as a proxy for semantic similarity. For syntactic similarity we threshold the differences in tree edit distance (of syntax parses) as a proxy for syntactic similarity. We also evaluate two other variants of our model on this task. In the model variant polarized-VAE (wo) (see we use BLEU scores as a heuristic proxy for estimating semantic similarity, while keeping the syntactic training unchanged. We also experiment with heuristics for syntax in polarized-VAE (len) where we use length as a heuristic proxy for syntax, while still making use of the ground truth similarity labels for the semantic training. Finally we combine these two heuristics in polarized-VAE (wo, len) which can be viewed as an unsupervised variant that does not make use of any labels or syntax trees. In comparison to Bao et al. On the other hand, we perform slightly worse in BLEU w.r.t. xsem. Our model does a better job at matching the syntax of the sentence xsyn as indicated by the lower TED score w.r.t. xsyn.
By feeding 1000 sentences from the test set to the Polarized-VAE, we obtain their corresponding semantic (zsem) and syntax (zsyn) latent vectors. We then empirically compute the correlation between zsem and zsyn. To analyze the level of similarity of information represented in zsem and zsyn, we report the maximum absolute correlation (max across all pairs of dimensions) and also the mean absolute correlation. A higher value of correlation would indicate that there is more overlapping information learnt by the semantic and syntactic encoders. This demonstrates that the 2 latent spaces learned by our model encode sufficiently different information.
In general, the additional abstracts without titles improve the generalization ability on the test set. Interestingly, even when σ=100% (all titles are observed), the joint training objective still yields a better performance than using Lsup alone. Presumably, since the joint training objective requires the latent representation to be capable of reconstructing the input paragraph, in addition to generating a title, the learned representation may better capture the entire structure (meaning) of the paragraph. We also empirically observed that titles generated under the joint training objective are more likely to use the words appearing in the corresponding paragraph (i.e., more extractive), while the the titles generated using the purely supervised objective Lsup, tend to use wording more freely, thus more abstractive. One possible explanation is that, for the joint training strategy, since the reconstructed paragraph and title are all generated from latent representation h, the text fragments that are used for reconstructing the input paragraph are more likely to be leveraged when “building” the title, thus the title bears more resemblance to the input paragraph.
If an example is forgotten at least once or is never learnt during training, we call it “forgettable”. To remove the effect of bias in label distribution, we sample forgettable examples for each label to keep the label distribution the same as the original MNLI training data (i.e., 33% from each of the three label). It is worth noting that the larger the model, the fewer the forgettable examples. We also include the performance of the models on the development set of MNLI.
A growing body of literature suggests that increasing the capacity of deep networks results in better generalization Belkin et al. These results usually assume there is no distribution shift between train and test sets. We investigate whether robustness to the distribution shift studied in this paper may appear “for free” in models with a larger number of parameters. To that end, we apply our method to the “large” version of BERT, BERTLARGE , which achieves better performance in the MNLI dataset Devlin et al. We see that BERTLARGE generalizes on HANS significantly better than BERTBASE (72.3% vs 58.3%), confirming – in this setting – that larger models seem more robust. We also observe a +5% increase in performance as a result of finetuning on forgettables from weaker models, supporting the applicability of the method to larger architectures.
On snli, the BiLSTM achieves an accuracy of 81.2% with a softmax classifier and 81.0% with DkNN.
The first experiment studies the performance of the knowledge-based weak PLDA training, and compare it with the strong training that uses the human-labelled data. For comparison, the results with cosine scoring (NO PLDA) are also presented. We first observe that most of the PLDA models outperform the cosine scoring. This is particular interesting for the weak training approach, where only inaccurate labels are used. This confirms our conjecture that it is possible to use weak labels derived from prior knowledge to train PLDA, at least in scenarios where the prior knowledge is correct.
Besides the analysis on different reasoning types, we also look into the performance over questions with different first tokens in the development set, which provide us an automatic categorization of questions. Since the tweet contexts are short, there are only a small number of named entities to choose from, which could make the answer pattern easy to learn. On the other hand, the neural models fail to perform well on the “Why” questions, and the results of neural baselines are even worse than that of the matching baseline. We find that these questions generally have longer answer phrases than other types of questions, with the average answer length being 3.74 compared to 2.13 for any other types. Also, since all the answers are written by humans instead of just spans from the context, these abstractive answers can make it even harder for current models to handle. We also observe that when people write “Why” questions, they tend to copy word spans from the tweet, potentially making the task easier for the matching baseline.
A very recent work Therefore, they establish a new EA benchmark that follows real-life distribution. The evaluation benchmark consists of both cross-lingual datasets, i.e., EN-FR, EN-DE, and mono-lingual datasets, i.e., DBP-WD, and DBP-YG. The degree of an entity is defined as the number of relational triples in which the entity participates. -S1T1 that in EN-FR and DBP-YG, entities with degree less than three account for over 50%, and nearly half of the entities in EN-DE and DBP-WD are with degree 1 or 2. It confirms that the majority of the entities in KG are long-tail and have very few connections to others. Moreover, it follows that the performance of long-tail entities is much worse than those of higher degrees (the accuracies of high-degree entities triple and even quadruple those of entities with degree 1), despite that RSNs is the leading method on the benchmark. This suggests that current methods fail to effectively handle entities in tail, and hence, restrains the overall performance. Thereby, it is of significance to revisit the EA pipeline with a particular emphasis on long-tail entities.
Solutions in the first group merely harness structural information for aligning. BootEA and KECG achieve better performance than MTransE and IPTransE due to the carefully designed alignment-oriented KG embedding framework and attention-based graph embedding model, respectively. RSNs further advances the results by taking into account long-term relational dependencies between entities, which can capture more structural signals for alignment. TransEdge attains the best performance due to its edge-centric KG embedding and the bootstrapping strategy. MuGNN fails to yield promising results as there are no aligned relations on SRPRS, where rule transferring cannot work, and the number of detected rules is rather limited. Noteworthily, Hits@1 values on most datasets are below 50%, showing the inefficiency of solely relying on KG structure, especially when long-tail entities account for the majority.
Note that for DAT, the degree refers to the initial degree distribution (as the completion process alters entity degrees). Although for popular entities, the performance also increases, the improvement over RSNs narrows greatly in comparison to the entities in tail. Moreover, DAT outperforms RDGCN across all degree groups in four datasets, despite that both of them utilize entity name information as an external signal for EA.
This task of multi-label classification is different from previous tasks in that the model needs to predict the binary label for each of the 11 classes given a tweet. The task is difficult in terms of three aspects. Firstly, some of the classes have opposite emotions (such as optimism and pessimism) but may have been labeled both as true. Secondly, it is not trivial to distinguish similar emotions such as joy, love, and optimism, which will include a lot of noise in the labels and make it hard to perform classification during training. For emotion and sentiment intensity datasets, each tweet sample has both an ordinal label (coarse; {0,1,2,3} for emotion, {-3,-2,-1,0,1,2,3} for sentiment) and real-value regression label (fine-grained; [0,1]).
Also, SPR factor as a new feature has been instrumental in categorizing rumors. On the other hand, the classification accuracy using the combination of these previous and new features is improved from 0.762 to 0.828 using RF classifier. This result demonstrates the positive impact of new proposed features and "power of spread" factor on improving the classification accuracy. The result of this analysis was an average F-measure of 0.828. In this study, the average F-measure is improved from 0.762 (first experiment) to 0.828 (fourth experiment) by focusing only on content features.
In the table, the suffix W indicates that their down-weighting scheme has been used, while the suffix R indicates the removal of the first principal component. They report values of a∈[10−4,10−3] as giving the best results and used a=10−3 for all their experiments. We observe that our results are competitive with the embeddings of Arora et al. It is important to note that the scores obtained from supervised task-specific PSL embeddings trained for the purpose of semantic similarity outperform our method on both SICK and average STS 2014, which is expected as our model is trained purely unsupervised.
Also, to prevent overfitting, for each sentence we use dropout on its list of n-grams R(S)∖{U(S)}, where U(S) is the set of all unigrams contained in sentence S. After empirically trying multiple dropout schemes , we find that dropping K n-grams (n>1) for each sentence is giving superior results compared to dropping each token with some fixed probability. This dropout mechanism would negatively impact shorter sentences. The regularization can be pushed further by applying L1 regularization to the word vectors. Encouraging sparsity in the embedding vectors is particularly beneficial for high dimension h. The additional soft thresholding in every SGD step adds negligible computational cost. We train two models on each dataset, one with unigrams only and one with unigrams and bigrams. Our C++ implementation builds upon the FastText library Joulin et al. We will make our code and pre-trained models available open-source.
The evaluation uses 1M candidate documents for en-fr and 0.6M for en-es. We obtain the best performance from our hierarchical models, HiDE∗. Adapting the sentence embeddings prior to pooling, HiDEDNN→pooling performs better than attempting to adapt the representation after pooling, HiDEpooling→DNN. Document BoW embeddings outperform Sentence-Avg, showing training a simple model for document-level representations (DAN) outperforms pooling of sentence embeddings from a complex model (Transformer).
P@1 is evaluated using all of the UN documents in a target language as translation candidates. The prior state-of-the-art is \newcitejakob2010. Using both the official and noisy sentence segmentations, HiDEDNN→pooling outperforms \newcitejakob2010, a heavily engineered system that incorporates both MT and monolingual duplicated document detection.
We further explore how the performance of sentence-level models affect the performance of document-level models that incorporate sentence-embeddings. We use different encoder configurations to produce sentence embeddings of varying quality as expressed by P@1 results for sentence-level retrieval on the UN dataset. While sentence encoding quality does impact document-level performance, the HiDE model is surprisingly robust once the sentence encoder reaches around 66% P@1, whereas the Sentence-Avg model requires much higher quality sentence-level embeddings (around 85% for en-fr, and 80% for en-es). The robustness of HiDE model provides a means for obtaining high-quality document embeddings without high-quality sentence embeddings, and thus provides the option to trade-off sentence-level embedding quality for speed and memory performance.
For the ADR task, to encode the output labels we use the IO encoding scheme where each word is labeled with one of the following labels: (1) I-ADR (ADR mention), (2) I -Other (mention category other than ADR), (3) O (others), (4) PAD (padding token). Since our entity of interest is ADR, we report the results on ADR only. An example tweet annotated with IO-encoding is as follows. “@BLENDOSO LamictalO andO trileptalO andO seroquelO ofO courseO theO seroquelO IO takeO inO severeO situationsO becauseO weightI-ADR gainI-ADR isO notO coolO”. We report the F1-score, Precision and Recall computed using approximate matching as follows. The F1-score is the harmonic-mean of the Precision and Recall values. All results are reported using 10-fold cross-validation along with the standard deviation across the folds. For both the datasets, it should be noted that Cocos et al. It is clear that re-implementation with Adam optimizer enhances the performance, which is consistent with the general consensus around Adam optimizer. The corresponding results can be seen in row 3. It is clear that adding KB-based embeddings enhances the performance over the baseline, due to the external knowledge added in the form of KB embeddings.
S3SS5SSS0Px1 Setup. We consider the setup where models are presented with examples from each class sequentially (single-task continual learning). Here, each model has to learn information from newly introduced examples to be able to correctly classify documents into this new class, but they cannot train from previously seen classes while doing so. Last, we experiment with the generative shared LSTM model. Our training procedure for this model is as follows. We first train the LSTM language model part on a large amount of unlabeled data. When training the language model on unlabeled data, we remove the class-specific bias component by and set the class embedding vy to be a random vector ~y with a bounded norm ∥~vy∥2≤1. After we pretrain the shared components, we freeze their parameters and tune the class embedding vy as well as the class-specific softmax bias by on the labeled training data. The benefit of this training—compared to having a separate LSTM model for each class—is that it is faster to train (in the presence of examples from a new class). Given a new class y, we only need to learn two vectors: vy and by. that the generative shared LSTM model trained with this procedure approaches the performance of its equivalent model that can see examples from all classes, and performs competitively with the generative independent LSTMs model.
S3SS6SSS0Px3 Generative LSTM. The model learns to generate from points in the label semantic space using the labeled documents. The model may infer how to generate a document about politics without ever having seen such an example in the training data. Similar to the discriminative case, we fix V, which in this case plays the role of class embeddings, and train other parts of the model on all training data except examples from the hidden class (we use the generative shared LSTM since naturally the generative independent LSTMs cannot be used in this experiment). We observe on the development set that this model is able to predict examples from the unseen class with high precision, but very low recall (≈1%). We design a self-training algorithm that add predicted hidden class examples from the development set to the training set and allow the model to train on these predicted examples. We can see that for most hidden classes, the generative model achieves good performance. For example, on the AG News dataset, the model performs reasonably well for any of the hidden classes. For a more difficult dataset where the overall accuracy is not very high such as Yahoo, the precision of the hidden class is lower, and as a result the recall also suffers. Nonetheless, the model is still able to achieve reasonable overall accuracy in some cases (recall that the accuracy of this model trained on the full dataset without any hidden class is 69.3).
In this case, the model does not perform as well since the precision of predicting hidden classes drops significantly, introducing too much noise in the training data. However, the model is still able to learn some useful information since the overall accuracy is still higher than 50% (the accuracy of models that are only trained on two classes without any zero-shot learning of the hidden classes).
On the English-to-German translation task, our model achieves a BLEU score of 41.0, outperforming all of the previously published single models by a large margin of 2.3 BLEU score. On the English-to-Czech translation task, our model also outperforms the best previously reported single models by an impressive margin of 2 BLEU points. In fact, our single model already outperforms previous state-of-the-art models that use ensembling. The advantages of our method are also verified by the metric chrF++.
The human evaluation results for unsupervised paraphrase generation using standard VAE, SIVAE-i and SIVAE-c are shown in Paraphrases generated by SIVAE- i are more diverse under different syntactic templates, compared to SIVAE-c and standard VAE. All three models show better paraphrasing performance on the wiki90M dataset.
Our first observation is that the rate of non-aligned tokens for the direction Spanish-polysynthetic language is far lower than the other way around. The highest rates of non-aligned tokens are found for Wixarika and Yorem Nokki with 0.617 and 0.616, respectively. For Nahuatl and Spanish, this rate is with 0.448 notably lower. On the other hand, the translation direction from Spanish to our polysynthetic languages seems to work much better and shows less variability. The lowest rate is obtained for Yorem Nokki with 0.264, followed by Nahuatl with 0.277, and Wixarika with 0.35. For both directions, translation with Wixarika got the highest non-alignment rates. This suggests that the phenomenon might be related to the number of morphemes per word: \newcitekann2018 showed that Wixarika has the highest morphemes-per-word rate among the languages considered here.
Row 34, “MVCNN (overall)”, shows performance of the best configuration of MVCNN, optimized on dev. This version uses five versions of word embeddings, four filter sizes (3, 5, 7, 9), both mutual-learning and pretraining, three convolution layers for Senti140 task and two convolution layers for the other tasks. Overall, our system gets the best results, beating all baselines.
To avoid having examples that closely resemble one another in training and test, the projects in the training, test, and validation sets are disjoint, similar to \newciteMovshovitz-AttiasCohen13PredictingProgrammingComments. Of the 7,239 examples in our final dataset, 833 of them were extracted from the diffs used in \newcitepanthaplackel2020associating. Including code and comment tokens that appear at least twice in the training data as well as the predefined edit keywords, the code and comment vocabulary sizes are 5,945 and 3,642 respectively.
: We report automatic metrics averaged across three random initializations for all learned models, and use bootstrap tests Berg-Kirkpatrick et al. While reranking using Cold appears to help the generation model, it still substantially underperforms all other models, across all metrics. Although this model is trained on considerably more data, it does not have access to Cold during training and uses fewer inputs and consequently has less context than the edit model. Reranking slightly deteriorates the edit model’s performance with respect to SARI; however, it provides statistically significant improvements on most other metrics.
The layer-wise residual connection (Res-L) structure can be seen in Fig. As we expected, it’s not necessary to add residual connections to shallow networks. Performances degrade when residual connections are used in a 2-layer LSTM. Res-L always performs better than Res-I. This is reasonable because Res-L tries to learn the residue of the high-level abstract feature while Res-I just learns the residue of the input feature. When the LSTM is as deep as 4 layers, Res-L starts to work and the lowest CERs are achieved when the LSTM has 8 layers. As training a 8-layer LSTM is time-consuming, we perform the GAN experiments with a 4-layer LSTM generator in the following.
Our model improved the macro-AUC by 0.013, the micro-F1 by 0.013, the precision@8 by 0.025, the precision@15 by 0.023. In addition, our model achieved comparable performance on the micro-AUC and a slightly worse macro-F1. More importantly, we observed that our model is able to attain stable good results from the standard deviations.
Perotte et al. \shortciteperotte2013diagnosis used the SVM to predict ICD codes from clinical text and their method obtained 0.293 micro-F1. By contrast, our model outperformed their method by 0.171 in micro-F1. Baumel et al. \shortcitebaumel2018multi utilized the attention mechanism and GRU for automated ICD coding. Our model outperformed their model by 0.098 in micro-F1. Our model also outperformed the state-of-the-art model, CAML or DR-CAML, by 0.024, 0.002, 0.003, 0.007 and 0.021 in all evaluation metrics.
For multi-model tasks, we can do either Single Model prediction (SM), restricting training and testing of the predictor within a single model, or Multi-Model (MM) prediction using a categorical model feature. For all tasks, our single model predictor is able to more accurately estimate the evaluation score of unseen experiments compared to the single model baselines, confirming our hypothesis that the there exists a correlation that can be captured between experimental settings and the downstream performance of NLP systems. The language-wise baselines are much stronger than the simple mean value baseline but still perform worse than our single model predictor. Similarly, the model-wise baseline significantly outperforms the mean value baseline because results from other models reveal much information about the dataset. Even so, our multi-model predictor still outperforms the model-wise baseline.
The partition of the folds is consistent between the human study and the training/evaluation for the predictor. While the first sheet is intended to familiarize the participants with the task, the second sheet fairly adopts the training/evaluation setting for our predictor. In addition, the participants make even more accurate guesses after acquiring more information on experimental records in other folds. In neither case, though, are the human experts competitive to our predictor. In fact, only one of the participants achieved performance comparable to our predictor.
In that context, we consider multi-sense word anchors as noise for the unsupervised mapping model in MUSE. So we remove all multi-sense word anchors color =orangetodo: color =orangepz : There is some redundancy here We apply the spectral clustering algorithm Wang et al. Then for each multi-sense word, we replace its average anchor embedding with cluster-level average anchor embeddings. Removing noisy information about multi-sense words is therefore very beneficial in this case. Replacing multi-sense word average anchor embeddings with cluster-level average anchors embeddings achieves the best result by using 1st LSTM output layer of ELMo.
In particular, we observe that the improvements across different models and the development and test sets are unstable. The evaluation on the dev set shows that gaze features from early and context group used as multiple auxiliary tasks (early and context feats aux) modestly improve the model in comparison with the baseline. With respect to the results on the test set, the most informative features are: mean fix dur, which improves the LAS by +0.46, and first fix dur by +0.33.
Under this setup, the gains decrease in comparison with the results on the parallel setup. This could be partially caused by not using parallel data. The improvements seem to be more consistent between the development and test sets. This could be related to the fact that we use a larger (more representative) treebank. When looking at the dev set scores, the most discriminative gaze feature is mean fix dur that increases LAS by +0.17 and first fix dur by +0.14. On the other hand, evaluation on the test set shows that the most informative gaze features are from the context group learned as multiple auxiliary tasks (context feats aux) and they improve the LAS score by +0.18, followed by fix prob and w−1 fix prob with +0.13, total fix dur with +0.12 and late feats aux with +0.10. Results from both datasets suggest that grouping gaze features and treating them as multiple auxiliary tasks can improve the model’s learning.
We modify the dataset to include only articles with abstracts, resulting in total of 42,000 out of 57,037 articles. We use abstracts as the gold summary and train the model to extract up to 15 sentences that are most similar to the abstract. To simulate an online training environment, we divide the articles into tasks of 5,000 articles, ordered by ascending publication time. The model initially learns on tasks with older articles and gradually transits through to the newest articles in an online fashion.
We report two runs of “ETS” as their performances varied significantly between them. The proposed technique performs better than “ETS” as well as the best performing entries across all test sets in terms of both the metrics. It is important to note that all techniques use labeled data in supervised learning mode whereas the proposed technique requires labeled data only from the source question. This is a significant feature of the proposed technique, demonstrating that using labeled data from the source question along with generic similarity measures between the student and model answers can result in efficient ASAG in the target question without any labeled data.
For the other three datasets with ordinal labels, there was no prior work based on transfer learning approach to ASAG. We followed the convention in transfer learning literature of comparing against a skyline and a baseline: Baseline (Sup-BL): Supervised models are built using labeled data from a source question and applied as-it-is to a target question. Skyline (Sup-SL): Supervised models are built assuming labeled data is available for all questions (including target). Performance is measured by training a model on every question and applied on the same. Performance of transfer learning techniques should be in between the baseline and skyline - closer to the skyline, better it is. : Table V compares the question-wise MAE of the proposed algorithm against the skyline and baseline on all the 21 questions in the CSD dataset. For Sup-BL and the proposed technique, for each question we consider all remaining 20 questions as source one at a time and report the best MAE obtained. Firstly, we note that all methods exhibit performance variations across questions. Variance of SUP-BL, CCA classifier, SUP-SL and the proposed technique are 0.12, 4.46, 0.16 and 0.24 respectively. Thirdly, the proposed algorithm which combines the text and numeric classifier in a weighted ensemble yields significantly lower error rates than the constituent classifiers. This demonstrates the fact that the ensemble exploits their complimentary nature effectively towards improving the overall performance.
This implies that most of the document clusters (events) we obtain are pure: each event only contains documents that talk about the same event. In comparison, the homogeneity for the other two methods is much lower. The reason is that we adopt two layers of graph-based clustering to group documents into events with more appropriate granularity.
For simplicity, the example assumes that the reference order is monotonic and that hypotheses and reference translations contain exactly the same words. According to both metrics, hypothesis (a) is worse than (b), although in (a) only two adjacent words are swapped while in (b) the two halves of the sentence are swapped. \namecite Talbot:11 introduce yet another reordering-specific metric, called fuzzy reordering score (FRS) which, like the KRS, is independent from lexical choice and measures the similarity between a sentence’s reference reordering and the reordering produced by an SMT system (or by a pre-ordering technique). However, while \nameciteBirch:10 employed the Kendall’s Tau between the two sentence permutations, \nameciteTalbot:11 count the smallest number of chunks that the hypothesis permutation must be divided into to align with the reference permutation. This corresponds precisely to the fragmentation penalty of METEOR except that the alignment is performed between permutations and not between translations. Like METEOR, FRS makes no difference between short and long-range reordering errors (cf. \namecite Stanojevic:14: EMNLP argue for a hierarchical treatment of reordering evaluation, where word sequences can be grouped recursively into larger blocks. Given this factorization, the counts of monotone (1 2) versus other permutation nodes — (2 1), (3 1 4 2), etc. — are used as features in a linear model of translation quality (BEER) trained to correlate with the human ranking of a set of MT system outputs. \nameciteStanojevic:14: SSST extend this work with a stand-alone reordering metric that considers all possible tree factorizations of a permutation (permutation forest) and that gives recursively less importance to lower nodes in the tree (i.e. covering smaller spans). Hierarchical permutation metrics are shown to better correlate with human judgements than string-based permutation metrics like the Kendall ’s Tau distance K.
Model Accuracy: One of the most important metrics for evaluating our system is the overall accuracy for intent detection as we strictly control the number of replies for each intent in the product. The naive frequency-based approach has an accuracy of 15.5% which is simply the population of the top-1 intent classes. The best performing model is Char-CNN model with an accuracy of 77.2%, followed by the NNC with an accuracy of 75.9%. Word-CNN performs slightly (75.6%) worse compared to NNC.
The EEND models trained with simulated training set were overfitted to the specific overlap ratio of the training set. We expected that the overfitting would be mitigated by using domain adaptation. As expected, the domain adaptation significantly reduced the DER; our system thus achieved even better results than those of the x-vector-based system.
We see that initially, adding the set split with BPE using 1,000 merge operation (+1000 row) causes the performance to drop slightly. Nonetheless, the performance increases when more data are added (i.e. +5000, +10000, +20000 rows). Moreover, it even outperforms the model trained with the Deepcut-split dataset alone.
As it can be seen, our system obtains the best published results by a large margin, surpassing previous unsupervised NMT systems by around 10 BLEU points in French-English (both directions), and more than 7 BLEU points in German-English (both directions and datasets). Our unsupervised tuning procedure further improves results, bringing an improvement of over 1 BLEU point in both French-English directions, although its contribution is somewhat weaker for German-to-English (almost 1 BLEU point in WMT 2014 but only 0.2 in WMT 2016), and does not make any difference for English-to-German.
In addition to that, the results for the constrained variants of this SMT system justify some of the simplifications required by our approach. In particular, removing lexical reordering and constraining the phrase table to the most frequent n-grams, as we do for our initial system, has a relatively small effect, with a drop of less than 1 BLEU point in all cases, and as little as 0.28 in some. This shows the importance of tuning in SMT, suggesting that these results could be further improved if one had access to a small parallel corpus for tuning.
We compare human performance to BERT and BigBird. However on MRPC, QQP, and QNLI, Bigbird and BERT outperform our annotators. The results on QQP are particularly surprising: BERT and BigBird score over 12 F1 points better than our annotators. Our annotators, however, are only given 20 examples and a short set of instructions for training, while BERT and BigBird are fine-tuned on the 364k-example QQP training set. In addition, we find it difficult to compose concise instructions for QQP that actually match the supplied labels. If given more training data, it is possible that our annotators could better learn relatively subtle label definitions that better fit the corpus.
For single dialogue features, intent and entities types provide the largest performance boost compared to other single dialogue features, and this demonstrates the effectiveness of using intent and types of entities for discourse relation identification. Other three features maintain the same level of performance, except a large drop in precision with respect to sentiment. One possible explanation is that our sentiment classification results are obtained using the Sentiment Annotator from Stanford CoreNLP Toolkit, which is trained on movie reviews corpus Manning et al. et al. The nature of training data is not suitable for our dialogue corpus in this task.
We use the default parameter for DER models. We also show the result of the DER model trained and tested on the PDTB dataset for comparison marked as “DER (PDTB)”. The first observation is that the DER model performs surprisingly well with an F1 score of 0.76 on the new dialogue discourse relation dataset Edina-DR with p-value of 0.008, which demonstrates its strong adaptability to the task of discourse relation identification in dialogues. Comparing the same DER model on PDTB, the large drop in F1 score shows the difference between formal and informal data. We also find that the model with dialogue features enhance the performance by 1% on F1 score with p-value 0.006, which indicates the potential of using dialogue features to further enhance discourse relation identification models.
As can be observed from Tab. This confirms the advantage of the 2D-CTC formulation, which is the essential reason for the improvements in recognition performance. ’s decoder, the state-of-the-art method up to date, with the same base model as 2D-CTC. The evaluated performances and training/test speed are shown in Tab. As can be seen, the proposed 2D-CTC model achieves higher or comparable performances in regular text datasets and outperforms attention decoder in irregular text datasets.
The major difference between the loss function of 2D-CTC and that of vanilla CTC is that the former introduces another dimension in the formulation, thus theoretically has higher computation complexity. As shown in Tab. The actual runtime of the 2D-CTC loss is slightly longer than that of the vanilla CTC loss, which means that 2D-CTC can obtain notable improvement in accuracy, with only a marginal decrease in speed.
Results. This result is expected since the structure of SQL query is essentially a graph despite its expressions in sequence and a graph encoder is able to capture much more information directly in graph. Among all Graph2Seq models, our Graph2Seq model performed best, in part due to a more effective graph encoder. Tree2Seq achieves better performance compared to Seq2Seq since its tree-based encoder explicitly takes the syntactic structure of a SQL query into consideration. Two variants of the Graph2Seq models can substantially outperform Tree2Seq, which demonstrates that a general graph to sequence model that is independent of different structural information in complex data is very useful. Interestingly, we also observe that Graph2Seq-PGE (pooling-based graph embedding) performs better than Graph2Seq-NGE (node-based graph embedding). One potential reason is that the node-based graph embedding method artificially added a super node in graph which changes the original graph topology and brings unnecessary noise into the graph.
As a second evaluation, we consider the task of generating full descriptions. BLEU scores are reported up to 4-gram. For comparison, we include results from recently proposed models. Our model, despite being simpler, achieves similar results to state of the art results. It is interesting to note that our results are very close to the human agreement scores.
We configured 18 systems for English and MT queries, by combining each of the four basic systems: (a) Keyword, (b) Word Embedding, (c) DBpedia and (d) Hypernym. The system (a) is the baseline system whereas (b), (c) and (d) are systems based on QE using Word Embedding, DBpedia and Hypernym knowledge graph respectively. The average query lengths for baseline systems are 18.67(EN) and 19.96(MT) words. And the average word additions are Word Embedding: 30.98(EN) and 35.34(MT); DBpedia: 25.81(EN) and 31.32(MT); Hypernym: 21.58(EN) and 29.04(MT); Best system: 78.3(EN) and 95.6(MT). All the systems are experimented in two settings - Dev and Test. The search ranking scores are calculated for all 18 systems using BM25 algorithm. We tuned BM25 hyper-parameters, k1 and b on Dev set to get the optimized values where k1 controls non-linear term frequency normalization and b controls to what degree document length normalizes tf values. The score for each query is calculated based on 10 existing documents to re-rank them. The ”System” column compares QE, Keyword baseline without QE, and previously-published state-of-the-art methods in this task (UH-PRHLT, SVM+TK); the QR column shows whether the query was English (monolingual setup) or Machine-translated English (cross-language setup). The Δ column displays the difference between test MAP against the baseline.
HyperIM effectively takes advantage of the label hierarchical structure comparing with EXAM, SLEEC and DXML. EXAM uses the interaction mechanism to learn word-label similarities, whereas clear connections between the words and the label hierarchy can’t be captured since its label embeddings are randomly initialized. The fact that HyperIM achieves better results than EXAM further confirms that HyperIM benefits from the retention of the hierarchical label relations. Meanwhile, the word embeddings learned by HyperIM have strong connections to the label structure, which is helpful to the measurement of word-label similarities and the acquirement of the label-aware document representations. SLEEC and DXML take the label correlations into account. However, the label correlations they use are captured from the label matrix, e.g. embedding the label co-occurrence graph, which may be influenced by tail labels. For HyperIM, the label relations are determined from the label hierarchy, so the embeddings of labels with parent-child relations are dependable to be correlated.
4.3.1 Effects of the word-length bias The integration of a word-bias in the attention mechanism seems detrimental to segmentation performance, and results obtained with bias are lower than those obtained with base, except for the sentence exact-match metric (X). To assess whether the introduction of word-length bias actually encourages target units to “attend more” to longer source word in bias, we compute the correlation between the length of source word and the quantity of attention these words receive (for each source position, we sum attention column-wise: ∑i~αij). bias increases the correlation between word lengths and attention, but this correlation being already high for all methods (base, or aux and aux+ratio), our attempt to increase it proves here detrimental to segmentation.
Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. , Advising Finegan-Dollak et al. These datasets have been studied for decades in both the NLP community Warren and Pereira et al. Yaghmazadeh et al. Compared with other datasets, Spider contains databases with multiple tables and contains SQL queries including many complex SQL components. For example, Spider contains about twice more nested queries and 10 times more ORDER BY (LIMIT) and GROUP BY (HAVING) components than the total of previous text-to-SQL datasets. Spider has 200 distinct databases covering 138 different domains such as college, club, TV show, government, etc. Most domains have one database, thus containing 20-50 questions, and a few domains such as flight information have multiple databases with more than 100 questions in total. On average, each database in Spider has 27.6 columns and 8.8 foreign keys. The average question length and SQL length are about 13 and 21 respectively. Our task uses different databases for training and testing, evaluating the cross-domain performance. Therefore, Spider is the only one text-to-SQL dataset that contains both databases with multiple tables in different domains and complex SQL queries It tests the ability of a system to generalize to not only new SQL queries and database schemas but also new domains.
Finally, we propose two models as baselines for WikiReading Recycled: the reproduced Basic seq2seq and the dual-encoder model. In addition, we evaluate an ensemble of four best-performing checkpoints on the validation set. The dual-encoder model outperforms the Basic seq2seq as in the case of WikiReading task. The former achieved Mean-MultiLabel-F1 of 79.5%. Additionally, the ensemble submission improved the single-best model by 0.5 points.
The first part of the table refers to previous works, whereas the second part presents our experiments and it is separated with a double horizontal line. The evaluation results confirm that our model performs constantly better then current state-of-the-art baselines, which supports the effectiveness of the approach.
In the first stage, we select candidate catchphrases from each patent’s abstract. Empirically, we observe that all catchphrases are n-gram noun phrases, for example, unigrams (e.g. communication, dielectrometry, etc.), bigrams (e.g. consecutive bit, voice synthesizer, etc.), trigrams (e.g. integrated circuit device, hydrogen chloride gas, etc.) or quadrigrams (e.g. commercially available synthesis tool, electric signal processing board, etc.). We, therefore, perform part-of-speech-tagging (POS) of each abstract text to identify noun phrases. Currently, we leverage python’s state-of-the-art NLP library Note that, we experimented with two text processing approaches before noun phrase identification: (i) with stopwords (WS), and (ii) without stopwords (WOS). WS represents that no stopwords were removed from the abstracts, whereas, WOS represents that all stopwords in the abstract text were removed beforehand. Abstracts with stopwords (WS) led to better quality extraction results due to the existence of stop-words in noun phrases.
We outperform all baselines by a substantially high margin. The second best system in terms of precision is KEA, whereas the second-best system in terms of recall is a mix between KLIP and KEA. The baseline Legal performed worst among all the baselines, which is possible because of the fact that the authors take a logarithm of the sum of all the scores rather than the sum of the logarithms of the scores. The former measure undermines the contribution of the scores from each term and is therefore ineffective and is rather unintuitive.
We show results for the whole test dataset, which includes queries with and without user context, as well as the dataset with user context only. To assess statistical significance we have performed 1,000 bootstrap samples over the test queries and computed 95% confidence intervals using those samples. the plots on the right are for the context only dataset. We have only plotted one variant of each metric since the others are very similar.
ResNet architecture ( Before the introduction of ResNet’s, deep CNNs were used for most image processing tasks. Hence we used different types of ResNet model architectures, namely ResNet-101 and ResNet-152 to check the classification accuracy in different cases. ResNet model architecture was first tested on the ImageNet dataset where the number of classes for classification was 1000. Since our task of emotion classification requires us to learn features from both text and image, we use the ResNet model to learn a feature vector from an image. We use the implementation of ResNet by Kotikalapudi which is open-sourced and available on macro F1 score observed using different ResNet model architectures.
Barbieri et al. We investigated the same in the context of multimodal emotion classification using SOTA model architectures, i.e., ResNet-152 for learning image features, attention mechanism to learn features from text and merge these features to form a hidden layer. We then use a softmax layer on top of this for classification. Finally, we combined the three modalities – textual, visual, and emoji. We then merge the outputs of these layers and train a softmax on top of this for classification. This combination of all the modalities results in better classification accuracy.
CELT with BERT pre-train significantly outperforms the baselines on both datasets. Compared to ATIS, Snips includes multiple domains and has a larger vocabulary. For the more complex Snips dataset, CELT with BERT pre-train achieves intent accuracy of 98.3% (from 97.3%), slot F1 of 96.4% (from 91.8%), and sentence accuracy of 91.9% (from 80.9%). On ATIS, CELT achieves intent accuracy of 97.4% (from 95.0%), slot F1 of 95.9% (from 95.2%), and sentence accuracy of 87.9% (from 83.4%). The gain from CELT with BERT pre-train on Snips over the baselines is much more significant on slot F1 and sentence frame accuracy than intent accuracy. Further analysis shows that 53.4% of slots in the Snips test set can be found in Wikipedia. Since BERT pre-training data includes English Wikipedia, the model may have encoded the knowledge in representations and hence improves slot F1 and sentence accuracy. Ablation analysis on Snips shows that when fine-tuning the BERT pre-train model separately for IC and SF, intent accuracy drops to 97.8±0.4% (from 98.3±0.3%), and slot F1 drops to 96.3±0.1% (from 96.4±0.2%). These results demonstrate that joint modeling in CELT improves the performance for both tasks. We compare CELT models with different fine-tuning epochs.
when doing so. While our numbers are generally better, the results are also consistent with Xie et al.
Our next set of experiments were run using 10-fold cross-validation on Set A in order to have a more stable estimate of the feature performance. We performed these evaluations for the concatenation of the five features for four variations: extracted from the post text, extracted from the target title, extracted from both and concatenated, and concatenated from both followed by feature selection. A clear improvement can be seen for the concatenation of the two sources (post text and target title). Again, feature selection led to deterioration of the overall performance.
Dist-n and Distc-n (n=1,2,3) scores (Li et al. Dist-n is an indicator of repetitiveness within a single summary while Distc-n indicates the diversity of different generations. Finally, as done by Chu and Liu , we use a classifier to check whether the sentiment of the summary is consistent with the sentiment of input reviews (Sentiment Acc. We extend this method to check whether the correct product category can also be inferred from the summary, we report Fcategory the micro F-score of the multi-label category classifier. The proposed multi-input self-supervised model with control codes perform consistently better in the Yelp dataset across the benchmarked models, inlcuding the recent neural unuspervised models of MeanSum and H-VAE. Note that because of the concurrent nature of the Bražinskas et al. For MeanSum we re-run their provided checkpoint and run evaluation through the same pipeline.
On the summary level our model outperforms all the baselines, meaning, our model is capable of generating more rich and less repetitive summaries. On the level of all generations our model generates text with more diversity than MeanSum. In general however extractive models tend to have more diversity on the corpus level as they directly copy from each input separately, while abstractive models tend to learn repetitive patterns present in the training set.
We grouped all the systems into three categories: unsupervised (Unsup.), supervised (Sup.), and semi-supervised (Semisup.) We found that the supervised systems worked much better than the unsupervised counterparts, which implies that the small amount of labeled data is necessary for better performance. We also noticed that within the supervised systems, the systems using deep learning (CNN or LSTM) models worked better than the systems using metric learning method, which shows the power of deep learning models for short text modeling. Our “semi-cnn” system got the best performance on almost all the datasets.
Note that the measure of interest for LAMBADA is the average success of a model at predicting the target word, i.e., accuracy (unlike in standard language modeling , we know that the missing LAMBADA words can be precisely predicted by humans, so good models should be able to accomplish the same feat, rather than just assigning a high probability to them). However, as we observe a bottoming effect with accuracy, we also report perplexity and median rank of correct word, to better compare the models.
Both Is and Csls demonstrate compelling empirical performance in mitigating the hubness problem. R@Ks and also Med r, Mean r are improved by a large margin with both methods. In most configurations, Csls is slightly better than Is on improving text→image inference while Is is better at image→text. The best results (line 3.8, 3.9) are even better than the recently reported state-of-the-art Wu et al. This suggests that the hubness problem deserves much more attention and careful selection of inference methods is vital for text-image matching.
In this section, we evaluate the contribution of contextual information, with varying levels of supervision psup. ∅, V, C and (C,V) — we observe similar trends for the other context models. Results highlight that contextual knowledge acquired from the source domain can be transferred to the target domain, as M(CSH) significantly outperforms the Random baseline. As expected, it is not as useful as visual information: M(V)MFR\scalebox0.8\textless M(CSH), where MFR\scalebox0.8\textless means lower MFR scores, i.e. better performances. Interestingly, as the learned prior model M(∅) is also able to generalize, we show that visual frequency can somehow be learned from textual semantics, which extends previous work where word embeddings were shown to be a good predictor of textual frequency (frequence).
Only 11% of the instances in the transformed sample ITA1 contain word pairs that have polarity other than contradiction. Thus, a model relying only on this factor could achieve an accuracy of 89%. We investigate if the predicted labels on instances in ITA1 are associated with the polarity of the transformed word pair. For all models, independence tests are highly significant (CE: χ2(6)=30.69, DAM: χ2(6)=101.26, ESIM: χ2(6)=64.40). For example, when the polarity is contradiction, around 98.5% of the predictions are contradictions; however, this figure changes when the polarity is neutral where the rate of correct predictions (contradictions) fall to 80.7%, and a more dramatic fall is observed when the word pairs are unseen (polarity none) where only 50% of the predictions are correct. This is strong evidence that the models learned to rely on polarity.
First, we determined the correlation of the cosine similarity (inner product) between text and math encodings. For each document, section or abstract, we calculated the similarities with all the other docs/secs/abs in both text and math encodings (different vector spaces) seperately. This means, we investigated whether if two documents are similar in their text encoding, they are also similar in their math encoding. The low correlations indicate that in principle, the independence of text and math encodings leave a potential for improvement of ML algorithms by combining the two, which was explored. Besides, it suggests that in a Recommender System for STEM documents, it will be beneficial to provide the user with weighting parameters for text and math (if relatively uncorrelated), to customize the recommendations. While for the classification, the Text_tfidf encoding was outperforming the others, for the clustering doc/sec2vecMath_surroundings and abs2vecMath_opid encodings are the best. For both classification and clustering, the x2vec encodings yielded better results than the tf-idf encodings and text outperformed math encodings. However, for the clustering, the combination of text and math slightly outperforms the separate encodings. All in all, our research questions were answered as:
Overall, the experimental results show that both proposed models achieve significant improvement on performance and robustness over benchmarks across all tasks.
“– SingleQA” refers to the model with the question type classifier and single-hop QA model removed. This model performs slightly better on multi-hop questions, while the model failure rate is higher. The result suggests that having an explicit model to handle sub-questions is indeed helpful for intermediate answer extraction. On the other hand, our model performs much worse on multi-hop questions when the question type classifier and multi-hop QA models are removed, although it achieves better performance on the first sub-questions.
The results indicate that the emojis are embedded next to very different words across models. The most agreeing platforms are iOS and Android, where an average of 153 words are common across the top 1,000 in the emojis. We also note that Windows has a much lower average agreement than other models. Finally, all platforms are extremely different from a random sample. This means that combining tweets from all platforms, as is done in many analytical tasks, will yield a significantly different representation than considering each platform individually.
Precision, recall, and F1-scores are all weighted scores. As the table shows, all of our fine-tuned models performed better than the baseline BLSTM system in every regard, and XLM-RoBERTa large was the best overall, with the highest weighted F1-score. Interestingly, when we shift our focus to the monolingual models, we see that the English model performed worse than the Spanish model. The multilingual BERT-model sits right in the middle of these two when we look at the performance metrics.
Considering the fact that each category was represented at least ten times and pairs did not fall into one single category (cf. , we saw our first requirement as fulfilled. Variation of values assigned to each pair keeps within limits: A difference of three categories was observed for two pairs (1%), a difference of two categories for only six pairs (4%), a difference of one category for 62 pairs (41%) and no difference was observed for 82 pairs (54%). A variation of four categories was not observed (cf.
As regards Harbsafe-162, we chose a similar conception of the rating scale as Hill et al. Unfortunately, we have not made the contrast between similarity and relatedness just as clear as Hill et al. The scale points are labeled by category names that are supposed to show a degree of similarity. The labels on our scale, however, are problematic, since not all of them fit the linguistic example data. Point 2 is especially problematic since it is labeled “slightly similar”. This label is simply not applicable to the example: Or how is a pilot slightly similar to a plane? Our rating scale therefore shows the criticized confusion of similarity and relatedness. Point 2 on our scale should rather have been named “dissimilar and directly related”, while point 1 should have been named “dissimilar and indirectly related”. Consequently, we should be able to find pairs that show this confusion of similarity and relatedness in Harbsafe-162, that is, dissimilar pairs that were rated as similar and some similar pairs that were rated as dissimilar. All in all, we rated 134 pairs unanimously.
We observe that is used more frequently in the Irma solidarity tweets (Rank 3) but not in the Irma tweets that do not express solidarity. In the top 10 Irma emojis used in tweets not expressing solidarity, we also observe more negatively valenced emojis, including and . In addition, is used across all four sets, albeit at different ranks (e.g. Rank 1 in Irma solidarity and Rank 6 in Paris solidarity tweets).
To ensure that the benefit of the reconstructor-based pragmatic approach, which uses two models, is not due solely to a model combination effect, we also compare to an ensemble of two base models (S0 ×2). This ensemble uses a weighted combination of scores of two independently-trained S0 models, following Eq.
The SR1 model increases the coverage ratio when compared to S0 across all attributes, showing that using the reconstruction model score to select outputs does lead to an increase in mentions for each attribute. Coverage ratios increase for SD1 as well in four out of six categories, but the increase is typically less than that produced by SR1. While SD1 optimizes less explicitly for attribute mentions than SR1, it still provides a potential method to control generated outputs by choosing alternate distractors. The highest coverage ratio for each attribute is usually obtained when masking that attribute in the distractor MR (entries on the main diagonal, underlined), in particular for FamilyFriendly (FF), Food, PriceRange (PR), and Area. However, masking a single attribute sometimes results in decreasing the coverage ratio, and we also observe substantial increases from masking other attributes: e.g., masking either FamilyFriendly or CustomerRating (CR) produces an equal increase in coverage ratio for the CustomerRating attribute. This may reflect underlying correlations in the training data, as these two attributes have a small number of possible values (3 and 7, respectively).
The results are similar to the results obtained by merging the labels after training, with the models generally outputting entailment for all HANS examples, whether that was the correct answer or not.
The overall evaluating process are as follows: (1) To obtain the a response and its context, we need to choose the open-domain generative dialogue systems. In order to make the conclusions more reliable, three generative models are chosen according to the difference of the decoding process: (a) Seq2Seq-attn Bahdanau et al. Copynet Gu et al. Transformer Vaswani et al. More details can be found in Appendix. (2) 100 samples are randomly sampled from these generated pairs of the response and context. Then the human judgements are obtained, Shuman. Specifically, 3 volunteers are asked not only to provide an overall scores but also to provide scores from fluency, coherence, and engagement. We ask the volunteers to rate each pair of the context and response on a scale of 1-6 (very bad to very good). (3) All kinds of the automatic evaluation metrics are used to obtain the scores of the samples, Smetric. (4) In order to measure the correlation between these automatic evaluation metrics and the human judgments, Pearson and Spearman correlation are adopted to obtain the correlation between Shuman and Smetric. Pearson and Spearman correlations estimate linear and monotonic correlation and are widely used in the automatic evaluation of open-domain generative dialogue systems such as RUBER Tao et al. The higher the correlation between Shuman and Smetric, the closer the metric to human judgments. If the p-value is less than 0.01, the relationship between human judgments and the automatic metrics is significant.
3.4.6 Case Study We can make the conclusions as follows: (1) Because of occasional exact word overlapping, word-overlap-based metrics provide small scores, and it is not appropriate for evaluating the open-domain dialogue systems. We obtain the average score of the BLEU on all the datasets, and the result is 0.173; (2) The scores generated by the embedding-based metrics such as BERTScore and Embedding Average are usually very high because the sentence representation is very fuzzy. We also get the average scores of the embedding-based metrics on all datasets, and it is 0.814; (3) The learning-based metrics such as BERT-RUBER and PONE can provide balanced (average 0.473 of PONE) and, appropriate score and the PONE ’s scores are closer to the human judgments.
We also performed a sensitivity analysis on the parse-tree model to observe its robustness. For this, we combined the arguments from all the 4 datasets and ran the model on the combined set: 1) using only 10 instances in the dataset, 2) using only 100 instances in the dataset, 3) using 1000 instances in the dataset, and 4) using all the instances in the dataset. The prototype argument strings, for this dataset, were synthesized according to the method mentioned earlier. For the cases which did not involve the whole dataset, we randomly sampled 5 times from the whole corpus and averaged the results. As we can see, the results show that the proposed model is relatively robust and invariant with the amount of data.
In order to have fair baselines, we implemented three models that performed very well We performed all of them for 5 epochs, with sparse_categorical_crossentropy log loss function, mlp_depth=2 and mlp_drop_out=0.2. We trained them on the complete 160K training set, which was not the case for the main model. Baseline models are: Our experiment with the new dataset found the proposed model’s accuracy and F1 score to be 0.981. This is a 2.1 percent jump from the best available CNNs and 2.3 percent from Attention RNN Fine-tuned BERTBASE model performed better than the first three baselines, reaching close to 97 percent accuracy, 1.1 percent behind the proposed model. In addition, the combination of RNN-CNN was not successful in this task and did not improve results compared to the CNN model.
The models word2vec, doc2vec and GOW retain the same number of features across the four data sets. For both variants tf and tf-idf of the BOW model, the number of features after PCA dimensionality reduction varies depending on the data set. For example, the dimensionality of the 20News feature space is about ten times higher than in Brown. The variants of the GOW model are always constructed from the same set of network measures and the size of features vectors in word2vec and doc2vec models is predetermined by the size of the parameter. The network-based model is systematically underperforming regardless of the task or evaluation metrics. Still, some additional remarks should be noted. The number of features used in all three network-based model variants with averaging, quartiles and histograms are 19, 68 and 128, respectively, which is lower than in other models (see Although GOW is lagging behind in all rankings, in some occasions it can still be the representation of choice, especially if we look for a low-dimensional document representation model robust to noise. Namely, note that the network-based model requires no extensive text preprocessing, which can be useful for fast document classification implementation in low-resources languages, which lack text preprocessing tools and resources.
Here we show the quality of the predictions using our proposed combination of day-level and hour-level prediction, defined as 2-levels prediction. Moreover, we also show the quality of the predictions obtained with a 1-level prediction, performed by applying only the hour-level prediction (with linear regression) and selecting the 10% activations with the associated highest probability, yielding approximately the same number of predicted activations as the 2-levels prediction. In both datasets the 2-levels model outperforms the simple 1-level model, leading to higher accuracy in classifying which days will present an activation and lower error in the prediction of the hour of activation.
Based on the results, CWSJT provides a significant boost on the performance over all datasets.
As shown, CWSJT and CWSCT perform better than other strategies. CWSCT is to let the confidence network to be trained separately, while still being able to enjoy shared learned information from the target network. However, it is less efficient as we need two rounds of training on weakly labeled data.
(orindicates that the improvements or degradations are statistically significant, at the 0.05 level using the paired two-tailed t-test. For all model, the improvement/degradations is with respect to the “weak supervision only” baseline For CWSJT, the improvement over all baselines is considered and the Bonferroni correction is applied on the significant tests.) Results and Discussion. We have also report statistical significance of F1 improvements using two-tailed paired t-test with p_value<0.05, with Bonferroni correction. Our method is the best performing among all the baselines.
As shown, similar to the ranking task, CWSJT and CWSCT perform better than other strategies. Although CWSCT is slightly better (not statistically significant) in terms of effectiveness compared to CWSJT, it is not as efficient as CWSJT during training.
This implies that users rely heavily on their word choice and language to convey meaning and emotion. This implies that with some careful thinking and pointed model construction, we should be able to improve upon our baseline unimodal results through the integration of additional modalities into our model.
The comparison results demonstrate that our model E2EECPE consistently outperforms the baselines for the main task (emotion-cause pair extraction) with regard to recall and F1, indicating the representation power and the effectiveness of our model. Nevertheless, we also observe that our model performs less well in precision than the two-stage baseline models. With additional observation that the baseline models are performing poorly on recall, we conjecture the existing models suffer from predicting only few testing instances as pairs.
w/o auxiliary and a relatively minor drop of E2EECPE w/o position compared with E2EECPE, verifying the remarkable benefit of the multi-task learning schema. Meanwhile, the results that E2EECPE w/o position only differs slightly from E2EECPE based on all metrics, indicate the fact that imposing position information is still of importance.
With increases of η, drops of F1 are noted, implying potential loss of extracted pairs. In addition, we also speculate that the reason why the best value is not around 0.5 (the expectation of random variables ranging uniformly from 0 to 1) is that the element-wise multiplication of a position weight matrix with the sigmoid-activated pair matrix produces a smaller expectation (as upper bound decreases).
In the first iteration, the precision value was 0.8 or higher, and the recall value was over 0.5. From this, we found that even if a few the journal names were input as seeds, we could extract many journal names by using the left and right context features as a pattern. However, in the second iteration, the newly extracted journal names were reduced by half, and although the precision value was reduced by 0.2, the recall value hardly improved. These results showed that using the left and right bigrams as clues to extract the journal names were effective. However, these results also suggested that other features were necessary to extract the journal names while maintaining a high F-measure when using the bootstrap approach.
Quantitative Evaluation. We test each model’s ability to rank word pairs according to their semantic similarity, a task commonly used to gauge the quality of WEs. As is typical for evaluation, we measure the Spearman’s rank correlation between the similarity scores produced by the model and those produced by humans. We see that the SD-SG and SD-CBOW perform better than their 50 dimensional counterparts but worse than their 200 dimensional counterparts. All scores are relatively competitive though, separated by no more than 0.1. Qualitative Evaluation. Observing that the SD-SG and SD-CBOW models perform comparatively to finite versions somewhere between 50 and 200 dimensions, we qualitatively examine their distributions over vector dimensionalities. each vector after training the SD-CBOW model. As expected, the distribution is long-tailed, and vague words occupy the tail while specific words are found in the head. As shown by the annotations, the word photon has an expected dimensionality of 19 while the homograph race has 150. Note that expected dimensionality correlates with word frequency—due to the fact that multi-sense words, by definition, can be used more frequently—but does not follow it strictly. For instance, the word william is the 482nd most frequently occurring word in the corpus but has an expected length of 62, which is closer to the lengths of much rarer words (around 20-40 dimensions) than to similarly frequent words. For race, we see that the distribution does indeed have at least two modes: the first at around 70 dimensions represents car racing, as determined by computing nearest neighbors with that dimension as a cutoff, while the second at around 100 dimensions encodes the anthropological meaning.
We note that the model hyperparameters were tuned on the CoNLL04 development set. Character and dependency based features all had a notable impact on performance for either dataset. On the hand, while position embeddings had a positive effect on the ADE dataset, performance gains were negligible when testing on CoNLL04. For the CoNLL04 dataset, we find that character based features had little effect on precision while improving recall substantially.
The best results are obtained from the combination of simulator and real data. The best real data only results (selecting over algorithm and training strategy) on both tasks outperform the best results using simulator data, i.e. using Cont-MemN2N with the Train AQ / TestAQ setting) 0.774 and 0.797 is obtained vs. 0.714 and 0.788 for Tasks 4 and 8 respectively. This is despite there being far fewer examples of real data compared to simulator data. Overall we obtain two main conclusions from this additional experiment: (i) real data is indeed measurably superior to simulated data for training our models; (ii) in all cases (across different algorithms, tasks and data types – be they real data, simulated data or combinations) the bot asking questions (AQ) outperforms it only answering questions and not asking them (QA). The latter reinforces the main result of the paper.
The best results are obtained from the combination of simulator and real data. The best real data only results (selecting over algorithm and training strategy) on both tasks outperform the best results using simulator data, i.e. using Cont-MemN2N with the Train AQ / TestAQ setting) 0.774 and 0.797 is obtained vs. 0.714 and 0.788 for Tasks 4 and 8 respectively. This is despite there being far fewer examples of real data compared to simulator data. Overall we obtain two main conclusions from this additional experiment: (i) real data is indeed measurably superior to simulated data for training our models; (ii) in all cases (across different algorithms, tasks and data types – be they real data, simulated data or combinations) the bot asking questions (AQ) outperforms it only answering questions and not asking them (QA). The latter reinforces the main result of the paper.
The best results are obtained from the combination of simulator and real data. The best real data only results (selecting over algorithm and training strategy) on both tasks outperform the best results using simulator data, i.e. using Cont-MemN2N with the Train AQ / TestAQ setting) 0.774 and 0.797 is obtained vs. 0.714 and 0.788 for Tasks 4 and 8 respectively. This is despite there being far fewer examples of real data compared to simulator data. Overall we obtain two main conclusions from this additional experiment: (i) real data is indeed measurably superior to simulated data for training our models; (ii) in all cases (across different algorithms, tasks and data types – be they real data, simulated data or combinations) the bot asking questions (AQ) outperforms it only answering questions and not asking them (QA). The latter reinforces the main result of the paper.
In this setup, we used relatively small models: single-layer LSTMs with 256 hidden units, taking 32-dimensional word or output symbol embeddings as input to each cell. In each case, the parsing TBRU takes input from a right-to-left shift-only TBRU. Under these settings, the pure encoder/decoder seq2seq model simply does not have the capacity to parse newswire text with any degree of accuracy, but the TBRU-based approach is nearly state-of-the-art at the same exact computational cost. As a point of comparison and an alternative to using input pointers, we also implemented an attention mechanism within DRAGNN. We used the dot-product formulation from \newciteparikh2016decomposable, where r(si) in the parser takes in all of the shift-only TBRU’s hidden states and RNN aggregates over them.
RoBERTa and ALBERT. We observe a significant increase in Accuracy, Precision-1, and F1 and a slight increase in Recall-1 and Acc&F1 in our best-performing ensemble model as compared to the base models.
We discern that Model 5 performs the best for Accuracy, Precision-1, F1, and Acc&F1 metrics, and Model 1 performs the best for Recall-1 metric among all the compared models. Since Model 5 outperforms all other models in four out of five metrics, it is the best predictive model for the task and is referred to as Our Model in the paper.
Upon performing a correlation study between Impact and the previously characterized labels using Pearson’s correlation coefficient[Rodgers1988], we observe a very small positive correlation between the variables. the maximum ρ of 0.046 between Impact and Emotional Disclosure represents that Impact is characteristically distinct from the previously predicted labels.
Following Yimam et al. , we used Macro-F1 score to evaluate performance and for comparison with previous work on the datasets. We used Logistic Regression for all our experiments, as it allowed for easy exploration of feature combinations, and in initial experiments we found that it performed better than Random Forests. We evaluated both using the full feature set described before, as well as a two-feature baseline using the number of tokens of the target and its language-normalised number of characters. Our full 25-features model improves on the baseline in all cases, with the biggest increase of over 16 percentage points seen for the EN-News dataset. However, the state of the art for German remains the Shared Task baseline (75.5) ( Furthermore, the mean score for our system (79.2) is close to the mean of the best performing models (81.0), which are different systems, while using simpler features and learning algorithm.
The main characteristic of Jp-News is the use of a set of Japanese news articles that contain similar passages on the same topics. These similar passages make it difficult for IR models to find the most relevant passage to each question.
This allows automatically identifying a given model’s strengths – for example, here questions about Human Health, Material Properties, and Earth’s Inner Core are well addressed by the BERT-QA model, and achieve well above the average QA performance of 49%. Similarly, areas of deficit include Changes of State, Reproduction, and Food Chain Processes questions, which see below-average QA performance. The lowest performing class, Safety Procedures, demonstrates that while this model has strong performance in many areas of scientific reasoning, it is worse than chance at answering questions about safety, and would be inappropriate to deploy for safety-critical tasks.
Though the classification procedure was fine-grained compared to other question classification taxonomies, containing an unusually large number of classes (406), overall raw interannotator agreement before resolution was high (Cohen’s κ = 0.58). When labels are truncated to a maximum taxonomy depth of N, raw interannotator increases to κ = 0.85
The time and model parameters required to train FS techniques with Ensemble Classifiers are significantly very less compared to neural network architectures like LSTM, CNN.
Experiment 2: Next, we design an experiment where the feed-forward network is given two hidden states hx and hy, generated from two concatenated sentences x and y respectively, and is tasked with predicting whether sentence x is a substring of sentence [x;y] (x concatenated with y). The input to the model is [hx;hy]. The results depict that we can more accurately determine that the second sentence is a substring of the concatenated sentence, which in turn suggests that the representation ht better encodes the final elements in the sequence rather than the initial ones.
Experiment 1: For the first experiment we constructed a template of six sentences: “you are X", ‘he is very X", “i am very X", “he is not X", “i am not X" where X ∈ {daxy, tall, ok, fat, fit}. For a given X and a template, we measured the cosine similarity of the encoder representation before and after observing X. For each word, the similarity score was aggregated over all template instances. We observe that the average similarity for the word “daxy" is much higher than the others, suggesting that the encoder hidden state remains largely unchanged even after observing “daxy.”
Our first observation is that, even with the baseline feature set, QE obtains respectable correlation scores, proving feasible as a method to predict interpreter performance. Our trimmed feature set performs moderately better than the baseline for Japanese, and slightly under-performs for French and Italian. However, our proposed, interpreter-focused model out-performs in all language settings with notable gains in particular for EN-JA(A-Rank) (+0.104), achieving its highest accuracy on the EN-FR dataset.
In our experiments below, we consider the following ten NLP tasks, with one dataset for each task. CCG Tagging (ccg) is a sequence tagging problem that assigns a logical type to every token. We use the standard splits for CCG super-tagging from the CCGBank [Hockenmaier:2007:CCC]. Chunking (chu) identifies continuous spans of tokens that form syntactic units such as noun phrases or verb phrases. We use the standard splits for syntactic chunking from the English Penn Treebank [Marcus:ea:93]. Sentence Compression (com) We use the publicly available subset of the Google Compression dataset [filippova:13], which has token-level annotations of word deletions. Semantic frames (fnt) We use FrameNet 1.5 for jointly predicting target words that trigger frames, and deciding on the correct frame in context. POS tagging (pos) We use a dataset of tweets annotated for Universal part-of-speech tags [Petrov:ea:11]. Hyperlink Prediction (hyp) We use the hypertext corpus from \newcitespitkovsky2010 and predict what sequences of words have been bracketed with hyperlinks. Keyphrase Detection (key) This task amounts to detecting keyphrases in scientific publications. We use the SemEval 2017 Task 10 dataset. MWE Detection (mwe) We use the Streusle corpus [schneider2015corpus] to learn to identify multi-word expressions (on my own, cope with). Super-sense tagging (sem) We use the standard splits for the Semcor dataset, predicting coarse-grained semantic types of nouns and verbs (super-senses). Super-sense Tagging (str) As for the MWE task, we use the Streusle corpus, jointly predicting brackets and coarse-grained semantic types of the multi-word expressions. We train single-task bi-LSTMs for each of the ten tasks, as well as one multi-task model for each of the pairs between the tasks, yielding 90 directed pairs of the form ⟨Tmain,{Tmain, Taux}⟩. The single-task models are trained for 25,000 batches, while multi-task models are trained for 50,000 batches to account for the uniform drawing of the two tasks at every iteration in the multi-task setup. The relative gains and losses from MTL over the single-task models (see We see that chunking and high-level semantic tagging generally contribute most to other tasks, while hyperlinks do not significantly improve any other task. On the receiving end, we see that multiword and hyperlink detection seem to profit most from several auxiliary tasks. Symbiotic relationships are formed, e.g., by POS and CCG-tagging, or MWE and compression.
The first observation is that there is a strong signal in our meta-learning features. In almost four in five cases, we can predict the outcome of the MTL experiment from the data and the single task experiments, which gives validity to our feature analysis. We also see that the features derived from the single task inductions are the most important. In fact, using only data-inherent features, the F1 score of the positive class is worse than the majority baseline.
FastSent The performance of SkipThought vectors shows that rich sentence semantics can be inferred from the content of adjacent sentences. FastSent is a simple additive (log-linear) sentence model designed to exploit the same signal, but at much lower computational expense. Given a BOW representation of some sentence in context, the model simply predicts adjacent sentences (also represented as BOW) . The log-linear models (SkipGram, CBOW, ParagraphVec and FastSent) were trained for one epoch on one CPU core. The representation dimension d for these models was found after tuning d∈{100,200,300,400,500} on the validation set. All other models were trained on one GPU. The S(D)AE models were trained for one epoch (≈8 days). The SkipThought model was trained for two weeks, covering just under one epoch. , performance was monitored on held-out training data and training was stopped after 24 hours after a plateau in cost. The NMT models were trained for 72 hours. This can limit their possible applications. For instance, while it was easy to make an online demo for fast querying of near neighbours in the CBOW and FastSent spaces, it was not practical for other models owing to memory footprint, encoding time and representation dimension.
Gate Activations We first studied the average activation of each individual gate in the models by averaging them when running generation on the test set. We analysed the hybrid models because their reading gate to output gate activation ratio (rj/oj) shows clear tradeoff between the LM and the conditioning vector components. the ratio of the reading gate to the output gate activation (rj/oj) have strong correlations to performance: a better performance (row 3 >row 2>row 1) seems to come from models that can learn a longer word dependency (higher forget gate ft activations) and a better conditioning vector (therefore higher reading to output gate ratio rj/oj).
The model we use has two layers of convolution, pooling and tanh transformations using 6 and 14 feature maps with widths 7 and 5, respectively. The word embeddings are 60 dimensional. This exactly follows the set up used in Kalchbrenner et. al. As a consequence, our model has substantially fewer parameters than the model of Kalchbrenner et. al.. Both our model, with 46 errors, and Kalchbrenner’s, with 45 errors, are very close in performance and significantly better than other competitors. Our model achieves what is, to the best of our knowledge, the third best published result on this data set. We found this result to be very encouraging because this dataset is too small to train a ConvNet model as well as we would like too. Our main challenge in achieving good performance on this task was regularising the model strongly enough that it would not overfit.
We compare the accuracy of Naïve Bayes to summaries of different sizes created by taking the top ranked sentences using the visualisation technique. Even keeping only 20% of each review the accuracy of the classifier trained on full reviews drops by less than 1% on the test set. As a baseline we also report the accuracy of the same classifier on summaries created by choosing random sentences from each review, and it is clear from the results that the summaries we create preserve a significant amount of information that is lost by choosing random sentences.
Adding a shortlist improves translation speed significantly. Enabling int16 multiplication without memoization hurts performance; with memoization we see improvements for single-sentence translation and similar performance to MKL for batched translation. With auto-tuning, single-sentence translation achieves the same performance as before and batched translation improves. In both cases the auto-tuning algorithm was able to choose a good solution. In the single-sentence case we would always use the int16 product. In the batched case a mix performs better than a hard choice.
MrRNN-ActEnt learns the activities and entities in a supervised manner, that is, the information about activities and entities is available during training. In contrast, all the other models used for comparison are trained in an unsupervised manner. As can be observed from the table, the proposed model significantly outperforms the unsupervised baselines on each metric. This suggests that Mask & Focus can predict the correct activities and entities with higher accuracy and hence, can aid in resolving the technical support problems of Ubuntu dataset.
So, we conduct a human study to compare the quality of the responses generated by Mask & Focus and other existing approaches. Annotators were shown a conversation-context along with the responses generated by unsupervised models (i.e) Mask & Focus, HRED, VHRED and MrRNN-Noun. The name of the algorithms were anonymized and we randomly shuffled the order as well. All annotators were asked to rate the relevance of the response on a scale of 0-4. We noticed that existing approaches often generated generic responses such as ”what are you talking about” or ”i see”. So, we also requested the annotators to mark such generic responses. We randomly sampled 100 context response pairs from the dataset, and collected a total of 500 relevance annotations. GR Ratio indicate the ratio of responses that were generic. Mask & Focus’s ability to produce topically coherent responses is reflected by the significantly low generic response ratio. The relevance scored of Mask & Focus is significantly better that the existing approaches. A low relevance score on the gold responses captures the inherent difficulty in modelling the Ubuntu Corpus.
Our results show that A-GEM outperforms the standard encoder-decoder model Enc-Dec, although it is worse than MbPA on both tasks. Local adaptation (MbPA) and sparse experience replay (Replay) help mitigate catastrophic forgetting compared to Enc-Dec, but a combination of them is needed to achieve the best performance (MbPA++).
We investigate variants of MbPA++ that store only 50% and 10% of training examples. We randomly decide whether to write an example to memory or not (with probability 0.5 or 0.1). The results demonstrate that while the performance of the model degrades as the number of stored examples decreases, the model is still able to maintain a reasonably high performance even with only 10% memory capacity of the full model.
We investigate variants of MbPA++ that store only 50% and 10% of training examples. We randomly decide whether to write an example to memory or not (with probability 0.5 or 0.1). The results demonstrate that while the performance of the model degrades as the number of stored examples decreases, the model is still able to maintain a reasonably high performance even with only 10% memory capacity of the full model.
As shown, the attentive dual encoder (ADE) outperforms other baselines on Recall@1 accuracy. As a result, KVPM is not able to accurately select a response from a large candidate list.
In no case is the F1-Measure below 90%, and in many cases it is over 95%. In future work, we will be validating these results further across roles and with more experimental trials; however, early results do seem to indicate that a promising word embedding-based approach is a promising avenue for obtaining accurate role relevance predictions without investing significantly in manual feature engineering effort.
We compare performance of a single model for each direction with the performance of published single models trained on bitext data only. This can be seen as a clear indicator of the quality of the mined data.
Length prediction turns out to be a difficult task for most of the models. Models which rely on the recurrent architectures such as LSTM, STV, T2V have sufficient capacity to perform well in modeling the tweet length. Also BLSTM is the best in modeling slang words. BLSTM outperforms the LSTM variant in all the tasks except ‘Content’, which signifies the power of using the information flowing from both the directions of the tweet. T2V which is expected to perform well in this task because of its ability to work at a more fine level (i.e., characters) performs the worst. In fact T2V does not outperform other models in any task, which could be mainly due to the fact that the hashtags which are used for supervision in learning tweet representations reduces the generalization capability of the tweets beyond hashtag prediction. Prediction tasks such as ‘Content’ and ‘Hashtag’ seem to be less difficult as all the models perform nearly optimal for them. The superior performance of all the models for the ‘Content’ task in particular is unlike the relatively lower performance reported for in [5], mainly because of the short length of the tweets. The most surprising result is when the BOM model turned out to be the best in ‘Word Order’ task, as the model by nature loses the word order. This might be due to the correlation between word order patterns and the occurrences of specific words. BOM has also proven to perform well for identifying the named entities in the tweet.
As the DIM model was not tested on the CMU_DoG dataset, we used the code released by the original authors Gu et al. Results show that the FIRE model outperforms previous methods by margins larger than 2.8% on original personas and 4.1% on revised personas on the PERSONA-CHAT dataset, as well as 3.1% on the CMU_DoG dataset in terms of top-1 accuracy R@1, achieving a new state-of-the-art performance for knowledge-grounded response selection in retrieval-based chatbots.
We conducted the ablation tests as follows. First, we ablate the iteratively referring by setting the number of iterations L to one. Then we removed the pre-filter. The performances of these ablation models were worse than before, leading to a drop in terms of selection accuracy, which demonstrated the effectiveness of these components in the FIRE model. Furthermore, we discussed the single context-response or knowledge-response matching in order to show the effectiveness of the context filter and the knowledge filter separately. Three experiments were designed as follows: (1) single context-response matching without knowledge; (2) context-response matching first and then knowledge fusion at a fine-grained utterance-level, as the IMNutr model in Gu et al. (3) context filtering first and then the context-response matching. It shows that the fusion after matching and the filtering before matching can both improve the performance with the help of knowledge. Meanwhile, the filtering before matching outperformed the fusion after matching by a large margin, which demonstrated the effectiveness of the context filter. Also, we designed similar experiments for the knowledge-response matching and we can observe the same trend, which demonstrated the effectiveness of the knowledge filter.
We see that our method provides a strong boost on accuracy over all three datasets. Rows 4-10 show the accuracy of recent top-performing methods. On the MR and CR datasets, our model outperforms all the baselines. Their neural network has combined diverse sets of pre-trained word embeddings (while we use only word2vec) and contained more neural layers and parameters than our model. This is because in the NER task we have used logic rules that introduce extra dependencies between adjacent tag positions as well as multiple instances, making the explicit joint inference of q useful for fulfilling these structured constraints.
To further investigate the effectiveness of our framework in integrating structured rule knowledge, we compare with an extensive array of other possible integration approaches. We see that: 1) Although all methods lead to different degrees of improvement, our framework outperforms all other competitors with a large margin. 2) Another advantage of our method is that we only train one set of neural parameters, as opposed to two separate sets as in the pipelined approach. 3) The distilled student network “ -Rule-p” achieves much superior accuracy compared to the base CNN, as well as “-project” and “-opt-project” which explicitly project CNN to the rule-constrained subspace. This validates that our distillation procedure transfers the structured knowledge into the neural parameters effectively.
For example, “first man on the moon” and “Neil Armstrong” are both lexicalizations of dbr: Neil_Armstrong. In this step, we want to identify all entities, properties and classes, which the question could refer to. To achieve this, we use the following rules: All IRIs are searched whose lexicalization (up to stemming) is an n-gram N (up to stemming) in the question. If an n-gram N is a stop word (like “is”, “are”, “of”, “give”, …) , then we exclude the IRIs associated to it. This is due to the observation that the semantics are important to understand a question and the fact that stop words do not carry a lot of semantics. Moreover, by removing the stop words the time needed in the next step is decreased.
For example, “first man on the moon” and “Neil Armstrong” are both lexicalizations of dbr: Neil_Armstrong. In this step, we want to identify all entities, properties and classes, which the question could refer to. To achieve this, we use the following rules: All IRIs are searched whose lexicalization (up to stemming) is an n-gram N (up to stemming) in the question. If an n-gram N is a stop word (like “is”, “are”, “of”, “give”, …) , then we exclude the IRIs associated to it. This is due to the observation that the semantics are important to understand a question and the fact that stop words do not carry a lot of semantics. Moreover, by removing the stop words the time needed in the next step is decreased.
Multiple KBs: The only available benchmark that tackles multiple KBs was presented in QALD-4 task 2. The KBs are rather small and perfectly interlinked. This is not the case over the considered KBs. We therefore evaluated the ability to query multiple KBs differently. We run the questions of the QALD-6 benchmark, which was designed for DBpedia, both over DBpedia (only) and over DBpedia, Wikidata, MusicBrainz, DBLP and Freebase. Note that, while the original questions have a solution over DBpedia, a good answer could also be found over the other datasets. We therefore manually checked whether the answers that were found in other KBs are right (independently from which KB was chosen by the QA system to answer it). WDAqua-core1 choose 53 times to answer a question over DBpedia, 39 over Wikidata and the other 8 times over a different KB. Note that we get better results when querying multiple KBs. Globally we get better recall and lower precision which is expected. While scalability is an issue, we are able to pick the right KB to find the answer!
We also run a feature ablation test. On A-test, where the inferences are elicited from humans, removal of similarity- and bow-based features together results in the largest performance drop. On B-test, by contrast, removing similarity and bow features results in a comparable performance drop to removing seq2seq features. These observations point to statistical differences between human-elicited and auto-generated hypotheses, a motivating point of the JOCI corpus.
Outlier Detection results. The first thing to note is that CP-S outperforms the other methods across each Outlier Detection metric. Since the WikiSem500 dataset is semantically focused, performance at this task demonstrates the quality of semantic information encoded in our embeddings.
As we can see, our embeddings very clearly outperform the random embedding at this task. They even outperform CBOW on both of these datasets. It is worth including these results as the word similarity task is a very common way of evaluating embedding quality in the literature. However, due to the many intrinsic problems with evaluating word embeddings using word similarity [wordsim_problems], we do not discuss this further.
Now, we carefully study the extent to which Neural Assistant models can handle large KBs. We get the set of weakly labeled positive triples for every example and fill up the rest of KB with randomly sampled negative examples both at train and test time. The goal of this experiment is to study the effect of KB size on Neural Assistant performance. Another way to look at this experiment is to study the extent to which our model can tolerate the errors of a retrieval system. The BLEU score and Entity F-1 scores for Neural Assistant reduce as the KB size increases. The model is able to incorporate external knowledge effectively as long as the KB size is 2000 triples or smaller. Beyond that, the Entity F-1 score degrades quite rapidly. Our experiments show that in some cases distant supervision helps the model to get better performance particularly higher entity F-1 score but not in all cases. Finally, we report results from using the entire KB at test time using a model that is trained with 5,000 triples at train time without distant supervision. In this setting, the entity F-1 score is quite low indicating since the model is not able to select the relevant entities from the knowledge base at test time. The model cannot consume the entire KB at train time as it runs out of memory on ML accelerators.
To check the performance of the model with features as part of the embedding, we compared the model to the baseline model. A baseline model is a naive model assuming to be the least possible intelligent system. Here we achieved the baseline model by training the neural network only on the 100-dimensional word embeddings. To understand the significance of every feature, we trained the model considering a feature at a time.
Our baseline results surpass many of the previous results, which we attribute to the way that we initialize the decoding process. Instead of directly copying the source embeddings to the decoder input, we use an interpolated version of the encoder outputs as the decoder input, which allows the encoder to transform the source embeddings into a more usable form. Note that a similar technique is adopted in wei2019imitation, but our model structure and optimization are much simpler as we do not have any imitation module for detailed teacher guidance.
S5SS0SSS0Px4 BLEU under Different Sentence Lengths We can see that the baseline NAR model’s performance drops quickly as sentence length increases, whereas the NAR model trained with monolingual data degrades less over longer sentences, which demonstrates that external monolingual data improves the NAR model’s generalization ability.
In this section, we evaluate the performance of the state-of-the-art entailment models on predicting a score for the generated utterances. In particular, the conversation history H is treated as a premise, whereas the generated response r acts as a hypothesis. We pick two state-of-the-art NLI models (i.e., ESIM Chen et al. These models were trained on the InferConvAI dataset. During evaluation, we use our test dialogue corpus from Reddit and OpenSubtitles, in which the majority vote of the 4-scale human rating constitutes the labels. Both models reach reasonable performance in this setting, while BERT outperforms ESIM. Note that this experiment examines the generalization capabilities of these inference models as the test datasets are drawn from an entirely different distribution than the training corpus. The test utterances that are predicted as entailment tend to be rated higher than other utterances, exhibiting that the entailment models correlate quite well with what humans perceive as a coherent response. Another observation is that the inference models often classify acontextual and off-topic responses as neutral and the annotators typically dislike these types of responses. This contributes to the lower scores of neutral-detected responses compared to responses predicted as contradiction.
5.2.1 Word-level Metrics We consider as evaluation metrics baselines three textual similarity metrics Liu et al. These word-level embedding metrics have been proven to correlate with human judgment marginally better than other world-overlap metrics (e.g., BLEU, ROUGE and METEOR) Liu et al. One critical flaw of these embedding metrics is that they assume that each word is independent of the other words in the sentence. Further, the sentence is treated as a bag-of-words, disregarding words order and dependencies that are known to be substantial for understanding the semantic of a sentence. We can notice that the three metrics A, G and E correlate weakly with human judgment in both datasets, demonstrating the need for a well-designed automated metric that provides an accurate evaluation of dialogues.
TextRank has a performance close to the Pointer Generator on English corpora (ratio between 0.85 to 1.21) but not in other languages (ratio between 0.37 to 0.65).
In experiment 1, when original BERT was used without domain-specific pre-training on MIMIC discharge summary and fine tuning was conducted in a centralized manner, the F1 score of 0.784 was achieved for i2b2 2010 and 0.728 for i2b2 2012. In experiment 2, where both BERT pre-training and fine tuning was conducted in a centralized manner, the F1 was 0.858 for i2b2 2010 and 0.741 for i2b2 2012. In experiment 3, where BERT was pre-trained in a federated manner and fine tuned using centralized data, the F1 was 0.820 for i2b2 2010 and 0.735 for i2b2 2012.
After manually aligning the acts across datasets, we still observed poor performance on the task. Looking at the various training and validation set DAs in the manually curated unified representation, we noticed some semantically similar acts which were confusing our tagger. Some examples are: [topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex] Mod1: offer/select- I found a show for 7.30 pm / I found shows for 5 pm and 7 pm. We merge these acts. Mod2: user-request/ sys-request- What is the phone number?/What kind of food would you like? We merge these acts. Mod3: affirm(x=y)/affirm + inform(x= y)- affirm with slots is equivalent to separate affirm and inform DAs, for eg. ‘yes, 7pm’ can become affirm, inform(time=7pm) from affirm(time=7pm). We split them. Mod4: reqalts/reqmore- Is there anything else?/Can i help you with anything else? We merge these acts. We merged/split DAs like the aforementioned ones, as they can easily be restored using other information. For example, if multiple results are offered, we could convert an offer act to a select act, or depending on the agent, we can convert a request act to a user-request or a sys-request.
The results show improvements when unlabeled (0.712 vs 0.702) or labeled (0.734 vs 0.702) target domain data is available.
Pipeline Recognition. The ultimate goal of pun recognition is to establish a pipeline to detect and then locate puns. Joint achieves suboptimal performance and the authors of Joint attribute the performance drop to error propagation. In contrast, PCPR improves the F1-scores against Joint by 24.6% and 20.0% on two pun types.
Ablation Study. To better understand the effectiveness of each component in PCPR, we conduct an ablation study on the homographic puns of the SemEval dataset. Note that we use the average pooling as an alternative when we remove the phonological attention module. As a result, we can see the drop after removing each of the three features. It shows that all these components are essential for PCPR to recognize puns.
However, as can be seen from the column labels, the first reviewer visited restaurants belonging to all four classes, while the second one only visited restaurants of class 2: the second reviewer is clearly a less noisy data point.
Consistent with earlier work While the C-Value method outperforms the identity baseline, it generally lags behind other methods across the board. Our novel TOR method is competitive with the neural methods, with both TOR1M and TOR100M outperforming the CNN in terms of micro-average. TOR1M exhibits a clear performance decrease for rare titles, as shown by its low macro averaged scores. However, feeding the same algorithm with 100 million vacancy titles instead, scores show a substantial boost. The TOR method is over 100 times faster than both BERT and the convolutional model, as well as having a smaller memory footprint. This makes our method especially interesting for applications with strict timing requirements or massive amounts of data. For applications where timing is of lesser importance, the TOR method can still be beneficial: the hybrid models, combining TOR with a more typical NER model, show consistent performance improvements across the board. This is especially clear in the improved title-level accuracy, showing that the inherent hierarchical structure of job and vacancy titles can be leveraged to improve general-purpose models. Our method is extremely efficient, compatible with any NER method and easy to implement, making for an easy way to improve job matching systems. By construction, the evaluation setup reflects the discovery of previously fully unknown job titles, showing that these methods are of particular interest for the (semi-)automated expansion of job market ontologies, leveraging data-driven insights to keep standards up to date in a job market that is changing faster than ever. During the review phase for this paper, we applied our method at the behest of VDAB, the Flemish employment agency. In this project, our technique was used to suggest new titles for its Competent standard. As Competent is written in Dutch, we used the RobBERT model introduced by \newcitedelobelle2020robbert. We found results to be comparable to those obtained in English on the ESCO ontology, with the main difference being a higher macro averaged score, likely to be the consequence of the different methodology used to construct Competent. These results show that our method generalises across multiple languages and occupational taxonomies.
Compared with GBS and DBA, SE-Attn gives a better performance in overall translation quality. The performance of DBA is slightly worse than GBS due to the aggressive pruning. In addition, it is shown that SE-Attn is comparable to Transformer in decoding efficiency, because it adds little time cost to the standard NMT framework.
From this table, it can be seen that the performance of GBS degrades quickly as the number of the noises in constraints increases. Compared with Transformer, GBS only achieves modest gains when 20% of the constraints are noisy; but it completely fails if there are more than 2 noises in the constraints. On the contrary, all of our models under our framework are more robust than GBS for handling noisy constraints.
From this table, we can see that the performance of GBS is very sensitive to the noisy rate of constraints. In particular, GBS fails to improve the translation quality even if there is only one noise in the constraints. On the other hand, two models with the constraint memories are more stable as the number of noises increases. DE-Gate is unaffected by the noises number and SE-Attn achieves the gains up to 2.2 BLEU points with one noise, which coincides with the observations on the Chinese-to-English task.
For Chinese-to-English task, it is observed that DE-Gate and SE-Attn outperform Transformer by a margin of 1.5 and 1.4 BLEU points. In a further comparison, we find the performance of GBS is even worse than the Transformer baseline. The main reason is that the noisy rate of the automatically generated constraints is up to 61.65\%. We also find a similar result on French-to-English task, which further demonstrates the usefulness of the proposed framework.
We observe that almost all of our BMTL decoders (in both BMTL1 and BMTL2) outperform the corresponding baseline models across all three languages, with an improvement of upto 2 BLEU points. This exhibits our architecture’s ability to learn more robust encoded representations, irrespective of language, input units, and combination of output segmentations.
Symbol accuracy is more convincing than BLEU, as the correctness of symbols is a prerequisite of correct execution. Due to the workload of checking the executable results manually, we only include baselines Concat, E2ECR, and the best baseline Copy+Anon. Results demonstrate the superiority of FAnDa over baselines in understanding context and interpreting the semantics of follow-up queries. It also shows that FAnDa is parser-independent and can be incorporated into any semantic parser to improve the context understanding ability cost-effectively.
It can observed that decoder features are best performing (9.1 % EER) followed by acoustic LSTM (11 % EER) and char LSTM (20 % EER). It can also be inferred from the table that the features are complimentary in nature since the combination of features never degrades over the single features. The final performance of the proposed model is 5.2% EER. It can be inferred that the combination performs better than individual features in all operating points.
Here mdcrf+pos refers to our model architecture and multi-source refers to our multi-lingual transfer approach. Malaviya et al. Nivre et al. Malaviya et al. We use the same experimental resources for comparison and for a fair comparison we do not fine-tune on the target language. Of the four language pairs tested by Malaviya et al. We see that under both settings our approach outperforms the baselines by a significant margin for both the language pairs.
To prevent data leakage, it is crucial to mask certain tokens in the scouting reports such as player names, numeric quantities, and to a lesser extent, references to names of organizations. To illustrate the importance of this step, we conduct a study using the scaled F1 measure proposed in Kessler_2017. We find that a text classification model could achieve 100% recall by simply learning associations between mentions of the prospects in their scouting reports. Thus, a model could infer by the presence of the term ”Alford” that the report is about Blue Jays prospect Anthony Alford, who the model has previously seen the label for. To prevent this, we use the Natural Language Toolkit (NLTK) Loper:2002: NNL:1118108.1118117 to mask named entities with a special token. Numeric quantities can also have a similar effect - reports may refer to higher numeric values (throwing harder, a higher signing bonus, a high batting average), which could cause the same effect. We elect to mask this data as well, though the necessity of doing so is less than that of masking players’ names.
This triangle consists of the three vowels /a/, /i/ and /u/ represented in the space of the two first formant frequencies F1 and F2 For information, ellipses of dispersion are also indicated on these plots. The first main conclusion is the significant reduction of the vocalic space as speech becomes less articulated. Indeed, as the articulatory trajectories are less marked, the resulting acoustic targets are less separated in the vocalic space. This may partially explain the lowest intelligibility in hypoarticulated speech. On the contrary, the enhanced acoustic contrast is the result of the efforts of the speaker under hyperarticulation.
In this section we describe the specific features that we were able to extract from the datasets. Furthermore, for the training dataset, LEGO, we perform an analysis of the distributions of the features among the data, in order to perform a comparison with the previously defined expectations.
We do not show the rows related to feature categories that did not provide relevant results. This represents an improvement of 11 percentage points over the chance classifier.
What cases does ADA improve on? To gain more insight into the improvements observed on using ADA, we perform a manual analysis of out-of-domain examples that BERT labels incorrectly, but BERT-A gets right. We carry out this analysis on 50 examples from TimeBank and LitBank each. We observe that an overwhelming number of cases from TimeBank use vocabulary in contexts unique to news (43/50 or 86%). This includes examples of financial events, political events and reporting events that are rarer in literature, indicating that ADA manages to reduce event extraction models’ reliance on lexical features. We make similar observations for LitBank though the proportion of improvement cases with literature-specific vocabulary is more modest (22/50 or 44%). These cases include examples with archaic vocabulary, words that have a different meaning in literary contexts and human/ animal actions, which are not common in news.
Self-training creates a teacher model from labeled data, which is then used to label a large amount of unlabeled data. Both labeled and unlabeled datasets are jointly used to train a student model. We use 1% of the training data as Dl, with the remaining 99% used as Du. BERT-A acts as T, while S is a vanilla BERT model. Self-training improves model performance by nearly 7 F1 points on average. Increase on TimeBank is much higher which may be due to the high precision-low recall tendency of the teacher model.
