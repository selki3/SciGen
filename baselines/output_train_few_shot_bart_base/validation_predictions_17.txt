2 shows the performance of the treelstm model on the large movie review dataset . our recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the iterative approach shows better performance on training .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left .
2 shows the performance of the hyper parameters optimization strategies for each model with different representation . the maximum pooling strategy consistently performs better in all model variations with different number of hyper parameters . as shown in fig . 2 , the hgn model with the smallest number of hyper parameters has the highest f1 score .
1 shows the effect of using the shortest dependency path on each relation type . our model obtains the best f1 ( in 5 - fold ) with sdp and achieves the same f1 score ( in 9 - fold ) . our model outperforms macro - averaged models in both relation type and dependency path .
results in table 3 show that the y - 3 model outperforms all the other models in terms of f1 and f1 score .
3 shows the paragraph level on the essay and essay level . the results are presented in table 3 . in both cases , the results are significantly better than those obtained by [ empty ] on the [ mst - parser ] dataset . moreover , the accuracy of the word embeddings is significantly higher than those on the mst - parser dataset .
4 shows the c - f1 scores for the two indicated systems ; for the lstm - parser system , it is 60 . 62 ± 3 . 54 and 58 . 24 ± 2 . 87 respectively .
shown in table 1 , original and false test scores are shown in bold . original scores are slightly better than those of tgen + and tgen − but still slightly worse than original scores . the results are presented in tables 1 and 2 .
shown in table 1 , the original e2e data and the cleaned version ( the number of distinct mrs , total number of textual references , ser ) are measured by our slot matching script , see section 3 . the original and our cleaned versions have a significant margin over the original , with a marginal drop of 0 . 5pt / 2pt compared to the original .
3 shows the results of original and original test methods . original ( tgen − ) and tgen − ( tgen + ( rouge - lstm ) are the most distinctive features of both systems . original < cider > is the only one that performs better than tgen + on all tests . the other two are completely unbalanced , with only marginal differences in performance between the original and the original . the only exception is the one that does not belong to the original dataset .
results of manual error analysis of tgen on a sample of 100 instances from the original test set ( table 4 ) . we found a total of 22 errors ( 17 . 6 % ) in the original training set ( 14 . 6 % ) . these errors are caused by slight disfluencies in the training set .
3 shows the performance of our proposed dcgcn model on the external and external datasets compared to the previous state - of - the - art models . our proposed model outperforms all the baselines except for seq2seqk ( konstas et al . , 2017 ) and tree2str ( learned and song , 2018 ) . however , it does not achieve the best performance on the bias metric compared to other baselines , namely , snrg and pbmt ( damonte and cohen , 2016 ) .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points ( normalized by the number of parameters ) compared to the seq2seqb model ( our model achieves 27 . 5 points ) . note that our model size is smaller than the ensemble model , and therefore requires fewer parameters to model .
3 shows the english - language b and c scores of the models trained on the baselines of english - german and english - czech . the results are shown in table 3 . the best performing model is bow + gcn ( bastings et al . , 2017 ) . we observe that the single model performs better than the other baselines in english , german and czech , with the exception of seq2seqb , which performs slightly worse in english .
5 shows the effect of the number of layers inside dc on the performance of the layers in table 5 . we observe that when we add layers of layers , we get a significant drop in performance compared to the previous state of the art model .
6 shows the performance of baselines with residual connections . with residual connections , we observe that the gcn with the highest gcn performance is comparable to the dcgcn2 ( 25 . 2 % ) on all but one of the four comparisons .
model f1 shows the performance of the dcgcn models when the number of models is increased . the results are presented in table vii . the models performing best in these scenarios are shown in bold . the first set of models shows a significant drop in performance compared to previous models .
8 shows the ablation study results for the dev set of amr15 . we observe that the dense blocks in the i - th block are less dense than the dense ones in the ii - th .
table 9 shows the ablation study results for the graph encoder and the lstm decoder . encoder modules used in table 9 show that the multi - decoder design has the best performance on both datasets .
7 shows the performance of our initialization strategies on probing tasks . our paper shows that our approach obtains the best performance with a minimum of false positives .
1 shows the performance of our method in terms of depth and subtense . we observe that our approach obtains the best performance with a minimum of 50 % precision .
1 shows the performance of our model compared to other methods . our model outperforms all the other models except subj and sick - r . cbow shows significant performance improvement on both mrpc and mpqa datasets . on the other hand , it performs slightly worse than the other two methods on both datasets .
results on unsupervised downstream tasks attained by our models are shown in table 3 . our model outperforms both hybrid and cmp in all but one of these cases .
8 shows the performance of our initialization strategies on supervised downstream tasks . our paper shows that our approach outperforms all the baselines except for sst2 and sst5 .
6 shows the performance for different training objectives on unsupervised downstream tasks . we observe that the cbow - r model outperforms all the other models except for the one that is supervised .
1 shows the performance of our method in terms of depth and subtraction . we observe that cbow - r outperforms all the baselines except for the one that does not have a boundary . it obtains the best performance on every metric with a gap of 2 . 5 points .
subj and sick - r models outperform all the other methods except for the ones that do not use the subj embeddings . subj has the advantage of training on a larger corpus , hence leading to better performance . cbow - r also outperforms the sst2 and sst5 models in terms of mrpc score .
system performance in [ italic ] e + per and e + e + misc scores are shown in table 1 . all systems trained on the same dataset have the same org and per scores and are comparable in all metrics ( except for the fact that the system relies on the word " signal " . we observe that all the system trained on a single loc is comparable in both languages ( mil - nd , mil - nd ) and all the other systems using the same number of parameters ( misc ) . in both cases , the system performs better than both the original and the supervised learning systems .
2 shows the results on the test set under two settings . name matching and supervised learning achieve 95 % confidence intervals of f1 scores . these results are shown in table 2 . our system outperforms all the models except mil - nd in terms of e + p score .
6 : entailment ( ent ) and ref ( g2s - gat ) in table 6 shows that all models trained on the same dataset are better than those trained on g2s .
3 shows the results of models trained on the ldc2015e86 dataset . the results are presented in table 3 . we observe that the model performs substantially better than the baselines on both datasets , with the exception of ldc2017t10 , which is slightly worse than the baseline .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . our model outperforms the previous stateof - the - art models by a significant margin .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . bilstm significantly outperforms the strong lemmatization baseline on both development set .
results are shown in table 1 . we observe that the g2s - gin model outperforms all the other models in terms of sentence length , sentence length and sentence length .
shown in table 8 , the fraction of elements missing in the output that are not present in the input ( g2s - gin ) , for the test set of ldc2017t10 . these tokens are used to calculate the number of tokens in the generated sentence .
4 shows the accuracy of our approach with respect to target languages . we use the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . our approach obtains the best performance with 96 . 7 % accuracy .
2 shows the pos and sem tags accuracy with baselines and an upper bound . the results are shown in table 2 . word2tag is the most frequently used classifier , followed by unsupemb , word3tag and word2tag .
results are shown in table 1 . our system outperforms all the other methods except for the one that we use . our model obtains the best performance on both datasets .
5 shows the accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we find that for all but english , there is a noticeable drop in accuracy .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ , a difference of 2 . 7 points .
1 shows the accuracies when training directly towards a single task . for pan16 , we trained directly towards the single task ( the pan16 task ) . the results are shown in table 1 . sentiment is the most difficult part of the training , with a gap of 3 . 5 % in performance .
2 shows the balanced and unbalanced data splits . the classifiers trained on the pan16 dataset are named after the gender of the participants in the conversation , and the classifier trained on this dataset is named pan16 . the classifier is trained on gender - neutral data , and is able to distinguish between balanced and balanced data splits across all classes .
performance on different datasets with an adversarial training is shown in table 3 . the performance on the training datasets is the difference between the attacker score and the corresponding adversary ’ s accuracy on the corresponding training dataset .
6 shows the accuracies of the protected attribute with different encoders . embedding with rnn embeddings significantly improves the performance , but it is harder to distinguish the performance of rnn from embedded .
3 shows the performance of our model compared to other models trained on the same dataset . our model outperforms all the other models except for the one that relies on finetune embeddings . we observe that our lstm model achieves the best performance on both datasets when trained on a single dataset . the results are shown in table 3 . it is clear from the results that our model performs better on the two datasets than the other ones .
3 shows the performance of our model compared to the previous stateof - the - art models . the results are presented in table 3 . our model outperforms all the baselines except for the one that has the shortest time to train .
results of experiment 1 are shown in table 1 . our model outperforms all the other models in terms of err and f1 scores . we observe that the amapolar time model significantly outperforms both the yelp and amafull time datasets by a significant margin .
3 shows the bleu score on wmt14 english - german translation task . our model outperforms all the stateof - the - art models except for sru , which has the advantage of decoding one sentence per training batch .
4 shows the performance of our model on squad dataset . our model obtains the best match / f1 score ( 71 . 1 / 83 . 83 ) on the model with a parameter number of 2 . 67m and a f1 score of 74 . 56 / 82 . 50 . we observe that the lstm model outperforms all the baselines except for the sru model , which obtains a better match / f1 score . the results of " # params " are shown in table 4 .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number in the reported result ( lample et al . , 2016 ) . however , it does not show significant performance improvement over lrn .
performance on snli task with base + ln setting and test perplexity on ptb task with base setting . table 7 shows the performance of our model with respect to snli and ptb .
system retrieval and word embeddings are presented in table 1 . word embedders are trained on the system with the word " trans " . the word " strs " is used to describe the system and the type of attention being applied to the attention being trained on it . the system is described in terms of attention span , attention span and attention span . sent attention span ( mtr ) is the most sophisticated in the system , with a maximum attention span of 2 . 5 sessions . it is clear from the table that the attention span is concentrated on attention span with a minimum of two attention spans .
4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all systems is reported in table 4 . our system outperforms all the automatic systems except seq2seq ,
3 shows the performance of all the models trained on the corpus dataset . our model outperforms all the other models except for the ones trained on docsub . we observe that our model performs on par with the best on both corpus and docsub datasets . however , it performs slightly worse than our model on both datasets .
3 shows the performance of all the models trained on the corpus dataset . our model outperforms all the other models except for the ones trained on docsub . we observe that our model performs on par with the best on both corpus and docsub datasets . however , it performs slightly worse than our model on both datasets .
3 shows the performance of all the models trained on the corpus dataset . our model outperforms all the other models except for the ones trained on docsub . we observe that our model performs on par with the best on both corpus and docsub datasets . in both cases , the model performs slightly worse than the other two models .
embeddings are shown in table 1 . our system achieves the best performance on all metrics with a gap of 1 . 78 points between the maxdepth and maxdepth of eurparl . europarl achieves the highest score with a depthcohesion score of 3 . 86 points . however , it does not achieve the best score on the two metrics .
embeddings are shown in table 1 . our system achieves the best performance on all metrics with a minimum of 1 . 5roots . europarl achieves the highest score with a maxdepth of 2 . 42 .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf is the enhanced version of the original visdial model , and r0 , r2 , r3 denote regressive loss , weighted softmax loss , and generalized ranking loss .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . our model outperforms all the baselines except for coatt , which shows the effectiveness of hidden dictionary learning .
5 shows the performance of our models on hard and soft alignments . the results are summarized in table 5 . we observe that the wmd - prec model outperforms the hmd - f1 model on hard alignments , but it is slightly worse than fi - en .
3 shows the performance of the baselines on the test set of ruse ( ruse ) and sent - mover ( svm ) . the results are summarized in table 3 . the baselines are significantly better than the baseline bertscore - f1 score on all test set except for the one in which it was tested .
3 shows the performance of baselines trained on bleu - 2 and sfhotel . the results of these baselines are summarized in table 3 . we observe that the baselines significantly outperform the baseline on all metrics , with the exception of the bertscore - f1 score .
3 shows the metric and baselines scores of all models trained on the same setting . the summaries are summarized in table 3 . the most striking thing about the summaries is that they do not rely on bertscore - recall . they rely on word embeddings instead of elmo . this shows that the clustering of baselines leads to incorrect summaries of scores .
results are shown in table 3 . we observe that the m0 model outperforms the m1 model in terms of word embeddings . however , it does not match the performance of the m2 model on the simuli - vec dataset , indicating that the model performs better on the pp dataset .
results are shown in table 3 . semantic preservation and transfer quality are the most important aspects of the semantic preservation task . the clustering performance of all the models is significantly better than those of the other baselines , indicating that semantic preservation has a high correlation with semantic preservation . syntactic preservation is the most difficult part of semantic preservation , but it is easier to achieve with a single set of features than with multiple sets of features .
5 shows the results of human validation . we report the average number of instances per sentence that match the human evaluation scores . our model outperforms both the machine and human evaluations with a margin of 0 . 67 .
results are shown in table 3 . we observe that the m0 model outperforms the m1 model by a significant margin . in fact , it even outperforms both the m2 and m3 models in terms of grammatical accuracy .
results on yelp sentiment transfer are shown in table 6 . our best models achieve higher bleu ( acc ∗ ) than those using simple - transfer and unsupervised models ( yelp , 2018 ) . however , the difference is less pronounced for those using single - decoder embeddings , indicating that the modeling relies on multiple classifiers to achieve the best acc ∗ score .
statistics for nested disfluencies are shown in table 2 . reparandum length is the average number of tokens predicted to be disfluent . the percentage of tokens that are correctly predicted to have a number of repetition tokens is 8 . 8 % overall .
3 shows the relative frequency of rephrases correctly predicted as disfluent for both the reparandum and the repair ( table 3 ) . the fraction of tokens predicted to contain a content word is shown in parentheses , indicating that the disfluencies in the reparandum are relatively small ( 0 . 01 % ) and that the function - function contains only one word .
results are shown in table 3 . we observe that the best models using text + innovations outperform the single model in terms of dev and test mean . in particular , we observe that when using text and innovations , the model achieves the best results .
2 compares our model with the state - of - art algorithms on the fnc - 1 test dataset . our model achieves the best performance when we discuss topics with the opposing party , and the worst performance when the topic is unrelated .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . compared to previous approaches , burstysimdater significantly outperforms all previous methods .
3 shows the accuracy ( % ) of our method with and without attention . the results show that it is possible to improve the performance by using word attention and graph attention for the word attention task .
3 shows the performance of all models trained on the same dataset . we observe that the best performing model is the jvmee model , which performs on par with the best state - of - the - art on all datasets .
1 and table 2 show the identification and classification results for all stages of the event . our method outperforms all the methods except the one that we use for cross - event identification . we observe that both the method and the classification results are statistically significant , with the exception of the case of the single - sex variant .
results are shown in table 1 . all but fine - tuned - lm models outperform all the other models except for those that do not use the word embeddings . the results are presented in tables 1 and 2 .
results on the dev set and on the test set are shown in table 4 . fine - tuned train dev with only subsets of the code - switched data . this shows that fine - tuning can improve the train dev performance by a significant margin .
5 shows the performance of our model on the dev set and the test set , compared to the monolingual model trained on the fine - tuned - disc set .
shown in table 7 , type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . precision ( p ) , recall ( f1 ) and f1 - score ( f ) are all statistically significant improvements over the baseline ( 72 . 80 % vs . 62 . 97 % ) , and the f1 scores are significantly lower ( 71 . 61 vs . 69 . 61 % ) .
5 shows the precision ( p ) , recall ( f1 ) and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . note the significant improvement in precision ( p ≤ 0 . 01 ) over the baseline , and the f1 - score ( f = 0 . 005 ) .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . we use syntactic - sg embeddings as the base for wordnet and wordnet 3 . 1 , and we use glove - retro as the wrapper for the semantic skipgram embedding . the results on the original paper are summarized in tables 1 and 2 . these results show that the syntactic embedding of wordnet has a significant impact on the ppa performance , as it does not need to rely on syntactic tags . we also use the wordnet semantic embedding feature , which improves the performance of the wordnet semantic network .
2 shows the performance of our rbg dependency parser with features from various pp attachment predictors and oracle attachments . the results are shown in table 2 .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 .
2 shows the results of domain tuning for image caption translation . our model outperforms the en - de model in terms of bleu % scores . subsatellite tuning improves the multi30k model by 3 . 5 points .
3 shows the performance of subs1m models trained on en - de and on flickr16 . subsequently , the models using the same h + ms - coco and domain - tuned models outperform all the other models except for the ones using the mscoco17 dataset . the results are summarized in table 3 . we observe that , when domaintuned and trained on the flickr16 dataset , all the models with the least overlap with the original embeddings perform better .
4 shows bleu scores in terms of automatic captions ( the best one or all 5 ) . the results with en - de are shown in table 4 . as expected , using only the best five captions for the task at hand , the results with mscoco17 are very similar .
5 compares the performance of different approaches for integrating visual information . we observe that enc - gate and dec - gate have the highest correlation with low bleu % scores , which indicates that the approach is more effective for visual information integration . as shown in table 5 , using the multi30k + ms - coco + subs3mlm embeddings improves the performance for both models .
3 shows the performance of subs3m on the en - de and flickr16 datasets compared to subs6m on both datasets . we observe that the embeddings alone do not improve performance , however , when combined with the multi - lingual model , the performance remains the same . submitting the visual features alone does improve the performance , but does not improve the accuracy .
results are shown in table 1 . we observe that the en - fr - rnn - ff model outperforms all the other models except for the one that relies on word - of - speech embeddings . in addition , we observe that for all the models that rely on word analogy , the word analogy alone does not improve performance .
1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 shows the vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 .
5 shows the bleu and ter scores for the rev systems . the automatic evaluation scores ( bleu ) are shown in table 5 . it can be seen that the en - es - rnn - rev system outperforms the other two systems in terms of ter scores .
results on flickr8k are shown in table 2 . the hierarchical supervised model achieves the best performance with a 0 . 0 recall rate .
results on synthetically spoken coco are shown in table 1 . the model trained on the embeddings of chrupala2017representations is comparable to the one trained on audio2vec - u . however , the difference is less pronounced .
1 shows the results of the different classifiers compared to the original on sst - 2 . for example , orig < c > turns in a < u > screenplay that is very clever at the edges ; it ’ s so clever you want to hate it . < r > dan ( rnn ) and rnn ( cnn ) also have similar results . for cnn , the edges edges edges of the screenplay are so clever that you want hate hate it to turn on a dime . for rnn , we report further examples in table 1 . for dan , we use the same classifiers as the original .
2 shows the results of fine - tuning on sst - 2 . the results are shown in table 2 . the most striking ones are the ones that have increased , decreased or stayed the same in the number of occurrences with respect to the original sentence . these numbers indicate that the amount of words that have been added to the sentence has not increased or decreased .
shown in table 3 , the sentiment changes in sst - 2 when the negative labels are flipped to positive . this shows that the effect of the flipped negative labels on sentiment is very positive .
results are presented in table 1 . we observe that the positive effect of sift on the performance of pubmed is less pronounced than the negative effect on sst - 2 . the negative effect is mostly due to the large size of the corpus ( p < 0 . 001 ) and the fact that there is no significant difference in performance between the two groups ( p > 0 . 01 ) .
