table 2 shows the performance of the treelstm model on training , with the large movie review dataset , and tensorflow ’ s iterative approach .
table 1 shows the results of the treernn model implemented with recursive dataflow graphs , using a linear dataset of varying tree balancedness .
4 - 5 shows the performance of the max pooling strategy for each model with different representation . the maximum pooling strategies outperform all other models with the same representation .
table 1 shows the effect of using the shortest dependency path on each relation type . our model achieves the best f1 ( in 5 - fold ) without sdp . we also observe that our model achieves better f1 with sdp than the model achieves with ndp .
results are presented in table 3 . y - 3 outperforms y - 2 in terms of f1 100 % and f1 50 % respectively . the results are shown in table 4 .
results are presented in table 3 . the results of the mst - parser test are summarized in table 4 . the results are shown in table 5 . we observe that the results of our test are significantly better than those of the other test sets . our results show that the performance of the test set is comparable to that of the previous test set . as expected , the results are comparable to those obtained in the previous study .
table 4 shows the performance of the two indicated systems on the lstm - parser and lstms - parser , respectively .
results are presented in table 3 . the results of the original and the new models are summarized in table 4 . the original model outperforms the original model by a significant margin . the new models outperform the original ones by a large margin .
results are presented in table 1 . the original e2e data and our cleaned version are comparable in terms of the number of distinct mrs , total number of textual references , and number of slot matching scripts ( ser ) as measured by our slot matching script .
results are presented in table 1 . original and tgen models are shown in table 2 . the original model outperforms the original model by a significant margin . the original models outperform the original models by a large margin .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . the results are summarized in table 4 . we found that the original tgen model had a significant number of errors , and that it had slight disfluencies .
results are presented in table 1 . table 2 . table 3 shows the performance of the models in table 3 . all models outperform all models in terms of performance . table 2 shows the results for all models .
results on amr17 are presented in table 2 . the model size of dcgcn ( ours ) achieves 24 . 5 bleu points . our model size is comparable to that of ggnn2seq ( our ) in terms of parameters , respectively .
results are presented in table 3 . the results of our model are summarized in table 4 . our model outperforms all the other models in terms of both english - german and english - czech , respectively . we show that our models outperform all other models when it comes to english - language and german - language data .
table 5 shows the effect of the number of layers inside dc on the overall performance of the layers in dc . the effect of layers on the performance of dc is shown in table 5 .
table 6 shows the performance of the baselines on gcns with residual connections . the results are shown in table 6 . gcn + rc + la ( 4 ) outperforms all baselines except dcgcn2 ( 6 ) in terms of residual connections , with the exception of gcgcn3 , which outperforms both baselines .
3 shows the performance of dcgcn ( 2 ) on the model compared to dcgcnf ( 3 ) in terms of the number of participants in the model . the results are shown in table 3 .
table 8 shows the results of the ablation study on amr15 . - { i } dense block denotes removing the dense connections in the i - th block . the results are shown in table 8 . in the ablation study , the dense blocks are replaced by the dense ones . this results in a significant reduction in the number of dense connections .
3 shows the results of the ablation study for the graph encoder and the lstm decoder . the results are shown in table 9 . we observe that the embeddings used in the graph encoder outperform the other models in terms of performance .
table 7 shows the performance of our initialization strategies on probing tasks . our model outperforms all the other models in the table 7 .
results are presented in table 3 . table 3 shows the results of our model . the results of the model are summarized in table 4 .
results are presented in table 3 . the results of the cbow / 784 model are summarized in table 4 . our model outperforms all the other models in terms of performance . we also observe that the model improves the performance of both the hybrid and hybrid models by 0 . 2 % and 0 . 4 % respectively .
3 shows the relative change on unsupervised downstream tasks attained by our models compared to hybrid . the results are shown in table 3 .
table 8 shows the results of our initialization strategies on supervised downstream tasks . our model outperforms all the other models in terms of performance .
results are presented in table 6 . cmow - c outperforms cbow - r on unsupervised downstream tasks .
results are presented in table 1 . the results are summarized in table 2 . cmow - c outperforms cbow - r in terms of depth , depth and depth . the results of the cbow model are shown in table 3 .
results are presented in table 3 . the results are summarized in table 4 . cmow - c and cbow - r outperform the other two models in terms of performance . the results show that the cmow model outperforms the other three models by a significant margin .
results are presented in table 3 . our model outperforms all other models in terms of e + org , e + per , and e + misc . the results of our model outperform all the other models . we also observe that our model performs better than all the models that outperform our model . however , our model still outperforms our model by a significant margin . in [ italic ] e + loc , we observe that the model performs significantly better than the model that outperforms the model in all other respects .
results on the test set under two settings are shown in table 2 . our model outperforms the previous model in terms of f1 scores . the results of our model outperform that of the previous models . we also observe that the system outperforms all the other models by a significant margin .
table 6 shows the results of the entailment ( ent ) model compared to the model ( g2s - ggnn ) in table 6 . the results are summarized in table 7 . we observe that the model outperforms the model by a significant margin .
results are presented in table 3 . the model outperforms the ldc2015e86 model in terms of performance . we observe that the models outperform the models by a significant margin . in particular , we observe that ldc2017e86 outperforms both the model and the model .
results on ldc2015e86 test set are presented in table 3 . the results are shown in table 4 . gigaword data are used to train models with additional gigawords .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the ablation results show that bilstm improves the performance of the model by a significant margin over the previous model .
results are presented in table 3 . the results of the model outperform those of the g2s - gin - ggnn - gat model . our model outperforms all other models in terms of the graph diameter .
table 8 shows the results of the ldc2017t10 test set . the results are presented in table 8 . the results show that the model outperforms the reference sentences in terms of the fraction of elements in the input graph that are missing in the generated sentence .
4 shows the results of the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) .
results are presented in table 2 . table 2 shows the accuracy of the mft and word2tag embeddings , respectively .
results are presented in table 3 . table 3 shows the performance of pos tagging accuracy on both the pos and pos tagging accuracy datasets . the results are summarized in table 4 . pos tagged accuracy outperforms both pos and pagging accuracy .
results are presented in table 5 . the results of the uni - bidirectional and res - residual nmt encoders are shown in table 4 .
results are shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy on different datasets is shown in the table 8 .
table 1 shows the results of training directly towards a single task . the results are shown in table 1 .
table 2 shows the results of the protected attribute leakage experiments in pan16 and pan16 . the results are shown in table 2 . in pan16 , the results are similar to that of pan16 in terms of the balanced and unbalanced data splits .
table 3 shows the performance on different datasets with an adversarial training . the difference between the attacker score and the corresponding adversary score is shown in table 3 . our model outperforms all the other datasets with the same training performance .
table 6 shows the performance of the protected attribute with different encoders . the performance of rnn and rnn is comparable to that of embedding guarded .
results are presented in table 3 . this model outperforms the previous model by a significant margin . the results are shown in table 4 . these models outperform the previous models in terms of performance and performance . we also observe that the results of this model outperform all the other models on the model .
results are presented in table 5 . this model outperforms the previous models in terms of time and performance . the results are shown in table 4 . we also show that the model performs better than the previous model on the basis of time .
results are presented in table 4 . this model outperforms the previous model by a significant margin . the results of the model outperform all other models in terms of err performance . in table 4 , we compare the performance of this model with that of our previous model . we also compare the results of our current model with those of the previous models .
table 3 shows the bleu score on the wmt14 english - german translation task . our model outperforms all the other models in terms of time in the training batch .
table 4 shows the performance of the model on squad dataset . the model outperforms all the other models in terms of match / f1 - score . we also observe that the models outperform all the models on the model , except for the model that outperforms the model . in particular , we observe that all models are better than the models that outperform the model when it comes to match / f1 score .
3 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number of the model , while sru denotes the reported result of the ner task .
results are presented in table 7 . table 7 shows the performance of our model on snli task with base setting and ptb task with ln setting .
results are presented in table 3 . the results are summarized in table 4 . the word - based system retrieval ( mtr ) outperforms the human system in terms of system retrieval . in particular , it outperforms both the human and human systems . when using the word word , the human systems outperform the human ones by a significant margin . as a result , the human systems are outperform both the system and the system , respectively .
4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( table 4 ) . the best results are shown in bold , with the highest standard deviation being 1 . 2 .
results are presented in table 3 . the results of our study are shown in table 4 . our results show that our model outperforms all the other models in terms of performance . we also observe that our models outperform both the df and df models .
results are presented in table 3 . the results of the test set are shown in table 4 . our results show that our model outperforms all the other models in terms of performance . we also observe that the results of our test set outperform the performance of the other three models .
results are presented in table 3 . the results of the test set are shown in table 4 . our results show that our model outperforms all the other models in terms of performance . we also observe that the results of our test set outperform the performance of all other models except for our model .
results are presented in table 1 . the results of our model are summarized in table 2 . table 1 shows the performance of the model in terms of depthcohesion . the results are shown in table 3 . our model outperforms all other models except for our model , which outperforms our model by a significant margin .
results are presented in table 1 . the results of our model are summarized in table 2 . table 1 shows the performance of the model in terms of terms of depthcohesion . the results are shown in table 3 . our model outperforms all other models except for our model , which outperforms our model by a significant margin .
results are presented in table 1 . lf is the enhanced version of visdial v1 . 0 , and r2 is the improved version .
table 2 shows the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . p2 is implemented by the implementations in section 5 .
table 5 shows the performance of the hmd - prec and wmd - prec on hard and soft alignments .
results are presented in table 1 . metrics and baselines are shown in table 2 . the metrics are summarized in table 3 . our model outperforms all the other models in terms of direct assessment and direct assessment .
results are presented in table 2 . the bleu - 2 model outperforms all the other models in terms of terms of performance , except for sfhotel .
results are presented in table 2 . the results are shown in table 3 . our model outperforms all the other models in terms of metric and bertscore - recall , with the exception of wmd - 1 , which outperforms both the meteor and leic scores .
results are presented in table 3 . the results show that the m0 model outperforms the m2 model by a significant margin . we also observe that m0 models outperform m2 models in terms of terms of performance . in particular , m0 outperforms m2 by a large margin .
results are presented in table 3 . we present the results of our model on the semantic preservation dataset . the results of the model on semantic preservation are summarized in table 4 . our model outperforms all the other models in terms of transfer quality , transfer quality and transfer quality . for semantic preservation , we observe that the transfer quality of semantic preservation is significantly better than that of semantic preservation .
3 shows the results of human sentence - level validation of the metrics . the results of the human sentence level validation are shown in table 5 . our model outperforms all other models in terms of semantic preservation and semantic preservation . we also observe that the human sentences are more accurate than the machine and human judgments .
results are presented in table 3 . the results show that the m0 model outperforms the m2 model in terms of terms of performance . m0 models outperform the m1 model by a significant margin . in particular , m0 outperforms m2 models by a large margin .
results on yelp sentiment transfer are shown in table 6 . our best models achieve higher bleu than those that are restricted to the same 1000 sentences and human references . the best models outperform those that were restricted to 1000 sentences , but the best ones outperform the ones that were limited to 1000 words . we also observe that the best model outperforms the worst ones by a significant margin .
table 2 shows the percentage of disfluent reparandum tokens that were correctly predicted as disfluencies . we observe that repetition tokens are more likely to have disfluency than repetition tokens . in particular , repetition tokens have the greatest effect on the accuracy of the disfluences .
table 3 shows the relative frequency of disfluent rephrases correctly predicted for disfluencies in both the reparandum and the repair ( content - content ) . the percentage of tokens correctly predicted to contain a content word is shown in table 3 .
results are presented in table 3 . the results of our model are summarized in table 4 . our model outperforms all the other models in terms of dev and innovations performance . we observe that the model achieves the best performance on both datasets , with the exception of the single model .
table 2 shows the performance of word2vec embedding on the fnc - 1 test dataset . our model outperforms the state - of - art models in terms of accuracy .
3 shows the performance of the unified model on the apw and nyt datasets for the document dating problem ( table 2 ) .
table 3 shows the accuracy of the word attention and graph attention for the task . in table 3 , we show that word attention improves the performance of the component models with and without attention .
results are presented in table 1 . table 1 shows the performance of the models in table 2 . table 2 shows the results of the model in table 3 .
results are presented in table 1 . table 1 shows the results of the cross - event model . in table 1 , we observe that the method outperforms the other methods in terms of identification and classification . the results of cross - event are shown in table 2 . we observe that cross - - event models outperform the other models in both terms of identification and classification .
results are presented in table 1 . all models are shown in table 2 . all models have the same results , except for the ones that have the best performance . the best performance is achieved in the english - only - lm model , which has the highest performance .
results on the dev set and the test set using discriminative training with only subsets of the code - switched data .
results are presented in table 5 . the performance on the dev set and on the test set is similar to that on the monolingual set . our model achieves the best performance on both sets , with the exception of the gold sentence .
results are shown in table 7 . precision ( p ) , recall ( f1 ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( table 7 ) . the precision ( p ) and recall ( f1 ) - score are statistically significant improvements over the previous two datasets .
results are presented in table 5 . precision ( p ) , recall ( f1 ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset are statistically significant improvements over the previous model .
results on belinkov2014exploring ’ s ppa test set are presented in table 1 . glove - retro outperforms all other embeddings on wordnet , verbnet , and wordnet .
table 2 shows the results of the rbg dependency parser with features coming from various pp attachment predictors and oracle attachments .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model .
results are presented in table 2 . table 2 shows the results of the embedding of subtitle data and domain tuning for image caption translation ( bleu % scores ) .
results show that subs1m outperforms subdomain - tuned models in terms of performance . the results are shown in table 1 . subdomain tuned models outperform subdomain tuning models on both en - de and in - de ( table 2 ) .
3 shows the bleu scores in terms of the automatic image captions . the results are shown in table 4 . the best automatic captions are the ones with the best automatic images captions , while the worst ones are those with only the best ones .
results are presented in table 5 . we compare the performance of enc - gate and dec - gate on the en - de model . the results are summarized in table 4 . the performance of the enc - gated model outperforms that of the dec - gates model in terms of bleu % scores . our results show that enc - gate outperforms dec - gate on both the ende model and the en de model .
3 shows the performance of subs3m and subs6m on the en - de model . the results are shown in table 3 . sub3m outperforms subs2m in terms of performance , but the performance is slightly lower than subs6ms . in particular , the performance improvement in subs3ms is due to the use of the multi - lingual model .
results are presented in table 3 . we observe that en - fr - rnn - ff outperforms en - es - ht in terms of translation performance . in table 3 , we compare the performance of en - ffr - ht and en - e - ht , respectively . the results are shown in table 4 .
3 shows the number of parallel sentences in the train , test and development splits for the language pairs we used .
table 2 shows the results of training vocabularies for the english , french and spanish data .
table 5 shows the results of the automatic evaluation scores for the rev systems . bleu and ter are shown in table 5 . the results of automatic evaluation are similar to those obtained from the original rev system .
results on flickr8k are presented in table 2 . vgs is the visually supervised model from chrupala2017representations , and rsaimage is the visual supervised model .
results are presented in table 1 . we observe that the performance of the audio2vec - u model is comparable to that of the segmatch model . our model outperforms both segmatch and segmatch in terms of performance .
3 shows the results of the different classifiers compared to the original on sst - 2 . we report the results in table 1 . in the original , we found that the edges of the screenplay were so clever that we want to hate it . however , in the new version , the edges are much better than the edges .
table 2 shows the results of fine - tuning on sst - 2 . the results are shown in table 2 . we observe that the number of words in the original sentence has increased , decreased or stayed the same through fine tuning . our model also shows the difference in the percentage of words that have been added to the sentence .
3 shows the changes in sentiment in sst - 2 compared to the original sentence . the results are shown in table 3 . in the case where negative labels are flipped to positive , the results are similar to that of positive labels .
results are presented in table 3 . the results are summarized in table 4 . we observe that the performance of the model is significantly better than that of the previous model . however , the results are still significantly worse than the previous models . in particular , we observe that our model outperforms other models in terms of performance .
