2 : throughput and training on the recursive framework , and tensorflow ’ s iterative approach , with the large movie review dataset as our training dataset . the recursive approach performs the best on inference with efficient parallel execution of the tree nodes , while the folding technique shows better performance on training . as table 2 shows , both the recur and the iterative approach give comparable performance on inference ,
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left to consider .
2 shows the performance of the hyper parameters optimization strategies for each model with different representation . softplus achieves the best performance with the maximum number of hyper parameters and the number of feature maps . the max pooling strategy consistently performs better in all model variations , and the sigmoid model outperforms all the other models using the same number of parameters . moreover , the boost function performs best in the multi - model setup , as measured by the f1 metric reported in table 2 .
1 shows the effect of using the shortest dependency path on each relation type . it can be observed that the macro - averaged model achieves the best f1 ( in 5 - fold ) with sdp as the dependency path , and the best diff . diff . is the f1 obtained by re - scoring the relation types with the sdp embeddings . the results are shown in table 1 . our model outperforms all the comparison models with a large margin .
results are presented in table 3 . the performance of y - 3 compared to y - 2 shows that , when combined with effective paragraph selection , the performance of the two models increases significantly .
results are presented in table 1 . the results of the best - performing models are reported in tables 1 and 2 . our model significantly outperforms the competition on all three metrics , with the exception of the paragraph level , where our model significantly improves the results in the final .
4 shows the c - f1 scores for the two indicated systems ; the lstm - parser shows lower performance than the majority systems , i . e . it achieves 60 . 62 ± 3 . 54 % overall improvement over the majority system , and its paragraph performance is higher than the others .
performance of original and original models on the test set is presented in table 4 . the results are presented in tables 4 and 5 . the original model outperforms all the other models except for the one that has been modified for the purpose of improving interpretability . the rouge - lstm model achieved the best results on both test set with an absolute improvement of 10 . 31 % on the bleu score and a 2 . 36 % improvement on the mouge score .
results for the original and the cleaned versions are shown in table 1 . the cleaned version has the highest number of distinct mrs and the average number of instances as measured by our slot matching script , see section 3 .
performance of original and original models on the test set is presented in table 2 . original models generally perform better than the original model on all tests , however the differences in accuracy between original and original models are significant . this result is reflected in the bleu score ( p < 0 . 001 ) and rouge - l scores ( p ≤ 0 . 01 ) . the error reduction on the original model is almost entirely due to the small size of the training set and the high accuracy of the derivational set . this results in a significant drop in performance for the two scenarios compared to the original .
results of manual error analysis on a sample of 100 instances from the original test set are shown in table 4 . in addition , we also found slight disfluencies in the training data as well as errors in the fine - tuned tgen , as shown in fig . 4 .
model the performance of our dcgcn model is presented in table 1 . the best performance is achieved on the external model , with a gap of 3 . 5 % in performance on the single model compared to the previous state - of - the - art model .
results on amr17 are shown in table 2 . our model achieves 24 . 5 bleu points and achieves a comparable e model size to the seq2seqb ensemble . the results also indicate that the model size and the number of parameters are important for future research in this direction .
results are shown in table 1 . the best performing model is the ggnn2seqb , which appears in english - german and english - czech , while the english - language model performs in german and turkish . we observe that the single model performs better than the other models in both languages , the difference in performance between the single and the single model is minimal , however , the difference between the two models is significant , we note that the bow + gcn model ( bastings et al . , 2017 ) is more than balanced , it achieves the best results with a 17 . 2 % overall improvement on average compared to the previous state - of - the - art model , which we base our model on .
5 shows the effect of the number of layers inside the dc block on the performance of the model design when we add the layers of layers that contribute to the overall effect of dc block expansion . we observe that for all but one of the layers , there is a significant ( p < 0 . 001 ) drop in performance as a result of the increased coverage of the layer that contributes to the growth of the network .
results are shown in table 6 . rc + la denotes gcns with residual connections , and dcgcn4 ( 27 ) shows that it has residual connections with multiple gcns . however , when we only consider the bias metric , gcn is still inferior to other baselines in terms of residual connections and gcn performance is only slightly better than the previous state of the art .
model f1 shows that dcgcn is comparable in performance to other state - of - the - art models in terms of number of models , model b , model c and model d . the results are summarized in table 2 .
8 shows the ablation study results for amr15 . it can be seen that the dense blocks reduce the performance of the model when removing the dense connections in the dev set .
results in table 9 show that the global encoder and the lstm decoder have similar performance on the graph encoder . however , the differences in performance between the two encoders are less pronounced for the two models .
investigate the effect of different initialization strategies on probing tasks . our paper shows that the initialization strategies have a significant impact on the performance of the probing tasks , as measured by the number of tokens in the subjnum and the average position of the objects in the context , respectively .
are presented in table 4 . the first group shows the performance of our method on the subtense level . our cbow / 400 model achieves the best performance on both subtense and subtense metrics , while the other two have lower performance . as the table shows , the threshold function on the threshold is significantly less effective than that on the upper boundary .
are presented in table 3 . our model outperforms all the other models except for the one that has been tested on the web ( sick - e ) . our model achieves superior performance on both mr and mpqa tests , while outperforming both the sst2 and sick - r baseline by a margin of 2 . 6 % on the mrpc test set . it also outperforms both the original and the alternative modes of cmp . cbow / 784 shows a slight improvement over the performance of the original model on both tests , but still performs slightly worse than the alternative model . this suggests that the performance gain comes from a better understanding of the human judgement .
results on unsupervised downstream tasks attained by our models are shown in table 3 . hybrid models outperform both cmp and cmp on all downstream tasks , though it has the advantage of being more stable . on the sts13 dataset , cbow shows a slight improvement over the performance of cmp .
results for the initialization strategies on supervised downstream tasks are shown in table 8 . our paper shows that our approach improves the performance by 3 . 8 points over the best state - of - the - art model on mpqa and sst2 , while surpassing the sick - e model by 3 points .
results for the unsupervised tasks are shown in table 6 . the best performance is on the sts12 , followed by the best performance on sts15 . these results show that the cbow - r model can significantly outperform the supervised approach on both tasks . furthermore , the performance drops significantly when training with supervised tasks , as shown in fig . 6 .
can be seen in table 3 the performance of cbow - r and its variants on the subtense subjnum . as the table shows , the method has the best performance on both subtense and subtense contexts , while it has the worst performance on the deep subtense subset . this highlights the differences in performance between the two approaches . cbow shows a slight improvement on the upper boundary of subtense , while the difference on subtense is modest .
subj and sick - r are comparable in terms of mrpc performance . our model outperforms all the other methods except for the one that has been tested on the sst2 dataset ( which is used on sst3 ) . however , it has the advantage of training on a larger corpus , which results in significantly better performance on the mrpc dataset than those on the other two datasets . this suggests that the similarity of the subj model to the original cmow - c model may be due to the high quality of the training data and the low overlap between the two methods . this validates our hypothesis that subj is a better complement to mpqa , and that it is better to train on larger datasets .
system performance in [ italic ] e + per and e + misc scores are reported in table 3 . supervised learning models ( mil - nd ) outperform all systems except for the one that does not use org and misc . the results of the best - performing model are summarized in table 1 . the system performs well in all aspects , with the exception of the org metric , where it does not perform well in the exceptional case of the " supervised learning " setting . as can be seen , the performance of all models that do not rely on org or misc is in the low - supervision setting , which means that the performance obtained by using the best performing model may not be significant even at the cost of losing the data .
2 : the results on the test set under two settings are shown in table 2 . our system outperforms all the models in terms of e + p and f1 scores , the results show that the supervised learning model can improve the system ' s performance in all settings . supervised learning models achieve better results with 87 % confidence intervals and 94 % e + f1 score , as can be seen , the system performs well in both settings , however , the performance drop in the best case when using supervised learning models is only 0 . 9 % compared to the previous state of the art model .
table 6 , we report the ref and ref scores of all models that exceed the 50 % threshold for ref ( g2s - gat ) , and the average ref score of ref - based models , which are generally more accurate than ref . ref also significantly outperforms ref , indicating that ref is important for model success .
results are presented in table 1 . the results of our model outperform the best previous work on every metric by a significant margin . the results show that the g2s models have superior generalization ability on all metrics , with the exception of the eor metric , which is more related to the ldc2015e86 and ldc2017t10 .
results on ldc2015e86 test set are shown in table 3 . our model outperforms the previous state - of - the - art models on both gigaword and external benchmarks .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results show that the use of bilstm improves the model ' s performance by 3 . 8pp over the strong lemma baseline .
results are presented in table 4 . the results are summarized in terms of the average number of frames compared to the baseline g2s - gin model , in particular , the results show that the model performs better when compared to other models with similar features . we also observe that the recall length of the sentences is shorter than those of the original models , indicating that the models are more suitable for the task at hand . when recall length was increased , the model performed better than the other models , showing that recall length and recall length were the most important components of the sentence .
shown in table 8 , the fraction of elements that are missing in the input graph that are present in the generated sentence ( g2s - gin ) , which is used in the test set of ldc2017t10 . it is clear from table 8 that the g2s models are better than the reference sentences , and that the productivity gain fromadded is greater than the gain from gold , indicating that the model has good interpretability .
4 shows the performance of our model on the parallel corpus in which the target languages are encoded . our model outperforms all the other models with a large corpus ( 200k words ) in terms of word embeddings accuracy .
2 shows the pos and sem tagging accuracy with baselines and an upper bound . unsupemb embeddings are the most frequent tags and word2tag is the most frequently used classifier .
results are presented in table 4 . our proposed method outperforms all the other methods we tested , improving upon the performance of our proposed method by 3 . 8 points .
5 shows the accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . our uni model improves the precision with a 3 . 8 % increase in accuracy over the standard embeddings .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in p - value < 0 . 05 . on the training set of pan16 , the attacker scored a weighted average of 10 . 2 % on a training set with a matching adversary score of 9 . 7 % on that set .
results in table 1 show that the training directly towards a single task can significantly improve the performance for pan16 participants .
2 shows the effect of the additional cost term on the balanced & unbalanced task acc and the unbalanced leakage rates . dial models show severe over - fitting since the balanced and unbalanced data splits are rare in the multi - task setting . sentiment models show that the presence of the gender - neutral classifiers can help the model to better interpret the messages generated by the additional classifiers .
performance on different datasets with an adversarial training set is shown in table 3 . the difference between the attacker score and the corresponding adversary ’ s accuracy is significant , as the training set increases , the leakage increases , the classifier trained on pan16 is more likely to classify the target as a gender - neutral group , sentiment is less effective , and the age of the target is less important .
6 shows the performance of the embeddings for different encoders . embedding guarded is more stable , while embedded leaky is better .
results are presented in table 2 . the results of our model outperform the best previous work on both datasets . our model achieves the best performance on the two datasets , while the model has the advantage of finetune features , it does not achieve the best results on the three datasets we observe that the lstm model performs on par with the original wt2 model on both the training and the final dataset , this model further improves upon the results of the previous work by outperforming both the original and the model by a significant margin . in particular , the performance improvement on the wt2 dataset is due to the higher performance of the finetune feature set , in addition , our model achieves a performance improvement of 3 . 8 % over the previous state - of - the - art model ,
performance of our model compared to previous models on the acc andbert datasets is presented in table 4 . the results are presented in tables 4 and 5 . table 4 shows that our model significantly outperforms previous models in both the acc / bert and the time - to - params datasets . when using the lstm , the model achieves the best performance on both datasets .
results are presented in table 1 . the results of experiment 1 show that our model significantly improves upon the yelp model in terms of err and polar time . on the other hand , our model performs slightly worse than yelp , indicating that the model is more suitable for a larger corpus . table 1 shows the performance of our model in the ama and full time settings . we observe that the ama and full time settings are comparable on both datasets , however , the difference between the average time and the average number of seconds on the yelp dataset is less pronounced for this model , this model exhibits a significant drop in performance compared to other models that use the arr .
3 shows the bleu score on the wmt14 english - german translation task , compared to the previous best state - of - the - art model , on the newstest2014 dataset . the model significantly outperforms other models in terms of decoding one sentence , as measured by the number of iterations used to encode the sentences in the translation task . as shown in table 3 , the sru model achieves a comparable performance to the other two models , both when training with the same training set and when decoding the sentences .
4 shows the model ' s performance on squad dataset . the results published by wang et al . ( 2017 ) show that the parameter number of our # params increases with the growth of the model . however , the rnet model obtains a lower match / f1 score than the previous best model , lstm , and sru , which achieves the best performance .
6 shows the f1 score on conll - 2003 english ner task . it can be seen that the lstm model considerably improves the performance by increasing the parameter number in the low - supervision settings .
performance of elrn on snli task with base + ln setting and test perplexity on ptb task with base setting setting .
system retrieval and word embeddings are presented in table 4 . the word embedding system ( ren et al . , 2017 ) is the most effective in terms of system evaluation . it exhibits considerable performance improvement when trained with multiple systems , with an absolute improvement of 3 . 5 % when using all the data available from the last published state - of - the - art systems . retrieeval is beneficial for both human and system evaluation , with a marginal drop of 0 . 3 % compared to the previous state of the art .
4 presents the human evaluation results on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all systems is reported in table 4 . the results are highlighted in bold , with the highest standard deviation being 1 . 0 for each system being ranked in top 1 or 2 for overall quality and the lowest average being 2 . 3 for accuracy .
are presented in table vii . our proposed system outperforms all the base lines except for the one that embeds the word " doc " in the embeddings . rather than docsub , our proposed system performs on par with df , docsub and tf , and thereby outperforms the df baseline on all metrics except the p < 0 . 05 . for df , we see that our proposed docsub wrapper outperforms both the original europarl and the original ted talks wrapper .
are presented in table 3 . the results are summarized in terms of p < 0 . 001 . for english , we see that the best performing model is europarl , followed by the more realistic docsub model . on the df model , our model outperforms all the other models except for the one that we use on the docsub dataset . our model achieves the best performance on both datasets with an absolute improvement of 0 . 01 on average compared to the previous state of the art .
are presented in table 3 . the results are summarized in terms of p < 0 . 001 . for english , we see that the best performing model is europarl , followed by ted talks . on the df dataset , our model performs slightly better than both the original and the new embeddings . however , the difference between the performance of our model and the original ones is less pronounced for docsub ,
metrics are presented in table 1 . our system achieves the best performance with a depth - cohesion score of 1 . 78 on a single metric compared to the previous best state - of - the - art model , docsub . europarl also achieves the highest score with a depthdepth score of 3 . 86 on the metric , while the gap between the maxdepth and the averagedepth of our model is close to those of the original source .
are shown in table 1 . our system achieves the best performance on every metric with a gap of 1 . 5 % on the dsim metric , compared to the previous best performance by europarl . on the other hand , our system performs slightly worse than our joint model , on the docsub metric , and on the hclust metric , on a larger scale . this is reflected in the depthcohesion metric , which measures the depth of the relation with other metrics , such as the dimension of the word embeddings .
performance of our enhanced model on the validation set of visdial v1 . 0 is shown in table 1 . the enhanced version of our model shows lower performance than the original visdial model , however , it is comparable to the enhanced version we used in the experiments of applying our principles . we observe that the enhanced model yields significantly better answer score sampling and the hidden dictionary learning , respectively , compared to the original version .
performance ( ndcg % ) of the ablative studies on different models is shown in table 2 . using p2 improves the performance ( i . e . , better coatt score ) and rva score ( p60 ) .
5 compares the performance of our models on hard and soft alignments . the results are summarized in table 5 . our model significantly outperforms the strong lemma baseline on hard alignments and fi - en , and significantly improves on the soft one .
3 presents the results on the direct assessment and eureor metrics . our proposed model outperforms the baseline on both metrics with a gap of 3 . 5 points from the last published results ( ruse - f1 ) . the results are summarized in table 3 .
performance on the sfhotel and smd datasets is reported in table 1 . the results of these models are summarized in terms of bleu - 1 and bertscore - f1 , respectively . these models significantly outperform the baseline on both datasets , with the exception of the exceptional case of w2v , where the bagel - 1 model significantly improves on the baseline .
performance of the models according to these baselines is reported in table 3 . the results are summarized in bold . leic scores significantly outperform the meteor score by a margin of 0 . 7 points on m1 while spice scores are slightly better than the leic score by 0 . 3 points .
results are shown in table 2 . the performance of all models that use the word embeddings improves over the baseline on the simuli - vuli metric , in particular , the improvement is more pronounced on the shen - 1 metric , where our model performs better than the other two models .
results are presented in table 4 . we observe that the transfer quality and transfer quality scores are the most important components for the semantic preservation and semantic preservation tasks . semantic preservation is the most difficult part of the preservation dataset to solve , yelp also exhibits a significant drop in performance compared to other methods , the semantic preservation scores are significantly better than those of the other two baselines , indicating that yelp models are more suitable for semantic preservation . the results of the best performing model are summarized in tables 4 and 5 .
5 shows the human evaluation results on the acc and pp metrics for each dataset , as shown in table 5 . the results are shown in the table 5 . the results show that the accuracy obtained by the human evaluations is higher than the average rate of machine evaluations , indicating that the evaluation results are more accurate .
results are shown in table 4 . the performance of all models that use the word " shen - 1 " is slightly better than the performance of other models using the same word embeddings , as can be seen , the performance on the sim metric is slightly worse than the other two models , i . e . m0 : m0 [ italic ] + para + 2d = 0 . 817 and m6 : m7 : m6 + ( para + 2d ) shows lower performance than the m3 model because of the larger size of the gap between the two models .
results on yelp sentiment transfer are shown in table 6 . our best model achieves higher bleu than those using simple - transfer , and the best acc ∗ model is restricted to 1000 words . we also observe that the multi - decoder model performs slightly worse than the best previous work on this metric , but it is comparable with the best recent work on the fu - 1 dataset . the best results on the three metrics are obtained using the simple transfer model ( yuan et al . , 2018 ) , which is similar to the success of previous work , but without the classifier in use . as table 6 shows , the best model is yang2018unsupervised , which indicates that the transfer model is more suitable for the task .
statistics for nested disfluencies are shown in table 2 . the percentage of repetition tokens that were correctly predicted to be disfluent ( i . e . a repetition token ) is 8 % overall , compared to the rate at which repetition tokens were predicted to belong to disfluency .
3 shows the relative frequency of rephrases correctly predicted as disfluent for both the reparandum and the repair ( table 3 ) . the fraction of tokens predicted to contain a content word is shown in parentheses , indicating that the disfluencies in the reparandum are localized in the content - content domain .
results are presented in table 4 . the best results are obtained using the best - performing text + innovations model , in addition , we also included text + raw models in our model as innovations , improving the model ' s performance by 0 . 2 % on the single test and overall improvement by 1 . 3 % . text + raw model outperforms the single model in terms of dev and test mean , as can be seen , the early model is more appealing than the late model due to the high overlap between text and innovations ,
performance comparison with state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model achieves the best performance with the help of a low - supervision threshold , and significantly outperforms the rnn - based model in the micro f1 ( % ) test set . the accuracy of word2vec embedding is slightly improved with rnn embeddings , however , the performance remains the same with a reduction in noise .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . neuraldater significantly outperforms all previous models except burstysimdater .
3 shows the performance of our method with and without attention . it achieves the best performance with 61 . 8 % and 65 . 6 % accuracy , respectively , compared to the previous state of the art model , ac - gcn .
model performance on the 1 / 1 and 2 / n test set is reported in table 1 . our model outperforms all the other models on both test set with a large margin . the results are presented in tables 1 and 2 , with the exception of jimee . we observe that for the 2 / 1 test set , our model performs on par with the best state - of - the - art on all test set .
3 shows the performance of our method on the event threshold . our method outperforms all the other methods in terms of both the identification and the argument threshold . in particular , we see that the method has the best performance on both the event and the threshold threshold .
can be seen in table 1 , all the fine - tuned models appear to have better performance on the dev perp and test wer tasks compared to english - only - lm .
results on the dev set and on the test set are shown in table 4 . fine - tuned train dev outperforms fine - tuned train dev , showing that incorporating only subsets of code - switched data can improve the train dev performance .
5 shows the performance of our model on the dev set and the test set , compared to the monolingual model . fine - tuned - disc improves the model ' s performance on both the dev and test set by 3 . 8 points , though it is less effective in the test setting .
results for the three eye - tracking datasets are shown in table 7 . type - aggregated gaze features improved the precision ( p < 0 . 001 ) and f1 - score ( f1 ) by 0 . 01 % in the conll - 2003 dataset , which shows statistically significant improvement .
5 shows the precision and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . type - aggregation features improve recall , but do not improve f1 - score ( p < 0 . 001 ) .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . the embeddings obtained by using autoextend rothe and schütze ( 2015 ) are used in wordnet 3 . 1 . however , the semantic embedding obtained by glove - retro can be further improved with a boost of performance from 0 . 3 to 1 . 4 . wordnet has been shown to perform similarly to the original wordnet , but it uses syntactic embedding instead . it achieves the best performance with an f1 of 1 . 0 and a ppa score of 89 . 7 .
performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . as table 2 shows , using oracle pp as the dependency parser results in a better performance than using lstm - pp alone . the results also indicate that oracle pp is beneficial for pp attachment prediction , improving the accuracy and precision of the model .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . it shows the significant drop in ppa acc . from the baseline to the low baseline .
2 shows the results with domain - tuned captions . our model outperforms all the other models with a large margin . adding subtitle data and domain tuning for image caption translation ( bleu % scores ) improves the multi30k model by 3 . 8 points over the baseline .
results are shown in table 4 . the models trained on the flickr16 domain are slightly better than the subs1m model on the en - de model , however , the improvements overdomain - tuned models are more striking on the largerickr17 dataset , in particular , these models show that domain - tuning improves the performance of the subs2m model when combined with domain - aware training data .
4 shows the bleu scores in terms of the model captions added with the automatic captions . as can be seen , the models using the multi30k model outperform all the models with the exception of mscoco17 . the model with the best bleus score in the en - de setting is better than the model using the monocap .
5 compares our approach with prior works on en - de embeddings ( lin et al . , 2017 ) . as shown in table 5 , using multi30k + ms - coco + subs3mlm , and detectron mask surface , enc - gate also improves the bleu % scores for visual information ( table 5 ) . finally , we can see that the enc - gate approach improves the performance of the model when combined with a dec - gate layer .
results are shown in table 4 . the best performances are obtained on the en - de and on the flickr16 datasets , subs3m also outperforms the subs6m model on both metrics , as the table shows , the multi - lingual approach relies on word embeddings and lexical features to derive the output . moreover , the text - only approach achieves an overall improvement of 2 . 36 % on the overall performance of the two metrics , which shows the value of combining the visual features and the semantic features of the three metrics . as can be seen , the combination of the visual and semantic features further boosts performance by 2 . 53 % over the baselines . further improving performance by incorporating the semantic and syntactic features improves the performance by 3 . 36 % . finally , the performance drop by 0 . 18 % over subs6
3 shows the performance of our model compared to other models using the word embeddings . the results are summarized in table 3 . we observe that our model outperforms all the alternatives except the original ones on mtld , and that the differences in translation quality are minimal .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 shows the quality of the english , french and spanish vocabularies used for our models . our model outperforms both the english and french datasets by a significant margin .
system reference bleu and ter scores for the rev systems are shown in table 5 . automatic evaluation scores ( bleu ) show that the re - rev systems perform better than the original ones , but still exceed ter and en - fr - rev .
results on flickr8k are shown in table 2 . segmatch embedding model can improve the recall performance for the target model by increasing the recall frequency and thereby increasing the mean f1 by 0 . 0 .
results on synthetically spoken coco are shown in table 1 . the visually supervised model outperforms the similarly supervised audio2vec - u model in terms of recall @ 10 and mean mfcc score , the results also indicate that segmatch embedding model can improve the interpretability of the discourse .
can be seen in table 1 the difference in the use of these classifiers compared to the original on sst - 2 . for example , cnn turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . as shown in the appendix , the rnn classifier also makes use of the edges edges of a screenplay screenplay .
2 shows the part - of - speech ( pos ) changes in sst - 2 . the numbers indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning . these results show that the value of goodness has not decreased , however , as those numbers have increased with time .
shown in table 3 , the change in sentiment from positive to negative is larger than that in the original sst - 2 . this indicates that the effect of the flipped sentiment signal is having a positive effect on sentiment .
results are presented in table 2 . attractively , the performance gap between positive and negative ( p < 0 . 001 ) indicates that our approach is more effective than the approaches suggested by vaswani et al . ( 2017 ) . however , the results are less striking when compare to other methods of leveraging word embeddings , such as sift , pubmed and sst - 2 . table 2 presents the results of re - scoring our proposed method for the pubmed and pubmed datasets . our approach is similar in that it improves the interpretability without sacrificing too many correct answers . however , it is less effective when compare with other methods . it is clear that the use of sift improves interpretability , which underscores the competitiveness of our method .
