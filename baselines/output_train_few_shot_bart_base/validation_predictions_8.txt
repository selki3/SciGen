shown in table 2 , the treelstm model performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training .
shown in table 1 , the balanced dataset exhibits the highest throughput compared to the linear dataset when the batch size increases to 25 .
results for each model with different representation are shown in table 2 . the max pooling strategy consistently performs better in all model variations . as shown in fig . 2 , the model with the highest number of hyper parameters has the best performance in all models with different representations .
table 1 shows the effect of using the shortest dependency path on each relation type . the best f1 ( in 5 - fold ) with sdp is shown in table 1 . with sdp , the relation type has the highest f1 and the best diff .
results are shown in table 3 . the y - 3 model outperforms all the other models in terms of f1 and f1 scores .
results are shown in table 1 . the results of the test set are presented in the table presented in table 2 . our model outperforms all the other methods except mst - parser and mate . as expected , our model achieves the best performance on both test sets .
4 shows the c - f1 scores for the two indicated systems ; essay vs . paragraph level is shown in table 4 . for the lstm - parser system , the average score is 60 . 62 ± 3 . 54 compared to the majority of the systems .
results are shown in table 3 . the original and the original models are presented in the same manner as the original ones . the original model outperforms all the other models except for the one that is completely cleaned . in addition , the original model is more accurate than the original one .
shown in table 1 , we compare the original and the cleaned e2e data with the original ones . we show that the original model has the highest number of distinct mrs , while the cleaned version has the smallest number of instances .
results are shown in table 3 . the original and the original embeddings are presented in the same manner as the correct ones . we observe that the error rate is significantly higher than those of the wrong ones , indicating that the system is not suitable for the task .
results of manual error analysis on a sample of 100 instances from the original test set are shown in table 4 . adding the correct values to the training data reduces the number of errors . correcting the wrong values reduces the chance of disfluencies .
results are presented in table 3 . the best models for the external and external datasets are the dcgcn ( single ) and the graphlstm ( multi - task ) datasets . all models outperform all the other models in the external dataset by a significant margin .
results on amr17 are presented in table 2 . the model size of the ensemble models is shown in terms of bleu points . as expected , the model size is significantly larger than those of the single and ensemble models .
results are presented in table 3 . the results of the single - sample model are shown in bold . we observe that the english - german model outperforms all the other models in terms of performance . in addition , we observe that both the german - language and english - czech datasets are significantly better than those in english - language datasets .
table 5 : the effect of the number of layers inside dc on the performance of the layer layers . table 5 shows the effect of these layers on the overall performance of dc . we observe that the layers in dc are larger than those in the other layers , indicating that the layer size of the layers is important .
6 : comparisons with baselines . + rc denotes gcns with residual connections . the results are shown in table 6 . with residual connections , we see that the gcn with the highest gcn performance is more likely to have residual connections than those without .
results are shown in table 3 . the dcgcn model outperforms all the other models in terms of performance . the results show that the model has the best performance on the b - test compared to other models .
8 shows the ablation study on the dev set of amr15 . - { i } dense blocks denotes removing the dense connections in the i - th block , while - { 4 } dense ones denotes removing them in the dense blocks .
table 9 shows the ablation study for the graph encoder and the lstm decoder . the results are shown in table 9 . the best performing encoder modules are the ones used in the graph encoder .
7 shows the results for initialization strategies on probing tasks . the results are shown in table 7 . our model outperforms all the other approaches except for the one that we used in the previous study .
results are presented in table 3 . table 3 shows the performance of our method in terms of depth and subtraction . we observe that our method outperforms all the other methods except for the cbow / 400 model , which is more difficult to achieve .
results are presented in table 3 . we observe that the cbow / 784 model outperforms all the other models except subj and sst2 models in terms of mrpc performance .
results on unsupervised downstream tasks attained by our models are shown in table 3 . our model outperforms both hybrid and hybrid models in terms of performance , with the exception of sts13 .
8 shows the performance of initialization strategies on supervised downstream tasks . our paper shows that the best initialization strategies are sst2 and sst5 , while the worst ones are subj and subj .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the cmow - r model outperforms the cbow - c model on both training objectives . the results are shown in table 6 .
results are shown in table 3 . the best performing method is cbow - r , which outperforms all the other methods in terms of depth and subjnum . as expected , the best performance is on the subtense level , while the best performing ones are on the subjnum level .
results are presented in table 3 . we observe that the cbow - r model outperforms all the other methods except subj and sick - r models in terms of mrpc .
results are shown in table 3 . the system performs better than all the other systems in the e + loc and e + per metrics . in [ italic ] e + loc and e + per metrics , the system outperforms all the systems except for the one that does not have the org metric . we observe that the system performs significantly worse than the system in both e + loc and per metrics , the results of the system are summarized in table 4 .
results on the test set under two settings are shown in table 2 . name matching improves the performance for all models . in [ italic ] e + p and e + f1 scores are shown . supervised learning improves performance by 2 . 5 % compared to the previous state - of - the - art model . for all models , the improvement in performance is due to the higher precision of the model .
table 6 : entailment ( ent ) in the model compared to ref ( g2s - gat ) . the results are summarized in table 6 . we observe that the model outperforms all the other models in terms of performance , with the exception of s2s and g2s .
results are shown in table 3 . the model outperforms the ldc2017t10 and ldc2015e86 by a significant margin . further , the models outperform the models in both categories .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 . our model outperforms all the other models except for the ones trained with the external data . we observe that our model performs better when trained with gigawords .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . we observe that bilstm significantly reduces the size of the model compared to other models .
results are shown in table 3 . the model outperforms all the other models in terms of sentence length . for example , s2s - ggnn has a significantly larger sentence length than the model , and a significantly smaller sentence size .
shown in table 8 , the fraction of elements in the output that are missing in the input ( g2s - gin ) that are not present in the generated sentence ( miss ) is significantly higher than those in the original sentence ( ggnn ) .
shown in table 4 , the accuracy of pos and ar models on a smaller parallel corpus ( 200k sentences ) is significantly higher than those using the 4th nmt encoding layer .
table 2 : pos and sem tagging accuracy with baselines and an upper bound . mft : most frequent tag ; unsupemb : most frequently tag ; word2tag : more frequent tag .
results are presented in table 3 . the pos tagging accuracy scores are shown in table 1 . we observe that the accuracy scores of the pos tagging accuracy scores are significantly higher than those of the other models . in addition , we observe that both the accuracy and precision scores for both the pos and the pos datasets are significantly better than those for both models .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages .
8 shows the performance of the attacker on different datasets . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 8 . the difference is caused by the training set being held out .
shown in table 1 , training directly towards a single task can significantly improve the performance of pan16 .
table 2 shows the performance of the protected attribute in the balanced and unbalanced data splits . the results are shown in table 2 . in the balanced data splits , there is a significant drop in performance compared to the unbalanced ones . we observe that the word " race " is the most important part of the conversation .
3 shows the performance on different datasets with an adversarial training . the performance on the task dataset is shown in table 3 . the difference between the attacker score and the corresponding adversary ’ s accuracy is explained by the difference in δ .
6 : accuracies of the protected attribute with different encoders . the examples shown in table 6 show that embedding guarded and embedded encoders can significantly improve the performance of the model .
results are shown in table 3 . the model outperforms all the other models in terms of both base and finetune . in particular , the model performs better than all the models except for the model with the exception of the lstm model , which performs worse on both datasets . we also observe that the model achieves better performance on the two datasets than the other ones .
results are presented in table 5 . the model outperforms all the other models in terms of time and accuracy . we observe that the lstm model is more accurate at the task time than the other model . it also performs better on the time - to - params task , as shown in table 6 . our model also outperforms the previous models in the base time task .
results are shown in table 3 . the model outperforms all the other models in terms of time - based err . we observe that this model significantly improves the performance of our model on both the yahoo and google time datasets .
3 shows the bleu score on the wmt14 english - german translation task compared to the previous state - of - the - art model . in addition , we show the performance of the model on the german translation task as well as on the english translation task .
4 shows the performance of our model on the squad dataset . our model outperforms all the models except lrn and sru in terms of match / f1 score . in addition , our model performs better than all the other models except for the lstm model , which performs worse . we observe that the parameter number of base is significantly higher than that of sru .
6 shows the f1 score on conll - 2003 english ner task . the lstm and sru models outperform all the other models in terms of parameter number . sru model outperforms all the models except for the ones with the higher f1 scores .
shown in table 7 , the snli model outperforms the ptb model on snli task with base + ln setting and test perplexity on ptb task .
results are shown in table 3 . word embeddings significantly improve the performance of system retrieval compared to human models . in particular , word embedding significantly improves the performance for human models compared to those trained on the human model . the results are summarized in table 4 . as expected , the word embedding method significantly improves performance for both human and system models .
4 : human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best results among all systems are highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the worst results are shown in table 4 .
results are shown in table 3 . the results of the experiments are shown to be significantly worse than those of the previous models . for example , the results for corpus and docsub are reported in table 1 . we observe that the performance of corpus is comparable to that of docsub .
results are shown in table 3 . the results of the experiments are presented in table 1 . we observe that the performance of the embeddings is comparable to those of the other embedders . in particular , the results are comparable to the ones of the two other embedding systems .
results are shown in table 3 . the results of the experiments are presented in tables 1 and 2 . we observe that the best performing embeddings are those on the df and tf datasets , while the worst ones are the ones on the tf dataset . for the df dataset , we observe that both the embedding and the translation performance are comparable .
embeddings are shown in table 1 . the results show that the depthcohesion of our models is significantly better than that of the other models . we also observe that our models are significantly more accurate than those of other models , such as docsub and docmax .
embeddings are shown in table 1 . the results are presented in the table below . our model achieves the best performance on both metric and depthcohesion metrics . the best performance is on the metric metric , with a 2 . 73 % improvement over the previous state of the art model .
shown in table 1 , we compare the performance of the models on the validation set of visdial v1 . 0 with the enhanced version of the visdial model . the results are shown in the table 1 . the enhanced model performs better than the enhanced one .
2 shows the performance ( ndcg % ) of different ablative studies on different models on visdial v1 . 0 validation set . our model outperforms all the other models in terms of accuracy .
5 shows the performance on hard and soft alignments compared to hard alignments . the results are shown in table 5 .
results are shown in table 3 . the metrics and baselines for direct assessment are presented in table 1 . we observe that the baselines are significantly better than those for the direct assessment model . for example , we observe that our model outperforms all the other models in direct assessment by a significant margin .
results are shown in table 3 . the bagel model outperforms all the baselines except for the one that sets the bertscore - f1 score . we observe that the bleu - 1 score is significantly better than the other baselines on both datasets .
results are presented in table 3 . the metric and baselines scores are shown in bold . for example , the meteor score is 0 . 939 and 0 . 949 , respectively , compared to the baseline score of 0 . 749 . for the bertscore - recall score , the baselines are reported in table 4 .
results are shown in table 3 . the results show that the m0 + para + lang model outperforms the other models in terms of performance . as expected , the performance of the m6 + model is lower than those of the previous models , indicating that the model is more suitable for the task .
results are presented in table 3 . we observe that our model outperforms all the other models in terms of transfer quality and transfer quality . in particular , we observe that the transfer quality scores are significantly better than those of the other three models .
5 presents the results of human sentence - level validation . the results are shown in table 5 . the results of the human sentence level validation are summarized in terms of the number of sentences that match the human sentences , and the percentage of sentences in the sentence that match .
results are shown in table 3 . the results show that the m0 + para + lang model outperforms the other models in terms of performance . in particular , the performance of the m2 + model is significantly better than those of the previous models . moreover , the results of the model outperform the previous model by a significant margin .
results on yelp sentiment transfer are shown in table 6 . our best models achieve higher bleu than prior work at similar levels of acc ∗ ( see table 6 ) . our best model achieves the highest acc score on the same 1000 sentences and human references , but it is less than the best model .
shown in table 2 , the number of reparandum tokens that were correctly predicted as disfluent is significantly higher than the number that were predicted as nested disfluencies .
3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - content ) .
results are shown in table 3 . the model outperforms all the other models in terms of dev and innovations . in addition , the model achieves the best performance on both test sets , with the exception of the single model , which achieves the worst performance on test set .
2 shows the performance of our model on the fnc - 1 test dataset compared to the state - of - art embeddings on cnn - based sentence embedding .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the unified model significantly outperforms all previous models .
3 shows the effectiveness of word attention and graph attention for this task . the results are shown in table 3 . for word attention , we compare the performance of the component models with and without attention .
results are shown in table 1 . the model outperforms all the other models except for the one that performs the best on the first stage . the results show that the model performs better on the second stage .
table 3 , we present the results of our method with respect to the identification and classification task . we show that our method outperforms all the other methods in terms of identification , classification task , and event identification task . in particular , we observe that the identification task is the most important part of the trigger task .
results are shown in table 3 . all the models shown in the table are in english , with the exception of the spanish - only - lm model , which has the best performance on dev perp and test acc .
4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the results are shown in table 4 .
5 shows the performance on the dev set compared to the monolingual set . the best performance is on the test set , where the code - switched sentence outperforms the gold sentence .
shown in table 7 , type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset shows statistically significant improvement in precision ( p < 0 . 001 ) .
5 shows the precision ( p ) , recall ( f1 ) and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . the precision ( p > 0 . 05 ) is significantly better than the recall ( p ≤ 0 . 01 ) .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . syntactic - sg embeddings are used in wordnet and wordnet 3 . 1 , and they are used to embed the wordnet and verbnet embedding . glove - retro is used in the original paper , and it uses syntactic skipgram embedding as well as the syntactic embedding of wordnet , verbnet , and golov et al . ( 2015 ) on the wordnet dataset .
results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments are shown in table 2 .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing context sensitivity is shown in table 3 .
shown in table 2 , we add subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun are shown in the table 2 .
results are shown in table 3 . the subs1m model outperforms all the other models in the en - de model . subdomain - tuned models outperform all the others in terms of performance , with the exception of the subs2m model , which outperforms both the original and the original models . we observe that the subdomain tuned models perform better than the original ones .
4 shows the bleu scores of the models with the best automatic captions . the results with marian amun are shown in table 4 . the model with the most captions outperforms the model with only one or all 5 .
5 compares the performance of the two strategies for integrating visual information . we observe that enc - gate and dec - gate have the highest bleu % scores compared to the other two strategies . in particular , enc - gates have the best performance compared to dec - gated . the results are shown in table 5 .
results are shown in table 3 . the subs3m model outperforms all the other models except subs6m models in terms of text - only performance . in addition , subs3ms outperforms other models in both terms of performance and quality .
results are shown in table 3 . we observe that the en - fr - ff model outperforms the other models in terms of translation performance . in addition , we observe that both the enfr - ht model and the out - of - the - box model outperform both the original and the original models .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 : training vocabularies for the english , french and spanish data used for our models . the results are shown in tables 2 and 2 .
5 : automatic evaluation scores ( bleu and ter ) for the rev systems . the automatic evaluation scores are shown in table 5 . as shown in fig . 5 , the system reference obtains a bleu score of 37 . 8 and 42 . 7 , respectively , compared to the previous state - of - the - art system .
results on flickr8k are shown in table 2 . vgs is the visually supervised model from chrupala2017representations . it achieves the best performance with a 2 . 5 % recall rate .
results on synthetically spoken coco are shown in table 1 . we observe that the vgs model outperforms all the other models in terms of recall and chance .
we report further examples in table 1 . for example , orig < c > turns in a screenplay that < u > at the edges ; it ’ s so clever you want to hate it . for cnn , we report that the edges of the screenplay are so clever that it can be used to hate hate it . for rnn , we show that the turns in the screenplay is so clever , the edges are so clear and the curves so clever . we also show that we want hate hate hate love hate hate to be used in the same sentence as the original .
table 2 shows that fine - tuning has not changed the number of words in sst - 2 . the results are shown in table 2 . the number of occurrences in the original sentence has increased , decreased or stayed the same .
3 : sentiment score changes in sst - 2 . the results are shown in table 3 . negative labels are flipped to positive and vice versa . the results indicate that the sentiment increases in positive and negative contexts .
results are presented in table 3 . the results of the study are summarized in terms of the number of instances in the corpus compared to those in the previous literature . in particular , the performance of sift is significantly higher than those of the other two datasets . as expected , the results of our study are significantly worse than those from the previous work . however , the difference between the two datasets is less pronounced , indicating that the results are more consistent .
