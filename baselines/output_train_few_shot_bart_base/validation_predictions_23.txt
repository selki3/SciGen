2 shows the performance of our iterative approach on the large movie review dataset . as table 2 shows , the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the iteration approach shows better performance on training .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left to train the model .
2 shows the performance of the max pooling strategies for each model with different number of hyper parameters . softplus performs better in all model variations , and on all model iterations with different performance metrics .
1 shows the effect of using the shortest dependency path on each relation type . it can be seen that the macro - averaged model achieves the best f1 ( in 5 - fold ) with sdp , and the smaller variation in diff . diff . is due to the shorter dependency path . we can also see that the approach that relies on sdp improves the f1 by a noticeable margin .
can be seen in table 3 the performance of y - 3 compared to y - 2 in terms of f1 and average ranking .
results are presented in table 1 . all models trained on mst - parser outperform the state - of - the - art model in terms of all metrics . for example , all models except for those trained on the abstracts of word2vec are better than the others on all metrics except for the paragraph level .
4 shows the c - f1 scores for the two indicated systems ; for the lstm - parser system , it is 60 . 62 ± 1 . 57 and 56 . 24 ± 2 . 87 respectively , compared to the majority performances of the other systems .
can be seen in table 4 the performance of original and original models on the test set . the results are presented in table 5 . original models perform better than the original on all tests except for the one that has the correct number of parameters . they are slightly worse than original models on some tests , but still superior on others . for example , the original model performs better than tgen and tgen on all test sets .
shown in table 1 , the original and the cleaned versions have the highest number of distinct mrs , total number of textual references and ser as measured by our slot matching script , see section 3 .
3 shows the results for original and original test sets . the results are presented in table 3 . original and original test sets are shown in bold . they are slightly worse than the original on some of the test sets . for example , their bleu score is significantly worse than those of tgen + on other tests . this is reflected in the fact that both the original and the original scores are slightly better on the test set .
4 shows the results of manual error analysis on a sample of 100 instances from the original test set . we found significant numbers of errors ( added , missed , slight disfluencies ) in the training set , as shown in table 4 . the percentage of errors we found in the original training set was significantly higher than the number of misclassified instances we found .
model performance on the external and internal datasets is reported in table 1 . the best performances are achieved by the dcgcn model ( song et al . , 2016 ) which significantly outperforms the state - of - the - art tree2str model on both external and external datasets .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points and achieves a comparable e / s score to seq2seqb . the results are shown in table 2 .
3 shows the results for english - german and english - czech . the results are presented in table 3 . we observe that the single model performs better in english - language than the other models that rely on the cnn + gcn model , both in english and german . the difference in performance between single and multi - language models is minimal , the results show that the ability to choose the best features for each language is relatively high , however , the difference is significant in english , where the average number of features is significantly higher than in other languages .
5 shows the effect of the number of layers inside dc on the performance of the layers in table 5 . we observe that for every layer that has one layer , there are three layers that contribute to the overall performance .
6 shows the performance of gcns with residual connections . rc denotes gcn with gcn connections , and dcgcn4 ( 27 ) shows significant performance improvement . with residual connections , gcn shows marginal improvements .
model a shows the performance of dcgcn models when trained on state - of - the - art data . the results are presented in table 1 . dcgcnn models outperform dcgcgcn in all but one of the cases when they are trained on a single data set .
8 shows the ablation study results for amr15 in terms of density of the connections in the dev set . it can be seen that removing the dense connections severely reduces the performance of the model , as shown in table 8 .
shown in table 9 , the models used in the graph encoder and the lstm decoder have varying coverage mechanisms . encoder modules use the best performance , but their coverage mechanisms are only present in the single - decoder setup .
investigate the effect of different initialization strategies on probing tasks . we show in table 7 the results for each initialization strategy on the output of tables 7 and 8 . the results are summarized in bold . for example , our method obtains the best performance on the depth - based subjnum task . it outperforms both glorot and somo in both ways .
are presented in table 3 . we observe that the best performing method is h - cbow . it obtains the best performance on every metric with a gap of 3 . 5 points in precision from the last published results .
are presented in table 3 . our model outperforms all the other methods except for the one that cmp uses . cbow shows significant performance improvement over both mpqa and sick - r . it also outperforms both sst2 and sst5 in terms of mrpc score . on the other hand , it has the advantage of training on a larger corpus and performs better on all three models .
3 shows the relative change from hybrid to hybrid on unsupervised downstream tasks attained by our models . the results are shown in table 3 . our model obtains the best performance with respect to the three downstream tasks .
8 shows the performance of initialization strategies on supervised downstream tasks . our paper shows that glorot and sst2 perform better than both the original and alternative approaches on three of the four tasks . on the other hand , sick - e performs slightly worse than sst - b and sts - b .
6 shows the performance for different training objectives on the unsupervised tasks . the cbow - c model outperforms the other two methods on three of the four tasks .
empirically show that cbow - r outperforms both cbow and somo in depth and topconst subtasks . the results are presented in table 1 . we observe that the method obtains the best performance when bound to a particular object . it obtains a significant improvement in the quality of the nested subjnum and the overall score on the two tasks .
subj and sick - r perform comparably to other methods in terms of mrpc and trec . cbow - r outperforms all the other methods except for sst2 and sst5 except for the one that it uses for mpqa . it obtains the best performance on all three of the mrpc test sets . it outperforms both sick and sts - b by a significant margin .
3 shows the results for all systems with e + and per metrics . our system outperforms all the systems except for the one that does not use org and misc . the results are summarized in table 3 . our model obtains the best results with the best org scores and the best per scores . it achieves the best result with a combined org score of 43 . 57 / 71 . 03 and 43 . 45 / 59 . 86 on the single - domain test set in supervised learning ( mil - nd ) . we observe that the best performing model is mil - nd , which improves upon the state - of - the - art model by 3 points .
2 shows the results on the test set under two settings . our system outperforms all the models except for mil - nd , which shows the diminishing returns from using supervised learning . the results are shown in table 2 . our model achieves the best results with 96 % confidence intervals of f1 score . further , the results show that the supervised learning model can significantly improve the e + p scores and the f1 scores of the model in both settings .
6 shows the results of ref and ref for all models except for those that do not use ref . ref significantly outperforms ref in all but one of the cases when ref is used only for food .
results are shown in table 1 . the models trained on the ldc2015e86 outperform the state - of - the - art models on all metrics except for the key metrics of eor and f1 .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 . our model outperforms the previous stateof - the - art model on both sets .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . it can be seen that the use of bilstm improves the model ' s performance by a noticeable margin .
results are shown in table 1 . we observe that the g2s model outperforms the other models in terms of sentence length and sentence length on both δ and sentence length . on the larger δ scale , s2s shows significant improvement on both metrics with a 5 . 9 % overall improvement on the average sentence length . the results are also statistically significant on the small - scale δ scale with a gap of 2 . 5 % on the large - scale baseline .
shown in table 8 , the fraction of elements that are missing in the input graph that are present in the generated sentence ( g2s - gin ) , is much smaller than those in the output , indicating that the model can rely on syntactic cues to derive its output .
4 shows the performance of the two approaches using the 4th nmt encoding layer . our model outperforms both the standard embeddings and pos tags with a large corpus .
2 shows the pos and sem tags accuracy with baselines and an upper bound . accuracies are reported in table 2 . we use unsupemb embeddings as the classifier and word2tag as the upper bound for both baselines .
can be seen in table 4 . our results are summarized in terms of accuracy and tagging accuracy . we observe that the accuracy obtained by our method significantly outperforms the performance of the other methods .
5 shows the accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ ( p < 0 . 001 ) .
results in table 1 show that training directly towards a single task can improve the performance for both groups .
2 shows the effect of the additional cost term on the balanced and unbalanced task averages . pretrained attribute leakage can be seen in table 2 , where we see that the presence of gender - neutral tags can further improve the task performance for both groups .
performance on different datasets is shown in table 3 . for each training example , we trained our mention classifier with a weighted average of 62 . 5 % chance to reach the highest accuracy . for both training examples , the difference between the average age of the target and the corresponding adversary is 9 . 2 % .
6 shows the concuracies of the protected attribute with different encoders . embedding with rnn embeddings can further improve the performance for the model by 3 . 5 points .
results are shown in table 4 . the results of our model outperform the best state - of - the - art models on all three datasets . we observe that the lstm model performs better on both the training and the finetune datasets , it also outperforms both the original wt2 and the wt2 model in terms of both feature sets . it is clear from the results of the second study that this model is better than the original model on both datasets .
3 shows the performance of our model compared to previous models . the results are summarized in table 3 . we observe that our model significantly outperforms previous models in terms of both acc andbert time . it is clear from the results that the lstm model is better at both tasks .
3 shows the performance of our model compared to other models using the same time - based approach . our model outperforms all the other models except for the one that uses google translate . the results are summarized in table 3 . we observe that the amapolar time model significantly outperforms both the yelp and full time datasets in terms of err performance . this model also significantly improves the recall on both datasets when using the ama - polar time and full time modes .
3 shows the bleu score on the wmt14 english - german translation task . it can be seen that the tokenized approach has a significant impact on translation performance , as measured by the time in seconds in the model ' s final training batch .
4 shows the performance of our model with respect to match / f1 score on squad dataset . the model obtains the best performance with a parameter number of 2 . 67m and a f1 score of 1 . 83 . it can be observed that all the models using this model have higher parameter numbers than the ones using lstm .
6 shows the f1 score on conll - 2003 english ner task . it can be seen that the lstm model significantly outperforms the other models in terms of parameter number in the evaluation set .
7 shows the performance of our model with base + ln setting and test perplexity on snli task with ptb setting . the results are shown in table 7 .
system evaluation results are shown in table 1 . word embeddings are used in the system evaluation tasks for both human and machine learning tasks . system retrieval is the most sophisticated of the three systems , with an absolute improvement of 2 . 5 % over previous state - of - the - art systems . docs are used for both system evaluation and word - based attention . all the word methods used for system evaluation seem to be beneficial for both systems ,
4 shows the human evaluation results on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performing system is seq2seq , which achieves a 3 . 2 overall improvement over human evaluation on k 1000 and k 2000 .
3 shows the performance of all the models trained on the corpus dataset . our model outperforms all the base models except for those trained on europarl . in fact , it even outperforms both the df and docsub datasets by a significant margin . on the other hand , it still performs slightly worse than some of the other models , such as docsub and hclust .
3 shows the performance of all the models trained on the corpus dataset . the results are summarized in table 3 . our model outperforms all the other models except for the one that embeds the word embeddings . we observe that the best performing model is europarl , which outperforms both the df and docsub models .
3 shows the performance of all the models trained on the corpus dataset . our model outperforms all the other models except for the ones using the df embeddings . for example , we see that europarl is better than both df and docsub on all three datasets , while docsub is slightly worse than docsub .
embeddings are shown in table 1 . they are based on the best performing metric on the corpus dataset , namely , depthcohesion . europarl is closer to the maxdepth of our model on every metric , but is slightly worse than our maxdepth metric .
embeddings are shown in table 1 . they are based on the best performing metric on the corpus dataset , namely , depthcohesion . they complement the maxdepth of our model with a minimum of 1 . 5mm and 1 . 1mm respectively . they outperform the best baseline on every metric by a significant margin . our model achieves the best performance on both corpus and europarl metrics .
experimental results are shown in table 1 . we use the enhanced version of our nlst model as a comparison for the experiments of applying our system to the validation set of visdial v1 . 0 . lf outperforms both the original and enhanced versions .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the validation set of visdial v1 . 0 . using only p2 indicates the most effective one ( i . e . , hidden dictionary learning ) on the three models .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . they show that the hmd - prec model significantly outperforms the other models on both hard alignments by a significant margin .
3 presents the results of our approach in terms of direct assessment and bertscore - f1 on the test set of hotpotqa . the results are summarized in table 3 . we observe that the approach significantly outperforms the baselines on both sets , with the exception of the one in which it obtains the best performance .
3 presents the bagel and sfhotel scores on the test set of hotpotqa . the results are summarized in table 3 . we observe that the baseline bert scores significantly outperform the baseline on all three metrics , with the exception of bleu - 2 .
performance of the models according to these baselines is reported in table 3 . the results are summarized in terms of leic scores and bert score - recall scores . they are broken down into three categories : " m1 " , " m2 " and " w2 " . the leic score - recall scores are reported in tables 3 and 4 .
experimental results are shown in table 3 . we observe that for all models that rely on word embeddings , their performance is comparable to that of the original model ( m0 ) .
results are shown in table 4 . semantic preservation and transfer quality are the most important aspects of the semantic preservation and semantic preservation tasks . syntactic preservation tasks are further improved with the addition of semantic preservation features . for semantic preservation , we see that the clustering quality of all the models improves significantly with the transfer quality of the two types of documents . the semantic preservation approaches are particularly beneficial for semantic preservation as well as semantic preservation .
5 shows the human evaluation results . we show the results of human evaluation using the [ italic ] ρ b / w negative pp and human ratings of fluency . the results are shown in table 5 . it is clear from the table that the accuracy obtained by using these metrics is relatively high ( p < 0 . 001 ) for both language and human evaluations .
experimental results are shown in table 4 . we observe that the m0 model outperforms the m1 model in terms of both grammatical and syntactic performance . in particular , the performance drop is most pronounced for m1 because shen - 1 has less chance to perform well on pp .
results on yelp sentiment transfer are shown in table 6 . our best model achieves higher bleu than any other model using simple - transfer or n - gram embeddings , but only slightly outperforms the best model using only one classifier , multi - decoder outperforms both the best and worst state - of - the - art models in terms of acc ∗ . we also observe that the use of multiple classifiers in the transfer setup severely affects the model ' s ability to interpret sentiment .
statistics for nested disfluencies are shown in table 2 . the percentage of repetition tokens that were correctly predicted to be disfluent is slightly higher than the rate at which repetition tokens were predicted as disfluency .
3 shows the relative frequency of rephrases correctly predicted as disfluent for both the reparandum and the repair ( table 3 ) . the fraction of tokens predicted to contain a content word is shown in parentheses , indicating that the disfluencies in the reparandum are relatively small .
experimental results are presented in table 4 . we observe that the text model outperforms the single model in terms of both dev and test quality . text + innovations model achieves the best results with 86 . 28 % true improvement over text + raw model , in addition , text + innovations models achieve the best overall performance with 87 . 48 % true improvements .
performance comparison with state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model achieves the best performance with the help of rnn - based sentence embedding .
2 shows the performance of different approaches for the document dating problem on the apw and nyt datasets . the best performing approach is attentive neuraldater .
3 shows the effectiveness of both word attention and graph attention for this task . it can be seen in table 3 that the ac - gcn model performs substantially better than the other two models in terms of word attention .
experimental results are shown in table 1 . the best performing models are jnn , dmcnn and trigger . all models perform better than the other models in all but one of the comparisons .
experimental results are shown in table 1 . all methods used for this analysis show that the method performs well in the single - attention scenario . in particular , the method has the advantage of significantly better performance on the event with a larger number of participants . both the method and the threshold for each event are statistically significant , with the exception of the case of the one in which the model is used . the method is able to handle a large number of scenarios with different stages of the event classification .
can be seen in table 4 . all models trained on the spanish - only - lm model outperform all the other models except for the ones that do not use pascal - vocallelle . all but one are better than the others in english - only .
4 shows the results on the train dev and test set . fine - tuning achieves the best results with only subsets of the code - switched data in the dev set .
can be seen in table 5 . accuracy on the dev set and on the test set is reported in terms of the type of gold sentence in the gold sentence . fine - tuned - disc shows lower performance than fine - tuned - disc on both sets .
results are shown in table 7 . precision ( p ) , recall ( f1 ) and f1 - score ( f ) are all statistically significant improvements over the type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvement is statistically significant ( p < 0 . 001 ) on the two metrics , with a marginal drop of 0 . 01 point from the baseline .
5 shows precision ( p ) , recall ( f1 ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . type - aggregation features significantly improve recall ( p ≤ 0 . 00 ) and recall ( f – 0 . 01 ) by using type features alone .
experimental results on belinkov2014exploring ’ s ppa test set are shown in table 1 . the hpcd approach uses syntactic - sg embeddings as the base for wordnet , and it uses glove - extended vectors . the results on the original paper are outlined in tables 1 and 2 . these results show that the syntactic embedding methods can further improve wordnet performance by adding syntactic features such as the semantic skipgram embedding layer .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . it can be seen in table 3 that removing the sense and context sensitive features severely decreases the ppa acc . by 0 . 3 points .
2 shows the performance of domain - tuned models compared to multi30k in terms of image caption translation . subsultrained models outperform both en - de and mscoco17 , showing that domain tuning leads to better interpretability .
3 shows the performance of subs1m on en - de and in - de datasets . the results are summarized in table 3 . all the models shown in the table show that the domain - tuned models perform better on the larger datasets , with the exception of mscoco17 , which shows the diminishing returns from domain tuning . subdomain - tuning improves the results for all models with at least one exception .
4 shows bleu scores in terms of the automatic captions added after adding the best ones or all 5 models . the results are shown in table 4 . it can be observed that the multi30k model outperforms all the other models when using only the best five models .
5 compares the output of two approaches for integrating visual information . we observe that enc - gate and dec - gate perform better than the other approaches we consider , as shown in fig . 5 , both the w - based and img - based approaches outperform their counterparts in both visual information and decoding .
1 shows the performance of subs3m and subs6m on the single - domain datasets . the results are presented in table 2 . sub - categories based on the visual features of the two datasets are slightly less striking than those of the other two models , but still superior to the others in terms of performance . we observe that the combination of visual features and the text - only features boosts performance for the two models . however , this is only statistically significant when we consider the effect of the multi - domain model on the output quality of the three models .
performance on mtld is reported in table 3 . we observe that the best performing model is en - fr - rnn - ff , which results in significantly better performance than the other two models . table 3 shows the results for both systems . for the english learner , we use the word " fr " , which means that the model is more suitable for the task .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 shows the english , french and spanish vocabularies used for our models . the results are shown in table 2 . our model outperforms the best performing french model by a large margin .
system reference bleu and ter scores for the rev systems are presented in table 5 . automatic evaluation scores ( bleu ) and ter are shown in bold . the systems evaluated by ter consistently show lower performance than en - fr and en - es - rev .
2 shows the performance of our visually supervised model compared to other supervised models on flickr8k . the results are shown in table 2 .
experimental results on synthetically spoken coco are shown in table 1 . the model trained on the embedded embeddings achieves the best performance with a 10 . 5 % recall rate .
shown in table 1 , one of the classifiers for cnn ( rnn ) turns in a < u > turn in a screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . as shown in fig . 1 , the other classifier for cnn also makes use of the edges as well as the curves .
2 shows the results for part - of - speech ( pos ) changes in sst - 2 . the results are shown in table 2 . they indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning .
shown in table 3 , the sentiment changes in sst - 2 when negative labels are flipped to positive . this shows that the effect of the flipped sentiment signal is very positive for both systems .
results are presented in table 1 . we observe that the transfer learning ability ( pmi ) is beneficial for both positive and negative states , improving the interpretability for both approaches . however , it is less effective for both negative and positive states . this suggests that more research should be done to further improve interpretability . the results are summarized in tables 1 and 2 . table 1 shows that the use of smift improves interpretability without a drop in performance .
