2 shows the performance of the treelstm model on our recursive framework and tensorflow ' s iterative approach , with the large movie review dataset . throughput performance on training is comparable to that of recur , which shows better performance on inference .
results in table 1 show the performance of the treernn model when the batch size increases from 1 to 25 , and when the number of iterations is increased from 10 to 25 .
results for each model with different representation are presented in table 2 . the max pooling strategy consistently performs better in all model variations . it also outperforms all models with different representations .
results in table 1 show the effect of using the shortest dependency path on each relation type . the best f1 ( in 5 - fold ) with sdp is achieved with a f1 of 71 . 86 , with a diff .
results are presented in table 3 . y - 3 outperforms y - 2 in terms of f1 and r - f1 . the results are shown in table 4 .
results are presented in table 1 . the results of our test set are summarized in terms of paragraph level and f1 scores . our model outperforms all other methods except mst - parser , which achieves the best performance . we also observe that our model achieves the highest performance on the test set , with the exception of our model achieving the highest f1 score .
4 shows the performance of the lstm - parser and stagblcc systems compared to the previous state - of - the - art systems . the results are shown in table 4 .
results are presented in table 1 . the original model outperforms both the original and the original models in terms of bleu and accuracy . the results are summarized in table 2 . the original model performs better than the original model by a margin of 3 . 93 % compared to 1 . 8181 % on the original .
results are presented in table 1 . the original e2e data and our cleaned version are shown in bold . the original and the cleaned versions have the highest number of distinct mrs and the highest percentage of textual references , respectively . the cleaned version has the highest mrs , with the exception of the slot matching script .
results are presented in table 1 . original and tgen models outperform original models in all but one of the three categories . the original model outperforms the original model in all three categories except bleu and ser . the tgen model performs better in both categories , with the exception of ser , where it performs worse than original models .
4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . we found that the original training set had a significant number of errors , and that it had a slight disfluency .
results are presented in table 1 . all models outperform all models in terms of performance , with the exception of snrg ( song et al . , 2017 ) and pbmt ( pourdamghani and cohen , 2018 ) . all model outperforms all models except for snrg , which outperforms snrg .
results on amr17 are presented in table 2 . our model achieves 24 . 5 bleu points , which indicates that the model size in terms of parameters is comparable to seq2seqb . we observe that the performance of our model is comparable with that of our other models , which are comparable in size to our own .
results are presented in table 1 . our model outperforms the previous best - performing models in english - german and english - czech . the results are summarized in table 2 . we observe that our model performs better than all the other models in both languages .
5 shows the effect of the number of layers inside dc on the performance of the layers inside the dc . table 5 shows that dc has a significant effect on the quality of the layer inside dc , as shown in table 5 . we observe that when we add layers of layers , dc has the greatest effect on performance .
6 shows the performance of the baselines with residual connections . the results are shown in table 6 . rc + la ( 2 ) and gcn + rc ( 4 ) are comparable in performance with baselines . however , when using residual connections , gcn + rc + la is comparable to the baseline .
results are presented in table 1 . we observe that dcgcn ( 2 ) outperforms all other models in terms of performance , with a significant drop in performance from the previous state - of - the - art model .
8 shows the results of ablation study on amr15 . the results are shown in table 8 . table 8 shows that removing the dense connections in the i - th block reduces the number of connections .
9 shows the ablation study for the graph encoder and the lstm decoder . our model outperforms all the other models in terms of coverage , with the exception of dcgcn4 .
7 shows the performance of our initialization strategies on probing tasks . our results are summarized in table 7 . our model outperforms all the other approaches except for glorot and subjnum . we also outperform all the others in the evaluation .
results are presented in table 1 . the best performing models are h - cbow and h - cmow / 400 , which outperform all the other models in terms of depth and precision . in terms of precision , the best performing model outperforms all the models except for the ones with the best precision .
results are presented in table 1 . subj and sst2 outperform all other models except subj . subj has the best performance on both mrpc and mpqa , and has the highest mrpc score . sick - e has the worst performance on the mrpc test set , and the best mrpc scores are on sst1 .
results are presented in table 3 . our model outperforms both hybrid and cmp on unsupervised downstream tasks attained by our models .
8 shows the performance of our initialization strategies on supervised downstream tasks . our model outperforms all previous models except subj and mpqa . we also outperform all models except sst2 and sst5 in terms of initialization performance .
6 shows the results for different training objectives on the unsupervised downstream tasks . cmow - c outperforms cbow - r on all three tasks except sts13 , sts14 and sts15 .
results are presented in table 1 . the results are summarized in table 2 . the best performing method is cbow - r , which outperforms both cbow and somo - r in terms of depth and coordinv . however , it does not outperform both the best performing methods in both domains .
results are presented in table 1 . subj and sst2 outperform all the other models except subj . subj outperforms subj in terms of mrpc and mpqa performance . sick - e outperforms sst1 in both mrpc performance and mrpc score . it is clear that subj has superior mrpc scores .
results are presented in table 1 . our system outperforms all other systems in terms of e + org and per . we also observe that our system performs better than all the other systems , we observe that the system performs best in all aspects of the e + org test set . the results of our system are summarized in table 2 . in table 1 , we observe the performance of the system in all metrics . name matching performance is comparable to that of the previous system , however , the results are slightly worse than the previous state of the art model .
results on the test set under two settings are shown in table 2 . name matching improves e + p and e + f1 scores by 3 . 5 % compared to the previous model . supervised learning improves e − p by 2 . 3 % compared with the previous state - of - the - art model ( mil - nd ) .
6 : entailment ( ent ) and ref ( g2s - gin ) in table 6 shows the results of our model compared to other models . the results of the model outperform all models except those that do not have ref . we observe that the model performs better than other models in terms of the number of entries compared to the average number of ones .
results are presented in table 1 . our model outperforms all the other models in terms of bleu and meteor scores .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model outperforms all the other models in terms of size and size .
results are presented in table 1 . our model outperforms all the models except g2s - gin and gat in terms of sentence length and sentence length . the results are shown in table 2 . we observe that the model performs better on sentence length than the other models .
results are shown in table 8 . our model outperforms all the other models in the test set of ldc2017t10 .
4 shows the performance of the models trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the results are shown in table 4 .
results in table 2 show that word2tag has the highest pos and sem accuracy with baselines and an upper bound . the results are shown in bold . the results of unsupemb are presented in the table 2 .
results are presented in table 1 . table 1 shows the performance of the pos tagging accuracy and the accuracy of the results for each metric . the results are summarized in table 2 . we observe that the accuracy is comparable to that of the previous best performing metric .
5 shows the accuracy of our uni and res encoders over all non - english target languages , averaged over all four layers of the 4 - layer uni / bidirectional / residual nmt encoderers .
8 shows the performance of the attacker on different datasets . the difference between the attacker score and the corresponding adversary ’ s accuracy is significant .
1 shows the performance of pan16 and pan16 when training directly towards a single task . the results are presented in table 1 .
table 2 shows the performance of pan16 and pan16 on the balanced task and unbalanced task averages . the results are presented in table 2 . the results show that pan16 has the best performance on balanced task averages , but the results are slightly worse than pan16 .
3 shows the performance on different datasets with an adversarial training . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 3 .
6 : accuracies of the protected attribute with different encoders . the results are shown in table 6 .
results are presented in table 1 . our model outperforms other models in terms of base and finetune performance . we observe that our model performs better than other models on both datasets . the results of our model are summarized in table 2 . it outperforms all models except lrn , which performs better on both models .
results are presented in table 5 . our model outperforms all models except gru and atr in terms of training time . we observe that gru outperforms both lstm and sru in both training time and training time , respectively .
results are presented in table 1 . our model outperforms all the other models in terms of err performance . we observe that our model improves upon the performance of both yahoo and google time err by a significant margin . the results of our model are summarized in table 2 .
3 shows the bleu score of our model on wmt14 english - german translation task . our model outperforms all previous models except gru and sru in terms of decoding one sentence per training batch .
4 shows the performance of our model on squad dataset . our model outperforms all models except lrn , sru and atr ( 2017 ) in terms of match / f1 score . we also observe that our model performs better than all models in the baselines , with the exception of lrn ( 2017 ) , which performs worse than the other models .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number in the reported result , while sru denotes the performance of the model .
results are shown in table 7 . our model achieves the best performance on snli task with base + ln setting and test perplexity on ptb task with base setting .
results are presented in table 1 . the word - based system retrieval ( mtr ) outperforms the human system ( r - 2 ) in terms of performance . in particular , it outperforms both human and system retrieval in fact , it is better than both human systems ( rtr , r - 2 ) . in addition , the word model outperforms human systems , it is clear that human systems are more effective than system ones , and that this is due to the fact that both systems are trained on the same data .
4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( k 1000 ) . the best performance among all systems is achieved on a test set of 1 , 2 , 3 , and 1 . 2 , respectively .
results are presented in table 1 . the results of our experiments are shown in table 2 . the results are summarized in table 3 . we observe that our model outperforms all the other models in terms of performance . we also observe that the performance of our model is comparable to that of other models , and that it performs better than other models .
results are presented in table 1 . the results are shown in table 2 . we observe that the performance of our model is comparable to that of the other models in terms of p < 0 . 01 , p > 0 . 02 , and p > 1 . 05 . our model outperforms both the df and df models in both cases .
results are presented in table 1 . the results are shown in table 2 . we observe that the performance of our model is comparable to that of the other models in terms of p < 0 . 05 and p < 1 . 05 . our model outperforms both the df and df models , but is slightly better than the df model .
results are presented in table 1 . the results are shown in table 2 . europarl achieves the best performance on both metric and metric metrics . it achieves the highest performance on metric metrics , with the exception of df , where it achieves the worst performance .
results are presented in table 1 . the results are summarized in the table below . our model outperforms all the other models except for europarl , which achieves the best performance . the model achieves the highest performance on both metric and metric metrics . however , it does not achieve the best results on metric metrics such as metric depth and metric depth .
results are presented in table 1 . the enhanced version of lf outperforms the original visdial v1 . 0 model by a significant margin .
2 shows the performance ( ndcg % ) of different ablative studies on different models on visdial v1 . 0 validation set . the results are shown in table 2 .
5 shows the performance on hard and soft alignments compared to hmd - f1 + bert on hard alignments .
results are presented in table 1 . the baselines are shown in bold , with the exception of ruse ( * ) and w2v , which show the performance of the baselines on the test set . as expected , the results are significantly better than those of the other baselines .
results are presented in table 1 . the bagel and sfhotel baselines outperform all the baselines except meteor and bertscore - f1 in terms of performance .
results are presented in table 1 . the baselines are shown in bold , indicating that the model is more accurate than the baselines . in particular , the model outperforms the models in terms of bertscore - recall scores .
results are presented in table 3 . we observe that the m0 model outperforms the m1 model by a significant margin . in particular , it outperforms both the m2 model and the m3 model by significantly improving the performance .
results are presented in table 1 . the results are summarized in terms of transfer quality and transfer quality . for semantic preservation , our model outperforms all the other models except m0 , m7 , and m7 in the transfer quality metric . as expected , the results are similar for semantic preservation and semantic preservation .
5 shows the results of human sentence - level validation . the results are shown in table 5 . our model outperforms other models in terms of semantic preservation and accuracy . we also observe that our model performs better than those of other models .
results are presented in table 1 . we observe that the m0 model outperforms the m1 model by a significant margin . in particular , it outperforms m1 , m2 , m3 , and m6 + in terms of performance .
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu and the highest acc ∗ on the same 1000 sentences and human references . however , it is not clear whether it achieves the best acc or not . the best models are simple - transfer and unsupervised , and the best ones are limited to 1000 sentences . our best models ( yelp 2018 and yang2018unsupervised ) achieve higher acc than those trained on the previous model .
2 shows the percentage of reparandum tokens that were correctly predicted as disfluent , compared to repetition tokens . for repetition tokens , we observe that the repetition tokens are more likely to be misfluent than the disfluency tokens .
3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - function ) . the fraction of tokens predicted to contain a word in each category is shown in table 3 . the percentage of tokens correctly predicted for each category shows the percentage of disfluency that contains a word .
results are presented in table 1 . the model outperforms all the other models in terms of dev mean and test mean . in particular , it outperforms both the single and innovations models in dev mean .
2 shows the performance of word2vec on the fnc - 1 test dataset . our model outperforms the state - of - art word2vec embeddings on both test datasets . our model achieves the best performance with respect to both topics .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the best performing method is burstysimdater .
3 shows the performance of neuraldater with and without attention . the results show the effectiveness of both word attention and graph attention for this task .
results are presented in table 1 . the best performing models are jvmee , dmcnn , jmee , and jrnn .
results are presented in table 1 . our method outperforms other methods in terms of identification and classification . in particular , it outperforms all other methods except trigger , which is used in cross - event contexts . we observe that the method is more accurate than the other methods .
results are presented in table 1 . all models are comparable in performance , with the exception of spanish - only - lm , which has the best performance on dev perp , test acc and wer .
results on the test set are shown in table 4 . fine - tuned training with only subsets of the code - switched data in the dev set and on the train test set . with the exception of train test , fine - tuned training has the highest performance , with a drop of 0 . 5 % in train dev .
5 shows the performance on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( monolingual ) . the results are summarized in table 5 .
results are shown in table 7 . for the conll - 2003 dataset , type - aggregated gaze features are trained on all three eye - tracking datasets and tested on the same benchmark . the precision ( p ) , recall ( f1 ) and f1 - score ( f ) are reported in tables 7 and 7 .
5 shows the precision ( p ) , recall ( f1 ) and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . syntactic - sg embeddings are used in wordnet , verbnet , and glove - extended , respectively . they are used for embedding wordnet and wordnet with syntactic skipgram embedding . the results on the original paper are summarized in tables 1 and 2 .
results are presented in table 2 . the rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . in particular , it has the highest ppa acc . and ppa ppa of all the models .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model . table 3 shows the ppa accomplishment ( ppa ) of the model with respect to attention .
results are presented in table 2 . subsfull and domain tuning outperform domain tuning for image caption translation ( bleu % scores ) . subdomain tuning outperforms domain - tuned , but does not improve the performance .
results are presented in table 1 . subdomain - tuned models outperform subs1m models in both en - de and out - of - fr . the results are shown in tables 1 and 2 . in both cases , the models are significantly better than subs1ms models in terms of performance . as expected , the results are significantly worse than those obtained in the original models .
4 shows bleu scores in terms of automatic captions ( only the best ones or all 5 ) . the results with marian amun are shown in table 4 . the best ones with the best image captions are obtained with the exception of multi30k .
5 compares the performance of the two strategies for integrating visual information with enc - gate and dec - gate . the results are summarized in table 5 . in terms of bleu % scores , we use transformer , multi30k + ms - coco + subs3mlm , and detectron mask surface . we also compare the results of the three strategies for embedding visual information . for example , in the en - de setting , we used transformer and multiterramlm .
results are presented in table 1 . sub - text - only models outperform subs3m models in terms of performance , with the exception of mscoco17 , which outperforms all models except for the ones with multi - lingual features . the performance of the models with the best performance on the en - de model is comparable to those with the most consistent performance .
results are presented in table 1 . the results are shown in tables 1 and 2 . in table 1 , we compare the performance of our model with those of other models . we also compare the results of our models with the best performing ones . our model outperforms other models in terms of performance .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 shows the results of training vocabularies for the english , french and spanish data for our models .
5 shows the bleu and ter scores for the rev systems compared to the original rev system . in particular , ter scores are significantly higher than en - fr - rev and en - es - rnn - rev .
results on flickr8k are presented in table 2 . the vgs model outperforms all the other models in terms of recall @ 10 , and achieves the best performance .
results on synthetically spoken coco are presented in table 1 . our model outperforms all previous models except audio2vec - u , which achieves the best performance .
we report further examples in table 1 . for example , cnn turns on a on ( in the the the edges of the screenplay ) and on a ( in the margins of the script ) when the edges are in the corners of the screen . this is so clever that it makes it easier to hate it . for rnn , we show the results of rnn and rnn .
results in table 2 show that fine - tuning has not changed the number of words in sst - 2 . the results are shown in the table 2 . for example , the rnp score is 69 . 0 % compared to 69 . 5 % in the original sentence .
results are shown in table 3 . the results indicate that the positive and negative labels are flipped to positive , and vice versa . in the case of cnn , the change in sentiment is significant .
results are presented in table 1 . the results are summarized in terms of ppmi ( p < 0 . 001 ) and pimi ( ppmi ) ( p ≤ 0 . 01 ) . in addition , the results are reported in table 2 . our model outperforms all the other models except for sst - 2 . we also observe that the performance of our model is comparable to those of other models .
