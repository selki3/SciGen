2 : throughput for processing the treelstm model on our recursive framework , and tensorflow ' s iterative approach , with the large movie review dataset . as table 2 shows , the recursive approach performs the best on inference with efficient parallel execution of the tree nodes , while the iteration approach shows better performance on training . further , the use of gpu exploitation improves the performance on both inference and training datasets .
shown in table 1 show that the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , when the batch size increases from 1 to 25 , it exhibits the same performance improvement as the linear dataset .
performance for each model with different representation is shown in table 2 . the max pooling strategy consistently performs better in all model variations . it also improves the model learning rate and the model dropout prob . as shown in fig . 2 , using different representation weights helps the model to achieve the best performance . finally , using sigmoid embeddings boosts the model performance .
1 shows the effect of using the shortest dependency path on each relation type . our model obtains the best f1 ( in 5 - fold ) with sdp and the macro - averaged model improves the f1 by 21 . 23 points ( in fig . 1 ) .
results in table 3 show that y - 3 significantly outperforms the other models in terms of f1 and f1 score .
3 presents the results of our model on the test set . our model achieves the best results on all test sets with a minimum of 50 % chance to achieve the highest score . the results are presented in table 3 . all the results obtained by the model are reported in tables 3 and 4 .
4 : c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 .
3 shows the performance of our system on the original test set . the results are shown in table 3 . original and original systems perform better than the others on all test sets except for the one that is used in table 4 . the performance of original and original is significantly worse than those of the other two systems . this is mostly due to the fact that the training set contains only one error which can be corrected with a single error .
shown in table 1 , the original e2e data and our cleaned version are comparable ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) .
performance on original and original test is reported in table 1 . original results are shown in bold . original scores are presented in tables 1 and 2 . these results are statistically significant even under the difficult requirement of a uniform number of parameters . they also include the bleu score of the original and the number of correct answers for each correct answer . they include the scores of correct answer percentage and wrong answer percentage . original test results are reported in tables 2 and 3 .
results of manual error analysis of tgen on a sample of 100 instances from the original test set are shown in table 4 . adding , mispelling , and slight disfluencies are all absolute numbers of errors we found ( i . e . , adding , misspelling , wrong values ) . cleaned , misclassified , and whitelists are small but significant ( e . g . , missing , misaligned ) . we found that the original training set had a large number of errors , which we found to be caused by misaligned values .
3 shows the performance of our models on the external and external datasets compared to the previous state - of - the - art models . our model outperforms all the other models except for seq2seqk ( konstas et al . , 2016 ) and tree2str ( song et al . 2017 ) . note that our model performs better on the single dataset compared to other models that rely on multiple factors .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points and achieves 27 . 5 overall e score . the results are shown in tables 2 and 3 .
3 shows the results for english - german and english - czech . our model outperforms all the other models except for the one that we included in table 3 . the results are shown in tables 1 and 2 . we observe that the single model performs better in english - language than in german , and that the difference in performance between the two is minimal . our model obtains the best performance in both languages .
5 shows the effect of the number of layers inside dc on the performance of the model when we add them all together . as table 5 shows , for every layer of dc , we add one layer of layers that contribute to the performance .
6 compares gcn with baselines . + rc denotes gcns with residual connections . as shown in table 6 , when gcn has residual connections , the gcn performs better than all baselines except dcgcn .
model f1 shows that dcgcn model outperforms all the other models in terms of performance on all metrics except b .
8 shows the ablation study for amr15 . the results show that removing the dense connections in the i - th block improves the performance for the model .
shown in table 9 , the models used in the graph encoder and the lstm decoder have achieved ablation study results for both datasets . as table 9 shows , the global model achieves the best results with a gap of 3 . 5 points in the performance of the single layer encoder compared to the previous state of the art .
results for initialization strategies on probing tasks are shown in table 7 . our paper shows that our approach obtains the best performance on all probing tasks . the results are reported in tables 7 and 8 .
3 shows the performance of our method in terms of depth and subtraction . our method outperforms all the other methods except for cbow / 400 , which shows the diminishing returns from adding features to the subtraction function . moreover , our method obtains a superior performance on every metric .
results are shown in table 3 . our model outperforms all the other models except for the one that cmp uses . cbow shows significant performance improvement over both mpqa and sick - r models . it also outperforms both the sst2 and sst5 models in terms of mrpc score . on the other hand , it does not achieve the best performance on all three models .
results on unsupervised downstream tasks attained by our models are shown in table 3 . our model outperforms both hybrid and cmp on all downstream tasks , except for the ones attained by cmp .
8 shows the performance of initialization strategies on supervised downstream tasks . our paper shows that our approach improves the performance by 3 . 8 points over the previous state of the art model on all three tasks .
6 shows the performance for different training objectives on the unsupervised downstream tasks . our model outperforms all the models except cmow - c on all the tasks except for the ones that are supervised .
3 shows the performance of our method on the subtense and threshold metrics . our method outperforms all the other methods except for cbow - c , which shows the diminishing returns from mixing subtasks . the results are summarized in table 3 .
3 presents the results of our method on the subj and mpqa datasets . our model outperforms all the other methods except for the ones that do not use cbow - r .
3 shows the e + and per scores of system models trained on the italic and supervised learning systems . our model outperforms all the systems except for the one that relies on name matching . the results are summarized in table 3 . name matching is the most difficult part of the system to achieve , with a gap of 3 . 5 points in the org score . it is clear from table 3 that the combination of all the factors that contribute to the success of the model is crucial for the model to achieve its goal .
results on the test set under two settings are shown in table 2 . our system outperforms all the models except for the one that trained on the word2 dataset . this confirms our belief that automatic learning can improve the performance of the system when trained on both datasets .
6 : entailment ( ent ) and ref compared to gen ( g2s - gin ) are shown in table 6 . the model outperforms all the other models except for those that do not have ref . as table 6 shows , when ref and gen are combined , the model achieves the best performance .
results are shown in table 1 . the models trained on the ldc2015e86 outperform all the other models except for the ones trained on ldc2017t10 . note that the model obtains the best performance on all 10 test sets when trained on a single dataset .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . our model outperforms all the models except for the ones that are trained using pre - trained models .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model outperforms all the other models except for the one that has the smallest size .
results are shown in table 3 . we observe that for all models , the average number of frames taken to represent the sentence length is lower than that of g2s - gin , indicating that the model is more effective at predicting sentence length and sentence length .
shown in table 8 , the fraction of elements that are missing in the output that are present in the generated sentence ( g2s - gin ) , for the test set of ldc2017t10 . note that these tokens are used in the comparison of the reference sentences . they are used to compare the output of the models with the ones that are in the input .
4 shows the performance of our model when trained with different target languages on a smaller parallel corpus ( 200k sentences ) . our model outperforms all the other models using features extracted from the 4th nmt encoding layer .
2 shows the pos and sem tagging accuracy with baselines and an upper bound . accuracies are reported in table 2 . word2tag is the most frequently used classifier , followed by unsupemb embeddings . the accuracy is reported in tables 2 and 3 .
results are shown in table 1 . table 1 shows that our method outperforms all the other methods except for the one that we tested . our method obtains the best performance on both datasets .
5 shows the accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . note that our model obtains a 3 . 4x improvement over the previous state - of - the - art model .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ . in pan16 , the attacker scored 10 . 2 % higher on the training set compared to our baseline . similarly , the difference between our baseline score and our corresponding adversary ' s accuracy is 9 . 7 % higher .
results in table 1 show that training directly towards a single task can improve the performance for both groups . for pan16 , we trained directly towards the single task .
2 shows the performance of the classifiers when they are included in the conversation . the classifier is named pan16 and is responsible for the balanced and unbalanced data splits . it is clear from table 2 that the classifier can easily detect instances of classifiers that are not part of the conversation , such as gender , age , gender , classifier , and gender .
performance on different datasets with an adversarial training is shown in table 3 . the performance on the training datasets is the difference between the attacker score and the corresponding adversary ’ s accuracy . in pan16 , the performance is significantly worse than that of the corresponding trained classifier .
6 shows the accuracies of the protected attribute with different encoders . embedding with rnn embeddings can improve the performance for both models .
3 shows the performance of our model on the two datasets . our model outperforms all the models except for the ones that use finetune features . the results are summarized in table 3 . this model achieves the best performance on both datasets when trained on a single dataset . finally , we observe that our model performs better on two datasets : the original lstm model achieves a superior performance on the one dataset , while the second dataset performs slightly worse on the other datasets .
3 shows the performance of our model compared to other models trained on the same dataset . our model outperforms all the other models except for the one that relies on the lstm dataset . the results are summarized in table 3 .
results of experiment 1 are shown in table 1 . our model outperforms all the other models except for the one that appears in experiment 1 . our model achieves the best performance on both datasets . the results are summarized in tables 1 and 2 . table 1 shows the performance of our model when compared to other models in the same dataset . we observe that the average time taken to compute our model is closer to the original time of the model , which suggests that the model is more suitable for the task at hand .
3 shows the bleu score on the test set of wmt14 english - german translation task . our model obtains a 3 . 5 % improvement over the previous state - of - the - art model on all test sets except for the german ones . the model outperforms all the other models except for sru .
4 shows the match / f1 score of models trained on squad dataset . the results published by wang et al . ( 2017 ) show that the parameter number of base can be significantly increased with the increase in parameter number . as shown in table 4 , the model trained on the lstm dataset can significantly increase match / score with the expansion of base . similarly , the sru model can significantly improve match / f1 score with the addition of additional parameter numbers .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the reported result lample et al . ( 2016 ) . it also indicates the performance of the model in the ner tasks . as shown in fig . 6 , the use of the parameter number in the model improves the performance . it can be observed that the lrn model outperforms all the other models in terms of performance .
performance on snli task with base + ln setting and test perplexity on ptb task with base setting . table 7 shows the performance of our model with respect to snli and ptb tasks .
results are shown in table 1 . word embeddings are used to improve the performance of system retrieval and system re - evaluation . sent attention is beneficial for both human and machine learning tasks , as it improves the results for both systems when using word2 and word3 . word3 improves the performance for both tasks as well as for all systems .
4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all systems is highlighted in bold , with statistical significance marked with ∗ . the highest standard deviation among all evaluations is 1 . 2 .
results are shown in table 3 . our model outperforms all the other models except for the ones that do not use the word " tables " . our model obtains the best performance on all the test sets except for those that use docsub . the results are reported in tables 1 and 2 .
results are shown in table 3 . the results are presented in tables 1 and 2 . we observe that all the models trained on the corpus dataset outperform the baseline on all metrics except for the ones that are used on the docsub dataset . these models use the best performance on all datasets except the one that is used on corpus .
results are shown in table 3 . our model outperforms all the other models except for the ones that do not use the word " tables " . our model obtains the best performance on all the test sets . it outperforms both the df and docsub datasets by a significant margin . on the df dataset , we observe that all the models using the word ' tables ' perform better than the others .
embeddings are shown in table 3 . our system achieves the best performance on all metrics with a gap of 1 . 78 points between the official score of eurparl and europarl , while achieving the best score on every metric . our model achieves the highest score on both metrics with an absolute improvement of 3 . 86 points .
embeddings are shown in table 1 . our system achieves the best performance with a minimum of 3 . 5roots on every metric compared to the maxdepth of europarl . our model outperforms all the baselines except for the ones that embed maxdepth . this is reflected in the numberroots scores of our model , which are derived from the same source ( table 1 ) .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . our enhanced model outperforms the original visdial model by a significant margin . we observe that when using the enhanced l2 model , the r0 , r2 , r3 and r3 metrics are comparable .
performance ( ndcg % ) of ablative studies on different models on the validation set of visdial v1 . 0 is shown in table 2 . using p2 indicates the most effective one ( i . e . , hidden dictionary learning ) compared to using p1 alone . note that only applying p2 with the history shortcut can improve the ngncg performance .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . we observe that the hmd - prec model outperforms all the other models except for the one that uses bert .
3 presents the performance of our model on the test set of ruse . our model outperforms all the baseline models except for those that do not use bertscore - f1 . the results are summarized in table 3 .
3 presents the performance of our models on the setting of table 1 . our model outperforms all the baseline baselines except for bleu - 1 , which obtains the best performance on both sets .
performance of the models on the setting is reported in table 3 . the results are reported in tables 1 and 2 . the summaries are summarized in terms of leic scores , which are derived from the leic score of the benchmark word - mover . they are presented in bold , with the exception of the summaries of those using elmo and p < 0 . 001 for both sets .
results are shown in table 3 . we observe that for all models except sim , our model performs better than the best on all three metrics .
3 presents the results of our model on the transfer quality and semantic preservation tasks . our model outperforms all the other models on both tasks with a large margin . the results are summarized in table 3 . we observe that the semantic preservation task is more complicated than the syntactic preservation task , and that it requires a lot of data to achieve the best results .
5 shows the results of human validation of these metrics . the results are shown in table 5 . it is clear from the table that human evaluations have a significant impact on the performance of the model , as measured by the percentage of errors generated by the model that match human evaluations .
results are shown in table 3 . we observe that for all models , para + para + lang representation improves performance . this is reflected in the performance of the models when trained on simuli - simuli .
6 shows the results on yelp sentiment transfer , where bleu is between 1000 and 1000 sentences and human references achieve the highest acc ∗ score . our best models ( right table ) achieve similar results to those using simple - transfer , but use different classifiers in use . we also observe that the use of multi - decoder reduces the performance of the model when using multiple classifiers . as table 6 shows , using only one classifier in the transfer setup can improve performance .
statistics for nested disfluencies are shown in table 2 . the percentage of tokens that were correctly predicted as disfluent is reported in tables 2 and 2 .
3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - disfluent ) . the fraction of tokens predicted as errors is shown in parentheses . as shown in table 3 , the percentage of errors predicted as correct is lower for both the content - and the function - function .
results are shown in table 3 . we observe that when text + innovations are used as rewards , the model outperforms single model in terms of dev and rewards . as these results show , the combination of text and innovations improves the model ' s performance when using the best - performing model .
2 compares our model with the state - of - art algorithms on the fnc - 1 test dataset . our model achieves the best performance with the help of word2vec embeddings . it also achieves the highest accuracy with a boost of 3 . 36 points in micro f1 score .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . compared to previous methods , burstysimdater significantly outperforms all previous methods .
3 shows the accuracy ( % ) of our method with and without attention . this results show the effectiveness of both word attention and graph attention for this task . compared to ac - gcn , neuraldater obtains a higher f1 score .
3 shows the performance of models trained on the same training set . our model outperforms all the models except for the ones trained on jvmee . the results are reported in tables 1 and 2 .
3 shows the performance of our method on event identification . our method outperforms all the other methods in terms of both event identification and event classification . for event identification , we use cross - event analysis ( cao et al . , 2018 ) . the method obtains an absolute advantage over all the methods except for the one that obtains the most accurate identification . in both cases , the identification results are reported in table 3 .
results are shown in table 3 . all models trained on english - only - lm outperform all the other models except for those trained on spanish - only . the results are summarized in tables 1 and 2 .
results on the dev set and on the test set are shown in table 4 . fine - tuned training with only subsets of the code - switched data in it achieves the best results .
5 shows the performance on the dev set and on the test set , according to the type of the gold sentence in the set . we observe that fine - tuning gives the best performance , but monolingual does not improve .
7 shows the precision ( p ) , recall ( f1 ) and f1 scores for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . note the significant improvement ( p ≤ 0 . 05 ) in precision ( p > 0 . 01 ) over the previous state of the art model , as shown in fig . 7 .
5 shows precision ( p ) , recall ( f1 ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . note the significant improvement in precision ( p ≤ 0 . 01 ) over the baseline , as these features are used to train the type inference models .
results on belinkov2014exploring ’ s ppa test set . our system uses syntactic - sg embeddings obtained by running autoextend rothe and schütze ( 2015 ) on glove . we also use syntactic skipgram to embed the semantic tags . the results on the original paper are summarized in table 1 . we use glove - retro as the base for wordnet 3 . 1 and wordnet 4 . 1 . we use it as a complement to wordnet pretrained by faruqui et al . , 2015 .
performance of our system with features coming from various pp attachment predictors and oracle attachments is shown in table 2 . the results from our system are shown in tables 2 .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing these two factors is shown in table 3 .
2 shows the performance of our model when combined with domain - tuned captions . our model outperforms the en - de model in terms of bleu % scores and multi30k scores . adding subtitle data and domain tuning for image caption translation ( bleu % ) improves the results for all models .
results are shown in table 4 . subdomain - tuned subs1m outperforms all models except for the ones that do not use the word embeddings . in the en - de setting , the models trained on mscoco17 outperform all the models except those trained on the single - domain basis .
4 shows bleu scores in terms of automatic captions ( only the best ones or all 5 ) . as shown in table 4 , adding multi30k captions shows that adding automatic image captions improves bleus scores in % . the results with marian amun confirm our hypothesis that adding multiple captions to a single image is beneficial for improving image quality .
5 compares the performance of our strategies for integrating visual information with en - de and dec - gate . our model outperforms all the other approaches except for the one using multi30k + ms - coco + subs3mlm embeddings . in addition , we observe that enc - gate also improves the bleu % scores for both visual information ( i . e . , when enc - gates is used with a mask , not a window ) .
3 shows the performance of subs3m and subs6m on the en - de dataset . the results are summarized in table 3 . sub - categories without the visual features are presented in bold . as expected , the performance is slightly worse when using multi - lingual features , as shown in fig . 3 , the combination of visual features and the text - only features results in a better performance . finally , we observe that combining the visual and acoustic features improves the performance .
3 shows the performance of the models trained on the word en - fr - ff . the results are summarized in table 3 . the first group shows the results of the best performing model on the test set . the second group shows that the best performance is achieved on the marginal improvement set on mtld .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 : training vocabularies for the english , french and spanish data used for our models . the results are shown in tables 2 and 3 .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) for the systems are shown in table 5 . the system reference obtains ter scores comparable to those of en - fr and en - es - rnn .
results on flickr8k are shown in table 2 . the model trained on chrupala2017representations achieves the highest performance with a 0 . 2 recall score .
results on synthetically spoken coco are shown in table 1 . the model trained on chrupala2017representations achieves the best performance with a 3 . 9 % recall rate compared to the baseline .
1 shows the results of the different classifiers compared to the original on sst - 2 . for example , orig < cao et al . ( 2017 ) turns in a < u > screenplay that is very clever at the edges ; it ’ s so clever you want to hate it . also , for cnn ( 2017 ) , she turns on a on ( in the margins of the screenplay ) . again , this shows the effectiveness of these classifiers . we report further examples in table 1 . for dan , we show that the edges are very clever and the curves are clever . similarly , for rnn , we see that when a paragraph is in the edges of a screenplay , the paragraph is more clever than in the margins .
2 : part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . these numbers indicate that there is no need to worry about the accuracy of the word choice .
3 shows the change in sentiment with respect to the original sentence in sst - 2 . note that negative labels are flipped to positive when the sentence is flipped to negative . this shows that the effect of the flipped sentiment on sentiment is very small .
results are presented in tables 1 and 2 . the results are summarized in table 1 . attractiveness is the most distinctive feature of our model , followed by positive and negative aspects . as these results show , the performance of our approach is comparable to those of other research methods ( table 1 ) . the most striking thing about our model is that it is able to distinguish between negative and positive aspects of the model without asking questions .
