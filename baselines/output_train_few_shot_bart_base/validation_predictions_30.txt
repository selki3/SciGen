2 shows the performance of the treelstm model on the recursive framework and the iterative approach , with the large movie review dataset as our training dataset . as table 2 shows , the recursive approach performs the best on inference with efficient parallel execution of the tree nodes .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because the batch size increases to 25 .
2 shows the performance of the different parameter optimization strategies for each model with different representation . we report the average number of hyper parameters in the validation set and the number of feature maps in the output set . the max pooling strategy consistently performs better in all model variations . the hyper parameters activation func . and l2 reg . achieve the best performance with different number of parameters .
1 shows the effect of using the shortest dependency path on each relation type . as table 1 shows , macro - averaged models achieve the best f1 ( in 5 - fold ) with sdp , and the macro - adapted models do not have to worry about different dependency paths .
3 shows the performance of the three models compared to the previous stateof - the - art models . in general terms , the results are slightly better than those of y - 3 , but still superior .
3 shows the paragraph level and the average number of questions for each submission . we report both the official score ( from each submission ) and the result of re - scoring the entries for the final submission . the results are presented in tables 1 and 2 .
4 shows the c - f1 scores for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . paragraph , on the other hand , has higher performance .
3 shows the performance of all models for each domain . original , original , and symmetric delete ( svm ) are all better than the others . original is more than 50 % better than tgen , tgen + is 35 . 5 % better on average , meteor and rouge - l are only slightly better than original . the cleaned and corrected systems perform on par with the original , but on a larger scale .
1 compares the original and the cleaned versions of our e2e data . we report the number of distinct mrs , total number of textual references , and the average number of slot matching words as measured by our slot matching script , see section 3 . original and dev scores have been shown in table 1 , but we have not managed to obtain a comprehensive list of all the instances from which our original and dev scores were derived .
3 shows the performance of original and original test methods for each domain . original scores are presented in table 3 . the best performances are obtained by sc - lstm , rouge - l and cider . the original scores have been consistently better than both the original and the original scores . however , their performance is still significantly worse than those of tgen and tgen .
4 shows the absolute numbers of errors we found in the manual error analysis of tgen ( see table 4 ) . for the original dataset , we found 22 errors ( 17 . 6 % ) and 15 . 6 % of those were misclassified as missing .
model < cao et al . , 2016 ) and parallelstm achieve state - of - the - art results on all models with a gap of 2 . 5m in performance from the last published results ( table 1 ) .
2 presents the results on amr17 . our model achieves 24 . 5 bleu points and achieves 30 . 6 overall e ( which means that it has better performance than the ensemble model ) .
3 shows the results for english - german and english - czech . the results are shown in table 3 . the best performing single model is ggnn2seq , which takes the best performing english - language model . we notice that the single model performs slightly worse than the other two models in terms of performance in both languages .
5 shows the effect of the number of layers inside the dc stack on the performance of our model in table 5 . as table 5 shows , when we add layers of dc to the stack , we get a reduction of 3 . 5 % overall .
6 compares gcn with baselines with residual connections . rc + la ( 2 ) and dcgcn3 ( 27 ) show that the residual connections gcn has better performance than the baselines .
model f1 shows that dcgcn significantly outperforms other state - of - the - art models in terms of performance on all metrics when we include the number of models in our model , and the percentage of models that exceed this threshold .
8 shows the ablation study results for amr15 . - { i , 4 } dense blocks denotes removing the dense connections in the i - th block . however , this does not improve the performance for dcgcn4 .
9 shows the ablation study results for the graph encoder and the lstm decoder . encoder modules used in table 9 show that the global network and the multi - factor encoder have superior performance , but the gap is narrower than expected .
investigate the effects of different initialization strategies on probing tasks . we report in table 7 the results for each initialization strategy . our paper shows that our method obtains the best performance with a gap of 10 . 5 % in the precision score .
1 and table 2 summarize our results on the hidden test set of cbow / 400 . we observe that our method outperforms all the other methods except for the one that requires a greater attention span . subjnum and coordinv are the only ones that perform better on both hidden test sets . however , their performance is still significantly worse than those on the other two sets .
1 shows the performance of all models when trained with subj and mpqa . our model outperforms all the alternatives except cmp except for the one that has the worst performance . subj has the highest mrpc score and sick - e performs slightly better than sst2 and sst5 . however , it has the advantage of training on a larger corpus . we observe that both the original cbow / 784 and its variants have poor performance on mrpc . this suggests that more reliance on superficial cues may result in better performance .
3 shows the relative change with respect to hybrid over unsupervised downstream tasks attained by our models . we observe that cbow and cmp perform similarly to each other on almost all downstream tasks , except for the ones where they are used less frequently .
8 shows the performance for initialization strategies on supervised downstream tasks . our paper shows that our approach improves the performance by 3 . 8 points over the best state - of - the - art model on three of the four tasks .
6 shows the performance for different training objectives on the unsupervised tasks . the best performances are on the sts12 , sts15 and sts16 . the worst performance is on sts14 , where we get 21 . 2 % improvement .
observe that our method outperforms all the other methods except cbow - r models in terms of depth and event extraction .
subj and sick - r perform comparably to other methods when trained on the same domain . however , subj has the advantage of training on a larger corpus and is comparable with mpqa and sst2 . also , sick scores are significantly lower than those of other methods , i . e . , sst3 , sst4 and srs - b , respectively .
3 shows the e + and per scores of all systems trained on the same domain . our system outperforms all the systems except for the one that does not use org and misc . the results are summarized in table 3 . supervised learning models ( mil - nd and mil - nd ) achieve outstanding results when trained on all the data available from the single domain ( e . g . , name match , all per ) and the multi - domain learning model . finally , the performance of the supervised learning models that do not rely on org or misc is significantly better than those using plain averaged word embeddings . in fact , the only exception is when using only one domain , which underscores the competitiveness of supervised learning .
2 shows the results on the test set under two settings . our system achieves the best results with 95 % confidence intervals of f1 scores . supervised learning ( model 1 ) achieves the highest e + p score and the best f1 score of 82 . 57 % , both in ( supervised learning ) and ( adapted learning ) . the results are shown in table 2 .
6 shows the results of ref and ref for all models except for those that do not have ref ( table 6 ) . ref significantly outperforms ref in all but one of the cases where ref is used .
1 and table 2 summarize our results on the ldc2015e86 and ldc2017t10 datasets . the results are summarized in table 2 . our model outperforms all the other models except for the one that has the better eor score .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . our model outperforms all the models except for the one that is pre - trained with additional data .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . it can be observed that the bilstm significantly boosts the performance of the model .
results are shown in table 4 . after applying the weighted average of the sentence length and the average number of frames for each frame , we get a reduction of 2 . 5 % compared to g2s - gin .
shown in table 8 , the fraction of elements that are missing in the input graph that are present in the generated sentence ( g2s - gin ) , are used in the comparison to derive the summaries for the test set of ldc2017t10 . note that the slightly larger miss fraction refers to the reference sentences in the output , and not the output .
4 shows the performance of our approach with respect to target languages . we use the 4th nmt encoding layer , trained with 200k sentences . we observe that the accuracy obtained using pos features is significantly better than those using epm .
2 shows the pos and sem tagging accuracy with baselines and an upper bound . mft is the most frequent tag ; unsupemb is the second most frequently tag . word2tag is the third most frequently tagged word embeddings .
3 shows the performance of all the methods tested on the data . our proposed method outperforms all the other methods except for the one that we tested in table 1 .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages .
8 shows the performance of an attacker on different datasets . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in the mean of the weighted average number of tokens .
statistics are shown in table 1 . accuracy on the single task is significantly higher than pan16 , indicating that training directly towards a single task can improve the training performance .
2 shows the effect of the additional cost term on the balanced & unbalanced data splits . the classifiers trained on the pan16 dataset receive a significant drop in performance as a result of the increased cost term . however , the imbalance is still prevalent in the unbalanced dataset , sentiment and gender are the most prevalent classifiers for this data .
performance on different datasets with an adversarial training set is shown in table 3 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ . sentiment and language performance are similar across all datasets , with the exception of the one with gender - neutral features .
6 shows the performance of the protected attribute for different encoders . embedding guarded embeddings have higher performance than embedded ( 60 . 5 % ) and rnn ( 59 . 3 % ) .
3 shows the performance of our model compared to other models . our model outperforms all the other models with a large gap in performance between the max and max performance . the results are summarized in table 3 . we observe that the lstm model performs better on both the training and the finetune tasks . however , the best results are obtained on the wt2 dataset ( upadhyay et al . , 2017 ) which shows the diminishing returns from training on a single model .
3 shows the performance of our model compared to previous models on the acc andbert datasets . the results are summarized in table 3 . our model obtains the best performance on both datasets with a minimum of time to train .
3 shows the performance of our model compared to other models using the same time span . our model outperforms all the other models except for the one that is used in the amapolar time and full time settings .
3 shows the bleu score of our model on the newstest2014 dataset in terms of translation time . as seen in fig . 3 , our model obtains a significant improvement in performance over the state - of - the - art model on wmt14 german translation task .
4 shows the performance of our model on squad dataset . the results published by wang et al . ( 2017 ) show that our # params model significantly outperforms other models in terms of match / f1 score . however , our model obtains a lower f1 score than other models because its parameter number is smaller .
6 shows the f1 score on conll - 2003 english ner task . the leading models lstm and lrn achieve higher performance than the reported result ( lample et al . , 2016 ) .
performance of elrn on snli task with base + ln setting and test perplexity on ptb task with base setting .
3 shows the performance of the word models for each domain . the word models trained on the oracle retrieval dataset are presented in table 4 . all the systems trained on this dataset are state - of - the - art . in general terms , all the systems in the system are better than the previous state of the art systems . retrieeval is beneficial for both systems ,
4 presents the human evaluation results on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( k 2000 ) . the best performing system is seq2seq , which achieves an overall score of 28 . 2 / 71 . 5 on the k 100 and k 1000 datasets , respectively , compared to 25 . 0 / 29 . 2 for the other systems .
3 shows the performance of all the models trained on the corpus dataset . our proposed system outperforms all the other models except for the one that we use , namely , europarl , ted talks and docsub . the results are broken down in table 3 . for corpus , we observe that the performance obtained by our system is significantly better than those by other models . for docsub , we see that our system performs on par with the best performing corpus model .
3 shows the performance of all the models trained on the corpus dataset compared to the previous state - of - the - art models . the results are summarized in table 3 . our proposed model outperforms all the other models except for the one that we chose , namely , docsub . docsub has the worst performance , while europarl is close on the other two .
3 shows the performance of all the models trained on the corpus dataset . our proposed system outperforms all the other models except for the one that we use , namely , europarl , ted talks and docsub . the results are broken down in table 3 . for corpus , we observe that the performance on the three models is significantly better than those on the other two .
3 compares our maxdepth and averagedepth scores on corpus and europarl . our joint model achieves the best performance with a gap of 1 . 78 points on each metric compared to the previous best state - of - the - art model . we also observe that our joint model exceeds the baseline on all metrics except our metric , namely , docsub and hclust .
3 compares our maxdepth and averagedepth measures on corpus and europarl . our joint model achieves the best performance with a gap of 1 . 5 % on each metric compared to the previous best state - of - the - art model . we also observe that our joint model exceeds the baseline on all metrics except our metric , namely , the depthcohesion metric , which measures the quality of the extracted word embeddings with a minimum of 10 % .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . we note that lf is the enhanced version of the original visdial model , and r2 is the more sophisticated one .
performance ( ndcg % ) of different models on different validation set is shown in table 2 . using only p2 indicates the most effective one ( i . e . , hidden dictionary learning ) compared to using p2 .
5 compares the performance of our models on hard and soft alignments . we observe that our hmd - recall + bert model outperforms all the other models except for the one that do not use bert .
3 provides exact scores for each setting . our proposed approach outperforms all the baseline metrics with a gap of 3 . 5 points from the last published results ( ruse - f1 ) .
3 presents the performance of our baseline models on the sfhotel and smd datasets . the results are summarized in table 3 . we observe that the baseline scores significantly outperform the baseline on all three metrics , with the exception of bleu - 2 .
performance of the models according to these baselines is reported in table vi . the results are summarized in table vii . the summaries displayed in the tables are broken down in terms of eor and spice scores . eor significantly outperforms the leic scores of all the other baselines ,
3 shows the performance of different models compared to the original ones . for example , we see that m0 has the best performance on para with the shen - 1 baseline while m6 has the worst performance on the pp .
3 shows the performance of all the models on the transfer quality and transfer quality datasets . the results are summarized in table 3 . semantic preservation and semantic preservation are the most difficult tasks to solve . however , the best results are obtained on yelp , indicating that the semantic preservation approach is superior to the sentiment - free one on all three datasets .
5 shows the human evaluation results on the acc and pp metrics for each dataset . we report in table 5 the results of human evaluation using the [ italic ] ρ b / w negative pp and human ratings of fluency . these results show that the accuracy obtained by using these metrics can be significantly higher than the machine evaluations .
3 shows the performance of different models compared to the original ones . in general terms , we see that all the models perform better when they have more data in their para + lang representation .
6 shows the results on yelp sentiment transfer , where bleu is between 1000 and 1000 sentences and human references are restricted to 1000 words . our best models achieve higher bleus than those using simple - transfer or n - word embeddings . however , their acc ∗ score is considerably lower than any other model using the same classifier .
statistics for nested disfluencies are shown in table 2 . reparandum length is the average number of tokens predicted to be in the correct disfluency range .
3 shows the relative frequency of rephrases correctly predicted as disfluent for both the content - content and the reparandum . the fraction of tokens that contain a content word is shown in parentheses , indicating that the disfluencies in both the reparandum and the repair are significant .
1 shows the performance of all models when using text + innovations . the results are presented in table 2 . text + innovations significantly improve the model performance over text + raw over the single model , and the best model overall has 86 . 57 % improvement on average compared to the previous state of the art .
performance comparison with the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model achieves the best performance with 94 . 53 % accuracy and 82 . 43 % self - attention .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the summaries generated by burstysimdater significantly outperform all previous models .
3 shows the performance of our method with and without attention . it achieves the best performance with 61 . 8 % and 65 . 6 % on average , respectively , compared to ac - gcn ( 60 . 2 % and 63 . 2 % ) .
model performance in table 1 shows that all models perform similarly to the best state - of - the - art models on all stages except trigger .
3 shows the method ' s performance on each event . all the methods used for this analysis seem to handle a significant amount of data . in general terms , all the methods show a significant drop in performance compared to the previous state of the art .
can be seen in table 4 , all the fine - tuned models perform better than the original ones when trained on the single - domain model .
results on the dev set and on the test set are shown in table 4 . fine - tuned train dev outperforms fine - tuned train dev with only subsets of code - switched data .
5 shows the performance of our model on the dev set and the test set . it can be seen that fine - tuned models generally outperform monolingual models in terms of performance .
performance of type - aggregated gaze features for the three eye - tracking datasets is shown in table 7 . the improvement in precision ( p ) , recall ( f1 ) and f1 - score ( tables v - viii ) is statistically significant , with the exception of the conll - 2003 dataset , where the effect of the additional gaze features is less pronounced .
5 shows precision ( p ) , recall ( f1 ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . the improvement in precision ( p ≤ 0 . 01 ) is statistically significant , with a drop of 0 . 03 point from the previous performance .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . the embeddings obtained by applying autoextend rothe and schütze ( 2015 ) on glove are used in wordnet 3 . 1 . however , they do not need to be used as part of the hpcd ( full ) system , since it uses syntactic - sg embedding instead . further , we note that the semantic embedding obtained by using skipgram gives a performance gain of 2 . 8 % over the original embeddings .
results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . as table 2 shows , using oracle pp as the dependency parser results in consistently better performance than the original rbg model . however , the difference is less pronounced for oracle pp ,
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . it shows the significant drop in ppa acc . from 91 . 7 % to 89 . 7 % .
2 shows the results with domain - tuned and multi30k captions . our model outperforms all the en - de models except for the one that has domain tuning .
3 shows the performance of the subs1m models when domain - tuned and en - de trained . the results are summarized in table 3 . sub - categories shown in bold have low performance on all metrics , with a slight improvement on the performance for flickr16 . however , the improvements are more pronounced for mscoco17 , whose training data are in the single - domain setting .
4 shows the bleu scores in terms of the models using automatic captions . as can be seen , when only using one or all 5 models , the model performs significantly worse than the model using multi30k .
5 compares our approach with prior works on en - de and captions . we observe that the best performing model is the multi30k + subs3mlm model , which allows the embedding of visual information . further , we observe that enc - gate and dec - gate have a significant impact on the visual information score , which shows the diminishing returns from mixing visual information with enc - gates .
1 shows the performance of subs3m with different visual features compared to subs6m on en - de . sub - categories as adjectives antonyms and performer action have higher performance on the largerickr16 and flickr17 datasets , respectively . as seen in the second group of comparisons , the combination of visual features and the text - only features boosts performance considerably . however , for the larger flickr16 dataset , the performance remains the same .
3 shows the performance of all the models compared to the original ones . we observe that for all but one of these models , the performance is significantly worse than those by en - fr - rnn - ff .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 shows the vocabularies for the english , french and spanish data used for our models . our system outperforms all the other models with a large gap in performance .
system reference bleu and ter scores for the rev systems are shown in table 5 . automatic evaluation scores ( bleu ) and ter metrics show severe overfitting since these systems rely on word embeddings .
2 shows the vgs performance on flickr8k . the mean rank of our model is 711 , which means that it has a relatively high recall rate .
results on synthetically spoken coco are shown in table 1 . the visually supervised model outperforms the similarly supervised audio2vec - u model in terms of recall @ 10 and mean rank .
1 shows the results for each classifier compared to the original on sst - 2 . for example , orig ( which is used for embeddings ) turns in a < u > screenplay that has edges edges edges and curves and is so clever you want to hate it . also , for cnn , we see that the edges edges of a screenplay are very clever ( hence , we report further examples in table 1 ) .
2 shows the part - of - speech ( pos ) changes in sst - 2 . we see that the number of occurrences have increased , decreased or stayed the same through fine - tuning . these numbers indicate that the amount of words in the sentence has not increased significantly , indicating that the language has not changed .
3 shows the changes in sentiment as the number of negative labels is flipped from positive to negative . these numbers indicate that the sentiment increases with the growth of positive sentiment .
table 2 , we report the performance of our method with respect to word embeddings andpubmed . results are summarized in table 2 . while the positive performance is encouraging , the negative performance is less pronounced for sst - 2 . our joint model outperforms other methods in terms of ppmi score .
