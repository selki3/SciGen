2 shows the performance of our recursive approach on the large movie review dataset compared to the iterative approach . the results are shown in table 2 . the recursive approach performs the best on training , while the recursive approach shows better performance on inference .
1 shows the performance of the balanced and balanced datasets compared to the linear ones . when the batch size increases from 1 to 25 , the balanced dataset exhibits the highest throughput , but at the same time exhibits the smallest performance improvement .
2 presents the results for each model with different representation . we show that the max pooling strategy consistently performs better in all model variations . it is clear from table 2 that the multi - dimensional representation of the hyper parameters is important for the model to achieve the best performance . as expected , the hgn outperforms all the other baselines except sigmoid and ud v1 . 3 .
1 shows the effect of using the shortest dependency path on each relation type . our system achieves the best f1 ( in 5 - fold ) with sdp and macro - averaged features . the results are shown in table 1 . it is clear from the results that our system can improve the f1 without sdp .
results are presented in table 3 . we observe that y - 3 significantly outperforms y - 2 in terms of f1 and f1 score .
results are presented in table 1 . the best results obtained by mst - parser are shown in the table 1 . the best performance obtained by [ empty ] is achieved on paragraph level f1 . in addition , the best performance achieved by [ emst - parser ] is obtained on the essay level , which is comparable to that achieved by the f1 baseline .
4 shows the c - f1 scores for the two indicated systems ; for the lstm - parser , it is 60 . 62 ± 3 . 54 and 69 . 24 ± 2 . 87 respectively .
3 shows the performance of the original and the second set of test sets . the results are presented in table 3 . the original and second set are shown in bold . the two sets of test set have completely different performance on each of the three sets . the original is better than the original , but it is closer to the original . the second set , sc - lstm , is slightly worse than the first set . it is clear from the results that the original has a lot to do with the fact that it has been trained on the wrong dataset .
1 compares our original and our cleaned versions . the results are shown in table 1 . the original and the cleaned versions have the highest number of distinct mrs and the average number of instances of ser as measured by our slot matching script , see section 3 . the cleaned version has the highest mrs , but it has the smallest ser . it is clear from table 1 that the original and original e2e data are not comparable in quality .
results are presented in table 1 . original and original models are shown in bold . original models outperform tgen and tgen + in all but one of the cases where the accuracy is less than expected . original model outperforms both the original and the original model . the results are summarized in tables 1 and 2 . for table 3 , we compare the performance of the original and the original models on the test set . the performance of original models is reported in table 3 . the difference between original and original models is minimal , however it does appear to be due to the nature of the training set .
4 shows the results of manual error analysis on a sample of 100 instances from the original test set . we found that removing the wrong values from the training set caused a significant drop in performance . we also found that adding the missing values caused a slight disflip .
3 shows the performance of all models compared to the previous state - of - the - art models . all models perform better than all the other models except for seq2seqk ( song et al . , 2016 ) and pbmt ( konstas et al . ( 2016 ) on the external and external datasets , while snrg performs substantially better on the single and multi - task datasets .
2 presents the results on amr17 . our model achieves a bleu score of 27 . 5 , which indicates that it is better to rely on ensemble models than single models . the results are summarized in table 2 .
results are shown in table 1 . the english - language model outperforms the german - based bow + gcn and the english - czech model by a significant margin . we observe that the single model performs better than the other two models in english - and german - language , the results are presented in table 2 . it is clear from the results that the multi - language approach is beneficial for english - speaking learner .
5 shows the effect of the number of layers inside dc on the performance of the layers . table 5 shows that for all the layers , there is a significant effect on performance . for example , for example , there are no perceptible difference in performance between the two layers , i . e . , there is no noticeable difference at all in the performance for both sets .
6 shows the performance of gcns with residual connections compared to baselines . with residual connections , gcn exhibits the best performance . the results are shown in table 6 . rc + la ( 2 ) and gcn + rc ( 6 ) show that the residual connections are beneficial for gcn .
3 shows the performance of our dcgcn model compared to the previous state - of - the - art models . the results are presented in table 3 . the results show that the approach achieves the best performance when the number of models is used in combination with a minimum of training data .
8 shows the ablation study on the dev set of amr15 . the results show that removing the dense connections in the i - th block leads to a better performance for the model .
9 presents the ablation study for the graph encoder and the lstm decoder . the results are presented in table 9 . the results show that the global network and the multi - decoder have the most impact on the performance .
7 shows the performance of our initialization strategies on probing tasks . our paper shows that our approach outperforms the state - of - the - art method on all three tasks . the results show that our method significantly improves the performance on the two tasks .
3 presents the results of our method . the results are presented in table 3 . the results show that our method outperforms all the baselines except for the one that has the best performing feature set , cbow / 400 . as expected , the results are slightly less striking than those of the other baselines , but still comparable to the performance of the two .
subj and sick - r show the results for all models tested . the results are presented in table 1 . subj outperforms all the other models except for the one that is used in subj . subj has the best mrpc score and is comparable to both sick and mpqa in terms of mrpc performance . it is clear that subj is superior to both the original and the original mrpc scores . this suggests that the combination of the two methods is beneficial for both mrpc and subj performance .
3 shows the relative change from cmp to cmp on unsupervised downstream tasks attained by our models . the results are shown in table 3 . our model outperforms both cmp and cmp in all downstream tasks .
8 shows the performance of our initialization strategies on supervised downstream tasks . our paper shows that our approach outperforms all the baselines except subj and mpqa except for sst2 , which shows that the initialization strategies are more suitable for the task . the results are presented in table 8 . our proposed method outperforms both subj ( which we tested in the lab ) and sick - r ,
6 shows the performance for different training objectives on unsupervised downstream tasks . the best performance is obtained on the sts12 and sts14 datasets , while the worst performance is achieved on sts15 . the results are reported in table 6 .
3 presents the results of our method . the results are presented in table 3 . the topconst method outperforms both cbow - r and somo - r in terms of depth . it obtains the best performance on all three metrics , with the exception of the subjnum . it achieves the best results on the sub - categories of subjnum and topconst . it also outperforms somo and wc - r by a margin of 3 . 5 points .
subj and sick - r show the performance of the models trained on subj . the results are presented in table 1 . subj outperforms both sst2 and sst5 in terms of mrpc performance . it is clear that subj has superior performance on both mrpc and mpqa datasets , and that it is better to train the models on both datasets than to train them on the other two methods . this confirms the effectiveness of the subj approach . the best performance is obtained on the mrpc dataset , which shows that the combination of the two approaches can improve the results for both datasets .
3 shows the e + and per scores for all systems tested . our system outperforms all the baselines except for the one in [ italic ] where it has the advantage of better org and misc scores . the results are summarized in table 3 . the system achieves the best e + org scores and the best per scores on all three metrics . it also outperforms the other baselines in terms of e + and per scores , which shows that the system is more suitable for the task at hand .
results on the test set under two settings are shown in table 2 . the system achieves the best performance with 95 % confidence intervals . it outperforms the previous state - of - the - art model , mil - nd , in terms of e + p and f1 scores . it achieves the highest f1 score with a 2 . 36 % boost over the previous model , which shows the effectiveness of the supervised learning approach . the results show that the combination of supervised learning and supervised learning improves the performance of the two models .
6 : entailment ( ent ) with ref and ref is presented in table 6 . our model outperforms all the other models except g2s - gat in terms of ref score . ref significantly outperforms ref on all three models except for those that do not use ref .
results are shown in table 1 . the best performing models are s2s , g2s - ggnn and g3s - gat . note that these models outperform the ldc2017t10 and ldc2015e86 in terms of bleu score .
3 shows the results on ldc2015e86 test set when trained with additional gigaword data . the results are shown in table 3 . our model outperforms the previous state - of - the - art model by a significant margin .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results show that bilstm significantly improves the performance of the model on the development set by 2 . 5x compared to the baseline .
results are shown in table 1 . the results are presented in bold . we observe that the g2s - gin model outperforms all the other models in terms of terms of sentence length and sentence length . it also shows that when the sentence length is lengthened , the graph diameter is shorter , indicating that the model is more suitable for the task .
shown in table 8 , the fraction of elements in the output that are missing in the input ( g2s - gin ) that are in the generated sentence ( miss ) . these tokens are used in the comparison to compare to the reference sentences in the ldc2017t10 test set .
4 shows the performance of our approach with respect to target languages . we use the 4th layer of the nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the results are shown in table 4 . our approach outperforms the previous state - of - the - art approach by 3 points .
2 shows the pos and sem accuracy with baselines and an upper bound . the results are shown in table 2 . word2tag has the best overall performance and is the most frequently used classifier . it also outperforms word3tag and word4tag .
results are presented in table 4 . we observe that the accuracy obtained by our method outperforms all the other methods except for the one that we use . the results show that our method significantly improves the performance of the two methods .
5 shows the accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 .
8 shows the performance of the trained attacker on different datasets . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 8 . the trained adversary is trained on a training set with 10 % held - out compared to the trained adversary .
1 shows the performance of our system when training directly towards a single task . the results are presented in table 1 . the results show that our system obtains the best performance when trained directly towards the single task , and is comparable to pan16 in terms of performance .
2 presents the results of the study on the balanced task and unbalanced task splits . the results are presented in table 2 . the classifier ( pan16 ) consistently leads the classifier with the highest percentage of balanced and balanced data splits . it is clear from table 2 that this classifier is able to detect instances of classifiers that are misclassified as having negative or positive messages .
3 shows the performance on different datasets with an adversarial training set . the performance on the training set is shown in table 3 . dial is the difference between the attacker score and the corresponding adversary ’ s accuracy . sentiment and gender are the most important factors in the performance of the trained classifier . gender and age are also important factors , as are age and gender . we observe that gender - neutral features are important in the selection of targets .
6 shows the performance of embedded and guarded encoders . the results are shown in table 6 . embedding is easier than embedding it with a different encoder .
results are presented in table 1 . the results show that the lstm model outperforms the previous state - of - the - art models in all aspects , with the exception of the finetune model . it also outperforms both the wt2 and wt2 models in terms of feature set . we observe that the enhanced feature set ( the " finetune " feature set ) significantly improves the performance of the original wt2 model over the strong lemma baseline . further improving the feature set further improves the results for the current model .
3 presents the results of our second study . the results are presented in table 3 . the results show that the lstm model significantly outperforms the previous state - of - the - art model in terms of both acc andbert time . it is clear from the results that this model is better at both the task and the time - to - model task . as expected , the performance of this model significantly improves over previous models .
3 shows the performance of our model compared to the previous state - of - the - art lstm model . our model outperforms all the other models except for the one that is used in the amapolar time dataset . the results are summarized in table 3 . we observe that our model significantly outperforms the alternatives in terms of both time and recall . table 3 shows that our approach obtains the best performance on both datasets . it obtains a better recall rate than the alternatives . this model obtains an absolute improvement over the average recall rate on both the ama and yelp datasets .
3 shows the bleu score on wmt14 english - german translation task . it can be seen that the tokenized approach has a significant impact on the performance of the system , as shown in table 3 .
4 shows the performance of our model on squad dataset . the results published by wang et al . ( 2017 ) show that our approach significantly improves match / f1 score over the baselines of lstm and sru . as expected , our model obtains a better match / f1 score than other baselines . we observe that the sru model outperforms all baselines except for the two baselines that it obtains ( adr and lrn ) .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number and sru denotes the reported result . it is clear from table 6 that the use of # params improves the performance of the system .
7 shows the performance of our model on snli task with base + ln setting and test perplexity on ptb task with base setting setting .
results are shown in table 1 . word embeddings are used to train the system for word - based evaluation . system retrieval is the most popular method , followed by system - based evaluations . the word embedding method ( mtr ) significantly improves the results for both systems , with the best performance for both for the system and for the task - based ones . using word - based features , the system is more suitable for word based evaluation , with a slight improvement for system evaluation .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the results are highlighted in bold , with statistical significance marked with ∗ ( p < 0 . 0005 ) . the best performing system is seq2seq , which is ranked in the top 1 or 2 for overall quality .
3 shows the performance of all the models tested on the test set . the results are presented in table 3 . the best performing models are europarl , df , docsub and docsub . the worst performing ones are eurparl ( p < 0 . 005 ) and term , while the best ones are docsub ( p > 0 . 05 ) . these models outperform all the other models except for the two that are used in this set .
3 shows the performance of all the models tested on the test set . the results are presented in table 3 . the best performing models are europarl , df , docsub and docsub . these models outperform all the other baselines except for the one that is used in the final set : docsub , docmax and docmax . the performance of these models is reported in table 4 . the most striking performance is that of the two that are tested on each of the three baselines . the two that feature the most significant performance drops are the ones used on docsub ( which is the only one that does not rely on the word " talks " . the other two are used in docmax , where the difference between the performance between the two is minimal .
3 shows the performance of all the models tested on the test set . the results are presented in table 3 . the best performing models are europarl , df , docsub and docsub . the worst performing ones are eurparl ( p < 0 . 005 ) and term ( p > 0 . 01 ) . the best performance is achieved by term , which is comparable to the performance achieved by the other two models . the performance of the three models is reported in table 4 . the most striking performance is obtained by the combination of the word " talks " and " tables " . the best performances are obtained by combining the word ' talks ' with the word “ tables . ” the results of term are shown in table 5 .
3 presents the results of our joint study . our joint study found that our approach significantly improves the joint study quality over the baselines of both corpus and europarl . the results are presented in table 3 . the results show that our joint model significantly outperforms both the existing baselines in terms of depth and dimensioncohesion .
3 presents the results of our joint study . our joint study found that our joint approach has the best overall performance on all three datasets . the results are presented in table 3 . the results show that the joint study outperforms all the other baselines except for the two that are used in the corpus dataset . we also observe that the results are slightly better than those of the other two baselines , eurparl and eurmax .
1 shows the performance ( ndcg % ) of our enhanced model on the validation set of visdial v1 . 0 . the enhanced model outperforms the original visdial model in terms of r0 , r2 , r3 and gsm .
2 shows the performance ( ndcg % ) of different ablative studies on different models on visdial v1 . 0 validation set . the results are shown in table 2 . p2 outperforms p1 and p2 by a margin of 3 . 5 % .
5 presents the results on hard and soft alignments . the results are presented in table 5 . we observe that the hmd - prec model outperforms the wmd - f1 model on hard alignments , but it is closer to the strong lemma baseline than the soft one .
3 presents the results of our approach . the results are presented in table 3 . our approach outperforms all the baselines except ruse ( * ) and sent - mover ( * ) by a margin of 0 . 7 points , which indicates that the approach is beneficial for both groups .
3 presents the bagel and sfhotel scores on the test set . the results are presented in table 3 . the baselines are significantly better than the baseline bleu - 1 and bertscore - f1 scores .
3 presents the performance of the models trained on the baselines . the results are summarized in table 3 . the summaries are presented in terms of leic scores and bert score - recall scores . they show that the combination of elmo and spice scores significantly improve the summaries for all models , with the exception of those using w2v .
results are shown in table 4 . the results are presented in table 5 . we observe that the m0 model outperforms the previous state - of - the - art on all metrics except for the shen - 1 metric , which shows that it is better to rely on plain word embeddings than on syntactic or semantic cues . as expected , the performance of m0 models is lower than those of m1 and m2 , but still comparable to m1 .
results are presented in table 4 . we show the results of our model on the semantic and semantic preservation datasets . the results show that the semantic preservation approach significantly improves the transfer quality over the syntactic preservation baseline . semantic preservation outperforms semantic preservation on both datasets , syntactic preservation is the most important part of semantic preservation , the semantic preservation baseline is significantly better than semantic preservation and δpp is the only one that relies on syntactic features .
5 presents the results of human and machine validation . the results are shown in table 5 . both sim and human ratings of semantic preservation are statistically significant ( p < 0 . 001 ) on the test set of yelp and sim . the results of the human evaluation show that the quality of the sentence is comparable to that of the machine and human evaluations ( p > 0 . 01 ) .
results are shown in table 4 . we observe that for all models , the word embeddings have a significant impact on performance . for example , for m1 , the presence of para + para + lang has a significant effect , but it is less pronounced for m6 . for m6 , the performance drop is minimal ( i . e . less than 0 . 7 % ) compared to m3 .
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu and the highest acc ∗ score . however , it is not clear that this is due to different classifiers in use . the results on simple - transfer are not included as they are worse than those on the same 1000 sentences . it is clear from table 6 that the use of the classifier in the sentiment transfer setup is a contributing factor to the performance of the model , and that it is difficult to distinguish between the two classes of sentiment transfer . this is evident from the fact that the two classifiers are used in the same sentence .
2 shows the performance of nested disfluencies . the number of tokens predicted to be disfluent is reported in table 2 . reparandum tokens are typically shorter than repetition tokens , and are more frequently used as repetition tokens .
3 shows the relative frequency of rephrases correctly predicted as disfluent for both the reparandum and the repair ( table 3 ) . the fraction of tokens predicted to contain a content word is shown in table 3 . it is clear from the table that the disfluencies predicted as the content word have a significant impact on the prediction accuracy . table 3 shows that the accuracy of predicting the number of tokens contained in a disfluency is low , indicating that there is a need to design a better way to interpret the word .
results are shown in table 4 . we observe that the best models are text + innovations , text + text + greats . text + innovations significantly improve the model ' s performance over both the single and multi - factor models . the results are presented in table 5 . in the single - factor model , we see that the text + raw model outperforms all the other models in terms of dev and test mean . table 5 shows the results of our model with respect to both features . it is clear from the results that the combination of text and innovations significantly improves the model performance .
2 shows the performance of our model on the fnc - 1 test dataset compared to the state - of - art word2vec embeddings . our model achieves the best performance with a low f1 score . it also outperforms the rnn - based sentence embedding by a margin of 3 . 42 points .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous methods .
3 shows the effectiveness of our method for word attention and graph attention for this task . it is clear from table 3 that it improves upon the performance of word attention by a significant margin .
3 shows the performance of the models trained on the pre - trained jvmee dataset . the results are presented in table 3 . the models performed best on all three stages , with the exception of trigger , which is the only one that performs better on all stages . the performance of all models is reported in table 1 .
3 shows the performance of our method on the single event . our method outperforms all the other methods in terms of both event identification and event classification . we show that the method significantly improves the predictive performance for both event and event identification . the results are presented in table 3 . the method significantly outperforms the previous state of the art method on all three scenarios .
results are presented in table 4 . all the models shown in table 1 show that their performance is comparable to that of the original spanish - only - lm model . however , for english - only , the performance gap is larger than that of all the other models . the results show that the fine - tuned models are superior to the original ones in terms of dev metrics .
4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . we show that fine - tuning achieves the best results with only a marginal difference in train dev and train test performance .
5 shows the performance of our system on the dev set and the test set , compared to monolingual and code - switched systems . the results are shown in table 5 . we observe that fine - tuned systems perform better than monolingually trained systems .
results in table 7 show that type - aggregated gaze features significantly improve recall and f1 - score for the three eye - tracking datasets trained on the same test set .
5 shows the performance of type - aggregated gaze features for the conll - 2003 dataset . precision ( p ) , recall ( f1 ) and f1 - score ( f ) are all statistically significant improvements over the baseline , and type combined features show significant performance improvement .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . the hpcd approach relies on syntactic - sg embeddings , and it relies on semantic skipgram embedding . the results on the original paper are summarized in tables 1 and 2 . these results show that the syntactic embedding of glove vectors is beneficial for wordnet and wordnet 3 . 1 , but does not improve the performance of the original wordnet . it improves the semantic embedding performance by 2 . 5 points over the previous state - of - the - art .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . it can be seen that the removal of context sensitivity significantly improves the ppa acc . by 0 . 5 points .
2 shows the results of domain tuning for image caption translation . the results are presented in table 2 . subsatellite tuning improves the multi30k performance by 3 . 5 points over the previous state of the art model . subdomain tuning also boosts the bleu % scores by 2 . 3 points .
results are shown in table 1 . subdomain - tuned models outperform subs1m in all aspects except for the en - de setting . the results are summarized in terms of the performance of the models in table 2 . we observe that the performance obtained by domain - tuning is comparable to the performance achieved by the other two models , namely , on flickr16 and mscoco17 .
4 shows bleu scores in terms of multi30k captions . the results are shown in table 4 . the best results with the best one or all 5 captions are obtained with marian amun ( see table 4 ) . the results with mscoco17 ( see also table 4 ) show that adding automatic captions improves bleus scores in all cases .
5 compares the performance of the three approaches with respect to en - de and dec - gate . the results are summarized in table 5 . we observe that enc - gate significantly improves the bleu % scores for both models . in addition , it improves the performance for both datasets . multi30k + ms - coco + subs3mlm achieves the best results with a 69 . 86 % bbleu % improvement over the baseline .
3 shows the performance of subs3m and subs4m on the en - de dataset . the results are presented in table 4 . sub3m outperforms all the other models except for the one that relies on word embeddings and is text - only . as expected , the results are slightly better than those of subs6m ( which relies on syntactic features ) . the performance of the two models is further improved with the addition of semantic features . the results show that the combination of syntactic and semantic features improves the performance for both models .
results are shown in table 1 . we observe that the best performing en - fr model outperforms the best - performing ones on both datasets . the results are summarized in table 2 .
1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . it is clear from table 1 that these language pairs perform well in the development splits .
2 : training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 . our model outperforms all the other test sets except en – es .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) show that the system performs better than the previous state - of - the - art rev system .
results on flickr8k are shown in table 2 . the vgs model achieves the best performance with a median rank of 15 . it outperforms all the other models except segmatch and rsaimage .
results on synthetically spoken coco are shown in table 1 . the first row labeled vgs shows the performance of the visually supervised model from chrupala2017representations . the second row labeled audio2vec - u shows the results of a visual supervised model trained on the schematized rsaimage dataset .
1 shows the results of the different classifiers compared to the original on sst - 2 . we report further examples in table 1 . for example , cnn turns in a < u > screenplay that has edges at the edges ; it ’ s so clever you want to hate it . rnn shows similar results . similarly , dan shows that it has the advantage of having the edges edges in the screenplay as well as the curves . it is clear from the examples that the rnn classifier is clever enough to make it easier to hate hate it .
2 shows the results of fine - tuning on sst - 2 . the results are shown in table 2 . we notice that the number of occurrences have increased , decreased or stayed the same , indicating that there is a need to fine - tune the sentence .
3 shows the effect of the flipped sentiment label on sentiment . the results are shown in table 3 . the results show that the negative sentiment labels are more effective in boosting sentiment than the positive ones .
results are presented in table 1 . the results are summarized in terms of p < 0 . 001 and p > 0 . 005 . in addition , the results are reported in tables 1 and 2 . these results show that the competitive nature of our approach leads to a better interpretability of our proposed method . it is clear from table 1 that the approach is beneficial for both research and evaluation .
