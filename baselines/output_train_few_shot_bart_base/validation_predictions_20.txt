2 shows the performance of our iterative approach on the large movie review dataset compared to the traditional recursive approach , which performs better on inference with more training time .
results in table 1 show that the balanced dataset exhibits the highest throughput , but at the same time suffers from the high degree of parallelization .
2 shows the performance of the max pooling strategies for each model with different number of parameters . our system achieves the best performance with a 4 - 5 . 5 - fold boost in performance over the traditional pooling scheme . we also use sigmoid as a parameter sharing scheme . the maximum pooling strategy consistently performs better in all model variations .
1 shows the effect of using the shortest dependency path on each relation type . it can be seen that macro - averaged models achieve the best f1 ( in 5 - fold ) with sdp as well as the best diff . as shown in fig . 1 , the two relation types have the same dependency path as the original ones .
results in table 3 show that for all three models , the average f1 and average r1 are closer to those of y - 3 , while for y - 2 , the gap is less than 50 % .
results are presented in table 3 . our proposed method outperforms all the methods except paragraph embeddings with a minimum of 50 % accuracy . we observe that our proposed method achieves the best results when trained on a single dataset .
4 shows the c - f1 scores for the two indicated systems ; for the lstm - parser , it is 60 . 62 ± 3 . 54 and 57 . 24 ± 2 . 87 respectively .
results are shown in table 3 . original and original train performance are slightly better than those of the original , but still slightly worse than the original . our system performs better than both the original and the wrong ones .
shown in table 1 , the original and the cleaned versions have the highest number of distinct mrs and the average number of errors as measured by our slot matching script , see section 3 .
results are shown in table 4 . original and original test results are presented in bold . our system performs better than all the other methods except for the one that we included in the original ( tgen + ) .
results of manual error analysis of tgen on a sample of 100 instances from the original test set . we found a total absolute number of errors ( 17 ) and a slight amount of disfluencies ( 14 ) .
results are shown in table 1 . all models perform better than all the base models except for seq2seqk ( konstas et al . , 2016 ) and all the other models that perform better in the single - model setting .
2 shows the results on amr17 . our model achieves 27 . 5 bleu points on the model size compared to the previous best state - of - the - art model , seq2seqb . the results are summarized in table 2 .
results in table 3 show that our system performs well in english - german and czech , with the exception of english - czech , where it performs slightly worse than the other two models .
5 shows the effect of the number of layers inside the dc block on the performance of our model . we observe that for all layers , there is a significant drop in performance when we add layers of supporting layers .
6 shows the performance of our models with residual connections . rc + la ( 2 ) and gcn + rc ( 4 ) show significant performance improvement over previous models .
model 3 shows the performance of our dcgcn model compared to the previous state - of - the - art models in terms of bias metric .
8 shows the ablation study results for amr15 with respect to the density of the connections in the i - th block . the results are shown in table 8 . - { 4 , 3 , 4 } dense blocks = 25 . 5 % more dense blocks and 53 . 1 % more overall .
show the ablation study results for the two types of encoder and the lstm decoder . encoder modules used in table 9 show that the global network and the multi - decoder have superior performance .
7 shows the performance of our initialization strategies on probing tasks . our paper shows that our method outperforms all the base - based methods except for glorot , which shows lower performance .
are presented in table 3 . we observe that our method outperforms all the other methods except the cbow / 400 model , which shows the diminishing returns from mixing different input types .
results are shown in table 3 . our model outperforms all the other methods except cmp . cbow / 784 has the best performance on both mr and mpqa datasets . it even outperforms sst2 and sst5 in terms of mrpc score . it also beats both sick - e and sts - b by a noticeable margin .
results on unsupervised downstream tasks attained by our models are shown in table 3 . cbow shows the relative change with respect to hybrid compared to cmp . similarly , cmp shows a slight improvement . the results show that cbow and cmp have comparable performance with the original cbow model .
8 shows the performance of initialization strategies on supervised downstream tasks . our paper shows that glorot and sst2 perform better than both mpqa and sick - b on all three supervised tasks .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the best performance is on the sts12 and sts14 tasks , which are supervised by the cbow - r team .
results are shown in table 3 . our approach outperforms all the base - based methods except for cbow - r , which has a lower precision .
3 presents the results of our method with respect to subj and mpqa . our cbow - r model outperforms all the other methods except for the one that uses sst2 and sst5 .
system performance in [ italic ] e + and per is reported in table 3 . our system outperforms all the systems except for the one that does not use org and multi - task learning ( mil - nd ) .
2 shows the results on the test set under two settings . name matching and supervised learning achieve the best results with 95 % confidence intervals . these results show that the combination of pre - trained and supervised learning improves the general performance of the system .
6 shows the entailment numbers for all models with the same number of ref and ref members compared to the original ones .
results in table 3 show that all models trained on the ldc2015e86 dataset perform similarly to the best state - of - the - art models on all metrics except for the meteor metric .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 .
results of the ablation study on the ldc2017t10 development set are shown in table 4 .
results are shown in table 4 . we observe that g2s - gat has the better overall performance on average compared to other models that do not have the same number of frames . as shown in fig . 4 , the model has a lower overall recall rate compared to those of other models with larger frames .
shown in table 8 , the fraction of missing elements in the output that are present in the input ( g2s - gin ) that are missing in the generated sentence ( miss ) . it is clear from the table 8 that the g2s models are better than the reference sentences .
4 shows the performance of our method with respect to target languages . it achieves the best performance with 96 . 7 % accuracy on a single parallel corpus ( 200k sentences ) .
2 shows the pos and sem tagging accuracy with baselines and an upper bound . accuracies are reported in table 2 . word2tag has the best overall performance with a 91 . 5 % upper bound and 91 . 41 % overall accuracy .
results reported in table 3 show that our method significantly outperforms the competition on three of the four coreference tasks .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages .
performance on different datasets is shown in table 8 . the average age of the attacker is 9 . 7 % and the corresponding gender is 8 . 3 % better .
results in table 1 show that training directly towards a single task can improve the performance for pan16 by 3 points .
2 shows the status of the protected attribute leakage in the context of balanced and unbalanced task averages . dial and text tags have the highest coverage , however , the balanced task averages are lower than the unbalanced ones .
performance on different datasets with an adversarial training set is shown in table 3 . for each training set , we use the weighted average number of tokens to predict the performance of an adversary . the average weighted average of tokens is 62 . 5 , which indicates that the training set contains a reasonable selection of appropriate tokens for each context .
6 shows the concuracies of the protected attribute with different encoders . embedding with rnn embeddings can improve the performance for both models .
results in table 3 show that our proposed lstm outperforms all the base models except for the one that uses finetune embeddings . the results of our model are summarized in table 4 .
performance of our model on the training data is reported in table 4 . our model significantly outperforms the previous stateof - the - art models in terms of both training time and total time .
results of experiment 1 are shown in table 1 . our model improves upon the best state - of - the - art model with a 4 . 42 % boost on average compared to the original lstm model .
3 shows the bleu score on the test set of wmt14 english - german translation task . our system improves upon the state - of - the - art gru model by 0 . 99 % in training time and by 1 . 15 % in decoding one sentence .
4 shows the performance of our model with respect to match / f1 score on squad dataset . our model obtains the best performance with a parameter number of 2 . 44m and a f1 score of 75 . 83 / 83 . 83 on the model with an absolute boost of 1 . 67m .
6 shows the f1 score on conll - 2003 english ner task . it can be seen that the lstm model significantly outperforms the other models in terms of parameter number .
performance on snli task with base + ln setting and test perplexity on ptb task with the base setting set .
system retrieval and system re - evaluation are presented in table 4 . word embeddings are used for both system and word - based tasks . all the word based systems ( mtr , mtr , rtr and word2 ) use the best performance on both systems . the word based system ( rtr ) is particularly effective for both systems , with the exception of the system of mtr which is used for word - based systems .
4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 8 . the best performance among all systems is reported in table 4 .
results are shown in table 3 . our proposed system outperforms all the base models except for the one that embeds the word " doc " in the embeddings . it further improves the performance of docsub on all datasets except for those that embed docsub .
results are shown in table 3 . our proposed system outperforms all the base models except for the one that embeds the word embeddings . it closely matches the performance of df and docsub , however it is slightly worse than the other base models . for example , df has the worst performance on both datasets while docsub has the best performance .
3 shows the performance of all the models trained on the corpus dataset compared to those trained on europarl . the results are summarized in table 3 . our model outperforms all the other models except for the one that we trained on . for example , our model performs slightly better than the others on both corpus and docsub datasets .
embeddings are shown in table 3 . our system achieves the best performance on all metrics with a gap of 1 . 78 points between the official score of eurparl and europarl ( + 1 . 78 ) and the maxdepth of our system , which is comparable to the best score on docsub . we also observe that our system has the best overall performance on both metric metrics .
embeddings are shown in table 3 . our system achieves the best performance with a maxdepth of 9 . 43 and a depth of 1 . 71 on the corpus dataset . europarl is slightly better than our maxdepth , but still slightly worse than our system .
performance ( ndcg % ) on the validation set of visdial v1 . 0 . lf is the enhanced version of the original visdial model , and r0 , r2 , r3 denote regressive loss and weighted softmax loss , respectively .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . the best performing model is coatt ( p2 ) .
5 shows the performance of our models on hard and soft alignments . the results are summarized in table 5 . our hmd - prec model outperforms all the other models except for the one that uses bert .
3 presents the results of our approach with respect to direct assessment and bertscore - f1 scores . the results are summarized in table 3 . our approach obtains the best performance with a minimum of 0 . 685 on average compared to the baseline .
3 presents the bagel and sfhotel scores on the validation set . our proposed bertscore - f1 model outperforms all the baseline baselines except for those using bleu - 1 .
performance of the models according to the meteor and spice scores is reported in table 3 . the results are summarized in bold . leic scores significantly outperform the meteor scores by a margin of 0 . 7 and 0 . 8 respectively .
results are shown in table 3 . we observe that the m0 model performs better than the m1 model on all three datasets except for the one that has the shen - 1 label .
3 presents the results of our model on the transfer quality and semantic preservation tasks . our model outperforms all the other baselines on both datasets with a gap of 3 . 5 points .
5 presents the results of human and machine validation . we show the results for paragraph - level and sentence - level validation . the results are shown in table 5 . both sim and human ratings of semantic preservation ( p = 0 . 67 ) are significantly better than those by human .
results are shown in table 4 . we observe that the m0 model performs better than the m1 model on all three metrics except for the shen - 1 metric , which shows the performance of the matrix when trained on a single dataset .
results on yelp sentiment transfer are shown in table 6 . our best models achieve higher bleu than those using simple - transfer or multilingual embeddings , and achieve the highest acc ∗ score . however , the results are slightly worse than those obtained using simple transfer because of different classifiers in use . multi - decoder model achieves the best acc score , but it is slightly better than the best state - of - the - art model .
2 shows the number of repetition tokens that were correctly predicted as disfluencies . reparandum length is the average number of tokens predicted to be in the correct disfluency .
3 shows the relative frequency of rephrases correctly predicted as disfluent for both the reparandum and the repair ( table 3 ) . the fraction of tokens predicted to contain a content word is small , but larger than the fraction predicted as containing a function word .
results are shown in table 4 . all models trained on text + innovations outperform single model in terms of dev and test mean . in addition , all models tested on single data have the best performance .
performance of our model on the fnc - 1 test dataset is shown in table 2 . our model achieves the best performance with a minimum of 0 . 31 f1 and 0 . 38 f1 on the standard embeddings .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . neuraldater significantly outperforms all previous methods in terms of accuracy .
3 shows the performance of our method with and without attention . accuracy ( % ) shows the effectiveness of both word attention and graph attention for this task . as shown in table 3 , the accuracy of ac - gcn is significantly better than that of neuraldater .
results are shown in table 1 . all models perform better than the baseline on all stages except for the one where embedding + t performs worse than jnn .
3 shows the performance of our method with respect to event identification . our method significantly outperforms the previous state - of - the - art method in terms of both event identification and event classification . all the methods used for this task have a significant impact on the performance .
results in table 3 show that all the models trained on the spanish - only network outperform all the other models except for the ones that do not use concatenated word embeddings . all the fine - tuned models show lower performance on the dev perp and test wer datasets compared to the original ones .
results on the train dev and test set are shown in table 4 . fine - tuned train dev with only subsets of code - switched data in it , and on the test set , with only a small amount of train dev .
5 shows the performance of our system on the dev set and the test set , compared to fine - tuned - disc . the results are summarized in table 5 . our system obtains the best performance on both sets .
results in table 7 show that type - aggregated gaze features significantly improve recall and f1 - score for the three eye - tracking datasets tested on the conll - 2003 dataset ( p ≤ 0 . 01 ) .
5 shows precision ( p ) , recall ( f1 ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . note the significant improvement in precision ( p ≤ 0 . 01 ) over the type combined gaze features .
results on belinkov2014exploring ’ s ppa test set . the glove embeddings are derived from the original wordnet ( full ) embedding and are used in wordnet 3 . 1 . they have been recently tested on wordnet 4 . 1 , and it has been shown to perform well on the validation set . the results on the original paper are summarized in table 1 . we use the syntactic - sg embedding as the base for wordnet , and the glosve - retro embedding is used on the wordnet3 . 1 development set . this confirms the viability of syntactic embedding in ppa .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are presented in table 2 .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . it is clear from the table 3 that removing the sense and context sensitive features severely decreases ppa acc . by 0 . 3 points .
2 shows the performance of our model with domain - tuned and multi30k decoding . the results are shown in table 2 . subsfull decoding improves the model ' s performance by 3 . 5 points over the over - fitting of the single30k model .
results are shown in table 4 . we observe that subs1m outperforms all the other models with domain - tuned training data on both datasets , with the exception of mscoco17 , which shows the diminishing returns from domain tuning .
4 shows bleu scores in terms of autocap 1 and multi30k on the larger en - de datasets . the results are shown in table 4 . the results with only the best one or all 5 captions are better than those with a larger ensemble .
results in table 5 show that enc - gate and dec - gate both improve the visual performance ( bleu % scores ) for both datasets ( mscoco17 et al . , 2017 ) . enc - gate also improves the performance ( 37 . 86 % ) on the en - de dataset .
performance of subs3m and subs4m is presented in table 4 . we observe that the combination of text - only and multi - lingual features improves performance over the baselines of subs2m , as shown in fig . 3 .
3 shows the performance of our system compared to the previous best state - of - the - art en - fr - ff model on mtld . the results are summarized in table 3 . we observe that our system performs better than both the original and the alternative .
1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 : training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 .
5 shows the bleu and ter scores for the rev systems . the automatic evaluation scores ( bleu ) are significantly worse than those of the base - based systems ( ours , ter ) .
2 shows the vgs performance on flickr8k compared to the mean mfcc score from chrupala2017representations . the results are summarized in table 2 .
results on synthetically spoken coco are shown in table 1 . the model trained on the embeddings of chrupala2017representations is comparable to the one trained on audio2vec - u .
1 shows the results of different classifiers compared to the original on sst - 2 . for example , orig < cao et al . ( 2017 ) turns in a < u > screenplay that is slightly better at the edges and it ’ s so clever you want to hate it . rnn ( 2016 ) also makes good use of these classifiers .
2 shows the results of fine - tuning on sst - 2 . the results indicate that the number of words in the original sentence has increased , decreased or stayed the same as the amount of occurrences in the new sentence . similarly , the percentage of words that have been added to the sentence has remained the same , however , as has the frequency of occurrences remaining the same .
results in table 3 show that the sentiment change in sst - 2 from positive to negative is larger than those in the original sentence .
results are presented in table 3 . attracting pmi consistently improves results for both groups ( 98 % vs 98 % ) . however , it is difficult to distinguish between positive and negative pmi scores . our joint model outperforms all the base models except sst - 2 .
