results are presented in table 2 . the recursive approach performs the best on training , while the iterative approach shows the best performance on training .
results are shown in table 1 . the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size is larger .
results for each model with different representation are shown in table 2 . the max pooling strategy consistently performs better in all model variations , with the exception of ud v1 . 3 .
results are shown in table 1 . the shortest dependency path on each relation type is used on each dependency path , the best f1 ( in 5 - fold ) with sdp is achieved by using the shortest possible dependency path . however , when using sdp , the f1 is significantly lower than when using a macro - averaged dependency path only .
results are shown in table 1 . y - 3 : y ( y - 3 ) achieves a 50 % f1 score on the f1 test compared to r - f1 score of 50 % and f1 50 % on the r - f1 test . the results are presented in table 2 . we observe that y - 2 : y is significantly better at f1 100 % compared to the previous state - of - the - art model , but significantly worse than the baseline model .
results are presented in table 1 . the results are shown in table 2 . our model outperforms all the other methods except mst - parser and mate . we also observe that our model performs significantly better on the essay level than the other models .
shown in table 4 , the average performance for the two indicated systems is 60 . 62 ± 3 . 54 % compared to the majority performances over the runs given in table 2 .
results are presented in table 1 . the original model outperforms the original model in all but one of the three cases . the results are shown in table 2 . in table 1 , we show the results of the original and the correct model . we also show the performance of the new model on the test set , which is slightly better than the original .
results are presented in table 1 . the original e2e data and our cleaned version are shown in bold . we also observe that the number of distinct mrs , total number of textual references , and number of slot matching scripts ( ser ) are significantly higher than the cleaned version . further , we observe that both the original and the cleaned versions of the original data are significantly more likely to contain textual references than the original ones , which indicates that the original model is less likely to have textual references .
results are presented in table 1 . the original model outperforms the original model in all but one of the three cases . the results are shown in table 2 . the results of the original and tgen models are presented as the results of our test set . we also observe that the results are slightly better than the original models in both cases .
results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) are shown in table 4 .
results are presented in table 1 . all models outperform all the other models except for tree2str ( konstas et al . , 2017 ) and snrg ( pourdamghani and cohen , 2018 ) . we observe that all models perform better than all the models except the ones that do not perform well .
results on amr17 are presented in table 2 . our model achieves a bleu score of 27 . 5 on the model size in terms of parameters , respectively . we note that our model size is comparable to that of seq2seqb ( 2016 ) , which achieves an e score of 28 . 5 and a f score of 30 . 5 . the results are shown in the table 2 .
results are presented in table 1 . the results are shown in table 2 . our model outperforms the previous best - performing models in english - german and english - czech , respectively . we also observe that the results are significantly better than those of the other two models .
table 5 shows the effect of the number of layers inside dc on the performance of the layers in table 5 . we observe that dc has a significant effect on the quality of the layer of layers in dc . the results of table 5 show that dc is more likely to have layers of layers than the other layers .
results are shown in table 6 . gcn + rc + la ( 2 ) and dcgcn3 ( 9 ) are comparable to baselines in terms of residual connections . we observe that the residual connections between gcns with residual connections are significantly higher than those without residual connections , indicating that residual connections have a significant effect on gcn performance . further , we observe that gcn + la is comparable to residual connections with gcn , but with a significant drop in residual connections compared to the baseline baseline . however , the difference between residual connections and residual connections is less pronounced for gcn ( 9 ) .
results are presented in table 1 . the results show that dcgcn ( 2 ) outperforms all the other models in terms of performance , with a significant drop in performance from the previous state - of - the - art model .
8 shows the results of ablation study on the dev set of amr15 . - { i } dense blocks denotes removing the dense connections in the i - th block .
table 9 shows the ablation study for the graph encoder and the lstm decoder . the results are shown in table 9 . our model outperforms all the other models except for dcgcn4 , which outperforms our model by a significant margin .
results are shown in table 7 . we show that our initialization strategies outperform all the other approaches except glorot , which outperforms all the others . as expected , our method outperforms the other methods on probing tasks .
results are presented in table 1 . the results show that the h - cmow / 400 method outperforms all the other methods in terms of depth and length . however , the results are slightly worse than those of the other two methods .
results are presented in table 1 . we observe that the cbow / 784 model outperforms both the sst2 model and the mpqa model by a significant margin . the results are shown in table 2 . our model also outperforms the other two models in terms of performance .
results are presented in table 3 . the results show that cbow outperforms hybrid on unsupervised downstream tasks attained by our models . cbow also outperforms both hybrid and cbow on unvised upstream tasks .
results are shown in table 8 . our model outperforms all the other models except for the ones using glorot and subj , which outperform all the models except subj .
results are shown in table 6 . cmow - c outperforms cbow - r on the unsupervised downstream tasks on the sts12 and sts14 datasets .
results are presented in table 1 . the results show that the cbow - r method outperforms both cbow and somo in terms of depth and coordinv , respectively .
results are presented in table 1 . we observe that the cmow - r model outperforms the sst2 model by a significant margin . the results are shown in table 2 . it also outperforms all the other methods except for the subj model .
results are presented in table 1 . the system achieves the best e + org and e + per scores in both systems . in [ italic ] and table 1 shows the performance of the system in terms of both e + and per scores . we also observe that the system outperforms all the other systems except for the one that does not have a single org score ( mil - nd ) . the results are shown in table 2 . our system performs better than all the systems except mil - nd , which performs better in both cases .
results on the test set under two settings are shown in table 2 . in [ italic ] e + p and e + f1 scores are shown as the results of the training set under the two settings . the results on table 2 show that the system performs significantly better than the model trained on the same test set . we also observe that it performs significantly worse than the system trained on a single setting , the performance of the system on the second setting is shown as a result of the fact that it does not perform well on the first setting .
table 6 : entailment ( ent ) by model and ref ( g2s - gin ) . the results are shown in table 6 . we observe that the model outperforms the ref model by a significant margin .
results are presented in table 1 . we observe that the model outperforms the ldc2015e86 model by a significant margin . the results are shown in table 2 . however , the results are not statistically significant , indicating that the models outperform the models in terms of performance .
results on ldc2015e86 test set are shown in table 3 . the model trained with additional gigaword data is significantly more accurate than the previous model trained using the same model . we also observe that the model is more likely to outperform the original model when trained with gigawords data .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the ablation studies show that bilstm significantly improves the performance of the model on the development set .
results are presented in table 1 . we observe that our model significantly outperforms the previous models in terms of sentence length and sentence length . the results are shown in table 2 . our model outperforms both the previous model and the g2s - gin model by a significant margin . in table 2 , we observe that the average sentence length is significantly larger than the average word length .
shown in table 8 , the fraction of elements that are missing in the input graph that are not present in the generated sentence ( miss ) , for the test set of ldc2017t10 .
shown in table 4 , the accuracy of pos and ar on a smaller parallel corpus ( 200k sentences ) is significantly higher than that of en and ru on a larger parallel corpus .
results in table 2 show that using unsupervised word embeddings improves the accuracy with baselines and an upper bound . the results are shown in table 1 .
results are presented in table 1 . table 1 shows the performance of the pos tagging accuracy and the pos - tagging accuracy scores for both datasets . the pos - based accuracy scores are significantly better than those of the other two datasets , table 2 shows the results of both datasets in table 2 .
results are shown in table 5 . the best results are obtained from the uni and res encoders , averaged over all non - english targets .
shown in table 8 , the attacker score is significantly higher than the corresponding adversary ’ s performance on different datasets .
table 1 shows the performance of pan16 and pan16 when training directly towards a single task .
table 2 shows the results of the protected attribute leakage experiments in pan16 and pan16 . the results are shown in table 2 . in pan16 , we observe a significant drop in the average number of instances in the balanced and unbalanced data splits compared to the unbalanced ones .
table 3 shows the performance of pan16 on different datasets with an adversarial training . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 3 . in pan16 , the gender of the target is significantly higher than that of the corresponding target , indicating that the target has a higher chance of reaching the target .
shown in table 6 , the embeddings of the protected attribute with different encoders are shown in bold . embedding rnn and rnn with the same encoder is shown in red .
results are presented in table 1 . the results are shown in table 2 . this model outperforms the previous model by a significant margin . it also outperforms all the other models in terms of model performance . however , it still outperforms both the original model and the new model by significantly improving model performance by a margin of 0 . 01 % compared to the previous state of the art model .
results are presented in table 5 . this model outperforms all models except gru , which outperforms both gru and sru by a significant margin . the results are shown in table 6 . gru outperforms gru by 2 . 41m and 3 . 36m , respectively . however , gru performs significantly worse than gru in terms of training time and training time . we observe that gru does not outperform sru in training time by a large margin .
results are presented in table 3 . we show that our model outperforms all the other models in terms of err performance , with the exception of the lstm model , which outperforms both the yahoo and yelppolar time err by a significant margin . the results of our model are shown in table 4 . it outperforms the previous model by a margin of 0 . 9 % on the yahoo err and by 0 . 8 % on yelp .
3 shows the bleu score on wmt14 english - german translation task . the model outperforms all the other models except sru and gru in terms of time in the training batch measured from 0 . 2k training steps on newstest2014 dataset .
results published by wang et al . ( 2017 ) are shown in table 4 . we observe that the parameter number of base is significantly higher than that of sru and lrn ( 2017 ) . we also observe that sru is significantly better than sru in match / f1 score , indicating that the model is more likely to perform well on squad dataset . further , we observe that lrn and sru are comparable in match and f1 score . however , sru outperforms sru by a considerable margin . the results show that the sru model performs better than the model in match - score score , as shown by the table 4 .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number and sru denotes the reported result lample et al . ( 2016 ) . sru also shows the reported results ( see table 6 ) . lrn and lrn also show that the model can perform better than sru and gru in ner tasks .
results are shown in table 7 . snli and ptb task with base + ln setting and test perplexity on snli task with ptb setting .
results are presented in table 1 . the word - based system retrieval ( mtr ) outperforms the human system ( r - 2 ) by a margin of 0 . 05 % compared to 0 . 01 % for the human model . the results are shown in table 2 . word - based systems ( rtr ) are more effective than system - based ones , but still outperform human ones by a considerable margin . in table 2 , the word " sentiment " is used to describe the results of using the system as a reference . it is clear that the word ' sentiment ' is beneficial for both human and machine learning , but it is not beneficial for machine learning . as a result , it is less beneficial to machine learning than human ones .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best results among all systems are shown in table 4 , with the highest standard deviation being 1 . 2 and the highest average being 2 . 3 for the overall quality .
results are presented in table 2 . the results are shown in table 1 . we observe that the results of our experiments are significantly better than those of the other two models . our results show that our model outperforms both the df and df models by a significant margin . however , our model performs worse than the df model by a considerable margin , indicating that the performance of our model is not comparable to that of the df models .
results are presented in table 2 . the results are shown in table 1 . we note that the results are slightly better than those obtained by the previous model , but still slightly worse than the previous state - of - the - art model . further , we note that our model outperforms all the other models in terms of performance .
results are presented in table 2 . the results are shown in table 1 . we observe that the results of our experiments are significantly better than those of the other two models . our results show that our model outperforms both the df and df models by a significant margin . however , our model performs worse than the df model by a considerable margin .
results are presented in table 3 . the results are shown in table 4 . europarl achieves the best performance on both metric and metric embeddings . however , it does not achieve the best depthcohesion performance on all metrics . in particular , it achieves the worst performance on metric embedding , with a drop of 1 . 78 points compared to the previous state - of - the - art metric .
results are presented in table 1 . the results are shown in table 2 . europarl achieves the best performance on both metric and metric embeddings . however , it does not achieve the best depthcohesion , which is a significant drop from the previous state - of - the - art model .
shown in table 1 , lf outperforms the original visdial model on the validation set of visdial v1 . 0 .
results are shown in table 2 . the best performing model on the visdial v1 . 0 validation set is p2 , which is implemented in section 5 with the history shortcut .
results are presented in table 5 . the hmd - f1 + bert model outperforms all the other hmd - prec models on hard and soft alignments . it also outperforms both the wmd - recall model and the ruse model on hard alignments as well .
results are presented in table 1 . metrics and baselines are shown in table 2 . we observe that the baselines are significantly better than the baseline , indicating that the model is more suitable for the task at hand . our model outperforms the baseline by a significant margin .
results are presented in table 1 . the bleu - 2 model outperforms meteor and sfhotel by a significant margin . we observe that the baselines are significantly better than the baseline , indicating that the model is more suitable for the task .
results are presented in table 2 . the baselines are shown in table 1 . our model outperforms all the models except wmd - 1 , w2v , and word - mover on the metric and baselines datasets . we observe that the baselines for both models are significantly better than those for the other models .
results are presented in table 3 . the results show that m0 [ italic ] + para + lang significantly outperforms m1 ( m2 + lang ) and m6 + ( m6 + lang ) , respectively . however , the results are slightly worse than those of m1 and m2 ( m3 + lang ) .
results are presented in table 1 . the results show that our model outperforms all the other models in terms of transfer quality , transfer quality and transfer quality . we also observe that the results are significantly better than those of the previous models , indicating that the model is more likely to outperform the previous model by a significant margin .
5 presents the results of human sentence - level validation of the metrics for each dataset . the results are shown in table 5 . the results of the human sentence level validation are comparable to those of the machine and human . however , the results are slightly worse for the machine than the human .
results are presented in table 3 . the results show that m0 [ italic ] + para + lang = 0 . 818 and 0 . 719 , respectively , compared to the previous state - of - the - art model ( m1 + m2 + m3 + m4 + m5 + m0 + m6 + model ) , respectively , outperforms both the m2 + and m3 + models by a significant margin . we also observe that the m0 + model outperforms the m1 + model by a considerable margin , with the exception of the m4 + model , which outperforms m5 + by a large margin .
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu and the highest acc ∗ on the same 1000 sentences and human references . however , our best model performs significantly worse than our previous work , which is due to the fact that the classifiers in use are much more complicated to encode than those in the previous work .
shown in table 2 , the number of reparandum tokens that were correctly predicted as disfluent is slightly higher than the number predicted as nested disfluencies , indicating that the disfluency is more likely to be caused by repetition .
shown in table 3 , the number of tokens correctly predicted to contain a content word in both the reparandum and the repair ( content - content ) is significantly higher than the number predicted as disfluent for disfluencies that contain only the content word .
results are presented in table 1 . the model outperforms all the other models in terms of dev mean and test mean . in addition , it outperforms both the single model and the multi - model model by a significant margin . we observe that the model performs better on the single and multi - task tasks than the multi task tasks , indicating that it is more likely to outperform all the models except the single models .
results are presented in table 2 . our model outperforms the state - of - art word2vec embeddings on the fnc - 1 test dataset by a significant margin .
shown in table 2 , our unified model significantly outperforms all previous methods on the apw and nyt datasets .
3 shows the effectiveness of word attention and graph attention for this task . the results show that word attention improves the performance of the component models with and without attention .
results are presented in table 1 . we observe that the jnn model outperforms the previous state - of - the - art models in all but one of the three stages .
results are presented in table 1 . we observe that the method outperforms all the other methods in terms of identification , classification , and f1 scores . the results are shown in table 2 . in table 1 , we observe that both the method and the classification scores are significantly higher than those of the other approaches .
results are presented in table 2 . all models are shown in table 1 . the results show that all models are comparable in terms of performance , with the exception of english - only - lm showing the best performance . in english , all models show the same performance , except for those that show the worst performance .
results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the results are shown in table 4 .
results are shown in table 5 . the best performance is achieved on the dev set and on the test set , according to the type of the gold sentence in each sentence in the set .
shown in table 7 , type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset are significantly more accurate than type combined gaze features .
5 presents the results for conll - 2003 . the results show that type - aggregated gaze features significantly improve recall and recall performance . we also observe that type combined gaze features also improve recall performance by a significant margin .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . syntactic - sg embeddings are used in wordnet , verbnet , and glove - extended , respectively . the syntactic embedding is used in the original paper , and it uses syntactic skipgram embedding .
results are presented in table 2 . the rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . we observe that rbg has the highest ppa accomplomplomplishment on the full uas and full uas datasets . however , it does not have the best uas performance on the partial uas dataset .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model .
results are presented in table 2 . subdomain - tuned embeddings outperform domain tuning for image caption translation ( bleu % scores ) . subsubdomain tuning outperforms domain tuning , but does not improve the performance of multi30k .
results are presented in table 1 . we observe that subs1m outperforms all other models except for those with domain - tuned features . the results are shown in table 2 . our model outperforms the other models in terms of performance , with the exception of mscoco17 , which outperforms our model by a significant margin . we also observe that our model performs better than other models when using domain tuned features .
4 shows bleu scores in terms of automatic image captions ( only the best one or all 5 ) . the results with marian amun are shown in table 4 .
results are presented in table 5 . we observe that enc - gate and dec - gate embeddings significantly improve the bleu % scores for embedding visual information ( see table 5 ) .
results are presented in table 3 . we observe that subs3m has the best performance when combined with multi - lingual features ( e . g . , text - only features , mscoco17 and gn2048 ) . the results are shown in table 4 . our model outperforms all the other models in terms of performance , with the exception of subs6m , which has the worst performance on the en - de dataset .
results are presented in table 1 . we observe that en - fr - rnn - ff and en - es - ht are comparable in terms of translation performance . the results are shown in table 2 . in table 1 , we observe that the translation performance is comparable to the mtld performance on both datasets . however , the difference in translation performance between the two datasets is significant .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
table 2 : training vocabularies for the english , french and spanish data used for our models .
5 presents the results for the rev systems using automatic evaluation scores ( bleu and ter ) for the system reference . the results are shown in table 5 .
results on flickr8k are presented in table 2 . the vgs model is shown in the row labeled vgs , which shows that it performs better than the standard rsaimage model .
results are presented in table 1 . we show the results of a visually supervised model from chrupala2017representations . the results are shown in the table 1 . we observe that the performance of the model is significantly better than that of the previous model .
we report further examples in table 1 . for example , cnn turns on a on ( in the the the edges of the screenplay ) and on a curve ( in the the corners of the screen ) . this shows that cnn is very clever at the edges ; it ’ s so clever you want to hate it . we also show that rnn and rnn are very clever , but rnn is not .
results in table 2 show that fine - tuning has not changed the number of words in sst - 2 . the results are shown in the table 2 . the number of occurrences in the original sentence has not increased , decreased or stayed the same .
results are shown in table 3 . the results show that the positive and negative labels are flipped to positive and vice versa . however , the negative label is flipped to negative , indicating that the sentiment is still negative .
results are presented in table 3 . the results are shown in table 4 . we observe that the performance of sst - 2 and sift - 2 is significantly higher than those of the other two models . however , the results are not statistically significant , indicating that the results of the experiment are more likely to be positive than negative . in addition , we observe that sift performs significantly better than the two other models , which indicates that the experiment is beneficial for both models .
