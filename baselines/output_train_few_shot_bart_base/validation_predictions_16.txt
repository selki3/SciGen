2 shows the performance of our recursive framework on the large movie review dataset , with the best performance on training compared to using the iterative approach .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset .
2 shows the performance of our max pooling strategy for each model with different representation . our system achieves the best performance with different number of parameters and the average number of frames per parameter . the maximum pooling scheme consistently performs better in all model variations . the hgn outperforms all the other approaches except for softplus . hgn performs best with the maximum number of iterations .
1 shows the effect of using the shortest dependency path on each relation type . our system achieves the best f1 ( in 5 - fold ) with sdp , and the best diff . by using it as a dependency path . we also observe that the macro - averaged model outperforms all the other models in terms of f1 and diff .
results in table 3 show that y - 3 significantly outperforms y - 2 in terms of f1 and f1 score , both of which are statistically significant ( y - 3 ) .
3 presents the results of our method in terms of paragraph level and f1 scores . our method achieves the best results with 50 % and 50 % respectively on the test set , respectively .
shown in table 4 , the average c - f1 score for each system is lower than the average score for the other two systems , lstm - parser and paramount .
results are shown in table 1 . original and original scores are shown as follows : original scores have a generally high bleu score , while the other two show lower scores . original scores tend to be more predictive , with a slight improvement over the original score .
shown in table 1 , the original e2e data and our cleaned version are comparable in terms of number of distinct mrs , total number of textual references , and ser as measured by our slot matching script , see section 3 .
results are shown in table 1 . original models outperform the original on every metric except bleu , nist and cider . the best performing model is the sc - lstm , which performs on par with both the original and the wrong metric .
4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . we found that adding incorrect values to the training set caused a significant drop in performance . adding incorrect values caused a slight drop in accuracy .
3 shows the performance of our models on the external and external datasets compared to the previous state - of - the - art models . our dcgcn model significantly outperforms all the other models except for seq2seqk ( konstas et al . , 2017 ) in terms of bias metric , with the exception of snrg , which performs better in the single - sample setting .
results on amr17 are shown in table 2 . our model achieves a bleu score of 27 . 5 on the model size in terms of parameters , respectively . the results show that our model size is comparable to that of the seq2seqb ensemble model .
3 shows the results for english - german and english - czech . the results are shown in table 3 . the best performing model is the bow + gcn model , which achieves the best results in english - language and german - language . we observe that the single model outperforms the other models in both languages , with the exception of english - korean , where the performance is lower .
5 shows the effect of the number of layers inside dc on the overall performance of the model . table 5 shows that for every layer of dc , there are two layers of layers that contribute significantly to the performance . for example , for example , the smallest layer , m , contributes significantly less than the larger layer ,
6 shows the performance of our models with residual connections . with residual connections , our model achieves the best performance with a 21 . 2 % overall improvement over the baseline .
3 shows the performance of our dcgcn model compared to other models in the literature . the results are summarized in table 3 . our model obtains the best performance with a minimum of 300 examples per model .
8 shows the ablation study results for the dev set of amr15 . - { i , 4 } dense blocks denotes removing the dense connections in the i - th block . it is clear from table 8 that removing these dense blocks does not improve the performance of the model .
table 9 shows the ablation study results for the graph encoder and the lstm decoder . the results show that the global network and the multi - decoder have the best performance , with a gap of 3 . 5 points between the two .
7 shows the performance of our initialization strategies on probing tasks . our paper shows that our method obtains the best performance with a minimum of 0 . 01 % false start .
3 presents the results of our method on the subtense and subtense metrics . our method obtains the best results with a 34 . 4 % improvement over the previous state - of - the - art model .
results are shown in table 3 . our model outperforms all the other models except for the one that uses sick - e and sst5 . our cbow model improves the mrpc score by 0 . 2 % on average compared to the previous best model .
results on unsupervised downstream tasks attained by our models are shown in table 3 . our model outperforms both hybrid and cmp in terms of overall performance .
8 shows the performance of our initialization strategies on supervised downstream tasks . our paper shows that our approach outperforms the best state - of - the - art models on all three tasks , with the exception of sst2 .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the best performance is on the sts12 and sts14 datasets , where the cbow - r model outperforms the other models on both tasks .
results are shown in table 3 . the best performing models are cbow and cbow - r , both of which show strong performance on the subtraction and subtraction tasks . cbow shows the best performance on both metrics , with a slight improvement over the previous state of the art model .
3 presents the results of our method on the mrpc test set . our model outperforms all the other methods except for sick - e , which is better at predicting mrpc scores . it also outperforms both the sst2 and sst5 scores by a margin of 3 . 5 - 4 . 5 points .
3 presents the results of our system in table 3 . our system obtains e + loc , e + per and all misc scores in [ italic ] e + misc and topicale + per scores . our model obtains the best results with a minimum of 50 % org and 50 % per scores , and achieves the best e + org score . the results are summarized in table 4 . we observe that the best performance is obtained with the best org scores and the best per scores are obtained using the best state - of - the - art features .
results on the test set under two settings are shown in table 2 . our system achieves the best performance with 95 % confidence intervals of f1 score . our model achieves the highest e + p score with a score of 42 . 90 Â± 0 . 72 , which shows the effectiveness of the supervised learning model in improving the performance .
6 : entailment ( ent ) and ref ( g2s - gat ) are shown in table 6 . our model outperforms all the other models except for those that do not use ref .
3 presents the results of our model on the ldc2015e86 and ldc2017t10 datasets . our model outperforms all the other models except for the ones that do not achieve the best results . the results are summarized in table 3 .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . bilstm significantly outperforms the strong lemmatization baseline on both development set .
results are shown in table 3 . we observe that for all models , the average number of frames per sentence is significantly lower than the average length of the sentences , indicating that the model is more suitable for the task at hand . for the g2s model , we observe lower average sentence length and average sentence size , which indicates that the models are more suitable to handle large datasets .
shown in table 8 , the fraction of elements missing in the output that are present in the input ( g2s - gin ) is the smallest of the three models that are used in the ldc2017t10 test set . it is clear from table 8 that these models do not need to rely on token lemmas to generate sentences .
4 shows the performance of our system on a smaller parallel corpus ( 200k sentences ) with different target languages trained with different nmt encoding layers .
2 shows the pos and sem accuracy with baselines and an upper bound . accuracies are shown in table 2 . the best performing model is word2tag , which relies on unsupervised embeddings .
results are shown in table 3 . our system outperforms all the other methods except for the one that significantly improves the accuracy . our model obtains the best performance on both datasets .
5 shows the accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . our results show that our approach achieves the best performance with a 94 . 9 % overall improvement over the previous state of the art .
8 shows the performance of our model on different datasets . our model obtains the best performance on the training set 10 % held - out compared to the baseline , and the corresponding adversary gets the worst performance .
1 shows the performance of our system when training directly towards a single task . our system obtains the best performance with a minimum of training time .
2 presents the results of the study on the balanced and unbalanced data splits . we observe that the presence of the gender - neutral label in the data splits leads to a significant drop in the performance of the model in the balanced task , and consequently , a drop in performance in the unbalanced task .
3 shows the performance of our system on different datasets with an adversarial training . our system achieves the best performance with a weighted average of 62 . 5 % on each dataset compared to 62 . 3 % on the other two datasets .
6 shows the performance of different encoders for different variants of the protected attribute . embedding leaky is easier for rnn to do than it is for embedded .
results of our second study are shown in table 3 . our first study shows that our lstm model outperforms the best state - of - the - art models in terms of both base and finetune performance . the results show that our model performs well on both datasets , with the exception of the wt2 dataset , where it performs slightly worse than the other models . we observe that the performance of our model is mostly due to the large size of the training set and the fact that it relies on a relatively small amount of training data .
results are shown in table 5 . we show the performance of our lstm model on the test set of hotpotqa . our model is significantly better than the previous state - of - the - art model in terms of time and accuracy . it also outperforms all the other models when it comes to training time . the results of our model are summarized in table 6 .
3 shows the performance of our model compared to the previous state - of - the - art models . our model obtains the best performance on both datasets with a minimum of time difference . the results are summarized in table 3 . we observe that our model performs better than both the original lstm and the original amafull time model .
3 shows the bleu score of our model on wmt14 english - german translation task . our system obtains the best bleus score on the test set , measured in seconds . our model outperforms all the state - of - the - art models except for the sru model , which is slightly better than the other models .
4 shows the performance of our model with respect to match / f1 score on squad dataset . our model obtains the best performance with a f1 score of 69 . 83 / 83 . 83 . we also observe that our model performs better than other models with higher f1 scores as well .
6 shows the f1 score on conll - 2003 english ner task . the lstm model significantly outperforms the other models in terms of parameter number . as shown in table 6 , the average f1 scores of the models is significantly lower than those of other models .
7 shows the performance of our model on snli task with base + ln setting and test perplexity on ptb task with base setting setting .
results are shown in table 1 . word embeddings significantly improve the performance of the system for both human and system retrieval . sent attention significantly improves the performance for both systems , word attention significantly outperforms human attention , the word attention significantly boosts performance for the system , with a boost of 3 . 05 % over human attention .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . our system ranked in the top 1 or 2 for each of these metrics .
results are shown in table 3 . our joint model outperforms all the other models except for the two that we use , namely , europarl and ted talks . the results are reported in tables 1 and 2 .
results are shown in table 3 . we observe that all the models trained on the corpus dataset are significantly better than those trained on europarl . however , for all the other models , our results are slightly worse than those on the other two datasets .
results are shown in table 3 . our joint model outperforms all the other models except for the two that we use , namely , europarl and ted talks . the results are reported in tables 1 and 2 .
results are shown in table 3 . our system achieves the best performance with a gap of 1 . 78 % on the metric compared to the previous best performance on both metric . our model obtains the best overall score with an absolute improvement of 3 . 86 % . our system obtains a better overall score than both the other two systems .
3 presents the results of our joint study . our joint study found that our joint model has the best overall performance on both metric and metric metrics . the results are summarized in table 3 . our joint model achieves the best performance on the metric metrics with a gap of 2 . 42 % in the score compared to the previous state of the art .
shown in table 1 , the performance of our enhanced model on the validation set of visdial v1 . 0 is shown in the table 1 . the enhanced model significantly outperforms the original model in terms of r0 and r2 scores .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . our model obtains the best performance ( i . e . , better coatt score ) on the two sets .
5 compares the performance of our models on hard and soft alignments . we observe that the hmd - prec model outperforms all the other models except for wmd - f1 ,
3 presents the results of our model on the test set of ruse ( ruse ) and sent - mover ( w2v ) . the results are summarized in table 3 . we observe that our model significantly outperforms the baseline bertscore - f1 score by a margin of 0 . 005 and 0 . 701 respectively .
3 presents the bagel and sfhotel scores on the test set . our bleu - 2 model outperforms all the baselines except for the ones that do not use bertscore .
3 presents the results of our model on the baselines . our model improves upon the leic score by 0 . 7 points on the m1 and m2 scores by applying bertscore - recall to the models .
results are shown in table 3 . we observe that the m0 model outperforms the m1 model on all metrics except for the performance of the shen - 1 score , which indicates that the model is more suitable for the task .
3 presents the results of our model on the transfer quality and semantic preservation scores . our model outperforms the previous state - of - the - art model on both datasets . the results show that the semantic preservation and transfer quality scores are comparable across all domains , with the exception of the case of the semantic preservation dataset , where the difference between the two is less pronounced . semantic preservation scores are significantly lower than those of the other two baselines , indicating that semantic preservation is more important for semantic preservation .
5 shows the results of human and machine validation . the results are shown in table 5 . both sim and human ratings of semantic preservation are significantly higher than those of sim , indicating that the system is able to match sentence quality with human judgments .
results are shown in table 3 . we observe that the m0 model outperforms the m1 model in terms of sentence quality , but still performs worse than the m2 model . in particular , we observe that for the shen - 1 model , our model performs better than the previous state of the art model .
6 shows the results on yelp sentiment transfer , where bleu is between 1000 and 1000 sentences and human references achieve the highest acc â score . our best model outperforms all the other models except for the one that is restricted to 1000 sentences . the results on the simple - transfer task are shown in table 6 . multi - decoder models achieve the best acc score , but the difference is much smaller . sentiment embedding is the most difficult task for the model , and it requires a lot of training time to learn .
statistics for nested disfluencies are shown in table 2 . the number of tokens predicted to be disfluent is slightly less than the number predicted as nested , but still significantly less than repetition .
3 shows the percentage of rephrases correctly predicted as disfluent for both the reparandum and the repair ( table 3 ) . the percentage of tokens predicted to contain a content word is shown in table 3 . the fraction of tokens that contain a non - content word is reported as 0 . 61 ( 30 % ) and 0 . 58 ( 52 % ) ( 31 % ) respectively .
results are shown in table 4 . we observe that the best performing models are text + innovations , while the best model is text + text + fine - tune . text + innovations improve the model ' s performance by 0 . 2 point over the single model , while fine - tuning the model improves the model by 1 point .
2 compares our model with the state - of - art algorithms on the fnc - 1 test dataset . our model achieves the best performance with a low f1 score of 28 . 53 / 71 . 43 on the test dataset , which shows the state of the art performance of our model .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . our unified model significantly outperforms all previous models except for burstysimdater .
3 shows the performance of our method with and without attention . it shows the effectiveness of both word attention and graph attention for this task . the accuracy of the ac - gcn of neuraldater is 63 . 9 % .
3 shows the performance of our model on each stage . our model outperforms all the other models except for the one that performs best on every stage . we observe that our model performs better on all stages , with the exception of the last stage when it performs worse than the others .
3 shows the performance of our method in the event of a single event . our method obtains the best results with a precision of 68 . 7 % and a f1 of 50 . 9 % , respectively , compared to the previous state - of - the - art method , which achieves the best f1 score .
results are shown in table 4 . all models except for the ones that use fine - tuned word embeddings outperform all the other models in terms of test performance .
4 shows the results on the dev set and on the test set using fine - tuned training with only subsets of the code - switched data . our system achieves the best results with a 50 % train dev and 75 % train test score .
5 shows the performance of our system on the dev set and the test set , compared to the monolingual performance of fine - tuneddisc . the results are summarized in table 5 . our system obtains the best performance on both the dev and test sets .
results in table 7 show that type - aggregated gaze features significantly improve the precision ( p < 0 . 01 ) and f1 score ( p â¤ 0 . 05 ) for the three eye - tracking datasets , respectively , compared to the baseline .
5 shows the precision and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . precision ( p ) , recall ( f1 - score ( f ) , f1 score ( g ) and f2 score ( h ) are all statistically significant improvements over the baseline ( p â¤ 0 . 05 ) .
results on belinkov2014exploring â s ppa test set are shown in table 1 . our hpcd model uses syntactic - sg embeddings as the base for wordnet and wordnet 3 . 1 . it relies on the syntactic embedding of skipgram and glove - retro vectors . the results on the original paper are summarized in table 2 . we use syntactic and semantic embedding as the baseline for our wordnet model . we use the same embedding scheme as the original wordnet , but we use glovec as the embedding layer . as expected , our model performs slightly better than the other two models , but still performs slightly worse than the original .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . we report the ppa acc . ( normalized by the number of frames ) as shown in table 3 .
2 shows the performance of our model with domain - tuned and multi30k decoding data . our model outperforms both en - de and flickr17 in terms of bleu % scores . subsfull decoding data also improves the model ' s translation performance .
3 shows the performance of subs1m on en - de and in - de settings . our model outperforms all the other models except for the ones that use domain - tuned h + ms - coco and mscoco17 .
4 shows bleu scores in terms of multi30k captions . our model outperforms all the models except for the ones with the best ones . the results are shown in table 4 . adding automatic captions with the help of a single attn . helps the model to improve its performance .
5 compares the performance of our approaches with those using multi30k + ms - coco + subs3mlm , enc - gate and dec - gate . the results are summarized in table 5 . we observe that our approach achieves the best results with a 69 . 86 % bleu % score on the en - de dataset .
3 shows the performance of the subs3m model compared to the subs6m model in terms of visual features . our model outperforms both the en - de model and the mscoco17 model on all metrics except for the performance on the flickr16 dataset . as expected , our model performs better than the other models in all metrics ,
results are shown in table 3 . we observe that the best performing model is the en - fr - rnn - ff model , which significantly outperforms the best state - of - the - art model .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 : training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 . our model obtains the best results with a combined score of 113 , 692 .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) show that the system performs better than the previous state - of - the - art systems in terms of translation quality .
results on flickr8k are shown in table 2 . our vgs model outperforms the previous stateof - the - art model by a significant margin .
results on synthetically spoken coco are shown in table 1 . our model outperforms the previous state - of - the - art models in terms of both recall and chance .
1 shows the results of different classifiers compared to the original on sst - 2 . we report further examples in table 1 . for example , cnn turns in a < u > screenplay that shows the edges at the edges ; it â s so clever you want to hate it . table 1 shows that for rnn , the edges of the screenplay are much more interesting than the edges . for dan , the turns in the screenplay were much better than the original .
2 shows the results of fine - tuning on sst - 2 . the results are shown in table 2 . we observe that the number of occurrences have increased , decreased or stayed the same through fine - tune . as expected , the percentage points in the average sentence have not increased , but remain the same . the results show the diminishing returns on the accuracy of our system .
shown in table 3 , the effect of negative sentiment on sentiment is less pronounced in sst - 2 compared to positive sentiment .
results are presented in table 3 . the results are summarized in terms of average ppmi ( p < 0 . 001 ) and average fpmi of 0 . 005 . as expected , the results are significantly better than those of the competitive groups . our joint model outperforms both the competitive and competitive approaches , with a notable exception .
