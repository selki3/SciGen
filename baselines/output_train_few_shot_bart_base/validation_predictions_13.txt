2 shows the performance of our recursive framework on the large movie review dataset compared to our iterative approach , which performs better on inference with efficient parallel execution of the tree nodes .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset .
2 presents the results for each model with different representation . we show that the max pooling strategy consistently outperforms the single - factor approach in all model variations . we also show the performance of sigmoid and softplus models with different number of parameters .
1 shows the effect of using the shortest dependency path on each relation type . our model achieves the best f1 ( in 5 - fold ) with sdp and the best diff . the results are shown in table 1 . our model outperforms the macro - averaged model in both f1 and diff .
results are shown in table 3 . y - 3 outperforms y - 2 in terms of f1 and f1 50 % on average . on the other hand , the difference in f1 is less pronounced for y - 4 .
results are presented in table 1 . the results of our method are summarized in terms of paragraph level and f1 scores . our method outperforms all the methods except mst - parser on paragraph level . as expected , the results of all methods are significantly better than those of the other methods .
4 shows the c - f1 scores for the two indicated systems ; essay vs . paragraph level . the results are shown in tables 4 and 5 .
results are presented in table 1 . the original and the original systems are shown in bold . all the errors in the original are corrected on the test set . the errors in both sets are caused by incorrect labeling . the error detection system is consistently worse than the original on both sets .
shown in table 1 , the original and the cleaned versions of the e2e data are comparable in terms of number of distinct mrs , total number of textual references , and ser as measured by our slot matching script .
results are shown in table 1 . the original and the original embeddings are the best performing on the test set . the errors are caused by incorrect translation . the error is caused by the incorrect translation of the data .
4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . we found a total of 22 errors in the original training set , which we found to be slight disfluencies in the disfluency detection set .
results are shown in table 1 . all models outperform all the other models in terms of both external and external evaluations . however , for seq2seqk , all models perform better than all the models except for tree2str .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points in terms of parameters , compared to the previous best state - of - the - art model , seq2seqb .
results are shown in table 1 . the results are presented in english - czech , german , french , russian , turkish , russian and turkish . in english - german , the results are reported in tables 1 and 2 . as expected , the english - language results are slightly worse than those in german , although the difference is less pronounced .
5 shows the effect of the number of layers inside dc on the overall performance of the layers . table 5 shows that when we add layers of layers , the effect is less pronounced for all layers .
results are shown in table 6 . with residual connections , gcns with rc + la ( 4 ) and residual connections ( 6 ) outperform all baselines except for dcgcn , which has residual connections with residual connections .
results are shown in table 3 . we observe that the dcgcn model outperforms all the other models in terms of performance . the results show that when the model is pre - trained , the model performs better than other models .
8 shows the ablation study results for amr15 . the results are shown in table 8 . the first example shows that removing the dense blocks reduces the number of connections in the dev set . the second example shows the performance of the model when removing the layers of dense blocks .
table 9 shows the ablation study results for the graph encoder and the lstm decoder . the results are summarized in table 9 . our model outperforms the previous state - of - the - art encoder in terms of coverage .
7 shows the performance of our initialization strategies on probing tasks . our paper shows that our method outperforms the previous best state - of - the - art methods on all three tasks .
results are presented in table 1 . we observe that the best performing method is the h - cbow / 400 model , which achieves the best performance on all three metrics .
results are shown in table 1 . our model outperforms all the other methods except subj and mpqa except for subj .
results on unsupervised downstream tasks attained by our models are shown in table 3 . our model outperforms both hybrid and cmp in all but one of these cases .
8 shows the performance of our initialization strategies on supervised downstream tasks . our paper shows that our approach outperforms all the other approaches except for sst2 and sst5 , which are more difficult to design .
6 shows the results for different training objectives on the unsupervised downstream tasks . the results are summarized in table 6 . for example , cbow - c outperforms all the other methods except for sts13 , which shows that the training objectives are more suitable for the task .
results are shown in table 1 . we observe that our method outperforms all the other methods except cbow - r , which is more accurate .
3 presents the results of our method on the subj and mpqa datasets . our model outperforms all the other methods except subj , which is superior in both categories .
3 presents the results of our system in table 3 . our system outperforms all the other systems in terms of e + org and per . in fact , it is the only system that performs better than all the systems that do not rely on org .
results on the test set under two settings are shown in table 2 . our system outperforms all the previous models in terms of e + p , f1 and e + r scores .
6 : entailment ( ent ) and ref ( ref ) results are shown in table 6 . the results of ref and gen are presented in bold . ref significantly outperforms ref , but ref is superior .
results are presented in table 1 . we observe that the ldc2015e86 model outperforms all the other models in terms of bleu and eor .
3 shows the results on ldc2015e86 test set when additional gigaword data is trained . our model outperforms the previous state - of - the - art models in both external and external settings .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results show that the bilstm significantly improves the model ' s performance when combined with a larger number of parameters .
results are shown in table 1 . we observe that the average number of frames per sentence is significantly larger than the average length of the sentences in the model , indicating that the model is more accurate at predicting sentence length . the results are reported in table 2 . our model outperforms all the other models in terms of sentence length and sentence length , with the exception of g2s - gin .
shown in table 8 , the fraction of elements in the output that are missing in the input graph that are not present in the generated sentence ( g2s - gin ) . this shows that the g2s model is able to distinguish between the reference sentences without missing any elements .
4 shows the accuracy of our approach with different target languages on a smaller parallel corpus ( 200k sentences ) . our model outperforms all the other methods except pos , which is more accurate .
2 shows the pos and sem accuracy with baselines and an upper bound . the results are shown in table 2 . word2tag is the most frequently used classifier , followed by unsupemb . the best results are obtained using baselines .
results are presented in table 4 . we observe that the accuracy obtained by our method is comparable to those obtained by the other methods . our results show that our method outperforms all the methods on both datasets .
5 shows the accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english targets .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ . in pan16 , the attacker scored 10 % higher on the training set compared to the trained adversary ' s accuracy .
1 shows the accuracies when training directly towards a single task . for pan16 , we trained directly towards the task . the results are shown in table 1 . sentiment and gender are the most important factors in the performance of the training .
2 presents the results of the study on the balanced and unbalanced data splits . the results are presented in tables 2 and 3 . we observe that the presence of the word " race " in the conversation splits the data into balanced & unbalanced splits .
3 shows the performance on different datasets with an adversarial training . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 3 . the training results are shown in bold .
6 shows the accuracies of the protected attribute with different encoders . embedding rnn is more difficult than embedding rnn , but the improvement is more pronounced when embeddings are nested .
results are shown in table 1 . our model outperforms the previous state - of - the - art lstm model on all metrics , with the exception of finetune . the results show that our model performs better than the other models on all three metrics . we also observe that this model is more suitable for training on both the wt2 and wt2 datasets as well as on the other two datasets . further improving performance on both datasets is beneficial for both datasets
results are shown in table 5 . our model outperforms all the previous models in terms of training time . we observe that when training time is increased , the time to training time decreases significantly .
results are shown in table 1 . we observe that our model outperforms the best state - of - the - art models on all metrics except for the amapolar time metric . our model also outperforms both the yelp and amafull time datasets in terms of err performance .
3 shows the bleu score on wmt14 english - german translation task . our model outperforms all the other models in terms of decoding time , with the exception of sru , which is more accurate .
4 shows the performance of our model on squad dataset . our model outperforms all the other models in terms of match / f1 score . we observe that the sru model performs better than the lstm model , and is more accurate in match and f1 scores . however , it is less accurate than our model .
6 shows the f1 score on conll - 2003 english ner task . the lstm model outperforms all the other models in terms of parameter number . in fact , it even outperforms the sru model in the ner tasks .
7 shows the accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting setting .
results are shown in table 1 . word embeddings are used in all systems except system retrieval . sent attention is beneficial for both systems , word attention improves the performance of both systems when using the same information . the word attention helps the system to better interpret the information and to improve interpretability . using word attention improves interpretability and recall accuracy .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all the systems is reported in table 4 . the results are highlighted in bold , with statistical significance marked with ∗ and ∗ indicating that the system is in top 1 or 2 for overall quality .
results are shown in table 1 . we observe that the best performing embeddings are on the df and tf datasets , while the best ones are on docsub . the results are reported in table 2 .
results are shown in table 1 . the results of our experiments are presented in tables 1 and 2 . we observe that our approach outperforms both the df and tf datasets in terms of performance .
results are shown in table 1 . we observe that the best performing embeddings are on the df and tf datasets , while the best ones are on docsub and docsub .
results are shown in table 1 . the results are presented in table 2 . we observe that our approach achieves the best performance on all metrics , with a slight improvement over the baseline on the df metric . however , our approach obtains the worst results .
results are shown in table 1 . we observe that our approach achieves the best results on all metrics , with the exception of depthcohesion , which is slightly worse than our approach . the results are reported in table 2 .
1 shows the performance ( ndcg % ) of our enhanced model on the validation set of visdial v1 . 0 . the enhanced model outperforms the original visdial model in terms of r0 , r2 and r3 .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . the results are shown in table 2 . when using p2 alone , the results are comparable to those using coatt .
5 presents the results on hard and soft alignments . the results are summarized in table 5 . the hmd - f1 model outperforms all the other models in terms of bert scores .
3 presents the results of the direct assessment and de - en evaluations . the results are summarized in table 3 . the best results obtained by baselines are obtained using ruse ( * ) and sent - mover ( * ) on the test set . the best performance is obtained using bertscore - f1 on both test sets .
3 presents the bagel and sfhotel scores on the validation set . the results are presented in table 3 . the bertscore - f1 scores are significantly better than those of bleu - 1 , but are comparable to those of other baselines .
results are shown in table 1 . the leic scores are reported in bold . leic score - recall is reported in table 2 . however , the results are not consistent across all metrics , indicating that the lecithin scores are not predictive of future performance .
results are shown in table 3 . we observe that the m0 model outperforms the m1 model in terms of semantic performance . the results show that when trained on the shen - 1 dataset , the model performs better than the previous state of the art .
results are presented in table 3 . we observe that the semantic preservation and semantic preservation features are the most important aspects of semantic preservation . the results are summarized in tables 1 and 2 . semantic preservation features have the most significant impact on semantic preservation , while semantic preservation has the smallest impact .
5 presents the results of human sentence - level validation . the results are summarized in table 5 . our model outperforms all the previous methods in terms of accuracy , with the exception of predicate accuracy .
results are shown in table 3 . we observe that the m0 model outperforms the m1 model in terms of grammatical accuracy . in fact , it outperforms both the m2 model and the m3 model by a margin of 3 . 5 points .
results on yelp sentiment transfer are shown in table 6 . our best models achieve higher bleu than those using simple - transfer or unsupervised embeddings . however , the results are slightly worse than those obtained using simple transfer , indicating that there is a need to simplify the transfer process .
2 shows the number of tokens that were correctly predicted to be disfluent as well as the percentage of tokens predicted as disfluencies as well .
3 shows the percentage of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - content ) . the fraction of tokens predicted to contain the content word is shown in table 3 . the percentage of tokens that contain the word is in parentheses .
results are shown in table 1 . we observe that the best performing model is the text + innovations model , while the best model is text + texts + innovations . the results are presented in table 2 .
2 shows the performance of our model on the fnc - 1 test dataset compared to the state - of - art embeddings on the cnn - based sentence embedding dataset . our model significantly outperforms all the other methods in terms of accuracy .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the best performing method is burstysimdater .
3 shows the accuracy ( % ) of our method with and without attention . the results are shown in table 3 . our method outperforms all the other methods in terms of word attention .
results are shown in table 1 . the best performing model is the jnn model , which outperforms all the other models in terms of performance .
3 presents the results of our method . our method outperforms all the other methods in terms of identification and classification . the results are presented in table 3 . in all but one case , the identification method is better than the other method . in both cases , the method is more accurate in both cases .
results are shown in table 1 . all models trained on english - only - lm outperform all the other models except for those trained on spanish - only . the results are summarized in table 2 .
4 shows the results on the training set and on the test set using discriminative training with only subsets of the code - switched data .
5 shows the performance of our model on the dev set and the test set , compared to the monolingual model . the results are summarized in table 5 .
shown in table 7 , type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . precision ( p ) , recall ( f1 ) and f1 - score ( f ) are all statistically significant improvements over the previous state of the art model .
5 shows the precision ( p ) , recall ( f1 ) and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . the results are shown in table 5 .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . the hpcd approach relies on syntactic embeddings obtained by embedding skipgram vectors in wordnet . it uses syntactic - sg embedding as the base embedding layer . the results on the original paper are summarized in tables 1 and 2 . these results show that the semantic embedding layers obtained by glove - retro are beneficial for wordnet and wordnet 3 . 1 , but do not improve the performance of wordnet over the original embedding set .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are presented in table 2 .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 .
2 shows the results of domain tuning for image caption translation . our model outperforms all the other models in terms of bleu % scores . subsfull domain tuning improves the multi30k model by 3 . 5 points .
results are shown in table 1 . subdomain - tuned subs1m outperforms all the other models in the en - de test set , the results are reported in table 2 . the results are presented in bold , with the exception of mscoco17 , which shows that the domain - tuning performance is beneficial for both datasets .
4 shows bleu scores in terms of multi30k captions . the results are summarized in table 4 . the best results with the best captions are shown in bold . adding automatic captions ( dual attn . ) helps the model to better interpret the image captions in a better manner .
5 compares the results of our approach with the best state - of - the - art systems . we observe that enc - gate and dec - gate significantly improve the bleu % scores for the en - de model .
results in table 3 show that subs3m is better than subs6m in terms of multi - lingual features . the results in en - de are shown in table 4 . in the en - fr setting , the performance improvements are comparable to subs2m , but the improvements are more pronounced in the text - only setting . also , the improvements in the semantic features are less pronounced on the enfr setting .
results are shown in table 1 . the best results are obtained on mtld and en - fr - ff , while the worst performance is obtained on tftr . table 1 shows the results for both systems .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 shows the results of training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) show that the system is better than the previous state - of - the - art rev system .
results on flickr8k are shown in table 2 . the vgs model outperforms all the other models in terms of recall @ 10 ( % ) and f1 scores .
results on synthetically spoken coco are shown in table 1 . our model outperforms all the previous models in terms of recall @ 10 ( % ) with a slight improvement .
1 shows the results of the different classifiers compared to the original on sst - 2 . we report further examples in table 1 . for example , cnn turns in a < u > screenplay that is very clever at the edges ; it ’ s so clever you want to hate it . rnn also has a similar effect .
2 shows the results of fine - tuning on sst - 2 . the results indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning . the results are shown in table 2 . for example , there is a noticeable drop in the percentage points with respect to the original sentence .
results in table 3 show that the sentiment gains when the negative labels are flipped to positive . this shows that sentiment increases with respect to the original sentence , and vice versa .
results are presented in table 1 . the results are summarized in bold . our results are consistent across all three categories , with the exception of the positive category , where our results are more consistent .
