2 shows the performance of the treelstm model on the recursive framework , with the large movie review dataset , and tensorflow ’ s iterative approach performing the best on the training dataset .
shown in table 1 , the balanced dataset exhibits the highest throughput compared to the linear dataset when the batch size increases from 1 to 25 . the difference between the balanced and the linear datasets shows that there is only a small room of performance improvement left , w . r . t parallelization .
2 shows the performance of the max pooling strategies for each model with different representation . the maximum pooling strategy consistently performs better in all model variations . as shown in table 2 , the max pooling strategy outperforms all the other models except the sigmoid model , which has different representation size .
1 shows the effect of using the shortest dependency path on each relation type . the best f1 ( in 5 - fold ) with sdp shows the f1 of the relation type without sdp . we also show that using sdp improves f1 by 3 . 5 points compared to the macro - averaged f1 . finally , we show the performance of the best relation type with the smallest dependency path .
results are shown in table 3 . y - 3 outperforms y - 2 in terms of f1 and r - f1 by a significant margin . on the other hand , the difference in f1 is small , and the difference between the two is large . the difference between y - 4 and y - 5 shows that the difference is small .
3 presents the results of our test set on the test set . our results are presented in table 3 . the results are shown in bold . as expected , the results are significantly better than those of mst - parser and mate . in addition , the performance improvement is comparable to that of f1 and f1 .
4 shows the c - f1 ( 100 % ) in % for the two indicated systems ; for the lstm - parser system , it is 60 . 62 ± 3 . 54 % , and 69 . 24 ± 2 . 87 % for the paragraph system .
3 shows the performance of the original and the original models on the test set . the results are presented in table 3 . the original model outperforms the original model by a significant margin . as expected , the results are slightly worse than the original , but still comparable to the original .
shown in table 1 , we compare the original and the cleaned e2e data with the original ones . we show the number of distinct mrs , total number of textual references , and number of slot matching scripts as measured by our slot matching script , see section 3 . further , we show the percentage of instances that are distinct from the original , and the percentage that are not .
3 presents the results of the original and the original experiments . the results are presented in table 3 . the original and original models outperform the original models in all but one of the three cases . in addition , the original model outperforms the original on all three of the test sets except for the one in which the tgen + model is used . table 3 shows the performance of the original model on the test set . as expected , the results are slightly worse than the original .
4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . we found a total absolute number of errors of 0 . 05 , 0 . 01 and 0 . 03 , respectively , for each of the 100 instances we found a slight disfluency .
results are presented in table 3 . all models outperform all the other models except for seq2seqk ( konstas et al . , 2017 ) and tree2str ( song and cohen , 2018 ) . all model outperforms all the others except snrg and pbmt ( pourdamghani et al . 2017 ) . graphlstm achieves the best performance on the external and external datasets . the only exception is snrg , which achieves the worst performance on both datasets .
2 shows the bleu scores on amr17 . the model size of the ensemble models is shown in table 2 . as expected , the model size is significantly larger than the original seq2seqb model . we observe that the ensemble model performs better than the single model .
3 presents the results for english - german and english - czech . the results are presented in table 3 . our model outperforms the previous best - performing models in both languages by a significant margin . we observe that the english - language models outperform the german models in terms of bias and cias .
5 shows the effect of the number of layers inside dc on the performance of the model . table 5 shows that dc has a significant effect on the quality of the layers inside the dc . we observe that when the layers are aligned , the effect is less pronounced than when they are aligned with the other layers .
6 shows the performance of gcns with residual connections with baselines . the results are shown in table 6 . rc + la ( 2 ) shows that the gcn has residual connections to the residual connections . as expected , the results are slightly worse than those of dcgcn2 ( 4 ) . however , the performance is still comparable to the previous state - of - the - art gcn .
3 shows the performance of the dcgcn model on the test set . the results are shown in table 3 . we observe that the average number of dms is significantly higher than the average bms , indicating that the model is more suitable for the task at hand . as expected , the average dms are significantly lower than those of other models .
8 shows the ablation study for amr15 on the dev set . - { i } dense blocks denotes removing the dense connections in the i - th block . further , - { 3 , 4 } dense connections denote removing the layers of the dense blocks . note that - { 4 , 3 , 4 , 5 } dense block denotes removing all the layers that are not dense . table 8 also shows the performance of our model on amr14 on the dev set .
9 shows the ablation study for the graph encoder and the lstm decoder . encoder modules have the best performance in terms of coverage , with a significant drop in performance compared to the previous state - of - the - art encoder model . the best performance is achieved on the global encoder with a gap of 3 . 5 points in coverage . on the lsm decoder , the gap is 2 . 6 points .
7 shows the performance of our initialization strategies on probing tasks . our model outperforms all the other approaches except subjnum and glorot , which are comparable in terms of precision .
3 presents the results of our method on the subtraction task . the results are presented in table 3 . the results show that our method outperforms all the other methods in terms of depth and subtraction . as expected , the results are similar across all the subjnum tasks except for the one in which our method obtains the best performance .
3 presents the results of our model on the subj and mpqa datasets . our model outperforms all the other models except for subj , which is more comparable in terms of performance . subj also outperforms both the sst2 and sst5 datasets by a significant margin . sick - e outperforms subj by 3 . 6 % on the mrpc dataset .
3 shows the relative performance of our models on unsupervised downstream tasks attained by our models . we show the relative change with respect to hybrid compared to cbow . our model outperforms both the hybrid and the hybrid models in terms of performance .
8 shows the performance of our initialization strategies on supervised downstream tasks . our model outperforms all the other models except subj and mpqa , which are comparable in performance to subj . we also outperform all the models except sick - e .
6 shows the performance for different training objectives on unsupervised downstream tasks . the best performance is on sts12 and sts14 , respectively , compared to cmow - c , which shows a better performance .
results are shown in table 3 . we observe that cbow - c outperforms all the other models in terms of depth and subjnum . the difference between the two models is due to the fact that the subsjnum and subjnum are subtracted from the length of the subjnum . as expected , the difference between these models is small , but the difference is large .
3 presents the results of our model on the subj and mpqa datasets . our model outperforms all the other models except sst2 and sst5 on both datasets except for subj , which outperforms sst1 and sick - b .
results are shown in table 3 . the system outperforms all the other systems in terms of e + org and per . in [ italic ] e + e + per , the system performs better than all the systems except for the ones that do not use the org feature . we observe that all the system models do not rely on org features , which indicates that the system does not need to rely on e + or per features to perform well . further , we observe that both the system and the system that performs the best in e + and per are superior to the system trained on the original e + loc .
2 shows the f1 scores on the test set under two settings . name matching improves the e + p score by 3 . 5 points compared to the previous state - of - the - art model . supervised learning improves e + f1 score by 2 . 3 points compared with the baseline . the difference between the baseline and the baseline is due to the fact that the model trained on the model outperforms the baseline model in terms of p1 and e + r scores .
6 : entailment ( ent ) and ref ( ref ) in the model outperform all the other models except g2s - gin , which outperforms all the models except s2s .
results are shown in table 3 . the best models outperform the best models in terms of bleu and meteor scores . we observe that the best model outperforms the best ones in both categories . note that the ldc2015e86 and ldc2017t10 scores are significantly better than those of the other two models .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . bilstm significantly outperforms the other two models on the development set .
results are shown in table 3 . our model outperforms all the other models in terms of sentence length and sentence length . we observe that the s2s - ggnn model significantly improves sentence length by 3 . 5 % compared to the g2s model . the results are reported in table 4 . as expected , the model significantly outperforms the model in sentence length , but it does not improve sentence length significantly compared to other models .
shown in table 8 , the fraction of elements in the output that are missing in the input that are present in the generated sentence ( miss ) , for the test set of ldc2017t10 . we show that the model outperforms the reference sentences by a significant margin .
4 shows the accuracy of our approach on the smaller parallel corpus ( 200k sentences ) . our approach outperforms all the other approaches except for pos , which is more accurate .
2 shows the accuracy of our model with baselines and an upper bound . our model outperforms all the other models except word2tag , which has a lower bound .
results are presented in table 3 . table 3 shows the performance of the pos tagging accuracy and f1 scores for each of the four test sets . the performance of both sets is comparable to those of the other two baselines . in the case of the fr and fr datasets , the performance is comparable with the fr datasets .
5 shows the accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages .
8 shows the performance of the attacker on different datasets . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 8 .
shown in table 1 , training directly towards a single task can improve the performance for pan16 .
2 shows the effect of the protected attribute leakage on the performance of task and unbalanced task splits . the results are shown in table 2 . we observe that the presence of the protected attribute leakage leads to a drop in performance for both task splits .
3 shows the performance on different datasets with an adversarial training . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 3 . in the case of pan16 , the difference is 0 . 5 points compared to 0 . 9 points for pan16 . on the other hand , we see a slight improvement in performance on pan16 compared to pan16 with a similar training .
6 shows the concatenation of the protected attribute with different encoders . for example , embedding rnn with a different encoder can improve the performance of the embeddings .
results are shown in table 3 . this model outperforms all the other models in terms of base and finetune performance . the results show that this model improves upon the previous state - of - the - art lstm model by a significant margin . however , it still outperforms the previous model in both base and finetune performances . in addition , it improves the performance of the current model by 2 . 5x on the original model . further improvements are expected in the second half of the study .
results are shown in table 5 . our model outperforms all the other models in terms of time and distance . the difference between the two models is due to the fact that the lstm model takes longer time to train compared to the previous model . as expected , the difference in time between the baseline and the baseline is small , but the difference is large . we observe that the difference between baseline and baseline time is relatively small , indicating that the model is more suitable for the task at hand .
results are shown in table 3 . our model outperforms all the other models in terms of err performance . we observe that our model improves upon the performance of the original lstm model by a significant margin . the difference between our model and the previous model is due to the difference in performance between the two models . it improves upon our model by 2 . 5x on average compared to the previous state - of - the - art model . on the other hand , our model performs slightly worse than the original model on the yahoo time and yahoo time datasets .
3 shows the bleu score on wmt14 english - german translation task . the model outperforms all the other models except sru and gru in terms of decoding one sentence . in addition , it outperforms the other two models by a significant margin . sru outperforms both gru and sru by a considerable margin .
4 shows the performance of our model on squad dataset . our model outperforms all the models in terms of match / f1 score . we observe that our model performs better than all models except lrn and sru . the performance of lrn is comparable to that of lstm and atr . however , we observe that the performance is slightly worse than lrn . in addition , the sru model performs worse than all the other models except for lrn ( which performs better ) . the results show that the model is more suitable for the task at hand .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number in the reported result . gru also shows the performance of the model in the ner tasks . sru also exhibits the best performance . lrn shows the improvement over the previous state - of - the - art lrn model .
7 shows the performance of our model on snli task with base + ln setting and test perplexity on ptb task with base setting . the results are shown in table 7 .
results are shown in table 3 . the word embeddings outperform the system retrieval word embedding improves the performance of the system by 2 . 5 % compared to the previous state - of - the - art system . sent messages improve the performance by 3 . 5 points compared to previous state of the art systems . oracle retrieval improves the overall performance by 4 . 5 % . in addition , the word embedding feature improves the system performance by 1 . 5 - 2 . 5 % , compared to 2 . 3 - 4 . 6 % on the previous state of the art system .
4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best results are shown in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . as expected , the best performance is achieved on the k 1000 and k 1000 datasets , respectively , with the highest standard deviation being 1 . 2 .
results are shown in table 3 . our model outperforms all the other models in terms of performance . we observe that the best performing model is europarl , while the best ones are docsub and docsub . the best performing models are df , docsub , docmax and docmax .
results are shown in table 3 . we observe that our approach outperforms all the other approaches except for the two that we use in corpus . the results are reported in table 4 . in corpus , we observe that the performance of our approach is comparable to those of the other two approaches .
results are shown in table 3 . we show the performance of our models on the test set . our models outperform all the other models except for the two that we tested . the results show that our model outperforms both the df and df models in terms of performance . for the df model , we see that our approach outperforms the df models by a significant margin .
results are shown in table 3 . the results are presented in table 4 . as expected , the results are significantly worse than those of corpus and europarl in terms of depthcohesion . for corpus , the performance is comparable to that of dsim and df , but considerably worse than that of df .
results are shown in table 3 . the results are presented in table 4 . as expected , the results are significantly worse than those of corpus and europarl in terms of depthcohesion . for corpus , the performance is comparable to that of dsim and df , but closer to the baseline .
shown in table 1 , we compare the performance of our model on the validation set of visdial v1 . 0 . we observe that the enhanced model outperforms the original visdial model by a significant margin . the difference between the enhanced and enhanced model is that it performs better in the hidden dictionary learning task .
2 shows the performance ( ndcg % ) of different ablative studies on different models on visdial v1 . 0 validation set . the best performing model is the one using the hidden dictionary learning shortcut .
5 shows the performance on hard and soft alignments compared to hard alignments . the hmd - f1 model outperforms all the other models except for ruse .
3 presents the performance of our model on the direct assessment test set . our model outperforms all the other models on the test set by a significant margin . the results are shown in table 3 . as expected , the results are significantly worse than those of the baseline models .
3 presents the bagel and sfhotel performance on the test set . the bleu - 1 baseline outperforms all the other baselines except for bertscore - f1 and meteor , which both outperform the baseline .
3 presents the metric and baselines scores for the two models . the metric scores are shown in table 3 . leic scores are reported in terms of bertscore - recall and spice scores . bert scores are derived from the leic score , which shows that bert score is superior to leic on the m2 and m2 datasets .
results are shown in table 3 . we observe that the m0 model outperforms the m1 model by a significant margin . in particular , it outperforms both the m2 model and the m3 model by significantly improving the performance on the sim model .
results are presented in table 3 . our model outperforms all the other models in terms of transfer quality and transfer quality . the results show that the transfer quality improvements are beneficial for semantic preservation and semantic preservation . we observe that transfer quality improves over semantic preservation , however , transfer quality improvement is less significant than semantic preservation due to the high transfer quality of the semantic preservation datasets . semantic preservation improves upon semantic preservation with the addition of semantic preservation features .
5 shows the results of human sentence - level validation . the results are shown in table 5 . sim and human ratings of semantic preservation are significantly better than those of human . moreover , the performance of sim and pp is comparable to that of human , indicating that the human judgments are superior to the machine judgments .
results are shown in table 3 . we observe that the m0 model outperforms the m1 model by a significant margin . in particular , it outperforms both the m2 model and the m3 model by significantly improving the performance of the shen - 1 model .
results on yelp sentiment transfer are shown in table 6 . the best models achieve higher bleu than those using the same classifier . we also observe that the best models outperform all the other models in terms of acc ∗ . multi - decoder outperforms all the models except for the ones using the best classifiers . we observe that simple - transfer outperforms the best model by a significant margin . our best model outperforms both the best and the worst models in the sentiment transfer task by a considerable margin .
2 shows the percentage of reparandum tokens that were correctly predicted as disfluent , compared to repetition tokens . the number of repetition tokens is shown in table 2 . in the nested disfluencies , the repetition tokens are the most frequently predicted to be disfluent ( i . e . , 1 - 2 ) .
3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - content ) . the fraction of tokens correctly predicted to contain the content word is shown in table 3 . as shown in fig . 3 , the number of tokens that contain the word is significantly higher than the number predicted to have the word contained in the reparandum .
results are presented in table 3 . the model outperforms all the other models in terms of dev and innovations . in addition , the model improves upon the best state - of - the - art model by 0 . 2 points on dev and 1 . 5 points on innovations .
2 shows the performance of our model on the fnc - 1 test dataset compared to the state - of - art embeddings on cnn - based sentence embedding . our model achieves the best performance on both test datasets .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the best performing alternative is burstysimdater , which significantly outperforms all previous methods .
3 shows the performance of our model with and without attention . we show the effectiveness of word attention and graph attention for this task . in table 3 , we show that our model obtains the best performance on word attention compared to the other two models .
3 shows the performance of our model on each stage of the test set . our model outperforms all the other models in terms of performance on all stages except for the argument stage , where it performs best .
3 presents the results of cross - event training . our method outperforms all the previous methods in terms of identification and classification . cross - event feature - based training outperforms both the previous method and the previous one by a significant margin . in the case of the trigger feature - aligned training set , our approach outperforms the previous two methods in both cases .
results are shown in table 3 . all models are comparable in terms of dev perp , dev wer and test acc . we observe that all models outperform all models except for the spanish - only model , which is comparable in features and features .
4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . we show the performance of fine - tuned with only the train dev and full train test .
5 shows the performance on the dev set compared to the monolingual set , and on the test set , according to the type of gold sentence in the gold sentence .
7 shows the performance of type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . precision ( p ) , recall ( f1 ) and f1 - score ( f ) show statistically significant improvement over the previous state of the art model .
5 shows the precision ( p ) , recall ( f1 ) and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset ( table 5 ) . note the significant improvement in precision ( p ≤ 0 . 05 ) over the previous state - of - the - art model .
results on belinkov2014exploring ’ s ppa test set . syntactic - sg embeddings are derived from the original wordnet , and they are used in wordnet 3 . 1 . glove - retro embedding is derived from syntactic skipgram , and it uses syntactic sg embedding . the results on the original paper are shown in table 1 .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 . our system outperforms all the other models in terms of uas and ppa accomplishment .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing context sensitivity and sensitivity ( ppa ) is shown in table 3 . we observe that the loss of context sensitivity significantly affects the ppa acc .
shown in table 2 , adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun are shown in bold . subsfull and domain - tuned models outperform multi30k models in terms of bleu % score .
3 shows the performance of subs1m on en - de and en - fr . the results are shown in table 3 . subdomain - tuned subs1ms outperform the subs2m model on both datasets . as expected , the performance is comparable to the subs - de model , but the difference is less pronounced on the en - france dataset . moreover , the improvements are less pronounced in the sub - de dataset .
4 shows bleu scores in terms of automatic image captions ( only the best one or all 5 ) . the results with marian amun are shown in table 4 . as expected , the results with the best five captions are significantly worse than those with only the best ones . in the en - de example , the best 5 captions were obtained with the worst ones .
5 shows the bleu % scores for embedding visual information . multi30k + ms - coco + subs3mlm embeddings outperforms enc - gate and dec - gate on the en - de model . the results are summarized in table 5 . we observe that enc - gates outperforms dec - gating on both datasets . in addition , we observe that decoding the visual information improves the performance of the enc - gate model .
3 shows the performance of subs3m on en - de and in - de , respectively , compared to subs6m and subs7m , respectively . sub3m outperforms all the other models in terms of multi - lingual performance , with the exception of mscoco17 , which outperforms the subs6ms model . the performance of the two models is comparable to that of the other two models , however , the performance is slightly worse .
results are shown in table 3 . we observe that en - fr - rnn - ff outperforms all the other models in terms of ttr and mtld . in addition , we observe a significant drop in performance on mtld compared to enfr - ht . the difference between the two models is due to the large difference in performance between the original two models .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used . the number of pairs in each language pair is shown in bold .
2 shows the training vocabularies for the english , french and spanish data used for our models .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) show that the system is better than the previous state - of - the - art rev system . as expected , automatic evaluation scores show a significant improvement over the previous state of the rv system .
2 shows the performance of the vgs model on flickr8k . the results are shown in table 2 . we observe that vgs outperforms segmatch and segmatch in terms of recall performance , we also observe that segmatch performs better than segmatch ,
shown in table 1 , the vgs model outperforms all the other models in terms of recall @ 10 and roco . the difference between the two models is statistically significant .
shown in table 1 , we use the same classifiers as the original on sst - 2 . we report further examples in the appendix . for example , cnn turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . dan < c > shows that the edges of the screenplay are so clever that it shows the shape of the dialog . for rnn , we show that the dialog is so clever , it shows that we want hate hate hate . for dan , we see that dialog is more clever than the original .
2 presents the results of fine - tuning on sst - 2 . the results are shown in table 2 . we observe that the number of words in the original sentence has increased , decreased or stayed the same , indicating that fine tuning has not changed the overall quality of the sentence .
3 shows the change in sentiment with respect to the original sentence in sst - 2 . the results are shown in table 3 . the results indicate that the switch in sentiment from positive to negative has a positive effect on the sentiment score .
results are presented in table 3 . the results are shown in bold . in addition , the results are summarized in table 4 . as expected , the positive effect is less pronounced than the negative effect . however , the negative impact is more pronounced on the performance of the pubmed corpus . we observe that the positive effects are less pronounced when compared to the negative effects of the negative ones . on the positive side , we observe that sift performs better than the positive ones .
