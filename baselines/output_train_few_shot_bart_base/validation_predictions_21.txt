2 shows the performance of our iterative approach on the large movie review dataset . the approach performs the best on inference with efficient parallel execution of the tree nodes , while the iteration approach shows lower performance on training .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left .
2 shows the performance of the max pooling strategies for each model with different representation . our approach achieves the best performance with the maximum number of hyper parameters and the average number of feature maps . softplus also outperforms sigmoid in all model variations . the maximum pooling strategy consistently performs better in the multi - model setup with different number of parameters .
1 shows the effect of using the shortest dependency path on each relation type . it can be seen that macro - averaged models achieve the best f1 ( in 5 - fold ) with sdp as well as the best diff . as shown in table 1 , macro - adapted models generally achieve better f1 scores than those using sdp . also , the performance gain from using only one dependency path is less than those obtained using sdp .
results in table 3 show that y - 3 significantly outperforms the state - of - the - art models in terms of f1 and r - f1 .
3 presents the results of our method on the essay level . the results of the best performing method are presented in table 3 . we observe that the approach achieves the highest performance on the paragraph level , surpassing the threshold of f1 by 3 % .
4 shows the c - f1 scores for the two indicated systems ; for the lstm - parser system , it is 60 . 62 ± 3 . 54 and 58 . 24 ± 2 . 87 respectively , compared to the majority performances of the other systems .
3 shows the performance of our system on the original and wrong test sets . the results are presented in table 3 . original and original systems perform better than all the other systems except for the one that has been tested on the wrong test set . for the original test set , we have only managed to improve the bleu score by 0 . 01 points .
shown in table 1 , the original e2e data and our cleaned version are comparable in terms of the number of distinct mrs , total number of textual references , and ser as measured by our slot matching script , see section 3 .
3 shows the performance of original and original test methods . the results are presented in table 3 . original and original test methods have the best performance on both tests . they are comparable in terms of bleu score , accuracy and precision . however , their performance is slightly worse than those of the original .
4 shows the results of manual error analysis on a sample of 100 instances from the original test set . the errors we found were caused by slight disfluencies in the training data , as shown in table 4 . additionally , there was a significant amount of misspelling in the original training data as well as a slight amount of mispelling .
model the performance of all models is reported in table 1 . the best performances are achieved by the state - of - the - art graphlstm ( song et al . , 2016 ) with a gap of 3 . 5 % on the external model compared to the previous best performance on the single model .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points on the model size in terms of parameters , compared to the previous best state - of - the - art model , ggnn2seq .
3 shows the results for english - german and english - czech . the results are presented in table 3 . the best performing model is bow + gcn ( bastings et al . , 2018 ) . the only exception is english - kochi , where the single model performs slightly worse than the other models . on the english - language dataset , the bach et al . ( 2018 ) model outperforms both the previous methods in terms of bias and coding .
5 shows the effect of the number of layers inside the dc block on the performance of the model design . as table 5 shows , for every layer with at least one layer , the model achieves the best performance with a minimum of 50 layers .
6 : comparisons with baselines . rc + la denotes gcns with residual connections . as shown in table 6 , when gcn is trained on residual connections , the gcn performs better than all baselines except dcgcn2 . however , when trained on multiple clusters , the improvement is only 2 . 5 percent .
model f1 shows the performance of dcgcn models when trained on state - of - the - art datacenters . the results are presented in table vii . however , the results are slightly worse than those reported by previous models .
8 shows the ablation study for amr15 . - { i , 4 } dense blocks denotes removing the dense connections in the i - th block . it is clear from table 8 that removing these dense blocks helps the model to improve its performance .
9 shows the ablation study for the graph encoder and the lstm decoder . encoder modules used in table 9 show that the global network and the multi - decoder have the best performance .
results for initialization strategies on probing tasks are shown in table 7 . our paper shows that our approach obtains the best performance on all probing tasks . it achieves the best results on the subtense and threshold tasks , while outperforming the other approaches by 3 - 4 points .
1 and table 2 summarize our results on the subtense and subtense subtasks . we observe that cbow / 400 has the best performance on both subtasks and on the deep subsampling tasks . it also outperforms the other approaches , such as topconst and coordinv with a boost of 3 . 5 points . moreover , it surpasses the previous state - of - the - art on all subtasks except for the one that has the worst performance .
3 shows the performance of our model compared to other methods . our model outperforms all the other methods except for the one that cmp relies on . cbow / 784 shows a significant performance drop compared to sst2 and sst5 on mrpc , while sick - r performs better on ssts - b .
results on unsupervised downstream tasks attained by our models are shown in table 3 . our model outperforms both hybrid and cmp in all but one of the four cases .
8 shows the performance of initialization strategies on supervised downstream tasks . our paper shows that our approach improves upon the state - of - the - art model sick - e by 3 . 8 points in the standardization task , outperforming all the alternatives except subj and mpqa .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the cbow - r model outperforms all the other models except for the one that is supervised .
performance of cbow - r compared to other methods is presented in table 4 . the topconst method outperforms all the methods except for the one that requires a significant increase in precision . it also exceeds the strong baselines like coordinv and topconst due to the higher precision of the subjnum and the higher bound size of the object . cbow improves performance on both subsets of the training set by 3 . 8 points .
subj and sick - r are comparable in terms of mrpc performance . cbow - r outperforms all the methods except sst2 and sst5 except for subj , which is closer in size to mpqa . on the other hand , it has the advantage of training on a larger corpus , which underscores the competitiveness of subj .
system performance in [ italic ] e + per and e + misc scores are shown in table 4 . supervised learning systems outperform all systems except for the one that does not use the org feature . name matching and multi - task learning systems achieve the best results in both systems with respect to cross - domain learning , we observe that the combination of all org features and the per scores is the most important part of the system performance improvement for theitalic e + per scores .
results on the test set under two settings are shown in table 2 . our system outperforms all the models except for the one that trained on the word2vec dataset . this confirms the viability of supervised learning in the system .
6 shows the entailment results ( ent ) for all models except those that do not belong to the " g2s " category . ref and ref significantly outperform ref in all but one of the cases .
results in table 3 show that the models trained on the proposed ldc2017t10 outperform the state - of - the - art models on all metrics except for the keystroke of " bleu " .
3 shows the results on ldc2015e86 test set when trained with additional gigaword data . our model outperforms the previous state - of - the - art models on all three benchmarks .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . bilstm significantly boosts the performance of the model compared to those without .
results are presented in table 4 . we observe that for all models , the average number of frames per sentence is significantly lower than that of g2s - gin , indicating that the model is more suitable for the task at hand . in particular , we observe lower precision on the graph diameter and average sentence length .
shown in table 8 , the fraction of elements missing in the output that are present in the reference sentence ( g2s - gin ) , for the test set of ldc2017t10 . it is clear from table 8 that the use of token lemmas in the model can improve the picture quality for the gold analogy task .
4 shows the performance of our approach with respect to target languages . we use the 4th nmt layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) .
2 shows the pos and sem tagging accuracy with baselines and an upper bound . accuracies are shown in table 2 . we use unsupervised word embeddings as the classifier , and word2tag as the upper bound encoder .
results are presented in table 4 . table 4 shows that our approach significantly improves the performance of our model compared to the previous methods .
5 shows the accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . as table 5 shows , for bi and res , pos achieves an accuracy of 87 . 9 % and 94 . 5 % respectively over the english target languages , respectively , with a boost of 3 . 6 % in accuracy over res .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ , a difference of 2 . 7 % in the performance of pan16 .
results in table 1 show that the training directly towards a single task can improve the performance for pan16 participants .
2 shows the status of the protected attribute leakage in the context of balanced and unbalanced data splits . dial models trained on pan16 consistently outperform the state - of - the - art model in both instances . however , the asymmetric nature of the data splits shows a significant drop in performance for the classifier .
performance on different datasets with an adversarial training is shown in table 3 . the performance on the training datasets is the difference between the performance of the trained classifier and the corresponding adversary ’ s accuracy .
6 shows the concatenation of the protected attribute with different encoders . embedding is more difficult than embedding it in a different encoder .
results of experiment 1 are presented in table 1 . our model outperforms the previous stateof - the - art models on all three metrics . the results show that our approach achieves the best performance on both the training and the finetune tasks . we observe that the lstm model performs on par with the original wt2 model , but on the wt2 dataset , it performs slightly better than the original model . it is clear from the results that this approach is superior to both the original and the original models .
performance of our model is reported in table 4 . the results of experiment 1 show that our approach significantly improves the performance of the model when trained on a larger dataset . it also improves the bert time by 3 . 5 % over previous models .
3 shows the performance of our model compared to the previous state - of - the - art models . our model outperforms all the other models except for the one that is used in the amafull time dataset . the results of " amafull time " and " polar time " are shown in table 3 .
3 shows the bleu score on wmt14 english - german translation task . our model outperforms all the state - of - the - art models except for the one that uses the gold - based sru decoder .
4 shows the performance of our model on squad dataset . the results published by wang et al . ( 2017 ) show that our model significantly improves match / f1 score with the parameter number of base . however , it does not improve significantly over the model of lstm , which shows the diminishing returns from increasing parameter number .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the reported result lample et al . ( 2016 ) . it also indicates the performance of the model when trained with a parameter number greater than the number of parameters .
shown in table 7 , the accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting .
system retrieval and system re - evaluation are presented in table 4 . word - based systems ( mtr ) outperform human on all three metrics . the word - based system approach is particularly effective for system evaluation , with an absolute improvement of 2 . 5 % over the previous state - of - the - art systems . system retrieeval is particularly useful for human , with a 2 . 4 % boost over the performance of previous state of the art systems when using the word " evaluation " .
4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 8 . the best performance among all systems is highlighted in bold .
3 shows the performance of all the models trained on the corpus dataset . the results are presented in table 3 . our model outperforms all the other models except for the one that is used on the df dataset . it is clear from table 3 that the approach used on corpus is superior to the others on all the three datasets .
3 shows the performance of all the models trained on the corpus dataset . the results are presented in table 3 . our model outperforms all the other models except for the one that is used on the docsub dataset . on the other hand , our model performs slightly worse than the others . for example , it obtains the best performance on docsub and docsub datasets .
3 shows the performance of all the models trained on the corpus dataset . our model outperforms all the other models except for the one that is used on the docsub dataset . the results are summarized in table 3 . for corpus , our model performs slightly worse than the others . however , it outperforms both the df and docsub datasets .
3 shows the performance of our model compared to the previous best state - of - the - art models . our model achieves the best performance on all metrics with a gap of 1 . 78 points in the dimensioncohesion metric . it outperforms both the df and docsub metric by a significant margin .
3 shows the performance of our model compared to the previous best state - of - the - art models . our model achieves the best performance on every metric with a gap of 1 . 5 points . our joint model outperforms all the other baselines except for the one that is strictly related to the word embeddings . europarl also achieves a better performance on docsub and hclust datasets .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf is the enhanced version of the original visdial model . the results are shown in table 1 . it is clear from the table 1 that the enhanced variant of the lfn model performs better than the original model .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . the best performing model is lrva , which relies on the history shortcut .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . we observe that the hmd - prec model outperforms all the other models except for those using bert .
3 presents the results of our approach on the test set of eureor . our approach obtains the best performance on all metrics with a minimum of 0 . 5 bertscore - f1 score . the results are presented in table 3 .
3 presents the bagel and sfhotel performance on the test set . the results are summarized in table 3 . the baseline bleu - 1 improves significantly over the baseline on all metrics with a gap of 3 . 5 points .
performance of the models according to these baselines is reported in table 4 . the results are summarized in terms of leic score - recall , which means that the models trained on the m2 dataset generally perform better than those using spice . however , leic scores are slightly worse than spice , indicating that the reliance on pre - trained models may have a significant impact on performance .
results are shown in table 4 . we observe that for all models , the presence of para + para + lang improves the performance . however , for the m1 model , it does not improve significantly .
3 presents the results of our model on the transfer quality and semantic preservation tasks . our model outperforms the previous stateof - the - art models in all three domains . the results of the model with the highest transfer quality are reported in table 3 . semantic preservation tasks are particularly difficult to solve with a single set of features . however , the best results are obtained on the semantic preservation task , which underscores the importance of the semantic preservation task in the multi - domain setup .
5 shows the results of human validation on three of the four datasets . the first example shows the performance of our system on the test set of yelp , followed by the validation set of sim . the second example shows that our system performs well on both test sets .
results are shown in table 4 . the performance of m1 + m2 + m3 + m4 + m5 + m6 + m7 + m0 + lang is comparable to that of sim or pp . however , for m6 , the performance gap is narrower than that of m3 : m0 [ italic ] + para + lang .
results on yelp sentiment transfer are shown in table 6 . our best models achieve higher bleu than those using simple - transfer , but the highest acc ∗ score is achieved by using the classifier multi - decoder embeddings achieve the best bleus , but only slightly outperforming the best model using the same classifier . we also observe that the use of classifiers in the transfer domain severely affects the model ' s ability to distinguish between transfer and non - sentiment contexts .
statistics for nested disfluencies are shown in table 2 . reparandum length is the average number of tokens predicted to be in disfluent state . it is clear from the table 2 that the repetition disfluency contributes to the overall accuracy of the model .
3 shows the relative frequency of rephrases correctly predicted as disfluent for both the reparandum and the repair ( table 3 ) . the fraction of tokens predicted to contain a content word is shown in parentheses , indicating that the disfluencies in the reparandum are relatively small .
results are presented in table 4 . we observe that for all models that use text + innovations , the best performance is achieved on the model with a minimum of 0 . 2 boost in α .
performance comparison with state - of - art algorithms on the fnc - 1 test dataset . our model achieves the best performance with a low f1 score of 0 . 43 on the simple f1 test set .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the unified model significantly outperforms all previous models .
3 shows the performance of our method in the context of word attention and graph attention . it achieves a comparable performance to the state - of - the - art model using ac - gcn attention .
3 shows the performance of all models trained on the same training set . our model outperforms all state - of - the - art models except for the one that performs on the " trigger " stage . on the " n " stage , our model performs better than all the models except jvmee .
3 shows the performance of our method in the event of a single event . our method outperforms all state - of - the - art methods in terms of both event identification and event classification . in particular , the method has the advantage of cross - event performance .
can be seen in table 4 . all models trained on the spanish - only - lm setup outperform all the models except for the ones that do not use the word " attention " .
results on the train dev and test set are shown in table 4 . fine - tuned training with only subsets of the code - switched data in it achieves a significant improvement over the previous state - of - the - art model , on the training set .
performance on the dev set and on the test set is shown in table 5 . fine - tuned - disc models outperform monolingual models on both the dev and test set .
shown in table 7 , type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvement in precision ( p ) , recall ( f1 ) and f1 - score ( f ) for using type - based gaze features is statistically significant improvement over the baseline .
5 shows precision ( p ) , recall ( f1 ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . note the significant improvement in precision ( p ≤ 0 . 01 ) over the type combined dataset , as shown in fig . 5 .
results on belinkov2014exploring ’ s ppa test set . the hpcd approach uses syntactic - sg embeddings obtained by using autoextend rothe and schütze ( 2015 ) on glove . the results on the original paper are shown in table 1 . we use syntactic and semantic embedding as the base for wordnet and wordnet 3 . 1 . as shown in fig . 1 , type embedding is the most important part of the semantic - sg architecture , and it improves the performance of wordnet over the syntactic embedding .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results from table 2 show that the combination of oracle pp and lstm - pp pre - trained models improves the model ' s ppa acc . by 3 . 5 points .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . note the significant drop in ppa acc . from 90 . 7 % to 90 . 9 % for the model , which shows the diminishing returns from removing context sensitivity .
2 shows the performance of domain - tuned models compared to multi30k models . the results are shown in table 2 . adding subtitle data and domain tuning for image caption translation ( bleu % scores ) with subsfull domain tuning improves the model performance by 3 . 5 points .
results are shown in table 4 . subdomain - tuned subs1m models outperform all the other models in terms of en - de models except for those using mscoco17 , which shows the diminishing returns from domain tuning . table 4 : a , b , c , d , a , e , t . subdomain tuning improves performance for all models with at least one subdomain tuning .
4 shows bleu scores in terms of automatic captions ( the best one or all 5 ) . the results with marian amun are shown in table 4 . the model using the multi30k feature set outperforms all the models except for the one that has the best image captions in the final set .
5 compares the performance of various approaches for integrating visual information . we observe that enc - gate and dec - gate achieve the best results ( bleu % scores ) on the largerickr16 and mscoco17 datasets . further , we observe that the approach developed by multi30k + ms - coco + subs3mlm achieves the best performance ( 37 . 40 % ) on the largeickr17 dataset .
performance of subs3m compared to subs4m is presented in table 4 . the best performances are obtained on the en - de and on the flickr16 datasets , while the best performance is achieved on the mscoco17 dataset . sub3m also improves upon the performance of the single - domain model with the addition of the multi - domain feature set . multi - domain features improve performance by 3 . 5pp over subs4
performance on mtld compared to en - fr - ff is reported in table 4 . the results of the best performing model are reported in tables 4 and 5 . table 4 summarizes the results of our model on the word analogy task . the results are summarized in table 6 .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 : training vocabularies for the english , french and spanish data used for our models . the results are shown in tables 2 and 3 .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) for the system reference are shown in table 5 . the system reference obtains ter scores comparable to the en - fr - rnn - rev baseline .
2 shows the vgs performance on flickr8k . the model trained on chrupala2017representations is comparable to segmatch in terms of recall @ 10 , but achieves a lower recall rate .
results on synthetically spoken coco are shown in table 1 . the model trained on the embedded embeddings of chrupala2017representations is significantly better than the similarly supervised audio2vec - u model .
1 shows the results of different classifiers compared to the original on sst - 2 . for example , orig < cao et al . ( 2017 ) turns in a < u > screenplay that is slightly curved at the edges and it ’ s so clever you want to hate it . similarly , dan ( 2016 ) also shows very similar results . since the embedding of rnn in the screenplay is so clever , we want hate hate it to turn in a screenplay as well .
2 shows the results for part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . these results show that the value of goodness is relatively high for the two types of speech , which indicate the effectiveness of our approach .
shown in table 3 , the effect of the flipped switch on sentiment is less pronounced in sst - 2 than in the case where negative labels are flipped to positive .
results of experiment 1 are presented in table 1 . it is clear from the table that the use of word embeddings improves the interpretability for both positive and negative evaluations . however , it does not improve the performance for both negative and positive evaluations . this suggests that more research should be done to further improve interpretability .
