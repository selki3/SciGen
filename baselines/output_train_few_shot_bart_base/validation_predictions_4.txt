results are presented in table 2 . the recursive approach performs the best on training , while the iterative approach shows the best performance on training .
results are presented in table 1 . the balanced dataset exhibits the highest throughput when the batch size increases from 1 to 25 . however , it does not improve as well as the linear dataset , which exhibits the lowest throughput .
results are presented in table 2 . the max pooling strategy performs best in all models with different representation sizes . it achieves the best performance in all model variations .
results are presented in table 1 . the shortest dependency path on each relation type is used to improve the performance of the relation type . it is shown that the shortest one - step dependency path achieves the best f1 in 5 - fold .
results are presented in table table 3 . y - 3 : y - 3 shows the performance of y - 2 : y and y - 4 : y . the results are shown in table 3 in table 4 .
results are presented in table table 2 . the results of our test set are shown in table 3 . our results show that mst - parser achieves the best performance on the test set . our test set achieves the highest performance on all test sets . we observe that the results are comparable to those obtained on the previous test set , with the exception of the one in which the results were significantly lower than the previous set .
4 shows the performance of the lstm - parser and stagblcc systems over the runs given in table 2 . the performance of both systems is significantly lower than that of the other systems .
results are presented in table table table we show the results of our test set . the results of the test set are shown in table 2 . the results show that the original model is more accurate than the original . we observe that the results are comparable to that of the original , but the original is slightly less accurate .
results are presented in table 1 . the original and the cleaned versions of the e2e data are shown in the table 1 . both the original and cleaned versions have the highest number of distinct mrs and the lowest number of textual references . the cleaned version has the highest percentage of mrs , with the lowest percentage of residual mrs in the cleaned version .
results are presented in table table table we show the results of the original and the original tests . the results are shown in table 1 . original and original results are summarized in table 2 . original results show that the original results are significantly better than the original , but the original result is still slightly worse than original results .
results of manual error analysis of tgen on a sample of 100 instances from the original test set . table 4 shows the absolute number of errors we found in the original training set . in particular , we found slight disfluencies in both the original and the new training sets . in both cases , the errors in both training sets were significant .
results are presented in table table table 2 . table 2 shows the results of our experiments . table 1 shows the performance of our models in table 1 . all models are shown in table 2 , with the exception of seq2seqk ( song et al . , 2017 ) and tree2str ( konstas and cohen , 2018 ) . the results of table 1 show that all models are comparable in performance .
results on amr17 are presented in table 2 . the model size is presented in terms of bleu points , which indicates that the model size has a significant effect on the performance .
results are presented in table reported in table 2 . the results show that our model outperforms all the other models in terms of english - german and english - czech , with the exception of pb - smt ( beck et al . , 2018 ) .
5 shows the effect of the number of layers inside dc on the performance of the model . table 5 shows that each layer has a significant effect on the overall performance of dc . for example , the layers in dc have a significant impact on performance .
table 6 shows the performance of baselines on gcns with residual connections . we observe that the baselines have a significant effect on gcn performance .
results show that dcgcn ( 2 ) achieves the best performance in all three models , with the best results in all four models .
results in table 8 show that amr15 has the highest density of dense blocks on the dev set . in table 8 , we observe that the dense blocks have the most density of the dense ones .
results are presented in table table 9 . the graph encoder and the lstm decoder are shown in fig . 9 . we observe that our model outperforms all the other models in terms of co - ordination and co - coverage .
results are shown in table table 7 . table 7 shows the performance of our initialization strategies on probing tasks . we show that our approach achieves the best results on all probing tasks , with the best performance on all tasks .
results are presented in table . table 1 shows the results of our method on the table 1 . table 2 shows the performance of our approach on the table . table 3 shows that our method achieves the best performance on all three metrics . table 4 shows that the method achieves a high performance on both metrics .
results are presented in table . we observe that the cbow / 784 model outperforms all the other models in terms of performance . we also observe that it outperforms both the sst2 and sst5 model by 0 . 2 % and 0 . 4 % respectively on the mrpc model , respectively .
results are presented in table 3 . the cbow model outperforms hybrid on unsupervised downstream tasks attained by our model . cbow improves on both hybrid and hybrid models .
results are presented in table table table we show the results of our model on the sst2 task . our model outperforms all the other models on the mrpc task . the results are shown in table . we show that our model achieves the best performance on the mpc task , which is comparable to that of our previous model .
results are presented in table table table 6 . cmow - c and cbow - r have the highest performance on the unsupervised downstream tasks , with the best performance on sts13 and sts14 . the results are shown in table 6 , and the results are summarized in table 7 .
results show that cbow - r achieves the best performance on all three tests . it achieves the highest performance on both tests . the results are shown in table
results are presented in table table 2 . we observe that cbow - r outperforms all the other models in terms of both mrpc and sst2 . the results are shown in table . we also observe that the cmow - c model outperforms the other two models in the mrpc model by a significant margin . however , we observe that both models outperform the other three models by a large margin .
results are presented in table we show the results of our model in table 1 . the results are shown in table 2 . our model outperforms all the other models in terms of e + org and e + per . the results show that our model achieves the best performance in all three cases . we observe that the model achieves best results in both e + and per .
results on the test set under two settings are shown in table 2 . our system achieves the highest f1 score in all three settings . our model achieves the best f1 scores in both settings . we observe that our model achieves a higher f1 than other models .
results are presented in table table table we show the results of our model on the entailment table 6 . we observe that s2s - ggnn outperforms all the other models in terms of ref and ref , with the exception of the g2s model .
results are presented in table reported in table table table 3 . the results of the ldc2017t10 and ldc2015e86 models are shown in table . the results show that the model outperforms all the other models in terms of performance . we observe that the models outperform all the models except for the ones that do not outperform the model .
results on ldc2015e86 test set are presented in table 3 . we observe that the gigaword data are significantly better than the external data , indicating that the model is more likely to perform better in the external context .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . bilstm improves the performance of the model by 2 . 5 % compared to the previous model . it also improves performance by 3 . 5 % .
results are presented in table 2 . we show that the model outperforms all other models in terms of graph diameter . the models outperform all models except g2s - ggnn , which outperforms both models by a significant margin . in table 2 , we observe that the models are significantly outperforming all models in graph ddi .
results are shown in table 8 . we observe that the g2s - gin model outperforms all the other models in the ldc2017t10 test set by a significant margin .
results are shown in table 4 . our model achieves the highest sem and pos accuracy on a smaller parallel corpus ( 200k sentences ) .
results are presented in table 2 . our model achieves the highest pos and sem accuracy with baselines and an upper bound . we observe that our model outperforms all other models except word2tag , which achieves the best performance .
results show that pos tagging accuracy is significantly better than the previous state - of - the - art system . the results are shown in table
5 shows the pos and sem accuracy with different layers of nmt encoders , averaged over all non - english target languages .
results are shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy on different datasets is shown in the table 8 .
results are presented in table 1 . we observe that our training directly towards a single task improves the performance of pan16 and pan16 on the pan16 task .
results are presented in table 2 . we observe that the difference between the correct and unbalanced data splits is due to the fact that the correct data splits are more balanced than the unbalanced ones .
results are shown in table 3 . we show the performance of pan16 and pan16 on different datasets with an adversarial training . we observe that pan16 outperforms pan16 in terms of word selection and word selection .
6 shows the performance of the embedding guarded attribute with different encoders . the performance of embedded guarded is comparable to that of rnn and rnn .
results are presented in table we show that this model outperforms all previous models in terms of ptb base , finetune base , and sru base . the results show that these models outperform all other models on both the ptb base and the finetune base . however , the results are not statistically significant .
results are presented in table table 3 . the results are shown in table 4 . we observe that the lstm model outperforms all the other models in terms of time and performance .
results are presented in table table we show the results of our model in table 2 . we observe that our model outperforms all the other models in terms of err . we also observe that this model performs better than all the models in table 1 .
3 shows the bleu score on wmt14 english - german translation task . our model achieves the best performance on all three tasks , with the exception of gru .
results published by wang et al . ( 2017 ) are shown in table 4 . our model achieves the best performance on the squad dataset . we observe that our model achieves a better performance than our model on the lstm dataset , which is comparable to that of elmo .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number in the reported result . gru and sru are the only two models to achieve the same f1 scores .
results are shown in table 7 . our model achieves the best performance on snli task with base + ln setting and test perplexity with base setting .
results are presented in table table 2 . word and word are used to describe the results of our experiments . the word and word results are shown in table 3 . we observe that the word and the word responses are significantly better than the word response . in particular , we observe that word and sentence responses are more effective than word responses .
4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best results are shown in table 4 . the best performance is achieved by seq2seq , which achieves a 2 . 3 out of 5 score .
results are presented in table table table we show the results of our experiments . the results of the experiments are shown in table . our results show that our results are comparable to those of the previous results . we observe that the results are similar to those obtained in the previous experiments .
results are presented in table table table we show the results of our experiments . the results of the experiments are shown in table . our results show that our results are comparable to those of the previous results . we observe that the results are similar to those obtained in the previous experiments .
results are presented in table table table we show the results of our experiments . the results of the experiments are shown in table . we observe that our results are comparable to those of our previous experiments . our results show that our approach outperforms all the other approaches except for ted talks .
results are presented in table table 2 . the results are shown in table 3 . our results show that our model achieves the best results in terms of depthcohesion . our model achieves a high score of 1 . 78 out of 1 , 000 , which is significantly higher than our baseline . our system achieves a higher score of 0 . 01 out of 2 , 000 .
results are presented in table table 2 . the results are shown in table 3 . our results show that our model achieves the best performance on both metric and metric metrics . our model achieves a high level of performance on the metric metrics , which we consider to be the best for the metric metric . we also observe that the model achieves better performance on metric metrics than our model .
results are presented in table 1 . lf is the enhanced version of visdial v1 . 0 . it achieves the best performance on the validation set . the results are shown in table 2 .
results are shown in table 2 . p2 is implemented by the visdial v1 . 0 validation set . the results show that p2 improves the performance of the ablative studies on different models . the performance of p2 on both models is comparable to that of the previous model .
5 shows the performance of hmd - prec on hard alignments and soft alignments . the results are shown in table 5 . the hmd - prec performance is comparable to that of ruse .
results are presented in table table table we show the results of our test set . the results of the test set are shown in table reported in table . we observe that our model outperforms all the other models in terms of direct assessment and direct assessment . our model achieves the best performance on all the models .
results show that bleu - 1 and bertscore - f1 are significantly better than meteor - mover on both sets . the results are shown in table . the results indicate that the baselines are significantly more accurate than the baseline .
results are presented in table table table we show that our model achieves the best performance on the meteor and leic test set . our model achieves a high score of 0 . 939 out of a possible 1 , 000 on the meteor test set , which is comparable to the leic - recall set in table 2 . we observe that the model achieves high performance on both sets of test sets .
results are presented in table we show that m0 and m2 are comparable in terms of performance . m0 outperforms m2 on both sim and gm , but m2 outperforms both m2 and m3 on both sim and gm . we observe that the m0 model outperforms the m2 model by a significant margin .
results are presented in table reported in table table 2 . we show the results of our model in table 3 . the results show that our model outperforms all the other models in terms of transfer quality and transfer quality . our model achieves the best transfer quality in both cases .
5 shows the results of human sentence - level validation . the results are shown in table 5 . the results of the human sentence level validation are comparable to that of the machine and human sentences . we observe that the human sentences are more accurate than the machine ones .
results are presented in table we show that m0 and m2 are comparable in terms of performance . m0 outperforms m2 on both sim and gm , but m2 outperforms both m2 and m3 on both sim and gm . we observe that the m0 model outperforms the m2 model by a significant margin .
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu and the highest acc ∗ . our best models achieve the best acc and fu - 1 scores , but the best ones are restricted to 1000 sentences .
results are presented in table 2 . reparandum tokens that were correctly predicted as disfluent are shown in the table 2 . the number of repetition tokens is shown in fig . 2 . we observe that the repetition tokens are more likely to be disfluent .
3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - function ) .
results are presented in table we show that our model outperforms all the other models in terms of dev and innovations . our model achieves the best dev and innovation performance on all three tests .
3 shows the performance of word2vec embedding on the fnc - 1 test dataset . our model achieves the best performance on both the test dataset and the benchmark dataset .
results are presented in table 2 . our unified model outperforms all previous methods on the apw and nyt datasets for the document dating problem .
3 shows the performance of the model with and without attention . the results are presented in table 3 . the model with the highest accuracy is neuraldater , with the lowest accuracy .
results are presented in table table table we show the results of our model on the jvmee and jmee stage . our model outperforms all the other models in terms of performance . we observe that our model achieves the best performance on all three stages of the test set .
results are presented in table table 2 . table 2 shows the results of our method in table 3 . we observe that the method outperforms all the other methods in terms of identification and classification . in table 3 , we observe that all the methods in table 2 show that the system outperforms the previous methods in both cases .
results are presented in table . all models are shown in table reported in table table 1 . we observe that all models are comparable in terms of dev perp and dev wer . all models have the best performance on the test set , but only one model has the highest performance on test perp compared to the other models .
results on the train dev set and on the test set are shown in table 4 . we observe that fine - tuned training with only subsets of the code - switched data can significantly improve train dev performance .
5 shows the performance on the dev set compared to the monolingual set . our model achieves the best performance on both the dev and mono sets .
results show that type - aggregated gaze features have a significant effect on recall , recall and f1 - score .
results are presented in table 5 . for the conll - 2003 dataset , we use type - aggregated gaze features for recall , recall , and recall .
results on belinkov2014exploring ’ s ppa test set are presented in table 1 . glove - retro is a syntactic - sg - based embeddings system , and it uses syntactic skipgram to embed the synset embedding in wordnet , verbnet , and wordnet . it also uses the syntacticskipgram embedding , which improves the performance of wordnet and verbnet . however , it does not improve the performance on wordnet or wordnet by much , as it does on verbnet ( see table 1 ) .
results from rbg are presented in table 2 . rbg has the highest ppa accomplishment on both uas and uas , with the exception of hpcd , which has the lowest ppaaccomplishment .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model .
results are shown in table table table 2 . table 2 shows the results for the en - de and mscoco17 datasets . subsfull domain tuning improves the performance of the multi30k dataset by 3 . 5 points .
results show that subs1m outperforms subdomain - tuned models in both en - de and in - de settings . subdomain tuned models outperform subdomain tuned ones in all but one of the two cases . in both cases , subdomain tuning outperforms domain tuned ones .
4 shows the bleu scores for all models with marian amun . the results are presented in table 4 . the model with the best image captions ( dual attn . ) and the best multi30k ( concat ) are shown in the table 4 .
5 shows the performance of the enc - gate and dec - gate strategies for integrating visual information ( bleu % scores ) . the results are presented in table . we observe that enc - gates are the most effective approach to integrate visual information into the visual information .
results show that subs3m and subs6m are significantly better than subs2m in terms of text - only features . sub3m is superior to subs4m in both terms of performance and performance . we also observe that subs5m is better at embedding visual features into text - based features , which is evident in the performance of subs7m as well .
results show that en - fr - t - ff and en - rnn - ff are comparable in terms of mtld and mtld performance .
3 shows the number of parallel sentences in the train , test and development splits for the language pairs .
results are presented in table 2 . the english , french and spanish vocabularies used for training are shown in the table 2 .
5 shows the bleu and ter scores for the rev systems . the system reference scores for rev are shown in table 5 . we observe that the system reference score is significantly higher than the original reference score .
results on flickr8k are presented in table 2 . vgs is the visually supervised model from chrupala2017representations . we observe that the vgs model outperforms segmatch and segmatch in terms of performance .
results are presented in table 1 . we show the performance of the audio2vec - u model on the coco dataset . we observe that the performance is comparable to that of the previous model .
3 shows the results of the different classifiers compared to the original on sst - 2 . we report the results in table 1 . we show that the rnn classifier improves the performance of the original . it improves the quality of the dialog .
results are presented in table 2 . table 2 shows the results of fine - tuning on sst - 2 . the results are shown in the table 2 . we observe that the number of words in the original sentence has increased , decreased or stayed the same through fine tuning .
results are shown in table 3 . the positive and negative labels in sst - 2 are flipped to positive , while the negative labels are flipped back to negative .
results are presented in table 1 . the results are shown in table 2 . the results of the experiment are summarized in table 3 . we observe that the results are consistent with the results of previous experiments . our results are similar to those of the previous experiments , with the exception of sift .
