2 shows the performance of our iterative approach on the large movie review dataset compared to the traditional recursive approach , which performs better on inference with efficient parallel execution of tree nodes .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , which exhibits the smallest performance improvement .
2 shows the performance of the max pooling strategies for each model with different number of parameters . our system achieves the best performance with a 4 - fold boost in performance compared to softplus . the number of iterations in the validation set increases with the growth of the hyper parameters . the maximum pooling strategy consistently performs better in all model variations . the sigmoid model outperforms all the other models with different representation .
1 shows the effect of using the shortest dependency path on each relation type . our model achieves the best f1 ( in 5 - fold ) with sdp whereas the macro - averaged model does not achieve the best performance .
results in table 3 show that the y - 3 model outperforms the previous stateof - the - art models in terms of recall .
results are shown in table 1 . the results of the best performing model are presented in the table 1 . our model achieves the highest score with 50 % of the entries on the essay level . we also observe that the accuracy of our model is significantly better than those of the other models .
4 shows the c - f1 scores for the two indicated systems ( the lstm - parser and the paragraph system ) over the runs given in table 2 . the mean performances for both systems are lower than those for the other systems .
results are shown in table 1 . the original and the new models perform better than the original on all tests except for the one that has been cleaned . table 1 shows the performance of all models that have been tested on the original dataset . the results are presented in tables 1 and 2 . table 1 summarizes the results for each system . the original model performs slightly worse than the other two , but it has the advantage of being more accurate .
shown in table 1 , the original and the cleaned versions have the highest number of distinct mrs and the average number of instances of ser as measured by our slot matching script , see section 3 .
results are shown in table 1 . original and original test results are presented in bold . the two variants of the original ( tgen âˆ’ and tgen + ) consistently outperform each other on all tests except for the one that has the correct number of parameters . table 1 shows the performance of all test sets with respect to the original score . original scores are slightly better than the other two , but still slightly worse than the original scores .
results of manual error analysis of tgen on a sample of 100 instances from the original test set ( see table 4 ) . we found 22 errors ( 17 . 6 % ) in the original training set , and 15 . 6 % in the slight disfluencies ( 14 . 6 % ) . these errors are mostly caused by incorrect labels .
model performance on the external and internal datasets is reported in table 1 . the best performances are achieved by the dcgcn ( single ) model , which achieves a 25 . 2 % improvement over the state - of - the - art model on the external dataset .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points ( paired t - test ) on the model size compared to seq2seqb ( ours ) by using the same number of parameters .
3 shows the results for english - german and english - czech . the results are presented in table 3 . we observe that the single model performs better than the other models in both languages . the results in english - language and german are statistically significant , with the exception of english - kochi , where the difference in performance between the two models is less pronounced . our joint model outperforms both the single and the multi - language model in english .
5 shows the effect of the number of layers inside dc on the performance of the layers in table 5 . we observe that the smallest layer , i . e . , the layer that contains the most layers , contributes significantly to the overall performance of dc .
results are shown in table 6 . rc + la denotes gcns with residual connections , and dcgcn4 ( 27 ) shows that the residual connections are important for gcn to make good interpretable predictions .
model f1 shows the performance of dcgcn models when trained on state - of - the - art data . the results are presented in table 1 . dcgcnn models generally perform better than other models in terms of both performance and recall . however , when trained only on state of the art data ( dcgcn + 2 ) , the results are less consistent .
8 shows the ablation study results for amr15 ( see table 8 ) . it can be seen that removing the dense connections severely decreases the performance .
table 9 , we show the ablation study results for the graph encoder and the lstm decoder . the results are shown in table 9 . the global encoder has the worst performance , while the global one has the highest performance .
7 shows the performance of our initialization strategies on probing tasks . our paper shows that it is possible to improve the performance by adding more context to the initialization strategies . however , the performance drop is still significant .
are presented in table 1 . the first group shows the performance of our method in terms of subtraction . our cbow / 400 model outperforms all the other methods except for the one that has the better performance on the subtraction test set . also , our h - cmow model has the worse performance on both subtraction and subtraction tests . however , it has the advantage of having a higher precision rate than somo and wc due to the higher recall rate .
1 shows the performance of all models except cmp . our model outperforms all the other models except for the one that cmp has chosen . cbow has the best performance on both mr and mpqa datasets . it also outperforms both the sst2 and sst5 datasets in terms of mrpc score . however , it has the worst performance on the two mrpc datasets . the difference between cmp and cmp is less pronounced on the mrpc dataset , but it is larger on the sick - e dataset .
results on unsupervised downstream tasks attained by our models are shown in table 3 . our model outperforms the hybrid model on all downstream tasks , except for the one that is supervised by cmp . cbow shows the relative decrease in performance with respect to hybrid .
8 shows the performance of initialization strategies on supervised downstream tasks . our paper shows that our approach improves upon the state - of - the - art model on all downstream tasks , outperforming both the mpqa and sst2 tasks .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the best performances are on the sts12 and sts14 datasets , which show that cbow - r performs better on these tasks . however , the performance drops significantly for the other supervised tasks , as shown in table 6 .
results are shown in table 1 . the first set shows that cbow has the best performance on both subtraction and subtraction tests . the second set shows the performance of the method on the subtraction test set . cbow - r shows lower performance than somo , but it has the advantage of having a greater number of parameters . the results are consistent across all metrics , with the exception of the subjnum test set , which shows the diminishing returns from mixing two sets of parameters at once . these results are statistically significant even at the cost of sacrificing too many parameters for a single set .
subj and sick - r perform comparably to other methods in terms of mrpc . our model outperforms all the other methods except for sst2 and sst5 , which both perform better on mrpc and mpqa datasets .
3 shows the e + and per scores of all systems trained on the italic dataset ( including the one that performs best in table 3 ) . the system performs better than all the other systems except for those that do not use the word " match " . the results are summarized in table 3 . the system ' s e + org score ( which shows the performance of the model when combined with the number of instances in the corpus ) is significantly better than any other supervised learning model ( e . g . , in [ italic ] e + per and in ( italic ) e + misc scores . we observe that the combination learning method ( mil - nd ) outperforms all supervised learning methods except for one that does not rely on word matching . this suggests that the quality of supervised learning models can be improved with more training data .
2 shows the results on the test set under two settings . the first set shows the performance of the supervised learning model ( mil - nd ) in terms of e + p score and f1 score . supervised learning models show lower performance than the original model , which shows the diminishing returns from mixing data . the second set shows that the combination of supervised learning and supervised learning models significantly improves the performance . we observe that the accuracy of the model in the two settings is relatively high , indicating that the model performs well in both scenarios .
6 shows the performance of all models that ref and ref exceed ref on the test set . ref significantly outperforms ref in all but one of the cases , where ref exceeds ref .
table 3 , we present the results of the models trained on the ldc2017t10 dataset . the results are presented in table 3 . the models consistently outperform the state - of - the - art models on all metrics , with the exception of the " ldc2015e86 " metric having the worst performance .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 . our model outperforms the previous stateof - the - art models in both external and internal settings .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . bilstm significantly outperforms the other models in terms of both number of frames and type .
results are shown in table 1 . we observe that the g2s model significantly outperforms the other models in terms of sentence length and sentence length . the results are reported in graph diameter metric ( g2s - gin ) , which measures the average sentence length of the two models compared to each other . finally , the averagesentence length of both models is significantly lower than those of the other two models .
shown in table 8 , the fraction of elements in the output that are missing in the input graph that are present in the generated sentence ( g2s - gin ) , which shows the performance of the model when adding those elements to the output graph . note that these tokens are used to represent reference sentences , not output .
4 shows the performance of our method with respect to target languages extracted from the 4th nmt encoding layer . our model obtains the best performance with 96 . 7 % accuracy on a single corpus ( 200k sentences ) .
2 shows the pos and sem tags accuracy with baselines and an upper bound . accuracies are reported in table 2 . we use unsupemb embeddings as the classifier and word2tag as the upper bound tags . the results are presented in tables 2 and 3 .
results are presented in table 4 . our results are summarized in terms of accuracy and precision . our model outperforms all the other methods except for the one that significantly improves the accuracy . we observe that our model significantly outperforms the competition on all metrics .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . our uni model shows a 4 . 5 % increase in accuracy over the standard res model . res , on the other hand , shows a 2 . 4 % increase .
performance on different datasets is shown in table 8 . the average age of the attacker is 9 . 7 % and the corresponding adversary is 15 . 3 % more likely to receive the notification .
results in table 1 show that the training directly towards a single task can improve the performance for the learner .
2 shows the effect of the additional cost term on the balanced and unbalanced task averages . the classifiers trained on the pan16 dataset are able to detect instances of gender - neutral tweets in the balanced task averages , but are unable to detect them . the classifier trained on pan16 has the worst performance on both datasets .
performance on different datasets with an adversarial training set is shown in table 3 . pretrained participants receive a significant drop in performance compared to those using the trained classifier .
6 shows the performance of different encoders for different instances of the protected attribute . embedding has the advantage of having the same number of instances as the embeddings . however , the performance drop for rnn is much worse for the embedded variant .
results in table 2 show that our model outperforms other models in terms of both modeling and finetune . the results of our model are summarized in table 1 . we observe that our lstm model achieves the best performance with a minimum of 2 . 5m iterations on the training dataset . however , the results are slightly worse on the final dataset , indicating that the model is more suitable for the task at hand . finally , we observe that the sru model performs similarly to the other models when trained on a larger corpus ( e . g . , the wt2 dataset ) , with a smaller number of models contributing to the performance improvement .
results are shown in table 5 . we observe that our model significantly outperforms other models in terms of both acc andbert time . the difference in performance between this model and other models is not statistically significant , however it is significant .
3 shows the performance of our model compared to other models . our model improves upon the best stateof - the - art models on three of the four datasets . the results are summarized in table 3 . we observe that the amapolar time model significantly outperforms the other models in terms of err . table 3 shows that our model performs better on the three datasets .
3 shows the bleu score on the wmt14 english - german translation task , measured from 0 . 2k training steps on tesla p100 , and measured in seconds on the newstest2014 dataset . our model outperforms all the stateof - the - art models except for the sru model , which has the advantage of decoding one sentence in seconds .
4 shows the performance of our model with respect to match / f1 score on squad dataset . the results published by wang et al . ( 2017 ) show that our model obtains the best performance with a parameter number of 2 . 67m and a f1 score of 1 . 83 . these results show that the sru model has superior performance to other models with similar parameter numbers .
6 shows the f1 score on conll - 2003 english ner task . it can be seen that the lstm model significantly outperforms the other models in terms of parameter number . however , it does not achieve the best performance .
7 shows the performance of our model with base + ln setting and test perplexity on snli task with base setting . the results are shown in table 7 .
results are shown in table 1 . word models trained on the word " italic " outperform human in all aspects except for system retrieval . the word " sent " model ( mtr ) is particularly effective for both system and multi - task learning . all the word models ( except for mtr ) are significantly better than human in terms of system evaluation . sentrieval is the most effective , with a b - 2 average of 7 . 55 / 8 . 64 on the system compared to the previous state - of - the - art model . oracle and mtr feature - values significantly outperform both human and machine learning models . when using word " sent " in combination with word " error " , the performance drops significantly .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all the automatic systems is reported in table 4 . our system outperforms all the other systems except seq2seq in terms of interpretability .
results are shown in table 1 . the most representative models are europarl , ted talks , and docsub . the worst performing models are df , docsub , and eurparl . however , these models outperform all the other models except for those that do not use the word " trick " . these models use word " talks " instead of " tables " . we observe that the most representative of these models is the " ted talks " model , which is based on the best performing embeddings .
results are shown in table 3 . the most representative models are en , french - based , and spanish - based . they outperform all the other models except for the two that belong to the " traditional " embedding group , namely , docsub . however , the best performing model is europarl , which performs slightly worse than the other two .
3 shows the performance of all the models trained on the corpus dataset . our model outperforms all the other models except for the ones using the word " host " . our model performs on par with the best on all the datasets except for those using docsub . the results are reported in table 3 . we observe that our model significantly outperforms the competition on both corpus and docsub datasets .
are shown in table 1 . our system achieves the best performance with a depth - cohesion score of 1 . 78 on the metric compared to the previous best stateof - the - art systems . our joint model outperforms all the other models except for the one that has the smallest coverage . we observe that our joint model has the highest truedepth score , which shows the competitiveness of the two systems .
are shown in table 1 . our system achieves the best performance with a maxdepth of 9 . 43 on the metric compared to the previous best stateof - the - art systems , europarl , maxdepth and depthcohesion . our joint model outperforms all the other models except for the one that has the highest maxdepth . we observe that our joint model has the worst performance on both metric with a gap of 1 . 5 points .
performance ( ndcg % ) on the validation set of visdial v1 . 0 . lf is the enhanced version of the original visdial model , and r2 is weighted softmax loss , respectively .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . the best performing model is lrv , which can be seen in table 2 .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . we observe that the hmd - prec model outperforms all the other models except for the one that uses bert .
3 presents the results of our model on the test set of ruse ( ruse ) with respect to de - en and k2v . the results are summarized in table 3 . our model significantly outperforms the baseline on both metrics . for example , it can be seen that the lemmatization baseline ( memor ) significantly improves the results for both targets ( e . g . , noreg . , p - en < 0 . 005 ) and the direct assessment baseline ( epmor ) .
3 presents the bagel and sfhotel scores on the validation set . the results are summarized in table 3 . the baseline bleu - 1 scores are significantly better than the other baselines on both sets . the bertscore - f1 scores show significant performance improvement over the baseline scores on all three sets .
performance of the models according to these baselines is reported in table 3 . the summaries are presented in bold . the leic score - recall scores ( p < 0 . 001 ) consistently outperform the meteor score - mover baseline on three of the four metrics ( m1 , m2 , and w2v ) . the scores computed using elmo and p scores are significantly higher than those computed using spice ( p > 0 . 005 ) on m2 .
results are shown in table 2 . we observe that for all models except for those using m0 + para + lang , the performance drops significantly when using only pure word embeddings . this is evident from the fact that the shen - 1 model performs better on sim than on pp .
results are shown in table 3 . we observe that the semantic preservation and transfer quality scores are the most important aspects of semantic preservation . semantic preservation scores are significantly better than those of the other two types , indicating that semantic preservation is more important for semantic preservation than the transfer quality . the semantic preservation scores computed using yelp data are significantly worse than those computed using m2 and m7 datasets . however , the difference is less pronounced in the case of the two types of data that are strongly related to semantic preservation ( cf . table 3 ) .
5 shows the performance of human and machine models on the acc and pp metrics . the results are shown in table 5 . both sim and human ratings of semantic preservation are statistically significant ( paired t - test , p < 0 . 01 ) on the standard yelp dataset , while pp consistently shows higher performance . the results of human evaluations show that the quality of the semantic preservation evaluations is comparable to that of the machine evaluations .
results are shown in table 4 . we observe that for all models except m1 , there is a significant drop in performance compared to the previous state of the art model ( m0 + 2d ) .
results on yelp sentiment transfer are shown in table 6 . our best models achieve higher bleu than those using simple - transfer or n - word embeddings , but the average acc score is lower than that of any other model using the same classifier . we also observe that the use of multiple classifiers in the transfer setup ( e . g . , delete / retrieve ) has a generally positive effect on acc âˆ— , but it is less significant than that using the original model .
statistics for nested disfluencies are shown in table 2 . the percentage of tokens that are correctly predicted to be disfluent is slightly higher than the rate at which repetition tokens are predicted to belong to disfluency .
3 shows the percentage of rephrases correctly predicted as disfluent for both the reparandum and the repair ( table 3 ) . the percentage of tokens predicted to contain a content word is in parentheses , indicating that the disfluencies in the reparandum are less pronounced for the repair than those in thereparandum .
results are shown in table 2 . we observe that the text model outperforms the single model in terms of both dev and test mean . in particular , text + innovations significantly improves the model ' s performance in the early and late stages . text + innovations also improve the predictive performance of the model in the longer term .
performance of our model on the fnc - 1 test dataset is shown in table 2 . our model achieves the state - of - art performance ( 28 . 53 % vs . 28 . 53 % ) on the three test datasets . it also achieves the best performance on the micro f1 dataset ( 29 . 53 % ) .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . neuraldater significantly outperforms all previous models except maxent - joint .
3 shows the performance of our method with and without attention . it obtains a 3 . 2 % improvement over the performance obtained by ac - gcn ( which relies on word attention ) .
3 shows the performance of all models trained on the same training set . our model outperforms all the models except for the one that performs on the " trigger " stage . the results are reported in table 3 . we observe that all models performed on the trigger stage outperform the other models in terms of performance .
3 shows the performance of our method in the event of a single argument . our method outperforms the previous stateof - the - art method in all but one of the cases . all the methods used in this data are statistically significant , with the exception of the case of the one in which the object was not detected .
results are shown in table 1 . all models except for the one that pre - trained the test documents in english ( cs - only - lm ) outperform the other models in both languages . for example , all the models shown in the table show lower performance on the dev perp and test wer datasets . however , the results are slightly worse on the english - only dataset , which shows that fine - tuning the model can improve the performance for all models . moreover , the performance drops significantly on the spanish - only subset when trained with the additional layer of redundancy . this shows that the redundancy removal technique is beneficial for both languages ,
results on the train dev and test set are shown in table 4 . fine - tuned models outperform fine - tuned models with only subsets of code - switched data in the dev set .
5 shows the performance on the dev and test set compared to the monolingual model of the gold sentence in the test set . fine - tuned - disc shows lower performance than fine - tuned - disc on both sets , but it is comparable to the performance of fine - switched - disc .
results in table 7 show that type - aggregated gaze features significantly improve recall ( p < 0 . 01 ) and f1 - score ( p > 0 . 05 ) for the three eye - tracking datasets , respectively .
5 shows precision ( p ) , recall ( f1 ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . the results are shown in table 5 . type - aggregation features significantly improve recall ( 95 % vs . 94 % ) for the original dataset .
results on belinkov2014exploring â€™ s ppa test set are shown in table 1 . the glove embeddings are derived from the original wordnet ( which has since been adapted to wordnet 3 . 1 ) . they are used in wordnet 2 . 1 , and it uses syntactic skipgram embedding . the results on the original paper are summarized in tables 1 and 2 . these results show that the syntactic embedding methods can further improve wordnet performance . syntactic - sg embedding gives a boost of performance over syntactic ones .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 . all the models using oracle pp as their dependency parser have good performance .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . it is clear from the table that removing the context sensitivity and the sensitivity scores hurts the model negatively .
2 shows the performance of domain - tuned models compared to multi30k models . the results are shown in table 2 . adding subtitle data and domain tuning for image caption translation ( bleu % scores ) improves the results for both models . subsfull domain tuning improves the performance for both datasets . however , the multi - domain tuning results are slightly worse than those for en - de .
results are shown in table 4 . subdomain - tuned models outperform subs1m models in all but one of the comparisons . the results are summarized in terms of a / a scores on the en - de dataset , with the exception of the case of mscoco17 , where the a / c scores are significantly lower than those for subs1ms . table 4 shows that domain - tuning improves the results for all models except for those that use the word " coco " .
4 shows bleu scores in terms of automatic captions ( the best one or all 5 ) . the results are shown in table 4 . the models using the multi30k model outperform all the models using concatenated captions except for those using the single30k dataset .
5 compares the performance of different strategies for integrating visual information ( bleu % scores ) . we observe that the best performing model is img , which uses dec - gate as the enc - gate layer . the results are summarized in table 5 . enc - gate alone improves the bleu % score by 2 . 36 points . finally , we observe that decoding the visual information leads to better interpretability .
1 shows the performance of subs3m with different visual features compared to subs6m ( which relies on word embeddings ) . subsequent improvements on the flickr16 dataset show that the semantic features combined with the multi - lingual features contribute to the overall performance of the model . moreover , the combination of semantic features and the text - only features further improve the performance . the combination of the semantic and syntactic features further boosts the quality of the models .
3 shows the performance of the best models using the en - fr - ff model compared to the alternatives en - es - rnn - ff . the results are reported in table 3 . we observe that for all but one of these models , the performance is significantly better than those using the l2r - based model .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used . the average number of pairs in these splits is 1 , 472 .
2 shows the performance of our model with respect to the english , french and spanish vocabularies . the results are shown in table 2 . our model outperforms all the other models with a large gap in performance .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) and ter measures are shown in table 5 . the system consistently outperforms the other two systems in terms of rev performance .
2 shows the performance of our visually supervised model compared to the standard rsaimage model from chrupala2017representations . the results are shown in table 2 . our model obtains a significantly higher recall rate than the baseline model .
results on synthetically spoken coco are shown in table 1 . our model outperforms the previous stateof - the - art models in terms of recall @ 10 and mean mfcc score .
1 shows the results of different classifiers compared to the original on sst - 2 . for example , cnn turns in a < u > screenplay that has edges at the edges ; it â€™ s so clever you want to hate it . table 1 shows that the rnn classifier , when combined with other classifiers , can significantly improve the results .
2 shows that fine - tuning has indeed increased the number of occurrences in sst - 2 from the original sentence to the ones that have stayed the same . this indicates that the value of goodness has not decreased , however it has increased with the amount of occurrences that have been finetuned .
shown in table 3 , the sentiment changes in sst - 2 when the negative labels are flipped to positive . this shows that the effect of the flipped sentiment signal is very positive . however , this does not translate well into negative sentiment .
results are presented in table 1 . the results are summarized in terms of ppmi scores . our joint model outperforms the competition on both positive and negative test sets . on the positive test set , our joint model ( sift - 2 ) scores 98 % on positive test sets and 99 % on negative test set . our results are consistent across all test sets , with the exception of the case of sst - 2 .
