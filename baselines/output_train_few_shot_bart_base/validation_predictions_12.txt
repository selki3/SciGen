2 shows the performance of our recursive approach on the large movie review dataset compared to the iterative approach , which performs better on training .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , when the batch size increases from 1 to 25 , it exhibits the smallest performance improvement .
2 presents the results for each model with different representation . the max pooling strategy consistently performs better in all model variations . it is clear from table 2 that the multi - factor approach is beneficial for all models with different representations .
1 shows the effect of using the shortest dependency path on each relation type . the results are shown in table 1 . we observe that the macro - averaged model achieves the best f1 ( in 5 - fold ) without sdp . however , it does not achieve the best diff .
results are shown in table 3 . the y - 3 model outperforms the previous state - of - the - art models in terms of f1 and f1 accuracy .
results are reported in table 1 . we observe that the results obtained by mst - parser are comparable to those obtained by our method , with a significant difference in performance between the two methods .
4 shows the c - f1 ( 100 % ) in % for the two indicated systems ; for the lstm - parser system , it is 60 . 62 ± 3 . 54 % , and 58 . 24 ± 2 . 87 % , respectively .
results are shown in table 3 . the original and the original results are presented in table 4 . the results are summarized in bold . original results show that tgen + and tgen − are more accurate at predicting correct answers than the wrong ones . however , they are slightly worse than the original on both datasets .
1 compares our original and our cleaned e2e datasets with the original ones . the results are summarized in table 1 . we find that the original and the cleaned versions have the highest number of distinct mrs and the average number of instances of ser as measured by our slot matching script , see section 3 .
results are shown in table 1 . original and original results are presented in bold . original results show that tgen + performs better than tgen − in all but one of the three cases , while the other two show lower performance .
results of manual error analysis on a sample of 100 instances from the original test set are shown in table 4 . we found that adding incorrect values to the training set caused a slight disfluency in the training data .
table 3 , we present the results of our joint model on the external and external datasets . the results are summarized in table 3 . the dcgcn model outperforms the previous state - of - the - art on both the external dataset and the seq2seqk ( konstas et al . , 2017 ) by a significant margin .
2 shows the results on amr17 . our model achieves a bleu score of 27 . 5 on the model size compared to seq2seqb ( ours ) of 28 . 3m .
3 presents the results for english - german and english - czech . the results are presented in table 3 . we observe that the single model outperforms the other models in both languages , with the exception of bow + gcn ( bastings et al . , 2018 ) in english - language and german - language .
5 shows the effect of the number of layers inside dc on the overall performance of the layers . table 5 shows that when we add layers of layers to dc , we get a reduction of 3 layers .
results are shown in table 6 . with residual connections , gcn + rc + la ( 2 ) shows a slight improvement over the previous state - of - the - art gcn .
results are shown in table 3 . we observe that dcgcn achieves the best performance on all three metrics , with a slight improvement on the bias metric .
8 shows the ablation study results for amr15 . the results show that removing the dense connections in the i - th block improves the performance for the model .
table 9 shows the ablation study results for the graph encoder and the lstm decoder . the results are summarized in tables 9 and 10 . the results show that the multi - decoder design improves upon the previous state - of - the - art dcgcn4 by a significant margin .
7 presents the results for initialization strategies on probing tasks . our paper shows that glorot outperforms the previous state - of - the - art approaches in terms of precision . the results are summarized in table 7 .
results are presented in table 3 . table 3 shows the results of our method on the subtraction and subtraction tasks . the results show that our method obtains the best results on both subsjnum and subjnum metrics . the results also show that the h - cbow achieves the best performance on both subtraction metrics .
3 presents the results of our method on the subj and mpqa datasets . the results are summarized in table 3 . the cbow / 784 model outperforms both sst2 and sst5 in terms of mrpc performance . however , it is still superior to both sick - e and subj on both mrpc datasets .
results on unsupervised downstream tasks attained by our models are shown in table 3 . the cbow model outperforms both hybrid and cmp in terms of downstream performance .
table 8 shows the results for initialization strategies on supervised downstream tasks . our paper shows that glorot outperforms subj and mpqa in terms of initialization performance . however , it does not improve significantly over subj , and is still inferior to subj .
6 shows the performance for different training objectives on the unsupervised downstream tasks . we observe that cmow - c performs better than cbow - r on the supervised downstream tasks ,
results are shown in table 3 . the best performing method is cbow - r , which obtains the best results on the subjnum and the coordinv datasets . however , it does not achieve the best performance on the subjnum datasets . this is due to the fact that it obtains a lower precision on the subsjnum dataset .
3 presents the results of our method on subj and mpqa datasets . the results are summarized in table 3 . we observe that the cbow - r model outperforms both sst2 and sst5 in terms of mrpc performance . on the other hand , it performs better than both sick - e and subj on both mrpc datasets .
results are shown in table 3 . in [ italic ] e + org and per datasets , the system performs better than the previous state - of - the - art model in all but one of the cases .
results on the test set under two settings are shown in table 2 . name matching improves upon the previous state - of - the - art on all metrics except e + p and f1 scores .
6 : entailment ( ent ) and ref ( neu ) results are shown in table 6 . the results are summarized in bold . s2s and g2s - ggnn outperform all the models except ref ,
results are shown in table 3 . the results are presented in tables 1 and 2 . table 3 show that the ldc2017t10 model outperforms the previous state - of - the - art models in all but one of the three cases .
3 shows the results on ldc2015e86 test set when trained with additional gigaword data . our model outperforms the previous state - of - the - art on both external and external datasets .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . bilstm significantly improves the model ' s size by 3 . 5 % compared to the previous state of the art .
results are shown in table 3 . we observe that g2s - ggnn significantly improves upon the previous state - of - the - art model in terms of sentence length and sentence length .
shown in table 8 , the fraction of elements in the output that are not present in the input ( added ) that are missing in the generated sentence ( g2s - gin ) , are used in the comparison to the reference sentences ( miss ) .
shown in table 4 , the accuracy of pos with different target languages is comparable to that using the 4th nmt encoding layer .
2 shows the pos and sem accuracy with baselines and an upper bound . the results are shown in table 2 . word2tag outperforms unsupemb and word3tag in both baselines .
results are presented in table 4 . table 4 shows the performance of our method on the three datasets . the results are reported in tables 1 and 2 . table 4 show that our method outperforms the previous state - of - the - art on both datasets .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ . in pan16 , the attacker scored 10 % higher on the training set compared to pan16 .
1 shows the performance of pan16 when training directly towards a single task . the results are shown in table 1 . we observe that pan16 significantly outperforms pan16 in terms of recall performance .
2 presents the results of the study on the balanced task and unbalanced task splits . the results are shown in table 2 . we observe that the presence of the gender - neutral mention attribute leads to a significant drop in the performance for balanced task splits compared to unbalanced data splits .
3 shows the performance on different datasets with an adversarial training . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 3 .
6 shows the performance of embedded and guarded with different encoders . the results are shown in table 6 . embedding is more difficult than embeddings , as shown in fig . 6 .
results are presented in table 3 . the results of the second study are summarized in table 4 . we observe that the lstm model outperforms the previous state - of - the - art on both base and finetune datasets . however , it does not improve upon the performance of the previous model . finally , it performs slightly worse than the previous work on the wt2 dataset .
results are presented in table 5 . we observe that the lstm model performs better than the previous state - of - the - art model in terms of training time . however , it does not achieve the best performance when training on a larger dataset . further , the time taken to train on the larger dataset is not comparable to that of training on the smaller dataset .
results are shown in table 3 . we observe that our approach improves upon the state - of - the - art lstm on both yahoo and google docs . however , it does not improve upon the performance of the original amapolar time model .
3 shows the bleu score on the wmt14 english - german translation task . as table 3 shows , the training time in seconds ( i . e . , 0 . 99 training steps ) is comparable to the time in the previous state - of - the - art gru model . however , the difference is less pronounced for sru .
4 shows the performance of our model on squad dataset . we observe that our model obtains the best match / f1 score on the model with the parameter number of 2 . 44m . as expected , the sru model achieves the best performance with a boost of 1 . 67m compared to the baseline .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number in the reported result , and sru denotes the result lample et al . ( 2016 ) .
results are shown in table 7 . snli model outperforms the ptb model on snli task with base + ln setting and test perplexity on ptb task with base setting setting .
results are shown in table 3 . word embeddings ( mtr ) w / oracle retrieval ( b - 2 ) and mtr ( r - 4 ) are the most effective methods for system evaluation . as expected , the system evaluation results are slightly worse than those of human , but still comparable to human . in particular , the results are less striking for human , as the results of system evaluation are less pronounced than for human .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the results are summarized in table 4 . the best performing system is seq2seq , which is ranked in top 1 or 2 for overall quality .
results are shown in table 3 . the results are presented in tables 1 and 2 . we observe that the best performing embeddings are in df , tf , docsub and docsub , respectively . however , the results are not statistically significant .
results are shown in table 3 . the results are presented in tables 1 and 2 . we observe that our approach outperforms the approaches of both df and tf on both datasets . however , the results are still slightly worse than those of df .
results are shown in table 3 . the results are presented in tables 1 and 2 . we observe that our approach outperforms the approaches of both df and tf on both datasets . however , the results are still slightly worse than those of df .
results are shown in table 3 . we observe that the maxdepth of our model is significantly better than that of the other two models , namely , docsub and europarl . however , the gap between maxdepth and maxdepth is still larger than that between the two models .
results are shown in table 1 . we observe that the maxdepth of our model is significantly lower than that of the other two models , eurparl and mindepth . this is due to the fact that our model performs better on the df than on the tf .
results are shown in table 1 . the enhanced lf model outperforms the original visdial v1 . 0 model by a significant margin . the difference is in the r0 and r2 scores , however , in the hidden dictionary learning setup .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . the results are shown in table 2 . using p2 indicates the most effective method for hidden dictionary learning .
5 presents the results on hard and soft alignments . the results are summarized in table 5 . we observe that the hmd - f1 + bert model outperforms the previous state - of - the - art on hard alignments ,
3 presents the results of the direct assessment task . the results are summarized in table 3 . our approach obtains the best results with respect to both de - en and fi - en . we observe that the results obtained by our approach are slightly better than those obtained by the previous approach .
3 presents the bagel and sfhotel scores on the test set . the results are summarized in table 3 . we observe that bleu - 1 achieves the best bertscore - f1 score on both sets . however , it does not achieve the best f1 score .
results are shown in table 3 . the summaries obtained by spice are presented in bold . however , the summaries shown in bold are slightly worse than those obtained by word - mover . for example , leic scores are 0 . 939 and 0 . 749 respectively , respectively , compared to the leic score of 0 . 864 . we observe that word - mover significantly outperforms wmd - 1 on both metrics .
results are shown in table 3 . we observe that m0 [ italic ] + para + lang significantly improves the performance of the model on the sim task compared to the previous state of the art .
results are presented in table 3 . we observe that the transfer quality and transfer quality scores are the most important aspects of semantic preservation . the results are summarized in table 4 . for semantic preservation , we observe that semantic preservation improves over the previous state of the art .
5 presents the results of human sentence - level validation . the results are summarized in table 5 . the results show that the human judgement is better than the machine and human ratings of fluency . however , the results are not statistically significant ( table 5 ) .
results are shown in table 3 . we observe that m0 [ italic ] + para + lang = 0 . 818 on a single test set compared to the previous state - of - the - art m0 ( m1 + m2 + m3 + m4 + m5 + m6 + model ) improves the performance of the model by 0 . 7 points on the sim test set .
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu and the highest acc ∗ on the same 1000 sentences compared to prior work . however , the results are slightly worse than those obtained by simple - transfer .
2 shows the percentage of disfluencies that were correctly predicted as disfluent , compared to the percentage predicted as nested . reparandum length is reported in table 2 .
3 shows the percentage of tokens correctly predicted to contain a content word in both the reparandum and the repair ( content - content ) . the fraction of tokens that contain the content word is shown in table 3 . table 3 shows that the disfluencies predicted as disfluent have a significant effect on the accuracy of the prediction .
results are shown in table 3 . text + innovations outperform text + text in both cases . table 3 shows that text + innovations significantly improve the performance of the model in both the early and late stages . in the late stages of the study , the best performance is achieved on the single test , while the best results are obtained on the multi - sample test .
2 shows the performance of our model on the fnc - 1 test dataset compared to the state - of - art embeddings on cnn - based sentence embedding . the results are summarized in table 2 . our model achieves the best performance on both datasets .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better , lower is better ) .
3 shows the accuracy of our method for word attention and graph attention compared to the previous state - of - the - art .
results are shown in table 3 . the best performing model is jrnn , which outperforms the previous state - of - the - art on every stage except argument .
3 presents the results of our method with respect to the identification and classification task . our method obtains the best results with a precision of 68 . 7 % and a f1 score of 44 . 1 % on the trigger dataset .
results are shown in table 3 . all models outperform all the models except for the spanish - only model . the results are summarized in table 4 . we observe that all models perform better on the dev perp and test wer datasets .
4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . results on the train test set are shown in table 4 .
5 shows the performance on the dev set compared to monolingual and code - switched settings . the results are summarized in table 5 .
results are shown in table 7 . for type - aggregated gaze features trained on the conll - 2003 dataset , precision ( p ) , recall ( f1 ) and f1 - score ( f ) are reported in tables 7 and 8 . the results are summarized in bold .
5 shows the precision ( p ) , recall ( f1 ) and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . the results are shown in table 5 .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . syntactic - sg embeddings are used in wordnet , and glove - retro is used in verbnet 3 . 1 . the results on the original paper are summarized in tables 1 and 2 . we note that the syntactic embedding of the semantic skipgram vectors is the most important part of wordnet .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model . table 3 shows the effect of removing context sensitivity and the ppa acc . from table 3 , we observe that the loss of context sensitivity leads to a reduction in ppaacc .
2 shows the results of domain tuning for image caption translation . the results are summarized in table 2 . subsfull domain tuning improves the multi30k performance by 3 . 6 points compared to the previous state of the art model .
results are shown in table 3 . subdomain - tuned subs1m outperforms all models except mscoco17 , which shows that domain - tuning improves the quality of the models for both en - de and in - de settings .
4 shows the bleu scores in terms of multi30k captions . the results are summarized in table 4 . the best results are obtained by adding automatic image captions ( dual attn . ) to the en - de dataset . however , the best results with the single - attn . are obtained with the exception of mscoco17 dataset .
5 compares the performance of our approach with previous approaches . we observe that enc - gate and dec - gate perform better than both approaches on the en - de dataset and on the flickr17 dataset .
3 shows the performance of subs3m compared to subs6m on the en - de dataset . the results are summarized in table 3 . sub3m outperforms all the models except for the one that relies on text - only features . as expected , the results are slightly worse for the ensemble - of - 3 dataset , but still superior for the multi - lingual dataset .
results are shown in table 3 . we observe that the en - fr - ff model outperforms the previous state - of - the - art models in terms of translation performance . the results are summarized in table 4 .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 shows the results for the english , french and spanish data used for our model . the results are summarized in table 2 .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) show that the system performs better than the previous state - of - the - art rev system .
2 presents the results of our supervised model on flickr8k . the results are summarized in table 2 . we observe that the vgs model outperforms the previous state - of - the - art rsaimage model by a significant margin .
results on synthetically spoken coco are shown in table 1 . the results are summarized in the table 1 . we observe that the hierarchical embeddings generated by audio2vec - u outperform the supervised ones by a margin of 3 . 9 % .
1 shows the results of the different classifiers compared to the original on sst - 2 . we report further examples in table 1 . for example , cnn turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . similarly , for rnn , the edges of the screenplay are shown in the appendix .
2 presents the results of fine - tuning on sst - 2 . the results are shown in table 2 . we observe that the number of occurrences have increased , decreased or stayed the same through fine - tuning . as expected , the percentage of occurrences that have not changed with respect to the original sentence has increased .
3 shows the change in sentiment between positive and negative labels with respect to sst - 2 . the results are shown in table 3 .
results are presented in table 3 . the results are summarized in table 4 . while the results are encouraging , it is difficult to distinguish between positive and negative aspects of the results . this is mostly due to the small size of the corpus ( table 4 ) and the fact that the corpus is relatively small ( e . g . , 0 . 5415 vs 0 . 5315 ) .
