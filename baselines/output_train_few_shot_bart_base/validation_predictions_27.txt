2 : throughput and training on the recursive framework , and tensorflow ' s iterative approach , with the large movie review dataset in table 2 . the recursive approach performs the best on the training dataset , while the iteration approach shows better performance on the inference dataset .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left to design a better system .
2 shows the performance of the hyper parameters optimization strategies for each model with different representation . our system achieves the best performance with different number of hyper parameters and the maximum number of feature maps . the max pooling strategy consistently performs better in all model variations . the hyper parameters activation func . gives a 0 . 87 f1 score over the best pooling strategies . the sigmoid model performs similarly to the softplus model in terms of number of parameters .
1 shows the effect of using the shortest dependency path on each relation type . it can be observed that macro - averaged models achieve the best f1 ( in 5 - fold ) with sdp , and the same f1 score ( in 10 - fold ) . the comparison of macro - adapted models with different dependency paths is shown in table 1 . the difference is minimal , however , when using only one dependency path .
results are shown in table 3 . in general terms , the y - 3 model outperforms the comparable y - 2 model in terms of f1 score , however it has the advantage of having 50 % and 100 % higher f1 scores .
results are shown in table 4 . the results of the best performing model are presented in the table . all the models except mst - parser have achieved state - of - the - art results . in addition , the average number of entries in the essay level is significantly higher than those in the other categories .
4 shows the c - f1 scores for the two indicated systems ; essay vs . paragraph level . the lstm - parser system shows lower performance than the majority systems , however it is comparable in terms of overall performance .
results are shown in table 4 . the original and the new systems perform better than the original on all tests except for those on the rare instances when tgen is used . the difference between original and original is less pronounced , but still significant . the meteor model outperforms all the other methods except for the one that relies on word - error removal . the rouge - lstm model ( which relies on syntactic and semantic information ) is particularly bad for this task . it is clear from table 4 that the original and the original methods are more effective for the task at hand .
shown in table 1 , the original and the cleaned versions have the highest number of distinct mrs , the average number of instances and the number of concatenated instances as measured by our slot matching script , see section 3 .
performance of original and original test methods on the bleu dataset is presented in table 4 . the results are presented in bold . original methods generally perform better than the original ones , however , their performance is slightly worse than those of tgen + on the other two datasets .
results of manual error analysis on a sample of 100 instances from the original test set of tgen ( see table 4 ) . the errors we found were caused by slight disfluencies in the training set ( i . e . , adding incorrect values , wrong values ) . as can be seen , the number of errors in the original training set is much smaller than the ones we found ( e . g . , missing values ) .
model the performance of our dcgcn model compared to other state - of - the - art models on the external and external datasets is reported in table 1 . the difference between the performance of the external model and the seq2seqk ( konstas et al . , 2016 ) is minimal , but significant , with an absolute improvement of 2 . 6 points over the previous state - ofthe - art model .
results on amr17 are shown in table 2 . our model achieves 24 . 5 bleu points ( paired t - test ) and achieves 27 . 5 ( bleu ) points . the results also show that dcgcn behaves similarly to the ensemble seq2seqb model in terms of parameters .
results of experiment 1 are shown in table 1 . the best performing model is bow + gcn ( bastings et al . , 2017 ) . the difference in performance between single and single is minimal , however , the difference in english - language bias and english - czech bias is significant , the most representative model is the ggnn2seqb , which takes the single model from the published literature and applies it to the english - speaking corpus . it is clear from the results that both the single and the co - ed model benefit from the same corpus .
5 shows the effect of the number of layers inside the dc stack on the performance of the model in table 5 . the first case shows the diminishing returns from mixing layers of the stack that contribute to the growth of the network . table 5 shows that for every layer that contributes to the overall improvement , there are two more layers that contribute less .
6 shows that gcns with residual connections have comparable performance with baselines . rc + la shows that the gcn has residual connections with some baselines , such as dcgcn2 ( 27 ) . however , when gcn is trained on residual connections , the performance drops significantly .
model f1 shows that dcgcn models outperform the previous state - of - the - art models on all metrics except bias metric . in fact , the results are slightly worse than those of acgcn except for the exceptional case of " bias metric " .
8 shows the ablation study results for amr15 . the results show that removing the dense connections places the most important importance for the model .
table 9 , the ablation study for the graph encoder and the lstm decoder . the results of " - global node " and " - linear combination " are shown in table 9 . the two types of encoder have similar performance : the original dcgcn4 encoder has 25 . 5 % performance improvement over the other two baselines .
results for initialization strategies on probing tasks are shown in table 7 . our paper shows that our method significantly improves the performance over other initialization strategies .
1 and table 2 summarize our results on the hidden test set of cbow / 400 . our proposed method outperforms the previous state - of - the - art methods on every metric by a significant margin . from left to right our method obtains the best performance . its topconst method achieves the best results with a 34 . 4 % increase in precision .
1 shows the performance of our method compared to other methods . our cbow model outperforms all the other methods except for the one that cmp uses . cbow even outperforms mpqa and sst2 in terms of mrpc score . however , it has the advantage of training on a larger corpus . its mrpc scores are significantly lower than those of other methods suggesting that the model is more suitable for hybridization .
results on unsupervised downstream tasks attained by our models are shown in table 3 . our model outperforms both hybrid and cmp in terms of overall performance . on the sts13 dataset , it achieves a comparable performance to cmp . however , the difference is less pronounced for sts15 , which indicates that cbow models are better at downstream tasks .
8 shows the performance for initialization strategies on supervised downstream tasks . our paper shows that our method outperforms all the alternatives except subj and mpqa except for the one that is pre - trained on the upstream tasks . it also beats the sst2 and sst5 baseline by a margin of 2 . 8 points .
6 shows the performance for different training objectives on the unsupervised tasks . our approach outperforms the supervised ones on three of the four tasks . on the other two , cbow - r shows lower performance .
1 shows the performance of our method . our cbow - r model outperforms the previous models in every respect . from left to right , it obtains the best performance on every metric with a gap of 2 . 5 points from the last published results ( table 1 ) .
1 shows the performance of subj and sick - r models compared to other methods . the subj models outperform all the other methods except for the cbow - r model , which obtains the best performance on mrpc . also , subj has outperformed sst2 and sst5 in terms of recall scores . however , it has the advantage of training on a larger corpus , which underscores the competitiveness of sick and subj .
system results in table 3 show that all org and per metrics are better than the state - of - the - art systems in all cases ( except for the exceptional case of " all org " in which the system relies on a single entity for its evaluation . supervised learning ( mil - nd ) achieves the best results with a minimum of 50 % org , 50 % per and 98 % e + misc scores . the difference between the performance of supervised learning and supervised learning is less pronounced in theitalic case , when using only plain averaged word embeddings . however , for the " supervised learning " dataset , there is a significant difference in performance between the two systems .
2 : the results on the test set under two settings are shown in table 2 . our system achieves the best results with 95 % confidence intervals of all p , all r and all f1 scores . supervised learning , in turn , improves e + p scores by 2 . 36 points over the previous state of the art model , mil - nd ( model 2 ) .
6 shows the results of ref and ref compared to ref on the entailment ( table 6 ) . ref significantly outperforms ref in all cases except for the case when ref is used with ref disambiguation . retrieving ref data significantly improves the generalization results for all models except those that use ref .
table 3 , we present the results of experiment 1 and experiment 2 . the results are presented in table 3 . the models perform comparably to other high - level models , such as the ldc2015e86 and ldc2017t10 . however , their performance is slightly worse than those of other models that rely on pre - trained models .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . our model outperforms all the models except for the one that is pre - trained with external data . also , our model obtains a significantly better bleu score compared to other models .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . bilstm significantly boosts the performance of the model compared to other models .
results are shown in table 4 . the results are summarized in terms of the average number of frames compared to the baseline g2s - gin model , in particular , the results show that when using a longer sentence span , the recall length and the average sentence length have less effect on the model ' s performance . finally , the averagesentence length has increased since the introduction of the word " g2s " . note that the increased recall length has resulted in a lower recall score for the model ( which shows the diminishing returns from using a shorter sentence span ) .
shown in table 8 , the fraction of elements in the output that are not present in the input graph that are missing in the generated sentence ( g2s - gin ) . it is clear from table 8 that the use of token lemmas in the model results in a significant improvement over the baselines in ldc2017t10 .
4 shows the performance of our method with respect to target languages . it can be seen that the use of word embeddings improves the semantic performance for some target languages , however it does not improve the performance for others .
2 shows the pos and sem tags accuracy with baselines and an upper bound . accuracies are shown in table 2 . word2tag is the most frequent classifier , followed by unsupemb embeddings . the results are presented in tables 2 and 3 .
results are presented in table 4 . our proposed system improves upon the performance of the previous state - of - the - art systems by 3 . 8 points in terms of accuracy .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . our uni model shows marked improvement over the previous state - of - the - art on three of the four layers .
performance on two datasets is shown in table 8 . the attacker scored significantly worse on the training set than the corresponding adversary on both datasets .
1 shows the accuracies when training directly towards a single task . for pan16 , the training data generated by pan16 is presented in table 1 . the training data also contains gender specific training data .
2 shows the effect of the additional cost term on the balanced and unbalanced task averages . the classifiers trained on the pan16 dataset are named after specific groups of words . they cause a significant drop in performance in the balanced task averages , and in the unbalanced case , a drop of 6 . 5 points in performance .
performance on different datasets with an adversarial training set is shown in table 3 . the difference between the performance of the trained classifier and the corresponding adversary is reported in δ ( difference between the attacker score and match rate ) . sentiment and gender are the most important factors in predicting an adversary ’ s performance . the classifier trained on pan16 is particularly sensitive to gender - based stereotypes . the age - based classification system , pan16 , predicts an adversary to have a high probability of losing the task .
6 shows the performance of the embeddings for different encoders . embedding guarded is easier for the model to learn , since it relies less on the word " host " . however , it is harder to learn from the examples .
results in table 2 show that our proposed lstm outperforms the previous stateof - the - art models on both base and finetune tasks . the results of our model outperform the strong lemma baseline on both datasets , in particular , it achieves a higher performance on the largescale wt2 and wt2 + finetune task , this model further improves the model ' s performance on both subsets of the wt2 dataset . finally , the results of the second study show that the use of dynamic modeling can further improve the results for both datasets .
performance of our model compared to previous models on the acc andbert datasets is reported in table 4 . the results of experiment 1 show that our approach significantly improves the model ' s performance on both datasets when trained and tested on the same dataset .
3 shows the performance of our model compared to other models . our model improves upon the best state - of - the - art models on three of the four datasets . the results are summarized in table 3 .
3 shows the bleu score on wmt14 english - german translation task . our model obtains the best performance with a 2 . 67 % improvement over the state - of - the - art model on the gold - standard2014 dataset . it also outperforms other models in terms of decoding one sentence .
4 shows the model ' s performance on squad dataset . the results published by wang et al . ( 2017 ) show that rnet models significantly outperform other models in terms of match / f1 score . however , the best models do not exceed the upper boundary of the baselines , which shows the diminishing returns from using superficial features .
6 shows the f1 score of our model on conll - 2003 english ner task . it can be seen that the lstm model significantly improves the performance by increasing the parameter number .
7 shows the performance of our model with base + ln setting and test perplexity on snli task with base setting . it can be observed that our model significantly improves upon the state - of - the - art on ptb task with base setting .
results are shown in table 1 . word models trained on the oracle retrieval dataset outperform human models in terms of system evaluation . in particular , word models ( mtr ) and word models ( hochreiter and schmidhuber , 1997 ) are particularly effective for system evaluation : the word models trained upon the oracle dataset ( reiter et al . , 1997 ) achieve the best results with a minimum of 0 . 05 % error on the system compared to the previous state - of - the - art . sent attention methods ( mtd ) are beneficial for both human and system evaluation ,
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performing system is seq2seq , ranked in top 1 or 2 for each of the four aspects .
results are shown in table vii . our proposed system outperforms the previous state - of - the - art systems on all three datasets except for the one that embeds the word embeddings . it obtains the best performance on the three datasets , outperforms df , docsub and tf by a significant margin .
table 2 , we compare against the previous state - of - the - art systems on three of the four datasets : corpus , europarl , and ted talks . the results are summarized in table 2 . our joint model outperforms all the other baselines except for the one that embeds the word " host " . the difference is most prevalent on the corpus dataset , where the difference between the performance of the two is most pronounced on the three datasets .
results are shown in table vii . our proposed system outperforms the previous state - of - the - art systems on all three datasets except for the one that is used in the distractor and fullwiki setting . it closely matches the performance of df , docsub and hclust . on the other hand , it performs slightly worse than the other two baselines , namely , europarl and ted talks , both of which belong to the same class .
embeddings for corpus are shown in table 3 . our system achieves the best performance with a minimum of 3 . 5roots on each metric compared to the previous state - of - the - art systems . europarl also outperforms the others in terms of depthcohesion on both metric , with a gap of 1 . 78roots per metric compared with the previous best performance .
embeddings for corpus are shown in table 3 . our system achieves the best performance with a minimum of 3 . 5roots on each metric compared to the previous state - of - the - art . europarl also achieves the highest score with a maxdepth of 9 . 43 . on the other hand , it achieves the worst result with a depthcohesion score of 2 . 29 .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . we show that lf significantly outperforms the enhanced version of qt in terms of answer score sampling .
performance ( ndcg % ) of different models on the validation set of visdial v1 . 0 is shown in table 2 . the best performing model is lrv , which can be seen in section 5 . 7 .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . our hmd - recall + bert model outperforms all the other models except for the strong lemma baseline , which shows the diminishing returns from using bert .
3 provides exact scores for each metric . our proposed system outperforms the baseline on every metric by a noticeable margin . the most striking thing about the bertscore - f1 score is that it is significantly better on the " direct " metric compared to the " baseline " .
3 presents the bagel and sfhotel scores . the results are summarized in table 3 . the baseline bleu - 1 scores significantly outperform the baseline on all metrics except for those of " q " .
performance of the models according to these baselines is reported in table vi . the results are summarized in table vii . the summaries displayed in bold indicate that the semantic features captured by the models are significant enough to result in measurable improvement . however , the leic scores ( barely providing a performance improvement over the baseline ) are significantly worse than those by the word " mover " .
results are shown in table 4 . the performance of the models that use the word " shen - 1 " is slightly worse than those using " m0 " . however , it is still comparable with the performance of m1 and m6 using the same type of lexical programming .
results are shown in table 4 . semantic preservation and transfer quality are the most important aspects of the preservation and semantic preservation tasks . the results show that the semantic preservation approaches significantly improve over the transfer quality baseline , the semantic preservation approach further improves with the addition of semantic preservation features . finally , the semantic preserving features are further improved with the expansion of the model to semantic preservation .
5 shows the human evaluation results . we show the results of human evaluation using the [ italic ] ρ b / w negative pp and human ratings of fluency . it can be observed that the accuracy obtained by using these metrics is high , indicating that the quality of the generated metrics is relatively high .
results are shown in table 4 . the performance of the models that use the word " shen - 1 " is slightly worse than those using " m0 " . however , it is still comparable with the performance of m3 and m6 using the same type of lexical encoder .
results on yelp sentiment transfer are shown in table 6 . our best model achieves higher bleu ( acc ∗ ) than those using simple - transfer , but only slightly outperform the best model , yang2018unsupervised . we also observe that the use of classifiers in the transfer domain severely limits the model ' s ability to achieve acc ∗ .
statistics for nested disfluencies are shown in table 2 . the percentage of tokens that are correctly predicted to be disfluent is reported in tables 2 and 3 .
3 shows the relative frequency of rephrases correctly predicted as disfluent for both the reparandum and the repair ( table 3 ) . the fraction of tokens predicted to contain a content word is shown in table 3 .
results are shown in table 4 . the results are presented in bold . text + innovations significantly improve the dev and test results , in addition , text + innovations improve the model ' s performance by 0 . 2 points over the single model . from the table , we also observe that the use of innovations in the early and late stages of development leads to better predictive performance .
performance of our model on the fnc - 1 test dataset is shown in table 2 . our model achieves the state - of - art results ( 95 . 53 % vs . 94 . 53 % ) , and the accuracy ( % ) disagree with our model is 82 . 43 % ( 87 . 53 % ) .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . compared to previous methods , burstysimdater significantly outperforms maxent - joint .
3 shows the performance of our method with and without attention . the accuracy ( % ) of neuraldater shows that it has the advantage of better word attention and graph attention . it also exhibits the competitiveness of using graph attention for this task .
results are shown in table 1 . the best performing models are jnn , dmcnn and jrnn . the jnn model outperforms all the other models except for the one that embedding + t embeddings pre - trained it on . the results are presented in tables 1 and 2 .
experimental results of all methods are shown in table 1 . all methods cause a significant difference in the identification and termination of events . in particular , the method that relies on the word " trigger " has the most significant impact on the event identification performance . the method used in experiment 1 shows severe overfitting of the event with a large number of parameters .
can be seen in table 4 , all the fine - tuned models perform better than the others when trained on the dev perp and test wer . moreover , the results are slightly worse for english - only than for spanish - only .
results on the dev set and on the test set are shown in table 4 . fine - tuned training with only subsets of the code - switched data gives a significant improvement over the performance of fine - tuned training .
5 shows the performance of our system on the dev set and the test set , compared to fine - tuned - disc . the results are summarized in table 5 . our system performs slightly worse than the monolingual model on both sets .
results for the conll - 2003 dataset are shown in table 7 . precision ( p ) , recall ( f1 ) and f1 - score ( f ) by using type - aggregated gaze features trained on the three eye - tracking datasets and tested on the same single dataset . note that the drop in precision between the two metrics indicates that the training set has a significant impact on the model performance .
5 shows the precision ( p ≤ 0 . 05 ) on the conll - 2003 dataset and f1 - score ( f1 ) for using type - aggregated gaze features . the results are shown in table 5 . type - aggregation features significantly improve recall , but do not improve f1 score significantly .
results on belinkov2014exploring ’ s ppa test set . the hpcd approach relies on syntactic embeddings obtained by using autoextend rothe and schütze ( 2015 ) for wordnet 3 . 1 . it also uses syntactic - sg embedding . the results on the original paper are shown in table 1 . glove - retro is an alternative to skipgram on wordnet , and it uses the syntactic features obtained by fauri et al . ( 2015 ) .
results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . table 2 shows the results from the models using oracle pp as the dependency parser . oracle pp also provides a significant boost in ppa acc . compared to other dependency parsers .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . it shows the significant effect of attention removal on the ppa acc . ( normalized by the number of frames ) on the model performance .
2 : adding subtitle data and domain tuning for image caption translation ( marian amun et al . , 2018 ) with subsfull embeddings improves the results for all models except for those using mscoco17 . adding domain - tuned data improves the multi30k performance by 3 . 8 points .
results are shown in table 4 . subdomain - tuned subs1m models outperform the other models in terms of en - de performance on all datasets except for the one that is used in the flickr16 cluster . the results are presented in tables 4 and 5 .
4 shows bleu scores in terms of automatic captions added by marian amun ( marian amun ) . as expected , the better multi30k model outperforms all the other models except for the one that she chose .
5 compares our approach with prior works on en - de embeddings . the results are summarized in table 5 . enc - gate and dec - gate achieve the highest bleu % scores ( 37 . 40 % ) on a single dataset , which shows that combining visual information with enc - gate improves the interpretability .
performance of subs3m compared to subs6m is presented in table 4 . the results are presented in bold . sub - categories based on visual features dominate the performance of the en - de models , i . e . the " intensemble - of - 3 " model , while the " multi - lingual " model exhibits the most distinctive features .
3 summarizes the results of our method in table 3 . it can be seen that the alternative approaches that rely on word embeddings outperform the original ones , such as the en - fr - wff model , mtld ( which relies on lexical cues ) , and chime - 4 .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 : training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 . our system obtains the best performance on each language .
system reference bleu and ter scores for the rev systems are shown in table 5 . automatic evaluation scores ( bleu ) show that the re - rev systems perform better than the other systems when using only ter features .
2 shows the performance of our supervised model compared to the strong lemma baseline on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . the results are shown in table 2 .
experimental results on synthetically spoken coco ( 2016 ) are shown in table 1 . the model trained on the embedded embeddings of chrupala2017representations is significantly better than the similarly supervised audio2vec - u model . the difference is less pronounced for rsaimage .
1 shows the results for each classifier compared to the original on sst - 2 . for example , orig ( the original ) turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . note that for cnn , the edges of the screenplay are slightly curved , as shown in table 1 . similarly , for rnn , the corners are slightly concatenated with other classifiers .
2 : part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning . these results show that the value of goodness has not decreased significantly since fine - tune .
shown in table 3 , the change in sentiment from positive to negative is larger than that in the original sst - 2 .
table 2 , we report the results of our method with positive and negative evaluations . our joint model improves upon the strong lemma baseline by 10 % on the pubmed metric ( sst - 2 ) . on the other hand , it is less effective .
