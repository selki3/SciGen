results are presented in table 2 . the results of the recursive approach are shown in table 1 . the recursive approach performs better on training than the recur approach , which performs worse on training .
table 1 shows the performance improvement for the balanced dataset compared to the linear dataset . the balanced dataset exhibits the highest throughput compared to linear datasets .
results for each model with different representation are presented in table 2 . the maximum pooling strategy performs better in all model variations than the max pooling approach .
3 shows the effect of using the shortest dependency path on each relation type . the results are shown in table 1 . our model achieves the best f1 ( in 5 - fold ) without sdp .
results are shown in table 1 . y - 3 : y - 3 has a significantly higher f1 score than y - 2 . the results are comparable to y - 1 : y , but the results are significantly lower .
results are shown in table 1 . we compare the performance of the mst - parser with the results of our test . the results of the test are presented in table 2 . our test results are summarized in table 3 .
results are shown in table 4 . the performance of the two indicated systems is significantly lower than that of the other two systems . lstm - parser outperforms all the other systems .
results are presented in table 1 . we show the results of the original and the original in table 2 . we observe that the original is more accurate than the original . we also observe that it is more likely to be wrong than incorrect .
results for the original e2e dataset are shown in table 1 . we compare the performance of the original and the cleaned versions with the original . we also compare the results of our original and our cleaned versions to the original ones .
results are presented in table 1 . we show the results of the original and the original . the original results are shown in table 2 . the original results are similar to the original results , except that the original is more accurate .
results of manual error analysis of tgen on a sample of 100 instances from the original test set are shown in table 4 . our results show that tgen has a large number of errors compared to the original set .
results are presented in table 1 . all models outperform all models except for the ones with the highest performance . our model outperforms all models in terms of all - in - all performance .
results on amr17 are presented in table 2 . we show the performance of the model size in terms of bleu points . our model size is significantly smaller than that of ggnn2seqb .
results are presented in table 1 . we show that the english - german model outperforms the spanish - czech model by a significant margin . the results are shown in table 2 . we also observe that the german - english model outperform the spanish one by a large margin .
table 5 shows the effect of the number of layers inside dc on the performance of the layers . the effect of these layers on the overall performance of dc is shown in table 5 .
results are presented in table 6 . the performance of gcns with residual connections with baselines is comparable to the performance of baselines with the baselines . we observe that the residual connections have a significant effect on gcn performance compared to baselines without residual connections . our results show that residual connections are significantly better than residual connections , as shown in table 5 . gcn has a significantly lower performance than baselines using baselines , with a significant decrease in residual connections compared to baseline .
results are presented in table 1 . we observe that dcgcn ( 2 ) outperforms all models in terms of performance . however , we observe that it outperforms the other models by significantly outperforms both models .
table 8 shows the performance of amr15 on the dev set . we observe that the density of the dense blocks is significantly lower than that of the other dense blocks .
3 shows the results of the ablation study for the graph encoder and the lstm decoder in table 9 . the ablation study shows that the ablation study outperforms the previous model in terms of performance . the ablation studies outperform the previous one by a significant margin . we observe that the ablated model outperforms both the previous two models .
results are presented in table 7 . we show that our initialization strategies outperform all other probing strategies on probing tasks . we also show that we outperform our probing tasks on all probing tasks , with the exception of our probing task .
results are presented in table 1 . table 1 shows the results of our method . the results of the method are shown in table 2 .
results are presented in table 1 . the results are shown in table 2 . sst2 and sst5 outperform sst1 in terms of performance . our model outperforms sst3 by 0 . 2 % and 0 . 1 % respectively .
results on unsupervised downstream tasks are presented in table 3 . the results are shown in the table 3 . our model achieves the best performance with respect to hybrid .
results are presented in table 8 . we show that our initialization strategies outperform all other approaches on supervised downstream tasks . our model outperforms all the other approaches except for sst2 .
results are presented in table 6 . the results for the unsupervised downstream tasks are shown in figure 6 . cmow - c performs better than cbow - r on both the supervised downstream tasks .
results are presented in table 1 . the results are shown in table 2 . the results show that cbow - r outperforms the other methods in terms of depth and depth .
results are presented in table 1 . we present the results of the cmow - r and cbow - c method . the results are shown in table 2 . we observe that the results are significantly better than those of the other methods . we also observe that our results are better than that of the previous methods .
results are presented in table 1 . the results are shown in table 2 . name matching is the most common feature in the system . name matching improves the performance of the system by a significant margin . the results of name matching improve the performance by a large margin by a considerable margin .
results on the test set under two settings are shown in table 2 . we show the performance of the system under the two settings . our system outperforms all the other models . the results of our system outperform all other models except mil - nd .
results are presented in table 6 . we show the results of the entailment ( entailment ) test . our model outperforms the model by a significant margin . the results of our model outperform that of the model .
results are presented in table 1 . we show that the model outperforms the ldc2017t10 model by a significant margin . we also observe that the models outperform ldc2015e86 by a large margin .
results on ldc2015e86 test set are presented in table 3 . we show that the model performs better than the external model when trained with gigaword data .
results of the ablation study on the ldc2017t10 development set are presented in table 4 . the ablation results are shown in table 3 . we observe that bilstm improves the performance of the model by a significant margin .
results are presented in table 1 . we show that the model outperforms the model in terms of the graph diameter . we observe that s2s - gin outperforms all models except for the model , which outperforms both model and model . our models outperform the model by a significant margin .
results are shown in table 8 . we compare the results of the ldc2017t10 test set with the reference sentences . our model outperforms the reference ones by a significant margin . the results of our model outperform the benchmark set by a large margin .
results are presented in table 4 . the results of the 4th nmt encoding layer are shown in the table 4 .
results are presented in table 2 . the results are shown in table 1 . the results show that word embeddings are the most frequent tag . word embedding is the most frequently used tag .
results are presented in table 1 . the pos tagging accuracy scores are shown in table 2 . our results are consistent with the results of our previous study . the results are similar to those of the previous two studies .
results are presented in table 5 . the results of the uni - bidirectional and res - residual nmt encoders are shown in figure 5 .
results are shown in table 8 . the results are on a training set with 10 % held - out . the difference between the attacker score and the corresponding adversary â€™ s accuracy is significant .
table 1 shows the results of training directly towards a single task . the results are shown in table 1 .
table 2 shows the results of the protected attribute leakage experiments . the results are shown in table 2 . we observe that the results are significantly worse than expected .
results are shown in table 3 . the performance of the trained adversary is the difference between the performance of trained adversary and trained adversary . we show that trained adversary scores are significantly lower than trained adversary score .
3 shows the performance of the embedding guarded attribute with different encoders . the performance of embedded guarded is comparable to that of rnn .
results are presented in table 1 . this model outperforms the previous model . the results are shown in table 2 . we also observe that this model performs better than the previous models . we observe that it outperforms all the other models .
results are presented in table 5 . this model outperforms all the other models in the table . the results are shown in table 4 .
results are presented in table 1 . this model outperforms all the other models in the table . we show the results of our model in table 2 . the results of the model outperform all other models . we also show that the model performs better than all the models in our model .
3 shows the bleu score on wmt14 english - german translation task . our model outperforms the previous model by 0 . 99 % and 1 . 15 % respectively . we also observe that our model performs better than the current model by 1 . 01 % and 0 . 01 % .
results published by wang et al . ( 2017 ) show that the model performs better than the model on squad dataset . the model outperforms the model in terms of the parameter number . we observe that the models outperform the model by a significant margin . we also observe that we outperform our model on the model .
results on conll - 2003 english ner task are shown in table 6 . the f1 score for the lstm model is 90 . 94 . our model outperforms all the other models except gru , which outperforms gru .
results are shown in table 7 . snli task with base + ln setting and ptb task with base setting . ptb tasks with base setting is significantly worse than snli .
results are shown in table 1 . the results are presented in table 2 . we observe that the word word word is more effective than word word . the word word has a higher impact on the performance of the system than word - based word word , word word is less effective than word - based words .
results are presented in table 4 . the results of the human evaluation are shown in bold . the best performance of human evaluation is achieved on a scale of 1 to 5 . we observe that human evaluation outperforms human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont ) . we also observe that the best performance among human evaluations is achieved with the highest standard deviation of 1 . 0 .
results are presented in table 1 . we show that the results of the ted talks dataset are comparable to those of the corpus dataset . the results of our dataset are similar to that of corpus . our results show that our corpus dataset outperforms the corpus dataset by a significant margin .
results are presented in table 1 . we show that the results of the ted talks dataset are comparable to those of the corpus dataset . the results of our experiments are similar to that of corpus and corpus . our results show that our results are comparable with corpus .
results are presented in table 1 . we show that the results of the ted talks dataset are significantly better than those of the other datasets . the results are shown in table 2 . our results show that ted talks outperforms the other two datasets in terms of performance .
results are shown in table 1 . the results are presented in table 2 . the results show that the numberroots are significantly better than the number of roots . we also observe that the depthcohesion coefficient is significantly lower than the maxdepth coefficient . our results also show that our results are better than that of the other models .
results are shown in table 1 . the results are presented in table 2 . the results show that the performance of the model is comparable to that of the original model . we also observe that the results of the new model are comparable to those of the previous model .
results are presented in table 1 . the performance of the model on the validation set of visdial v1 . 0 is similar to that of the original visdial model .
results are presented in table 2 . the performance of the two models on visdial v1 . 0 validation set is shown in table 1 . the performance on both models is comparable to that of the other two models . our model outperforms the other three models on the visdial validation set .
results are presented in table 5 . table 5 shows the performance of hmd - prec on hard alignments and soft alignments .
results are presented in table 1 . the results of the bertscore - f1 test are shown in table 2 . our model achieves the best performance for the test . the model achieves a better performance than the other models . we observe that our model achieves better results than those of the other model .
results are shown in table 1 . the bleu - 1 model outperforms the bertscore - f1 model by a significant margin . the beru - 2 models outperform the bagel - 1 models by a large margin .
results are presented in table 2 . we use the bertscore - recall model to compare the baselines to the meteor model . the baselines are shown in table 1 . our model outperforms our model by 0 . 939 percentage points .
results are presented in table 1 . m1 and m2 outperform m2 in terms of performance . the results are shown in table 2 . we observe that m1 outperforms m2 by a significant margin .
results are presented in table 1 . we show the results of our model . our model outperforms all the other models . the results of the model outperform all other models except for the model that outperforms the model by a significant margin .
3 shows the results of human sentence - level validation of human sentences . the results are shown in table 5 . our results show that human sentences perform better than machine and human judgments . we also show that the human sentences are more accurate than machine sentences .
results are presented in table 1 . m1 and m2 outperform m2 in terms of performance . the results are shown in table 2 . we observe that m1 outperforms m2 by a significant margin .
results on yelp sentiment transfer are shown in table 6 . our best models achieve the highest bleu than our best models . we also show that our best model outperforms our best ones by a significant margin . the best models outperform the best models by a large margin .
results are presented in table 2 . the number of disfluent reparandum tokens that were correctly predicted as disfluencies is significantly higher than the number of nested disfluency tokens .
table 3 shows the relative frequency of disfluent rephrases correctly predicted for disfluencies that contain a content word in both the reparandum and the repair ( content - function ) . we show the percentage of disfluency predicted for each category .
results are presented in table 1 . the results of our model are shown in table 2 . our model outperforms the previous model by a factor of 0 . 2 . we observe that the results of the model outperform that of the other model . we also observe that our models outperform the other models in terms of dev mean and dev mean .
3 shows the performance of word2vec embedding on the fnc - 1 test dataset . our model outperforms all the other models on the test dataset , except for the cnn - based model . we also outperform all other models by a significant margin .
3 shows the performance of the unified model on the apw and nyt datasets for document dating . the unified model outperforms all previous models .
3 shows the effectiveness of word attention and graph attention for word attention compared to word attention . the results are shown in table 3 . the word attention performance is similar to that of graph attention .
results are presented in table 1 . we show the performance of our model in table 2 . our model outperforms all the other models in the table .
results are presented in table 1 . we show that the method outperforms the other methods in the table . our model outperforms all other methods except the one that outperforms our method . we observe that the system outperforms both the method and the classification method in that it outperforms other methods .
results are presented in table 1 . all models are shown in table 2 . all models outperform all models except for english - only - lm , which outperforms all models in terms of performance .
results on the train test set are shown in table 4 . the train test performance is comparable to that on the test set .
results are shown in table 5 . the performance on the dev set is significantly better than on the monolingual set . we compare the performance of the two sets on the test set , as shown in figure 5 .
results are shown in table 7 . precision ( p ) , recall ( f1 ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the precision ( p ) and recall scores are shown to be significantly better than the recall scores .
results are presented in table 5 . precision ( p ) , recall ( f1 ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset ( table 5 ) . the precision ( p ) is significantly better than the recall - based gaze features . the f1 score is significantly lower than that of the pre - trained gaze features ( p ) .
results on belinkov2014exploring â€™ s ppa test set are presented in table 1 . we show the results of the hpcd ( full ) and glove - retro ( full ) . we also show the performance of the lstm - pp test set . the performance of both systems is comparable to that of the original paper .
results from rbg are presented in table 2 . rbg has the highest ppa accomplishment in the uas . we also see that rbg outperforms all other ppaaccomplomplomplishment models .
table 3 shows the effect of removing sense priors and context sensitivity from the model .
results are presented in table 2 . the results are shown in table 1 . we add subtitle data and domain tuning for image caption translation ( bleu % scores ) . we also add subdomain - tuned subtitle data to our dataset .
results are shown in table 1 . we observe that the subs1m model outperforms the subs2m model in terms of performance . subdomain - tuned models outperform the subs3m model by a significant margin . we also observe that subdomain tuning outperforms subdomain tuned models by a large margin .
3 shows the bleu scores for the automatic image captions . the results are shown in table 4 . the results with marian amun are significantly better than the results with flickr16 .
results are presented in table 5 . we compare the performance of the two strategies for integrating visual information . we show that the enc - gate and dec - gate strategies outperform the other strategies . our model outperforms the other two strategies by a significant margin . the results of our model outperform those of the other three strategies , but the results are slightly worse than that of the first two .
results are shown in table 1 . subensemble - of - 3 features outperform multi - lingual features . the performance of subs3m is comparable to that of subs6m . we observe that the performance of subensemble of 3 features outperforms the performance achieved by subs2m ( i . e . , the performance is comparable with the performance obtained by subs4m ) .
results are presented in table 1 . we compare the performance of yule â€™ s i with that of the mtld . yule achieves a better performance than mtld by using mtld and mtld , respectively . in table 1 , we compare yule with mtld to mtld in terms of en - fr - rnn - ff and en - es - t - ff .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs .
3 shows the results of training vocabularies for the english , french and spanish data .
3 shows the results of automatic evaluation scores for rev systems . bleu and ter are shown in table 5 . ter is the automatic evaluation score for the rev system .
results on flickr8k are presented in table 2 . we show that the vgs model outperforms the segmatch model by a significant margin .
results are presented in table 1 . we show the performance of the audio2vec - u model . the performance of audio2vec ( u ) is comparable to that of the other two models .
3 shows the results of the different classifiers compared to the original on sst - 2 . we report the results in table 1 . in the first example , cnn turns on a on ( in the the the edges ) . we note that the on is so clever that it â€™ s so clever you want to hate it .
3 shows the results of fine - tuning on sst - 2 . the results are shown in table 2 . we observe that the number of words in the original sentence has not increased or decreased .
3 shows the results of the sst - 2 experiment . the results are shown in table 3 . the results show that the score increases in positive and negative sentiment , while the score decreases in negative sentiment .
results are presented in table 1 . the results are shown in table 2 . we observe that the results are significantly better than the results of the previous study . we also observe that our results are better than those of other studies . our results are consistent with our previous work .
