2 shows the performance of our recursive framework on the large movie review dataset compared to tensorflow ’ s iterative approach . as table 2 shows , using the recursive framework improves the performance on inference with efficient parallel execution of the tree nodes . further , the use of gpu exploitation improves the model ' s performance on training , as does the number of iterations required to perform the task .
table 1 shows the performance of the treernn model implemented with recursive dataflow graphs . the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization , but at the same time suffers from the small room of performance improvement left , w . r . t parallelization .
2 shows the performance of the hyper parameters optimization strategies for each model with different representation . our approach achieves the best performance with the maximum number of hyper parameters and the average number of feature maps . the max pooling strategy consistently performs better in all model variations with different number of parameters .
1 shows the effect of using the shortest dependency path on each relation type . it can be observed that macro - averaged models achieve the best f1 ( in 5 - fold ) with sdp , while micro - averaging models perform slightly worse . the comparison of our model with the strongest dependency path can be seen in table 1 .
results in table 3 show that for non - y - 3 models , the average f1 score is closer to 50 % , while for y - 3 , the gap between f1 and f1 is close to 50 % .
3 presents the results on the paragraph level . results are presented in table 3 . the results of the best performing model outperform the best state - of - the - art model in terms of both word analogy and sentence comprehension . as expected , the accuracy of the model improves as the word analogy becomes more accurate .
4 shows the c - f1 scores for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . paragraph also shows lower performance , at 60 . 62 ± 3 . 54 % vs . 77 . 42 ± 2 . 57 % for the majority systems .
3 shows the performance of our system for each error generation . our system performs on par with the best previous stateof - the - art model , sc - lstm . the results are presented in table 3 . table 3 shows that our system performs better than the original on both bleu and rouge - l . the performance of the original is slightly worse than those of tgen , but still comparable to the best original model . we notice a drop in performance between the original and the new model due to different training set size .
1 compares our original and our cleaned versions of e2e with the original ( see section 3 ) . the number of distinct mrs , total number of textual references , and ser as measured by our slot matching script , see table 1 . our cleaned version has 15 % more mrs and 39 % more ser than the original .
performance of original and tgen models on the test set of hotpotqa is presented in table 1 . the results are presented in tables 1 and 2 . table 1 shows that the original model outperforms all the base models except for the one that tgen + has used . table 1 also compares the performance of the original model with other widely used methods . the result of re - scoring the original is reported in table 2 . we observe that the accuracy obtained by using the best performing model may vary depending on the underlying model .
results of manual error analysis of tgen on a sample of 100 instances from the original test set are shown in table 4 . we found a total absolute number of errors ( 17 ) in our analysis , which we found to be slight but significant ( 14 % ) . we also found a large percentage of errors in our system ( 17 % ) .
model the performance of our dcgcn model is presented in table 1 . the first set of models achieves the best performance with a gap of 3 . 5 % on the external model compared to the previous state - of - the - art models .
2 presents the results on amr17 . our model achieves 24 . 5 bleu points ( paired t - test ) and achieves 57 . 9 bleu points . across all models , we observe that dcgcn performs significantly better than the ensemble model .
results of experiment 1 are shown in table 1 . the best performing model is bow + gcn ( bastings et al . , 2017 ) . the bach et al . ( 2018 ) model outperforms both the published and unpublished work on every metric by a significant margin . it achieves the best results in english - language and german - language , while the average bias metric drops by 1 . 8 points in german .
5 shows the effect of the number of layers inside dc on the performance of our model in table 5 . the first set of layers shows the diminishing returns from mixing multiple layers of the stack . as table 5 shows , for every layer , there is a drop of 1 . 5 points in performance .
6 compares gcn with baselines . rc + la ( 2 ) and residual connections show that gcns with residual connections have higher performance than those without . however , compared to dcgcn2 ( 6 ) , gcn has higher performance . we observe that when gcn is trained on residual connections , the gcn improves performance .
model f1 shows that dcgcn ( 2 ) outperforms all state - of - the - art models in terms of bias metric , and in general terms , the improvement is modest but significant , reaching a high level of performance even under the difficult requirement of high - supervision .
8 shows the ablation study on the dev set of amr15 . the results show that removing the dense connections severely decreases the performance of the model .
table 9 , we show the ablation study results for the four types of decoder modules used in the standard lstm decoder . the results are presented in table 9 . the hierarchical structure of the encoder modules ( 25 . 9 % ) leads to a significantly better performance than the other four types .
results for initialization strategies on probing tasks are shown in table 7 . our paper shows that our method significantly improves the performance on the subtense task by 2 . 8 points .
1 and table 2 summarize our results on the hidden test set of cbow / 400 . we observe that our approach obtains the best performance on every metric with a gap of 10 . 5 % in precision . it also improves on the coordinv and subjnum scores by 3 . 8 points .
cbow / 784 improves upon the best state - of - the - art models subj and mpqa by 3 . 8 % in terms of mrpc score . however , it still outperforms both sst2 and sst5 by a margin of 2 . 6 % in mrpc score . sick - e also improves by 2 . 4 % in sst - b score . it also outperforms the sst3 score by 1 . 7 % in the mrpc test set .
results on unsupervised downstream tasks attained by our models are shown in table 3 . our model outperforms the hybrid model on all downstream tasks , except for sts13 , which shows the relative change with respect to hybrid . on the sts14 dataset , it achieves a performance improvement of 25 . 6 % compared to 27 . 2 % under the previous model .
8 shows the performance of our initialization strategies on supervised downstream tasks . our paper shows that our approach improves the performance by 3 . 8 points over the best state - of - the - art model on mpqa , sst2 and sst5 .
6 shows the performance for different training objectives on the unsupervised downstream tasks . cbow - r improves on the sts12 and sts14 tasks . however , it does not outperform the cmow - c model on all the supervised tasks .
1 and table 2 summarize our results on the subtense and nested subjnum tasks . we observe that cbow - r significantly outperforms somo in both dimension and subtense subtasks , and that it has a better grasp on the hierarchical nature of the subjnum tasks .
subj and sick - r perform comparably to other methods except for the cbow - c model , which obtains the best performance on both mr and sst2 datasets . however , subj has the worse performance on mpqa dataset and is significantly less effective on sst5 datasets . this suggests that cbow has better generalization ability on both datasets . it obtains better performance on the mr / sst2 dataset than sst3 , sst - b and sts - b .
3 presents the results of all methods for e + and per . our system outperforms all stateof - the - art methods except for the one that does not use org and misc . the results of the best performing system , mil - nd , are presented in table 3 . we observe that the combination of human - generated and entity - generated information ( e . g . name matching , multi - factor learning ) significantly improves the general performance of the system , however , it does not improve the e + org score significantly .
results on the test set under two settings are shown in table 2 . our system outperforms all the models except mil - nd ( model 1 ) in terms of e + p score and f1 score . supervised learning improves the results for all models except for those that do not use the supervised learning model . as expected , the performance drop significantly for both models when using supervised learning . the results of re - training the models after applying the best performing feature set ( mil - nd ) is shown in tables 2 and 3 .
table 6 , we report the results of ref and ref compared with ref , and gen scores for g2s - gat . ref significantly outperforms ref but does not exceed ref by a significant margin . epm also exhibits significant over - fitting since ref alone does not improve the general performance of the model .
1 and table 2 summarize our results on the ldc2015e86 and ldc2017t10 datasets . our model outperforms all the base lines except for the one that has the " eor " label on it . note that our model has the advantage of significantly better performance on both datasets .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . our model obtains a better performance on the external and internal test sets than the ones trained with the pre - trained g2s - ggnn .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results of this study show that the use of bilstm improves the model ' s performance .
results are presented in table 4 . the results displayed in the table seem to indicate that the g2s - gin model has the best overall performance when compared to other models in terms of sentence length . as expected , the average number of frames taken to compute the sentence length of the model drops significantly as the model grows and grows . we observe that for the 50 - 240 δ range , the model achieves the best performance .
shown in table 8 , the fraction of elements that are missing in the input graph that are present in the generated sentence ( g2s - gin ) , for the test set of ldc2017t10 . note that the use of token lemmas in the model results in a better picture , but it does not improve the picture for the gold model .
4 shows the performance of our approach using the 4th nmt encoding layer . our model outperforms both the standard embeddings and pos tags with a large corpus ( 200k sentences ) .
2 shows the pos and sem tagging accuracy with baselines and an upper bound . accuracies are shown in table 2 . using unsupervised word embeddings improves the model ' s performance . word2tag also exhibits higher accuracy .
results are presented in table 4 . our proposed method outperforms all the base methods except for the one that verifies our hypothesis . our method obtains the best performance . our results show that our proposed method significantly improves the performance for both groups .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we find that for bi , pos accuracy is 87 . 9 % and res accuracy is 91 . 9 % . for bi , our model achieves a 3 . 8 % improvement over the previous state of the art model .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ . in pan16 , the attacker scored 10 % higher on the training set compared to the previous best performance . similarly , the gender - neutral approach contributed 9 . 7 % higher performance .
results in table 1 show that training directly towards a single task can improve the performance for all training participants .
2 shows the effect of the additional cost term on the balanced and unbalanced task averages . the classifiers trained on the pan16 dataset have been shown to have a significant impact on the task performance . however , the presence of the gender - neutral classifier in the data has resulted in a significant drop in performance , leading to a drop in overall performance .
performance on different datasets with an adversarial training set is shown in table 3 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ . sentiment and gender are the most important factors in predicting whether an attacker will miss a task or not . in pan16 , the average age of the participants is 62 . 5 and the average gender is 58 . 5 .
6 shows the performance of different encoders when embeddings are trained on the same protected attribute . embedding is more stable , but the performance is worse when trained on rnn .
results reported in table 1 show that our approach outperforms the previous stateof - the - art models on both base and finetune models . the results of our model seem to indicate that the use of dynamic modeling improves the model ' s performance on the largescale wt2 and wt2 + finetune datasets . however , it is harder to achieve the best results on both datasets when using the dynamic modeling approach . table 1 shows that our model performs on par with the strong lemma baseline of the original wt2 model ( which yang et al . ( 2018 ) and the finetuning baseline of wt2 + finetune dataset . our model obtains the best performance on both subsets .
performance of our model on the test set of hotpotqa in the distractor and fullwiki setting is presented in table 4 . we observe that our model significantly outperforms previous models in terms of training time , both on the training set as well as the total number of iterations . the difference between our model and the previous model is minimal , however we observe that it has the advantage of training on a larger corpus .
3 shows the performance of our model compared to previous models . our model improves upon the best state - of - the - art model on three of the four datasets . the results are summarized in table 3 .
3 shows the bleu score of our model on wmt14 english - german translation task . as table 3 shows , the tokenization approach has the best performance , measured in seconds , on the newstest2014 dataset . it also outperforms the state - of - the - art gru model by a noticeable margin .
4 shows the performance of our model with respect to match / f1 score . the results published by wang et al . ( 2017 ) show that our # params model significantly outperforms the model with a parameter number of 2 . 44m and a f1 score of 73 . 83 / 83 . 86 on squad dataset . however , our model obtains a better match / score score than all the other models with the same parameter number .
6 shows the f1 score of our model on conll - 2003 english ner task . lstm * denotes the model with the highest parameter number . it can be observed that our model significantly outperforms other models in terms of number of parameters .
7 shows the performance of our model with base + ln setting and test perplexity on snli task with base setting . it can be seen that our model performs better than the other models on both snli and ptb task .
results are shown in table 1 . word models trained on the oracle retrieval dataset ( mtr ) outperform human models in terms of system evaluation . table 1 shows the results for both human models using the word " retrieval " . the word " evaluation " is used to describe the results of the system evaluations . the system evaluations performed by human models are significantly better than the previous state of the art systems . using the word ' evaluation ' as the base case , the human model obtains the best results with a minimum of 80 % chance of error on the system .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all systems is reported in table 4 . our system is ranked in the top 1 or 2 for each of the four aspects .
results are presented in table vii . our proposed model outperforms all the base models except for the one that embeds the word embeddings . it closely matches the performance of df , docsub and tf . however , it performs slightly worse than the other base models . for example , our model performs slightly better than df , tf and docsub , but significantly worse than our model .
3 shows the performance of all the models trained on the corpus dataset compared to the baseline . our proposed model outperforms all the base models except for the one that embeds the word embeddings . it obtains the best performance on all three datasets , outperforming df , docsub and hclust . the results are summarized in table 3 . for df , we use the best performing model , europarl , and ted talks .
3 shows the performance of all the models trained on the corpus dataset compared to the baseline . our proposed model outperforms all the base models except for the one that relies on word embeddings . it obtains the best performance on all three datasets , outperforming both the baseline and the debiased docsub baseline . the results are summarized in table 3 . for corpus , we use the best performing model , europarl , which performs on par with the best baseline model .
embeddings for our dataset are shown in table 3 . our approach obtains the best performance on every metric with a gap of 1 . 78 points from our baseline . our joint model outperforms all the base models except df , docsub and hclust . we observe that our joint model has the worst performance on both metric with an absolute improvement of 1 point over our baseline model .
embeddings for our dataset are shown in table 1 . our approach obtains the best performance on every metric with a gap of 1 . 5 points . our joint model outperforms all the base models except df , docsub and hclust . our model achieves a better performance on all metrics with a drop of 1 point from our baseline .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf outperforms the enhanced version of qt , r1 , r2 and r3 by a noticeable margin .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the validation set of visdial v1 . 0 . the best performing model is lrv , which can be seen in table 2 .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . our hmd - recall + bert model outperforms all the other models except for the one that relies on word embeddings for alignment .
3 presents the results of our approach with respect to baselines . our approach obtains the best performance on every metric with a minimum of 0 . 5 bertscore - f1 score . the results are presented in table 3 . the most striking thing about our approach is that it obtains a superior score on all metrics without sacrificing too many correct answers .
performance of these models on the sfhotel test set is presented in table 4 . the bleu - 1 model outperforms all the baseline models except for those using bertscore - f1 . table 4 shows the performance of the models trained on the smd dataset .
performance of the models according to these baselines is reported in table 4 . the results are summarized in bold . leic scores significantly outperform the meteor scores of both m1 and m2 while spice scores are slightly worse than those of m2 .
results are shown in table 4 . the m1 model outperforms all the other models except the m2 model in terms of word embeddings . as expected , the performance drop between m1 and m2 models is much lower than that of m2 .
3 presents the results of all models tested on the validation set of hotpotqa . the results are summarized in table 3 . we observe that the transfer quality and the semantic preservation scores are the most important components of semantic preservation . semantic preservation and transfer quality scores have low correlation with semantic preservation , however , the best results are obtained using the multi - factor setup ( m6 , m7 ) and m7 .
5 presents the results of human validation using the [ italic ] ρ b / w negative pp and human ratings of fluency . it can be seen that both the quality of the generated sentences and the percentage of negative pp are significant , improving the performance for both sim and human .
results are presented in table 4 . the results are summarized in table 5 . we observe that the m0 model outperforms the m2 model on all metrics except for the shen - 1 metric , which shows the diminishing returns from using syntactic or semantic information . in particular , the performance drop between m0 and m6 shows that the use of syntactic and semantic information can improve the model ' s performance .
results on yelp sentiment transfer are shown in table 6 . our best models achieve higher bleu than those using simple - transfer or n - word embeddings , but the highest acc ∗ score is found in the single - sentiment weighted corpus of fu - 1 . multi - decoder models perform slightly worse than the best previous work on this metric , but are comparable to the best recent model , yang2018unsupervised . sentiment transfer is restricted to 1000 sentences and human references , so the improvement is less striking than in previous work , where we used the best performing model . we also observe that the use of classifiers in the transfer setup hurts the model ' s performance , since it is more likely to misplace the inputs .
statistics for nested disfluencies are shown in table 2 . the percentage of repetition tokens that were correctly predicted as disfluent was 8 % , which means that the accuracy obtained by rephrase could be higher than expected .
3 shows the relative frequency of rephrases predicted as disfluent for both the reparandum and the repair ( table 3 ) . the fraction of tokens predicted to contain a content word is shown in parentheses , indicating that the disfluencies in the word contain a lot of content word . table 3 shows that the percentage predicted as containing a word is significantly higher than the number predicted as contained in the original .
results of experiment 1 are presented in table 2 . we observe that the use of text + innovations improves the model ' s performance over the single model by 0 . 2 points over the best performing model on every metric .
performance of our model on the fnc - 1 test dataset is shown in table 2 . our model achieves the state - of - art performance with the help of rnn - based sentence embedding . however , it does not improve significantly over the widely used rnn embeddings .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the best performing model is neuraldater . the ac - gcn model significantly outperforms all previous models .
3 shows the performance of our method with and without attention . it obtains a 3 . 8 % improvement over the previous state of the art model on word attention and graph attention .
model performance in table 1 shows that all models trained on jvmee outperform the baseline on every stage except trigger . the performance gap between model 1 / 1 and model 2 / n is 2 . 5 points , which shows the diminishing returns from mixing pre - trained and un - trained models . however , for all models , the performance gap is much larger than that of jvmno .
1 and table 2 show the performance of our method on the event domain . our method outperforms all state - of - the - art methods in terms of both event identification and event classification . the method has the advantage of significantly better performance on both event and event identification .
can be seen in table 4 . all but fine - tuned - lm models outperform the baseline model in terms of dev perp , test acc and test wer , respectively . however , fine - tuning improves the results for all models except for those that do not use the word embeddings .
results on the dev set and on the test set are shown in table 4 . fine - tuned training with only subsets of the code - switched data in it achieves the best results . this shows that using discriminative training , the model can improve the train dev performance and the train test performance .
5 shows the performance of our model on the dev set and the test set , compared to fine - tuned - disc . our model obtains the best performance on both sets , outperforming both the monolingual and code - switched models .
results for the conll - 2003 dataset are shown in table 7 . precision ( p ) , recall ( f1 ) and f1 - score ( f ) show statistically significant improvement over the type - aggregated gaze features trained on all three eye - tracking datasets and tested on the same single dataset .
5 shows precision ( p ) , recall ( f1 ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . type - aggregation features significantly improve recall ( p ≤ 0 . 01 ) for the comparison with the original embeddings , indicating that the use of pre - trained gaze features improves the recall .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . the hpcd approach relies on syntactic embeddings obtained through autoextend rothe and schütze ( 2015 ) and glove - retro . the results on the original paper are presented in tables 1 and 2 . these results use the best performing base z - based embedding schemes . wordnet 3 . 1 has the best performance , but it has the worst performance . we notice a drop in performance between the original and the development set of wordnet .
results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . table 2 shows that using oracle pp as the dependency parser , rbg improves upon the best state - of - the - art ppa acc . model by 3 points .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . it shows the significant drop in ppa acc . from 90 % to 94 % for the model , which shows the diminishing returns from removing context sensitivity .
2 shows the performance of domain - tuned models compared to the en - de model . the results are summarized in table 2 . subdomain tuning improves the multi30k model by 3 . 8 points , but does not improve the bleu % scores for image caption translation .
results are shown in table 4 . subdomain - tuned subs1m models outperform the models in all but one of the comparisons . the results are summarized in tables 4 and 5 . table 4 shows that incorporating domain - tuning improves the results for all models except for those using the mscoco17 dataset .
4 shows bleu scores in terms of the model captions added using the best five models . the results are shown in table 4 . as expected , the multi30k model outperforms all the other models except for the one that had the best 5 models in the model .
5 compares the performance of our approach with prior approaches on en - de and flickr16 . as table 5 shows , using the multi30k + ms - coco + subs3mlm embeddings improves the bleu % scores for both visual information and dec - gate . as expected , our approach has poor performance on both datasets ( e . g . , enc - gate = 44 . 9 % ) , which shows the diminishing returns from mixing visual information with enc - gates .
performance of subs3m compared to subs6m is presented in table 4 . the best performances are obtained on the en - de ( " flickr16 " and " mscoco17 " datasets , respectively . table 4 compares the performance of these models on the single - domain datasets . as expected , the ensemble - of - 3 model outperforms all the other models except for the one that relies on word embeddings . subsequent work on the flickr16 dataset shows that the combination of the visual features and the output of the modules further improves performance .
1 and table 2 summarize our results on the word embeddings for english , spanish , french , dutch , russian , russian and turkish . the results are summarized in table 2 . we observe that the best performing model is the en - fr - rnn - ff model , which significantly outperforms the alternatives in terms of translation quality .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used . we used en – fr and en – es as the training splits .
2 provides an ablation study on the english , french and spanish vocabularies used for our models . we use the best performing src model , en – es , while trg has the worst performance .
system reference bleu and ter scores for the rev systems are presented in table 5 . automatic evaluation scores ( bleu ) show that ter and en - fr - rev have far worse performance than the baseline , indicating that future work may need to design more sophisticated systems to better interpret these data .
2 shows the performance of our model compared to the previous supervised model on flickr8k . the results are summarized in table 2 . our model obtains a higher recall rate than the average rsaimage model .
experimental results on synthetically spoken coco ( vgs ) are shown in table 1 . the model trained on the embeddings of chrupala2017representations is comparable to the best performing rsaimage model . however , our model is significantly worse than the baseline model .
1 shows the results for each classifier compared to the original on sst - 2 . for example , cnn turns in a < u > screenplay that has edges edges edges and curves and is very clever at the edges ; it ’ s so clever you want to hate it . similarly , for rnn , the edges edges of the screenplay have curves and forms similar to those of the original .
2 shows the results for part - of - speech ( pos ) changes in sst - 2 . the results indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning . these results show that the value of goodness has not decreased , however , the percentage of occurrences that have remained the same has increased .
results in table 3 show that sentiment has been flipped from positive to negative in sst - 2 . this shows that the effect of the flipped sentiment signal is significant .
table 2 , it can be seen that more than 50 % of return on investment ( sst - 2 ) is positive , while 50 % is negative . this suggests that more research is needed to improve interpretability . table 2 also highlights the impact of using word embeddings for scientific research . it can be observed that the use of psi improves interpretability without a drop in performance . however , it is difficult to confirm whether a model has good interpretability or whether it has a deficiency .
