2 : throughput and training on the recursive framework , and tensorflow ' s iterative approach , with the large movie review dataset . with the expansion of the batch size from 1 to 25 , the system achieves the best performance on both inference and training datasets . as the table 2 shows , the recursive approach performs the best on both training and inference datasets , while it requires significantly more computation time . further , both the expansion size and the number of iterations required to train the treelstm model achieve the best results .
1 shows the overall performance of the treernn model implemented with recursive dataflow graphs . the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left to train the system .
2 shows the performance of the max pooling strategies for each model with different hyper parameters . the performance of conll08 is shown in table 2 . the hyper parameters generated by softplus perform better in all model variations with different number of parameters . sigmoid also outperforms softplus in the number of iterations with different f1 scores . as shown in the second example , the maximum pooling strategy achieves the best performance with a 4 - 5 representation .
1 shows the effect of using the shortest dependency path on each relation type . it can be observed that the macro - averaged model achieves the best f1 ( in 5 - fold ) with sdp , and the smaller difference in diff . diff . is due to the shorter dependency path . the relation types that rely on sdp as dependency path have the highest f1 and diff .
results in table 3 show that for all three models , the average f1 and average r - f1 are closer to 50 % , while for y - 3 , the gap is closer to 60 % .
results are presented in table 1 . all models trained on mst - parser outperform the baseline on all metrics except for the paragraph level , which indicates that the model performs well on the essay level . the results of the best performing model are reported in tables 1 and 2 .
4 shows the c - f1 scores for the two indicated systems ; the best performance is 60 . 62 ± 3 . 54 on the runs given in table 2 .
results are shown in table 4 . the original and the new systems perform better on bleu , nist and meteor , while the original has a slight improvement on accuracy . the difference between original and original is less pronounced , however it is statistically significant .
shown in table 1 , the original and the cleaned versions have the highest number of distinct mrs , total number of textual references , and the number of instances as measured by our slot matching script , see section 3 . the cleaned version has the highest percentage of instances , but it has the smallest number of fragments .
performance of original and original models on test set is presented in table 4 . the results are presented in bold . original models generally perform better than tgen , while original models tend to have worse performance . the difference between original and original models is less pronounced , however the difference between accuracy between original and original is larger .
results of manual error analysis of tgen on a sample of 100 instances from the original test set is shown in table 4 . the percentage of errors we found in the original dataset was 15 % , so we found no significant difference in those numbers .
model the performance of all models is presented in table 1 . the most representative model is the dcgcn ( single ) model , which performs better than all the other models except for seq2seqk ( konstas et al . , 2017 ) . on the external model , all models perform better than tree2str and pbmt ( mikolov and favre , 2016 ) . however , the difference between the performance of the two is minimal , i . e . , the difference is not statistically significant ( table 1 ) .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points on the model size in terms of parameters , compared to the previous best state - of - the - art model , seq2seqb ( beck et al . , 2018 ) . with respect to ensemble models , we observe that dcgcn ( ours ) achieves a comparable performance to the best state of the art model , on the size of the ensemble model .
results in table 1 show that the single model performs better than the other models that rely on the cnn + gcn model . the results in english - german and english - czech show that both the single and the multi - step model perform similarly to the original bach et al . , the difference in performance between single and multi - step models is minimal , however , as the difference between the english - language and german - language models is considerable . we notice that the gap between the single / multistep model and the seq2seqb model is small , however , this difference is not statistically significant , as it does not represent a significant big performance drop .
5 shows the effect of the number of layers inside dc on the performance of the layers in table 5 . as table 5 shows , for every layer with at least one layer , there are two layers that contribute significantly to the performance . for example , for " italic " n , there is only one layer that contributes substantially to the overall performance .
6 compares gcn with baselines . rc + la denotes gcns with residual connections . as shown in table 6 , when gcn has residual connections , the gcn gains 0 . 7 % overall gcn + rc + la ( 2 ) . however , when we have residual connections with multiple gcns , the performance drops significantly . with the exception of dcgcn2 ( 6 ) , the performance remains the same with all baselines except for dcgcgcn3 ( 9 ) .
model f1 shows the performance of dcgcn models when trained on a single set of 1000 examples . the results are presented in table vii . the first set of models shows that when trained only on 300 examples , the performance drops significantly when trained with a larger number of models .
8 shows the ablation study results for amr15 . the results show that removing the dense blocks severely decreases the performance of the model .
9 shows the ablation study for the graph encoder and the lstm decoder . encoder modules used in table 9 show that the multi - decoder design has the best performance . the gap between the quality of the original encoder modules and the global node is narrower .
results for initialization strategies on probing tasks are shown in table 7 . our paper shows that our method significantly outperforms the competition on all three tasks .
observe that cbow / 400 has the best performance on both subtense and subtense subtasks , while it has the worst performance on the subtense subjnum .
subj and sick - b have the best performance on both mr and mpqa datasets . cbow / 784 shows a slight improvement over the performance of sst2 , while it has the worse performance . on the other hand , it is still superior to both sick and subj in terms of mrpc score , which suggests that the superior performance of both methods may be due to the high accuracy of subj .
results on unsupervised downstream tasks attained by our models are shown in table 3 . they show the relative change with respect to hybrid as well as to cdow . cbow shows a slight improvement over cmp .
8 shows the performance for initialization strategies on supervised downstream tasks . our paper shows that our approach improves upon the performance of subj and mpqa by 3 . 8 points over sick - e and sst5 .
6 shows the performance for different training objectives on the unsupervised tasks . cbow - c improves significantly on the sts12 and sts14 tasks , while cmow - r performs slightly worse .
observe that cbow - r significantly outperforms somo and wc on every metric except the subtense one , where it obtains the best performance . in particular , it achieves the best results on subjnum and coordinv subtasks , respectively .
subj and sick - r perform better than all the other methods except for cbow - c , which obtains the best performance on both mr and sst2 .
system performance in [ italic ] e + per and supervised learning ( mil - nd ) is significantly better than all systems except for the one that does not use org and does not use misc . in all but one of these systems , the performance improvement is greater than that of any other system that uses org or misc in the original setup . the difference between the performance of all systems is less pronounced in the case of the single - supervised learning setup , where the difference in performance between the original and the supervised learning setup is minimal .
2 shows the results on the test set under two settings . name matching and supervised learning achieve 95 % confidence intervals of f1 scores , which are shown in table 2 . in both settings , the system performs better than the previous state of the art model , mil - nd ( model 1 ) achieves a better e + p score of 42 . 90 ± 0 . 59 and 83 . 12 ± 1 . 15 in [ italic ] e + f1 score . further improvements are seen in the performance of the supervised learning model ( mil - nd , model 2 ) which achieves a comparable performance to the original model on both tests . finally , the improvement in performance is almost entirely due to the enhanced recall ability of the model .
6 : entailment ( ent ) and ref = ref ( g2s - gat ) are the most representative models . ref significantly outperforms other models in terms of ref , ref + ref scores , and epm scores are considerably better than those of s2s .
results in table 1 show that the models trained on the ldc2015e86 outperform the best stateof - the - art models on all metrics , except for the meteor metric , which is more accurate .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . our model outperforms the previous stateof - the - art model on both sets .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . it can be observed that the use of bilstm improves the generalization ability of the model .
results are shown in table 4 . the most representative models are s2s , g2s - gin and g2sg - ggnn . note that the average sentence length of these models is shorter than the average number of frames , indicating that the model is more suitable for the task at hand . finally , the averagesentence length of the models is much shorter than those of other models indicating that there is a need to design more sophisticated models to achieve better results .
shown in table 8 , the fraction of elements that are missing in the input graph that are present in the generated sentence ( g2s - gin ) , is the smallest of the three models that are used in the comparison . it is clear from table 8 that the use of token lemmas in the model results in a significant improvement over the output of the reference sentences .
4 shows the performance of the two approaches using the 4th nmt encoding layer . it can be seen that both approaches have high accuracy when trained with different target languages .
2 shows the pos and sem tags accuracy with baselines and an upper bound . accuracies are shown in table 2 . using unsupervised word embeddings improves the semantic performance for word2tag , while the upper bound decreases the accuracy .
results are presented in table 4 . table 4 shows that the accuracy obtained by applying the best performing method is substantially superior to the performance obtained by using the standard schemas algorithm . the results also show that the combination of pos tagging accuracy and spagging accuracy significantly outperform the competition on both tasks .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we find that for all but bi , pos accuracy is 87 . 9 % and res accuracy is 91 . 9 % .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is 10 . 2 % on a training set 10 % held - out . it is clear from the table 8 that the asymmetric nature of the training set leads to a significant drop in performance for the attacker on these datasets .
results in table 1 show that the training directly towards a single task can improve the performance for both groups .
2 shows the effect of the additional cost term on the balanced & unbalanced task acc and task leakage splits . the classifier pan16 is particularly sensitive to the word " task " . it can be observed that both the classifier and the gender - neutral part of the data are at risk of leakage , as shown in fig . 2 .
performance on different datasets with an adversarial training set is shown in table 3 . the difference between the performance of the trained classifier and the corresponding adversary is the difference between their training accuracy and that of the corresponding training set . sentiment and gender are the most important factors in predicting an attacker ’ s task performance . in pan16 , the average age of the participants is 62 . 5 and the average gender is 57 . 5 . we notice that the training set contains gender - neutral features , as these are used to train the classifier .
6 shows the concuracies of the protected attribute with different encoders . embedding with rnn embeddings can further improve the performance for both models .
results are shown in table 4 . the first set of results show that the lstm model outperforms both the original and the finetuned wt2 model by a significant margin . it further improves upon the strong lemma baseline on both wt2 and wt2 by 3 . 5 points . finally , it achieves the best performance on both sets , with an absolute improvement of 2 . 3 points over the previous state of the art model on the wt2 dataset .
performance of our model compared to previous models is reported in table 4 . the results of experiment 1 show that our model significantly outperforms previous models in terms of both acc andbert time , the difference in performance between experiment 1 and experiment 2 is minimal , however it is significant due to the high accuracy of the model in relation to both datasets . we observe that the difference in training time from experiment 1 to experiment 2 indicates that the model is more suitable for the task at hand . when training time is increased , the model performs better on the training time and on the bleu dataset .
3 shows the performance of our model compared to previous work on the amapolar time and yelpfull time datasets . the results are summarized in table 3 . we observe that our model significantly outperforms other models in terms of both err and polar time , as the results of experiment 1 show , the difference between ama and full time is minimal , however , this model significantly improves the performance on both datasets .
3 shows the bleu score on the wmt14 english - german translation task . it is clear from table 3 that the tokenized approach has far superior performance to the state - of - the - art model on both datasets . further , it is clear that the value - pooled approach has a significant impact on translation performance .
4 shows the performance of our model with respect to match / f1 score on squad dataset . the results published by wang et al . ( 2017 ) show that our model significantly outperforms the baselines with a parameter number of 2 . 42m and a f1 score of 73 . 83 / 83 . 83 . however , it is still inferior to the strong lemma baseline of lstm due to the higher parameter number .
6 shows the f1 score on conll - 2003 english ner task . the lstm model significantly outperforms the other models in terms of parameter number . it can be observed that lrn even achieves better f1 scores than gru and atr .
7 shows the performance of our model with base + ln setting and test perplexity on snli task with base setting . it can be seen that both models have the advantage of better interpretability .
results are shown in table 2 . word embeddings are used for system retrieval , while word - based mtr is used for word - based systems . all systems trained on the word " retrieval " are significantly better than the system trained on it .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all systems is highlighted in bold , with statistical significance marked with ∗ . seq2seq performed similarly to retrieval on k 1000 and k 2000 , with the highest standard deviation of 1 . 2 for each system being ranked in the top 1 or 2 for overall quality .
results are shown in table vii . the most representative models are europarl , libris , docsub and eurparl ( p < 0 . 005 ) . they outperform all the other models except for the one that pretends to be docsub . in particular , the two that pretrain their models are significantly better than the others . for example , for docsub , the performance of docsub is slightly worse than the other two models , but still comparable .
3 shows the performance of all the models trained on the corpus dataset . the most representative ones are europarl , libris , docsub and eurparl . they perform slightly better than the others on all three datasets . however , their performance is slightly worse than those on the other two datasets . for example , on docsub , we see that the difference in performance between the two datasets is less pronounced , indicating that the model performs better on both datasets .
results are shown in table vii . the most representative models are europarl , libris , docsub and eurparl . they all outperform the baseline on all three datasets except for the one that is slightly better on the f1 metric . in particular , they have higher performance on docsub compared to the baseline . similarly , for the other two datasets , the difference between the performance of the best two models is much smaller .
embeddings are shown in table 1 . we observe that our system achieves the best performance on every metric with a gap of 1 . 78 points in the truedepth metric . this compares to the performance of eurparl and europarl , which achieves the highest depthcohesion .
3 shows the roots and metric scores of corpus and europarl . our joint model achieves the best overall score on both metrics . relative to the baseline , our joint model is better than both the baseline and the maxdepth of our model . on the other hand , our model is slightly worse than the baseline on all metrics .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf outperforms the enhanced version of qt , r1 , r2 , and r3 by a noticeable margin .
performance ( ndcg % ) of ablative studies on different models on the visdial v1 . 0 validation set is shown in table 2 . using only p2 indicates the most effective one ( i . e . , hidden dictionary learning ) compared to using p2 with the history shortcut .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . the most striking thing about these models is that their bert scores are significantly lower than those of other baselines , indicating that the hard alignments are harder to detect .
performance of these models on the direct assessment and bertscore - f1 benchmarks is presented in table 4 . the results are presented in bold . the most representative models are ruse ( * ) and f1 ( * ) which significantly outperform the baselines on both sets .
3 presents the bagel and sfhotel scores . the results are summarized in table 3 . the baseline bleu - 1 improves significantly over the baseline on all metrics with a gap of 0 . 5 points from baseline to 0 . 9 points . across all metrics , bertscore - f1 improves by 0 . 3 points .
performance of the models according to these baselines is reported in table 1 . the results are summarized in bold . the summaries are presented in low - supervision settings . they are broken down into three categories : metric , epm , spice and word - mover . epm significantly outperforms the leic baseline on both m1 and m2 .
results are shown in table vii . the most striking thing about this data is that it relies on plain word embeddings instead of lexical representations . this is reflected in the fact that the shen - 1 model performs better on sim than on pp , indicating that the importance of syntactic representations .
results are shown in table 4 . semantic preservation and transfer quality are the most important aspects of semantic preservation , while semantic preservation is the least important . syntactic preservation and semantic preservation are further improved with the addition of semantic tags . the semantic preservation baseline is significantly better than the transfer quality baseline , indicating that semantic preservation relies less on syntactic or semantic information . semantic preservation improves significantly over syntactic preservation δsim , indicating the semantic preservation quality is more important for semantic preservation .
5 shows the results for human evaluation . it is clear from the table that the method requires a considerable amount of effort and time to obtain the correct score . the results are shown in table 5 . sim significantly outperforms human evaluations with 94 % accuracy , indicating that the quality of the sentence is high .
results are shown in table 4 . the most striking thing about this data is that it relies on plain word embeddings instead of lexical representations . this is evident from the large difference in performance between m2 and m6 , compared to m3 , whose use of syntactic representations improves the performance .
6 shows the results on yelp sentiment transfer , where bleu is between 1000 and 1000 sentences and human references are restricted to 1000 words . our best model achieves the highest acc ∗ score at the level of simple - transfer , but it is less than the best model by a significant margin . the results on the single - decoder model are shown in tables 6 and 7 . these results show that the use of multiple classifiers in the transfer setup severely limits the model ' s ability to learn the semantic information required to make the transfer sentence . we also see that the transfer model performs slightly worse than the original model on both datasets , indicating that there is a need to design more sophisticated features for the transfer to achieve the best results .
statistics for nested disfluencies are shown in table 2 . the percentage of repetition tokens that were correctly predicted as disfluent is 8 . 8 % overall , compared to the rate at which repetition tokens were predicted as nested .
3 shows the relative frequency of rephrases correctly predicted as disfluent for both the reparandum and the repair ( table 3 ) . the fraction of tokens predicted to contain a content word in the reparandum is 15 % higher than the fraction predicted as containing a function word . table 3 also shows the distribution of the tokens predicted as contained in the repair and the fraction of those predicted as contain a word that contains a word .
results are shown in table 4 . text + innovations significantly improve the model ' s performance over single model by 0 . 2 points over text + innovations , in addition , text + text + model improvements have a generally positive effect on model performance , the text + data + innovations model achieves the best performance with 86 . 48 % overall improvement over the single model . the results are summarized in table 5 .
performance of our model on the fnc - 1 test dataset is shown in table 2 . our model achieves the state - of - art performance with the help of rnn - based sentence embedding . the accuracy ( % ) disagree with our model is lower than that of word2vec embedding .
2 shows the performance of different methods for the document dating problem on the apw and nyt datasets . the best performing model is attentive neuraldater . on the nyt dataset , it obtains 62 . 2 % higher accuracy .
3 shows the effectiveness of both word attention and graph attention for this task . accuracy ( % ) comparison of our model with and without attention is shown in table 3 .
performance of all models is shown in table 1 . embedding + t model outperforms trigger , jnn , and other models that perform similarly on all stages except for the one in which it performs best .
ert and f1 measures are shown in table 1 . all methods show significant performance improvement on both types of event . in general , all methods show a significant improvement on the performance of the model when combined with the threshold for cross - event identification . the method is particularly effective for event classification , with a large margin for error due to the high precision of the method .
results are shown in table 1 . all : cs - only - lm , fine - tuned - lm and all : shuffled - lm show lower performance on dev perp and test wer , respectively , compared to the original spanish - only model . note that for english - only , the gap between the original and the original embeddings is much narrower , so we see less variation in performance between the two models .
results on the dev set and on the test set are shown in table 4 . fine - tuned training with only subsets of code - switched data gives a 25 % train dev boost and a 75 % train test boost .
5 shows the performance of our model on the dev set and the test set , compared to fine - tuned - disc . it can be observed that the difference in performance between the gold sentence and the code - switched sentence is less pronounced in dev set , but still comparable to monolingual language .
7 shows precision ( p ) , recall ( f1 ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the results are shown in table 7 . type - aggregation features have a statistically significant improvement ( p ≤ 0 . 01 ) over the baseline , indicating that the use of pre - trained gaze features has a significant impact .
5 shows precision ( p ) , recall ( f1 ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . type - aggregation features significantly improve the precision ( p ≤ 0 . 01 ) over the baseline , indicating that the use of pre - trained gaze features improves the recall .
results on belinkov2014exploring ’ s ppa test set . syntactic - sg embeddings are the most useful for wordnet , and it improves upon the syntactic embedding obtained by using autoextend rothe and schütze ( 2015 ) . glove - retro also improves the f1 score by 3 . 1 points on the original wordnet dataset , and by 4 . 3 points on wordnet 2 . 1 .
performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results from table 2 show that the combination of oracle pp and lstm - pp pre - trained models significantly boosts ppa acc . performance . however , the biggest drop is from full uas to partial uas .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . it can be observed that the effect is less pronounced for the full model , but still significant for the subtasks .
2 shows the results with domain - tuned and multi30k datasets . subsfull embeddings improve the image caption translation performance by 9 . 5 % over the model on en - de , while the additional domain tuning improves the bleu % scores by 3 . 5 % .
results are shown in table 4 . subdomain - tuned subs1m models outperform all the other models except for those using mscoco17 embeddings . the results are presented in the en - de setting , where the a < cao et al . ( 2017 ) classifier performs best . with domain - tuning , the a − coco model performs better than the other two models on all but flickr16 .
4 shows bleu scores in terms of the automatic captions added after adding the best ones or all 5 models . the results with the best five models are shown in table 4 . as expected , the model using the multi30k model outperforms all the en - de models except for the one that has the best three models .
5 compares the results of different strategies for integrating visual information ( including enc - gate and dec - gate ) . we observe that the best performing model is the multi30k + ms - coco + subs3mlm model , which performs better on larger datasets . further , we observe that for larger datasets , enc - gating improves the bleu % scores by 2 . 5 points . finally , the best performances are obtained on flickr16 , which shows the effectiveness of our approach .
performance of subs3m compared to subs6m is presented in table 4 . the best performances are obtained using the en - de embeddings layer , while the best performance on flickr16 is achieved using the mscoco17 layer . sub - text - only features , such as the text - only feature , and the multi - lingual feature , result in a better performance . finally , the performance of the subs3mo layer is improved with the addition of the semantic features , improving the overall performance by 2 . 36 points over subs2m .
performance on mtld compared to en - fr - ff is reported in table vii . the results of the best performing model are reported in tables vii and viii . as expected , the performance of these models is significantly worse than those of the other two baselines . table vii shows that for all but the case of simes et al . ( 2017 ) , the performance gap between the two is much smaller .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 : training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 .
system reference bleu and ter scores for the rev systems are shown in table 5 . automatic evaluation scores ( bleu ) and ter are both below the threshold for automatic evaluation , indicating that the system is well - equipped to handle these tasks .
results on flickr8k are shown in table 2 . pretrained vgs models outperform the supervised model from chrupala2017representations .
results on synthetically spoken coco are shown in table 1 . the visual supervised model outperforms the similarly supervised audio2vec - u model by a significant margin .
1 shows the results for different classifiers compared to the original on sst - 2 . for example , cnn turns in a < u > screenplay that has edges at the edges and turns on a on ( in the the the edges ) . this is because it “ s so clever you want to hate it . ” similarly , for rnn ( which has edges edges ) and cnn ( in the the margins ) . these are all examples of the ways in which the word embeddings can be improved .
2 : part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning . these results show that the value of goodness has not decreased , however , the amount of occurrences has increased , either by a significant amount or by a considerable amount .
shown in table 3 , the effect of the flipped switch on sentiment is less pronounced in sst - 2 than in the case where negative labels are flipped to positive .
results of experiment 1 are presented in table 1 . it is clear from the table that the use of word embeddings improves the interpretability for both positive and negative evaluations . as expected , the difference between the performance of sst - 2 and sift - 2 is less pronounced for positive evaluations , whereas for negative evaluations , it is much more pronounced for negative ones . the results are summarized in tables 1 and 2 .
