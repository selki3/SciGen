2 shows the performance of our recursive approach on the large movie review dataset compared to our iterative approach , which shows better performance on training compared to the recursive approach . inference performance is comparable to that of recur , but in the inference and inference settings , both perform better .
shown in table 1 , the balanced dataset exhibits the highest throughput , but at the same time performs slightly worse than the linear dataset .
2 shows the performance of the max pooling strategy for each model with different representation . the performance of conll08 is shown in table 2 . it achieves the best performance in all model variations , with the exception of the sigmoid model , where it performs worse than softplus . the hgn model performs better in all models with different representations .
1 shows the effect of using the shortest dependency path on each relation type . our approach shows that our approach achieves the best f1 ( in 5 - fold ) with sdp and the best diff . on the relation type , as shown in table 1 . we observe that the approach is more efficient than macro - averaged approaches , and that it improves the f1 by a significant margin .
results are shown in table 3 . the y - 3 model outperforms the y - 2 model in terms of f1 and f1 scores . in general , the y3 model achieves better performance than the y2 model .
3 shows the performance of our model on the essay level . our model outperforms all the other models except for mate , which performs better on the paragraph level and f1 test set . we observe that mate outperforms mate on both test set , while mate performs worse on the evaluation set . the results of mate are shown in table 3 . the performance of mate on the test set is shown in fig . 3 . in mate , we observe that the accuracy of the model is comparable to that of mst - parser on the validation set . mate performs slightly better than mate , but is closer to f1 .
4 shows the c - f1 scores for the two indicated systems ; essay vs . paragraph level . the results are shown in table 4 . the lstm - parser outperforms all the other systems except for the essay .
results are shown in table 3 . the original and the original results show that the tgen + model are better than the original on all tests except for the one in which the model is completely clean . the results are presented in table 4 . the original model outperforms all the other models except for those that are completely cleaned . it is clear from table 4 that the original model is better than both the original and wrong ones . this is evident from the fact that the new model is more likely to have errors than those that were left in the original . the performance of the original models is also evident in table 5 .
shown in table 1 , the original and the cleaned e2e data are comparable in terms of mrs and ser as measured by our slot matching script , see section 3 .
results are shown in table 3 . original and original models outperform the original model in all but one of the three cases . the original model outperforms both the original and the original models in terms of bleu score . in addition , both the original and the original models perform better than the original ones . the performance of the original model is comparable to that of the original , but the difference is less pronounced in the original . the accuracy of the tgen + model is less significant in both cases .
4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . we found that adding incorrect values to the training set caused a significant drop in accuracy . we also found that removing incorrect values caused a drop in performance .
table 3 , we show the performance of our model on the external and external datasets . our model outperforms all the other models on both datasets except for seq2seqk , where it performs better than all the models except for snrg .
2 shows the results on amr17 . our model achieves a bleu score of 25 . 5 on the model size in terms of parameters , compared to the previous best state - of - the - art model , seq2seqb ( 2016 ) .
results are presented in table 3 . the best performing models are bow + gcn and bow - gcn ( beck et al . , 2018 ) . we observe that the single model outperforms all the other models in english - german , with the exception of birnn and seq2seqb . we also observe that both the single and the multi - language models outperform all the others in terms of performance .
table 5 shows the effect of the number of layers inside dc on the performance of the layers in the network . we observe that dc has the greatest effect on the overall performance of all the layers , with the exception of the layer with the most layers .
6 shows the performance of the baselines with residual connections . gcn + rc ( 2 ) and gcn + la ( 4 ) show significant performance improvement over baselines . the results show that the residual connections gcn has better gcn performance than those without .
3 shows the performance of the dcgcn model compared to other models . the results are presented in table 3 . our model outperforms all the other models in terms of performance . in particular , we see a significant drop in performance compared to the previous models .
8 shows the ablation study for amr15 . - { i , 4 } dense blocks denotes removing the dense connections in the i - th block . table 8 shows that the reduction in the number of dense blocks indicates that the model is able to maintain the high quality of the network .
table 9 shows the ablation study for the graph encoder and the lstm decoder . the results are shown in tables 9 and 9 . the best results are obtained in the table 9 , where the word encoder has the best correlation with the graph attention dataset .
7 shows the performance of our initialization strategies on probing tasks . our paper shows that our approach outperforms the state - of - the - art glorot model in terms of depth and length .
results are presented in table 3 . the best results are shown in bold . our model outperforms all the other methods except for the one that has the best performance . we observe that our model obtains the best results in terms of depth and distance , while the other models have the worst results . we also observe that the h - cmow / 400 model achieves better results in depth than the other approaches .
3 presents the results of our model on the subj and sick - r datasets . our model outperforms all the other models except subj except for the one that has the best performance . subj outperforms both the mpqa and sst2 datasets in terms of mrpc performance . cbow / 784 achieves the best results on both datasets . it also outperforms the other two models in both mrpc and subj datasets by a margin of 0 . 2 % and - 0 . 4 % respectively .
3 shows the performance of our model on unsupervised downstream tasks attained by our model . cbow shows the relative change with respect to hybrid compared to cbow . the results are shown in table 3 . the cbow model outperforms both hybrid and cmp in both downstream tasks .
table 8 shows the performance of our initialization strategies on supervised downstream tasks . our paper shows that our approach outperforms all the other approaches except subj and mpqa except for subj , which are more difficult to predict . subj outperforms subj in all but one of the three metrics , while sst2 performs better than sst3 and sst5 .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the cmow - c model outperforms the cbow - r model on both the sts12 and sts14 tasks .
results are presented in table 3 . we observe that cbow - c outperforms all the other methods in terms of depth and subjnum and coordinv metrics . it obtains the best results on both metrics , with the best performance on both metric . on the subjnum metric , it obtains a better performance than somo - r , and is comparable to both cbow and somo . the best results are obtained on the metric " tense " metric , with a slight improvement on the previous state - of - the - art model . the results are shown in table 4 .
3 presents the results of our model on subj and sick - r . the results are presented in table 3 . our model outperforms all the models except subj except for the one that has the best mrpc score . it also outperforms both the sst2 and sst5 scores on both mrpc scores . it is clear that our model is better than both the subj scores and the mpqa scores on the mrpc test set . however , it is still better than the other two methods .
3 shows the e + org and per scores for all systems in the table . our system outperforms all the systems except for the one that has the best org score and the one with the worst per score . we observe that our system performs better than all the other systems in terms of e + loc and e + per scores . the results are shown in table 3 . our model obtains the best e + loc score and best per scores in all the three scenarios . it obtains an e + org score as well as the best per score in all but one of the three cases . the system obtains a better e + od score than the other two systems , but it performs worse than both our model .
results on the test set under two settings are shown in table 2 . our system outperforms all the other models in e + p and e + f1 scores . our model achieves the best performance in all three settings . we observe that the system performs better than all the models except for the one in which it performs best . we also observe that our model performs better on the e + p test than all models except the one that performs best in both settings .
6 shows the results of ref and ref on the model compared to the previous state - of - the - art models . note that ref outperforms ref in all but one of the cases , while ref surpasses ref by 3 points .
results are shown in table 3 . we observe that the ldc2017t10 model outperforms all the other models in terms of bleu scores . the ldc2015e86 model is significantly more accurate than the previous state - of - the - art models , but still performs worse than the best models .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . our model outperforms the previous state - of - the - art models in both external and external settings .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results show that bilstm can significantly improve the performance of the model when combined with a large number of features . this is evident from table 4 , when we compare the results of our model with other models .
results are shown in table 3 . we observe that g2s - gin significantly outperforms the model in terms of sentence length and sentence length . in particular , the model significantly improves sentence length on average compared to the model , which shows that the model is more accurate at predicting sentences .
shown in table 8 , the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( g2s - gin ) . this shows that the g2s model is better than the other models in terms of token lemmas , as shown in fig . 8 .
shown in table 4 , the accuracy of pos and ar models is comparable to those trained with different target languages on a smaller parallel corpus ( 200k sentences ) . however , pos models outperform ar models in terms of semantic accuracy .
2 shows the accuracy of our model with baselines and an upper bound . the results are shown in table 2 . our model outperforms all the other models except word2tag , which has a lower bound on baselines . we also observe that our model has higher baselines than other models .
results are presented in table 3 . table 3 shows the performance of our model on the three datasets . our model outperforms all the other models in terms of accuracy . the results show that our model significantly outperforms the competition on all three datasets except for the one in which it performs best .
5 shows the accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . our model shows that our approach achieves the best accuracy with the best performance over all four layers .
8 shows the performance of the attacker on different datasets . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 8 . we also show the difference in performance between the training set and the target target .
1 shows the accuracies when training directly towards a single task . for pan16 , we trained directly towards the single task , with the exception of dial . the results are shown in table 1 . the gender of the participants is the most important factor in predicting the performance of the task , and the age of participants is also important .
2 shows the performance of our model in the balanced and unbalanced data splits . we observe that the word " race " has a significant effect on the balanced data splits , as shown in table 2 . in the unbalanced dataset , the word ' race ' has a large impact on the task performance , as does gender .
3 shows the performance on different datasets with an adversarial training . our model outperforms all the adversarial models except for pan16 , which has the advantage of predicting the performance of the corresponding adversary .
6 shows the concuracies of the protected attribute with different encoders . for example , rnn embeddings the word " leaky " in different contexts , while embedding it in the same domain gives the same performance .
3 shows the performance of our model on the two datasets . our model outperforms the state - of - the - art lstm in both base and finetune settings . the results are shown in table 3 . we observe that our model performs well on both datasets , with the exception of the wt2 dataset , where it performs slightly worse than the original model . our model also outperforms both the original and the original models in terms of performance . in particular , we observe that this model performs better than the other models in both datasets .
results are presented in table 5 . we show the performance of our model in relation to the time taken to train our models . our model outperforms all the models in terms of training time . in particular , we observe that our model is better at training time than all the other models except for our model . it also performs better on the time - to - model task as well as the time to train .
3 shows the performance of our model compared to previous work on the amapolar time dataset . the results are presented in table 3 . we show that our model outperforms all the other models in terms of err performance on both datasets . table 3 shows that the amapolar time dataset is comparable to both the yahoo time dataset and the amafull time dataset . the results of this model are shown in tables 3 and 4 . we observe that the difference in performance between our model and the original model is small but significant .
3 shows the bleu score of our model on wmt14 english - german translation task . our model outperforms all the other models in terms of time in decoding one sentence measured on the newstest2014 dataset . it is clear that our model is able to decode one sentence at a time .
4 shows the performance of our model on squad dataset . our model outperforms all the models except lrn and sru in terms of match / f1 score . as expected , our model performs better than all models except sru except for the sru model , which performs worse than sru .
6 shows the f1 score on conll - 2003 english ner task . the lstm model outperforms all the models except sru and sru in terms of parameter number . the sru model achieves the best f1 scores with a score of 90 . 94 . the lrn model achieves a better performance than sru .
7 shows the performance of our model on snli task with base + ln setting and test perplexity on ptb task with base setting .
results are shown in table 3 . word embeddings are used to improve the performance of the system for both human and machine learning tasks . system retrieval improves performance by 2 . 5 % compared to the previous state - of - the - art model . sentiment improvements are reported in table 4 . the word embedding improves performance for human , while machine learning improves the performance for the machine learning task . in machine learning contexts , the word embedded embedding helps to improve performance . log embedding is beneficial for the human , as it helps the system to learn more about the task .
4 shows the human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all systems is shown in bold , with statistical significance marked with ∗ indicating that the system is in the top 1 or 2 for overall quality . we also show the performance of seq2seq and retrieval , which are comparable in quality to human evaluation .
results are shown in table 3 . the results are presented in tables 1 and 2 . our model outperforms all the other models in terms of performance . we observe that the best performing model is term , while term has the best performance . term outperforms term and docsub on many metrics , but term is slightly better than term . we also observe that term performs better than the other two models in all but one of the three cases .
results are shown in table 3 . the results are presented in tables 1 and 2 . our model outperforms all the other models in terms of performance . we observe that the best performing model is term , while term has the best performance . term outperforms term and docsub on many metrics , but term is slightly better than term . we also observe that term ' s performance is comparable to term on all metrics , with the exception of term having the worst performance . in particular , term performs worse than the other two models .
results are shown in table 3 . the results are presented in tables 1 and 2 . our model outperforms all the other models in terms of performance . we observe that the best performing model is term , while term outperforms term . term and term are better than term on many metrics , but term is slightly worse than tf and tf , respectively . we also observe that term has the best performance on both datasets , with term significantly outperforming term in both domains .
results are shown in table 3 . our model achieves the best results on both metric and metric metrics . our approach outperforms the best on all metric metrics except for the metric of depthcohesion . we observe that our model achieves better results on metric metrics than those of the other two models . the results show that our approach is more accurate on metric terms than the best ones .
results are shown in table 3 . our model outperforms all the other models in terms of metric and depthcohesion metrics . we observe that our model achieves the best performance on both metric and metric metrics . it also outperforms both the df and docsub metrics on the metric metrics , with the exception of docsub , where our model obtains the best results . the results are summarized in table 4 . we also observe that the numberroots metric is slightly better than the maxdepth metric on the df metric , but still outperforms our model .
1 shows the performance of our model on the validation set of visdial v1 . 0 . lf outperforms the original visdial model in terms of r0 , r2 , r3 and weighted softmax loss , respectively . qt is the enhanced version , and r2 and r3 denote the hidden dictionary learning . lf + p1 denote the regressive loss of the word embeddings , while r3 denotes the semantic loss .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . our model outperforms all the other baselines except coatt and rva .
5 shows the performance of hmd - f1 + bert on hard alignments and on soft alignments . the results are shown in table 5 . the hmd model outperforms both the soft and hard models in terms of bert performance .
3 shows the performance of our model on the direct assessment task compared to the baselines . our model outperforms all the other models in terms of metrics and bertscore - f1 scores . the results are summarized in table 3 . for example , our model obtains the best performance on both direct assessment and de - en . however , it obtains a lower performance than our model , which obtains only the best results on both datasets .
3 presents the bagel and sfhotel scores on the test set . our model outperforms all the baselines except for bleu - 1 , which is closer to the baseline . we observe that our model significantly outperforms both the baseline and the baseline in terms of accuracy .
3 presents the metric and baselines scores of our models . the results are presented in table 3 . we observe that our model significantly outperforms the baselines on all metrics except for the m2 metric , which shows that the model performs well on both metrics . our model obtains a metric score of 0 . 939 and 0 . 749 , respectively , compared to leic ' s 0 . 839 .
results are shown in table 3 . we observe that the m0 model outperforms the m1 model in all but one of the four scenarios . in sim , we observe that m0 models outperform m1 models in terms of word embeddings . in pp , we see that the shen - 1 model performs better than the m2 model .
3 presents the results of our model on the transfer quality and transfer quality metrics . we show that our model outperforms the best state - of - the - art models in both domains . we observe that the semantic and semantic preservation metrics are comparable across all domains , with the exception of semantic preservation , where semantic preservation is closer to semantic preservation than semantic preservation . the results of the semantic preservation metric are summarized in table 3 . semantic preservation is comparable with semantic preservation on all domains except the semantic ones .
5 shows the results of human sentence - level validation . the results are shown in table 5 . our model outperforms both the human and machine models in terms of acc and pp scores . sim outperforms the machine model in both domains , with the exception of semantic preservation .
results are shown in table 3 . we observe that the m0 model outperforms the m1 model in all but one of the four scenarios . it is clear that the shen - 1 model performs better than the m2 model on both sim and pp datasets . however , the performance of the m6 model is slightly worse on pp than on sim .
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu and the highest acc ∗ score . our worst model outperforms all the other models except for fu - 1 , which achieves the best acc ∇ score . we also observe that our best model , yang2018unsupervised , outperforms both the best and the worst models in both domains . in addition , we observe that the quality of our model is comparable to that of the best models in the previous work . the difference between the two is not significant , but the difference is significant , indicating that our model has a better understanding of sentiment transfer .
2 shows the percentage of disfluencies that were correctly predicted as disfluent , compared to the percentage predicted as nested . reparandum tokens are generally shorter than repetition tokens , indicating that the disfluency is less likely to cause errors in the prediction .
3 shows the percentage of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - content ) . the percentage of tokens predicted to contain the content word is shown in table 3 . the fraction of tokens that contain the word is in parentheses , indicating that the disfluency is not a significant issue for the model .
results are shown in table 3 . we observe that the best models are text + innovations , while the best model is text + text . in addition , we observe that in the early and late stages of the model , the text + features model outperforms all the other models in terms of dev best .
2 shows the performance of our model on the fnc - 1 test dataset compared to the state - of - art embeddings on cnn - based sentence embedding . our model achieves the best performance on both test datasets . it also achieves the highest accuracy on the micro f1 dataset , which shows the state of the art of word2vec .
2 shows the accuracy of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . our unified model significantly outperforms all previous methods except burstysimdater .
3 shows the accuracy ( % ) of our method for word attention compared to the output of graph attention . the results are shown in table 3 . neuraldater outperforms both graph attention and ac - gcn in word attention comparisons . it is clear that the attention - based model is more effective than the output - based one .
3 shows the performance of our model on each stage compared to the previous state of the art . our model outperforms all the state - of - the - art models except for the one that performs best on the argument stage . we observe that the jvmee model performs better on all stages compared to all the other models except the one in which it performs worse . this suggests that our model is more likely to perform better than other models .
3 presents the results of our method on the event identification task . our method outperforms all the other methods in terms of both identification and classification . in particular , we show that our method has the best predictive performance on both trigger and classification task . in both cases , the identification task is more difficult to solve than the classification task . we show that the method obtains the best results on both event identification tasks .
results are shown in table 3 . all models are comparable in terms of performance on dev perp , dev wer and test acc . we observe that all models outperform all the models except for the spanish - only model , which is comparable in performance to english - only . however , all models perform worse than english , with the exception of those that do not have the best performance .
4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . we show that fine - tuned train dev outperforms the state - of - the - art in both train dev and train test .
5 shows the performance on the dev set and the test set , compared to monolingual and code - switched models . the results are shown in table 5 . the performance of fine - tuned models is comparable to the performance of the mono model .
7 shows the precision ( p ) , recall ( f1 ) and f1 scores for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the results are shown in table 7 . the precision ( p > 0 . 05 ) is significantly better than the recall ( p ≤ 0 . 01 ) on the pre - trained dataset .
5 shows the precision ( p ) , recall ( f1 ) and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . the results are shown in table 5 . type - aggregation features significantly improve recall ( p < 0 . 01 ) and recall ( p ≤ 0 . 05 ) .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . the hpcd system uses syntactic - sg embeddings as the base embedding for wordnet and wordnet 3 . 1 , and it uses semantic skipgram embedding as the embedding layer . glove - retro outperforms the syntactic embedding layers of wordnet , but it has the advantage of using syntactic sg embedding . the results on the original paper show that the semantic embedding features are beneficial for wordnet , and the semantic semantic embedding helps wordnet to better interpret the semantic information .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 . we can see that our model outperforms all the other models in terms of ppa accomplishment .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . we show the ppa accomplishment ( ppa ) on the model as shown in table 3 . it is clear that the loss of context sensitivity is a significant part of the performance of the model , but it is not significant .
2 shows the results of domain tuning for image caption translation . our model outperforms all the domain - tuned models in terms of bleu % scores . subdomain tuning improves the multi30k model by 3 . 5 points compared to domain tuning .
3 shows the performance of subs1m overdomain - tuned models in en - de and in - de . the results are shown in table 3 . subdomain tuning improves the performance for all models except for those that do not have domain tuning . we observe that subs2m outperforms all the other models in terms of performance , with the exception of the one that has domain tuning on all models . the results show that domain - tuning improves the quality of the model , with a slight drop in performance for the subset that does not use domain tuning ,
4 shows bleu scores in terms of multi30k captions compared to the best one or all 5 models . the results with marian amun are shown in table 4 . as expected , the model outperforms all the models except for the one that has the best image captions . the model with the best two captions outperforms the model with only the best ones .
5 shows the bleu % scores of the three strategies for integrating visual information . we observe that enc - gate and dec - gate embeddings significantly improve the performance of the two approaches . in the en - de dataset , we observe that the enc - gated approach outperforms both the pretrained and unsupervised approaches in terms of decoding visual information ( e . g . , capt - gate ) .
3 shows the performance of subs3m compared to subs6m in terms of text - only and multi - lingual features . sub3m outperforms all the other models except for the one that relies on word embeddings . in the en - de example , the subs2m model outperforms both the other model and the single - language model , with the exception of mscoco17 , where the model performs better than the other two models .
results are shown in table 3 . we show the results of our model on the mtld test set . our model outperforms all the other models on the test set except for en - fr - rnn - ff , which is comparable to the best state - of - the - art model .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used . the average number of lines in each language pair is 459 , 633 .
2 shows the training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 .
5 shows the bleu and ter scores for the rev systems . the automatic evaluation scores ( bleu ) show that the system is better than the previous state - of - the - art rev system . ter shows that the rev systems are more accurate than the original ones .
2 shows the vgs performance on flickr8k . the results are shown in table 2 . the vgs model outperforms the standard rsaimage model by a margin of 3 . 5 points .
results on synthetically spoken coco are shown in table 1 . our model outperforms all the previous models in terms of recall @ 10 , with the exception of the one that performs better on rsaimage .
1 shows the results of the different classifiers compared to the original on sst - 2 . we report further examples in table 1 . for example , cnn turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . rnn shows the same results as the original . dan shows that the edges of the screenplay are more interesting than the edges . rnn also shows that it is clever to have the edges edges edges in the screenplay . it is so clever that it shows that you want hate hate hate . it shows that when the edges are in the right places , it is easier to hate hate it . cnn shows similar results as rnn .
2 shows the results of fine - tuning on sst - 2 . the results are shown in table 2 . we observe that the number of occurrences have increased , decreased or stayed the same for the same reason as for the original sentence . we also see that the accuracy of the word " nouns " has not increased .
3 shows the change in sentiment from positive to negative in sst - 2 compared to the original sentence . the results are shown in table 3 .
results are presented in table 3 . the results are summarized in bold . our approach shows that the best approach is to investigate the negative aspects of the word , and to compare the results with the positive ones . it is clear that our approach is more effective than the negative ones . however , the results are not statistically significant , indicating that the approach is less effective than positive .
