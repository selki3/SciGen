2 : throughput for processing the treelstm model on our recursive framework and tensorflow ’ s iterative approach , with the large movie review dataset as our training dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the recursive approach shows better performance on training .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left to train the system .
2 shows the performance of the max pooling strategies for each model with different number of hyper parameters . softplus performs better in all model variations . sigmoid also outperforms softplus in terms of the number of parameters in the validation set and the f1 score in the multi - model setting . finally , the automatic parameters activation func . performs slightly better in the singlemodel setting than softplus , indicating that the pooling strategy is more effective in boosting the model performance .
1 shows the effect of using the shortest dependency path on each relation type . it can be observed that macro - averaged models achieve the best f1 ( in 5 - fold ) with sdp as well as the macro - adapted model doing the same thing . the comparison of our model with the strongest dependency path is shown in table 1 .
3 shows the performance of the three models compared to each other on the test set . in general terms , the results are slightly better than those of y - 3 because the training set is more aligned with the user .
3 presents the results on the wordparagraph level . our proposed method achieves state - of - the - art results in terms of f1 score . the results are presented in table 3 .
4 shows the c - f1 scores for the two indicated systems at the essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . paragraph , on the other hand , achieves a better performance overall .
3 shows the bleu score on the original and symmetric test sets . the results are shown in table 4 . original is better than the original , but still slightly worse than symmetrical test set . for the rouge - l test set , we used the best cleaned tgen and tgen + with a minimum of false positives . we also used sc - lstm as our test set for the sc - lexm dataset , which we conditioned on the fact that the word " original " had no effect on the results .
1 shows the results for the original and the cleaned versions . we report the number of distinct mrs , total number of textual references , and ser as measured by our slot matching script , see section 3 . the results are shown in table 1 . the original and our cleaned version have higher mrs and higher ser than the original .
3 shows the test set on the original and original datasets . our system performs better than both the original and the original on both datasets . we observe that the bleu score computed by term is significantly worse than the original due to the high overlap of original and tgen , hence leading to a drop in performance .
4 shows the results of manual error analysis on a sample of 100 instances from the original test set . the errors we found were caused by slight disfluencies in the training set ( i . e . , adding incorrect values , wrong values , etc . ) , but never caused a significant drop in accuracy ( e . g . , no miss , no miss ) .
model the performance of our dcgcn model is reported in table 1 . the first set of models shows that the hierarchical nature of the model can be improved with a reasonable selection of external and external keyphrases .
2 shows the results on amr17 . gcnseq achieves 24 . 5 bleu points and achieves 27 . 5 overall epmu points . the difference between our model size and the seq2seqb baseline is minimal , but significant .
3 shows the results for english - language and czech - language . our model outperforms the previous state - of - the - art models in both languages . the results are shown in table 3 . we observe that the single model performs better in english than the multi - step model , the difference in performance between single and single is minimal , however , the difference between the average bias metric and the average czech - language ranking is significant , considering the fact that the language - specific bow + gcn model relies on word embeddings instead of cvn , we observe that it is better to choose a single model than to rely on cvn .
5 shows the effect of the number of layers inside dc on the quality of the layers in table 5 . we observe that for every layer that has one layer , there are two more layers that have to be added to dc .
6 shows that gcn has residual connections with multiple baselines . rc + la denotes gcns with residual connections . we observe that dcgcn2 ( 27 ) shows very similar performance to the baseline gcn in terms of overall connections .
model f1 shows that dcgcn outperforms the previous state - of - the - art models in all three cases when the number of iterations is considered .
8 shows the ablation study results for amr15 . the results are shown in table 8 . - { i , 4 } dense blocks denotes removing the dense connections in the i - th block . - ( i , 3 ) 3 - dense blocks shows the diminishing returns from mixing the dense and the - th blocks .
shown in table 9 , the global encoder and the lstm decoder use the best performing dual encoder design . the ablation study shows that the multi - decoder design performs similarly to the original encoder design , with the exception of the single encoder .
investigate the effects of different initialization strategies on probing tasks . we show in table 7 the results for each initialization strategy that we base it on . our paper shows that our method obtains the best performance with a minimum of false start .
1 and table 2 summarize our results on the subtense and subtense subtasks . we observe that cbow / 400 has the best performance on both subtasks , while its performance on the other two subtasks is slightly worse .
1 shows the performance of our method compared to other methods . our cbow model outperforms all the alternatives except mpqa except for cbow / 784 , which obtains a 3 . 8 % improvement in mrpc score . it also outperforms both sst2 and sst5 in terms of recall score . we observe that the hybrid model performs better on both mrpc and sick - b tests . however , it has the advantage of training on a larger corpus , which underscores the competitiveness of subj . our model improves the recall score by 0 . 4 % compared to the previous state of the art .
3 shows the relative change with respect to hybrid on unsupervised downstream tasks attained by our models . our model obtains the best performance on sts12 , sts14 and sts15 compared to hybrid .
8 shows the performance for initialization strategies on supervised downstream tasks . our paper shows that our approach improves upon the state - of - the - art methods on mpqa and sst2 by 3 points .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the best performances are on the sts12 , sts15 and sts16 . the worst performance is on sts14 .
1 shows the performance of our method in depth and subtense . our cbow - r model outperforms both the previous models in both ways . in both cases , the difference in precision between the two methods is less pronounced .
subj and sick - r are comparable in terms of mrpc performance . however , contrary to popular belief , cbow - r performs better on mpqa and sst2 datasets . it has the worse performance on both mrpc and subj datasets , which suggests that the similarity between the two methods is due to different training set size .
3 shows the e + org scores of all systems trained on the same domain . our system obtains the best e + org scores and the best per scores . supervised learning ( mil - nd , mil - nd and term - nd ) achieve the best results with a minimum of error and a high e + misc score . we observe that the supervised learning approach , especially in [ italic ] and rep datasets , significantly outperforms the automatic learning approach by a margin of 2 . 5 - 2 . 5 points in both cases .
2 shows the results on the test set under two settings . our system achieves the best results with 95 % confidence intervals of f1 score . supervised learning achieves the highest e + p score and the best f1 scores are shown in table 2 . we observe that the automatic learning model , mil - nd , is more accurate than the supervised learning model in both cases . finally , the difference between the accuracy of the original and the simulated learning model is less pronounced in [ italic ] than in [ mil - nd ] . the difference in accuracy between the simulated and supervised learning models is minimal , however , this difference is not statistically significant in ( italic ) because the training model performs better in both settings .
6 shows the results of ref and ref compared to ref on the symmetric keyphrases . ref significantly outperforms ref when compared to gen , indicating that ref is better than ref alone . retrieving the ref scores from the original g2s - gat model can improve the generalization results for all models .
1 and table 2 summarize our results on the ldc2015e86 and ldc2017t10 datasets . our model outperforms the previous stateof - the - art models on both metrics , both in terms of bleu and eor .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are reported in table 3 . our model obtains a significantly better bleu score than the previous state of the art model .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . bilstm significantly boosts the performance of the model compared to other methods .
results are shown in table 4 . we observe that the comparison models generally have better performance on sentence length and sentence length compared to the baseline models . in particular , we notice that the g2s - gin model has the worse performance on sentences with average length and average sentence length .
shown in table 8 , the fraction of elements that are missing in the input graph that are present in the generated sentence ( g2s - gin ) , which shows that the use of token lemmas in the modeling can further improve the results for the test set .
4 shows the performance of our approach using the 4th nmt encoding layer . our model outperforms both the standard embeddings and pos tags with a large corpus ( 200k sentences ) . in fact , our model achieves the best performance with 96 % accuracy .
2 shows the pos and sem tagging accuracy with baselines and an upper bound . mft is the most frequent tag and word2tag is the second most frequently tag . note the lower bound on the upper bound indicating that the classifier relies on unsupervised embeddings .
3 shows the performance of our method on the word - level test set . our results tabulated at table 4 shows that our approach exceeds the previous state of the art methods on all metrics .
5 shows the accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we find that for bi , pos accuracy is 87 . 9 % and res accuracy 87 . 5 % .
8 shows the performance of an attacker on two different datasets . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 8 . in both datasets , the attacker has a significant advantage .
1 shows the accuracies when training directly towards a single task . for pan16 training , we trained with the word " task " .
2 shows the effect of the additional cost term on the balanced and unbalanced task averages . the classifiers trained on the pan16 dataset are able to detect both balanced & unbalanced data splits . sentiment and gender tags are particularly difficult to detect , since the imbalance in the balanced dataset is often caused by gender - neutral tags .
3 shows the performances on different datasets with an adversarial training set . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 3 . sentiment and gender are the most important factors in predicting whether an attacker will reach the task goal . in pan16 , the training set is based on gender - neutral training . sentiment alone gives a significant performance boost .
6 shows the performance of the embeddings for different encoders . embedding guarded is qualitatively different from the case when the protected attribute is added to rnn . similarly , embedded is more stable and therefore requires fewer instances to embed .
results are shown in table 4 . the first set of results show that our lstm model outperforms the competition on both base and finetune tasks . however , it does not achieve the best performance on both fronts when trained on the same dataset . finally , we observe that our model performs better on both datasets when trained with the same set of parameters . we observe that training on the wt2 dataset requires significantly more data than available on the other datasets , this is evident from the results of our second set of models , which we report in table 5 .
3 shows the performance of our model on the test set of hotpotqa in the distractor and fullwiki setting . our model obtains the best performance on both datasets with a minimum of time to train . when training with lstm , the model takes 3 . 5 seconds to train , and 3 . 3 seconds to complete the task . this model also achieves the best bleu score .
3 shows the evaluation results . our model improves upon the best state - of - the - art model by 4 . 36 points on average compared to yelppolar time . the results are summarized in table 3 .
3 shows the bleu score on wmt14 english - german translation task . our model improves upon the state - of - the - art gru by 0 . 2k in training time and by 1 . 5k in decoding one sentence .
4 shows the performance of our model on squad dataset . the results published by wang et al . ( 2017 ) show that our # params model can significantly improve match / f1 score over the base model . however , it still performs worse than other models because of the parameter number and the high f1 score . we observe that our model obtains the best performance when using only elmo features .
6 shows the f1 score of our model on conll - 2003 english ner task . it can be seen that all the models use the same parameter number in the task . however , only lstm * has the best performance .
7 shows the test results on snli with base + ln setting and test perplexity on ptb task with base setting . the results are shown in table 7 .
3 shows the results for word analogy task . word analogy tasks are presented in table 4 . all the word analogy tasks use the word " retrieval " . system retrieval is beneficial for both human and machine learning tasks . the word " evaluation " task is particularly beneficial for the system , with the maximum number of iterations taking place at the level of 1 . 11 . in general terms , the word ' evaluation ' task performs better than the other two methods . when using the combination of word analogy and sentence embeddings , the system achieves the best results with a minimum of error on both tasks .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . our system ranked in top 1 or 2 for overall quality . the results of retrieval are shown in table 4 .
3 shows the results for english and french . our joint model outperforms all the base lines except for the one that embeds the word " doc " . among all the test sets , we see that our joint model performs better on both datasets , with the exception of docsub . europarl is slightly worse than df and docsub , both in terms of accuracy and target weight .
3 shows the results for english and french . our joint model outperforms all the base lines except for the one that we chose for embeddings . the results are broken down in table 3 . for english , our model performs slightly worse than the other two baselines on three of the four datasets . the difference is most pronounced on the df dataset , where europarl and librisparl both perform slightly better than the others .
3 shows the results for english and french . our joint model outperforms all the base lines except for the one that we chose for embeddings . the results are broken down in table 3 . for english , our model performs slightly worse than the other two baselines on three of the four datasets . the difference is most prevalent on docsub , where europarl and its variants perform slightly better than the others .
3 shows the roots scores for corpus and europarl . our joint model improves upon the best baseline by 1 . 78 points on each metric compared to the previous state - of - the - art . our model is better than both the baseline and the maxdepth metrics on both metric . we find that our joint model achieves a better depthcohesion score than the other two baselines . this confirms our hypothesis that the semantic quality of the word embeddings can be improved with a high degree of accuracy .
3 shows the roots scores for corpus and europarl . our system obtains the best performance on both datasets . our joint model exceeds the baseline on both metrics with a significant improvement on the depthcohesion metric . we also exceed the maxdepth metric by 1 . 31 points on both metric , which shows that our joint model is better than both the baseline and the new metric . our model achieves the best score with a gap of 2 . 42 points on the metric compared to the previous state of the art .
experimental results are shown in table 1 . the enhanced version of lf outperforms the enhanced version we used in the experiments of applying our principles on the validation set of visdial v1 . 0 . we notice significant performance drop in the performance between qt and r2 compared to lf , indicating that the additional cost term is important to boosting the quality of the model .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the validation set of visdial v1 . 0 . using only p2 indicates the most effective one ( i . e . , hidden dictionary learning ) . note that only applying p2 with the history shortcut can improve the results .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . our model significantly outperforms the strong lemma baseline on hard alignments and fi - en in both cases .
3 presents the results of our approach on the test set of ruse + sent - mover . the results are summarized in table 3 . our approach outperforms both the baseline and the bertscore - f1 score by a noticeable margin .
3 presents the bagel and sfhotel scores on the test set of hotpotqa . the results are summarized in table 3 . the baseline bleu - 1 improves significantly over the baseline on both sets with a significant improvement in bertscore - f1 score .
performance of the models according to these baselines is reported in table 4 . the results are summarized in bold . the summaries displayed in bold indicate that the semantic information injected into the models are significant enough to result in measurable improvement . however , the leic score ( p < 0 . 001 ) consistently shows very high performance even when using elmo and bert as the baseline . the morph - mover model significantly outperforms the baseline on both m1 and m2 .
results are shown in table 4 . all the models trained on sim outperform the baseline on all metrics except for the one that was trained on the shen - 1 dataset . the results are slightly worse than those on sim , indicating that the importance of word embeddings in the initialization setup .
results are shown in table 4 . we observe that the transfer quality and transfer quality scores are the most important components in the semantic preservation and semantic preservation tasks . semantic preservation is further improved with the addition of semantic tags . the semantic preservation scores are significantly better than those of the other two baselines , indicating that semantic preservation is more important for semantic preservation . yelp is particularly sensitive to semantic preservation ,
5 shows the human evaluation results . we use the [ italic ] ρ b / w negative pp and human ratings of fluency . these results are shown in table 5 . the results of human evaluation show that the accuracy obtained by using these metrics is significantly higher than the rate obtained by machine .
results are shown in table 4 . all the models trained on sim outperform the baseline on all metrics except for the one that was trained on the shen - 1 dataset . in particular , we see that the m0 model performs better than the m1 model on both datasets .
6 shows the results on yelp sentiment transfer , where bleu is between 1000 and 1000 sentences and human references are restricted to 1000 words . our best model achieves the highest acc ∗ score ( 31 . 4 % higher than yang2018unsupervised ) . however , this improvement is not significant because the classifiers in use are worse than those in the previous work .
2 shows the true number of repetition tokens that were correctly predicted as disfluencies . the statistics for nested disfluency are shown in table 2 .
3 shows the relative frequency of rephrases predicted as disfluent for both the reparandum and the repair ( table 3 ) . the fraction of tokens predicted to contain a content word is in parentheses , indicating that the function contains a lot of content word . we found that the percentage of tokens that contain the content word was significantly higher than the percentage predicted as containing the word .
results are shown in table 4 . we observe that the text + innovations model outperforms the single model in terms of dev and test scores . in particular , we observe that when text + raw is used in combination with innovations , the model performs better in the low - supervision settings . text + innovations also improve the model ' s performance in the high - vision settings ,
2 shows the performance of our model with the state - of - art algorithms on the fnc - 1 test dataset . the results are shown in table 2 . our model achieves the best performance in the two scenarios .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the unified model significantly outperforms all previous models .
3 shows the performance of our method in the context of word attention and graph attention . accuracy ( % ) shows that neuraldater performs better than both the original method and the graph attention model .
3 shows the performance of our model on the test set of hotpotqa in the distractor and multi - stage setting . our model obtains the best performance on both test set and on the production set . the results are presented in table 3 . embedding + t models perform better than trigger , dmcnn and jrnn , but do not outperform the baseline on all test set .
3 shows the method ' s performance on the single - event test set . all the methods used for this data set are shown in table 4 . in all but one case , the error threshold for each case is significantly higher than the threshold for all the other methods . both the method and the classifier used to solve the event are completely automatic .
can be seen in table 4 the results for english and spanish , respectively . all except for fine - tuned - lm are better than the original spanish - only - lm model .
4 shows the results on the dev set and the test set using discriminative training with only subsets of code - switched data . we observe that fine - tuning achieves the best results with a 25 % train dev and 50 % test set .
5 shows the performance on the dev and test set compared to fine - tuned - disc . in particular , we see that fine - tuned rccs outperforms monolingual rccs in both cases .
7 shows the precision and f1 scores for using type - aggregated gaze features trained on the three eye - tracking datasets . the results are shown in table 7 . precision ( p ≤ 0 . 005 ) is significantly better than f1 - score on the conll - 2003 dataset .
5 shows precision , recall and f1 - score for using type - aggregated gaze features on the conll - 2003 dataset . the improvement in precision is statistically significant ( p < 0 . 01 ) with type combined features , indicating that the use of these features improves the recall function .
results on belinkov2014exploring ’ s ppa test set . the hpcd approach relies on syntactic embeddings obtained by applying autoextend rothe and schütze ( 2015 ) to wordnet 3 . 1 . however , it uses syntactic skipgram instead of glove - retro . the results on the original paper are shown in table 1 . syntactic - sg vectors are used to embed wordnet vectors with syntactic tags . wordnet has the advantage of being more stable , but it has the disadvantage of being able to reuse the same synset embedding from previous works . we notice a drop in performance between the original and the development set of wordnet , it is clear from the results that adapting the semantic embedding to the syntactic ones hurts wordnet performance .
2 shows the results from the rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results from table 2 show that when using oracle pp as the dependency parser , rbg achieves 94 . 60 ppa acc . and 98 . 97 ppa accuracy . oracle pp also helps the system to interpret pp attachments more effectively .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . we report the ppa acc . ( normalized by the number of frames ) in table 3 . it is clear from the table that removing context sensitivity and sensitivity hurts the model negatively .
2 : adding subtitle data and domain tuning for image caption translation ( marian amun et al . , 2018 ) with subsfull embeddings . we find that combining domain - tuned and multi30k decoding improves interpretability across multiple datasets .
3 shows the performance of subs1m in the en - de setup . the models trained on flickr16 and mscoco17 are slightly outperform the models usingdomain - tuned h + ms - coco on both datasets . the results are summarized in table 3 .
4 shows the bleu scores in terms of the automatic captions added after adding the best ones or all 5 models . as expected , the results are very bad when using only single captions . however , when using multi30k as the decoder , we get a slight improvement ( 7 . 8 % ) over the previous state of the art .
5 compares the strategies for integrating visual information with our en - de embeddings . we use transformer , multi30k + subs3mlm , detectron mask surface and mscoco17 decoding schemes . the results are summarized in table 5 . enc - gate and dec - gate decoding have a generally positive effect ( bleu % scores ) on visual information , but we see a drop in performance when we switch to pure captions instead of enc - gate .
3m performs better than subs3m in terms of text - only and multi - lingual modes . subsequent improvements on the flickr16 and mscoco17 datasets are shown in table 4 . moreover , the improvements on both datasets are larger than those on the en - de dataset . adding the visual features boosts performance , but does not improve the accuracy of the subs2m model .
3 shows the performance of all models when en - fr is trained and tested on the mtld dataset . the results are summarized in table 3 . we observe that the translation performed better on mtld than the original one , indicating that the semantic information injected into the lexical information by the additional cost term is important to the model .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 shows the performance of our method in terms of training vocabularies for english , french and spanish . the results are summarized in table 2 . our model obtains the best performances in each language .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) and ter measures are shown in table 5 . we observe that the two systems significantly outperform each other in terms of rev performance .
2 shows the performance of our visually supervised model compared to the standard rsaimage model from flickr8k . the results are shown in table 2 . the difference in performance is statistically significant with a 0 . 2 % absolute difference .
results on synthetically spoken coco are shown in table 1 . the visually supervised model outperforms the similarly supervised audio2vec - u model in terms of recall @ 10 and mean mfcc score .
1 shows the results for each classifier compared to the original on sst - 2 . for example , orig ( which is used for embeddings ) turns in a < u > screenplay that has edges edges edges and curves . for cnn , the edges edges are edges and the curve is in the lower case . for the three other languages , we use rnn and rnn as verbs .
2 shows the results for part - of - speech ( pos ) changes in sst - 2 . the results indicate that fine - tuning has not resulted in significant improvement in the quality of the word .
3 shows the changes in sentiment as well as the percentage points in f1 from positive to negative . the results indicate that the switch to positive sentiment has a positive effect on sentiment .
results are presented in table 2 . we observe that the competitive advantage of sift over libris - sst - 2 is significant , however it is less pronounced for pubmed . table 2 also highlights the competitiveness of our method with respect to word embeddings . as expected , our approach significantly improves upon the strong lemma baseline on the pubmed test set . in addition , our joint model significantly boosts the ppmi by 10 % ( p < 0 . 001 ) .
