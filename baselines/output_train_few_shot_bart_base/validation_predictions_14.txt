2 shows the performance of our recursive approach on the large movie review dataset compared to the iterative approach , which performs better on training .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , when the batch size increases from 1 to 25 , it does not improve as well as the linear dataset .
2 shows the performance of the max pooling strategy for each model with different representation . the average number of parameters is 1 . 66e - 01 and the number of feature maps are 1 . 79e - 03 . the maximum pooling approach consistently performs better in all model variations . the hgn outperforms both the hgn and hgn based on the original hgn implementation .
1 shows the effect of using the shortest dependency path on each relation type . with sdp as dependency path , our model achieves the best f1 ( in 5 - fold ) and the best diff . with the sdp dependency path . in the macro - averaged setting , we can see that our model obtains a better f1 score with sdp than with the baselines .
3 shows the performance of the y - 3 model compared to the previous state - of - the - art models .
3 presents the results of our method in terms of paragraph level . the results are presented in table 3 . our method outperforms all the methods except mst - parser except for the one that achieves the highest f1 score .
4 shows the c - f1 scores for the two indicated systems ; the lstm - parser shows lower performance than the other systems .
3 shows the bleu score on the original and false test sets . the original scores are slightly better than the original scores on the false test set , but still slightly worse than the original scores .
1 compares our original and our cleaned e2e datasets with the original ones ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . we find that the original and the cleaned versions have the highest percentage of mrs and the average number of concatenated mrs is higher than the original .
3 shows the bleu score on the original and false test sets . the original scores are slightly better than the original scores , but still slightly worse than the correct ones . for the false test set , we use the original score of tgen + with a slight improvement .
4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . we found a total of 22 errors in the original training set , and a slight disflacement in the disfluency analysis set .
3 shows the performance of our approach on the external and external datasets compared to the previous state - of - the - art approaches . our approach outperforms all the other approaches except for seq2seqk ( konstas et al . , 2017 ) and tree2str ( lanigan et al . 2018 ) .
2 : main results on amr17 are shown in table 2 . our model achieves 24 . 5 bleu points in terms of parameters , compared to the previous best state - of - the - art model , ggnn2seqb .
3 shows the english - german b and english - czech c scores . the results are presented in table 3 . the best performing model is bow + gcn ( bastings et al . , 2018 ) . the best results are in english - korean , where the average b and average c scores are significantly higher than those in german . we also observe that the best performing single model is birnn , which is based on the same corpus . in addition , the best performance is achieved in german , with a gap of 3 . 5 points between the two languages .
5 : the effect of the number of layers inside dc is shown in table 5 . the first layer of layers in dc has the most significant effect on the performance of the layer with the least layers . the second layer , m , has the largest effect , with a 3 . 6 layer overall .
6 : comparisons with baselines . + rc denotes gcns with residual connections . gcn + rc ( 2 ) shows that the gcn has residual connections with the baselines , as shown in fig . 6 .
3 shows the performance of our dcgcn model compared to other models . the results are summarized in table 3 .
8 shows the ablation study results for amr15 with respect to the number of connections in the i - th block . the results are shown in table 8 . - { i , 4 } dense blocks denote removing the dense connections in amr14 , whereas - { 4 } dense block denotes removing the layers of connections that are not dense .
9 shows the ablation study for the graph encoder and the lstm decoder . the results are presented in tables 9 and 10 . the best results are obtained by combining the two aspects of the graph attention and the semantic attention .
7 shows the results for initialization strategies on probing tasks . our paper shows that our method obtains the best performance when the gap is less than the gap between the bshift and subjnum tokens .
3 presents the results of our method on the subtraction and relation subtraction tasks . the results are summarized in table 3 . the best performing method is h - cbow / 400 , which has the best performance on both subtraction metrics . it also has the highest correlation with relation length and relation length , which shows the effectiveness of the method .
3 shows the performance of our model on the mrpc test set . our model outperforms all the other baselines except for the one that has the best mrpc score . it also outperforms both the sst2 and sst5 scores on both tests .
3 shows the relative change in performance with respect to cmp compared to hybrid . cbow shows a slight improvement over the baseline performance on unsupervised downstream tasks attained by our models .
8 shows the performance of initialization strategies on supervised downstream tasks . our paper shows that the best initialization strategies are sst2 and sst5 , while the best sst1 performs better on mpqa .
6 shows the performance for different training objectives on unsupervised downstream tasks . the best performance is on the sts13 task , where the cbow - r model outperforms both the other two methods .
results are shown in table 3 . the best performing method is cbow - r , which obtains the highest accuracy on the subjnum and the topconst metrics . it achieves the best performance on the subjnum metric as well as on the topconst metric , with a 2 . 2 % improvement on the overall score .
3 shows the performance of subj and sick - r on the mrpc test set . the results are summarized in table 3 . subj outperforms all the other methods except for the one that has the better mrpc score . on the sst2 test set , subj performs better than both the other two methods . however , it has the advantage of outperforming both sick and sst3 on both mrpc tests .
3 shows the e + and per scores of our system in table 3 . name matching is the most difficult part of the system to do , with a drop of 3 points in org and a drop in per scores from the previous state of the art . our system obtains the best e + org scores in [ italic ] e + per scores and is comparable to the best state - of - the - art systems in both systems . our model obtains an e + org score of 57 . 86 and a per score of 48 . 86 .
2 shows the results on the test set under two settings . name matching ( model 1 ) and supervised learning ( model 2 ) show 95 % confidence intervals of f1 scores in [ italic ] e + p < 0 . 72 and 82 % f1 score in supervised learning , respectively . in both settings , the system performs better than both the model 1 and the model 2 , showing the high precision of the model in both settings .
6 : entailment ( ent ) and ref ( g2s - gat ) are the most distinctive features of the model compared to those of s2s and g2s .
results are shown in table 3 . the best performing models are the ldc2015e86 and ldc2017t10 , respectively .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 .
results of the ablation study on the ldc2017t10 development set are shown in table 4 .
results are shown in table 3 . we observe that the average number of frames per sentence is slightly larger than the average length of the sentence , indicating that the model is better at predicting sentence length and sentence length . the g2s - gin model also shows lower precision , which indicates that it is better to predict sentence length than sentence length , and therefore less likely to give incorrect answers .
shown in table 8 , the fraction of elements in the output that are missing in the input ( g2s - gin ) that are not present in the generated sentence ( miss ) . this shows the importance of token lemmas in the production sentence .
4 shows the accuracy of our approach with respect to target languages . our approach obtains the best performance on a smaller corpus ( 200k sentences ) than the one using the 4th nmt layer .
2 shows the pos and sem accuracy with baselines and an upper bound . the results are shown in table 2 . word2tag is the most frequent classifier , followed by unsupemb .
results are presented in table 4 . the results are summarized in terms of accuracy and precision . our model outperforms all the other methods except for the one that we used in the previous section .
5 shows the accuracy with features from different layers of the uni / bidirectional / residual nmt encoders , averaged over all non - english target languages .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ .
1 : accuracies when training directly towards a single task . for pan16 , the training data are shown in table 1 . the training data show that the word " sentiment " has the greatest effect on the performance .
2 shows the effect of classifiers on the balanced and unbalanced task averages . when classifiers are included in the conversation , the effect is less pronounced than when they are not included . this shows that classifiers have a generally positive effect on the task performance .
3 shows the performance on different datasets with an adversarial training . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 3 . sentiment and gender are the most important factors in predicting whether an object will belong to a given class or not .
6 : accuracies of the protected attribute with different encoders . the rnn embeddings perform better than the embedded ones .
3 shows the performance of our lstm model compared to the previous state - of - the - art models . the results are summarized in table 3 . our model outperforms both the original wt2 model and the finetune model by a significant margin . in addition , our model achieves the best performance on both the wt2 and wt2 datasets . finally , it achieves the highest performance on the two datasets , with a slight improvement on the final dataset . we also observe that our approach obtains the best results on both datasets .
3 shows the performance of our lstm model compared to the previous state - of - the - art model . the results are summarized in table 3 . we show that our model has the best performance when trained on a single dataset . when trained on multiple datasets , the time taken to train is significantly shorter than that of the previous model .
results are shown in table 4 . the lstm model outperforms all the other models in terms of err and time . we observe that the average time taken to compute the amapolar time is slightly better than the average yahoo time .
3 shows the bleu score on wmt14 english - german translation task . our model outperforms all the other models in terms of decoding time , with the only exception of sru , which is better at german translation .
4 : exact match / f1 score on squad dataset with respect to the parameter number of # params and the number of parameters . the results published by wang et al . ( 2017 ) are shown in table 4 . as expected , the lstm model obtains a better match / score than the sru model . in addition , it obtains an average f1 score of 69 . 83 / 83 . 83 . 12 and an average score of 75 . 06 / 82 . 76 .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number in the reported result , and sru denotes the result . it is clear from table 6 that the use of # params leads to better performance .
shown in table 7 , the lrn performance on snli task with base + ln setting and test perplexity on ptb task with base setting setting .
2 shows the performance of the word " sent " in relation to system retrieval . the word " sent " is used to describe the type of attention generated by the system when the attention is applied to the system . it is clear from the results that the word ' sent ' is beneficial for both human and machine learning . as expected , word - based attention is beneficial , as it helps the system to learn more about the task at hand ( e . g . , when it is used in the context of the task ) . in general terms , the word attention ( mtr ) is beneficial in both cases , improving the overall performance of both systems . in particular , it helps to distinguish between human attention and the machine learning attention . when attention is directed towards the system , it is beneficial to inform the system about the results of the attention being applied .
4 : human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all systems is reported in table 4 , with a 2 . 3 % overall improvement over the best performance .
results are shown in table 4 . the results are presented in table 5 . the best performing embeddings are the ones on the df and docsub datasets , while the worst ones are on the docsub dataset . for the df dataset , we see that both the original " ted talks " and " tables " are better performing than the others .
results are shown in table 3 . the results are presented in table 4 . the best performing embeddings are the ones from corpus and docsub , while the worst ones are those from docsub . these are the only ones that do not rely on word - of - speech features .
results are shown in table 4 . the results are presented in table 5 . the best performing embeddings are the ones from corpus and docsub , while the best ones are from europarl . the worst performing ones are those from df , docsub and hclust .
are shown in table 3 . our approach achieves the best results with a minimum of 3 . 5roots and a maximum of 1 . 8roots . we observe that our approach obtains the best result with the smallest gap between the maxdepth and maxdepth of our model , which is comparable to that of docsub .
3 presents the results of our joint model . our joint model achieves the best results on both metric metrics . the results are summarized in table 3 . the best results are obtained by applying maxdepth and maxdepthcohesion on the two datasets , respectively . our model outperforms both the df and docsub datasets by a significant margin .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf is the enhanced version of the original visdial model , and r2 is the weighted softmax loss , respectively .
2 shows the performance ( ndcg % ) of different ablative studies on different models on visdial v1 . 0 validation set . the best performing model is the one using the hidden dictionary learning method .
5 compares our hmd - recall and wmd - prec models with soft alignments . the results are summarized in table 5 . the hmd models outperform both the soft and hard alignments by a significant margin .
3 presents the results of the direct assessment and bertscore - f1 measures . the results are summarized in table 3 . the best performing baselines are ruse ( ruse ) and f1 ( f1 ) . the most striking thing about these metrics is that their performance is significantly worse than those of other baselines . for example , on the one hand , there is a slight improvement ( p < 0 . 001 ) on the direct achiever metric , while on the other hand , the improvement is modest ( p > 0 . 01 ) .
3 presents the bagel and sfhotel scores on the validation set . the results are presented in table 3 . the bleu - 2 scores are significantly better than the baseline bertscore - f1 scores on both sets .
3 presents the metric and baseline scores of the models trained on the m1 and m2 datasets . the results are summarized in table 3 . the leic scores are presented in bold , indicating that the model is well - equipped to handle the task at hand . the bertscore - recall scores are reported in tables 3 and 4 , respectively . the average bert score is 0 . 939 on m2 and 0 . 749 on m1 datasets .
3 shows the performance of the models trained on sim2 and sim3 . the results are summarized in table 3 . the m1 model outperforms both the m2 model and the m3 model in terms of scalability .
3 presents the results of our model on the semantic preservation and semantic preservation datasets . the results are summarized in table 3 . we show that our model has the best overall performance on both datasets . for semantic preservation , we have the best performance with a 2 . 5 / 3 . 5 overall improvement over the previous state of the art model . as expected , the results are slightly better than those of the other two models .
5 presents the results of human sentence - level validation . the results are summarized in table 5 . we show that the human ratings of semantic preservation are higher than the machine ratings , indicating that the quality of the sentence is better than the accuracy of the judgement .
results are shown in table 4 . in particular , we observe that the m0 model performs better than the m1 model on both sim and pp datasets .
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu ( acc ∗ ) and the highest acc ∗ ( normalization ) score ( from 1000 to 1000 ) . however , our best model performs worse than the others in the classifiers used in the previous work . this is due to different classifiers in use , such as domain embedding and domain name embedding .
2 shows the number of tokens that are correctly predicted to be disfluent . reparandum length is reported in table 2 , while repetition length is in the range of 1 - 2 .
3 : relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - content ) . table 3 shows the fraction of tokens predicted to contain a word in either the reparandum or the repair , and the fraction predicted as a function in both cases .
results are shown in table 4 . the best results are obtained by combining text and innovations with dev data . text + innovations significantly improve the dev score by 0 . 2 on the single test , while text + text + innovations achieve the best dev score . in addition , the improvement by 1 . 1 on the multi - sample test is due to the higher quality of the text + raw data .
2 compares our model with the state - of - art algorithms on the fnc - 1 test dataset . our model achieves the best performance when we agree to disagree with a topic and discuss it with a co - author .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the attentive neuraldater significantly outperforms all previous methods except burstysimdater .
3 shows the performance of our method with and without attention . it is clear from table 3 that it is effective for word attention and graph attention , as shown in the table .
3 shows the performance of our model in each stage . our model outperforms the state - of - the - art on all stages except the argument stage , where it performs better than the other models .
2 presents the results of our method with respect to identification and classification . our method obtains a significant advantage over the traditional method due to the fact that it has a relatively high correlation with event identification . in particular , it has the advantage of having a better understanding of event classification .
results are shown in table 4 . all models trained on the spanish - only - lm outperform all the models except for the ones trained on english - only .
4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data in the training set . in both cases , fine - tuning achieves the best results .
5 shows the performance on the dev set and the test set , compared to the monolingual model .
7 shows the precision ( p ) , recall ( f1 ) and f1 scores for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset .
5 shows the precision ( p ) , recall ( f1 ) and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset .
results on the original wordnet test set are shown in table 1 . the hpcd approach relies on syntactic - sg embeddings and relies on the semantic skipgram embedding obtained by the original research paper . it uses syntactic sg embedding as the base embedding for wordnet and wordnet 3 . 1 . as expected , it requires a significant amount of syntacticsg embedding to achieve the best performance . the results are summarized in fig . 1 .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model .
2 : adding subtitle data and domain tuning for image caption translation ( bleu % scores ) with the help of subsfull embeddings . the results are summarized in table 2 . adding the domain - tuned data with the multi30k dataset improves the bleu % score by 3 . 5 points over the domain tuning baseline .
3 shows the performance of the subs1m model compared to the subs2m model in the en - de setting . the results are summarized in table 3 . subdomain - tuned models perform better than the other models , with the exception of mscoco17 , which shows a slight improvement in performance .
4 shows bleu scores in terms of automatic captions ( the best one or all 5 ) . the results are summarized in table 4 . as expected , the multi30k model outperforms all the other models except for the one with the best image captions .
5 shows the performance of our approach with respect to en - de and dec - gate . the results are summarized in table 5 . we observe that the enc - gate approach outperforms the other approaches in terms of bleu % scores .
3 shows the performance of subs3m compared to subs6m in the en - de setting . sub3m outperforms the other models in terms of multi - lingual performance , with the exception of mscoco17 , which is closer to the german - speaking set . the performance of the " intensemble - of - 3 " model is slightly better than the " entities " model .
results are shown in table 3 . we observe that the en - fr - ht model outperforms the other two models in terms of translation performance . in particular , it obtains the best performance on mtld and en - es - ht .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 : training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) show that the re - rev systems are better than the en - fr - rnn - rev baseline .
2 shows the vgs performance on flickr8k . the results are shown in table 2 . the vgs is the visually supervised model from chrupala2017representations .
results on synthetically spoken coco are shown in table 1 . the vgs model achieves the best performance with a 10 . 5 % recall rate .
1 shows the results of the different classifiers compared to the original on sst - 2 . for example , cnn turns in a < u > screenplay that has edges at the edges ; it ’ s so clever you want to hate it . rnn shows similar results . similarly , dan shows that when cnn turns on a on ( in the margins of a screenplay ) , it has edges edges edges .
2 : part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively .
3 : sentiment changes in sst - 2 . the results are shown in table 3 . positive labels are flipped to positive , and negative labels are also flipped to negative .
results are presented in table 3 . the results are summarized in table 4 . in general , the results are positive , while the negative ones are statistically significant ( p < 0 . 001 ) . in particular , the performance of sift is strongly positive , indicating that it is able to distinguish between positive and negative aspects of a research report .
